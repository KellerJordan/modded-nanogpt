import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = (16 / 8) * 0.8
    if x > 0.66:
        lr_max = (24 / 8) * 0.8
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Sun Nov 30 02:40:22 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   36C    P0            124W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0            109W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   34C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2160 train_time:93ms step_avg:92.98ms
step:2/2160 train_time:133ms step_avg:66.41ms
step:3/2160 train_time:152ms step_avg:50.81ms
step:4/2160 train_time:173ms step_avg:43.26ms
step:5/2160 train_time:199ms step_avg:39.75ms
step:6/2160 train_time:294ms step_avg:49.05ms
step:7/2160 train_time:313ms step_avg:44.65ms
step:8/2160 train_time:341ms step_avg:42.60ms
step:9/2160 train_time:375ms step_avg:41.64ms
step:10/2160 train_time:408ms step_avg:40.76ms
step:11/2160 train_time:442ms step_avg:40.17ms
step:12/2160 train_time:475ms step_avg:39.57ms
step:13/2160 train_time:509ms step_avg:39.16ms
step:14/2160 train_time:542ms step_avg:38.72ms
step:15/2160 train_time:577ms step_avg:38.44ms
step:16/2160 train_time:610ms step_avg:38.11ms
step:17/2160 train_time:644ms step_avg:37.86ms
step:18/2160 train_time:677ms step_avg:37.59ms
step:19/2160 train_time:713ms step_avg:37.51ms
step:20/2160 train_time:744ms step_avg:37.19ms
step:21/2160 train_time:778ms step_avg:37.06ms
step:22/2160 train_time:811ms step_avg:36.88ms
step:23/2160 train_time:845ms step_avg:36.75ms
step:24/2160 train_time:878ms step_avg:36.59ms
step:25/2160 train_time:912ms step_avg:36.50ms
step:26/2160 train_time:945ms step_avg:36.35ms
step:27/2160 train_time:980ms step_avg:36.28ms
step:28/2160 train_time:1013ms step_avg:36.16ms
step:29/2160 train_time:1047ms step_avg:36.09ms
step:30/2160 train_time:1080ms step_avg:35.98ms
step:31/2160 train_time:1114ms step_avg:35.94ms
step:32/2160 train_time:1147ms step_avg:35.84ms
step:33/2160 train_time:1181ms step_avg:35.80ms
step:34/2160 train_time:1215ms step_avg:35.73ms
step:35/2160 train_time:1250ms step_avg:35.70ms
step:36/2160 train_time:1283ms step_avg:35.63ms
step:37/2160 train_time:1318ms step_avg:35.62ms
step:38/2160 train_time:1351ms step_avg:35.56ms
step:39/2160 train_time:1386ms step_avg:35.53ms
step:40/2160 train_time:1419ms step_avg:35.47ms
step:41/2160 train_time:1454ms step_avg:35.45ms
step:42/2160 train_time:1487ms step_avg:35.40ms
step:43/2160 train_time:1521ms step_avg:35.38ms
step:44/2160 train_time:1554ms step_avg:35.32ms
step:45/2160 train_time:1589ms step_avg:35.30ms
step:46/2160 train_time:1622ms step_avg:35.26ms
step:47/2160 train_time:1657ms step_avg:35.25ms
step:48/2160 train_time:1690ms step_avg:35.21ms
step:49/2160 train_time:1724ms step_avg:35.18ms
step:50/2160 train_time:1757ms step_avg:35.14ms
step:51/2160 train_time:1791ms step_avg:35.12ms
step:52/2160 train_time:1824ms step_avg:35.08ms
step:53/2160 train_time:1859ms step_avg:35.07ms
step:54/2160 train_time:1892ms step_avg:35.04ms
step:55/2160 train_time:1926ms step_avg:35.02ms
step:56/2160 train_time:1959ms step_avg:34.98ms
step:57/2160 train_time:1993ms step_avg:34.97ms
step:58/2160 train_time:2026ms step_avg:34.94ms
step:59/2160 train_time:2060ms step_avg:34.92ms
step:60/2160 train_time:2094ms step_avg:34.89ms
step:61/2160 train_time:2127ms step_avg:34.88ms
step:62/2160 train_time:2160ms step_avg:34.84ms
step:63/2160 train_time:2195ms step_avg:34.84ms
step:64/2160 train_time:2228ms step_avg:34.82ms
step:65/2160 train_time:2263ms step_avg:34.81ms
step:66/2160 train_time:2296ms step_avg:34.78ms
step:67/2160 train_time:2330ms step_avg:34.78ms
step:68/2160 train_time:2363ms step_avg:34.75ms
step:69/2160 train_time:2397ms step_avg:34.74ms
step:70/2160 train_time:2430ms step_avg:34.72ms
step:71/2160 train_time:2464ms step_avg:34.71ms
step:72/2160 train_time:2498ms step_avg:34.69ms
step:73/2160 train_time:2532ms step_avg:34.68ms
step:74/2160 train_time:2565ms step_avg:34.66ms
step:75/2160 train_time:2599ms step_avg:34.66ms
step:76/2160 train_time:2632ms step_avg:34.63ms
step:77/2160 train_time:2666ms step_avg:34.63ms
step:78/2160 train_time:2699ms step_avg:34.61ms
step:79/2160 train_time:2734ms step_avg:34.61ms
step:80/2160 train_time:2767ms step_avg:34.58ms
step:81/2160 train_time:2801ms step_avg:34.58ms
step:82/2160 train_time:2834ms step_avg:34.56ms
step:83/2160 train_time:2869ms step_avg:34.56ms
step:84/2160 train_time:2902ms step_avg:34.54ms
step:85/2160 train_time:2936ms step_avg:34.55ms
step:86/2160 train_time:2970ms step_avg:34.53ms
step:87/2160 train_time:3004ms step_avg:34.52ms
step:88/2160 train_time:3037ms step_avg:34.51ms
step:89/2160 train_time:3071ms step_avg:34.50ms
step:90/2160 train_time:3104ms step_avg:34.48ms
step:91/2160 train_time:3138ms step_avg:34.48ms
step:92/2160 train_time:3171ms step_avg:34.47ms
step:93/2160 train_time:3205ms step_avg:34.47ms
step:94/2160 train_time:3239ms step_avg:34.45ms
step:95/2160 train_time:3273ms step_avg:34.45ms
step:96/2160 train_time:3306ms step_avg:34.43ms
step:97/2160 train_time:3340ms step_avg:34.43ms
step:98/2160 train_time:3373ms step_avg:34.42ms
step:99/2160 train_time:3407ms step_avg:34.41ms
step:100/2160 train_time:3440ms step_avg:34.40ms
step:101/2160 train_time:3475ms step_avg:34.41ms
step:102/2160 train_time:3508ms step_avg:34.39ms
step:103/2160 train_time:3542ms step_avg:34.39ms
step:104/2160 train_time:3576ms step_avg:34.38ms
step:105/2160 train_time:3610ms step_avg:34.38ms
step:106/2160 train_time:3643ms step_avg:34.37ms
step:107/2160 train_time:3677ms step_avg:34.36ms
step:108/2160 train_time:3710ms step_avg:34.35ms
step:109/2160 train_time:3744ms step_avg:34.35ms
step:110/2160 train_time:3777ms step_avg:34.34ms
step:111/2160 train_time:3812ms step_avg:34.34ms
step:112/2160 train_time:3845ms step_avg:34.33ms
step:113/2160 train_time:3879ms step_avg:34.33ms
step:114/2160 train_time:3912ms step_avg:34.32ms
step:115/2160 train_time:3946ms step_avg:34.32ms
step:116/2160 train_time:3979ms step_avg:34.30ms
step:117/2160 train_time:4014ms step_avg:34.31ms
step:118/2160 train_time:4047ms step_avg:34.30ms
step:119/2160 train_time:4081ms step_avg:34.30ms
step:120/2160 train_time:4115ms step_avg:34.29ms
step:121/2160 train_time:4149ms step_avg:34.29ms
step:122/2160 train_time:4182ms step_avg:34.28ms
step:123/2160 train_time:4216ms step_avg:34.27ms
step:124/2160 train_time:4249ms step_avg:34.27ms
step:125/2160 train_time:4283ms step_avg:34.26ms
step:126/2160 train_time:4316ms step_avg:34.25ms
step:127/2160 train_time:4350ms step_avg:34.25ms
step:128/2160 train_time:4383ms step_avg:34.25ms
step:129/2160 train_time:4417ms step_avg:34.24ms
step:130/2160 train_time:4451ms step_avg:34.24ms
step:131/2160 train_time:4485ms step_avg:34.24ms
step:132/2160 train_time:4518ms step_avg:34.23ms
step:133/2160 train_time:4553ms step_avg:34.23ms
step:134/2160 train_time:4586ms step_avg:34.22ms
step:135/2160 train_time:4620ms step_avg:34.22ms
step:136/2160 train_time:4653ms step_avg:34.21ms
step:137/2160 train_time:4687ms step_avg:34.21ms
step:138/2160 train_time:4720ms step_avg:34.20ms
step:139/2160 train_time:4754ms step_avg:34.20ms
step:140/2160 train_time:4788ms step_avg:34.20ms
step:141/2160 train_time:4822ms step_avg:34.20ms
step:142/2160 train_time:4855ms step_avg:34.19ms
step:143/2160 train_time:4889ms step_avg:34.19ms
step:144/2160 train_time:4922ms step_avg:34.18ms
step:145/2160 train_time:4956ms step_avg:34.18ms
step:146/2160 train_time:4989ms step_avg:34.17ms
step:147/2160 train_time:5023ms step_avg:34.17ms
step:148/2160 train_time:5056ms step_avg:34.16ms
step:149/2160 train_time:5090ms step_avg:34.16ms
step:150/2160 train_time:5124ms step_avg:34.16ms
step:151/2160 train_time:5157ms step_avg:34.16ms
step:152/2160 train_time:5191ms step_avg:34.15ms
step:153/2160 train_time:5225ms step_avg:34.15ms
step:154/2160 train_time:5258ms step_avg:34.14ms
step:155/2160 train_time:5292ms step_avg:34.14ms
step:156/2160 train_time:5325ms step_avg:34.13ms
step:157/2160 train_time:5359ms step_avg:34.13ms
step:158/2160 train_time:5392ms step_avg:34.13ms
step:159/2160 train_time:5426ms step_avg:34.13ms
step:160/2160 train_time:5459ms step_avg:34.12ms
step:161/2160 train_time:5494ms step_avg:34.12ms
step:162/2160 train_time:5527ms step_avg:34.11ms
step:163/2160 train_time:5561ms step_avg:34.11ms
step:164/2160 train_time:5594ms step_avg:34.11ms
step:165/2160 train_time:5628ms step_avg:34.11ms
step:166/2160 train_time:5661ms step_avg:34.10ms
step:167/2160 train_time:5695ms step_avg:34.10ms
step:168/2160 train_time:5729ms step_avg:34.10ms
step:169/2160 train_time:5763ms step_avg:34.10ms
step:170/2160 train_time:5796ms step_avg:34.09ms
step:171/2160 train_time:5830ms step_avg:34.09ms
step:172/2160 train_time:5863ms step_avg:34.09ms
step:173/2160 train_time:5897ms step_avg:34.09ms
step:174/2160 train_time:5930ms step_avg:34.08ms
step:175/2160 train_time:5964ms step_avg:34.08ms
step:176/2160 train_time:5997ms step_avg:34.08ms
step:177/2160 train_time:6031ms step_avg:34.08ms
step:178/2160 train_time:6064ms step_avg:34.07ms
step:179/2160 train_time:6098ms step_avg:34.07ms
step:180/2160 train_time:6132ms step_avg:34.06ms
step:181/2160 train_time:6166ms step_avg:34.06ms
step:182/2160 train_time:6199ms step_avg:34.06ms
step:183/2160 train_time:6234ms step_avg:34.06ms
step:184/2160 train_time:6267ms step_avg:34.06ms
step:185/2160 train_time:6301ms step_avg:34.06ms
step:186/2160 train_time:6334ms step_avg:34.06ms
step:187/2160 train_time:6368ms step_avg:34.05ms
step:188/2160 train_time:6401ms step_avg:34.05ms
step:189/2160 train_time:6435ms step_avg:34.05ms
step:190/2160 train_time:6468ms step_avg:34.04ms
step:191/2160 train_time:6503ms step_avg:34.05ms
step:192/2160 train_time:6536ms step_avg:34.04ms
step:193/2160 train_time:6570ms step_avg:34.04ms
step:194/2160 train_time:6603ms step_avg:34.04ms
step:195/2160 train_time:6637ms step_avg:34.03ms
step:196/2160 train_time:6670ms step_avg:34.03ms
step:197/2160 train_time:6704ms step_avg:34.03ms
step:198/2160 train_time:6737ms step_avg:34.03ms
step:199/2160 train_time:6771ms step_avg:34.03ms
step:200/2160 train_time:6804ms step_avg:34.02ms
step:201/2160 train_time:6838ms step_avg:34.02ms
step:202/2160 train_time:6871ms step_avg:34.02ms
step:203/2160 train_time:6905ms step_avg:34.02ms
step:204/2160 train_time:6939ms step_avg:34.01ms
step:205/2160 train_time:6973ms step_avg:34.01ms
step:206/2160 train_time:7006ms step_avg:34.01ms
step:207/2160 train_time:7040ms step_avg:34.01ms
step:208/2160 train_time:7073ms step_avg:34.00ms
step:209/2160 train_time:7107ms step_avg:34.00ms
step:210/2160 train_time:7140ms step_avg:34.00ms
step:211/2160 train_time:7175ms step_avg:34.00ms
step:212/2160 train_time:7208ms step_avg:34.00ms
step:213/2160 train_time:7242ms step_avg:34.00ms
step:214/2160 train_time:7275ms step_avg:33.99ms
step:215/2160 train_time:7309ms step_avg:34.00ms
step:216/2160 train_time:7343ms step_avg:33.99ms
step:217/2160 train_time:7377ms step_avg:33.99ms
step:218/2160 train_time:7410ms step_avg:33.99ms
step:219/2160 train_time:7444ms step_avg:33.99ms
step:220/2160 train_time:7477ms step_avg:33.99ms
step:221/2160 train_time:7511ms step_avg:33.99ms
step:222/2160 train_time:7544ms step_avg:33.98ms
step:223/2160 train_time:7578ms step_avg:33.98ms
step:224/2160 train_time:7611ms step_avg:33.98ms
step:225/2160 train_time:7645ms step_avg:33.98ms
step:226/2160 train_time:7678ms step_avg:33.98ms
step:227/2160 train_time:7713ms step_avg:33.98ms
step:228/2160 train_time:7746ms step_avg:33.97ms
step:229/2160 train_time:7780ms step_avg:33.97ms
step:230/2160 train_time:7813ms step_avg:33.97ms
step:231/2160 train_time:7847ms step_avg:33.97ms
step:232/2160 train_time:7880ms step_avg:33.96ms
step:233/2160 train_time:7914ms step_avg:33.97ms
step:234/2160 train_time:7948ms step_avg:33.96ms
step:235/2160 train_time:7981ms step_avg:33.96ms
step:236/2160 train_time:8015ms step_avg:33.96ms
step:237/2160 train_time:8049ms step_avg:33.96ms
step:238/2160 train_time:8081ms step_avg:33.96ms
step:239/2160 train_time:8116ms step_avg:33.96ms
step:240/2160 train_time:8149ms step_avg:33.95ms
step:241/2160 train_time:8183ms step_avg:33.95ms
step:242/2160 train_time:8216ms step_avg:33.95ms
step:243/2160 train_time:8250ms step_avg:33.95ms
step:244/2160 train_time:8283ms step_avg:33.95ms
step:245/2160 train_time:8317ms step_avg:33.95ms
step:246/2160 train_time:8350ms step_avg:33.94ms
step:247/2160 train_time:8384ms step_avg:33.94ms
step:248/2160 train_time:8417ms step_avg:33.94ms
step:249/2160 train_time:8451ms step_avg:33.94ms
step:250/2160 train_time:8484ms step_avg:33.94ms
step:250/2160 val_loss:4.3304 train_time:8520ms step_avg:34.08ms
step:251/2160 train_time:8539ms step_avg:34.02ms
step:252/2160 train_time:8557ms step_avg:33.96ms
step:253/2160 train_time:8590ms step_avg:33.95ms
step:254/2160 train_time:8623ms step_avg:33.95ms
step:255/2160 train_time:8663ms step_avg:33.97ms
step:256/2160 train_time:8699ms step_avg:33.98ms
step:257/2160 train_time:8734ms step_avg:33.98ms
step:258/2160 train_time:8767ms step_avg:33.98ms
step:259/2160 train_time:8802ms step_avg:33.98ms
step:260/2160 train_time:8835ms step_avg:33.98ms
step:261/2160 train_time:8870ms step_avg:33.98ms
step:262/2160 train_time:8903ms step_avg:33.98ms
step:263/2160 train_time:8937ms step_avg:33.98ms
step:264/2160 train_time:8969ms step_avg:33.97ms
step:265/2160 train_time:9003ms step_avg:33.97ms
step:266/2160 train_time:9036ms step_avg:33.97ms
step:267/2160 train_time:9070ms step_avg:33.97ms
step:268/2160 train_time:9103ms step_avg:33.97ms
step:269/2160 train_time:9137ms step_avg:33.97ms
step:270/2160 train_time:9170ms step_avg:33.96ms
step:271/2160 train_time:9204ms step_avg:33.96ms
step:272/2160 train_time:9237ms step_avg:33.96ms
step:273/2160 train_time:9271ms step_avg:33.96ms
step:274/2160 train_time:9304ms step_avg:33.96ms
step:275/2160 train_time:9338ms step_avg:33.96ms
step:276/2160 train_time:9371ms step_avg:33.95ms
step:277/2160 train_time:9405ms step_avg:33.95ms
step:278/2160 train_time:9437ms step_avg:33.95ms
step:279/2160 train_time:9472ms step_avg:33.95ms
step:280/2160 train_time:9504ms step_avg:33.94ms
step:281/2160 train_time:9539ms step_avg:33.95ms
step:282/2160 train_time:9572ms step_avg:33.94ms
step:283/2160 train_time:9606ms step_avg:33.94ms
step:284/2160 train_time:9640ms step_avg:33.94ms
step:285/2160 train_time:9674ms step_avg:33.94ms
step:286/2160 train_time:9707ms step_avg:33.94ms
step:287/2160 train_time:9742ms step_avg:33.94ms
step:288/2160 train_time:9775ms step_avg:33.94ms
step:289/2160 train_time:9809ms step_avg:33.94ms
step:290/2160 train_time:9843ms step_avg:33.94ms
step:291/2160 train_time:9877ms step_avg:33.94ms
step:292/2160 train_time:9910ms step_avg:33.94ms
step:293/2160 train_time:9944ms step_avg:33.94ms
step:294/2160 train_time:9978ms step_avg:33.94ms
step:295/2160 train_time:10012ms step_avg:33.94ms
step:296/2160 train_time:10046ms step_avg:33.94ms
step:297/2160 train_time:10080ms step_avg:33.94ms
step:298/2160 train_time:10113ms step_avg:33.94ms
step:299/2160 train_time:10147ms step_avg:33.94ms
step:300/2160 train_time:10180ms step_avg:33.93ms
step:301/2160 train_time:10214ms step_avg:33.93ms
step:302/2160 train_time:10247ms step_avg:33.93ms
step:303/2160 train_time:10281ms step_avg:33.93ms
step:304/2160 train_time:10314ms step_avg:33.93ms
step:305/2160 train_time:10348ms step_avg:33.93ms
step:306/2160 train_time:10381ms step_avg:33.93ms
step:307/2160 train_time:10415ms step_avg:33.93ms
step:308/2160 train_time:10448ms step_avg:33.92ms
step:309/2160 train_time:10482ms step_avg:33.92ms
step:310/2160 train_time:10515ms step_avg:33.92ms
step:311/2160 train_time:10549ms step_avg:33.92ms
step:312/2160 train_time:10583ms step_avg:33.92ms
step:313/2160 train_time:10617ms step_avg:33.92ms
step:314/2160 train_time:10650ms step_avg:33.92ms
step:315/2160 train_time:10684ms step_avg:33.92ms
step:316/2160 train_time:10717ms step_avg:33.92ms
step:317/2160 train_time:10752ms step_avg:33.92ms
step:318/2160 train_time:10785ms step_avg:33.91ms
step:319/2160 train_time:10819ms step_avg:33.92ms
step:320/2160 train_time:10853ms step_avg:33.91ms
step:321/2160 train_time:10887ms step_avg:33.92ms
step:322/2160 train_time:10920ms step_avg:33.91ms
step:323/2160 train_time:10954ms step_avg:33.91ms
step:324/2160 train_time:10987ms step_avg:33.91ms
step:325/2160 train_time:11021ms step_avg:33.91ms
step:326/2160 train_time:11055ms step_avg:33.91ms
step:327/2160 train_time:11089ms step_avg:33.91ms
step:328/2160 train_time:11122ms step_avg:33.91ms
step:329/2160 train_time:11156ms step_avg:33.91ms
step:330/2160 train_time:11189ms step_avg:33.91ms
step:331/2160 train_time:11223ms step_avg:33.91ms
step:332/2160 train_time:11256ms step_avg:33.90ms
step:333/2160 train_time:11290ms step_avg:33.90ms
step:334/2160 train_time:11323ms step_avg:33.90ms
step:335/2160 train_time:11357ms step_avg:33.90ms
step:336/2160 train_time:11390ms step_avg:33.90ms
step:337/2160 train_time:11424ms step_avg:33.90ms
step:338/2160 train_time:11457ms step_avg:33.90ms
step:339/2160 train_time:11491ms step_avg:33.90ms
step:340/2160 train_time:11525ms step_avg:33.90ms
step:341/2160 train_time:11559ms step_avg:33.90ms
step:342/2160 train_time:11592ms step_avg:33.89ms
step:343/2160 train_time:11626ms step_avg:33.89ms
step:344/2160 train_time:11659ms step_avg:33.89ms
step:345/2160 train_time:11693ms step_avg:33.89ms
step:346/2160 train_time:11727ms step_avg:33.89ms
step:347/2160 train_time:11761ms step_avg:33.89ms
step:348/2160 train_time:11794ms step_avg:33.89ms
step:349/2160 train_time:11829ms step_avg:33.89ms
step:350/2160 train_time:11862ms step_avg:33.89ms
step:351/2160 train_time:11896ms step_avg:33.89ms
step:352/2160 train_time:11929ms step_avg:33.89ms
step:353/2160 train_time:11963ms step_avg:33.89ms
step:354/2160 train_time:11997ms step_avg:33.89ms
step:355/2160 train_time:12031ms step_avg:33.89ms
step:356/2160 train_time:12064ms step_avg:33.89ms
step:357/2160 train_time:12099ms step_avg:33.89ms
step:358/2160 train_time:12132ms step_avg:33.89ms
step:359/2160 train_time:12166ms step_avg:33.89ms
step:360/2160 train_time:12198ms step_avg:33.88ms
step:361/2160 train_time:12233ms step_avg:33.89ms
step:362/2160 train_time:12266ms step_avg:33.88ms
step:363/2160 train_time:12300ms step_avg:33.88ms
step:364/2160 train_time:12333ms step_avg:33.88ms
step:365/2160 train_time:12367ms step_avg:33.88ms
step:366/2160 train_time:12400ms step_avg:33.88ms
step:367/2160 train_time:12434ms step_avg:33.88ms
step:368/2160 train_time:12467ms step_avg:33.88ms
step:369/2160 train_time:12501ms step_avg:33.88ms
step:370/2160 train_time:12534ms step_avg:33.88ms
step:371/2160 train_time:12568ms step_avg:33.88ms
step:372/2160 train_time:12601ms step_avg:33.87ms
step:373/2160 train_time:12636ms step_avg:33.88ms
step:374/2160 train_time:12669ms step_avg:33.87ms
step:375/2160 train_time:12703ms step_avg:33.87ms
step:376/2160 train_time:12736ms step_avg:33.87ms
step:377/2160 train_time:12770ms step_avg:33.87ms
step:378/2160 train_time:12803ms step_avg:33.87ms
step:379/2160 train_time:12837ms step_avg:33.87ms
step:380/2160 train_time:12870ms step_avg:33.87ms
step:381/2160 train_time:12904ms step_avg:33.87ms
step:382/2160 train_time:12937ms step_avg:33.87ms
step:383/2160 train_time:12972ms step_avg:33.87ms
step:384/2160 train_time:13005ms step_avg:33.87ms
step:385/2160 train_time:13040ms step_avg:33.87ms
step:386/2160 train_time:13072ms step_avg:33.87ms
step:387/2160 train_time:13107ms step_avg:33.87ms
step:388/2160 train_time:13140ms step_avg:33.86ms
step:389/2160 train_time:13174ms step_avg:33.87ms
step:390/2160 train_time:13207ms step_avg:33.86ms
step:391/2160 train_time:13241ms step_avg:33.86ms
step:392/2160 train_time:13274ms step_avg:33.86ms
step:393/2160 train_time:13308ms step_avg:33.86ms
step:394/2160 train_time:13341ms step_avg:33.86ms
step:395/2160 train_time:13375ms step_avg:33.86ms
step:396/2160 train_time:13408ms step_avg:33.86ms
step:397/2160 train_time:13442ms step_avg:33.86ms
step:398/2160 train_time:13475ms step_avg:33.86ms
step:399/2160 train_time:13509ms step_avg:33.86ms
step:400/2160 train_time:13542ms step_avg:33.86ms
step:401/2160 train_time:13576ms step_avg:33.86ms
step:402/2160 train_time:13609ms step_avg:33.85ms
step:403/2160 train_time:13643ms step_avg:33.85ms
step:404/2160 train_time:13676ms step_avg:33.85ms
step:405/2160 train_time:13710ms step_avg:33.85ms
step:406/2160 train_time:13744ms step_avg:33.85ms
step:407/2160 train_time:13778ms step_avg:33.85ms
step:408/2160 train_time:13811ms step_avg:33.85ms
step:409/2160 train_time:13845ms step_avg:33.85ms
step:410/2160 train_time:13878ms step_avg:33.85ms
step:411/2160 train_time:13913ms step_avg:33.85ms
step:412/2160 train_time:13946ms step_avg:33.85ms
step:413/2160 train_time:13980ms step_avg:33.85ms
step:414/2160 train_time:14013ms step_avg:33.85ms
step:415/2160 train_time:14047ms step_avg:33.85ms
step:416/2160 train_time:14080ms step_avg:33.85ms
step:417/2160 train_time:14114ms step_avg:33.85ms
step:418/2160 train_time:14147ms step_avg:33.84ms
step:419/2160 train_time:14181ms step_avg:33.85ms
step:420/2160 train_time:14215ms step_avg:33.84ms
step:421/2160 train_time:14249ms step_avg:33.85ms
step:422/2160 train_time:14282ms step_avg:33.84ms
step:423/2160 train_time:14316ms step_avg:33.84ms
step:424/2160 train_time:14349ms step_avg:33.84ms
step:425/2160 train_time:14383ms step_avg:33.84ms
step:426/2160 train_time:14416ms step_avg:33.84ms
step:427/2160 train_time:14450ms step_avg:33.84ms
step:428/2160 train_time:14484ms step_avg:33.84ms
step:429/2160 train_time:14517ms step_avg:33.84ms
step:430/2160 train_time:14550ms step_avg:33.84ms
step:431/2160 train_time:14584ms step_avg:33.84ms
step:432/2160 train_time:14617ms step_avg:33.84ms
step:433/2160 train_time:14652ms step_avg:33.84ms
step:434/2160 train_time:14685ms step_avg:33.84ms
step:435/2160 train_time:14719ms step_avg:33.84ms
step:436/2160 train_time:14752ms step_avg:33.83ms
step:437/2160 train_time:14786ms step_avg:33.84ms
step:438/2160 train_time:14819ms step_avg:33.83ms
step:439/2160 train_time:14854ms step_avg:33.84ms
step:440/2160 train_time:14887ms step_avg:33.83ms
step:441/2160 train_time:14921ms step_avg:33.83ms
step:442/2160 train_time:14954ms step_avg:33.83ms
step:443/2160 train_time:14988ms step_avg:33.83ms
step:444/2160 train_time:15021ms step_avg:33.83ms
step:445/2160 train_time:15055ms step_avg:33.83ms
step:446/2160 train_time:15089ms step_avg:33.83ms
step:447/2160 train_time:15122ms step_avg:33.83ms
step:448/2160 train_time:15155ms step_avg:33.83ms
step:449/2160 train_time:15190ms step_avg:33.83ms
step:450/2160 train_time:15223ms step_avg:33.83ms
step:451/2160 train_time:15257ms step_avg:33.83ms
step:452/2160 train_time:15290ms step_avg:33.83ms
step:453/2160 train_time:15324ms step_avg:33.83ms
step:454/2160 train_time:15357ms step_avg:33.83ms
step:455/2160 train_time:15392ms step_avg:33.83ms
step:456/2160 train_time:15425ms step_avg:33.83ms
step:457/2160 train_time:15458ms step_avg:33.83ms
step:458/2160 train_time:15491ms step_avg:33.82ms
step:459/2160 train_time:15525ms step_avg:33.82ms
step:460/2160 train_time:15558ms step_avg:33.82ms
step:461/2160 train_time:15593ms step_avg:33.82ms
step:462/2160 train_time:15626ms step_avg:33.82ms
step:463/2160 train_time:15660ms step_avg:33.82ms
step:464/2160 train_time:15693ms step_avg:33.82ms
step:465/2160 train_time:15727ms step_avg:33.82ms
step:466/2160 train_time:15760ms step_avg:33.82ms
step:467/2160 train_time:15795ms step_avg:33.82ms
step:468/2160 train_time:15828ms step_avg:33.82ms
step:469/2160 train_time:15862ms step_avg:33.82ms
step:470/2160 train_time:15895ms step_avg:33.82ms
step:471/2160 train_time:15929ms step_avg:33.82ms
step:472/2160 train_time:15962ms step_avg:33.82ms
step:473/2160 train_time:15996ms step_avg:33.82ms
step:474/2160 train_time:16030ms step_avg:33.82ms
step:475/2160 train_time:16064ms step_avg:33.82ms
step:476/2160 train_time:16096ms step_avg:33.82ms
step:477/2160 train_time:16131ms step_avg:33.82ms
step:478/2160 train_time:16165ms step_avg:33.82ms
step:479/2160 train_time:16199ms step_avg:33.82ms
step:480/2160 train_time:16232ms step_avg:33.82ms
step:481/2160 train_time:16266ms step_avg:33.82ms
step:482/2160 train_time:16299ms step_avg:33.82ms
step:483/2160 train_time:16333ms step_avg:33.82ms
step:484/2160 train_time:16367ms step_avg:33.82ms
step:485/2160 train_time:16401ms step_avg:33.82ms
step:486/2160 train_time:16434ms step_avg:33.81ms
step:487/2160 train_time:16468ms step_avg:33.82ms
step:488/2160 train_time:16501ms step_avg:33.81ms
step:489/2160 train_time:16535ms step_avg:33.81ms
step:490/2160 train_time:16569ms step_avg:33.81ms
step:491/2160 train_time:16603ms step_avg:33.81ms
step:492/2160 train_time:16636ms step_avg:33.81ms
step:493/2160 train_time:16670ms step_avg:33.81ms
step:494/2160 train_time:16703ms step_avg:33.81ms
step:495/2160 train_time:16737ms step_avg:33.81ms
step:496/2160 train_time:16770ms step_avg:33.81ms
step:497/2160 train_time:16804ms step_avg:33.81ms
step:498/2160 train_time:16837ms step_avg:33.81ms
step:499/2160 train_time:16872ms step_avg:33.81ms
step:500/2160 train_time:16905ms step_avg:33.81ms
step:500/2160 val_loss:4.0048 train_time:16941ms step_avg:33.88ms
step:501/2160 train_time:16961ms step_avg:33.86ms
step:502/2160 train_time:16980ms step_avg:33.82ms
step:503/2160 train_time:17010ms step_avg:33.82ms
step:504/2160 train_time:17044ms step_avg:33.82ms
step:505/2160 train_time:17080ms step_avg:33.82ms
step:506/2160 train_time:17114ms step_avg:33.82ms
step:507/2160 train_time:17149ms step_avg:33.83ms
step:508/2160 train_time:17182ms step_avg:33.82ms
step:509/2160 train_time:17217ms step_avg:33.83ms
step:510/2160 train_time:17250ms step_avg:33.82ms
step:511/2160 train_time:17284ms step_avg:33.82ms
step:512/2160 train_time:17317ms step_avg:33.82ms
step:513/2160 train_time:17351ms step_avg:33.82ms
step:514/2160 train_time:17384ms step_avg:33.82ms
step:515/2160 train_time:17418ms step_avg:33.82ms
step:516/2160 train_time:17451ms step_avg:33.82ms
step:517/2160 train_time:17485ms step_avg:33.82ms
step:518/2160 train_time:17518ms step_avg:33.82ms
step:519/2160 train_time:17552ms step_avg:33.82ms
step:520/2160 train_time:17585ms step_avg:33.82ms
step:521/2160 train_time:17619ms step_avg:33.82ms
step:522/2160 train_time:17652ms step_avg:33.82ms
step:523/2160 train_time:17686ms step_avg:33.82ms
step:524/2160 train_time:17718ms step_avg:33.81ms
step:525/2160 train_time:17752ms step_avg:33.81ms
step:526/2160 train_time:17785ms step_avg:33.81ms
step:527/2160 train_time:17819ms step_avg:33.81ms
step:528/2160 train_time:17852ms step_avg:33.81ms
step:529/2160 train_time:17887ms step_avg:33.81ms
step:530/2160 train_time:17920ms step_avg:33.81ms
step:531/2160 train_time:17954ms step_avg:33.81ms
step:532/2160 train_time:17988ms step_avg:33.81ms
step:533/2160 train_time:18022ms step_avg:33.81ms
step:534/2160 train_time:18055ms step_avg:33.81ms
step:535/2160 train_time:18091ms step_avg:33.81ms
step:536/2160 train_time:18124ms step_avg:33.81ms
step:537/2160 train_time:18158ms step_avg:33.81ms
step:538/2160 train_time:18192ms step_avg:33.81ms
step:539/2160 train_time:18226ms step_avg:33.81ms
step:540/2160 train_time:18259ms step_avg:33.81ms
step:541/2160 train_time:18293ms step_avg:33.81ms
step:542/2160 train_time:18326ms step_avg:33.81ms
step:543/2160 train_time:18361ms step_avg:33.81ms
step:544/2160 train_time:18394ms step_avg:33.81ms
step:545/2160 train_time:18428ms step_avg:33.81ms
step:546/2160 train_time:18461ms step_avg:33.81ms
step:547/2160 train_time:18495ms step_avg:33.81ms
step:548/2160 train_time:18528ms step_avg:33.81ms
step:549/2160 train_time:18563ms step_avg:33.81ms
step:550/2160 train_time:18595ms step_avg:33.81ms
step:551/2160 train_time:18630ms step_avg:33.81ms
step:552/2160 train_time:18663ms step_avg:33.81ms
step:553/2160 train_time:18697ms step_avg:33.81ms
step:554/2160 train_time:18730ms step_avg:33.81ms
step:555/2160 train_time:18764ms step_avg:33.81ms
step:556/2160 train_time:18797ms step_avg:33.81ms
step:557/2160 train_time:18831ms step_avg:33.81ms
step:558/2160 train_time:18865ms step_avg:33.81ms
step:559/2160 train_time:18899ms step_avg:33.81ms
step:560/2160 train_time:18932ms step_avg:33.81ms
step:561/2160 train_time:18966ms step_avg:33.81ms
step:562/2160 train_time:18999ms step_avg:33.81ms
step:563/2160 train_time:19033ms step_avg:33.81ms
step:564/2160 train_time:19067ms step_avg:33.81ms
step:565/2160 train_time:19101ms step_avg:33.81ms
step:566/2160 train_time:19134ms step_avg:33.81ms
step:567/2160 train_time:19169ms step_avg:33.81ms
step:568/2160 train_time:19202ms step_avg:33.81ms
step:569/2160 train_time:19237ms step_avg:33.81ms
step:570/2160 train_time:19270ms step_avg:33.81ms
step:571/2160 train_time:19304ms step_avg:33.81ms
step:572/2160 train_time:19337ms step_avg:33.81ms
step:573/2160 train_time:19372ms step_avg:33.81ms
step:574/2160 train_time:19405ms step_avg:33.81ms
step:575/2160 train_time:19439ms step_avg:33.81ms
step:576/2160 train_time:19472ms step_avg:33.81ms
step:577/2160 train_time:19506ms step_avg:33.81ms
step:578/2160 train_time:19539ms step_avg:33.80ms
step:579/2160 train_time:19574ms step_avg:33.81ms
step:580/2160 train_time:19607ms step_avg:33.80ms
step:581/2160 train_time:19641ms step_avg:33.81ms
step:582/2160 train_time:19674ms step_avg:33.80ms
step:583/2160 train_time:19708ms step_avg:33.80ms
step:584/2160 train_time:19741ms step_avg:33.80ms
step:585/2160 train_time:19775ms step_avg:33.80ms
step:586/2160 train_time:19808ms step_avg:33.80ms
step:587/2160 train_time:19843ms step_avg:33.80ms
step:588/2160 train_time:19876ms step_avg:33.80ms
step:589/2160 train_time:19910ms step_avg:33.80ms
step:590/2160 train_time:19943ms step_avg:33.80ms
step:591/2160 train_time:19977ms step_avg:33.80ms
step:592/2160 train_time:20010ms step_avg:33.80ms
step:593/2160 train_time:20045ms step_avg:33.80ms
step:594/2160 train_time:20078ms step_avg:33.80ms
step:595/2160 train_time:20112ms step_avg:33.80ms
step:596/2160 train_time:20146ms step_avg:33.80ms
step:597/2160 train_time:20180ms step_avg:33.80ms
step:598/2160 train_time:20213ms step_avg:33.80ms
step:599/2160 train_time:20248ms step_avg:33.80ms
step:600/2160 train_time:20281ms step_avg:33.80ms
step:601/2160 train_time:20315ms step_avg:33.80ms
step:602/2160 train_time:20348ms step_avg:33.80ms
step:603/2160 train_time:20382ms step_avg:33.80ms
step:604/2160 train_time:20415ms step_avg:33.80ms
step:605/2160 train_time:20449ms step_avg:33.80ms
step:606/2160 train_time:20482ms step_avg:33.80ms
step:607/2160 train_time:20517ms step_avg:33.80ms
step:608/2160 train_time:20550ms step_avg:33.80ms
step:609/2160 train_time:20584ms step_avg:33.80ms
step:610/2160 train_time:20617ms step_avg:33.80ms
step:611/2160 train_time:20651ms step_avg:33.80ms
step:612/2160 train_time:20684ms step_avg:33.80ms
step:613/2160 train_time:20718ms step_avg:33.80ms
step:614/2160 train_time:20751ms step_avg:33.80ms
step:615/2160 train_time:20785ms step_avg:33.80ms
step:616/2160 train_time:20818ms step_avg:33.80ms
step:617/2160 train_time:20853ms step_avg:33.80ms
step:618/2160 train_time:20886ms step_avg:33.80ms
step:619/2160 train_time:20920ms step_avg:33.80ms
step:620/2160 train_time:20953ms step_avg:33.80ms
step:621/2160 train_time:20988ms step_avg:33.80ms
step:622/2160 train_time:21021ms step_avg:33.80ms
step:623/2160 train_time:21055ms step_avg:33.80ms
step:624/2160 train_time:21088ms step_avg:33.80ms
step:625/2160 train_time:21122ms step_avg:33.80ms
step:626/2160 train_time:21155ms step_avg:33.79ms
step:627/2160 train_time:21189ms step_avg:33.79ms
step:628/2160 train_time:21223ms step_avg:33.79ms
step:629/2160 train_time:21257ms step_avg:33.79ms
step:630/2160 train_time:21290ms step_avg:33.79ms
step:631/2160 train_time:21325ms step_avg:33.80ms
step:632/2160 train_time:21358ms step_avg:33.79ms
step:633/2160 train_time:21392ms step_avg:33.80ms
step:634/2160 train_time:21426ms step_avg:33.79ms
step:635/2160 train_time:21460ms step_avg:33.79ms
step:636/2160 train_time:21492ms step_avg:33.79ms
step:637/2160 train_time:21527ms step_avg:33.79ms
step:638/2160 train_time:21560ms step_avg:33.79ms
step:639/2160 train_time:21594ms step_avg:33.79ms
step:640/2160 train_time:21627ms step_avg:33.79ms
step:641/2160 train_time:21661ms step_avg:33.79ms
step:642/2160 train_time:21694ms step_avg:33.79ms
step:643/2160 train_time:21729ms step_avg:33.79ms
step:644/2160 train_time:21762ms step_avg:33.79ms
step:645/2160 train_time:21796ms step_avg:33.79ms
step:646/2160 train_time:21829ms step_avg:33.79ms
step:647/2160 train_time:21863ms step_avg:33.79ms
step:648/2160 train_time:21896ms step_avg:33.79ms
step:649/2160 train_time:21931ms step_avg:33.79ms
step:650/2160 train_time:21964ms step_avg:33.79ms
step:651/2160 train_time:21998ms step_avg:33.79ms
step:652/2160 train_time:22031ms step_avg:33.79ms
step:653/2160 train_time:22065ms step_avg:33.79ms
step:654/2160 train_time:22099ms step_avg:33.79ms
step:655/2160 train_time:22133ms step_avg:33.79ms
step:656/2160 train_time:22166ms step_avg:33.79ms
step:657/2160 train_time:22200ms step_avg:33.79ms
step:658/2160 train_time:22233ms step_avg:33.79ms
step:659/2160 train_time:22268ms step_avg:33.79ms
step:660/2160 train_time:22301ms step_avg:33.79ms
step:661/2160 train_time:22336ms step_avg:33.79ms
step:662/2160 train_time:22369ms step_avg:33.79ms
step:663/2160 train_time:22403ms step_avg:33.79ms
step:664/2160 train_time:22436ms step_avg:33.79ms
step:665/2160 train_time:22471ms step_avg:33.79ms
step:666/2160 train_time:22504ms step_avg:33.79ms
step:667/2160 train_time:22538ms step_avg:33.79ms
step:668/2160 train_time:22571ms step_avg:33.79ms
step:669/2160 train_time:22606ms step_avg:33.79ms
step:670/2160 train_time:22639ms step_avg:33.79ms
step:671/2160 train_time:22673ms step_avg:33.79ms
step:672/2160 train_time:22706ms step_avg:33.79ms
step:673/2160 train_time:22740ms step_avg:33.79ms
step:674/2160 train_time:22773ms step_avg:33.79ms
step:675/2160 train_time:22808ms step_avg:33.79ms
step:676/2160 train_time:22841ms step_avg:33.79ms
step:677/2160 train_time:22875ms step_avg:33.79ms
step:678/2160 train_time:22908ms step_avg:33.79ms
step:679/2160 train_time:22942ms step_avg:33.79ms
step:680/2160 train_time:22975ms step_avg:33.79ms
step:681/2160 train_time:23010ms step_avg:33.79ms
step:682/2160 train_time:23043ms step_avg:33.79ms
step:683/2160 train_time:23077ms step_avg:33.79ms
step:684/2160 train_time:23110ms step_avg:33.79ms
step:685/2160 train_time:23144ms step_avg:33.79ms
step:686/2160 train_time:23177ms step_avg:33.79ms
step:687/2160 train_time:23212ms step_avg:33.79ms
step:688/2160 train_time:23245ms step_avg:33.79ms
step:689/2160 train_time:23279ms step_avg:33.79ms
step:690/2160 train_time:23312ms step_avg:33.79ms
step:691/2160 train_time:23347ms step_avg:33.79ms
step:692/2160 train_time:23380ms step_avg:33.79ms
step:693/2160 train_time:23414ms step_avg:33.79ms
step:694/2160 train_time:23447ms step_avg:33.79ms
step:695/2160 train_time:23482ms step_avg:33.79ms
step:696/2160 train_time:23515ms step_avg:33.79ms
step:697/2160 train_time:23549ms step_avg:33.79ms
step:698/2160 train_time:23582ms step_avg:33.79ms
step:699/2160 train_time:23617ms step_avg:33.79ms
step:700/2160 train_time:23650ms step_avg:33.79ms
step:701/2160 train_time:23684ms step_avg:33.79ms
step:702/2160 train_time:23717ms step_avg:33.78ms
step:703/2160 train_time:23751ms step_avg:33.79ms
step:704/2160 train_time:23785ms step_avg:33.78ms
step:705/2160 train_time:23819ms step_avg:33.79ms
step:706/2160 train_time:23852ms step_avg:33.78ms
step:707/2160 train_time:23886ms step_avg:33.78ms
step:708/2160 train_time:23920ms step_avg:33.78ms
step:709/2160 train_time:23980ms step_avg:33.82ms
step:710/2160 train_time:24040ms step_avg:33.86ms
step:711/2160 train_time:24102ms step_avg:33.90ms
step:712/2160 train_time:24162ms step_avg:33.94ms
step:713/2160 train_time:24224ms step_avg:33.97ms
step:714/2160 train_time:24283ms step_avg:34.01ms
step:715/2160 train_time:24346ms step_avg:34.05ms
step:716/2160 train_time:24405ms step_avg:34.09ms
step:717/2160 train_time:24468ms step_avg:34.12ms
step:718/2160 train_time:24527ms step_avg:34.16ms
step:719/2160 train_time:24589ms step_avg:34.20ms
step:720/2160 train_time:24648ms step_avg:34.23ms
step:721/2160 train_time:24709ms step_avg:34.27ms
step:722/2160 train_time:24769ms step_avg:34.31ms
step:723/2160 train_time:24830ms step_avg:34.34ms
step:724/2160 train_time:24890ms step_avg:34.38ms
step:725/2160 train_time:24951ms step_avg:34.42ms
step:726/2160 train_time:25011ms step_avg:34.45ms
step:727/2160 train_time:25072ms step_avg:34.49ms
step:728/2160 train_time:25132ms step_avg:34.52ms
step:729/2160 train_time:25193ms step_avg:34.56ms
step:730/2160 train_time:25252ms step_avg:34.59ms
step:731/2160 train_time:25314ms step_avg:34.63ms
step:732/2160 train_time:25375ms step_avg:34.66ms
step:733/2160 train_time:25436ms step_avg:34.70ms
step:734/2160 train_time:25496ms step_avg:34.74ms
step:735/2160 train_time:25558ms step_avg:34.77ms
step:736/2160 train_time:25618ms step_avg:34.81ms
step:737/2160 train_time:25680ms step_avg:34.84ms
step:738/2160 train_time:25740ms step_avg:34.88ms
step:739/2160 train_time:25801ms step_avg:34.91ms
step:740/2160 train_time:25861ms step_avg:34.95ms
step:741/2160 train_time:25923ms step_avg:34.98ms
step:742/2160 train_time:25983ms step_avg:35.02ms
step:743/2160 train_time:26044ms step_avg:35.05ms
step:744/2160 train_time:26103ms step_avg:35.09ms
step:745/2160 train_time:26165ms step_avg:35.12ms
step:746/2160 train_time:26224ms step_avg:35.15ms
step:747/2160 train_time:26285ms step_avg:35.19ms
step:748/2160 train_time:26344ms step_avg:35.22ms
step:749/2160 train_time:26407ms step_avg:35.26ms
step:750/2160 train_time:26466ms step_avg:35.29ms
step:750/2160 val_loss:3.8595 train_time:26529ms step_avg:35.37ms
step:751/2160 train_time:26552ms step_avg:35.36ms
step:752/2160 train_time:26590ms step_avg:35.36ms
step:753/2160 train_time:26652ms step_avg:35.39ms
step:754/2160 train_time:26711ms step_avg:35.43ms
step:755/2160 train_time:26774ms step_avg:35.46ms
step:756/2160 train_time:26834ms step_avg:35.49ms
step:757/2160 train_time:26895ms step_avg:35.53ms
step:758/2160 train_time:26953ms step_avg:35.56ms
step:759/2160 train_time:27014ms step_avg:35.59ms
step:760/2160 train_time:27072ms step_avg:35.62ms
step:761/2160 train_time:27132ms step_avg:35.65ms
step:762/2160 train_time:27191ms step_avg:35.68ms
step:763/2160 train_time:27251ms step_avg:35.72ms
step:764/2160 train_time:27311ms step_avg:35.75ms
step:765/2160 train_time:27372ms step_avg:35.78ms
step:766/2160 train_time:27433ms step_avg:35.81ms
step:767/2160 train_time:27500ms step_avg:35.85ms
step:768/2160 train_time:27562ms step_avg:35.89ms
step:769/2160 train_time:27625ms step_avg:35.92ms
step:770/2160 train_time:27684ms step_avg:35.95ms
step:771/2160 train_time:27745ms step_avg:35.99ms
step:772/2160 train_time:27805ms step_avg:36.02ms
step:773/2160 train_time:27866ms step_avg:36.05ms
step:774/2160 train_time:27925ms step_avg:36.08ms
step:775/2160 train_time:27986ms step_avg:36.11ms
step:776/2160 train_time:28045ms step_avg:36.14ms
step:777/2160 train_time:28106ms step_avg:36.17ms
step:778/2160 train_time:28165ms step_avg:36.20ms
step:779/2160 train_time:28225ms step_avg:36.23ms
step:780/2160 train_time:28284ms step_avg:36.26ms
step:781/2160 train_time:28345ms step_avg:36.29ms
step:782/2160 train_time:28407ms step_avg:36.33ms
step:783/2160 train_time:28471ms step_avg:36.36ms
step:784/2160 train_time:28532ms step_avg:36.39ms
step:785/2160 train_time:28593ms step_avg:36.42ms
step:786/2160 train_time:28653ms step_avg:36.45ms
step:787/2160 train_time:28715ms step_avg:36.49ms
step:788/2160 train_time:28774ms step_avg:36.51ms
step:789/2160 train_time:28835ms step_avg:36.55ms
step:790/2160 train_time:28894ms step_avg:36.57ms
step:791/2160 train_time:28955ms step_avg:36.61ms
step:792/2160 train_time:29015ms step_avg:36.64ms
step:793/2160 train_time:29077ms step_avg:36.67ms
step:794/2160 train_time:29136ms step_avg:36.70ms
step:795/2160 train_time:29197ms step_avg:36.73ms
step:796/2160 train_time:29257ms step_avg:36.75ms
step:797/2160 train_time:29318ms step_avg:36.79ms
step:798/2160 train_time:29378ms step_avg:36.81ms
step:799/2160 train_time:29441ms step_avg:36.85ms
step:800/2160 train_time:29501ms step_avg:36.88ms
step:801/2160 train_time:29563ms step_avg:36.91ms
step:802/2160 train_time:29622ms step_avg:36.94ms
step:803/2160 train_time:29683ms step_avg:36.97ms
step:804/2160 train_time:29743ms step_avg:36.99ms
step:805/2160 train_time:29805ms step_avg:37.02ms
step:806/2160 train_time:29865ms step_avg:37.05ms
step:807/2160 train_time:29926ms step_avg:37.08ms
step:808/2160 train_time:29986ms step_avg:37.11ms
step:809/2160 train_time:30047ms step_avg:37.14ms
step:810/2160 train_time:30107ms step_avg:37.17ms
step:811/2160 train_time:30169ms step_avg:37.20ms
step:812/2160 train_time:30228ms step_avg:37.23ms
step:813/2160 train_time:30289ms step_avg:37.26ms
step:814/2160 train_time:30350ms step_avg:37.28ms
step:815/2160 train_time:30411ms step_avg:37.31ms
step:816/2160 train_time:30470ms step_avg:37.34ms
step:817/2160 train_time:30532ms step_avg:37.37ms
step:818/2160 train_time:30591ms step_avg:37.40ms
step:819/2160 train_time:30653ms step_avg:37.43ms
step:820/2160 train_time:30712ms step_avg:37.45ms
step:821/2160 train_time:30774ms step_avg:37.48ms
step:822/2160 train_time:30834ms step_avg:37.51ms
step:823/2160 train_time:30896ms step_avg:37.54ms
step:824/2160 train_time:30956ms step_avg:37.57ms
step:825/2160 train_time:31019ms step_avg:37.60ms
step:826/2160 train_time:31078ms step_avg:37.63ms
step:827/2160 train_time:31140ms step_avg:37.65ms
step:828/2160 train_time:31201ms step_avg:37.68ms
step:829/2160 train_time:31262ms step_avg:37.71ms
step:830/2160 train_time:31322ms step_avg:37.74ms
step:831/2160 train_time:31384ms step_avg:37.77ms
step:832/2160 train_time:31443ms step_avg:37.79ms
step:833/2160 train_time:31504ms step_avg:37.82ms
step:834/2160 train_time:31564ms step_avg:37.85ms
step:835/2160 train_time:31625ms step_avg:37.87ms
step:836/2160 train_time:31685ms step_avg:37.90ms
step:837/2160 train_time:31747ms step_avg:37.93ms
step:838/2160 train_time:31807ms step_avg:37.96ms
step:839/2160 train_time:31869ms step_avg:37.98ms
step:840/2160 train_time:31929ms step_avg:38.01ms
step:841/2160 train_time:31990ms step_avg:38.04ms
step:842/2160 train_time:32050ms step_avg:38.06ms
step:843/2160 train_time:32112ms step_avg:38.09ms
step:844/2160 train_time:32171ms step_avg:38.12ms
step:845/2160 train_time:32232ms step_avg:38.14ms
step:846/2160 train_time:32291ms step_avg:38.17ms
step:847/2160 train_time:32353ms step_avg:38.20ms
step:848/2160 train_time:32412ms step_avg:38.22ms
step:849/2160 train_time:32473ms step_avg:38.25ms
step:850/2160 train_time:32533ms step_avg:38.27ms
step:851/2160 train_time:32594ms step_avg:38.30ms
step:852/2160 train_time:32654ms step_avg:38.33ms
step:853/2160 train_time:32715ms step_avg:38.35ms
step:854/2160 train_time:32776ms step_avg:38.38ms
step:855/2160 train_time:32838ms step_avg:38.41ms
step:856/2160 train_time:32898ms step_avg:38.43ms
step:857/2160 train_time:32959ms step_avg:38.46ms
step:858/2160 train_time:33019ms step_avg:38.48ms
step:859/2160 train_time:33080ms step_avg:38.51ms
step:860/2160 train_time:33139ms step_avg:38.53ms
step:861/2160 train_time:33200ms step_avg:38.56ms
step:862/2160 train_time:33259ms step_avg:38.58ms
step:863/2160 train_time:33321ms step_avg:38.61ms
step:864/2160 train_time:33381ms step_avg:38.63ms
step:865/2160 train_time:33442ms step_avg:38.66ms
step:866/2160 train_time:33502ms step_avg:38.69ms
step:867/2160 train_time:33563ms step_avg:38.71ms
step:868/2160 train_time:33622ms step_avg:38.74ms
step:869/2160 train_time:33684ms step_avg:38.76ms
step:870/2160 train_time:33743ms step_avg:38.79ms
step:871/2160 train_time:33804ms step_avg:38.81ms
step:872/2160 train_time:33864ms step_avg:38.84ms
step:873/2160 train_time:33925ms step_avg:38.86ms
step:874/2160 train_time:33985ms step_avg:38.88ms
step:875/2160 train_time:34046ms step_avg:38.91ms
step:876/2160 train_time:34106ms step_avg:38.93ms
step:877/2160 train_time:34167ms step_avg:38.96ms
step:878/2160 train_time:34227ms step_avg:38.98ms
step:879/2160 train_time:34288ms step_avg:39.01ms
step:880/2160 train_time:34348ms step_avg:39.03ms
step:881/2160 train_time:34408ms step_avg:39.06ms
step:882/2160 train_time:34467ms step_avg:39.08ms
step:883/2160 train_time:34528ms step_avg:39.10ms
step:884/2160 train_time:34588ms step_avg:39.13ms
step:885/2160 train_time:34648ms step_avg:39.15ms
step:886/2160 train_time:34708ms step_avg:39.17ms
step:887/2160 train_time:34769ms step_avg:39.20ms
step:888/2160 train_time:34829ms step_avg:39.22ms
step:889/2160 train_time:34890ms step_avg:39.25ms
step:890/2160 train_time:34951ms step_avg:39.27ms
step:891/2160 train_time:35011ms step_avg:39.29ms
step:892/2160 train_time:35071ms step_avg:39.32ms
step:893/2160 train_time:35132ms step_avg:39.34ms
step:894/2160 train_time:35191ms step_avg:39.36ms
step:895/2160 train_time:35252ms step_avg:39.39ms
step:896/2160 train_time:35312ms step_avg:39.41ms
step:897/2160 train_time:35373ms step_avg:39.44ms
step:898/2160 train_time:35433ms step_avg:39.46ms
step:899/2160 train_time:35494ms step_avg:39.48ms
step:900/2160 train_time:35553ms step_avg:39.50ms
step:901/2160 train_time:35615ms step_avg:39.53ms
step:902/2160 train_time:35675ms step_avg:39.55ms
step:903/2160 train_time:35737ms step_avg:39.58ms
step:904/2160 train_time:35797ms step_avg:39.60ms
step:905/2160 train_time:35858ms step_avg:39.62ms
step:906/2160 train_time:35918ms step_avg:39.64ms
step:907/2160 train_time:35980ms step_avg:39.67ms
step:908/2160 train_time:36040ms step_avg:39.69ms
step:909/2160 train_time:36102ms step_avg:39.72ms
step:910/2160 train_time:36161ms step_avg:39.74ms
step:911/2160 train_time:36223ms step_avg:39.76ms
step:912/2160 train_time:36283ms step_avg:39.78ms
step:913/2160 train_time:36345ms step_avg:39.81ms
step:914/2160 train_time:36404ms step_avg:39.83ms
step:915/2160 train_time:36465ms step_avg:39.85ms
step:916/2160 train_time:36524ms step_avg:39.87ms
step:917/2160 train_time:36585ms step_avg:39.90ms
step:918/2160 train_time:36645ms step_avg:39.92ms
step:919/2160 train_time:36706ms step_avg:39.94ms
step:920/2160 train_time:36766ms step_avg:39.96ms
step:921/2160 train_time:36827ms step_avg:39.99ms
step:922/2160 train_time:36888ms step_avg:40.01ms
step:923/2160 train_time:36950ms step_avg:40.03ms
step:924/2160 train_time:37010ms step_avg:40.05ms
step:925/2160 train_time:37070ms step_avg:40.08ms
step:926/2160 train_time:37129ms step_avg:40.10ms
step:927/2160 train_time:37190ms step_avg:40.12ms
step:928/2160 train_time:37250ms step_avg:40.14ms
step:929/2160 train_time:37311ms step_avg:40.16ms
step:930/2160 train_time:37370ms step_avg:40.18ms
step:931/2160 train_time:37432ms step_avg:40.21ms
step:932/2160 train_time:37492ms step_avg:40.23ms
step:933/2160 train_time:37553ms step_avg:40.25ms
step:934/2160 train_time:37613ms step_avg:40.27ms
step:935/2160 train_time:37674ms step_avg:40.29ms
step:936/2160 train_time:37733ms step_avg:40.31ms
step:937/2160 train_time:37795ms step_avg:40.34ms
step:938/2160 train_time:37855ms step_avg:40.36ms
step:939/2160 train_time:37916ms step_avg:40.38ms
step:940/2160 train_time:37976ms step_avg:40.40ms
step:941/2160 train_time:38038ms step_avg:40.42ms
step:942/2160 train_time:38098ms step_avg:40.44ms
step:943/2160 train_time:38159ms step_avg:40.47ms
step:944/2160 train_time:38219ms step_avg:40.49ms
step:945/2160 train_time:38281ms step_avg:40.51ms
step:946/2160 train_time:38341ms step_avg:40.53ms
step:947/2160 train_time:38404ms step_avg:40.55ms
step:948/2160 train_time:38463ms step_avg:40.57ms
step:949/2160 train_time:38524ms step_avg:40.59ms
step:950/2160 train_time:38583ms step_avg:40.61ms
step:951/2160 train_time:38645ms step_avg:40.64ms
step:952/2160 train_time:38705ms step_avg:40.66ms
step:953/2160 train_time:38766ms step_avg:40.68ms
step:954/2160 train_time:38826ms step_avg:40.70ms
step:955/2160 train_time:38888ms step_avg:40.72ms
step:956/2160 train_time:38948ms step_avg:40.74ms
step:957/2160 train_time:39009ms step_avg:40.76ms
step:958/2160 train_time:39069ms step_avg:40.78ms
step:959/2160 train_time:39129ms step_avg:40.80ms
step:960/2160 train_time:39189ms step_avg:40.82ms
step:961/2160 train_time:39251ms step_avg:40.84ms
step:962/2160 train_time:39310ms step_avg:40.86ms
step:963/2160 train_time:39371ms step_avg:40.88ms
step:964/2160 train_time:39430ms step_avg:40.90ms
step:965/2160 train_time:39492ms step_avg:40.92ms
step:966/2160 train_time:39551ms step_avg:40.94ms
step:967/2160 train_time:39612ms step_avg:40.96ms
step:968/2160 train_time:39672ms step_avg:40.98ms
step:969/2160 train_time:39734ms step_avg:41.01ms
step:970/2160 train_time:39793ms step_avg:41.02ms
step:971/2160 train_time:39855ms step_avg:41.04ms
step:972/2160 train_time:39915ms step_avg:41.06ms
step:973/2160 train_time:39977ms step_avg:41.09ms
step:974/2160 train_time:40037ms step_avg:41.11ms
step:975/2160 train_time:40098ms step_avg:41.13ms
step:976/2160 train_time:40159ms step_avg:41.15ms
step:977/2160 train_time:40221ms step_avg:41.17ms
step:978/2160 train_time:40281ms step_avg:41.19ms
step:979/2160 train_time:40343ms step_avg:41.21ms
step:980/2160 train_time:40403ms step_avg:41.23ms
step:981/2160 train_time:40464ms step_avg:41.25ms
step:982/2160 train_time:40524ms step_avg:41.27ms
step:983/2160 train_time:40585ms step_avg:41.29ms
step:984/2160 train_time:40645ms step_avg:41.31ms
step:985/2160 train_time:40706ms step_avg:41.33ms
step:986/2160 train_time:40767ms step_avg:41.35ms
step:987/2160 train_time:40828ms step_avg:41.37ms
step:988/2160 train_time:40889ms step_avg:41.39ms
step:989/2160 train_time:40950ms step_avg:41.41ms
step:990/2160 train_time:41010ms step_avg:41.42ms
step:991/2160 train_time:41071ms step_avg:41.44ms
step:992/2160 train_time:41131ms step_avg:41.46ms
step:993/2160 train_time:41192ms step_avg:41.48ms
step:994/2160 train_time:41252ms step_avg:41.50ms
step:995/2160 train_time:41313ms step_avg:41.52ms
step:996/2160 train_time:41373ms step_avg:41.54ms
step:997/2160 train_time:41434ms step_avg:41.56ms
step:998/2160 train_time:41494ms step_avg:41.58ms
step:999/2160 train_time:41555ms step_avg:41.60ms
step:1000/2160 train_time:41615ms step_avg:41.62ms
step:1000/2160 val_loss:3.6961 train_time:41678ms step_avg:41.68ms
step:1001/2160 train_time:41697ms step_avg:41.66ms
step:1002/2160 train_time:41738ms step_avg:41.66ms
step:1003/2160 train_time:41803ms step_avg:41.68ms
step:1004/2160 train_time:41866ms step_avg:41.70ms
step:1005/2160 train_time:41927ms step_avg:41.72ms
step:1006/2160 train_time:41987ms step_avg:41.74ms
step:1007/2160 train_time:42048ms step_avg:41.76ms
step:1008/2160 train_time:42107ms step_avg:41.77ms
step:1009/2160 train_time:42168ms step_avg:41.79ms
step:1010/2160 train_time:42226ms step_avg:41.81ms
step:1011/2160 train_time:42288ms step_avg:41.83ms
step:1012/2160 train_time:42347ms step_avg:41.84ms
step:1013/2160 train_time:42408ms step_avg:41.86ms
step:1014/2160 train_time:42467ms step_avg:41.88ms
step:1015/2160 train_time:42527ms step_avg:41.90ms
step:1016/2160 train_time:42586ms step_avg:41.92ms
step:1017/2160 train_time:42649ms step_avg:41.94ms
step:1018/2160 train_time:42710ms step_avg:41.95ms
step:1019/2160 train_time:42774ms step_avg:41.98ms
step:1020/2160 train_time:42834ms step_avg:41.99ms
step:1021/2160 train_time:42896ms step_avg:42.01ms
step:1022/2160 train_time:42956ms step_avg:42.03ms
step:1023/2160 train_time:43017ms step_avg:42.05ms
step:1024/2160 train_time:43076ms step_avg:42.07ms
step:1025/2160 train_time:43137ms step_avg:42.08ms
step:1026/2160 train_time:43197ms step_avg:42.10ms
step:1027/2160 train_time:43257ms step_avg:42.12ms
step:1028/2160 train_time:43316ms step_avg:42.14ms
step:1029/2160 train_time:43377ms step_avg:42.15ms
step:1030/2160 train_time:43436ms step_avg:42.17ms
step:1031/2160 train_time:43497ms step_avg:42.19ms
step:1032/2160 train_time:43557ms step_avg:42.21ms
step:1033/2160 train_time:43619ms step_avg:42.23ms
step:1034/2160 train_time:43679ms step_avg:42.24ms
step:1035/2160 train_time:43741ms step_avg:42.26ms
step:1036/2160 train_time:43801ms step_avg:42.28ms
step:1037/2160 train_time:43864ms step_avg:42.30ms
step:1038/2160 train_time:43924ms step_avg:42.32ms
step:1039/2160 train_time:43986ms step_avg:42.33ms
step:1040/2160 train_time:44046ms step_avg:42.35ms
step:1041/2160 train_time:44107ms step_avg:42.37ms
step:1042/2160 train_time:44167ms step_avg:42.39ms
step:1043/2160 train_time:44228ms step_avg:42.40ms
step:1044/2160 train_time:44287ms step_avg:42.42ms
step:1045/2160 train_time:44348ms step_avg:42.44ms
step:1046/2160 train_time:44407ms step_avg:42.45ms
step:1047/2160 train_time:44468ms step_avg:42.47ms
step:1048/2160 train_time:44527ms step_avg:42.49ms
step:1049/2160 train_time:44589ms step_avg:42.51ms
step:1050/2160 train_time:44649ms step_avg:42.52ms
step:1051/2160 train_time:44711ms step_avg:42.54ms
step:1052/2160 train_time:44771ms step_avg:42.56ms
step:1053/2160 train_time:44834ms step_avg:42.58ms
step:1054/2160 train_time:44894ms step_avg:42.59ms
step:1055/2160 train_time:44956ms step_avg:42.61ms
step:1056/2160 train_time:45015ms step_avg:42.63ms
step:1057/2160 train_time:45076ms step_avg:42.65ms
step:1058/2160 train_time:45136ms step_avg:42.66ms
step:1059/2160 train_time:45196ms step_avg:42.68ms
step:1060/2160 train_time:45256ms step_avg:42.69ms
step:1061/2160 train_time:45317ms step_avg:42.71ms
step:1062/2160 train_time:45376ms step_avg:42.73ms
step:1063/2160 train_time:45437ms step_avg:42.74ms
step:1064/2160 train_time:45497ms step_avg:42.76ms
step:1065/2160 train_time:45559ms step_avg:42.78ms
step:1066/2160 train_time:45618ms step_avg:42.79ms
step:1067/2160 train_time:45681ms step_avg:42.81ms
step:1068/2160 train_time:45741ms step_avg:42.83ms
step:1069/2160 train_time:45804ms step_avg:42.85ms
step:1070/2160 train_time:45864ms step_avg:42.86ms
step:1071/2160 train_time:45926ms step_avg:42.88ms
step:1072/2160 train_time:45987ms step_avg:42.90ms
step:1073/2160 train_time:46049ms step_avg:42.92ms
step:1074/2160 train_time:46108ms step_avg:42.93ms
step:1075/2160 train_time:46170ms step_avg:42.95ms
step:1076/2160 train_time:46229ms step_avg:42.96ms
step:1077/2160 train_time:46291ms step_avg:42.98ms
step:1078/2160 train_time:46350ms step_avg:43.00ms
step:1079/2160 train_time:46411ms step_avg:43.01ms
step:1080/2160 train_time:46470ms step_avg:43.03ms
step:1081/2160 train_time:46532ms step_avg:43.04ms
step:1082/2160 train_time:46591ms step_avg:43.06ms
step:1083/2160 train_time:46653ms step_avg:43.08ms
step:1084/2160 train_time:46713ms step_avg:43.09ms
step:1085/2160 train_time:46775ms step_avg:43.11ms
step:1086/2160 train_time:46835ms step_avg:43.13ms
step:1087/2160 train_time:46896ms step_avg:43.14ms
step:1088/2160 train_time:46956ms step_avg:43.16ms
step:1089/2160 train_time:47017ms step_avg:43.17ms
step:1090/2160 train_time:47076ms step_avg:43.19ms
step:1091/2160 train_time:47137ms step_avg:43.21ms
step:1092/2160 train_time:47197ms step_avg:43.22ms
step:1093/2160 train_time:47257ms step_avg:43.24ms
step:1094/2160 train_time:47317ms step_avg:43.25ms
step:1095/2160 train_time:47379ms step_avg:43.27ms
step:1096/2160 train_time:47438ms step_avg:43.28ms
step:1097/2160 train_time:47500ms step_avg:43.30ms
step:1098/2160 train_time:47560ms step_avg:43.31ms
step:1099/2160 train_time:47622ms step_avg:43.33ms
step:1100/2160 train_time:47682ms step_avg:43.35ms
step:1101/2160 train_time:47744ms step_avg:43.36ms
step:1102/2160 train_time:47804ms step_avg:43.38ms
step:1103/2160 train_time:47866ms step_avg:43.40ms
step:1104/2160 train_time:47925ms step_avg:43.41ms
step:1105/2160 train_time:47988ms step_avg:43.43ms
step:1106/2160 train_time:48047ms step_avg:43.44ms
step:1107/2160 train_time:48109ms step_avg:43.46ms
step:1108/2160 train_time:48169ms step_avg:43.47ms
step:1109/2160 train_time:48230ms step_avg:43.49ms
step:1110/2160 train_time:48289ms step_avg:43.50ms
step:1111/2160 train_time:48351ms step_avg:43.52ms
step:1112/2160 train_time:48410ms step_avg:43.53ms
step:1113/2160 train_time:48472ms step_avg:43.55ms
step:1114/2160 train_time:48531ms step_avg:43.56ms
step:1115/2160 train_time:48593ms step_avg:43.58ms
step:1116/2160 train_time:48652ms step_avg:43.60ms
step:1117/2160 train_time:48714ms step_avg:43.61ms
step:1118/2160 train_time:48774ms step_avg:43.63ms
step:1119/2160 train_time:48836ms step_avg:43.64ms
step:1120/2160 train_time:48896ms step_avg:43.66ms
step:1121/2160 train_time:48958ms step_avg:43.67ms
step:1122/2160 train_time:49018ms step_avg:43.69ms
step:1123/2160 train_time:49079ms step_avg:43.70ms
step:1124/2160 train_time:49138ms step_avg:43.72ms
step:1125/2160 train_time:49199ms step_avg:43.73ms
step:1126/2160 train_time:49259ms step_avg:43.75ms
step:1127/2160 train_time:49320ms step_avg:43.76ms
step:1128/2160 train_time:49379ms step_avg:43.78ms
step:1129/2160 train_time:49441ms step_avg:43.79ms
step:1130/2160 train_time:49501ms step_avg:43.81ms
step:1131/2160 train_time:49562ms step_avg:43.82ms
step:1132/2160 train_time:49623ms step_avg:43.84ms
step:1133/2160 train_time:49685ms step_avg:43.85ms
step:1134/2160 train_time:49745ms step_avg:43.87ms
step:1135/2160 train_time:49806ms step_avg:43.88ms
step:1136/2160 train_time:49866ms step_avg:43.90ms
step:1137/2160 train_time:49927ms step_avg:43.91ms
step:1138/2160 train_time:49987ms step_avg:43.93ms
step:1139/2160 train_time:50049ms step_avg:43.94ms
step:1140/2160 train_time:50109ms step_avg:43.95ms
step:1141/2160 train_time:50170ms step_avg:43.97ms
step:1142/2160 train_time:50229ms step_avg:43.98ms
step:1143/2160 train_time:50290ms step_avg:44.00ms
step:1144/2160 train_time:50349ms step_avg:44.01ms
step:1145/2160 train_time:50411ms step_avg:44.03ms
step:1146/2160 train_time:50470ms step_avg:44.04ms
step:1147/2160 train_time:50532ms step_avg:44.06ms
step:1148/2160 train_time:50592ms step_avg:44.07ms
step:1149/2160 train_time:50654ms step_avg:44.09ms
step:1150/2160 train_time:50714ms step_avg:44.10ms
step:1151/2160 train_time:50776ms step_avg:44.11ms
step:1152/2160 train_time:50836ms step_avg:44.13ms
step:1153/2160 train_time:50897ms step_avg:44.14ms
step:1154/2160 train_time:50957ms step_avg:44.16ms
step:1155/2160 train_time:51018ms step_avg:44.17ms
step:1156/2160 train_time:51078ms step_avg:44.18ms
step:1157/2160 train_time:51139ms step_avg:44.20ms
step:1158/2160 train_time:51198ms step_avg:44.21ms
step:1159/2160 train_time:51259ms step_avg:44.23ms
step:1160/2160 train_time:51319ms step_avg:44.24ms
step:1161/2160 train_time:51380ms step_avg:44.25ms
step:1162/2160 train_time:51439ms step_avg:44.27ms
step:1163/2160 train_time:51501ms step_avg:44.28ms
step:1164/2160 train_time:51561ms step_avg:44.30ms
step:1165/2160 train_time:51624ms step_avg:44.31ms
step:1166/2160 train_time:51684ms step_avg:44.33ms
step:1167/2160 train_time:51746ms step_avg:44.34ms
step:1168/2160 train_time:51805ms step_avg:44.35ms
step:1169/2160 train_time:51867ms step_avg:44.37ms
step:1170/2160 train_time:51927ms step_avg:44.38ms
step:1171/2160 train_time:51989ms step_avg:44.40ms
step:1172/2160 train_time:52048ms step_avg:44.41ms
step:1173/2160 train_time:52110ms step_avg:44.42ms
step:1174/2160 train_time:52170ms step_avg:44.44ms
step:1175/2160 train_time:52231ms step_avg:44.45ms
step:1176/2160 train_time:52290ms step_avg:44.46ms
step:1177/2160 train_time:52351ms step_avg:44.48ms
step:1178/2160 train_time:52411ms step_avg:44.49ms
step:1179/2160 train_time:52473ms step_avg:44.51ms
step:1180/2160 train_time:52532ms step_avg:44.52ms
step:1181/2160 train_time:52594ms step_avg:44.53ms
step:1182/2160 train_time:52654ms step_avg:44.55ms
step:1183/2160 train_time:52716ms step_avg:44.56ms
step:1184/2160 train_time:52776ms step_avg:44.57ms
step:1185/2160 train_time:52838ms step_avg:44.59ms
step:1186/2160 train_time:52898ms step_avg:44.60ms
step:1187/2160 train_time:52959ms step_avg:44.62ms
step:1188/2160 train_time:53018ms step_avg:44.63ms
step:1189/2160 train_time:53079ms step_avg:44.64ms
step:1190/2160 train_time:53139ms step_avg:44.65ms
step:1191/2160 train_time:53200ms step_avg:44.67ms
step:1192/2160 train_time:53259ms step_avg:44.68ms
step:1193/2160 train_time:53321ms step_avg:44.69ms
step:1194/2160 train_time:53380ms step_avg:44.71ms
step:1195/2160 train_time:53442ms step_avg:44.72ms
step:1196/2160 train_time:53502ms step_avg:44.73ms
step:1197/2160 train_time:53564ms step_avg:44.75ms
step:1198/2160 train_time:53624ms step_avg:44.76ms
step:1199/2160 train_time:53686ms step_avg:44.78ms
step:1200/2160 train_time:53746ms step_avg:44.79ms
step:1201/2160 train_time:53807ms step_avg:44.80ms
step:1202/2160 train_time:53867ms step_avg:44.81ms
step:1203/2160 train_time:53929ms step_avg:44.83ms
step:1204/2160 train_time:53988ms step_avg:44.84ms
step:1205/2160 train_time:54050ms step_avg:44.85ms
step:1206/2160 train_time:54110ms step_avg:44.87ms
step:1207/2160 train_time:54171ms step_avg:44.88ms
step:1208/2160 train_time:54230ms step_avg:44.89ms
step:1209/2160 train_time:54291ms step_avg:44.91ms
step:1210/2160 train_time:54350ms step_avg:44.92ms
step:1211/2160 train_time:54411ms step_avg:44.93ms
step:1212/2160 train_time:54471ms step_avg:44.94ms
step:1213/2160 train_time:54533ms step_avg:44.96ms
step:1214/2160 train_time:54593ms step_avg:44.97ms
step:1215/2160 train_time:54654ms step_avg:44.98ms
step:1216/2160 train_time:54715ms step_avg:45.00ms
step:1217/2160 train_time:54777ms step_avg:45.01ms
step:1218/2160 train_time:54837ms step_avg:45.02ms
step:1219/2160 train_time:54898ms step_avg:45.04ms
step:1220/2160 train_time:54957ms step_avg:45.05ms
step:1221/2160 train_time:55018ms step_avg:45.06ms
step:1222/2160 train_time:55078ms step_avg:45.07ms
step:1223/2160 train_time:55139ms step_avg:45.09ms
step:1224/2160 train_time:55198ms step_avg:45.10ms
step:1225/2160 train_time:55260ms step_avg:45.11ms
step:1226/2160 train_time:55319ms step_avg:45.12ms
step:1227/2160 train_time:55380ms step_avg:45.13ms
step:1228/2160 train_time:55440ms step_avg:45.15ms
step:1229/2160 train_time:55503ms step_avg:45.16ms
step:1230/2160 train_time:55563ms step_avg:45.17ms
step:1231/2160 train_time:55625ms step_avg:45.19ms
step:1232/2160 train_time:55685ms step_avg:45.20ms
step:1233/2160 train_time:55746ms step_avg:45.21ms
step:1234/2160 train_time:55807ms step_avg:45.22ms
step:1235/2160 train_time:55868ms step_avg:45.24ms
step:1236/2160 train_time:55928ms step_avg:45.25ms
step:1237/2160 train_time:55989ms step_avg:45.26ms
step:1238/2160 train_time:56049ms step_avg:45.27ms
step:1239/2160 train_time:56110ms step_avg:45.29ms
step:1240/2160 train_time:56170ms step_avg:45.30ms
step:1241/2160 train_time:56232ms step_avg:45.31ms
step:1242/2160 train_time:56291ms step_avg:45.32ms
step:1243/2160 train_time:56353ms step_avg:45.34ms
step:1244/2160 train_time:56414ms step_avg:45.35ms
step:1245/2160 train_time:56475ms step_avg:45.36ms
step:1246/2160 train_time:56536ms step_avg:45.37ms
step:1247/2160 train_time:56598ms step_avg:45.39ms
step:1248/2160 train_time:56658ms step_avg:45.40ms
step:1249/2160 train_time:56719ms step_avg:45.41ms
step:1250/2160 train_time:56778ms step_avg:45.42ms
step:1250/2160 val_loss:3.5780 train_time:56841ms step_avg:45.47ms
step:1251/2160 train_time:56860ms step_avg:45.45ms
step:1252/2160 train_time:56902ms step_avg:45.45ms
step:1253/2160 train_time:56965ms step_avg:45.46ms
step:1254/2160 train_time:57025ms step_avg:45.47ms
step:1255/2160 train_time:57087ms step_avg:45.49ms
step:1256/2160 train_time:57147ms step_avg:45.50ms
step:1257/2160 train_time:57207ms step_avg:45.51ms
step:1258/2160 train_time:57267ms step_avg:45.52ms
step:1259/2160 train_time:57329ms step_avg:45.54ms
step:1260/2160 train_time:57388ms step_avg:45.55ms
step:1261/2160 train_time:57448ms step_avg:45.56ms
step:1262/2160 train_time:57507ms step_avg:45.57ms
step:1263/2160 train_time:57569ms step_avg:45.58ms
step:1264/2160 train_time:57629ms step_avg:45.59ms
step:1265/2160 train_time:57690ms step_avg:45.60ms
step:1266/2160 train_time:57750ms step_avg:45.62ms
step:1267/2160 train_time:57813ms step_avg:45.63ms
step:1268/2160 train_time:57874ms step_avg:45.64ms
step:1269/2160 train_time:57937ms step_avg:45.66ms
step:1270/2160 train_time:57998ms step_avg:45.67ms
step:1271/2160 train_time:58059ms step_avg:45.68ms
step:1272/2160 train_time:58119ms step_avg:45.69ms
step:1273/2160 train_time:58180ms step_avg:45.70ms
step:1274/2160 train_time:58239ms step_avg:45.71ms
step:1275/2160 train_time:58301ms step_avg:45.73ms
step:1276/2160 train_time:58359ms step_avg:45.74ms
step:1277/2160 train_time:58420ms step_avg:45.75ms
step:1278/2160 train_time:58480ms step_avg:45.76ms
step:1279/2160 train_time:58541ms step_avg:45.77ms
step:1280/2160 train_time:58600ms step_avg:45.78ms
step:1281/2160 train_time:58662ms step_avg:45.79ms
step:1282/2160 train_time:58722ms step_avg:45.80ms
step:1283/2160 train_time:58784ms step_avg:45.82ms
step:1284/2160 train_time:58844ms step_avg:45.83ms
step:1285/2160 train_time:58907ms step_avg:45.84ms
step:1286/2160 train_time:58967ms step_avg:45.85ms
step:1287/2160 train_time:59030ms step_avg:45.87ms
step:1288/2160 train_time:59089ms step_avg:45.88ms
step:1289/2160 train_time:59151ms step_avg:45.89ms
step:1290/2160 train_time:59210ms step_avg:45.90ms
step:1291/2160 train_time:59271ms step_avg:45.91ms
step:1292/2160 train_time:59331ms step_avg:45.92ms
step:1293/2160 train_time:59391ms step_avg:45.93ms
step:1294/2160 train_time:59451ms step_avg:45.94ms
step:1295/2160 train_time:59512ms step_avg:45.96ms
step:1296/2160 train_time:59571ms step_avg:45.97ms
step:1297/2160 train_time:59632ms step_avg:45.98ms
step:1298/2160 train_time:59692ms step_avg:45.99ms
step:1299/2160 train_time:59754ms step_avg:46.00ms
step:1300/2160 train_time:59814ms step_avg:46.01ms
step:1301/2160 train_time:59875ms step_avg:46.02ms
step:1302/2160 train_time:59936ms step_avg:46.03ms
step:1303/2160 train_time:59998ms step_avg:46.05ms
step:1304/2160 train_time:60057ms step_avg:46.06ms
step:1305/2160 train_time:60118ms step_avg:46.07ms
step:1306/2160 train_time:60178ms step_avg:46.08ms
step:1307/2160 train_time:60239ms step_avg:46.09ms
step:1308/2160 train_time:60298ms step_avg:46.10ms
step:1309/2160 train_time:60359ms step_avg:46.11ms
step:1310/2160 train_time:60419ms step_avg:46.12ms
step:1311/2160 train_time:60480ms step_avg:46.13ms
step:1312/2160 train_time:60540ms step_avg:46.14ms
step:1313/2160 train_time:60601ms step_avg:46.15ms
step:1314/2160 train_time:60660ms step_avg:46.16ms
step:1315/2160 train_time:60722ms step_avg:46.18ms
step:1316/2160 train_time:60782ms step_avg:46.19ms
step:1317/2160 train_time:60845ms step_avg:46.20ms
step:1318/2160 train_time:60905ms step_avg:46.21ms
step:1319/2160 train_time:60967ms step_avg:46.22ms
step:1320/2160 train_time:61028ms step_avg:46.23ms
step:1321/2160 train_time:61090ms step_avg:46.25ms
step:1322/2160 train_time:61149ms step_avg:46.26ms
step:1323/2160 train_time:61210ms step_avg:46.27ms
step:1324/2160 train_time:61270ms step_avg:46.28ms
step:1325/2160 train_time:61331ms step_avg:46.29ms
step:1326/2160 train_time:61390ms step_avg:46.30ms
step:1327/2160 train_time:61452ms step_avg:46.31ms
step:1328/2160 train_time:61511ms step_avg:46.32ms
step:1329/2160 train_time:61573ms step_avg:46.33ms
step:1330/2160 train_time:61633ms step_avg:46.34ms
step:1331/2160 train_time:61694ms step_avg:46.35ms
step:1332/2160 train_time:61753ms step_avg:46.36ms
step:1333/2160 train_time:61814ms step_avg:46.37ms
step:1334/2160 train_time:61874ms step_avg:46.38ms
step:1335/2160 train_time:61936ms step_avg:46.39ms
step:1336/2160 train_time:61996ms step_avg:46.40ms
step:1337/2160 train_time:62057ms step_avg:46.42ms
step:1338/2160 train_time:62117ms step_avg:46.43ms
step:1339/2160 train_time:62178ms step_avg:46.44ms
step:1340/2160 train_time:62238ms step_avg:46.45ms
step:1341/2160 train_time:62299ms step_avg:46.46ms
step:1342/2160 train_time:62358ms step_avg:46.47ms
step:1343/2160 train_time:62420ms step_avg:46.48ms
step:1344/2160 train_time:62480ms step_avg:46.49ms
step:1345/2160 train_time:62541ms step_avg:46.50ms
step:1346/2160 train_time:62601ms step_avg:46.51ms
step:1347/2160 train_time:62663ms step_avg:46.52ms
step:1348/2160 train_time:62723ms step_avg:46.53ms
step:1349/2160 train_time:62784ms step_avg:46.54ms
step:1350/2160 train_time:62845ms step_avg:46.55ms
step:1351/2160 train_time:62906ms step_avg:46.56ms
step:1352/2160 train_time:62967ms step_avg:46.57ms
step:1353/2160 train_time:63030ms step_avg:46.59ms
step:1354/2160 train_time:63089ms step_avg:46.59ms
step:1355/2160 train_time:63151ms step_avg:46.61ms
step:1356/2160 train_time:63210ms step_avg:46.62ms
step:1357/2160 train_time:63272ms step_avg:46.63ms
step:1358/2160 train_time:63331ms step_avg:46.64ms
step:1359/2160 train_time:63392ms step_avg:46.65ms
step:1360/2160 train_time:63451ms step_avg:46.66ms
step:1361/2160 train_time:63512ms step_avg:46.67ms
step:1362/2160 train_time:63572ms step_avg:46.68ms
step:1363/2160 train_time:63634ms step_avg:46.69ms
step:1364/2160 train_time:63694ms step_avg:46.70ms
step:1365/2160 train_time:63756ms step_avg:46.71ms
step:1366/2160 train_time:63816ms step_avg:46.72ms
step:1367/2160 train_time:63878ms step_avg:46.73ms
step:1368/2160 train_time:63939ms step_avg:46.74ms
step:1369/2160 train_time:63999ms step_avg:46.75ms
step:1370/2160 train_time:64058ms step_avg:46.76ms
step:1371/2160 train_time:64120ms step_avg:46.77ms
step:1372/2160 train_time:64179ms step_avg:46.78ms
step:1373/2160 train_time:64241ms step_avg:46.79ms
step:1374/2160 train_time:64300ms step_avg:46.80ms
step:1375/2160 train_time:64362ms step_avg:46.81ms
step:1376/2160 train_time:64421ms step_avg:46.82ms
step:1377/2160 train_time:64484ms step_avg:46.83ms
step:1378/2160 train_time:64543ms step_avg:46.84ms
step:1379/2160 train_time:64605ms step_avg:46.85ms
step:1380/2160 train_time:64665ms step_avg:46.86ms
step:1381/2160 train_time:64727ms step_avg:46.87ms
step:1382/2160 train_time:64787ms step_avg:46.88ms
step:1383/2160 train_time:64850ms step_avg:46.89ms
step:1384/2160 train_time:64909ms step_avg:46.90ms
step:1385/2160 train_time:64970ms step_avg:46.91ms
step:1386/2160 train_time:65030ms step_avg:46.92ms
step:1387/2160 train_time:65092ms step_avg:46.93ms
step:1388/2160 train_time:65152ms step_avg:46.94ms
step:1389/2160 train_time:65213ms step_avg:46.95ms
step:1390/2160 train_time:65273ms step_avg:46.96ms
step:1391/2160 train_time:65335ms step_avg:46.97ms
step:1392/2160 train_time:65394ms step_avg:46.98ms
step:1393/2160 train_time:65455ms step_avg:46.99ms
step:1394/2160 train_time:65515ms step_avg:47.00ms
step:1395/2160 train_time:65576ms step_avg:47.01ms
step:1396/2160 train_time:65636ms step_avg:47.02ms
step:1397/2160 train_time:65697ms step_avg:47.03ms
step:1398/2160 train_time:65758ms step_avg:47.04ms
step:1399/2160 train_time:65819ms step_avg:47.05ms
step:1400/2160 train_time:65879ms step_avg:47.06ms
step:1401/2160 train_time:65940ms step_avg:47.07ms
step:1402/2160 train_time:66000ms step_avg:47.08ms
step:1403/2160 train_time:66062ms step_avg:47.09ms
step:1404/2160 train_time:66122ms step_avg:47.10ms
step:1405/2160 train_time:66184ms step_avg:47.11ms
step:1406/2160 train_time:66244ms step_avg:47.12ms
step:1407/2160 train_time:66306ms step_avg:47.13ms
step:1408/2160 train_time:66366ms step_avg:47.13ms
step:1409/2160 train_time:66428ms step_avg:47.15ms
step:1410/2160 train_time:66487ms step_avg:47.15ms
step:1411/2160 train_time:66549ms step_avg:47.16ms
step:1412/2160 train_time:66609ms step_avg:47.17ms
step:1413/2160 train_time:66670ms step_avg:47.18ms
step:1414/2160 train_time:66730ms step_avg:47.19ms
step:1415/2160 train_time:66791ms step_avg:47.20ms
step:1416/2160 train_time:66879ms step_avg:47.23ms
step:1417/2160 train_time:66969ms step_avg:47.26ms
step:1418/2160 train_time:67057ms step_avg:47.29ms
step:1419/2160 train_time:67147ms step_avg:47.32ms
step:1420/2160 train_time:67235ms step_avg:47.35ms
step:1421/2160 train_time:67324ms step_avg:47.38ms
step:1422/2160 train_time:67412ms step_avg:47.41ms
step:1423/2160 train_time:67501ms step_avg:47.44ms
step:1424/2160 train_time:67589ms step_avg:47.46ms
step:1425/2160 train_time:67680ms step_avg:47.49ms
step:1426/2160 train_time:67767ms step_avg:47.52ms
step:1427/2160 train_time:67856ms step_avg:47.55ms
step:1428/2160 train_time:67943ms step_avg:47.58ms
step:1429/2160 train_time:68033ms step_avg:47.61ms
step:1430/2160 train_time:68120ms step_avg:47.64ms
step:1431/2160 train_time:68209ms step_avg:47.67ms
step:1432/2160 train_time:68297ms step_avg:47.69ms
step:1433/2160 train_time:68386ms step_avg:47.72ms
step:1434/2160 train_time:68473ms step_avg:47.75ms
step:1435/2160 train_time:68563ms step_avg:47.78ms
step:1436/2160 train_time:68650ms step_avg:47.81ms
step:1437/2160 train_time:68741ms step_avg:47.84ms
step:1438/2160 train_time:68828ms step_avg:47.86ms
step:1439/2160 train_time:68917ms step_avg:47.89ms
step:1440/2160 train_time:69004ms step_avg:47.92ms
step:1441/2160 train_time:69093ms step_avg:47.95ms
step:1442/2160 train_time:69180ms step_avg:47.98ms
step:1443/2160 train_time:69271ms step_avg:48.00ms
step:1444/2160 train_time:69358ms step_avg:48.03ms
step:1445/2160 train_time:69447ms step_avg:48.06ms
step:1446/2160 train_time:69534ms step_avg:48.09ms
step:1447/2160 train_time:69623ms step_avg:48.12ms
step:1448/2160 train_time:69710ms step_avg:48.14ms
step:1449/2160 train_time:69800ms step_avg:48.17ms
step:1450/2160 train_time:69888ms step_avg:48.20ms
step:1451/2160 train_time:69978ms step_avg:48.23ms
step:1452/2160 train_time:70064ms step_avg:48.25ms
step:1453/2160 train_time:70153ms step_avg:48.28ms
step:1454/2160 train_time:70240ms step_avg:48.31ms
step:1455/2160 train_time:70330ms step_avg:48.34ms
step:1456/2160 train_time:70416ms step_avg:48.36ms
step:1457/2160 train_time:70506ms step_avg:48.39ms
step:1458/2160 train_time:70593ms step_avg:48.42ms
step:1459/2160 train_time:70682ms step_avg:48.45ms
step:1460/2160 train_time:70770ms step_avg:48.47ms
step:1461/2160 train_time:70860ms step_avg:48.50ms
step:1462/2160 train_time:70947ms step_avg:48.53ms
step:1463/2160 train_time:71036ms step_avg:48.55ms
step:1464/2160 train_time:71122ms step_avg:48.58ms
step:1465/2160 train_time:71212ms step_avg:48.61ms
step:1466/2160 train_time:71299ms step_avg:48.63ms
step:1467/2160 train_time:71388ms step_avg:48.66ms
step:1468/2160 train_time:71475ms step_avg:48.69ms
step:1469/2160 train_time:71564ms step_avg:48.72ms
step:1470/2160 train_time:71651ms step_avg:48.74ms
step:1471/2160 train_time:71741ms step_avg:48.77ms
step:1472/2160 train_time:71828ms step_avg:48.80ms
step:1473/2160 train_time:71917ms step_avg:48.82ms
step:1474/2160 train_time:72004ms step_avg:48.85ms
step:1475/2160 train_time:72093ms step_avg:48.88ms
step:1476/2160 train_time:72181ms step_avg:48.90ms
step:1477/2160 train_time:72270ms step_avg:48.93ms
step:1478/2160 train_time:72357ms step_avg:48.96ms
step:1479/2160 train_time:72447ms step_avg:48.98ms
step:1480/2160 train_time:72534ms step_avg:49.01ms
step:1481/2160 train_time:72623ms step_avg:49.04ms
step:1482/2160 train_time:72711ms step_avg:49.06ms
step:1483/2160 train_time:72800ms step_avg:49.09ms
step:1484/2160 train_time:72888ms step_avg:49.12ms
step:1485/2160 train_time:72977ms step_avg:49.14ms
step:1486/2160 train_time:73064ms step_avg:49.17ms
step:1487/2160 train_time:73152ms step_avg:49.19ms
step:1488/2160 train_time:73240ms step_avg:49.22ms
step:1489/2160 train_time:73330ms step_avg:49.25ms
step:1490/2160 train_time:73417ms step_avg:49.27ms
step:1491/2160 train_time:73505ms step_avg:49.30ms
step:1492/2160 train_time:73593ms step_avg:49.32ms
step:1493/2160 train_time:73682ms step_avg:49.35ms
step:1494/2160 train_time:73769ms step_avg:49.38ms
step:1495/2160 train_time:73859ms step_avg:49.40ms
step:1496/2160 train_time:73947ms step_avg:49.43ms
step:1497/2160 train_time:74036ms step_avg:49.46ms
step:1498/2160 train_time:74123ms step_avg:49.48ms
step:1499/2160 train_time:74212ms step_avg:49.51ms
step:1500/2160 train_time:74300ms step_avg:49.53ms
step:1500/2160 val_loss:3.4948 train_time:74389ms step_avg:49.59ms
step:1501/2160 train_time:74408ms step_avg:49.57ms
step:1502/2160 train_time:74477ms step_avg:49.59ms
step:1503/2160 train_time:74567ms step_avg:49.61ms
step:1504/2160 train_time:74657ms step_avg:49.64ms
step:1505/2160 train_time:74745ms step_avg:49.66ms
step:1506/2160 train_time:74831ms step_avg:49.69ms
step:1507/2160 train_time:74919ms step_avg:49.71ms
step:1508/2160 train_time:75005ms step_avg:49.74ms
step:1509/2160 train_time:75093ms step_avg:49.76ms
step:1510/2160 train_time:75180ms step_avg:49.79ms
step:1511/2160 train_time:75269ms step_avg:49.81ms
step:1512/2160 train_time:75358ms step_avg:49.84ms
step:1513/2160 train_time:75450ms step_avg:49.87ms
step:1514/2160 train_time:75539ms step_avg:49.89ms
step:1515/2160 train_time:75630ms step_avg:49.92ms
step:1516/2160 train_time:75717ms step_avg:49.95ms
step:1517/2160 train_time:75807ms step_avg:49.97ms
step:1518/2160 train_time:75894ms step_avg:50.00ms
step:1519/2160 train_time:75983ms step_avg:50.02ms
step:1520/2160 train_time:76070ms step_avg:50.05ms
step:1521/2160 train_time:76159ms step_avg:50.07ms
step:1522/2160 train_time:76245ms step_avg:50.10ms
step:1523/2160 train_time:76335ms step_avg:50.12ms
step:1524/2160 train_time:76423ms step_avg:50.15ms
step:1525/2160 train_time:76512ms step_avg:50.17ms
step:1526/2160 train_time:76601ms step_avg:50.20ms
step:1527/2160 train_time:76691ms step_avg:50.22ms
step:1528/2160 train_time:76780ms step_avg:50.25ms
step:1529/2160 train_time:76869ms step_avg:50.27ms
step:1530/2160 train_time:76956ms step_avg:50.30ms
step:1531/2160 train_time:77045ms step_avg:50.32ms
step:1532/2160 train_time:77131ms step_avg:50.35ms
step:1533/2160 train_time:77220ms step_avg:50.37ms
step:1534/2160 train_time:77308ms step_avg:50.40ms
step:1535/2160 train_time:77398ms step_avg:50.42ms
step:1536/2160 train_time:77486ms step_avg:50.45ms
step:1537/2160 train_time:77576ms step_avg:50.47ms
step:1538/2160 train_time:77663ms step_avg:50.50ms
step:1539/2160 train_time:77752ms step_avg:50.52ms
step:1540/2160 train_time:77839ms step_avg:50.54ms
step:1541/2160 train_time:77928ms step_avg:50.57ms
step:1542/2160 train_time:78015ms step_avg:50.59ms
step:1543/2160 train_time:78104ms step_avg:50.62ms
step:1544/2160 train_time:78192ms step_avg:50.64ms
step:1545/2160 train_time:78280ms step_avg:50.67ms
step:1546/2160 train_time:78368ms step_avg:50.69ms
step:1547/2160 train_time:78458ms step_avg:50.72ms
step:1548/2160 train_time:78546ms step_avg:50.74ms
step:1549/2160 train_time:78635ms step_avg:50.77ms
step:1550/2160 train_time:78723ms step_avg:50.79ms
step:1551/2160 train_time:78812ms step_avg:50.81ms
step:1552/2160 train_time:78899ms step_avg:50.84ms
step:1553/2160 train_time:78988ms step_avg:50.86ms
step:1554/2160 train_time:79075ms step_avg:50.88ms
step:1555/2160 train_time:79164ms step_avg:50.91ms
step:1556/2160 train_time:79252ms step_avg:50.93ms
step:1557/2160 train_time:79341ms step_avg:50.96ms
step:1558/2160 train_time:79429ms step_avg:50.98ms
step:1559/2160 train_time:79519ms step_avg:51.01ms
step:1560/2160 train_time:79607ms step_avg:51.03ms
step:1561/2160 train_time:79697ms step_avg:51.05ms
step:1562/2160 train_time:79784ms step_avg:51.08ms
step:1563/2160 train_time:79873ms step_avg:51.10ms
step:1564/2160 train_time:79959ms step_avg:51.12ms
step:1565/2160 train_time:80049ms step_avg:51.15ms
step:1566/2160 train_time:80136ms step_avg:51.17ms
step:1567/2160 train_time:80225ms step_avg:51.20ms
step:1568/2160 train_time:80312ms step_avg:51.22ms
step:1569/2160 train_time:80401ms step_avg:51.24ms
step:1570/2160 train_time:80490ms step_avg:51.27ms
step:1571/2160 train_time:80580ms step_avg:51.29ms
step:1572/2160 train_time:80667ms step_avg:51.31ms
step:1573/2160 train_time:80756ms step_avg:51.34ms
step:1574/2160 train_time:80844ms step_avg:51.36ms
step:1575/2160 train_time:80932ms step_avg:51.39ms
step:1576/2160 train_time:81020ms step_avg:51.41ms
step:1577/2160 train_time:81110ms step_avg:51.43ms
step:1578/2160 train_time:81197ms step_avg:51.46ms
step:1579/2160 train_time:81287ms step_avg:51.48ms
step:1580/2160 train_time:81375ms step_avg:51.50ms
step:1581/2160 train_time:81465ms step_avg:51.53ms
step:1582/2160 train_time:81553ms step_avg:51.55ms
step:1583/2160 train_time:81642ms step_avg:51.57ms
step:1584/2160 train_time:81730ms step_avg:51.60ms
step:1585/2160 train_time:81820ms step_avg:51.62ms
step:1586/2160 train_time:81907ms step_avg:51.64ms
step:1587/2160 train_time:81996ms step_avg:51.67ms
step:1588/2160 train_time:82084ms step_avg:51.69ms
step:1589/2160 train_time:82172ms step_avg:51.71ms
step:1590/2160 train_time:82260ms step_avg:51.74ms
step:1591/2160 train_time:82349ms step_avg:51.76ms
step:1592/2160 train_time:82437ms step_avg:51.78ms
step:1593/2160 train_time:82527ms step_avg:51.81ms
step:1594/2160 train_time:82614ms step_avg:51.83ms
step:1595/2160 train_time:82705ms step_avg:51.85ms
step:1596/2160 train_time:82792ms step_avg:51.87ms
step:1597/2160 train_time:82881ms step_avg:51.90ms
step:1598/2160 train_time:82968ms step_avg:51.92ms
step:1599/2160 train_time:83058ms step_avg:51.94ms
step:1600/2160 train_time:83146ms step_avg:51.97ms
step:1601/2160 train_time:83235ms step_avg:51.99ms
step:1602/2160 train_time:83322ms step_avg:52.01ms
step:1603/2160 train_time:83411ms step_avg:52.03ms
step:1604/2160 train_time:83499ms step_avg:52.06ms
step:1605/2160 train_time:83590ms step_avg:52.08ms
step:1606/2160 train_time:83679ms step_avg:52.10ms
step:1607/2160 train_time:83769ms step_avg:52.13ms
step:1608/2160 train_time:83857ms step_avg:52.15ms
step:1609/2160 train_time:83947ms step_avg:52.17ms
step:1610/2160 train_time:84034ms step_avg:52.20ms
step:1611/2160 train_time:84123ms step_avg:52.22ms
step:1612/2160 train_time:84211ms step_avg:52.24ms
step:1613/2160 train_time:84300ms step_avg:52.26ms
step:1614/2160 train_time:84387ms step_avg:52.28ms
step:1615/2160 train_time:84475ms step_avg:52.31ms
step:1616/2160 train_time:84563ms step_avg:52.33ms
step:1617/2160 train_time:84652ms step_avg:52.35ms
step:1618/2160 train_time:84739ms step_avg:52.37ms
step:1619/2160 train_time:84829ms step_avg:52.40ms
step:1620/2160 train_time:84916ms step_avg:52.42ms
step:1621/2160 train_time:85007ms step_avg:52.44ms
step:1622/2160 train_time:85094ms step_avg:52.46ms
step:1623/2160 train_time:85183ms step_avg:52.48ms
step:1624/2160 train_time:85269ms step_avg:52.51ms
step:1625/2160 train_time:85359ms step_avg:52.53ms
step:1626/2160 train_time:85446ms step_avg:52.55ms
step:1627/2160 train_time:85535ms step_avg:52.57ms
step:1628/2160 train_time:85623ms step_avg:52.59ms
step:1629/2160 train_time:85712ms step_avg:52.62ms
step:1630/2160 train_time:85800ms step_avg:52.64ms
step:1631/2160 train_time:85889ms step_avg:52.66ms
step:1632/2160 train_time:85977ms step_avg:52.68ms
step:1633/2160 train_time:86066ms step_avg:52.70ms
step:1634/2160 train_time:86153ms step_avg:52.73ms
step:1635/2160 train_time:86242ms step_avg:52.75ms
step:1636/2160 train_time:86329ms step_avg:52.77ms
step:1637/2160 train_time:86418ms step_avg:52.79ms
step:1638/2160 train_time:86507ms step_avg:52.81ms
step:1639/2160 train_time:86596ms step_avg:52.83ms
step:1640/2160 train_time:86684ms step_avg:52.86ms
step:1641/2160 train_time:86773ms step_avg:52.88ms
step:1642/2160 train_time:86860ms step_avg:52.90ms
step:1643/2160 train_time:86950ms step_avg:52.92ms
step:1644/2160 train_time:87038ms step_avg:52.94ms
step:1645/2160 train_time:87127ms step_avg:52.96ms
step:1646/2160 train_time:87215ms step_avg:52.99ms
step:1647/2160 train_time:87305ms step_avg:53.01ms
step:1648/2160 train_time:87392ms step_avg:53.03ms
step:1649/2160 train_time:87482ms step_avg:53.05ms
step:1650/2160 train_time:87570ms step_avg:53.07ms
step:1651/2160 train_time:87659ms step_avg:53.09ms
step:1652/2160 train_time:87746ms step_avg:53.12ms
step:1653/2160 train_time:87835ms step_avg:53.14ms
step:1654/2160 train_time:87924ms step_avg:53.16ms
step:1655/2160 train_time:88013ms step_avg:53.18ms
step:1656/2160 train_time:88100ms step_avg:53.20ms
step:1657/2160 train_time:88189ms step_avg:53.22ms
step:1658/2160 train_time:88277ms step_avg:53.24ms
step:1659/2160 train_time:88367ms step_avg:53.27ms
step:1660/2160 train_time:88454ms step_avg:53.29ms
step:1661/2160 train_time:88544ms step_avg:53.31ms
step:1662/2160 train_time:88631ms step_avg:53.33ms
step:1663/2160 train_time:88721ms step_avg:53.35ms
step:1664/2160 train_time:88808ms step_avg:53.37ms
step:1665/2160 train_time:88897ms step_avg:53.39ms
step:1666/2160 train_time:88984ms step_avg:53.41ms
step:1667/2160 train_time:89073ms step_avg:53.43ms
step:1668/2160 train_time:89162ms step_avg:53.45ms
step:1669/2160 train_time:89251ms step_avg:53.48ms
step:1670/2160 train_time:89338ms step_avg:53.50ms
step:1671/2160 train_time:89428ms step_avg:53.52ms
step:1672/2160 train_time:89515ms step_avg:53.54ms
step:1673/2160 train_time:89606ms step_avg:53.56ms
step:1674/2160 train_time:89694ms step_avg:53.58ms
step:1675/2160 train_time:89783ms step_avg:53.60ms
step:1676/2160 train_time:89869ms step_avg:53.62ms
step:1677/2160 train_time:89960ms step_avg:53.64ms
step:1678/2160 train_time:90048ms step_avg:53.66ms
step:1679/2160 train_time:90136ms step_avg:53.68ms
step:1680/2160 train_time:90224ms step_avg:53.70ms
step:1681/2160 train_time:90313ms step_avg:53.73ms
step:1682/2160 train_time:90401ms step_avg:53.75ms
step:1683/2160 train_time:90489ms step_avg:53.77ms
step:1684/2160 train_time:90577ms step_avg:53.79ms
step:1685/2160 train_time:90667ms step_avg:53.81ms
step:1686/2160 train_time:90755ms step_avg:53.83ms
step:1687/2160 train_time:90844ms step_avg:53.85ms
step:1688/2160 train_time:90931ms step_avg:53.87ms
step:1689/2160 train_time:91020ms step_avg:53.89ms
step:1690/2160 train_time:91107ms step_avg:53.91ms
step:1691/2160 train_time:91196ms step_avg:53.93ms
step:1692/2160 train_time:91284ms step_avg:53.95ms
step:1693/2160 train_time:91373ms step_avg:53.97ms
step:1694/2160 train_time:91461ms step_avg:53.99ms
step:1695/2160 train_time:91550ms step_avg:54.01ms
step:1696/2160 train_time:91637ms step_avg:54.03ms
step:1697/2160 train_time:91726ms step_avg:54.05ms
step:1698/2160 train_time:91814ms step_avg:54.07ms
step:1699/2160 train_time:91903ms step_avg:54.09ms
step:1700/2160 train_time:91990ms step_avg:54.11ms
step:1701/2160 train_time:92079ms step_avg:54.13ms
step:1702/2160 train_time:92166ms step_avg:54.15ms
step:1703/2160 train_time:92256ms step_avg:54.17ms
step:1704/2160 train_time:92344ms step_avg:54.19ms
step:1705/2160 train_time:92433ms step_avg:54.21ms
step:1706/2160 train_time:92521ms step_avg:54.23ms
step:1707/2160 train_time:92610ms step_avg:54.25ms
step:1708/2160 train_time:92698ms step_avg:54.27ms
step:1709/2160 train_time:92787ms step_avg:54.29ms
step:1710/2160 train_time:92874ms step_avg:54.31ms
step:1711/2160 train_time:92964ms step_avg:54.33ms
step:1712/2160 train_time:93052ms step_avg:54.35ms
step:1713/2160 train_time:93141ms step_avg:54.37ms
step:1714/2160 train_time:93228ms step_avg:54.39ms
step:1715/2160 train_time:93317ms step_avg:54.41ms
step:1716/2160 train_time:93405ms step_avg:54.43ms
step:1717/2160 train_time:93494ms step_avg:54.45ms
step:1718/2160 train_time:93581ms step_avg:54.47ms
step:1719/2160 train_time:93670ms step_avg:54.49ms
step:1720/2160 train_time:93758ms step_avg:54.51ms
step:1721/2160 train_time:93848ms step_avg:54.53ms
step:1722/2160 train_time:93936ms step_avg:54.55ms
step:1723/2160 train_time:94025ms step_avg:54.57ms
step:1724/2160 train_time:94112ms step_avg:54.59ms
step:1725/2160 train_time:94201ms step_avg:54.61ms
step:1726/2160 train_time:94289ms step_avg:54.63ms
step:1727/2160 train_time:94378ms step_avg:54.65ms
step:1728/2160 train_time:94465ms step_avg:54.67ms
step:1729/2160 train_time:94554ms step_avg:54.69ms
step:1730/2160 train_time:94641ms step_avg:54.71ms
step:1731/2160 train_time:94730ms step_avg:54.73ms
step:1732/2160 train_time:94818ms step_avg:54.75ms
step:1733/2160 train_time:94909ms step_avg:54.77ms
step:1734/2160 train_time:94996ms step_avg:54.78ms
step:1735/2160 train_time:95086ms step_avg:54.80ms
step:1736/2160 train_time:95173ms step_avg:54.82ms
step:1737/2160 train_time:95262ms step_avg:54.84ms
step:1738/2160 train_time:95350ms step_avg:54.86ms
step:1739/2160 train_time:95440ms step_avg:54.88ms
step:1740/2160 train_time:95526ms step_avg:54.90ms
step:1741/2160 train_time:95615ms step_avg:54.92ms
step:1742/2160 train_time:95703ms step_avg:54.94ms
step:1743/2160 train_time:95793ms step_avg:54.96ms
step:1744/2160 train_time:95880ms step_avg:54.98ms
step:1745/2160 train_time:95969ms step_avg:55.00ms
step:1746/2160 train_time:96057ms step_avg:55.02ms
step:1747/2160 train_time:96147ms step_avg:55.04ms
step:1748/2160 train_time:96234ms step_avg:55.05ms
step:1749/2160 train_time:96324ms step_avg:55.07ms
step:1750/2160 train_time:96411ms step_avg:55.09ms
step:1750/2160 val_loss:3.3942 train_time:96502ms step_avg:55.14ms
step:1751/2160 train_time:96521ms step_avg:55.12ms
step:1752/2160 train_time:96592ms step_avg:55.13ms
step:1753/2160 train_time:96684ms step_avg:55.15ms
step:1754/2160 train_time:96772ms step_avg:55.17ms
step:1755/2160 train_time:96861ms step_avg:55.19ms
step:1756/2160 train_time:96947ms step_avg:55.21ms
step:1757/2160 train_time:97035ms step_avg:55.23ms
step:1758/2160 train_time:97122ms step_avg:55.25ms
step:1759/2160 train_time:97210ms step_avg:55.26ms
step:1760/2160 train_time:97300ms step_avg:55.28ms
step:1761/2160 train_time:97388ms step_avg:55.30ms
step:1762/2160 train_time:97477ms step_avg:55.32ms
step:1763/2160 train_time:97569ms step_avg:55.34ms
step:1764/2160 train_time:97657ms step_avg:55.36ms
step:1765/2160 train_time:97749ms step_avg:55.38ms
step:1766/2160 train_time:97837ms step_avg:55.40ms
step:1767/2160 train_time:97926ms step_avg:55.42ms
step:1768/2160 train_time:98013ms step_avg:55.44ms
step:1769/2160 train_time:98101ms step_avg:55.46ms
step:1770/2160 train_time:98187ms step_avg:55.47ms
step:1771/2160 train_time:98277ms step_avg:55.49ms
step:1772/2160 train_time:98365ms step_avg:55.51ms
step:1773/2160 train_time:98456ms step_avg:55.53ms
step:1774/2160 train_time:98545ms step_avg:55.55ms
step:1775/2160 train_time:98635ms step_avg:55.57ms
step:1776/2160 train_time:98723ms step_avg:55.59ms
step:1777/2160 train_time:98812ms step_avg:55.61ms
step:1778/2160 train_time:98900ms step_avg:55.62ms
step:1779/2160 train_time:98989ms step_avg:55.64ms
step:1780/2160 train_time:99077ms step_avg:55.66ms
step:1781/2160 train_time:99165ms step_avg:55.68ms
step:1782/2160 train_time:99252ms step_avg:55.70ms
step:1783/2160 train_time:99342ms step_avg:55.72ms
step:1784/2160 train_time:99430ms step_avg:55.73ms
step:1785/2160 train_time:99520ms step_avg:55.75ms
step:1786/2160 train_time:99608ms step_avg:55.77ms
step:1787/2160 train_time:99698ms step_avg:55.79ms
step:1788/2160 train_time:99787ms step_avg:55.81ms
step:1789/2160 train_time:99876ms step_avg:55.83ms
step:1790/2160 train_time:99963ms step_avg:55.85ms
step:1791/2160 train_time:100052ms step_avg:55.86ms
step:1792/2160 train_time:100139ms step_avg:55.88ms
step:1793/2160 train_time:100228ms step_avg:55.90ms
step:1794/2160 train_time:100315ms step_avg:55.92ms
step:1795/2160 train_time:100405ms step_avg:55.94ms
step:1796/2160 train_time:100492ms step_avg:55.95ms
step:1797/2160 train_time:100582ms step_avg:55.97ms
step:1798/2160 train_time:100671ms step_avg:55.99ms
step:1799/2160 train_time:100761ms step_avg:56.01ms
step:1800/2160 train_time:100848ms step_avg:56.03ms
step:1801/2160 train_time:100937ms step_avg:56.05ms
step:1802/2160 train_time:101024ms step_avg:56.06ms
step:1803/2160 train_time:101112ms step_avg:56.08ms
step:1804/2160 train_time:101200ms step_avg:56.10ms
step:1805/2160 train_time:101289ms step_avg:56.12ms
step:1806/2160 train_time:101377ms step_avg:56.13ms
step:1807/2160 train_time:101466ms step_avg:56.15ms
step:1808/2160 train_time:101554ms step_avg:56.17ms
step:1809/2160 train_time:101643ms step_avg:56.19ms
step:1810/2160 train_time:101730ms step_avg:56.20ms
step:1811/2160 train_time:101820ms step_avg:56.22ms
step:1812/2160 train_time:101907ms step_avg:56.24ms
step:1813/2160 train_time:101996ms step_avg:56.26ms
step:1814/2160 train_time:102084ms step_avg:56.28ms
step:1815/2160 train_time:102173ms step_avg:56.29ms
step:1816/2160 train_time:102260ms step_avg:56.31ms
step:1817/2160 train_time:102349ms step_avg:56.33ms
step:1818/2160 train_time:102437ms step_avg:56.35ms
step:1819/2160 train_time:102527ms step_avg:56.36ms
step:1820/2160 train_time:102614ms step_avg:56.38ms
step:1821/2160 train_time:102704ms step_avg:56.40ms
step:1822/2160 train_time:102792ms step_avg:56.42ms
step:1823/2160 train_time:102880ms step_avg:56.43ms
step:1824/2160 train_time:102968ms step_avg:56.45ms
step:1825/2160 train_time:103057ms step_avg:56.47ms
step:1826/2160 train_time:103144ms step_avg:56.49ms
step:1827/2160 train_time:103233ms step_avg:56.50ms
step:1828/2160 train_time:103321ms step_avg:56.52ms
step:1829/2160 train_time:103410ms step_avg:56.54ms
step:1830/2160 train_time:103498ms step_avg:56.56ms
step:1831/2160 train_time:103588ms step_avg:56.57ms
step:1832/2160 train_time:103675ms step_avg:56.59ms
step:1833/2160 train_time:103766ms step_avg:56.61ms
step:1834/2160 train_time:103853ms step_avg:56.63ms
step:1835/2160 train_time:103943ms step_avg:56.64ms
step:1836/2160 train_time:104030ms step_avg:56.66ms
step:1837/2160 train_time:104120ms step_avg:56.68ms
step:1838/2160 train_time:104207ms step_avg:56.70ms
step:1839/2160 train_time:104297ms step_avg:56.71ms
step:1840/2160 train_time:104384ms step_avg:56.73ms
step:1841/2160 train_time:104474ms step_avg:56.75ms
step:1842/2160 train_time:104562ms step_avg:56.77ms
step:1843/2160 train_time:104652ms step_avg:56.78ms
step:1844/2160 train_time:104739ms step_avg:56.80ms
step:1845/2160 train_time:104828ms step_avg:56.82ms
step:1846/2160 train_time:104915ms step_avg:56.83ms
step:1847/2160 train_time:105005ms step_avg:56.85ms
step:1848/2160 train_time:105093ms step_avg:56.87ms
step:1849/2160 train_time:105182ms step_avg:56.89ms
step:1850/2160 train_time:105269ms step_avg:56.90ms
step:1851/2160 train_time:105358ms step_avg:56.92ms
step:1852/2160 train_time:105445ms step_avg:56.94ms
step:1853/2160 train_time:105534ms step_avg:56.95ms
step:1854/2160 train_time:105622ms step_avg:56.97ms
step:1855/2160 train_time:105711ms step_avg:56.99ms
step:1856/2160 train_time:105798ms step_avg:57.00ms
step:1857/2160 train_time:105888ms step_avg:57.02ms
step:1858/2160 train_time:105976ms step_avg:57.04ms
step:1859/2160 train_time:106066ms step_avg:57.06ms
step:1860/2160 train_time:106153ms step_avg:57.07ms
step:1861/2160 train_time:106242ms step_avg:57.09ms
step:1862/2160 train_time:106330ms step_avg:57.11ms
step:1863/2160 train_time:106419ms step_avg:57.12ms
step:1864/2160 train_time:106506ms step_avg:57.14ms
step:1865/2160 train_time:106596ms step_avg:57.16ms
step:1866/2160 train_time:106683ms step_avg:57.17ms
step:1867/2160 train_time:106773ms step_avg:57.19ms
step:1868/2160 train_time:106861ms step_avg:57.21ms
step:1869/2160 train_time:106951ms step_avg:57.22ms
step:1870/2160 train_time:107038ms step_avg:57.24ms
step:1871/2160 train_time:107128ms step_avg:57.26ms
step:1872/2160 train_time:107215ms step_avg:57.27ms
step:1873/2160 train_time:107304ms step_avg:57.29ms
step:1874/2160 train_time:107391ms step_avg:57.31ms
step:1875/2160 train_time:107481ms step_avg:57.32ms
step:1876/2160 train_time:107569ms step_avg:57.34ms
step:1877/2160 train_time:107658ms step_avg:57.36ms
step:1878/2160 train_time:107746ms step_avg:57.37ms
step:1879/2160 train_time:107836ms step_avg:57.39ms
step:1880/2160 train_time:107924ms step_avg:57.41ms
step:1881/2160 train_time:108013ms step_avg:57.42ms
step:1882/2160 train_time:108101ms step_avg:57.44ms
step:1883/2160 train_time:108191ms step_avg:57.46ms
step:1884/2160 train_time:108278ms step_avg:57.47ms
step:1885/2160 train_time:108368ms step_avg:57.49ms
step:1886/2160 train_time:108455ms step_avg:57.51ms
step:1887/2160 train_time:108545ms step_avg:57.52ms
step:1888/2160 train_time:108633ms step_avg:57.54ms
step:1889/2160 train_time:108723ms step_avg:57.56ms
step:1890/2160 train_time:108811ms step_avg:57.57ms
step:1891/2160 train_time:108899ms step_avg:57.59ms
step:1892/2160 train_time:108988ms step_avg:57.60ms
step:1893/2160 train_time:109077ms step_avg:57.62ms
step:1894/2160 train_time:109165ms step_avg:57.64ms
step:1895/2160 train_time:109254ms step_avg:57.65ms
step:1896/2160 train_time:109342ms step_avg:57.67ms
step:1897/2160 train_time:109431ms step_avg:57.69ms
step:1898/2160 train_time:109518ms step_avg:57.70ms
step:1899/2160 train_time:109607ms step_avg:57.72ms
step:1900/2160 train_time:109694ms step_avg:57.73ms
step:1901/2160 train_time:109784ms step_avg:57.75ms
step:1902/2160 train_time:109872ms step_avg:57.77ms
step:1903/2160 train_time:109962ms step_avg:57.78ms
step:1904/2160 train_time:110050ms step_avg:57.80ms
step:1905/2160 train_time:110138ms step_avg:57.82ms
step:1906/2160 train_time:110225ms step_avg:57.83ms
step:1907/2160 train_time:110315ms step_avg:57.85ms
step:1908/2160 train_time:110402ms step_avg:57.86ms
step:1909/2160 train_time:110491ms step_avg:57.88ms
step:1910/2160 train_time:110578ms step_avg:57.89ms
step:1911/2160 train_time:110668ms step_avg:57.91ms
step:1912/2160 train_time:110756ms step_avg:57.93ms
step:1913/2160 train_time:110847ms step_avg:57.94ms
step:1914/2160 train_time:110934ms step_avg:57.96ms
step:1915/2160 train_time:111023ms step_avg:57.98ms
step:1916/2160 train_time:111110ms step_avg:57.99ms
step:1917/2160 train_time:111200ms step_avg:58.01ms
step:1918/2160 train_time:111287ms step_avg:58.02ms
step:1919/2160 train_time:111376ms step_avg:58.04ms
step:1920/2160 train_time:111464ms step_avg:58.05ms
step:1921/2160 train_time:111553ms step_avg:58.07ms
step:1922/2160 train_time:111640ms step_avg:58.09ms
step:1923/2160 train_time:111730ms step_avg:58.10ms
step:1924/2160 train_time:111816ms step_avg:58.12ms
step:1925/2160 train_time:111906ms step_avg:58.13ms
step:1926/2160 train_time:111993ms step_avg:58.15ms
step:1927/2160 train_time:112082ms step_avg:58.16ms
step:1928/2160 train_time:112170ms step_avg:58.18ms
step:1929/2160 train_time:112259ms step_avg:58.20ms
step:1930/2160 train_time:112347ms step_avg:58.21ms
step:1931/2160 train_time:112436ms step_avg:58.23ms
step:1932/2160 train_time:112523ms step_avg:58.24ms
step:1933/2160 train_time:112612ms step_avg:58.26ms
step:1934/2160 train_time:112699ms step_avg:58.27ms
step:1935/2160 train_time:112789ms step_avg:58.29ms
step:1936/2160 train_time:112877ms step_avg:58.30ms
step:1937/2160 train_time:112967ms step_avg:58.32ms
step:1938/2160 train_time:113055ms step_avg:58.34ms
step:1939/2160 train_time:113144ms step_avg:58.35ms
step:1940/2160 train_time:113232ms step_avg:58.37ms
step:1941/2160 train_time:113322ms step_avg:58.38ms
step:1942/2160 train_time:113409ms step_avg:58.40ms
step:1943/2160 train_time:113499ms step_avg:58.41ms
step:1944/2160 train_time:113587ms step_avg:58.43ms
step:1945/2160 train_time:113675ms step_avg:58.44ms
step:1946/2160 train_time:113762ms step_avg:58.46ms
step:1947/2160 train_time:113852ms step_avg:58.48ms
step:1948/2160 train_time:113940ms step_avg:58.49ms
step:1949/2160 train_time:114029ms step_avg:58.51ms
step:1950/2160 train_time:114117ms step_avg:58.52ms
step:1951/2160 train_time:114207ms step_avg:58.54ms
step:1952/2160 train_time:114296ms step_avg:58.55ms
step:1953/2160 train_time:114385ms step_avg:58.57ms
step:1954/2160 train_time:114473ms step_avg:58.58ms
step:1955/2160 train_time:114562ms step_avg:58.60ms
step:1956/2160 train_time:114650ms step_avg:58.61ms
step:1957/2160 train_time:114738ms step_avg:58.63ms
step:1958/2160 train_time:114826ms step_avg:58.64ms
step:1959/2160 train_time:114915ms step_avg:58.66ms
step:1960/2160 train_time:115002ms step_avg:58.67ms
step:1961/2160 train_time:115092ms step_avg:58.69ms
step:1962/2160 train_time:115179ms step_avg:58.70ms
step:1963/2160 train_time:115269ms step_avg:58.72ms
step:1964/2160 train_time:115357ms step_avg:58.74ms
step:1965/2160 train_time:115446ms step_avg:58.75ms
step:1966/2160 train_time:115535ms step_avg:58.77ms
step:1967/2160 train_time:115624ms step_avg:58.78ms
step:1968/2160 train_time:115711ms step_avg:58.80ms
step:1969/2160 train_time:115800ms step_avg:58.81ms
step:1970/2160 train_time:115887ms step_avg:58.83ms
step:1971/2160 train_time:115976ms step_avg:58.84ms
step:1972/2160 train_time:116065ms step_avg:58.86ms
step:1973/2160 train_time:116154ms step_avg:58.87ms
step:1974/2160 train_time:116241ms step_avg:58.89ms
step:1975/2160 train_time:116330ms step_avg:58.90ms
step:1976/2160 train_time:116417ms step_avg:58.92ms
step:1977/2160 train_time:116507ms step_avg:58.93ms
step:1978/2160 train_time:116594ms step_avg:58.95ms
step:1979/2160 train_time:116683ms step_avg:58.96ms
step:1980/2160 train_time:116771ms step_avg:58.98ms
step:1981/2160 train_time:116860ms step_avg:58.99ms
step:1982/2160 train_time:116948ms step_avg:59.01ms
step:1983/2160 train_time:117038ms step_avg:59.02ms
step:1984/2160 train_time:117125ms step_avg:59.03ms
step:1985/2160 train_time:117214ms step_avg:59.05ms
step:1986/2160 train_time:117301ms step_avg:59.06ms
step:1987/2160 train_time:117391ms step_avg:59.08ms
step:1988/2160 train_time:117479ms step_avg:59.09ms
step:1989/2160 train_time:117569ms step_avg:59.11ms
step:1990/2160 train_time:117656ms step_avg:59.12ms
step:1991/2160 train_time:117746ms step_avg:59.14ms
step:1992/2160 train_time:117833ms step_avg:59.15ms
step:1993/2160 train_time:117922ms step_avg:59.17ms
step:1994/2160 train_time:118009ms step_avg:59.18ms
step:1995/2160 train_time:118099ms step_avg:59.20ms
step:1996/2160 train_time:118186ms step_avg:59.21ms
step:1997/2160 train_time:118275ms step_avg:59.23ms
step:1998/2160 train_time:118363ms step_avg:59.24ms
step:1999/2160 train_time:118452ms step_avg:59.26ms
step:2000/2160 train_time:118540ms step_avg:59.27ms
step:2000/2160 val_loss:3.3142 train_time:118629ms step_avg:59.31ms
step:2001/2160 train_time:118649ms step_avg:59.29ms
step:2002/2160 train_time:118719ms step_avg:59.30ms
step:2003/2160 train_time:118812ms step_avg:59.32ms
step:2004/2160 train_time:118900ms step_avg:59.33ms
step:2005/2160 train_time:118987ms step_avg:59.35ms
step:2006/2160 train_time:119075ms step_avg:59.36ms
step:2007/2160 train_time:119162ms step_avg:59.37ms
step:2008/2160 train_time:119249ms step_avg:59.39ms
step:2009/2160 train_time:119339ms step_avg:59.40ms
step:2010/2160 train_time:119426ms step_avg:59.42ms
step:2011/2160 train_time:119516ms step_avg:59.43ms
step:2012/2160 train_time:119607ms step_avg:59.45ms
step:2013/2160 train_time:119698ms step_avg:59.46ms
step:2014/2160 train_time:119788ms step_avg:59.48ms
step:2015/2160 train_time:119878ms step_avg:59.49ms
step:2016/2160 train_time:119965ms step_avg:59.51ms
step:2017/2160 train_time:120054ms step_avg:59.52ms
step:2018/2160 train_time:120140ms step_avg:59.53ms
step:2019/2160 train_time:120229ms step_avg:59.55ms
step:2020/2160 train_time:120316ms step_avg:59.56ms
step:2021/2160 train_time:120404ms step_avg:59.58ms
step:2022/2160 train_time:120492ms step_avg:59.59ms
step:2023/2160 train_time:120582ms step_avg:59.61ms
step:2024/2160 train_time:120671ms step_avg:59.62ms
step:2025/2160 train_time:120761ms step_avg:59.64ms
step:2026/2160 train_time:120849ms step_avg:59.65ms
step:2027/2160 train_time:120939ms step_avg:59.66ms
step:2028/2160 train_time:121026ms step_avg:59.68ms
step:2029/2160 train_time:121115ms step_avg:59.69ms
step:2030/2160 train_time:121202ms step_avg:59.71ms
step:2031/2160 train_time:121291ms step_avg:59.72ms
step:2032/2160 train_time:121378ms step_avg:59.73ms
step:2033/2160 train_time:121468ms step_avg:59.75ms
step:2034/2160 train_time:121556ms step_avg:59.76ms
step:2035/2160 train_time:121646ms step_avg:59.78ms
step:2036/2160 train_time:121733ms step_avg:59.79ms
step:2037/2160 train_time:121823ms step_avg:59.81ms
step:2038/2160 train_time:121911ms step_avg:59.82ms
step:2039/2160 train_time:122000ms step_avg:59.83ms
step:2040/2160 train_time:122087ms step_avg:59.85ms
step:2041/2160 train_time:122176ms step_avg:59.86ms
step:2042/2160 train_time:122263ms step_avg:59.87ms
step:2043/2160 train_time:122352ms step_avg:59.89ms
step:2044/2160 train_time:122439ms step_avg:59.90ms
step:2045/2160 train_time:122528ms step_avg:59.92ms
step:2046/2160 train_time:122616ms step_avg:59.93ms
step:2047/2160 train_time:122707ms step_avg:59.94ms
step:2048/2160 train_time:122795ms step_avg:59.96ms
step:2049/2160 train_time:122884ms step_avg:59.97ms
step:2050/2160 train_time:122972ms step_avg:59.99ms
step:2051/2160 train_time:123061ms step_avg:60.00ms
step:2052/2160 train_time:123148ms step_avg:60.01ms
step:2053/2160 train_time:123237ms step_avg:60.03ms
step:2054/2160 train_time:123324ms step_avg:60.04ms
step:2055/2160 train_time:123413ms step_avg:60.06ms
step:2056/2160 train_time:123501ms step_avg:60.07ms
step:2057/2160 train_time:123590ms step_avg:60.08ms
step:2058/2160 train_time:123679ms step_avg:60.10ms
step:2059/2160 train_time:123768ms step_avg:60.11ms
step:2060/2160 train_time:123855ms step_avg:60.12ms
step:2061/2160 train_time:123944ms step_avg:60.14ms
step:2062/2160 train_time:124032ms step_avg:60.15ms
step:2063/2160 train_time:124121ms step_avg:60.17ms
step:2064/2160 train_time:124209ms step_avg:60.18ms
step:2065/2160 train_time:124298ms step_avg:60.19ms
step:2066/2160 train_time:124385ms step_avg:60.21ms
step:2067/2160 train_time:124475ms step_avg:60.22ms
step:2068/2160 train_time:124563ms step_avg:60.23ms
step:2069/2160 train_time:124652ms step_avg:60.25ms
step:2070/2160 train_time:124739ms step_avg:60.26ms
step:2071/2160 train_time:124830ms step_avg:60.28ms
step:2072/2160 train_time:124918ms step_avg:60.29ms
step:2073/2160 train_time:125008ms step_avg:60.30ms
step:2074/2160 train_time:125096ms step_avg:60.32ms
step:2075/2160 train_time:125184ms step_avg:60.33ms
step:2076/2160 train_time:125272ms step_avg:60.34ms
step:2077/2160 train_time:125361ms step_avg:60.36ms
step:2078/2160 train_time:125448ms step_avg:60.37ms
step:2079/2160 train_time:125537ms step_avg:60.38ms
step:2080/2160 train_time:125625ms step_avg:60.40ms
step:2081/2160 train_time:125714ms step_avg:60.41ms
step:2082/2160 train_time:125802ms step_avg:60.42ms
step:2083/2160 train_time:125891ms step_avg:60.44ms
step:2084/2160 train_time:125978ms step_avg:60.45ms
step:2085/2160 train_time:126067ms step_avg:60.46ms
step:2086/2160 train_time:126154ms step_avg:60.48ms
step:2087/2160 train_time:126244ms step_avg:60.49ms
step:2088/2160 train_time:126332ms step_avg:60.50ms
step:2089/2160 train_time:126421ms step_avg:60.52ms
step:2090/2160 train_time:126508ms step_avg:60.53ms
step:2091/2160 train_time:126597ms step_avg:60.54ms
step:2092/2160 train_time:126684ms step_avg:60.56ms
step:2093/2160 train_time:126774ms step_avg:60.57ms
step:2094/2160 train_time:126861ms step_avg:60.58ms
step:2095/2160 train_time:126950ms step_avg:60.60ms
step:2096/2160 train_time:127037ms step_avg:60.61ms
step:2097/2160 train_time:127126ms step_avg:60.62ms
step:2098/2160 train_time:127213ms step_avg:60.64ms
step:2099/2160 train_time:127302ms step_avg:60.65ms
step:2100/2160 train_time:127390ms step_avg:60.66ms
step:2101/2160 train_time:127481ms step_avg:60.68ms
step:2102/2160 train_time:127568ms step_avg:60.69ms
step:2103/2160 train_time:127657ms step_avg:60.70ms
step:2104/2160 train_time:127744ms step_avg:60.71ms
step:2105/2160 train_time:127834ms step_avg:60.73ms
step:2106/2160 train_time:127922ms step_avg:60.74ms
step:2107/2160 train_time:128011ms step_avg:60.75ms
step:2108/2160 train_time:128098ms step_avg:60.77ms
step:2109/2160 train_time:128187ms step_avg:60.78ms
step:2110/2160 train_time:128275ms step_avg:60.79ms
step:2111/2160 train_time:128364ms step_avg:60.81ms
step:2112/2160 train_time:128452ms step_avg:60.82ms
step:2113/2160 train_time:128541ms step_avg:60.83ms
step:2114/2160 train_time:128629ms step_avg:60.85ms
step:2115/2160 train_time:128719ms step_avg:60.86ms
step:2116/2160 train_time:128807ms step_avg:60.87ms
step:2117/2160 train_time:128896ms step_avg:60.89ms
step:2118/2160 train_time:128983ms step_avg:60.90ms
step:2119/2160 train_time:129073ms step_avg:60.91ms
step:2120/2160 train_time:129161ms step_avg:60.92ms
step:2121/2160 train_time:129251ms step_avg:60.94ms
step:2122/2160 train_time:129338ms step_avg:60.95ms
step:2123/2160 train_time:129429ms step_avg:60.97ms
step:2124/2160 train_time:129516ms step_avg:60.98ms
step:2125/2160 train_time:129606ms step_avg:60.99ms
step:2126/2160 train_time:129694ms step_avg:61.00ms
step:2127/2160 train_time:129784ms step_avg:61.02ms
step:2128/2160 train_time:129873ms step_avg:61.03ms
step:2129/2160 train_time:129962ms step_avg:61.04ms
step:2130/2160 train_time:130050ms step_avg:61.06ms
step:2131/2160 train_time:130140ms step_avg:61.07ms
step:2132/2160 train_time:130228ms step_avg:61.08ms
step:2133/2160 train_time:130317ms step_avg:61.10ms
step:2134/2160 train_time:130405ms step_avg:61.11ms
step:2135/2160 train_time:130495ms step_avg:61.12ms
step:2136/2160 train_time:130582ms step_avg:61.13ms
step:2137/2160 train_time:130672ms step_avg:61.15ms
step:2138/2160 train_time:130759ms step_avg:61.16ms
step:2139/2160 train_time:130849ms step_avg:61.17ms
step:2140/2160 train_time:130937ms step_avg:61.19ms
step:2141/2160 train_time:131027ms step_avg:61.20ms
step:2142/2160 train_time:131114ms step_avg:61.21ms
step:2143/2160 train_time:131205ms step_avg:61.23ms
step:2144/2160 train_time:131293ms step_avg:61.24ms
step:2145/2160 train_time:131383ms step_avg:61.25ms
step:2146/2160 train_time:131471ms step_avg:61.26ms
step:2147/2160 train_time:131561ms step_avg:61.28ms
step:2148/2160 train_time:131648ms step_avg:61.29ms
step:2149/2160 train_time:131737ms step_avg:61.30ms
step:2150/2160 train_time:131826ms step_avg:61.31ms
step:2151/2160 train_time:131916ms step_avg:61.33ms
step:2152/2160 train_time:132003ms step_avg:61.34ms
step:2153/2160 train_time:132092ms step_avg:61.35ms
step:2154/2160 train_time:132180ms step_avg:61.37ms
step:2155/2160 train_time:132271ms step_avg:61.38ms
step:2156/2160 train_time:132358ms step_avg:61.39ms
step:2157/2160 train_time:132448ms step_avg:61.40ms
step:2158/2160 train_time:132536ms step_avg:61.42ms
step:2159/2160 train_time:132625ms step_avg:61.43ms
step:2160/2160 train_time:132713ms step_avg:61.44ms
step:2160/2160 val_loss:3.2788 train_time:132803ms step_avg:61.48ms
peak memory allocated: 29896 MiB reserved: 61208 MiB

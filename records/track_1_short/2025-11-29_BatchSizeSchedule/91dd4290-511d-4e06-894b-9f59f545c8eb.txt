import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = (16 / 8) * 0.8
    if x > 0.66:
        lr_max = (24 / 8) * 0.8
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Sun Nov 30 03:16:59 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   36C    P0            122W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0            108W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   34C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.07ms
step:1/2160 train_time:95ms step_avg:94.96ms
step:2/2160 train_time:134ms step_avg:66.91ms
step:3/2160 train_time:153ms step_avg:51.10ms
step:4/2160 train_time:174ms step_avg:43.45ms
step:5/2160 train_time:208ms step_avg:41.55ms
step:6/2160 train_time:291ms step_avg:48.48ms
step:7/2160 train_time:314ms step_avg:44.85ms
step:8/2160 train_time:347ms step_avg:43.34ms
step:9/2160 train_time:381ms step_avg:42.29ms
step:10/2160 train_time:413ms step_avg:41.34ms
step:11/2160 train_time:448ms step_avg:40.71ms
step:12/2160 train_time:481ms step_avg:40.05ms
step:13/2160 train_time:515ms step_avg:39.62ms
step:14/2160 train_time:548ms step_avg:39.14ms
step:15/2160 train_time:582ms step_avg:38.83ms
step:16/2160 train_time:615ms step_avg:38.45ms
step:17/2160 train_time:649ms step_avg:38.20ms
step:18/2160 train_time:682ms step_avg:37.90ms
step:19/2160 train_time:717ms step_avg:37.71ms
step:20/2160 train_time:749ms step_avg:37.47ms
step:21/2160 train_time:784ms step_avg:37.31ms
step:22/2160 train_time:817ms step_avg:37.12ms
step:23/2160 train_time:851ms step_avg:36.99ms
step:24/2160 train_time:884ms step_avg:36.84ms
step:25/2160 train_time:918ms step_avg:36.72ms
step:26/2160 train_time:951ms step_avg:36.58ms
step:27/2160 train_time:985ms step_avg:36.50ms
step:28/2160 train_time:1019ms step_avg:36.38ms
step:29/2160 train_time:1053ms step_avg:36.31ms
step:30/2160 train_time:1086ms step_avg:36.19ms
step:31/2160 train_time:1120ms step_avg:36.13ms
step:32/2160 train_time:1153ms step_avg:36.03ms
step:33/2160 train_time:1187ms step_avg:35.98ms
step:34/2160 train_time:1221ms step_avg:35.93ms
step:35/2160 train_time:1256ms step_avg:35.89ms
step:36/2160 train_time:1290ms step_avg:35.82ms
step:37/2160 train_time:1324ms step_avg:35.79ms
step:38/2160 train_time:1358ms step_avg:35.73ms
step:39/2160 train_time:1392ms step_avg:35.69ms
step:40/2160 train_time:1425ms step_avg:35.63ms
step:41/2160 train_time:1460ms step_avg:35.60ms
step:42/2160 train_time:1493ms step_avg:35.54ms
step:43/2160 train_time:1528ms step_avg:35.52ms
step:44/2160 train_time:1561ms step_avg:35.47ms
step:45/2160 train_time:1595ms step_avg:35.45ms
step:46/2160 train_time:1629ms step_avg:35.41ms
step:47/2160 train_time:1663ms step_avg:35.37ms
step:48/2160 train_time:1695ms step_avg:35.32ms
step:49/2160 train_time:1730ms step_avg:35.31ms
step:50/2160 train_time:1763ms step_avg:35.26ms
step:51/2160 train_time:1797ms step_avg:35.24ms
step:52/2160 train_time:1831ms step_avg:35.22ms
step:53/2160 train_time:1865ms step_avg:35.18ms
step:54/2160 train_time:1898ms step_avg:35.14ms
step:55/2160 train_time:1932ms step_avg:35.13ms
step:56/2160 train_time:1965ms step_avg:35.09ms
step:57/2160 train_time:2000ms step_avg:35.08ms
step:58/2160 train_time:2033ms step_avg:35.05ms
step:59/2160 train_time:2067ms step_avg:35.03ms
step:60/2160 train_time:2100ms step_avg:35.00ms
step:61/2160 train_time:2134ms step_avg:34.99ms
step:62/2160 train_time:2167ms step_avg:34.95ms
step:63/2160 train_time:2202ms step_avg:34.95ms
step:64/2160 train_time:2235ms step_avg:34.92ms
step:65/2160 train_time:2269ms step_avg:34.91ms
step:66/2160 train_time:2302ms step_avg:34.88ms
step:67/2160 train_time:2336ms step_avg:34.87ms
step:68/2160 train_time:2370ms step_avg:34.85ms
step:69/2160 train_time:2404ms step_avg:34.85ms
step:70/2160 train_time:2438ms step_avg:34.82ms
step:71/2160 train_time:2472ms step_avg:34.82ms
step:72/2160 train_time:2506ms step_avg:34.80ms
step:73/2160 train_time:2540ms step_avg:34.79ms
step:74/2160 train_time:2573ms step_avg:34.77ms
step:75/2160 train_time:2608ms step_avg:34.77ms
step:76/2160 train_time:2641ms step_avg:34.75ms
step:77/2160 train_time:2675ms step_avg:34.75ms
step:78/2160 train_time:2708ms step_avg:34.72ms
step:79/2160 train_time:2743ms step_avg:34.72ms
step:80/2160 train_time:2776ms step_avg:34.70ms
step:81/2160 train_time:2811ms step_avg:34.70ms
step:82/2160 train_time:2843ms step_avg:34.68ms
step:83/2160 train_time:2878ms step_avg:34.67ms
step:84/2160 train_time:2911ms step_avg:34.65ms
step:85/2160 train_time:2945ms step_avg:34.65ms
step:86/2160 train_time:2978ms step_avg:34.63ms
step:87/2160 train_time:3013ms step_avg:34.63ms
step:88/2160 train_time:3045ms step_avg:34.61ms
step:89/2160 train_time:3080ms step_avg:34.60ms
step:90/2160 train_time:3113ms step_avg:34.59ms
step:91/2160 train_time:3147ms step_avg:34.58ms
step:92/2160 train_time:3180ms step_avg:34.56ms
step:93/2160 train_time:3214ms step_avg:34.56ms
step:94/2160 train_time:3247ms step_avg:34.55ms
step:95/2160 train_time:3281ms step_avg:34.54ms
step:96/2160 train_time:3314ms step_avg:34.52ms
step:97/2160 train_time:3349ms step_avg:34.53ms
step:98/2160 train_time:3382ms step_avg:34.51ms
step:99/2160 train_time:3416ms step_avg:34.51ms
step:100/2160 train_time:3449ms step_avg:34.49ms
step:101/2160 train_time:3484ms step_avg:34.49ms
step:102/2160 train_time:3517ms step_avg:34.48ms
step:103/2160 train_time:3551ms step_avg:34.48ms
step:104/2160 train_time:3584ms step_avg:34.46ms
step:105/2160 train_time:3619ms step_avg:34.46ms
step:106/2160 train_time:3652ms step_avg:34.45ms
step:107/2160 train_time:3686ms step_avg:34.45ms
step:108/2160 train_time:3719ms step_avg:34.44ms
step:109/2160 train_time:3754ms step_avg:34.44ms
step:110/2160 train_time:3787ms step_avg:34.43ms
step:111/2160 train_time:3821ms step_avg:34.43ms
step:112/2160 train_time:3854ms step_avg:34.41ms
step:113/2160 train_time:3889ms step_avg:34.41ms
step:114/2160 train_time:3922ms step_avg:34.40ms
step:115/2160 train_time:3956ms step_avg:34.40ms
step:116/2160 train_time:3989ms step_avg:34.39ms
step:117/2160 train_time:4023ms step_avg:34.38ms
step:118/2160 train_time:4056ms step_avg:34.37ms
step:119/2160 train_time:4090ms step_avg:34.37ms
step:120/2160 train_time:4123ms step_avg:34.36ms
step:121/2160 train_time:4158ms step_avg:34.36ms
step:122/2160 train_time:4191ms step_avg:34.35ms
step:123/2160 train_time:4225ms step_avg:34.35ms
step:124/2160 train_time:4259ms step_avg:34.34ms
step:125/2160 train_time:4293ms step_avg:34.35ms
step:126/2160 train_time:4326ms step_avg:34.34ms
step:127/2160 train_time:4361ms step_avg:34.34ms
step:128/2160 train_time:4393ms step_avg:34.32ms
step:129/2160 train_time:4428ms step_avg:34.33ms
step:130/2160 train_time:4461ms step_avg:34.31ms
step:131/2160 train_time:4495ms step_avg:34.31ms
step:132/2160 train_time:4528ms step_avg:34.30ms
step:133/2160 train_time:4562ms step_avg:34.30ms
step:134/2160 train_time:4595ms step_avg:34.29ms
step:135/2160 train_time:4629ms step_avg:34.29ms
step:136/2160 train_time:4662ms step_avg:34.28ms
step:137/2160 train_time:4697ms step_avg:34.28ms
step:138/2160 train_time:4730ms step_avg:34.27ms
step:139/2160 train_time:4764ms step_avg:34.27ms
step:140/2160 train_time:4797ms step_avg:34.26ms
step:141/2160 train_time:4831ms step_avg:34.26ms
step:142/2160 train_time:4864ms step_avg:34.26ms
step:143/2160 train_time:4898ms step_avg:34.25ms
step:144/2160 train_time:4931ms step_avg:34.25ms
step:145/2160 train_time:4966ms step_avg:34.25ms
step:146/2160 train_time:4998ms step_avg:34.24ms
step:147/2160 train_time:5033ms step_avg:34.24ms
step:148/2160 train_time:5066ms step_avg:34.23ms
step:149/2160 train_time:5100ms step_avg:34.23ms
step:150/2160 train_time:5133ms step_avg:34.22ms
step:151/2160 train_time:5168ms step_avg:34.22ms
step:152/2160 train_time:5201ms step_avg:34.22ms
step:153/2160 train_time:5235ms step_avg:34.22ms
step:154/2160 train_time:5268ms step_avg:34.21ms
step:155/2160 train_time:5303ms step_avg:34.21ms
step:156/2160 train_time:5335ms step_avg:34.20ms
step:157/2160 train_time:5370ms step_avg:34.20ms
step:158/2160 train_time:5403ms step_avg:34.19ms
step:159/2160 train_time:5437ms step_avg:34.20ms
step:160/2160 train_time:5470ms step_avg:34.19ms
step:161/2160 train_time:5504ms step_avg:34.19ms
step:162/2160 train_time:5537ms step_avg:34.18ms
step:163/2160 train_time:5572ms step_avg:34.18ms
step:164/2160 train_time:5605ms step_avg:34.17ms
step:165/2160 train_time:5639ms step_avg:34.18ms
step:166/2160 train_time:5672ms step_avg:34.17ms
step:167/2160 train_time:5706ms step_avg:34.17ms
step:168/2160 train_time:5739ms step_avg:34.16ms
step:169/2160 train_time:5773ms step_avg:34.16ms
step:170/2160 train_time:5807ms step_avg:34.16ms
step:171/2160 train_time:5841ms step_avg:34.16ms
step:172/2160 train_time:5874ms step_avg:34.15ms
step:173/2160 train_time:5908ms step_avg:34.15ms
step:174/2160 train_time:5941ms step_avg:34.15ms
step:175/2160 train_time:5975ms step_avg:34.14ms
step:176/2160 train_time:6008ms step_avg:34.14ms
step:177/2160 train_time:6042ms step_avg:34.14ms
step:178/2160 train_time:6075ms step_avg:34.13ms
step:179/2160 train_time:6110ms step_avg:34.13ms
step:180/2160 train_time:6143ms step_avg:34.13ms
step:181/2160 train_time:6177ms step_avg:34.13ms
step:182/2160 train_time:6210ms step_avg:34.12ms
step:183/2160 train_time:6245ms step_avg:34.12ms
step:184/2160 train_time:6278ms step_avg:34.12ms
step:185/2160 train_time:6312ms step_avg:34.12ms
step:186/2160 train_time:6345ms step_avg:34.11ms
step:187/2160 train_time:6379ms step_avg:34.11ms
step:188/2160 train_time:6413ms step_avg:34.11ms
step:189/2160 train_time:6447ms step_avg:34.11ms
step:190/2160 train_time:6479ms step_avg:34.10ms
step:191/2160 train_time:6514ms step_avg:34.11ms
step:192/2160 train_time:6547ms step_avg:34.10ms
step:193/2160 train_time:6581ms step_avg:34.10ms
step:194/2160 train_time:6614ms step_avg:34.09ms
step:195/2160 train_time:6649ms step_avg:34.10ms
step:196/2160 train_time:6681ms step_avg:34.09ms
step:197/2160 train_time:6716ms step_avg:34.09ms
step:198/2160 train_time:6749ms step_avg:34.08ms
step:199/2160 train_time:6783ms step_avg:34.09ms
step:200/2160 train_time:6816ms step_avg:34.08ms
step:201/2160 train_time:6850ms step_avg:34.08ms
step:202/2160 train_time:6883ms step_avg:34.08ms
step:203/2160 train_time:6918ms step_avg:34.08ms
step:204/2160 train_time:6951ms step_avg:34.07ms
step:205/2160 train_time:6985ms step_avg:34.07ms
step:206/2160 train_time:7018ms step_avg:34.07ms
step:207/2160 train_time:7052ms step_avg:34.07ms
step:208/2160 train_time:7085ms step_avg:34.06ms
step:209/2160 train_time:7119ms step_avg:34.06ms
step:210/2160 train_time:7152ms step_avg:34.06ms
step:211/2160 train_time:7187ms step_avg:34.06ms
step:212/2160 train_time:7220ms step_avg:34.06ms
step:213/2160 train_time:7254ms step_avg:34.06ms
step:214/2160 train_time:7287ms step_avg:34.05ms
step:215/2160 train_time:7321ms step_avg:34.05ms
step:216/2160 train_time:7354ms step_avg:34.05ms
step:217/2160 train_time:7389ms step_avg:34.05ms
step:218/2160 train_time:7422ms step_avg:34.04ms
step:219/2160 train_time:7456ms step_avg:34.04ms
step:220/2160 train_time:7489ms step_avg:34.04ms
step:221/2160 train_time:7523ms step_avg:34.04ms
step:222/2160 train_time:7557ms step_avg:34.04ms
step:223/2160 train_time:7591ms step_avg:34.04ms
step:224/2160 train_time:7624ms step_avg:34.04ms
step:225/2160 train_time:7658ms step_avg:34.03ms
step:226/2160 train_time:7691ms step_avg:34.03ms
step:227/2160 train_time:7725ms step_avg:34.03ms
step:228/2160 train_time:7758ms step_avg:34.03ms
step:229/2160 train_time:7793ms step_avg:34.03ms
step:230/2160 train_time:7825ms step_avg:34.02ms
step:231/2160 train_time:7860ms step_avg:34.03ms
step:232/2160 train_time:7893ms step_avg:34.02ms
step:233/2160 train_time:7927ms step_avg:34.02ms
step:234/2160 train_time:7960ms step_avg:34.02ms
step:235/2160 train_time:7995ms step_avg:34.02ms
step:236/2160 train_time:8028ms step_avg:34.02ms
step:237/2160 train_time:8062ms step_avg:34.02ms
step:238/2160 train_time:8095ms step_avg:34.01ms
step:239/2160 train_time:8129ms step_avg:34.01ms
step:240/2160 train_time:8162ms step_avg:34.01ms
step:241/2160 train_time:8197ms step_avg:34.01ms
step:242/2160 train_time:8229ms step_avg:34.01ms
step:243/2160 train_time:8263ms step_avg:34.01ms
step:244/2160 train_time:8296ms step_avg:34.00ms
step:245/2160 train_time:8331ms step_avg:34.00ms
step:246/2160 train_time:8364ms step_avg:34.00ms
step:247/2160 train_time:8398ms step_avg:34.00ms
step:248/2160 train_time:8431ms step_avg:34.00ms
step:249/2160 train_time:8466ms step_avg:34.00ms
step:250/2160 train_time:8499ms step_avg:33.99ms
step:250/2160 val_loss:4.2974 train_time:8534ms step_avg:34.14ms
step:251/2160 train_time:8555ms step_avg:34.08ms
step:252/2160 train_time:8574ms step_avg:34.02ms
step:253/2160 train_time:8606ms step_avg:34.01ms
step:254/2160 train_time:8640ms step_avg:34.02ms
step:255/2160 train_time:8677ms step_avg:34.03ms
step:256/2160 train_time:8711ms step_avg:34.03ms
step:257/2160 train_time:8746ms step_avg:34.03ms
step:258/2160 train_time:8779ms step_avg:34.03ms
step:259/2160 train_time:8814ms step_avg:34.03ms
step:260/2160 train_time:8847ms step_avg:34.03ms
step:261/2160 train_time:8882ms step_avg:34.03ms
step:262/2160 train_time:8915ms step_avg:34.03ms
step:263/2160 train_time:8949ms step_avg:34.03ms
step:264/2160 train_time:8982ms step_avg:34.02ms
step:265/2160 train_time:9017ms step_avg:34.02ms
step:266/2160 train_time:9050ms step_avg:34.02ms
step:267/2160 train_time:9084ms step_avg:34.02ms
step:268/2160 train_time:9117ms step_avg:34.02ms
step:269/2160 train_time:9151ms step_avg:34.02ms
step:270/2160 train_time:9183ms step_avg:34.01ms
step:271/2160 train_time:9218ms step_avg:34.01ms
step:272/2160 train_time:9251ms step_avg:34.01ms
step:273/2160 train_time:9285ms step_avg:34.01ms
step:274/2160 train_time:9318ms step_avg:34.01ms
step:275/2160 train_time:9352ms step_avg:34.01ms
step:276/2160 train_time:9385ms step_avg:34.00ms
step:277/2160 train_time:9419ms step_avg:34.00ms
step:278/2160 train_time:9452ms step_avg:34.00ms
step:279/2160 train_time:9486ms step_avg:34.00ms
step:280/2160 train_time:9519ms step_avg:34.00ms
step:281/2160 train_time:9553ms step_avg:34.00ms
step:282/2160 train_time:9587ms step_avg:34.00ms
step:283/2160 train_time:9621ms step_avg:34.00ms
step:284/2160 train_time:9655ms step_avg:34.00ms
step:285/2160 train_time:9689ms step_avg:34.00ms
step:286/2160 train_time:9722ms step_avg:33.99ms
step:287/2160 train_time:9757ms step_avg:34.00ms
step:288/2160 train_time:9790ms step_avg:33.99ms
step:289/2160 train_time:9824ms step_avg:33.99ms
step:290/2160 train_time:9857ms step_avg:33.99ms
step:291/2160 train_time:9892ms step_avg:33.99ms
step:292/2160 train_time:9925ms step_avg:33.99ms
step:293/2160 train_time:9960ms step_avg:33.99ms
step:294/2160 train_time:9993ms step_avg:33.99ms
step:295/2160 train_time:10027ms step_avg:33.99ms
step:296/2160 train_time:10060ms step_avg:33.99ms
step:297/2160 train_time:10094ms step_avg:33.99ms
step:298/2160 train_time:10127ms step_avg:33.98ms
step:299/2160 train_time:10162ms step_avg:33.99ms
step:300/2160 train_time:10195ms step_avg:33.98ms
step:301/2160 train_time:10229ms step_avg:33.98ms
step:302/2160 train_time:10262ms step_avg:33.98ms
step:303/2160 train_time:10296ms step_avg:33.98ms
step:304/2160 train_time:10328ms step_avg:33.98ms
step:305/2160 train_time:10363ms step_avg:33.98ms
step:306/2160 train_time:10395ms step_avg:33.97ms
step:307/2160 train_time:10430ms step_avg:33.97ms
step:308/2160 train_time:10463ms step_avg:33.97ms
step:309/2160 train_time:10497ms step_avg:33.97ms
step:310/2160 train_time:10530ms step_avg:33.97ms
step:311/2160 train_time:10564ms step_avg:33.97ms
step:312/2160 train_time:10597ms step_avg:33.96ms
step:313/2160 train_time:10631ms step_avg:33.96ms
step:314/2160 train_time:10664ms step_avg:33.96ms
step:315/2160 train_time:10698ms step_avg:33.96ms
step:316/2160 train_time:10731ms step_avg:33.96ms
step:317/2160 train_time:10766ms step_avg:33.96ms
step:318/2160 train_time:10799ms step_avg:33.96ms
step:319/2160 train_time:10833ms step_avg:33.96ms
step:320/2160 train_time:10866ms step_avg:33.96ms
step:321/2160 train_time:10900ms step_avg:33.96ms
step:322/2160 train_time:10933ms step_avg:33.95ms
step:323/2160 train_time:10968ms step_avg:33.96ms
step:324/2160 train_time:11001ms step_avg:33.95ms
step:325/2160 train_time:11035ms step_avg:33.95ms
step:326/2160 train_time:11068ms step_avg:33.95ms
step:327/2160 train_time:11102ms step_avg:33.95ms
step:328/2160 train_time:11135ms step_avg:33.95ms
step:329/2160 train_time:11170ms step_avg:33.95ms
step:330/2160 train_time:11203ms step_avg:33.95ms
step:331/2160 train_time:11237ms step_avg:33.95ms
step:332/2160 train_time:11270ms step_avg:33.95ms
step:333/2160 train_time:11304ms step_avg:33.95ms
step:334/2160 train_time:11337ms step_avg:33.94ms
step:335/2160 train_time:11371ms step_avg:33.94ms
step:336/2160 train_time:11405ms step_avg:33.94ms
step:337/2160 train_time:11439ms step_avg:33.94ms
step:338/2160 train_time:11472ms step_avg:33.94ms
step:339/2160 train_time:11506ms step_avg:33.94ms
step:340/2160 train_time:11539ms step_avg:33.94ms
step:341/2160 train_time:11573ms step_avg:33.94ms
step:342/2160 train_time:11606ms step_avg:33.94ms
step:343/2160 train_time:11641ms step_avg:33.94ms
step:344/2160 train_time:11674ms step_avg:33.94ms
step:345/2160 train_time:11708ms step_avg:33.94ms
step:346/2160 train_time:11741ms step_avg:33.93ms
step:347/2160 train_time:11775ms step_avg:33.93ms
step:348/2160 train_time:11808ms step_avg:33.93ms
step:349/2160 train_time:11843ms step_avg:33.93ms
step:350/2160 train_time:11876ms step_avg:33.93ms
step:351/2160 train_time:11911ms step_avg:33.93ms
step:352/2160 train_time:11943ms step_avg:33.93ms
step:353/2160 train_time:11978ms step_avg:33.93ms
step:354/2160 train_time:12011ms step_avg:33.93ms
step:355/2160 train_time:12045ms step_avg:33.93ms
step:356/2160 train_time:12078ms step_avg:33.93ms
step:357/2160 train_time:12112ms step_avg:33.93ms
step:358/2160 train_time:12145ms step_avg:33.92ms
step:359/2160 train_time:12179ms step_avg:33.93ms
step:360/2160 train_time:12212ms step_avg:33.92ms
step:361/2160 train_time:12247ms step_avg:33.92ms
step:362/2160 train_time:12280ms step_avg:33.92ms
step:363/2160 train_time:12314ms step_avg:33.92ms
step:364/2160 train_time:12347ms step_avg:33.92ms
step:365/2160 train_time:12381ms step_avg:33.92ms
step:366/2160 train_time:12414ms step_avg:33.92ms
step:367/2160 train_time:12448ms step_avg:33.92ms
step:368/2160 train_time:12481ms step_avg:33.92ms
step:369/2160 train_time:12515ms step_avg:33.92ms
step:370/2160 train_time:12548ms step_avg:33.91ms
step:371/2160 train_time:12583ms step_avg:33.92ms
step:372/2160 train_time:12615ms step_avg:33.91ms
step:373/2160 train_time:12649ms step_avg:33.91ms
step:374/2160 train_time:12682ms step_avg:33.91ms
step:375/2160 train_time:12716ms step_avg:33.91ms
step:376/2160 train_time:12749ms step_avg:33.91ms
step:377/2160 train_time:12784ms step_avg:33.91ms
step:378/2160 train_time:12817ms step_avg:33.91ms
step:379/2160 train_time:12851ms step_avg:33.91ms
step:380/2160 train_time:12884ms step_avg:33.90ms
step:381/2160 train_time:12919ms step_avg:33.91ms
step:382/2160 train_time:12952ms step_avg:33.91ms
step:383/2160 train_time:12986ms step_avg:33.91ms
step:384/2160 train_time:13019ms step_avg:33.90ms
step:385/2160 train_time:13053ms step_avg:33.91ms
step:386/2160 train_time:13087ms step_avg:33.90ms
step:387/2160 train_time:13121ms step_avg:33.90ms
step:388/2160 train_time:13154ms step_avg:33.90ms
step:389/2160 train_time:13188ms step_avg:33.90ms
step:390/2160 train_time:13221ms step_avg:33.90ms
step:391/2160 train_time:13256ms step_avg:33.90ms
step:392/2160 train_time:13289ms step_avg:33.90ms
step:393/2160 train_time:13323ms step_avg:33.90ms
step:394/2160 train_time:13356ms step_avg:33.90ms
step:395/2160 train_time:13390ms step_avg:33.90ms
step:396/2160 train_time:13423ms step_avg:33.90ms
step:397/2160 train_time:13457ms step_avg:33.90ms
step:398/2160 train_time:13490ms step_avg:33.90ms
step:399/2160 train_time:13525ms step_avg:33.90ms
step:400/2160 train_time:13558ms step_avg:33.90ms
step:401/2160 train_time:13592ms step_avg:33.90ms
step:402/2160 train_time:13625ms step_avg:33.89ms
step:403/2160 train_time:13660ms step_avg:33.89ms
step:404/2160 train_time:13693ms step_avg:33.89ms
step:405/2160 train_time:13727ms step_avg:33.89ms
step:406/2160 train_time:13760ms step_avg:33.89ms
step:407/2160 train_time:13794ms step_avg:33.89ms
step:408/2160 train_time:13827ms step_avg:33.89ms
step:409/2160 train_time:13861ms step_avg:33.89ms
step:410/2160 train_time:13894ms step_avg:33.89ms
step:411/2160 train_time:13928ms step_avg:33.89ms
step:412/2160 train_time:13961ms step_avg:33.89ms
step:413/2160 train_time:13996ms step_avg:33.89ms
step:414/2160 train_time:14029ms step_avg:33.89ms
step:415/2160 train_time:14063ms step_avg:33.89ms
step:416/2160 train_time:14096ms step_avg:33.88ms
step:417/2160 train_time:14130ms step_avg:33.88ms
step:418/2160 train_time:14163ms step_avg:33.88ms
step:419/2160 train_time:14197ms step_avg:33.88ms
step:420/2160 train_time:14231ms step_avg:33.88ms
step:421/2160 train_time:14265ms step_avg:33.88ms
step:422/2160 train_time:14298ms step_avg:33.88ms
step:423/2160 train_time:14332ms step_avg:33.88ms
step:424/2160 train_time:14365ms step_avg:33.88ms
step:425/2160 train_time:14399ms step_avg:33.88ms
step:426/2160 train_time:14432ms step_avg:33.88ms
step:427/2160 train_time:14467ms step_avg:33.88ms
step:428/2160 train_time:14500ms step_avg:33.88ms
step:429/2160 train_time:14534ms step_avg:33.88ms
step:430/2160 train_time:14567ms step_avg:33.88ms
step:431/2160 train_time:14601ms step_avg:33.88ms
step:432/2160 train_time:14634ms step_avg:33.88ms
step:433/2160 train_time:14668ms step_avg:33.88ms
step:434/2160 train_time:14701ms step_avg:33.87ms
step:435/2160 train_time:14735ms step_avg:33.87ms
step:436/2160 train_time:14769ms step_avg:33.87ms
step:437/2160 train_time:14803ms step_avg:33.87ms
step:438/2160 train_time:14836ms step_avg:33.87ms
step:439/2160 train_time:14870ms step_avg:33.87ms
step:440/2160 train_time:14903ms step_avg:33.87ms
step:441/2160 train_time:14937ms step_avg:33.87ms
step:442/2160 train_time:14970ms step_avg:33.87ms
step:443/2160 train_time:15004ms step_avg:33.87ms
step:444/2160 train_time:15037ms step_avg:33.87ms
step:445/2160 train_time:15072ms step_avg:33.87ms
step:446/2160 train_time:15105ms step_avg:33.87ms
step:447/2160 train_time:15139ms step_avg:33.87ms
step:448/2160 train_time:15172ms step_avg:33.87ms
step:449/2160 train_time:15206ms step_avg:33.87ms
step:450/2160 train_time:15239ms step_avg:33.86ms
step:451/2160 train_time:15273ms step_avg:33.87ms
step:452/2160 train_time:15306ms step_avg:33.86ms
step:453/2160 train_time:15341ms step_avg:33.86ms
step:454/2160 train_time:15373ms step_avg:33.86ms
step:455/2160 train_time:15408ms step_avg:33.86ms
step:456/2160 train_time:15441ms step_avg:33.86ms
step:457/2160 train_time:15475ms step_avg:33.86ms
step:458/2160 train_time:15508ms step_avg:33.86ms
step:459/2160 train_time:15542ms step_avg:33.86ms
step:460/2160 train_time:15575ms step_avg:33.86ms
step:461/2160 train_time:15609ms step_avg:33.86ms
step:462/2160 train_time:15642ms step_avg:33.86ms
step:463/2160 train_time:15677ms step_avg:33.86ms
step:464/2160 train_time:15710ms step_avg:33.86ms
step:465/2160 train_time:15744ms step_avg:33.86ms
step:466/2160 train_time:15777ms step_avg:33.86ms
step:467/2160 train_time:15811ms step_avg:33.86ms
step:468/2160 train_time:15844ms step_avg:33.85ms
step:469/2160 train_time:15878ms step_avg:33.86ms
step:470/2160 train_time:15911ms step_avg:33.85ms
step:471/2160 train_time:15945ms step_avg:33.85ms
step:472/2160 train_time:15978ms step_avg:33.85ms
step:473/2160 train_time:16012ms step_avg:33.85ms
step:474/2160 train_time:16046ms step_avg:33.85ms
step:475/2160 train_time:16080ms step_avg:33.85ms
step:476/2160 train_time:16113ms step_avg:33.85ms
step:477/2160 train_time:16147ms step_avg:33.85ms
step:478/2160 train_time:16180ms step_avg:33.85ms
step:479/2160 train_time:16214ms step_avg:33.85ms
step:480/2160 train_time:16247ms step_avg:33.85ms
step:481/2160 train_time:16281ms step_avg:33.85ms
step:482/2160 train_time:16315ms step_avg:33.85ms
step:483/2160 train_time:16349ms step_avg:33.85ms
step:484/2160 train_time:16381ms step_avg:33.85ms
step:485/2160 train_time:16415ms step_avg:33.85ms
step:486/2160 train_time:16448ms step_avg:33.84ms
step:487/2160 train_time:16482ms step_avg:33.84ms
step:488/2160 train_time:16516ms step_avg:33.84ms
step:489/2160 train_time:16550ms step_avg:33.84ms
step:490/2160 train_time:16583ms step_avg:33.84ms
step:491/2160 train_time:16617ms step_avg:33.84ms
step:492/2160 train_time:16650ms step_avg:33.84ms
step:493/2160 train_time:16685ms step_avg:33.84ms
step:494/2160 train_time:16718ms step_avg:33.84ms
step:495/2160 train_time:16752ms step_avg:33.84ms
step:496/2160 train_time:16785ms step_avg:33.84ms
step:497/2160 train_time:16819ms step_avg:33.84ms
step:498/2160 train_time:16853ms step_avg:33.84ms
step:499/2160 train_time:16886ms step_avg:33.84ms
step:500/2160 train_time:16919ms step_avg:33.84ms
step:500/2160 val_loss:4.0425 train_time:16955ms step_avg:33.91ms
step:501/2160 train_time:16976ms step_avg:33.88ms
step:502/2160 train_time:16995ms step_avg:33.85ms
step:503/2160 train_time:17027ms step_avg:33.85ms
step:504/2160 train_time:17061ms step_avg:33.85ms
step:505/2160 train_time:17097ms step_avg:33.85ms
step:506/2160 train_time:17130ms step_avg:33.85ms
step:507/2160 train_time:17165ms step_avg:33.86ms
step:508/2160 train_time:17198ms step_avg:33.85ms
step:509/2160 train_time:17232ms step_avg:33.86ms
step:510/2160 train_time:17266ms step_avg:33.85ms
step:511/2160 train_time:17300ms step_avg:33.86ms
step:512/2160 train_time:17333ms step_avg:33.85ms
step:513/2160 train_time:17367ms step_avg:33.85ms
step:514/2160 train_time:17400ms step_avg:33.85ms
step:515/2160 train_time:17434ms step_avg:33.85ms
step:516/2160 train_time:17467ms step_avg:33.85ms
step:517/2160 train_time:17500ms step_avg:33.85ms
step:518/2160 train_time:17533ms step_avg:33.85ms
step:519/2160 train_time:17567ms step_avg:33.85ms
step:520/2160 train_time:17600ms step_avg:33.85ms
step:521/2160 train_time:17634ms step_avg:33.85ms
step:522/2160 train_time:17668ms step_avg:33.85ms
step:523/2160 train_time:17701ms step_avg:33.85ms
step:524/2160 train_time:17734ms step_avg:33.84ms
step:525/2160 train_time:17768ms step_avg:33.84ms
step:526/2160 train_time:17801ms step_avg:33.84ms
step:527/2160 train_time:17835ms step_avg:33.84ms
step:528/2160 train_time:17868ms step_avg:33.84ms
step:529/2160 train_time:17902ms step_avg:33.84ms
step:530/2160 train_time:17936ms step_avg:33.84ms
step:531/2160 train_time:17970ms step_avg:33.84ms
step:532/2160 train_time:18004ms step_avg:33.84ms
step:533/2160 train_time:18039ms step_avg:33.84ms
step:534/2160 train_time:18073ms step_avg:33.84ms
step:535/2160 train_time:18107ms step_avg:33.85ms
step:536/2160 train_time:18140ms step_avg:33.84ms
step:537/2160 train_time:18175ms step_avg:33.85ms
step:538/2160 train_time:18208ms step_avg:33.84ms
step:539/2160 train_time:18243ms step_avg:33.85ms
step:540/2160 train_time:18275ms step_avg:33.84ms
step:541/2160 train_time:18310ms step_avg:33.84ms
step:542/2160 train_time:18343ms step_avg:33.84ms
step:543/2160 train_time:18378ms step_avg:33.84ms
step:544/2160 train_time:18411ms step_avg:33.84ms
step:545/2160 train_time:18445ms step_avg:33.84ms
step:546/2160 train_time:18478ms step_avg:33.84ms
step:547/2160 train_time:18512ms step_avg:33.84ms
step:548/2160 train_time:18544ms step_avg:33.84ms
step:549/2160 train_time:18579ms step_avg:33.84ms
step:550/2160 train_time:18612ms step_avg:33.84ms
step:551/2160 train_time:18646ms step_avg:33.84ms
step:552/2160 train_time:18679ms step_avg:33.84ms
step:553/2160 train_time:18713ms step_avg:33.84ms
step:554/2160 train_time:18746ms step_avg:33.84ms
step:555/2160 train_time:18780ms step_avg:33.84ms
step:556/2160 train_time:18814ms step_avg:33.84ms
step:557/2160 train_time:18848ms step_avg:33.84ms
step:558/2160 train_time:18880ms step_avg:33.84ms
step:559/2160 train_time:18915ms step_avg:33.84ms
step:560/2160 train_time:18948ms step_avg:33.83ms
step:561/2160 train_time:18982ms step_avg:33.84ms
step:562/2160 train_time:19015ms step_avg:33.83ms
step:563/2160 train_time:19049ms step_avg:33.84ms
step:564/2160 train_time:19083ms step_avg:33.84ms
step:565/2160 train_time:19117ms step_avg:33.84ms
step:566/2160 train_time:19150ms step_avg:33.83ms
step:567/2160 train_time:19185ms step_avg:33.84ms
step:568/2160 train_time:19218ms step_avg:33.83ms
step:569/2160 train_time:19252ms step_avg:33.83ms
step:570/2160 train_time:19285ms step_avg:33.83ms
step:571/2160 train_time:19320ms step_avg:33.83ms
step:572/2160 train_time:19353ms step_avg:33.83ms
step:573/2160 train_time:19387ms step_avg:33.83ms
step:574/2160 train_time:19420ms step_avg:33.83ms
step:575/2160 train_time:19454ms step_avg:33.83ms
step:576/2160 train_time:19487ms step_avg:33.83ms
step:577/2160 train_time:19521ms step_avg:33.83ms
step:578/2160 train_time:19554ms step_avg:33.83ms
step:579/2160 train_time:19589ms step_avg:33.83ms
step:580/2160 train_time:19621ms step_avg:33.83ms
step:581/2160 train_time:19655ms step_avg:33.83ms
step:582/2160 train_time:19688ms step_avg:33.83ms
step:583/2160 train_time:19723ms step_avg:33.83ms
step:584/2160 train_time:19756ms step_avg:33.83ms
step:585/2160 train_time:19790ms step_avg:33.83ms
step:586/2160 train_time:19823ms step_avg:33.83ms
step:587/2160 train_time:19857ms step_avg:33.83ms
step:588/2160 train_time:19890ms step_avg:33.83ms
step:589/2160 train_time:19924ms step_avg:33.83ms
step:590/2160 train_time:19957ms step_avg:33.83ms
step:591/2160 train_time:19991ms step_avg:33.83ms
step:592/2160 train_time:20024ms step_avg:33.82ms
step:593/2160 train_time:20059ms step_avg:33.83ms
step:594/2160 train_time:20091ms step_avg:33.82ms
step:595/2160 train_time:20126ms step_avg:33.83ms
step:596/2160 train_time:20159ms step_avg:33.82ms
step:597/2160 train_time:20193ms step_avg:33.82ms
step:598/2160 train_time:20227ms step_avg:33.82ms
step:599/2160 train_time:20261ms step_avg:33.82ms
step:600/2160 train_time:20294ms step_avg:33.82ms
step:601/2160 train_time:20329ms step_avg:33.82ms
step:602/2160 train_time:20362ms step_avg:33.82ms
step:603/2160 train_time:20396ms step_avg:33.82ms
step:604/2160 train_time:20429ms step_avg:33.82ms
step:605/2160 train_time:20463ms step_avg:33.82ms
step:606/2160 train_time:20496ms step_avg:33.82ms
step:607/2160 train_time:20531ms step_avg:33.82ms
step:608/2160 train_time:20563ms step_avg:33.82ms
step:609/2160 train_time:20598ms step_avg:33.82ms
step:610/2160 train_time:20631ms step_avg:33.82ms
step:611/2160 train_time:20665ms step_avg:33.82ms
step:612/2160 train_time:20698ms step_avg:33.82ms
step:613/2160 train_time:20732ms step_avg:33.82ms
step:614/2160 train_time:20765ms step_avg:33.82ms
step:615/2160 train_time:20799ms step_avg:33.82ms
step:616/2160 train_time:20832ms step_avg:33.82ms
step:617/2160 train_time:20866ms step_avg:33.82ms
step:618/2160 train_time:20899ms step_avg:33.82ms
step:619/2160 train_time:20934ms step_avg:33.82ms
step:620/2160 train_time:20967ms step_avg:33.82ms
step:621/2160 train_time:21001ms step_avg:33.82ms
step:622/2160 train_time:21034ms step_avg:33.82ms
step:623/2160 train_time:21069ms step_avg:33.82ms
step:624/2160 train_time:21101ms step_avg:33.82ms
step:625/2160 train_time:21135ms step_avg:33.82ms
step:626/2160 train_time:21168ms step_avg:33.82ms
step:627/2160 train_time:21203ms step_avg:33.82ms
step:628/2160 train_time:21236ms step_avg:33.82ms
step:629/2160 train_time:21270ms step_avg:33.82ms
step:630/2160 train_time:21303ms step_avg:33.81ms
step:631/2160 train_time:21338ms step_avg:33.82ms
step:632/2160 train_time:21371ms step_avg:33.81ms
step:633/2160 train_time:21405ms step_avg:33.82ms
step:634/2160 train_time:21438ms step_avg:33.81ms
step:635/2160 train_time:21472ms step_avg:33.81ms
step:636/2160 train_time:21505ms step_avg:33.81ms
step:637/2160 train_time:21540ms step_avg:33.82ms
step:638/2160 train_time:21573ms step_avg:33.81ms
step:639/2160 train_time:21607ms step_avg:33.81ms
step:640/2160 train_time:21641ms step_avg:33.81ms
step:641/2160 train_time:21675ms step_avg:33.81ms
step:642/2160 train_time:21708ms step_avg:33.81ms
step:643/2160 train_time:21742ms step_avg:33.81ms
step:644/2160 train_time:21776ms step_avg:33.81ms
step:645/2160 train_time:21810ms step_avg:33.81ms
step:646/2160 train_time:21843ms step_avg:33.81ms
step:647/2160 train_time:21877ms step_avg:33.81ms
step:648/2160 train_time:21910ms step_avg:33.81ms
step:649/2160 train_time:21945ms step_avg:33.81ms
step:650/2160 train_time:21978ms step_avg:33.81ms
step:651/2160 train_time:22012ms step_avg:33.81ms
step:652/2160 train_time:22045ms step_avg:33.81ms
step:653/2160 train_time:22080ms step_avg:33.81ms
step:654/2160 train_time:22113ms step_avg:33.81ms
step:655/2160 train_time:22147ms step_avg:33.81ms
step:656/2160 train_time:22180ms step_avg:33.81ms
step:657/2160 train_time:22214ms step_avg:33.81ms
step:658/2160 train_time:22247ms step_avg:33.81ms
step:659/2160 train_time:22282ms step_avg:33.81ms
step:660/2160 train_time:22315ms step_avg:33.81ms
step:661/2160 train_time:22349ms step_avg:33.81ms
step:662/2160 train_time:22382ms step_avg:33.81ms
step:663/2160 train_time:22417ms step_avg:33.81ms
step:664/2160 train_time:22449ms step_avg:33.81ms
step:665/2160 train_time:22484ms step_avg:33.81ms
step:666/2160 train_time:22517ms step_avg:33.81ms
step:667/2160 train_time:22551ms step_avg:33.81ms
step:668/2160 train_time:22584ms step_avg:33.81ms
step:669/2160 train_time:22619ms step_avg:33.81ms
step:670/2160 train_time:22652ms step_avg:33.81ms
step:671/2160 train_time:22686ms step_avg:33.81ms
step:672/2160 train_time:22720ms step_avg:33.81ms
step:673/2160 train_time:22754ms step_avg:33.81ms
step:674/2160 train_time:22787ms step_avg:33.81ms
step:675/2160 train_time:22821ms step_avg:33.81ms
step:676/2160 train_time:22854ms step_avg:33.81ms
step:677/2160 train_time:22888ms step_avg:33.81ms
step:678/2160 train_time:22921ms step_avg:33.81ms
step:679/2160 train_time:22955ms step_avg:33.81ms
step:680/2160 train_time:22988ms step_avg:33.81ms
step:681/2160 train_time:23022ms step_avg:33.81ms
step:682/2160 train_time:23055ms step_avg:33.81ms
step:683/2160 train_time:23089ms step_avg:33.81ms
step:684/2160 train_time:23122ms step_avg:33.80ms
step:685/2160 train_time:23157ms step_avg:33.81ms
step:686/2160 train_time:23190ms step_avg:33.80ms
step:687/2160 train_time:23224ms step_avg:33.80ms
step:688/2160 train_time:23257ms step_avg:33.80ms
step:689/2160 train_time:23291ms step_avg:33.80ms
step:690/2160 train_time:23324ms step_avg:33.80ms
step:691/2160 train_time:23358ms step_avg:33.80ms
step:692/2160 train_time:23391ms step_avg:33.80ms
step:693/2160 train_time:23425ms step_avg:33.80ms
step:694/2160 train_time:23458ms step_avg:33.80ms
step:695/2160 train_time:23493ms step_avg:33.80ms
step:696/2160 train_time:23526ms step_avg:33.80ms
step:697/2160 train_time:23560ms step_avg:33.80ms
step:698/2160 train_time:23593ms step_avg:33.80ms
step:699/2160 train_time:23628ms step_avg:33.80ms
step:700/2160 train_time:23661ms step_avg:33.80ms
step:701/2160 train_time:23695ms step_avg:33.80ms
step:702/2160 train_time:23728ms step_avg:33.80ms
step:703/2160 train_time:23762ms step_avg:33.80ms
step:704/2160 train_time:23795ms step_avg:33.80ms
step:705/2160 train_time:23829ms step_avg:33.80ms
step:706/2160 train_time:23862ms step_avg:33.80ms
step:707/2160 train_time:23896ms step_avg:33.80ms
step:708/2160 train_time:23931ms step_avg:33.80ms
step:709/2160 train_time:23991ms step_avg:33.84ms
step:710/2160 train_time:24050ms step_avg:33.87ms
step:711/2160 train_time:24112ms step_avg:33.91ms
step:712/2160 train_time:24172ms step_avg:33.95ms
step:713/2160 train_time:24233ms step_avg:33.99ms
step:714/2160 train_time:24293ms step_avg:34.02ms
step:715/2160 train_time:24355ms step_avg:34.06ms
step:716/2160 train_time:24414ms step_avg:34.10ms
step:717/2160 train_time:24476ms step_avg:34.14ms
step:718/2160 train_time:24535ms step_avg:34.17ms
step:719/2160 train_time:24597ms step_avg:34.21ms
step:720/2160 train_time:24656ms step_avg:34.24ms
step:721/2160 train_time:24717ms step_avg:34.28ms
step:722/2160 train_time:24777ms step_avg:34.32ms
step:723/2160 train_time:24838ms step_avg:34.35ms
step:724/2160 train_time:24898ms step_avg:34.39ms
step:725/2160 train_time:24959ms step_avg:34.43ms
step:726/2160 train_time:25019ms step_avg:34.46ms
step:727/2160 train_time:25080ms step_avg:34.50ms
step:728/2160 train_time:25140ms step_avg:34.53ms
step:729/2160 train_time:25202ms step_avg:34.57ms
step:730/2160 train_time:25262ms step_avg:34.61ms
step:731/2160 train_time:25323ms step_avg:34.64ms
step:732/2160 train_time:25383ms step_avg:34.68ms
step:733/2160 train_time:25444ms step_avg:34.71ms
step:734/2160 train_time:25504ms step_avg:34.75ms
step:735/2160 train_time:25566ms step_avg:34.78ms
step:736/2160 train_time:25626ms step_avg:34.82ms
step:737/2160 train_time:25687ms step_avg:34.85ms
step:738/2160 train_time:25747ms step_avg:34.89ms
step:739/2160 train_time:25809ms step_avg:34.92ms
step:740/2160 train_time:25868ms step_avg:34.96ms
step:741/2160 train_time:25930ms step_avg:34.99ms
step:742/2160 train_time:25990ms step_avg:35.03ms
step:743/2160 train_time:26052ms step_avg:35.06ms
step:744/2160 train_time:26112ms step_avg:35.10ms
step:745/2160 train_time:26174ms step_avg:35.13ms
step:746/2160 train_time:26233ms step_avg:35.17ms
step:747/2160 train_time:26294ms step_avg:35.20ms
step:748/2160 train_time:26354ms step_avg:35.23ms
step:749/2160 train_time:26415ms step_avg:35.27ms
step:750/2160 train_time:26475ms step_avg:35.30ms
step:750/2160 val_loss:3.8608 train_time:26538ms step_avg:35.38ms
step:751/2160 train_time:26560ms step_avg:35.37ms
step:752/2160 train_time:26598ms step_avg:35.37ms
step:753/2160 train_time:26659ms step_avg:35.40ms
step:754/2160 train_time:26720ms step_avg:35.44ms
step:755/2160 train_time:26782ms step_avg:35.47ms
step:756/2160 train_time:26843ms step_avg:35.51ms
step:757/2160 train_time:26905ms step_avg:35.54ms
step:758/2160 train_time:26964ms step_avg:35.57ms
step:759/2160 train_time:27025ms step_avg:35.61ms
step:760/2160 train_time:27083ms step_avg:35.64ms
step:761/2160 train_time:27143ms step_avg:35.67ms
step:762/2160 train_time:27202ms step_avg:35.70ms
step:763/2160 train_time:27263ms step_avg:35.73ms
step:764/2160 train_time:27323ms step_avg:35.76ms
step:765/2160 train_time:27384ms step_avg:35.80ms
step:766/2160 train_time:27447ms step_avg:35.83ms
step:767/2160 train_time:27514ms step_avg:35.87ms
step:768/2160 train_time:27574ms step_avg:35.90ms
step:769/2160 train_time:27635ms step_avg:35.94ms
step:770/2160 train_time:27694ms step_avg:35.97ms
step:771/2160 train_time:27756ms step_avg:36.00ms
step:772/2160 train_time:27815ms step_avg:36.03ms
step:773/2160 train_time:27877ms step_avg:36.06ms
step:774/2160 train_time:27936ms step_avg:36.09ms
step:775/2160 train_time:27998ms step_avg:36.13ms
step:776/2160 train_time:28056ms step_avg:36.16ms
step:777/2160 train_time:28117ms step_avg:36.19ms
step:778/2160 train_time:28177ms step_avg:36.22ms
step:779/2160 train_time:28238ms step_avg:36.25ms
step:780/2160 train_time:28297ms step_avg:36.28ms
step:781/2160 train_time:28359ms step_avg:36.31ms
step:782/2160 train_time:28420ms step_avg:36.34ms
step:783/2160 train_time:28483ms step_avg:36.38ms
step:784/2160 train_time:28544ms step_avg:36.41ms
step:785/2160 train_time:28606ms step_avg:36.44ms
step:786/2160 train_time:28666ms step_avg:36.47ms
step:787/2160 train_time:28728ms step_avg:36.50ms
step:788/2160 train_time:28788ms step_avg:36.53ms
step:789/2160 train_time:28850ms step_avg:36.56ms
step:790/2160 train_time:28909ms step_avg:36.59ms
step:791/2160 train_time:28971ms step_avg:36.63ms
step:792/2160 train_time:29031ms step_avg:36.66ms
step:793/2160 train_time:29092ms step_avg:36.69ms
step:794/2160 train_time:29151ms step_avg:36.71ms
step:795/2160 train_time:29212ms step_avg:36.75ms
step:796/2160 train_time:29272ms step_avg:36.77ms
step:797/2160 train_time:29333ms step_avg:36.80ms
step:798/2160 train_time:29393ms step_avg:36.83ms
step:799/2160 train_time:29455ms step_avg:36.86ms
step:800/2160 train_time:29515ms step_avg:36.89ms
step:801/2160 train_time:29576ms step_avg:36.92ms
step:802/2160 train_time:29636ms step_avg:36.95ms
step:803/2160 train_time:29698ms step_avg:36.98ms
step:804/2160 train_time:29758ms step_avg:37.01ms
step:805/2160 train_time:29819ms step_avg:37.04ms
step:806/2160 train_time:29879ms step_avg:37.07ms
step:807/2160 train_time:29940ms step_avg:37.10ms
step:808/2160 train_time:30000ms step_avg:37.13ms
step:809/2160 train_time:30061ms step_avg:37.16ms
step:810/2160 train_time:30120ms step_avg:37.19ms
step:811/2160 train_time:30181ms step_avg:37.22ms
step:812/2160 train_time:30241ms step_avg:37.24ms
step:813/2160 train_time:30302ms step_avg:37.27ms
step:814/2160 train_time:30362ms step_avg:37.30ms
step:815/2160 train_time:30424ms step_avg:37.33ms
step:816/2160 train_time:30483ms step_avg:37.36ms
step:817/2160 train_time:30545ms step_avg:37.39ms
step:818/2160 train_time:30604ms step_avg:37.41ms
step:819/2160 train_time:30666ms step_avg:37.44ms
step:820/2160 train_time:30725ms step_avg:37.47ms
step:821/2160 train_time:30787ms step_avg:37.50ms
step:822/2160 train_time:30846ms step_avg:37.53ms
step:823/2160 train_time:30907ms step_avg:37.55ms
step:824/2160 train_time:30968ms step_avg:37.58ms
step:825/2160 train_time:31029ms step_avg:37.61ms
step:826/2160 train_time:31089ms step_avg:37.64ms
step:827/2160 train_time:31150ms step_avg:37.67ms
step:828/2160 train_time:31209ms step_avg:37.69ms
step:829/2160 train_time:31271ms step_avg:37.72ms
step:830/2160 train_time:31331ms step_avg:37.75ms
step:831/2160 train_time:31392ms step_avg:37.78ms
step:832/2160 train_time:31451ms step_avg:37.80ms
step:833/2160 train_time:31512ms step_avg:37.83ms
step:834/2160 train_time:31572ms step_avg:37.86ms
step:835/2160 train_time:31634ms step_avg:37.88ms
step:836/2160 train_time:31693ms step_avg:37.91ms
step:837/2160 train_time:31755ms step_avg:37.94ms
step:838/2160 train_time:31814ms step_avg:37.96ms
step:839/2160 train_time:31876ms step_avg:37.99ms
step:840/2160 train_time:31936ms step_avg:38.02ms
step:841/2160 train_time:31998ms step_avg:38.05ms
step:842/2160 train_time:32058ms step_avg:38.07ms
step:843/2160 train_time:32119ms step_avg:38.10ms
step:844/2160 train_time:32178ms step_avg:38.13ms
step:845/2160 train_time:32240ms step_avg:38.15ms
step:846/2160 train_time:32300ms step_avg:38.18ms
step:847/2160 train_time:32361ms step_avg:38.21ms
step:848/2160 train_time:32421ms step_avg:38.23ms
step:849/2160 train_time:32482ms step_avg:38.26ms
step:850/2160 train_time:32541ms step_avg:38.28ms
step:851/2160 train_time:32603ms step_avg:38.31ms
step:852/2160 train_time:32663ms step_avg:38.34ms
step:853/2160 train_time:32725ms step_avg:38.36ms
step:854/2160 train_time:32784ms step_avg:38.39ms
step:855/2160 train_time:32846ms step_avg:38.42ms
step:856/2160 train_time:32906ms step_avg:38.44ms
step:857/2160 train_time:32967ms step_avg:38.47ms
step:858/2160 train_time:33028ms step_avg:38.49ms
step:859/2160 train_time:33089ms step_avg:38.52ms
step:860/2160 train_time:33148ms step_avg:38.54ms
step:861/2160 train_time:33210ms step_avg:38.57ms
step:862/2160 train_time:33271ms step_avg:38.60ms
step:863/2160 train_time:33333ms step_avg:38.62ms
step:864/2160 train_time:33392ms step_avg:38.65ms
step:865/2160 train_time:33453ms step_avg:38.67ms
step:866/2160 train_time:33513ms step_avg:38.70ms
step:867/2160 train_time:33575ms step_avg:38.73ms
step:868/2160 train_time:33634ms step_avg:38.75ms
step:869/2160 train_time:33695ms step_avg:38.77ms
step:870/2160 train_time:33755ms step_avg:38.80ms
step:871/2160 train_time:33816ms step_avg:38.82ms
step:872/2160 train_time:33875ms step_avg:38.85ms
step:873/2160 train_time:33937ms step_avg:38.87ms
step:874/2160 train_time:33997ms step_avg:38.90ms
step:875/2160 train_time:34059ms step_avg:38.92ms
step:876/2160 train_time:34119ms step_avg:38.95ms
step:877/2160 train_time:34180ms step_avg:38.97ms
step:878/2160 train_time:34241ms step_avg:39.00ms
step:879/2160 train_time:34303ms step_avg:39.02ms
step:880/2160 train_time:34362ms step_avg:39.05ms
step:881/2160 train_time:34424ms step_avg:39.07ms
step:882/2160 train_time:34484ms step_avg:39.10ms
step:883/2160 train_time:34546ms step_avg:39.12ms
step:884/2160 train_time:34605ms step_avg:39.15ms
step:885/2160 train_time:34667ms step_avg:39.17ms
step:886/2160 train_time:34727ms step_avg:39.19ms
step:887/2160 train_time:34788ms step_avg:39.22ms
step:888/2160 train_time:34848ms step_avg:39.24ms
step:889/2160 train_time:34909ms step_avg:39.27ms
step:890/2160 train_time:34969ms step_avg:39.29ms
step:891/2160 train_time:35031ms step_avg:39.32ms
step:892/2160 train_time:35091ms step_avg:39.34ms
step:893/2160 train_time:35152ms step_avg:39.36ms
step:894/2160 train_time:35212ms step_avg:39.39ms
step:895/2160 train_time:35275ms step_avg:39.41ms
step:896/2160 train_time:35336ms step_avg:39.44ms
step:897/2160 train_time:35397ms step_avg:39.46ms
step:898/2160 train_time:35457ms step_avg:39.48ms
step:899/2160 train_time:35518ms step_avg:39.51ms
step:900/2160 train_time:35577ms step_avg:39.53ms
step:901/2160 train_time:35638ms step_avg:39.55ms
step:902/2160 train_time:35698ms step_avg:39.58ms
step:903/2160 train_time:35760ms step_avg:39.60ms
step:904/2160 train_time:35819ms step_avg:39.62ms
step:905/2160 train_time:35880ms step_avg:39.65ms
step:906/2160 train_time:35940ms step_avg:39.67ms
step:907/2160 train_time:36002ms step_avg:39.69ms
step:908/2160 train_time:36062ms step_avg:39.72ms
step:909/2160 train_time:36125ms step_avg:39.74ms
step:910/2160 train_time:36184ms step_avg:39.76ms
step:911/2160 train_time:36246ms step_avg:39.79ms
step:912/2160 train_time:36306ms step_avg:39.81ms
step:913/2160 train_time:36367ms step_avg:39.83ms
step:914/2160 train_time:36427ms step_avg:39.85ms
step:915/2160 train_time:36489ms step_avg:39.88ms
step:916/2160 train_time:36548ms step_avg:39.90ms
step:917/2160 train_time:36609ms step_avg:39.92ms
step:918/2160 train_time:36669ms step_avg:39.94ms
step:919/2160 train_time:36731ms step_avg:39.97ms
step:920/2160 train_time:36791ms step_avg:39.99ms
step:921/2160 train_time:36852ms step_avg:40.01ms
step:922/2160 train_time:36911ms step_avg:40.03ms
step:923/2160 train_time:36973ms step_avg:40.06ms
step:924/2160 train_time:37033ms step_avg:40.08ms
step:925/2160 train_time:37094ms step_avg:40.10ms
step:926/2160 train_time:37154ms step_avg:40.12ms
step:927/2160 train_time:37215ms step_avg:40.15ms
step:928/2160 train_time:37275ms step_avg:40.17ms
step:929/2160 train_time:37336ms step_avg:40.19ms
step:930/2160 train_time:37396ms step_avg:40.21ms
step:931/2160 train_time:37457ms step_avg:40.23ms
step:932/2160 train_time:37517ms step_avg:40.25ms
step:933/2160 train_time:37578ms step_avg:40.28ms
step:934/2160 train_time:37638ms step_avg:40.30ms
step:935/2160 train_time:37700ms step_avg:40.32ms
step:936/2160 train_time:37759ms step_avg:40.34ms
step:937/2160 train_time:37821ms step_avg:40.36ms
step:938/2160 train_time:37881ms step_avg:40.38ms
step:939/2160 train_time:37942ms step_avg:40.41ms
step:940/2160 train_time:38002ms step_avg:40.43ms
step:941/2160 train_time:38064ms step_avg:40.45ms
step:942/2160 train_time:38124ms step_avg:40.47ms
step:943/2160 train_time:38185ms step_avg:40.49ms
step:944/2160 train_time:38245ms step_avg:40.51ms
step:945/2160 train_time:38306ms step_avg:40.54ms
step:946/2160 train_time:38366ms step_avg:40.56ms
step:947/2160 train_time:38427ms step_avg:40.58ms
step:948/2160 train_time:38486ms step_avg:40.60ms
step:949/2160 train_time:38547ms step_avg:40.62ms
step:950/2160 train_time:38607ms step_avg:40.64ms
step:951/2160 train_time:38668ms step_avg:40.66ms
step:952/2160 train_time:38728ms step_avg:40.68ms
step:953/2160 train_time:38789ms step_avg:40.70ms
step:954/2160 train_time:38849ms step_avg:40.72ms
step:955/2160 train_time:38910ms step_avg:40.74ms
step:956/2160 train_time:38970ms step_avg:40.76ms
step:957/2160 train_time:39032ms step_avg:40.79ms
step:958/2160 train_time:39091ms step_avg:40.81ms
step:959/2160 train_time:39152ms step_avg:40.83ms
step:960/2160 train_time:39212ms step_avg:40.85ms
step:961/2160 train_time:39274ms step_avg:40.87ms
step:962/2160 train_time:39333ms step_avg:40.89ms
step:963/2160 train_time:39394ms step_avg:40.91ms
step:964/2160 train_time:39453ms step_avg:40.93ms
step:965/2160 train_time:39515ms step_avg:40.95ms
step:966/2160 train_time:39574ms step_avg:40.97ms
step:967/2160 train_time:39636ms step_avg:40.99ms
step:968/2160 train_time:39695ms step_avg:41.01ms
step:969/2160 train_time:39757ms step_avg:41.03ms
step:970/2160 train_time:39817ms step_avg:41.05ms
step:971/2160 train_time:39878ms step_avg:41.07ms
step:972/2160 train_time:39938ms step_avg:41.09ms
step:973/2160 train_time:40001ms step_avg:41.11ms
step:974/2160 train_time:40060ms step_avg:41.13ms
step:975/2160 train_time:40122ms step_avg:41.15ms
step:976/2160 train_time:40182ms step_avg:41.17ms
step:977/2160 train_time:40244ms step_avg:41.19ms
step:978/2160 train_time:40304ms step_avg:41.21ms
step:979/2160 train_time:40366ms step_avg:41.23ms
step:980/2160 train_time:40425ms step_avg:41.25ms
step:981/2160 train_time:40487ms step_avg:41.27ms
step:982/2160 train_time:40547ms step_avg:41.29ms
step:983/2160 train_time:40608ms step_avg:41.31ms
step:984/2160 train_time:40668ms step_avg:41.33ms
step:985/2160 train_time:40729ms step_avg:41.35ms
step:986/2160 train_time:40789ms step_avg:41.37ms
step:987/2160 train_time:40851ms step_avg:41.39ms
step:988/2160 train_time:40910ms step_avg:41.41ms
step:989/2160 train_time:40972ms step_avg:41.43ms
step:990/2160 train_time:41033ms step_avg:41.45ms
step:991/2160 train_time:41095ms step_avg:41.47ms
step:992/2160 train_time:41155ms step_avg:41.49ms
step:993/2160 train_time:41217ms step_avg:41.51ms
step:994/2160 train_time:41276ms step_avg:41.53ms
step:995/2160 train_time:41338ms step_avg:41.55ms
step:996/2160 train_time:41398ms step_avg:41.56ms
step:997/2160 train_time:41459ms step_avg:41.58ms
step:998/2160 train_time:41519ms step_avg:41.60ms
step:999/2160 train_time:41581ms step_avg:41.62ms
step:1000/2160 train_time:41641ms step_avg:41.64ms
step:1000/2160 val_loss:3.6927 train_time:41704ms step_avg:41.70ms
step:1001/2160 train_time:41726ms step_avg:41.68ms
step:1002/2160 train_time:41765ms step_avg:41.68ms
step:1003/2160 train_time:41830ms step_avg:41.70ms
step:1004/2160 train_time:41891ms step_avg:41.72ms
step:1005/2160 train_time:41952ms step_avg:41.74ms
step:1006/2160 train_time:42012ms step_avg:41.76ms
step:1007/2160 train_time:42073ms step_avg:41.78ms
step:1008/2160 train_time:42132ms step_avg:41.80ms
step:1009/2160 train_time:42193ms step_avg:41.82ms
step:1010/2160 train_time:42252ms step_avg:41.83ms
step:1011/2160 train_time:42313ms step_avg:41.85ms
step:1012/2160 train_time:42372ms step_avg:41.87ms
step:1013/2160 train_time:42433ms step_avg:41.89ms
step:1014/2160 train_time:42493ms step_avg:41.91ms
step:1015/2160 train_time:42553ms step_avg:41.92ms
step:1016/2160 train_time:42614ms step_avg:41.94ms
step:1017/2160 train_time:42677ms step_avg:41.96ms
step:1018/2160 train_time:42738ms step_avg:41.98ms
step:1019/2160 train_time:42802ms step_avg:42.00ms
step:1020/2160 train_time:42862ms step_avg:42.02ms
step:1021/2160 train_time:42923ms step_avg:42.04ms
step:1022/2160 train_time:42983ms step_avg:42.06ms
step:1023/2160 train_time:43045ms step_avg:42.08ms
step:1024/2160 train_time:43105ms step_avg:42.09ms
step:1025/2160 train_time:43167ms step_avg:42.11ms
step:1026/2160 train_time:43226ms step_avg:42.13ms
step:1027/2160 train_time:43288ms step_avg:42.15ms
step:1028/2160 train_time:43347ms step_avg:42.17ms
step:1029/2160 train_time:43408ms step_avg:42.18ms
step:1030/2160 train_time:43468ms step_avg:42.20ms
step:1031/2160 train_time:43529ms step_avg:42.22ms
step:1032/2160 train_time:43589ms step_avg:42.24ms
step:1033/2160 train_time:43651ms step_avg:42.26ms
step:1034/2160 train_time:43712ms step_avg:42.27ms
step:1035/2160 train_time:43774ms step_avg:42.29ms
step:1036/2160 train_time:43835ms step_avg:42.31ms
step:1037/2160 train_time:43897ms step_avg:42.33ms
step:1038/2160 train_time:43956ms step_avg:42.35ms
step:1039/2160 train_time:44018ms step_avg:42.37ms
step:1040/2160 train_time:44077ms step_avg:42.38ms
step:1041/2160 train_time:44139ms step_avg:42.40ms
step:1042/2160 train_time:44198ms step_avg:42.42ms
step:1043/2160 train_time:44260ms step_avg:42.43ms
step:1044/2160 train_time:44319ms step_avg:42.45ms
step:1045/2160 train_time:44381ms step_avg:42.47ms
step:1046/2160 train_time:44440ms step_avg:42.49ms
step:1047/2160 train_time:44502ms step_avg:42.50ms
step:1048/2160 train_time:44562ms step_avg:42.52ms
step:1049/2160 train_time:44623ms step_avg:42.54ms
step:1050/2160 train_time:44684ms step_avg:42.56ms
step:1051/2160 train_time:44746ms step_avg:42.57ms
step:1052/2160 train_time:44806ms step_avg:42.59ms
step:1053/2160 train_time:44869ms step_avg:42.61ms
step:1054/2160 train_time:44928ms step_avg:42.63ms
step:1055/2160 train_time:44991ms step_avg:42.65ms
step:1056/2160 train_time:45051ms step_avg:42.66ms
step:1057/2160 train_time:45112ms step_avg:42.68ms
step:1058/2160 train_time:45172ms step_avg:42.70ms
step:1059/2160 train_time:45233ms step_avg:42.71ms
step:1060/2160 train_time:45293ms step_avg:42.73ms
step:1061/2160 train_time:45354ms step_avg:42.75ms
step:1062/2160 train_time:45415ms step_avg:42.76ms
step:1063/2160 train_time:45476ms step_avg:42.78ms
step:1064/2160 train_time:45536ms step_avg:42.80ms
step:1065/2160 train_time:45597ms step_avg:42.81ms
step:1066/2160 train_time:45657ms step_avg:42.83ms
step:1067/2160 train_time:45718ms step_avg:42.85ms
step:1068/2160 train_time:45778ms step_avg:42.86ms
step:1069/2160 train_time:45840ms step_avg:42.88ms
step:1070/2160 train_time:45900ms step_avg:42.90ms
step:1071/2160 train_time:45961ms step_avg:42.91ms
step:1072/2160 train_time:46021ms step_avg:42.93ms
step:1073/2160 train_time:46083ms step_avg:42.95ms
step:1074/2160 train_time:46143ms step_avg:42.96ms
step:1075/2160 train_time:46205ms step_avg:42.98ms
step:1076/2160 train_time:46265ms step_avg:43.00ms
step:1077/2160 train_time:46326ms step_avg:43.01ms
step:1078/2160 train_time:46386ms step_avg:43.03ms
step:1079/2160 train_time:46447ms step_avg:43.05ms
step:1080/2160 train_time:46508ms step_avg:43.06ms
step:1081/2160 train_time:46570ms step_avg:43.08ms
step:1082/2160 train_time:46629ms step_avg:43.10ms
step:1083/2160 train_time:46690ms step_avg:43.11ms
step:1084/2160 train_time:46750ms step_avg:43.13ms
step:1085/2160 train_time:46811ms step_avg:43.14ms
step:1086/2160 train_time:46872ms step_avg:43.16ms
step:1087/2160 train_time:46933ms step_avg:43.18ms
step:1088/2160 train_time:46992ms step_avg:43.19ms
step:1089/2160 train_time:47055ms step_avg:43.21ms
step:1090/2160 train_time:47113ms step_avg:43.22ms
step:1091/2160 train_time:47175ms step_avg:43.24ms
step:1092/2160 train_time:47235ms step_avg:43.26ms
step:1093/2160 train_time:47296ms step_avg:43.27ms
step:1094/2160 train_time:47356ms step_avg:43.29ms
step:1095/2160 train_time:47418ms step_avg:43.30ms
step:1096/2160 train_time:47478ms step_avg:43.32ms
step:1097/2160 train_time:47539ms step_avg:43.34ms
step:1098/2160 train_time:47599ms step_avg:43.35ms
step:1099/2160 train_time:47660ms step_avg:43.37ms
step:1100/2160 train_time:47720ms step_avg:43.38ms
step:1101/2160 train_time:47781ms step_avg:43.40ms
step:1102/2160 train_time:47840ms step_avg:43.41ms
step:1103/2160 train_time:47902ms step_avg:43.43ms
step:1104/2160 train_time:47961ms step_avg:43.44ms
step:1105/2160 train_time:48023ms step_avg:43.46ms
step:1106/2160 train_time:48083ms step_avg:43.47ms
step:1107/2160 train_time:48145ms step_avg:43.49ms
step:1108/2160 train_time:48205ms step_avg:43.51ms
step:1109/2160 train_time:48268ms step_avg:43.52ms
step:1110/2160 train_time:48328ms step_avg:43.54ms
step:1111/2160 train_time:48389ms step_avg:43.55ms
step:1112/2160 train_time:48449ms step_avg:43.57ms
step:1113/2160 train_time:48511ms step_avg:43.59ms
step:1114/2160 train_time:48571ms step_avg:43.60ms
step:1115/2160 train_time:48632ms step_avg:43.62ms
step:1116/2160 train_time:48691ms step_avg:43.63ms
step:1117/2160 train_time:48752ms step_avg:43.65ms
step:1118/2160 train_time:48813ms step_avg:43.66ms
step:1119/2160 train_time:48875ms step_avg:43.68ms
step:1120/2160 train_time:48936ms step_avg:43.69ms
step:1121/2160 train_time:48997ms step_avg:43.71ms
step:1122/2160 train_time:49056ms step_avg:43.72ms
step:1123/2160 train_time:49118ms step_avg:43.74ms
step:1124/2160 train_time:49178ms step_avg:43.75ms
step:1125/2160 train_time:49240ms step_avg:43.77ms
step:1126/2160 train_time:49300ms step_avg:43.78ms
step:1127/2160 train_time:49361ms step_avg:43.80ms
step:1128/2160 train_time:49421ms step_avg:43.81ms
step:1129/2160 train_time:49482ms step_avg:43.83ms
step:1130/2160 train_time:49542ms step_avg:43.84ms
step:1131/2160 train_time:49604ms step_avg:43.86ms
step:1132/2160 train_time:49663ms step_avg:43.87ms
step:1133/2160 train_time:49725ms step_avg:43.89ms
step:1134/2160 train_time:49784ms step_avg:43.90ms
step:1135/2160 train_time:49847ms step_avg:43.92ms
step:1136/2160 train_time:49907ms step_avg:43.93ms
step:1137/2160 train_time:49969ms step_avg:43.95ms
step:1138/2160 train_time:50028ms step_avg:43.96ms
step:1139/2160 train_time:50090ms step_avg:43.98ms
step:1140/2160 train_time:50149ms step_avg:43.99ms
step:1141/2160 train_time:50211ms step_avg:44.01ms
step:1142/2160 train_time:50271ms step_avg:44.02ms
step:1143/2160 train_time:50332ms step_avg:44.04ms
step:1144/2160 train_time:50391ms step_avg:44.05ms
step:1145/2160 train_time:50452ms step_avg:44.06ms
step:1146/2160 train_time:50512ms step_avg:44.08ms
step:1147/2160 train_time:50573ms step_avg:44.09ms
step:1148/2160 train_time:50633ms step_avg:44.11ms
step:1149/2160 train_time:50695ms step_avg:44.12ms
step:1150/2160 train_time:50754ms step_avg:44.13ms
step:1151/2160 train_time:50817ms step_avg:44.15ms
step:1152/2160 train_time:50876ms step_avg:44.16ms
step:1153/2160 train_time:50938ms step_avg:44.18ms
step:1154/2160 train_time:50998ms step_avg:44.19ms
step:1155/2160 train_time:51060ms step_avg:44.21ms
step:1156/2160 train_time:51119ms step_avg:44.22ms
step:1157/2160 train_time:51181ms step_avg:44.24ms
step:1158/2160 train_time:51240ms step_avg:44.25ms
step:1159/2160 train_time:51301ms step_avg:44.26ms
step:1160/2160 train_time:51361ms step_avg:44.28ms
step:1161/2160 train_time:51423ms step_avg:44.29ms
step:1162/2160 train_time:51483ms step_avg:44.31ms
step:1163/2160 train_time:51544ms step_avg:44.32ms
step:1164/2160 train_time:51604ms step_avg:44.33ms
step:1165/2160 train_time:51666ms step_avg:44.35ms
step:1166/2160 train_time:51727ms step_avg:44.36ms
step:1167/2160 train_time:51789ms step_avg:44.38ms
step:1168/2160 train_time:51849ms step_avg:44.39ms
step:1169/2160 train_time:51911ms step_avg:44.41ms
step:1170/2160 train_time:51970ms step_avg:44.42ms
step:1171/2160 train_time:52032ms step_avg:44.43ms
step:1172/2160 train_time:52092ms step_avg:44.45ms
step:1173/2160 train_time:52153ms step_avg:44.46ms
step:1174/2160 train_time:52214ms step_avg:44.48ms
step:1175/2160 train_time:52275ms step_avg:44.49ms
step:1176/2160 train_time:52334ms step_avg:44.50ms
step:1177/2160 train_time:52396ms step_avg:44.52ms
step:1178/2160 train_time:52456ms step_avg:44.53ms
step:1179/2160 train_time:52518ms step_avg:44.54ms
step:1180/2160 train_time:52577ms step_avg:44.56ms
step:1181/2160 train_time:52639ms step_avg:44.57ms
step:1182/2160 train_time:52699ms step_avg:44.58ms
step:1183/2160 train_time:52760ms step_avg:44.60ms
step:1184/2160 train_time:52820ms step_avg:44.61ms
step:1185/2160 train_time:52882ms step_avg:44.63ms
step:1186/2160 train_time:52942ms step_avg:44.64ms
step:1187/2160 train_time:53004ms step_avg:44.65ms
step:1188/2160 train_time:53064ms step_avg:44.67ms
step:1189/2160 train_time:53125ms step_avg:44.68ms
step:1190/2160 train_time:53185ms step_avg:44.69ms
step:1191/2160 train_time:53247ms step_avg:44.71ms
step:1192/2160 train_time:53307ms step_avg:44.72ms
step:1193/2160 train_time:53369ms step_avg:44.73ms
step:1194/2160 train_time:53428ms step_avg:44.75ms
step:1195/2160 train_time:53490ms step_avg:44.76ms
step:1196/2160 train_time:53549ms step_avg:44.77ms
step:1197/2160 train_time:53611ms step_avg:44.79ms
step:1198/2160 train_time:53670ms step_avg:44.80ms
step:1199/2160 train_time:53732ms step_avg:44.81ms
step:1200/2160 train_time:53791ms step_avg:44.83ms
step:1201/2160 train_time:53853ms step_avg:44.84ms
step:1202/2160 train_time:53912ms step_avg:44.85ms
step:1203/2160 train_time:53974ms step_avg:44.87ms
step:1204/2160 train_time:54034ms step_avg:44.88ms
step:1205/2160 train_time:54095ms step_avg:44.89ms
step:1206/2160 train_time:54155ms step_avg:44.90ms
step:1207/2160 train_time:54217ms step_avg:44.92ms
step:1208/2160 train_time:54277ms step_avg:44.93ms
step:1209/2160 train_time:54338ms step_avg:44.94ms
step:1210/2160 train_time:54397ms step_avg:44.96ms
step:1211/2160 train_time:54458ms step_avg:44.97ms
step:1212/2160 train_time:54518ms step_avg:44.98ms
step:1213/2160 train_time:54579ms step_avg:44.99ms
step:1214/2160 train_time:54638ms step_avg:45.01ms
step:1215/2160 train_time:54700ms step_avg:45.02ms
step:1216/2160 train_time:54759ms step_avg:45.03ms
step:1217/2160 train_time:54821ms step_avg:45.05ms
step:1218/2160 train_time:54880ms step_avg:45.06ms
step:1219/2160 train_time:54942ms step_avg:45.07ms
step:1220/2160 train_time:55001ms step_avg:45.08ms
step:1221/2160 train_time:55063ms step_avg:45.10ms
step:1222/2160 train_time:55123ms step_avg:45.11ms
step:1223/2160 train_time:55185ms step_avg:45.12ms
step:1224/2160 train_time:55245ms step_avg:45.13ms
step:1225/2160 train_time:55306ms step_avg:45.15ms
step:1226/2160 train_time:55366ms step_avg:45.16ms
step:1227/2160 train_time:55427ms step_avg:45.17ms
step:1228/2160 train_time:55487ms step_avg:45.19ms
step:1229/2160 train_time:55549ms step_avg:45.20ms
step:1230/2160 train_time:55608ms step_avg:45.21ms
step:1231/2160 train_time:55670ms step_avg:45.22ms
step:1232/2160 train_time:55729ms step_avg:45.23ms
step:1233/2160 train_time:55790ms step_avg:45.25ms
step:1234/2160 train_time:55850ms step_avg:45.26ms
step:1235/2160 train_time:55911ms step_avg:45.27ms
step:1236/2160 train_time:55971ms step_avg:45.28ms
step:1237/2160 train_time:56033ms step_avg:45.30ms
step:1238/2160 train_time:56092ms step_avg:45.31ms
step:1239/2160 train_time:56154ms step_avg:45.32ms
step:1240/2160 train_time:56214ms step_avg:45.33ms
step:1241/2160 train_time:56276ms step_avg:45.35ms
step:1242/2160 train_time:56335ms step_avg:45.36ms
step:1243/2160 train_time:56397ms step_avg:45.37ms
step:1244/2160 train_time:56457ms step_avg:45.38ms
step:1245/2160 train_time:56518ms step_avg:45.40ms
step:1246/2160 train_time:56578ms step_avg:45.41ms
step:1247/2160 train_time:56639ms step_avg:45.42ms
step:1248/2160 train_time:56698ms step_avg:45.43ms
step:1249/2160 train_time:56759ms step_avg:45.44ms
step:1250/2160 train_time:56819ms step_avg:45.46ms
step:1250/2160 val_loss:3.5746 train_time:56881ms step_avg:45.51ms
step:1251/2160 train_time:56900ms step_avg:45.48ms
step:1252/2160 train_time:56943ms step_avg:45.48ms
step:1253/2160 train_time:57007ms step_avg:45.50ms
step:1254/2160 train_time:57070ms step_avg:45.51ms
step:1255/2160 train_time:57132ms step_avg:45.52ms
step:1256/2160 train_time:57192ms step_avg:45.53ms
step:1257/2160 train_time:57252ms step_avg:45.55ms
step:1258/2160 train_time:57311ms step_avg:45.56ms
step:1259/2160 train_time:57372ms step_avg:45.57ms
step:1260/2160 train_time:57430ms step_avg:45.58ms
step:1261/2160 train_time:57491ms step_avg:45.59ms
step:1262/2160 train_time:57550ms step_avg:45.60ms
step:1263/2160 train_time:57611ms step_avg:45.61ms
step:1264/2160 train_time:57670ms step_avg:45.63ms
step:1265/2160 train_time:57731ms step_avg:45.64ms
step:1266/2160 train_time:57791ms step_avg:45.65ms
step:1267/2160 train_time:57854ms step_avg:45.66ms
step:1268/2160 train_time:57918ms step_avg:45.68ms
step:1269/2160 train_time:57981ms step_avg:45.69ms
step:1270/2160 train_time:58041ms step_avg:45.70ms
step:1271/2160 train_time:58103ms step_avg:45.71ms
step:1272/2160 train_time:58165ms step_avg:45.73ms
step:1273/2160 train_time:58226ms step_avg:45.74ms
step:1274/2160 train_time:58285ms step_avg:45.75ms
step:1275/2160 train_time:58346ms step_avg:45.76ms
step:1276/2160 train_time:58405ms step_avg:45.77ms
step:1277/2160 train_time:58466ms step_avg:45.78ms
step:1278/2160 train_time:58526ms step_avg:45.79ms
step:1279/2160 train_time:58587ms step_avg:45.81ms
step:1280/2160 train_time:58646ms step_avg:45.82ms
step:1281/2160 train_time:58708ms step_avg:45.83ms
step:1282/2160 train_time:58768ms step_avg:45.84ms
step:1283/2160 train_time:58830ms step_avg:45.85ms
step:1284/2160 train_time:58891ms step_avg:45.87ms
step:1285/2160 train_time:58953ms step_avg:45.88ms
step:1286/2160 train_time:59014ms step_avg:45.89ms
step:1287/2160 train_time:59075ms step_avg:45.90ms
step:1288/2160 train_time:59135ms step_avg:45.91ms
step:1289/2160 train_time:59197ms step_avg:45.92ms
step:1290/2160 train_time:59256ms step_avg:45.94ms
step:1291/2160 train_time:59318ms step_avg:45.95ms
step:1292/2160 train_time:59377ms step_avg:45.96ms
step:1293/2160 train_time:59439ms step_avg:45.97ms
step:1294/2160 train_time:59498ms step_avg:45.98ms
step:1295/2160 train_time:59560ms step_avg:45.99ms
step:1296/2160 train_time:59620ms step_avg:46.00ms
step:1297/2160 train_time:59682ms step_avg:46.02ms
step:1298/2160 train_time:59740ms step_avg:46.02ms
step:1299/2160 train_time:59802ms step_avg:46.04ms
step:1300/2160 train_time:59862ms step_avg:46.05ms
step:1301/2160 train_time:59924ms step_avg:46.06ms
step:1302/2160 train_time:59984ms step_avg:46.07ms
step:1303/2160 train_time:60046ms step_avg:46.08ms
step:1304/2160 train_time:60107ms step_avg:46.09ms
step:1305/2160 train_time:60169ms step_avg:46.11ms
step:1306/2160 train_time:60229ms step_avg:46.12ms
step:1307/2160 train_time:60292ms step_avg:46.13ms
step:1308/2160 train_time:60351ms step_avg:46.14ms
step:1309/2160 train_time:60413ms step_avg:46.15ms
step:1310/2160 train_time:60473ms step_avg:46.16ms
step:1311/2160 train_time:60534ms step_avg:46.17ms
step:1312/2160 train_time:60594ms step_avg:46.18ms
step:1313/2160 train_time:60655ms step_avg:46.20ms
step:1314/2160 train_time:60714ms step_avg:46.21ms
step:1315/2160 train_time:60776ms step_avg:46.22ms
step:1316/2160 train_time:60835ms step_avg:46.23ms
step:1317/2160 train_time:60897ms step_avg:46.24ms
step:1318/2160 train_time:60957ms step_avg:46.25ms
step:1319/2160 train_time:61019ms step_avg:46.26ms
step:1320/2160 train_time:61079ms step_avg:46.27ms
step:1321/2160 train_time:61141ms step_avg:46.28ms
step:1322/2160 train_time:61201ms step_avg:46.29ms
step:1323/2160 train_time:61263ms step_avg:46.31ms
step:1324/2160 train_time:61323ms step_avg:46.32ms
step:1325/2160 train_time:61384ms step_avg:46.33ms
step:1326/2160 train_time:61444ms step_avg:46.34ms
step:1327/2160 train_time:61506ms step_avg:46.35ms
step:1328/2160 train_time:61566ms step_avg:46.36ms
step:1329/2160 train_time:61628ms step_avg:46.37ms
step:1330/2160 train_time:61687ms step_avg:46.38ms
step:1331/2160 train_time:61749ms step_avg:46.39ms
step:1332/2160 train_time:61809ms step_avg:46.40ms
step:1333/2160 train_time:61871ms step_avg:46.41ms
step:1334/2160 train_time:61931ms step_avg:46.42ms
step:1335/2160 train_time:61992ms step_avg:46.44ms
step:1336/2160 train_time:62052ms step_avg:46.45ms
step:1337/2160 train_time:62113ms step_avg:46.46ms
step:1338/2160 train_time:62173ms step_avg:46.47ms
step:1339/2160 train_time:62234ms step_avg:46.48ms
step:1340/2160 train_time:62294ms step_avg:46.49ms
step:1341/2160 train_time:62356ms step_avg:46.50ms
step:1342/2160 train_time:62417ms step_avg:46.51ms
step:1343/2160 train_time:62478ms step_avg:46.52ms
step:1344/2160 train_time:62537ms step_avg:46.53ms
step:1345/2160 train_time:62599ms step_avg:46.54ms
step:1346/2160 train_time:62658ms step_avg:46.55ms
step:1347/2160 train_time:62720ms step_avg:46.56ms
step:1348/2160 train_time:62779ms step_avg:46.57ms
step:1349/2160 train_time:62840ms step_avg:46.58ms
step:1350/2160 train_time:62900ms step_avg:46.59ms
step:1351/2160 train_time:62961ms step_avg:46.60ms
step:1352/2160 train_time:63020ms step_avg:46.61ms
step:1353/2160 train_time:63082ms step_avg:46.62ms
step:1354/2160 train_time:63142ms step_avg:46.63ms
step:1355/2160 train_time:63204ms step_avg:46.64ms
step:1356/2160 train_time:63264ms step_avg:46.65ms
step:1357/2160 train_time:63326ms step_avg:46.67ms
step:1358/2160 train_time:63386ms step_avg:46.68ms
step:1359/2160 train_time:63448ms step_avg:46.69ms
step:1360/2160 train_time:63509ms step_avg:46.70ms
step:1361/2160 train_time:63571ms step_avg:46.71ms
step:1362/2160 train_time:63631ms step_avg:46.72ms
step:1363/2160 train_time:63692ms step_avg:46.73ms
step:1364/2160 train_time:63752ms step_avg:46.74ms
step:1365/2160 train_time:63813ms step_avg:46.75ms
step:1366/2160 train_time:63873ms step_avg:46.76ms
step:1367/2160 train_time:63934ms step_avg:46.77ms
step:1368/2160 train_time:63993ms step_avg:46.78ms
step:1369/2160 train_time:64054ms step_avg:46.79ms
step:1370/2160 train_time:64114ms step_avg:46.80ms
step:1371/2160 train_time:64175ms step_avg:46.81ms
step:1372/2160 train_time:64235ms step_avg:46.82ms
step:1373/2160 train_time:64297ms step_avg:46.83ms
step:1374/2160 train_time:64358ms step_avg:46.84ms
step:1375/2160 train_time:64420ms step_avg:46.85ms
step:1376/2160 train_time:64480ms step_avg:46.86ms
step:1377/2160 train_time:64541ms step_avg:46.87ms
step:1378/2160 train_time:64600ms step_avg:46.88ms
step:1379/2160 train_time:64661ms step_avg:46.89ms
step:1380/2160 train_time:64721ms step_avg:46.90ms
step:1381/2160 train_time:64782ms step_avg:46.91ms
step:1382/2160 train_time:64842ms step_avg:46.92ms
step:1383/2160 train_time:64904ms step_avg:46.93ms
step:1384/2160 train_time:64963ms step_avg:46.94ms
step:1385/2160 train_time:65025ms step_avg:46.95ms
step:1386/2160 train_time:65085ms step_avg:46.96ms
step:1387/2160 train_time:65148ms step_avg:46.97ms
step:1388/2160 train_time:65208ms step_avg:46.98ms
step:1389/2160 train_time:65270ms step_avg:46.99ms
step:1390/2160 train_time:65330ms step_avg:47.00ms
step:1391/2160 train_time:65392ms step_avg:47.01ms
step:1392/2160 train_time:65451ms step_avg:47.02ms
step:1393/2160 train_time:65513ms step_avg:47.03ms
step:1394/2160 train_time:65573ms step_avg:47.04ms
step:1395/2160 train_time:65634ms step_avg:47.05ms
step:1396/2160 train_time:65693ms step_avg:47.06ms
step:1397/2160 train_time:65754ms step_avg:47.07ms
step:1398/2160 train_time:65814ms step_avg:47.08ms
step:1399/2160 train_time:65875ms step_avg:47.09ms
step:1400/2160 train_time:65935ms step_avg:47.10ms
step:1401/2160 train_time:65996ms step_avg:47.11ms
step:1402/2160 train_time:66057ms step_avg:47.12ms
step:1403/2160 train_time:66119ms step_avg:47.13ms
step:1404/2160 train_time:66179ms step_avg:47.14ms
step:1405/2160 train_time:66240ms step_avg:47.15ms
step:1406/2160 train_time:66300ms step_avg:47.16ms
step:1407/2160 train_time:66362ms step_avg:47.17ms
step:1408/2160 train_time:66421ms step_avg:47.17ms
step:1409/2160 train_time:66482ms step_avg:47.18ms
step:1410/2160 train_time:66542ms step_avg:47.19ms
step:1411/2160 train_time:66604ms step_avg:47.20ms
step:1412/2160 train_time:66663ms step_avg:47.21ms
step:1413/2160 train_time:66725ms step_avg:47.22ms
step:1414/2160 train_time:66785ms step_avg:47.23ms
step:1415/2160 train_time:66847ms step_avg:47.24ms
step:1416/2160 train_time:66936ms step_avg:47.27ms
step:1417/2160 train_time:67025ms step_avg:47.30ms
step:1418/2160 train_time:67112ms step_avg:47.33ms
step:1419/2160 train_time:67202ms step_avg:47.36ms
step:1420/2160 train_time:67290ms step_avg:47.39ms
step:1421/2160 train_time:67380ms step_avg:47.42ms
step:1422/2160 train_time:67467ms step_avg:47.44ms
step:1423/2160 train_time:67555ms step_avg:47.47ms
step:1424/2160 train_time:67643ms step_avg:47.50ms
step:1425/2160 train_time:67733ms step_avg:47.53ms
step:1426/2160 train_time:67820ms step_avg:47.56ms
step:1427/2160 train_time:67909ms step_avg:47.59ms
step:1428/2160 train_time:67997ms step_avg:47.62ms
step:1429/2160 train_time:68086ms step_avg:47.65ms
step:1430/2160 train_time:68173ms step_avg:47.67ms
step:1431/2160 train_time:68262ms step_avg:47.70ms
step:1432/2160 train_time:68350ms step_avg:47.73ms
step:1433/2160 train_time:68439ms step_avg:47.76ms
step:1434/2160 train_time:68527ms step_avg:47.79ms
step:1435/2160 train_time:68617ms step_avg:47.82ms
step:1436/2160 train_time:68704ms step_avg:47.84ms
step:1437/2160 train_time:68795ms step_avg:47.87ms
step:1438/2160 train_time:68882ms step_avg:47.90ms
step:1439/2160 train_time:68971ms step_avg:47.93ms
step:1440/2160 train_time:69059ms step_avg:47.96ms
step:1441/2160 train_time:69149ms step_avg:47.99ms
step:1442/2160 train_time:69237ms step_avg:48.01ms
step:1443/2160 train_time:69327ms step_avg:48.04ms
step:1444/2160 train_time:69414ms step_avg:48.07ms
step:1445/2160 train_time:69503ms step_avg:48.10ms
step:1446/2160 train_time:69591ms step_avg:48.13ms
step:1447/2160 train_time:69680ms step_avg:48.16ms
step:1448/2160 train_time:69768ms step_avg:48.18ms
step:1449/2160 train_time:69858ms step_avg:48.21ms
step:1450/2160 train_time:69946ms step_avg:48.24ms
step:1451/2160 train_time:70035ms step_avg:48.27ms
step:1452/2160 train_time:70122ms step_avg:48.29ms
step:1453/2160 train_time:70211ms step_avg:48.32ms
step:1454/2160 train_time:70298ms step_avg:48.35ms
step:1455/2160 train_time:70387ms step_avg:48.38ms
step:1456/2160 train_time:70474ms step_avg:48.40ms
step:1457/2160 train_time:70563ms step_avg:48.43ms
step:1458/2160 train_time:70650ms step_avg:48.46ms
step:1459/2160 train_time:70739ms step_avg:48.48ms
step:1460/2160 train_time:70827ms step_avg:48.51ms
step:1461/2160 train_time:70916ms step_avg:48.54ms
step:1462/2160 train_time:71004ms step_avg:48.57ms
step:1463/2160 train_time:71094ms step_avg:48.59ms
step:1464/2160 train_time:71181ms step_avg:48.62ms
step:1465/2160 train_time:71271ms step_avg:48.65ms
step:1466/2160 train_time:71358ms step_avg:48.68ms
step:1467/2160 train_time:71448ms step_avg:48.70ms
step:1468/2160 train_time:71536ms step_avg:48.73ms
step:1469/2160 train_time:71625ms step_avg:48.76ms
step:1470/2160 train_time:71713ms step_avg:48.78ms
step:1471/2160 train_time:71802ms step_avg:48.81ms
step:1472/2160 train_time:71890ms step_avg:48.84ms
step:1473/2160 train_time:71978ms step_avg:48.86ms
step:1474/2160 train_time:72065ms step_avg:48.89ms
step:1475/2160 train_time:72155ms step_avg:48.92ms
step:1476/2160 train_time:72242ms step_avg:48.94ms
step:1477/2160 train_time:72331ms step_avg:48.97ms
step:1478/2160 train_time:72419ms step_avg:49.00ms
step:1479/2160 train_time:72508ms step_avg:49.02ms
step:1480/2160 train_time:72595ms step_avg:49.05ms
step:1481/2160 train_time:72685ms step_avg:49.08ms
step:1482/2160 train_time:72773ms step_avg:49.10ms
step:1483/2160 train_time:72862ms step_avg:49.13ms
step:1484/2160 train_time:72949ms step_avg:49.16ms
step:1485/2160 train_time:73038ms step_avg:49.18ms
step:1486/2160 train_time:73125ms step_avg:49.21ms
step:1487/2160 train_time:73215ms step_avg:49.24ms
step:1488/2160 train_time:73303ms step_avg:49.26ms
step:1489/2160 train_time:73393ms step_avg:49.29ms
step:1490/2160 train_time:73480ms step_avg:49.32ms
step:1491/2160 train_time:73569ms step_avg:49.34ms
step:1492/2160 train_time:73657ms step_avg:49.37ms
step:1493/2160 train_time:73747ms step_avg:49.39ms
step:1494/2160 train_time:73834ms step_avg:49.42ms
step:1495/2160 train_time:73925ms step_avg:49.45ms
step:1496/2160 train_time:74013ms step_avg:49.47ms
step:1497/2160 train_time:74103ms step_avg:49.50ms
step:1498/2160 train_time:74191ms step_avg:49.53ms
step:1499/2160 train_time:74279ms step_avg:49.55ms
step:1500/2160 train_time:74366ms step_avg:49.58ms
step:1500/2160 val_loss:3.4926 train_time:74456ms step_avg:49.64ms
step:1501/2160 train_time:74476ms step_avg:49.62ms
step:1502/2160 train_time:74545ms step_avg:49.63ms
step:1503/2160 train_time:74634ms step_avg:49.66ms
step:1504/2160 train_time:74723ms step_avg:49.68ms
step:1505/2160 train_time:74811ms step_avg:49.71ms
step:1506/2160 train_time:74897ms step_avg:49.73ms
step:1507/2160 train_time:74985ms step_avg:49.76ms
step:1508/2160 train_time:75071ms step_avg:49.78ms
step:1509/2160 train_time:75161ms step_avg:49.81ms
step:1510/2160 train_time:75248ms step_avg:49.83ms
step:1511/2160 train_time:75338ms step_avg:49.86ms
step:1512/2160 train_time:75433ms step_avg:49.89ms
step:1513/2160 train_time:75524ms step_avg:49.92ms
step:1514/2160 train_time:75612ms step_avg:49.94ms
step:1515/2160 train_time:75701ms step_avg:49.97ms
step:1516/2160 train_time:75788ms step_avg:49.99ms
step:1517/2160 train_time:75876ms step_avg:50.02ms
step:1518/2160 train_time:75962ms step_avg:50.04ms
step:1519/2160 train_time:76050ms step_avg:50.07ms
step:1520/2160 train_time:76137ms step_avg:50.09ms
step:1521/2160 train_time:76225ms step_avg:50.12ms
step:1522/2160 train_time:76314ms step_avg:50.14ms
step:1523/2160 train_time:76403ms step_avg:50.17ms
step:1524/2160 train_time:76494ms step_avg:50.19ms
step:1525/2160 train_time:76583ms step_avg:50.22ms
step:1526/2160 train_time:76671ms step_avg:50.24ms
step:1527/2160 train_time:76760ms step_avg:50.27ms
step:1528/2160 train_time:76848ms step_avg:50.29ms
step:1529/2160 train_time:76936ms step_avg:50.32ms
step:1530/2160 train_time:77023ms step_avg:50.34ms
step:1531/2160 train_time:77111ms step_avg:50.37ms
step:1532/2160 train_time:77197ms step_avg:50.39ms
step:1533/2160 train_time:77286ms step_avg:50.42ms
step:1534/2160 train_time:77374ms step_avg:50.44ms
step:1535/2160 train_time:77465ms step_avg:50.47ms
step:1536/2160 train_time:77552ms step_avg:50.49ms
step:1537/2160 train_time:77643ms step_avg:50.52ms
step:1538/2160 train_time:77730ms step_avg:50.54ms
step:1539/2160 train_time:77820ms step_avg:50.57ms
step:1540/2160 train_time:77907ms step_avg:50.59ms
step:1541/2160 train_time:77995ms step_avg:50.61ms
step:1542/2160 train_time:78082ms step_avg:50.64ms
step:1543/2160 train_time:78170ms step_avg:50.66ms
step:1544/2160 train_time:78259ms step_avg:50.69ms
step:1545/2160 train_time:78349ms step_avg:50.71ms
step:1546/2160 train_time:78437ms step_avg:50.74ms
step:1547/2160 train_time:78527ms step_avg:50.76ms
step:1548/2160 train_time:78616ms step_avg:50.79ms
step:1549/2160 train_time:78706ms step_avg:50.81ms
step:1550/2160 train_time:78793ms step_avg:50.83ms
step:1551/2160 train_time:78882ms step_avg:50.86ms
step:1552/2160 train_time:78969ms step_avg:50.88ms
step:1553/2160 train_time:79058ms step_avg:50.91ms
step:1554/2160 train_time:79145ms step_avg:50.93ms
step:1555/2160 train_time:79234ms step_avg:50.95ms
step:1556/2160 train_time:79322ms step_avg:50.98ms
step:1557/2160 train_time:79411ms step_avg:51.00ms
step:1558/2160 train_time:79499ms step_avg:51.03ms
step:1559/2160 train_time:79590ms step_avg:51.05ms
step:1560/2160 train_time:79678ms step_avg:51.08ms
step:1561/2160 train_time:79769ms step_avg:51.10ms
step:1562/2160 train_time:79856ms step_avg:51.12ms
step:1563/2160 train_time:79944ms step_avg:51.15ms
step:1564/2160 train_time:80031ms step_avg:51.17ms
step:1565/2160 train_time:80120ms step_avg:51.19ms
step:1566/2160 train_time:80207ms step_avg:51.22ms
step:1567/2160 train_time:80296ms step_avg:51.24ms
step:1568/2160 train_time:80385ms step_avg:51.27ms
step:1569/2160 train_time:80475ms step_avg:51.29ms
step:1570/2160 train_time:80562ms step_avg:51.31ms
step:1571/2160 train_time:80652ms step_avg:51.34ms
step:1572/2160 train_time:80739ms step_avg:51.36ms
step:1573/2160 train_time:80828ms step_avg:51.38ms
step:1574/2160 train_time:80916ms step_avg:51.41ms
step:1575/2160 train_time:81006ms step_avg:51.43ms
step:1576/2160 train_time:81092ms step_avg:51.45ms
step:1577/2160 train_time:81182ms step_avg:51.48ms
step:1578/2160 train_time:81268ms step_avg:51.50ms
step:1579/2160 train_time:81358ms step_avg:51.53ms
step:1580/2160 train_time:81447ms step_avg:51.55ms
step:1581/2160 train_time:81535ms step_avg:51.57ms
step:1582/2160 train_time:81622ms step_avg:51.59ms
step:1583/2160 train_time:81711ms step_avg:51.62ms
step:1584/2160 train_time:81799ms step_avg:51.64ms
step:1585/2160 train_time:81890ms step_avg:51.67ms
step:1586/2160 train_time:81977ms step_avg:51.69ms
step:1587/2160 train_time:82066ms step_avg:51.71ms
step:1588/2160 train_time:82154ms step_avg:51.73ms
step:1589/2160 train_time:82244ms step_avg:51.76ms
step:1590/2160 train_time:82331ms step_avg:51.78ms
step:1591/2160 train_time:82421ms step_avg:51.80ms
step:1592/2160 train_time:82508ms step_avg:51.83ms
step:1593/2160 train_time:82597ms step_avg:51.85ms
step:1594/2160 train_time:82685ms step_avg:51.87ms
step:1595/2160 train_time:82774ms step_avg:51.90ms
step:1596/2160 train_time:82861ms step_avg:51.92ms
step:1597/2160 train_time:82951ms step_avg:51.94ms
step:1598/2160 train_time:83038ms step_avg:51.96ms
step:1599/2160 train_time:83128ms step_avg:51.99ms
step:1600/2160 train_time:83216ms step_avg:52.01ms
step:1601/2160 train_time:83305ms step_avg:52.03ms
step:1602/2160 train_time:83392ms step_avg:52.06ms
step:1603/2160 train_time:83481ms step_avg:52.08ms
step:1604/2160 train_time:83568ms step_avg:52.10ms
step:1605/2160 train_time:83658ms step_avg:52.12ms
step:1606/2160 train_time:83746ms step_avg:52.15ms
step:1607/2160 train_time:83835ms step_avg:52.17ms
step:1608/2160 train_time:83922ms step_avg:52.19ms
step:1609/2160 train_time:84012ms step_avg:52.21ms
step:1610/2160 train_time:84099ms step_avg:52.24ms
step:1611/2160 train_time:84189ms step_avg:52.26ms
step:1612/2160 train_time:84277ms step_avg:52.28ms
step:1613/2160 train_time:84367ms step_avg:52.30ms
step:1614/2160 train_time:84454ms step_avg:52.33ms
step:1615/2160 train_time:84543ms step_avg:52.35ms
step:1616/2160 train_time:84631ms step_avg:52.37ms
step:1617/2160 train_time:84720ms step_avg:52.39ms
step:1618/2160 train_time:84807ms step_avg:52.42ms
step:1619/2160 train_time:84896ms step_avg:52.44ms
step:1620/2160 train_time:84985ms step_avg:52.46ms
step:1621/2160 train_time:85074ms step_avg:52.48ms
step:1622/2160 train_time:85161ms step_avg:52.50ms
step:1623/2160 train_time:85251ms step_avg:52.53ms
step:1624/2160 train_time:85338ms step_avg:52.55ms
step:1625/2160 train_time:85427ms step_avg:52.57ms
step:1626/2160 train_time:85515ms step_avg:52.59ms
step:1627/2160 train_time:85604ms step_avg:52.61ms
step:1628/2160 train_time:85692ms step_avg:52.64ms
step:1629/2160 train_time:85783ms step_avg:52.66ms
step:1630/2160 train_time:85871ms step_avg:52.68ms
step:1631/2160 train_time:85959ms step_avg:52.70ms
step:1632/2160 train_time:86047ms step_avg:52.73ms
step:1633/2160 train_time:86137ms step_avg:52.75ms
step:1634/2160 train_time:86224ms step_avg:52.77ms
step:1635/2160 train_time:86313ms step_avg:52.79ms
step:1636/2160 train_time:86400ms step_avg:52.81ms
step:1637/2160 train_time:86490ms step_avg:52.83ms
step:1638/2160 train_time:86577ms step_avg:52.86ms
step:1639/2160 train_time:86667ms step_avg:52.88ms
step:1640/2160 train_time:86754ms step_avg:52.90ms
step:1641/2160 train_time:86844ms step_avg:52.92ms
step:1642/2160 train_time:86932ms step_avg:52.94ms
step:1643/2160 train_time:87021ms step_avg:52.96ms
step:1644/2160 train_time:87108ms step_avg:52.99ms
step:1645/2160 train_time:87197ms step_avg:53.01ms
step:1646/2160 train_time:87284ms step_avg:53.03ms
step:1647/2160 train_time:87373ms step_avg:53.05ms
step:1648/2160 train_time:87461ms step_avg:53.07ms
step:1649/2160 train_time:87550ms step_avg:53.09ms
step:1650/2160 train_time:87637ms step_avg:53.11ms
step:1651/2160 train_time:87727ms step_avg:53.14ms
step:1652/2160 train_time:87815ms step_avg:53.16ms
step:1653/2160 train_time:87904ms step_avg:53.18ms
step:1654/2160 train_time:87991ms step_avg:53.20ms
step:1655/2160 train_time:88080ms step_avg:53.22ms
step:1656/2160 train_time:88168ms step_avg:53.24ms
step:1657/2160 train_time:88256ms step_avg:53.26ms
step:1658/2160 train_time:88344ms step_avg:53.28ms
step:1659/2160 train_time:88433ms step_avg:53.30ms
step:1660/2160 train_time:88520ms step_avg:53.33ms
step:1661/2160 train_time:88609ms step_avg:53.35ms
step:1662/2160 train_time:88697ms step_avg:53.37ms
step:1663/2160 train_time:88787ms step_avg:53.39ms
step:1664/2160 train_time:88874ms step_avg:53.41ms
step:1665/2160 train_time:88963ms step_avg:53.43ms
step:1666/2160 train_time:89051ms step_avg:53.45ms
step:1667/2160 train_time:89141ms step_avg:53.47ms
step:1668/2160 train_time:89228ms step_avg:53.49ms
step:1669/2160 train_time:89318ms step_avg:53.52ms
step:1670/2160 train_time:89405ms step_avg:53.54ms
step:1671/2160 train_time:89494ms step_avg:53.56ms
step:1672/2160 train_time:89580ms step_avg:53.58ms
step:1673/2160 train_time:89670ms step_avg:53.60ms
step:1674/2160 train_time:89758ms step_avg:53.62ms
step:1675/2160 train_time:89847ms step_avg:53.64ms
step:1676/2160 train_time:89934ms step_avg:53.66ms
step:1677/2160 train_time:90023ms step_avg:53.68ms
step:1678/2160 train_time:90110ms step_avg:53.70ms
step:1679/2160 train_time:90200ms step_avg:53.72ms
step:1680/2160 train_time:90288ms step_avg:53.74ms
step:1681/2160 train_time:90378ms step_avg:53.76ms
step:1682/2160 train_time:90466ms step_avg:53.78ms
step:1683/2160 train_time:90555ms step_avg:53.81ms
step:1684/2160 train_time:90642ms step_avg:53.83ms
step:1685/2160 train_time:90730ms step_avg:53.85ms
step:1686/2160 train_time:90819ms step_avg:53.87ms
step:1687/2160 train_time:90909ms step_avg:53.89ms
step:1688/2160 train_time:90997ms step_avg:53.91ms
step:1689/2160 train_time:91086ms step_avg:53.93ms
step:1690/2160 train_time:91174ms step_avg:53.95ms
step:1691/2160 train_time:91264ms step_avg:53.97ms
step:1692/2160 train_time:91351ms step_avg:53.99ms
step:1693/2160 train_time:91440ms step_avg:54.01ms
step:1694/2160 train_time:91527ms step_avg:54.03ms
step:1695/2160 train_time:91617ms step_avg:54.05ms
step:1696/2160 train_time:91706ms step_avg:54.07ms
step:1697/2160 train_time:91794ms step_avg:54.09ms
step:1698/2160 train_time:91882ms step_avg:54.11ms
step:1699/2160 train_time:91971ms step_avg:54.13ms
step:1700/2160 train_time:92058ms step_avg:54.15ms
step:1701/2160 train_time:92148ms step_avg:54.17ms
step:1702/2160 train_time:92236ms step_avg:54.19ms
step:1703/2160 train_time:92326ms step_avg:54.21ms
step:1704/2160 train_time:92413ms step_avg:54.23ms
step:1705/2160 train_time:92502ms step_avg:54.25ms
step:1706/2160 train_time:92590ms step_avg:54.27ms
step:1707/2160 train_time:92679ms step_avg:54.29ms
step:1708/2160 train_time:92766ms step_avg:54.31ms
step:1709/2160 train_time:92855ms step_avg:54.33ms
step:1710/2160 train_time:92942ms step_avg:54.35ms
step:1711/2160 train_time:93031ms step_avg:54.37ms
step:1712/2160 train_time:93118ms step_avg:54.39ms
step:1713/2160 train_time:93209ms step_avg:54.41ms
step:1714/2160 train_time:93296ms step_avg:54.43ms
step:1715/2160 train_time:93386ms step_avg:54.45ms
step:1716/2160 train_time:93473ms step_avg:54.47ms
step:1717/2160 train_time:93563ms step_avg:54.49ms
step:1718/2160 train_time:93650ms step_avg:54.51ms
step:1719/2160 train_time:93739ms step_avg:54.53ms
step:1720/2160 train_time:93827ms step_avg:54.55ms
step:1721/2160 train_time:93917ms step_avg:54.57ms
step:1722/2160 train_time:94004ms step_avg:54.59ms
step:1723/2160 train_time:94094ms step_avg:54.61ms
step:1724/2160 train_time:94181ms step_avg:54.63ms
step:1725/2160 train_time:94271ms step_avg:54.65ms
step:1726/2160 train_time:94359ms step_avg:54.67ms
step:1727/2160 train_time:94449ms step_avg:54.69ms
step:1728/2160 train_time:94536ms step_avg:54.71ms
step:1729/2160 train_time:94626ms step_avg:54.73ms
step:1730/2160 train_time:94714ms step_avg:54.75ms
step:1731/2160 train_time:94803ms step_avg:54.77ms
step:1732/2160 train_time:94890ms step_avg:54.79ms
step:1733/2160 train_time:94979ms step_avg:54.81ms
step:1734/2160 train_time:95067ms step_avg:54.83ms
step:1735/2160 train_time:95157ms step_avg:54.85ms
step:1736/2160 train_time:95246ms step_avg:54.87ms
step:1737/2160 train_time:95336ms step_avg:54.89ms
step:1738/2160 train_time:95424ms step_avg:54.90ms
step:1739/2160 train_time:95513ms step_avg:54.92ms
step:1740/2160 train_time:95601ms step_avg:54.94ms
step:1741/2160 train_time:95690ms step_avg:54.96ms
step:1742/2160 train_time:95777ms step_avg:54.98ms
step:1743/2160 train_time:95867ms step_avg:55.00ms
step:1744/2160 train_time:95954ms step_avg:55.02ms
step:1745/2160 train_time:96044ms step_avg:55.04ms
step:1746/2160 train_time:96132ms step_avg:55.06ms
step:1747/2160 train_time:96221ms step_avg:55.08ms
step:1748/2160 train_time:96308ms step_avg:55.10ms
step:1749/2160 train_time:96398ms step_avg:55.12ms
step:1750/2160 train_time:96486ms step_avg:55.13ms
step:1750/2160 val_loss:3.3924 train_time:96577ms step_avg:55.19ms
step:1751/2160 train_time:96597ms step_avg:55.17ms
step:1752/2160 train_time:96669ms step_avg:55.18ms
step:1753/2160 train_time:96763ms step_avg:55.20ms
step:1754/2160 train_time:96852ms step_avg:55.22ms
step:1755/2160 train_time:96940ms step_avg:55.24ms
step:1756/2160 train_time:97027ms step_avg:55.25ms
step:1757/2160 train_time:97115ms step_avg:55.27ms
step:1758/2160 train_time:97202ms step_avg:55.29ms
step:1759/2160 train_time:97290ms step_avg:55.31ms
step:1760/2160 train_time:97377ms step_avg:55.33ms
step:1761/2160 train_time:97465ms step_avg:55.35ms
step:1762/2160 train_time:97554ms step_avg:55.37ms
step:1763/2160 train_time:97646ms step_avg:55.39ms
step:1764/2160 train_time:97735ms step_avg:55.41ms
step:1765/2160 train_time:97827ms step_avg:55.43ms
step:1766/2160 train_time:97914ms step_avg:55.44ms
step:1767/2160 train_time:98003ms step_avg:55.46ms
step:1768/2160 train_time:98091ms step_avg:55.48ms
step:1769/2160 train_time:98178ms step_avg:55.50ms
step:1770/2160 train_time:98265ms step_avg:55.52ms
step:1771/2160 train_time:98354ms step_avg:55.54ms
step:1772/2160 train_time:98442ms step_avg:55.55ms
step:1773/2160 train_time:98531ms step_avg:55.57ms
step:1774/2160 train_time:98620ms step_avg:55.59ms
step:1775/2160 train_time:98711ms step_avg:55.61ms
step:1776/2160 train_time:98799ms step_avg:55.63ms
step:1777/2160 train_time:98889ms step_avg:55.65ms
step:1778/2160 train_time:98978ms step_avg:55.67ms
step:1779/2160 train_time:99066ms step_avg:55.69ms
step:1780/2160 train_time:99153ms step_avg:55.70ms
step:1781/2160 train_time:99242ms step_avg:55.72ms
step:1782/2160 train_time:99328ms step_avg:55.74ms
step:1783/2160 train_time:99417ms step_avg:55.76ms
step:1784/2160 train_time:99504ms step_avg:55.78ms
step:1785/2160 train_time:99593ms step_avg:55.79ms
step:1786/2160 train_time:99681ms step_avg:55.81ms
step:1787/2160 train_time:99772ms step_avg:55.83ms
step:1788/2160 train_time:99861ms step_avg:55.85ms
step:1789/2160 train_time:99950ms step_avg:55.87ms
step:1790/2160 train_time:100038ms step_avg:55.89ms
step:1791/2160 train_time:100127ms step_avg:55.91ms
step:1792/2160 train_time:100214ms step_avg:55.92ms
step:1793/2160 train_time:100303ms step_avg:55.94ms
step:1794/2160 train_time:100389ms step_avg:55.96ms
step:1795/2160 train_time:100478ms step_avg:55.98ms
step:1796/2160 train_time:100565ms step_avg:55.99ms
step:1797/2160 train_time:100656ms step_avg:56.01ms
step:1798/2160 train_time:100744ms step_avg:56.03ms
step:1799/2160 train_time:100835ms step_avg:56.05ms
step:1800/2160 train_time:100922ms step_avg:56.07ms
step:1801/2160 train_time:101012ms step_avg:56.09ms
step:1802/2160 train_time:101100ms step_avg:56.10ms
step:1803/2160 train_time:101189ms step_avg:56.12ms
step:1804/2160 train_time:101275ms step_avg:56.14ms
step:1805/2160 train_time:101364ms step_avg:56.16ms
step:1806/2160 train_time:101451ms step_avg:56.17ms
step:1807/2160 train_time:101540ms step_avg:56.19ms
step:1808/2160 train_time:101628ms step_avg:56.21ms
step:1809/2160 train_time:101717ms step_avg:56.23ms
step:1810/2160 train_time:101805ms step_avg:56.25ms
step:1811/2160 train_time:101895ms step_avg:56.26ms
step:1812/2160 train_time:101983ms step_avg:56.28ms
step:1813/2160 train_time:102072ms step_avg:56.30ms
step:1814/2160 train_time:102159ms step_avg:56.32ms
step:1815/2160 train_time:102249ms step_avg:56.34ms
step:1816/2160 train_time:102335ms step_avg:56.35ms
step:1817/2160 train_time:102424ms step_avg:56.37ms
step:1818/2160 train_time:102511ms step_avg:56.39ms
step:1819/2160 train_time:102602ms step_avg:56.41ms
step:1820/2160 train_time:102690ms step_avg:56.42ms
step:1821/2160 train_time:102779ms step_avg:56.44ms
step:1822/2160 train_time:102867ms step_avg:56.46ms
step:1823/2160 train_time:102957ms step_avg:56.48ms
step:1824/2160 train_time:103045ms step_avg:56.49ms
step:1825/2160 train_time:103134ms step_avg:56.51ms
step:1826/2160 train_time:103222ms step_avg:56.53ms
step:1827/2160 train_time:103311ms step_avg:56.55ms
step:1828/2160 train_time:103398ms step_avg:56.56ms
step:1829/2160 train_time:103488ms step_avg:56.58ms
step:1830/2160 train_time:103575ms step_avg:56.60ms
step:1831/2160 train_time:103665ms step_avg:56.62ms
step:1832/2160 train_time:103753ms step_avg:56.63ms
step:1833/2160 train_time:103842ms step_avg:56.65ms
step:1834/2160 train_time:103931ms step_avg:56.67ms
step:1835/2160 train_time:104020ms step_avg:56.69ms
step:1836/2160 train_time:104108ms step_avg:56.70ms
step:1837/2160 train_time:104197ms step_avg:56.72ms
step:1838/2160 train_time:104285ms step_avg:56.74ms
step:1839/2160 train_time:104376ms step_avg:56.76ms
step:1840/2160 train_time:104463ms step_avg:56.77ms
step:1841/2160 train_time:104553ms step_avg:56.79ms
step:1842/2160 train_time:104641ms step_avg:56.81ms
step:1843/2160 train_time:104730ms step_avg:56.83ms
step:1844/2160 train_time:104817ms step_avg:56.84ms
step:1845/2160 train_time:104906ms step_avg:56.86ms
step:1846/2160 train_time:104994ms step_avg:56.88ms
step:1847/2160 train_time:105084ms step_avg:56.89ms
step:1848/2160 train_time:105171ms step_avg:56.91ms
step:1849/2160 train_time:105261ms step_avg:56.93ms
step:1850/2160 train_time:105349ms step_avg:56.95ms
step:1851/2160 train_time:105437ms step_avg:56.96ms
step:1852/2160 train_time:105525ms step_avg:56.98ms
step:1853/2160 train_time:105614ms step_avg:57.00ms
step:1854/2160 train_time:105700ms step_avg:57.01ms
step:1855/2160 train_time:105789ms step_avg:57.03ms
step:1856/2160 train_time:105878ms step_avg:57.05ms
step:1857/2160 train_time:105968ms step_avg:57.06ms
step:1858/2160 train_time:106056ms step_avg:57.08ms
step:1859/2160 train_time:106145ms step_avg:57.10ms
step:1860/2160 train_time:106233ms step_avg:57.11ms
step:1861/2160 train_time:106321ms step_avg:57.13ms
step:1862/2160 train_time:106409ms step_avg:57.15ms
step:1863/2160 train_time:106498ms step_avg:57.16ms
step:1864/2160 train_time:106586ms step_avg:57.18ms
step:1865/2160 train_time:106675ms step_avg:57.20ms
step:1866/2160 train_time:106763ms step_avg:57.22ms
step:1867/2160 train_time:106852ms step_avg:57.23ms
step:1868/2160 train_time:106941ms step_avg:57.25ms
step:1869/2160 train_time:107030ms step_avg:57.27ms
step:1870/2160 train_time:107118ms step_avg:57.28ms
step:1871/2160 train_time:107209ms step_avg:57.30ms
step:1872/2160 train_time:107297ms step_avg:57.32ms
step:1873/2160 train_time:107387ms step_avg:57.33ms
step:1874/2160 train_time:107474ms step_avg:57.35ms
step:1875/2160 train_time:107562ms step_avg:57.37ms
step:1876/2160 train_time:107651ms step_avg:57.38ms
step:1877/2160 train_time:107739ms step_avg:57.40ms
step:1878/2160 train_time:107826ms step_avg:57.42ms
step:1879/2160 train_time:107915ms step_avg:57.43ms
step:1880/2160 train_time:108002ms step_avg:57.45ms
step:1881/2160 train_time:108092ms step_avg:57.46ms
step:1882/2160 train_time:108180ms step_avg:57.48ms
step:1883/2160 train_time:108269ms step_avg:57.50ms
step:1884/2160 train_time:108357ms step_avg:57.51ms
step:1885/2160 train_time:108447ms step_avg:57.53ms
step:1886/2160 train_time:108535ms step_avg:57.55ms
step:1887/2160 train_time:108623ms step_avg:57.56ms
step:1888/2160 train_time:108711ms step_avg:57.58ms
step:1889/2160 train_time:108801ms step_avg:57.60ms
step:1890/2160 train_time:108888ms step_avg:57.61ms
step:1891/2160 train_time:108977ms step_avg:57.63ms
step:1892/2160 train_time:109064ms step_avg:57.65ms
step:1893/2160 train_time:109153ms step_avg:57.66ms
step:1894/2160 train_time:109241ms step_avg:57.68ms
step:1895/2160 train_time:109331ms step_avg:57.69ms
step:1896/2160 train_time:109420ms step_avg:57.71ms
step:1897/2160 train_time:109510ms step_avg:57.73ms
step:1898/2160 train_time:109597ms step_avg:57.74ms
step:1899/2160 train_time:109687ms step_avg:57.76ms
step:1900/2160 train_time:109776ms step_avg:57.78ms
step:1901/2160 train_time:109865ms step_avg:57.79ms
step:1902/2160 train_time:109952ms step_avg:57.81ms
step:1903/2160 train_time:110041ms step_avg:57.82ms
step:1904/2160 train_time:110128ms step_avg:57.84ms
step:1905/2160 train_time:110217ms step_avg:57.86ms
step:1906/2160 train_time:110305ms step_avg:57.87ms
step:1907/2160 train_time:110395ms step_avg:57.89ms
step:1908/2160 train_time:110483ms step_avg:57.90ms
step:1909/2160 train_time:110572ms step_avg:57.92ms
step:1910/2160 train_time:110660ms step_avg:57.94ms
step:1911/2160 train_time:110748ms step_avg:57.95ms
step:1912/2160 train_time:110836ms step_avg:57.97ms
step:1913/2160 train_time:110927ms step_avg:57.99ms
step:1914/2160 train_time:111014ms step_avg:58.00ms
step:1915/2160 train_time:111104ms step_avg:58.02ms
step:1916/2160 train_time:111191ms step_avg:58.03ms
step:1917/2160 train_time:111281ms step_avg:58.05ms
step:1918/2160 train_time:111367ms step_avg:58.06ms
step:1919/2160 train_time:111457ms step_avg:58.08ms
step:1920/2160 train_time:111545ms step_avg:58.10ms
step:1921/2160 train_time:111634ms step_avg:58.11ms
step:1922/2160 train_time:111722ms step_avg:58.13ms
step:1923/2160 train_time:111810ms step_avg:58.14ms
step:1924/2160 train_time:111899ms step_avg:58.16ms
step:1925/2160 train_time:111989ms step_avg:58.18ms
step:1926/2160 train_time:112076ms step_avg:58.19ms
step:1927/2160 train_time:112166ms step_avg:58.21ms
step:1928/2160 train_time:112253ms step_avg:58.22ms
step:1929/2160 train_time:112343ms step_avg:58.24ms
step:1930/2160 train_time:112432ms step_avg:58.25ms
step:1931/2160 train_time:112521ms step_avg:58.27ms
step:1932/2160 train_time:112607ms step_avg:58.29ms
step:1933/2160 train_time:112697ms step_avg:58.30ms
step:1934/2160 train_time:112785ms step_avg:58.32ms
step:1935/2160 train_time:112874ms step_avg:58.33ms
step:1936/2160 train_time:112962ms step_avg:58.35ms
step:1937/2160 train_time:113051ms step_avg:58.36ms
step:1938/2160 train_time:113138ms step_avg:58.38ms
step:1939/2160 train_time:113229ms step_avg:58.40ms
step:1940/2160 train_time:113316ms step_avg:58.41ms
step:1941/2160 train_time:113406ms step_avg:58.43ms
step:1942/2160 train_time:113493ms step_avg:58.44ms
step:1943/2160 train_time:113583ms step_avg:58.46ms
step:1944/2160 train_time:113670ms step_avg:58.47ms
step:1945/2160 train_time:113759ms step_avg:58.49ms
step:1946/2160 train_time:113846ms step_avg:58.50ms
step:1947/2160 train_time:113935ms step_avg:58.52ms
step:1948/2160 train_time:114023ms step_avg:58.53ms
step:1949/2160 train_time:114113ms step_avg:58.55ms
step:1950/2160 train_time:114202ms step_avg:58.57ms
step:1951/2160 train_time:114291ms step_avg:58.58ms
step:1952/2160 train_time:114380ms step_avg:58.60ms
step:1953/2160 train_time:114469ms step_avg:58.61ms
step:1954/2160 train_time:114557ms step_avg:58.63ms
step:1955/2160 train_time:114646ms step_avg:58.64ms
step:1956/2160 train_time:114733ms step_avg:58.66ms
step:1957/2160 train_time:114822ms step_avg:58.67ms
step:1958/2160 train_time:114910ms step_avg:58.69ms
step:1959/2160 train_time:114999ms step_avg:58.70ms
step:1960/2160 train_time:115087ms step_avg:58.72ms
step:1961/2160 train_time:115177ms step_avg:58.73ms
step:1962/2160 train_time:115264ms step_avg:58.75ms
step:1963/2160 train_time:115353ms step_avg:58.76ms
step:1964/2160 train_time:115441ms step_avg:58.78ms
step:1965/2160 train_time:115530ms step_avg:58.79ms
step:1966/2160 train_time:115617ms step_avg:58.81ms
step:1967/2160 train_time:115706ms step_avg:58.82ms
step:1968/2160 train_time:115794ms step_avg:58.84ms
step:1969/2160 train_time:115883ms step_avg:58.85ms
step:1970/2160 train_time:115971ms step_avg:58.87ms
step:1971/2160 train_time:116059ms step_avg:58.88ms
step:1972/2160 train_time:116147ms step_avg:58.90ms
step:1973/2160 train_time:116237ms step_avg:58.91ms
step:1974/2160 train_time:116325ms step_avg:58.93ms
step:1975/2160 train_time:116415ms step_avg:58.94ms
step:1976/2160 train_time:116503ms step_avg:58.96ms
step:1977/2160 train_time:116592ms step_avg:58.97ms
step:1978/2160 train_time:116681ms step_avg:58.99ms
step:1979/2160 train_time:116770ms step_avg:59.00ms
step:1980/2160 train_time:116857ms step_avg:59.02ms
step:1981/2160 train_time:116947ms step_avg:59.03ms
step:1982/2160 train_time:117034ms step_avg:59.05ms
step:1983/2160 train_time:117124ms step_avg:59.06ms
step:1984/2160 train_time:117212ms step_avg:59.08ms
step:1985/2160 train_time:117301ms step_avg:59.09ms
step:1986/2160 train_time:117389ms step_avg:59.11ms
step:1987/2160 train_time:117478ms step_avg:59.12ms
step:1988/2160 train_time:117566ms step_avg:59.14ms
step:1989/2160 train_time:117657ms step_avg:59.15ms
step:1990/2160 train_time:117745ms step_avg:59.17ms
step:1991/2160 train_time:117834ms step_avg:59.18ms
step:1992/2160 train_time:117921ms step_avg:59.20ms
step:1993/2160 train_time:118010ms step_avg:59.21ms
step:1994/2160 train_time:118097ms step_avg:59.23ms
step:1995/2160 train_time:118187ms step_avg:59.24ms
step:1996/2160 train_time:118274ms step_avg:59.26ms
step:1997/2160 train_time:118363ms step_avg:59.27ms
step:1998/2160 train_time:118451ms step_avg:59.29ms
step:1999/2160 train_time:118541ms step_avg:59.30ms
step:2000/2160 train_time:118629ms step_avg:59.31ms
step:2000/2160 val_loss:3.3126 train_time:118720ms step_avg:59.36ms
step:2001/2160 train_time:118740ms step_avg:59.34ms
step:2002/2160 train_time:118814ms step_avg:59.35ms
step:2003/2160 train_time:118909ms step_avg:59.37ms
step:2004/2160 train_time:118997ms step_avg:59.38ms
step:2005/2160 train_time:119085ms step_avg:59.39ms
step:2006/2160 train_time:119171ms step_avg:59.41ms
step:2007/2160 train_time:119260ms step_avg:59.42ms
step:2008/2160 train_time:119347ms step_avg:59.44ms
step:2009/2160 train_time:119435ms step_avg:59.45ms
step:2010/2160 train_time:119522ms step_avg:59.46ms
step:2011/2160 train_time:119611ms step_avg:59.48ms
step:2012/2160 train_time:119701ms step_avg:59.49ms
step:2013/2160 train_time:119794ms step_avg:59.51ms
step:2014/2160 train_time:119883ms step_avg:59.53ms
step:2015/2160 train_time:119974ms step_avg:59.54ms
step:2016/2160 train_time:120061ms step_avg:59.55ms
step:2017/2160 train_time:120150ms step_avg:59.57ms
step:2018/2160 train_time:120237ms step_avg:59.58ms
step:2019/2160 train_time:120325ms step_avg:59.60ms
step:2020/2160 train_time:120412ms step_avg:59.61ms
step:2021/2160 train_time:120501ms step_avg:59.62ms
step:2022/2160 train_time:120589ms step_avg:59.64ms
step:2023/2160 train_time:120679ms step_avg:59.65ms
step:2024/2160 train_time:120767ms step_avg:59.67ms
step:2025/2160 train_time:120857ms step_avg:59.68ms
step:2026/2160 train_time:120946ms step_avg:59.70ms
step:2027/2160 train_time:121037ms step_avg:59.71ms
step:2028/2160 train_time:121125ms step_avg:59.73ms
step:2029/2160 train_time:121213ms step_avg:59.74ms
step:2030/2160 train_time:121300ms step_avg:59.75ms
step:2031/2160 train_time:121389ms step_avg:59.77ms
step:2032/2160 train_time:121475ms step_avg:59.78ms
step:2033/2160 train_time:121564ms step_avg:59.80ms
step:2034/2160 train_time:121652ms step_avg:59.81ms
step:2035/2160 train_time:121742ms step_avg:59.82ms
step:2036/2160 train_time:121830ms step_avg:59.84ms
step:2037/2160 train_time:121921ms step_avg:59.85ms
step:2038/2160 train_time:122009ms step_avg:59.87ms
step:2039/2160 train_time:122099ms step_avg:59.88ms
step:2040/2160 train_time:122186ms step_avg:59.89ms
step:2041/2160 train_time:122275ms step_avg:59.91ms
step:2042/2160 train_time:122362ms step_avg:59.92ms
step:2043/2160 train_time:122451ms step_avg:59.94ms
step:2044/2160 train_time:122538ms step_avg:59.95ms
step:2045/2160 train_time:122627ms step_avg:59.96ms
step:2046/2160 train_time:122715ms step_avg:59.98ms
step:2047/2160 train_time:122805ms step_avg:59.99ms
step:2048/2160 train_time:122893ms step_avg:60.01ms
step:2049/2160 train_time:122983ms step_avg:60.02ms
step:2050/2160 train_time:123071ms step_avg:60.03ms
step:2051/2160 train_time:123161ms step_avg:60.05ms
step:2052/2160 train_time:123249ms step_avg:60.06ms
step:2053/2160 train_time:123338ms step_avg:60.08ms
step:2054/2160 train_time:123425ms step_avg:60.09ms
step:2055/2160 train_time:123514ms step_avg:60.10ms
step:2056/2160 train_time:123601ms step_avg:60.12ms
step:2057/2160 train_time:123690ms step_avg:60.13ms
step:2058/2160 train_time:123778ms step_avg:60.15ms
step:2059/2160 train_time:123868ms step_avg:60.16ms
step:2060/2160 train_time:123955ms step_avg:60.17ms
step:2061/2160 train_time:124045ms step_avg:60.19ms
step:2062/2160 train_time:124132ms step_avg:60.20ms
step:2063/2160 train_time:124222ms step_avg:60.21ms
step:2064/2160 train_time:124309ms step_avg:60.23ms
step:2065/2160 train_time:124399ms step_avg:60.24ms
step:2066/2160 train_time:124487ms step_avg:60.25ms
step:2067/2160 train_time:124575ms step_avg:60.27ms
step:2068/2160 train_time:124663ms step_avg:60.28ms
step:2069/2160 train_time:124753ms step_avg:60.30ms
step:2070/2160 train_time:124840ms step_avg:60.31ms
step:2071/2160 train_time:124929ms step_avg:60.32ms
step:2072/2160 train_time:125018ms step_avg:60.34ms
step:2073/2160 train_time:125107ms step_avg:60.35ms
step:2074/2160 train_time:125196ms step_avg:60.36ms
step:2075/2160 train_time:125285ms step_avg:60.38ms
step:2076/2160 train_time:125372ms step_avg:60.39ms
step:2077/2160 train_time:125463ms step_avg:60.41ms
step:2078/2160 train_time:125550ms step_avg:60.42ms
step:2079/2160 train_time:125640ms step_avg:60.43ms
step:2080/2160 train_time:125727ms step_avg:60.45ms
step:2081/2160 train_time:125816ms step_avg:60.46ms
step:2082/2160 train_time:125904ms step_avg:60.47ms
step:2083/2160 train_time:125993ms step_avg:60.49ms
step:2084/2160 train_time:126081ms step_avg:60.50ms
step:2085/2160 train_time:126170ms step_avg:60.51ms
step:2086/2160 train_time:126258ms step_avg:60.53ms
step:2087/2160 train_time:126348ms step_avg:60.54ms
step:2088/2160 train_time:126435ms step_avg:60.55ms
step:2089/2160 train_time:126525ms step_avg:60.57ms
step:2090/2160 train_time:126613ms step_avg:60.58ms
step:2091/2160 train_time:126703ms step_avg:60.59ms
step:2092/2160 train_time:126791ms step_avg:60.61ms
step:2093/2160 train_time:126881ms step_avg:60.62ms
step:2094/2160 train_time:126968ms step_avg:60.63ms
step:2095/2160 train_time:127058ms step_avg:60.65ms
step:2096/2160 train_time:127145ms step_avg:60.66ms
step:2097/2160 train_time:127235ms step_avg:60.67ms
step:2098/2160 train_time:127323ms step_avg:60.69ms
step:2099/2160 train_time:127413ms step_avg:60.70ms
step:2100/2160 train_time:127500ms step_avg:60.71ms
step:2101/2160 train_time:127590ms step_avg:60.73ms
step:2102/2160 train_time:127677ms step_avg:60.74ms
step:2103/2160 train_time:127766ms step_avg:60.75ms
step:2104/2160 train_time:127853ms step_avg:60.77ms
step:2105/2160 train_time:127943ms step_avg:60.78ms
step:2106/2160 train_time:128030ms step_avg:60.79ms
step:2107/2160 train_time:128120ms step_avg:60.81ms
step:2108/2160 train_time:128207ms step_avg:60.82ms
step:2109/2160 train_time:128298ms step_avg:60.83ms
step:2110/2160 train_time:128385ms step_avg:60.85ms
step:2111/2160 train_time:128474ms step_avg:60.86ms
step:2112/2160 train_time:128562ms step_avg:60.87ms
step:2113/2160 train_time:128651ms step_avg:60.89ms
step:2114/2160 train_time:128739ms step_avg:60.90ms
step:2115/2160 train_time:128829ms step_avg:60.91ms
step:2116/2160 train_time:128916ms step_avg:60.92ms
step:2117/2160 train_time:129005ms step_avg:60.94ms
step:2118/2160 train_time:129092ms step_avg:60.95ms
step:2119/2160 train_time:129181ms step_avg:60.96ms
step:2120/2160 train_time:129268ms step_avg:60.98ms
step:2121/2160 train_time:129358ms step_avg:60.99ms
step:2122/2160 train_time:129446ms step_avg:61.00ms
step:2123/2160 train_time:129535ms step_avg:61.01ms
step:2124/2160 train_time:129622ms step_avg:61.03ms
step:2125/2160 train_time:129712ms step_avg:61.04ms
step:2126/2160 train_time:129799ms step_avg:61.05ms
step:2127/2160 train_time:129890ms step_avg:61.07ms
step:2128/2160 train_time:129978ms step_avg:61.08ms
step:2129/2160 train_time:130068ms step_avg:61.09ms
step:2130/2160 train_time:130156ms step_avg:61.11ms
step:2131/2160 train_time:130245ms step_avg:61.12ms
step:2132/2160 train_time:130333ms step_avg:61.13ms
step:2133/2160 train_time:130424ms step_avg:61.15ms
step:2134/2160 train_time:130511ms step_avg:61.16ms
step:2135/2160 train_time:130601ms step_avg:61.17ms
step:2136/2160 train_time:130689ms step_avg:61.18ms
step:2137/2160 train_time:130778ms step_avg:61.20ms
step:2138/2160 train_time:130866ms step_avg:61.21ms
step:2139/2160 train_time:130955ms step_avg:61.22ms
step:2140/2160 train_time:131044ms step_avg:61.24ms
step:2141/2160 train_time:131133ms step_avg:61.25ms
step:2142/2160 train_time:131221ms step_avg:61.26ms
step:2143/2160 train_time:131311ms step_avg:61.27ms
step:2144/2160 train_time:131399ms step_avg:61.29ms
step:2145/2160 train_time:131490ms step_avg:61.30ms
step:2146/2160 train_time:131578ms step_avg:61.31ms
step:2147/2160 train_time:131667ms step_avg:61.33ms
step:2148/2160 train_time:131755ms step_avg:61.34ms
step:2149/2160 train_time:131844ms step_avg:61.35ms
step:2150/2160 train_time:131932ms step_avg:61.36ms
step:2151/2160 train_time:132021ms step_avg:61.38ms
step:2152/2160 train_time:132109ms step_avg:61.39ms
step:2153/2160 train_time:132199ms step_avg:61.40ms
step:2154/2160 train_time:132288ms step_avg:61.42ms
step:2155/2160 train_time:132378ms step_avg:61.43ms
step:2156/2160 train_time:132466ms step_avg:61.44ms
step:2157/2160 train_time:132555ms step_avg:61.45ms
step:2158/2160 train_time:132643ms step_avg:61.47ms
step:2159/2160 train_time:132733ms step_avg:61.48ms
step:2160/2160 train_time:132821ms step_avg:61.49ms
step:2160/2160 val_loss:3.2767 train_time:132913ms step_avg:61.53ms
peak memory allocated: 29896 MiB reserved: 61640 MiB

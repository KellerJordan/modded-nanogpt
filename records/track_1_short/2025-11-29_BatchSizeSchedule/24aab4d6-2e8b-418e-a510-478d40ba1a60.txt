import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Fri Dec  5 20:34:27 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   35C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   30C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   30C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          166741      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    0   N/A  N/A          166742      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          166743      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          166744      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          166745      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          166746      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          166747      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          166748      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    1   N/A  N/A          166742      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    2   N/A  N/A          166743      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    3   N/A  N/A          166744      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    4   N/A  N/A          166745      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    5   N/A  N/A          166746      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    6   N/A  N/A          166747      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    7   N/A  N/A          166748      C   /home/ubuntu/.venv/bin/python3         1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:97ms step_avg:97.39ms
step:2/2160 train_time:143ms step_avg:71.48ms
step:3/2160 train_time:166ms step_avg:55.35ms
step:4/2160 train_time:190ms step_avg:47.58ms
step:5/2160 train_time:212ms step_avg:42.41ms
step:6/2160 train_time:386ms step_avg:64.38ms
step:7/2160 train_time:440ms step_avg:62.88ms
step:8/2160 train_time:473ms step_avg:59.16ms
step:9/2160 train_time:507ms step_avg:56.30ms
step:10/2160 train_time:540ms step_avg:53.97ms
step:11/2160 train_time:573ms step_avg:52.12ms
step:12/2160 train_time:606ms step_avg:50.52ms
step:13/2160 train_time:640ms step_avg:49.26ms
step:14/2160 train_time:673ms step_avg:48.11ms
step:15/2160 train_time:708ms step_avg:47.18ms
step:16/2160 train_time:741ms step_avg:46.30ms
step:17/2160 train_time:775ms step_avg:45.57ms
step:18/2160 train_time:808ms step_avg:44.88ms
step:19/2160 train_time:842ms step_avg:44.29ms
step:20/2160 train_time:875ms step_avg:43.73ms
step:21/2160 train_time:909ms step_avg:43.28ms
step:22/2160 train_time:942ms step_avg:42.81ms
step:23/2160 train_time:976ms step_avg:42.44ms
step:24/2160 train_time:1009ms step_avg:42.04ms
step:25/2160 train_time:1043ms step_avg:41.73ms
step:26/2160 train_time:1076ms step_avg:41.40ms
step:27/2160 train_time:1110ms step_avg:41.13ms
step:28/2160 train_time:1144ms step_avg:40.85ms
step:29/2160 train_time:1178ms step_avg:40.60ms
step:30/2160 train_time:1211ms step_avg:40.35ms
step:31/2160 train_time:1244ms step_avg:40.14ms
step:32/2160 train_time:1278ms step_avg:39.92ms
step:33/2160 train_time:1311ms step_avg:39.74ms
step:34/2160 train_time:1345ms step_avg:39.55ms
step:35/2160 train_time:1379ms step_avg:39.40ms
step:36/2160 train_time:1412ms step_avg:39.23ms
step:37/2160 train_time:1447ms step_avg:39.11ms
step:38/2160 train_time:1481ms step_avg:38.96ms
step:39/2160 train_time:1514ms step_avg:38.83ms
step:40/2160 train_time:1548ms step_avg:38.69ms
step:41/2160 train_time:1582ms step_avg:38.58ms
step:42/2160 train_time:1615ms step_avg:38.45ms
step:43/2160 train_time:1649ms step_avg:38.35ms
step:44/2160 train_time:1682ms step_avg:38.23ms
step:45/2160 train_time:1716ms step_avg:38.13ms
step:46/2160 train_time:1749ms step_avg:38.03ms
step:47/2160 train_time:1784ms step_avg:37.95ms
step:48/2160 train_time:1817ms step_avg:37.86ms
step:49/2160 train_time:1851ms step_avg:37.78ms
step:50/2160 train_time:1884ms step_avg:37.69ms
step:51/2160 train_time:1918ms step_avg:37.62ms
step:52/2160 train_time:1951ms step_avg:37.53ms
step:53/2160 train_time:1986ms step_avg:37.47ms
step:54/2160 train_time:2019ms step_avg:37.39ms
step:55/2160 train_time:2053ms step_avg:37.32ms
step:56/2160 train_time:2086ms step_avg:37.25ms
step:57/2160 train_time:2120ms step_avg:37.19ms
step:58/2160 train_time:2153ms step_avg:37.12ms
step:59/2160 train_time:2187ms step_avg:37.06ms
step:60/2160 train_time:2220ms step_avg:36.99ms
step:61/2160 train_time:2254ms step_avg:36.95ms
step:62/2160 train_time:2287ms step_avg:36.88ms
step:63/2160 train_time:2321ms step_avg:36.83ms
step:64/2160 train_time:2354ms step_avg:36.78ms
step:65/2160 train_time:2388ms step_avg:36.74ms
step:66/2160 train_time:2421ms step_avg:36.68ms
step:67/2160 train_time:2455ms step_avg:36.64ms
step:68/2160 train_time:2488ms step_avg:36.59ms
step:69/2160 train_time:2523ms step_avg:36.56ms
step:70/2160 train_time:2556ms step_avg:36.52ms
step:71/2160 train_time:2590ms step_avg:36.48ms
step:72/2160 train_time:2623ms step_avg:36.43ms
step:73/2160 train_time:2658ms step_avg:36.40ms
step:74/2160 train_time:2690ms step_avg:36.36ms
step:75/2160 train_time:2725ms step_avg:36.33ms
step:76/2160 train_time:2758ms step_avg:36.30ms
step:77/2160 train_time:2792ms step_avg:36.26ms
step:78/2160 train_time:2825ms step_avg:36.22ms
step:79/2160 train_time:2859ms step_avg:36.20ms
step:80/2160 train_time:2892ms step_avg:36.15ms
step:81/2160 train_time:2926ms step_avg:36.13ms
step:82/2160 train_time:2960ms step_avg:36.09ms
step:83/2160 train_time:2993ms step_avg:36.06ms
step:84/2160 train_time:3026ms step_avg:36.03ms
step:85/2160 train_time:3060ms step_avg:36.00ms
step:86/2160 train_time:3093ms step_avg:35.96ms
step:87/2160 train_time:3128ms step_avg:35.95ms
step:88/2160 train_time:3161ms step_avg:35.92ms
step:89/2160 train_time:3195ms step_avg:35.89ms
step:90/2160 train_time:3228ms step_avg:35.86ms
step:91/2160 train_time:3261ms step_avg:35.84ms
step:92/2160 train_time:3295ms step_avg:35.81ms
step:93/2160 train_time:3328ms step_avg:35.79ms
step:94/2160 train_time:3362ms step_avg:35.76ms
step:95/2160 train_time:3395ms step_avg:35.74ms
step:96/2160 train_time:3428ms step_avg:35.71ms
step:97/2160 train_time:3463ms step_avg:35.70ms
step:98/2160 train_time:3496ms step_avg:35.67ms
step:99/2160 train_time:3530ms step_avg:35.65ms
step:100/2160 train_time:3563ms step_avg:35.63ms
step:101/2160 train_time:3597ms step_avg:35.61ms
step:102/2160 train_time:3630ms step_avg:35.59ms
step:103/2160 train_time:3664ms step_avg:35.57ms
step:104/2160 train_time:3697ms step_avg:35.55ms
step:105/2160 train_time:3731ms step_avg:35.53ms
step:106/2160 train_time:3764ms step_avg:35.51ms
step:107/2160 train_time:3798ms step_avg:35.50ms
step:108/2160 train_time:3831ms step_avg:35.47ms
step:109/2160 train_time:3865ms step_avg:35.46ms
step:110/2160 train_time:3898ms step_avg:35.44ms
step:111/2160 train_time:3932ms step_avg:35.43ms
step:112/2160 train_time:3965ms step_avg:35.41ms
step:113/2160 train_time:3999ms step_avg:35.39ms
step:114/2160 train_time:4032ms step_avg:35.37ms
step:115/2160 train_time:4066ms step_avg:35.36ms
step:116/2160 train_time:4099ms step_avg:35.34ms
step:117/2160 train_time:4133ms step_avg:35.33ms
step:118/2160 train_time:4166ms step_avg:35.31ms
step:119/2160 train_time:4200ms step_avg:35.30ms
step:120/2160 train_time:4233ms step_avg:35.28ms
step:121/2160 train_time:4267ms step_avg:35.27ms
step:122/2160 train_time:4300ms step_avg:35.25ms
step:123/2160 train_time:4334ms step_avg:35.24ms
step:124/2160 train_time:4367ms step_avg:35.22ms
step:125/2160 train_time:4401ms step_avg:35.21ms
step:126/2160 train_time:4434ms step_avg:35.19ms
step:127/2160 train_time:4468ms step_avg:35.18ms
step:128/2160 train_time:4502ms step_avg:35.17ms
step:129/2160 train_time:4535ms step_avg:35.16ms
step:130/2160 train_time:4568ms step_avg:35.14ms
step:131/2160 train_time:4603ms step_avg:35.14ms
step:132/2160 train_time:4636ms step_avg:35.12ms
step:133/2160 train_time:4670ms step_avg:35.11ms
step:134/2160 train_time:4703ms step_avg:35.10ms
step:135/2160 train_time:4737ms step_avg:35.09ms
step:136/2160 train_time:4770ms step_avg:35.07ms
step:137/2160 train_time:4804ms step_avg:35.06ms
step:138/2160 train_time:4837ms step_avg:35.05ms
step:139/2160 train_time:4871ms step_avg:35.04ms
step:140/2160 train_time:4904ms step_avg:35.03ms
step:141/2160 train_time:4938ms step_avg:35.02ms
step:142/2160 train_time:4971ms step_avg:35.01ms
step:143/2160 train_time:5005ms step_avg:35.00ms
step:144/2160 train_time:5038ms step_avg:34.99ms
step:145/2160 train_time:5072ms step_avg:34.98ms
step:146/2160 train_time:5105ms step_avg:34.97ms
step:147/2160 train_time:5139ms step_avg:34.96ms
step:148/2160 train_time:5172ms step_avg:34.94ms
step:149/2160 train_time:5206ms step_avg:34.94ms
step:150/2160 train_time:5239ms step_avg:34.93ms
step:151/2160 train_time:5273ms step_avg:34.92ms
step:152/2160 train_time:5306ms step_avg:34.91ms
step:153/2160 train_time:5340ms step_avg:34.90ms
step:154/2160 train_time:5373ms step_avg:34.89ms
step:155/2160 train_time:5407ms step_avg:34.88ms
step:156/2160 train_time:5440ms step_avg:34.87ms
step:157/2160 train_time:5474ms step_avg:34.86ms
step:158/2160 train_time:5507ms step_avg:34.85ms
step:159/2160 train_time:5541ms step_avg:34.85ms
step:160/2160 train_time:5574ms step_avg:34.84ms
step:161/2160 train_time:5608ms step_avg:34.84ms
step:162/2160 train_time:5642ms step_avg:34.82ms
step:163/2160 train_time:5676ms step_avg:34.82ms
step:164/2160 train_time:5709ms step_avg:34.81ms
step:165/2160 train_time:5743ms step_avg:34.81ms
step:166/2160 train_time:5776ms step_avg:34.80ms
step:167/2160 train_time:5810ms step_avg:34.79ms
step:168/2160 train_time:5843ms step_avg:34.78ms
step:169/2160 train_time:5877ms step_avg:34.77ms
step:170/2160 train_time:5910ms step_avg:34.76ms
step:171/2160 train_time:5944ms step_avg:34.76ms
step:172/2160 train_time:5977ms step_avg:34.75ms
step:173/2160 train_time:6010ms step_avg:34.74ms
step:174/2160 train_time:6043ms step_avg:34.73ms
step:175/2160 train_time:6077ms step_avg:34.73ms
step:176/2160 train_time:6110ms step_avg:34.72ms
step:177/2160 train_time:6144ms step_avg:34.71ms
step:178/2160 train_time:6177ms step_avg:34.70ms
step:179/2160 train_time:6211ms step_avg:34.70ms
step:180/2160 train_time:6244ms step_avg:34.69ms
step:181/2160 train_time:6278ms step_avg:34.69ms
step:182/2160 train_time:6311ms step_avg:34.68ms
step:183/2160 train_time:6345ms step_avg:34.67ms
step:184/2160 train_time:6378ms step_avg:34.66ms
step:185/2160 train_time:6412ms step_avg:34.66ms
step:186/2160 train_time:6445ms step_avg:34.65ms
step:187/2160 train_time:6479ms step_avg:34.65ms
step:188/2160 train_time:6512ms step_avg:34.64ms
step:189/2160 train_time:6546ms step_avg:34.63ms
step:190/2160 train_time:6579ms step_avg:34.62ms
step:191/2160 train_time:6613ms step_avg:34.62ms
step:192/2160 train_time:6646ms step_avg:34.61ms
step:193/2160 train_time:6679ms step_avg:34.61ms
step:194/2160 train_time:6712ms step_avg:34.60ms
step:195/2160 train_time:6747ms step_avg:34.60ms
step:196/2160 train_time:6779ms step_avg:34.59ms
step:197/2160 train_time:6813ms step_avg:34.59ms
step:198/2160 train_time:6846ms step_avg:34.58ms
step:199/2160 train_time:6880ms step_avg:34.58ms
step:200/2160 train_time:6913ms step_avg:34.57ms
step:201/2160 train_time:6948ms step_avg:34.57ms
step:202/2160 train_time:6981ms step_avg:34.56ms
step:203/2160 train_time:7015ms step_avg:34.56ms
step:204/2160 train_time:7048ms step_avg:34.55ms
step:205/2160 train_time:7082ms step_avg:34.55ms
step:206/2160 train_time:7115ms step_avg:34.54ms
step:207/2160 train_time:7149ms step_avg:34.54ms
step:208/2160 train_time:7182ms step_avg:34.53ms
step:209/2160 train_time:7216ms step_avg:34.53ms
step:210/2160 train_time:7249ms step_avg:34.52ms
step:211/2160 train_time:7283ms step_avg:34.52ms
step:212/2160 train_time:7316ms step_avg:34.51ms
step:213/2160 train_time:7350ms step_avg:34.51ms
step:214/2160 train_time:7383ms step_avg:34.50ms
step:215/2160 train_time:7417ms step_avg:34.50ms
step:216/2160 train_time:7450ms step_avg:34.49ms
step:217/2160 train_time:7484ms step_avg:34.49ms
step:218/2160 train_time:7517ms step_avg:34.48ms
step:219/2160 train_time:7551ms step_avg:34.48ms
step:220/2160 train_time:7584ms step_avg:34.47ms
step:221/2160 train_time:7618ms step_avg:34.47ms
step:222/2160 train_time:7651ms step_avg:34.46ms
step:223/2160 train_time:7685ms step_avg:34.46ms
step:224/2160 train_time:7718ms step_avg:34.45ms
step:225/2160 train_time:7752ms step_avg:34.45ms
step:226/2160 train_time:7785ms step_avg:34.45ms
step:227/2160 train_time:7819ms step_avg:34.44ms
step:228/2160 train_time:7852ms step_avg:34.44ms
step:229/2160 train_time:7886ms step_avg:34.44ms
step:230/2160 train_time:7920ms step_avg:34.43ms
step:231/2160 train_time:7953ms step_avg:34.43ms
step:232/2160 train_time:7986ms step_avg:34.42ms
step:233/2160 train_time:8020ms step_avg:34.42ms
step:234/2160 train_time:8053ms step_avg:34.42ms
step:235/2160 train_time:8088ms step_avg:34.42ms
step:236/2160 train_time:8121ms step_avg:34.41ms
step:237/2160 train_time:8155ms step_avg:34.41ms
step:238/2160 train_time:8187ms step_avg:34.40ms
step:239/2160 train_time:8222ms step_avg:34.40ms
step:240/2160 train_time:8254ms step_avg:34.39ms
step:241/2160 train_time:8288ms step_avg:34.39ms
step:242/2160 train_time:8321ms step_avg:34.39ms
step:243/2160 train_time:8355ms step_avg:34.38ms
step:244/2160 train_time:8388ms step_avg:34.38ms
step:245/2160 train_time:8423ms step_avg:34.38ms
step:246/2160 train_time:8456ms step_avg:34.37ms
step:247/2160 train_time:8490ms step_avg:34.37ms
step:248/2160 train_time:8523ms step_avg:34.37ms
step:249/2160 train_time:8557ms step_avg:34.36ms
step:250/2160 train_time:8589ms step_avg:34.36ms
step:250/2160 val_loss:4.3059 train_time:8625ms step_avg:34.50ms
step:251/2160 train_time:8649ms step_avg:34.46ms
step:252/2160 train_time:8672ms step_avg:34.41ms
step:253/2160 train_time:8695ms step_avg:34.37ms
step:254/2160 train_time:8728ms step_avg:34.36ms
step:255/2160 train_time:8766ms step_avg:34.38ms
step:256/2160 train_time:8801ms step_avg:34.38ms
step:257/2160 train_time:8837ms step_avg:34.39ms
step:258/2160 train_time:8870ms step_avg:34.38ms
step:259/2160 train_time:8904ms step_avg:34.38ms
step:260/2160 train_time:8937ms step_avg:34.37ms
step:261/2160 train_time:8971ms step_avg:34.37ms
step:262/2160 train_time:9004ms step_avg:34.37ms
step:263/2160 train_time:9038ms step_avg:34.36ms
step:264/2160 train_time:9071ms step_avg:34.36ms
step:265/2160 train_time:9105ms step_avg:34.36ms
step:266/2160 train_time:9138ms step_avg:34.35ms
step:267/2160 train_time:9171ms step_avg:34.35ms
step:268/2160 train_time:9204ms step_avg:34.34ms
step:269/2160 train_time:9238ms step_avg:34.34ms
step:270/2160 train_time:9271ms step_avg:34.34ms
step:271/2160 train_time:9305ms step_avg:34.33ms
step:272/2160 train_time:9337ms step_avg:34.33ms
step:273/2160 train_time:9371ms step_avg:34.33ms
step:274/2160 train_time:9404ms step_avg:34.32ms
step:275/2160 train_time:9438ms step_avg:34.32ms
step:276/2160 train_time:9471ms step_avg:34.31ms
step:277/2160 train_time:9504ms step_avg:34.31ms
step:278/2160 train_time:9537ms step_avg:34.31ms
step:279/2160 train_time:9571ms step_avg:34.30ms
step:280/2160 train_time:9604ms step_avg:34.30ms
step:281/2160 train_time:9638ms step_avg:34.30ms
step:282/2160 train_time:9671ms step_avg:34.29ms
step:283/2160 train_time:9705ms step_avg:34.29ms
step:284/2160 train_time:9738ms step_avg:34.29ms
step:285/2160 train_time:9773ms step_avg:34.29ms
step:286/2160 train_time:9806ms step_avg:34.29ms
step:287/2160 train_time:9840ms step_avg:34.29ms
step:288/2160 train_time:9873ms step_avg:34.28ms
step:289/2160 train_time:9907ms step_avg:34.28ms
step:290/2160 train_time:9940ms step_avg:34.28ms
step:291/2160 train_time:9974ms step_avg:34.28ms
step:292/2160 train_time:10007ms step_avg:34.27ms
step:293/2160 train_time:10041ms step_avg:34.27ms
step:294/2160 train_time:10074ms step_avg:34.27ms
step:295/2160 train_time:10108ms step_avg:34.26ms
step:296/2160 train_time:10141ms step_avg:34.26ms
step:297/2160 train_time:10175ms step_avg:34.26ms
step:298/2160 train_time:10208ms step_avg:34.26ms
step:299/2160 train_time:10242ms step_avg:34.26ms
step:300/2160 train_time:10275ms step_avg:34.25ms
step:301/2160 train_time:10309ms step_avg:34.25ms
step:302/2160 train_time:10342ms step_avg:34.24ms
step:303/2160 train_time:10376ms step_avg:34.24ms
step:304/2160 train_time:10409ms step_avg:34.24ms
step:305/2160 train_time:10443ms step_avg:34.24ms
step:306/2160 train_time:10476ms step_avg:34.23ms
step:307/2160 train_time:10509ms step_avg:34.23ms
step:308/2160 train_time:10543ms step_avg:34.23ms
step:309/2160 train_time:10577ms step_avg:34.23ms
step:310/2160 train_time:10610ms step_avg:34.22ms
step:311/2160 train_time:10643ms step_avg:34.22ms
step:312/2160 train_time:10676ms step_avg:34.22ms
step:313/2160 train_time:10711ms step_avg:34.22ms
step:314/2160 train_time:10743ms step_avg:34.21ms
step:315/2160 train_time:10778ms step_avg:34.22ms
step:316/2160 train_time:10811ms step_avg:34.21ms
step:317/2160 train_time:10845ms step_avg:34.21ms
step:318/2160 train_time:10878ms step_avg:34.21ms
step:319/2160 train_time:10912ms step_avg:34.21ms
step:320/2160 train_time:10945ms step_avg:34.20ms
step:321/2160 train_time:10979ms step_avg:34.20ms
step:322/2160 train_time:11012ms step_avg:34.20ms
step:323/2160 train_time:11046ms step_avg:34.20ms
step:324/2160 train_time:11079ms step_avg:34.19ms
step:325/2160 train_time:11113ms step_avg:34.19ms
step:326/2160 train_time:11146ms step_avg:34.19ms
step:327/2160 train_time:11180ms step_avg:34.19ms
step:328/2160 train_time:11213ms step_avg:34.19ms
step:329/2160 train_time:11247ms step_avg:34.18ms
step:330/2160 train_time:11280ms step_avg:34.18ms
step:331/2160 train_time:11314ms step_avg:34.18ms
step:332/2160 train_time:11347ms step_avg:34.18ms
step:333/2160 train_time:11381ms step_avg:34.18ms
step:334/2160 train_time:11414ms step_avg:34.17ms
step:335/2160 train_time:11447ms step_avg:34.17ms
step:336/2160 train_time:11480ms step_avg:34.17ms
step:337/2160 train_time:11515ms step_avg:34.17ms
step:338/2160 train_time:11548ms step_avg:34.17ms
step:339/2160 train_time:11582ms step_avg:34.16ms
step:340/2160 train_time:11615ms step_avg:34.16ms
step:341/2160 train_time:11648ms step_avg:34.16ms
step:342/2160 train_time:11681ms step_avg:34.16ms
step:343/2160 train_time:11716ms step_avg:34.16ms
step:344/2160 train_time:11749ms step_avg:34.15ms
step:345/2160 train_time:11782ms step_avg:34.15ms
step:346/2160 train_time:11815ms step_avg:34.15ms
step:347/2160 train_time:11849ms step_avg:34.15ms
step:348/2160 train_time:11882ms step_avg:34.14ms
step:349/2160 train_time:11916ms step_avg:34.14ms
step:350/2160 train_time:11949ms step_avg:34.14ms
step:351/2160 train_time:11983ms step_avg:34.14ms
step:352/2160 train_time:12016ms step_avg:34.14ms
step:353/2160 train_time:12050ms step_avg:34.14ms
step:354/2160 train_time:12083ms step_avg:34.13ms
step:355/2160 train_time:12117ms step_avg:34.13ms
step:356/2160 train_time:12150ms step_avg:34.13ms
step:357/2160 train_time:12184ms step_avg:34.13ms
step:358/2160 train_time:12217ms step_avg:34.13ms
step:359/2160 train_time:12251ms step_avg:34.13ms
step:360/2160 train_time:12284ms step_avg:34.12ms
step:361/2160 train_time:12318ms step_avg:34.12ms
step:362/2160 train_time:12351ms step_avg:34.12ms
step:363/2160 train_time:12385ms step_avg:34.12ms
step:364/2160 train_time:12418ms step_avg:34.11ms
step:365/2160 train_time:12451ms step_avg:34.11ms
step:366/2160 train_time:12484ms step_avg:34.11ms
step:367/2160 train_time:12519ms step_avg:34.11ms
step:368/2160 train_time:12552ms step_avg:34.11ms
step:369/2160 train_time:12585ms step_avg:34.11ms
step:370/2160 train_time:12618ms step_avg:34.10ms
step:371/2160 train_time:12652ms step_avg:34.10ms
step:372/2160 train_time:12685ms step_avg:34.10ms
step:373/2160 train_time:12719ms step_avg:34.10ms
step:374/2160 train_time:12752ms step_avg:34.10ms
step:375/2160 train_time:12786ms step_avg:34.10ms
step:376/2160 train_time:12819ms step_avg:34.09ms
step:377/2160 train_time:12853ms step_avg:34.09ms
step:378/2160 train_time:12886ms step_avg:34.09ms
step:379/2160 train_time:12920ms step_avg:34.09ms
step:380/2160 train_time:12953ms step_avg:34.09ms
step:381/2160 train_time:12987ms step_avg:34.09ms
step:382/2160 train_time:13020ms step_avg:34.08ms
step:383/2160 train_time:13054ms step_avg:34.08ms
step:384/2160 train_time:13087ms step_avg:34.08ms
step:385/2160 train_time:13121ms step_avg:34.08ms
step:386/2160 train_time:13154ms step_avg:34.08ms
step:387/2160 train_time:13188ms step_avg:34.08ms
step:388/2160 train_time:13221ms step_avg:34.07ms
step:389/2160 train_time:13255ms step_avg:34.07ms
step:390/2160 train_time:13288ms step_avg:34.07ms
step:391/2160 train_time:13322ms step_avg:34.07ms
step:392/2160 train_time:13355ms step_avg:34.07ms
step:393/2160 train_time:13389ms step_avg:34.07ms
step:394/2160 train_time:13422ms step_avg:34.07ms
step:395/2160 train_time:13456ms step_avg:34.07ms
step:396/2160 train_time:13489ms step_avg:34.06ms
step:397/2160 train_time:13523ms step_avg:34.06ms
step:398/2160 train_time:13556ms step_avg:34.06ms
step:399/2160 train_time:13590ms step_avg:34.06ms
step:400/2160 train_time:13623ms step_avg:34.06ms
step:401/2160 train_time:13657ms step_avg:34.06ms
step:402/2160 train_time:13690ms step_avg:34.05ms
step:403/2160 train_time:13724ms step_avg:34.05ms
step:404/2160 train_time:13757ms step_avg:34.05ms
step:405/2160 train_time:13791ms step_avg:34.05ms
step:406/2160 train_time:13824ms step_avg:34.05ms
step:407/2160 train_time:13858ms step_avg:34.05ms
step:408/2160 train_time:13891ms step_avg:34.05ms
step:409/2160 train_time:13925ms step_avg:34.05ms
step:410/2160 train_time:13958ms step_avg:34.04ms
step:411/2160 train_time:13992ms step_avg:34.04ms
step:412/2160 train_time:14026ms step_avg:34.04ms
step:413/2160 train_time:14060ms step_avg:34.04ms
step:414/2160 train_time:14093ms step_avg:34.04ms
step:415/2160 train_time:14126ms step_avg:34.04ms
step:416/2160 train_time:14159ms step_avg:34.04ms
step:417/2160 train_time:14193ms step_avg:34.04ms
step:418/2160 train_time:14226ms step_avg:34.03ms
step:419/2160 train_time:14261ms step_avg:34.04ms
step:420/2160 train_time:14294ms step_avg:34.03ms
step:421/2160 train_time:14328ms step_avg:34.03ms
step:422/2160 train_time:14361ms step_avg:34.03ms
step:423/2160 train_time:14395ms step_avg:34.03ms
step:424/2160 train_time:14428ms step_avg:34.03ms
step:425/2160 train_time:14462ms step_avg:34.03ms
step:426/2160 train_time:14495ms step_avg:34.03ms
step:427/2160 train_time:14529ms step_avg:34.03ms
step:428/2160 train_time:14562ms step_avg:34.02ms
step:429/2160 train_time:14596ms step_avg:34.02ms
step:430/2160 train_time:14629ms step_avg:34.02ms
step:431/2160 train_time:14663ms step_avg:34.02ms
step:432/2160 train_time:14696ms step_avg:34.02ms
step:433/2160 train_time:14729ms step_avg:34.02ms
step:434/2160 train_time:14762ms step_avg:34.01ms
step:435/2160 train_time:14796ms step_avg:34.01ms
step:436/2160 train_time:14829ms step_avg:34.01ms
step:437/2160 train_time:14863ms step_avg:34.01ms
step:438/2160 train_time:14896ms step_avg:34.01ms
step:439/2160 train_time:14930ms step_avg:34.01ms
step:440/2160 train_time:14963ms step_avg:34.01ms
step:441/2160 train_time:14997ms step_avg:34.01ms
step:442/2160 train_time:15031ms step_avg:34.01ms
step:443/2160 train_time:15064ms step_avg:34.01ms
step:444/2160 train_time:15097ms step_avg:34.00ms
step:445/2160 train_time:15131ms step_avg:34.00ms
step:446/2160 train_time:15164ms step_avg:34.00ms
step:447/2160 train_time:15198ms step_avg:34.00ms
step:448/2160 train_time:15231ms step_avg:34.00ms
step:449/2160 train_time:15266ms step_avg:34.00ms
step:450/2160 train_time:15298ms step_avg:34.00ms
step:451/2160 train_time:15333ms step_avg:34.00ms
step:452/2160 train_time:15366ms step_avg:33.99ms
step:453/2160 train_time:15400ms step_avg:33.99ms
step:454/2160 train_time:15433ms step_avg:33.99ms
step:455/2160 train_time:15466ms step_avg:33.99ms
step:456/2160 train_time:15500ms step_avg:33.99ms
step:457/2160 train_time:15534ms step_avg:33.99ms
step:458/2160 train_time:15567ms step_avg:33.99ms
step:459/2160 train_time:15601ms step_avg:33.99ms
step:460/2160 train_time:15634ms step_avg:33.99ms
step:461/2160 train_time:15668ms step_avg:33.99ms
step:462/2160 train_time:15701ms step_avg:33.99ms
step:463/2160 train_time:15736ms step_avg:33.99ms
step:464/2160 train_time:15769ms step_avg:33.98ms
step:465/2160 train_time:15803ms step_avg:33.98ms
step:466/2160 train_time:15836ms step_avg:33.98ms
step:467/2160 train_time:15870ms step_avg:33.98ms
step:468/2160 train_time:15902ms step_avg:33.98ms
step:469/2160 train_time:15936ms step_avg:33.98ms
step:470/2160 train_time:15969ms step_avg:33.98ms
step:471/2160 train_time:16003ms step_avg:33.98ms
step:472/2160 train_time:16036ms step_avg:33.98ms
step:473/2160 train_time:16070ms step_avg:33.97ms
step:474/2160 train_time:16103ms step_avg:33.97ms
step:475/2160 train_time:16137ms step_avg:33.97ms
step:476/2160 train_time:16170ms step_avg:33.97ms
step:477/2160 train_time:16204ms step_avg:33.97ms
step:478/2160 train_time:16237ms step_avg:33.97ms
step:479/2160 train_time:16271ms step_avg:33.97ms
step:480/2160 train_time:16304ms step_avg:33.97ms
step:481/2160 train_time:16338ms step_avg:33.97ms
step:482/2160 train_time:16372ms step_avg:33.97ms
step:483/2160 train_time:16405ms step_avg:33.97ms
step:484/2160 train_time:16438ms step_avg:33.96ms
step:485/2160 train_time:16472ms step_avg:33.96ms
step:486/2160 train_time:16505ms step_avg:33.96ms
step:487/2160 train_time:16540ms step_avg:33.96ms
step:488/2160 train_time:16573ms step_avg:33.96ms
step:489/2160 train_time:16607ms step_avg:33.96ms
step:490/2160 train_time:16640ms step_avg:33.96ms
step:491/2160 train_time:16674ms step_avg:33.96ms
step:492/2160 train_time:16707ms step_avg:33.96ms
step:493/2160 train_time:16741ms step_avg:33.96ms
step:494/2160 train_time:16774ms step_avg:33.96ms
step:495/2160 train_time:16808ms step_avg:33.96ms
step:496/2160 train_time:16841ms step_avg:33.95ms
step:497/2160 train_time:16875ms step_avg:33.95ms
step:498/2160 train_time:16908ms step_avg:33.95ms
step:499/2160 train_time:16942ms step_avg:33.95ms
step:500/2160 train_time:16975ms step_avg:33.95ms
step:500/2160 val_loss:4.0088 train_time:17010ms step_avg:34.02ms
step:501/2160 train_time:17033ms step_avg:34.00ms
step:502/2160 train_time:17056ms step_avg:33.98ms
step:503/2160 train_time:17080ms step_avg:33.96ms
step:504/2160 train_time:17113ms step_avg:33.95ms
step:505/2160 train_time:17149ms step_avg:33.96ms
step:506/2160 train_time:17183ms step_avg:33.96ms
step:507/2160 train_time:17218ms step_avg:33.96ms
step:508/2160 train_time:17252ms step_avg:33.96ms
step:509/2160 train_time:17286ms step_avg:33.96ms
step:510/2160 train_time:17319ms step_avg:33.96ms
step:511/2160 train_time:17353ms step_avg:33.96ms
step:512/2160 train_time:17386ms step_avg:33.96ms
step:513/2160 train_time:17420ms step_avg:33.96ms
step:514/2160 train_time:17452ms step_avg:33.95ms
step:515/2160 train_time:17486ms step_avg:33.95ms
step:516/2160 train_time:17520ms step_avg:33.95ms
step:517/2160 train_time:17553ms step_avg:33.95ms
step:518/2160 train_time:17586ms step_avg:33.95ms
step:519/2160 train_time:17620ms step_avg:33.95ms
step:520/2160 train_time:17653ms step_avg:33.95ms
step:521/2160 train_time:17687ms step_avg:33.95ms
step:522/2160 train_time:17720ms step_avg:33.95ms
step:523/2160 train_time:17753ms step_avg:33.94ms
step:524/2160 train_time:17786ms step_avg:33.94ms
step:525/2160 train_time:17820ms step_avg:33.94ms
step:526/2160 train_time:17853ms step_avg:33.94ms
step:527/2160 train_time:17887ms step_avg:33.94ms
step:528/2160 train_time:17920ms step_avg:33.94ms
step:529/2160 train_time:17953ms step_avg:33.94ms
step:530/2160 train_time:17986ms step_avg:33.94ms
step:531/2160 train_time:18021ms step_avg:33.94ms
step:532/2160 train_time:18054ms step_avg:33.94ms
step:533/2160 train_time:18089ms step_avg:33.94ms
step:534/2160 train_time:18122ms step_avg:33.94ms
step:535/2160 train_time:18157ms step_avg:33.94ms
step:536/2160 train_time:18190ms step_avg:33.94ms
step:537/2160 train_time:18225ms step_avg:33.94ms
step:538/2160 train_time:18257ms step_avg:33.94ms
step:539/2160 train_time:18292ms step_avg:33.94ms
step:540/2160 train_time:18325ms step_avg:33.93ms
step:541/2160 train_time:18359ms step_avg:33.93ms
step:542/2160 train_time:18391ms step_avg:33.93ms
step:543/2160 train_time:18426ms step_avg:33.93ms
step:544/2160 train_time:18459ms step_avg:33.93ms
step:545/2160 train_time:18493ms step_avg:33.93ms
step:546/2160 train_time:18526ms step_avg:33.93ms
step:547/2160 train_time:18560ms step_avg:33.93ms
step:548/2160 train_time:18592ms step_avg:33.93ms
step:549/2160 train_time:18627ms step_avg:33.93ms
step:550/2160 train_time:18659ms step_avg:33.93ms
step:551/2160 train_time:18693ms step_avg:33.93ms
step:552/2160 train_time:18726ms step_avg:33.92ms
step:553/2160 train_time:18760ms step_avg:33.92ms
step:554/2160 train_time:18793ms step_avg:33.92ms
step:555/2160 train_time:18827ms step_avg:33.92ms
step:556/2160 train_time:18860ms step_avg:33.92ms
step:557/2160 train_time:18894ms step_avg:33.92ms
step:558/2160 train_time:18927ms step_avg:33.92ms
step:559/2160 train_time:18961ms step_avg:33.92ms
step:560/2160 train_time:18994ms step_avg:33.92ms
step:561/2160 train_time:19028ms step_avg:33.92ms
step:562/2160 train_time:19061ms step_avg:33.92ms
step:563/2160 train_time:19095ms step_avg:33.92ms
step:564/2160 train_time:19128ms step_avg:33.91ms
step:565/2160 train_time:19162ms step_avg:33.92ms
step:566/2160 train_time:19195ms step_avg:33.91ms
step:567/2160 train_time:19230ms step_avg:33.91ms
step:568/2160 train_time:19263ms step_avg:33.91ms
step:569/2160 train_time:19297ms step_avg:33.91ms
step:570/2160 train_time:19330ms step_avg:33.91ms
step:571/2160 train_time:19364ms step_avg:33.91ms
step:572/2160 train_time:19398ms step_avg:33.91ms
step:573/2160 train_time:19432ms step_avg:33.91ms
step:574/2160 train_time:19464ms step_avg:33.91ms
step:575/2160 train_time:19498ms step_avg:33.91ms
step:576/2160 train_time:19531ms step_avg:33.91ms
step:577/2160 train_time:19565ms step_avg:33.91ms
step:578/2160 train_time:19598ms step_avg:33.91ms
step:579/2160 train_time:19632ms step_avg:33.91ms
step:580/2160 train_time:19665ms step_avg:33.90ms
step:581/2160 train_time:19699ms step_avg:33.91ms
step:582/2160 train_time:19732ms step_avg:33.90ms
step:583/2160 train_time:19766ms step_avg:33.90ms
step:584/2160 train_time:19799ms step_avg:33.90ms
step:585/2160 train_time:19833ms step_avg:33.90ms
step:586/2160 train_time:19865ms step_avg:33.90ms
step:587/2160 train_time:19900ms step_avg:33.90ms
step:588/2160 train_time:19932ms step_avg:33.90ms
step:589/2160 train_time:19967ms step_avg:33.90ms
step:590/2160 train_time:20000ms step_avg:33.90ms
step:591/2160 train_time:20034ms step_avg:33.90ms
step:592/2160 train_time:20067ms step_avg:33.90ms
step:593/2160 train_time:20100ms step_avg:33.90ms
step:594/2160 train_time:20133ms step_avg:33.89ms
step:595/2160 train_time:20167ms step_avg:33.89ms
step:596/2160 train_time:20200ms step_avg:33.89ms
step:597/2160 train_time:20234ms step_avg:33.89ms
step:598/2160 train_time:20267ms step_avg:33.89ms
step:599/2160 train_time:20301ms step_avg:33.89ms
step:600/2160 train_time:20335ms step_avg:33.89ms
step:601/2160 train_time:20369ms step_avg:33.89ms
step:602/2160 train_time:20402ms step_avg:33.89ms
step:603/2160 train_time:20436ms step_avg:33.89ms
step:604/2160 train_time:20469ms step_avg:33.89ms
step:605/2160 train_time:20504ms step_avg:33.89ms
step:606/2160 train_time:20537ms step_avg:33.89ms
step:607/2160 train_time:20571ms step_avg:33.89ms
step:608/2160 train_time:20603ms step_avg:33.89ms
step:609/2160 train_time:20637ms step_avg:33.89ms
step:610/2160 train_time:20670ms step_avg:33.89ms
step:611/2160 train_time:20705ms step_avg:33.89ms
step:612/2160 train_time:20738ms step_avg:33.89ms
step:613/2160 train_time:20772ms step_avg:33.89ms
step:614/2160 train_time:20805ms step_avg:33.88ms
step:615/2160 train_time:20839ms step_avg:33.88ms
step:616/2160 train_time:20872ms step_avg:33.88ms
step:617/2160 train_time:20906ms step_avg:33.88ms
step:618/2160 train_time:20939ms step_avg:33.88ms
step:619/2160 train_time:20973ms step_avg:33.88ms
step:620/2160 train_time:21006ms step_avg:33.88ms
step:621/2160 train_time:21040ms step_avg:33.88ms
step:622/2160 train_time:21073ms step_avg:33.88ms
step:623/2160 train_time:21107ms step_avg:33.88ms
step:624/2160 train_time:21140ms step_avg:33.88ms
step:625/2160 train_time:21174ms step_avg:33.88ms
step:626/2160 train_time:21207ms step_avg:33.88ms
step:627/2160 train_time:21242ms step_avg:33.88ms
step:628/2160 train_time:21275ms step_avg:33.88ms
step:629/2160 train_time:21309ms step_avg:33.88ms
step:630/2160 train_time:21342ms step_avg:33.88ms
step:631/2160 train_time:21376ms step_avg:33.88ms
step:632/2160 train_time:21409ms step_avg:33.88ms
step:633/2160 train_time:21443ms step_avg:33.88ms
step:634/2160 train_time:21476ms step_avg:33.87ms
step:635/2160 train_time:21510ms step_avg:33.87ms
step:636/2160 train_time:21543ms step_avg:33.87ms
step:637/2160 train_time:21577ms step_avg:33.87ms
step:638/2160 train_time:21610ms step_avg:33.87ms
step:639/2160 train_time:21644ms step_avg:33.87ms
step:640/2160 train_time:21677ms step_avg:33.87ms
step:641/2160 train_time:21711ms step_avg:33.87ms
step:642/2160 train_time:21744ms step_avg:33.87ms
step:643/2160 train_time:21778ms step_avg:33.87ms
step:644/2160 train_time:21811ms step_avg:33.87ms
step:645/2160 train_time:21846ms step_avg:33.87ms
step:646/2160 train_time:21879ms step_avg:33.87ms
step:647/2160 train_time:21912ms step_avg:33.87ms
step:648/2160 train_time:21945ms step_avg:33.87ms
step:649/2160 train_time:21979ms step_avg:33.87ms
step:650/2160 train_time:22012ms step_avg:33.87ms
step:651/2160 train_time:22047ms step_avg:33.87ms
step:652/2160 train_time:22080ms step_avg:33.86ms
step:653/2160 train_time:22113ms step_avg:33.86ms
step:654/2160 train_time:22146ms step_avg:33.86ms
step:655/2160 train_time:22180ms step_avg:33.86ms
step:656/2160 train_time:22213ms step_avg:33.86ms
step:657/2160 train_time:22247ms step_avg:33.86ms
step:658/2160 train_time:22281ms step_avg:33.86ms
step:659/2160 train_time:22315ms step_avg:33.86ms
step:660/2160 train_time:22348ms step_avg:33.86ms
step:661/2160 train_time:22382ms step_avg:33.86ms
step:662/2160 train_time:22415ms step_avg:33.86ms
step:663/2160 train_time:22449ms step_avg:33.86ms
step:664/2160 train_time:22482ms step_avg:33.86ms
step:665/2160 train_time:22516ms step_avg:33.86ms
step:666/2160 train_time:22549ms step_avg:33.86ms
step:667/2160 train_time:22583ms step_avg:33.86ms
step:668/2160 train_time:22617ms step_avg:33.86ms
step:669/2160 train_time:22651ms step_avg:33.86ms
step:670/2160 train_time:22683ms step_avg:33.86ms
step:671/2160 train_time:22717ms step_avg:33.86ms
step:672/2160 train_time:22750ms step_avg:33.85ms
step:673/2160 train_time:22785ms step_avg:33.86ms
step:674/2160 train_time:22818ms step_avg:33.85ms
step:675/2160 train_time:22852ms step_avg:33.85ms
step:676/2160 train_time:22885ms step_avg:33.85ms
step:677/2160 train_time:22919ms step_avg:33.85ms
step:678/2160 train_time:22951ms step_avg:33.85ms
step:679/2160 train_time:22986ms step_avg:33.85ms
step:680/2160 train_time:23019ms step_avg:33.85ms
step:681/2160 train_time:23052ms step_avg:33.85ms
step:682/2160 train_time:23085ms step_avg:33.85ms
step:683/2160 train_time:23119ms step_avg:33.85ms
step:684/2160 train_time:23152ms step_avg:33.85ms
step:685/2160 train_time:23186ms step_avg:33.85ms
step:686/2160 train_time:23219ms step_avg:33.85ms
step:687/2160 train_time:23253ms step_avg:33.85ms
step:688/2160 train_time:23286ms step_avg:33.85ms
step:689/2160 train_time:23320ms step_avg:33.85ms
step:690/2160 train_time:23353ms step_avg:33.85ms
step:691/2160 train_time:23388ms step_avg:33.85ms
step:692/2160 train_time:23421ms step_avg:33.85ms
step:693/2160 train_time:23455ms step_avg:33.85ms
step:694/2160 train_time:23488ms step_avg:33.84ms
step:695/2160 train_time:23522ms step_avg:33.84ms
step:696/2160 train_time:23555ms step_avg:33.84ms
step:697/2160 train_time:23589ms step_avg:33.84ms
step:698/2160 train_time:23622ms step_avg:33.84ms
step:699/2160 train_time:23656ms step_avg:33.84ms
step:700/2160 train_time:23689ms step_avg:33.84ms
step:701/2160 train_time:23723ms step_avg:33.84ms
step:702/2160 train_time:23757ms step_avg:33.84ms
step:703/2160 train_time:23791ms step_avg:33.84ms
step:704/2160 train_time:23824ms step_avg:33.84ms
step:705/2160 train_time:23858ms step_avg:33.84ms
step:706/2160 train_time:23891ms step_avg:33.84ms
step:707/2160 train_time:23925ms step_avg:33.84ms
step:708/2160 train_time:23959ms step_avg:33.84ms
step:709/2160 train_time:24019ms step_avg:33.88ms
step:710/2160 train_time:24078ms step_avg:33.91ms
step:711/2160 train_time:24138ms step_avg:33.95ms
step:712/2160 train_time:24198ms step_avg:33.99ms
step:713/2160 train_time:24259ms step_avg:34.02ms
step:714/2160 train_time:24319ms step_avg:34.06ms
step:715/2160 train_time:24380ms step_avg:34.10ms
step:716/2160 train_time:24440ms step_avg:34.13ms
step:717/2160 train_time:24501ms step_avg:34.17ms
step:718/2160 train_time:24560ms step_avg:34.21ms
step:719/2160 train_time:24622ms step_avg:34.24ms
step:720/2160 train_time:24681ms step_avg:34.28ms
step:721/2160 train_time:24743ms step_avg:34.32ms
step:722/2160 train_time:24803ms step_avg:34.35ms
step:723/2160 train_time:24864ms step_avg:34.39ms
step:724/2160 train_time:24923ms step_avg:34.42ms
step:725/2160 train_time:24983ms step_avg:34.46ms
step:726/2160 train_time:25042ms step_avg:34.49ms
step:727/2160 train_time:25104ms step_avg:34.53ms
step:728/2160 train_time:25162ms step_avg:34.56ms
step:729/2160 train_time:25223ms step_avg:34.60ms
step:730/2160 train_time:25282ms step_avg:34.63ms
step:731/2160 train_time:25343ms step_avg:34.67ms
step:732/2160 train_time:25403ms step_avg:34.70ms
step:733/2160 train_time:25463ms step_avg:34.74ms
step:734/2160 train_time:25523ms step_avg:34.77ms
step:735/2160 train_time:25584ms step_avg:34.81ms
step:736/2160 train_time:25643ms step_avg:34.84ms
step:737/2160 train_time:25704ms step_avg:34.88ms
step:738/2160 train_time:25764ms step_avg:34.91ms
step:739/2160 train_time:25824ms step_avg:34.95ms
step:740/2160 train_time:25884ms step_avg:34.98ms
step:741/2160 train_time:25944ms step_avg:35.01ms
step:742/2160 train_time:26004ms step_avg:35.05ms
step:743/2160 train_time:26065ms step_avg:35.08ms
step:744/2160 train_time:26124ms step_avg:35.11ms
step:745/2160 train_time:26185ms step_avg:35.15ms
step:746/2160 train_time:26244ms step_avg:35.18ms
step:747/2160 train_time:26306ms step_avg:35.22ms
step:748/2160 train_time:26365ms step_avg:35.25ms
step:749/2160 train_time:26425ms step_avg:35.28ms
step:750/2160 train_time:26485ms step_avg:35.31ms
step:750/2160 val_loss:3.8504 train_time:26545ms step_avg:35.39ms
step:751/2160 train_time:26569ms step_avg:35.38ms
step:752/2160 train_time:26605ms step_avg:35.38ms
step:753/2160 train_time:26668ms step_avg:35.42ms
step:754/2160 train_time:26734ms step_avg:35.46ms
step:755/2160 train_time:26799ms step_avg:35.50ms
step:756/2160 train_time:26858ms step_avg:35.53ms
step:757/2160 train_time:26919ms step_avg:35.56ms
step:758/2160 train_time:26977ms step_avg:35.59ms
step:759/2160 train_time:27037ms step_avg:35.62ms
step:760/2160 train_time:27096ms step_avg:35.65ms
step:761/2160 train_time:27156ms step_avg:35.68ms
step:762/2160 train_time:27214ms step_avg:35.71ms
step:763/2160 train_time:27274ms step_avg:35.75ms
step:764/2160 train_time:27333ms step_avg:35.78ms
step:765/2160 train_time:27392ms step_avg:35.81ms
step:766/2160 train_time:27452ms step_avg:35.84ms
step:767/2160 train_time:27514ms step_avg:35.87ms
step:768/2160 train_time:27574ms step_avg:35.90ms
step:769/2160 train_time:27638ms step_avg:35.94ms
step:770/2160 train_time:27699ms step_avg:35.97ms
step:771/2160 train_time:27761ms step_avg:36.01ms
step:772/2160 train_time:27820ms step_avg:36.04ms
step:773/2160 train_time:27881ms step_avg:36.07ms
step:774/2160 train_time:27940ms step_avg:36.10ms
step:775/2160 train_time:28001ms step_avg:36.13ms
step:776/2160 train_time:28060ms step_avg:36.16ms
step:777/2160 train_time:28121ms step_avg:36.19ms
step:778/2160 train_time:28180ms step_avg:36.22ms
step:779/2160 train_time:28241ms step_avg:36.25ms
step:780/2160 train_time:28299ms step_avg:36.28ms
step:781/2160 train_time:28361ms step_avg:36.31ms
step:782/2160 train_time:28420ms step_avg:36.34ms
step:783/2160 train_time:28482ms step_avg:36.38ms
step:784/2160 train_time:28543ms step_avg:36.41ms
step:785/2160 train_time:28606ms step_avg:36.44ms
step:786/2160 train_time:28666ms step_avg:36.47ms
step:787/2160 train_time:28727ms step_avg:36.50ms
step:788/2160 train_time:28787ms step_avg:36.53ms
step:789/2160 train_time:28848ms step_avg:36.56ms
step:790/2160 train_time:28907ms step_avg:36.59ms
step:791/2160 train_time:28968ms step_avg:36.62ms
step:792/2160 train_time:29028ms step_avg:36.65ms
step:793/2160 train_time:29089ms step_avg:36.68ms
step:794/2160 train_time:29148ms step_avg:36.71ms
step:795/2160 train_time:29209ms step_avg:36.74ms
step:796/2160 train_time:29268ms step_avg:36.77ms
step:797/2160 train_time:29329ms step_avg:36.80ms
step:798/2160 train_time:29389ms step_avg:36.83ms
step:799/2160 train_time:29449ms step_avg:36.86ms
step:800/2160 train_time:29509ms step_avg:36.89ms
step:801/2160 train_time:29570ms step_avg:36.92ms
step:802/2160 train_time:29630ms step_avg:36.94ms
step:803/2160 train_time:29690ms step_avg:36.97ms
step:804/2160 train_time:29750ms step_avg:37.00ms
step:805/2160 train_time:29811ms step_avg:37.03ms
step:806/2160 train_time:29870ms step_avg:37.06ms
step:807/2160 train_time:29930ms step_avg:37.09ms
step:808/2160 train_time:29990ms step_avg:37.12ms
step:809/2160 train_time:30050ms step_avg:37.14ms
step:810/2160 train_time:30109ms step_avg:37.17ms
step:811/2160 train_time:30171ms step_avg:37.20ms
step:812/2160 train_time:30230ms step_avg:37.23ms
step:813/2160 train_time:30291ms step_avg:37.26ms
step:814/2160 train_time:30350ms step_avg:37.29ms
step:815/2160 train_time:30411ms step_avg:37.31ms
step:816/2160 train_time:30471ms step_avg:37.34ms
step:817/2160 train_time:30532ms step_avg:37.37ms
step:818/2160 train_time:30591ms step_avg:37.40ms
step:819/2160 train_time:30652ms step_avg:37.43ms
step:820/2160 train_time:30711ms step_avg:37.45ms
step:821/2160 train_time:30771ms step_avg:37.48ms
step:822/2160 train_time:30830ms step_avg:37.51ms
step:823/2160 train_time:30890ms step_avg:37.53ms
step:824/2160 train_time:30950ms step_avg:37.56ms
step:825/2160 train_time:31010ms step_avg:37.59ms
step:826/2160 train_time:31070ms step_avg:37.61ms
step:827/2160 train_time:31131ms step_avg:37.64ms
step:828/2160 train_time:31190ms step_avg:37.67ms
step:829/2160 train_time:31250ms step_avg:37.70ms
step:830/2160 train_time:31309ms step_avg:37.72ms
step:831/2160 train_time:31370ms step_avg:37.75ms
step:832/2160 train_time:31429ms step_avg:37.77ms
step:833/2160 train_time:31490ms step_avg:37.80ms
step:834/2160 train_time:31549ms step_avg:37.83ms
step:835/2160 train_time:31610ms step_avg:37.86ms
step:836/2160 train_time:31670ms step_avg:37.88ms
step:837/2160 train_time:31730ms step_avg:37.91ms
step:838/2160 train_time:31790ms step_avg:37.94ms
step:839/2160 train_time:31850ms step_avg:37.96ms
step:840/2160 train_time:31909ms step_avg:37.99ms
step:841/2160 train_time:31970ms step_avg:38.01ms
step:842/2160 train_time:32030ms step_avg:38.04ms
step:843/2160 train_time:32091ms step_avg:38.07ms
step:844/2160 train_time:32150ms step_avg:38.09ms
step:845/2160 train_time:32211ms step_avg:38.12ms
step:846/2160 train_time:32270ms step_avg:38.14ms
step:847/2160 train_time:32331ms step_avg:38.17ms
step:848/2160 train_time:32391ms step_avg:38.20ms
step:849/2160 train_time:32451ms step_avg:38.22ms
step:850/2160 train_time:32510ms step_avg:38.25ms
step:851/2160 train_time:32571ms step_avg:38.27ms
step:852/2160 train_time:32631ms step_avg:38.30ms
step:853/2160 train_time:32691ms step_avg:38.32ms
step:854/2160 train_time:32750ms step_avg:38.35ms
step:855/2160 train_time:32811ms step_avg:38.38ms
step:856/2160 train_time:32870ms step_avg:38.40ms
step:857/2160 train_time:32931ms step_avg:38.43ms
step:858/2160 train_time:32990ms step_avg:38.45ms
step:859/2160 train_time:33051ms step_avg:38.48ms
step:860/2160 train_time:33110ms step_avg:38.50ms
step:861/2160 train_time:33171ms step_avg:38.53ms
step:862/2160 train_time:33230ms step_avg:38.55ms
step:863/2160 train_time:33290ms step_avg:38.57ms
step:864/2160 train_time:33349ms step_avg:38.60ms
step:865/2160 train_time:33410ms step_avg:38.62ms
step:866/2160 train_time:33469ms step_avg:38.65ms
step:867/2160 train_time:33530ms step_avg:38.67ms
step:868/2160 train_time:33590ms step_avg:38.70ms
step:869/2160 train_time:33650ms step_avg:38.72ms
step:870/2160 train_time:33709ms step_avg:38.75ms
step:871/2160 train_time:33770ms step_avg:38.77ms
step:872/2160 train_time:33829ms step_avg:38.80ms
step:873/2160 train_time:33890ms step_avg:38.82ms
step:874/2160 train_time:33949ms step_avg:38.84ms
step:875/2160 train_time:34010ms step_avg:38.87ms
step:876/2160 train_time:34069ms step_avg:38.89ms
step:877/2160 train_time:34130ms step_avg:38.92ms
step:878/2160 train_time:34190ms step_avg:38.94ms
step:879/2160 train_time:34250ms step_avg:38.97ms
step:880/2160 train_time:34310ms step_avg:38.99ms
step:881/2160 train_time:34371ms step_avg:39.01ms
step:882/2160 train_time:34430ms step_avg:39.04ms
step:883/2160 train_time:34491ms step_avg:39.06ms
step:884/2160 train_time:34550ms step_avg:39.08ms
step:885/2160 train_time:34610ms step_avg:39.11ms
step:886/2160 train_time:34670ms step_avg:39.13ms
step:887/2160 train_time:34730ms step_avg:39.15ms
step:888/2160 train_time:34790ms step_avg:39.18ms
step:889/2160 train_time:34851ms step_avg:39.20ms
step:890/2160 train_time:34910ms step_avg:39.22ms
step:891/2160 train_time:34971ms step_avg:39.25ms
step:892/2160 train_time:35030ms step_avg:39.27ms
step:893/2160 train_time:35091ms step_avg:39.30ms
step:894/2160 train_time:35150ms step_avg:39.32ms
step:895/2160 train_time:35211ms step_avg:39.34ms
step:896/2160 train_time:35270ms step_avg:39.36ms
step:897/2160 train_time:35331ms step_avg:39.39ms
step:898/2160 train_time:35390ms step_avg:39.41ms
step:899/2160 train_time:35451ms step_avg:39.43ms
step:900/2160 train_time:35510ms step_avg:39.46ms
step:901/2160 train_time:35570ms step_avg:39.48ms
step:902/2160 train_time:35629ms step_avg:39.50ms
step:903/2160 train_time:35690ms step_avg:39.52ms
step:904/2160 train_time:35750ms step_avg:39.55ms
step:905/2160 train_time:35810ms step_avg:39.57ms
step:906/2160 train_time:35870ms step_avg:39.59ms
step:907/2160 train_time:35930ms step_avg:39.61ms
step:908/2160 train_time:35990ms step_avg:39.64ms
step:909/2160 train_time:36051ms step_avg:39.66ms
step:910/2160 train_time:36110ms step_avg:39.68ms
step:911/2160 train_time:36171ms step_avg:39.70ms
step:912/2160 train_time:36230ms step_avg:39.73ms
step:913/2160 train_time:36291ms step_avg:39.75ms
step:914/2160 train_time:36350ms step_avg:39.77ms
step:915/2160 train_time:36411ms step_avg:39.79ms
step:916/2160 train_time:36470ms step_avg:39.81ms
step:917/2160 train_time:36531ms step_avg:39.84ms
step:918/2160 train_time:36590ms step_avg:39.86ms
step:919/2160 train_time:36651ms step_avg:39.88ms
step:920/2160 train_time:36710ms step_avg:39.90ms
step:921/2160 train_time:36770ms step_avg:39.92ms
step:922/2160 train_time:36830ms step_avg:39.95ms
step:923/2160 train_time:36891ms step_avg:39.97ms
step:924/2160 train_time:36950ms step_avg:39.99ms
step:925/2160 train_time:37011ms step_avg:40.01ms
step:926/2160 train_time:37070ms step_avg:40.03ms
step:927/2160 train_time:37131ms step_avg:40.05ms
step:928/2160 train_time:37190ms step_avg:40.07ms
step:929/2160 train_time:37251ms step_avg:40.10ms
step:930/2160 train_time:37310ms step_avg:40.12ms
step:931/2160 train_time:37371ms step_avg:40.14ms
step:932/2160 train_time:37431ms step_avg:40.16ms
step:933/2160 train_time:37491ms step_avg:40.18ms
step:934/2160 train_time:37551ms step_avg:40.20ms
step:935/2160 train_time:37611ms step_avg:40.23ms
step:936/2160 train_time:37670ms step_avg:40.25ms
step:937/2160 train_time:37731ms step_avg:40.27ms
step:938/2160 train_time:37791ms step_avg:40.29ms
step:939/2160 train_time:37851ms step_avg:40.31ms
step:940/2160 train_time:37910ms step_avg:40.33ms
step:941/2160 train_time:37970ms step_avg:40.35ms
step:942/2160 train_time:38029ms step_avg:40.37ms
step:943/2160 train_time:38090ms step_avg:40.39ms
step:944/2160 train_time:38149ms step_avg:40.41ms
step:945/2160 train_time:38209ms step_avg:40.43ms
step:946/2160 train_time:38269ms step_avg:40.45ms
step:947/2160 train_time:38330ms step_avg:40.48ms
step:948/2160 train_time:38389ms step_avg:40.49ms
step:949/2160 train_time:38450ms step_avg:40.52ms
step:950/2160 train_time:38510ms step_avg:40.54ms
step:951/2160 train_time:38570ms step_avg:40.56ms
step:952/2160 train_time:38630ms step_avg:40.58ms
step:953/2160 train_time:38691ms step_avg:40.60ms
step:954/2160 train_time:38750ms step_avg:40.62ms
step:955/2160 train_time:38811ms step_avg:40.64ms
step:956/2160 train_time:38870ms step_avg:40.66ms
step:957/2160 train_time:38930ms step_avg:40.68ms
step:958/2160 train_time:38990ms step_avg:40.70ms
step:959/2160 train_time:39051ms step_avg:40.72ms
step:960/2160 train_time:39110ms step_avg:40.74ms
step:961/2160 train_time:39171ms step_avg:40.76ms
step:962/2160 train_time:39230ms step_avg:40.78ms
step:963/2160 train_time:39291ms step_avg:40.80ms
step:964/2160 train_time:39350ms step_avg:40.82ms
step:965/2160 train_time:39411ms step_avg:40.84ms
step:966/2160 train_time:39470ms step_avg:40.86ms
step:967/2160 train_time:39532ms step_avg:40.88ms
step:968/2160 train_time:39591ms step_avg:40.90ms
step:969/2160 train_time:39652ms step_avg:40.92ms
step:970/2160 train_time:39711ms step_avg:40.94ms
step:971/2160 train_time:39772ms step_avg:40.96ms
step:972/2160 train_time:39832ms step_avg:40.98ms
step:973/2160 train_time:39892ms step_avg:41.00ms
step:974/2160 train_time:39951ms step_avg:41.02ms
step:975/2160 train_time:40012ms step_avg:41.04ms
step:976/2160 train_time:40071ms step_avg:41.06ms
step:977/2160 train_time:40133ms step_avg:41.08ms
step:978/2160 train_time:40192ms step_avg:41.10ms
step:979/2160 train_time:40252ms step_avg:41.12ms
step:980/2160 train_time:40311ms step_avg:41.13ms
step:981/2160 train_time:40372ms step_avg:41.15ms
step:982/2160 train_time:40431ms step_avg:41.17ms
step:983/2160 train_time:40492ms step_avg:41.19ms
step:984/2160 train_time:40551ms step_avg:41.21ms
step:985/2160 train_time:40611ms step_avg:41.23ms
step:986/2160 train_time:40670ms step_avg:41.25ms
step:987/2160 train_time:40731ms step_avg:41.27ms
step:988/2160 train_time:40790ms step_avg:41.29ms
step:989/2160 train_time:40851ms step_avg:41.30ms
step:990/2160 train_time:40909ms step_avg:41.32ms
step:991/2160 train_time:40971ms step_avg:41.34ms
step:992/2160 train_time:41030ms step_avg:41.36ms
step:993/2160 train_time:41091ms step_avg:41.38ms
step:994/2160 train_time:41151ms step_avg:41.40ms
step:995/2160 train_time:41211ms step_avg:41.42ms
step:996/2160 train_time:41270ms step_avg:41.44ms
step:997/2160 train_time:41330ms step_avg:41.45ms
step:998/2160 train_time:41390ms step_avg:41.47ms
step:999/2160 train_time:41450ms step_avg:41.49ms
step:1000/2160 train_time:41510ms step_avg:41.51ms
step:1000/2160 val_loss:3.6888 train_time:41570ms step_avg:41.57ms
step:1001/2160 train_time:41595ms step_avg:41.55ms
step:1002/2160 train_time:41633ms step_avg:41.55ms
step:1003/2160 train_time:41699ms step_avg:41.57ms
step:1004/2160 train_time:41764ms step_avg:41.60ms
step:1005/2160 train_time:41826ms step_avg:41.62ms
step:1006/2160 train_time:41884ms step_avg:41.63ms
step:1007/2160 train_time:41945ms step_avg:41.65ms
step:1008/2160 train_time:42004ms step_avg:41.67ms
step:1009/2160 train_time:42064ms step_avg:41.69ms
step:1010/2160 train_time:42123ms step_avg:41.71ms
step:1011/2160 train_time:42183ms step_avg:41.72ms
step:1012/2160 train_time:42242ms step_avg:41.74ms
step:1013/2160 train_time:42302ms step_avg:41.76ms
step:1014/2160 train_time:42361ms step_avg:41.78ms
step:1015/2160 train_time:42422ms step_avg:41.79ms
step:1016/2160 train_time:42482ms step_avg:41.81ms
step:1017/2160 train_time:42545ms step_avg:41.83ms
step:1018/2160 train_time:42607ms step_avg:41.85ms
step:1019/2160 train_time:42670ms step_avg:41.87ms
step:1020/2160 train_time:42730ms step_avg:41.89ms
step:1021/2160 train_time:42792ms step_avg:41.91ms
step:1022/2160 train_time:42851ms step_avg:41.93ms
step:1023/2160 train_time:42912ms step_avg:41.95ms
step:1024/2160 train_time:42971ms step_avg:41.96ms
step:1025/2160 train_time:43031ms step_avg:41.98ms
step:1026/2160 train_time:43090ms step_avg:42.00ms
step:1027/2160 train_time:43150ms step_avg:42.02ms
step:1028/2160 train_time:43210ms step_avg:42.03ms
step:1029/2160 train_time:43270ms step_avg:42.05ms
step:1030/2160 train_time:43329ms step_avg:42.07ms
step:1031/2160 train_time:43389ms step_avg:42.08ms
step:1032/2160 train_time:43449ms step_avg:42.10ms
step:1033/2160 train_time:43510ms step_avg:42.12ms
step:1034/2160 train_time:43570ms step_avg:42.14ms
step:1035/2160 train_time:43631ms step_avg:42.16ms
step:1036/2160 train_time:43691ms step_avg:42.17ms
step:1037/2160 train_time:43752ms step_avg:42.19ms
step:1038/2160 train_time:43811ms step_avg:42.21ms
step:1039/2160 train_time:43872ms step_avg:42.23ms
step:1040/2160 train_time:43931ms step_avg:42.24ms
step:1041/2160 train_time:43991ms step_avg:42.26ms
step:1042/2160 train_time:44051ms step_avg:42.27ms
step:1043/2160 train_time:44111ms step_avg:42.29ms
step:1044/2160 train_time:44170ms step_avg:42.31ms
step:1045/2160 train_time:44230ms step_avg:42.33ms
step:1046/2160 train_time:44290ms step_avg:42.34ms
step:1047/2160 train_time:44350ms step_avg:42.36ms
step:1048/2160 train_time:44409ms step_avg:42.37ms
step:1049/2160 train_time:44470ms step_avg:42.39ms
step:1050/2160 train_time:44530ms step_avg:42.41ms
step:1051/2160 train_time:44591ms step_avg:42.43ms
step:1052/2160 train_time:44650ms step_avg:42.44ms
step:1053/2160 train_time:44712ms step_avg:42.46ms
step:1054/2160 train_time:44771ms step_avg:42.48ms
step:1055/2160 train_time:44832ms step_avg:42.49ms
step:1056/2160 train_time:44891ms step_avg:42.51ms
step:1057/2160 train_time:44951ms step_avg:42.53ms
step:1058/2160 train_time:45010ms step_avg:42.54ms
step:1059/2160 train_time:45071ms step_avg:42.56ms
step:1060/2160 train_time:45130ms step_avg:42.58ms
step:1061/2160 train_time:45190ms step_avg:42.59ms
step:1062/2160 train_time:45249ms step_avg:42.61ms
step:1063/2160 train_time:45309ms step_avg:42.62ms
step:1064/2160 train_time:45369ms step_avg:42.64ms
step:1065/2160 train_time:45429ms step_avg:42.66ms
step:1066/2160 train_time:45489ms step_avg:42.67ms
step:1067/2160 train_time:45549ms step_avg:42.69ms
step:1068/2160 train_time:45609ms step_avg:42.71ms
step:1069/2160 train_time:45670ms step_avg:42.72ms
step:1070/2160 train_time:45730ms step_avg:42.74ms
step:1071/2160 train_time:45791ms step_avg:42.76ms
step:1072/2160 train_time:45851ms step_avg:42.77ms
step:1073/2160 train_time:45911ms step_avg:42.79ms
step:1074/2160 train_time:45970ms step_avg:42.80ms
step:1075/2160 train_time:46031ms step_avg:42.82ms
step:1076/2160 train_time:46090ms step_avg:42.83ms
step:1077/2160 train_time:46150ms step_avg:42.85ms
step:1078/2160 train_time:46209ms step_avg:42.87ms
step:1079/2160 train_time:46269ms step_avg:42.88ms
step:1080/2160 train_time:46329ms step_avg:42.90ms
step:1081/2160 train_time:46389ms step_avg:42.91ms
step:1082/2160 train_time:46448ms step_avg:42.93ms
step:1083/2160 train_time:46509ms step_avg:42.94ms
step:1084/2160 train_time:46569ms step_avg:42.96ms
step:1085/2160 train_time:46630ms step_avg:42.98ms
step:1086/2160 train_time:46689ms step_avg:42.99ms
step:1087/2160 train_time:46750ms step_avg:43.01ms
step:1088/2160 train_time:46809ms step_avg:43.02ms
step:1089/2160 train_time:46871ms step_avg:43.04ms
step:1090/2160 train_time:46930ms step_avg:43.06ms
step:1091/2160 train_time:46991ms step_avg:43.07ms
step:1092/2160 train_time:47050ms step_avg:43.09ms
step:1093/2160 train_time:47110ms step_avg:43.10ms
step:1094/2160 train_time:47169ms step_avg:43.12ms
step:1095/2160 train_time:47230ms step_avg:43.13ms
step:1096/2160 train_time:47289ms step_avg:43.15ms
step:1097/2160 train_time:47350ms step_avg:43.16ms
step:1098/2160 train_time:47409ms step_avg:43.18ms
step:1099/2160 train_time:47469ms step_avg:43.19ms
step:1100/2160 train_time:47529ms step_avg:43.21ms
step:1101/2160 train_time:47589ms step_avg:43.22ms
step:1102/2160 train_time:47649ms step_avg:43.24ms
step:1103/2160 train_time:47711ms step_avg:43.26ms
step:1104/2160 train_time:47770ms step_avg:43.27ms
step:1105/2160 train_time:47831ms step_avg:43.29ms
step:1106/2160 train_time:47890ms step_avg:43.30ms
step:1107/2160 train_time:47950ms step_avg:43.32ms
step:1108/2160 train_time:48009ms step_avg:43.33ms
step:1109/2160 train_time:48070ms step_avg:43.35ms
step:1110/2160 train_time:48129ms step_avg:43.36ms
step:1111/2160 train_time:48189ms step_avg:43.37ms
step:1112/2160 train_time:48248ms step_avg:43.39ms
step:1113/2160 train_time:48309ms step_avg:43.40ms
step:1114/2160 train_time:48368ms step_avg:43.42ms
step:1115/2160 train_time:48429ms step_avg:43.43ms
step:1116/2160 train_time:48488ms step_avg:43.45ms
step:1117/2160 train_time:48549ms step_avg:43.46ms
step:1118/2160 train_time:48608ms step_avg:43.48ms
step:1119/2160 train_time:48670ms step_avg:43.49ms
step:1120/2160 train_time:48730ms step_avg:43.51ms
step:1121/2160 train_time:48790ms step_avg:43.52ms
step:1122/2160 train_time:48849ms step_avg:43.54ms
step:1123/2160 train_time:48911ms step_avg:43.55ms
step:1124/2160 train_time:48970ms step_avg:43.57ms
step:1125/2160 train_time:49030ms step_avg:43.58ms
step:1126/2160 train_time:49090ms step_avg:43.60ms
step:1127/2160 train_time:49150ms step_avg:43.61ms
step:1128/2160 train_time:49209ms step_avg:43.62ms
step:1129/2160 train_time:49269ms step_avg:43.64ms
step:1130/2160 train_time:49329ms step_avg:43.65ms
step:1131/2160 train_time:49389ms step_avg:43.67ms
step:1132/2160 train_time:49449ms step_avg:43.68ms
step:1133/2160 train_time:49509ms step_avg:43.70ms
step:1134/2160 train_time:49569ms step_avg:43.71ms
step:1135/2160 train_time:49629ms step_avg:43.73ms
step:1136/2160 train_time:49689ms step_avg:43.74ms
step:1137/2160 train_time:49750ms step_avg:43.76ms
step:1138/2160 train_time:49810ms step_avg:43.77ms
step:1139/2160 train_time:49871ms step_avg:43.78ms
step:1140/2160 train_time:49930ms step_avg:43.80ms
step:1141/2160 train_time:49991ms step_avg:43.81ms
step:1142/2160 train_time:50050ms step_avg:43.83ms
step:1143/2160 train_time:50110ms step_avg:43.84ms
step:1144/2160 train_time:50169ms step_avg:43.85ms
step:1145/2160 train_time:50230ms step_avg:43.87ms
step:1146/2160 train_time:50289ms step_avg:43.88ms
step:1147/2160 train_time:50350ms step_avg:43.90ms
step:1148/2160 train_time:50409ms step_avg:43.91ms
step:1149/2160 train_time:50470ms step_avg:43.92ms
step:1150/2160 train_time:50529ms step_avg:43.94ms
step:1151/2160 train_time:50589ms step_avg:43.95ms
step:1152/2160 train_time:50649ms step_avg:43.97ms
step:1153/2160 train_time:50710ms step_avg:43.98ms
step:1154/2160 train_time:50769ms step_avg:43.99ms
step:1155/2160 train_time:50830ms step_avg:44.01ms
step:1156/2160 train_time:50890ms step_avg:44.02ms
step:1157/2160 train_time:50951ms step_avg:44.04ms
step:1158/2160 train_time:51010ms step_avg:44.05ms
step:1159/2160 train_time:51071ms step_avg:44.06ms
step:1160/2160 train_time:51131ms step_avg:44.08ms
step:1161/2160 train_time:51191ms step_avg:44.09ms
step:1162/2160 train_time:51250ms step_avg:44.11ms
step:1163/2160 train_time:51311ms step_avg:44.12ms
step:1164/2160 train_time:51371ms step_avg:44.13ms
step:1165/2160 train_time:51431ms step_avg:44.15ms
step:1166/2160 train_time:51491ms step_avg:44.16ms
step:1167/2160 train_time:51551ms step_avg:44.17ms
step:1168/2160 train_time:51611ms step_avg:44.19ms
step:1169/2160 train_time:51672ms step_avg:44.20ms
step:1170/2160 train_time:51731ms step_avg:44.21ms
step:1171/2160 train_time:51792ms step_avg:44.23ms
step:1172/2160 train_time:51851ms step_avg:44.24ms
step:1173/2160 train_time:51912ms step_avg:44.26ms
step:1174/2160 train_time:51971ms step_avg:44.27ms
step:1175/2160 train_time:52032ms step_avg:44.28ms
step:1176/2160 train_time:52092ms step_avg:44.30ms
step:1177/2160 train_time:52153ms step_avg:44.31ms
step:1178/2160 train_time:52211ms step_avg:44.32ms
step:1179/2160 train_time:52272ms step_avg:44.34ms
step:1180/2160 train_time:52331ms step_avg:44.35ms
step:1181/2160 train_time:52391ms step_avg:44.36ms
step:1182/2160 train_time:52450ms step_avg:44.37ms
step:1183/2160 train_time:52511ms step_avg:44.39ms
step:1184/2160 train_time:52570ms step_avg:44.40ms
step:1185/2160 train_time:52631ms step_avg:44.41ms
step:1186/2160 train_time:52691ms step_avg:44.43ms
step:1187/2160 train_time:52751ms step_avg:44.44ms
step:1188/2160 train_time:52810ms step_avg:44.45ms
step:1189/2160 train_time:52871ms step_avg:44.47ms
step:1190/2160 train_time:52931ms step_avg:44.48ms
step:1191/2160 train_time:52991ms step_avg:44.49ms
step:1192/2160 train_time:53051ms step_avg:44.51ms
step:1193/2160 train_time:53111ms step_avg:44.52ms
step:1194/2160 train_time:53171ms step_avg:44.53ms
step:1195/2160 train_time:53232ms step_avg:44.55ms
step:1196/2160 train_time:53291ms step_avg:44.56ms
step:1197/2160 train_time:53351ms step_avg:44.57ms
step:1198/2160 train_time:53411ms step_avg:44.58ms
step:1199/2160 train_time:53471ms step_avg:44.60ms
step:1200/2160 train_time:53531ms step_avg:44.61ms
step:1201/2160 train_time:53592ms step_avg:44.62ms
step:1202/2160 train_time:53651ms step_avg:44.63ms
step:1203/2160 train_time:53711ms step_avg:44.65ms
step:1204/2160 train_time:53770ms step_avg:44.66ms
step:1205/2160 train_time:53831ms step_avg:44.67ms
step:1206/2160 train_time:53890ms step_avg:44.69ms
step:1207/2160 train_time:53951ms step_avg:44.70ms
step:1208/2160 train_time:54010ms step_avg:44.71ms
step:1209/2160 train_time:54071ms step_avg:44.72ms
step:1210/2160 train_time:54130ms step_avg:44.74ms
step:1211/2160 train_time:54191ms step_avg:44.75ms
step:1212/2160 train_time:54250ms step_avg:44.76ms
step:1213/2160 train_time:54311ms step_avg:44.77ms
step:1214/2160 train_time:54370ms step_avg:44.79ms
step:1215/2160 train_time:54431ms step_avg:44.80ms
step:1216/2160 train_time:54490ms step_avg:44.81ms
step:1217/2160 train_time:54550ms step_avg:44.82ms
step:1218/2160 train_time:54610ms step_avg:44.84ms
step:1219/2160 train_time:54670ms step_avg:44.85ms
step:1220/2160 train_time:54730ms step_avg:44.86ms
step:1221/2160 train_time:54791ms step_avg:44.87ms
step:1222/2160 train_time:54850ms step_avg:44.89ms
step:1223/2160 train_time:54911ms step_avg:44.90ms
step:1224/2160 train_time:54970ms step_avg:44.91ms
step:1225/2160 train_time:55030ms step_avg:44.92ms
step:1226/2160 train_time:55089ms step_avg:44.93ms
step:1227/2160 train_time:55150ms step_avg:44.95ms
step:1228/2160 train_time:55210ms step_avg:44.96ms
step:1229/2160 train_time:55270ms step_avg:44.97ms
step:1230/2160 train_time:55330ms step_avg:44.98ms
step:1231/2160 train_time:55390ms step_avg:45.00ms
step:1232/2160 train_time:55449ms step_avg:45.01ms
step:1233/2160 train_time:55510ms step_avg:45.02ms
step:1234/2160 train_time:55570ms step_avg:45.03ms
step:1235/2160 train_time:55631ms step_avg:45.05ms
step:1236/2160 train_time:55690ms step_avg:45.06ms
step:1237/2160 train_time:55751ms step_avg:45.07ms
step:1238/2160 train_time:55810ms step_avg:45.08ms
step:1239/2160 train_time:55871ms step_avg:45.09ms
step:1240/2160 train_time:55931ms step_avg:45.11ms
step:1241/2160 train_time:55992ms step_avg:45.12ms
step:1242/2160 train_time:56051ms step_avg:45.13ms
step:1243/2160 train_time:56112ms step_avg:45.14ms
step:1244/2160 train_time:56171ms step_avg:45.15ms
step:1245/2160 train_time:56232ms step_avg:45.17ms
step:1246/2160 train_time:56291ms step_avg:45.18ms
step:1247/2160 train_time:56352ms step_avg:45.19ms
step:1248/2160 train_time:56411ms step_avg:45.20ms
step:1249/2160 train_time:56472ms step_avg:45.21ms
step:1250/2160 train_time:56531ms step_avg:45.22ms
step:1250/2160 val_loss:3.5716 train_time:56592ms step_avg:45.27ms
step:1251/2160 train_time:56616ms step_avg:45.26ms
step:1252/2160 train_time:56653ms step_avg:45.25ms
step:1253/2160 train_time:56717ms step_avg:45.27ms
step:1254/2160 train_time:56782ms step_avg:45.28ms
step:1255/2160 train_time:56844ms step_avg:45.29ms
step:1256/2160 train_time:56903ms step_avg:45.31ms
step:1257/2160 train_time:56964ms step_avg:45.32ms
step:1258/2160 train_time:57023ms step_avg:45.33ms
step:1259/2160 train_time:57083ms step_avg:45.34ms
step:1260/2160 train_time:57142ms step_avg:45.35ms
step:1261/2160 train_time:57203ms step_avg:45.36ms
step:1262/2160 train_time:57262ms step_avg:45.37ms
step:1263/2160 train_time:57322ms step_avg:45.39ms
step:1264/2160 train_time:57381ms step_avg:45.40ms
step:1265/2160 train_time:57441ms step_avg:45.41ms
step:1266/2160 train_time:57501ms step_avg:45.42ms
step:1267/2160 train_time:57566ms step_avg:45.44ms
step:1268/2160 train_time:57627ms step_avg:45.45ms
step:1269/2160 train_time:57690ms step_avg:45.46ms
step:1270/2160 train_time:57750ms step_avg:45.47ms
step:1271/2160 train_time:57811ms step_avg:45.48ms
step:1272/2160 train_time:57871ms step_avg:45.50ms
step:1273/2160 train_time:57932ms step_avg:45.51ms
step:1274/2160 train_time:57991ms step_avg:45.52ms
step:1275/2160 train_time:58052ms step_avg:45.53ms
step:1276/2160 train_time:58111ms step_avg:45.54ms
step:1277/2160 train_time:58172ms step_avg:45.55ms
step:1278/2160 train_time:58231ms step_avg:45.56ms
step:1279/2160 train_time:58291ms step_avg:45.58ms
step:1280/2160 train_time:58350ms step_avg:45.59ms
step:1281/2160 train_time:58410ms step_avg:45.60ms
step:1282/2160 train_time:58469ms step_avg:45.61ms
step:1283/2160 train_time:58530ms step_avg:45.62ms
step:1284/2160 train_time:58589ms step_avg:45.63ms
step:1285/2160 train_time:58650ms step_avg:45.64ms
step:1286/2160 train_time:58710ms step_avg:45.65ms
step:1287/2160 train_time:58772ms step_avg:45.67ms
step:1288/2160 train_time:58832ms step_avg:45.68ms
step:1289/2160 train_time:58892ms step_avg:45.69ms
step:1290/2160 train_time:58951ms step_avg:45.70ms
step:1291/2160 train_time:59012ms step_avg:45.71ms
step:1292/2160 train_time:59071ms step_avg:45.72ms
step:1293/2160 train_time:59132ms step_avg:45.73ms
step:1294/2160 train_time:59192ms step_avg:45.74ms
step:1295/2160 train_time:59253ms step_avg:45.75ms
step:1296/2160 train_time:59311ms step_avg:45.77ms
step:1297/2160 train_time:59373ms step_avg:45.78ms
step:1298/2160 train_time:59432ms step_avg:45.79ms
step:1299/2160 train_time:59493ms step_avg:45.80ms
step:1300/2160 train_time:59554ms step_avg:45.81ms
step:1301/2160 train_time:59616ms step_avg:45.82ms
step:1302/2160 train_time:59676ms step_avg:45.83ms
step:1303/2160 train_time:59738ms step_avg:45.85ms
step:1304/2160 train_time:59797ms step_avg:45.86ms
step:1305/2160 train_time:59859ms step_avg:45.87ms
step:1306/2160 train_time:59919ms step_avg:45.88ms
step:1307/2160 train_time:59980ms step_avg:45.89ms
step:1308/2160 train_time:60040ms step_avg:45.90ms
step:1309/2160 train_time:60101ms step_avg:45.91ms
step:1310/2160 train_time:60161ms step_avg:45.92ms
step:1311/2160 train_time:60222ms step_avg:45.94ms
step:1312/2160 train_time:60282ms step_avg:45.95ms
step:1313/2160 train_time:60343ms step_avg:45.96ms
step:1314/2160 train_time:60402ms step_avg:45.97ms
step:1315/2160 train_time:60464ms step_avg:45.98ms
step:1316/2160 train_time:60523ms step_avg:45.99ms
step:1317/2160 train_time:60585ms step_avg:46.00ms
step:1318/2160 train_time:60644ms step_avg:46.01ms
step:1319/2160 train_time:60705ms step_avg:46.02ms
step:1320/2160 train_time:60765ms step_avg:46.03ms
step:1321/2160 train_time:60825ms step_avg:46.04ms
step:1322/2160 train_time:60885ms step_avg:46.06ms
step:1323/2160 train_time:60946ms step_avg:46.07ms
step:1324/2160 train_time:61006ms step_avg:46.08ms
step:1325/2160 train_time:61066ms step_avg:46.09ms
step:1326/2160 train_time:61125ms step_avg:46.10ms
step:1327/2160 train_time:61186ms step_avg:46.11ms
step:1328/2160 train_time:61245ms step_avg:46.12ms
step:1329/2160 train_time:61306ms step_avg:46.13ms
step:1330/2160 train_time:61365ms step_avg:46.14ms
step:1331/2160 train_time:61426ms step_avg:46.15ms
step:1332/2160 train_time:61485ms step_avg:46.16ms
step:1333/2160 train_time:61547ms step_avg:46.17ms
step:1334/2160 train_time:61606ms step_avg:46.18ms
step:1335/2160 train_time:61667ms step_avg:46.19ms
step:1336/2160 train_time:61725ms step_avg:46.20ms
step:1337/2160 train_time:61786ms step_avg:46.21ms
step:1338/2160 train_time:61845ms step_avg:46.22ms
step:1339/2160 train_time:61906ms step_avg:46.23ms
step:1340/2160 train_time:61966ms step_avg:46.24ms
step:1341/2160 train_time:62027ms step_avg:46.25ms
step:1342/2160 train_time:62087ms step_avg:46.26ms
step:1343/2160 train_time:62148ms step_avg:46.28ms
step:1344/2160 train_time:62207ms step_avg:46.29ms
step:1345/2160 train_time:62268ms step_avg:46.30ms
step:1346/2160 train_time:62327ms step_avg:46.31ms
step:1347/2160 train_time:62387ms step_avg:46.32ms
step:1348/2160 train_time:62447ms step_avg:46.33ms
step:1349/2160 train_time:62508ms step_avg:46.34ms
step:1350/2160 train_time:62568ms step_avg:46.35ms
step:1351/2160 train_time:62629ms step_avg:46.36ms
step:1352/2160 train_time:62688ms step_avg:46.37ms
step:1353/2160 train_time:62749ms step_avg:46.38ms
step:1354/2160 train_time:62808ms step_avg:46.39ms
step:1355/2160 train_time:62869ms step_avg:46.40ms
step:1356/2160 train_time:62928ms step_avg:46.41ms
step:1357/2160 train_time:62988ms step_avg:46.42ms
step:1358/2160 train_time:63047ms step_avg:46.43ms
step:1359/2160 train_time:63108ms step_avg:46.44ms
step:1360/2160 train_time:63168ms step_avg:46.45ms
step:1361/2160 train_time:63229ms step_avg:46.46ms
step:1362/2160 train_time:63289ms step_avg:46.47ms
step:1363/2160 train_time:63349ms step_avg:46.48ms
step:1364/2160 train_time:63409ms step_avg:46.49ms
step:1365/2160 train_time:63470ms step_avg:46.50ms
step:1366/2160 train_time:63529ms step_avg:46.51ms
step:1367/2160 train_time:63590ms step_avg:46.52ms
step:1368/2160 train_time:63649ms step_avg:46.53ms
step:1369/2160 train_time:63710ms step_avg:46.54ms
step:1370/2160 train_time:63769ms step_avg:46.55ms
step:1371/2160 train_time:63829ms step_avg:46.56ms
step:1372/2160 train_time:63888ms step_avg:46.57ms
step:1373/2160 train_time:63949ms step_avg:46.58ms
step:1374/2160 train_time:64008ms step_avg:46.59ms
step:1375/2160 train_time:64069ms step_avg:46.60ms
step:1376/2160 train_time:64128ms step_avg:46.60ms
step:1377/2160 train_time:64189ms step_avg:46.61ms
step:1378/2160 train_time:64248ms step_avg:46.62ms
step:1379/2160 train_time:64309ms step_avg:46.63ms
step:1380/2160 train_time:64369ms step_avg:46.64ms
step:1381/2160 train_time:64429ms step_avg:46.65ms
step:1382/2160 train_time:64488ms step_avg:46.66ms
step:1383/2160 train_time:64549ms step_avg:46.67ms
step:1384/2160 train_time:64609ms step_avg:46.68ms
step:1385/2160 train_time:64669ms step_avg:46.69ms
step:1386/2160 train_time:64728ms step_avg:46.70ms
step:1387/2160 train_time:64789ms step_avg:46.71ms
step:1388/2160 train_time:64848ms step_avg:46.72ms
step:1389/2160 train_time:64909ms step_avg:46.73ms
step:1390/2160 train_time:64969ms step_avg:46.74ms
step:1391/2160 train_time:65029ms step_avg:46.75ms
step:1392/2160 train_time:65089ms step_avg:46.76ms
step:1393/2160 train_time:65149ms step_avg:46.77ms
step:1394/2160 train_time:65209ms step_avg:46.78ms
step:1395/2160 train_time:65270ms step_avg:46.79ms
step:1396/2160 train_time:65328ms step_avg:46.80ms
step:1397/2160 train_time:65389ms step_avg:46.81ms
step:1398/2160 train_time:65449ms step_avg:46.82ms
step:1399/2160 train_time:65509ms step_avg:46.83ms
step:1400/2160 train_time:65568ms step_avg:46.83ms
step:1401/2160 train_time:65629ms step_avg:46.84ms
step:1402/2160 train_time:65688ms step_avg:46.85ms
step:1403/2160 train_time:65749ms step_avg:46.86ms
step:1404/2160 train_time:65809ms step_avg:46.87ms
step:1405/2160 train_time:65870ms step_avg:46.88ms
step:1406/2160 train_time:65929ms step_avg:46.89ms
step:1407/2160 train_time:65989ms step_avg:46.90ms
step:1408/2160 train_time:66048ms step_avg:46.91ms
step:1409/2160 train_time:66109ms step_avg:46.92ms
step:1410/2160 train_time:66169ms step_avg:46.93ms
step:1411/2160 train_time:66229ms step_avg:46.94ms
step:1412/2160 train_time:66289ms step_avg:46.95ms
step:1413/2160 train_time:66349ms step_avg:46.96ms
step:1414/2160 train_time:66408ms step_avg:46.96ms
step:1415/2160 train_time:66469ms step_avg:46.97ms
step:1416/2160 train_time:66557ms step_avg:47.00ms
step:1417/2160 train_time:66645ms step_avg:47.03ms
step:1418/2160 train_time:66732ms step_avg:47.06ms
step:1419/2160 train_time:66821ms step_avg:47.09ms
step:1420/2160 train_time:66908ms step_avg:47.12ms
step:1421/2160 train_time:66996ms step_avg:47.15ms
step:1422/2160 train_time:67083ms step_avg:47.18ms
step:1423/2160 train_time:67172ms step_avg:47.20ms
step:1424/2160 train_time:67258ms step_avg:47.23ms
step:1425/2160 train_time:67347ms step_avg:47.26ms
step:1426/2160 train_time:67435ms step_avg:47.29ms
step:1427/2160 train_time:67523ms step_avg:47.32ms
step:1428/2160 train_time:67610ms step_avg:47.35ms
step:1429/2160 train_time:67698ms step_avg:47.37ms
step:1430/2160 train_time:67786ms step_avg:47.40ms
step:1431/2160 train_time:67876ms step_avg:47.43ms
step:1432/2160 train_time:67962ms step_avg:47.46ms
step:1433/2160 train_time:68052ms step_avg:47.49ms
step:1434/2160 train_time:68138ms step_avg:47.52ms
step:1435/2160 train_time:68227ms step_avg:47.55ms
step:1436/2160 train_time:68313ms step_avg:47.57ms
step:1437/2160 train_time:68401ms step_avg:47.60ms
step:1438/2160 train_time:68489ms step_avg:47.63ms
step:1439/2160 train_time:68577ms step_avg:47.66ms
step:1440/2160 train_time:68664ms step_avg:47.68ms
step:1441/2160 train_time:68753ms step_avg:47.71ms
step:1442/2160 train_time:68840ms step_avg:47.74ms
step:1443/2160 train_time:68929ms step_avg:47.77ms
step:1444/2160 train_time:69016ms step_avg:47.80ms
step:1445/2160 train_time:69105ms step_avg:47.82ms
step:1446/2160 train_time:69193ms step_avg:47.85ms
step:1447/2160 train_time:69282ms step_avg:47.88ms
step:1448/2160 train_time:69368ms step_avg:47.91ms
step:1449/2160 train_time:69456ms step_avg:47.93ms
step:1450/2160 train_time:69543ms step_avg:47.96ms
step:1451/2160 train_time:69633ms step_avg:47.99ms
step:1452/2160 train_time:69720ms step_avg:48.02ms
step:1453/2160 train_time:69809ms step_avg:48.04ms
step:1454/2160 train_time:69896ms step_avg:48.07ms
step:1455/2160 train_time:69985ms step_avg:48.10ms
step:1456/2160 train_time:70072ms step_avg:48.13ms
step:1457/2160 train_time:70161ms step_avg:48.15ms
step:1458/2160 train_time:70249ms step_avg:48.18ms
step:1459/2160 train_time:70337ms step_avg:48.21ms
step:1460/2160 train_time:70424ms step_avg:48.24ms
step:1461/2160 train_time:70513ms step_avg:48.26ms
step:1462/2160 train_time:70599ms step_avg:48.29ms
step:1463/2160 train_time:70688ms step_avg:48.32ms
step:1464/2160 train_time:70775ms step_avg:48.34ms
step:1465/2160 train_time:70864ms step_avg:48.37ms
step:1466/2160 train_time:70950ms step_avg:48.40ms
step:1467/2160 train_time:71039ms step_avg:48.42ms
step:1468/2160 train_time:71126ms step_avg:48.45ms
step:1469/2160 train_time:71215ms step_avg:48.48ms
step:1470/2160 train_time:71302ms step_avg:48.50ms
step:1471/2160 train_time:71391ms step_avg:48.53ms
step:1472/2160 train_time:71477ms step_avg:48.56ms
step:1473/2160 train_time:71565ms step_avg:48.58ms
step:1474/2160 train_time:71653ms step_avg:48.61ms
step:1475/2160 train_time:71741ms step_avg:48.64ms
step:1476/2160 train_time:71828ms step_avg:48.66ms
step:1477/2160 train_time:71917ms step_avg:48.69ms
step:1478/2160 train_time:72003ms step_avg:48.72ms
step:1479/2160 train_time:72094ms step_avg:48.74ms
step:1480/2160 train_time:72181ms step_avg:48.77ms
step:1481/2160 train_time:72270ms step_avg:48.80ms
step:1482/2160 train_time:72356ms step_avg:48.82ms
step:1483/2160 train_time:72446ms step_avg:48.85ms
step:1484/2160 train_time:72532ms step_avg:48.88ms
step:1485/2160 train_time:72621ms step_avg:48.90ms
step:1486/2160 train_time:72708ms step_avg:48.93ms
step:1487/2160 train_time:72798ms step_avg:48.96ms
step:1488/2160 train_time:72885ms step_avg:48.98ms
step:1489/2160 train_time:72974ms step_avg:49.01ms
step:1490/2160 train_time:73062ms step_avg:49.03ms
step:1491/2160 train_time:73151ms step_avg:49.06ms
step:1492/2160 train_time:73238ms step_avg:49.09ms
step:1493/2160 train_time:73326ms step_avg:49.11ms
step:1494/2160 train_time:73413ms step_avg:49.14ms
step:1495/2160 train_time:73502ms step_avg:49.16ms
step:1496/2160 train_time:73589ms step_avg:49.19ms
step:1497/2160 train_time:73678ms step_avg:49.22ms
step:1498/2160 train_time:73766ms step_avg:49.24ms
step:1499/2160 train_time:73854ms step_avg:49.27ms
step:1500/2160 train_time:73940ms step_avg:49.29ms
step:1500/2160 val_loss:3.4702 train_time:74030ms step_avg:49.35ms
step:1501/2160 train_time:74055ms step_avg:49.34ms
step:1502/2160 train_time:74124ms step_avg:49.35ms
step:1503/2160 train_time:74217ms step_avg:49.38ms
step:1504/2160 train_time:74305ms step_avg:49.41ms
step:1505/2160 train_time:74394ms step_avg:49.43ms
step:1506/2160 train_time:74480ms step_avg:49.46ms
step:1507/2160 train_time:74568ms step_avg:49.48ms
step:1508/2160 train_time:74654ms step_avg:49.51ms
step:1509/2160 train_time:74742ms step_avg:49.53ms
step:1510/2160 train_time:74827ms step_avg:49.55ms
step:1511/2160 train_time:74915ms step_avg:49.58ms
step:1512/2160 train_time:75006ms step_avg:49.61ms
step:1513/2160 train_time:75098ms step_avg:49.64ms
step:1514/2160 train_time:75186ms step_avg:49.66ms
step:1515/2160 train_time:75277ms step_avg:49.69ms
step:1516/2160 train_time:75363ms step_avg:49.71ms
step:1517/2160 train_time:75451ms step_avg:49.74ms
step:1518/2160 train_time:75537ms step_avg:49.76ms
step:1519/2160 train_time:75625ms step_avg:49.79ms
step:1520/2160 train_time:75710ms step_avg:49.81ms
step:1521/2160 train_time:75798ms step_avg:49.83ms
step:1522/2160 train_time:75885ms step_avg:49.86ms
step:1523/2160 train_time:75973ms step_avg:49.88ms
step:1524/2160 train_time:76063ms step_avg:49.91ms
step:1525/2160 train_time:76152ms step_avg:49.94ms
step:1526/2160 train_time:76240ms step_avg:49.96ms
step:1527/2160 train_time:76329ms step_avg:49.99ms
step:1528/2160 train_time:76416ms step_avg:50.01ms
step:1529/2160 train_time:76505ms step_avg:50.04ms
step:1530/2160 train_time:76592ms step_avg:50.06ms
step:1531/2160 train_time:76679ms step_avg:50.08ms
step:1532/2160 train_time:76765ms step_avg:50.11ms
step:1533/2160 train_time:76853ms step_avg:50.13ms
step:1534/2160 train_time:76940ms step_avg:50.16ms
step:1535/2160 train_time:77029ms step_avg:50.18ms
step:1536/2160 train_time:77117ms step_avg:50.21ms
step:1537/2160 train_time:77207ms step_avg:50.23ms
step:1538/2160 train_time:77295ms step_avg:50.26ms
step:1539/2160 train_time:77385ms step_avg:50.28ms
step:1540/2160 train_time:77472ms step_avg:50.31ms
step:1541/2160 train_time:77561ms step_avg:50.33ms
step:1542/2160 train_time:77647ms step_avg:50.35ms
step:1543/2160 train_time:77735ms step_avg:50.38ms
step:1544/2160 train_time:77822ms step_avg:50.40ms
step:1545/2160 train_time:77910ms step_avg:50.43ms
step:1546/2160 train_time:77997ms step_avg:50.45ms
step:1547/2160 train_time:78086ms step_avg:50.48ms
step:1548/2160 train_time:78174ms step_avg:50.50ms
step:1549/2160 train_time:78263ms step_avg:50.52ms
step:1550/2160 train_time:78350ms step_avg:50.55ms
step:1551/2160 train_time:78439ms step_avg:50.57ms
step:1552/2160 train_time:78525ms step_avg:50.60ms
step:1553/2160 train_time:78614ms step_avg:50.62ms
step:1554/2160 train_time:78702ms step_avg:50.64ms
step:1555/2160 train_time:78791ms step_avg:50.67ms
step:1556/2160 train_time:78877ms step_avg:50.69ms
step:1557/2160 train_time:78966ms step_avg:50.72ms
step:1558/2160 train_time:79053ms step_avg:50.74ms
step:1559/2160 train_time:79142ms step_avg:50.76ms
step:1560/2160 train_time:79229ms step_avg:50.79ms
step:1561/2160 train_time:79319ms step_avg:50.81ms
step:1562/2160 train_time:79406ms step_avg:50.84ms
step:1563/2160 train_time:79495ms step_avg:50.86ms
step:1564/2160 train_time:79582ms step_avg:50.88ms
step:1565/2160 train_time:79670ms step_avg:50.91ms
step:1566/2160 train_time:79757ms step_avg:50.93ms
step:1567/2160 train_time:79845ms step_avg:50.95ms
step:1568/2160 train_time:79932ms step_avg:50.98ms
step:1569/2160 train_time:80022ms step_avg:51.00ms
step:1570/2160 train_time:80109ms step_avg:51.02ms
step:1571/2160 train_time:80199ms step_avg:51.05ms
step:1572/2160 train_time:80286ms step_avg:51.07ms
step:1573/2160 train_time:80375ms step_avg:51.10ms
step:1574/2160 train_time:80462ms step_avg:51.12ms
step:1575/2160 train_time:80551ms step_avg:51.14ms
step:1576/2160 train_time:80638ms step_avg:51.17ms
step:1577/2160 train_time:80727ms step_avg:51.19ms
step:1578/2160 train_time:80813ms step_avg:51.21ms
step:1579/2160 train_time:80903ms step_avg:51.24ms
step:1580/2160 train_time:80990ms step_avg:51.26ms
step:1581/2160 train_time:81079ms step_avg:51.28ms
step:1582/2160 train_time:81167ms step_avg:51.31ms
step:1583/2160 train_time:81256ms step_avg:51.33ms
step:1584/2160 train_time:81343ms step_avg:51.35ms
step:1585/2160 train_time:81432ms step_avg:51.38ms
step:1586/2160 train_time:81520ms step_avg:51.40ms
step:1587/2160 train_time:81608ms step_avg:51.42ms
step:1588/2160 train_time:81695ms step_avg:51.45ms
step:1589/2160 train_time:81784ms step_avg:51.47ms
step:1590/2160 train_time:81871ms step_avg:51.49ms
step:1591/2160 train_time:81960ms step_avg:51.51ms
step:1592/2160 train_time:82046ms step_avg:51.54ms
step:1593/2160 train_time:82135ms step_avg:51.56ms
step:1594/2160 train_time:82223ms step_avg:51.58ms
step:1595/2160 train_time:82311ms step_avg:51.61ms
step:1596/2160 train_time:82399ms step_avg:51.63ms
step:1597/2160 train_time:82488ms step_avg:51.65ms
step:1598/2160 train_time:82575ms step_avg:51.67ms
step:1599/2160 train_time:82663ms step_avg:51.70ms
step:1600/2160 train_time:82750ms step_avg:51.72ms
step:1601/2160 train_time:82837ms step_avg:51.74ms
step:1602/2160 train_time:82925ms step_avg:51.76ms
step:1603/2160 train_time:83013ms step_avg:51.79ms
step:1604/2160 train_time:83100ms step_avg:51.81ms
step:1605/2160 train_time:83189ms step_avg:51.83ms
step:1606/2160 train_time:83275ms step_avg:51.85ms
step:1607/2160 train_time:83364ms step_avg:51.88ms
step:1608/2160 train_time:83451ms step_avg:51.90ms
step:1609/2160 train_time:83541ms step_avg:51.92ms
step:1610/2160 train_time:83628ms step_avg:51.94ms
step:1611/2160 train_time:83716ms step_avg:51.97ms
step:1612/2160 train_time:83803ms step_avg:51.99ms
step:1613/2160 train_time:83892ms step_avg:52.01ms
step:1614/2160 train_time:83979ms step_avg:52.03ms
step:1615/2160 train_time:84068ms step_avg:52.05ms
step:1616/2160 train_time:84155ms step_avg:52.08ms
step:1617/2160 train_time:84244ms step_avg:52.10ms
step:1618/2160 train_time:84331ms step_avg:52.12ms
step:1619/2160 train_time:84421ms step_avg:52.14ms
step:1620/2160 train_time:84507ms step_avg:52.16ms
step:1621/2160 train_time:84596ms step_avg:52.19ms
step:1622/2160 train_time:84683ms step_avg:52.21ms
step:1623/2160 train_time:84771ms step_avg:52.23ms
step:1624/2160 train_time:84858ms step_avg:52.25ms
step:1625/2160 train_time:84947ms step_avg:52.27ms
step:1626/2160 train_time:85034ms step_avg:52.30ms
step:1627/2160 train_time:85125ms step_avg:52.32ms
step:1628/2160 train_time:85212ms step_avg:52.34ms
step:1629/2160 train_time:85301ms step_avg:52.36ms
step:1630/2160 train_time:85388ms step_avg:52.39ms
step:1631/2160 train_time:85478ms step_avg:52.41ms
step:1632/2160 train_time:85564ms step_avg:52.43ms
step:1633/2160 train_time:85652ms step_avg:52.45ms
step:1634/2160 train_time:85740ms step_avg:52.47ms
step:1635/2160 train_time:85828ms step_avg:52.49ms
step:1636/2160 train_time:85915ms step_avg:52.52ms
step:1637/2160 train_time:86004ms step_avg:52.54ms
step:1638/2160 train_time:86091ms step_avg:52.56ms
step:1639/2160 train_time:86180ms step_avg:52.58ms
step:1640/2160 train_time:86267ms step_avg:52.60ms
step:1641/2160 train_time:86356ms step_avg:52.62ms
step:1642/2160 train_time:86444ms step_avg:52.65ms
step:1643/2160 train_time:86532ms step_avg:52.67ms
step:1644/2160 train_time:86619ms step_avg:52.69ms
step:1645/2160 train_time:86708ms step_avg:52.71ms
step:1646/2160 train_time:86795ms step_avg:52.73ms
step:1647/2160 train_time:86884ms step_avg:52.75ms
step:1648/2160 train_time:86971ms step_avg:52.77ms
step:1649/2160 train_time:87060ms step_avg:52.80ms
step:1650/2160 train_time:87147ms step_avg:52.82ms
step:1651/2160 train_time:87237ms step_avg:52.84ms
step:1652/2160 train_time:87323ms step_avg:52.86ms
step:1653/2160 train_time:87411ms step_avg:52.88ms
step:1654/2160 train_time:87499ms step_avg:52.90ms
step:1655/2160 train_time:87587ms step_avg:52.92ms
step:1656/2160 train_time:87674ms step_avg:52.94ms
step:1657/2160 train_time:87762ms step_avg:52.96ms
step:1658/2160 train_time:87849ms step_avg:52.98ms
step:1659/2160 train_time:87938ms step_avg:53.01ms
step:1660/2160 train_time:88025ms step_avg:53.03ms
step:1661/2160 train_time:88113ms step_avg:53.05ms
step:1662/2160 train_time:88200ms step_avg:53.07ms
step:1663/2160 train_time:88289ms step_avg:53.09ms
step:1664/2160 train_time:88376ms step_avg:53.11ms
step:1665/2160 train_time:88465ms step_avg:53.13ms
step:1666/2160 train_time:88552ms step_avg:53.15ms
step:1667/2160 train_time:88641ms step_avg:53.17ms
step:1668/2160 train_time:88727ms step_avg:53.19ms
step:1669/2160 train_time:88817ms step_avg:53.22ms
step:1670/2160 train_time:88905ms step_avg:53.24ms
step:1671/2160 train_time:88994ms step_avg:53.26ms
step:1672/2160 train_time:89081ms step_avg:53.28ms
step:1673/2160 train_time:89170ms step_avg:53.30ms
step:1674/2160 train_time:89257ms step_avg:53.32ms
step:1675/2160 train_time:89347ms step_avg:53.34ms
step:1676/2160 train_time:89434ms step_avg:53.36ms
step:1677/2160 train_time:89523ms step_avg:53.38ms
step:1678/2160 train_time:89609ms step_avg:53.40ms
step:1679/2160 train_time:89698ms step_avg:53.42ms
step:1680/2160 train_time:89785ms step_avg:53.44ms
step:1681/2160 train_time:89874ms step_avg:53.46ms
step:1682/2160 train_time:89961ms step_avg:53.48ms
step:1683/2160 train_time:90050ms step_avg:53.51ms
step:1684/2160 train_time:90138ms step_avg:53.53ms
step:1685/2160 train_time:90226ms step_avg:53.55ms
step:1686/2160 train_time:90313ms step_avg:53.57ms
step:1687/2160 train_time:90402ms step_avg:53.59ms
step:1688/2160 train_time:90489ms step_avg:53.61ms
step:1689/2160 train_time:90577ms step_avg:53.63ms
step:1690/2160 train_time:90664ms step_avg:53.65ms
step:1691/2160 train_time:90753ms step_avg:53.67ms
step:1692/2160 train_time:90841ms step_avg:53.69ms
step:1693/2160 train_time:90930ms step_avg:53.71ms
step:1694/2160 train_time:91017ms step_avg:53.73ms
step:1695/2160 train_time:91107ms step_avg:53.75ms
step:1696/2160 train_time:91195ms step_avg:53.77ms
step:1697/2160 train_time:91285ms step_avg:53.79ms
step:1698/2160 train_time:91372ms step_avg:53.81ms
step:1699/2160 train_time:91460ms step_avg:53.83ms
step:1700/2160 train_time:91547ms step_avg:53.85ms
step:1701/2160 train_time:91636ms step_avg:53.87ms
step:1702/2160 train_time:91723ms step_avg:53.89ms
step:1703/2160 train_time:91813ms step_avg:53.91ms
step:1704/2160 train_time:91900ms step_avg:53.93ms
step:1705/2160 train_time:91989ms step_avg:53.95ms
step:1706/2160 train_time:92076ms step_avg:53.97ms
step:1707/2160 train_time:92165ms step_avg:53.99ms
step:1708/2160 train_time:92252ms step_avg:54.01ms
step:1709/2160 train_time:92342ms step_avg:54.03ms
step:1710/2160 train_time:92428ms step_avg:54.05ms
step:1711/2160 train_time:92518ms step_avg:54.07ms
step:1712/2160 train_time:92605ms step_avg:54.09ms
step:1713/2160 train_time:92693ms step_avg:54.11ms
step:1714/2160 train_time:92781ms step_avg:54.13ms
step:1715/2160 train_time:92869ms step_avg:54.15ms
step:1716/2160 train_time:92956ms step_avg:54.17ms
step:1717/2160 train_time:93045ms step_avg:54.19ms
step:1718/2160 train_time:93133ms step_avg:54.21ms
step:1719/2160 train_time:93222ms step_avg:54.23ms
step:1720/2160 train_time:93309ms step_avg:54.25ms
step:1721/2160 train_time:93398ms step_avg:54.27ms
step:1722/2160 train_time:93486ms step_avg:54.29ms
step:1723/2160 train_time:93574ms step_avg:54.31ms
step:1724/2160 train_time:93661ms step_avg:54.33ms
step:1725/2160 train_time:93749ms step_avg:54.35ms
step:1726/2160 train_time:93836ms step_avg:54.37ms
step:1727/2160 train_time:93926ms step_avg:54.39ms
step:1728/2160 train_time:94013ms step_avg:54.41ms
step:1729/2160 train_time:94102ms step_avg:54.43ms
step:1730/2160 train_time:94188ms step_avg:54.44ms
step:1731/2160 train_time:94277ms step_avg:54.46ms
step:1732/2160 train_time:94364ms step_avg:54.48ms
step:1733/2160 train_time:94452ms step_avg:54.50ms
step:1734/2160 train_time:94539ms step_avg:54.52ms
step:1735/2160 train_time:94628ms step_avg:54.54ms
step:1736/2160 train_time:94715ms step_avg:54.56ms
step:1737/2160 train_time:94805ms step_avg:54.58ms
step:1738/2160 train_time:94892ms step_avg:54.60ms
step:1739/2160 train_time:94982ms step_avg:54.62ms
step:1740/2160 train_time:95068ms step_avg:54.64ms
step:1741/2160 train_time:95158ms step_avg:54.66ms
step:1742/2160 train_time:95245ms step_avg:54.68ms
step:1743/2160 train_time:95334ms step_avg:54.70ms
step:1744/2160 train_time:95422ms step_avg:54.71ms
step:1745/2160 train_time:95510ms step_avg:54.73ms
step:1746/2160 train_time:95597ms step_avg:54.75ms
step:1747/2160 train_time:95685ms step_avg:54.77ms
step:1748/2160 train_time:95773ms step_avg:54.79ms
step:1749/2160 train_time:95862ms step_avg:54.81ms
step:1750/2160 train_time:95948ms step_avg:54.83ms
step:1750/2160 val_loss:3.3792 train_time:96037ms step_avg:54.88ms
step:1751/2160 train_time:96065ms step_avg:54.86ms
step:1752/2160 train_time:96128ms step_avg:54.87ms
step:1753/2160 train_time:96223ms step_avg:54.89ms
step:1754/2160 train_time:96311ms step_avg:54.91ms
step:1755/2160 train_time:96400ms step_avg:54.93ms
step:1756/2160 train_time:96487ms step_avg:54.95ms
step:1757/2160 train_time:96574ms step_avg:54.97ms
step:1758/2160 train_time:96661ms step_avg:54.98ms
step:1759/2160 train_time:96748ms step_avg:55.00ms
step:1760/2160 train_time:96835ms step_avg:55.02ms
step:1761/2160 train_time:96923ms step_avg:55.04ms
step:1762/2160 train_time:97010ms step_avg:55.06ms
step:1763/2160 train_time:97101ms step_avg:55.08ms
step:1764/2160 train_time:97190ms step_avg:55.10ms
step:1765/2160 train_time:97280ms step_avg:55.12ms
step:1766/2160 train_time:97367ms step_avg:55.13ms
step:1767/2160 train_time:97456ms step_avg:55.15ms
step:1768/2160 train_time:97543ms step_avg:55.17ms
step:1769/2160 train_time:97630ms step_avg:55.19ms
step:1770/2160 train_time:97716ms step_avg:55.21ms
step:1771/2160 train_time:97804ms step_avg:55.23ms
step:1772/2160 train_time:97890ms step_avg:55.24ms
step:1773/2160 train_time:97980ms step_avg:55.26ms
step:1774/2160 train_time:98067ms step_avg:55.28ms
step:1775/2160 train_time:98158ms step_avg:55.30ms
step:1776/2160 train_time:98246ms step_avg:55.32ms
step:1777/2160 train_time:98336ms step_avg:55.34ms
step:1778/2160 train_time:98422ms step_avg:55.36ms
step:1779/2160 train_time:98511ms step_avg:55.37ms
step:1780/2160 train_time:98598ms step_avg:55.39ms
step:1781/2160 train_time:98685ms step_avg:55.41ms
step:1782/2160 train_time:98771ms step_avg:55.43ms
step:1783/2160 train_time:98860ms step_avg:55.45ms
step:1784/2160 train_time:98946ms step_avg:55.46ms
step:1785/2160 train_time:99035ms step_avg:55.48ms
step:1786/2160 train_time:99123ms step_avg:55.50ms
step:1787/2160 train_time:99214ms step_avg:55.52ms
step:1788/2160 train_time:99302ms step_avg:55.54ms
step:1789/2160 train_time:99392ms step_avg:55.56ms
step:1790/2160 train_time:99479ms step_avg:55.57ms
step:1791/2160 train_time:99567ms step_avg:55.59ms
step:1792/2160 train_time:99654ms step_avg:55.61ms
step:1793/2160 train_time:99742ms step_avg:55.63ms
step:1794/2160 train_time:99829ms step_avg:55.65ms
step:1795/2160 train_time:99919ms step_avg:55.67ms
step:1796/2160 train_time:100005ms step_avg:55.68ms
step:1797/2160 train_time:100094ms step_avg:55.70ms
step:1798/2160 train_time:100182ms step_avg:55.72ms
step:1799/2160 train_time:100272ms step_avg:55.74ms
step:1800/2160 train_time:100359ms step_avg:55.76ms
step:1801/2160 train_time:100449ms step_avg:55.77ms
step:1802/2160 train_time:100536ms step_avg:55.79ms
step:1803/2160 train_time:100624ms step_avg:55.81ms
step:1804/2160 train_time:100710ms step_avg:55.83ms
step:1805/2160 train_time:100798ms step_avg:55.84ms
step:1806/2160 train_time:100886ms step_avg:55.86ms
step:1807/2160 train_time:100974ms step_avg:55.88ms
step:1808/2160 train_time:101061ms step_avg:55.90ms
step:1809/2160 train_time:101150ms step_avg:55.91ms
step:1810/2160 train_time:101238ms step_avg:55.93ms
step:1811/2160 train_time:101327ms step_avg:55.95ms
step:1812/2160 train_time:101415ms step_avg:55.97ms
step:1813/2160 train_time:101503ms step_avg:55.99ms
step:1814/2160 train_time:101590ms step_avg:56.00ms
step:1815/2160 train_time:101679ms step_avg:56.02ms
step:1816/2160 train_time:101765ms step_avg:56.04ms
step:1817/2160 train_time:101854ms step_avg:56.06ms
step:1818/2160 train_time:101941ms step_avg:56.07ms
step:1819/2160 train_time:102029ms step_avg:56.09ms
step:1820/2160 train_time:102117ms step_avg:56.11ms
step:1821/2160 train_time:102206ms step_avg:56.13ms
step:1822/2160 train_time:102294ms step_avg:56.14ms
step:1823/2160 train_time:102382ms step_avg:56.16ms
step:1824/2160 train_time:102470ms step_avg:56.18ms
step:1825/2160 train_time:102560ms step_avg:56.20ms
step:1826/2160 train_time:102646ms step_avg:56.21ms
step:1827/2160 train_time:102735ms step_avg:56.23ms
step:1828/2160 train_time:102822ms step_avg:56.25ms
step:1829/2160 train_time:102912ms step_avg:56.27ms
step:1830/2160 train_time:102998ms step_avg:56.28ms
step:1831/2160 train_time:103087ms step_avg:56.30ms
step:1832/2160 train_time:103175ms step_avg:56.32ms
step:1833/2160 train_time:103265ms step_avg:56.34ms
step:1834/2160 train_time:103351ms step_avg:56.35ms
step:1835/2160 train_time:103441ms step_avg:56.37ms
step:1836/2160 train_time:103528ms step_avg:56.39ms
step:1837/2160 train_time:103617ms step_avg:56.41ms
step:1838/2160 train_time:103704ms step_avg:56.42ms
step:1839/2160 train_time:103793ms step_avg:56.44ms
step:1840/2160 train_time:103880ms step_avg:56.46ms
step:1841/2160 train_time:103969ms step_avg:56.47ms
step:1842/2160 train_time:104056ms step_avg:56.49ms
step:1843/2160 train_time:104144ms step_avg:56.51ms
step:1844/2160 train_time:104232ms step_avg:56.52ms
step:1845/2160 train_time:104321ms step_avg:56.54ms
step:1846/2160 train_time:104409ms step_avg:56.56ms
step:1847/2160 train_time:104498ms step_avg:56.58ms
step:1848/2160 train_time:104584ms step_avg:56.59ms
step:1849/2160 train_time:104673ms step_avg:56.61ms
step:1850/2160 train_time:104760ms step_avg:56.63ms
step:1851/2160 train_time:104849ms step_avg:56.64ms
step:1852/2160 train_time:104936ms step_avg:56.66ms
step:1853/2160 train_time:105025ms step_avg:56.68ms
step:1854/2160 train_time:105112ms step_avg:56.69ms
step:1855/2160 train_time:105201ms step_avg:56.71ms
step:1856/2160 train_time:105289ms step_avg:56.73ms
step:1857/2160 train_time:105380ms step_avg:56.75ms
step:1858/2160 train_time:105467ms step_avg:56.76ms
step:1859/2160 train_time:105557ms step_avg:56.78ms
step:1860/2160 train_time:105643ms step_avg:56.80ms
step:1861/2160 train_time:105732ms step_avg:56.81ms
step:1862/2160 train_time:105819ms step_avg:56.83ms
step:1863/2160 train_time:105907ms step_avg:56.85ms
step:1864/2160 train_time:105994ms step_avg:56.86ms
step:1865/2160 train_time:106083ms step_avg:56.88ms
step:1866/2160 train_time:106169ms step_avg:56.90ms
step:1867/2160 train_time:106259ms step_avg:56.91ms
step:1868/2160 train_time:106347ms step_avg:56.93ms
step:1869/2160 train_time:106435ms step_avg:56.95ms
step:1870/2160 train_time:106522ms step_avg:56.96ms
step:1871/2160 train_time:106611ms step_avg:56.98ms
step:1872/2160 train_time:106698ms step_avg:57.00ms
step:1873/2160 train_time:106786ms step_avg:57.01ms
step:1874/2160 train_time:106874ms step_avg:57.03ms
step:1875/2160 train_time:106963ms step_avg:57.05ms
step:1876/2160 train_time:107050ms step_avg:57.06ms
step:1877/2160 train_time:107139ms step_avg:57.08ms
step:1878/2160 train_time:107226ms step_avg:57.10ms
step:1879/2160 train_time:107315ms step_avg:57.11ms
step:1880/2160 train_time:107402ms step_avg:57.13ms
step:1881/2160 train_time:107490ms step_avg:57.15ms
step:1882/2160 train_time:107578ms step_avg:57.16ms
step:1883/2160 train_time:107667ms step_avg:57.18ms
step:1884/2160 train_time:107754ms step_avg:57.19ms
step:1885/2160 train_time:107842ms step_avg:57.21ms
step:1886/2160 train_time:107929ms step_avg:57.23ms
step:1887/2160 train_time:108019ms step_avg:57.24ms
step:1888/2160 train_time:108105ms step_avg:57.26ms
step:1889/2160 train_time:108195ms step_avg:57.28ms
step:1890/2160 train_time:108282ms step_avg:57.29ms
step:1891/2160 train_time:108371ms step_avg:57.31ms
step:1892/2160 train_time:108458ms step_avg:57.32ms
step:1893/2160 train_time:108547ms step_avg:57.34ms
step:1894/2160 train_time:108635ms step_avg:57.36ms
step:1895/2160 train_time:108723ms step_avg:57.37ms
step:1896/2160 train_time:108810ms step_avg:57.39ms
step:1897/2160 train_time:108899ms step_avg:57.41ms
step:1898/2160 train_time:108986ms step_avg:57.42ms
step:1899/2160 train_time:109076ms step_avg:57.44ms
step:1900/2160 train_time:109162ms step_avg:57.45ms
step:1901/2160 train_time:109251ms step_avg:57.47ms
step:1902/2160 train_time:109339ms step_avg:57.49ms
step:1903/2160 train_time:109428ms step_avg:57.50ms
step:1904/2160 train_time:109515ms step_avg:57.52ms
step:1905/2160 train_time:109604ms step_avg:57.53ms
step:1906/2160 train_time:109691ms step_avg:57.55ms
step:1907/2160 train_time:109780ms step_avg:57.57ms
step:1908/2160 train_time:109867ms step_avg:57.58ms
step:1909/2160 train_time:109956ms step_avg:57.60ms
step:1910/2160 train_time:110043ms step_avg:57.61ms
step:1911/2160 train_time:110133ms step_avg:57.63ms
step:1912/2160 train_time:110219ms step_avg:57.65ms
step:1913/2160 train_time:110308ms step_avg:57.66ms
step:1914/2160 train_time:110396ms step_avg:57.68ms
step:1915/2160 train_time:110484ms step_avg:57.69ms
step:1916/2160 train_time:110573ms step_avg:57.71ms
step:1917/2160 train_time:110661ms step_avg:57.73ms
step:1918/2160 train_time:110749ms step_avg:57.74ms
step:1919/2160 train_time:110837ms step_avg:57.76ms
step:1920/2160 train_time:110924ms step_avg:57.77ms
step:1921/2160 train_time:111014ms step_avg:57.79ms
step:1922/2160 train_time:111100ms step_avg:57.80ms
step:1923/2160 train_time:111189ms step_avg:57.82ms
step:1924/2160 train_time:111277ms step_avg:57.84ms
step:1925/2160 train_time:111365ms step_avg:57.85ms
step:1926/2160 train_time:111452ms step_avg:57.87ms
step:1927/2160 train_time:111541ms step_avg:57.88ms
step:1928/2160 train_time:111629ms step_avg:57.90ms
step:1929/2160 train_time:111718ms step_avg:57.91ms
step:1930/2160 train_time:111804ms step_avg:57.93ms
step:1931/2160 train_time:111893ms step_avg:57.95ms
step:1932/2160 train_time:111980ms step_avg:57.96ms
step:1933/2160 train_time:112069ms step_avg:57.98ms
step:1934/2160 train_time:112157ms step_avg:57.99ms
step:1935/2160 train_time:112245ms step_avg:58.01ms
step:1936/2160 train_time:112334ms step_avg:58.02ms
step:1937/2160 train_time:112422ms step_avg:58.04ms
step:1938/2160 train_time:112510ms step_avg:58.05ms
step:1939/2160 train_time:112599ms step_avg:58.07ms
step:1940/2160 train_time:112686ms step_avg:58.09ms
step:1941/2160 train_time:112775ms step_avg:58.10ms
step:1942/2160 train_time:112861ms step_avg:58.12ms
step:1943/2160 train_time:112950ms step_avg:58.13ms
step:1944/2160 train_time:113037ms step_avg:58.15ms
step:1945/2160 train_time:113126ms step_avg:58.16ms
step:1946/2160 train_time:113214ms step_avg:58.18ms
step:1947/2160 train_time:113303ms step_avg:58.19ms
step:1948/2160 train_time:113389ms step_avg:58.21ms
step:1949/2160 train_time:113479ms step_avg:58.22ms
step:1950/2160 train_time:113566ms step_avg:58.24ms
step:1951/2160 train_time:113656ms step_avg:58.26ms
step:1952/2160 train_time:113743ms step_avg:58.27ms
step:1953/2160 train_time:113831ms step_avg:58.29ms
step:1954/2160 train_time:113918ms step_avg:58.30ms
step:1955/2160 train_time:114007ms step_avg:58.32ms
step:1956/2160 train_time:114094ms step_avg:58.33ms
step:1957/2160 train_time:114182ms step_avg:58.35ms
step:1958/2160 train_time:114269ms step_avg:58.36ms
step:1959/2160 train_time:114359ms step_avg:58.38ms
step:1960/2160 train_time:114446ms step_avg:58.39ms
step:1961/2160 train_time:114535ms step_avg:58.41ms
step:1962/2160 train_time:114622ms step_avg:58.42ms
step:1963/2160 train_time:114711ms step_avg:58.44ms
step:1964/2160 train_time:114798ms step_avg:58.45ms
step:1965/2160 train_time:114887ms step_avg:58.47ms
step:1966/2160 train_time:114974ms step_avg:58.48ms
step:1967/2160 train_time:115062ms step_avg:58.50ms
step:1968/2160 train_time:115149ms step_avg:58.51ms
step:1969/2160 train_time:115239ms step_avg:58.53ms
step:1970/2160 train_time:115326ms step_avg:58.54ms
step:1971/2160 train_time:115416ms step_avg:58.56ms
step:1972/2160 train_time:115503ms step_avg:58.57ms
step:1973/2160 train_time:115592ms step_avg:58.59ms
step:1974/2160 train_time:115680ms step_avg:58.60ms
step:1975/2160 train_time:115769ms step_avg:58.62ms
step:1976/2160 train_time:115856ms step_avg:58.63ms
step:1977/2160 train_time:115944ms step_avg:58.65ms
step:1978/2160 train_time:116030ms step_avg:58.66ms
step:1979/2160 train_time:116119ms step_avg:58.68ms
step:1980/2160 train_time:116206ms step_avg:58.69ms
step:1981/2160 train_time:116295ms step_avg:58.71ms
step:1982/2160 train_time:116383ms step_avg:58.72ms
step:1983/2160 train_time:116471ms step_avg:58.73ms
step:1984/2160 train_time:116559ms step_avg:58.75ms
step:1985/2160 train_time:116649ms step_avg:58.76ms
step:1986/2160 train_time:116737ms step_avg:58.78ms
step:1987/2160 train_time:116826ms step_avg:58.80ms
step:1988/2160 train_time:116914ms step_avg:58.81ms
step:1989/2160 train_time:117003ms step_avg:58.82ms
step:1990/2160 train_time:117089ms step_avg:58.84ms
step:1991/2160 train_time:117179ms step_avg:58.85ms
step:1992/2160 train_time:117266ms step_avg:58.87ms
step:1993/2160 train_time:117356ms step_avg:58.88ms
step:1994/2160 train_time:117443ms step_avg:58.90ms
step:1995/2160 train_time:117532ms step_avg:58.91ms
step:1996/2160 train_time:117619ms step_avg:58.93ms
step:1997/2160 train_time:117708ms step_avg:58.94ms
step:1998/2160 train_time:117795ms step_avg:58.96ms
step:1999/2160 train_time:117884ms step_avg:58.97ms
step:2000/2160 train_time:117972ms step_avg:58.99ms
step:2000/2160 val_loss:3.3101 train_time:118061ms step_avg:59.03ms
step:2001/2160 train_time:118085ms step_avg:59.01ms
step:2002/2160 train_time:118149ms step_avg:59.02ms
step:2003/2160 train_time:118245ms step_avg:59.03ms
step:2004/2160 train_time:118336ms step_avg:59.05ms
step:2005/2160 train_time:118425ms step_avg:59.06ms
step:2006/2160 train_time:118511ms step_avg:59.08ms
step:2007/2160 train_time:118599ms step_avg:59.09ms
step:2008/2160 train_time:118685ms step_avg:59.11ms
step:2009/2160 train_time:118772ms step_avg:59.12ms
step:2010/2160 train_time:118858ms step_avg:59.13ms
step:2011/2160 train_time:118946ms step_avg:59.15ms
step:2012/2160 train_time:119035ms step_avg:59.16ms
step:2013/2160 train_time:119125ms step_avg:59.18ms
step:2014/2160 train_time:119214ms step_avg:59.19ms
step:2015/2160 train_time:119305ms step_avg:59.21ms
step:2016/2160 train_time:119393ms step_avg:59.22ms
step:2017/2160 train_time:119482ms step_avg:59.24ms
step:2018/2160 train_time:119568ms step_avg:59.25ms
step:2019/2160 train_time:119656ms step_avg:59.27ms
step:2020/2160 train_time:119742ms step_avg:59.28ms
step:2021/2160 train_time:119831ms step_avg:59.29ms
step:2022/2160 train_time:119917ms step_avg:59.31ms
step:2023/2160 train_time:120006ms step_avg:59.32ms
step:2024/2160 train_time:120094ms step_avg:59.34ms
step:2025/2160 train_time:120184ms step_avg:59.35ms
step:2026/2160 train_time:120273ms step_avg:59.36ms
step:2027/2160 train_time:120363ms step_avg:59.38ms
step:2028/2160 train_time:120451ms step_avg:59.39ms
step:2029/2160 train_time:120540ms step_avg:59.41ms
step:2030/2160 train_time:120626ms step_avg:59.42ms
step:2031/2160 train_time:120714ms step_avg:59.44ms
step:2032/2160 train_time:120800ms step_avg:59.45ms
step:2033/2160 train_time:120888ms step_avg:59.46ms
step:2034/2160 train_time:120975ms step_avg:59.48ms
step:2035/2160 train_time:121064ms step_avg:59.49ms
step:2036/2160 train_time:121153ms step_avg:59.51ms
step:2037/2160 train_time:121243ms step_avg:59.52ms
step:2038/2160 train_time:121331ms step_avg:59.53ms
step:2039/2160 train_time:121421ms step_avg:59.55ms
step:2040/2160 train_time:121509ms step_avg:59.56ms
step:2041/2160 train_time:121597ms step_avg:59.58ms
step:2042/2160 train_time:121684ms step_avg:59.59ms
step:2043/2160 train_time:121772ms step_avg:59.60ms
step:2044/2160 train_time:121859ms step_avg:59.62ms
step:2045/2160 train_time:121947ms step_avg:59.63ms
step:2046/2160 train_time:122034ms step_avg:59.65ms
step:2047/2160 train_time:122123ms step_avg:59.66ms
step:2048/2160 train_time:122212ms step_avg:59.67ms
step:2049/2160 train_time:122302ms step_avg:59.69ms
step:2050/2160 train_time:122389ms step_avg:59.70ms
step:2051/2160 train_time:122478ms step_avg:59.72ms
step:2052/2160 train_time:122566ms step_avg:59.73ms
step:2053/2160 train_time:122654ms step_avg:59.74ms
step:2054/2160 train_time:122740ms step_avg:59.76ms
step:2055/2160 train_time:122830ms step_avg:59.77ms
step:2056/2160 train_time:122916ms step_avg:59.78ms
step:2057/2160 train_time:123005ms step_avg:59.80ms
step:2058/2160 train_time:123092ms step_avg:59.81ms
step:2059/2160 train_time:123182ms step_avg:59.83ms
step:2060/2160 train_time:123270ms step_avg:59.84ms
step:2061/2160 train_time:123359ms step_avg:59.85ms
step:2062/2160 train_time:123447ms step_avg:59.87ms
step:2063/2160 train_time:123536ms step_avg:59.88ms
step:2064/2160 train_time:123623ms step_avg:59.89ms
step:2065/2160 train_time:123712ms step_avg:59.91ms
step:2066/2160 train_time:123799ms step_avg:59.92ms
step:2067/2160 train_time:123888ms step_avg:59.94ms
step:2068/2160 train_time:123975ms step_avg:59.95ms
step:2069/2160 train_time:124064ms step_avg:59.96ms
step:2070/2160 train_time:124152ms step_avg:59.98ms
step:2071/2160 train_time:124240ms step_avg:59.99ms
step:2072/2160 train_time:124329ms step_avg:60.00ms
step:2073/2160 train_time:124417ms step_avg:60.02ms
step:2074/2160 train_time:124505ms step_avg:60.03ms
step:2075/2160 train_time:124593ms step_avg:60.04ms
step:2076/2160 train_time:124681ms step_avg:60.06ms
step:2077/2160 train_time:124770ms step_avg:60.07ms
step:2078/2160 train_time:124857ms step_avg:60.09ms
step:2079/2160 train_time:124945ms step_avg:60.10ms
step:2080/2160 train_time:125033ms step_avg:60.11ms
step:2081/2160 train_time:125121ms step_avg:60.13ms
step:2082/2160 train_time:125208ms step_avg:60.14ms
step:2083/2160 train_time:125297ms step_avg:60.15ms
step:2084/2160 train_time:125384ms step_avg:60.17ms
step:2085/2160 train_time:125474ms step_avg:60.18ms
step:2086/2160 train_time:125561ms step_avg:60.19ms
step:2087/2160 train_time:125649ms step_avg:60.21ms
step:2088/2160 train_time:125737ms step_avg:60.22ms
step:2089/2160 train_time:125826ms step_avg:60.23ms
step:2090/2160 train_time:125913ms step_avg:60.25ms
step:2091/2160 train_time:126001ms step_avg:60.26ms
step:2092/2160 train_time:126088ms step_avg:60.27ms
step:2093/2160 train_time:126177ms step_avg:60.29ms
step:2094/2160 train_time:126263ms step_avg:60.30ms
step:2095/2160 train_time:126352ms step_avg:60.31ms
step:2096/2160 train_time:126439ms step_avg:60.32ms
step:2097/2160 train_time:126529ms step_avg:60.34ms
step:2098/2160 train_time:126616ms step_avg:60.35ms
step:2099/2160 train_time:126705ms step_avg:60.36ms
step:2100/2160 train_time:126792ms step_avg:60.38ms
step:2101/2160 train_time:126881ms step_avg:60.39ms
step:2102/2160 train_time:126968ms step_avg:60.40ms
step:2103/2160 train_time:127058ms step_avg:60.42ms
step:2104/2160 train_time:127145ms step_avg:60.43ms
step:2105/2160 train_time:127234ms step_avg:60.44ms
step:2106/2160 train_time:127321ms step_avg:60.46ms
step:2107/2160 train_time:127411ms step_avg:60.47ms
step:2108/2160 train_time:127498ms step_avg:60.48ms
step:2109/2160 train_time:127587ms step_avg:60.50ms
step:2110/2160 train_time:127674ms step_avg:60.51ms
step:2111/2160 train_time:127763ms step_avg:60.52ms
step:2112/2160 train_time:127850ms step_avg:60.53ms
step:2113/2160 train_time:127939ms step_avg:60.55ms
step:2114/2160 train_time:128026ms step_avg:60.56ms
step:2115/2160 train_time:128114ms step_avg:60.57ms
step:2116/2160 train_time:128202ms step_avg:60.59ms
step:2117/2160 train_time:128292ms step_avg:60.60ms
step:2118/2160 train_time:128379ms step_avg:60.61ms
step:2119/2160 train_time:128468ms step_avg:60.63ms
step:2120/2160 train_time:128555ms step_avg:60.64ms
step:2121/2160 train_time:128644ms step_avg:60.65ms
step:2122/2160 train_time:128731ms step_avg:60.67ms
step:2123/2160 train_time:128820ms step_avg:60.68ms
step:2124/2160 train_time:128908ms step_avg:60.69ms
step:2125/2160 train_time:128997ms step_avg:60.70ms
step:2126/2160 train_time:129085ms step_avg:60.72ms
step:2127/2160 train_time:129174ms step_avg:60.73ms
step:2128/2160 train_time:129262ms step_avg:60.74ms
step:2129/2160 train_time:129352ms step_avg:60.76ms
step:2130/2160 train_time:129439ms step_avg:60.77ms
step:2131/2160 train_time:129528ms step_avg:60.78ms
step:2132/2160 train_time:129615ms step_avg:60.80ms
step:2133/2160 train_time:129705ms step_avg:60.81ms
step:2134/2160 train_time:129792ms step_avg:60.82ms
step:2135/2160 train_time:129881ms step_avg:60.83ms
step:2136/2160 train_time:129969ms step_avg:60.85ms
step:2137/2160 train_time:130057ms step_avg:60.86ms
step:2138/2160 train_time:130145ms step_avg:60.87ms
step:2139/2160 train_time:130235ms step_avg:60.89ms
step:2140/2160 train_time:130322ms step_avg:60.90ms
step:2141/2160 train_time:130413ms step_avg:60.91ms
step:2142/2160 train_time:130500ms step_avg:60.92ms
step:2143/2160 train_time:130589ms step_avg:60.94ms
step:2144/2160 train_time:130676ms step_avg:60.95ms
step:2145/2160 train_time:130766ms step_avg:60.96ms
step:2146/2160 train_time:130853ms step_avg:60.98ms
step:2147/2160 train_time:130943ms step_avg:60.99ms
step:2148/2160 train_time:131030ms step_avg:61.00ms
step:2149/2160 train_time:131119ms step_avg:61.01ms
step:2150/2160 train_time:131207ms step_avg:61.03ms
step:2151/2160 train_time:131296ms step_avg:61.04ms
step:2152/2160 train_time:131384ms step_avg:61.05ms
step:2153/2160 train_time:131473ms step_avg:61.06ms
step:2154/2160 train_time:131560ms step_avg:61.08ms
step:2155/2160 train_time:131649ms step_avg:61.09ms
step:2156/2160 train_time:131736ms step_avg:61.10ms
step:2157/2160 train_time:131825ms step_avg:61.12ms
step:2158/2160 train_time:131912ms step_avg:61.13ms
step:2159/2160 train_time:132002ms step_avg:61.14ms
step:2160/2160 train_time:132090ms step_avg:61.15ms
step:2160/2160 val_loss:3.2784 train_time:132179ms step_avg:61.19ms
peak memory allocated: 29707 MiB reserved: 44536 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 and layer_idx !=0 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        first_bm = 1 * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, final_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"v1/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer shows no degradation with context length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
smear_gate_params = [p for n, p in model.named_parameters() if "smear" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params+smear_gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate, args.ws_validate_final_layer
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250724+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sun Sep 21 22:33:55 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   43C    P0            126W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   31C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   38C    P0            125W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   39C    P0            125W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   38C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           62023      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A           62024      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           62025      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           62026      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           62027      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           62028      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           62029      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           62030      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           62024      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A           62025      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A           62026      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A           62027      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A           62028      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A           62029      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A           62030      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/1680 train_time:159ms step_avg:158.58ms
step:2/1680 train_time:182ms step_avg:90.96ms
step:3/1680 train_time:244ms step_avg:81.43ms
step:4/1680 train_time:331ms step_avg:82.63ms
step:5/1680 train_time:418ms step_avg:83.66ms
step:6/1680 train_time:506ms step_avg:84.41ms
step:7/1680 train_time:595ms step_avg:84.95ms
step:8/1680 train_time:683ms step_avg:85.35ms
step:9/1680 train_time:771ms step_avg:85.65ms
step:10/1680 train_time:859ms step_avg:85.93ms
step:11/1680 train_time:948ms step_avg:86.15ms
step:12/1680 train_time:1038ms step_avg:86.52ms
step:13/1680 train_time:1132ms step_avg:87.04ms
step:14/1680 train_time:1223ms step_avg:87.34ms
step:15/1680 train_time:1312ms step_avg:87.45ms
step:16/1680 train_time:1402ms step_avg:87.60ms
step:17/1680 train_time:1491ms step_avg:87.69ms
step:18/1680 train_time:1579ms step_avg:87.74ms
step:19/1680 train_time:1668ms step_avg:87.81ms
step:20/1680 train_time:1756ms step_avg:87.82ms
step:21/1680 train_time:1845ms step_avg:87.86ms
step:22/1680 train_time:1934ms step_avg:87.89ms
step:23/1680 train_time:2025ms step_avg:88.04ms
step:24/1680 train_time:2115ms step_avg:88.14ms
step:25/1680 train_time:2206ms step_avg:88.23ms
step:26/1680 train_time:2295ms step_avg:88.29ms
step:27/1680 train_time:2385ms step_avg:88.35ms
step:28/1680 train_time:2474ms step_avg:88.36ms
step:29/1680 train_time:2563ms step_avg:88.37ms
step:30/1680 train_time:2652ms step_avg:88.39ms
step:31/1680 train_time:2740ms step_avg:88.40ms
step:32/1680 train_time:2829ms step_avg:88.41ms
step:33/1680 train_time:2918ms step_avg:88.41ms
step:34/1680 train_time:3007ms step_avg:88.45ms
step:35/1680 train_time:3097ms step_avg:88.48ms
step:36/1680 train_time:3187ms step_avg:88.53ms
step:37/1680 train_time:3277ms step_avg:88.57ms
step:38/1680 train_time:3367ms step_avg:88.62ms
step:39/1680 train_time:3457ms step_avg:88.63ms
step:40/1680 train_time:3545ms step_avg:88.62ms
step:41/1680 train_time:3634ms step_avg:88.63ms
step:42/1680 train_time:3723ms step_avg:88.65ms
step:43/1680 train_time:3813ms step_avg:88.68ms
step:44/1680 train_time:3901ms step_avg:88.65ms
step:45/1680 train_time:3990ms step_avg:88.67ms
step:46/1680 train_time:4079ms step_avg:88.67ms
step:47/1680 train_time:4169ms step_avg:88.70ms
step:48/1680 train_time:4259ms step_avg:88.73ms
step:49/1680 train_time:4348ms step_avg:88.74ms
step:50/1680 train_time:4438ms step_avg:88.76ms
step:51/1680 train_time:4527ms step_avg:88.77ms
step:52/1680 train_time:4617ms step_avg:88.79ms
step:53/1680 train_time:4707ms step_avg:88.81ms
step:54/1680 train_time:4796ms step_avg:88.81ms
step:55/1680 train_time:4884ms step_avg:88.80ms
step:56/1680 train_time:4973ms step_avg:88.81ms
step:57/1680 train_time:5064ms step_avg:88.83ms
step:58/1680 train_time:5153ms step_avg:88.84ms
step:59/1680 train_time:5242ms step_avg:88.85ms
step:60/1680 train_time:5331ms step_avg:88.85ms
step:61/1680 train_time:5420ms step_avg:88.86ms
step:62/1680 train_time:5510ms step_avg:88.87ms
step:63/1680 train_time:5599ms step_avg:88.87ms
step:64/1680 train_time:5688ms step_avg:88.87ms
step:65/1680 train_time:5776ms step_avg:88.87ms
step:66/1680 train_time:5866ms step_avg:88.88ms
step:67/1680 train_time:5955ms step_avg:88.88ms
step:68/1680 train_time:6045ms step_avg:88.90ms
step:69/1680 train_time:6134ms step_avg:88.91ms
step:70/1680 train_time:6224ms step_avg:88.92ms
step:71/1680 train_time:6313ms step_avg:88.91ms
step:72/1680 train_time:6402ms step_avg:88.92ms
step:73/1680 train_time:6491ms step_avg:88.92ms
step:74/1680 train_time:6580ms step_avg:88.92ms
step:75/1680 train_time:6669ms step_avg:88.92ms
step:76/1680 train_time:6759ms step_avg:88.93ms
step:77/1680 train_time:6848ms step_avg:88.94ms
step:78/1680 train_time:6938ms step_avg:88.94ms
step:79/1680 train_time:7027ms step_avg:88.95ms
step:80/1680 train_time:7116ms step_avg:88.95ms
step:81/1680 train_time:7206ms step_avg:88.96ms
step:82/1680 train_time:7295ms step_avg:88.96ms
step:83/1680 train_time:7384ms step_avg:88.96ms
step:84/1680 train_time:7473ms step_avg:88.97ms
step:85/1680 train_time:7562ms step_avg:88.97ms
step:86/1680 train_time:7651ms step_avg:88.97ms
step:87/1680 train_time:7740ms step_avg:88.96ms
step:88/1680 train_time:7829ms step_avg:88.96ms
step:89/1680 train_time:7918ms step_avg:88.96ms
step:90/1680 train_time:8007ms step_avg:88.96ms
step:91/1680 train_time:8096ms step_avg:88.97ms
step:92/1680 train_time:8185ms step_avg:88.97ms
step:93/1680 train_time:8275ms step_avg:88.98ms
step:94/1680 train_time:8364ms step_avg:88.98ms
step:95/1680 train_time:8453ms step_avg:88.98ms
step:96/1680 train_time:8542ms step_avg:88.98ms
step:97/1680 train_time:8631ms step_avg:88.98ms
step:98/1680 train_time:8719ms step_avg:88.97ms
step:99/1680 train_time:8808ms step_avg:88.97ms
step:100/1680 train_time:8898ms step_avg:88.98ms
step:101/1680 train_time:8987ms step_avg:88.99ms
step:102/1680 train_time:9076ms step_avg:88.98ms
step:103/1680 train_time:9167ms step_avg:89.00ms
step:104/1680 train_time:9256ms step_avg:89.00ms
step:105/1680 train_time:9346ms step_avg:89.01ms
step:106/1680 train_time:9435ms step_avg:89.01ms
step:107/1680 train_time:9524ms step_avg:89.00ms
step:108/1680 train_time:9614ms step_avg:89.01ms
step:109/1680 train_time:9704ms step_avg:89.02ms
step:110/1680 train_time:9793ms step_avg:89.02ms
step:111/1680 train_time:9881ms step_avg:89.02ms
step:112/1680 train_time:9971ms step_avg:89.02ms
step:113/1680 train_time:10060ms step_avg:89.02ms
step:114/1680 train_time:10149ms step_avg:89.02ms
step:115/1680 train_time:10238ms step_avg:89.03ms
step:116/1680 train_time:10328ms step_avg:89.03ms
step:117/1680 train_time:10416ms step_avg:89.03ms
step:118/1680 train_time:10505ms step_avg:89.03ms
step:119/1680 train_time:10594ms step_avg:89.03ms
step:120/1680 train_time:10683ms step_avg:89.02ms
step:121/1680 train_time:10772ms step_avg:89.03ms
step:122/1680 train_time:10861ms step_avg:89.02ms
step:123/1680 train_time:10949ms step_avg:89.02ms
step:124/1680 train_time:11038ms step_avg:89.02ms
step:125/1680 train_time:11127ms step_avg:89.02ms
step:125/1680 val_loss:4.3057 train_time:11217ms step_avg:89.74ms
step:126/1680 train_time:11241ms step_avg:89.21ms
step:127/1680 train_time:11309ms step_avg:89.05ms
step:128/1680 train_time:11408ms step_avg:89.13ms
step:129/1680 train_time:11501ms step_avg:89.16ms
step:130/1680 train_time:11590ms step_avg:89.16ms
step:131/1680 train_time:11679ms step_avg:89.15ms
step:132/1680 train_time:11766ms step_avg:89.14ms
step:133/1680 train_time:11854ms step_avg:89.13ms
step:134/1680 train_time:11942ms step_avg:89.12ms
step:135/1680 train_time:12030ms step_avg:89.11ms
step:136/1680 train_time:12119ms step_avg:89.11ms
step:137/1680 train_time:12209ms step_avg:89.12ms
step:138/1680 train_time:12299ms step_avg:89.12ms
step:139/1680 train_time:12391ms step_avg:89.14ms
step:140/1680 train_time:12481ms step_avg:89.15ms
step:141/1680 train_time:12571ms step_avg:89.16ms
step:142/1680 train_time:12661ms step_avg:89.16ms
step:143/1680 train_time:12749ms step_avg:89.16ms
step:144/1680 train_time:12838ms step_avg:89.15ms
step:145/1680 train_time:12926ms step_avg:89.14ms
step:146/1680 train_time:13014ms step_avg:89.14ms
step:147/1680 train_time:13102ms step_avg:89.13ms
step:148/1680 train_time:13190ms step_avg:89.12ms
step:149/1680 train_time:13280ms step_avg:89.12ms
step:150/1680 train_time:13370ms step_avg:89.14ms
step:151/1680 train_time:13461ms step_avg:89.14ms
step:152/1680 train_time:13550ms step_avg:89.15ms
step:153/1680 train_time:13640ms step_avg:89.15ms
step:154/1680 train_time:13729ms step_avg:89.15ms
step:155/1680 train_time:13820ms step_avg:89.16ms
step:156/1680 train_time:13908ms step_avg:89.15ms
step:157/1680 train_time:13997ms step_avg:89.15ms
step:158/1680 train_time:14085ms step_avg:89.15ms
step:159/1680 train_time:14173ms step_avg:89.14ms
step:160/1680 train_time:14262ms step_avg:89.14ms
step:161/1680 train_time:14351ms step_avg:89.14ms
step:162/1680 train_time:14441ms step_avg:89.14ms
step:163/1680 train_time:14531ms step_avg:89.15ms
step:164/1680 train_time:14621ms step_avg:89.15ms
step:165/1680 train_time:14710ms step_avg:89.15ms
step:166/1680 train_time:14800ms step_avg:89.16ms
step:167/1680 train_time:14888ms step_avg:89.15ms
step:168/1680 train_time:14976ms step_avg:89.15ms
step:169/1680 train_time:15065ms step_avg:89.14ms
step:170/1680 train_time:15154ms step_avg:89.14ms
step:171/1680 train_time:15243ms step_avg:89.14ms
step:172/1680 train_time:15332ms step_avg:89.14ms
step:173/1680 train_time:15421ms step_avg:89.14ms
step:174/1680 train_time:15511ms step_avg:89.14ms
step:175/1680 train_time:15600ms step_avg:89.14ms
step:176/1680 train_time:15689ms step_avg:89.14ms
step:177/1680 train_time:15778ms step_avg:89.14ms
step:178/1680 train_time:15867ms step_avg:89.14ms
step:179/1680 train_time:15956ms step_avg:89.14ms
step:180/1680 train_time:16044ms step_avg:89.13ms
step:181/1680 train_time:16133ms step_avg:89.13ms
step:182/1680 train_time:16222ms step_avg:89.13ms
step:183/1680 train_time:16312ms step_avg:89.14ms
step:184/1680 train_time:16401ms step_avg:89.14ms
step:185/1680 train_time:16490ms step_avg:89.14ms
step:186/1680 train_time:16579ms step_avg:89.14ms
step:187/1680 train_time:16668ms step_avg:89.14ms
step:188/1680 train_time:16758ms step_avg:89.14ms
step:189/1680 train_time:16846ms step_avg:89.13ms
step:190/1680 train_time:16935ms step_avg:89.13ms
step:191/1680 train_time:17025ms step_avg:89.13ms
step:192/1680 train_time:17114ms step_avg:89.13ms
step:193/1680 train_time:17203ms step_avg:89.13ms
step:194/1680 train_time:17291ms step_avg:89.13ms
step:195/1680 train_time:17380ms step_avg:89.13ms
step:196/1680 train_time:17468ms step_avg:89.12ms
step:197/1680 train_time:17557ms step_avg:89.12ms
step:198/1680 train_time:17646ms step_avg:89.12ms
step:199/1680 train_time:17736ms step_avg:89.12ms
step:200/1680 train_time:17825ms step_avg:89.12ms
step:201/1680 train_time:17914ms step_avg:89.12ms
step:202/1680 train_time:18003ms step_avg:89.12ms
step:203/1680 train_time:18092ms step_avg:89.12ms
step:204/1680 train_time:18181ms step_avg:89.12ms
step:205/1680 train_time:18270ms step_avg:89.12ms
step:206/1680 train_time:18359ms step_avg:89.12ms
step:207/1680 train_time:18449ms step_avg:89.12ms
step:208/1680 train_time:18538ms step_avg:89.13ms
step:209/1680 train_time:18627ms step_avg:89.12ms
step:210/1680 train_time:18716ms step_avg:89.13ms
step:211/1680 train_time:18806ms step_avg:89.13ms
step:212/1680 train_time:18895ms step_avg:89.13ms
step:213/1680 train_time:18984ms step_avg:89.13ms
step:214/1680 train_time:19073ms step_avg:89.13ms
step:215/1680 train_time:19162ms step_avg:89.13ms
step:216/1680 train_time:19252ms step_avg:89.13ms
step:217/1680 train_time:19340ms step_avg:89.13ms
step:218/1680 train_time:19430ms step_avg:89.13ms
step:219/1680 train_time:19520ms step_avg:89.13ms
step:220/1680 train_time:19609ms step_avg:89.13ms
step:221/1680 train_time:19698ms step_avg:89.13ms
step:222/1680 train_time:19786ms step_avg:89.13ms
step:223/1680 train_time:19874ms step_avg:89.12ms
step:224/1680 train_time:19963ms step_avg:89.12ms
step:225/1680 train_time:20052ms step_avg:89.12ms
step:226/1680 train_time:20141ms step_avg:89.12ms
step:227/1680 train_time:20231ms step_avg:89.12ms
step:228/1680 train_time:20320ms step_avg:89.12ms
step:229/1680 train_time:20410ms step_avg:89.13ms
step:230/1680 train_time:20498ms step_avg:89.12ms
step:231/1680 train_time:20588ms step_avg:89.13ms
step:232/1680 train_time:20677ms step_avg:89.12ms
step:233/1680 train_time:20765ms step_avg:89.12ms
step:234/1680 train_time:20853ms step_avg:89.12ms
step:235/1680 train_time:20942ms step_avg:89.12ms
step:236/1680 train_time:21031ms step_avg:89.12ms
step:237/1680 train_time:21121ms step_avg:89.12ms
step:238/1680 train_time:21210ms step_avg:89.12ms
step:239/1680 train_time:21299ms step_avg:89.12ms
step:240/1680 train_time:21389ms step_avg:89.12ms
step:241/1680 train_time:21478ms step_avg:89.12ms
step:242/1680 train_time:21567ms step_avg:89.12ms
step:243/1680 train_time:21657ms step_avg:89.12ms
step:244/1680 train_time:21745ms step_avg:89.12ms
step:245/1680 train_time:21834ms step_avg:89.12ms
step:246/1680 train_time:21923ms step_avg:89.12ms
step:247/1680 train_time:22012ms step_avg:89.12ms
step:248/1680 train_time:22101ms step_avg:89.12ms
step:249/1680 train_time:22189ms step_avg:89.11ms
step:250/1680 train_time:22279ms step_avg:89.11ms
step:250/1680 val_loss:3.9744 train_time:22370ms step_avg:89.48ms
step:251/1680 train_time:22393ms step_avg:89.21ms
step:252/1680 train_time:22463ms step_avg:89.14ms
step:253/1680 train_time:22558ms step_avg:89.16ms
step:254/1680 train_time:22650ms step_avg:89.17ms
step:255/1680 train_time:22738ms step_avg:89.17ms
step:256/1680 train_time:22826ms step_avg:89.16ms
step:257/1680 train_time:22913ms step_avg:89.16ms
step:258/1680 train_time:23001ms step_avg:89.15ms
step:259/1680 train_time:23088ms step_avg:89.14ms
step:260/1680 train_time:23176ms step_avg:89.14ms
step:261/1680 train_time:23264ms step_avg:89.13ms
step:262/1680 train_time:23352ms step_avg:89.13ms
step:263/1680 train_time:23442ms step_avg:89.13ms
step:264/1680 train_time:23533ms step_avg:89.14ms
step:265/1680 train_time:23623ms step_avg:89.14ms
step:266/1680 train_time:23713ms step_avg:89.15ms
step:267/1680 train_time:23801ms step_avg:89.14ms
step:268/1680 train_time:23890ms step_avg:89.14ms
step:269/1680 train_time:23977ms step_avg:89.13ms
step:270/1680 train_time:24066ms step_avg:89.13ms
step:271/1680 train_time:24154ms step_avg:89.13ms
step:272/1680 train_time:24242ms step_avg:89.12ms
step:273/1680 train_time:24329ms step_avg:89.12ms
step:274/1680 train_time:24418ms step_avg:89.12ms
step:275/1680 train_time:24508ms step_avg:89.12ms
step:276/1680 train_time:24598ms step_avg:89.12ms
step:277/1680 train_time:24688ms step_avg:89.13ms
step:278/1680 train_time:24777ms step_avg:89.12ms
step:279/1680 train_time:24865ms step_avg:89.12ms
step:280/1680 train_time:24954ms step_avg:89.12ms
step:281/1680 train_time:25042ms step_avg:89.12ms
step:282/1680 train_time:25131ms step_avg:89.12ms
step:283/1680 train_time:25219ms step_avg:89.11ms
step:284/1680 train_time:25308ms step_avg:89.11ms
step:285/1680 train_time:25397ms step_avg:89.11ms
step:286/1680 train_time:25487ms step_avg:89.11ms
step:287/1680 train_time:25576ms step_avg:89.12ms
step:288/1680 train_time:25666ms step_avg:89.12ms
step:289/1680 train_time:25755ms step_avg:89.12ms
step:290/1680 train_time:25845ms step_avg:89.12ms
step:291/1680 train_time:25934ms step_avg:89.12ms
step:292/1680 train_time:26023ms step_avg:89.12ms
step:293/1680 train_time:26111ms step_avg:89.12ms
step:294/1680 train_time:26200ms step_avg:89.11ms
step:295/1680 train_time:26288ms step_avg:89.11ms
step:296/1680 train_time:26377ms step_avg:89.11ms
step:297/1680 train_time:26466ms step_avg:89.11ms
step:298/1680 train_time:26556ms step_avg:89.11ms
step:299/1680 train_time:26646ms step_avg:89.12ms
step:300/1680 train_time:26736ms step_avg:89.12ms
step:301/1680 train_time:26825ms step_avg:89.12ms
step:302/1680 train_time:26914ms step_avg:89.12ms
step:303/1680 train_time:27003ms step_avg:89.12ms
step:304/1680 train_time:27091ms step_avg:89.12ms
step:305/1680 train_time:27180ms step_avg:89.11ms
step:306/1680 train_time:27268ms step_avg:89.11ms
step:307/1680 train_time:27356ms step_avg:89.11ms
step:308/1680 train_time:27445ms step_avg:89.11ms
step:309/1680 train_time:27535ms step_avg:89.11ms
step:310/1680 train_time:27624ms step_avg:89.11ms
step:311/1680 train_time:27713ms step_avg:89.11ms
step:312/1680 train_time:27803ms step_avg:89.11ms
step:313/1680 train_time:27892ms step_avg:89.11ms
step:314/1680 train_time:27981ms step_avg:89.11ms
step:315/1680 train_time:28070ms step_avg:89.11ms
step:316/1680 train_time:28158ms step_avg:89.11ms
step:317/1680 train_time:28247ms step_avg:89.11ms
step:318/1680 train_time:28337ms step_avg:89.11ms
step:319/1680 train_time:28426ms step_avg:89.11ms
step:320/1680 train_time:28514ms step_avg:89.11ms
step:321/1680 train_time:28604ms step_avg:89.11ms
step:322/1680 train_time:28693ms step_avg:89.11ms
step:323/1680 train_time:28782ms step_avg:89.11ms
step:324/1680 train_time:28872ms step_avg:89.11ms
step:325/1680 train_time:28960ms step_avg:89.11ms
step:326/1680 train_time:29049ms step_avg:89.11ms
step:327/1680 train_time:29138ms step_avg:89.11ms
step:328/1680 train_time:29227ms step_avg:89.11ms
step:329/1680 train_time:29316ms step_avg:89.10ms
step:330/1680 train_time:29404ms step_avg:89.10ms
step:331/1680 train_time:29494ms step_avg:89.11ms
step:332/1680 train_time:29584ms step_avg:89.11ms
step:333/1680 train_time:29673ms step_avg:89.11ms
step:334/1680 train_time:29763ms step_avg:89.11ms
step:335/1680 train_time:29852ms step_avg:89.11ms
step:336/1680 train_time:29941ms step_avg:89.11ms
step:337/1680 train_time:30030ms step_avg:89.11ms
step:338/1680 train_time:30119ms step_avg:89.11ms
step:339/1680 train_time:30208ms step_avg:89.11ms
step:340/1680 train_time:30297ms step_avg:89.11ms
step:341/1680 train_time:30385ms step_avg:89.11ms
step:342/1680 train_time:30475ms step_avg:89.11ms
step:343/1680 train_time:30565ms step_avg:89.11ms
step:344/1680 train_time:30655ms step_avg:89.11ms
step:345/1680 train_time:30743ms step_avg:89.11ms
step:346/1680 train_time:30833ms step_avg:89.11ms
step:347/1680 train_time:30923ms step_avg:89.11ms
step:348/1680 train_time:31011ms step_avg:89.11ms
step:349/1680 train_time:31100ms step_avg:89.11ms
step:350/1680 train_time:31189ms step_avg:89.11ms
step:351/1680 train_time:31278ms step_avg:89.11ms
step:352/1680 train_time:31367ms step_avg:89.11ms
step:353/1680 train_time:31457ms step_avg:89.11ms
step:354/1680 train_time:31545ms step_avg:89.11ms
step:355/1680 train_time:31635ms step_avg:89.11ms
step:356/1680 train_time:31724ms step_avg:89.11ms
step:357/1680 train_time:31813ms step_avg:89.11ms
step:358/1680 train_time:31902ms step_avg:89.11ms
step:359/1680 train_time:31992ms step_avg:89.11ms
step:360/1680 train_time:32081ms step_avg:89.11ms
step:361/1680 train_time:32170ms step_avg:89.11ms
step:362/1680 train_time:32259ms step_avg:89.11ms
step:363/1680 train_time:32348ms step_avg:89.11ms
step:364/1680 train_time:32436ms step_avg:89.11ms
step:365/1680 train_time:32525ms step_avg:89.11ms
step:366/1680 train_time:32614ms step_avg:89.11ms
step:367/1680 train_time:32703ms step_avg:89.11ms
step:368/1680 train_time:32791ms step_avg:89.11ms
step:369/1680 train_time:32880ms step_avg:89.11ms
step:370/1680 train_time:32968ms step_avg:89.10ms
step:371/1680 train_time:33057ms step_avg:89.10ms
step:372/1680 train_time:33146ms step_avg:89.10ms
step:373/1680 train_time:33236ms step_avg:89.10ms
step:374/1680 train_time:33325ms step_avg:89.10ms
step:375/1680 train_time:33414ms step_avg:89.10ms
step:375/1680 val_loss:3.8185 train_time:33504ms step_avg:89.34ms
step:376/1680 train_time:33527ms step_avg:89.17ms
step:377/1680 train_time:33595ms step_avg:89.11ms
step:378/1680 train_time:33691ms step_avg:89.13ms
step:379/1680 train_time:33783ms step_avg:89.14ms
step:380/1680 train_time:33871ms step_avg:89.14ms
step:381/1680 train_time:33959ms step_avg:89.13ms
step:382/1680 train_time:34047ms step_avg:89.13ms
step:383/1680 train_time:34135ms step_avg:89.13ms
step:384/1680 train_time:34223ms step_avg:89.12ms
step:385/1680 train_time:34311ms step_avg:89.12ms
step:386/1680 train_time:34398ms step_avg:89.12ms
step:387/1680 train_time:34487ms step_avg:89.11ms
step:388/1680 train_time:34577ms step_avg:89.12ms
step:389/1680 train_time:34669ms step_avg:89.12ms
step:390/1680 train_time:34759ms step_avg:89.13ms
step:391/1680 train_time:34848ms step_avg:89.13ms
step:392/1680 train_time:34937ms step_avg:89.13ms
step:393/1680 train_time:35026ms step_avg:89.12ms
step:394/1680 train_time:35113ms step_avg:89.12ms
step:395/1680 train_time:35202ms step_avg:89.12ms
step:396/1680 train_time:35291ms step_avg:89.12ms
step:397/1680 train_time:35379ms step_avg:89.12ms
step:398/1680 train_time:35467ms step_avg:89.11ms
step:399/1680 train_time:35556ms step_avg:89.11ms
step:400/1680 train_time:35647ms step_avg:89.12ms
step:401/1680 train_time:35737ms step_avg:89.12ms
step:402/1680 train_time:35828ms step_avg:89.12ms
step:403/1680 train_time:35916ms step_avg:89.12ms
step:404/1680 train_time:36005ms step_avg:89.12ms
step:405/1680 train_time:36094ms step_avg:89.12ms
step:406/1680 train_time:36183ms step_avg:89.12ms
step:407/1680 train_time:36270ms step_avg:89.12ms
step:408/1680 train_time:36358ms step_avg:89.11ms
step:409/1680 train_time:36447ms step_avg:89.11ms
step:410/1680 train_time:36535ms step_avg:89.11ms
step:411/1680 train_time:36624ms step_avg:89.11ms
step:412/1680 train_time:36714ms step_avg:89.11ms
step:413/1680 train_time:36804ms step_avg:89.11ms
step:414/1680 train_time:36894ms step_avg:89.12ms
step:415/1680 train_time:36983ms step_avg:89.12ms
step:416/1680 train_time:37072ms step_avg:89.12ms
step:417/1680 train_time:37162ms step_avg:89.12ms
step:418/1680 train_time:37250ms step_avg:89.11ms
step:419/1680 train_time:37338ms step_avg:89.11ms
step:420/1680 train_time:37427ms step_avg:89.11ms
step:421/1680 train_time:37516ms step_avg:89.11ms
step:422/1680 train_time:37604ms step_avg:89.11ms
step:423/1680 train_time:37693ms step_avg:89.11ms
step:424/1680 train_time:37782ms step_avg:89.11ms
step:425/1680 train_time:37872ms step_avg:89.11ms
step:426/1680 train_time:37960ms step_avg:89.11ms
step:427/1680 train_time:38050ms step_avg:89.11ms
step:428/1680 train_time:38139ms step_avg:89.11ms
step:429/1680 train_time:38228ms step_avg:89.11ms
step:430/1680 train_time:38317ms step_avg:89.11ms
step:431/1680 train_time:38406ms step_avg:89.11ms
step:432/1680 train_time:38495ms step_avg:89.11ms
step:433/1680 train_time:38584ms step_avg:89.11ms
step:434/1680 train_time:38674ms step_avg:89.11ms
step:435/1680 train_time:38763ms step_avg:89.11ms
step:436/1680 train_time:38852ms step_avg:89.11ms
step:437/1680 train_time:38942ms step_avg:89.11ms
step:438/1680 train_time:39031ms step_avg:89.11ms
step:439/1680 train_time:39120ms step_avg:89.11ms
step:440/1680 train_time:39209ms step_avg:89.11ms
step:441/1680 train_time:39298ms step_avg:89.11ms
step:442/1680 train_time:39387ms step_avg:89.11ms
step:443/1680 train_time:39476ms step_avg:89.11ms
step:444/1680 train_time:39564ms step_avg:89.11ms
step:445/1680 train_time:39654ms step_avg:89.11ms
step:446/1680 train_time:39743ms step_avg:89.11ms
step:447/1680 train_time:39833ms step_avg:89.11ms
step:448/1680 train_time:39922ms step_avg:89.11ms
step:449/1680 train_time:40011ms step_avg:89.11ms
step:450/1680 train_time:40100ms step_avg:89.11ms
step:451/1680 train_time:40189ms step_avg:89.11ms
step:452/1680 train_time:40277ms step_avg:89.11ms
step:453/1680 train_time:40366ms step_avg:89.11ms
step:454/1680 train_time:40455ms step_avg:89.11ms
step:455/1680 train_time:40544ms step_avg:89.11ms
step:456/1680 train_time:40634ms step_avg:89.11ms
step:457/1680 train_time:40724ms step_avg:89.11ms
step:458/1680 train_time:40813ms step_avg:89.11ms
step:459/1680 train_time:40902ms step_avg:89.11ms
step:460/1680 train_time:40991ms step_avg:89.11ms
step:461/1680 train_time:41081ms step_avg:89.11ms
step:462/1680 train_time:41169ms step_avg:89.11ms
step:463/1680 train_time:41258ms step_avg:89.11ms
step:464/1680 train_time:41347ms step_avg:89.11ms
step:465/1680 train_time:41436ms step_avg:89.11ms
step:466/1680 train_time:41526ms step_avg:89.11ms
step:467/1680 train_time:41616ms step_avg:89.11ms
step:468/1680 train_time:41703ms step_avg:89.11ms
step:469/1680 train_time:41793ms step_avg:89.11ms
step:470/1680 train_time:41882ms step_avg:89.11ms
step:471/1680 train_time:41972ms step_avg:89.11ms
step:472/1680 train_time:42061ms step_avg:89.11ms
step:473/1680 train_time:42150ms step_avg:89.11ms
step:474/1680 train_time:42239ms step_avg:89.11ms
step:475/1680 train_time:42328ms step_avg:89.11ms
step:476/1680 train_time:42417ms step_avg:89.11ms
step:477/1680 train_time:42506ms step_avg:89.11ms
step:478/1680 train_time:42595ms step_avg:89.11ms
step:479/1680 train_time:42683ms step_avg:89.11ms
step:480/1680 train_time:42772ms step_avg:89.11ms
step:481/1680 train_time:42861ms step_avg:89.11ms
step:482/1680 train_time:42951ms step_avg:89.11ms
step:483/1680 train_time:43041ms step_avg:89.11ms
step:484/1680 train_time:43130ms step_avg:89.11ms
step:485/1680 train_time:43218ms step_avg:89.11ms
step:486/1680 train_time:43308ms step_avg:89.11ms
step:487/1680 train_time:43397ms step_avg:89.11ms
step:488/1680 train_time:43485ms step_avg:89.11ms
step:489/1680 train_time:43573ms step_avg:89.11ms
step:490/1680 train_time:43662ms step_avg:89.11ms
step:491/1680 train_time:43752ms step_avg:89.11ms
step:492/1680 train_time:43841ms step_avg:89.11ms
step:493/1680 train_time:43930ms step_avg:89.11ms
step:494/1680 train_time:44020ms step_avg:89.11ms
step:495/1680 train_time:44109ms step_avg:89.11ms
step:496/1680 train_time:44198ms step_avg:89.11ms
step:497/1680 train_time:44288ms step_avg:89.11ms
step:498/1680 train_time:44377ms step_avg:89.11ms
step:499/1680 train_time:44466ms step_avg:89.11ms
step:500/1680 train_time:44555ms step_avg:89.11ms
step:500/1680 val_loss:3.7194 train_time:44645ms step_avg:89.29ms
step:501/1680 train_time:44667ms step_avg:89.16ms
step:502/1680 train_time:44736ms step_avg:89.12ms
step:503/1680 train_time:44834ms step_avg:89.13ms
step:504/1680 train_time:44924ms step_avg:89.13ms
step:505/1680 train_time:45013ms step_avg:89.13ms
step:506/1680 train_time:45101ms step_avg:89.13ms
step:507/1680 train_time:45190ms step_avg:89.13ms
step:508/1680 train_time:45278ms step_avg:89.13ms
step:509/1680 train_time:45366ms step_avg:89.13ms
step:510/1680 train_time:45454ms step_avg:89.13ms
step:511/1680 train_time:45542ms step_avg:89.12ms
step:512/1680 train_time:45631ms step_avg:89.12ms
step:513/1680 train_time:45721ms step_avg:89.13ms
step:514/1680 train_time:45813ms step_avg:89.13ms
step:515/1680 train_time:45904ms step_avg:89.13ms
step:516/1680 train_time:45994ms step_avg:89.14ms
step:517/1680 train_time:46083ms step_avg:89.13ms
step:518/1680 train_time:46171ms step_avg:89.13ms
step:519/1680 train_time:46260ms step_avg:89.13ms
step:520/1680 train_time:46349ms step_avg:89.13ms
step:521/1680 train_time:46437ms step_avg:89.13ms
step:522/1680 train_time:46525ms step_avg:89.13ms
step:523/1680 train_time:46613ms step_avg:89.13ms
step:524/1680 train_time:46702ms step_avg:89.13ms
step:525/1680 train_time:46793ms step_avg:89.13ms
step:526/1680 train_time:46885ms step_avg:89.13ms
step:527/1680 train_time:46976ms step_avg:89.14ms
step:528/1680 train_time:47062ms step_avg:89.13ms
step:529/1680 train_time:47151ms step_avg:89.13ms
step:530/1680 train_time:47240ms step_avg:89.13ms
step:531/1680 train_time:47329ms step_avg:89.13ms
step:532/1680 train_time:47418ms step_avg:89.13ms
step:533/1680 train_time:47506ms step_avg:89.13ms
step:534/1680 train_time:47596ms step_avg:89.13ms
step:535/1680 train_time:47685ms step_avg:89.13ms
step:536/1680 train_time:47774ms step_avg:89.13ms
step:537/1680 train_time:47864ms step_avg:89.13ms
step:538/1680 train_time:47954ms step_avg:89.13ms
step:539/1680 train_time:48043ms step_avg:89.13ms
step:540/1680 train_time:48131ms step_avg:89.13ms
step:541/1680 train_time:48221ms step_avg:89.13ms
step:542/1680 train_time:48309ms step_avg:89.13ms
step:543/1680 train_time:48398ms step_avg:89.13ms
step:544/1680 train_time:48487ms step_avg:89.13ms
step:545/1680 train_time:48575ms step_avg:89.13ms
step:546/1680 train_time:48665ms step_avg:89.13ms
step:547/1680 train_time:48754ms step_avg:89.13ms
step:548/1680 train_time:48843ms step_avg:89.13ms
step:549/1680 train_time:48934ms step_avg:89.13ms
step:550/1680 train_time:49024ms step_avg:89.14ms
step:551/1680 train_time:49114ms step_avg:89.14ms
step:552/1680 train_time:49203ms step_avg:89.14ms
step:553/1680 train_time:49293ms step_avg:89.14ms
step:554/1680 train_time:49383ms step_avg:89.14ms
step:555/1680 train_time:49473ms step_avg:89.14ms
step:556/1680 train_time:49563ms step_avg:89.14ms
step:557/1680 train_time:49653ms step_avg:89.14ms
step:558/1680 train_time:49744ms step_avg:89.15ms
step:559/1680 train_time:49835ms step_avg:89.15ms
step:560/1680 train_time:49925ms step_avg:89.15ms
step:561/1680 train_time:50016ms step_avg:89.16ms
step:562/1680 train_time:50106ms step_avg:89.16ms
step:563/1680 train_time:50197ms step_avg:89.16ms
step:564/1680 train_time:50288ms step_avg:89.16ms
step:565/1680 train_time:50377ms step_avg:89.16ms
step:566/1680 train_time:50468ms step_avg:89.17ms
step:567/1680 train_time:50557ms step_avg:89.17ms
step:568/1680 train_time:50649ms step_avg:89.17ms
step:569/1680 train_time:50739ms step_avg:89.17ms
step:570/1680 train_time:50829ms step_avg:89.17ms
step:571/1680 train_time:50920ms step_avg:89.18ms
step:572/1680 train_time:51011ms step_avg:89.18ms
step:573/1680 train_time:51101ms step_avg:89.18ms
step:574/1680 train_time:51191ms step_avg:89.18ms
step:575/1680 train_time:51282ms step_avg:89.19ms
step:576/1680 train_time:51372ms step_avg:89.19ms
step:577/1680 train_time:51462ms step_avg:89.19ms
step:578/1680 train_time:51552ms step_avg:89.19ms
step:579/1680 train_time:51643ms step_avg:89.19ms
step:580/1680 train_time:51733ms step_avg:89.19ms
step:581/1680 train_time:51823ms step_avg:89.20ms
step:582/1680 train_time:51913ms step_avg:89.20ms
step:583/1680 train_time:52004ms step_avg:89.20ms
step:584/1680 train_time:52094ms step_avg:89.20ms
step:585/1680 train_time:52184ms step_avg:89.20ms
step:586/1680 train_time:52274ms step_avg:89.21ms
step:587/1680 train_time:52367ms step_avg:89.21ms
step:588/1680 train_time:52457ms step_avg:89.21ms
step:589/1680 train_time:52547ms step_avg:89.21ms
step:590/1680 train_time:52637ms step_avg:89.22ms
step:591/1680 train_time:52727ms step_avg:89.22ms
step:592/1680 train_time:52817ms step_avg:89.22ms
step:593/1680 train_time:52907ms step_avg:89.22ms
step:594/1680 train_time:52997ms step_avg:89.22ms
step:595/1680 train_time:53089ms step_avg:89.22ms
step:596/1680 train_time:53178ms step_avg:89.23ms
step:597/1680 train_time:53274ms step_avg:89.24ms
step:598/1680 train_time:53359ms step_avg:89.23ms
step:599/1680 train_time:53451ms step_avg:89.23ms
step:600/1680 train_time:53541ms step_avg:89.23ms
step:601/1680 train_time:53631ms step_avg:89.24ms
step:602/1680 train_time:53722ms step_avg:89.24ms
step:603/1680 train_time:53811ms step_avg:89.24ms
step:604/1680 train_time:53902ms step_avg:89.24ms
step:605/1680 train_time:53992ms step_avg:89.24ms
step:606/1680 train_time:54083ms step_avg:89.25ms
step:607/1680 train_time:54173ms step_avg:89.25ms
step:608/1680 train_time:54263ms step_avg:89.25ms
step:609/1680 train_time:54353ms step_avg:89.25ms
step:610/1680 train_time:54443ms step_avg:89.25ms
step:611/1680 train_time:54534ms step_avg:89.25ms
step:612/1680 train_time:54624ms step_avg:89.25ms
step:613/1680 train_time:54714ms step_avg:89.26ms
step:614/1680 train_time:54803ms step_avg:89.26ms
step:615/1680 train_time:54893ms step_avg:89.26ms
step:616/1680 train_time:54984ms step_avg:89.26ms
step:617/1680 train_time:55075ms step_avg:89.26ms
step:618/1680 train_time:55166ms step_avg:89.27ms
step:619/1680 train_time:55256ms step_avg:89.27ms
step:620/1680 train_time:55348ms step_avg:89.27ms
step:621/1680 train_time:55438ms step_avg:89.27ms
step:622/1680 train_time:55528ms step_avg:89.27ms
step:623/1680 train_time:55618ms step_avg:89.27ms
step:624/1680 train_time:55708ms step_avg:89.27ms
step:625/1680 train_time:55798ms step_avg:89.28ms
step:625/1680 val_loss:3.6190 train_time:55890ms step_avg:89.42ms
step:626/1680 train_time:55913ms step_avg:89.32ms
step:627/1680 train_time:55981ms step_avg:89.28ms
step:628/1680 train_time:56079ms step_avg:89.30ms
step:629/1680 train_time:56171ms step_avg:89.30ms
step:630/1680 train_time:56260ms step_avg:89.30ms
step:631/1680 train_time:56349ms step_avg:89.30ms
step:632/1680 train_time:56437ms step_avg:89.30ms
step:633/1680 train_time:56526ms step_avg:89.30ms
step:634/1680 train_time:56615ms step_avg:89.30ms
step:635/1680 train_time:56704ms step_avg:89.30ms
step:636/1680 train_time:56795ms step_avg:89.30ms
step:637/1680 train_time:56887ms step_avg:89.30ms
step:638/1680 train_time:56979ms step_avg:89.31ms
step:639/1680 train_time:57072ms step_avg:89.31ms
step:640/1680 train_time:57163ms step_avg:89.32ms
step:641/1680 train_time:57253ms step_avg:89.32ms
step:642/1680 train_time:57343ms step_avg:89.32ms
step:643/1680 train_time:57432ms step_avg:89.32ms
step:644/1680 train_time:57521ms step_avg:89.32ms
step:645/1680 train_time:57611ms step_avg:89.32ms
step:646/1680 train_time:57700ms step_avg:89.32ms
step:647/1680 train_time:57791ms step_avg:89.32ms
step:648/1680 train_time:57883ms step_avg:89.33ms
step:649/1680 train_time:57973ms step_avg:89.33ms
step:650/1680 train_time:58064ms step_avg:89.33ms
step:651/1680 train_time:58154ms step_avg:89.33ms
step:652/1680 train_time:58245ms step_avg:89.33ms
step:653/1680 train_time:58335ms step_avg:89.33ms
step:654/1680 train_time:58425ms step_avg:89.33ms
step:655/1680 train_time:58514ms step_avg:89.33ms
step:656/1680 train_time:58604ms step_avg:89.33ms
step:657/1680 train_time:58693ms step_avg:89.33ms
step:658/1680 train_time:58783ms step_avg:89.34ms
step:659/1680 train_time:58873ms step_avg:89.34ms
step:660/1680 train_time:58965ms step_avg:89.34ms
step:661/1680 train_time:59056ms step_avg:89.34ms
step:662/1680 train_time:59146ms step_avg:89.34ms
step:663/1680 train_time:59237ms step_avg:89.35ms
step:664/1680 train_time:59327ms step_avg:89.35ms
step:665/1680 train_time:59417ms step_avg:89.35ms
step:666/1680 train_time:59506ms step_avg:89.35ms
step:667/1680 train_time:59596ms step_avg:89.35ms
step:668/1680 train_time:59685ms step_avg:89.35ms
step:669/1680 train_time:59775ms step_avg:89.35ms
step:670/1680 train_time:59866ms step_avg:89.35ms
step:671/1680 train_time:59956ms step_avg:89.35ms
step:672/1680 train_time:60049ms step_avg:89.36ms
step:673/1680 train_time:60138ms step_avg:89.36ms
step:674/1680 train_time:60229ms step_avg:89.36ms
step:675/1680 train_time:60319ms step_avg:89.36ms
step:676/1680 train_time:60408ms step_avg:89.36ms
step:677/1680 train_time:60499ms step_avg:89.36ms
step:678/1680 train_time:60588ms step_avg:89.36ms
step:679/1680 train_time:60683ms step_avg:89.37ms
step:680/1680 train_time:60767ms step_avg:89.36ms
step:681/1680 train_time:60858ms step_avg:89.36ms
step:682/1680 train_time:60949ms step_avg:89.37ms
step:683/1680 train_time:61039ms step_avg:89.37ms
step:684/1680 train_time:61129ms step_avg:89.37ms
step:685/1680 train_time:61219ms step_avg:89.37ms
step:686/1680 train_time:61309ms step_avg:89.37ms
step:687/1680 train_time:61400ms step_avg:89.37ms
step:688/1680 train_time:61490ms step_avg:89.38ms
step:689/1680 train_time:61580ms step_avg:89.38ms
step:690/1680 train_time:61670ms step_avg:89.38ms
step:691/1680 train_time:61760ms step_avg:89.38ms
step:692/1680 train_time:61851ms step_avg:89.38ms
step:693/1680 train_time:61941ms step_avg:89.38ms
step:694/1680 train_time:62033ms step_avg:89.38ms
step:695/1680 train_time:62123ms step_avg:89.39ms
step:696/1680 train_time:62213ms step_avg:89.39ms
step:697/1680 train_time:62303ms step_avg:89.39ms
step:698/1680 train_time:62394ms step_avg:89.39ms
step:699/1680 train_time:62483ms step_avg:89.39ms
step:700/1680 train_time:62573ms step_avg:89.39ms
step:701/1680 train_time:62663ms step_avg:89.39ms
step:702/1680 train_time:62754ms step_avg:89.39ms
step:703/1680 train_time:62844ms step_avg:89.39ms
step:704/1680 train_time:62934ms step_avg:89.40ms
step:705/1680 train_time:63024ms step_avg:89.40ms
step:706/1680 train_time:63115ms step_avg:89.40ms
step:707/1680 train_time:63205ms step_avg:89.40ms
step:708/1680 train_time:63295ms step_avg:89.40ms
step:709/1680 train_time:63388ms step_avg:89.41ms
step:710/1680 train_time:63475ms step_avg:89.40ms
step:711/1680 train_time:63566ms step_avg:89.40ms
step:712/1680 train_time:63655ms step_avg:89.40ms
step:713/1680 train_time:63745ms step_avg:89.40ms
step:714/1680 train_time:63835ms step_avg:89.40ms
step:715/1680 train_time:63925ms step_avg:89.41ms
step:716/1680 train_time:64015ms step_avg:89.41ms
step:717/1680 train_time:64105ms step_avg:89.41ms
step:718/1680 train_time:64195ms step_avg:89.41ms
step:719/1680 train_time:64285ms step_avg:89.41ms
step:720/1680 train_time:64375ms step_avg:89.41ms
step:721/1680 train_time:64466ms step_avg:89.41ms
step:722/1680 train_time:64556ms step_avg:89.41ms
step:723/1680 train_time:64645ms step_avg:89.41ms
step:724/1680 train_time:64735ms step_avg:89.41ms
step:725/1680 train_time:64825ms step_avg:89.41ms
step:726/1680 train_time:64915ms step_avg:89.41ms
step:727/1680 train_time:65005ms step_avg:89.42ms
step:728/1680 train_time:65095ms step_avg:89.42ms
step:729/1680 train_time:65185ms step_avg:89.42ms
step:730/1680 train_time:65276ms step_avg:89.42ms
step:731/1680 train_time:65366ms step_avg:89.42ms
step:732/1680 train_time:65456ms step_avg:89.42ms
step:733/1680 train_time:65546ms step_avg:89.42ms
step:734/1680 train_time:65636ms step_avg:89.42ms
step:735/1680 train_time:65726ms step_avg:89.42ms
step:736/1680 train_time:65816ms step_avg:89.42ms
step:737/1680 train_time:65906ms step_avg:89.42ms
step:738/1680 train_time:65995ms step_avg:89.42ms
step:739/1680 train_time:66087ms step_avg:89.43ms
step:740/1680 train_time:66177ms step_avg:89.43ms
step:741/1680 train_time:66267ms step_avg:89.43ms
step:742/1680 train_time:66357ms step_avg:89.43ms
step:743/1680 train_time:66446ms step_avg:89.43ms
step:744/1680 train_time:66536ms step_avg:89.43ms
step:745/1680 train_time:66626ms step_avg:89.43ms
step:746/1680 train_time:66716ms step_avg:89.43ms
step:747/1680 train_time:66805ms step_avg:89.43ms
step:748/1680 train_time:66895ms step_avg:89.43ms
step:749/1680 train_time:66986ms step_avg:89.43ms
step:750/1680 train_time:67076ms step_avg:89.43ms
step:750/1680 val_loss:3.5659 train_time:67168ms step_avg:89.56ms
step:751/1680 train_time:67191ms step_avg:89.47ms
step:752/1680 train_time:67262ms step_avg:89.44ms
step:753/1680 train_time:67359ms step_avg:89.45ms
step:754/1680 train_time:67451ms step_avg:89.46ms
step:755/1680 train_time:67541ms step_avg:89.46ms
step:756/1680 train_time:67630ms step_avg:89.46ms
step:757/1680 train_time:67719ms step_avg:89.46ms
step:758/1680 train_time:67808ms step_avg:89.46ms
step:759/1680 train_time:67898ms step_avg:89.46ms
step:760/1680 train_time:67986ms step_avg:89.46ms
step:761/1680 train_time:68075ms step_avg:89.46ms
step:762/1680 train_time:68165ms step_avg:89.46ms
step:763/1680 train_time:68257ms step_avg:89.46ms
step:764/1680 train_time:68349ms step_avg:89.46ms
step:765/1680 train_time:68443ms step_avg:89.47ms
step:766/1680 train_time:68533ms step_avg:89.47ms
step:767/1680 train_time:68624ms step_avg:89.47ms
step:768/1680 train_time:68714ms step_avg:89.47ms
step:769/1680 train_time:68803ms step_avg:89.47ms
step:770/1680 train_time:68893ms step_avg:89.47ms
step:771/1680 train_time:68981ms step_avg:89.47ms
step:772/1680 train_time:69070ms step_avg:89.47ms
step:773/1680 train_time:69160ms step_avg:89.47ms
step:774/1680 train_time:69250ms step_avg:89.47ms
step:775/1680 train_time:69342ms step_avg:89.47ms
step:776/1680 train_time:69433ms step_avg:89.48ms
step:777/1680 train_time:69525ms step_avg:89.48ms
step:778/1680 train_time:69616ms step_avg:89.48ms
step:779/1680 train_time:69706ms step_avg:89.48ms
step:780/1680 train_time:69795ms step_avg:89.48ms
step:781/1680 train_time:69884ms step_avg:89.48ms
step:782/1680 train_time:69974ms step_avg:89.48ms
step:783/1680 train_time:70064ms step_avg:89.48ms
step:784/1680 train_time:70153ms step_avg:89.48ms
step:785/1680 train_time:70244ms step_avg:89.48ms
step:786/1680 train_time:70334ms step_avg:89.48ms
step:787/1680 train_time:70425ms step_avg:89.49ms
step:788/1680 train_time:70516ms step_avg:89.49ms
step:789/1680 train_time:70607ms step_avg:89.49ms
step:790/1680 train_time:70698ms step_avg:89.49ms
step:791/1680 train_time:70787ms step_avg:89.49ms
step:792/1680 train_time:70877ms step_avg:89.49ms
step:793/1680 train_time:70967ms step_avg:89.49ms
step:794/1680 train_time:71057ms step_avg:89.49ms
step:795/1680 train_time:71146ms step_avg:89.49ms
step:796/1680 train_time:71236ms step_avg:89.49ms
step:797/1680 train_time:71326ms step_avg:89.49ms
step:798/1680 train_time:71417ms step_avg:89.49ms
step:799/1680 train_time:71507ms step_avg:89.50ms
step:800/1680 train_time:71598ms step_avg:89.50ms
step:801/1680 train_time:71688ms step_avg:89.50ms
step:802/1680 train_time:71779ms step_avg:89.50ms
step:803/1680 train_time:71868ms step_avg:89.50ms
step:804/1680 train_time:71958ms step_avg:89.50ms
step:805/1680 train_time:72049ms step_avg:89.50ms
step:806/1680 train_time:72139ms step_avg:89.50ms
step:807/1680 train_time:72228ms step_avg:89.50ms
step:808/1680 train_time:72318ms step_avg:89.50ms
step:809/1680 train_time:72408ms step_avg:89.50ms
step:810/1680 train_time:72499ms step_avg:89.51ms
step:811/1680 train_time:72589ms step_avg:89.51ms
step:812/1680 train_time:72680ms step_avg:89.51ms
step:813/1680 train_time:72769ms step_avg:89.51ms
step:814/1680 train_time:72859ms step_avg:89.51ms
step:815/1680 train_time:72949ms step_avg:89.51ms
step:816/1680 train_time:73039ms step_avg:89.51ms
step:817/1680 train_time:73129ms step_avg:89.51ms
step:818/1680 train_time:73218ms step_avg:89.51ms
step:819/1680 train_time:73308ms step_avg:89.51ms
step:820/1680 train_time:73399ms step_avg:89.51ms
step:821/1680 train_time:73488ms step_avg:89.51ms
step:822/1680 train_time:73579ms step_avg:89.51ms
step:823/1680 train_time:73669ms step_avg:89.51ms
step:824/1680 train_time:73759ms step_avg:89.51ms
step:825/1680 train_time:73848ms step_avg:89.51ms
step:826/1680 train_time:73938ms step_avg:89.51ms
step:827/1680 train_time:74028ms step_avg:89.51ms
step:828/1680 train_time:74119ms step_avg:89.52ms
step:829/1680 train_time:74209ms step_avg:89.52ms
step:830/1680 train_time:74299ms step_avg:89.52ms
step:831/1680 train_time:74389ms step_avg:89.52ms
step:832/1680 train_time:74479ms step_avg:89.52ms
step:833/1680 train_time:74569ms step_avg:89.52ms
step:834/1680 train_time:74659ms step_avg:89.52ms
step:835/1680 train_time:74748ms step_avg:89.52ms
step:836/1680 train_time:74843ms step_avg:89.53ms
step:837/1680 train_time:74928ms step_avg:89.52ms
step:838/1680 train_time:75019ms step_avg:89.52ms
step:839/1680 train_time:75109ms step_avg:89.52ms
step:840/1680 train_time:75199ms step_avg:89.52ms
step:841/1680 train_time:75290ms step_avg:89.52ms
step:842/1680 train_time:75380ms step_avg:89.53ms
step:843/1680 train_time:75470ms step_avg:89.53ms
step:844/1680 train_time:75560ms step_avg:89.53ms
step:845/1680 train_time:75651ms step_avg:89.53ms
step:846/1680 train_time:75742ms step_avg:89.53ms
step:847/1680 train_time:75832ms step_avg:89.53ms
step:848/1680 train_time:75922ms step_avg:89.53ms
step:849/1680 train_time:76013ms step_avg:89.53ms
step:850/1680 train_time:76104ms step_avg:89.53ms
step:851/1680 train_time:76193ms step_avg:89.53ms
step:852/1680 train_time:76284ms step_avg:89.54ms
step:853/1680 train_time:76375ms step_avg:89.54ms
step:854/1680 train_time:76465ms step_avg:89.54ms
step:855/1680 train_time:76555ms step_avg:89.54ms
step:856/1680 train_time:76646ms step_avg:89.54ms
step:857/1680 train_time:76741ms step_avg:89.55ms
step:858/1680 train_time:76827ms step_avg:89.54ms
step:859/1680 train_time:76918ms step_avg:89.54ms
step:860/1680 train_time:77009ms step_avg:89.54ms
step:861/1680 train_time:77098ms step_avg:89.54ms
step:862/1680 train_time:77188ms step_avg:89.55ms
step:863/1680 train_time:77279ms step_avg:89.55ms
step:864/1680 train_time:77369ms step_avg:89.55ms
step:865/1680 train_time:77458ms step_avg:89.55ms
step:866/1680 train_time:77548ms step_avg:89.55ms
step:867/1680 train_time:77639ms step_avg:89.55ms
step:868/1680 train_time:77728ms step_avg:89.55ms
step:869/1680 train_time:77819ms step_avg:89.55ms
step:870/1680 train_time:77909ms step_avg:89.55ms
step:871/1680 train_time:77999ms step_avg:89.55ms
step:872/1680 train_time:78089ms step_avg:89.55ms
step:873/1680 train_time:78178ms step_avg:89.55ms
step:874/1680 train_time:78268ms step_avg:89.55ms
step:875/1680 train_time:78358ms step_avg:89.55ms
step:875/1680 val_loss:3.5196 train_time:78450ms step_avg:89.66ms
step:876/1680 train_time:78473ms step_avg:89.58ms
step:877/1680 train_time:78545ms step_avg:89.56ms
step:878/1680 train_time:78640ms step_avg:89.57ms
step:879/1680 train_time:78733ms step_avg:89.57ms
step:880/1680 train_time:78823ms step_avg:89.57ms
step:881/1680 train_time:78912ms step_avg:89.57ms
step:882/1680 train_time:79001ms step_avg:89.57ms
step:883/1680 train_time:79089ms step_avg:89.57ms
step:884/1680 train_time:79178ms step_avg:89.57ms
step:885/1680 train_time:79267ms step_avg:89.57ms
step:886/1680 train_time:79356ms step_avg:89.57ms
step:887/1680 train_time:79447ms step_avg:89.57ms
step:888/1680 train_time:79539ms step_avg:89.57ms
step:889/1680 train_time:79633ms step_avg:89.58ms
step:890/1680 train_time:79725ms step_avg:89.58ms
step:891/1680 train_time:79815ms step_avg:89.58ms
step:892/1680 train_time:79906ms step_avg:89.58ms
step:893/1680 train_time:79995ms step_avg:89.58ms
step:894/1680 train_time:80085ms step_avg:89.58ms
step:895/1680 train_time:80177ms step_avg:89.58ms
step:896/1680 train_time:80264ms step_avg:89.58ms
step:897/1680 train_time:80353ms step_avg:89.58ms
step:898/1680 train_time:80444ms step_avg:89.58ms
step:899/1680 train_time:80536ms step_avg:89.58ms
step:900/1680 train_time:80628ms step_avg:89.59ms
step:901/1680 train_time:80720ms step_avg:89.59ms
step:902/1680 train_time:80810ms step_avg:89.59ms
step:903/1680 train_time:80900ms step_avg:89.59ms
step:904/1680 train_time:80989ms step_avg:89.59ms
step:905/1680 train_time:81078ms step_avg:89.59ms
step:906/1680 train_time:81168ms step_avg:89.59ms
step:907/1680 train_time:81258ms step_avg:89.59ms
step:908/1680 train_time:81348ms step_avg:89.59ms
step:909/1680 train_time:81438ms step_avg:89.59ms
step:910/1680 train_time:81529ms step_avg:89.59ms
step:911/1680 train_time:81621ms step_avg:89.59ms
step:912/1680 train_time:81712ms step_avg:89.60ms
step:913/1680 train_time:81804ms step_avg:89.60ms
step:914/1680 train_time:81894ms step_avg:89.60ms
step:915/1680 train_time:81985ms step_avg:89.60ms
step:916/1680 train_time:82075ms step_avg:89.60ms
step:917/1680 train_time:82165ms step_avg:89.60ms
step:918/1680 train_time:82254ms step_avg:89.60ms
step:919/1680 train_time:82344ms step_avg:89.60ms
step:920/1680 train_time:82434ms step_avg:89.60ms
step:921/1680 train_time:82524ms step_avg:89.60ms
step:922/1680 train_time:82614ms step_avg:89.60ms
step:923/1680 train_time:82705ms step_avg:89.61ms
step:924/1680 train_time:82796ms step_avg:89.61ms
step:925/1680 train_time:82887ms step_avg:89.61ms
step:926/1680 train_time:82978ms step_avg:89.61ms
step:927/1680 train_time:83068ms step_avg:89.61ms
step:928/1680 train_time:83157ms step_avg:89.61ms
step:929/1680 train_time:83248ms step_avg:89.61ms
step:930/1680 train_time:83339ms step_avg:89.61ms
step:931/1680 train_time:83429ms step_avg:89.61ms
step:932/1680 train_time:83519ms step_avg:89.61ms
step:933/1680 train_time:83609ms step_avg:89.61ms
step:934/1680 train_time:83699ms step_avg:89.61ms
step:935/1680 train_time:83790ms step_avg:89.61ms
step:936/1680 train_time:83880ms step_avg:89.62ms
step:937/1680 train_time:83970ms step_avg:89.62ms
step:938/1680 train_time:84059ms step_avg:89.62ms
step:939/1680 train_time:84150ms step_avg:89.62ms
step:940/1680 train_time:84240ms step_avg:89.62ms
step:941/1680 train_time:84331ms step_avg:89.62ms
step:942/1680 train_time:84420ms step_avg:89.62ms
step:943/1680 train_time:84510ms step_avg:89.62ms
step:944/1680 train_time:84601ms step_avg:89.62ms
step:945/1680 train_time:84691ms step_avg:89.62ms
step:946/1680 train_time:84783ms step_avg:89.62ms
step:947/1680 train_time:84874ms step_avg:89.62ms
step:948/1680 train_time:84963ms step_avg:89.62ms
step:949/1680 train_time:85053ms step_avg:89.62ms
step:950/1680 train_time:85143ms step_avg:89.62ms
step:951/1680 train_time:85233ms step_avg:89.62ms
step:952/1680 train_time:85323ms step_avg:89.63ms
step:953/1680 train_time:85414ms step_avg:89.63ms
step:954/1680 train_time:85503ms step_avg:89.63ms
step:955/1680 train_time:85594ms step_avg:89.63ms
step:956/1680 train_time:85685ms step_avg:89.63ms
step:957/1680 train_time:85775ms step_avg:89.63ms
step:958/1680 train_time:85867ms step_avg:89.63ms
step:959/1680 train_time:85957ms step_avg:89.63ms
step:960/1680 train_time:86048ms step_avg:89.63ms
step:961/1680 train_time:86138ms step_avg:89.63ms
step:962/1680 train_time:86227ms step_avg:89.63ms
step:963/1680 train_time:86318ms step_avg:89.63ms
step:964/1680 train_time:86407ms step_avg:89.63ms
step:965/1680 train_time:86499ms step_avg:89.64ms
step:966/1680 train_time:86588ms step_avg:89.64ms
step:967/1680 train_time:86678ms step_avg:89.64ms
step:968/1680 train_time:86769ms step_avg:89.64ms
step:969/1680 train_time:86860ms step_avg:89.64ms
step:970/1680 train_time:86950ms step_avg:89.64ms
step:971/1680 train_time:87040ms step_avg:89.64ms
step:972/1680 train_time:87131ms step_avg:89.64ms
step:973/1680 train_time:87221ms step_avg:89.64ms
step:974/1680 train_time:87310ms step_avg:89.64ms
step:975/1680 train_time:87400ms step_avg:89.64ms
step:976/1680 train_time:87489ms step_avg:89.64ms
step:977/1680 train_time:87579ms step_avg:89.64ms
step:978/1680 train_time:87669ms step_avg:89.64ms
step:979/1680 train_time:87759ms step_avg:89.64ms
step:980/1680 train_time:87849ms step_avg:89.64ms
step:981/1680 train_time:87939ms step_avg:89.64ms
step:982/1680 train_time:88029ms step_avg:89.64ms
step:983/1680 train_time:88119ms step_avg:89.64ms
step:984/1680 train_time:88209ms step_avg:89.64ms
step:985/1680 train_time:88299ms step_avg:89.64ms
step:986/1680 train_time:88389ms step_avg:89.64ms
step:987/1680 train_time:88480ms step_avg:89.65ms
step:988/1680 train_time:88569ms step_avg:89.65ms
step:989/1680 train_time:88660ms step_avg:89.65ms
step:990/1680 train_time:88750ms step_avg:89.65ms
step:991/1680 train_time:88841ms step_avg:89.65ms
step:992/1680 train_time:88931ms step_avg:89.65ms
step:993/1680 train_time:89021ms step_avg:89.65ms
step:994/1680 train_time:89111ms step_avg:89.65ms
step:995/1680 train_time:89201ms step_avg:89.65ms
step:996/1680 train_time:89292ms step_avg:89.65ms
step:997/1680 train_time:89381ms step_avg:89.65ms
step:998/1680 train_time:89471ms step_avg:89.65ms
step:999/1680 train_time:89560ms step_avg:89.65ms
step:1000/1680 train_time:89651ms step_avg:89.65ms
step:1000/1680 val_loss:3.4696 train_time:89742ms step_avg:89.74ms
step:1001/1680 train_time:89765ms step_avg:89.68ms
step:1002/1680 train_time:89837ms step_avg:89.66ms
step:1003/1680 train_time:89932ms step_avg:89.66ms
step:1004/1680 train_time:90023ms step_avg:89.66ms
step:1005/1680 train_time:90113ms step_avg:89.67ms
step:1006/1680 train_time:90203ms step_avg:89.66ms
step:1007/1680 train_time:90292ms step_avg:89.66ms
step:1008/1680 train_time:90382ms step_avg:89.66ms
step:1009/1680 train_time:90470ms step_avg:89.66ms
step:1010/1680 train_time:90559ms step_avg:89.66ms
step:1011/1680 train_time:90648ms step_avg:89.66ms
step:1012/1680 train_time:90738ms step_avg:89.66ms
step:1013/1680 train_time:90830ms step_avg:89.66ms
step:1014/1680 train_time:90922ms step_avg:89.67ms
step:1015/1680 train_time:91013ms step_avg:89.67ms
step:1016/1680 train_time:91104ms step_avg:89.67ms
step:1017/1680 train_time:91195ms step_avg:89.67ms
step:1018/1680 train_time:91285ms step_avg:89.67ms
step:1019/1680 train_time:91375ms step_avg:89.67ms
step:1020/1680 train_time:91464ms step_avg:89.67ms
step:1021/1680 train_time:91553ms step_avg:89.67ms
step:1022/1680 train_time:91643ms step_avg:89.67ms
step:1023/1680 train_time:91733ms step_avg:89.67ms
step:1024/1680 train_time:91824ms step_avg:89.67ms
step:1025/1680 train_time:91915ms step_avg:89.67ms
step:1026/1680 train_time:92007ms step_avg:89.68ms
step:1027/1680 train_time:92098ms step_avg:89.68ms
step:1028/1680 train_time:92189ms step_avg:89.68ms
step:1029/1680 train_time:92279ms step_avg:89.68ms
step:1030/1680 train_time:92369ms step_avg:89.68ms
step:1031/1680 train_time:92457ms step_avg:89.68ms
step:1032/1680 train_time:92547ms step_avg:89.68ms
step:1033/1680 train_time:92637ms step_avg:89.68ms
step:1034/1680 train_time:92727ms step_avg:89.68ms
step:1035/1680 train_time:92816ms step_avg:89.68ms
step:1036/1680 train_time:92907ms step_avg:89.68ms
step:1037/1680 train_time:92998ms step_avg:89.68ms
step:1038/1680 train_time:93089ms step_avg:89.68ms
step:1039/1680 train_time:93180ms step_avg:89.68ms
step:1040/1680 train_time:93270ms step_avg:89.68ms
step:1041/1680 train_time:93360ms step_avg:89.68ms
step:1042/1680 train_time:93450ms step_avg:89.68ms
step:1043/1680 train_time:93539ms step_avg:89.68ms
step:1044/1680 train_time:93630ms step_avg:89.68ms
step:1045/1680 train_time:93719ms step_avg:89.68ms
step:1046/1680 train_time:93810ms step_avg:89.68ms
step:1047/1680 train_time:93900ms step_avg:89.68ms
step:1048/1680 train_time:93991ms step_avg:89.69ms
step:1049/1680 train_time:94081ms step_avg:89.69ms
step:1050/1680 train_time:94172ms step_avg:89.69ms
step:1051/1680 train_time:94262ms step_avg:89.69ms
step:1052/1680 train_time:94352ms step_avg:89.69ms
step:1053/1680 train_time:94442ms step_avg:89.69ms
step:1054/1680 train_time:94532ms step_avg:89.69ms
step:1055/1680 train_time:94621ms step_avg:89.69ms
step:1056/1680 train_time:94712ms step_avg:89.69ms
step:1057/1680 train_time:94802ms step_avg:89.69ms
step:1058/1680 train_time:94893ms step_avg:89.69ms
step:1059/1680 train_time:94983ms step_avg:89.69ms
step:1060/1680 train_time:95074ms step_avg:89.69ms
step:1061/1680 train_time:95165ms step_avg:89.69ms
step:1062/1680 train_time:95257ms step_avg:89.70ms
step:1063/1680 train_time:95346ms step_avg:89.69ms
step:1064/1680 train_time:95435ms step_avg:89.69ms
step:1065/1680 train_time:95525ms step_avg:89.70ms
step:1066/1680 train_time:95615ms step_avg:89.70ms
step:1067/1680 train_time:95705ms step_avg:89.70ms
step:1068/1680 train_time:95796ms step_avg:89.70ms
step:1069/1680 train_time:95886ms step_avg:89.70ms
step:1070/1680 train_time:95976ms step_avg:89.70ms
step:1071/1680 train_time:96067ms step_avg:89.70ms
step:1072/1680 train_time:96158ms step_avg:89.70ms
step:1073/1680 train_time:96248ms step_avg:89.70ms
step:1074/1680 train_time:96337ms step_avg:89.70ms
step:1075/1680 train_time:96428ms step_avg:89.70ms
step:1076/1680 train_time:96517ms step_avg:89.70ms
step:1077/1680 train_time:96607ms step_avg:89.70ms
step:1078/1680 train_time:96697ms step_avg:89.70ms
step:1079/1680 train_time:96787ms step_avg:89.70ms
step:1080/1680 train_time:96877ms step_avg:89.70ms
step:1081/1680 train_time:96967ms step_avg:89.70ms
step:1082/1680 train_time:97057ms step_avg:89.70ms
step:1083/1680 train_time:97151ms step_avg:89.71ms
step:1084/1680 train_time:97238ms step_avg:89.70ms
step:1085/1680 train_time:97328ms step_avg:89.70ms
step:1086/1680 train_time:97418ms step_avg:89.70ms
step:1087/1680 train_time:97508ms step_avg:89.70ms
step:1088/1680 train_time:97598ms step_avg:89.70ms
step:1089/1680 train_time:97687ms step_avg:89.70ms
step:1090/1680 train_time:97777ms step_avg:89.70ms
step:1091/1680 train_time:97868ms step_avg:89.71ms
step:1092/1680 train_time:97958ms step_avg:89.71ms
step:1093/1680 train_time:98049ms step_avg:89.71ms
step:1094/1680 train_time:98139ms step_avg:89.71ms
step:1095/1680 train_time:98231ms step_avg:89.71ms
step:1096/1680 train_time:98321ms step_avg:89.71ms
step:1097/1680 train_time:98411ms step_avg:89.71ms
step:1098/1680 train_time:98501ms step_avg:89.71ms
step:1099/1680 train_time:98592ms step_avg:89.71ms
step:1100/1680 train_time:98682ms step_avg:89.71ms
step:1101/1680 train_time:98773ms step_avg:89.71ms
step:1102/1680 train_time:98865ms step_avg:89.71ms
step:1103/1680 train_time:98956ms step_avg:89.72ms
step:1104/1680 train_time:99047ms step_avg:89.72ms
step:1105/1680 train_time:99139ms step_avg:89.72ms
step:1106/1680 train_time:99231ms step_avg:89.72ms
step:1107/1680 train_time:99321ms step_avg:89.72ms
step:1108/1680 train_time:99412ms step_avg:89.72ms
step:1109/1680 train_time:99502ms step_avg:89.72ms
step:1110/1680 train_time:99592ms step_avg:89.72ms
step:1111/1680 train_time:99683ms step_avg:89.72ms
step:1112/1680 train_time:99773ms step_avg:89.72ms
step:1113/1680 train_time:99864ms step_avg:89.73ms
step:1114/1680 train_time:99955ms step_avg:89.73ms
step:1115/1680 train_time:100047ms step_avg:89.73ms
step:1116/1680 train_time:100138ms step_avg:89.73ms
step:1117/1680 train_time:100230ms step_avg:89.73ms
step:1118/1680 train_time:100320ms step_avg:89.73ms
step:1119/1680 train_time:100411ms step_avg:89.73ms
step:1120/1680 train_time:100501ms step_avg:89.73ms
step:1121/1680 train_time:100592ms step_avg:89.73ms
step:1122/1680 train_time:100682ms step_avg:89.73ms
step:1123/1680 train_time:100773ms step_avg:89.74ms
step:1124/1680 train_time:100863ms step_avg:89.74ms
step:1125/1680 train_time:100953ms step_avg:89.74ms
step:1125/1680 val_loss:3.4156 train_time:101046ms step_avg:89.82ms
step:1126/1680 train_time:101069ms step_avg:89.76ms
step:1127/1680 train_time:101140ms step_avg:89.74ms
step:1128/1680 train_time:101238ms step_avg:89.75ms
step:1129/1680 train_time:101330ms step_avg:89.75ms
step:1130/1680 train_time:101421ms step_avg:89.75ms
step:1131/1680 train_time:101511ms step_avg:89.75ms
step:1132/1680 train_time:101601ms step_avg:89.75ms
step:1133/1680 train_time:101691ms step_avg:89.75ms
step:1134/1680 train_time:101781ms step_avg:89.75ms
step:1135/1680 train_time:101871ms step_avg:89.75ms
step:1136/1680 train_time:101961ms step_avg:89.75ms
step:1137/1680 train_time:102053ms step_avg:89.76ms
step:1138/1680 train_time:102147ms step_avg:89.76ms
step:1139/1680 train_time:102241ms step_avg:89.76ms
step:1140/1680 train_time:102332ms step_avg:89.77ms
step:1141/1680 train_time:102423ms step_avg:89.77ms
step:1142/1680 train_time:102514ms step_avg:89.77ms
step:1143/1680 train_time:102604ms step_avg:89.77ms
step:1144/1680 train_time:102694ms step_avg:89.77ms
step:1145/1680 train_time:102783ms step_avg:89.77ms
step:1146/1680 train_time:102873ms step_avg:89.77ms
step:1147/1680 train_time:102963ms step_avg:89.77ms
step:1148/1680 train_time:103055ms step_avg:89.77ms
step:1149/1680 train_time:103147ms step_avg:89.77ms
step:1150/1680 train_time:103241ms step_avg:89.77ms
step:1151/1680 train_time:103332ms step_avg:89.78ms
step:1152/1680 train_time:103423ms step_avg:89.78ms
step:1153/1680 train_time:103514ms step_avg:89.78ms
step:1154/1680 train_time:103604ms step_avg:89.78ms
step:1155/1680 train_time:103694ms step_avg:89.78ms
step:1156/1680 train_time:103784ms step_avg:89.78ms
step:1157/1680 train_time:103874ms step_avg:89.78ms
step:1158/1680 train_time:103965ms step_avg:89.78ms
step:1159/1680 train_time:104055ms step_avg:89.78ms
step:1160/1680 train_time:104147ms step_avg:89.78ms
step:1161/1680 train_time:104241ms step_avg:89.79ms
step:1162/1680 train_time:104331ms step_avg:89.79ms
step:1163/1680 train_time:104422ms step_avg:89.79ms
step:1164/1680 train_time:104513ms step_avg:89.79ms
step:1165/1680 train_time:104604ms step_avg:89.79ms
step:1166/1680 train_time:104694ms step_avg:89.79ms
step:1167/1680 train_time:104784ms step_avg:89.79ms
step:1168/1680 train_time:104874ms step_avg:89.79ms
step:1169/1680 train_time:104965ms step_avg:89.79ms
step:1170/1680 train_time:105055ms step_avg:89.79ms
step:1171/1680 train_time:105147ms step_avg:89.79ms
step:1172/1680 train_time:105237ms step_avg:89.79ms
step:1173/1680 train_time:105329ms step_avg:89.79ms
step:1174/1680 train_time:105420ms step_avg:89.80ms
step:1175/1680 train_time:105511ms step_avg:89.80ms
step:1176/1680 train_time:105601ms step_avg:89.80ms
step:1177/1680 train_time:105692ms step_avg:89.80ms
step:1178/1680 train_time:105782ms step_avg:89.80ms
step:1179/1680 train_time:105873ms step_avg:89.80ms
step:1180/1680 train_time:105963ms step_avg:89.80ms
step:1181/1680 train_time:106054ms step_avg:89.80ms
step:1182/1680 train_time:106145ms step_avg:89.80ms
step:1183/1680 train_time:106236ms step_avg:89.80ms
step:1184/1680 train_time:106329ms step_avg:89.80ms
step:1185/1680 train_time:106419ms step_avg:89.81ms
step:1186/1680 train_time:106510ms step_avg:89.81ms
step:1187/1680 train_time:106601ms step_avg:89.81ms
step:1188/1680 train_time:106692ms step_avg:89.81ms
step:1189/1680 train_time:106783ms step_avg:89.81ms
step:1190/1680 train_time:106874ms step_avg:89.81ms
step:1191/1680 train_time:106964ms step_avg:89.81ms
step:1192/1680 train_time:107054ms step_avg:89.81ms
step:1193/1680 train_time:107145ms step_avg:89.81ms
step:1194/1680 train_time:107235ms step_avg:89.81ms
step:1195/1680 train_time:107326ms step_avg:89.81ms
step:1196/1680 train_time:107419ms step_avg:89.81ms
step:1197/1680 train_time:107507ms step_avg:89.81ms
step:1198/1680 train_time:107598ms step_avg:89.81ms
step:1199/1680 train_time:107688ms step_avg:89.81ms
step:1200/1680 train_time:107779ms step_avg:89.82ms
step:1201/1680 train_time:107870ms step_avg:89.82ms
step:1202/1680 train_time:107960ms step_avg:89.82ms
step:1203/1680 train_time:108051ms step_avg:89.82ms
step:1204/1680 train_time:108141ms step_avg:89.82ms
step:1205/1680 train_time:108232ms step_avg:89.82ms
step:1206/1680 train_time:108323ms step_avg:89.82ms
step:1207/1680 train_time:108414ms step_avg:89.82ms
step:1208/1680 train_time:108505ms step_avg:89.82ms
step:1209/1680 train_time:108595ms step_avg:89.82ms
step:1210/1680 train_time:108686ms step_avg:89.82ms
step:1211/1680 train_time:108776ms step_avg:89.82ms
step:1212/1680 train_time:108868ms step_avg:89.83ms
step:1213/1680 train_time:108958ms step_avg:89.83ms
step:1214/1680 train_time:109049ms step_avg:89.83ms
step:1215/1680 train_time:109140ms step_avg:89.83ms
step:1216/1680 train_time:109231ms step_avg:89.83ms
step:1217/1680 train_time:109322ms step_avg:89.83ms
step:1218/1680 train_time:109413ms step_avg:89.83ms
step:1219/1680 train_time:109504ms step_avg:89.83ms
step:1220/1680 train_time:109595ms step_avg:89.83ms
step:1221/1680 train_time:109686ms step_avg:89.83ms
step:1222/1680 train_time:109776ms step_avg:89.83ms
step:1223/1680 train_time:109867ms step_avg:89.83ms
step:1224/1680 train_time:109958ms step_avg:89.83ms
step:1225/1680 train_time:110049ms step_avg:89.84ms
step:1226/1680 train_time:110139ms step_avg:89.84ms
step:1227/1680 train_time:110230ms step_avg:89.84ms
step:1228/1680 train_time:110323ms step_avg:89.84ms
step:1229/1680 train_time:110413ms step_avg:89.84ms
step:1230/1680 train_time:110503ms step_avg:89.84ms
step:1231/1680 train_time:110594ms step_avg:89.84ms
step:1232/1680 train_time:110685ms step_avg:89.84ms
step:1233/1680 train_time:110776ms step_avg:89.84ms
step:1234/1680 train_time:110868ms step_avg:89.84ms
step:1235/1680 train_time:110959ms step_avg:89.85ms
step:1236/1680 train_time:111049ms step_avg:89.85ms
step:1237/1680 train_time:111140ms step_avg:89.85ms
step:1238/1680 train_time:111231ms step_avg:89.85ms
step:1239/1680 train_time:111322ms step_avg:89.85ms
step:1240/1680 train_time:111413ms step_avg:89.85ms
step:1241/1680 train_time:111504ms step_avg:89.85ms
step:1242/1680 train_time:111594ms step_avg:89.85ms
step:1243/1680 train_time:111686ms step_avg:89.85ms
step:1244/1680 train_time:111776ms step_avg:89.85ms
step:1245/1680 train_time:111867ms step_avg:89.85ms
step:1246/1680 train_time:111957ms step_avg:89.85ms
step:1247/1680 train_time:112047ms step_avg:89.85ms
step:1248/1680 train_time:112138ms step_avg:89.85ms
step:1249/1680 train_time:112228ms step_avg:89.85ms
step:1250/1680 train_time:112318ms step_avg:89.85ms
step:1250/1680 val_loss:3.3782 train_time:112410ms step_avg:89.93ms
step:1251/1680 train_time:112433ms step_avg:89.87ms
step:1252/1680 train_time:112506ms step_avg:89.86ms
step:1253/1680 train_time:112600ms step_avg:89.86ms
step:1254/1680 train_time:112691ms step_avg:89.87ms
step:1255/1680 train_time:112782ms step_avg:89.87ms
step:1256/1680 train_time:112872ms step_avg:89.87ms
step:1257/1680 train_time:112961ms step_avg:89.87ms
step:1258/1680 train_time:113052ms step_avg:89.87ms
step:1259/1680 train_time:113142ms step_avg:89.87ms
step:1260/1680 train_time:113232ms step_avg:89.87ms
step:1261/1680 train_time:113323ms step_avg:89.87ms
step:1262/1680 train_time:113417ms step_avg:89.87ms
step:1263/1680 train_time:113510ms step_avg:89.87ms
step:1264/1680 train_time:113603ms step_avg:89.88ms
step:1265/1680 train_time:113695ms step_avg:89.88ms
step:1266/1680 train_time:113786ms step_avg:89.88ms
step:1267/1680 train_time:113875ms step_avg:89.88ms
step:1268/1680 train_time:113966ms step_avg:89.88ms
step:1269/1680 train_time:114055ms step_avg:89.88ms
step:1270/1680 train_time:114145ms step_avg:89.88ms
step:1271/1680 train_time:114234ms step_avg:89.88ms
step:1272/1680 train_time:114324ms step_avg:89.88ms
step:1273/1680 train_time:114417ms step_avg:89.88ms
step:1274/1680 train_time:114510ms step_avg:89.88ms
step:1275/1680 train_time:114603ms step_avg:89.88ms
step:1276/1680 train_time:114695ms step_avg:89.89ms
step:1277/1680 train_time:114785ms step_avg:89.89ms
step:1278/1680 train_time:114875ms step_avg:89.89ms
step:1279/1680 train_time:114965ms step_avg:89.89ms
step:1280/1680 train_time:115056ms step_avg:89.89ms
step:1281/1680 train_time:115146ms step_avg:89.89ms
step:1282/1680 train_time:115237ms step_avg:89.89ms
step:1283/1680 train_time:115327ms step_avg:89.89ms
step:1284/1680 train_time:115419ms step_avg:89.89ms
step:1285/1680 train_time:115511ms step_avg:89.89ms
step:1286/1680 train_time:115602ms step_avg:89.89ms
step:1287/1680 train_time:115693ms step_avg:89.89ms
step:1288/1680 train_time:115784ms step_avg:89.89ms
step:1289/1680 train_time:115875ms step_avg:89.89ms
step:1290/1680 train_time:115965ms step_avg:89.90ms
step:1291/1680 train_time:116055ms step_avg:89.90ms
step:1292/1680 train_time:116146ms step_avg:89.90ms
step:1293/1680 train_time:116237ms step_avg:89.90ms
step:1294/1680 train_time:116327ms step_avg:89.90ms
step:1295/1680 train_time:116419ms step_avg:89.90ms
step:1296/1680 train_time:116511ms step_avg:89.90ms
step:1297/1680 train_time:116602ms step_avg:89.90ms
step:1298/1680 train_time:116693ms step_avg:89.90ms
step:1299/1680 train_time:116785ms step_avg:89.90ms
step:1300/1680 train_time:116875ms step_avg:89.90ms
step:1301/1680 train_time:116965ms step_avg:89.90ms
step:1302/1680 train_time:117055ms step_avg:89.90ms
step:1303/1680 train_time:117145ms step_avg:89.90ms
step:1304/1680 train_time:117236ms step_avg:89.90ms
step:1305/1680 train_time:117326ms step_avg:89.91ms
step:1306/1680 train_time:117419ms step_avg:89.91ms
step:1307/1680 train_time:117510ms step_avg:89.91ms
step:1308/1680 train_time:117602ms step_avg:89.91ms
step:1309/1680 train_time:117694ms step_avg:89.91ms
step:1310/1680 train_time:117785ms step_avg:89.91ms
step:1311/1680 train_time:117875ms step_avg:89.91ms
step:1312/1680 train_time:117966ms step_avg:89.91ms
step:1313/1680 train_time:118056ms step_avg:89.91ms
step:1314/1680 train_time:118147ms step_avg:89.91ms
step:1315/1680 train_time:118238ms step_avg:89.92ms
step:1316/1680 train_time:118328ms step_avg:89.92ms
step:1317/1680 train_time:118420ms step_avg:89.92ms
step:1318/1680 train_time:118513ms step_avg:89.92ms
step:1319/1680 train_time:118605ms step_avg:89.92ms
step:1320/1680 train_time:118697ms step_avg:89.92ms
step:1321/1680 train_time:118787ms step_avg:89.92ms
step:1322/1680 train_time:118878ms step_avg:89.92ms
step:1323/1680 train_time:118969ms step_avg:89.92ms
step:1324/1680 train_time:119060ms step_avg:89.92ms
step:1325/1680 train_time:119151ms step_avg:89.93ms
step:1326/1680 train_time:119242ms step_avg:89.93ms
step:1327/1680 train_time:119332ms step_avg:89.93ms
step:1328/1680 train_time:119422ms step_avg:89.93ms
step:1329/1680 train_time:119513ms step_avg:89.93ms
step:1330/1680 train_time:119604ms step_avg:89.93ms
step:1331/1680 train_time:119695ms step_avg:89.93ms
step:1332/1680 train_time:119786ms step_avg:89.93ms
step:1333/1680 train_time:119878ms step_avg:89.93ms
step:1334/1680 train_time:119969ms step_avg:89.93ms
step:1335/1680 train_time:120059ms step_avg:89.93ms
step:1336/1680 train_time:120150ms step_avg:89.93ms
step:1337/1680 train_time:120242ms step_avg:89.93ms
step:1338/1680 train_time:120333ms step_avg:89.93ms
step:1339/1680 train_time:120423ms step_avg:89.94ms
step:1340/1680 train_time:120515ms step_avg:89.94ms
step:1341/1680 train_time:120604ms step_avg:89.94ms
step:1342/1680 train_time:120696ms step_avg:89.94ms
step:1343/1680 train_time:120786ms step_avg:89.94ms
step:1344/1680 train_time:120878ms step_avg:89.94ms
step:1345/1680 train_time:120970ms step_avg:89.94ms
step:1346/1680 train_time:121060ms step_avg:89.94ms
step:1347/1680 train_time:121151ms step_avg:89.94ms
step:1348/1680 train_time:121242ms step_avg:89.94ms
step:1349/1680 train_time:121333ms step_avg:89.94ms
step:1350/1680 train_time:121424ms step_avg:89.94ms
step:1351/1680 train_time:121515ms step_avg:89.94ms
step:1352/1680 train_time:121605ms step_avg:89.94ms
step:1353/1680 train_time:121696ms step_avg:89.95ms
step:1354/1680 train_time:121787ms step_avg:89.95ms
step:1355/1680 train_time:121878ms step_avg:89.95ms
step:1356/1680 train_time:121968ms step_avg:89.95ms
step:1357/1680 train_time:122058ms step_avg:89.95ms
step:1358/1680 train_time:122150ms step_avg:89.95ms
step:1359/1680 train_time:122241ms step_avg:89.95ms
step:1360/1680 train_time:122331ms step_avg:89.95ms
step:1361/1680 train_time:122421ms step_avg:89.95ms
step:1362/1680 train_time:122512ms step_avg:89.95ms
step:1363/1680 train_time:122603ms step_avg:89.95ms
step:1364/1680 train_time:122694ms step_avg:89.95ms
step:1365/1680 train_time:122784ms step_avg:89.95ms
step:1366/1680 train_time:122875ms step_avg:89.95ms
step:1367/1680 train_time:122966ms step_avg:89.95ms
step:1368/1680 train_time:123057ms step_avg:89.95ms
step:1369/1680 train_time:123148ms step_avg:89.95ms
step:1370/1680 train_time:123239ms step_avg:89.96ms
step:1371/1680 train_time:123330ms step_avg:89.96ms
step:1372/1680 train_time:123421ms step_avg:89.96ms
step:1373/1680 train_time:123512ms step_avg:89.96ms
step:1374/1680 train_time:123603ms step_avg:89.96ms
step:1375/1680 train_time:123694ms step_avg:89.96ms
step:1375/1680 val_loss:3.3428 train_time:123786ms step_avg:90.03ms
step:1376/1680 train_time:123809ms step_avg:89.98ms
step:1377/1680 train_time:123881ms step_avg:89.96ms
step:1378/1680 train_time:123978ms step_avg:89.97ms
step:1379/1680 train_time:124069ms step_avg:89.97ms
step:1380/1680 train_time:124158ms step_avg:89.97ms
step:1381/1680 train_time:124248ms step_avg:89.97ms
step:1382/1680 train_time:124337ms step_avg:89.97ms
step:1383/1680 train_time:124427ms step_avg:89.97ms
step:1384/1680 train_time:124516ms step_avg:89.97ms
step:1385/1680 train_time:124606ms step_avg:89.97ms
step:1386/1680 train_time:124696ms step_avg:89.97ms
step:1387/1680 train_time:124787ms step_avg:89.97ms
step:1388/1680 train_time:124881ms step_avg:89.97ms
step:1389/1680 train_time:124974ms step_avg:89.97ms
step:1390/1680 train_time:125066ms step_avg:89.98ms
step:1391/1680 train_time:125157ms step_avg:89.98ms
step:1392/1680 train_time:125247ms step_avg:89.98ms
step:1393/1680 train_time:125337ms step_avg:89.98ms
step:1394/1680 train_time:125427ms step_avg:89.98ms
step:1395/1680 train_time:125517ms step_avg:89.98ms
step:1396/1680 train_time:125607ms step_avg:89.98ms
step:1397/1680 train_time:125696ms step_avg:89.98ms
step:1398/1680 train_time:125787ms step_avg:89.98ms
step:1399/1680 train_time:125879ms step_avg:89.98ms
step:1400/1680 train_time:125971ms step_avg:89.98ms
step:1401/1680 train_time:126063ms step_avg:89.98ms
step:1402/1680 train_time:126155ms step_avg:89.98ms
step:1403/1680 train_time:126245ms step_avg:89.98ms
step:1404/1680 train_time:126336ms step_avg:89.98ms
step:1405/1680 train_time:126426ms step_avg:89.98ms
step:1406/1680 train_time:126515ms step_avg:89.98ms
step:1407/1680 train_time:126605ms step_avg:89.98ms
step:1408/1680 train_time:126696ms step_avg:89.98ms
step:1409/1680 train_time:126788ms step_avg:89.98ms
step:1410/1680 train_time:126879ms step_avg:89.99ms
step:1411/1680 train_time:126970ms step_avg:89.99ms
step:1412/1680 train_time:127062ms step_avg:89.99ms
step:1413/1680 train_time:127154ms step_avg:89.99ms
step:1414/1680 train_time:127245ms step_avg:89.99ms
step:1415/1680 train_time:127335ms step_avg:89.99ms
step:1416/1680 train_time:127425ms step_avg:89.99ms
step:1417/1680 train_time:127515ms step_avg:89.99ms
step:1418/1680 train_time:127608ms step_avg:89.99ms
step:1419/1680 train_time:127695ms step_avg:89.99ms
step:1420/1680 train_time:127786ms step_avg:89.99ms
step:1421/1680 train_time:127878ms step_avg:89.99ms
step:1422/1680 train_time:127969ms step_avg:89.99ms
step:1423/1680 train_time:128060ms step_avg:89.99ms
step:1424/1680 train_time:128151ms step_avg:89.99ms
step:1425/1680 train_time:128242ms step_avg:89.99ms
step:1426/1680 train_time:128333ms step_avg:89.99ms
step:1427/1680 train_time:128423ms step_avg:90.00ms
step:1428/1680 train_time:128513ms step_avg:90.00ms
step:1429/1680 train_time:128604ms step_avg:90.00ms
step:1430/1680 train_time:128695ms step_avg:90.00ms
step:1431/1680 train_time:128786ms step_avg:90.00ms
step:1432/1680 train_time:128877ms step_avg:90.00ms
step:1433/1680 train_time:128968ms step_avg:90.00ms
step:1434/1680 train_time:129059ms step_avg:90.00ms
step:1435/1680 train_time:129150ms step_avg:90.00ms
step:1436/1680 train_time:129242ms step_avg:90.00ms
step:1437/1680 train_time:129333ms step_avg:90.00ms
step:1438/1680 train_time:129423ms step_avg:90.00ms
step:1439/1680 train_time:129513ms step_avg:90.00ms
step:1440/1680 train_time:129603ms step_avg:90.00ms
step:1441/1680 train_time:129693ms step_avg:90.00ms
step:1442/1680 train_time:129785ms step_avg:90.00ms
step:1443/1680 train_time:129876ms step_avg:90.00ms
step:1444/1680 train_time:129967ms step_avg:90.00ms
step:1445/1680 train_time:130059ms step_avg:90.01ms
step:1446/1680 train_time:130151ms step_avg:90.01ms
step:1447/1680 train_time:130243ms step_avg:90.01ms
step:1448/1680 train_time:130333ms step_avg:90.01ms
step:1449/1680 train_time:130424ms step_avg:90.01ms
step:1450/1680 train_time:130514ms step_avg:90.01ms
step:1451/1680 train_time:130607ms step_avg:90.01ms
step:1452/1680 train_time:130694ms step_avg:90.01ms
step:1453/1680 train_time:130785ms step_avg:90.01ms
step:1454/1680 train_time:130876ms step_avg:90.01ms
step:1455/1680 train_time:130966ms step_avg:90.01ms
step:1456/1680 train_time:131057ms step_avg:90.01ms
step:1457/1680 train_time:131148ms step_avg:90.01ms
step:1458/1680 train_time:131240ms step_avg:90.01ms
step:1459/1680 train_time:131331ms step_avg:90.01ms
step:1460/1680 train_time:131422ms step_avg:90.02ms
step:1461/1680 train_time:131514ms step_avg:90.02ms
step:1462/1680 train_time:131605ms step_avg:90.02ms
step:1463/1680 train_time:131695ms step_avg:90.02ms
step:1464/1680 train_time:131785ms step_avg:90.02ms
step:1465/1680 train_time:131876ms step_avg:90.02ms
step:1466/1680 train_time:131966ms step_avg:90.02ms
step:1467/1680 train_time:132059ms step_avg:90.02ms
step:1468/1680 train_time:132151ms step_avg:90.02ms
step:1469/1680 train_time:132243ms step_avg:90.02ms
step:1470/1680 train_time:132334ms step_avg:90.02ms
step:1471/1680 train_time:132425ms step_avg:90.02ms
step:1472/1680 train_time:132515ms step_avg:90.02ms
step:1473/1680 train_time:132607ms step_avg:90.02ms
step:1474/1680 train_time:132697ms step_avg:90.02ms
step:1475/1680 train_time:132787ms step_avg:90.03ms
step:1476/1680 train_time:132878ms step_avg:90.03ms
step:1477/1680 train_time:132969ms step_avg:90.03ms
step:1478/1680 train_time:133060ms step_avg:90.03ms
step:1479/1680 train_time:133151ms step_avg:90.03ms
step:1480/1680 train_time:133243ms step_avg:90.03ms
step:1481/1680 train_time:133334ms step_avg:90.03ms
step:1482/1680 train_time:133425ms step_avg:90.03ms
step:1483/1680 train_time:133516ms step_avg:90.03ms
step:1484/1680 train_time:133607ms step_avg:90.03ms
step:1485/1680 train_time:133698ms step_avg:90.03ms
step:1486/1680 train_time:133788ms step_avg:90.03ms
step:1487/1680 train_time:133879ms step_avg:90.03ms
step:1488/1680 train_time:133970ms step_avg:90.03ms
step:1489/1680 train_time:134062ms step_avg:90.03ms
step:1490/1680 train_time:134154ms step_avg:90.04ms
step:1491/1680 train_time:134244ms step_avg:90.04ms
step:1492/1680 train_time:134334ms step_avg:90.04ms
step:1493/1680 train_time:134425ms step_avg:90.04ms
step:1494/1680 train_time:134516ms step_avg:90.04ms
step:1495/1680 train_time:134607ms step_avg:90.04ms
step:1496/1680 train_time:134697ms step_avg:90.04ms
step:1497/1680 train_time:134787ms step_avg:90.04ms
step:1498/1680 train_time:134879ms step_avg:90.04ms
step:1499/1680 train_time:134970ms step_avg:90.04ms
step:1500/1680 train_time:135063ms step_avg:90.04ms
step:1500/1680 val_loss:3.3137 train_time:135157ms step_avg:90.10ms
step:1501/1680 train_time:135180ms step_avg:90.06ms
step:1502/1680 train_time:135253ms step_avg:90.05ms
step:1503/1680 train_time:135352ms step_avg:90.05ms
step:1504/1680 train_time:135446ms step_avg:90.06ms
step:1505/1680 train_time:135534ms step_avg:90.06ms
step:1506/1680 train_time:135623ms step_avg:90.05ms
step:1507/1680 train_time:135712ms step_avg:90.05ms
step:1508/1680 train_time:135802ms step_avg:90.05ms
step:1509/1680 train_time:135892ms step_avg:90.05ms
step:1510/1680 train_time:135982ms step_avg:90.05ms
step:1511/1680 train_time:136071ms step_avg:90.05ms
step:1512/1680 train_time:136164ms step_avg:90.06ms
step:1513/1680 train_time:136259ms step_avg:90.06ms
step:1514/1680 train_time:136352ms step_avg:90.06ms
step:1515/1680 train_time:136444ms step_avg:90.06ms
step:1516/1680 train_time:136534ms step_avg:90.06ms
step:1517/1680 train_time:136623ms step_avg:90.06ms
step:1518/1680 train_time:136713ms step_avg:90.06ms
step:1519/1680 train_time:136802ms step_avg:90.06ms
step:1520/1680 train_time:136892ms step_avg:90.06ms
step:1521/1680 train_time:136982ms step_avg:90.06ms
step:1522/1680 train_time:137072ms step_avg:90.06ms
step:1523/1680 train_time:137163ms step_avg:90.06ms
step:1524/1680 train_time:137256ms step_avg:90.06ms
step:1525/1680 train_time:137349ms step_avg:90.07ms
step:1526/1680 train_time:137439ms step_avg:90.06ms
step:1527/1680 train_time:137530ms step_avg:90.07ms
step:1528/1680 train_time:137621ms step_avg:90.07ms
step:1529/1680 train_time:137711ms step_avg:90.07ms
step:1530/1680 train_time:137801ms step_avg:90.07ms
step:1531/1680 train_time:137891ms step_avg:90.07ms
step:1532/1680 train_time:137981ms step_avg:90.07ms
step:1533/1680 train_time:138072ms step_avg:90.07ms
step:1534/1680 train_time:138163ms step_avg:90.07ms
step:1535/1680 train_time:138254ms step_avg:90.07ms
step:1536/1680 train_time:138346ms step_avg:90.07ms
step:1537/1680 train_time:138439ms step_avg:90.07ms
step:1538/1680 train_time:138530ms step_avg:90.07ms
step:1539/1680 train_time:138621ms step_avg:90.07ms
step:1540/1680 train_time:138711ms step_avg:90.07ms
step:1541/1680 train_time:138801ms step_avg:90.07ms
step:1542/1680 train_time:138891ms step_avg:90.07ms
step:1543/1680 train_time:138981ms step_avg:90.07ms
step:1544/1680 train_time:139071ms step_avg:90.07ms
step:1545/1680 train_time:139162ms step_avg:90.07ms
step:1546/1680 train_time:139254ms step_avg:90.07ms
step:1547/1680 train_time:139344ms step_avg:90.07ms
step:1548/1680 train_time:139435ms step_avg:90.07ms
step:1549/1680 train_time:139526ms step_avg:90.08ms
step:1550/1680 train_time:139618ms step_avg:90.08ms
step:1551/1680 train_time:139709ms step_avg:90.08ms
step:1552/1680 train_time:139800ms step_avg:90.08ms
step:1553/1680 train_time:139890ms step_avg:90.08ms
step:1554/1680 train_time:139981ms step_avg:90.08ms
step:1555/1680 train_time:140071ms step_avg:90.08ms
step:1556/1680 train_time:140163ms step_avg:90.08ms
step:1557/1680 train_time:140255ms step_avg:90.08ms
step:1558/1680 train_time:140345ms step_avg:90.08ms
step:1559/1680 train_time:140435ms step_avg:90.08ms
step:1560/1680 train_time:140527ms step_avg:90.08ms
step:1561/1680 train_time:140618ms step_avg:90.08ms
step:1562/1680 train_time:140708ms step_avg:90.08ms
step:1563/1680 train_time:140798ms step_avg:90.08ms
step:1564/1680 train_time:140889ms step_avg:90.08ms
step:1565/1680 train_time:140981ms step_avg:90.08ms
step:1566/1680 train_time:141072ms step_avg:90.08ms
step:1567/1680 train_time:141163ms step_avg:90.09ms
step:1568/1680 train_time:141254ms step_avg:90.09ms
step:1569/1680 train_time:141345ms step_avg:90.09ms
step:1570/1680 train_time:141435ms step_avg:90.09ms
step:1571/1680 train_time:141526ms step_avg:90.09ms
step:1572/1680 train_time:141617ms step_avg:90.09ms
step:1573/1680 train_time:141708ms step_avg:90.09ms
step:1574/1680 train_time:141799ms step_avg:90.09ms
step:1575/1680 train_time:141889ms step_avg:90.09ms
step:1576/1680 train_time:141980ms step_avg:90.09ms
step:1577/1680 train_time:142070ms step_avg:90.09ms
step:1578/1680 train_time:142161ms step_avg:90.09ms
step:1579/1680 train_time:142252ms step_avg:90.09ms
step:1580/1680 train_time:142342ms step_avg:90.09ms
step:1581/1680 train_time:142433ms step_avg:90.09ms
step:1582/1680 train_time:142524ms step_avg:90.09ms
step:1583/1680 train_time:142614ms step_avg:90.09ms
step:1584/1680 train_time:142704ms step_avg:90.09ms
step:1585/1680 train_time:142795ms step_avg:90.09ms
step:1586/1680 train_time:142886ms step_avg:90.09ms
step:1587/1680 train_time:142976ms step_avg:90.09ms
step:1588/1680 train_time:143067ms step_avg:90.09ms
step:1589/1680 train_time:143157ms step_avg:90.09ms
step:1590/1680 train_time:143248ms step_avg:90.09ms
step:1591/1680 train_time:143339ms step_avg:90.09ms
step:1592/1680 train_time:143430ms step_avg:90.09ms
step:1593/1680 train_time:143522ms step_avg:90.10ms
step:1594/1680 train_time:143614ms step_avg:90.10ms
step:1595/1680 train_time:143703ms step_avg:90.10ms
step:1596/1680 train_time:143794ms step_avg:90.10ms
step:1597/1680 train_time:143885ms step_avg:90.10ms
step:1598/1680 train_time:143975ms step_avg:90.10ms
step:1599/1680 train_time:144066ms step_avg:90.10ms
step:1600/1680 train_time:144156ms step_avg:90.10ms
step:1601/1680 train_time:144251ms step_avg:90.10ms
step:1602/1680 train_time:144338ms step_avg:90.10ms
step:1603/1680 train_time:144428ms step_avg:90.10ms
step:1604/1680 train_time:144521ms step_avg:90.10ms
step:1605/1680 train_time:144614ms step_avg:90.10ms
step:1606/1680 train_time:144704ms step_avg:90.10ms
step:1607/1680 train_time:144795ms step_avg:90.10ms
step:1608/1680 train_time:144886ms step_avg:90.10ms
step:1609/1680 train_time:144977ms step_avg:90.10ms
step:1610/1680 train_time:145067ms step_avg:90.10ms
step:1611/1680 train_time:145158ms step_avg:90.10ms
step:1612/1680 train_time:145249ms step_avg:90.11ms
step:1613/1680 train_time:145340ms step_avg:90.11ms
step:1614/1680 train_time:145431ms step_avg:90.11ms
step:1615/1680 train_time:145521ms step_avg:90.11ms
step:1616/1680 train_time:145612ms step_avg:90.11ms
step:1617/1680 train_time:145703ms step_avg:90.11ms
step:1618/1680 train_time:145794ms step_avg:90.11ms
step:1619/1680 train_time:145884ms step_avg:90.11ms
step:1620/1680 train_time:145975ms step_avg:90.11ms
step:1621/1680 train_time:146065ms step_avg:90.11ms
step:1622/1680 train_time:146155ms step_avg:90.11ms
step:1623/1680 train_time:146247ms step_avg:90.11ms
step:1624/1680 train_time:146337ms step_avg:90.11ms
step:1625/1680 train_time:146429ms step_avg:90.11ms
step:1625/1680 val_loss:3.2902 train_time:146521ms step_avg:90.17ms
step:1626/1680 train_time:146544ms step_avg:90.13ms
step:1627/1680 train_time:146615ms step_avg:90.11ms
step:1628/1680 train_time:146709ms step_avg:90.12ms
step:1629/1680 train_time:146801ms step_avg:90.12ms
step:1630/1680 train_time:146890ms step_avg:90.12ms
step:1631/1680 train_time:146981ms step_avg:90.12ms
step:1632/1680 train_time:147070ms step_avg:90.12ms
step:1633/1680 train_time:147159ms step_avg:90.12ms
step:1634/1680 train_time:147248ms step_avg:90.12ms
step:1635/1680 train_time:147338ms step_avg:90.11ms
step:1636/1680 train_time:147428ms step_avg:90.12ms
step:1637/1680 train_time:147522ms step_avg:90.12ms
step:1638/1680 train_time:147614ms step_avg:90.12ms
step:1639/1680 train_time:147706ms step_avg:90.12ms
step:1640/1680 train_time:147798ms step_avg:90.12ms
step:1641/1680 train_time:147889ms step_avg:90.12ms
step:1642/1680 train_time:147980ms step_avg:90.12ms
step:1643/1680 train_time:148071ms step_avg:90.12ms
step:1644/1680 train_time:148161ms step_avg:90.12ms
step:1645/1680 train_time:148251ms step_avg:90.12ms
step:1646/1680 train_time:148342ms step_avg:90.12ms
step:1647/1680 train_time:148432ms step_avg:90.12ms
step:1648/1680 train_time:148524ms step_avg:90.12ms
step:1649/1680 train_time:148616ms step_avg:90.12ms
step:1650/1680 train_time:148707ms step_avg:90.13ms
step:1651/1680 train_time:148798ms step_avg:90.13ms
step:1652/1680 train_time:148890ms step_avg:90.13ms
step:1653/1680 train_time:148981ms step_avg:90.13ms
step:1654/1680 train_time:149071ms step_avg:90.13ms
step:1655/1680 train_time:149162ms step_avg:90.13ms
step:1656/1680 train_time:149253ms step_avg:90.13ms
step:1657/1680 train_time:149343ms step_avg:90.13ms
step:1658/1680 train_time:149434ms step_avg:90.13ms
step:1659/1680 train_time:149526ms step_avg:90.13ms
step:1660/1680 train_time:149616ms step_avg:90.13ms
step:1661/1680 train_time:149708ms step_avg:90.13ms
step:1662/1680 train_time:149799ms step_avg:90.13ms
step:1663/1680 train_time:149891ms step_avg:90.13ms
step:1664/1680 train_time:149982ms step_avg:90.13ms
step:1665/1680 train_time:150072ms step_avg:90.13ms
step:1666/1680 train_time:150163ms step_avg:90.13ms
step:1667/1680 train_time:150253ms step_avg:90.13ms
step:1668/1680 train_time:150343ms step_avg:90.13ms
step:1669/1680 train_time:150434ms step_avg:90.13ms
step:1670/1680 train_time:150524ms step_avg:90.13ms
step:1671/1680 train_time:150616ms step_avg:90.14ms
step:1672/1680 train_time:150706ms step_avg:90.14ms
step:1673/1680 train_time:150799ms step_avg:90.14ms
step:1674/1680 train_time:150888ms step_avg:90.14ms
step:1675/1680 train_time:150980ms step_avg:90.14ms
step:1676/1680 train_time:151072ms step_avg:90.14ms
step:1677/1680 train_time:151162ms step_avg:90.14ms
step:1678/1680 train_time:151254ms step_avg:90.14ms
step:1679/1680 train_time:151343ms step_avg:90.14ms
step:1680/1680 train_time:151433ms step_avg:90.14ms
step:1680/1680 val_loss:3.2798 train_time:151526ms step_avg:90.19ms
peak memory allocated: 31255 MiB reserved: 46334 MiB

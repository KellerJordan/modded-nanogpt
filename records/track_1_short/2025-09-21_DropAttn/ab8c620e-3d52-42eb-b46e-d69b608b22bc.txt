import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 and layer_idx !=0 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        first_bm = 1 * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, final_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"v1/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer shows no degradation with context length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
smear_gate_params = [p for n, p in model.named_parameters() if "smear" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params+smear_gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate, args.ws_validate_final_layer
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250724+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sun Sep 21 23:17:28 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   35C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           82769      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A           82770      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           82771      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           82772      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           82773      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           82774      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           82775      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           82776      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           82770      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A           82771      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A           82772      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A           82773      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A           82774      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A           82775      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A           82776      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/1680 train_time:151ms step_avg:151.32ms
step:2/1680 train_time:176ms step_avg:88.00ms
step:3/1680 train_time:237ms step_avg:78.91ms
step:4/1680 train_time:324ms step_avg:80.91ms
step:5/1680 train_time:412ms step_avg:82.36ms
step:6/1680 train_time:500ms step_avg:83.26ms
step:7/1680 train_time:587ms step_avg:83.91ms
step:8/1680 train_time:676ms step_avg:84.46ms
step:9/1680 train_time:764ms step_avg:84.85ms
step:10/1680 train_time:852ms step_avg:85.19ms
step:11/1680 train_time:940ms step_avg:85.47ms
step:12/1680 train_time:1031ms step_avg:85.89ms
step:13/1680 train_time:1122ms step_avg:86.33ms
step:14/1680 train_time:1213ms step_avg:86.66ms
step:15/1680 train_time:1302ms step_avg:86.83ms
step:16/1680 train_time:1392ms step_avg:86.98ms
step:17/1680 train_time:1480ms step_avg:87.07ms
step:18/1680 train_time:1568ms step_avg:87.13ms
step:19/1680 train_time:1656ms step_avg:87.18ms
step:20/1680 train_time:1744ms step_avg:87.22ms
step:21/1680 train_time:1833ms step_avg:87.28ms
step:22/1680 train_time:1922ms step_avg:87.35ms
step:23/1680 train_time:2011ms step_avg:87.42ms
step:24/1680 train_time:2101ms step_avg:87.54ms
step:25/1680 train_time:2193ms step_avg:87.70ms
step:26/1680 train_time:2282ms step_avg:87.77ms
step:27/1680 train_time:2372ms step_avg:87.84ms
step:28/1680 train_time:2460ms step_avg:87.87ms
step:29/1680 train_time:2549ms step_avg:87.91ms
step:30/1680 train_time:2638ms step_avg:87.94ms
step:31/1680 train_time:2727ms step_avg:87.96ms
step:32/1680 train_time:2815ms step_avg:87.98ms
step:33/1680 train_time:2904ms step_avg:87.99ms
step:34/1680 train_time:2993ms step_avg:88.03ms
step:35/1680 train_time:3083ms step_avg:88.09ms
step:36/1680 train_time:3173ms step_avg:88.15ms
step:37/1680 train_time:3263ms step_avg:88.20ms
step:38/1680 train_time:3353ms step_avg:88.25ms
step:39/1680 train_time:3443ms step_avg:88.27ms
step:40/1680 train_time:3532ms step_avg:88.31ms
step:41/1680 train_time:3622ms step_avg:88.34ms
step:42/1680 train_time:3711ms step_avg:88.36ms
step:43/1680 train_time:3801ms step_avg:88.39ms
step:44/1680 train_time:3889ms step_avg:88.39ms
step:45/1680 train_time:3978ms step_avg:88.41ms
step:46/1680 train_time:4068ms step_avg:88.43ms
step:47/1680 train_time:4157ms step_avg:88.46ms
step:48/1680 train_time:4247ms step_avg:88.48ms
step:49/1680 train_time:4336ms step_avg:88.50ms
step:50/1680 train_time:4426ms step_avg:88.51ms
step:51/1680 train_time:4516ms step_avg:88.55ms
step:52/1680 train_time:4605ms step_avg:88.56ms
step:53/1680 train_time:4695ms step_avg:88.58ms
step:54/1680 train_time:4783ms step_avg:88.57ms
step:55/1680 train_time:4872ms step_avg:88.58ms
step:56/1680 train_time:4961ms step_avg:88.59ms
step:57/1680 train_time:5050ms step_avg:88.59ms
step:58/1680 train_time:5139ms step_avg:88.60ms
step:59/1680 train_time:5229ms step_avg:88.62ms
step:60/1680 train_time:5318ms step_avg:88.63ms
step:61/1680 train_time:5407ms step_avg:88.64ms
step:62/1680 train_time:5497ms step_avg:88.66ms
step:63/1680 train_time:5586ms step_avg:88.66ms
step:64/1680 train_time:5674ms step_avg:88.66ms
step:65/1680 train_time:5763ms step_avg:88.66ms
step:66/1680 train_time:5852ms step_avg:88.66ms
step:67/1680 train_time:5940ms step_avg:88.66ms
step:68/1680 train_time:6030ms step_avg:88.67ms
step:69/1680 train_time:6119ms step_avg:88.68ms
step:70/1680 train_time:6208ms step_avg:88.69ms
step:71/1680 train_time:6298ms step_avg:88.71ms
step:72/1680 train_time:6387ms step_avg:88.71ms
step:73/1680 train_time:6476ms step_avg:88.71ms
step:74/1680 train_time:6565ms step_avg:88.72ms
step:75/1680 train_time:6656ms step_avg:88.74ms
step:76/1680 train_time:6745ms step_avg:88.75ms
step:77/1680 train_time:6834ms step_avg:88.76ms
step:78/1680 train_time:6923ms step_avg:88.75ms
step:79/1680 train_time:7012ms step_avg:88.76ms
step:80/1680 train_time:7101ms step_avg:88.76ms
step:81/1680 train_time:7190ms step_avg:88.77ms
step:82/1680 train_time:7280ms step_avg:88.78ms
step:83/1680 train_time:7369ms step_avg:88.78ms
step:84/1680 train_time:7457ms step_avg:88.78ms
step:85/1680 train_time:7546ms step_avg:88.78ms
step:86/1680 train_time:7636ms step_avg:88.80ms
step:87/1680 train_time:7725ms step_avg:88.79ms
step:88/1680 train_time:7815ms step_avg:88.81ms
step:89/1680 train_time:7904ms step_avg:88.81ms
step:90/1680 train_time:7993ms step_avg:88.82ms
step:91/1680 train_time:8082ms step_avg:88.81ms
step:92/1680 train_time:8171ms step_avg:88.82ms
step:93/1680 train_time:8260ms step_avg:88.82ms
step:94/1680 train_time:8349ms step_avg:88.82ms
step:95/1680 train_time:8438ms step_avg:88.82ms
step:96/1680 train_time:8528ms step_avg:88.83ms
step:97/1680 train_time:8617ms step_avg:88.84ms
step:98/1680 train_time:8707ms step_avg:88.85ms
step:99/1680 train_time:8797ms step_avg:88.85ms
step:100/1680 train_time:8885ms step_avg:88.85ms
step:101/1680 train_time:8974ms step_avg:88.85ms
step:102/1680 train_time:9063ms step_avg:88.85ms
step:103/1680 train_time:9152ms step_avg:88.85ms
step:104/1680 train_time:9241ms step_avg:88.85ms
step:105/1680 train_time:9330ms step_avg:88.85ms
step:106/1680 train_time:9420ms step_avg:88.87ms
step:107/1680 train_time:9508ms step_avg:88.86ms
step:108/1680 train_time:9597ms step_avg:88.86ms
step:109/1680 train_time:9686ms step_avg:88.86ms
step:110/1680 train_time:9776ms step_avg:88.87ms
step:111/1680 train_time:9865ms step_avg:88.88ms
step:112/1680 train_time:9955ms step_avg:88.88ms
step:113/1680 train_time:10043ms step_avg:88.88ms
step:114/1680 train_time:10133ms step_avg:88.89ms
step:115/1680 train_time:10222ms step_avg:88.89ms
step:116/1680 train_time:10311ms step_avg:88.89ms
step:117/1680 train_time:10400ms step_avg:88.89ms
step:118/1680 train_time:10489ms step_avg:88.89ms
step:119/1680 train_time:10577ms step_avg:88.88ms
step:120/1680 train_time:10666ms step_avg:88.88ms
step:121/1680 train_time:10755ms step_avg:88.88ms
step:122/1680 train_time:10844ms step_avg:88.88ms
step:123/1680 train_time:10934ms step_avg:88.89ms
step:124/1680 train_time:11022ms step_avg:88.89ms
step:125/1680 train_time:11112ms step_avg:88.89ms
step:125/1680 val_loss:4.2962 train_time:11203ms step_avg:89.62ms
step:126/1680 train_time:11227ms step_avg:89.10ms
step:127/1680 train_time:11294ms step_avg:88.93ms
step:128/1680 train_time:11393ms step_avg:89.01ms
step:129/1680 train_time:11487ms step_avg:89.05ms
step:130/1680 train_time:11575ms step_avg:89.04ms
step:131/1680 train_time:11664ms step_avg:89.04ms
step:132/1680 train_time:11752ms step_avg:89.03ms
step:133/1680 train_time:11839ms step_avg:89.02ms
step:134/1680 train_time:11927ms step_avg:89.01ms
step:135/1680 train_time:12015ms step_avg:89.00ms
step:136/1680 train_time:12104ms step_avg:89.00ms
step:137/1680 train_time:12192ms step_avg:88.99ms
step:138/1680 train_time:12282ms step_avg:89.00ms
step:139/1680 train_time:12374ms step_avg:89.02ms
step:140/1680 train_time:12466ms step_avg:89.04ms
step:141/1680 train_time:12555ms step_avg:89.04ms
step:142/1680 train_time:12644ms step_avg:89.05ms
step:143/1680 train_time:12733ms step_avg:89.04ms
step:144/1680 train_time:12821ms step_avg:89.04ms
step:145/1680 train_time:12909ms step_avg:89.03ms
step:146/1680 train_time:12998ms step_avg:89.03ms
step:147/1680 train_time:13085ms step_avg:89.02ms
step:148/1680 train_time:13174ms step_avg:89.01ms
step:149/1680 train_time:13263ms step_avg:89.02ms
step:150/1680 train_time:13354ms step_avg:89.02ms
step:151/1680 train_time:13444ms step_avg:89.03ms
step:152/1680 train_time:13534ms step_avg:89.04ms
step:153/1680 train_time:13624ms step_avg:89.04ms
step:154/1680 train_time:13712ms step_avg:89.04ms
step:155/1680 train_time:13802ms step_avg:89.04ms
step:156/1680 train_time:13891ms step_avg:89.04ms
step:157/1680 train_time:13979ms step_avg:89.04ms
step:158/1680 train_time:14067ms step_avg:89.03ms
step:159/1680 train_time:14156ms step_avg:89.03ms
step:160/1680 train_time:14245ms step_avg:89.03ms
step:161/1680 train_time:14333ms step_avg:89.03ms
step:162/1680 train_time:14424ms step_avg:89.04ms
step:163/1680 train_time:14512ms step_avg:89.03ms
step:164/1680 train_time:14603ms step_avg:89.04ms
step:165/1680 train_time:14691ms step_avg:89.04ms
step:166/1680 train_time:14780ms step_avg:89.04ms
step:167/1680 train_time:14870ms step_avg:89.04ms
step:168/1680 train_time:14958ms step_avg:89.04ms
step:169/1680 train_time:15048ms step_avg:89.04ms
step:170/1680 train_time:15136ms step_avg:89.04ms
step:171/1680 train_time:15225ms step_avg:89.03ms
step:172/1680 train_time:15314ms step_avg:89.03ms
step:173/1680 train_time:15404ms step_avg:89.04ms
step:174/1680 train_time:15493ms step_avg:89.04ms
step:175/1680 train_time:15583ms step_avg:89.05ms
step:176/1680 train_time:15672ms step_avg:89.04ms
step:177/1680 train_time:15761ms step_avg:89.04ms
step:178/1680 train_time:15850ms step_avg:89.05ms
step:179/1680 train_time:15938ms step_avg:89.04ms
step:180/1680 train_time:16027ms step_avg:89.04ms
step:181/1680 train_time:16116ms step_avg:89.04ms
step:182/1680 train_time:16205ms step_avg:89.04ms
step:183/1680 train_time:16295ms step_avg:89.04ms
step:184/1680 train_time:16385ms step_avg:89.05ms
step:185/1680 train_time:16473ms step_avg:89.04ms
step:186/1680 train_time:16562ms step_avg:89.04ms
step:187/1680 train_time:16651ms step_avg:89.04ms
step:188/1680 train_time:16741ms step_avg:89.05ms
step:189/1680 train_time:16830ms step_avg:89.05ms
step:190/1680 train_time:16919ms step_avg:89.05ms
step:191/1680 train_time:17007ms step_avg:89.04ms
step:192/1680 train_time:17095ms step_avg:89.04ms
step:193/1680 train_time:17184ms step_avg:89.04ms
step:194/1680 train_time:17273ms step_avg:89.03ms
step:195/1680 train_time:17363ms step_avg:89.04ms
step:196/1680 train_time:17451ms step_avg:89.04ms
step:197/1680 train_time:17541ms step_avg:89.04ms
step:198/1680 train_time:17629ms step_avg:89.04ms
step:199/1680 train_time:17719ms step_avg:89.04ms
step:200/1680 train_time:17807ms step_avg:89.04ms
step:201/1680 train_time:17897ms step_avg:89.04ms
step:202/1680 train_time:17985ms step_avg:89.04ms
step:203/1680 train_time:18074ms step_avg:89.03ms
step:204/1680 train_time:18163ms step_avg:89.04ms
step:205/1680 train_time:18252ms step_avg:89.03ms
step:206/1680 train_time:18340ms step_avg:89.03ms
step:207/1680 train_time:18429ms step_avg:89.03ms
step:208/1680 train_time:18518ms step_avg:89.03ms
step:209/1680 train_time:18607ms step_avg:89.03ms
step:210/1680 train_time:18696ms step_avg:89.03ms
step:211/1680 train_time:18785ms step_avg:89.03ms
step:212/1680 train_time:18874ms step_avg:89.03ms
step:213/1680 train_time:18964ms step_avg:89.03ms
step:214/1680 train_time:19053ms step_avg:89.03ms
step:215/1680 train_time:19142ms step_avg:89.03ms
step:216/1680 train_time:19231ms step_avg:89.03ms
step:217/1680 train_time:19321ms step_avg:89.03ms
step:218/1680 train_time:19409ms step_avg:89.03ms
step:219/1680 train_time:19499ms step_avg:89.04ms
step:220/1680 train_time:19588ms step_avg:89.04ms
step:221/1680 train_time:19676ms step_avg:89.03ms
step:222/1680 train_time:19765ms step_avg:89.03ms
step:223/1680 train_time:19853ms step_avg:89.03ms
step:224/1680 train_time:19943ms step_avg:89.03ms
step:225/1680 train_time:20031ms step_avg:89.03ms
step:226/1680 train_time:20121ms step_avg:89.03ms
step:227/1680 train_time:20210ms step_avg:89.03ms
step:228/1680 train_time:20299ms step_avg:89.03ms
step:229/1680 train_time:20388ms step_avg:89.03ms
step:230/1680 train_time:20477ms step_avg:89.03ms
step:231/1680 train_time:20566ms step_avg:89.03ms
step:232/1680 train_time:20655ms step_avg:89.03ms
step:233/1680 train_time:20743ms step_avg:89.03ms
step:234/1680 train_time:20833ms step_avg:89.03ms
step:235/1680 train_time:20923ms step_avg:89.03ms
step:236/1680 train_time:21011ms step_avg:89.03ms
step:237/1680 train_time:21100ms step_avg:89.03ms
step:238/1680 train_time:21189ms step_avg:89.03ms
step:239/1680 train_time:21279ms step_avg:89.03ms
step:240/1680 train_time:21368ms step_avg:89.03ms
step:241/1680 train_time:21457ms step_avg:89.03ms
step:242/1680 train_time:21545ms step_avg:89.03ms
step:243/1680 train_time:21634ms step_avg:89.03ms
step:244/1680 train_time:21724ms step_avg:89.03ms
step:245/1680 train_time:21814ms step_avg:89.04ms
step:246/1680 train_time:21904ms step_avg:89.04ms
step:247/1680 train_time:21993ms step_avg:89.04ms
step:248/1680 train_time:22081ms step_avg:89.04ms
step:249/1680 train_time:22170ms step_avg:89.04ms
step:250/1680 train_time:22259ms step_avg:89.04ms
step:250/1680 val_loss:3.9619 train_time:22351ms step_avg:89.40ms
step:251/1680 train_time:22374ms step_avg:89.14ms
step:252/1680 train_time:22444ms step_avg:89.06ms
step:253/1680 train_time:22543ms step_avg:89.10ms
step:254/1680 train_time:22634ms step_avg:89.11ms
step:255/1680 train_time:22723ms step_avg:89.11ms
step:256/1680 train_time:22811ms step_avg:89.10ms
step:257/1680 train_time:22899ms step_avg:89.10ms
step:258/1680 train_time:22987ms step_avg:89.10ms
step:259/1680 train_time:23075ms step_avg:89.09ms
step:260/1680 train_time:23164ms step_avg:89.09ms
step:261/1680 train_time:23252ms step_avg:89.09ms
step:262/1680 train_time:23341ms step_avg:89.09ms
step:263/1680 train_time:23431ms step_avg:89.09ms
step:264/1680 train_time:23524ms step_avg:89.10ms
step:265/1680 train_time:23615ms step_avg:89.11ms
step:266/1680 train_time:23705ms step_avg:89.12ms
step:267/1680 train_time:23795ms step_avg:89.12ms
step:268/1680 train_time:23883ms step_avg:89.12ms
step:269/1680 train_time:23971ms step_avg:89.11ms
step:270/1680 train_time:24059ms step_avg:89.11ms
step:271/1680 train_time:24147ms step_avg:89.10ms
step:272/1680 train_time:24235ms step_avg:89.10ms
step:273/1680 train_time:24324ms step_avg:89.10ms
step:274/1680 train_time:24412ms step_avg:89.10ms
step:275/1680 train_time:24503ms step_avg:89.10ms
step:276/1680 train_time:24593ms step_avg:89.11ms
step:277/1680 train_time:24683ms step_avg:89.11ms
step:278/1680 train_time:24773ms step_avg:89.11ms
step:279/1680 train_time:24863ms step_avg:89.11ms
step:280/1680 train_time:24952ms step_avg:89.11ms
step:281/1680 train_time:25041ms step_avg:89.11ms
step:282/1680 train_time:25129ms step_avg:89.11ms
step:283/1680 train_time:25218ms step_avg:89.11ms
step:284/1680 train_time:25307ms step_avg:89.11ms
step:285/1680 train_time:25396ms step_avg:89.11ms
step:286/1680 train_time:25486ms step_avg:89.11ms
step:287/1680 train_time:25575ms step_avg:89.11ms
step:288/1680 train_time:25665ms step_avg:89.11ms
step:289/1680 train_time:25755ms step_avg:89.12ms
step:290/1680 train_time:25844ms step_avg:89.12ms
step:291/1680 train_time:25934ms step_avg:89.12ms
step:292/1680 train_time:26022ms step_avg:89.12ms
step:293/1680 train_time:26110ms step_avg:89.11ms
step:294/1680 train_time:26200ms step_avg:89.12ms
step:295/1680 train_time:26288ms step_avg:89.11ms
step:296/1680 train_time:26378ms step_avg:89.11ms
step:297/1680 train_time:26467ms step_avg:89.11ms
step:298/1680 train_time:26557ms step_avg:89.12ms
step:299/1680 train_time:26646ms step_avg:89.12ms
step:300/1680 train_time:26736ms step_avg:89.12ms
step:301/1680 train_time:26825ms step_avg:89.12ms
step:302/1680 train_time:26914ms step_avg:89.12ms
step:303/1680 train_time:27003ms step_avg:89.12ms
step:304/1680 train_time:27092ms step_avg:89.12ms
step:305/1680 train_time:27181ms step_avg:89.12ms
step:306/1680 train_time:27270ms step_avg:89.12ms
step:307/1680 train_time:27360ms step_avg:89.12ms
step:308/1680 train_time:27449ms step_avg:89.12ms
step:309/1680 train_time:27538ms step_avg:89.12ms
step:310/1680 train_time:27628ms step_avg:89.12ms
step:311/1680 train_time:27717ms step_avg:89.12ms
step:312/1680 train_time:27806ms step_avg:89.12ms
step:313/1680 train_time:27896ms step_avg:89.12ms
step:314/1680 train_time:27985ms step_avg:89.12ms
step:315/1680 train_time:28074ms step_avg:89.12ms
step:316/1680 train_time:28162ms step_avg:89.12ms
step:317/1680 train_time:28251ms step_avg:89.12ms
step:318/1680 train_time:28340ms step_avg:89.12ms
step:319/1680 train_time:28429ms step_avg:89.12ms
step:320/1680 train_time:28519ms step_avg:89.12ms
step:321/1680 train_time:28608ms step_avg:89.12ms
step:322/1680 train_time:28698ms step_avg:89.12ms
step:323/1680 train_time:28787ms step_avg:89.12ms
step:324/1680 train_time:28877ms step_avg:89.13ms
step:325/1680 train_time:28965ms step_avg:89.12ms
step:326/1680 train_time:29054ms step_avg:89.12ms
step:327/1680 train_time:29143ms step_avg:89.12ms
step:328/1680 train_time:29231ms step_avg:89.12ms
step:329/1680 train_time:29320ms step_avg:89.12ms
step:330/1680 train_time:29408ms step_avg:89.12ms
step:331/1680 train_time:29498ms step_avg:89.12ms
step:332/1680 train_time:29587ms step_avg:89.12ms
step:333/1680 train_time:29677ms step_avg:89.12ms
step:334/1680 train_time:29766ms step_avg:89.12ms
step:335/1680 train_time:29856ms step_avg:89.12ms
step:336/1680 train_time:29945ms step_avg:89.12ms
step:337/1680 train_time:30035ms step_avg:89.12ms
step:338/1680 train_time:30124ms step_avg:89.12ms
step:339/1680 train_time:30212ms step_avg:89.12ms
step:340/1680 train_time:30303ms step_avg:89.13ms
step:341/1680 train_time:30391ms step_avg:89.12ms
step:342/1680 train_time:30481ms step_avg:89.12ms
step:343/1680 train_time:30570ms step_avg:89.13ms
step:344/1680 train_time:30661ms step_avg:89.13ms
step:345/1680 train_time:30750ms step_avg:89.13ms
step:346/1680 train_time:30839ms step_avg:89.13ms
step:347/1680 train_time:30928ms step_avg:89.13ms
step:348/1680 train_time:31018ms step_avg:89.13ms
step:349/1680 train_time:31108ms step_avg:89.13ms
step:350/1680 train_time:31197ms step_avg:89.13ms
step:351/1680 train_time:31286ms step_avg:89.13ms
step:352/1680 train_time:31375ms step_avg:89.13ms
step:353/1680 train_time:31464ms step_avg:89.13ms
step:354/1680 train_time:31553ms step_avg:89.13ms
step:355/1680 train_time:31642ms step_avg:89.13ms
step:356/1680 train_time:31732ms step_avg:89.14ms
step:357/1680 train_time:31822ms step_avg:89.14ms
step:358/1680 train_time:31910ms step_avg:89.13ms
step:359/1680 train_time:32000ms step_avg:89.14ms
step:360/1680 train_time:32088ms step_avg:89.13ms
step:361/1680 train_time:32177ms step_avg:89.13ms
step:362/1680 train_time:32266ms step_avg:89.13ms
step:363/1680 train_time:32354ms step_avg:89.13ms
step:364/1680 train_time:32443ms step_avg:89.13ms
step:365/1680 train_time:32532ms step_avg:89.13ms
step:366/1680 train_time:32622ms step_avg:89.13ms
step:367/1680 train_time:32710ms step_avg:89.13ms
step:368/1680 train_time:32800ms step_avg:89.13ms
step:369/1680 train_time:32889ms step_avg:89.13ms
step:370/1680 train_time:32978ms step_avg:89.13ms
step:371/1680 train_time:33067ms step_avg:89.13ms
step:372/1680 train_time:33157ms step_avg:89.13ms
step:373/1680 train_time:33246ms step_avg:89.13ms
step:374/1680 train_time:33335ms step_avg:89.13ms
step:375/1680 train_time:33424ms step_avg:89.13ms
step:375/1680 val_loss:3.8123 train_time:33515ms step_avg:89.37ms
step:376/1680 train_time:33541ms step_avg:89.21ms
step:377/1680 train_time:33609ms step_avg:89.15ms
step:378/1680 train_time:33702ms step_avg:89.16ms
step:379/1680 train_time:33790ms step_avg:89.16ms
step:380/1680 train_time:33879ms step_avg:89.15ms
step:381/1680 train_time:33967ms step_avg:89.15ms
step:382/1680 train_time:34056ms step_avg:89.15ms
step:383/1680 train_time:34145ms step_avg:89.15ms
step:384/1680 train_time:34232ms step_avg:89.15ms
step:385/1680 train_time:34322ms step_avg:89.15ms
step:386/1680 train_time:34410ms step_avg:89.15ms
step:387/1680 train_time:34501ms step_avg:89.15ms
step:388/1680 train_time:34592ms step_avg:89.16ms
step:389/1680 train_time:34683ms step_avg:89.16ms
step:390/1680 train_time:34773ms step_avg:89.16ms
step:391/1680 train_time:34863ms step_avg:89.16ms
step:392/1680 train_time:34950ms step_avg:89.16ms
step:393/1680 train_time:35039ms step_avg:89.16ms
step:394/1680 train_time:35127ms step_avg:89.16ms
step:395/1680 train_time:35217ms step_avg:89.16ms
step:396/1680 train_time:35305ms step_avg:89.15ms
step:397/1680 train_time:35393ms step_avg:89.15ms
step:398/1680 train_time:35483ms step_avg:89.15ms
step:399/1680 train_time:35572ms step_avg:89.15ms
step:400/1680 train_time:35664ms step_avg:89.16ms
step:401/1680 train_time:35753ms step_avg:89.16ms
step:402/1680 train_time:35844ms step_avg:89.16ms
step:403/1680 train_time:35932ms step_avg:89.16ms
step:404/1680 train_time:36022ms step_avg:89.16ms
step:405/1680 train_time:36110ms step_avg:89.16ms
step:406/1680 train_time:36199ms step_avg:89.16ms
step:407/1680 train_time:36288ms step_avg:89.16ms
step:408/1680 train_time:36377ms step_avg:89.16ms
step:409/1680 train_time:36466ms step_avg:89.16ms
step:410/1680 train_time:36556ms step_avg:89.16ms
step:411/1680 train_time:36646ms step_avg:89.16ms
step:412/1680 train_time:36736ms step_avg:89.16ms
step:413/1680 train_time:36826ms step_avg:89.17ms
step:414/1680 train_time:36915ms step_avg:89.17ms
step:415/1680 train_time:37005ms step_avg:89.17ms
step:416/1680 train_time:37093ms step_avg:89.17ms
step:417/1680 train_time:37182ms step_avg:89.17ms
step:418/1680 train_time:37271ms step_avg:89.17ms
step:419/1680 train_time:37361ms step_avg:89.17ms
step:420/1680 train_time:37450ms step_avg:89.17ms
step:421/1680 train_time:37539ms step_avg:89.17ms
step:422/1680 train_time:37629ms step_avg:89.17ms
step:423/1680 train_time:37718ms step_avg:89.17ms
step:424/1680 train_time:37808ms step_avg:89.17ms
step:425/1680 train_time:37897ms step_avg:89.17ms
step:426/1680 train_time:37986ms step_avg:89.17ms
step:427/1680 train_time:38074ms step_avg:89.17ms
step:428/1680 train_time:38164ms step_avg:89.17ms
step:429/1680 train_time:38252ms step_avg:89.17ms
step:430/1680 train_time:38341ms step_avg:89.17ms
step:431/1680 train_time:38430ms step_avg:89.17ms
step:432/1680 train_time:38519ms step_avg:89.17ms
step:433/1680 train_time:38609ms step_avg:89.17ms
step:434/1680 train_time:38699ms step_avg:89.17ms
step:435/1680 train_time:38789ms step_avg:89.17ms
step:436/1680 train_time:38878ms step_avg:89.17ms
step:437/1680 train_time:38968ms step_avg:89.17ms
step:438/1680 train_time:39057ms step_avg:89.17ms
step:439/1680 train_time:39146ms step_avg:89.17ms
step:440/1680 train_time:39235ms step_avg:89.17ms
step:441/1680 train_time:39324ms step_avg:89.17ms
step:442/1680 train_time:39413ms step_avg:89.17ms
step:443/1680 train_time:39502ms step_avg:89.17ms
step:444/1680 train_time:39591ms step_avg:89.17ms
step:445/1680 train_time:39680ms step_avg:89.17ms
step:446/1680 train_time:39771ms step_avg:89.17ms
step:447/1680 train_time:39860ms step_avg:89.17ms
step:448/1680 train_time:39949ms step_avg:89.17ms
step:449/1680 train_time:40039ms step_avg:89.17ms
step:450/1680 train_time:40128ms step_avg:89.17ms
step:451/1680 train_time:40217ms step_avg:89.17ms
step:452/1680 train_time:40305ms step_avg:89.17ms
step:453/1680 train_time:40395ms step_avg:89.17ms
step:454/1680 train_time:40484ms step_avg:89.17ms
step:455/1680 train_time:40573ms step_avg:89.17ms
step:456/1680 train_time:40663ms step_avg:89.17ms
step:457/1680 train_time:40752ms step_avg:89.17ms
step:458/1680 train_time:40843ms step_avg:89.18ms
step:459/1680 train_time:40932ms step_avg:89.18ms
step:460/1680 train_time:41022ms step_avg:89.18ms
step:461/1680 train_time:41111ms step_avg:89.18ms
step:462/1680 train_time:41200ms step_avg:89.18ms
step:463/1680 train_time:41288ms step_avg:89.18ms
step:464/1680 train_time:41377ms step_avg:89.17ms
step:465/1680 train_time:41466ms step_avg:89.18ms
step:466/1680 train_time:41555ms step_avg:89.17ms
step:467/1680 train_time:41645ms step_avg:89.18ms
step:468/1680 train_time:41735ms step_avg:89.18ms
step:469/1680 train_time:41825ms step_avg:89.18ms
step:470/1680 train_time:41915ms step_avg:89.18ms
step:471/1680 train_time:42005ms step_avg:89.18ms
step:472/1680 train_time:42094ms step_avg:89.18ms
step:473/1680 train_time:42185ms step_avg:89.19ms
step:474/1680 train_time:42273ms step_avg:89.18ms
step:475/1680 train_time:42364ms step_avg:89.19ms
step:476/1680 train_time:42452ms step_avg:89.19ms
step:477/1680 train_time:42542ms step_avg:89.19ms
step:478/1680 train_time:42631ms step_avg:89.19ms
step:479/1680 train_time:42720ms step_avg:89.19ms
step:480/1680 train_time:42809ms step_avg:89.19ms
step:481/1680 train_time:42898ms step_avg:89.19ms
step:482/1680 train_time:42988ms step_avg:89.19ms
step:483/1680 train_time:43076ms step_avg:89.19ms
step:484/1680 train_time:43166ms step_avg:89.19ms
step:485/1680 train_time:43255ms step_avg:89.19ms
step:486/1680 train_time:43345ms step_avg:89.19ms
step:487/1680 train_time:43434ms step_avg:89.19ms
step:488/1680 train_time:43523ms step_avg:89.19ms
step:489/1680 train_time:43612ms step_avg:89.19ms
step:490/1680 train_time:43702ms step_avg:89.19ms
step:491/1680 train_time:43791ms step_avg:89.19ms
step:492/1680 train_time:43880ms step_avg:89.19ms
step:493/1680 train_time:43971ms step_avg:89.19ms
step:494/1680 train_time:44060ms step_avg:89.19ms
step:495/1680 train_time:44149ms step_avg:89.19ms
step:496/1680 train_time:44239ms step_avg:89.19ms
step:497/1680 train_time:44328ms step_avg:89.19ms
step:498/1680 train_time:44417ms step_avg:89.19ms
step:499/1680 train_time:44507ms step_avg:89.19ms
step:500/1680 train_time:44597ms step_avg:89.19ms
step:500/1680 val_loss:3.7141 train_time:44688ms step_avg:89.38ms
step:501/1680 train_time:44711ms step_avg:89.24ms
step:502/1680 train_time:44780ms step_avg:89.20ms
step:503/1680 train_time:44875ms step_avg:89.22ms
step:504/1680 train_time:44965ms step_avg:89.22ms
step:505/1680 train_time:45053ms step_avg:89.21ms
step:506/1680 train_time:45142ms step_avg:89.21ms
step:507/1680 train_time:45230ms step_avg:89.21ms
step:508/1680 train_time:45318ms step_avg:89.21ms
step:509/1680 train_time:45407ms step_avg:89.21ms
step:510/1680 train_time:45496ms step_avg:89.21ms
step:511/1680 train_time:45585ms step_avg:89.21ms
step:512/1680 train_time:45677ms step_avg:89.21ms
step:513/1680 train_time:45769ms step_avg:89.22ms
step:514/1680 train_time:45860ms step_avg:89.22ms
step:515/1680 train_time:45951ms step_avg:89.22ms
step:516/1680 train_time:46040ms step_avg:89.22ms
step:517/1680 train_time:46129ms step_avg:89.22ms
step:518/1680 train_time:46218ms step_avg:89.22ms
step:519/1680 train_time:46306ms step_avg:89.22ms
step:520/1680 train_time:46394ms step_avg:89.22ms
step:521/1680 train_time:46483ms step_avg:89.22ms
step:522/1680 train_time:46572ms step_avg:89.22ms
step:523/1680 train_time:46661ms step_avg:89.22ms
step:524/1680 train_time:46753ms step_avg:89.22ms
step:525/1680 train_time:46843ms step_avg:89.22ms
step:526/1680 train_time:46933ms step_avg:89.23ms
step:527/1680 train_time:47022ms step_avg:89.23ms
step:528/1680 train_time:47112ms step_avg:89.23ms
step:529/1680 train_time:47200ms step_avg:89.23ms
step:530/1680 train_time:47289ms step_avg:89.23ms
step:531/1680 train_time:47378ms step_avg:89.22ms
step:532/1680 train_time:47467ms step_avg:89.22ms
step:533/1680 train_time:47556ms step_avg:89.22ms
step:534/1680 train_time:47645ms step_avg:89.22ms
step:535/1680 train_time:47735ms step_avg:89.22ms
step:536/1680 train_time:47824ms step_avg:89.22ms
step:537/1680 train_time:47915ms step_avg:89.23ms
step:538/1680 train_time:48004ms step_avg:89.23ms
step:539/1680 train_time:48094ms step_avg:89.23ms
step:540/1680 train_time:48183ms step_avg:89.23ms
step:541/1680 train_time:48273ms step_avg:89.23ms
step:542/1680 train_time:48362ms step_avg:89.23ms
step:543/1680 train_time:48451ms step_avg:89.23ms
step:544/1680 train_time:48540ms step_avg:89.23ms
step:545/1680 train_time:48629ms step_avg:89.23ms
step:546/1680 train_time:48719ms step_avg:89.23ms
step:547/1680 train_time:48809ms step_avg:89.23ms
step:548/1680 train_time:48899ms step_avg:89.23ms
step:549/1680 train_time:48990ms step_avg:89.24ms
step:550/1680 train_time:49081ms step_avg:89.24ms
step:551/1680 train_time:49171ms step_avg:89.24ms
step:552/1680 train_time:49261ms step_avg:89.24ms
step:553/1680 train_time:49351ms step_avg:89.24ms
step:554/1680 train_time:49441ms step_avg:89.24ms
step:555/1680 train_time:49531ms step_avg:89.25ms
step:556/1680 train_time:49623ms step_avg:89.25ms
step:557/1680 train_time:49714ms step_avg:89.25ms
step:558/1680 train_time:49804ms step_avg:89.25ms
step:559/1680 train_time:49895ms step_avg:89.26ms
step:560/1680 train_time:49985ms step_avg:89.26ms
step:561/1680 train_time:50077ms step_avg:89.26ms
step:562/1680 train_time:50168ms step_avg:89.27ms
step:563/1680 train_time:50259ms step_avg:89.27ms
step:564/1680 train_time:50349ms step_avg:89.27ms
step:565/1680 train_time:50440ms step_avg:89.27ms
step:566/1680 train_time:50531ms step_avg:89.28ms
step:567/1680 train_time:50622ms step_avg:89.28ms
step:568/1680 train_time:50712ms step_avg:89.28ms
step:569/1680 train_time:50803ms step_avg:89.28ms
step:570/1680 train_time:50894ms step_avg:89.29ms
step:571/1680 train_time:50984ms step_avg:89.29ms
step:572/1680 train_time:51075ms step_avg:89.29ms
step:573/1680 train_time:51166ms step_avg:89.29ms
step:574/1680 train_time:51257ms step_avg:89.30ms
step:575/1680 train_time:51347ms step_avg:89.30ms
step:576/1680 train_time:51437ms step_avg:89.30ms
step:577/1680 train_time:51528ms step_avg:89.30ms
step:578/1680 train_time:51619ms step_avg:89.31ms
step:579/1680 train_time:51709ms step_avg:89.31ms
step:580/1680 train_time:51800ms step_avg:89.31ms
step:581/1680 train_time:51890ms step_avg:89.31ms
step:582/1680 train_time:51980ms step_avg:89.31ms
step:583/1680 train_time:52070ms step_avg:89.31ms
step:584/1680 train_time:52161ms step_avg:89.32ms
step:585/1680 train_time:52251ms step_avg:89.32ms
step:586/1680 train_time:52342ms step_avg:89.32ms
step:587/1680 train_time:52433ms step_avg:89.32ms
step:588/1680 train_time:52524ms step_avg:89.33ms
step:589/1680 train_time:52615ms step_avg:89.33ms
step:590/1680 train_time:52706ms step_avg:89.33ms
step:591/1680 train_time:52797ms step_avg:89.34ms
step:592/1680 train_time:52888ms step_avg:89.34ms
step:593/1680 train_time:52979ms step_avg:89.34ms
step:594/1680 train_time:53069ms step_avg:89.34ms
step:595/1680 train_time:53159ms step_avg:89.34ms
step:596/1680 train_time:53250ms step_avg:89.34ms
step:597/1680 train_time:53340ms step_avg:89.35ms
step:598/1680 train_time:53430ms step_avg:89.35ms
step:599/1680 train_time:53520ms step_avg:89.35ms
step:600/1680 train_time:53611ms step_avg:89.35ms
step:601/1680 train_time:53701ms step_avg:89.35ms
step:602/1680 train_time:53792ms step_avg:89.35ms
step:603/1680 train_time:53882ms step_avg:89.36ms
step:604/1680 train_time:53973ms step_avg:89.36ms
step:605/1680 train_time:54063ms step_avg:89.36ms
step:606/1680 train_time:54153ms step_avg:89.36ms
step:607/1680 train_time:54244ms step_avg:89.36ms
step:608/1680 train_time:54334ms step_avg:89.36ms
step:609/1680 train_time:54424ms step_avg:89.37ms
step:610/1680 train_time:54515ms step_avg:89.37ms
step:611/1680 train_time:54605ms step_avg:89.37ms
step:612/1680 train_time:54695ms step_avg:89.37ms
step:613/1680 train_time:54785ms step_avg:89.37ms
step:614/1680 train_time:54877ms step_avg:89.38ms
step:615/1680 train_time:54968ms step_avg:89.38ms
step:616/1680 train_time:55058ms step_avg:89.38ms
step:617/1680 train_time:55148ms step_avg:89.38ms
step:618/1680 train_time:55238ms step_avg:89.38ms
step:619/1680 train_time:55328ms step_avg:89.38ms
step:620/1680 train_time:55419ms step_avg:89.39ms
step:621/1680 train_time:55510ms step_avg:89.39ms
step:622/1680 train_time:55600ms step_avg:89.39ms
step:623/1680 train_time:55690ms step_avg:89.39ms
step:624/1680 train_time:55781ms step_avg:89.39ms
step:625/1680 train_time:55872ms step_avg:89.39ms
step:625/1680 val_loss:3.6143 train_time:55963ms step_avg:89.54ms
step:626/1680 train_time:55987ms step_avg:89.44ms
step:627/1680 train_time:56058ms step_avg:89.41ms
step:628/1680 train_time:56158ms step_avg:89.42ms
step:629/1680 train_time:56249ms step_avg:89.43ms
step:630/1680 train_time:56339ms step_avg:89.43ms
step:631/1680 train_time:56427ms step_avg:89.43ms
step:632/1680 train_time:56517ms step_avg:89.43ms
step:633/1680 train_time:56606ms step_avg:89.42ms
step:634/1680 train_time:56695ms step_avg:89.42ms
step:635/1680 train_time:56783ms step_avg:89.42ms
step:636/1680 train_time:56872ms step_avg:89.42ms
step:637/1680 train_time:56962ms step_avg:89.42ms
step:638/1680 train_time:57057ms step_avg:89.43ms
step:639/1680 train_time:57150ms step_avg:89.44ms
step:640/1680 train_time:57241ms step_avg:89.44ms
step:641/1680 train_time:57331ms step_avg:89.44ms
step:642/1680 train_time:57422ms step_avg:89.44ms
step:643/1680 train_time:57510ms step_avg:89.44ms
step:644/1680 train_time:57599ms step_avg:89.44ms
step:645/1680 train_time:57688ms step_avg:89.44ms
step:646/1680 train_time:57778ms step_avg:89.44ms
step:647/1680 train_time:57867ms step_avg:89.44ms
step:648/1680 train_time:57958ms step_avg:89.44ms
step:649/1680 train_time:58050ms step_avg:89.45ms
step:650/1680 train_time:58143ms step_avg:89.45ms
step:651/1680 train_time:58234ms step_avg:89.45ms
step:652/1680 train_time:58324ms step_avg:89.45ms
step:653/1680 train_time:58415ms step_avg:89.46ms
step:654/1680 train_time:58505ms step_avg:89.46ms
step:655/1680 train_time:58596ms step_avg:89.46ms
step:656/1680 train_time:58685ms step_avg:89.46ms
step:657/1680 train_time:58774ms step_avg:89.46ms
step:658/1680 train_time:58864ms step_avg:89.46ms
step:659/1680 train_time:58954ms step_avg:89.46ms
step:660/1680 train_time:59045ms step_avg:89.46ms
step:661/1680 train_time:59136ms step_avg:89.46ms
step:662/1680 train_time:59228ms step_avg:89.47ms
step:663/1680 train_time:59319ms step_avg:89.47ms
step:664/1680 train_time:59411ms step_avg:89.47ms
step:665/1680 train_time:59500ms step_avg:89.47ms
step:666/1680 train_time:59591ms step_avg:89.48ms
step:667/1680 train_time:59681ms step_avg:89.48ms
step:668/1680 train_time:59771ms step_avg:89.48ms
step:669/1680 train_time:59861ms step_avg:89.48ms
step:670/1680 train_time:59951ms step_avg:89.48ms
step:671/1680 train_time:60041ms step_avg:89.48ms
step:672/1680 train_time:60132ms step_avg:89.48ms
step:673/1680 train_time:60224ms step_avg:89.49ms
step:674/1680 train_time:60316ms step_avg:89.49ms
step:675/1680 train_time:60407ms step_avg:89.49ms
step:676/1680 train_time:60499ms step_avg:89.49ms
step:677/1680 train_time:60589ms step_avg:89.50ms
step:678/1680 train_time:60679ms step_avg:89.50ms
step:679/1680 train_time:60769ms step_avg:89.50ms
step:680/1680 train_time:60859ms step_avg:89.50ms
step:681/1680 train_time:60949ms step_avg:89.50ms
step:682/1680 train_time:61040ms step_avg:89.50ms
step:683/1680 train_time:61130ms step_avg:89.50ms
step:684/1680 train_time:61221ms step_avg:89.51ms
step:685/1680 train_time:61313ms step_avg:89.51ms
step:686/1680 train_time:61403ms step_avg:89.51ms
step:687/1680 train_time:61494ms step_avg:89.51ms
step:688/1680 train_time:61583ms step_avg:89.51ms
step:689/1680 train_time:61673ms step_avg:89.51ms
step:690/1680 train_time:61764ms step_avg:89.51ms
step:691/1680 train_time:61854ms step_avg:89.51ms
step:692/1680 train_time:61944ms step_avg:89.51ms
step:693/1680 train_time:62034ms step_avg:89.52ms
step:694/1680 train_time:62125ms step_avg:89.52ms
step:695/1680 train_time:62217ms step_avg:89.52ms
step:696/1680 train_time:62307ms step_avg:89.52ms
step:697/1680 train_time:62399ms step_avg:89.53ms
step:698/1680 train_time:62491ms step_avg:89.53ms
step:699/1680 train_time:62582ms step_avg:89.53ms
step:700/1680 train_time:62674ms step_avg:89.53ms
step:701/1680 train_time:62763ms step_avg:89.53ms
step:702/1680 train_time:62853ms step_avg:89.53ms
step:703/1680 train_time:62943ms step_avg:89.53ms
step:704/1680 train_time:63033ms step_avg:89.54ms
step:705/1680 train_time:63124ms step_avg:89.54ms
step:706/1680 train_time:63215ms step_avg:89.54ms
step:707/1680 train_time:63305ms step_avg:89.54ms
step:708/1680 train_time:63396ms step_avg:89.54ms
step:709/1680 train_time:63487ms step_avg:89.54ms
step:710/1680 train_time:63578ms step_avg:89.55ms
step:711/1680 train_time:63668ms step_avg:89.55ms
step:712/1680 train_time:63758ms step_avg:89.55ms
step:713/1680 train_time:63848ms step_avg:89.55ms
step:714/1680 train_time:63939ms step_avg:89.55ms
step:715/1680 train_time:64029ms step_avg:89.55ms
step:716/1680 train_time:64120ms step_avg:89.55ms
step:717/1680 train_time:64210ms step_avg:89.55ms
step:718/1680 train_time:64301ms step_avg:89.56ms
step:719/1680 train_time:64391ms step_avg:89.56ms
step:720/1680 train_time:64481ms step_avg:89.56ms
step:721/1680 train_time:64571ms step_avg:89.56ms
step:722/1680 train_time:64662ms step_avg:89.56ms
step:723/1680 train_time:64753ms step_avg:89.56ms
step:724/1680 train_time:64843ms step_avg:89.56ms
step:725/1680 train_time:64933ms step_avg:89.56ms
step:726/1680 train_time:65024ms step_avg:89.56ms
step:727/1680 train_time:65114ms step_avg:89.56ms
step:728/1680 train_time:65204ms step_avg:89.57ms
step:729/1680 train_time:65294ms step_avg:89.57ms
step:730/1680 train_time:65385ms step_avg:89.57ms
step:731/1680 train_time:65476ms step_avg:89.57ms
step:732/1680 train_time:65566ms step_avg:89.57ms
step:733/1680 train_time:65657ms step_avg:89.57ms
step:734/1680 train_time:65747ms step_avg:89.57ms
step:735/1680 train_time:65838ms step_avg:89.58ms
step:736/1680 train_time:65928ms step_avg:89.58ms
step:737/1680 train_time:66020ms step_avg:89.58ms
step:738/1680 train_time:66111ms step_avg:89.58ms
step:739/1680 train_time:66201ms step_avg:89.58ms
step:740/1680 train_time:66292ms step_avg:89.58ms
step:741/1680 train_time:66383ms step_avg:89.59ms
step:742/1680 train_time:66473ms step_avg:89.59ms
step:743/1680 train_time:66564ms step_avg:89.59ms
step:744/1680 train_time:66654ms step_avg:89.59ms
step:745/1680 train_time:66744ms step_avg:89.59ms
step:746/1680 train_time:66835ms step_avg:89.59ms
step:747/1680 train_time:66925ms step_avg:89.59ms
step:748/1680 train_time:67016ms step_avg:89.59ms
step:749/1680 train_time:67106ms step_avg:89.59ms
step:750/1680 train_time:67197ms step_avg:89.60ms
step:750/1680 val_loss:3.5631 train_time:67289ms step_avg:89.72ms
step:751/1680 train_time:67313ms step_avg:89.63ms
step:752/1680 train_time:67384ms step_avg:89.61ms
step:753/1680 train_time:67482ms step_avg:89.62ms
step:754/1680 train_time:67574ms step_avg:89.62ms
step:755/1680 train_time:67664ms step_avg:89.62ms
step:756/1680 train_time:67754ms step_avg:89.62ms
step:757/1680 train_time:67843ms step_avg:89.62ms
step:758/1680 train_time:67932ms step_avg:89.62ms
step:759/1680 train_time:68021ms step_avg:89.62ms
step:760/1680 train_time:68110ms step_avg:89.62ms
step:761/1680 train_time:68200ms step_avg:89.62ms
step:762/1680 train_time:68291ms step_avg:89.62ms
step:763/1680 train_time:68384ms step_avg:89.63ms
step:764/1680 train_time:68477ms step_avg:89.63ms
step:765/1680 train_time:68569ms step_avg:89.63ms
step:766/1680 train_time:68660ms step_avg:89.63ms
step:767/1680 train_time:68750ms step_avg:89.64ms
step:768/1680 train_time:68840ms step_avg:89.64ms
step:769/1680 train_time:68930ms step_avg:89.64ms
step:770/1680 train_time:69019ms step_avg:89.64ms
step:771/1680 train_time:69108ms step_avg:89.63ms
step:772/1680 train_time:69198ms step_avg:89.63ms
step:773/1680 train_time:69289ms step_avg:89.64ms
step:774/1680 train_time:69380ms step_avg:89.64ms
step:775/1680 train_time:69472ms step_avg:89.64ms
step:776/1680 train_time:69564ms step_avg:89.64ms
step:777/1680 train_time:69655ms step_avg:89.65ms
step:778/1680 train_time:69745ms step_avg:89.65ms
step:779/1680 train_time:69835ms step_avg:89.65ms
step:780/1680 train_time:69925ms step_avg:89.65ms
step:781/1680 train_time:70015ms step_avg:89.65ms
step:782/1680 train_time:70104ms step_avg:89.65ms
step:783/1680 train_time:70194ms step_avg:89.65ms
step:784/1680 train_time:70284ms step_avg:89.65ms
step:785/1680 train_time:70375ms step_avg:89.65ms
step:786/1680 train_time:70466ms step_avg:89.65ms
step:787/1680 train_time:70558ms step_avg:89.65ms
step:788/1680 train_time:70649ms step_avg:89.66ms
step:789/1680 train_time:70740ms step_avg:89.66ms
step:790/1680 train_time:70830ms step_avg:89.66ms
step:791/1680 train_time:70920ms step_avg:89.66ms
step:792/1680 train_time:71010ms step_avg:89.66ms
step:793/1680 train_time:71100ms step_avg:89.66ms
step:794/1680 train_time:71191ms step_avg:89.66ms
step:795/1680 train_time:71281ms step_avg:89.66ms
step:796/1680 train_time:71372ms step_avg:89.66ms
step:797/1680 train_time:71463ms step_avg:89.67ms
step:798/1680 train_time:71555ms step_avg:89.67ms
step:799/1680 train_time:71645ms step_avg:89.67ms
step:800/1680 train_time:71736ms step_avg:89.67ms
step:801/1680 train_time:71826ms step_avg:89.67ms
step:802/1680 train_time:71917ms step_avg:89.67ms
step:803/1680 train_time:72007ms step_avg:89.67ms
step:804/1680 train_time:72097ms step_avg:89.67ms
step:805/1680 train_time:72187ms step_avg:89.67ms
step:806/1680 train_time:72278ms step_avg:89.67ms
step:807/1680 train_time:72369ms step_avg:89.68ms
step:808/1680 train_time:72461ms step_avg:89.68ms
step:809/1680 train_time:72552ms step_avg:89.68ms
step:810/1680 train_time:72642ms step_avg:89.68ms
step:811/1680 train_time:72732ms step_avg:89.68ms
step:812/1680 train_time:72822ms step_avg:89.68ms
step:813/1680 train_time:72913ms step_avg:89.68ms
step:814/1680 train_time:73002ms step_avg:89.68ms
step:815/1680 train_time:73093ms step_avg:89.68ms
step:816/1680 train_time:73183ms step_avg:89.69ms
step:817/1680 train_time:73273ms step_avg:89.69ms
step:818/1680 train_time:73364ms step_avg:89.69ms
step:819/1680 train_time:73455ms step_avg:89.69ms
step:820/1680 train_time:73544ms step_avg:89.69ms
step:821/1680 train_time:73636ms step_avg:89.69ms
step:822/1680 train_time:73727ms step_avg:89.69ms
step:823/1680 train_time:73818ms step_avg:89.69ms
step:824/1680 train_time:73909ms step_avg:89.69ms
step:825/1680 train_time:73999ms step_avg:89.70ms
step:826/1680 train_time:74089ms step_avg:89.70ms
step:827/1680 train_time:74180ms step_avg:89.70ms
step:828/1680 train_time:74270ms step_avg:89.70ms
step:829/1680 train_time:74361ms step_avg:89.70ms
step:830/1680 train_time:74451ms step_avg:89.70ms
step:831/1680 train_time:74541ms step_avg:89.70ms
step:832/1680 train_time:74631ms step_avg:89.70ms
step:833/1680 train_time:74721ms step_avg:89.70ms
step:834/1680 train_time:74812ms step_avg:89.70ms
step:835/1680 train_time:74902ms step_avg:89.70ms
step:836/1680 train_time:74993ms step_avg:89.70ms
step:837/1680 train_time:75083ms step_avg:89.70ms
step:838/1680 train_time:75173ms step_avg:89.70ms
step:839/1680 train_time:75264ms step_avg:89.71ms
step:840/1680 train_time:75355ms step_avg:89.71ms
step:841/1680 train_time:75445ms step_avg:89.71ms
step:842/1680 train_time:75536ms step_avg:89.71ms
step:843/1680 train_time:75627ms step_avg:89.71ms
step:844/1680 train_time:75718ms step_avg:89.71ms
step:845/1680 train_time:75809ms step_avg:89.71ms
step:846/1680 train_time:75899ms step_avg:89.72ms
step:847/1680 train_time:75989ms step_avg:89.72ms
step:848/1680 train_time:76079ms step_avg:89.72ms
step:849/1680 train_time:76169ms step_avg:89.72ms
step:850/1680 train_time:76260ms step_avg:89.72ms
step:851/1680 train_time:76351ms step_avg:89.72ms
step:852/1680 train_time:76441ms step_avg:89.72ms
step:853/1680 train_time:76531ms step_avg:89.72ms
step:854/1680 train_time:76621ms step_avg:89.72ms
step:855/1680 train_time:76712ms step_avg:89.72ms
step:856/1680 train_time:76802ms step_avg:89.72ms
step:857/1680 train_time:76894ms step_avg:89.72ms
step:858/1680 train_time:76984ms step_avg:89.73ms
step:859/1680 train_time:77074ms step_avg:89.73ms
step:860/1680 train_time:77165ms step_avg:89.73ms
step:861/1680 train_time:77256ms step_avg:89.73ms
step:862/1680 train_time:77345ms step_avg:89.73ms
step:863/1680 train_time:77436ms step_avg:89.73ms
step:864/1680 train_time:77526ms step_avg:89.73ms
step:865/1680 train_time:77617ms step_avg:89.73ms
step:866/1680 train_time:77707ms step_avg:89.73ms
step:867/1680 train_time:77798ms step_avg:89.73ms
step:868/1680 train_time:77888ms step_avg:89.73ms
step:869/1680 train_time:77979ms step_avg:89.73ms
step:870/1680 train_time:78069ms step_avg:89.73ms
step:871/1680 train_time:78160ms step_avg:89.74ms
step:872/1680 train_time:78251ms step_avg:89.74ms
step:873/1680 train_time:78341ms step_avg:89.74ms
step:874/1680 train_time:78431ms step_avg:89.74ms
step:875/1680 train_time:78521ms step_avg:89.74ms
step:875/1680 val_loss:3.5187 train_time:78612ms step_avg:89.84ms
step:876/1680 train_time:78636ms step_avg:89.77ms
step:877/1680 train_time:78706ms step_avg:89.74ms
step:878/1680 train_time:78805ms step_avg:89.76ms
step:879/1680 train_time:78897ms step_avg:89.76ms
step:880/1680 train_time:78986ms step_avg:89.76ms
step:881/1680 train_time:79075ms step_avg:89.76ms
step:882/1680 train_time:79164ms step_avg:89.76ms
step:883/1680 train_time:79254ms step_avg:89.75ms
step:884/1680 train_time:79342ms step_avg:89.75ms
step:885/1680 train_time:79432ms step_avg:89.75ms
step:886/1680 train_time:79521ms step_avg:89.75ms
step:887/1680 train_time:79612ms step_avg:89.75ms
step:888/1680 train_time:79706ms step_avg:89.76ms
step:889/1680 train_time:79801ms step_avg:89.77ms
step:890/1680 train_time:79893ms step_avg:89.77ms
step:891/1680 train_time:79983ms step_avg:89.77ms
step:892/1680 train_time:80073ms step_avg:89.77ms
step:893/1680 train_time:80163ms step_avg:89.77ms
step:894/1680 train_time:80253ms step_avg:89.77ms
step:895/1680 train_time:80342ms step_avg:89.77ms
step:896/1680 train_time:80431ms step_avg:89.77ms
step:897/1680 train_time:80520ms step_avg:89.77ms
step:898/1680 train_time:80611ms step_avg:89.77ms
step:899/1680 train_time:80703ms step_avg:89.77ms
step:900/1680 train_time:80796ms step_avg:89.77ms
step:901/1680 train_time:80886ms step_avg:89.77ms
step:902/1680 train_time:80976ms step_avg:89.77ms
step:903/1680 train_time:81066ms step_avg:89.77ms
step:904/1680 train_time:81156ms step_avg:89.77ms
step:905/1680 train_time:81246ms step_avg:89.77ms
step:906/1680 train_time:81336ms step_avg:89.77ms
step:907/1680 train_time:81425ms step_avg:89.77ms
step:908/1680 train_time:81515ms step_avg:89.77ms
step:909/1680 train_time:81606ms step_avg:89.78ms
step:910/1680 train_time:81697ms step_avg:89.78ms
step:911/1680 train_time:81788ms step_avg:89.78ms
step:912/1680 train_time:81880ms step_avg:89.78ms
step:913/1680 train_time:81972ms step_avg:89.78ms
step:914/1680 train_time:82062ms step_avg:89.78ms
step:915/1680 train_time:82153ms step_avg:89.78ms
step:916/1680 train_time:82243ms step_avg:89.78ms
step:917/1680 train_time:82333ms step_avg:89.78ms
step:918/1680 train_time:82423ms step_avg:89.79ms
step:919/1680 train_time:82513ms step_avg:89.79ms
step:920/1680 train_time:82603ms step_avg:89.79ms
step:921/1680 train_time:82694ms step_avg:89.79ms
step:922/1680 train_time:82784ms step_avg:89.79ms
step:923/1680 train_time:82875ms step_avg:89.79ms
step:924/1680 train_time:82965ms step_avg:89.79ms
step:925/1680 train_time:83057ms step_avg:89.79ms
step:926/1680 train_time:83146ms step_avg:89.79ms
step:927/1680 train_time:83237ms step_avg:89.79ms
step:928/1680 train_time:83326ms step_avg:89.79ms
step:929/1680 train_time:83417ms step_avg:89.79ms
step:930/1680 train_time:83507ms step_avg:89.79ms
step:931/1680 train_time:83599ms step_avg:89.79ms
step:932/1680 train_time:83690ms step_avg:89.80ms
step:933/1680 train_time:83780ms step_avg:89.80ms
step:934/1680 train_time:83871ms step_avg:89.80ms
step:935/1680 train_time:83962ms step_avg:89.80ms
step:936/1680 train_time:84053ms step_avg:89.80ms
step:937/1680 train_time:84143ms step_avg:89.80ms
step:938/1680 train_time:84233ms step_avg:89.80ms
step:939/1680 train_time:84323ms step_avg:89.80ms
step:940/1680 train_time:84413ms step_avg:89.80ms
step:941/1680 train_time:84504ms step_avg:89.80ms
step:942/1680 train_time:84594ms step_avg:89.80ms
step:943/1680 train_time:84685ms step_avg:89.80ms
step:944/1680 train_time:84776ms step_avg:89.80ms
step:945/1680 train_time:84866ms step_avg:89.81ms
step:946/1680 train_time:84958ms step_avg:89.81ms
step:947/1680 train_time:85048ms step_avg:89.81ms
step:948/1680 train_time:85139ms step_avg:89.81ms
step:949/1680 train_time:85229ms step_avg:89.81ms
step:950/1680 train_time:85319ms step_avg:89.81ms
step:951/1680 train_time:85410ms step_avg:89.81ms
step:952/1680 train_time:85501ms step_avg:89.81ms
step:953/1680 train_time:85592ms step_avg:89.81ms
step:954/1680 train_time:85682ms step_avg:89.81ms
step:955/1680 train_time:85773ms step_avg:89.81ms
step:956/1680 train_time:85864ms step_avg:89.82ms
step:957/1680 train_time:85954ms step_avg:89.82ms
step:958/1680 train_time:86045ms step_avg:89.82ms
step:959/1680 train_time:86135ms step_avg:89.82ms
step:960/1680 train_time:86225ms step_avg:89.82ms
step:961/1680 train_time:86315ms step_avg:89.82ms
step:962/1680 train_time:86405ms step_avg:89.82ms
step:963/1680 train_time:86496ms step_avg:89.82ms
step:964/1680 train_time:86586ms step_avg:89.82ms
step:965/1680 train_time:86677ms step_avg:89.82ms
step:966/1680 train_time:86767ms step_avg:89.82ms
step:967/1680 train_time:86859ms step_avg:89.82ms
step:968/1680 train_time:86949ms step_avg:89.82ms
step:969/1680 train_time:87039ms step_avg:89.82ms
step:970/1680 train_time:87129ms step_avg:89.82ms
step:971/1680 train_time:87219ms step_avg:89.82ms
step:972/1680 train_time:87309ms step_avg:89.82ms
step:973/1680 train_time:87400ms step_avg:89.83ms
step:974/1680 train_time:87491ms step_avg:89.83ms
step:975/1680 train_time:87582ms step_avg:89.83ms
step:976/1680 train_time:87672ms step_avg:89.83ms
step:977/1680 train_time:87763ms step_avg:89.83ms
step:978/1680 train_time:87854ms step_avg:89.83ms
step:979/1680 train_time:87944ms step_avg:89.83ms
step:980/1680 train_time:88034ms step_avg:89.83ms
step:981/1680 train_time:88125ms step_avg:89.83ms
step:982/1680 train_time:88216ms step_avg:89.83ms
step:983/1680 train_time:88307ms step_avg:89.83ms
step:984/1680 train_time:88397ms step_avg:89.83ms
step:985/1680 train_time:88487ms step_avg:89.83ms
step:986/1680 train_time:88578ms step_avg:89.84ms
step:987/1680 train_time:88669ms step_avg:89.84ms
step:988/1680 train_time:88760ms step_avg:89.84ms
step:989/1680 train_time:88851ms step_avg:89.84ms
step:990/1680 train_time:88942ms step_avg:89.84ms
step:991/1680 train_time:89032ms step_avg:89.84ms
step:992/1680 train_time:89122ms step_avg:89.84ms
step:993/1680 train_time:89212ms step_avg:89.84ms
step:994/1680 train_time:89303ms step_avg:89.84ms
step:995/1680 train_time:89393ms step_avg:89.84ms
step:996/1680 train_time:89483ms step_avg:89.84ms
step:997/1680 train_time:89574ms step_avg:89.84ms
step:998/1680 train_time:89664ms step_avg:89.84ms
step:999/1680 train_time:89755ms step_avg:89.85ms
step:1000/1680 train_time:89846ms step_avg:89.85ms
step:1000/1680 val_loss:3.4697 train_time:89938ms step_avg:89.94ms
step:1001/1680 train_time:89962ms step_avg:89.87ms
step:1002/1680 train_time:90030ms step_avg:89.85ms
step:1003/1680 train_time:90127ms step_avg:89.86ms
step:1004/1680 train_time:90218ms step_avg:89.86ms
step:1005/1680 train_time:90307ms step_avg:89.86ms
step:1006/1680 train_time:90397ms step_avg:89.86ms
step:1007/1680 train_time:90485ms step_avg:89.86ms
step:1008/1680 train_time:90575ms step_avg:89.86ms
step:1009/1680 train_time:90664ms step_avg:89.86ms
step:1010/1680 train_time:90753ms step_avg:89.85ms
step:1011/1680 train_time:90842ms step_avg:89.85ms
step:1012/1680 train_time:90933ms step_avg:89.85ms
step:1013/1680 train_time:91024ms step_avg:89.86ms
step:1014/1680 train_time:91116ms step_avg:89.86ms
step:1015/1680 train_time:91208ms step_avg:89.86ms
step:1016/1680 train_time:91298ms step_avg:89.86ms
step:1017/1680 train_time:91387ms step_avg:89.86ms
step:1018/1680 train_time:91477ms step_avg:89.86ms
step:1019/1680 train_time:91566ms step_avg:89.86ms
step:1020/1680 train_time:91655ms step_avg:89.86ms
step:1021/1680 train_time:91745ms step_avg:89.86ms
step:1022/1680 train_time:91836ms step_avg:89.86ms
step:1023/1680 train_time:91927ms step_avg:89.86ms
step:1024/1680 train_time:92019ms step_avg:89.86ms
step:1025/1680 train_time:92110ms step_avg:89.86ms
step:1026/1680 train_time:92201ms step_avg:89.86ms
step:1027/1680 train_time:92292ms step_avg:89.87ms
step:1028/1680 train_time:92381ms step_avg:89.86ms
step:1029/1680 train_time:92470ms step_avg:89.86ms
step:1030/1680 train_time:92560ms step_avg:89.86ms
step:1031/1680 train_time:92650ms step_avg:89.86ms
step:1032/1680 train_time:92740ms step_avg:89.86ms
step:1033/1680 train_time:92831ms step_avg:89.87ms
step:1034/1680 train_time:92922ms step_avg:89.87ms
step:1035/1680 train_time:93014ms step_avg:89.87ms
step:1036/1680 train_time:93105ms step_avg:89.87ms
step:1037/1680 train_time:93196ms step_avg:89.87ms
step:1038/1680 train_time:93286ms step_avg:89.87ms
step:1039/1680 train_time:93377ms step_avg:89.87ms
step:1040/1680 train_time:93467ms step_avg:89.87ms
step:1041/1680 train_time:93558ms step_avg:89.87ms
step:1042/1680 train_time:93647ms step_avg:89.87ms
step:1043/1680 train_time:93738ms step_avg:89.87ms
step:1044/1680 train_time:93827ms step_avg:89.87ms
step:1045/1680 train_time:93918ms step_avg:89.87ms
step:1046/1680 train_time:94009ms step_avg:89.87ms
step:1047/1680 train_time:94101ms step_avg:89.88ms
step:1048/1680 train_time:94192ms step_avg:89.88ms
step:1049/1680 train_time:94283ms step_avg:89.88ms
step:1050/1680 train_time:94373ms step_avg:89.88ms
step:1051/1680 train_time:94463ms step_avg:89.88ms
step:1052/1680 train_time:94554ms step_avg:89.88ms
step:1053/1680 train_time:94644ms step_avg:89.88ms
step:1054/1680 train_time:94734ms step_avg:89.88ms
step:1055/1680 train_time:94824ms step_avg:89.88ms
step:1056/1680 train_time:94914ms step_avg:89.88ms
step:1057/1680 train_time:95005ms step_avg:89.88ms
step:1058/1680 train_time:95097ms step_avg:89.88ms
step:1059/1680 train_time:95187ms step_avg:89.88ms
step:1060/1680 train_time:95278ms step_avg:89.89ms
step:1061/1680 train_time:95368ms step_avg:89.89ms
step:1062/1680 train_time:95459ms step_avg:89.89ms
step:1063/1680 train_time:95549ms step_avg:89.89ms
step:1064/1680 train_time:95640ms step_avg:89.89ms
step:1065/1680 train_time:95730ms step_avg:89.89ms
step:1066/1680 train_time:95820ms step_avg:89.89ms
step:1067/1680 train_time:95911ms step_avg:89.89ms
step:1068/1680 train_time:96002ms step_avg:89.89ms
step:1069/1680 train_time:96093ms step_avg:89.89ms
step:1070/1680 train_time:96183ms step_avg:89.89ms
step:1071/1680 train_time:96274ms step_avg:89.89ms
step:1072/1680 train_time:96364ms step_avg:89.89ms
step:1073/1680 train_time:96454ms step_avg:89.89ms
step:1074/1680 train_time:96545ms step_avg:89.89ms
step:1075/1680 train_time:96635ms step_avg:89.89ms
step:1076/1680 train_time:96726ms step_avg:89.89ms
step:1077/1680 train_time:96816ms step_avg:89.89ms
step:1078/1680 train_time:96907ms step_avg:89.89ms
step:1079/1680 train_time:96998ms step_avg:89.90ms
step:1080/1680 train_time:97088ms step_avg:89.90ms
step:1081/1680 train_time:97179ms step_avg:89.90ms
step:1082/1680 train_time:97270ms step_avg:89.90ms
step:1083/1680 train_time:97360ms step_avg:89.90ms
step:1084/1680 train_time:97450ms step_avg:89.90ms
step:1085/1680 train_time:97541ms step_avg:89.90ms
step:1086/1680 train_time:97631ms step_avg:89.90ms
step:1087/1680 train_time:97722ms step_avg:89.90ms
step:1088/1680 train_time:97813ms step_avg:89.90ms
step:1089/1680 train_time:97903ms step_avg:89.90ms
step:1090/1680 train_time:97993ms step_avg:89.90ms
step:1091/1680 train_time:98083ms step_avg:89.90ms
step:1092/1680 train_time:98174ms step_avg:89.90ms
step:1093/1680 train_time:98264ms step_avg:89.90ms
step:1094/1680 train_time:98354ms step_avg:89.90ms
step:1095/1680 train_time:98445ms step_avg:89.90ms
step:1096/1680 train_time:98536ms step_avg:89.91ms
step:1097/1680 train_time:98626ms step_avg:89.91ms
step:1098/1680 train_time:98718ms step_avg:89.91ms
step:1099/1680 train_time:98811ms step_avg:89.91ms
step:1100/1680 train_time:98902ms step_avg:89.91ms
step:1101/1680 train_time:98993ms step_avg:89.91ms
step:1102/1680 train_time:99085ms step_avg:89.91ms
step:1103/1680 train_time:99175ms step_avg:89.91ms
step:1104/1680 train_time:99267ms step_avg:89.92ms
step:1105/1680 train_time:99357ms step_avg:89.92ms
step:1106/1680 train_time:99448ms step_avg:89.92ms
step:1107/1680 train_time:99540ms step_avg:89.92ms
step:1108/1680 train_time:99631ms step_avg:89.92ms
step:1109/1680 train_time:99723ms step_avg:89.92ms
step:1110/1680 train_time:99815ms step_avg:89.92ms
step:1111/1680 train_time:99906ms step_avg:89.92ms
step:1112/1680 train_time:99998ms step_avg:89.93ms
step:1113/1680 train_time:100088ms step_avg:89.93ms
step:1114/1680 train_time:100179ms step_avg:89.93ms
step:1115/1680 train_time:100271ms step_avg:89.93ms
step:1116/1680 train_time:100361ms step_avg:89.93ms
step:1117/1680 train_time:100452ms step_avg:89.93ms
step:1118/1680 train_time:100543ms step_avg:89.93ms
step:1119/1680 train_time:100635ms step_avg:89.93ms
step:1120/1680 train_time:100725ms step_avg:89.93ms
step:1121/1680 train_time:100817ms step_avg:89.93ms
step:1122/1680 train_time:100908ms step_avg:89.94ms
step:1123/1680 train_time:101000ms step_avg:89.94ms
step:1124/1680 train_time:101090ms step_avg:89.94ms
step:1125/1680 train_time:101181ms step_avg:89.94ms
step:1125/1680 val_loss:3.4161 train_time:101273ms step_avg:90.02ms
step:1126/1680 train_time:101297ms step_avg:89.96ms
step:1127/1680 train_time:101367ms step_avg:89.94ms
step:1128/1680 train_time:101468ms step_avg:89.95ms
step:1129/1680 train_time:101561ms step_avg:89.96ms
step:1130/1680 train_time:101652ms step_avg:89.96ms
step:1131/1680 train_time:101741ms step_avg:89.96ms
step:1132/1680 train_time:101831ms step_avg:89.96ms
step:1133/1680 train_time:101921ms step_avg:89.96ms
step:1134/1680 train_time:102011ms step_avg:89.96ms
step:1135/1680 train_time:102100ms step_avg:89.96ms
step:1136/1680 train_time:102190ms step_avg:89.96ms
step:1137/1680 train_time:102282ms step_avg:89.96ms
step:1138/1680 train_time:102375ms step_avg:89.96ms
step:1139/1680 train_time:102469ms step_avg:89.96ms
step:1140/1680 train_time:102562ms step_avg:89.97ms
step:1141/1680 train_time:102653ms step_avg:89.97ms
step:1142/1680 train_time:102743ms step_avg:89.97ms
step:1143/1680 train_time:102833ms step_avg:89.97ms
step:1144/1680 train_time:102924ms step_avg:89.97ms
step:1145/1680 train_time:103014ms step_avg:89.97ms
step:1146/1680 train_time:103103ms step_avg:89.97ms
step:1147/1680 train_time:103193ms step_avg:89.97ms
step:1148/1680 train_time:103284ms step_avg:89.97ms
step:1149/1680 train_time:103376ms step_avg:89.97ms
step:1150/1680 train_time:103469ms step_avg:89.97ms
step:1151/1680 train_time:103561ms step_avg:89.97ms
step:1152/1680 train_time:103652ms step_avg:89.98ms
step:1153/1680 train_time:103743ms step_avg:89.98ms
step:1154/1680 train_time:103833ms step_avg:89.98ms
step:1155/1680 train_time:103925ms step_avg:89.98ms
step:1156/1680 train_time:104015ms step_avg:89.98ms
step:1157/1680 train_time:104105ms step_avg:89.98ms
step:1158/1680 train_time:104196ms step_avg:89.98ms
step:1159/1680 train_time:104286ms step_avg:89.98ms
step:1160/1680 train_time:104379ms step_avg:89.98ms
step:1161/1680 train_time:104471ms step_avg:89.98ms
step:1162/1680 train_time:104563ms step_avg:89.99ms
step:1163/1680 train_time:104655ms step_avg:89.99ms
step:1164/1680 train_time:104747ms step_avg:89.99ms
step:1165/1680 train_time:104838ms step_avg:89.99ms
step:1166/1680 train_time:104928ms step_avg:89.99ms
step:1167/1680 train_time:105019ms step_avg:89.99ms
step:1168/1680 train_time:105110ms step_avg:89.99ms
step:1169/1680 train_time:105201ms step_avg:89.99ms
step:1170/1680 train_time:105292ms step_avg:89.99ms
step:1171/1680 train_time:105384ms step_avg:89.99ms
step:1172/1680 train_time:105474ms step_avg:90.00ms
step:1173/1680 train_time:105565ms step_avg:90.00ms
step:1174/1680 train_time:105658ms step_avg:90.00ms
step:1175/1680 train_time:105749ms step_avg:90.00ms
step:1176/1680 train_time:105840ms step_avg:90.00ms
step:1177/1680 train_time:105930ms step_avg:90.00ms
step:1178/1680 train_time:106022ms step_avg:90.00ms
step:1179/1680 train_time:106112ms step_avg:90.00ms
step:1180/1680 train_time:106203ms step_avg:90.00ms
step:1181/1680 train_time:106295ms step_avg:90.00ms
step:1182/1680 train_time:106386ms step_avg:90.01ms
step:1183/1680 train_time:106478ms step_avg:90.01ms
step:1184/1680 train_time:106569ms step_avg:90.01ms
step:1185/1680 train_time:106660ms step_avg:90.01ms
step:1186/1680 train_time:106751ms step_avg:90.01ms
step:1187/1680 train_time:106842ms step_avg:90.01ms
step:1188/1680 train_time:106932ms step_avg:90.01ms
step:1189/1680 train_time:107023ms step_avg:90.01ms
step:1190/1680 train_time:107115ms step_avg:90.01ms
step:1191/1680 train_time:107206ms step_avg:90.01ms
step:1192/1680 train_time:107297ms step_avg:90.01ms
step:1193/1680 train_time:107388ms step_avg:90.02ms
step:1194/1680 train_time:107479ms step_avg:90.02ms
step:1195/1680 train_time:107571ms step_avg:90.02ms
step:1196/1680 train_time:107662ms step_avg:90.02ms
step:1197/1680 train_time:107753ms step_avg:90.02ms
step:1198/1680 train_time:107844ms step_avg:90.02ms
step:1199/1680 train_time:107934ms step_avg:90.02ms
step:1200/1680 train_time:108024ms step_avg:90.02ms
step:1201/1680 train_time:108116ms step_avg:90.02ms
step:1202/1680 train_time:108206ms step_avg:90.02ms
step:1203/1680 train_time:108296ms step_avg:90.02ms
step:1204/1680 train_time:108387ms step_avg:90.02ms
step:1205/1680 train_time:108478ms step_avg:90.02ms
step:1206/1680 train_time:108569ms step_avg:90.02ms
step:1207/1680 train_time:108660ms step_avg:90.03ms
step:1208/1680 train_time:108752ms step_avg:90.03ms
step:1209/1680 train_time:108843ms step_avg:90.03ms
step:1210/1680 train_time:108933ms step_avg:90.03ms
step:1211/1680 train_time:109024ms step_avg:90.03ms
step:1212/1680 train_time:109115ms step_avg:90.03ms
step:1213/1680 train_time:109206ms step_avg:90.03ms
step:1214/1680 train_time:109297ms step_avg:90.03ms
step:1215/1680 train_time:109387ms step_avg:90.03ms
step:1216/1680 train_time:109479ms step_avg:90.03ms
step:1217/1680 train_time:109570ms step_avg:90.03ms
step:1218/1680 train_time:109661ms step_avg:90.03ms
step:1219/1680 train_time:109752ms step_avg:90.03ms
step:1220/1680 train_time:109844ms step_avg:90.04ms
step:1221/1680 train_time:109936ms step_avg:90.04ms
step:1222/1680 train_time:110026ms step_avg:90.04ms
step:1223/1680 train_time:110117ms step_avg:90.04ms
step:1224/1680 train_time:110208ms step_avg:90.04ms
step:1225/1680 train_time:110299ms step_avg:90.04ms
step:1226/1680 train_time:110391ms step_avg:90.04ms
step:1227/1680 train_time:110482ms step_avg:90.04ms
step:1228/1680 train_time:110573ms step_avg:90.04ms
step:1229/1680 train_time:110664ms step_avg:90.04ms
step:1230/1680 train_time:110755ms step_avg:90.04ms
step:1231/1680 train_time:110846ms step_avg:90.05ms
step:1232/1680 train_time:110938ms step_avg:90.05ms
step:1233/1680 train_time:111029ms step_avg:90.05ms
step:1234/1680 train_time:111121ms step_avg:90.05ms
step:1235/1680 train_time:111212ms step_avg:90.05ms
step:1236/1680 train_time:111302ms step_avg:90.05ms
step:1237/1680 train_time:111393ms step_avg:90.05ms
step:1238/1680 train_time:111484ms step_avg:90.05ms
step:1239/1680 train_time:111574ms step_avg:90.05ms
step:1240/1680 train_time:111665ms step_avg:90.05ms
step:1241/1680 train_time:111756ms step_avg:90.05ms
step:1242/1680 train_time:111847ms step_avg:90.05ms
step:1243/1680 train_time:111938ms step_avg:90.05ms
step:1244/1680 train_time:112029ms step_avg:90.06ms
step:1245/1680 train_time:112121ms step_avg:90.06ms
step:1246/1680 train_time:112211ms step_avg:90.06ms
step:1247/1680 train_time:112302ms step_avg:90.06ms
step:1248/1680 train_time:112393ms step_avg:90.06ms
step:1249/1680 train_time:112483ms step_avg:90.06ms
step:1250/1680 train_time:112574ms step_avg:90.06ms
step:1250/1680 val_loss:3.3773 train_time:112667ms step_avg:90.13ms
step:1251/1680 train_time:112691ms step_avg:90.08ms
step:1252/1680 train_time:112761ms step_avg:90.06ms
step:1253/1680 train_time:112858ms step_avg:90.07ms
step:1254/1680 train_time:112951ms step_avg:90.07ms
step:1255/1680 train_time:113042ms step_avg:90.07ms
step:1256/1680 train_time:113133ms step_avg:90.07ms
step:1257/1680 train_time:113223ms step_avg:90.07ms
step:1258/1680 train_time:113313ms step_avg:90.07ms
step:1259/1680 train_time:113403ms step_avg:90.07ms
step:1260/1680 train_time:113492ms step_avg:90.07ms
step:1261/1680 train_time:113582ms step_avg:90.07ms
step:1262/1680 train_time:113674ms step_avg:90.07ms
step:1263/1680 train_time:113768ms step_avg:90.08ms
step:1264/1680 train_time:113861ms step_avg:90.08ms
step:1265/1680 train_time:113953ms step_avg:90.08ms
step:1266/1680 train_time:114044ms step_avg:90.08ms
step:1267/1680 train_time:114135ms step_avg:90.08ms
step:1268/1680 train_time:114226ms step_avg:90.08ms
step:1269/1680 train_time:114316ms step_avg:90.08ms
step:1270/1680 train_time:114407ms step_avg:90.08ms
step:1271/1680 train_time:114496ms step_avg:90.08ms
step:1272/1680 train_time:114586ms step_avg:90.08ms
step:1273/1680 train_time:114678ms step_avg:90.09ms
step:1274/1680 train_time:114772ms step_avg:90.09ms
step:1275/1680 train_time:114864ms step_avg:90.09ms
step:1276/1680 train_time:114956ms step_avg:90.09ms
step:1277/1680 train_time:115047ms step_avg:90.09ms
step:1278/1680 train_time:115137ms step_avg:90.09ms
step:1279/1680 train_time:115227ms step_avg:90.09ms
step:1280/1680 train_time:115318ms step_avg:90.09ms
step:1281/1680 train_time:115408ms step_avg:90.09ms
step:1282/1680 train_time:115498ms step_avg:90.09ms
step:1283/1680 train_time:115589ms step_avg:90.09ms
step:1284/1680 train_time:115681ms step_avg:90.09ms
step:1285/1680 train_time:115773ms step_avg:90.10ms
step:1286/1680 train_time:115865ms step_avg:90.10ms
step:1287/1680 train_time:115957ms step_avg:90.10ms
step:1288/1680 train_time:116048ms step_avg:90.10ms
step:1289/1680 train_time:116138ms step_avg:90.10ms
step:1290/1680 train_time:116230ms step_avg:90.10ms
step:1291/1680 train_time:116320ms step_avg:90.10ms
step:1292/1680 train_time:116411ms step_avg:90.10ms
step:1293/1680 train_time:116502ms step_avg:90.10ms
step:1294/1680 train_time:116593ms step_avg:90.10ms
step:1295/1680 train_time:116684ms step_avg:90.10ms
step:1296/1680 train_time:116776ms step_avg:90.10ms
step:1297/1680 train_time:116867ms step_avg:90.11ms
step:1298/1680 train_time:116960ms step_avg:90.11ms
step:1299/1680 train_time:117051ms step_avg:90.11ms
step:1300/1680 train_time:117142ms step_avg:90.11ms
step:1301/1680 train_time:117233ms step_avg:90.11ms
step:1302/1680 train_time:117323ms step_avg:90.11ms
step:1303/1680 train_time:117414ms step_avg:90.11ms
step:1304/1680 train_time:117505ms step_avg:90.11ms
step:1305/1680 train_time:117595ms step_avg:90.11ms
step:1306/1680 train_time:117685ms step_avg:90.11ms
step:1307/1680 train_time:117777ms step_avg:90.11ms
step:1308/1680 train_time:117869ms step_avg:90.11ms
step:1309/1680 train_time:117960ms step_avg:90.11ms
step:1310/1680 train_time:118053ms step_avg:90.12ms
step:1311/1680 train_time:118145ms step_avg:90.12ms
step:1312/1680 train_time:118235ms step_avg:90.12ms
step:1313/1680 train_time:118327ms step_avg:90.12ms
step:1314/1680 train_time:118417ms step_avg:90.12ms
step:1315/1680 train_time:118508ms step_avg:90.12ms
step:1316/1680 train_time:118598ms step_avg:90.12ms
step:1317/1680 train_time:118688ms step_avg:90.12ms
step:1318/1680 train_time:118779ms step_avg:90.12ms
step:1319/1680 train_time:118871ms step_avg:90.12ms
step:1320/1680 train_time:118962ms step_avg:90.12ms
step:1321/1680 train_time:119053ms step_avg:90.12ms
step:1322/1680 train_time:119145ms step_avg:90.12ms
step:1323/1680 train_time:119236ms step_avg:90.13ms
step:1324/1680 train_time:119328ms step_avg:90.13ms
step:1325/1680 train_time:119419ms step_avg:90.13ms
step:1326/1680 train_time:119509ms step_avg:90.13ms
step:1327/1680 train_time:119598ms step_avg:90.13ms
step:1328/1680 train_time:119689ms step_avg:90.13ms
step:1329/1680 train_time:119780ms step_avg:90.13ms
step:1330/1680 train_time:119871ms step_avg:90.13ms
step:1331/1680 train_time:119962ms step_avg:90.13ms
step:1332/1680 train_time:120054ms step_avg:90.13ms
step:1333/1680 train_time:120145ms step_avg:90.13ms
step:1334/1680 train_time:120236ms step_avg:90.13ms
step:1335/1680 train_time:120327ms step_avg:90.13ms
step:1336/1680 train_time:120418ms step_avg:90.13ms
step:1337/1680 train_time:120509ms step_avg:90.13ms
step:1338/1680 train_time:120599ms step_avg:90.13ms
step:1339/1680 train_time:120690ms step_avg:90.13ms
step:1340/1680 train_time:120780ms step_avg:90.13ms
step:1341/1680 train_time:120871ms step_avg:90.14ms
step:1342/1680 train_time:120963ms step_avg:90.14ms
step:1343/1680 train_time:121055ms step_avg:90.14ms
step:1344/1680 train_time:121147ms step_avg:90.14ms
step:1345/1680 train_time:121238ms step_avg:90.14ms
step:1346/1680 train_time:121328ms step_avg:90.14ms
step:1347/1680 train_time:121419ms step_avg:90.14ms
step:1348/1680 train_time:121511ms step_avg:90.14ms
step:1349/1680 train_time:121602ms step_avg:90.14ms
step:1350/1680 train_time:121692ms step_avg:90.14ms
step:1351/1680 train_time:121783ms step_avg:90.14ms
step:1352/1680 train_time:121874ms step_avg:90.14ms
step:1353/1680 train_time:121965ms step_avg:90.14ms
step:1354/1680 train_time:122057ms step_avg:90.15ms
step:1355/1680 train_time:122149ms step_avg:90.15ms
step:1356/1680 train_time:122240ms step_avg:90.15ms
step:1357/1680 train_time:122332ms step_avg:90.15ms
step:1358/1680 train_time:122423ms step_avg:90.15ms
step:1359/1680 train_time:122514ms step_avg:90.15ms
step:1360/1680 train_time:122605ms step_avg:90.15ms
step:1361/1680 train_time:122696ms step_avg:90.15ms
step:1362/1680 train_time:122786ms step_avg:90.15ms
step:1363/1680 train_time:122878ms step_avg:90.15ms
step:1364/1680 train_time:122969ms step_avg:90.15ms
step:1365/1680 train_time:123060ms step_avg:90.15ms
step:1366/1680 train_time:123152ms step_avg:90.16ms
step:1367/1680 train_time:123242ms step_avg:90.16ms
step:1368/1680 train_time:123333ms step_avg:90.16ms
step:1369/1680 train_time:123425ms step_avg:90.16ms
step:1370/1680 train_time:123516ms step_avg:90.16ms
step:1371/1680 train_time:123607ms step_avg:90.16ms
step:1372/1680 train_time:123698ms step_avg:90.16ms
step:1373/1680 train_time:123789ms step_avg:90.16ms
step:1374/1680 train_time:123880ms step_avg:90.16ms
step:1375/1680 train_time:123971ms step_avg:90.16ms
step:1375/1680 val_loss:3.3431 train_time:124062ms step_avg:90.23ms
step:1376/1680 train_time:124086ms step_avg:90.18ms
step:1377/1680 train_time:124156ms step_avg:90.16ms
step:1378/1680 train_time:124255ms step_avg:90.17ms
step:1379/1680 train_time:124347ms step_avg:90.17ms
step:1380/1680 train_time:124437ms step_avg:90.17ms
step:1381/1680 train_time:124526ms step_avg:90.17ms
step:1382/1680 train_time:124616ms step_avg:90.17ms
step:1383/1680 train_time:124706ms step_avg:90.17ms
step:1384/1680 train_time:124796ms step_avg:90.17ms
step:1385/1680 train_time:124885ms step_avg:90.17ms
step:1386/1680 train_time:124976ms step_avg:90.17ms
step:1387/1680 train_time:125068ms step_avg:90.17ms
step:1388/1680 train_time:125160ms step_avg:90.17ms
step:1389/1680 train_time:125255ms step_avg:90.18ms
step:1390/1680 train_time:125348ms step_avg:90.18ms
step:1391/1680 train_time:125438ms step_avg:90.18ms
step:1392/1680 train_time:125529ms step_avg:90.18ms
step:1393/1680 train_time:125619ms step_avg:90.18ms
step:1394/1680 train_time:125709ms step_avg:90.18ms
step:1395/1680 train_time:125800ms step_avg:90.18ms
step:1396/1680 train_time:125890ms step_avg:90.18ms
step:1397/1680 train_time:125980ms step_avg:90.18ms
step:1398/1680 train_time:126072ms step_avg:90.18ms
step:1399/1680 train_time:126164ms step_avg:90.18ms
step:1400/1680 train_time:126257ms step_avg:90.18ms
step:1401/1680 train_time:126349ms step_avg:90.18ms
step:1402/1680 train_time:126440ms step_avg:90.19ms
step:1403/1680 train_time:126531ms step_avg:90.19ms
step:1404/1680 train_time:126621ms step_avg:90.19ms
step:1405/1680 train_time:126711ms step_avg:90.19ms
step:1406/1680 train_time:126801ms step_avg:90.19ms
step:1407/1680 train_time:126892ms step_avg:90.19ms
step:1408/1680 train_time:126981ms step_avg:90.19ms
step:1409/1680 train_time:127074ms step_avg:90.19ms
step:1410/1680 train_time:127166ms step_avg:90.19ms
step:1411/1680 train_time:127257ms step_avg:90.19ms
step:1412/1680 train_time:127349ms step_avg:90.19ms
step:1413/1680 train_time:127440ms step_avg:90.19ms
step:1414/1680 train_time:127531ms step_avg:90.19ms
step:1415/1680 train_time:127622ms step_avg:90.19ms
step:1416/1680 train_time:127712ms step_avg:90.19ms
step:1417/1680 train_time:127802ms step_avg:90.19ms
step:1418/1680 train_time:127892ms step_avg:90.19ms
step:1419/1680 train_time:127983ms step_avg:90.19ms
step:1420/1680 train_time:128074ms step_avg:90.19ms
step:1421/1680 train_time:128166ms step_avg:90.19ms
step:1422/1680 train_time:128257ms step_avg:90.20ms
step:1423/1680 train_time:128349ms step_avg:90.20ms
step:1424/1680 train_time:128439ms step_avg:90.20ms
step:1425/1680 train_time:128530ms step_avg:90.20ms
step:1426/1680 train_time:128621ms step_avg:90.20ms
step:1427/1680 train_time:128712ms step_avg:90.20ms
step:1428/1680 train_time:128802ms step_avg:90.20ms
step:1429/1680 train_time:128893ms step_avg:90.20ms
step:1430/1680 train_time:128983ms step_avg:90.20ms
step:1431/1680 train_time:129075ms step_avg:90.20ms
step:1432/1680 train_time:129166ms step_avg:90.20ms
step:1433/1680 train_time:129258ms step_avg:90.20ms
step:1434/1680 train_time:129349ms step_avg:90.20ms
step:1435/1680 train_time:129440ms step_avg:90.20ms
step:1436/1680 train_time:129531ms step_avg:90.20ms
step:1437/1680 train_time:129622ms step_avg:90.20ms
step:1438/1680 train_time:129713ms step_avg:90.20ms
step:1439/1680 train_time:129804ms step_avg:90.20ms
step:1440/1680 train_time:129894ms step_avg:90.20ms
step:1441/1680 train_time:129985ms step_avg:90.21ms
step:1442/1680 train_time:130077ms step_avg:90.21ms
step:1443/1680 train_time:130168ms step_avg:90.21ms
step:1444/1680 train_time:130259ms step_avg:90.21ms
step:1445/1680 train_time:130350ms step_avg:90.21ms
step:1446/1680 train_time:130442ms step_avg:90.21ms
step:1447/1680 train_time:130532ms step_avg:90.21ms
step:1448/1680 train_time:130624ms step_avg:90.21ms
step:1449/1680 train_time:130716ms step_avg:90.21ms
step:1450/1680 train_time:130807ms step_avg:90.21ms
step:1451/1680 train_time:130897ms step_avg:90.21ms
step:1452/1680 train_time:130989ms step_avg:90.21ms
step:1453/1680 train_time:131079ms step_avg:90.21ms
step:1454/1680 train_time:131171ms step_avg:90.21ms
step:1455/1680 train_time:131261ms step_avg:90.21ms
step:1456/1680 train_time:131353ms step_avg:90.21ms
step:1457/1680 train_time:131443ms step_avg:90.21ms
step:1458/1680 train_time:131535ms step_avg:90.22ms
step:1459/1680 train_time:131626ms step_avg:90.22ms
step:1460/1680 train_time:131717ms step_avg:90.22ms
step:1461/1680 train_time:131808ms step_avg:90.22ms
step:1462/1680 train_time:131900ms step_avg:90.22ms
step:1463/1680 train_time:131991ms step_avg:90.22ms
step:1464/1680 train_time:132082ms step_avg:90.22ms
step:1465/1680 train_time:132174ms step_avg:90.22ms
step:1466/1680 train_time:132265ms step_avg:90.22ms
step:1467/1680 train_time:132356ms step_avg:90.22ms
step:1468/1680 train_time:132446ms step_avg:90.22ms
step:1469/1680 train_time:132537ms step_avg:90.22ms
step:1470/1680 train_time:132628ms step_avg:90.22ms
step:1471/1680 train_time:132719ms step_avg:90.22ms
step:1472/1680 train_time:132810ms step_avg:90.22ms
step:1473/1680 train_time:132902ms step_avg:90.23ms
step:1474/1680 train_time:132993ms step_avg:90.23ms
step:1475/1680 train_time:133083ms step_avg:90.23ms
step:1476/1680 train_time:133174ms step_avg:90.23ms
step:1477/1680 train_time:133266ms step_avg:90.23ms
step:1478/1680 train_time:133356ms step_avg:90.23ms
step:1479/1680 train_time:133447ms step_avg:90.23ms
step:1480/1680 train_time:133539ms step_avg:90.23ms
step:1481/1680 train_time:133629ms step_avg:90.23ms
step:1482/1680 train_time:133720ms step_avg:90.23ms
step:1483/1680 train_time:133812ms step_avg:90.23ms
step:1484/1680 train_time:133902ms step_avg:90.23ms
step:1485/1680 train_time:133994ms step_avg:90.23ms
step:1486/1680 train_time:134084ms step_avg:90.23ms
step:1487/1680 train_time:134176ms step_avg:90.23ms
step:1488/1680 train_time:134269ms step_avg:90.23ms
step:1489/1680 train_time:134360ms step_avg:90.23ms
step:1490/1680 train_time:134450ms step_avg:90.24ms
step:1491/1680 train_time:134541ms step_avg:90.24ms
step:1492/1680 train_time:134631ms step_avg:90.24ms
step:1493/1680 train_time:134723ms step_avg:90.24ms
step:1494/1680 train_time:134814ms step_avg:90.24ms
step:1495/1680 train_time:134904ms step_avg:90.24ms
step:1496/1680 train_time:134995ms step_avg:90.24ms
step:1497/1680 train_time:135086ms step_avg:90.24ms
step:1498/1680 train_time:135178ms step_avg:90.24ms
step:1499/1680 train_time:135269ms step_avg:90.24ms
step:1500/1680 train_time:135360ms step_avg:90.24ms
step:1500/1680 val_loss:3.3134 train_time:135453ms step_avg:90.30ms
step:1501/1680 train_time:135476ms step_avg:90.26ms
step:1502/1680 train_time:135549ms step_avg:90.25ms
step:1503/1680 train_time:135646ms step_avg:90.25ms
step:1504/1680 train_time:135737ms step_avg:90.25ms
step:1505/1680 train_time:135828ms step_avg:90.25ms
step:1506/1680 train_time:135918ms step_avg:90.25ms
step:1507/1680 train_time:136008ms step_avg:90.25ms
step:1508/1680 train_time:136098ms step_avg:90.25ms
step:1509/1680 train_time:136187ms step_avg:90.25ms
step:1510/1680 train_time:136278ms step_avg:90.25ms
step:1511/1680 train_time:136368ms step_avg:90.25ms
step:1512/1680 train_time:136459ms step_avg:90.25ms
step:1513/1680 train_time:136553ms step_avg:90.25ms
step:1514/1680 train_time:136647ms step_avg:90.26ms
step:1515/1680 train_time:136738ms step_avg:90.26ms
step:1516/1680 train_time:136830ms step_avg:90.26ms
step:1517/1680 train_time:136920ms step_avg:90.26ms
step:1518/1680 train_time:137009ms step_avg:90.26ms
step:1519/1680 train_time:137100ms step_avg:90.26ms
step:1520/1680 train_time:137189ms step_avg:90.26ms
step:1521/1680 train_time:137279ms step_avg:90.26ms
step:1522/1680 train_time:137370ms step_avg:90.26ms
step:1523/1680 train_time:137461ms step_avg:90.26ms
step:1524/1680 train_time:137553ms step_avg:90.26ms
step:1525/1680 train_time:137647ms step_avg:90.26ms
step:1526/1680 train_time:137738ms step_avg:90.26ms
step:1527/1680 train_time:137829ms step_avg:90.26ms
step:1528/1680 train_time:137920ms step_avg:90.26ms
step:1529/1680 train_time:138010ms step_avg:90.26ms
step:1530/1680 train_time:138101ms step_avg:90.26ms
step:1531/1680 train_time:138190ms step_avg:90.26ms
step:1532/1680 train_time:138280ms step_avg:90.26ms
step:1533/1680 train_time:138371ms step_avg:90.26ms
step:1534/1680 train_time:138462ms step_avg:90.26ms
step:1535/1680 train_time:138554ms step_avg:90.26ms
step:1536/1680 train_time:138646ms step_avg:90.26ms
step:1537/1680 train_time:138737ms step_avg:90.26ms
step:1538/1680 train_time:138830ms step_avg:90.27ms
step:1539/1680 train_time:138922ms step_avg:90.27ms
step:1540/1680 train_time:139012ms step_avg:90.27ms
step:1541/1680 train_time:139104ms step_avg:90.27ms
step:1542/1680 train_time:139194ms step_avg:90.27ms
step:1543/1680 train_time:139284ms step_avg:90.27ms
step:1544/1680 train_time:139374ms step_avg:90.27ms
step:1545/1680 train_time:139466ms step_avg:90.27ms
step:1546/1680 train_time:139557ms step_avg:90.27ms
step:1547/1680 train_time:139648ms step_avg:90.27ms
step:1548/1680 train_time:139739ms step_avg:90.27ms
step:1549/1680 train_time:139830ms step_avg:90.27ms
step:1550/1680 train_time:139921ms step_avg:90.27ms
step:1551/1680 train_time:140011ms step_avg:90.27ms
step:1552/1680 train_time:140103ms step_avg:90.27ms
step:1553/1680 train_time:140192ms step_avg:90.27ms
step:1554/1680 train_time:140282ms step_avg:90.27ms
step:1555/1680 train_time:140373ms step_avg:90.27ms
step:1556/1680 train_time:140465ms step_avg:90.27ms
step:1557/1680 train_time:140556ms step_avg:90.27ms
step:1558/1680 train_time:140648ms step_avg:90.27ms
step:1559/1680 train_time:140740ms step_avg:90.28ms
step:1560/1680 train_time:140831ms step_avg:90.28ms
step:1561/1680 train_time:140923ms step_avg:90.28ms
step:1562/1680 train_time:141015ms step_avg:90.28ms
step:1563/1680 train_time:141106ms step_avg:90.28ms
step:1564/1680 train_time:141197ms step_avg:90.28ms
step:1565/1680 train_time:141288ms step_avg:90.28ms
step:1566/1680 train_time:141379ms step_avg:90.28ms
step:1567/1680 train_time:141470ms step_avg:90.28ms
step:1568/1680 train_time:141562ms step_avg:90.28ms
step:1569/1680 train_time:141654ms step_avg:90.28ms
step:1570/1680 train_time:141744ms step_avg:90.28ms
step:1571/1680 train_time:141835ms step_avg:90.28ms
step:1572/1680 train_time:141928ms step_avg:90.29ms
step:1573/1680 train_time:142021ms step_avg:90.29ms
step:1574/1680 train_time:142112ms step_avg:90.29ms
step:1575/1680 train_time:142202ms step_avg:90.29ms
step:1576/1680 train_time:142292ms step_avg:90.29ms
step:1577/1680 train_time:142382ms step_avg:90.29ms
step:1578/1680 train_time:142474ms step_avg:90.29ms
step:1579/1680 train_time:142565ms step_avg:90.29ms
step:1580/1680 train_time:142656ms step_avg:90.29ms
step:1581/1680 train_time:142747ms step_avg:90.29ms
step:1582/1680 train_time:142838ms step_avg:90.29ms
step:1583/1680 train_time:142930ms step_avg:90.29ms
step:1584/1680 train_time:143023ms step_avg:90.29ms
step:1585/1680 train_time:143113ms step_avg:90.29ms
step:1586/1680 train_time:143205ms step_avg:90.29ms
step:1587/1680 train_time:143296ms step_avg:90.29ms
step:1588/1680 train_time:143387ms step_avg:90.29ms
step:1589/1680 train_time:143479ms step_avg:90.29ms
step:1590/1680 train_time:143569ms step_avg:90.29ms
step:1591/1680 train_time:143660ms step_avg:90.30ms
step:1592/1680 train_time:143751ms step_avg:90.30ms
step:1593/1680 train_time:143842ms step_avg:90.30ms
step:1594/1680 train_time:143934ms step_avg:90.30ms
step:1595/1680 train_time:144025ms step_avg:90.30ms
step:1596/1680 train_time:144115ms step_avg:90.30ms
step:1597/1680 train_time:144207ms step_avg:90.30ms
step:1598/1680 train_time:144299ms step_avg:90.30ms
step:1599/1680 train_time:144390ms step_avg:90.30ms
step:1600/1680 train_time:144481ms step_avg:90.30ms
step:1601/1680 train_time:144572ms step_avg:90.30ms
step:1602/1680 train_time:144662ms step_avg:90.30ms
step:1603/1680 train_time:144754ms step_avg:90.30ms
step:1604/1680 train_time:144845ms step_avg:90.30ms
step:1605/1680 train_time:144936ms step_avg:90.30ms
step:1606/1680 train_time:145028ms step_avg:90.30ms
step:1607/1680 train_time:145119ms step_avg:90.30ms
step:1608/1680 train_time:145211ms step_avg:90.31ms
step:1609/1680 train_time:145303ms step_avg:90.31ms
step:1610/1680 train_time:145393ms step_avg:90.31ms
step:1611/1680 train_time:145484ms step_avg:90.31ms
step:1612/1680 train_time:145575ms step_avg:90.31ms
step:1613/1680 train_time:145666ms step_avg:90.31ms
step:1614/1680 train_time:145757ms step_avg:90.31ms
step:1615/1680 train_time:145848ms step_avg:90.31ms
step:1616/1680 train_time:145939ms step_avg:90.31ms
step:1617/1680 train_time:146031ms step_avg:90.31ms
step:1618/1680 train_time:146122ms step_avg:90.31ms
step:1619/1680 train_time:146212ms step_avg:90.31ms
step:1620/1680 train_time:146304ms step_avg:90.31ms
step:1621/1680 train_time:146396ms step_avg:90.31ms
step:1622/1680 train_time:146487ms step_avg:90.31ms
step:1623/1680 train_time:146577ms step_avg:90.31ms
step:1624/1680 train_time:146668ms step_avg:90.31ms
step:1625/1680 train_time:146759ms step_avg:90.31ms
step:1625/1680 val_loss:3.2897 train_time:146851ms step_avg:90.37ms
step:1626/1680 train_time:146875ms step_avg:90.33ms
step:1627/1680 train_time:146950ms step_avg:90.32ms
step:1628/1680 train_time:147046ms step_avg:90.32ms
step:1629/1680 train_time:147139ms step_avg:90.32ms
step:1630/1680 train_time:147230ms step_avg:90.32ms
step:1631/1680 train_time:147319ms step_avg:90.32ms
step:1632/1680 train_time:147409ms step_avg:90.32ms
step:1633/1680 train_time:147498ms step_avg:90.32ms
step:1634/1680 train_time:147588ms step_avg:90.32ms
step:1635/1680 train_time:147677ms step_avg:90.32ms
step:1636/1680 train_time:147767ms step_avg:90.32ms
step:1637/1680 train_time:147859ms step_avg:90.32ms
step:1638/1680 train_time:147951ms step_avg:90.32ms
step:1639/1680 train_time:148046ms step_avg:90.33ms
step:1640/1680 train_time:148139ms step_avg:90.33ms
step:1641/1680 train_time:148230ms step_avg:90.33ms
step:1642/1680 train_time:148320ms step_avg:90.33ms
step:1643/1680 train_time:148411ms step_avg:90.33ms
step:1644/1680 train_time:148501ms step_avg:90.33ms
step:1645/1680 train_time:148591ms step_avg:90.33ms
step:1646/1680 train_time:148681ms step_avg:90.33ms
step:1647/1680 train_time:148771ms step_avg:90.33ms
step:1648/1680 train_time:148864ms step_avg:90.33ms
step:1649/1680 train_time:148957ms step_avg:90.33ms
step:1650/1680 train_time:149049ms step_avg:90.33ms
step:1651/1680 train_time:149141ms step_avg:90.33ms
step:1652/1680 train_time:149233ms step_avg:90.33ms
step:1653/1680 train_time:149324ms step_avg:90.34ms
step:1654/1680 train_time:149414ms step_avg:90.34ms
step:1655/1680 train_time:149505ms step_avg:90.34ms
step:1656/1680 train_time:149594ms step_avg:90.33ms
step:1657/1680 train_time:149685ms step_avg:90.33ms
step:1658/1680 train_time:149774ms step_avg:90.33ms
step:1659/1680 train_time:149866ms step_avg:90.34ms
step:1660/1680 train_time:149957ms step_avg:90.34ms
step:1661/1680 train_time:150048ms step_avg:90.34ms
step:1662/1680 train_time:150141ms step_avg:90.34ms
step:1663/1680 train_time:150233ms step_avg:90.34ms
step:1664/1680 train_time:150325ms step_avg:90.34ms
step:1665/1680 train_time:150415ms step_avg:90.34ms
step:1666/1680 train_time:150506ms step_avg:90.34ms
step:1667/1680 train_time:150596ms step_avg:90.34ms
step:1668/1680 train_time:150686ms step_avg:90.34ms
step:1669/1680 train_time:150777ms step_avg:90.34ms
step:1670/1680 train_time:150868ms step_avg:90.34ms
step:1671/1680 train_time:150959ms step_avg:90.34ms
step:1672/1680 train_time:151050ms step_avg:90.34ms
step:1673/1680 train_time:151143ms step_avg:90.34ms
step:1674/1680 train_time:151235ms step_avg:90.34ms
step:1675/1680 train_time:151326ms step_avg:90.34ms
step:1676/1680 train_time:151418ms step_avg:90.34ms
step:1677/1680 train_time:151509ms step_avg:90.35ms
step:1678/1680 train_time:151600ms step_avg:90.35ms
step:1679/1680 train_time:151690ms step_avg:90.35ms
step:1680/1680 train_time:151780ms step_avg:90.35ms
step:1680/1680 val_loss:3.2789 train_time:151872ms step_avg:90.40ms
peak memory allocated: 31255 MiB reserved: 46814 MiB

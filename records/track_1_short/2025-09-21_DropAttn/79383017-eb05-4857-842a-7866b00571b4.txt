import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 and layer_idx !=0 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        first_bm = 1 * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, final_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"v1/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer shows no degradation with context length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
smear_gate_params = [p for n, p in model.named_parameters() if "smear" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params+smear_gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate, args.ws_validate_final_layer
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250724+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sun Sep 21 23:32:35 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   44C    P0            127W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   39C    P0            125W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   40C    P0            126W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   39C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           89370      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A           89371      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           89372      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           89373      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           89374      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           89375      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           89376      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           89377      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           89371      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A           89372      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A           89373      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A           89374      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A           89375      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A           89376      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A           89377      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/1680 train_time:157ms step_avg:157.46ms
step:2/1680 train_time:180ms step_avg:90.12ms
step:3/1680 train_time:243ms step_avg:81.10ms
step:4/1680 train_time:330ms step_avg:82.52ms
step:5/1680 train_time:418ms step_avg:83.61ms
step:6/1680 train_time:506ms step_avg:84.33ms
step:7/1680 train_time:594ms step_avg:84.92ms
step:8/1680 train_time:683ms step_avg:85.39ms
step:9/1680 train_time:772ms step_avg:85.75ms
step:10/1680 train_time:860ms step_avg:86.04ms
step:11/1680 train_time:948ms step_avg:86.22ms
step:12/1680 train_time:1038ms step_avg:86.47ms
step:13/1680 train_time:1129ms step_avg:86.81ms
step:14/1680 train_time:1221ms step_avg:87.22ms
step:15/1680 train_time:1311ms step_avg:87.40ms
step:16/1680 train_time:1400ms step_avg:87.53ms
step:17/1680 train_time:1489ms step_avg:87.56ms
step:18/1680 train_time:1577ms step_avg:87.61ms
step:19/1680 train_time:1666ms step_avg:87.67ms
step:20/1680 train_time:1754ms step_avg:87.68ms
step:21/1680 train_time:1842ms step_avg:87.72ms
step:22/1680 train_time:1931ms step_avg:87.76ms
step:23/1680 train_time:2020ms step_avg:87.84ms
step:24/1680 train_time:2110ms step_avg:87.91ms
step:25/1680 train_time:2200ms step_avg:88.01ms
step:26/1680 train_time:2291ms step_avg:88.10ms
step:27/1680 train_time:2381ms step_avg:88.19ms
step:28/1680 train_time:2470ms step_avg:88.20ms
step:29/1680 train_time:2559ms step_avg:88.24ms
step:30/1680 train_time:2648ms step_avg:88.27ms
step:31/1680 train_time:2738ms step_avg:88.31ms
step:32/1680 train_time:2826ms step_avg:88.31ms
step:33/1680 train_time:2915ms step_avg:88.34ms
step:34/1680 train_time:3004ms step_avg:88.36ms
step:35/1680 train_time:3093ms step_avg:88.38ms
step:36/1680 train_time:3184ms step_avg:88.43ms
step:37/1680 train_time:3274ms step_avg:88.48ms
step:38/1680 train_time:3364ms step_avg:88.51ms
step:39/1680 train_time:3454ms step_avg:88.56ms
step:40/1680 train_time:3544ms step_avg:88.59ms
step:41/1680 train_time:3633ms step_avg:88.60ms
step:42/1680 train_time:3722ms step_avg:88.62ms
step:43/1680 train_time:3811ms step_avg:88.62ms
step:44/1680 train_time:3899ms step_avg:88.62ms
step:45/1680 train_time:3988ms step_avg:88.63ms
step:46/1680 train_time:4078ms step_avg:88.65ms
step:47/1680 train_time:4168ms step_avg:88.68ms
step:48/1680 train_time:4258ms step_avg:88.70ms
step:49/1680 train_time:4347ms step_avg:88.72ms
step:50/1680 train_time:4436ms step_avg:88.73ms
step:51/1680 train_time:4526ms step_avg:88.74ms
step:52/1680 train_time:4614ms step_avg:88.73ms
step:53/1680 train_time:4704ms step_avg:88.75ms
step:54/1680 train_time:4792ms step_avg:88.75ms
step:55/1680 train_time:4882ms step_avg:88.77ms
step:56/1680 train_time:4971ms step_avg:88.77ms
step:57/1680 train_time:5061ms step_avg:88.78ms
step:58/1680 train_time:5150ms step_avg:88.80ms
step:59/1680 train_time:5241ms step_avg:88.82ms
step:60/1680 train_time:5329ms step_avg:88.82ms
step:61/1680 train_time:5419ms step_avg:88.84ms
step:62/1680 train_time:5508ms step_avg:88.84ms
step:63/1680 train_time:5598ms step_avg:88.85ms
step:64/1680 train_time:5687ms step_avg:88.86ms
step:65/1680 train_time:5776ms step_avg:88.86ms
step:66/1680 train_time:5865ms step_avg:88.86ms
step:67/1680 train_time:5953ms step_avg:88.86ms
step:68/1680 train_time:6043ms step_avg:88.86ms
step:69/1680 train_time:6132ms step_avg:88.87ms
step:70/1680 train_time:6221ms step_avg:88.88ms
step:71/1680 train_time:6311ms step_avg:88.88ms
step:72/1680 train_time:6400ms step_avg:88.89ms
step:73/1680 train_time:6489ms step_avg:88.89ms
step:74/1680 train_time:6578ms step_avg:88.90ms
step:75/1680 train_time:6667ms step_avg:88.90ms
step:76/1680 train_time:6757ms step_avg:88.90ms
step:77/1680 train_time:6845ms step_avg:88.90ms
step:78/1680 train_time:6933ms step_avg:88.89ms
step:79/1680 train_time:7022ms step_avg:88.89ms
step:80/1680 train_time:7111ms step_avg:88.89ms
step:81/1680 train_time:7201ms step_avg:88.90ms
step:82/1680 train_time:7290ms step_avg:88.90ms
step:83/1680 train_time:7379ms step_avg:88.91ms
step:84/1680 train_time:7468ms step_avg:88.91ms
step:85/1680 train_time:7557ms step_avg:88.91ms
step:86/1680 train_time:7647ms step_avg:88.92ms
step:87/1680 train_time:7736ms step_avg:88.92ms
step:88/1680 train_time:7826ms step_avg:88.93ms
step:89/1680 train_time:7914ms step_avg:88.93ms
step:90/1680 train_time:8004ms step_avg:88.93ms
step:91/1680 train_time:8093ms step_avg:88.93ms
step:92/1680 train_time:8183ms step_avg:88.94ms
step:93/1680 train_time:8271ms step_avg:88.94ms
step:94/1680 train_time:8361ms step_avg:88.94ms
step:95/1680 train_time:8450ms step_avg:88.95ms
step:96/1680 train_time:8540ms step_avg:88.96ms
step:97/1680 train_time:8629ms step_avg:88.96ms
step:98/1680 train_time:8718ms step_avg:88.96ms
step:99/1680 train_time:8808ms step_avg:88.97ms
step:100/1680 train_time:8897ms step_avg:88.97ms
step:101/1680 train_time:8986ms step_avg:88.97ms
step:102/1680 train_time:9075ms step_avg:88.97ms
step:103/1680 train_time:9165ms step_avg:88.98ms
step:104/1680 train_time:9253ms step_avg:88.97ms
step:105/1680 train_time:9342ms step_avg:88.97ms
step:106/1680 train_time:9431ms step_avg:88.97ms
step:107/1680 train_time:9520ms step_avg:88.98ms
step:108/1680 train_time:9609ms step_avg:88.98ms
step:109/1680 train_time:9699ms step_avg:88.98ms
step:110/1680 train_time:9788ms step_avg:88.98ms
step:111/1680 train_time:9878ms step_avg:88.99ms
step:112/1680 train_time:9967ms step_avg:88.99ms
step:113/1680 train_time:10055ms step_avg:88.99ms
step:114/1680 train_time:10145ms step_avg:88.99ms
step:115/1680 train_time:10234ms step_avg:88.99ms
step:116/1680 train_time:10323ms step_avg:88.99ms
step:117/1680 train_time:10412ms step_avg:88.99ms
step:118/1680 train_time:10501ms step_avg:88.99ms
step:119/1680 train_time:10590ms step_avg:88.99ms
step:120/1680 train_time:10680ms step_avg:89.00ms
step:121/1680 train_time:10768ms step_avg:88.99ms
step:122/1680 train_time:10858ms step_avg:89.00ms
step:123/1680 train_time:10948ms step_avg:89.01ms
step:124/1680 train_time:11036ms step_avg:89.00ms
step:125/1680 train_time:11125ms step_avg:89.00ms
step:125/1680 val_loss:4.3252 train_time:11215ms step_avg:89.72ms
step:126/1680 train_time:11237ms step_avg:89.18ms
step:127/1680 train_time:11306ms step_avg:89.02ms
step:128/1680 train_time:11403ms step_avg:89.08ms
step:129/1680 train_time:11495ms step_avg:89.11ms
step:130/1680 train_time:11584ms step_avg:89.11ms
step:131/1680 train_time:11671ms step_avg:89.09ms
step:132/1680 train_time:11759ms step_avg:89.08ms
step:133/1680 train_time:11847ms step_avg:89.07ms
step:134/1680 train_time:11935ms step_avg:89.06ms
step:135/1680 train_time:12023ms step_avg:89.06ms
step:136/1680 train_time:12111ms step_avg:89.05ms
step:137/1680 train_time:12201ms step_avg:89.06ms
step:138/1680 train_time:12290ms step_avg:89.06ms
step:139/1680 train_time:12381ms step_avg:89.07ms
step:140/1680 train_time:12471ms step_avg:89.08ms
step:141/1680 train_time:12562ms step_avg:89.09ms
step:142/1680 train_time:12650ms step_avg:89.09ms
step:143/1680 train_time:12739ms step_avg:89.08ms
step:144/1680 train_time:12827ms step_avg:89.08ms
step:145/1680 train_time:12915ms step_avg:89.07ms
step:146/1680 train_time:13004ms step_avg:89.07ms
step:147/1680 train_time:13093ms step_avg:89.07ms
step:148/1680 train_time:13182ms step_avg:89.07ms
step:149/1680 train_time:13271ms step_avg:89.07ms
step:150/1680 train_time:13361ms step_avg:89.07ms
step:151/1680 train_time:13451ms step_avg:89.08ms
step:152/1680 train_time:13541ms step_avg:89.08ms
step:153/1680 train_time:13630ms step_avg:89.08ms
step:154/1680 train_time:13720ms step_avg:89.09ms
step:155/1680 train_time:13808ms step_avg:89.08ms
step:156/1680 train_time:13898ms step_avg:89.09ms
step:157/1680 train_time:13986ms step_avg:89.08ms
step:158/1680 train_time:14075ms step_avg:89.08ms
step:159/1680 train_time:14163ms step_avg:89.08ms
step:160/1680 train_time:14252ms step_avg:89.07ms
step:161/1680 train_time:14341ms step_avg:89.07ms
step:162/1680 train_time:14430ms step_avg:89.07ms
step:163/1680 train_time:14519ms step_avg:89.08ms
step:164/1680 train_time:14608ms step_avg:89.07ms
step:165/1680 train_time:14698ms step_avg:89.08ms
step:166/1680 train_time:14787ms step_avg:89.08ms
step:167/1680 train_time:14875ms step_avg:89.07ms
step:168/1680 train_time:14964ms step_avg:89.07ms
step:169/1680 train_time:15053ms step_avg:89.07ms
step:170/1680 train_time:15143ms step_avg:89.07ms
step:171/1680 train_time:15231ms step_avg:89.07ms
step:172/1680 train_time:15320ms step_avg:89.07ms
step:173/1680 train_time:15409ms step_avg:89.07ms
step:174/1680 train_time:15498ms step_avg:89.07ms
step:175/1680 train_time:15587ms step_avg:89.07ms
step:176/1680 train_time:15676ms step_avg:89.07ms
step:177/1680 train_time:15765ms step_avg:89.07ms
step:178/1680 train_time:15854ms step_avg:89.07ms
step:179/1680 train_time:15942ms step_avg:89.06ms
step:180/1680 train_time:16030ms step_avg:89.06ms
step:181/1680 train_time:16119ms step_avg:89.06ms
step:182/1680 train_time:16207ms step_avg:89.05ms
step:183/1680 train_time:16296ms step_avg:89.05ms
step:184/1680 train_time:16386ms step_avg:89.05ms
step:185/1680 train_time:16474ms step_avg:89.05ms
step:186/1680 train_time:16563ms step_avg:89.05ms
step:187/1680 train_time:16652ms step_avg:89.05ms
step:188/1680 train_time:16742ms step_avg:89.05ms
step:189/1680 train_time:16830ms step_avg:89.05ms
step:190/1680 train_time:16919ms step_avg:89.05ms
step:191/1680 train_time:17008ms step_avg:89.05ms
step:192/1680 train_time:17097ms step_avg:89.05ms
step:193/1680 train_time:17185ms step_avg:89.04ms
step:194/1680 train_time:17274ms step_avg:89.04ms
step:195/1680 train_time:17363ms step_avg:89.04ms
step:196/1680 train_time:17451ms step_avg:89.04ms
step:197/1680 train_time:17540ms step_avg:89.04ms
step:198/1680 train_time:17629ms step_avg:89.04ms
step:199/1680 train_time:17719ms step_avg:89.04ms
step:200/1680 train_time:17807ms step_avg:89.04ms
step:201/1680 train_time:17896ms step_avg:89.04ms
step:202/1680 train_time:17985ms step_avg:89.04ms
step:203/1680 train_time:18074ms step_avg:89.03ms
step:204/1680 train_time:18163ms step_avg:89.03ms
step:205/1680 train_time:18252ms step_avg:89.03ms
step:206/1680 train_time:18341ms step_avg:89.03ms
step:207/1680 train_time:18430ms step_avg:89.03ms
step:208/1680 train_time:18519ms step_avg:89.03ms
step:209/1680 train_time:18608ms step_avg:89.03ms
step:210/1680 train_time:18698ms step_avg:89.04ms
step:211/1680 train_time:18787ms step_avg:89.04ms
step:212/1680 train_time:18876ms step_avg:89.04ms
step:213/1680 train_time:18965ms step_avg:89.04ms
step:214/1680 train_time:19054ms step_avg:89.04ms
step:215/1680 train_time:19143ms step_avg:89.04ms
step:216/1680 train_time:19231ms step_avg:89.03ms
step:217/1680 train_time:19321ms step_avg:89.04ms
step:218/1680 train_time:19409ms step_avg:89.03ms
step:219/1680 train_time:19499ms step_avg:89.04ms
step:220/1680 train_time:19589ms step_avg:89.04ms
step:221/1680 train_time:19680ms step_avg:89.05ms
step:222/1680 train_time:19768ms step_avg:89.05ms
step:223/1680 train_time:19857ms step_avg:89.05ms
step:224/1680 train_time:19946ms step_avg:89.04ms
step:225/1680 train_time:20035ms step_avg:89.04ms
step:226/1680 train_time:20124ms step_avg:89.05ms
step:227/1680 train_time:20213ms step_avg:89.05ms
step:228/1680 train_time:20303ms step_avg:89.05ms
step:229/1680 train_time:20392ms step_avg:89.05ms
step:230/1680 train_time:20481ms step_avg:89.05ms
step:231/1680 train_time:20570ms step_avg:89.05ms
step:232/1680 train_time:20659ms step_avg:89.05ms
step:233/1680 train_time:20748ms step_avg:89.05ms
step:234/1680 train_time:20838ms step_avg:89.05ms
step:235/1680 train_time:20926ms step_avg:89.05ms
step:236/1680 train_time:21016ms step_avg:89.05ms
step:237/1680 train_time:21105ms step_avg:89.05ms
step:238/1680 train_time:21193ms step_avg:89.05ms
step:239/1680 train_time:21282ms step_avg:89.05ms
step:240/1680 train_time:21371ms step_avg:89.05ms
step:241/1680 train_time:21461ms step_avg:89.05ms
step:242/1680 train_time:21549ms step_avg:89.05ms
step:243/1680 train_time:21639ms step_avg:89.05ms
step:244/1680 train_time:21728ms step_avg:89.05ms
step:245/1680 train_time:21818ms step_avg:89.05ms
step:246/1680 train_time:21907ms step_avg:89.05ms
step:247/1680 train_time:21996ms step_avg:89.05ms
step:248/1680 train_time:22085ms step_avg:89.05ms
step:249/1680 train_time:22174ms step_avg:89.05ms
step:250/1680 train_time:22265ms step_avg:89.06ms
step:250/1680 val_loss:3.9699 train_time:22355ms step_avg:89.42ms
step:251/1680 train_time:22377ms step_avg:89.15ms
step:252/1680 train_time:22445ms step_avg:89.07ms
step:253/1680 train_time:22542ms step_avg:89.10ms
step:254/1680 train_time:22634ms step_avg:89.11ms
step:255/1680 train_time:22723ms step_avg:89.11ms
step:256/1680 train_time:22812ms step_avg:89.11ms
step:257/1680 train_time:22899ms step_avg:89.10ms
step:258/1680 train_time:22986ms step_avg:89.09ms
step:259/1680 train_time:23074ms step_avg:89.09ms
step:260/1680 train_time:23162ms step_avg:89.08ms
step:261/1680 train_time:23250ms step_avg:89.08ms
step:262/1680 train_time:23338ms step_avg:89.08ms
step:263/1680 train_time:23429ms step_avg:89.08ms
step:264/1680 train_time:23521ms step_avg:89.09ms
step:265/1680 train_time:23611ms step_avg:89.10ms
step:266/1680 train_time:23700ms step_avg:89.10ms
step:267/1680 train_time:23789ms step_avg:89.10ms
step:268/1680 train_time:23879ms step_avg:89.10ms
step:269/1680 train_time:23967ms step_avg:89.10ms
step:270/1680 train_time:24055ms step_avg:89.09ms
step:271/1680 train_time:24143ms step_avg:89.09ms
step:272/1680 train_time:24232ms step_avg:89.09ms
step:273/1680 train_time:24319ms step_avg:89.08ms
step:274/1680 train_time:24409ms step_avg:89.08ms
step:275/1680 train_time:24499ms step_avg:89.09ms
step:276/1680 train_time:24588ms step_avg:89.09ms
step:277/1680 train_time:24678ms step_avg:89.09ms
step:278/1680 train_time:24768ms step_avg:89.09ms
step:279/1680 train_time:24857ms step_avg:89.09ms
step:280/1680 train_time:24945ms step_avg:89.09ms
step:281/1680 train_time:25034ms step_avg:89.09ms
step:282/1680 train_time:25121ms step_avg:89.08ms
step:283/1680 train_time:25209ms step_avg:89.08ms
step:284/1680 train_time:25298ms step_avg:89.08ms
step:285/1680 train_time:25387ms step_avg:89.08ms
step:286/1680 train_time:25477ms step_avg:89.08ms
step:287/1680 train_time:25566ms step_avg:89.08ms
step:288/1680 train_time:25656ms step_avg:89.08ms
step:289/1680 train_time:25746ms step_avg:89.09ms
step:290/1680 train_time:25835ms step_avg:89.09ms
step:291/1680 train_time:25924ms step_avg:89.09ms
step:292/1680 train_time:26012ms step_avg:89.08ms
step:293/1680 train_time:26101ms step_avg:89.08ms
step:294/1680 train_time:26190ms step_avg:89.08ms
step:295/1680 train_time:26278ms step_avg:89.08ms
step:296/1680 train_time:26367ms step_avg:89.08ms
step:297/1680 train_time:26456ms step_avg:89.08ms
step:298/1680 train_time:26545ms step_avg:89.08ms
step:299/1680 train_time:26634ms step_avg:89.08ms
step:300/1680 train_time:26723ms step_avg:89.08ms
step:301/1680 train_time:26813ms step_avg:89.08ms
step:302/1680 train_time:26901ms step_avg:89.08ms
step:303/1680 train_time:26990ms step_avg:89.08ms
step:304/1680 train_time:27079ms step_avg:89.08ms
step:305/1680 train_time:27168ms step_avg:89.07ms
step:306/1680 train_time:27257ms step_avg:89.07ms
step:307/1680 train_time:27345ms step_avg:89.07ms
step:308/1680 train_time:27435ms step_avg:89.07ms
step:309/1680 train_time:27524ms step_avg:89.07ms
step:310/1680 train_time:27613ms step_avg:89.07ms
step:311/1680 train_time:27701ms step_avg:89.07ms
step:312/1680 train_time:27791ms step_avg:89.07ms
step:313/1680 train_time:27880ms step_avg:89.07ms
step:314/1680 train_time:27970ms step_avg:89.08ms
step:315/1680 train_time:28058ms step_avg:89.07ms
step:316/1680 train_time:28147ms step_avg:89.07ms
step:317/1680 train_time:28236ms step_avg:89.07ms
step:318/1680 train_time:28323ms step_avg:89.07ms
step:319/1680 train_time:28412ms step_avg:89.07ms
step:320/1680 train_time:28501ms step_avg:89.07ms
step:321/1680 train_time:28590ms step_avg:89.06ms
step:322/1680 train_time:28680ms step_avg:89.07ms
step:323/1680 train_time:28769ms step_avg:89.07ms
step:324/1680 train_time:28858ms step_avg:89.07ms
step:325/1680 train_time:28947ms step_avg:89.07ms
step:326/1680 train_time:29037ms step_avg:89.07ms
step:327/1680 train_time:29125ms step_avg:89.07ms
step:328/1680 train_time:29214ms step_avg:89.07ms
step:329/1680 train_time:29302ms step_avg:89.07ms
step:330/1680 train_time:29391ms step_avg:89.06ms
step:331/1680 train_time:29480ms step_avg:89.06ms
step:332/1680 train_time:29569ms step_avg:89.06ms
step:333/1680 train_time:29658ms step_avg:89.06ms
step:334/1680 train_time:29747ms step_avg:89.06ms
step:335/1680 train_time:29837ms step_avg:89.06ms
step:336/1680 train_time:29926ms step_avg:89.07ms
step:337/1680 train_time:30015ms step_avg:89.07ms
step:338/1680 train_time:30104ms step_avg:89.07ms
step:339/1680 train_time:30194ms step_avg:89.07ms
step:340/1680 train_time:30282ms step_avg:89.06ms
step:341/1680 train_time:30371ms step_avg:89.06ms
step:342/1680 train_time:30459ms step_avg:89.06ms
step:343/1680 train_time:30549ms step_avg:89.06ms
step:344/1680 train_time:30637ms step_avg:89.06ms
step:345/1680 train_time:30727ms step_avg:89.06ms
step:346/1680 train_time:30816ms step_avg:89.06ms
step:347/1680 train_time:30905ms step_avg:89.06ms
step:348/1680 train_time:30995ms step_avg:89.06ms
step:349/1680 train_time:31084ms step_avg:89.06ms
step:350/1680 train_time:31173ms step_avg:89.06ms
step:351/1680 train_time:31261ms step_avg:89.06ms
step:352/1680 train_time:31349ms step_avg:89.06ms
step:353/1680 train_time:31439ms step_avg:89.06ms
step:354/1680 train_time:31527ms step_avg:89.06ms
step:355/1680 train_time:31616ms step_avg:89.06ms
step:356/1680 train_time:31705ms step_avg:89.06ms
step:357/1680 train_time:31795ms step_avg:89.06ms
step:358/1680 train_time:31883ms step_avg:89.06ms
step:359/1680 train_time:31973ms step_avg:89.06ms
step:360/1680 train_time:32061ms step_avg:89.06ms
step:361/1680 train_time:32150ms step_avg:89.06ms
step:362/1680 train_time:32238ms step_avg:89.06ms
step:363/1680 train_time:32328ms step_avg:89.06ms
step:364/1680 train_time:32417ms step_avg:89.06ms
step:365/1680 train_time:32507ms step_avg:89.06ms
step:366/1680 train_time:32596ms step_avg:89.06ms
step:367/1680 train_time:32684ms step_avg:89.06ms
step:368/1680 train_time:32773ms step_avg:89.06ms
step:369/1680 train_time:32861ms step_avg:89.05ms
step:370/1680 train_time:32951ms step_avg:89.06ms
step:371/1680 train_time:33040ms step_avg:89.06ms
step:372/1680 train_time:33130ms step_avg:89.06ms
step:373/1680 train_time:33218ms step_avg:89.06ms
step:374/1680 train_time:33307ms step_avg:89.06ms
step:375/1680 train_time:33396ms step_avg:89.06ms
step:375/1680 val_loss:3.8187 train_time:33486ms step_avg:89.30ms
step:376/1680 train_time:33508ms step_avg:89.12ms
step:377/1680 train_time:33577ms step_avg:89.06ms
step:378/1680 train_time:33671ms step_avg:89.08ms
step:379/1680 train_time:33763ms step_avg:89.08ms
step:380/1680 train_time:33851ms step_avg:89.08ms
step:381/1680 train_time:33939ms step_avg:89.08ms
step:382/1680 train_time:34027ms step_avg:89.08ms
step:383/1680 train_time:34115ms step_avg:89.07ms
step:384/1680 train_time:34203ms step_avg:89.07ms
step:385/1680 train_time:34291ms step_avg:89.07ms
step:386/1680 train_time:34379ms step_avg:89.07ms
step:387/1680 train_time:34467ms step_avg:89.06ms
step:388/1680 train_time:34559ms step_avg:89.07ms
step:389/1680 train_time:34649ms step_avg:89.07ms
step:390/1680 train_time:34740ms step_avg:89.08ms
step:391/1680 train_time:34829ms step_avg:89.08ms
step:392/1680 train_time:34917ms step_avg:89.07ms
step:393/1680 train_time:35006ms step_avg:89.07ms
step:394/1680 train_time:35096ms step_avg:89.08ms
step:395/1680 train_time:35183ms step_avg:89.07ms
step:396/1680 train_time:35271ms step_avg:89.07ms
step:397/1680 train_time:35360ms step_avg:89.07ms
step:398/1680 train_time:35448ms step_avg:89.07ms
step:399/1680 train_time:35538ms step_avg:89.07ms
step:400/1680 train_time:35627ms step_avg:89.07ms
step:401/1680 train_time:35718ms step_avg:89.07ms
step:402/1680 train_time:35807ms step_avg:89.07ms
step:403/1680 train_time:35896ms step_avg:89.07ms
step:404/1680 train_time:35985ms step_avg:89.07ms
step:405/1680 train_time:36074ms step_avg:89.07ms
step:406/1680 train_time:36162ms step_avg:89.07ms
step:407/1680 train_time:36250ms step_avg:89.07ms
step:408/1680 train_time:36339ms step_avg:89.07ms
step:409/1680 train_time:36427ms step_avg:89.06ms
step:410/1680 train_time:36516ms step_avg:89.06ms
step:411/1680 train_time:36605ms step_avg:89.06ms
step:412/1680 train_time:36696ms step_avg:89.07ms
step:413/1680 train_time:36784ms step_avg:89.06ms
step:414/1680 train_time:36873ms step_avg:89.07ms
step:415/1680 train_time:36962ms step_avg:89.07ms
step:416/1680 train_time:37051ms step_avg:89.06ms
step:417/1680 train_time:37140ms step_avg:89.06ms
step:418/1680 train_time:37227ms step_avg:89.06ms
step:419/1680 train_time:37316ms step_avg:89.06ms
step:420/1680 train_time:37405ms step_avg:89.06ms
step:421/1680 train_time:37494ms step_avg:89.06ms
step:422/1680 train_time:37584ms step_avg:89.06ms
step:423/1680 train_time:37673ms step_avg:89.06ms
step:424/1680 train_time:37763ms step_avg:89.06ms
step:425/1680 train_time:37851ms step_avg:89.06ms
step:426/1680 train_time:37941ms step_avg:89.06ms
step:427/1680 train_time:38029ms step_avg:89.06ms
step:428/1680 train_time:38118ms step_avg:89.06ms
step:429/1680 train_time:38206ms step_avg:89.06ms
step:430/1680 train_time:38294ms step_avg:89.06ms
step:431/1680 train_time:38383ms step_avg:89.06ms
step:432/1680 train_time:38473ms step_avg:89.06ms
step:433/1680 train_time:38562ms step_avg:89.06ms
step:434/1680 train_time:38650ms step_avg:89.06ms
step:435/1680 train_time:38740ms step_avg:89.06ms
step:436/1680 train_time:38829ms step_avg:89.06ms
step:437/1680 train_time:38919ms step_avg:89.06ms
step:438/1680 train_time:39008ms step_avg:89.06ms
step:439/1680 train_time:39098ms step_avg:89.06ms
step:440/1680 train_time:39186ms step_avg:89.06ms
step:441/1680 train_time:39275ms step_avg:89.06ms
step:442/1680 train_time:39363ms step_avg:89.06ms
step:443/1680 train_time:39452ms step_avg:89.06ms
step:444/1680 train_time:39542ms step_avg:89.06ms
step:445/1680 train_time:39631ms step_avg:89.06ms
step:446/1680 train_time:39720ms step_avg:89.06ms
step:447/1680 train_time:39810ms step_avg:89.06ms
step:448/1680 train_time:39899ms step_avg:89.06ms
step:449/1680 train_time:39988ms step_avg:89.06ms
step:450/1680 train_time:40077ms step_avg:89.06ms
step:451/1680 train_time:40166ms step_avg:89.06ms
step:452/1680 train_time:40255ms step_avg:89.06ms
step:453/1680 train_time:40344ms step_avg:89.06ms
step:454/1680 train_time:40433ms step_avg:89.06ms
step:455/1680 train_time:40522ms step_avg:89.06ms
step:456/1680 train_time:40611ms step_avg:89.06ms
step:457/1680 train_time:40702ms step_avg:89.06ms
step:458/1680 train_time:40790ms step_avg:89.06ms
step:459/1680 train_time:40879ms step_avg:89.06ms
step:460/1680 train_time:40968ms step_avg:89.06ms
step:461/1680 train_time:41057ms step_avg:89.06ms
step:462/1680 train_time:41145ms step_avg:89.06ms
step:463/1680 train_time:41234ms step_avg:89.06ms
step:464/1680 train_time:41323ms step_avg:89.06ms
step:465/1680 train_time:41412ms step_avg:89.06ms
step:466/1680 train_time:41501ms step_avg:89.06ms
step:467/1680 train_time:41591ms step_avg:89.06ms
step:468/1680 train_time:41681ms step_avg:89.06ms
step:469/1680 train_time:41770ms step_avg:89.06ms
step:470/1680 train_time:41860ms step_avg:89.06ms
step:471/1680 train_time:41948ms step_avg:89.06ms
step:472/1680 train_time:42037ms step_avg:89.06ms
step:473/1680 train_time:42125ms step_avg:89.06ms
step:474/1680 train_time:42214ms step_avg:89.06ms
step:475/1680 train_time:42303ms step_avg:89.06ms
step:476/1680 train_time:42392ms step_avg:89.06ms
step:477/1680 train_time:42480ms step_avg:89.06ms
step:478/1680 train_time:42569ms step_avg:89.06ms
step:479/1680 train_time:42659ms step_avg:89.06ms
step:480/1680 train_time:42748ms step_avg:89.06ms
step:481/1680 train_time:42838ms step_avg:89.06ms
step:482/1680 train_time:42927ms step_avg:89.06ms
step:483/1680 train_time:43017ms step_avg:89.06ms
step:484/1680 train_time:43105ms step_avg:89.06ms
step:485/1680 train_time:43194ms step_avg:89.06ms
step:486/1680 train_time:43283ms step_avg:89.06ms
step:487/1680 train_time:43372ms step_avg:89.06ms
step:488/1680 train_time:43462ms step_avg:89.06ms
step:489/1680 train_time:43550ms step_avg:89.06ms
step:490/1680 train_time:43640ms step_avg:89.06ms
step:491/1680 train_time:43729ms step_avg:89.06ms
step:492/1680 train_time:43818ms step_avg:89.06ms
step:493/1680 train_time:43906ms step_avg:89.06ms
step:494/1680 train_time:43995ms step_avg:89.06ms
step:495/1680 train_time:44084ms step_avg:89.06ms
step:496/1680 train_time:44173ms step_avg:89.06ms
step:497/1680 train_time:44262ms step_avg:89.06ms
step:498/1680 train_time:44350ms step_avg:89.06ms
step:499/1680 train_time:44440ms step_avg:89.06ms
step:500/1680 train_time:44529ms step_avg:89.06ms
step:500/1680 val_loss:3.7149 train_time:44619ms step_avg:89.24ms
step:501/1680 train_time:44641ms step_avg:89.10ms
step:502/1680 train_time:44709ms step_avg:89.06ms
step:503/1680 train_time:44803ms step_avg:89.07ms
step:504/1680 train_time:44893ms step_avg:89.07ms
step:505/1680 train_time:44982ms step_avg:89.07ms
step:506/1680 train_time:45070ms step_avg:89.07ms
step:507/1680 train_time:45158ms step_avg:89.07ms
step:508/1680 train_time:45247ms step_avg:89.07ms
step:509/1680 train_time:45335ms step_avg:89.07ms
step:510/1680 train_time:45422ms step_avg:89.06ms
step:511/1680 train_time:45511ms step_avg:89.06ms
step:512/1680 train_time:45601ms step_avg:89.06ms
step:513/1680 train_time:45692ms step_avg:89.07ms
step:514/1680 train_time:45783ms step_avg:89.07ms
step:515/1680 train_time:45873ms step_avg:89.07ms
step:516/1680 train_time:45962ms step_avg:89.07ms
step:517/1680 train_time:46051ms step_avg:89.07ms
step:518/1680 train_time:46139ms step_avg:89.07ms
step:519/1680 train_time:46228ms step_avg:89.07ms
step:520/1680 train_time:46316ms step_avg:89.07ms
step:521/1680 train_time:46405ms step_avg:89.07ms
step:522/1680 train_time:46494ms step_avg:89.07ms
step:523/1680 train_time:46583ms step_avg:89.07ms
step:524/1680 train_time:46673ms step_avg:89.07ms
step:525/1680 train_time:46763ms step_avg:89.07ms
step:526/1680 train_time:46853ms step_avg:89.07ms
step:527/1680 train_time:46942ms step_avg:89.07ms
step:528/1680 train_time:47031ms step_avg:89.07ms
step:529/1680 train_time:47119ms step_avg:89.07ms
step:530/1680 train_time:47208ms step_avg:89.07ms
step:531/1680 train_time:47296ms step_avg:89.07ms
step:532/1680 train_time:47385ms step_avg:89.07ms
step:533/1680 train_time:47473ms step_avg:89.07ms
step:534/1680 train_time:47562ms step_avg:89.07ms
step:535/1680 train_time:47651ms step_avg:89.07ms
step:536/1680 train_time:47740ms step_avg:89.07ms
step:537/1680 train_time:47830ms step_avg:89.07ms
step:538/1680 train_time:47920ms step_avg:89.07ms
step:539/1680 train_time:48008ms step_avg:89.07ms
step:540/1680 train_time:48097ms step_avg:89.07ms
step:541/1680 train_time:48186ms step_avg:89.07ms
step:542/1680 train_time:48274ms step_avg:89.07ms
step:543/1680 train_time:48362ms step_avg:89.07ms
step:544/1680 train_time:48451ms step_avg:89.06ms
step:545/1680 train_time:48542ms step_avg:89.07ms
step:546/1680 train_time:48631ms step_avg:89.07ms
step:547/1680 train_time:48720ms step_avg:89.07ms
step:548/1680 train_time:48809ms step_avg:89.07ms
step:549/1680 train_time:48899ms step_avg:89.07ms
step:550/1680 train_time:48990ms step_avg:89.07ms
step:551/1680 train_time:49081ms step_avg:89.08ms
step:552/1680 train_time:49171ms step_avg:89.08ms
step:553/1680 train_time:49260ms step_avg:89.08ms
step:554/1680 train_time:49351ms step_avg:89.08ms
step:555/1680 train_time:49441ms step_avg:89.08ms
step:556/1680 train_time:49531ms step_avg:89.08ms
step:557/1680 train_time:49622ms step_avg:89.09ms
step:558/1680 train_time:49712ms step_avg:89.09ms
step:559/1680 train_time:49802ms step_avg:89.09ms
step:560/1680 train_time:49892ms step_avg:89.09ms
step:561/1680 train_time:49983ms step_avg:89.10ms
step:562/1680 train_time:50073ms step_avg:89.10ms
step:563/1680 train_time:50164ms step_avg:89.10ms
step:564/1680 train_time:50254ms step_avg:89.10ms
step:565/1680 train_time:50345ms step_avg:89.11ms
step:566/1680 train_time:50434ms step_avg:89.11ms
step:567/1680 train_time:50525ms step_avg:89.11ms
step:568/1680 train_time:50615ms step_avg:89.11ms
step:569/1680 train_time:50705ms step_avg:89.11ms
step:570/1680 train_time:50796ms step_avg:89.11ms
step:571/1680 train_time:50887ms step_avg:89.12ms
step:572/1680 train_time:50977ms step_avg:89.12ms
step:573/1680 train_time:51068ms step_avg:89.12ms
step:574/1680 train_time:51158ms step_avg:89.13ms
step:575/1680 train_time:51249ms step_avg:89.13ms
step:576/1680 train_time:51339ms step_avg:89.13ms
step:577/1680 train_time:51429ms step_avg:89.13ms
step:578/1680 train_time:51519ms step_avg:89.13ms
step:579/1680 train_time:51609ms step_avg:89.13ms
step:580/1680 train_time:51699ms step_avg:89.14ms
step:581/1680 train_time:51789ms step_avg:89.14ms
step:582/1680 train_time:51880ms step_avg:89.14ms
step:583/1680 train_time:51970ms step_avg:89.14ms
step:584/1680 train_time:52061ms step_avg:89.15ms
step:585/1680 train_time:52151ms step_avg:89.15ms
step:586/1680 train_time:52242ms step_avg:89.15ms
step:587/1680 train_time:52332ms step_avg:89.15ms
step:588/1680 train_time:52422ms step_avg:89.15ms
step:589/1680 train_time:52513ms step_avg:89.16ms
step:590/1680 train_time:52603ms step_avg:89.16ms
step:591/1680 train_time:52694ms step_avg:89.16ms
step:592/1680 train_time:52784ms step_avg:89.16ms
step:593/1680 train_time:52874ms step_avg:89.16ms
step:594/1680 train_time:52965ms step_avg:89.17ms
step:595/1680 train_time:53056ms step_avg:89.17ms
step:596/1680 train_time:53148ms step_avg:89.17ms
step:597/1680 train_time:53238ms step_avg:89.18ms
step:598/1680 train_time:53328ms step_avg:89.18ms
step:599/1680 train_time:53418ms step_avg:89.18ms
step:600/1680 train_time:53509ms step_avg:89.18ms
step:601/1680 train_time:53599ms step_avg:89.18ms
step:602/1680 train_time:53689ms step_avg:89.18ms
step:603/1680 train_time:53779ms step_avg:89.19ms
step:604/1680 train_time:53869ms step_avg:89.19ms
step:605/1680 train_time:53960ms step_avg:89.19ms
step:606/1680 train_time:54050ms step_avg:89.19ms
step:607/1680 train_time:54141ms step_avg:89.19ms
step:608/1680 train_time:54231ms step_avg:89.20ms
step:609/1680 train_time:54321ms step_avg:89.20ms
step:610/1680 train_time:54411ms step_avg:89.20ms
step:611/1680 train_time:54502ms step_avg:89.20ms
step:612/1680 train_time:54592ms step_avg:89.20ms
step:613/1680 train_time:54682ms step_avg:89.20ms
step:614/1680 train_time:54771ms step_avg:89.20ms
step:615/1680 train_time:54861ms step_avg:89.21ms
step:616/1680 train_time:54951ms step_avg:89.21ms
step:617/1680 train_time:55041ms step_avg:89.21ms
step:618/1680 train_time:55132ms step_avg:89.21ms
step:619/1680 train_time:55222ms step_avg:89.21ms
step:620/1680 train_time:55312ms step_avg:89.21ms
step:621/1680 train_time:55402ms step_avg:89.21ms
step:622/1680 train_time:55494ms step_avg:89.22ms
step:623/1680 train_time:55584ms step_avg:89.22ms
step:624/1680 train_time:55675ms step_avg:89.22ms
step:625/1680 train_time:55765ms step_avg:89.22ms
step:625/1680 val_loss:3.6163 train_time:55856ms step_avg:89.37ms
step:626/1680 train_time:55879ms step_avg:89.26ms
step:627/1680 train_time:55950ms step_avg:89.24ms
step:628/1680 train_time:56049ms step_avg:89.25ms
step:629/1680 train_time:56138ms step_avg:89.25ms
step:630/1680 train_time:56228ms step_avg:89.25ms
step:631/1680 train_time:56316ms step_avg:89.25ms
step:632/1680 train_time:56405ms step_avg:89.25ms
step:633/1680 train_time:56495ms step_avg:89.25ms
step:634/1680 train_time:56583ms step_avg:89.25ms
step:635/1680 train_time:56672ms step_avg:89.25ms
step:636/1680 train_time:56762ms step_avg:89.25ms
step:637/1680 train_time:56856ms step_avg:89.26ms
step:638/1680 train_time:56948ms step_avg:89.26ms
step:639/1680 train_time:57039ms step_avg:89.26ms
step:640/1680 train_time:57130ms step_avg:89.27ms
step:641/1680 train_time:57220ms step_avg:89.27ms
step:642/1680 train_time:57309ms step_avg:89.27ms
step:643/1680 train_time:57398ms step_avg:89.27ms
step:644/1680 train_time:57488ms step_avg:89.27ms
step:645/1680 train_time:57577ms step_avg:89.27ms
step:646/1680 train_time:57667ms step_avg:89.27ms
step:647/1680 train_time:57757ms step_avg:89.27ms
step:648/1680 train_time:57849ms step_avg:89.27ms
step:649/1680 train_time:57941ms step_avg:89.28ms
step:650/1680 train_time:58032ms step_avg:89.28ms
step:651/1680 train_time:58124ms step_avg:89.28ms
step:652/1680 train_time:58214ms step_avg:89.29ms
step:653/1680 train_time:58304ms step_avg:89.29ms
step:654/1680 train_time:58394ms step_avg:89.29ms
step:655/1680 train_time:58483ms step_avg:89.29ms
step:656/1680 train_time:58573ms step_avg:89.29ms
step:657/1680 train_time:58662ms step_avg:89.29ms
step:658/1680 train_time:58753ms step_avg:89.29ms
step:659/1680 train_time:58844ms step_avg:89.29ms
step:660/1680 train_time:58935ms step_avg:89.30ms
step:661/1680 train_time:59026ms step_avg:89.30ms
step:662/1680 train_time:59117ms step_avg:89.30ms
step:663/1680 train_time:59208ms step_avg:89.30ms
step:664/1680 train_time:59298ms step_avg:89.30ms
step:665/1680 train_time:59388ms step_avg:89.31ms
step:666/1680 train_time:59478ms step_avg:89.31ms
step:667/1680 train_time:59567ms step_avg:89.31ms
step:668/1680 train_time:59657ms step_avg:89.31ms
step:669/1680 train_time:59746ms step_avg:89.31ms
step:670/1680 train_time:59836ms step_avg:89.31ms
step:671/1680 train_time:59927ms step_avg:89.31ms
step:672/1680 train_time:60018ms step_avg:89.31ms
step:673/1680 train_time:60109ms step_avg:89.32ms
step:674/1680 train_time:60201ms step_avg:89.32ms
step:675/1680 train_time:60292ms step_avg:89.32ms
step:676/1680 train_time:60383ms step_avg:89.32ms
step:677/1680 train_time:60473ms step_avg:89.32ms
step:678/1680 train_time:60562ms step_avg:89.33ms
step:679/1680 train_time:60653ms step_avg:89.33ms
step:680/1680 train_time:60742ms step_avg:89.33ms
step:681/1680 train_time:60832ms step_avg:89.33ms
step:682/1680 train_time:60923ms step_avg:89.33ms
step:683/1680 train_time:61013ms step_avg:89.33ms
step:684/1680 train_time:61103ms step_avg:89.33ms
step:685/1680 train_time:61193ms step_avg:89.33ms
step:686/1680 train_time:61285ms step_avg:89.34ms
step:687/1680 train_time:61375ms step_avg:89.34ms
step:688/1680 train_time:61464ms step_avg:89.34ms
step:689/1680 train_time:61553ms step_avg:89.34ms
step:690/1680 train_time:61644ms step_avg:89.34ms
step:691/1680 train_time:61733ms step_avg:89.34ms
step:692/1680 train_time:61823ms step_avg:89.34ms
step:693/1680 train_time:61913ms step_avg:89.34ms
step:694/1680 train_time:62003ms step_avg:89.34ms
step:695/1680 train_time:62094ms step_avg:89.34ms
step:696/1680 train_time:62184ms step_avg:89.34ms
step:697/1680 train_time:62274ms step_avg:89.35ms
step:698/1680 train_time:62365ms step_avg:89.35ms
step:699/1680 train_time:62455ms step_avg:89.35ms
step:700/1680 train_time:62545ms step_avg:89.35ms
step:701/1680 train_time:62635ms step_avg:89.35ms
step:702/1680 train_time:62725ms step_avg:89.35ms
step:703/1680 train_time:62815ms step_avg:89.35ms
step:704/1680 train_time:62905ms step_avg:89.35ms
step:705/1680 train_time:62995ms step_avg:89.36ms
step:706/1680 train_time:63086ms step_avg:89.36ms
step:707/1680 train_time:63177ms step_avg:89.36ms
step:708/1680 train_time:63267ms step_avg:89.36ms
step:709/1680 train_time:63357ms step_avg:89.36ms
step:710/1680 train_time:63447ms step_avg:89.36ms
step:711/1680 train_time:63537ms step_avg:89.36ms
step:712/1680 train_time:63627ms step_avg:89.36ms
step:713/1680 train_time:63716ms step_avg:89.36ms
step:714/1680 train_time:63806ms step_avg:89.36ms
step:715/1680 train_time:63896ms step_avg:89.37ms
step:716/1680 train_time:63987ms step_avg:89.37ms
step:717/1680 train_time:64077ms step_avg:89.37ms
step:718/1680 train_time:64168ms step_avg:89.37ms
step:719/1680 train_time:64257ms step_avg:89.37ms
step:720/1680 train_time:64347ms step_avg:89.37ms
step:721/1680 train_time:64437ms step_avg:89.37ms
step:722/1680 train_time:64528ms step_avg:89.37ms
step:723/1680 train_time:64618ms step_avg:89.37ms
step:724/1680 train_time:64707ms step_avg:89.37ms
step:725/1680 train_time:64797ms step_avg:89.38ms
step:726/1680 train_time:64887ms step_avg:89.38ms
step:727/1680 train_time:64977ms step_avg:89.38ms
step:728/1680 train_time:65068ms step_avg:89.38ms
step:729/1680 train_time:65158ms step_avg:89.38ms
step:730/1680 train_time:65248ms step_avg:89.38ms
step:731/1680 train_time:65338ms step_avg:89.38ms
step:732/1680 train_time:65429ms step_avg:89.38ms
step:733/1680 train_time:65519ms step_avg:89.39ms
step:734/1680 train_time:65610ms step_avg:89.39ms
step:735/1680 train_time:65700ms step_avg:89.39ms
step:736/1680 train_time:65790ms step_avg:89.39ms
step:737/1680 train_time:65879ms step_avg:89.39ms
step:738/1680 train_time:65969ms step_avg:89.39ms
step:739/1680 train_time:66059ms step_avg:89.39ms
step:740/1680 train_time:66150ms step_avg:89.39ms
step:741/1680 train_time:66240ms step_avg:89.39ms
step:742/1680 train_time:66330ms step_avg:89.39ms
step:743/1680 train_time:66420ms step_avg:89.39ms
step:744/1680 train_time:66511ms step_avg:89.40ms
step:745/1680 train_time:66601ms step_avg:89.40ms
step:746/1680 train_time:66691ms step_avg:89.40ms
step:747/1680 train_time:66782ms step_avg:89.40ms
step:748/1680 train_time:66873ms step_avg:89.40ms
step:749/1680 train_time:66963ms step_avg:89.40ms
step:750/1680 train_time:67053ms step_avg:89.40ms
step:750/1680 val_loss:3.5647 train_time:67145ms step_avg:89.53ms
step:751/1680 train_time:67167ms step_avg:89.44ms
step:752/1680 train_time:67240ms step_avg:89.42ms
step:753/1680 train_time:67336ms step_avg:89.42ms
step:754/1680 train_time:67428ms step_avg:89.43ms
step:755/1680 train_time:67517ms step_avg:89.43ms
step:756/1680 train_time:67606ms step_avg:89.43ms
step:757/1680 train_time:67696ms step_avg:89.43ms
step:758/1680 train_time:67784ms step_avg:89.43ms
step:759/1680 train_time:67874ms step_avg:89.43ms
step:760/1680 train_time:67964ms step_avg:89.43ms
step:761/1680 train_time:68054ms step_avg:89.43ms
step:762/1680 train_time:68145ms step_avg:89.43ms
step:763/1680 train_time:68238ms step_avg:89.43ms
step:764/1680 train_time:68331ms step_avg:89.44ms
step:765/1680 train_time:68423ms step_avg:89.44ms
step:766/1680 train_time:68514ms step_avg:89.44ms
step:767/1680 train_time:68605ms step_avg:89.45ms
step:768/1680 train_time:68694ms step_avg:89.45ms
step:769/1680 train_time:68783ms step_avg:89.44ms
step:770/1680 train_time:68872ms step_avg:89.44ms
step:771/1680 train_time:68961ms step_avg:89.44ms
step:772/1680 train_time:69051ms step_avg:89.44ms
step:773/1680 train_time:69142ms step_avg:89.45ms
step:774/1680 train_time:69234ms step_avg:89.45ms
step:775/1680 train_time:69325ms step_avg:89.45ms
step:776/1680 train_time:69417ms step_avg:89.45ms
step:777/1680 train_time:69507ms step_avg:89.46ms
step:778/1680 train_time:69598ms step_avg:89.46ms
step:779/1680 train_time:69687ms step_avg:89.46ms
step:780/1680 train_time:69777ms step_avg:89.46ms
step:781/1680 train_time:69866ms step_avg:89.46ms
step:782/1680 train_time:69956ms step_avg:89.46ms
step:783/1680 train_time:70046ms step_avg:89.46ms
step:784/1680 train_time:70137ms step_avg:89.46ms
step:785/1680 train_time:70227ms step_avg:89.46ms
step:786/1680 train_time:70318ms step_avg:89.46ms
step:787/1680 train_time:70410ms step_avg:89.47ms
step:788/1680 train_time:70500ms step_avg:89.47ms
step:789/1680 train_time:70590ms step_avg:89.47ms
step:790/1680 train_time:70679ms step_avg:89.47ms
step:791/1680 train_time:70769ms step_avg:89.47ms
step:792/1680 train_time:70858ms step_avg:89.47ms
step:793/1680 train_time:70948ms step_avg:89.47ms
step:794/1680 train_time:71038ms step_avg:89.47ms
step:795/1680 train_time:71128ms step_avg:89.47ms
step:796/1680 train_time:71218ms step_avg:89.47ms
step:797/1680 train_time:71310ms step_avg:89.47ms
step:798/1680 train_time:71401ms step_avg:89.47ms
step:799/1680 train_time:71492ms step_avg:89.48ms
step:800/1680 train_time:71581ms step_avg:89.48ms
step:801/1680 train_time:71671ms step_avg:89.48ms
step:802/1680 train_time:71761ms step_avg:89.48ms
step:803/1680 train_time:71851ms step_avg:89.48ms
step:804/1680 train_time:71941ms step_avg:89.48ms
step:805/1680 train_time:72030ms step_avg:89.48ms
step:806/1680 train_time:72120ms step_avg:89.48ms
step:807/1680 train_time:72211ms step_avg:89.48ms
step:808/1680 train_time:72301ms step_avg:89.48ms
step:809/1680 train_time:72393ms step_avg:89.48ms
step:810/1680 train_time:72483ms step_avg:89.49ms
step:811/1680 train_time:72573ms step_avg:89.49ms
step:812/1680 train_time:72663ms step_avg:89.49ms
step:813/1680 train_time:72754ms step_avg:89.49ms
step:814/1680 train_time:72843ms step_avg:89.49ms
step:815/1680 train_time:72933ms step_avg:89.49ms
step:816/1680 train_time:73023ms step_avg:89.49ms
step:817/1680 train_time:73114ms step_avg:89.49ms
step:818/1680 train_time:73204ms step_avg:89.49ms
step:819/1680 train_time:73296ms step_avg:89.49ms
step:820/1680 train_time:73386ms step_avg:89.50ms
step:821/1680 train_time:73477ms step_avg:89.50ms
step:822/1680 train_time:73567ms step_avg:89.50ms
step:823/1680 train_time:73657ms step_avg:89.50ms
step:824/1680 train_time:73747ms step_avg:89.50ms
step:825/1680 train_time:73837ms step_avg:89.50ms
step:826/1680 train_time:73927ms step_avg:89.50ms
step:827/1680 train_time:74017ms step_avg:89.50ms
step:828/1680 train_time:74108ms step_avg:89.50ms
step:829/1680 train_time:74198ms step_avg:89.50ms
step:830/1680 train_time:74288ms step_avg:89.50ms
step:831/1680 train_time:74378ms step_avg:89.50ms
step:832/1680 train_time:74468ms step_avg:89.51ms
step:833/1680 train_time:74558ms step_avg:89.51ms
step:834/1680 train_time:74649ms step_avg:89.51ms
step:835/1680 train_time:74739ms step_avg:89.51ms
step:836/1680 train_time:74829ms step_avg:89.51ms
step:837/1680 train_time:74920ms step_avg:89.51ms
step:838/1680 train_time:75011ms step_avg:89.51ms
step:839/1680 train_time:75100ms step_avg:89.51ms
step:840/1680 train_time:75191ms step_avg:89.51ms
step:841/1680 train_time:75281ms step_avg:89.51ms
step:842/1680 train_time:75371ms step_avg:89.51ms
step:843/1680 train_time:75461ms step_avg:89.51ms
step:844/1680 train_time:75551ms step_avg:89.52ms
step:845/1680 train_time:75641ms step_avg:89.52ms
step:846/1680 train_time:75732ms step_avg:89.52ms
step:847/1680 train_time:75822ms step_avg:89.52ms
step:848/1680 train_time:75911ms step_avg:89.52ms
step:849/1680 train_time:76001ms step_avg:89.52ms
step:850/1680 train_time:76093ms step_avg:89.52ms
step:851/1680 train_time:76182ms step_avg:89.52ms
step:852/1680 train_time:76272ms step_avg:89.52ms
step:853/1680 train_time:76362ms step_avg:89.52ms
step:854/1680 train_time:76452ms step_avg:89.52ms
step:855/1680 train_time:76542ms step_avg:89.52ms
step:856/1680 train_time:76632ms step_avg:89.52ms
step:857/1680 train_time:76722ms step_avg:89.52ms
step:858/1680 train_time:76812ms step_avg:89.53ms
step:859/1680 train_time:76902ms step_avg:89.53ms
step:860/1680 train_time:76994ms step_avg:89.53ms
step:861/1680 train_time:77084ms step_avg:89.53ms
step:862/1680 train_time:77175ms step_avg:89.53ms
step:863/1680 train_time:77266ms step_avg:89.53ms
step:864/1680 train_time:77356ms step_avg:89.53ms
step:865/1680 train_time:77447ms step_avg:89.53ms
step:866/1680 train_time:77537ms step_avg:89.53ms
step:867/1680 train_time:77627ms step_avg:89.54ms
step:868/1680 train_time:77718ms step_avg:89.54ms
step:869/1680 train_time:77808ms step_avg:89.54ms
step:870/1680 train_time:77898ms step_avg:89.54ms
step:871/1680 train_time:77988ms step_avg:89.54ms
step:872/1680 train_time:78078ms step_avg:89.54ms
step:873/1680 train_time:78168ms step_avg:89.54ms
step:874/1680 train_time:78258ms step_avg:89.54ms
step:875/1680 train_time:78348ms step_avg:89.54ms
step:875/1680 val_loss:3.5170 train_time:78439ms step_avg:89.65ms
step:876/1680 train_time:78461ms step_avg:89.57ms
step:877/1680 train_time:78534ms step_avg:89.55ms
step:878/1680 train_time:78632ms step_avg:89.56ms
step:879/1680 train_time:78722ms step_avg:89.56ms
step:880/1680 train_time:78813ms step_avg:89.56ms
step:881/1680 train_time:78902ms step_avg:89.56ms
step:882/1680 train_time:78991ms step_avg:89.56ms
step:883/1680 train_time:79080ms step_avg:89.56ms
step:884/1680 train_time:79169ms step_avg:89.56ms
step:885/1680 train_time:79258ms step_avg:89.56ms
step:886/1680 train_time:79347ms step_avg:89.56ms
step:887/1680 train_time:79438ms step_avg:89.56ms
step:888/1680 train_time:79531ms step_avg:89.56ms
step:889/1680 train_time:79623ms step_avg:89.57ms
step:890/1680 train_time:79715ms step_avg:89.57ms
step:891/1680 train_time:79806ms step_avg:89.57ms
step:892/1680 train_time:79896ms step_avg:89.57ms
step:893/1680 train_time:79986ms step_avg:89.57ms
step:894/1680 train_time:80075ms step_avg:89.57ms
step:895/1680 train_time:80165ms step_avg:89.57ms
step:896/1680 train_time:80254ms step_avg:89.57ms
step:897/1680 train_time:80343ms step_avg:89.57ms
step:898/1680 train_time:80433ms step_avg:89.57ms
step:899/1680 train_time:80525ms step_avg:89.57ms
step:900/1680 train_time:80616ms step_avg:89.57ms
step:901/1680 train_time:80707ms step_avg:89.57ms
step:902/1680 train_time:80798ms step_avg:89.58ms
step:903/1680 train_time:80888ms step_avg:89.58ms
step:904/1680 train_time:80977ms step_avg:89.58ms
step:905/1680 train_time:81067ms step_avg:89.58ms
step:906/1680 train_time:81157ms step_avg:89.58ms
step:907/1680 train_time:81246ms step_avg:89.58ms
step:908/1680 train_time:81335ms step_avg:89.58ms
step:909/1680 train_time:81425ms step_avg:89.58ms
step:910/1680 train_time:81515ms step_avg:89.58ms
step:911/1680 train_time:81606ms step_avg:89.58ms
step:912/1680 train_time:81697ms step_avg:89.58ms
step:913/1680 train_time:81787ms step_avg:89.58ms
step:914/1680 train_time:81877ms step_avg:89.58ms
step:915/1680 train_time:81968ms step_avg:89.58ms
step:916/1680 train_time:82058ms step_avg:89.58ms
step:917/1680 train_time:82150ms step_avg:89.59ms
step:918/1680 train_time:82240ms step_avg:89.59ms
step:919/1680 train_time:82329ms step_avg:89.59ms
step:920/1680 train_time:82420ms step_avg:89.59ms
step:921/1680 train_time:82510ms step_avg:89.59ms
step:922/1680 train_time:82601ms step_avg:89.59ms
step:923/1680 train_time:82692ms step_avg:89.59ms
step:924/1680 train_time:82783ms step_avg:89.59ms
step:925/1680 train_time:82872ms step_avg:89.59ms
step:926/1680 train_time:82963ms step_avg:89.59ms
step:927/1680 train_time:83054ms step_avg:89.59ms
step:928/1680 train_time:83144ms step_avg:89.60ms
step:929/1680 train_time:83234ms step_avg:89.60ms
step:930/1680 train_time:83324ms step_avg:89.60ms
step:931/1680 train_time:83413ms step_avg:89.60ms
step:932/1680 train_time:83504ms step_avg:89.60ms
step:933/1680 train_time:83594ms step_avg:89.60ms
step:934/1680 train_time:83685ms step_avg:89.60ms
step:935/1680 train_time:83775ms step_avg:89.60ms
step:936/1680 train_time:83866ms step_avg:89.60ms
step:937/1680 train_time:83956ms step_avg:89.60ms
step:938/1680 train_time:84046ms step_avg:89.60ms
step:939/1680 train_time:84135ms step_avg:89.60ms
step:940/1680 train_time:84225ms step_avg:89.60ms
step:941/1680 train_time:84314ms step_avg:89.60ms
step:942/1680 train_time:84405ms step_avg:89.60ms
step:943/1680 train_time:84494ms step_avg:89.60ms
step:944/1680 train_time:84585ms step_avg:89.60ms
step:945/1680 train_time:84675ms step_avg:89.60ms
step:946/1680 train_time:84765ms step_avg:89.60ms
step:947/1680 train_time:84856ms step_avg:89.61ms
step:948/1680 train_time:84947ms step_avg:89.61ms
step:949/1680 train_time:85037ms step_avg:89.61ms
step:950/1680 train_time:85127ms step_avg:89.61ms
step:951/1680 train_time:85216ms step_avg:89.61ms
step:952/1680 train_time:85307ms step_avg:89.61ms
step:953/1680 train_time:85396ms step_avg:89.61ms
step:954/1680 train_time:85487ms step_avg:89.61ms
step:955/1680 train_time:85576ms step_avg:89.61ms
step:956/1680 train_time:85667ms step_avg:89.61ms
step:957/1680 train_time:85758ms step_avg:89.61ms
step:958/1680 train_time:85848ms step_avg:89.61ms
step:959/1680 train_time:85938ms step_avg:89.61ms
step:960/1680 train_time:86028ms step_avg:89.61ms
step:961/1680 train_time:86118ms step_avg:89.61ms
step:962/1680 train_time:86209ms step_avg:89.61ms
step:963/1680 train_time:86298ms step_avg:89.61ms
step:964/1680 train_time:86389ms step_avg:89.62ms
step:965/1680 train_time:86478ms step_avg:89.61ms
step:966/1680 train_time:86568ms step_avg:89.62ms
step:967/1680 train_time:86659ms step_avg:89.62ms
step:968/1680 train_time:86750ms step_avg:89.62ms
step:969/1680 train_time:86841ms step_avg:89.62ms
step:970/1680 train_time:86931ms step_avg:89.62ms
step:971/1680 train_time:87020ms step_avg:89.62ms
step:972/1680 train_time:87110ms step_avg:89.62ms
step:973/1680 train_time:87200ms step_avg:89.62ms
step:974/1680 train_time:87290ms step_avg:89.62ms
step:975/1680 train_time:87380ms step_avg:89.62ms
step:976/1680 train_time:87470ms step_avg:89.62ms
step:977/1680 train_time:87560ms step_avg:89.62ms
step:978/1680 train_time:87651ms step_avg:89.62ms
step:979/1680 train_time:87741ms step_avg:89.62ms
step:980/1680 train_time:87831ms step_avg:89.62ms
step:981/1680 train_time:87921ms step_avg:89.62ms
step:982/1680 train_time:88012ms step_avg:89.63ms
step:983/1680 train_time:88102ms step_avg:89.63ms
step:984/1680 train_time:88193ms step_avg:89.63ms
step:985/1680 train_time:88283ms step_avg:89.63ms
step:986/1680 train_time:88373ms step_avg:89.63ms
step:987/1680 train_time:88462ms step_avg:89.63ms
step:988/1680 train_time:88552ms step_avg:89.63ms
step:989/1680 train_time:88642ms step_avg:89.63ms
step:990/1680 train_time:88732ms step_avg:89.63ms
step:991/1680 train_time:88822ms step_avg:89.63ms
step:992/1680 train_time:88913ms step_avg:89.63ms
step:993/1680 train_time:89003ms step_avg:89.63ms
step:994/1680 train_time:89093ms step_avg:89.63ms
step:995/1680 train_time:89184ms step_avg:89.63ms
step:996/1680 train_time:89274ms step_avg:89.63ms
step:997/1680 train_time:89364ms step_avg:89.63ms
step:998/1680 train_time:89454ms step_avg:89.63ms
step:999/1680 train_time:89544ms step_avg:89.63ms
step:1000/1680 train_time:89635ms step_avg:89.63ms
step:1000/1680 val_loss:3.4686 train_time:89727ms step_avg:89.73ms
step:1001/1680 train_time:89749ms step_avg:89.66ms
step:1002/1680 train_time:89821ms step_avg:89.64ms
step:1003/1680 train_time:89915ms step_avg:89.65ms
step:1004/1680 train_time:90005ms step_avg:89.65ms
step:1005/1680 train_time:90094ms step_avg:89.65ms
step:1006/1680 train_time:90184ms step_avg:89.65ms
step:1007/1680 train_time:90273ms step_avg:89.65ms
step:1008/1680 train_time:90362ms step_avg:89.65ms
step:1009/1680 train_time:90451ms step_avg:89.64ms
step:1010/1680 train_time:90540ms step_avg:89.64ms
step:1011/1680 train_time:90629ms step_avg:89.64ms
step:1012/1680 train_time:90720ms step_avg:89.64ms
step:1013/1680 train_time:90814ms step_avg:89.65ms
step:1014/1680 train_time:90907ms step_avg:89.65ms
step:1015/1680 train_time:90998ms step_avg:89.65ms
step:1016/1680 train_time:91087ms step_avg:89.65ms
step:1017/1680 train_time:91177ms step_avg:89.65ms
step:1018/1680 train_time:91267ms step_avg:89.65ms
step:1019/1680 train_time:91356ms step_avg:89.65ms
step:1020/1680 train_time:91445ms step_avg:89.65ms
step:1021/1680 train_time:91534ms step_avg:89.65ms
step:1022/1680 train_time:91623ms step_avg:89.65ms
step:1023/1680 train_time:91713ms step_avg:89.65ms
step:1024/1680 train_time:91805ms step_avg:89.65ms
step:1025/1680 train_time:91896ms step_avg:89.66ms
step:1026/1680 train_time:91987ms step_avg:89.66ms
step:1027/1680 train_time:92078ms step_avg:89.66ms
step:1028/1680 train_time:92168ms step_avg:89.66ms
step:1029/1680 train_time:92258ms step_avg:89.66ms
step:1030/1680 train_time:92347ms step_avg:89.66ms
step:1031/1680 train_time:92436ms step_avg:89.66ms
step:1032/1680 train_time:92526ms step_avg:89.66ms
step:1033/1680 train_time:92616ms step_avg:89.66ms
step:1034/1680 train_time:92706ms step_avg:89.66ms
step:1035/1680 train_time:92797ms step_avg:89.66ms
step:1036/1680 train_time:92888ms step_avg:89.66ms
step:1037/1680 train_time:92978ms step_avg:89.66ms
step:1038/1680 train_time:93068ms step_avg:89.66ms
step:1039/1680 train_time:93159ms step_avg:89.66ms
step:1040/1680 train_time:93249ms step_avg:89.66ms
step:1041/1680 train_time:93339ms step_avg:89.66ms
step:1042/1680 train_time:93428ms step_avg:89.66ms
step:1043/1680 train_time:93518ms step_avg:89.66ms
step:1044/1680 train_time:93608ms step_avg:89.66ms
step:1045/1680 train_time:93698ms step_avg:89.66ms
step:1046/1680 train_time:93788ms step_avg:89.66ms
step:1047/1680 train_time:93879ms step_avg:89.66ms
step:1048/1680 train_time:93969ms step_avg:89.67ms
step:1049/1680 train_time:94059ms step_avg:89.67ms
step:1050/1680 train_time:94150ms step_avg:89.67ms
step:1051/1680 train_time:94241ms step_avg:89.67ms
step:1052/1680 train_time:94330ms step_avg:89.67ms
step:1053/1680 train_time:94421ms step_avg:89.67ms
step:1054/1680 train_time:94511ms step_avg:89.67ms
step:1055/1680 train_time:94601ms step_avg:89.67ms
step:1056/1680 train_time:94691ms step_avg:89.67ms
step:1057/1680 train_time:94781ms step_avg:89.67ms
step:1058/1680 train_time:94872ms step_avg:89.67ms
step:1059/1680 train_time:94963ms step_avg:89.67ms
step:1060/1680 train_time:95053ms step_avg:89.67ms
step:1061/1680 train_time:95143ms step_avg:89.67ms
step:1062/1680 train_time:95234ms step_avg:89.67ms
step:1063/1680 train_time:95324ms step_avg:89.67ms
step:1064/1680 train_time:95414ms step_avg:89.67ms
step:1065/1680 train_time:95505ms step_avg:89.68ms
step:1066/1680 train_time:95594ms step_avg:89.68ms
step:1067/1680 train_time:95685ms step_avg:89.68ms
step:1068/1680 train_time:95775ms step_avg:89.68ms
step:1069/1680 train_time:95865ms step_avg:89.68ms
step:1070/1680 train_time:95955ms step_avg:89.68ms
step:1071/1680 train_time:96046ms step_avg:89.68ms
step:1072/1680 train_time:96135ms step_avg:89.68ms
step:1073/1680 train_time:96226ms step_avg:89.68ms
step:1074/1680 train_time:96315ms step_avg:89.68ms
step:1075/1680 train_time:96405ms step_avg:89.68ms
step:1076/1680 train_time:96495ms step_avg:89.68ms
step:1077/1680 train_time:96584ms step_avg:89.68ms
step:1078/1680 train_time:96675ms step_avg:89.68ms
step:1079/1680 train_time:96765ms step_avg:89.68ms
step:1080/1680 train_time:96855ms step_avg:89.68ms
step:1081/1680 train_time:96946ms step_avg:89.68ms
step:1082/1680 train_time:97037ms step_avg:89.68ms
step:1083/1680 train_time:97127ms step_avg:89.68ms
step:1084/1680 train_time:97218ms step_avg:89.68ms
step:1085/1680 train_time:97309ms step_avg:89.69ms
step:1086/1680 train_time:97399ms step_avg:89.69ms
step:1087/1680 train_time:97489ms step_avg:89.69ms
step:1088/1680 train_time:97580ms step_avg:89.69ms
step:1089/1680 train_time:97669ms step_avg:89.69ms
step:1090/1680 train_time:97759ms step_avg:89.69ms
step:1091/1680 train_time:97849ms step_avg:89.69ms
step:1092/1680 train_time:97939ms step_avg:89.69ms
step:1093/1680 train_time:98028ms step_avg:89.69ms
step:1094/1680 train_time:98119ms step_avg:89.69ms
step:1095/1680 train_time:98210ms step_avg:89.69ms
step:1096/1680 train_time:98301ms step_avg:89.69ms
step:1097/1680 train_time:98391ms step_avg:89.69ms
step:1098/1680 train_time:98482ms step_avg:89.69ms
step:1099/1680 train_time:98572ms step_avg:89.69ms
step:1100/1680 train_time:98663ms step_avg:89.69ms
step:1101/1680 train_time:98754ms step_avg:89.69ms
step:1102/1680 train_time:98844ms step_avg:89.70ms
step:1103/1680 train_time:98936ms step_avg:89.70ms
step:1104/1680 train_time:99026ms step_avg:89.70ms
step:1105/1680 train_time:99117ms step_avg:89.70ms
step:1106/1680 train_time:99208ms step_avg:89.70ms
step:1107/1680 train_time:99299ms step_avg:89.70ms
step:1108/1680 train_time:99389ms step_avg:89.70ms
step:1109/1680 train_time:99481ms step_avg:89.70ms
step:1110/1680 train_time:99571ms step_avg:89.70ms
step:1111/1680 train_time:99663ms step_avg:89.71ms
step:1112/1680 train_time:99753ms step_avg:89.71ms
step:1113/1680 train_time:99845ms step_avg:89.71ms
step:1114/1680 train_time:99935ms step_avg:89.71ms
step:1115/1680 train_time:100026ms step_avg:89.71ms
step:1116/1680 train_time:100117ms step_avg:89.71ms
step:1117/1680 train_time:100208ms step_avg:89.71ms
step:1118/1680 train_time:100299ms step_avg:89.71ms
step:1119/1680 train_time:100389ms step_avg:89.71ms
step:1120/1680 train_time:100480ms step_avg:89.71ms
step:1121/1680 train_time:100570ms step_avg:89.71ms
step:1122/1680 train_time:100661ms step_avg:89.72ms
step:1123/1680 train_time:100751ms step_avg:89.72ms
step:1124/1680 train_time:100842ms step_avg:89.72ms
step:1125/1680 train_time:100933ms step_avg:89.72ms
step:1125/1680 val_loss:3.4147 train_time:101026ms step_avg:89.80ms
step:1126/1680 train_time:101048ms step_avg:89.74ms
step:1127/1680 train_time:101118ms step_avg:89.72ms
step:1128/1680 train_time:101222ms step_avg:89.74ms
step:1129/1680 train_time:101313ms step_avg:89.74ms
step:1130/1680 train_time:101403ms step_avg:89.74ms
step:1131/1680 train_time:101493ms step_avg:89.74ms
step:1132/1680 train_time:101583ms step_avg:89.74ms
step:1133/1680 train_time:101673ms step_avg:89.74ms
step:1134/1680 train_time:101762ms step_avg:89.74ms
step:1135/1680 train_time:101852ms step_avg:89.74ms
step:1136/1680 train_time:101944ms step_avg:89.74ms
step:1137/1680 train_time:102037ms step_avg:89.74ms
step:1138/1680 train_time:102130ms step_avg:89.75ms
step:1139/1680 train_time:102223ms step_avg:89.75ms
step:1140/1680 train_time:102313ms step_avg:89.75ms
step:1141/1680 train_time:102404ms step_avg:89.75ms
step:1142/1680 train_time:102494ms step_avg:89.75ms
step:1143/1680 train_time:102584ms step_avg:89.75ms
step:1144/1680 train_time:102674ms step_avg:89.75ms
step:1145/1680 train_time:102763ms step_avg:89.75ms
step:1146/1680 train_time:102854ms step_avg:89.75ms
step:1147/1680 train_time:102946ms step_avg:89.75ms
step:1148/1680 train_time:103037ms step_avg:89.75ms
step:1149/1680 train_time:103130ms step_avg:89.76ms
step:1150/1680 train_time:103221ms step_avg:89.76ms
step:1151/1680 train_time:103312ms step_avg:89.76ms
step:1152/1680 train_time:103403ms step_avg:89.76ms
step:1153/1680 train_time:103494ms step_avg:89.76ms
step:1154/1680 train_time:103583ms step_avg:89.76ms
step:1155/1680 train_time:103673ms step_avg:89.76ms
step:1156/1680 train_time:103763ms step_avg:89.76ms
step:1157/1680 train_time:103853ms step_avg:89.76ms
step:1158/1680 train_time:103945ms step_avg:89.76ms
step:1159/1680 train_time:104036ms step_avg:89.76ms
step:1160/1680 train_time:104128ms step_avg:89.77ms
step:1161/1680 train_time:104220ms step_avg:89.77ms
step:1162/1680 train_time:104312ms step_avg:89.77ms
step:1163/1680 train_time:104402ms step_avg:89.77ms
step:1164/1680 train_time:104493ms step_avg:89.77ms
step:1165/1680 train_time:104583ms step_avg:89.77ms
step:1166/1680 train_time:104674ms step_avg:89.77ms
step:1167/1680 train_time:104764ms step_avg:89.77ms
step:1168/1680 train_time:104855ms step_avg:89.77ms
step:1169/1680 train_time:104947ms step_avg:89.78ms
step:1170/1680 train_time:105038ms step_avg:89.78ms
step:1171/1680 train_time:105130ms step_avg:89.78ms
step:1172/1680 train_time:105221ms step_avg:89.78ms
step:1173/1680 train_time:105312ms step_avg:89.78ms
step:1174/1680 train_time:105403ms step_avg:89.78ms
step:1175/1680 train_time:105494ms step_avg:89.78ms
step:1176/1680 train_time:105583ms step_avg:89.78ms
step:1177/1680 train_time:105675ms step_avg:89.78ms
step:1178/1680 train_time:105765ms step_avg:89.78ms
step:1179/1680 train_time:105856ms step_avg:89.78ms
step:1180/1680 train_time:105947ms step_avg:89.79ms
step:1181/1680 train_time:106038ms step_avg:89.79ms
step:1182/1680 train_time:106130ms step_avg:89.79ms
step:1183/1680 train_time:106221ms step_avg:89.79ms
step:1184/1680 train_time:106312ms step_avg:89.79ms
step:1185/1680 train_time:106402ms step_avg:89.79ms
step:1186/1680 train_time:106492ms step_avg:89.79ms
step:1187/1680 train_time:106582ms step_avg:89.79ms
step:1188/1680 train_time:106672ms step_avg:89.79ms
step:1189/1680 train_time:106762ms step_avg:89.79ms
step:1190/1680 train_time:106853ms step_avg:89.79ms
step:1191/1680 train_time:106945ms step_avg:89.79ms
step:1192/1680 train_time:107036ms step_avg:89.79ms
step:1193/1680 train_time:107127ms step_avg:89.80ms
step:1194/1680 train_time:107218ms step_avg:89.80ms
step:1195/1680 train_time:107309ms step_avg:89.80ms
step:1196/1680 train_time:107399ms step_avg:89.80ms
step:1197/1680 train_time:107489ms step_avg:89.80ms
step:1198/1680 train_time:107580ms step_avg:89.80ms
step:1199/1680 train_time:107671ms step_avg:89.80ms
step:1200/1680 train_time:107760ms step_avg:89.80ms
step:1201/1680 train_time:107852ms step_avg:89.80ms
step:1202/1680 train_time:107943ms step_avg:89.80ms
step:1203/1680 train_time:108035ms step_avg:89.80ms
step:1204/1680 train_time:108127ms step_avg:89.81ms
step:1205/1680 train_time:108217ms step_avg:89.81ms
step:1206/1680 train_time:108308ms step_avg:89.81ms
step:1207/1680 train_time:108399ms step_avg:89.81ms
step:1208/1680 train_time:108490ms step_avg:89.81ms
step:1209/1680 train_time:108580ms step_avg:89.81ms
step:1210/1680 train_time:108672ms step_avg:89.81ms
step:1211/1680 train_time:108762ms step_avg:89.81ms
step:1212/1680 train_time:108854ms step_avg:89.81ms
step:1213/1680 train_time:108946ms step_avg:89.81ms
step:1214/1680 train_time:109037ms step_avg:89.82ms
step:1215/1680 train_time:109127ms step_avg:89.82ms
step:1216/1680 train_time:109218ms step_avg:89.82ms
step:1217/1680 train_time:109309ms step_avg:89.82ms
step:1218/1680 train_time:109400ms step_avg:89.82ms
step:1219/1680 train_time:109490ms step_avg:89.82ms
step:1220/1680 train_time:109580ms step_avg:89.82ms
step:1221/1680 train_time:109670ms step_avg:89.82ms
step:1222/1680 train_time:109760ms step_avg:89.82ms
step:1223/1680 train_time:109852ms step_avg:89.82ms
step:1224/1680 train_time:109942ms step_avg:89.82ms
step:1225/1680 train_time:110035ms step_avg:89.82ms
step:1226/1680 train_time:110127ms step_avg:89.83ms
step:1227/1680 train_time:110218ms step_avg:89.83ms
step:1228/1680 train_time:110308ms step_avg:89.83ms
step:1229/1680 train_time:110399ms step_avg:89.83ms
step:1230/1680 train_time:110489ms step_avg:89.83ms
step:1231/1680 train_time:110579ms step_avg:89.83ms
step:1232/1680 train_time:110669ms step_avg:89.83ms
step:1233/1680 train_time:110760ms step_avg:89.83ms
step:1234/1680 train_time:110852ms step_avg:89.83ms
step:1235/1680 train_time:110944ms step_avg:89.83ms
step:1236/1680 train_time:111035ms step_avg:89.83ms
step:1237/1680 train_time:111126ms step_avg:89.83ms
step:1238/1680 train_time:111217ms step_avg:89.84ms
step:1239/1680 train_time:111308ms step_avg:89.84ms
step:1240/1680 train_time:111398ms step_avg:89.84ms
step:1241/1680 train_time:111488ms step_avg:89.84ms
step:1242/1680 train_time:111579ms step_avg:89.84ms
step:1243/1680 train_time:111669ms step_avg:89.84ms
step:1244/1680 train_time:111760ms step_avg:89.84ms
step:1245/1680 train_time:111852ms step_avg:89.84ms
step:1246/1680 train_time:111943ms step_avg:89.84ms
step:1247/1680 train_time:112033ms step_avg:89.84ms
step:1248/1680 train_time:112124ms step_avg:89.84ms
step:1249/1680 train_time:112216ms step_avg:89.84ms
step:1250/1680 train_time:112307ms step_avg:89.85ms
step:1250/1680 val_loss:3.3759 train_time:112399ms step_avg:89.92ms
step:1251/1680 train_time:112421ms step_avg:89.86ms
step:1252/1680 train_time:112495ms step_avg:89.85ms
step:1253/1680 train_time:112591ms step_avg:89.86ms
step:1254/1680 train_time:112682ms step_avg:89.86ms
step:1255/1680 train_time:112772ms step_avg:89.86ms
step:1256/1680 train_time:112862ms step_avg:89.86ms
step:1257/1680 train_time:112952ms step_avg:89.86ms
step:1258/1680 train_time:113042ms step_avg:89.86ms
step:1259/1680 train_time:113132ms step_avg:89.86ms
step:1260/1680 train_time:113221ms step_avg:89.86ms
step:1261/1680 train_time:113312ms step_avg:89.86ms
step:1262/1680 train_time:113404ms step_avg:89.86ms
step:1263/1680 train_time:113498ms step_avg:89.86ms
step:1264/1680 train_time:113591ms step_avg:89.87ms
step:1265/1680 train_time:113682ms step_avg:89.87ms
step:1266/1680 train_time:113773ms step_avg:89.87ms
step:1267/1680 train_time:113863ms step_avg:89.87ms
step:1268/1680 train_time:113953ms step_avg:89.87ms
step:1269/1680 train_time:114042ms step_avg:89.87ms
step:1270/1680 train_time:114132ms step_avg:89.87ms
step:1271/1680 train_time:114221ms step_avg:89.87ms
step:1272/1680 train_time:114312ms step_avg:89.87ms
step:1273/1680 train_time:114405ms step_avg:89.87ms
step:1274/1680 train_time:114496ms step_avg:89.87ms
step:1275/1680 train_time:114589ms step_avg:89.87ms
step:1276/1680 train_time:114679ms step_avg:89.87ms
step:1277/1680 train_time:114771ms step_avg:89.88ms
step:1278/1680 train_time:114862ms step_avg:89.88ms
step:1279/1680 train_time:114951ms step_avg:89.88ms
step:1280/1680 train_time:115041ms step_avg:89.88ms
step:1281/1680 train_time:115131ms step_avg:89.88ms
step:1282/1680 train_time:115222ms step_avg:89.88ms
step:1283/1680 train_time:115313ms step_avg:89.88ms
step:1284/1680 train_time:115405ms step_avg:89.88ms
step:1285/1680 train_time:115496ms step_avg:89.88ms
step:1286/1680 train_time:115589ms step_avg:89.88ms
step:1287/1680 train_time:115680ms step_avg:89.88ms
step:1288/1680 train_time:115774ms step_avg:89.89ms
step:1289/1680 train_time:115866ms step_avg:89.89ms
step:1290/1680 train_time:115956ms step_avg:89.89ms
step:1291/1680 train_time:116048ms step_avg:89.89ms
step:1292/1680 train_time:116137ms step_avg:89.89ms
step:1293/1680 train_time:116227ms step_avg:89.89ms
step:1294/1680 train_time:116318ms step_avg:89.89ms
step:1295/1680 train_time:116409ms step_avg:89.89ms
step:1296/1680 train_time:116499ms step_avg:89.89ms
step:1297/1680 train_time:116591ms step_avg:89.89ms
step:1298/1680 train_time:116683ms step_avg:89.89ms
step:1299/1680 train_time:116775ms step_avg:89.90ms
step:1300/1680 train_time:116866ms step_avg:89.90ms
step:1301/1680 train_time:116956ms step_avg:89.90ms
step:1302/1680 train_time:117047ms step_avg:89.90ms
step:1303/1680 train_time:117137ms step_avg:89.90ms
step:1304/1680 train_time:117228ms step_avg:89.90ms
step:1305/1680 train_time:117318ms step_avg:89.90ms
step:1306/1680 train_time:117409ms step_avg:89.90ms
step:1307/1680 train_time:117499ms step_avg:89.90ms
step:1308/1680 train_time:117590ms step_avg:89.90ms
step:1309/1680 train_time:117681ms step_avg:89.90ms
step:1310/1680 train_time:117774ms step_avg:89.90ms
step:1311/1680 train_time:117865ms step_avg:89.90ms
step:1312/1680 train_time:117956ms step_avg:89.91ms
step:1313/1680 train_time:118047ms step_avg:89.91ms
step:1314/1680 train_time:118137ms step_avg:89.91ms
step:1315/1680 train_time:118227ms step_avg:89.91ms
step:1316/1680 train_time:118318ms step_avg:89.91ms
step:1317/1680 train_time:118410ms step_avg:89.91ms
step:1318/1680 train_time:118500ms step_avg:89.91ms
step:1319/1680 train_time:118592ms step_avg:89.91ms
step:1320/1680 train_time:118684ms step_avg:89.91ms
step:1321/1680 train_time:118775ms step_avg:89.91ms
step:1322/1680 train_time:118866ms step_avg:89.91ms
step:1323/1680 train_time:118957ms step_avg:89.91ms
step:1324/1680 train_time:119047ms step_avg:89.91ms
step:1325/1680 train_time:119138ms step_avg:89.92ms
step:1326/1680 train_time:119228ms step_avg:89.92ms
step:1327/1680 train_time:119318ms step_avg:89.92ms
step:1328/1680 train_time:119409ms step_avg:89.92ms
step:1329/1680 train_time:119501ms step_avg:89.92ms
step:1330/1680 train_time:119592ms step_avg:89.92ms
step:1331/1680 train_time:119683ms step_avg:89.92ms
step:1332/1680 train_time:119774ms step_avg:89.92ms
step:1333/1680 train_time:119865ms step_avg:89.92ms
step:1334/1680 train_time:119956ms step_avg:89.92ms
step:1335/1680 train_time:120047ms step_avg:89.92ms
step:1336/1680 train_time:120138ms step_avg:89.92ms
step:1337/1680 train_time:120229ms step_avg:89.92ms
step:1338/1680 train_time:120319ms step_avg:89.92ms
step:1339/1680 train_time:120410ms step_avg:89.93ms
step:1340/1680 train_time:120500ms step_avg:89.93ms
step:1341/1680 train_time:120591ms step_avg:89.93ms
step:1342/1680 train_time:120683ms step_avg:89.93ms
step:1343/1680 train_time:120774ms step_avg:89.93ms
step:1344/1680 train_time:120865ms step_avg:89.93ms
step:1345/1680 train_time:120955ms step_avg:89.93ms
step:1346/1680 train_time:121046ms step_avg:89.93ms
step:1347/1680 train_time:121138ms step_avg:89.93ms
step:1348/1680 train_time:121229ms step_avg:89.93ms
step:1349/1680 train_time:121321ms step_avg:89.93ms
step:1350/1680 train_time:121413ms step_avg:89.94ms
step:1351/1680 train_time:121505ms step_avg:89.94ms
step:1352/1680 train_time:121596ms step_avg:89.94ms
step:1353/1680 train_time:121687ms step_avg:89.94ms
step:1354/1680 train_time:121778ms step_avg:89.94ms
step:1355/1680 train_time:121870ms step_avg:89.94ms
step:1356/1680 train_time:121960ms step_avg:89.94ms
step:1357/1680 train_time:122051ms step_avg:89.94ms
step:1358/1680 train_time:122141ms step_avg:89.94ms
step:1359/1680 train_time:122232ms step_avg:89.94ms
step:1360/1680 train_time:122322ms step_avg:89.94ms
step:1361/1680 train_time:122413ms step_avg:89.94ms
step:1362/1680 train_time:122504ms step_avg:89.94ms
step:1363/1680 train_time:122595ms step_avg:89.95ms
step:1364/1680 train_time:122686ms step_avg:89.95ms
step:1365/1680 train_time:122777ms step_avg:89.95ms
step:1366/1680 train_time:122867ms step_avg:89.95ms
step:1367/1680 train_time:122958ms step_avg:89.95ms
step:1368/1680 train_time:123048ms step_avg:89.95ms
step:1369/1680 train_time:123139ms step_avg:89.95ms
step:1370/1680 train_time:123229ms step_avg:89.95ms
step:1371/1680 train_time:123320ms step_avg:89.95ms
step:1372/1680 train_time:123411ms step_avg:89.95ms
step:1373/1680 train_time:123503ms step_avg:89.95ms
step:1374/1680 train_time:123594ms step_avg:89.95ms
step:1375/1680 train_time:123686ms step_avg:89.95ms
step:1375/1680 val_loss:3.3412 train_time:123778ms step_avg:90.02ms
step:1376/1680 train_time:123800ms step_avg:89.97ms
step:1377/1680 train_time:123873ms step_avg:89.96ms
step:1378/1680 train_time:123969ms step_avg:89.96ms
step:1379/1680 train_time:124060ms step_avg:89.96ms
step:1380/1680 train_time:124151ms step_avg:89.96ms
step:1381/1680 train_time:124241ms step_avg:89.96ms
step:1382/1680 train_time:124330ms step_avg:89.96ms
step:1383/1680 train_time:124420ms step_avg:89.96ms
step:1384/1680 train_time:124510ms step_avg:89.96ms
step:1385/1680 train_time:124600ms step_avg:89.96ms
step:1386/1680 train_time:124690ms step_avg:89.96ms
step:1387/1680 train_time:124783ms step_avg:89.97ms
step:1388/1680 train_time:124876ms step_avg:89.97ms
step:1389/1680 train_time:124969ms step_avg:89.97ms
step:1390/1680 train_time:125061ms step_avg:89.97ms
step:1391/1680 train_time:125152ms step_avg:89.97ms
step:1392/1680 train_time:125242ms step_avg:89.97ms
step:1393/1680 train_time:125332ms step_avg:89.97ms
step:1394/1680 train_time:125421ms step_avg:89.97ms
step:1395/1680 train_time:125511ms step_avg:89.97ms
step:1396/1680 train_time:125601ms step_avg:89.97ms
step:1397/1680 train_time:125691ms step_avg:89.97ms
step:1398/1680 train_time:125783ms step_avg:89.97ms
step:1399/1680 train_time:125874ms step_avg:89.97ms
step:1400/1680 train_time:125967ms step_avg:89.98ms
step:1401/1680 train_time:126058ms step_avg:89.98ms
step:1402/1680 train_time:126150ms step_avg:89.98ms
step:1403/1680 train_time:126241ms step_avg:89.98ms
step:1404/1680 train_time:126331ms step_avg:89.98ms
step:1405/1680 train_time:126421ms step_avg:89.98ms
step:1406/1680 train_time:126512ms step_avg:89.98ms
step:1407/1680 train_time:126602ms step_avg:89.98ms
step:1408/1680 train_time:126693ms step_avg:89.98ms
step:1409/1680 train_time:126784ms step_avg:89.98ms
step:1410/1680 train_time:126877ms step_avg:89.98ms
step:1411/1680 train_time:126969ms step_avg:89.99ms
step:1412/1680 train_time:127060ms step_avg:89.99ms
step:1413/1680 train_time:127152ms step_avg:89.99ms
step:1414/1680 train_time:127243ms step_avg:89.99ms
step:1415/1680 train_time:127333ms step_avg:89.99ms
step:1416/1680 train_time:127424ms step_avg:89.99ms
step:1417/1680 train_time:127514ms step_avg:89.99ms
step:1418/1680 train_time:127604ms step_avg:89.99ms
step:1419/1680 train_time:127694ms step_avg:89.99ms
step:1420/1680 train_time:127784ms step_avg:89.99ms
step:1421/1680 train_time:127876ms step_avg:89.99ms
step:1422/1680 train_time:127967ms step_avg:89.99ms
step:1423/1680 train_time:128057ms step_avg:89.99ms
step:1424/1680 train_time:128150ms step_avg:89.99ms
step:1425/1680 train_time:128242ms step_avg:89.99ms
step:1426/1680 train_time:128332ms step_avg:89.99ms
step:1427/1680 train_time:128424ms step_avg:90.00ms
step:1428/1680 train_time:128514ms step_avg:90.00ms
step:1429/1680 train_time:128604ms step_avg:90.00ms
step:1430/1680 train_time:128694ms step_avg:90.00ms
step:1431/1680 train_time:128785ms step_avg:90.00ms
step:1432/1680 train_time:128876ms step_avg:90.00ms
step:1433/1680 train_time:128967ms step_avg:90.00ms
step:1434/1680 train_time:129057ms step_avg:90.00ms
step:1435/1680 train_time:129150ms step_avg:90.00ms
step:1436/1680 train_time:129242ms step_avg:90.00ms
step:1437/1680 train_time:129331ms step_avg:90.00ms
step:1438/1680 train_time:129422ms step_avg:90.00ms
step:1439/1680 train_time:129512ms step_avg:90.00ms
step:1440/1680 train_time:129603ms step_avg:90.00ms
step:1441/1680 train_time:129693ms step_avg:90.00ms
step:1442/1680 train_time:129784ms step_avg:90.00ms
step:1443/1680 train_time:129874ms step_avg:90.00ms
step:1444/1680 train_time:129965ms step_avg:90.00ms
step:1445/1680 train_time:130056ms step_avg:90.00ms
step:1446/1680 train_time:130149ms step_avg:90.01ms
step:1447/1680 train_time:130239ms step_avg:90.01ms
step:1448/1680 train_time:130330ms step_avg:90.01ms
step:1449/1680 train_time:130422ms step_avg:90.01ms
step:1450/1680 train_time:130512ms step_avg:90.01ms
step:1451/1680 train_time:130603ms step_avg:90.01ms
step:1452/1680 train_time:130693ms step_avg:90.01ms
step:1453/1680 train_time:130784ms step_avg:90.01ms
step:1454/1680 train_time:130874ms step_avg:90.01ms
step:1455/1680 train_time:130966ms step_avg:90.01ms
step:1456/1680 train_time:131057ms step_avg:90.01ms
step:1457/1680 train_time:131148ms step_avg:90.01ms
step:1458/1680 train_time:131239ms step_avg:90.01ms
step:1459/1680 train_time:131331ms step_avg:90.01ms
step:1460/1680 train_time:131423ms step_avg:90.02ms
step:1461/1680 train_time:131513ms step_avg:90.02ms
step:1462/1680 train_time:131604ms step_avg:90.02ms
step:1463/1680 train_time:131695ms step_avg:90.02ms
step:1464/1680 train_time:131786ms step_avg:90.02ms
step:1465/1680 train_time:131876ms step_avg:90.02ms
step:1466/1680 train_time:131968ms step_avg:90.02ms
step:1467/1680 train_time:132058ms step_avg:90.02ms
step:1468/1680 train_time:132149ms step_avg:90.02ms
step:1469/1680 train_time:132241ms step_avg:90.02ms
step:1470/1680 train_time:132332ms step_avg:90.02ms
step:1471/1680 train_time:132425ms step_avg:90.02ms
step:1472/1680 train_time:132516ms step_avg:90.02ms
step:1473/1680 train_time:132608ms step_avg:90.03ms
step:1474/1680 train_time:132699ms step_avg:90.03ms
step:1475/1680 train_time:132789ms step_avg:90.03ms
step:1476/1680 train_time:132879ms step_avg:90.03ms
step:1477/1680 train_time:132970ms step_avg:90.03ms
step:1478/1680 train_time:133061ms step_avg:90.03ms
step:1479/1680 train_time:133152ms step_avg:90.03ms
step:1480/1680 train_time:133243ms step_avg:90.03ms
step:1481/1680 train_time:133334ms step_avg:90.03ms
step:1482/1680 train_time:133426ms step_avg:90.03ms
step:1483/1680 train_time:133517ms step_avg:90.03ms
step:1484/1680 train_time:133608ms step_avg:90.03ms
step:1485/1680 train_time:133700ms step_avg:90.03ms
step:1486/1680 train_time:133789ms step_avg:90.03ms
step:1487/1680 train_time:133880ms step_avg:90.03ms
step:1488/1680 train_time:133970ms step_avg:90.03ms
step:1489/1680 train_time:134060ms step_avg:90.03ms
step:1490/1680 train_time:134151ms step_avg:90.03ms
step:1491/1680 train_time:134242ms step_avg:90.04ms
step:1492/1680 train_time:134333ms step_avg:90.04ms
step:1493/1680 train_time:134425ms step_avg:90.04ms
step:1494/1680 train_time:134517ms step_avg:90.04ms
step:1495/1680 train_time:134610ms step_avg:90.04ms
step:1496/1680 train_time:134701ms step_avg:90.04ms
step:1497/1680 train_time:134792ms step_avg:90.04ms
step:1498/1680 train_time:134882ms step_avg:90.04ms
step:1499/1680 train_time:134973ms step_avg:90.04ms
step:1500/1680 train_time:135063ms step_avg:90.04ms
step:1500/1680 val_loss:3.3118 train_time:135156ms step_avg:90.10ms
step:1501/1680 train_time:135178ms step_avg:90.06ms
step:1502/1680 train_time:135252ms step_avg:90.05ms
step:1503/1680 train_time:135349ms step_avg:90.05ms
step:1504/1680 train_time:135441ms step_avg:90.05ms
step:1505/1680 train_time:135533ms step_avg:90.06ms
step:1506/1680 train_time:135623ms step_avg:90.06ms
step:1507/1680 train_time:135712ms step_avg:90.05ms
step:1508/1680 train_time:135802ms step_avg:90.05ms
step:1509/1680 train_time:135890ms step_avg:90.05ms
step:1510/1680 train_time:135980ms step_avg:90.05ms
step:1511/1680 train_time:136070ms step_avg:90.05ms
step:1512/1680 train_time:136162ms step_avg:90.05ms
step:1513/1680 train_time:136256ms step_avg:90.06ms
step:1514/1680 train_time:136352ms step_avg:90.06ms
step:1515/1680 train_time:136447ms step_avg:90.06ms
step:1516/1680 train_time:136538ms step_avg:90.06ms
step:1517/1680 train_time:136629ms step_avg:90.07ms
step:1518/1680 train_time:136718ms step_avg:90.06ms
step:1519/1680 train_time:136809ms step_avg:90.07ms
step:1520/1680 train_time:136899ms step_avg:90.07ms
step:1521/1680 train_time:136989ms step_avg:90.07ms
step:1522/1680 train_time:137081ms step_avg:90.07ms
step:1523/1680 train_time:137171ms step_avg:90.07ms
step:1524/1680 train_time:137263ms step_avg:90.07ms
step:1525/1680 train_time:137355ms step_avg:90.07ms
step:1526/1680 train_time:137446ms step_avg:90.07ms
step:1527/1680 train_time:137538ms step_avg:90.07ms
step:1528/1680 train_time:137629ms step_avg:90.07ms
step:1529/1680 train_time:137719ms step_avg:90.07ms
step:1530/1680 train_time:137810ms step_avg:90.07ms
step:1531/1680 train_time:137900ms step_avg:90.07ms
step:1532/1680 train_time:137990ms step_avg:90.07ms
step:1533/1680 train_time:138081ms step_avg:90.07ms
step:1534/1680 train_time:138171ms step_avg:90.07ms
step:1535/1680 train_time:138262ms step_avg:90.07ms
step:1536/1680 train_time:138353ms step_avg:90.07ms
step:1537/1680 train_time:138445ms step_avg:90.07ms
step:1538/1680 train_time:138535ms step_avg:90.08ms
step:1539/1680 train_time:138626ms step_avg:90.08ms
step:1540/1680 train_time:138717ms step_avg:90.08ms
step:1541/1680 train_time:138808ms step_avg:90.08ms
step:1542/1680 train_time:138898ms step_avg:90.08ms
step:1543/1680 train_time:138989ms step_avg:90.08ms
step:1544/1680 train_time:139079ms step_avg:90.08ms
step:1545/1680 train_time:139170ms step_avg:90.08ms
step:1546/1680 train_time:139261ms step_avg:90.08ms
step:1547/1680 train_time:139351ms step_avg:90.08ms
step:1548/1680 train_time:139443ms step_avg:90.08ms
step:1549/1680 train_time:139534ms step_avg:90.08ms
step:1550/1680 train_time:139625ms step_avg:90.08ms
step:1551/1680 train_time:139716ms step_avg:90.08ms
step:1552/1680 train_time:139807ms step_avg:90.08ms
step:1553/1680 train_time:139898ms step_avg:90.08ms
step:1554/1680 train_time:139988ms step_avg:90.08ms
step:1555/1680 train_time:140079ms step_avg:90.08ms
step:1556/1680 train_time:140170ms step_avg:90.08ms
step:1557/1680 train_time:140261ms step_avg:90.08ms
step:1558/1680 train_time:140352ms step_avg:90.08ms
step:1559/1680 train_time:140442ms step_avg:90.08ms
step:1560/1680 train_time:140533ms step_avg:90.09ms
step:1561/1680 train_time:140625ms step_avg:90.09ms
step:1562/1680 train_time:140715ms step_avg:90.09ms
step:1563/1680 train_time:140806ms step_avg:90.09ms
step:1564/1680 train_time:140896ms step_avg:90.09ms
step:1565/1680 train_time:140988ms step_avg:90.09ms
step:1566/1680 train_time:141080ms step_avg:90.09ms
step:1567/1680 train_time:141170ms step_avg:90.09ms
step:1568/1680 train_time:141262ms step_avg:90.09ms
step:1569/1680 train_time:141352ms step_avg:90.09ms
step:1570/1680 train_time:141443ms step_avg:90.09ms
step:1571/1680 train_time:141533ms step_avg:90.09ms
step:1572/1680 train_time:141624ms step_avg:90.09ms
step:1573/1680 train_time:141715ms step_avg:90.09ms
step:1574/1680 train_time:141806ms step_avg:90.09ms
step:1575/1680 train_time:141895ms step_avg:90.09ms
step:1576/1680 train_time:141987ms step_avg:90.09ms
step:1577/1680 train_time:142078ms step_avg:90.09ms
step:1578/1680 train_time:142170ms step_avg:90.09ms
step:1579/1680 train_time:142260ms step_avg:90.09ms
step:1580/1680 train_time:142350ms step_avg:90.10ms
step:1581/1680 train_time:142441ms step_avg:90.10ms
step:1582/1680 train_time:142531ms step_avg:90.10ms
step:1583/1680 train_time:142622ms step_avg:90.10ms
step:1584/1680 train_time:142713ms step_avg:90.10ms
step:1585/1680 train_time:142804ms step_avg:90.10ms
step:1586/1680 train_time:142894ms step_avg:90.10ms
step:1587/1680 train_time:142986ms step_avg:90.10ms
step:1588/1680 train_time:143077ms step_avg:90.10ms
step:1589/1680 train_time:143169ms step_avg:90.10ms
step:1590/1680 train_time:143260ms step_avg:90.10ms
step:1591/1680 train_time:143350ms step_avg:90.10ms
step:1592/1680 train_time:143441ms step_avg:90.10ms
step:1593/1680 train_time:143532ms step_avg:90.10ms
step:1594/1680 train_time:143623ms step_avg:90.10ms
step:1595/1680 train_time:143714ms step_avg:90.10ms
step:1596/1680 train_time:143804ms step_avg:90.10ms
step:1597/1680 train_time:143895ms step_avg:90.10ms
step:1598/1680 train_time:143987ms step_avg:90.10ms
step:1599/1680 train_time:144078ms step_avg:90.11ms
step:1600/1680 train_time:144169ms step_avg:90.11ms
step:1601/1680 train_time:144261ms step_avg:90.11ms
step:1602/1680 train_time:144351ms step_avg:90.11ms
step:1603/1680 train_time:144442ms step_avg:90.11ms
step:1604/1680 train_time:144532ms step_avg:90.11ms
step:1605/1680 train_time:144624ms step_avg:90.11ms
step:1606/1680 train_time:144714ms step_avg:90.11ms
step:1607/1680 train_time:144805ms step_avg:90.11ms
step:1608/1680 train_time:144895ms step_avg:90.11ms
step:1609/1680 train_time:144988ms step_avg:90.11ms
step:1610/1680 train_time:145081ms step_avg:90.11ms
step:1611/1680 train_time:145171ms step_avg:90.11ms
step:1612/1680 train_time:145261ms step_avg:90.11ms
step:1613/1680 train_time:145352ms step_avg:90.11ms
step:1614/1680 train_time:145442ms step_avg:90.11ms
step:1615/1680 train_time:145532ms step_avg:90.11ms
step:1616/1680 train_time:145623ms step_avg:90.11ms
step:1617/1680 train_time:145714ms step_avg:90.11ms
step:1618/1680 train_time:145805ms step_avg:90.11ms
step:1619/1680 train_time:145895ms step_avg:90.11ms
step:1620/1680 train_time:145987ms step_avg:90.12ms
step:1621/1680 train_time:146077ms step_avg:90.12ms
step:1622/1680 train_time:146169ms step_avg:90.12ms
step:1623/1680 train_time:146260ms step_avg:90.12ms
step:1624/1680 train_time:146351ms step_avg:90.12ms
step:1625/1680 train_time:146442ms step_avg:90.12ms
step:1625/1680 val_loss:3.2879 train_time:146533ms step_avg:90.17ms
step:1626/1680 train_time:146555ms step_avg:90.13ms
step:1627/1680 train_time:146628ms step_avg:90.12ms
step:1628/1680 train_time:146725ms step_avg:90.13ms
step:1629/1680 train_time:146817ms step_avg:90.13ms
step:1630/1680 train_time:146907ms step_avg:90.13ms
step:1631/1680 train_time:146996ms step_avg:90.13ms
step:1632/1680 train_time:147086ms step_avg:90.13ms
step:1633/1680 train_time:147175ms step_avg:90.13ms
step:1634/1680 train_time:147265ms step_avg:90.13ms
step:1635/1680 train_time:147354ms step_avg:90.12ms
step:1636/1680 train_time:147445ms step_avg:90.13ms
step:1637/1680 train_time:147536ms step_avg:90.13ms
step:1638/1680 train_time:147632ms step_avg:90.13ms
step:1639/1680 train_time:147727ms step_avg:90.13ms
step:1640/1680 train_time:147818ms step_avg:90.13ms
step:1641/1680 train_time:147908ms step_avg:90.13ms
step:1642/1680 train_time:147998ms step_avg:90.13ms
step:1643/1680 train_time:148088ms step_avg:90.13ms
step:1644/1680 train_time:148178ms step_avg:90.13ms
step:1645/1680 train_time:148268ms step_avg:90.13ms
step:1646/1680 train_time:148359ms step_avg:90.13ms
step:1647/1680 train_time:148449ms step_avg:90.13ms
step:1648/1680 train_time:148541ms step_avg:90.13ms
step:1649/1680 train_time:148633ms step_avg:90.14ms
step:1650/1680 train_time:148726ms step_avg:90.14ms
step:1651/1680 train_time:148817ms step_avg:90.14ms
step:1652/1680 train_time:148909ms step_avg:90.14ms
step:1653/1680 train_time:148999ms step_avg:90.14ms
step:1654/1680 train_time:149089ms step_avg:90.14ms
step:1655/1680 train_time:149180ms step_avg:90.14ms
step:1656/1680 train_time:149270ms step_avg:90.14ms
step:1657/1680 train_time:149360ms step_avg:90.14ms
step:1658/1680 train_time:149451ms step_avg:90.14ms
step:1659/1680 train_time:149542ms step_avg:90.14ms
step:1660/1680 train_time:149635ms step_avg:90.14ms
step:1661/1680 train_time:149727ms step_avg:90.14ms
step:1662/1680 train_time:149820ms step_avg:90.14ms
step:1663/1680 train_time:149911ms step_avg:90.15ms
step:1664/1680 train_time:150002ms step_avg:90.15ms
step:1665/1680 train_time:150092ms step_avg:90.15ms
step:1666/1680 train_time:150183ms step_avg:90.15ms
step:1667/1680 train_time:150272ms step_avg:90.15ms
step:1668/1680 train_time:150363ms step_avg:90.15ms
step:1669/1680 train_time:150453ms step_avg:90.15ms
step:1670/1680 train_time:150544ms step_avg:90.15ms
step:1671/1680 train_time:150635ms step_avg:90.15ms
step:1672/1680 train_time:150727ms step_avg:90.15ms
step:1673/1680 train_time:150819ms step_avg:90.15ms
step:1674/1680 train_time:150911ms step_avg:90.15ms
step:1675/1680 train_time:151002ms step_avg:90.15ms
step:1676/1680 train_time:151092ms step_avg:90.15ms
step:1677/1680 train_time:151183ms step_avg:90.15ms
step:1678/1680 train_time:151273ms step_avg:90.15ms
step:1679/1680 train_time:151364ms step_avg:90.15ms
step:1680/1680 train_time:151454ms step_avg:90.15ms
step:1680/1680 val_loss:3.2774 train_time:151546ms step_avg:90.21ms
peak memory allocated: 31255 MiB reserved: 46194 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 and layer_idx !=0 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        first_bm = 1 * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, final_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"v1/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer shows no degradation with context length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
smear_gate_params = [p for n, p in model.named_parameters() if "smear" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params+smear_gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate, args.ws_validate_final_layer
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250724+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sun Sep 21 23:28:47 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   44C    P0            127W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   39C    P0            125W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   40C    P0            126W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   39C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           87713      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A           87714      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           87715      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           87716      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           87717      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           87718      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           87719      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           87720      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           87714      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A           87715      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A           87716      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A           87717      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A           87718      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A           87719      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A           87720      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/1680 train_time:158ms step_avg:157.91ms
step:2/1680 train_time:181ms step_avg:90.57ms
step:3/1680 train_time:243ms step_avg:81.05ms
step:4/1680 train_time:330ms step_avg:82.50ms
step:5/1680 train_time:418ms step_avg:83.59ms
step:6/1680 train_time:506ms step_avg:84.30ms
step:7/1680 train_time:594ms step_avg:84.86ms
step:8/1680 train_time:682ms step_avg:85.28ms
step:9/1680 train_time:772ms step_avg:85.73ms
step:10/1680 train_time:860ms step_avg:85.98ms
step:11/1680 train_time:948ms step_avg:86.22ms
step:12/1680 train_time:1037ms step_avg:86.43ms
step:13/1680 train_time:1129ms step_avg:86.88ms
step:14/1680 train_time:1221ms step_avg:87.21ms
step:15/1680 train_time:1311ms step_avg:87.40ms
step:16/1680 train_time:1400ms step_avg:87.52ms
step:17/1680 train_time:1489ms step_avg:87.60ms
step:18/1680 train_time:1578ms step_avg:87.65ms
step:19/1680 train_time:1667ms step_avg:87.73ms
step:20/1680 train_time:1756ms step_avg:87.79ms
step:21/1680 train_time:1844ms step_avg:87.82ms
step:22/1680 train_time:1933ms step_avg:87.88ms
step:23/1680 train_time:2023ms step_avg:87.95ms
step:24/1680 train_time:2113ms step_avg:88.04ms
step:25/1680 train_time:2203ms step_avg:88.10ms
step:26/1680 train_time:2293ms step_avg:88.17ms
step:27/1680 train_time:2382ms step_avg:88.22ms
step:28/1680 train_time:2472ms step_avg:88.27ms
step:29/1680 train_time:2561ms step_avg:88.29ms
step:30/1680 train_time:2650ms step_avg:88.32ms
step:31/1680 train_time:2739ms step_avg:88.35ms
step:32/1680 train_time:2828ms step_avg:88.38ms
step:33/1680 train_time:2917ms step_avg:88.38ms
step:34/1680 train_time:3005ms step_avg:88.39ms
step:35/1680 train_time:3094ms step_avg:88.41ms
step:36/1680 train_time:3184ms step_avg:88.45ms
step:37/1680 train_time:3274ms step_avg:88.50ms
step:38/1680 train_time:3364ms step_avg:88.52ms
step:39/1680 train_time:3453ms step_avg:88.55ms
step:40/1680 train_time:3543ms step_avg:88.57ms
step:41/1680 train_time:3631ms step_avg:88.57ms
step:42/1680 train_time:3720ms step_avg:88.57ms
step:43/1680 train_time:3810ms step_avg:88.61ms
step:44/1680 train_time:3899ms step_avg:88.61ms
step:45/1680 train_time:3988ms step_avg:88.62ms
step:46/1680 train_time:4077ms step_avg:88.63ms
step:47/1680 train_time:4166ms step_avg:88.64ms
step:48/1680 train_time:4255ms step_avg:88.64ms
step:49/1680 train_time:4344ms step_avg:88.65ms
step:50/1680 train_time:4433ms step_avg:88.67ms
step:51/1680 train_time:4522ms step_avg:88.67ms
step:52/1680 train_time:4611ms step_avg:88.67ms
step:53/1680 train_time:4700ms step_avg:88.68ms
step:54/1680 train_time:4790ms step_avg:88.71ms
step:55/1680 train_time:4879ms step_avg:88.71ms
step:56/1680 train_time:4969ms step_avg:88.73ms
step:57/1680 train_time:5057ms step_avg:88.72ms
step:58/1680 train_time:5147ms step_avg:88.75ms
step:59/1680 train_time:5236ms step_avg:88.75ms
step:60/1680 train_time:5326ms step_avg:88.76ms
step:61/1680 train_time:5415ms step_avg:88.78ms
step:62/1680 train_time:5504ms step_avg:88.78ms
step:63/1680 train_time:5593ms step_avg:88.78ms
step:64/1680 train_time:5682ms step_avg:88.79ms
step:65/1680 train_time:5772ms step_avg:88.80ms
step:66/1680 train_time:5861ms step_avg:88.81ms
step:67/1680 train_time:5951ms step_avg:88.82ms
step:68/1680 train_time:6040ms step_avg:88.82ms
step:69/1680 train_time:6129ms step_avg:88.83ms
step:70/1680 train_time:6218ms step_avg:88.83ms
step:71/1680 train_time:6309ms step_avg:88.86ms
step:72/1680 train_time:6398ms step_avg:88.86ms
step:73/1680 train_time:6487ms step_avg:88.86ms
step:74/1680 train_time:6576ms step_avg:88.86ms
step:75/1680 train_time:6665ms step_avg:88.86ms
step:76/1680 train_time:6754ms step_avg:88.87ms
step:77/1680 train_time:6843ms step_avg:88.87ms
step:78/1680 train_time:6932ms step_avg:88.88ms
step:79/1680 train_time:7022ms step_avg:88.88ms
step:80/1680 train_time:7111ms step_avg:88.89ms
step:81/1680 train_time:7199ms step_avg:88.88ms
step:82/1680 train_time:7289ms step_avg:88.88ms
step:83/1680 train_time:7378ms step_avg:88.89ms
step:84/1680 train_time:7467ms step_avg:88.89ms
step:85/1680 train_time:7556ms step_avg:88.89ms
step:86/1680 train_time:7645ms step_avg:88.90ms
step:87/1680 train_time:7734ms step_avg:88.89ms
step:88/1680 train_time:7823ms step_avg:88.90ms
step:89/1680 train_time:7913ms step_avg:88.91ms
step:90/1680 train_time:8001ms step_avg:88.90ms
step:91/1680 train_time:8090ms step_avg:88.90ms
step:92/1680 train_time:8179ms step_avg:88.90ms
step:93/1680 train_time:8269ms step_avg:88.91ms
step:94/1680 train_time:8357ms step_avg:88.91ms
step:95/1680 train_time:8447ms step_avg:88.91ms
step:96/1680 train_time:8535ms step_avg:88.91ms
step:97/1680 train_time:8624ms step_avg:88.91ms
step:98/1680 train_time:8713ms step_avg:88.91ms
step:99/1680 train_time:8802ms step_avg:88.91ms
step:100/1680 train_time:8890ms step_avg:88.90ms
step:101/1680 train_time:8980ms step_avg:88.91ms
step:102/1680 train_time:9069ms step_avg:88.91ms
step:103/1680 train_time:9157ms step_avg:88.91ms
step:104/1680 train_time:9247ms step_avg:88.91ms
step:105/1680 train_time:9336ms step_avg:88.91ms
step:106/1680 train_time:9425ms step_avg:88.91ms
step:107/1680 train_time:9514ms step_avg:88.92ms
step:108/1680 train_time:9604ms step_avg:88.93ms
step:109/1680 train_time:9693ms step_avg:88.93ms
step:110/1680 train_time:9782ms step_avg:88.93ms
step:111/1680 train_time:9872ms step_avg:88.93ms
step:112/1680 train_time:9960ms step_avg:88.93ms
step:113/1680 train_time:10050ms step_avg:88.94ms
step:114/1680 train_time:10138ms step_avg:88.93ms
step:115/1680 train_time:10227ms step_avg:88.93ms
step:116/1680 train_time:10316ms step_avg:88.93ms
step:117/1680 train_time:10404ms step_avg:88.92ms
step:118/1680 train_time:10493ms step_avg:88.92ms
step:119/1680 train_time:10582ms step_avg:88.93ms
step:120/1680 train_time:10671ms step_avg:88.92ms
step:121/1680 train_time:10760ms step_avg:88.93ms
step:122/1680 train_time:10850ms step_avg:88.94ms
step:123/1680 train_time:10939ms step_avg:88.93ms
step:124/1680 train_time:11029ms step_avg:88.94ms
step:125/1680 train_time:11117ms step_avg:88.94ms
step:125/1680 val_loss:4.3124 train_time:11208ms step_avg:89.66ms
step:126/1680 train_time:11230ms step_avg:89.13ms
step:127/1680 train_time:11299ms step_avg:88.97ms
step:128/1680 train_time:11398ms step_avg:89.04ms
step:129/1680 train_time:11491ms step_avg:89.08ms
step:130/1680 train_time:11579ms step_avg:89.07ms
step:131/1680 train_time:11668ms step_avg:89.07ms
step:132/1680 train_time:11756ms step_avg:89.06ms
step:133/1680 train_time:11844ms step_avg:89.05ms
step:134/1680 train_time:11931ms step_avg:89.04ms
step:135/1680 train_time:12019ms step_avg:89.03ms
step:136/1680 train_time:12107ms step_avg:89.03ms
step:137/1680 train_time:12196ms step_avg:89.02ms
step:138/1680 train_time:12286ms step_avg:89.03ms
step:139/1680 train_time:12378ms step_avg:89.05ms
step:140/1680 train_time:12469ms step_avg:89.07ms
step:141/1680 train_time:12559ms step_avg:89.07ms
step:142/1680 train_time:12648ms step_avg:89.07ms
step:143/1680 train_time:12736ms step_avg:89.06ms
step:144/1680 train_time:12825ms step_avg:89.06ms
step:145/1680 train_time:12913ms step_avg:89.06ms
step:146/1680 train_time:13001ms step_avg:89.05ms
step:147/1680 train_time:13090ms step_avg:89.05ms
step:148/1680 train_time:13178ms step_avg:89.04ms
step:149/1680 train_time:13266ms step_avg:89.04ms
step:150/1680 train_time:13356ms step_avg:89.04ms
step:151/1680 train_time:13447ms step_avg:89.05ms
step:152/1680 train_time:13537ms step_avg:89.06ms
step:153/1680 train_time:13626ms step_avg:89.06ms
step:154/1680 train_time:13715ms step_avg:89.06ms
step:155/1680 train_time:13804ms step_avg:89.06ms
step:156/1680 train_time:13893ms step_avg:89.06ms
step:157/1680 train_time:13982ms step_avg:89.06ms
step:158/1680 train_time:14070ms step_avg:89.05ms
step:159/1680 train_time:14158ms step_avg:89.04ms
step:160/1680 train_time:14247ms step_avg:89.04ms
step:161/1680 train_time:14336ms step_avg:89.05ms
step:162/1680 train_time:14426ms step_avg:89.05ms
step:163/1680 train_time:14515ms step_avg:89.05ms
step:164/1680 train_time:14605ms step_avg:89.05ms
step:165/1680 train_time:14694ms step_avg:89.06ms
step:166/1680 train_time:14783ms step_avg:89.06ms
step:167/1680 train_time:14872ms step_avg:89.06ms
step:168/1680 train_time:14961ms step_avg:89.05ms
step:169/1680 train_time:15050ms step_avg:89.06ms
step:170/1680 train_time:15138ms step_avg:89.05ms
step:171/1680 train_time:15227ms step_avg:89.05ms
step:172/1680 train_time:15316ms step_avg:89.05ms
step:173/1680 train_time:15406ms step_avg:89.05ms
step:174/1680 train_time:15496ms step_avg:89.06ms
step:175/1680 train_time:15586ms step_avg:89.06ms
step:176/1680 train_time:15676ms step_avg:89.07ms
step:177/1680 train_time:15765ms step_avg:89.07ms
step:178/1680 train_time:15854ms step_avg:89.07ms
step:179/1680 train_time:15943ms step_avg:89.07ms
step:180/1680 train_time:16032ms step_avg:89.07ms
step:181/1680 train_time:16121ms step_avg:89.07ms
step:182/1680 train_time:16210ms step_avg:89.07ms
step:183/1680 train_time:16298ms step_avg:89.06ms
step:184/1680 train_time:16388ms step_avg:89.06ms
step:185/1680 train_time:16476ms step_avg:89.06ms
step:186/1680 train_time:16566ms step_avg:89.06ms
step:187/1680 train_time:16655ms step_avg:89.06ms
step:188/1680 train_time:16744ms step_avg:89.06ms
step:189/1680 train_time:16832ms step_avg:89.06ms
step:190/1680 train_time:16921ms step_avg:89.06ms
step:191/1680 train_time:17010ms step_avg:89.06ms
step:192/1680 train_time:17098ms step_avg:89.05ms
step:193/1680 train_time:17187ms step_avg:89.05ms
step:194/1680 train_time:17276ms step_avg:89.05ms
step:195/1680 train_time:17365ms step_avg:89.05ms
step:196/1680 train_time:17453ms step_avg:89.05ms
step:197/1680 train_time:17542ms step_avg:89.05ms
step:198/1680 train_time:17631ms step_avg:89.05ms
step:199/1680 train_time:17721ms step_avg:89.05ms
step:200/1680 train_time:17809ms step_avg:89.05ms
step:201/1680 train_time:17898ms step_avg:89.05ms
step:202/1680 train_time:17988ms step_avg:89.05ms
step:203/1680 train_time:18076ms step_avg:89.04ms
step:204/1680 train_time:18165ms step_avg:89.04ms
step:205/1680 train_time:18254ms step_avg:89.04ms
step:206/1680 train_time:18343ms step_avg:89.04ms
step:207/1680 train_time:18431ms step_avg:89.04ms
step:208/1680 train_time:18520ms step_avg:89.04ms
step:209/1680 train_time:18611ms step_avg:89.05ms
step:210/1680 train_time:18700ms step_avg:89.05ms
step:211/1680 train_time:18788ms step_avg:89.04ms
step:212/1680 train_time:18878ms step_avg:89.04ms
step:213/1680 train_time:18967ms step_avg:89.05ms
step:214/1680 train_time:19055ms step_avg:89.04ms
step:215/1680 train_time:19144ms step_avg:89.04ms
step:216/1680 train_time:19233ms step_avg:89.04ms
step:217/1680 train_time:19321ms step_avg:89.04ms
step:218/1680 train_time:19410ms step_avg:89.04ms
step:219/1680 train_time:19498ms step_avg:89.03ms
step:220/1680 train_time:19587ms step_avg:89.03ms
step:221/1680 train_time:19676ms step_avg:89.03ms
step:222/1680 train_time:19765ms step_avg:89.03ms
step:223/1680 train_time:19853ms step_avg:89.03ms
step:224/1680 train_time:19942ms step_avg:89.03ms
step:225/1680 train_time:20031ms step_avg:89.02ms
step:226/1680 train_time:20119ms step_avg:89.02ms
step:227/1680 train_time:20208ms step_avg:89.02ms
step:228/1680 train_time:20297ms step_avg:89.02ms
step:229/1680 train_time:20386ms step_avg:89.02ms
step:230/1680 train_time:20474ms step_avg:89.02ms
step:231/1680 train_time:20563ms step_avg:89.02ms
step:232/1680 train_time:20652ms step_avg:89.02ms
step:233/1680 train_time:20741ms step_avg:89.02ms
step:234/1680 train_time:20830ms step_avg:89.02ms
step:235/1680 train_time:20919ms step_avg:89.02ms
step:236/1680 train_time:21008ms step_avg:89.02ms
step:237/1680 train_time:21096ms step_avg:89.01ms
step:238/1680 train_time:21185ms step_avg:89.01ms
step:239/1680 train_time:21274ms step_avg:89.01ms
step:240/1680 train_time:21363ms step_avg:89.01ms
step:241/1680 train_time:21452ms step_avg:89.01ms
step:242/1680 train_time:21540ms step_avg:89.01ms
step:243/1680 train_time:21628ms step_avg:89.01ms
step:244/1680 train_time:21717ms step_avg:89.00ms
step:245/1680 train_time:21805ms step_avg:89.00ms
step:246/1680 train_time:21894ms step_avg:89.00ms
step:247/1680 train_time:21983ms step_avg:89.00ms
step:248/1680 train_time:22072ms step_avg:89.00ms
step:249/1680 train_time:22161ms step_avg:89.00ms
step:250/1680 train_time:22249ms step_avg:89.00ms
step:250/1680 val_loss:3.9699 train_time:22339ms step_avg:89.36ms
step:251/1680 train_time:22361ms step_avg:89.09ms
step:252/1680 train_time:22430ms step_avg:89.01ms
step:253/1680 train_time:22527ms step_avg:89.04ms
step:254/1680 train_time:22618ms step_avg:89.05ms
step:255/1680 train_time:22707ms step_avg:89.05ms
step:256/1680 train_time:22796ms step_avg:89.05ms
step:257/1680 train_time:22884ms step_avg:89.04ms
step:258/1680 train_time:22971ms step_avg:89.04ms
step:259/1680 train_time:23059ms step_avg:89.03ms
step:260/1680 train_time:23147ms step_avg:89.03ms
step:261/1680 train_time:23236ms step_avg:89.03ms
step:262/1680 train_time:23324ms step_avg:89.02ms
step:263/1680 train_time:23414ms step_avg:89.03ms
step:264/1680 train_time:23505ms step_avg:89.03ms
step:265/1680 train_time:23595ms step_avg:89.04ms
step:266/1680 train_time:23684ms step_avg:89.04ms
step:267/1680 train_time:23772ms step_avg:89.03ms
step:268/1680 train_time:23861ms step_avg:89.03ms
step:269/1680 train_time:23949ms step_avg:89.03ms
step:270/1680 train_time:24037ms step_avg:89.03ms
step:271/1680 train_time:24125ms step_avg:89.02ms
step:272/1680 train_time:24213ms step_avg:89.02ms
step:273/1680 train_time:24302ms step_avg:89.02ms
step:274/1680 train_time:24391ms step_avg:89.02ms
step:275/1680 train_time:24480ms step_avg:89.02ms
step:276/1680 train_time:24569ms step_avg:89.02ms
step:277/1680 train_time:24658ms step_avg:89.02ms
step:278/1680 train_time:24748ms step_avg:89.02ms
step:279/1680 train_time:24838ms step_avg:89.02ms
step:280/1680 train_time:24927ms step_avg:89.02ms
step:281/1680 train_time:25015ms step_avg:89.02ms
step:282/1680 train_time:25104ms step_avg:89.02ms
step:283/1680 train_time:25193ms step_avg:89.02ms
step:284/1680 train_time:25281ms step_avg:89.02ms
step:285/1680 train_time:25370ms step_avg:89.02ms
step:286/1680 train_time:25460ms step_avg:89.02ms
step:287/1680 train_time:25548ms step_avg:89.02ms
step:288/1680 train_time:25637ms step_avg:89.02ms
step:289/1680 train_time:25726ms step_avg:89.02ms
step:290/1680 train_time:25815ms step_avg:89.02ms
step:291/1680 train_time:25904ms step_avg:89.02ms
step:292/1680 train_time:25992ms step_avg:89.01ms
step:293/1680 train_time:26081ms step_avg:89.01ms
step:294/1680 train_time:26169ms step_avg:89.01ms
step:295/1680 train_time:26258ms step_avg:89.01ms
step:296/1680 train_time:26345ms step_avg:89.01ms
step:297/1680 train_time:26434ms step_avg:89.00ms
step:298/1680 train_time:26523ms step_avg:89.00ms
step:299/1680 train_time:26612ms step_avg:89.00ms
step:300/1680 train_time:26701ms step_avg:89.00ms
step:301/1680 train_time:26790ms step_avg:89.00ms
step:302/1680 train_time:26879ms step_avg:89.00ms
step:303/1680 train_time:26966ms step_avg:89.00ms
step:304/1680 train_time:27056ms step_avg:89.00ms
step:305/1680 train_time:27145ms step_avg:89.00ms
step:306/1680 train_time:27235ms step_avg:89.00ms
step:307/1680 train_time:27323ms step_avg:89.00ms
step:308/1680 train_time:27412ms step_avg:89.00ms
step:309/1680 train_time:27501ms step_avg:89.00ms
step:310/1680 train_time:27590ms step_avg:89.00ms
step:311/1680 train_time:27680ms step_avg:89.00ms
step:312/1680 train_time:27768ms step_avg:89.00ms
step:313/1680 train_time:27857ms step_avg:89.00ms
step:314/1680 train_time:27947ms step_avg:89.00ms
step:315/1680 train_time:28036ms step_avg:89.00ms
step:316/1680 train_time:28125ms step_avg:89.00ms
step:317/1680 train_time:28215ms step_avg:89.01ms
step:318/1680 train_time:28303ms step_avg:89.00ms
step:319/1680 train_time:28392ms step_avg:89.00ms
step:320/1680 train_time:28482ms step_avg:89.00ms
step:321/1680 train_time:28571ms step_avg:89.00ms
step:322/1680 train_time:28659ms step_avg:89.00ms
step:323/1680 train_time:28749ms step_avg:89.00ms
step:324/1680 train_time:28838ms step_avg:89.01ms
step:325/1680 train_time:28928ms step_avg:89.01ms
step:326/1680 train_time:29018ms step_avg:89.01ms
step:327/1680 train_time:29107ms step_avg:89.01ms
step:328/1680 train_time:29196ms step_avg:89.01ms
step:329/1680 train_time:29284ms step_avg:89.01ms
step:330/1680 train_time:29374ms step_avg:89.01ms
step:331/1680 train_time:29463ms step_avg:89.01ms
step:332/1680 train_time:29551ms step_avg:89.01ms
step:333/1680 train_time:29640ms step_avg:89.01ms
step:334/1680 train_time:29729ms step_avg:89.01ms
step:335/1680 train_time:29819ms step_avg:89.01ms
step:336/1680 train_time:29908ms step_avg:89.01ms
step:337/1680 train_time:29997ms step_avg:89.01ms
step:338/1680 train_time:30085ms step_avg:89.01ms
step:339/1680 train_time:30174ms step_avg:89.01ms
step:340/1680 train_time:30263ms step_avg:89.01ms
step:341/1680 train_time:30352ms step_avg:89.01ms
step:342/1680 train_time:30440ms step_avg:89.01ms
step:343/1680 train_time:30530ms step_avg:89.01ms
step:344/1680 train_time:30618ms step_avg:89.01ms
step:345/1680 train_time:30706ms step_avg:89.00ms
step:346/1680 train_time:30795ms step_avg:89.00ms
step:347/1680 train_time:30883ms step_avg:89.00ms
step:348/1680 train_time:30972ms step_avg:89.00ms
step:349/1680 train_time:31061ms step_avg:89.00ms
step:350/1680 train_time:31150ms step_avg:89.00ms
step:351/1680 train_time:31239ms step_avg:89.00ms
step:352/1680 train_time:31328ms step_avg:89.00ms
step:353/1680 train_time:31417ms step_avg:89.00ms
step:354/1680 train_time:31507ms step_avg:89.00ms
step:355/1680 train_time:31596ms step_avg:89.00ms
step:356/1680 train_time:31684ms step_avg:89.00ms
step:357/1680 train_time:31773ms step_avg:89.00ms
step:358/1680 train_time:31862ms step_avg:89.00ms
step:359/1680 train_time:31951ms step_avg:89.00ms
step:360/1680 train_time:32040ms step_avg:89.00ms
step:361/1680 train_time:32129ms step_avg:89.00ms
step:362/1680 train_time:32218ms step_avg:89.00ms
step:363/1680 train_time:32307ms step_avg:89.00ms
step:364/1680 train_time:32395ms step_avg:89.00ms
step:365/1680 train_time:32484ms step_avg:89.00ms
step:366/1680 train_time:32573ms step_avg:89.00ms
step:367/1680 train_time:32662ms step_avg:89.00ms
step:368/1680 train_time:32751ms step_avg:89.00ms
step:369/1680 train_time:32840ms step_avg:89.00ms
step:370/1680 train_time:32929ms step_avg:89.00ms
step:371/1680 train_time:33019ms step_avg:89.00ms
step:372/1680 train_time:33108ms step_avg:89.00ms
step:373/1680 train_time:33197ms step_avg:89.00ms
step:374/1680 train_time:33286ms step_avg:89.00ms
step:375/1680 train_time:33375ms step_avg:89.00ms
step:375/1680 val_loss:3.8123 train_time:33465ms step_avg:89.24ms
step:376/1680 train_time:33487ms step_avg:89.06ms
step:377/1680 train_time:33557ms step_avg:89.01ms
step:378/1680 train_time:33649ms step_avg:89.02ms
step:379/1680 train_time:33739ms step_avg:89.02ms
step:380/1680 train_time:33827ms step_avg:89.02ms
step:381/1680 train_time:33915ms step_avg:89.02ms
step:382/1680 train_time:34003ms step_avg:89.01ms
step:383/1680 train_time:34091ms step_avg:89.01ms
step:384/1680 train_time:34180ms step_avg:89.01ms
step:385/1680 train_time:34267ms step_avg:89.01ms
step:386/1680 train_time:34356ms step_avg:89.01ms
step:387/1680 train_time:34446ms step_avg:89.01ms
step:388/1680 train_time:34537ms step_avg:89.01ms
step:389/1680 train_time:34627ms step_avg:89.01ms
step:390/1680 train_time:34718ms step_avg:89.02ms
step:391/1680 train_time:34807ms step_avg:89.02ms
step:392/1680 train_time:34897ms step_avg:89.02ms
step:393/1680 train_time:34985ms step_avg:89.02ms
step:394/1680 train_time:35073ms step_avg:89.02ms
step:395/1680 train_time:35162ms step_avg:89.02ms
step:396/1680 train_time:35250ms step_avg:89.01ms
step:397/1680 train_time:35338ms step_avg:89.01ms
step:398/1680 train_time:35427ms step_avg:89.01ms
step:399/1680 train_time:35517ms step_avg:89.01ms
step:400/1680 train_time:35606ms step_avg:89.01ms
step:401/1680 train_time:35696ms step_avg:89.02ms
step:402/1680 train_time:35786ms step_avg:89.02ms
step:403/1680 train_time:35876ms step_avg:89.02ms
step:404/1680 train_time:35964ms step_avg:89.02ms
step:405/1680 train_time:36054ms step_avg:89.02ms
step:406/1680 train_time:36142ms step_avg:89.02ms
step:407/1680 train_time:36230ms step_avg:89.02ms
step:408/1680 train_time:36319ms step_avg:89.02ms
step:409/1680 train_time:36407ms step_avg:89.01ms
step:410/1680 train_time:36497ms step_avg:89.02ms
step:411/1680 train_time:36586ms step_avg:89.02ms
step:412/1680 train_time:36675ms step_avg:89.02ms
step:413/1680 train_time:36765ms step_avg:89.02ms
step:414/1680 train_time:36855ms step_avg:89.02ms
step:415/1680 train_time:36945ms step_avg:89.02ms
step:416/1680 train_time:37034ms step_avg:89.02ms
step:417/1680 train_time:37123ms step_avg:89.02ms
step:418/1680 train_time:37212ms step_avg:89.02ms
step:419/1680 train_time:37301ms step_avg:89.02ms
step:420/1680 train_time:37388ms step_avg:89.02ms
step:421/1680 train_time:37478ms step_avg:89.02ms
step:422/1680 train_time:37566ms step_avg:89.02ms
step:423/1680 train_time:37656ms step_avg:89.02ms
step:424/1680 train_time:37745ms step_avg:89.02ms
step:425/1680 train_time:37835ms step_avg:89.02ms
step:426/1680 train_time:37924ms step_avg:89.02ms
step:427/1680 train_time:38014ms step_avg:89.03ms
step:428/1680 train_time:38103ms step_avg:89.03ms
step:429/1680 train_time:38192ms step_avg:89.03ms
step:430/1680 train_time:38281ms step_avg:89.02ms
step:431/1680 train_time:38368ms step_avg:89.02ms
step:432/1680 train_time:38457ms step_avg:89.02ms
step:433/1680 train_time:38546ms step_avg:89.02ms
step:434/1680 train_time:38635ms step_avg:89.02ms
step:435/1680 train_time:38725ms step_avg:89.02ms
step:436/1680 train_time:38814ms step_avg:89.02ms
step:437/1680 train_time:38902ms step_avg:89.02ms
step:438/1680 train_time:38992ms step_avg:89.02ms
step:439/1680 train_time:39081ms step_avg:89.02ms
step:440/1680 train_time:39170ms step_avg:89.02ms
step:441/1680 train_time:39260ms step_avg:89.02ms
step:442/1680 train_time:39348ms step_avg:89.02ms
step:443/1680 train_time:39437ms step_avg:89.02ms
step:444/1680 train_time:39526ms step_avg:89.02ms
step:445/1680 train_time:39615ms step_avg:89.02ms
step:446/1680 train_time:39704ms step_avg:89.02ms
step:447/1680 train_time:39794ms step_avg:89.02ms
step:448/1680 train_time:39883ms step_avg:89.02ms
step:449/1680 train_time:39972ms step_avg:89.03ms
step:450/1680 train_time:40062ms step_avg:89.03ms
step:451/1680 train_time:40150ms step_avg:89.02ms
step:452/1680 train_time:40239ms step_avg:89.03ms
step:453/1680 train_time:40328ms step_avg:89.02ms
step:454/1680 train_time:40417ms step_avg:89.02ms
step:455/1680 train_time:40505ms step_avg:89.02ms
step:456/1680 train_time:40594ms step_avg:89.02ms
step:457/1680 train_time:40683ms step_avg:89.02ms
step:458/1680 train_time:40772ms step_avg:89.02ms
step:459/1680 train_time:40862ms step_avg:89.02ms
step:460/1680 train_time:40950ms step_avg:89.02ms
step:461/1680 train_time:41039ms step_avg:89.02ms
step:462/1680 train_time:41127ms step_avg:89.02ms
step:463/1680 train_time:41217ms step_avg:89.02ms
step:464/1680 train_time:41306ms step_avg:89.02ms
step:465/1680 train_time:41396ms step_avg:89.02ms
step:466/1680 train_time:41484ms step_avg:89.02ms
step:467/1680 train_time:41573ms step_avg:89.02ms
step:468/1680 train_time:41662ms step_avg:89.02ms
step:469/1680 train_time:41751ms step_avg:89.02ms
step:470/1680 train_time:41840ms step_avg:89.02ms
step:471/1680 train_time:41929ms step_avg:89.02ms
step:472/1680 train_time:42018ms step_avg:89.02ms
step:473/1680 train_time:42107ms step_avg:89.02ms
step:474/1680 train_time:42196ms step_avg:89.02ms
step:475/1680 train_time:42285ms step_avg:89.02ms
step:476/1680 train_time:42374ms step_avg:89.02ms
step:477/1680 train_time:42464ms step_avg:89.02ms
step:478/1680 train_time:42553ms step_avg:89.02ms
step:479/1680 train_time:42642ms step_avg:89.02ms
step:480/1680 train_time:42730ms step_avg:89.02ms
step:481/1680 train_time:42821ms step_avg:89.02ms
step:482/1680 train_time:42910ms step_avg:89.03ms
step:483/1680 train_time:42999ms step_avg:89.03ms
step:484/1680 train_time:43089ms step_avg:89.03ms
step:485/1680 train_time:43179ms step_avg:89.03ms
step:486/1680 train_time:43268ms step_avg:89.03ms
step:487/1680 train_time:43357ms step_avg:89.03ms
step:488/1680 train_time:43445ms step_avg:89.03ms
step:489/1680 train_time:43534ms step_avg:89.03ms
step:490/1680 train_time:43625ms step_avg:89.03ms
step:491/1680 train_time:43713ms step_avg:89.03ms
step:492/1680 train_time:43802ms step_avg:89.03ms
step:493/1680 train_time:43891ms step_avg:89.03ms
step:494/1680 train_time:43980ms step_avg:89.03ms
step:495/1680 train_time:44069ms step_avg:89.03ms
step:496/1680 train_time:44158ms step_avg:89.03ms
step:497/1680 train_time:44247ms step_avg:89.03ms
step:498/1680 train_time:44336ms step_avg:89.03ms
step:499/1680 train_time:44426ms step_avg:89.03ms
step:500/1680 train_time:44515ms step_avg:89.03ms
step:500/1680 val_loss:3.7144 train_time:44605ms step_avg:89.21ms
step:501/1680 train_time:44627ms step_avg:89.08ms
step:502/1680 train_time:44696ms step_avg:89.04ms
step:503/1680 train_time:44791ms step_avg:89.05ms
step:504/1680 train_time:44883ms step_avg:89.05ms
step:505/1680 train_time:44973ms step_avg:89.05ms
step:506/1680 train_time:45062ms step_avg:89.05ms
step:507/1680 train_time:45150ms step_avg:89.05ms
step:508/1680 train_time:45238ms step_avg:89.05ms
step:509/1680 train_time:45326ms step_avg:89.05ms
step:510/1680 train_time:45414ms step_avg:89.05ms
step:511/1680 train_time:45502ms step_avg:89.05ms
step:512/1680 train_time:45591ms step_avg:89.04ms
step:513/1680 train_time:45681ms step_avg:89.05ms
step:514/1680 train_time:45774ms step_avg:89.05ms
step:515/1680 train_time:45863ms step_avg:89.05ms
step:516/1680 train_time:45953ms step_avg:89.06ms
step:517/1680 train_time:46042ms step_avg:89.06ms
step:518/1680 train_time:46132ms step_avg:89.06ms
step:519/1680 train_time:46220ms step_avg:89.06ms
step:520/1680 train_time:46309ms step_avg:89.05ms
step:521/1680 train_time:46397ms step_avg:89.05ms
step:522/1680 train_time:46485ms step_avg:89.05ms
step:523/1680 train_time:46574ms step_avg:89.05ms
step:524/1680 train_time:46663ms step_avg:89.05ms
step:525/1680 train_time:46753ms step_avg:89.05ms
step:526/1680 train_time:46843ms step_avg:89.06ms
step:527/1680 train_time:46933ms step_avg:89.06ms
step:528/1680 train_time:47023ms step_avg:89.06ms
step:529/1680 train_time:47112ms step_avg:89.06ms
step:530/1680 train_time:47202ms step_avg:89.06ms
step:531/1680 train_time:47289ms step_avg:89.06ms
step:532/1680 train_time:47378ms step_avg:89.06ms
step:533/1680 train_time:47466ms step_avg:89.05ms
step:534/1680 train_time:47555ms step_avg:89.05ms
step:535/1680 train_time:47644ms step_avg:89.05ms
step:536/1680 train_time:47734ms step_avg:89.06ms
step:537/1680 train_time:47824ms step_avg:89.06ms
step:538/1680 train_time:47913ms step_avg:89.06ms
step:539/1680 train_time:48003ms step_avg:89.06ms
step:540/1680 train_time:48093ms step_avg:89.06ms
step:541/1680 train_time:48182ms step_avg:89.06ms
step:542/1680 train_time:48270ms step_avg:89.06ms
step:543/1680 train_time:48358ms step_avg:89.06ms
step:544/1680 train_time:48446ms step_avg:89.06ms
step:545/1680 train_time:48536ms step_avg:89.06ms
step:546/1680 train_time:48624ms step_avg:89.05ms
step:547/1680 train_time:48713ms step_avg:89.06ms
step:548/1680 train_time:48802ms step_avg:89.06ms
step:549/1680 train_time:48894ms step_avg:89.06ms
step:550/1680 train_time:48984ms step_avg:89.06ms
step:551/1680 train_time:49074ms step_avg:89.06ms
step:552/1680 train_time:49164ms step_avg:89.07ms
step:553/1680 train_time:49255ms step_avg:89.07ms
step:554/1680 train_time:49345ms step_avg:89.07ms
step:555/1680 train_time:49434ms step_avg:89.07ms
step:556/1680 train_time:49524ms step_avg:89.07ms
step:557/1680 train_time:49616ms step_avg:89.08ms
step:558/1680 train_time:49706ms step_avg:89.08ms
step:559/1680 train_time:49796ms step_avg:89.08ms
step:560/1680 train_time:49886ms step_avg:89.08ms
step:561/1680 train_time:49977ms step_avg:89.08ms
step:562/1680 train_time:50066ms step_avg:89.09ms
step:563/1680 train_time:50158ms step_avg:89.09ms
step:564/1680 train_time:50249ms step_avg:89.09ms
step:565/1680 train_time:50339ms step_avg:89.10ms
step:566/1680 train_time:50429ms step_avg:89.10ms
step:567/1680 train_time:50519ms step_avg:89.10ms
step:568/1680 train_time:50610ms step_avg:89.10ms
step:569/1680 train_time:50700ms step_avg:89.10ms
step:570/1680 train_time:50790ms step_avg:89.10ms
step:571/1680 train_time:50880ms step_avg:89.11ms
step:572/1680 train_time:50971ms step_avg:89.11ms
step:573/1680 train_time:51061ms step_avg:89.11ms
step:574/1680 train_time:51151ms step_avg:89.11ms
step:575/1680 train_time:51241ms step_avg:89.12ms
step:576/1680 train_time:51331ms step_avg:89.12ms
step:577/1680 train_time:51422ms step_avg:89.12ms
step:578/1680 train_time:51512ms step_avg:89.12ms
step:579/1680 train_time:51602ms step_avg:89.12ms
step:580/1680 train_time:51692ms step_avg:89.12ms
step:581/1680 train_time:51783ms step_avg:89.13ms
step:582/1680 train_time:51874ms step_avg:89.13ms
step:583/1680 train_time:51964ms step_avg:89.13ms
step:584/1680 train_time:52055ms step_avg:89.14ms
step:585/1680 train_time:52146ms step_avg:89.14ms
step:586/1680 train_time:52236ms step_avg:89.14ms
step:587/1680 train_time:52326ms step_avg:89.14ms
step:588/1680 train_time:52416ms step_avg:89.14ms
step:589/1680 train_time:52506ms step_avg:89.14ms
step:590/1680 train_time:52597ms step_avg:89.15ms
step:591/1680 train_time:52686ms step_avg:89.15ms
step:592/1680 train_time:52778ms step_avg:89.15ms
step:593/1680 train_time:52869ms step_avg:89.15ms
step:594/1680 train_time:52959ms step_avg:89.16ms
step:595/1680 train_time:53050ms step_avg:89.16ms
step:596/1680 train_time:53140ms step_avg:89.16ms
step:597/1680 train_time:53230ms step_avg:89.16ms
step:598/1680 train_time:53321ms step_avg:89.16ms
step:599/1680 train_time:53411ms step_avg:89.17ms
step:600/1680 train_time:53502ms step_avg:89.17ms
step:601/1680 train_time:53592ms step_avg:89.17ms
step:602/1680 train_time:53683ms step_avg:89.17ms
step:603/1680 train_time:53773ms step_avg:89.18ms
step:604/1680 train_time:53863ms step_avg:89.18ms
step:605/1680 train_time:53953ms step_avg:89.18ms
step:606/1680 train_time:54043ms step_avg:89.18ms
step:607/1680 train_time:54134ms step_avg:89.18ms
step:608/1680 train_time:54224ms step_avg:89.18ms
step:609/1680 train_time:54314ms step_avg:89.19ms
step:610/1680 train_time:54403ms step_avg:89.19ms
step:611/1680 train_time:54493ms step_avg:89.19ms
step:612/1680 train_time:54583ms step_avg:89.19ms
step:613/1680 train_time:54674ms step_avg:89.19ms
step:614/1680 train_time:54765ms step_avg:89.19ms
step:615/1680 train_time:54855ms step_avg:89.20ms
step:616/1680 train_time:54945ms step_avg:89.20ms
step:617/1680 train_time:55036ms step_avg:89.20ms
step:618/1680 train_time:55126ms step_avg:89.20ms
step:619/1680 train_time:55217ms step_avg:89.20ms
step:620/1680 train_time:55306ms step_avg:89.20ms
step:621/1680 train_time:55397ms step_avg:89.21ms
step:622/1680 train_time:55487ms step_avg:89.21ms
step:623/1680 train_time:55578ms step_avg:89.21ms
step:624/1680 train_time:55668ms step_avg:89.21ms
step:625/1680 train_time:55760ms step_avg:89.22ms
step:625/1680 val_loss:3.6145 train_time:55852ms step_avg:89.36ms
step:626/1680 train_time:55874ms step_avg:89.26ms
step:627/1680 train_time:55944ms step_avg:89.23ms
step:628/1680 train_time:56043ms step_avg:89.24ms
step:629/1680 train_time:56133ms step_avg:89.24ms
step:630/1680 train_time:56222ms step_avg:89.24ms
step:631/1680 train_time:56311ms step_avg:89.24ms
step:632/1680 train_time:56400ms step_avg:89.24ms
step:633/1680 train_time:56489ms step_avg:89.24ms
step:634/1680 train_time:56578ms step_avg:89.24ms
step:635/1680 train_time:56667ms step_avg:89.24ms
step:636/1680 train_time:56757ms step_avg:89.24ms
step:637/1680 train_time:56853ms step_avg:89.25ms
step:638/1680 train_time:56945ms step_avg:89.26ms
step:639/1680 train_time:57036ms step_avg:89.26ms
step:640/1680 train_time:57127ms step_avg:89.26ms
step:641/1680 train_time:57217ms step_avg:89.26ms
step:642/1680 train_time:57306ms step_avg:89.26ms
step:643/1680 train_time:57396ms step_avg:89.26ms
step:644/1680 train_time:57485ms step_avg:89.26ms
step:645/1680 train_time:57575ms step_avg:89.26ms
step:646/1680 train_time:57664ms step_avg:89.26ms
step:647/1680 train_time:57755ms step_avg:89.27ms
step:648/1680 train_time:57847ms step_avg:89.27ms
step:649/1680 train_time:57938ms step_avg:89.27ms
step:650/1680 train_time:58031ms step_avg:89.28ms
step:651/1680 train_time:58121ms step_avg:89.28ms
step:652/1680 train_time:58213ms step_avg:89.28ms
step:653/1680 train_time:58302ms step_avg:89.28ms
step:654/1680 train_time:58392ms step_avg:89.28ms
step:655/1680 train_time:58481ms step_avg:89.28ms
step:656/1680 train_time:58571ms step_avg:89.29ms
step:657/1680 train_time:58660ms step_avg:89.29ms
step:658/1680 train_time:58751ms step_avg:89.29ms
step:659/1680 train_time:58842ms step_avg:89.29ms
step:660/1680 train_time:58934ms step_avg:89.29ms
step:661/1680 train_time:59025ms step_avg:89.30ms
step:662/1680 train_time:59116ms step_avg:89.30ms
step:663/1680 train_time:59206ms step_avg:89.30ms
step:664/1680 train_time:59297ms step_avg:89.30ms
step:665/1680 train_time:59386ms step_avg:89.30ms
step:666/1680 train_time:59476ms step_avg:89.30ms
step:667/1680 train_time:59567ms step_avg:89.31ms
step:668/1680 train_time:59657ms step_avg:89.31ms
step:669/1680 train_time:59747ms step_avg:89.31ms
step:670/1680 train_time:59837ms step_avg:89.31ms
step:671/1680 train_time:59927ms step_avg:89.31ms
step:672/1680 train_time:60018ms step_avg:89.31ms
step:673/1680 train_time:60109ms step_avg:89.32ms
step:674/1680 train_time:60199ms step_avg:89.32ms
step:675/1680 train_time:60289ms step_avg:89.32ms
step:676/1680 train_time:60379ms step_avg:89.32ms
step:677/1680 train_time:60469ms step_avg:89.32ms
step:678/1680 train_time:60559ms step_avg:89.32ms
step:679/1680 train_time:60650ms step_avg:89.32ms
step:680/1680 train_time:60739ms step_avg:89.32ms
step:681/1680 train_time:60830ms step_avg:89.32ms
step:682/1680 train_time:60920ms step_avg:89.33ms
step:683/1680 train_time:61011ms step_avg:89.33ms
step:684/1680 train_time:61101ms step_avg:89.33ms
step:685/1680 train_time:61191ms step_avg:89.33ms
step:686/1680 train_time:61281ms step_avg:89.33ms
step:687/1680 train_time:61370ms step_avg:89.33ms
step:688/1680 train_time:61460ms step_avg:89.33ms
step:689/1680 train_time:61550ms step_avg:89.33ms
step:690/1680 train_time:61639ms step_avg:89.33ms
step:691/1680 train_time:61729ms step_avg:89.33ms
step:692/1680 train_time:61819ms step_avg:89.33ms
step:693/1680 train_time:61910ms step_avg:89.34ms
step:694/1680 train_time:62000ms step_avg:89.34ms
step:695/1680 train_time:62091ms step_avg:89.34ms
step:696/1680 train_time:62182ms step_avg:89.34ms
step:697/1680 train_time:62272ms step_avg:89.34ms
step:698/1680 train_time:62361ms step_avg:89.34ms
step:699/1680 train_time:62451ms step_avg:89.34ms
step:700/1680 train_time:62542ms step_avg:89.35ms
step:701/1680 train_time:62632ms step_avg:89.35ms
step:702/1680 train_time:62723ms step_avg:89.35ms
step:703/1680 train_time:62813ms step_avg:89.35ms
step:704/1680 train_time:62904ms step_avg:89.35ms
step:705/1680 train_time:62994ms step_avg:89.35ms
step:706/1680 train_time:63085ms step_avg:89.36ms
step:707/1680 train_time:63175ms step_avg:89.36ms
step:708/1680 train_time:63265ms step_avg:89.36ms
step:709/1680 train_time:63356ms step_avg:89.36ms
step:710/1680 train_time:63446ms step_avg:89.36ms
step:711/1680 train_time:63535ms step_avg:89.36ms
step:712/1680 train_time:63626ms step_avg:89.36ms
step:713/1680 train_time:63716ms step_avg:89.36ms
step:714/1680 train_time:63806ms step_avg:89.36ms
step:715/1680 train_time:63896ms step_avg:89.37ms
step:716/1680 train_time:63988ms step_avg:89.37ms
step:717/1680 train_time:64078ms step_avg:89.37ms
step:718/1680 train_time:64168ms step_avg:89.37ms
step:719/1680 train_time:64258ms step_avg:89.37ms
step:720/1680 train_time:64349ms step_avg:89.37ms
step:721/1680 train_time:64439ms step_avg:89.37ms
step:722/1680 train_time:64530ms step_avg:89.38ms
step:723/1680 train_time:64619ms step_avg:89.38ms
step:724/1680 train_time:64710ms step_avg:89.38ms
step:725/1680 train_time:64800ms step_avg:89.38ms
step:726/1680 train_time:64890ms step_avg:89.38ms
step:727/1680 train_time:64981ms step_avg:89.38ms
step:728/1680 train_time:65071ms step_avg:89.38ms
step:729/1680 train_time:65160ms step_avg:89.38ms
step:730/1680 train_time:65251ms step_avg:89.38ms
step:731/1680 train_time:65340ms step_avg:89.38ms
step:732/1680 train_time:65431ms step_avg:89.39ms
step:733/1680 train_time:65521ms step_avg:89.39ms
step:734/1680 train_time:65612ms step_avg:89.39ms
step:735/1680 train_time:65701ms step_avg:89.39ms
step:736/1680 train_time:65792ms step_avg:89.39ms
step:737/1680 train_time:65881ms step_avg:89.39ms
step:738/1680 train_time:65971ms step_avg:89.39ms
step:739/1680 train_time:66061ms step_avg:89.39ms
step:740/1680 train_time:66153ms step_avg:89.40ms
step:741/1680 train_time:66243ms step_avg:89.40ms
step:742/1680 train_time:66333ms step_avg:89.40ms
step:743/1680 train_time:66423ms step_avg:89.40ms
step:744/1680 train_time:66514ms step_avg:89.40ms
step:745/1680 train_time:66604ms step_avg:89.40ms
step:746/1680 train_time:66695ms step_avg:89.40ms
step:747/1680 train_time:66785ms step_avg:89.40ms
step:748/1680 train_time:66875ms step_avg:89.41ms
step:749/1680 train_time:66965ms step_avg:89.41ms
step:750/1680 train_time:67055ms step_avg:89.41ms
step:750/1680 val_loss:3.5620 train_time:67148ms step_avg:89.53ms
step:751/1680 train_time:67171ms step_avg:89.44ms
step:752/1680 train_time:67243ms step_avg:89.42ms
step:753/1680 train_time:67339ms step_avg:89.43ms
step:754/1680 train_time:67431ms step_avg:89.43ms
step:755/1680 train_time:67520ms step_avg:89.43ms
step:756/1680 train_time:67609ms step_avg:89.43ms
step:757/1680 train_time:67698ms step_avg:89.43ms
step:758/1680 train_time:67787ms step_avg:89.43ms
step:759/1680 train_time:67877ms step_avg:89.43ms
step:760/1680 train_time:67966ms step_avg:89.43ms
step:761/1680 train_time:68055ms step_avg:89.43ms
step:762/1680 train_time:68145ms step_avg:89.43ms
step:763/1680 train_time:68238ms step_avg:89.43ms
step:764/1680 train_time:68331ms step_avg:89.44ms
step:765/1680 train_time:68423ms step_avg:89.44ms
step:766/1680 train_time:68513ms step_avg:89.44ms
step:767/1680 train_time:68603ms step_avg:89.44ms
step:768/1680 train_time:68692ms step_avg:89.44ms
step:769/1680 train_time:68781ms step_avg:89.44ms
step:770/1680 train_time:68871ms step_avg:89.44ms
step:771/1680 train_time:68960ms step_avg:89.44ms
step:772/1680 train_time:69050ms step_avg:89.44ms
step:773/1680 train_time:69141ms step_avg:89.44ms
step:774/1680 train_time:69232ms step_avg:89.45ms
step:775/1680 train_time:69322ms step_avg:89.45ms
step:776/1680 train_time:69413ms step_avg:89.45ms
step:777/1680 train_time:69503ms step_avg:89.45ms
step:778/1680 train_time:69595ms step_avg:89.45ms
step:779/1680 train_time:69685ms step_avg:89.45ms
step:780/1680 train_time:69775ms step_avg:89.46ms
step:781/1680 train_time:69864ms step_avg:89.45ms
step:782/1680 train_time:69954ms step_avg:89.46ms
step:783/1680 train_time:70044ms step_avg:89.46ms
step:784/1680 train_time:70134ms step_avg:89.46ms
step:785/1680 train_time:70225ms step_avg:89.46ms
step:786/1680 train_time:70316ms step_avg:89.46ms
step:787/1680 train_time:70407ms step_avg:89.46ms
step:788/1680 train_time:70499ms step_avg:89.47ms
step:789/1680 train_time:70588ms step_avg:89.47ms
step:790/1680 train_time:70678ms step_avg:89.47ms
step:791/1680 train_time:70769ms step_avg:89.47ms
step:792/1680 train_time:70859ms step_avg:89.47ms
step:793/1680 train_time:70948ms step_avg:89.47ms
step:794/1680 train_time:71039ms step_avg:89.47ms
step:795/1680 train_time:71129ms step_avg:89.47ms
step:796/1680 train_time:71219ms step_avg:89.47ms
step:797/1680 train_time:71309ms step_avg:89.47ms
step:798/1680 train_time:71400ms step_avg:89.47ms
step:799/1680 train_time:71492ms step_avg:89.48ms
step:800/1680 train_time:71582ms step_avg:89.48ms
step:801/1680 train_time:71673ms step_avg:89.48ms
step:802/1680 train_time:71763ms step_avg:89.48ms
step:803/1680 train_time:71853ms step_avg:89.48ms
step:804/1680 train_time:71943ms step_avg:89.48ms
step:805/1680 train_time:72033ms step_avg:89.48ms
step:806/1680 train_time:72123ms step_avg:89.48ms
step:807/1680 train_time:72213ms step_avg:89.48ms
step:808/1680 train_time:72304ms step_avg:89.49ms
step:809/1680 train_time:72395ms step_avg:89.49ms
step:810/1680 train_time:72485ms step_avg:89.49ms
step:811/1680 train_time:72575ms step_avg:89.49ms
step:812/1680 train_time:72665ms step_avg:89.49ms
step:813/1680 train_time:72757ms step_avg:89.49ms
step:814/1680 train_time:72847ms step_avg:89.49ms
step:815/1680 train_time:72937ms step_avg:89.49ms
step:816/1680 train_time:73027ms step_avg:89.49ms
step:817/1680 train_time:73117ms step_avg:89.50ms
step:818/1680 train_time:73207ms step_avg:89.50ms
step:819/1680 train_time:73297ms step_avg:89.50ms
step:820/1680 train_time:73388ms step_avg:89.50ms
step:821/1680 train_time:73479ms step_avg:89.50ms
step:822/1680 train_time:73570ms step_avg:89.50ms
step:823/1680 train_time:73661ms step_avg:89.50ms
step:824/1680 train_time:73751ms step_avg:89.50ms
step:825/1680 train_time:73841ms step_avg:89.50ms
step:826/1680 train_time:73931ms step_avg:89.51ms
step:827/1680 train_time:74021ms step_avg:89.51ms
step:828/1680 train_time:74110ms step_avg:89.51ms
step:829/1680 train_time:74202ms step_avg:89.51ms
step:830/1680 train_time:74292ms step_avg:89.51ms
step:831/1680 train_time:74382ms step_avg:89.51ms
step:832/1680 train_time:74472ms step_avg:89.51ms
step:833/1680 train_time:74562ms step_avg:89.51ms
step:834/1680 train_time:74653ms step_avg:89.51ms
step:835/1680 train_time:74742ms step_avg:89.51ms
step:836/1680 train_time:74833ms step_avg:89.51ms
step:837/1680 train_time:74922ms step_avg:89.51ms
step:838/1680 train_time:75012ms step_avg:89.51ms
step:839/1680 train_time:75103ms step_avg:89.51ms
step:840/1680 train_time:75193ms step_avg:89.52ms
step:841/1680 train_time:75282ms step_avg:89.52ms
step:842/1680 train_time:75373ms step_avg:89.52ms
step:843/1680 train_time:75463ms step_avg:89.52ms
step:844/1680 train_time:75553ms step_avg:89.52ms
step:845/1680 train_time:75643ms step_avg:89.52ms
step:846/1680 train_time:75733ms step_avg:89.52ms
step:847/1680 train_time:75823ms step_avg:89.52ms
step:848/1680 train_time:75913ms step_avg:89.52ms
step:849/1680 train_time:76003ms step_avg:89.52ms
step:850/1680 train_time:76093ms step_avg:89.52ms
step:851/1680 train_time:76183ms step_avg:89.52ms
step:852/1680 train_time:76273ms step_avg:89.52ms
step:853/1680 train_time:76363ms step_avg:89.52ms
step:854/1680 train_time:76454ms step_avg:89.52ms
step:855/1680 train_time:76543ms step_avg:89.52ms
step:856/1680 train_time:76634ms step_avg:89.53ms
step:857/1680 train_time:76724ms step_avg:89.53ms
step:858/1680 train_time:76814ms step_avg:89.53ms
step:859/1680 train_time:76905ms step_avg:89.53ms
step:860/1680 train_time:76996ms step_avg:89.53ms
step:861/1680 train_time:77087ms step_avg:89.53ms
step:862/1680 train_time:77178ms step_avg:89.53ms
step:863/1680 train_time:77268ms step_avg:89.53ms
step:864/1680 train_time:77359ms step_avg:89.54ms
step:865/1680 train_time:77449ms step_avg:89.54ms
step:866/1680 train_time:77539ms step_avg:89.54ms
step:867/1680 train_time:77629ms step_avg:89.54ms
step:868/1680 train_time:77719ms step_avg:89.54ms
step:869/1680 train_time:77809ms step_avg:89.54ms
step:870/1680 train_time:77899ms step_avg:89.54ms
step:871/1680 train_time:77989ms step_avg:89.54ms
step:872/1680 train_time:78079ms step_avg:89.54ms
step:873/1680 train_time:78170ms step_avg:89.54ms
step:874/1680 train_time:78260ms step_avg:89.54ms
step:875/1680 train_time:78351ms step_avg:89.54ms
step:875/1680 val_loss:3.5163 train_time:78443ms step_avg:89.65ms
step:876/1680 train_time:78465ms step_avg:89.57ms
step:877/1680 train_time:78537ms step_avg:89.55ms
step:878/1680 train_time:78632ms step_avg:89.56ms
step:879/1680 train_time:78723ms step_avg:89.56ms
step:880/1680 train_time:78813ms step_avg:89.56ms
step:881/1680 train_time:78903ms step_avg:89.56ms
step:882/1680 train_time:78991ms step_avg:89.56ms
step:883/1680 train_time:79080ms step_avg:89.56ms
step:884/1680 train_time:79170ms step_avg:89.56ms
step:885/1680 train_time:79259ms step_avg:89.56ms
step:886/1680 train_time:79349ms step_avg:89.56ms
step:887/1680 train_time:79439ms step_avg:89.56ms
step:888/1680 train_time:79533ms step_avg:89.56ms
step:889/1680 train_time:79626ms step_avg:89.57ms
step:890/1680 train_time:79717ms step_avg:89.57ms
step:891/1680 train_time:79808ms step_avg:89.57ms
step:892/1680 train_time:79898ms step_avg:89.57ms
step:893/1680 train_time:79987ms step_avg:89.57ms
step:894/1680 train_time:80077ms step_avg:89.57ms
step:895/1680 train_time:80166ms step_avg:89.57ms
step:896/1680 train_time:80255ms step_avg:89.57ms
step:897/1680 train_time:80345ms step_avg:89.57ms
step:898/1680 train_time:80435ms step_avg:89.57ms
step:899/1680 train_time:80528ms step_avg:89.57ms
step:900/1680 train_time:80618ms step_avg:89.58ms
step:901/1680 train_time:80709ms step_avg:89.58ms
step:902/1680 train_time:80799ms step_avg:89.58ms
step:903/1680 train_time:80890ms step_avg:89.58ms
step:904/1680 train_time:80979ms step_avg:89.58ms
step:905/1680 train_time:81068ms step_avg:89.58ms
step:906/1680 train_time:81157ms step_avg:89.58ms
step:907/1680 train_time:81247ms step_avg:89.58ms
step:908/1680 train_time:81337ms step_avg:89.58ms
step:909/1680 train_time:81428ms step_avg:89.58ms
step:910/1680 train_time:81519ms step_avg:89.58ms
step:911/1680 train_time:81610ms step_avg:89.58ms
step:912/1680 train_time:81700ms step_avg:89.58ms
step:913/1680 train_time:81791ms step_avg:89.58ms
step:914/1680 train_time:81881ms step_avg:89.59ms
step:915/1680 train_time:81972ms step_avg:89.59ms
step:916/1680 train_time:82061ms step_avg:89.59ms
step:917/1680 train_time:82151ms step_avg:89.59ms
step:918/1680 train_time:82240ms step_avg:89.59ms
step:919/1680 train_time:82331ms step_avg:89.59ms
step:920/1680 train_time:82421ms step_avg:89.59ms
step:921/1680 train_time:82512ms step_avg:89.59ms
step:922/1680 train_time:82602ms step_avg:89.59ms
step:923/1680 train_time:82693ms step_avg:89.59ms
step:924/1680 train_time:82784ms step_avg:89.59ms
step:925/1680 train_time:82874ms step_avg:89.59ms
step:926/1680 train_time:82964ms step_avg:89.59ms
step:927/1680 train_time:83054ms step_avg:89.59ms
step:928/1680 train_time:83144ms step_avg:89.60ms
step:929/1680 train_time:83234ms step_avg:89.60ms
step:930/1680 train_time:83326ms step_avg:89.60ms
step:931/1680 train_time:83415ms step_avg:89.60ms
step:932/1680 train_time:83506ms step_avg:89.60ms
step:933/1680 train_time:83596ms step_avg:89.60ms
step:934/1680 train_time:83687ms step_avg:89.60ms
step:935/1680 train_time:83777ms step_avg:89.60ms
step:936/1680 train_time:83867ms step_avg:89.60ms
step:937/1680 train_time:83958ms step_avg:89.60ms
step:938/1680 train_time:84048ms step_avg:89.60ms
step:939/1680 train_time:84137ms step_avg:89.60ms
step:940/1680 train_time:84227ms step_avg:89.60ms
step:941/1680 train_time:84317ms step_avg:89.60ms
step:942/1680 train_time:84407ms step_avg:89.60ms
step:943/1680 train_time:84496ms step_avg:89.60ms
step:944/1680 train_time:84587ms step_avg:89.60ms
step:945/1680 train_time:84678ms step_avg:89.61ms
step:946/1680 train_time:84768ms step_avg:89.61ms
step:947/1680 train_time:84858ms step_avg:89.61ms
step:948/1680 train_time:84948ms step_avg:89.61ms
step:949/1680 train_time:85038ms step_avg:89.61ms
step:950/1680 train_time:85128ms step_avg:89.61ms
step:951/1680 train_time:85217ms step_avg:89.61ms
step:952/1680 train_time:85308ms step_avg:89.61ms
step:953/1680 train_time:85398ms step_avg:89.61ms
step:954/1680 train_time:85489ms step_avg:89.61ms
step:955/1680 train_time:85579ms step_avg:89.61ms
step:956/1680 train_time:85670ms step_avg:89.61ms
step:957/1680 train_time:85760ms step_avg:89.61ms
step:958/1680 train_time:85851ms step_avg:89.61ms
step:959/1680 train_time:85940ms step_avg:89.61ms
step:960/1680 train_time:86031ms step_avg:89.62ms
step:961/1680 train_time:86120ms step_avg:89.62ms
step:962/1680 train_time:86211ms step_avg:89.62ms
step:963/1680 train_time:86300ms step_avg:89.62ms
step:964/1680 train_time:86391ms step_avg:89.62ms
step:965/1680 train_time:86481ms step_avg:89.62ms
step:966/1680 train_time:86572ms step_avg:89.62ms
step:967/1680 train_time:86661ms step_avg:89.62ms
step:968/1680 train_time:86752ms step_avg:89.62ms
step:969/1680 train_time:86842ms step_avg:89.62ms
step:970/1680 train_time:86933ms step_avg:89.62ms
step:971/1680 train_time:87023ms step_avg:89.62ms
step:972/1680 train_time:87113ms step_avg:89.62ms
step:973/1680 train_time:87202ms step_avg:89.62ms
step:974/1680 train_time:87292ms step_avg:89.62ms
step:975/1680 train_time:87382ms step_avg:89.62ms
step:976/1680 train_time:87472ms step_avg:89.62ms
step:977/1680 train_time:87562ms step_avg:89.62ms
step:978/1680 train_time:87653ms step_avg:89.62ms
step:979/1680 train_time:87743ms step_avg:89.63ms
step:980/1680 train_time:87833ms step_avg:89.63ms
step:981/1680 train_time:87923ms step_avg:89.63ms
step:982/1680 train_time:88013ms step_avg:89.63ms
step:983/1680 train_time:88103ms step_avg:89.63ms
step:984/1680 train_time:88194ms step_avg:89.63ms
step:985/1680 train_time:88284ms step_avg:89.63ms
step:986/1680 train_time:88374ms step_avg:89.63ms
step:987/1680 train_time:88464ms step_avg:89.63ms
step:988/1680 train_time:88555ms step_avg:89.63ms
step:989/1680 train_time:88646ms step_avg:89.63ms
step:990/1680 train_time:88736ms step_avg:89.63ms
step:991/1680 train_time:88826ms step_avg:89.63ms
step:992/1680 train_time:88917ms step_avg:89.63ms
step:993/1680 train_time:89006ms step_avg:89.63ms
step:994/1680 train_time:89095ms step_avg:89.63ms
step:995/1680 train_time:89186ms step_avg:89.63ms
step:996/1680 train_time:89276ms step_avg:89.63ms
step:997/1680 train_time:89366ms step_avg:89.64ms
step:998/1680 train_time:89458ms step_avg:89.64ms
step:999/1680 train_time:89548ms step_avg:89.64ms
step:1000/1680 train_time:89638ms step_avg:89.64ms
step:1000/1680 val_loss:3.4684 train_time:89730ms step_avg:89.73ms
step:1001/1680 train_time:89752ms step_avg:89.66ms
step:1002/1680 train_time:89823ms step_avg:89.64ms
step:1003/1680 train_time:89918ms step_avg:89.65ms
step:1004/1680 train_time:90010ms step_avg:89.65ms
step:1005/1680 train_time:90100ms step_avg:89.65ms
step:1006/1680 train_time:90189ms step_avg:89.65ms
step:1007/1680 train_time:90278ms step_avg:89.65ms
step:1008/1680 train_time:90367ms step_avg:89.65ms
step:1009/1680 train_time:90457ms step_avg:89.65ms
step:1010/1680 train_time:90546ms step_avg:89.65ms
step:1011/1680 train_time:90636ms step_avg:89.65ms
step:1012/1680 train_time:90727ms step_avg:89.65ms
step:1013/1680 train_time:90820ms step_avg:89.65ms
step:1014/1680 train_time:90913ms step_avg:89.66ms
step:1015/1680 train_time:91003ms step_avg:89.66ms
step:1016/1680 train_time:91093ms step_avg:89.66ms
step:1017/1680 train_time:91183ms step_avg:89.66ms
step:1018/1680 train_time:91272ms step_avg:89.66ms
step:1019/1680 train_time:91361ms step_avg:89.66ms
step:1020/1680 train_time:91449ms step_avg:89.66ms
step:1021/1680 train_time:91539ms step_avg:89.66ms
step:1022/1680 train_time:91628ms step_avg:89.66ms
step:1023/1680 train_time:91721ms step_avg:89.66ms
step:1024/1680 train_time:91812ms step_avg:89.66ms
step:1025/1680 train_time:91903ms step_avg:89.66ms
step:1026/1680 train_time:91994ms step_avg:89.66ms
step:1027/1680 train_time:92085ms step_avg:89.66ms
step:1028/1680 train_time:92176ms step_avg:89.66ms
step:1029/1680 train_time:92266ms step_avg:89.67ms
step:1030/1680 train_time:92356ms step_avg:89.67ms
step:1031/1680 train_time:92446ms step_avg:89.67ms
step:1032/1680 train_time:92535ms step_avg:89.67ms
step:1033/1680 train_time:92626ms step_avg:89.67ms
step:1034/1680 train_time:92716ms step_avg:89.67ms
step:1035/1680 train_time:92807ms step_avg:89.67ms
step:1036/1680 train_time:92898ms step_avg:89.67ms
step:1037/1680 train_time:92989ms step_avg:89.67ms
step:1038/1680 train_time:93079ms step_avg:89.67ms
step:1039/1680 train_time:93169ms step_avg:89.67ms
step:1040/1680 train_time:93259ms step_avg:89.67ms
step:1041/1680 train_time:93349ms step_avg:89.67ms
step:1042/1680 train_time:93439ms step_avg:89.67ms
step:1043/1680 train_time:93529ms step_avg:89.67ms
step:1044/1680 train_time:93618ms step_avg:89.67ms
step:1045/1680 train_time:93708ms step_avg:89.67ms
step:1046/1680 train_time:93798ms step_avg:89.67ms
step:1047/1680 train_time:93889ms step_avg:89.67ms
step:1048/1680 train_time:93978ms step_avg:89.67ms
step:1049/1680 train_time:94069ms step_avg:89.68ms
step:1050/1680 train_time:94160ms step_avg:89.68ms
step:1051/1680 train_time:94251ms step_avg:89.68ms
step:1052/1680 train_time:94340ms step_avg:89.68ms
step:1053/1680 train_time:94430ms step_avg:89.68ms
step:1054/1680 train_time:94521ms step_avg:89.68ms
step:1055/1680 train_time:94610ms step_avg:89.68ms
step:1056/1680 train_time:94701ms step_avg:89.68ms
step:1057/1680 train_time:94792ms step_avg:89.68ms
step:1058/1680 train_time:94882ms step_avg:89.68ms
step:1059/1680 train_time:94973ms step_avg:89.68ms
step:1060/1680 train_time:95064ms step_avg:89.68ms
step:1061/1680 train_time:95155ms step_avg:89.68ms
step:1062/1680 train_time:95245ms step_avg:89.68ms
step:1063/1680 train_time:95335ms step_avg:89.68ms
step:1064/1680 train_time:95424ms step_avg:89.68ms
step:1065/1680 train_time:95514ms step_avg:89.68ms
step:1066/1680 train_time:95604ms step_avg:89.68ms
step:1067/1680 train_time:95693ms step_avg:89.68ms
step:1068/1680 train_time:95783ms step_avg:89.68ms
step:1069/1680 train_time:95874ms step_avg:89.69ms
step:1070/1680 train_time:95964ms step_avg:89.69ms
step:1071/1680 train_time:96055ms step_avg:89.69ms
step:1072/1680 train_time:96146ms step_avg:89.69ms
step:1073/1680 train_time:96236ms step_avg:89.69ms
step:1074/1680 train_time:96326ms step_avg:89.69ms
step:1075/1680 train_time:96416ms step_avg:89.69ms
step:1076/1680 train_time:96506ms step_avg:89.69ms
step:1077/1680 train_time:96597ms step_avg:89.69ms
step:1078/1680 train_time:96687ms step_avg:89.69ms
step:1079/1680 train_time:96777ms step_avg:89.69ms
step:1080/1680 train_time:96867ms step_avg:89.69ms
step:1081/1680 train_time:96957ms step_avg:89.69ms
step:1082/1680 train_time:97048ms step_avg:89.69ms
step:1083/1680 train_time:97138ms step_avg:89.69ms
step:1084/1680 train_time:97229ms step_avg:89.69ms
step:1085/1680 train_time:97318ms step_avg:89.69ms
step:1086/1680 train_time:97408ms step_avg:89.69ms
step:1087/1680 train_time:97498ms step_avg:89.69ms
step:1088/1680 train_time:97588ms step_avg:89.69ms
step:1089/1680 train_time:97678ms step_avg:89.70ms
step:1090/1680 train_time:97768ms step_avg:89.70ms
step:1091/1680 train_time:97858ms step_avg:89.70ms
step:1092/1680 train_time:97948ms step_avg:89.70ms
step:1093/1680 train_time:98039ms step_avg:89.70ms
step:1094/1680 train_time:98129ms step_avg:89.70ms
step:1095/1680 train_time:98220ms step_avg:89.70ms
step:1096/1680 train_time:98310ms step_avg:89.70ms
step:1097/1680 train_time:98401ms step_avg:89.70ms
step:1098/1680 train_time:98492ms step_avg:89.70ms
step:1099/1680 train_time:98583ms step_avg:89.70ms
step:1100/1680 train_time:98674ms step_avg:89.70ms
step:1101/1680 train_time:98766ms step_avg:89.71ms
step:1102/1680 train_time:98856ms step_avg:89.71ms
step:1103/1680 train_time:98946ms step_avg:89.71ms
step:1104/1680 train_time:99038ms step_avg:89.71ms
step:1105/1680 train_time:99129ms step_avg:89.71ms
step:1106/1680 train_time:99221ms step_avg:89.71ms
step:1107/1680 train_time:99312ms step_avg:89.71ms
step:1108/1680 train_time:99403ms step_avg:89.71ms
step:1109/1680 train_time:99494ms step_avg:89.71ms
step:1110/1680 train_time:99584ms step_avg:89.72ms
step:1111/1680 train_time:99676ms step_avg:89.72ms
step:1112/1680 train_time:99767ms step_avg:89.72ms
step:1113/1680 train_time:99857ms step_avg:89.72ms
step:1114/1680 train_time:99948ms step_avg:89.72ms
step:1115/1680 train_time:100039ms step_avg:89.72ms
step:1116/1680 train_time:100130ms step_avg:89.72ms
step:1117/1680 train_time:100222ms step_avg:89.72ms
step:1118/1680 train_time:100313ms step_avg:89.73ms
step:1119/1680 train_time:100404ms step_avg:89.73ms
step:1120/1680 train_time:100495ms step_avg:89.73ms
step:1121/1680 train_time:100585ms step_avg:89.73ms
step:1122/1680 train_time:100676ms step_avg:89.73ms
step:1123/1680 train_time:100767ms step_avg:89.73ms
step:1124/1680 train_time:100858ms step_avg:89.73ms
step:1125/1680 train_time:100948ms step_avg:89.73ms
step:1125/1680 val_loss:3.4148 train_time:101040ms step_avg:89.81ms
step:1126/1680 train_time:101062ms step_avg:89.75ms
step:1127/1680 train_time:101135ms step_avg:89.74ms
step:1128/1680 train_time:101235ms step_avg:89.75ms
step:1129/1680 train_time:101326ms step_avg:89.75ms
step:1130/1680 train_time:101416ms step_avg:89.75ms
step:1131/1680 train_time:101506ms step_avg:89.75ms
step:1132/1680 train_time:101596ms step_avg:89.75ms
step:1133/1680 train_time:101686ms step_avg:89.75ms
step:1134/1680 train_time:101775ms step_avg:89.75ms
step:1135/1680 train_time:101865ms step_avg:89.75ms
step:1136/1680 train_time:101954ms step_avg:89.75ms
step:1137/1680 train_time:102047ms step_avg:89.75ms
step:1138/1680 train_time:102142ms step_avg:89.76ms
step:1139/1680 train_time:102234ms step_avg:89.76ms
step:1140/1680 train_time:102326ms step_avg:89.76ms
step:1141/1680 train_time:102417ms step_avg:89.76ms
step:1142/1680 train_time:102508ms step_avg:89.76ms
step:1143/1680 train_time:102598ms step_avg:89.76ms
step:1144/1680 train_time:102688ms step_avg:89.76ms
step:1145/1680 train_time:102777ms step_avg:89.76ms
step:1146/1680 train_time:102867ms step_avg:89.76ms
step:1147/1680 train_time:102957ms step_avg:89.76ms
step:1148/1680 train_time:103049ms step_avg:89.76ms
step:1149/1680 train_time:103142ms step_avg:89.77ms
step:1150/1680 train_time:103233ms step_avg:89.77ms
step:1151/1680 train_time:103325ms step_avg:89.77ms
step:1152/1680 train_time:103416ms step_avg:89.77ms
step:1153/1680 train_time:103508ms step_avg:89.77ms
step:1154/1680 train_time:103598ms step_avg:89.77ms
step:1155/1680 train_time:103689ms step_avg:89.77ms
step:1156/1680 train_time:103778ms step_avg:89.77ms
step:1157/1680 train_time:103868ms step_avg:89.77ms
step:1158/1680 train_time:103959ms step_avg:89.78ms
step:1159/1680 train_time:104050ms step_avg:89.78ms
step:1160/1680 train_time:104142ms step_avg:89.78ms
step:1161/1680 train_time:104232ms step_avg:89.78ms
step:1162/1680 train_time:104325ms step_avg:89.78ms
step:1163/1680 train_time:104415ms step_avg:89.78ms
step:1164/1680 train_time:104507ms step_avg:89.78ms
step:1165/1680 train_time:104598ms step_avg:89.78ms
step:1166/1680 train_time:104689ms step_avg:89.78ms
step:1167/1680 train_time:104779ms step_avg:89.78ms
step:1168/1680 train_time:104869ms step_avg:89.79ms
step:1169/1680 train_time:104960ms step_avg:89.79ms
step:1170/1680 train_time:105050ms step_avg:89.79ms
step:1171/1680 train_time:105143ms step_avg:89.79ms
step:1172/1680 train_time:105234ms step_avg:89.79ms
step:1173/1680 train_time:105327ms step_avg:89.79ms
step:1174/1680 train_time:105417ms step_avg:89.79ms
step:1175/1680 train_time:105508ms step_avg:89.79ms
step:1176/1680 train_time:105599ms step_avg:89.79ms
step:1177/1680 train_time:105689ms step_avg:89.80ms
step:1178/1680 train_time:105779ms step_avg:89.80ms
step:1179/1680 train_time:105870ms step_avg:89.80ms
step:1180/1680 train_time:105960ms step_avg:89.80ms
step:1181/1680 train_time:106051ms step_avg:89.80ms
step:1182/1680 train_time:106143ms step_avg:89.80ms
step:1183/1680 train_time:106232ms step_avg:89.80ms
step:1184/1680 train_time:106324ms step_avg:89.80ms
step:1185/1680 train_time:106414ms step_avg:89.80ms
step:1186/1680 train_time:106505ms step_avg:89.80ms
step:1187/1680 train_time:106597ms step_avg:89.80ms
step:1188/1680 train_time:106687ms step_avg:89.80ms
step:1189/1680 train_time:106778ms step_avg:89.80ms
step:1190/1680 train_time:106869ms step_avg:89.81ms
step:1191/1680 train_time:106960ms step_avg:89.81ms
step:1192/1680 train_time:107051ms step_avg:89.81ms
step:1193/1680 train_time:107142ms step_avg:89.81ms
step:1194/1680 train_time:107233ms step_avg:89.81ms
step:1195/1680 train_time:107325ms step_avg:89.81ms
step:1196/1680 train_time:107416ms step_avg:89.81ms
step:1197/1680 train_time:107507ms step_avg:89.81ms
step:1198/1680 train_time:107598ms step_avg:89.81ms
step:1199/1680 train_time:107689ms step_avg:89.82ms
step:1200/1680 train_time:107779ms step_avg:89.82ms
step:1201/1680 train_time:107870ms step_avg:89.82ms
step:1202/1680 train_time:107962ms step_avg:89.82ms
step:1203/1680 train_time:108051ms step_avg:89.82ms
step:1204/1680 train_time:108142ms step_avg:89.82ms
step:1205/1680 train_time:108232ms step_avg:89.82ms
step:1206/1680 train_time:108323ms step_avg:89.82ms
step:1207/1680 train_time:108414ms step_avg:89.82ms
step:1208/1680 train_time:108504ms step_avg:89.82ms
step:1209/1680 train_time:108595ms step_avg:89.82ms
step:1210/1680 train_time:108686ms step_avg:89.82ms
step:1211/1680 train_time:108776ms step_avg:89.82ms
step:1212/1680 train_time:108868ms step_avg:89.83ms
step:1213/1680 train_time:108959ms step_avg:89.83ms
step:1214/1680 train_time:109049ms step_avg:89.83ms
step:1215/1680 train_time:109140ms step_avg:89.83ms
step:1216/1680 train_time:109231ms step_avg:89.83ms
step:1217/1680 train_time:109322ms step_avg:89.83ms
step:1218/1680 train_time:109413ms step_avg:89.83ms
step:1219/1680 train_time:109504ms step_avg:89.83ms
step:1220/1680 train_time:109595ms step_avg:89.83ms
step:1221/1680 train_time:109686ms step_avg:89.83ms
step:1222/1680 train_time:109776ms step_avg:89.83ms
step:1223/1680 train_time:109868ms step_avg:89.83ms
step:1224/1680 train_time:109958ms step_avg:89.84ms
step:1225/1680 train_time:110049ms step_avg:89.84ms
step:1226/1680 train_time:110140ms step_avg:89.84ms
step:1227/1680 train_time:110230ms step_avg:89.84ms
step:1228/1680 train_time:110320ms step_avg:89.84ms
step:1229/1680 train_time:110412ms step_avg:89.84ms
step:1230/1680 train_time:110503ms step_avg:89.84ms
step:1231/1680 train_time:110594ms step_avg:89.84ms
step:1232/1680 train_time:110685ms step_avg:89.84ms
step:1233/1680 train_time:110775ms step_avg:89.84ms
step:1234/1680 train_time:110867ms step_avg:89.84ms
step:1235/1680 train_time:110958ms step_avg:89.84ms
step:1236/1680 train_time:111049ms step_avg:89.85ms
step:1237/1680 train_time:111140ms step_avg:89.85ms
step:1238/1680 train_time:111230ms step_avg:89.85ms
step:1239/1680 train_time:111321ms step_avg:89.85ms
step:1240/1680 train_time:111412ms step_avg:89.85ms
step:1241/1680 train_time:111503ms step_avg:89.85ms
step:1242/1680 train_time:111593ms step_avg:89.85ms
step:1243/1680 train_time:111684ms step_avg:89.85ms
step:1244/1680 train_time:111775ms step_avg:89.85ms
step:1245/1680 train_time:111866ms step_avg:89.85ms
step:1246/1680 train_time:111958ms step_avg:89.85ms
step:1247/1680 train_time:112048ms step_avg:89.85ms
step:1248/1680 train_time:112139ms step_avg:89.86ms
step:1249/1680 train_time:112228ms step_avg:89.85ms
step:1250/1680 train_time:112319ms step_avg:89.86ms
step:1250/1680 val_loss:3.3764 train_time:112412ms step_avg:89.93ms
step:1251/1680 train_time:112434ms step_avg:89.88ms
step:1252/1680 train_time:112508ms step_avg:89.86ms
step:1253/1680 train_time:112605ms step_avg:89.87ms
step:1254/1680 train_time:112696ms step_avg:89.87ms
step:1255/1680 train_time:112787ms step_avg:89.87ms
step:1256/1680 train_time:112877ms step_avg:89.87ms
step:1257/1680 train_time:112967ms step_avg:89.87ms
step:1258/1680 train_time:113056ms step_avg:89.87ms
step:1259/1680 train_time:113146ms step_avg:89.87ms
step:1260/1680 train_time:113235ms step_avg:89.87ms
step:1261/1680 train_time:113325ms step_avg:89.87ms
step:1262/1680 train_time:113416ms step_avg:89.87ms
step:1263/1680 train_time:113510ms step_avg:89.87ms
step:1264/1680 train_time:113605ms step_avg:89.88ms
step:1265/1680 train_time:113696ms step_avg:89.88ms
step:1266/1680 train_time:113787ms step_avg:89.88ms
step:1267/1680 train_time:113877ms step_avg:89.88ms
step:1268/1680 train_time:113967ms step_avg:89.88ms
step:1269/1680 train_time:114057ms step_avg:89.88ms
step:1270/1680 train_time:114146ms step_avg:89.88ms
step:1271/1680 train_time:114236ms step_avg:89.88ms
step:1272/1680 train_time:114326ms step_avg:89.88ms
step:1273/1680 train_time:114417ms step_avg:89.88ms
step:1274/1680 train_time:114512ms step_avg:89.88ms
step:1275/1680 train_time:114606ms step_avg:89.89ms
step:1276/1680 train_time:114697ms step_avg:89.89ms
step:1277/1680 train_time:114789ms step_avg:89.89ms
step:1278/1680 train_time:114879ms step_avg:89.89ms
step:1279/1680 train_time:114970ms step_avg:89.89ms
step:1280/1680 train_time:115060ms step_avg:89.89ms
step:1281/1680 train_time:115149ms step_avg:89.89ms
step:1282/1680 train_time:115240ms step_avg:89.89ms
step:1283/1680 train_time:115330ms step_avg:89.89ms
step:1284/1680 train_time:115421ms step_avg:89.89ms
step:1285/1680 train_time:115514ms step_avg:89.89ms
step:1286/1680 train_time:115605ms step_avg:89.90ms
step:1287/1680 train_time:115696ms step_avg:89.90ms
step:1288/1680 train_time:115787ms step_avg:89.90ms
step:1289/1680 train_time:115877ms step_avg:89.90ms
step:1290/1680 train_time:115968ms step_avg:89.90ms
step:1291/1680 train_time:116059ms step_avg:89.90ms
step:1292/1680 train_time:116149ms step_avg:89.90ms
step:1293/1680 train_time:116239ms step_avg:89.90ms
step:1294/1680 train_time:116330ms step_avg:89.90ms
step:1295/1680 train_time:116420ms step_avg:89.90ms
step:1296/1680 train_time:116513ms step_avg:89.90ms
step:1297/1680 train_time:116605ms step_avg:89.90ms
step:1298/1680 train_time:116695ms step_avg:89.90ms
step:1299/1680 train_time:116786ms step_avg:89.90ms
step:1300/1680 train_time:116876ms step_avg:89.90ms
step:1301/1680 train_time:116967ms step_avg:89.91ms
step:1302/1680 train_time:117057ms step_avg:89.91ms
step:1303/1680 train_time:117148ms step_avg:89.91ms
step:1304/1680 train_time:117238ms step_avg:89.91ms
step:1305/1680 train_time:117329ms step_avg:89.91ms
step:1306/1680 train_time:117419ms step_avg:89.91ms
step:1307/1680 train_time:117511ms step_avg:89.91ms
step:1308/1680 train_time:117604ms step_avg:89.91ms
step:1309/1680 train_time:117694ms step_avg:89.91ms
step:1310/1680 train_time:117786ms step_avg:89.91ms
step:1311/1680 train_time:117877ms step_avg:89.91ms
step:1312/1680 train_time:117967ms step_avg:89.91ms
step:1313/1680 train_time:118058ms step_avg:89.91ms
step:1314/1680 train_time:118149ms step_avg:89.92ms
step:1315/1680 train_time:118239ms step_avg:89.92ms
step:1316/1680 train_time:118331ms step_avg:89.92ms
step:1317/1680 train_time:118421ms step_avg:89.92ms
step:1318/1680 train_time:118513ms step_avg:89.92ms
step:1319/1680 train_time:118605ms step_avg:89.92ms
step:1320/1680 train_time:118696ms step_avg:89.92ms
step:1321/1680 train_time:118787ms step_avg:89.92ms
step:1322/1680 train_time:118877ms step_avg:89.92ms
step:1323/1680 train_time:118970ms step_avg:89.92ms
step:1324/1680 train_time:119061ms step_avg:89.92ms
step:1325/1680 train_time:119152ms step_avg:89.93ms
step:1326/1680 train_time:119242ms step_avg:89.93ms
step:1327/1680 train_time:119332ms step_avg:89.93ms
step:1328/1680 train_time:119423ms step_avg:89.93ms
step:1329/1680 train_time:119514ms step_avg:89.93ms
step:1330/1680 train_time:119606ms step_avg:89.93ms
step:1331/1680 train_time:119697ms step_avg:89.93ms
step:1332/1680 train_time:119787ms step_avg:89.93ms
step:1333/1680 train_time:119877ms step_avg:89.93ms
step:1334/1680 train_time:119968ms step_avg:89.93ms
step:1335/1680 train_time:120058ms step_avg:89.93ms
step:1336/1680 train_time:120149ms step_avg:89.93ms
step:1337/1680 train_time:120239ms step_avg:89.93ms
step:1338/1680 train_time:120330ms step_avg:89.93ms
step:1339/1680 train_time:120421ms step_avg:89.93ms
step:1340/1680 train_time:120512ms step_avg:89.93ms
step:1341/1680 train_time:120603ms step_avg:89.93ms
step:1342/1680 train_time:120694ms step_avg:89.94ms
step:1343/1680 train_time:120784ms step_avg:89.94ms
step:1344/1680 train_time:120875ms step_avg:89.94ms
step:1345/1680 train_time:120966ms step_avg:89.94ms
step:1346/1680 train_time:121056ms step_avg:89.94ms
step:1347/1680 train_time:121148ms step_avg:89.94ms
step:1348/1680 train_time:121238ms step_avg:89.94ms
step:1349/1680 train_time:121330ms step_avg:89.94ms
step:1350/1680 train_time:121419ms step_avg:89.94ms
step:1351/1680 train_time:121512ms step_avg:89.94ms
step:1352/1680 train_time:121603ms step_avg:89.94ms
step:1353/1680 train_time:121694ms step_avg:89.94ms
step:1354/1680 train_time:121784ms step_avg:89.94ms
step:1355/1680 train_time:121875ms step_avg:89.94ms
step:1356/1680 train_time:121966ms step_avg:89.95ms
step:1357/1680 train_time:122056ms step_avg:89.95ms
step:1358/1680 train_time:122147ms step_avg:89.95ms
step:1359/1680 train_time:122237ms step_avg:89.95ms
step:1360/1680 train_time:122328ms step_avg:89.95ms
step:1361/1680 train_time:122419ms step_avg:89.95ms
step:1362/1680 train_time:122510ms step_avg:89.95ms
step:1363/1680 train_time:122602ms step_avg:89.95ms
step:1364/1680 train_time:122693ms step_avg:89.95ms
step:1365/1680 train_time:122785ms step_avg:89.95ms
step:1366/1680 train_time:122875ms step_avg:89.95ms
step:1367/1680 train_time:122966ms step_avg:89.95ms
step:1368/1680 train_time:123058ms step_avg:89.95ms
step:1369/1680 train_time:123148ms step_avg:89.95ms
step:1370/1680 train_time:123239ms step_avg:89.96ms
step:1371/1680 train_time:123330ms step_avg:89.96ms
step:1372/1680 train_time:123420ms step_avg:89.96ms
step:1373/1680 train_time:123512ms step_avg:89.96ms
step:1374/1680 train_time:123603ms step_avg:89.96ms
step:1375/1680 train_time:123694ms step_avg:89.96ms
step:1375/1680 val_loss:3.3412 train_time:123787ms step_avg:90.03ms
step:1376/1680 train_time:123809ms step_avg:89.98ms
step:1377/1680 train_time:123882ms step_avg:89.97ms
step:1378/1680 train_time:123978ms step_avg:89.97ms
step:1379/1680 train_time:124069ms step_avg:89.97ms
step:1380/1680 train_time:124159ms step_avg:89.97ms
step:1381/1680 train_time:124250ms step_avg:89.97ms
step:1382/1680 train_time:124339ms step_avg:89.97ms
step:1383/1680 train_time:124429ms step_avg:89.97ms
step:1384/1680 train_time:124519ms step_avg:89.97ms
step:1385/1680 train_time:124608ms step_avg:89.97ms
step:1386/1680 train_time:124699ms step_avg:89.97ms
step:1387/1680 train_time:124792ms step_avg:89.97ms
step:1388/1680 train_time:124885ms step_avg:89.98ms
step:1389/1680 train_time:124978ms step_avg:89.98ms
step:1390/1680 train_time:125070ms step_avg:89.98ms
step:1391/1680 train_time:125160ms step_avg:89.98ms
step:1392/1680 train_time:125252ms step_avg:89.98ms
step:1393/1680 train_time:125343ms step_avg:89.98ms
step:1394/1680 train_time:125432ms step_avg:89.98ms
step:1395/1680 train_time:125522ms step_avg:89.98ms
step:1396/1680 train_time:125613ms step_avg:89.98ms
step:1397/1680 train_time:125703ms step_avg:89.98ms
step:1398/1680 train_time:125795ms step_avg:89.98ms
step:1399/1680 train_time:125887ms step_avg:89.98ms
step:1400/1680 train_time:125979ms step_avg:89.99ms
step:1401/1680 train_time:126071ms step_avg:89.99ms
step:1402/1680 train_time:126160ms step_avg:89.99ms
step:1403/1680 train_time:126251ms step_avg:89.99ms
step:1404/1680 train_time:126342ms step_avg:89.99ms
step:1405/1680 train_time:126433ms step_avg:89.99ms
step:1406/1680 train_time:126522ms step_avg:89.99ms
step:1407/1680 train_time:126612ms step_avg:89.99ms
step:1408/1680 train_time:126703ms step_avg:89.99ms
step:1409/1680 train_time:126794ms step_avg:89.99ms
step:1410/1680 train_time:126886ms step_avg:89.99ms
step:1411/1680 train_time:126978ms step_avg:89.99ms
step:1412/1680 train_time:127069ms step_avg:89.99ms
step:1413/1680 train_time:127160ms step_avg:89.99ms
step:1414/1680 train_time:127251ms step_avg:89.99ms
step:1415/1680 train_time:127342ms step_avg:89.99ms
step:1416/1680 train_time:127433ms step_avg:89.99ms
step:1417/1680 train_time:127524ms step_avg:90.00ms
step:1418/1680 train_time:127614ms step_avg:90.00ms
step:1419/1680 train_time:127704ms step_avg:90.00ms
step:1420/1680 train_time:127795ms step_avg:90.00ms
step:1421/1680 train_time:127887ms step_avg:90.00ms
step:1422/1680 train_time:127978ms step_avg:90.00ms
step:1423/1680 train_time:128069ms step_avg:90.00ms
step:1424/1680 train_time:128160ms step_avg:90.00ms
step:1425/1680 train_time:128251ms step_avg:90.00ms
step:1426/1680 train_time:128341ms step_avg:90.00ms
step:1427/1680 train_time:128432ms step_avg:90.00ms
step:1428/1680 train_time:128524ms step_avg:90.00ms
step:1429/1680 train_time:128614ms step_avg:90.00ms
step:1430/1680 train_time:128705ms step_avg:90.00ms
step:1431/1680 train_time:128796ms step_avg:90.00ms
step:1432/1680 train_time:128886ms step_avg:90.00ms
step:1433/1680 train_time:128978ms step_avg:90.01ms
step:1434/1680 train_time:129069ms step_avg:90.01ms
step:1435/1680 train_time:129160ms step_avg:90.01ms
step:1436/1680 train_time:129252ms step_avg:90.01ms
step:1437/1680 train_time:129342ms step_avg:90.01ms
step:1438/1680 train_time:129433ms step_avg:90.01ms
step:1439/1680 train_time:129523ms step_avg:90.01ms
step:1440/1680 train_time:129614ms step_avg:90.01ms
step:1441/1680 train_time:129704ms step_avg:90.01ms
step:1442/1680 train_time:129796ms step_avg:90.01ms
step:1443/1680 train_time:129887ms step_avg:90.01ms
step:1444/1680 train_time:129978ms step_avg:90.01ms
step:1445/1680 train_time:130069ms step_avg:90.01ms
step:1446/1680 train_time:130161ms step_avg:90.01ms
step:1447/1680 train_time:130252ms step_avg:90.01ms
step:1448/1680 train_time:130343ms step_avg:90.02ms
step:1449/1680 train_time:130433ms step_avg:90.02ms
step:1450/1680 train_time:130524ms step_avg:90.02ms
step:1451/1680 train_time:130614ms step_avg:90.02ms
step:1452/1680 train_time:130705ms step_avg:90.02ms
step:1453/1680 train_time:130796ms step_avg:90.02ms
step:1454/1680 train_time:130887ms step_avg:90.02ms
step:1455/1680 train_time:130978ms step_avg:90.02ms
step:1456/1680 train_time:131070ms step_avg:90.02ms
step:1457/1680 train_time:131161ms step_avg:90.02ms
step:1458/1680 train_time:131252ms step_avg:90.02ms
step:1459/1680 train_time:131342ms step_avg:90.02ms
step:1460/1680 train_time:131433ms step_avg:90.02ms
step:1461/1680 train_time:131524ms step_avg:90.02ms
step:1462/1680 train_time:131614ms step_avg:90.02ms
step:1463/1680 train_time:131705ms step_avg:90.02ms
step:1464/1680 train_time:131795ms step_avg:90.02ms
step:1465/1680 train_time:131887ms step_avg:90.03ms
step:1466/1680 train_time:131977ms step_avg:90.03ms
step:1467/1680 train_time:132068ms step_avg:90.03ms
step:1468/1680 train_time:132159ms step_avg:90.03ms
step:1469/1680 train_time:132250ms step_avg:90.03ms
step:1470/1680 train_time:132342ms step_avg:90.03ms
step:1471/1680 train_time:132433ms step_avg:90.03ms
step:1472/1680 train_time:132525ms step_avg:90.03ms
step:1473/1680 train_time:132616ms step_avg:90.03ms
step:1474/1680 train_time:132706ms step_avg:90.03ms
step:1475/1680 train_time:132797ms step_avg:90.03ms
step:1476/1680 train_time:132888ms step_avg:90.03ms
step:1477/1680 train_time:132979ms step_avg:90.03ms
step:1478/1680 train_time:133070ms step_avg:90.03ms
step:1479/1680 train_time:133161ms step_avg:90.03ms
step:1480/1680 train_time:133252ms step_avg:90.04ms
step:1481/1680 train_time:133343ms step_avg:90.04ms
step:1482/1680 train_time:133434ms step_avg:90.04ms
step:1483/1680 train_time:133525ms step_avg:90.04ms
step:1484/1680 train_time:133616ms step_avg:90.04ms
step:1485/1680 train_time:133707ms step_avg:90.04ms
step:1486/1680 train_time:133798ms step_avg:90.04ms
step:1487/1680 train_time:133889ms step_avg:90.04ms
step:1488/1680 train_time:133981ms step_avg:90.04ms
step:1489/1680 train_time:134072ms step_avg:90.04ms
step:1490/1680 train_time:134162ms step_avg:90.04ms
step:1491/1680 train_time:134253ms step_avg:90.04ms
step:1492/1680 train_time:134344ms step_avg:90.04ms
step:1493/1680 train_time:134435ms step_avg:90.04ms
step:1494/1680 train_time:134526ms step_avg:90.04ms
step:1495/1680 train_time:134616ms step_avg:90.04ms
step:1496/1680 train_time:134707ms step_avg:90.04ms
step:1497/1680 train_time:134797ms step_avg:90.04ms
step:1498/1680 train_time:134888ms step_avg:90.05ms
step:1499/1680 train_time:134979ms step_avg:90.05ms
step:1500/1680 train_time:135070ms step_avg:90.05ms
step:1500/1680 val_loss:3.3117 train_time:135161ms step_avg:90.11ms
step:1501/1680 train_time:135183ms step_avg:90.06ms
step:1502/1680 train_time:135254ms step_avg:90.05ms
step:1503/1680 train_time:135352ms step_avg:90.05ms
step:1504/1680 train_time:135444ms step_avg:90.06ms
step:1505/1680 train_time:135533ms step_avg:90.06ms
step:1506/1680 train_time:135623ms step_avg:90.05ms
step:1507/1680 train_time:135712ms step_avg:90.05ms
step:1508/1680 train_time:135803ms step_avg:90.05ms
step:1509/1680 train_time:135892ms step_avg:90.05ms
step:1510/1680 train_time:135983ms step_avg:90.06ms
step:1511/1680 train_time:136073ms step_avg:90.05ms
step:1512/1680 train_time:136165ms step_avg:90.06ms
step:1513/1680 train_time:136259ms step_avg:90.06ms
step:1514/1680 train_time:136352ms step_avg:90.06ms
step:1515/1680 train_time:136444ms step_avg:90.06ms
step:1516/1680 train_time:136534ms step_avg:90.06ms
step:1517/1680 train_time:136624ms step_avg:90.06ms
step:1518/1680 train_time:136714ms step_avg:90.06ms
step:1519/1680 train_time:136805ms step_avg:90.06ms
step:1520/1680 train_time:136896ms step_avg:90.06ms
step:1521/1680 train_time:136986ms step_avg:90.06ms
step:1522/1680 train_time:137077ms step_avg:90.06ms
step:1523/1680 train_time:137169ms step_avg:90.06ms
step:1524/1680 train_time:137261ms step_avg:90.07ms
step:1525/1680 train_time:137352ms step_avg:90.07ms
step:1526/1680 train_time:137444ms step_avg:90.07ms
step:1527/1680 train_time:137535ms step_avg:90.07ms
step:1528/1680 train_time:137625ms step_avg:90.07ms
step:1529/1680 train_time:137715ms step_avg:90.07ms
step:1530/1680 train_time:137806ms step_avg:90.07ms
step:1531/1680 train_time:137897ms step_avg:90.07ms
step:1532/1680 train_time:137988ms step_avg:90.07ms
step:1533/1680 train_time:138079ms step_avg:90.07ms
step:1534/1680 train_time:138170ms step_avg:90.07ms
step:1535/1680 train_time:138262ms step_avg:90.07ms
step:1536/1680 train_time:138353ms step_avg:90.07ms
step:1537/1680 train_time:138445ms step_avg:90.08ms
step:1538/1680 train_time:138536ms step_avg:90.08ms
step:1539/1680 train_time:138627ms step_avg:90.08ms
step:1540/1680 train_time:138716ms step_avg:90.08ms
step:1541/1680 train_time:138807ms step_avg:90.08ms
step:1542/1680 train_time:138897ms step_avg:90.08ms
step:1543/1680 train_time:138989ms step_avg:90.08ms
step:1544/1680 train_time:139080ms step_avg:90.08ms
step:1545/1680 train_time:139171ms step_avg:90.08ms
step:1546/1680 train_time:139262ms step_avg:90.08ms
step:1547/1680 train_time:139352ms step_avg:90.08ms
step:1548/1680 train_time:139444ms step_avg:90.08ms
step:1549/1680 train_time:139535ms step_avg:90.08ms
step:1550/1680 train_time:139625ms step_avg:90.08ms
step:1551/1680 train_time:139715ms step_avg:90.08ms
step:1552/1680 train_time:139806ms step_avg:90.08ms
step:1553/1680 train_time:139896ms step_avg:90.08ms
step:1554/1680 train_time:139988ms step_avg:90.08ms
step:1555/1680 train_time:140080ms step_avg:90.08ms
step:1556/1680 train_time:140170ms step_avg:90.08ms
step:1557/1680 train_time:140261ms step_avg:90.08ms
step:1558/1680 train_time:140352ms step_avg:90.08ms
step:1559/1680 train_time:140444ms step_avg:90.09ms
step:1560/1680 train_time:140534ms step_avg:90.09ms
step:1561/1680 train_time:140626ms step_avg:90.09ms
step:1562/1680 train_time:140716ms step_avg:90.09ms
step:1563/1680 train_time:140808ms step_avg:90.09ms
step:1564/1680 train_time:140898ms step_avg:90.09ms
step:1565/1680 train_time:140989ms step_avg:90.09ms
step:1566/1680 train_time:141079ms step_avg:90.09ms
step:1567/1680 train_time:141170ms step_avg:90.09ms
step:1568/1680 train_time:141261ms step_avg:90.09ms
step:1569/1680 train_time:141352ms step_avg:90.09ms
step:1570/1680 train_time:141444ms step_avg:90.09ms
step:1571/1680 train_time:141534ms step_avg:90.09ms
step:1572/1680 train_time:141625ms step_avg:90.09ms
step:1573/1680 train_time:141715ms step_avg:90.09ms
step:1574/1680 train_time:141807ms step_avg:90.09ms
step:1575/1680 train_time:141899ms step_avg:90.09ms
step:1576/1680 train_time:141989ms step_avg:90.09ms
step:1577/1680 train_time:142080ms step_avg:90.10ms
step:1578/1680 train_time:142171ms step_avg:90.10ms
step:1579/1680 train_time:142262ms step_avg:90.10ms
step:1580/1680 train_time:142353ms step_avg:90.10ms
step:1581/1680 train_time:142444ms step_avg:90.10ms
step:1582/1680 train_time:142535ms step_avg:90.10ms
step:1583/1680 train_time:142626ms step_avg:90.10ms
step:1584/1680 train_time:142716ms step_avg:90.10ms
step:1585/1680 train_time:142808ms step_avg:90.10ms
step:1586/1680 train_time:142899ms step_avg:90.10ms
step:1587/1680 train_time:142990ms step_avg:90.10ms
step:1588/1680 train_time:143081ms step_avg:90.10ms
step:1589/1680 train_time:143172ms step_avg:90.10ms
step:1590/1680 train_time:143262ms step_avg:90.10ms
step:1591/1680 train_time:143353ms step_avg:90.10ms
step:1592/1680 train_time:143443ms step_avg:90.10ms
step:1593/1680 train_time:143533ms step_avg:90.10ms
step:1594/1680 train_time:143624ms step_avg:90.10ms
step:1595/1680 train_time:143714ms step_avg:90.10ms
step:1596/1680 train_time:143805ms step_avg:90.10ms
step:1597/1680 train_time:143896ms step_avg:90.10ms
step:1598/1680 train_time:143987ms step_avg:90.10ms
step:1599/1680 train_time:144079ms step_avg:90.11ms
step:1600/1680 train_time:144171ms step_avg:90.11ms
step:1601/1680 train_time:144262ms step_avg:90.11ms
step:1602/1680 train_time:144352ms step_avg:90.11ms
step:1603/1680 train_time:144443ms step_avg:90.11ms
step:1604/1680 train_time:144534ms step_avg:90.11ms
step:1605/1680 train_time:144625ms step_avg:90.11ms
step:1606/1680 train_time:144716ms step_avg:90.11ms
step:1607/1680 train_time:144806ms step_avg:90.11ms
step:1608/1680 train_time:144898ms step_avg:90.11ms
step:1609/1680 train_time:144989ms step_avg:90.11ms
step:1610/1680 train_time:145081ms step_avg:90.11ms
step:1611/1680 train_time:145172ms step_avg:90.11ms
step:1612/1680 train_time:145263ms step_avg:90.11ms
step:1613/1680 train_time:145354ms step_avg:90.11ms
step:1614/1680 train_time:145445ms step_avg:90.11ms
step:1615/1680 train_time:145535ms step_avg:90.11ms
step:1616/1680 train_time:145627ms step_avg:90.12ms
step:1617/1680 train_time:145717ms step_avg:90.12ms
step:1618/1680 train_time:145809ms step_avg:90.12ms
step:1619/1680 train_time:145901ms step_avg:90.12ms
step:1620/1680 train_time:145991ms step_avg:90.12ms
step:1621/1680 train_time:146084ms step_avg:90.12ms
step:1622/1680 train_time:146174ms step_avg:90.12ms
step:1623/1680 train_time:146265ms step_avg:90.12ms
step:1624/1680 train_time:146355ms step_avg:90.12ms
step:1625/1680 train_time:146446ms step_avg:90.12ms
step:1625/1680 val_loss:3.2878 train_time:146537ms step_avg:90.18ms
step:1626/1680 train_time:146559ms step_avg:90.13ms
step:1627/1680 train_time:146632ms step_avg:90.12ms
step:1628/1680 train_time:146729ms step_avg:90.13ms
step:1629/1680 train_time:146822ms step_avg:90.13ms
step:1630/1680 train_time:146912ms step_avg:90.13ms
step:1631/1680 train_time:147002ms step_avg:90.13ms
step:1632/1680 train_time:147091ms step_avg:90.13ms
step:1633/1680 train_time:147181ms step_avg:90.13ms
step:1634/1680 train_time:147270ms step_avg:90.13ms
step:1635/1680 train_time:147360ms step_avg:90.13ms
step:1636/1680 train_time:147450ms step_avg:90.13ms
step:1637/1680 train_time:147542ms step_avg:90.13ms
step:1638/1680 train_time:147636ms step_avg:90.13ms
step:1639/1680 train_time:147728ms step_avg:90.13ms
step:1640/1680 train_time:147820ms step_avg:90.13ms
step:1641/1680 train_time:147911ms step_avg:90.13ms
step:1642/1680 train_time:148002ms step_avg:90.14ms
step:1643/1680 train_time:148092ms step_avg:90.14ms
step:1644/1680 train_time:148182ms step_avg:90.14ms
step:1645/1680 train_time:148271ms step_avg:90.13ms
step:1646/1680 train_time:148361ms step_avg:90.13ms
step:1647/1680 train_time:148451ms step_avg:90.13ms
step:1648/1680 train_time:148543ms step_avg:90.14ms
step:1649/1680 train_time:148636ms step_avg:90.14ms
step:1650/1680 train_time:148728ms step_avg:90.14ms
step:1651/1680 train_time:148820ms step_avg:90.14ms
step:1652/1680 train_time:148911ms step_avg:90.14ms
step:1653/1680 train_time:149002ms step_avg:90.14ms
step:1654/1680 train_time:149092ms step_avg:90.14ms
step:1655/1680 train_time:149183ms step_avg:90.14ms
step:1656/1680 train_time:149273ms step_avg:90.14ms
step:1657/1680 train_time:149363ms step_avg:90.14ms
step:1658/1680 train_time:149454ms step_avg:90.14ms
step:1659/1680 train_time:149546ms step_avg:90.14ms
step:1660/1680 train_time:149637ms step_avg:90.14ms
step:1661/1680 train_time:149729ms step_avg:90.14ms
step:1662/1680 train_time:149821ms step_avg:90.14ms
step:1663/1680 train_time:149911ms step_avg:90.15ms
step:1664/1680 train_time:150003ms step_avg:90.15ms
step:1665/1680 train_time:150093ms step_avg:90.15ms
step:1666/1680 train_time:150183ms step_avg:90.15ms
step:1667/1680 train_time:150273ms step_avg:90.15ms
step:1668/1680 train_time:150364ms step_avg:90.15ms
step:1669/1680 train_time:150453ms step_avg:90.15ms
step:1670/1680 train_time:150544ms step_avg:90.15ms
step:1671/1680 train_time:150636ms step_avg:90.15ms
step:1672/1680 train_time:150727ms step_avg:90.15ms
step:1673/1680 train_time:150819ms step_avg:90.15ms
step:1674/1680 train_time:150911ms step_avg:90.15ms
step:1675/1680 train_time:151003ms step_avg:90.15ms
step:1676/1680 train_time:151093ms step_avg:90.15ms
step:1677/1680 train_time:151183ms step_avg:90.15ms
step:1678/1680 train_time:151273ms step_avg:90.15ms
step:1679/1680 train_time:151364ms step_avg:90.15ms
step:1680/1680 train_time:151454ms step_avg:90.15ms
step:1680/1680 val_loss:3.2772 train_time:151547ms step_avg:90.21ms
peak memory allocated: 31255 MiB reserved: 46494 MiB

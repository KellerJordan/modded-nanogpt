import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 and layer_idx !=0 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        first_bm = 1 * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, final_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"v1/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer shows no degradation with context length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
smear_gate_params = [p for n, p in model.named_parameters() if "smear" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params+smear_gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate, args.ws_validate_final_layer
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250724+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sun Sep 21 22:20:27 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   30C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   31C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   35C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   30C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            9762      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A            9763      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A            9764      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A            9765      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A            9766      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A            9767      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A            9768      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A            9769      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A            9763      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A            9764      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A            9765      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A            9766      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A            9767      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A            9768      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A            9769      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/1680 train_time:156ms step_avg:155.71ms
step:2/1680 train_time:181ms step_avg:90.66ms
step:3/1680 train_time:243ms step_avg:80.89ms
step:4/1680 train_time:330ms step_avg:82.39ms
step:5/1680 train_time:417ms step_avg:83.43ms
step:6/1680 train_time:505ms step_avg:84.22ms
step:7/1680 train_time:593ms step_avg:84.76ms
step:8/1680 train_time:682ms step_avg:85.25ms
step:9/1680 train_time:772ms step_avg:85.72ms
step:10/1680 train_time:858ms step_avg:85.84ms
step:11/1680 train_time:947ms step_avg:86.07ms
step:12/1680 train_time:1037ms step_avg:86.45ms
step:13/1680 train_time:1129ms step_avg:86.85ms
step:14/1680 train_time:1221ms step_avg:87.19ms
step:15/1680 train_time:1312ms step_avg:87.47ms
step:16/1680 train_time:1400ms step_avg:87.50ms
step:17/1680 train_time:1489ms step_avg:87.61ms
step:18/1680 train_time:1578ms step_avg:87.68ms
step:19/1680 train_time:1668ms step_avg:87.80ms
step:20/1680 train_time:1757ms step_avg:87.83ms
step:21/1680 train_time:1845ms step_avg:87.86ms
step:22/1680 train_time:1934ms step_avg:87.91ms
step:23/1680 train_time:2024ms step_avg:88.01ms
step:24/1680 train_time:2115ms step_avg:88.12ms
step:25/1680 train_time:2206ms step_avg:88.23ms
step:26/1680 train_time:2296ms step_avg:88.29ms
step:27/1680 train_time:2385ms step_avg:88.34ms
step:28/1680 train_time:2475ms step_avg:88.41ms
step:29/1680 train_time:2565ms step_avg:88.45ms
step:30/1680 train_time:2654ms step_avg:88.46ms
step:31/1680 train_time:2743ms step_avg:88.49ms
step:32/1680 train_time:2832ms step_avg:88.51ms
step:33/1680 train_time:2921ms step_avg:88.51ms
step:34/1680 train_time:3011ms step_avg:88.55ms
step:35/1680 train_time:3100ms step_avg:88.58ms
step:36/1680 train_time:3191ms step_avg:88.64ms
step:37/1680 train_time:3281ms step_avg:88.67ms
step:38/1680 train_time:3370ms step_avg:88.69ms
step:39/1680 train_time:3460ms step_avg:88.71ms
step:40/1680 train_time:3549ms step_avg:88.72ms
step:41/1680 train_time:3638ms step_avg:88.73ms
step:42/1680 train_time:3727ms step_avg:88.74ms
step:43/1680 train_time:3816ms step_avg:88.74ms
step:44/1680 train_time:3906ms step_avg:88.76ms
step:45/1680 train_time:3995ms step_avg:88.79ms
step:46/1680 train_time:4085ms step_avg:88.81ms
step:47/1680 train_time:4175ms step_avg:88.82ms
step:48/1680 train_time:4264ms step_avg:88.84ms
step:49/1680 train_time:4354ms step_avg:88.86ms
step:50/1680 train_time:4444ms step_avg:88.88ms
step:51/1680 train_time:4534ms step_avg:88.90ms
step:52/1680 train_time:4624ms step_avg:88.92ms
step:53/1680 train_time:4714ms step_avg:88.94ms
step:54/1680 train_time:4803ms step_avg:88.95ms
step:55/1680 train_time:4892ms step_avg:88.95ms
step:56/1680 train_time:4981ms step_avg:88.95ms
step:57/1680 train_time:5071ms step_avg:88.97ms
step:58/1680 train_time:5160ms step_avg:88.97ms
step:59/1680 train_time:5249ms step_avg:88.97ms
step:60/1680 train_time:5339ms step_avg:88.98ms
step:61/1680 train_time:5428ms step_avg:88.99ms
step:62/1680 train_time:5518ms step_avg:89.00ms
step:63/1680 train_time:5608ms step_avg:89.02ms
step:64/1680 train_time:5698ms step_avg:89.03ms
step:65/1680 train_time:5787ms step_avg:89.02ms
step:66/1680 train_time:5876ms step_avg:89.03ms
step:67/1680 train_time:5966ms step_avg:89.05ms
step:68/1680 train_time:6056ms step_avg:89.06ms
step:69/1680 train_time:6145ms step_avg:89.06ms
step:70/1680 train_time:6235ms step_avg:89.08ms
step:71/1680 train_time:6325ms step_avg:89.09ms
step:72/1680 train_time:6415ms step_avg:89.10ms
step:73/1680 train_time:6505ms step_avg:89.11ms
step:74/1680 train_time:6595ms step_avg:89.12ms
step:75/1680 train_time:6684ms step_avg:89.12ms
step:76/1680 train_time:6774ms step_avg:89.13ms
step:77/1680 train_time:6864ms step_avg:89.14ms
step:78/1680 train_time:6953ms step_avg:89.14ms
step:79/1680 train_time:7042ms step_avg:89.14ms
step:80/1680 train_time:7131ms step_avg:89.14ms
step:81/1680 train_time:7220ms step_avg:89.14ms
step:82/1680 train_time:7309ms step_avg:89.14ms
step:83/1680 train_time:7398ms step_avg:89.14ms
step:84/1680 train_time:7488ms step_avg:89.15ms
step:85/1680 train_time:7578ms step_avg:89.15ms
step:86/1680 train_time:7668ms step_avg:89.16ms
step:87/1680 train_time:7758ms step_avg:89.17ms
step:88/1680 train_time:7847ms step_avg:89.18ms
step:89/1680 train_time:7937ms step_avg:89.18ms
step:90/1680 train_time:8026ms step_avg:89.18ms
step:91/1680 train_time:8115ms step_avg:89.18ms
step:92/1680 train_time:8205ms step_avg:89.18ms
step:93/1680 train_time:8295ms step_avg:89.20ms
step:94/1680 train_time:8385ms step_avg:89.20ms
step:95/1680 train_time:8474ms step_avg:89.20ms
step:96/1680 train_time:8563ms step_avg:89.20ms
step:97/1680 train_time:8653ms step_avg:89.21ms
step:98/1680 train_time:8744ms step_avg:89.22ms
step:99/1680 train_time:8833ms step_avg:89.22ms
step:100/1680 train_time:8923ms step_avg:89.23ms
step:101/1680 train_time:9012ms step_avg:89.23ms
step:102/1680 train_time:9102ms step_avg:89.23ms
step:103/1680 train_time:9191ms step_avg:89.23ms
step:104/1680 train_time:9281ms step_avg:89.24ms
step:105/1680 train_time:9370ms step_avg:89.24ms
step:106/1680 train_time:9460ms step_avg:89.24ms
step:107/1680 train_time:9549ms step_avg:89.24ms
step:108/1680 train_time:9638ms step_avg:89.25ms
step:109/1680 train_time:9729ms step_avg:89.26ms
step:110/1680 train_time:9819ms step_avg:89.26ms
step:111/1680 train_time:9908ms step_avg:89.26ms
step:112/1680 train_time:9997ms step_avg:89.26ms
step:113/1680 train_time:10089ms step_avg:89.29ms
step:114/1680 train_time:10176ms step_avg:89.26ms
step:115/1680 train_time:10265ms step_avg:89.26ms
step:116/1680 train_time:10355ms step_avg:89.26ms
step:117/1680 train_time:10444ms step_avg:89.27ms
step:118/1680 train_time:10534ms step_avg:89.27ms
step:119/1680 train_time:10624ms step_avg:89.27ms
step:120/1680 train_time:10713ms step_avg:89.28ms
step:121/1680 train_time:10802ms step_avg:89.28ms
step:122/1680 train_time:10893ms step_avg:89.28ms
step:123/1680 train_time:10981ms step_avg:89.28ms
step:124/1680 train_time:11070ms step_avg:89.27ms
step:125/1680 train_time:11158ms step_avg:89.26ms
step:125/1680 val_loss:4.3132 train_time:11248ms step_avg:89.98ms
step:126/1680 train_time:11272ms step_avg:89.46ms
step:127/1680 train_time:11340ms step_avg:89.29ms
step:128/1680 train_time:11437ms step_avg:89.35ms
step:129/1680 train_time:11529ms step_avg:89.38ms
step:130/1680 train_time:11619ms step_avg:89.37ms
step:131/1680 train_time:11707ms step_avg:89.37ms
step:132/1680 train_time:11795ms step_avg:89.36ms
step:133/1680 train_time:11883ms step_avg:89.35ms
step:134/1680 train_time:11971ms step_avg:89.34ms
step:135/1680 train_time:12060ms step_avg:89.33ms
step:136/1680 train_time:12149ms step_avg:89.33ms
step:137/1680 train_time:12238ms step_avg:89.33ms
step:138/1680 train_time:12328ms step_avg:89.33ms
step:139/1680 train_time:12420ms step_avg:89.35ms
step:140/1680 train_time:12511ms step_avg:89.36ms
step:141/1680 train_time:12601ms step_avg:89.37ms
step:142/1680 train_time:12690ms step_avg:89.37ms
step:143/1680 train_time:12779ms step_avg:89.36ms
step:144/1680 train_time:12868ms step_avg:89.36ms
step:145/1680 train_time:12956ms step_avg:89.35ms
step:146/1680 train_time:13044ms step_avg:89.35ms
step:147/1680 train_time:13132ms step_avg:89.33ms
step:148/1680 train_time:13221ms step_avg:89.33ms
step:149/1680 train_time:13311ms step_avg:89.33ms
step:150/1680 train_time:13401ms step_avg:89.34ms
step:151/1680 train_time:13492ms step_avg:89.35ms
step:152/1680 train_time:13582ms step_avg:89.35ms
step:153/1680 train_time:13672ms step_avg:89.36ms
step:154/1680 train_time:13760ms step_avg:89.35ms
step:155/1680 train_time:13850ms step_avg:89.35ms
step:156/1680 train_time:13938ms step_avg:89.35ms
step:157/1680 train_time:14027ms step_avg:89.34ms
step:158/1680 train_time:14115ms step_avg:89.34ms
step:159/1680 train_time:14204ms step_avg:89.33ms
step:160/1680 train_time:14293ms step_avg:89.33ms
step:161/1680 train_time:14384ms step_avg:89.34ms
step:162/1680 train_time:14473ms step_avg:89.34ms
step:163/1680 train_time:14563ms step_avg:89.35ms
step:164/1680 train_time:14653ms step_avg:89.35ms
step:165/1680 train_time:14742ms step_avg:89.35ms
step:166/1680 train_time:14831ms step_avg:89.34ms
step:167/1680 train_time:14920ms step_avg:89.34ms
step:168/1680 train_time:15009ms step_avg:89.34ms
step:169/1680 train_time:15099ms step_avg:89.34ms
step:170/1680 train_time:15187ms step_avg:89.34ms
step:171/1680 train_time:15276ms step_avg:89.33ms
step:172/1680 train_time:15366ms step_avg:89.34ms
step:173/1680 train_time:15455ms step_avg:89.34ms
step:174/1680 train_time:15545ms step_avg:89.34ms
step:175/1680 train_time:15634ms step_avg:89.34ms
step:176/1680 train_time:15723ms step_avg:89.33ms
step:177/1680 train_time:15811ms step_avg:89.33ms
step:178/1680 train_time:15900ms step_avg:89.33ms
step:179/1680 train_time:15990ms step_avg:89.33ms
step:180/1680 train_time:16079ms step_avg:89.33ms
step:181/1680 train_time:16169ms step_avg:89.33ms
step:182/1680 train_time:16258ms step_avg:89.33ms
step:183/1680 train_time:16347ms step_avg:89.33ms
step:184/1680 train_time:16437ms step_avg:89.33ms
step:185/1680 train_time:16526ms step_avg:89.33ms
step:186/1680 train_time:16615ms step_avg:89.33ms
step:187/1680 train_time:16704ms step_avg:89.33ms
step:188/1680 train_time:16793ms step_avg:89.32ms
step:189/1680 train_time:16882ms step_avg:89.32ms
step:190/1680 train_time:16970ms step_avg:89.32ms
step:191/1680 train_time:17060ms step_avg:89.32ms
step:192/1680 train_time:17149ms step_avg:89.32ms
step:193/1680 train_time:17239ms step_avg:89.32ms
step:194/1680 train_time:17328ms step_avg:89.32ms
step:195/1680 train_time:17417ms step_avg:89.32ms
step:196/1680 train_time:17507ms step_avg:89.32ms
step:197/1680 train_time:17598ms step_avg:89.33ms
step:198/1680 train_time:17688ms step_avg:89.33ms
step:199/1680 train_time:17777ms step_avg:89.33ms
step:200/1680 train_time:17866ms step_avg:89.33ms
step:201/1680 train_time:17955ms step_avg:89.33ms
step:202/1680 train_time:18044ms step_avg:89.32ms
step:203/1680 train_time:18133ms step_avg:89.32ms
step:204/1680 train_time:18222ms step_avg:89.32ms
step:205/1680 train_time:18311ms step_avg:89.32ms
step:206/1680 train_time:18401ms step_avg:89.32ms
step:207/1680 train_time:18490ms step_avg:89.33ms
step:208/1680 train_time:18580ms step_avg:89.33ms
step:209/1680 train_time:18670ms step_avg:89.33ms
step:210/1680 train_time:18759ms step_avg:89.33ms
step:211/1680 train_time:18849ms step_avg:89.33ms
step:212/1680 train_time:18939ms step_avg:89.33ms
step:213/1680 train_time:19028ms step_avg:89.33ms
step:214/1680 train_time:19117ms step_avg:89.33ms
step:215/1680 train_time:19206ms step_avg:89.33ms
step:216/1680 train_time:19295ms step_avg:89.33ms
step:217/1680 train_time:19385ms step_avg:89.33ms
step:218/1680 train_time:19474ms step_avg:89.33ms
step:219/1680 train_time:19563ms step_avg:89.33ms
step:220/1680 train_time:19653ms step_avg:89.33ms
step:221/1680 train_time:19742ms step_avg:89.33ms
step:222/1680 train_time:19830ms step_avg:89.33ms
step:223/1680 train_time:19920ms step_avg:89.33ms
step:224/1680 train_time:20009ms step_avg:89.33ms
step:225/1680 train_time:20099ms step_avg:89.33ms
step:226/1680 train_time:20188ms step_avg:89.33ms
step:227/1680 train_time:20277ms step_avg:89.32ms
step:228/1680 train_time:20366ms step_avg:89.33ms
step:229/1680 train_time:20455ms step_avg:89.33ms
step:230/1680 train_time:20545ms step_avg:89.33ms
step:231/1680 train_time:20634ms step_avg:89.32ms
step:232/1680 train_time:20723ms step_avg:89.32ms
step:233/1680 train_time:20811ms step_avg:89.32ms
step:234/1680 train_time:20901ms step_avg:89.32ms
step:235/1680 train_time:20990ms step_avg:89.32ms
step:236/1680 train_time:21078ms step_avg:89.32ms
step:237/1680 train_time:21168ms step_avg:89.31ms
step:238/1680 train_time:21256ms step_avg:89.31ms
step:239/1680 train_time:21346ms step_avg:89.31ms
step:240/1680 train_time:21435ms step_avg:89.31ms
step:241/1680 train_time:21524ms step_avg:89.31ms
step:242/1680 train_time:21614ms step_avg:89.32ms
step:243/1680 train_time:21703ms step_avg:89.31ms
step:244/1680 train_time:21792ms step_avg:89.31ms
step:245/1680 train_time:21881ms step_avg:89.31ms
step:246/1680 train_time:21971ms step_avg:89.31ms
step:247/1680 train_time:22060ms step_avg:89.31ms
step:248/1680 train_time:22150ms step_avg:89.32ms
step:249/1680 train_time:22239ms step_avg:89.31ms
step:250/1680 train_time:22329ms step_avg:89.31ms
step:250/1680 val_loss:3.9718 train_time:22419ms step_avg:89.68ms
step:251/1680 train_time:22444ms step_avg:89.42ms
step:252/1680 train_time:22512ms step_avg:89.33ms
step:253/1680 train_time:22607ms step_avg:89.36ms
step:254/1680 train_time:22697ms step_avg:89.36ms
step:255/1680 train_time:22786ms step_avg:89.36ms
step:256/1680 train_time:22874ms step_avg:89.35ms
step:257/1680 train_time:22962ms step_avg:89.35ms
step:258/1680 train_time:23050ms step_avg:89.34ms
step:259/1680 train_time:23139ms step_avg:89.34ms
step:260/1680 train_time:23227ms step_avg:89.33ms
step:261/1680 train_time:23315ms step_avg:89.33ms
step:262/1680 train_time:23404ms step_avg:89.33ms
step:263/1680 train_time:23497ms step_avg:89.34ms
step:264/1680 train_time:23589ms step_avg:89.35ms
step:265/1680 train_time:23679ms step_avg:89.36ms
step:266/1680 train_time:23769ms step_avg:89.36ms
step:267/1680 train_time:23858ms step_avg:89.36ms
step:268/1680 train_time:23947ms step_avg:89.35ms
step:269/1680 train_time:24035ms step_avg:89.35ms
step:270/1680 train_time:24123ms step_avg:89.35ms
step:271/1680 train_time:24212ms step_avg:89.34ms
step:272/1680 train_time:24300ms step_avg:89.34ms
step:273/1680 train_time:24389ms step_avg:89.34ms
step:274/1680 train_time:24479ms step_avg:89.34ms
step:275/1680 train_time:24570ms step_avg:89.35ms
step:276/1680 train_time:24660ms step_avg:89.35ms
step:277/1680 train_time:24749ms step_avg:89.35ms
step:278/1680 train_time:24839ms step_avg:89.35ms
step:279/1680 train_time:24928ms step_avg:89.35ms
step:280/1680 train_time:25017ms step_avg:89.35ms
step:281/1680 train_time:25106ms step_avg:89.35ms
step:282/1680 train_time:25196ms step_avg:89.35ms
step:283/1680 train_time:25285ms step_avg:89.35ms
step:284/1680 train_time:25374ms step_avg:89.34ms
step:285/1680 train_time:25463ms step_avg:89.34ms
step:286/1680 train_time:25553ms step_avg:89.35ms
step:287/1680 train_time:25643ms step_avg:89.35ms
step:288/1680 train_time:25732ms step_avg:89.35ms
step:289/1680 train_time:25822ms step_avg:89.35ms
step:290/1680 train_time:25912ms step_avg:89.35ms
step:291/1680 train_time:26001ms step_avg:89.35ms
step:292/1680 train_time:26090ms step_avg:89.35ms
step:293/1680 train_time:26183ms step_avg:89.36ms
step:294/1680 train_time:26267ms step_avg:89.34ms
step:295/1680 train_time:26356ms step_avg:89.34ms
step:296/1680 train_time:26446ms step_avg:89.34ms
step:297/1680 train_time:26536ms step_avg:89.35ms
step:298/1680 train_time:26625ms step_avg:89.35ms
step:299/1680 train_time:26715ms step_avg:89.35ms
step:300/1680 train_time:26804ms step_avg:89.35ms
step:301/1680 train_time:26894ms step_avg:89.35ms
step:302/1680 train_time:26983ms step_avg:89.35ms
step:303/1680 train_time:27072ms step_avg:89.35ms
step:304/1680 train_time:27162ms step_avg:89.35ms
step:305/1680 train_time:27250ms step_avg:89.34ms
step:306/1680 train_time:27339ms step_avg:89.34ms
step:307/1680 train_time:27428ms step_avg:89.34ms
step:308/1680 train_time:27517ms step_avg:89.34ms
step:309/1680 train_time:27607ms step_avg:89.34ms
step:310/1680 train_time:27699ms step_avg:89.35ms
step:311/1680 train_time:27788ms step_avg:89.35ms
step:312/1680 train_time:27878ms step_avg:89.35ms
step:313/1680 train_time:27968ms step_avg:89.35ms
step:314/1680 train_time:28057ms step_avg:89.35ms
step:315/1680 train_time:28146ms step_avg:89.35ms
step:316/1680 train_time:28235ms step_avg:89.35ms
step:317/1680 train_time:28325ms step_avg:89.35ms
step:318/1680 train_time:28414ms step_avg:89.35ms
step:319/1680 train_time:28503ms step_avg:89.35ms
step:320/1680 train_time:28592ms step_avg:89.35ms
step:321/1680 train_time:28681ms step_avg:89.35ms
step:322/1680 train_time:28771ms step_avg:89.35ms
step:323/1680 train_time:28861ms step_avg:89.35ms
step:324/1680 train_time:28950ms step_avg:89.35ms
step:325/1680 train_time:29039ms step_avg:89.35ms
step:326/1680 train_time:29129ms step_avg:89.35ms
step:327/1680 train_time:29218ms step_avg:89.35ms
step:328/1680 train_time:29307ms step_avg:89.35ms
step:329/1680 train_time:29397ms step_avg:89.35ms
step:330/1680 train_time:29487ms step_avg:89.35ms
step:331/1680 train_time:29577ms step_avg:89.36ms
step:332/1680 train_time:29666ms step_avg:89.36ms
step:333/1680 train_time:29756ms step_avg:89.36ms
step:334/1680 train_time:29845ms step_avg:89.36ms
step:335/1680 train_time:29934ms step_avg:89.36ms
step:336/1680 train_time:30024ms step_avg:89.36ms
step:337/1680 train_time:30114ms step_avg:89.36ms
step:338/1680 train_time:30202ms step_avg:89.36ms
step:339/1680 train_time:30292ms step_avg:89.36ms
step:340/1680 train_time:30385ms step_avg:89.37ms
step:341/1680 train_time:30470ms step_avg:89.35ms
step:342/1680 train_time:30560ms step_avg:89.36ms
step:343/1680 train_time:30648ms step_avg:89.35ms
step:344/1680 train_time:30738ms step_avg:89.36ms
step:345/1680 train_time:30828ms step_avg:89.36ms
step:346/1680 train_time:30916ms step_avg:89.35ms
step:347/1680 train_time:31005ms step_avg:89.35ms
step:348/1680 train_time:31095ms step_avg:89.35ms
step:349/1680 train_time:31184ms step_avg:89.35ms
step:350/1680 train_time:31275ms step_avg:89.36ms
step:351/1680 train_time:31364ms step_avg:89.36ms
step:352/1680 train_time:31454ms step_avg:89.36ms
step:353/1680 train_time:31544ms step_avg:89.36ms
step:354/1680 train_time:31634ms step_avg:89.36ms
step:355/1680 train_time:31724ms step_avg:89.36ms
step:356/1680 train_time:31812ms step_avg:89.36ms
step:357/1680 train_time:31901ms step_avg:89.36ms
step:358/1680 train_time:31989ms step_avg:89.36ms
step:359/1680 train_time:32079ms step_avg:89.36ms
step:360/1680 train_time:32168ms step_avg:89.36ms
step:361/1680 train_time:32258ms step_avg:89.36ms
step:362/1680 train_time:32348ms step_avg:89.36ms
step:363/1680 train_time:32438ms step_avg:89.36ms
step:364/1680 train_time:32527ms step_avg:89.36ms
step:365/1680 train_time:32616ms step_avg:89.36ms
step:366/1680 train_time:32706ms step_avg:89.36ms
step:367/1680 train_time:32796ms step_avg:89.36ms
step:368/1680 train_time:32885ms step_avg:89.36ms
step:369/1680 train_time:32974ms step_avg:89.36ms
step:370/1680 train_time:33064ms step_avg:89.36ms
step:371/1680 train_time:33154ms step_avg:89.36ms
step:372/1680 train_time:33243ms step_avg:89.36ms
step:373/1680 train_time:33333ms step_avg:89.36ms
step:374/1680 train_time:33422ms step_avg:89.36ms
step:375/1680 train_time:33512ms step_avg:89.37ms
step:375/1680 val_loss:3.8163 train_time:33602ms step_avg:89.61ms
step:376/1680 train_time:33627ms step_avg:89.43ms
step:377/1680 train_time:33699ms step_avg:89.39ms
step:378/1680 train_time:33795ms step_avg:89.40ms
step:379/1680 train_time:33885ms step_avg:89.41ms
step:380/1680 train_time:33974ms step_avg:89.41ms
step:381/1680 train_time:34063ms step_avg:89.41ms
step:382/1680 train_time:34154ms step_avg:89.41ms
step:383/1680 train_time:34241ms step_avg:89.40ms
step:384/1680 train_time:34329ms step_avg:89.40ms
step:385/1680 train_time:34417ms step_avg:89.40ms
step:386/1680 train_time:34505ms step_avg:89.39ms
step:387/1680 train_time:34594ms step_avg:89.39ms
step:388/1680 train_time:34685ms step_avg:89.40ms
step:389/1680 train_time:34777ms step_avg:89.40ms
step:390/1680 train_time:34867ms step_avg:89.40ms
step:391/1680 train_time:34957ms step_avg:89.40ms
step:392/1680 train_time:35046ms step_avg:89.40ms
step:393/1680 train_time:35136ms step_avg:89.40ms
step:394/1680 train_time:35225ms step_avg:89.40ms
step:395/1680 train_time:35313ms step_avg:89.40ms
step:396/1680 train_time:35402ms step_avg:89.40ms
step:397/1680 train_time:35491ms step_avg:89.40ms
step:398/1680 train_time:35580ms step_avg:89.40ms
step:399/1680 train_time:35671ms step_avg:89.40ms
step:400/1680 train_time:35761ms step_avg:89.40ms
step:401/1680 train_time:35851ms step_avg:89.40ms
step:402/1680 train_time:35941ms step_avg:89.41ms
step:403/1680 train_time:36031ms step_avg:89.41ms
step:404/1680 train_time:36120ms step_avg:89.41ms
step:405/1680 train_time:36209ms step_avg:89.40ms
step:406/1680 train_time:36297ms step_avg:89.40ms
step:407/1680 train_time:36386ms step_avg:89.40ms
step:408/1680 train_time:36475ms step_avg:89.40ms
step:409/1680 train_time:36564ms step_avg:89.40ms
step:410/1680 train_time:36654ms step_avg:89.40ms
step:411/1680 train_time:36744ms step_avg:89.40ms
step:412/1680 train_time:36835ms step_avg:89.40ms
step:413/1680 train_time:36925ms step_avg:89.41ms
step:414/1680 train_time:37015ms step_avg:89.41ms
step:415/1680 train_time:37104ms step_avg:89.41ms
step:416/1680 train_time:37194ms step_avg:89.41ms
step:417/1680 train_time:37283ms step_avg:89.41ms
step:418/1680 train_time:37372ms step_avg:89.41ms
step:419/1680 train_time:37460ms step_avg:89.40ms
step:420/1680 train_time:37551ms step_avg:89.41ms
step:421/1680 train_time:37640ms step_avg:89.41ms
step:422/1680 train_time:37729ms step_avg:89.41ms
step:423/1680 train_time:37820ms step_avg:89.41ms
step:424/1680 train_time:37909ms step_avg:89.41ms
step:425/1680 train_time:37999ms step_avg:89.41ms
step:426/1680 train_time:38087ms step_avg:89.41ms
step:427/1680 train_time:38176ms step_avg:89.41ms
step:428/1680 train_time:38265ms step_avg:89.40ms
step:429/1680 train_time:38355ms step_avg:89.40ms
step:430/1680 train_time:38443ms step_avg:89.40ms
step:431/1680 train_time:38533ms step_avg:89.40ms
step:432/1680 train_time:38623ms step_avg:89.40ms
step:433/1680 train_time:38713ms step_avg:89.41ms
step:434/1680 train_time:38802ms step_avg:89.41ms
step:435/1680 train_time:38892ms step_avg:89.41ms
step:436/1680 train_time:38982ms step_avg:89.41ms
step:437/1680 train_time:39072ms step_avg:89.41ms
step:438/1680 train_time:39162ms step_avg:89.41ms
step:439/1680 train_time:39251ms step_avg:89.41ms
step:440/1680 train_time:39340ms step_avg:89.41ms
step:441/1680 train_time:39429ms step_avg:89.41ms
step:442/1680 train_time:39518ms step_avg:89.41ms
step:443/1680 train_time:39607ms step_avg:89.41ms
step:444/1680 train_time:39697ms step_avg:89.41ms
step:445/1680 train_time:39787ms step_avg:89.41ms
step:446/1680 train_time:39876ms step_avg:89.41ms
step:447/1680 train_time:39966ms step_avg:89.41ms
step:448/1680 train_time:40055ms step_avg:89.41ms
step:449/1680 train_time:40145ms step_avg:89.41ms
step:450/1680 train_time:40234ms step_avg:89.41ms
step:451/1680 train_time:40324ms step_avg:89.41ms
step:452/1680 train_time:40413ms step_avg:89.41ms
step:453/1680 train_time:40503ms step_avg:89.41ms
step:454/1680 train_time:40593ms step_avg:89.41ms
step:455/1680 train_time:40682ms step_avg:89.41ms
step:456/1680 train_time:40771ms step_avg:89.41ms
step:457/1680 train_time:40861ms step_avg:89.41ms
step:458/1680 train_time:40951ms step_avg:89.41ms
step:459/1680 train_time:41043ms step_avg:89.42ms
step:460/1680 train_time:41129ms step_avg:89.41ms
step:461/1680 train_time:41218ms step_avg:89.41ms
step:462/1680 train_time:41308ms step_avg:89.41ms
step:463/1680 train_time:41397ms step_avg:89.41ms
step:464/1680 train_time:41486ms step_avg:89.41ms
step:465/1680 train_time:41576ms step_avg:89.41ms
step:466/1680 train_time:41665ms step_avg:89.41ms
step:467/1680 train_time:41755ms step_avg:89.41ms
step:468/1680 train_time:41845ms step_avg:89.41ms
step:469/1680 train_time:41934ms step_avg:89.41ms
step:470/1680 train_time:42025ms step_avg:89.41ms
step:471/1680 train_time:42114ms step_avg:89.41ms
step:472/1680 train_time:42203ms step_avg:89.41ms
step:473/1680 train_time:42294ms step_avg:89.42ms
step:474/1680 train_time:42384ms step_avg:89.42ms
step:475/1680 train_time:42474ms step_avg:89.42ms
step:476/1680 train_time:42563ms step_avg:89.42ms
step:477/1680 train_time:42654ms step_avg:89.42ms
step:478/1680 train_time:42741ms step_avg:89.42ms
step:479/1680 train_time:42830ms step_avg:89.42ms
step:480/1680 train_time:42920ms step_avg:89.42ms
step:481/1680 train_time:43010ms step_avg:89.42ms
step:482/1680 train_time:43100ms step_avg:89.42ms
step:483/1680 train_time:43189ms step_avg:89.42ms
step:484/1680 train_time:43278ms step_avg:89.42ms
step:485/1680 train_time:43368ms step_avg:89.42ms
step:486/1680 train_time:43458ms step_avg:89.42ms
step:487/1680 train_time:43548ms step_avg:89.42ms
step:488/1680 train_time:43637ms step_avg:89.42ms
step:489/1680 train_time:43727ms step_avg:89.42ms
step:490/1680 train_time:43816ms step_avg:89.42ms
step:491/1680 train_time:43905ms step_avg:89.42ms
step:492/1680 train_time:43995ms step_avg:89.42ms
step:493/1680 train_time:44084ms step_avg:89.42ms
step:494/1680 train_time:44173ms step_avg:89.42ms
step:495/1680 train_time:44262ms step_avg:89.42ms
step:496/1680 train_time:44352ms step_avg:89.42ms
step:497/1680 train_time:44441ms step_avg:89.42ms
step:498/1680 train_time:44531ms step_avg:89.42ms
step:499/1680 train_time:44620ms step_avg:89.42ms
step:500/1680 train_time:44710ms step_avg:89.42ms
step:500/1680 val_loss:3.7177 train_time:44800ms step_avg:89.60ms
step:501/1680 train_time:44826ms step_avg:89.47ms
step:502/1680 train_time:44895ms step_avg:89.43ms
step:503/1680 train_time:44988ms step_avg:89.44ms
step:504/1680 train_time:45079ms step_avg:89.44ms
step:505/1680 train_time:45168ms step_avg:89.44ms
step:506/1680 train_time:45256ms step_avg:89.44ms
step:507/1680 train_time:45345ms step_avg:89.44ms
step:508/1680 train_time:45434ms step_avg:89.44ms
step:509/1680 train_time:45523ms step_avg:89.44ms
step:510/1680 train_time:45611ms step_avg:89.43ms
step:511/1680 train_time:45700ms step_avg:89.43ms
step:512/1680 train_time:45790ms step_avg:89.43ms
step:513/1680 train_time:45881ms step_avg:89.44ms
step:514/1680 train_time:45972ms step_avg:89.44ms
step:515/1680 train_time:46063ms step_avg:89.44ms
step:516/1680 train_time:46153ms step_avg:89.44ms
step:517/1680 train_time:46242ms step_avg:89.44ms
step:518/1680 train_time:46331ms step_avg:89.44ms
step:519/1680 train_time:46420ms step_avg:89.44ms
step:520/1680 train_time:46508ms step_avg:89.44ms
step:521/1680 train_time:46596ms step_avg:89.44ms
step:522/1680 train_time:46685ms step_avg:89.44ms
step:523/1680 train_time:46774ms step_avg:89.43ms
step:524/1680 train_time:46863ms step_avg:89.43ms
step:525/1680 train_time:46955ms step_avg:89.44ms
step:526/1680 train_time:47046ms step_avg:89.44ms
step:527/1680 train_time:47136ms step_avg:89.44ms
step:528/1680 train_time:47226ms step_avg:89.44ms
step:529/1680 train_time:47316ms step_avg:89.44ms
step:530/1680 train_time:47405ms step_avg:89.44ms
step:531/1680 train_time:47494ms step_avg:89.44ms
step:532/1680 train_time:47583ms step_avg:89.44ms
step:533/1680 train_time:47672ms step_avg:89.44ms
step:534/1680 train_time:47761ms step_avg:89.44ms
step:535/1680 train_time:47850ms step_avg:89.44ms
step:536/1680 train_time:47941ms step_avg:89.44ms
step:537/1680 train_time:48031ms step_avg:89.44ms
step:538/1680 train_time:48121ms step_avg:89.44ms
step:539/1680 train_time:48211ms step_avg:89.44ms
step:540/1680 train_time:48301ms step_avg:89.45ms
step:541/1680 train_time:48389ms step_avg:89.44ms
step:542/1680 train_time:48478ms step_avg:89.44ms
step:543/1680 train_time:48567ms step_avg:89.44ms
step:544/1680 train_time:48657ms step_avg:89.44ms
step:545/1680 train_time:48746ms step_avg:89.44ms
step:546/1680 train_time:48836ms step_avg:89.44ms
step:547/1680 train_time:48926ms step_avg:89.44ms
step:548/1680 train_time:49017ms step_avg:89.45ms
step:549/1680 train_time:49108ms step_avg:89.45ms
step:550/1680 train_time:49200ms step_avg:89.45ms
step:551/1680 train_time:49290ms step_avg:89.46ms
step:552/1680 train_time:49380ms step_avg:89.46ms
step:553/1680 train_time:49470ms step_avg:89.46ms
step:554/1680 train_time:49560ms step_avg:89.46ms
step:555/1680 train_time:49650ms step_avg:89.46ms
step:556/1680 train_time:49742ms step_avg:89.46ms
step:557/1680 train_time:49832ms step_avg:89.47ms
step:558/1680 train_time:49922ms step_avg:89.47ms
step:559/1680 train_time:50013ms step_avg:89.47ms
step:560/1680 train_time:50106ms step_avg:89.48ms
step:561/1680 train_time:50196ms step_avg:89.48ms
step:562/1680 train_time:50287ms step_avg:89.48ms
step:563/1680 train_time:50379ms step_avg:89.48ms
step:564/1680 train_time:50469ms step_avg:89.48ms
step:565/1680 train_time:50559ms step_avg:89.49ms
step:566/1680 train_time:50649ms step_avg:89.49ms
step:567/1680 train_time:50740ms step_avg:89.49ms
step:568/1680 train_time:50830ms step_avg:89.49ms
step:569/1680 train_time:50921ms step_avg:89.49ms
step:570/1680 train_time:51012ms step_avg:89.49ms
step:571/1680 train_time:51103ms step_avg:89.50ms
step:572/1680 train_time:51194ms step_avg:89.50ms
step:573/1680 train_time:51286ms step_avg:89.50ms
step:574/1680 train_time:51377ms step_avg:89.51ms
step:575/1680 train_time:51467ms step_avg:89.51ms
step:576/1680 train_time:51558ms step_avg:89.51ms
step:577/1680 train_time:51649ms step_avg:89.51ms
step:578/1680 train_time:51739ms step_avg:89.51ms
step:579/1680 train_time:51829ms step_avg:89.52ms
step:580/1680 train_time:51921ms step_avg:89.52ms
step:581/1680 train_time:52011ms step_avg:89.52ms
step:582/1680 train_time:52102ms step_avg:89.52ms
step:583/1680 train_time:52193ms step_avg:89.52ms
step:584/1680 train_time:52283ms step_avg:89.53ms
step:585/1680 train_time:52374ms step_avg:89.53ms
step:586/1680 train_time:52465ms step_avg:89.53ms
step:587/1680 train_time:52556ms step_avg:89.53ms
step:588/1680 train_time:52648ms step_avg:89.54ms
step:589/1680 train_time:52739ms step_avg:89.54ms
step:590/1680 train_time:52830ms step_avg:89.54ms
step:591/1680 train_time:52922ms step_avg:89.55ms
step:592/1680 train_time:53012ms step_avg:89.55ms
step:593/1680 train_time:53103ms step_avg:89.55ms
step:594/1680 train_time:53193ms step_avg:89.55ms
step:595/1680 train_time:53284ms step_avg:89.55ms
step:596/1680 train_time:53375ms step_avg:89.56ms
step:597/1680 train_time:53466ms step_avg:89.56ms
step:598/1680 train_time:53556ms step_avg:89.56ms
step:599/1680 train_time:53647ms step_avg:89.56ms
step:600/1680 train_time:53738ms step_avg:89.56ms
step:601/1680 train_time:53828ms step_avg:89.56ms
step:602/1680 train_time:53920ms step_avg:89.57ms
step:603/1680 train_time:54011ms step_avg:89.57ms
step:604/1680 train_time:54101ms step_avg:89.57ms
step:605/1680 train_time:54191ms step_avg:89.57ms
step:606/1680 train_time:54283ms step_avg:89.58ms
step:607/1680 train_time:54374ms step_avg:89.58ms
step:608/1680 train_time:54464ms step_avg:89.58ms
step:609/1680 train_time:54555ms step_avg:89.58ms
step:610/1680 train_time:54646ms step_avg:89.58ms
step:611/1680 train_time:54737ms step_avg:89.59ms
step:612/1680 train_time:54828ms step_avg:89.59ms
step:613/1680 train_time:54918ms step_avg:89.59ms
step:614/1680 train_time:55009ms step_avg:89.59ms
step:615/1680 train_time:55100ms step_avg:89.59ms
step:616/1680 train_time:55190ms step_avg:89.59ms
step:617/1680 train_time:55282ms step_avg:89.60ms
step:618/1680 train_time:55373ms step_avg:89.60ms
step:619/1680 train_time:55464ms step_avg:89.60ms
step:620/1680 train_time:55554ms step_avg:89.60ms
step:621/1680 train_time:55645ms step_avg:89.60ms
step:622/1680 train_time:55735ms step_avg:89.61ms
step:623/1680 train_time:55826ms step_avg:89.61ms
step:624/1680 train_time:55918ms step_avg:89.61ms
step:625/1680 train_time:56008ms step_avg:89.61ms
step:625/1680 val_loss:3.6172 train_time:56101ms step_avg:89.76ms
step:626/1680 train_time:56126ms step_avg:89.66ms
step:627/1680 train_time:56194ms step_avg:89.62ms
step:628/1680 train_time:56294ms step_avg:89.64ms
step:629/1680 train_time:56385ms step_avg:89.64ms
step:630/1680 train_time:56474ms step_avg:89.64ms
step:631/1680 train_time:56562ms step_avg:89.64ms
step:632/1680 train_time:56650ms step_avg:89.64ms
step:633/1680 train_time:56738ms step_avg:89.63ms
step:634/1680 train_time:56828ms step_avg:89.63ms
step:635/1680 train_time:56916ms step_avg:89.63ms
step:636/1680 train_time:57007ms step_avg:89.63ms
step:637/1680 train_time:57098ms step_avg:89.64ms
step:638/1680 train_time:57190ms step_avg:89.64ms
step:639/1680 train_time:57282ms step_avg:89.64ms
step:640/1680 train_time:57373ms step_avg:89.65ms
step:641/1680 train_time:57462ms step_avg:89.64ms
step:642/1680 train_time:57552ms step_avg:89.64ms
step:643/1680 train_time:57642ms step_avg:89.65ms
step:644/1680 train_time:57731ms step_avg:89.64ms
step:645/1680 train_time:57820ms step_avg:89.64ms
step:646/1680 train_time:57909ms step_avg:89.64ms
step:647/1680 train_time:57999ms step_avg:89.64ms
step:648/1680 train_time:58089ms step_avg:89.64ms
step:649/1680 train_time:58181ms step_avg:89.65ms
step:650/1680 train_time:58272ms step_avg:89.65ms
step:651/1680 train_time:58362ms step_avg:89.65ms
step:652/1680 train_time:58452ms step_avg:89.65ms
step:653/1680 train_time:58542ms step_avg:89.65ms
step:654/1680 train_time:58632ms step_avg:89.65ms
step:655/1680 train_time:58722ms step_avg:89.65ms
step:656/1680 train_time:58811ms step_avg:89.65ms
step:657/1680 train_time:58900ms step_avg:89.65ms
step:658/1680 train_time:58990ms step_avg:89.65ms
step:659/1680 train_time:59081ms step_avg:89.65ms
step:660/1680 train_time:59171ms step_avg:89.65ms
step:661/1680 train_time:59261ms step_avg:89.65ms
step:662/1680 train_time:59352ms step_avg:89.66ms
step:663/1680 train_time:59443ms step_avg:89.66ms
step:664/1680 train_time:59532ms step_avg:89.66ms
step:665/1680 train_time:59621ms step_avg:89.66ms
step:666/1680 train_time:59711ms step_avg:89.66ms
step:667/1680 train_time:59801ms step_avg:89.66ms
step:668/1680 train_time:59891ms step_avg:89.66ms
step:669/1680 train_time:59980ms step_avg:89.66ms
step:670/1680 train_time:60070ms step_avg:89.66ms
step:671/1680 train_time:60160ms step_avg:89.66ms
step:672/1680 train_time:60250ms step_avg:89.66ms
step:673/1680 train_time:60341ms step_avg:89.66ms
step:674/1680 train_time:60431ms step_avg:89.66ms
step:675/1680 train_time:60521ms step_avg:89.66ms
step:676/1680 train_time:60611ms step_avg:89.66ms
step:677/1680 train_time:60701ms step_avg:89.66ms
step:678/1680 train_time:60791ms step_avg:89.66ms
step:679/1680 train_time:60880ms step_avg:89.66ms
step:680/1680 train_time:60970ms step_avg:89.66ms
step:681/1680 train_time:61059ms step_avg:89.66ms
step:682/1680 train_time:61150ms step_avg:89.66ms
step:683/1680 train_time:61240ms step_avg:89.66ms
step:684/1680 train_time:61330ms step_avg:89.66ms
step:685/1680 train_time:61420ms step_avg:89.66ms
step:686/1680 train_time:61509ms step_avg:89.66ms
step:687/1680 train_time:61599ms step_avg:89.66ms
step:688/1680 train_time:61689ms step_avg:89.66ms
step:689/1680 train_time:61779ms step_avg:89.67ms
step:690/1680 train_time:61869ms step_avg:89.66ms
step:691/1680 train_time:61958ms step_avg:89.66ms
step:692/1680 train_time:62048ms step_avg:89.67ms
step:693/1680 train_time:62139ms step_avg:89.67ms
step:694/1680 train_time:62230ms step_avg:89.67ms
step:695/1680 train_time:62321ms step_avg:89.67ms
step:696/1680 train_time:62411ms step_avg:89.67ms
step:697/1680 train_time:62501ms step_avg:89.67ms
step:698/1680 train_time:62592ms step_avg:89.67ms
step:699/1680 train_time:62682ms step_avg:89.67ms
step:700/1680 train_time:62772ms step_avg:89.67ms
step:701/1680 train_time:62862ms step_avg:89.67ms
step:702/1680 train_time:62951ms step_avg:89.67ms
step:703/1680 train_time:63042ms step_avg:89.68ms
step:704/1680 train_time:63132ms step_avg:89.68ms
step:705/1680 train_time:63222ms step_avg:89.68ms
step:706/1680 train_time:63312ms step_avg:89.68ms
step:707/1680 train_time:63402ms step_avg:89.68ms
step:708/1680 train_time:63492ms step_avg:89.68ms
step:709/1680 train_time:63584ms step_avg:89.68ms
step:710/1680 train_time:63674ms step_avg:89.68ms
step:711/1680 train_time:63763ms step_avg:89.68ms
step:712/1680 train_time:63854ms step_avg:89.68ms
step:713/1680 train_time:63944ms step_avg:89.68ms
step:714/1680 train_time:64034ms step_avg:89.68ms
step:715/1680 train_time:64123ms step_avg:89.68ms
step:716/1680 train_time:64213ms step_avg:89.68ms
step:717/1680 train_time:64305ms step_avg:89.69ms
step:718/1680 train_time:64395ms step_avg:89.69ms
step:719/1680 train_time:64485ms step_avg:89.69ms
step:720/1680 train_time:64575ms step_avg:89.69ms
step:721/1680 train_time:64666ms step_avg:89.69ms
step:722/1680 train_time:64757ms step_avg:89.69ms
step:723/1680 train_time:64847ms step_avg:89.69ms
step:724/1680 train_time:64937ms step_avg:89.69ms
step:725/1680 train_time:65027ms step_avg:89.69ms
step:726/1680 train_time:65117ms step_avg:89.69ms
step:727/1680 train_time:65208ms step_avg:89.69ms
step:728/1680 train_time:65299ms step_avg:89.70ms
step:729/1680 train_time:65389ms step_avg:89.70ms
step:730/1680 train_time:65480ms step_avg:89.70ms
step:731/1680 train_time:65570ms step_avg:89.70ms
step:732/1680 train_time:65660ms step_avg:89.70ms
step:733/1680 train_time:65751ms step_avg:89.70ms
step:734/1680 train_time:65841ms step_avg:89.70ms
step:735/1680 train_time:65931ms step_avg:89.70ms
step:736/1680 train_time:66023ms step_avg:89.71ms
step:737/1680 train_time:66111ms step_avg:89.70ms
step:738/1680 train_time:66201ms step_avg:89.70ms
step:739/1680 train_time:66290ms step_avg:89.70ms
step:740/1680 train_time:66381ms step_avg:89.70ms
step:741/1680 train_time:66471ms step_avg:89.70ms
step:742/1680 train_time:66561ms step_avg:89.70ms
step:743/1680 train_time:66652ms step_avg:89.71ms
step:744/1680 train_time:66742ms step_avg:89.71ms
step:745/1680 train_time:66832ms step_avg:89.71ms
step:746/1680 train_time:66922ms step_avg:89.71ms
step:747/1680 train_time:67012ms step_avg:89.71ms
step:748/1680 train_time:67101ms step_avg:89.71ms
step:749/1680 train_time:67191ms step_avg:89.71ms
step:750/1680 train_time:67281ms step_avg:89.71ms
step:750/1680 val_loss:3.5669 train_time:67372ms step_avg:89.83ms
step:751/1680 train_time:67398ms step_avg:89.74ms
step:752/1680 train_time:67466ms step_avg:89.72ms
step:753/1680 train_time:67561ms step_avg:89.72ms
step:754/1680 train_time:67652ms step_avg:89.72ms
step:755/1680 train_time:67742ms step_avg:89.73ms
step:756/1680 train_time:67832ms step_avg:89.72ms
step:757/1680 train_time:67921ms step_avg:89.72ms
step:758/1680 train_time:68010ms step_avg:89.72ms
step:759/1680 train_time:68099ms step_avg:89.72ms
step:760/1680 train_time:68188ms step_avg:89.72ms
step:761/1680 train_time:68278ms step_avg:89.72ms
step:762/1680 train_time:68370ms step_avg:89.72ms
step:763/1680 train_time:68463ms step_avg:89.73ms
step:764/1680 train_time:68556ms step_avg:89.73ms
step:765/1680 train_time:68648ms step_avg:89.74ms
step:766/1680 train_time:68738ms step_avg:89.74ms
step:767/1680 train_time:68828ms step_avg:89.74ms
step:768/1680 train_time:68917ms step_avg:89.74ms
step:769/1680 train_time:69006ms step_avg:89.74ms
step:770/1680 train_time:69095ms step_avg:89.73ms
step:771/1680 train_time:69184ms step_avg:89.73ms
step:772/1680 train_time:69274ms step_avg:89.73ms
step:773/1680 train_time:69364ms step_avg:89.73ms
step:774/1680 train_time:69456ms step_avg:89.74ms
step:775/1680 train_time:69548ms step_avg:89.74ms
step:776/1680 train_time:69640ms step_avg:89.74ms
step:777/1680 train_time:69731ms step_avg:89.74ms
step:778/1680 train_time:69821ms step_avg:89.74ms
step:779/1680 train_time:69911ms step_avg:89.74ms
step:780/1680 train_time:70002ms step_avg:89.75ms
step:781/1680 train_time:70091ms step_avg:89.75ms
step:782/1680 train_time:70181ms step_avg:89.75ms
step:783/1680 train_time:70271ms step_avg:89.75ms
step:784/1680 train_time:70361ms step_avg:89.75ms
step:785/1680 train_time:70452ms step_avg:89.75ms
step:786/1680 train_time:70544ms step_avg:89.75ms
step:787/1680 train_time:70636ms step_avg:89.75ms
step:788/1680 train_time:70726ms step_avg:89.75ms
step:789/1680 train_time:70816ms step_avg:89.75ms
step:790/1680 train_time:70906ms step_avg:89.75ms
step:791/1680 train_time:70995ms step_avg:89.75ms
step:792/1680 train_time:71084ms step_avg:89.75ms
step:793/1680 train_time:71174ms step_avg:89.75ms
step:794/1680 train_time:71264ms step_avg:89.75ms
step:795/1680 train_time:71353ms step_avg:89.75ms
step:796/1680 train_time:71443ms step_avg:89.75ms
step:797/1680 train_time:71534ms step_avg:89.75ms
step:798/1680 train_time:71625ms step_avg:89.76ms
step:799/1680 train_time:71715ms step_avg:89.76ms
step:800/1680 train_time:71805ms step_avg:89.76ms
step:801/1680 train_time:71897ms step_avg:89.76ms
step:802/1680 train_time:71987ms step_avg:89.76ms
step:803/1680 train_time:72077ms step_avg:89.76ms
step:804/1680 train_time:72166ms step_avg:89.76ms
step:805/1680 train_time:72256ms step_avg:89.76ms
step:806/1680 train_time:72347ms step_avg:89.76ms
step:807/1680 train_time:72438ms step_avg:89.76ms
step:808/1680 train_time:72529ms step_avg:89.76ms
step:809/1680 train_time:72620ms step_avg:89.76ms
step:810/1680 train_time:72711ms step_avg:89.77ms
step:811/1680 train_time:72801ms step_avg:89.77ms
step:812/1680 train_time:72892ms step_avg:89.77ms
step:813/1680 train_time:72983ms step_avg:89.77ms
step:814/1680 train_time:73072ms step_avg:89.77ms
step:815/1680 train_time:73162ms step_avg:89.77ms
step:816/1680 train_time:73252ms step_avg:89.77ms
step:817/1680 train_time:73342ms step_avg:89.77ms
step:818/1680 train_time:73433ms step_avg:89.77ms
step:819/1680 train_time:73523ms step_avg:89.77ms
step:820/1680 train_time:73613ms step_avg:89.77ms
step:821/1680 train_time:73703ms step_avg:89.77ms
step:822/1680 train_time:73793ms step_avg:89.77ms
step:823/1680 train_time:73883ms step_avg:89.77ms
step:824/1680 train_time:73974ms step_avg:89.77ms
step:825/1680 train_time:74066ms step_avg:89.78ms
step:826/1680 train_time:74155ms step_avg:89.78ms
step:827/1680 train_time:74245ms step_avg:89.78ms
step:828/1680 train_time:74335ms step_avg:89.78ms
step:829/1680 train_time:74425ms step_avg:89.78ms
step:830/1680 train_time:74515ms step_avg:89.78ms
step:831/1680 train_time:74606ms step_avg:89.78ms
step:832/1680 train_time:74696ms step_avg:89.78ms
step:833/1680 train_time:74787ms step_avg:89.78ms
step:834/1680 train_time:74877ms step_avg:89.78ms
step:835/1680 train_time:74967ms step_avg:89.78ms
step:836/1680 train_time:75059ms step_avg:89.78ms
step:837/1680 train_time:75148ms step_avg:89.78ms
step:838/1680 train_time:75238ms step_avg:89.78ms
step:839/1680 train_time:75328ms step_avg:89.78ms
step:840/1680 train_time:75417ms step_avg:89.78ms
step:841/1680 train_time:75509ms step_avg:89.78ms
step:842/1680 train_time:75599ms step_avg:89.78ms
step:843/1680 train_time:75689ms step_avg:89.78ms
step:844/1680 train_time:75779ms step_avg:89.79ms
step:845/1680 train_time:75870ms step_avg:89.79ms
step:846/1680 train_time:75961ms step_avg:89.79ms
step:847/1680 train_time:76052ms step_avg:89.79ms
step:848/1680 train_time:76142ms step_avg:89.79ms
step:849/1680 train_time:76232ms step_avg:89.79ms
step:850/1680 train_time:76322ms step_avg:89.79ms
step:851/1680 train_time:76413ms step_avg:89.79ms
step:852/1680 train_time:76502ms step_avg:89.79ms
step:853/1680 train_time:76593ms step_avg:89.79ms
step:854/1680 train_time:76683ms step_avg:89.79ms
step:855/1680 train_time:76778ms step_avg:89.80ms
step:856/1680 train_time:76862ms step_avg:89.79ms
step:857/1680 train_time:76953ms step_avg:89.79ms
step:858/1680 train_time:77043ms step_avg:89.79ms
step:859/1680 train_time:77134ms step_avg:89.79ms
step:860/1680 train_time:77223ms step_avg:89.79ms
step:861/1680 train_time:77313ms step_avg:89.79ms
step:862/1680 train_time:77403ms step_avg:89.79ms
step:863/1680 train_time:77493ms step_avg:89.79ms
step:864/1680 train_time:77582ms step_avg:89.79ms
step:865/1680 train_time:77672ms step_avg:89.79ms
step:866/1680 train_time:77763ms step_avg:89.80ms
step:867/1680 train_time:77853ms step_avg:89.80ms
step:868/1680 train_time:77943ms step_avg:89.80ms
step:869/1680 train_time:78033ms step_avg:89.80ms
step:870/1680 train_time:78123ms step_avg:89.80ms
step:871/1680 train_time:78213ms step_avg:89.80ms
step:872/1680 train_time:78303ms step_avg:89.80ms
step:873/1680 train_time:78393ms step_avg:89.80ms
step:874/1680 train_time:78483ms step_avg:89.80ms
step:875/1680 train_time:78573ms step_avg:89.80ms
step:875/1680 val_loss:3.5204 train_time:78664ms step_avg:89.90ms
step:876/1680 train_time:78689ms step_avg:89.83ms
step:877/1680 train_time:78757ms step_avg:89.80ms
step:878/1680 train_time:78853ms step_avg:89.81ms
step:879/1680 train_time:78944ms step_avg:89.81ms
step:880/1680 train_time:79033ms step_avg:89.81ms
step:881/1680 train_time:79122ms step_avg:89.81ms
step:882/1680 train_time:79211ms step_avg:89.81ms
step:883/1680 train_time:79300ms step_avg:89.81ms
step:884/1680 train_time:79389ms step_avg:89.81ms
step:885/1680 train_time:79478ms step_avg:89.81ms
step:886/1680 train_time:79569ms step_avg:89.81ms
step:887/1680 train_time:79659ms step_avg:89.81ms
step:888/1680 train_time:79751ms step_avg:89.81ms
step:889/1680 train_time:79844ms step_avg:89.81ms
step:890/1680 train_time:79935ms step_avg:89.81ms
step:891/1680 train_time:80025ms step_avg:89.81ms
step:892/1680 train_time:80115ms step_avg:89.82ms
step:893/1680 train_time:80206ms step_avg:89.82ms
step:894/1680 train_time:80295ms step_avg:89.82ms
step:895/1680 train_time:80385ms step_avg:89.82ms
step:896/1680 train_time:80474ms step_avg:89.82ms
step:897/1680 train_time:80565ms step_avg:89.82ms
step:898/1680 train_time:80655ms step_avg:89.82ms
step:899/1680 train_time:80749ms step_avg:89.82ms
step:900/1680 train_time:80841ms step_avg:89.82ms
step:901/1680 train_time:80932ms step_avg:89.82ms
step:902/1680 train_time:81022ms step_avg:89.82ms
step:903/1680 train_time:81112ms step_avg:89.82ms
step:904/1680 train_time:81202ms step_avg:89.83ms
step:905/1680 train_time:81292ms step_avg:89.83ms
step:906/1680 train_time:81381ms step_avg:89.82ms
step:907/1680 train_time:81470ms step_avg:89.82ms
step:908/1680 train_time:81560ms step_avg:89.82ms
step:909/1680 train_time:81650ms step_avg:89.82ms
step:910/1680 train_time:81744ms step_avg:89.83ms
step:911/1680 train_time:81835ms step_avg:89.83ms
step:912/1680 train_time:81926ms step_avg:89.83ms
step:913/1680 train_time:82016ms step_avg:89.83ms
step:914/1680 train_time:82107ms step_avg:89.83ms
step:915/1680 train_time:82198ms step_avg:89.83ms
step:916/1680 train_time:82287ms step_avg:89.83ms
step:917/1680 train_time:82376ms step_avg:89.83ms
step:918/1680 train_time:82466ms step_avg:89.83ms
step:919/1680 train_time:82555ms step_avg:89.83ms
step:920/1680 train_time:82646ms step_avg:89.83ms
step:921/1680 train_time:82737ms step_avg:89.83ms
step:922/1680 train_time:82828ms step_avg:89.83ms
step:923/1680 train_time:82918ms step_avg:89.84ms
step:924/1680 train_time:83008ms step_avg:89.84ms
step:925/1680 train_time:83098ms step_avg:89.84ms
step:926/1680 train_time:83190ms step_avg:89.84ms
step:927/1680 train_time:83279ms step_avg:89.84ms
step:928/1680 train_time:83370ms step_avg:89.84ms
step:929/1680 train_time:83459ms step_avg:89.84ms
step:930/1680 train_time:83550ms step_avg:89.84ms
step:931/1680 train_time:83640ms step_avg:89.84ms
step:932/1680 train_time:83730ms step_avg:89.84ms
step:933/1680 train_time:83821ms step_avg:89.84ms
step:934/1680 train_time:83911ms step_avg:89.84ms
step:935/1680 train_time:84003ms step_avg:89.84ms
step:936/1680 train_time:84094ms step_avg:89.84ms
step:937/1680 train_time:84185ms step_avg:89.85ms
step:938/1680 train_time:84276ms step_avg:89.85ms
step:939/1680 train_time:84366ms step_avg:89.85ms
step:940/1680 train_time:84457ms step_avg:89.85ms
step:941/1680 train_time:84547ms step_avg:89.85ms
step:942/1680 train_time:84637ms step_avg:89.85ms
step:943/1680 train_time:84727ms step_avg:89.85ms
step:944/1680 train_time:84818ms step_avg:89.85ms
step:945/1680 train_time:84907ms step_avg:89.85ms
step:946/1680 train_time:84998ms step_avg:89.85ms
step:947/1680 train_time:85089ms step_avg:89.85ms
step:948/1680 train_time:85180ms step_avg:89.85ms
step:949/1680 train_time:85271ms step_avg:89.85ms
step:950/1680 train_time:85361ms step_avg:89.85ms
step:951/1680 train_time:85451ms step_avg:89.85ms
step:952/1680 train_time:85542ms step_avg:89.85ms
step:953/1680 train_time:85631ms step_avg:89.85ms
step:954/1680 train_time:85722ms step_avg:89.86ms
step:955/1680 train_time:85812ms step_avg:89.86ms
step:956/1680 train_time:85903ms step_avg:89.86ms
step:957/1680 train_time:85993ms step_avg:89.86ms
step:958/1680 train_time:86083ms step_avg:89.86ms
step:959/1680 train_time:86174ms step_avg:89.86ms
step:960/1680 train_time:86264ms step_avg:89.86ms
step:961/1680 train_time:86355ms step_avg:89.86ms
step:962/1680 train_time:86446ms step_avg:89.86ms
step:963/1680 train_time:86535ms step_avg:89.86ms
step:964/1680 train_time:86626ms step_avg:89.86ms
step:965/1680 train_time:86716ms step_avg:89.86ms
step:966/1680 train_time:86807ms step_avg:89.86ms
step:967/1680 train_time:86897ms step_avg:89.86ms
step:968/1680 train_time:86987ms step_avg:89.86ms
step:969/1680 train_time:87078ms step_avg:89.86ms
step:970/1680 train_time:87168ms step_avg:89.86ms
step:971/1680 train_time:87258ms step_avg:89.86ms
step:972/1680 train_time:87349ms step_avg:89.86ms
step:973/1680 train_time:87441ms step_avg:89.87ms
step:974/1680 train_time:87530ms step_avg:89.87ms
step:975/1680 train_time:87620ms step_avg:89.87ms
step:976/1680 train_time:87711ms step_avg:89.87ms
step:977/1680 train_time:87801ms step_avg:89.87ms
step:978/1680 train_time:87891ms step_avg:89.87ms
step:979/1680 train_time:87981ms step_avg:89.87ms
step:980/1680 train_time:88072ms step_avg:89.87ms
step:981/1680 train_time:88163ms step_avg:89.87ms
step:982/1680 train_time:88253ms step_avg:89.87ms
step:983/1680 train_time:88345ms step_avg:89.87ms
step:984/1680 train_time:88436ms step_avg:89.87ms
step:985/1680 train_time:88526ms step_avg:89.87ms
step:986/1680 train_time:88616ms step_avg:89.87ms
step:987/1680 train_time:88707ms step_avg:89.88ms
step:988/1680 train_time:88796ms step_avg:89.87ms
step:989/1680 train_time:88886ms step_avg:89.87ms
step:990/1680 train_time:88976ms step_avg:89.88ms
step:991/1680 train_time:89067ms step_avg:89.88ms
step:992/1680 train_time:89157ms step_avg:89.88ms
step:993/1680 train_time:89247ms step_avg:89.88ms
step:994/1680 train_time:89337ms step_avg:89.88ms
step:995/1680 train_time:89428ms step_avg:89.88ms
step:996/1680 train_time:89519ms step_avg:89.88ms
step:997/1680 train_time:89609ms step_avg:89.88ms
step:998/1680 train_time:89700ms step_avg:89.88ms
step:999/1680 train_time:89790ms step_avg:89.88ms
step:1000/1680 train_time:89880ms step_avg:89.88ms
step:1000/1680 val_loss:3.4706 train_time:89971ms step_avg:89.97ms
step:1001/1680 train_time:89996ms step_avg:89.91ms
step:1002/1680 train_time:90064ms step_avg:89.88ms
step:1003/1680 train_time:90161ms step_avg:89.89ms
step:1004/1680 train_time:90251ms step_avg:89.89ms
step:1005/1680 train_time:90342ms step_avg:89.89ms
step:1006/1680 train_time:90432ms step_avg:89.89ms
step:1007/1680 train_time:90521ms step_avg:89.89ms
step:1008/1680 train_time:90611ms step_avg:89.89ms
step:1009/1680 train_time:90700ms step_avg:89.89ms
step:1010/1680 train_time:90790ms step_avg:89.89ms
step:1011/1680 train_time:90880ms step_avg:89.89ms
step:1012/1680 train_time:90971ms step_avg:89.89ms
step:1013/1680 train_time:91064ms step_avg:89.90ms
step:1014/1680 train_time:91157ms step_avg:89.90ms
step:1015/1680 train_time:91247ms step_avg:89.90ms
step:1016/1680 train_time:91338ms step_avg:89.90ms
step:1017/1680 train_time:91429ms step_avg:89.90ms
step:1018/1680 train_time:91519ms step_avg:89.90ms
step:1019/1680 train_time:91608ms step_avg:89.90ms
step:1020/1680 train_time:91697ms step_avg:89.90ms
step:1021/1680 train_time:91787ms step_avg:89.90ms
step:1022/1680 train_time:91877ms step_avg:89.90ms
step:1023/1680 train_time:91967ms step_avg:89.90ms
step:1024/1680 train_time:92059ms step_avg:89.90ms
step:1025/1680 train_time:92151ms step_avg:89.90ms
step:1026/1680 train_time:92242ms step_avg:89.90ms
step:1027/1680 train_time:92333ms step_avg:89.91ms
step:1028/1680 train_time:92423ms step_avg:89.91ms
step:1029/1680 train_time:92513ms step_avg:89.91ms
step:1030/1680 train_time:92603ms step_avg:89.91ms
step:1031/1680 train_time:92692ms step_avg:89.90ms
step:1032/1680 train_time:92782ms step_avg:89.90ms
step:1033/1680 train_time:92871ms step_avg:89.90ms
step:1034/1680 train_time:92962ms step_avg:89.91ms
step:1035/1680 train_time:93053ms step_avg:89.91ms
step:1036/1680 train_time:93143ms step_avg:89.91ms
step:1037/1680 train_time:93234ms step_avg:89.91ms
step:1038/1680 train_time:93324ms step_avg:89.91ms
step:1039/1680 train_time:93415ms step_avg:89.91ms
step:1040/1680 train_time:93505ms step_avg:89.91ms
step:1041/1680 train_time:93595ms step_avg:89.91ms
step:1042/1680 train_time:93684ms step_avg:89.91ms
step:1043/1680 train_time:93775ms step_avg:89.91ms
step:1044/1680 train_time:93865ms step_avg:89.91ms
step:1045/1680 train_time:93955ms step_avg:89.91ms
step:1046/1680 train_time:94045ms step_avg:89.91ms
step:1047/1680 train_time:94136ms step_avg:89.91ms
step:1048/1680 train_time:94226ms step_avg:89.91ms
step:1049/1680 train_time:94316ms step_avg:89.91ms
step:1050/1680 train_time:94407ms step_avg:89.91ms
step:1051/1680 train_time:94496ms step_avg:89.91ms
step:1052/1680 train_time:94586ms step_avg:89.91ms
step:1053/1680 train_time:94677ms step_avg:89.91ms
step:1054/1680 train_time:94766ms step_avg:89.91ms
step:1055/1680 train_time:94856ms step_avg:89.91ms
step:1056/1680 train_time:94946ms step_avg:89.91ms
step:1057/1680 train_time:95038ms step_avg:89.91ms
step:1058/1680 train_time:95129ms step_avg:89.91ms
step:1059/1680 train_time:95220ms step_avg:89.91ms
step:1060/1680 train_time:95310ms step_avg:89.92ms
step:1061/1680 train_time:95400ms step_avg:89.92ms
step:1062/1680 train_time:95491ms step_avg:89.92ms
step:1063/1680 train_time:95582ms step_avg:89.92ms
step:1064/1680 train_time:95671ms step_avg:89.92ms
step:1065/1680 train_time:95762ms step_avg:89.92ms
step:1066/1680 train_time:95852ms step_avg:89.92ms
step:1067/1680 train_time:95943ms step_avg:89.92ms
step:1068/1680 train_time:96034ms step_avg:89.92ms
step:1069/1680 train_time:96124ms step_avg:89.92ms
step:1070/1680 train_time:96215ms step_avg:89.92ms
step:1071/1680 train_time:96306ms step_avg:89.92ms
step:1072/1680 train_time:96397ms step_avg:89.92ms
step:1073/1680 train_time:96486ms step_avg:89.92ms
step:1074/1680 train_time:96577ms step_avg:89.92ms
step:1075/1680 train_time:96666ms step_avg:89.92ms
step:1076/1680 train_time:96756ms step_avg:89.92ms
step:1077/1680 train_time:96846ms step_avg:89.92ms
step:1078/1680 train_time:96936ms step_avg:89.92ms
step:1079/1680 train_time:97027ms step_avg:89.92ms
step:1080/1680 train_time:97117ms step_avg:89.92ms
step:1081/1680 train_time:97207ms step_avg:89.92ms
step:1082/1680 train_time:97298ms step_avg:89.92ms
step:1083/1680 train_time:97387ms step_avg:89.92ms
step:1084/1680 train_time:97479ms step_avg:89.93ms
step:1085/1680 train_time:97568ms step_avg:89.92ms
step:1086/1680 train_time:97659ms step_avg:89.93ms
step:1087/1680 train_time:97749ms step_avg:89.93ms
step:1088/1680 train_time:97840ms step_avg:89.93ms
step:1089/1680 train_time:97929ms step_avg:89.93ms
step:1090/1680 train_time:98019ms step_avg:89.93ms
step:1091/1680 train_time:98109ms step_avg:89.93ms
step:1092/1680 train_time:98200ms step_avg:89.93ms
step:1093/1680 train_time:98291ms step_avg:89.93ms
step:1094/1680 train_time:98381ms step_avg:89.93ms
step:1095/1680 train_time:98472ms step_avg:89.93ms
step:1096/1680 train_time:98563ms step_avg:89.93ms
step:1097/1680 train_time:98654ms step_avg:89.93ms
step:1098/1680 train_time:98744ms step_avg:89.93ms
step:1099/1680 train_time:98836ms step_avg:89.93ms
step:1100/1680 train_time:98926ms step_avg:89.93ms
step:1101/1680 train_time:99017ms step_avg:89.93ms
step:1102/1680 train_time:99108ms step_avg:89.93ms
step:1103/1680 train_time:99199ms step_avg:89.94ms
step:1104/1680 train_time:99290ms step_avg:89.94ms
step:1105/1680 train_time:99381ms step_avg:89.94ms
step:1106/1680 train_time:99472ms step_avg:89.94ms
step:1107/1680 train_time:99569ms step_avg:89.95ms
step:1108/1680 train_time:99656ms step_avg:89.94ms
step:1109/1680 train_time:99747ms step_avg:89.94ms
step:1110/1680 train_time:99837ms step_avg:89.94ms
step:1111/1680 train_time:99928ms step_avg:89.94ms
step:1112/1680 train_time:100018ms step_avg:89.94ms
step:1113/1680 train_time:100109ms step_avg:89.95ms
step:1114/1680 train_time:100200ms step_avg:89.95ms
step:1115/1680 train_time:100291ms step_avg:89.95ms
step:1116/1680 train_time:100382ms step_avg:89.95ms
step:1117/1680 train_time:100473ms step_avg:89.95ms
step:1118/1680 train_time:100565ms step_avg:89.95ms
step:1119/1680 train_time:100655ms step_avg:89.95ms
step:1120/1680 train_time:100746ms step_avg:89.95ms
step:1121/1680 train_time:100836ms step_avg:89.95ms
step:1122/1680 train_time:100927ms step_avg:89.95ms
step:1123/1680 train_time:101017ms step_avg:89.95ms
step:1124/1680 train_time:101109ms step_avg:89.95ms
step:1125/1680 train_time:101200ms step_avg:89.96ms
step:1125/1680 val_loss:3.4175 train_time:101292ms step_avg:90.04ms
step:1126/1680 train_time:101319ms step_avg:89.98ms
step:1127/1680 train_time:101388ms step_avg:89.96ms
step:1128/1680 train_time:101487ms step_avg:89.97ms
step:1129/1680 train_time:101577ms step_avg:89.97ms
step:1130/1680 train_time:101667ms step_avg:89.97ms
step:1131/1680 train_time:101757ms step_avg:89.97ms
step:1132/1680 train_time:101847ms step_avg:89.97ms
step:1133/1680 train_time:101937ms step_avg:89.97ms
step:1134/1680 train_time:102026ms step_avg:89.97ms
step:1135/1680 train_time:102116ms step_avg:89.97ms
step:1136/1680 train_time:102208ms step_avg:89.97ms
step:1137/1680 train_time:102304ms step_avg:89.98ms
step:1138/1680 train_time:102396ms step_avg:89.98ms
step:1139/1680 train_time:102488ms step_avg:89.98ms
step:1140/1680 train_time:102582ms step_avg:89.98ms
step:1141/1680 train_time:102668ms step_avg:89.98ms
step:1142/1680 train_time:102758ms step_avg:89.98ms
step:1143/1680 train_time:102848ms step_avg:89.98ms
step:1144/1680 train_time:102938ms step_avg:89.98ms
step:1145/1680 train_time:103027ms step_avg:89.98ms
step:1146/1680 train_time:103117ms step_avg:89.98ms
step:1147/1680 train_time:103208ms step_avg:89.98ms
step:1148/1680 train_time:103300ms step_avg:89.98ms
step:1149/1680 train_time:103392ms step_avg:89.98ms
step:1150/1680 train_time:103482ms step_avg:89.98ms
step:1151/1680 train_time:103573ms step_avg:89.99ms
step:1152/1680 train_time:103664ms step_avg:89.99ms
step:1153/1680 train_time:103754ms step_avg:89.99ms
step:1154/1680 train_time:103844ms step_avg:89.99ms
step:1155/1680 train_time:103935ms step_avg:89.99ms
step:1156/1680 train_time:104024ms step_avg:89.99ms
step:1157/1680 train_time:104115ms step_avg:89.99ms
step:1158/1680 train_time:104208ms step_avg:89.99ms
step:1159/1680 train_time:104299ms step_avg:89.99ms
step:1160/1680 train_time:104389ms step_avg:89.99ms
step:1161/1680 train_time:104480ms step_avg:89.99ms
step:1162/1680 train_time:104572ms step_avg:89.99ms
step:1163/1680 train_time:104662ms step_avg:89.99ms
step:1164/1680 train_time:104753ms step_avg:89.99ms
step:1165/1680 train_time:104843ms step_avg:89.99ms
step:1166/1680 train_time:104933ms step_avg:89.99ms
step:1167/1680 train_time:105024ms step_avg:89.99ms
step:1168/1680 train_time:105115ms step_avg:90.00ms
step:1169/1680 train_time:105206ms step_avg:90.00ms
step:1170/1680 train_time:105296ms step_avg:90.00ms
step:1171/1680 train_time:105387ms step_avg:90.00ms
step:1172/1680 train_time:105478ms step_avg:90.00ms
step:1173/1680 train_time:105569ms step_avg:90.00ms
step:1174/1680 train_time:105659ms step_avg:90.00ms
step:1175/1680 train_time:105750ms step_avg:90.00ms
step:1176/1680 train_time:105840ms step_avg:90.00ms
step:1177/1680 train_time:105931ms step_avg:90.00ms
step:1178/1680 train_time:106021ms step_avg:90.00ms
step:1179/1680 train_time:106111ms step_avg:90.00ms
step:1180/1680 train_time:106203ms step_avg:90.00ms
step:1181/1680 train_time:106294ms step_avg:90.00ms
step:1182/1680 train_time:106385ms step_avg:90.00ms
step:1183/1680 train_time:106476ms step_avg:90.00ms
step:1184/1680 train_time:106567ms step_avg:90.01ms
step:1185/1680 train_time:106658ms step_avg:90.01ms
step:1186/1680 train_time:106748ms step_avg:90.01ms
step:1187/1680 train_time:106839ms step_avg:90.01ms
step:1188/1680 train_time:106930ms step_avg:90.01ms
step:1189/1680 train_time:107020ms step_avg:90.01ms
step:1190/1680 train_time:107111ms step_avg:90.01ms
step:1191/1680 train_time:107203ms step_avg:90.01ms
step:1192/1680 train_time:107298ms step_avg:90.02ms
step:1193/1680 train_time:107385ms step_avg:90.01ms
step:1194/1680 train_time:107476ms step_avg:90.01ms
step:1195/1680 train_time:107566ms step_avg:90.01ms
step:1196/1680 train_time:107656ms step_avg:90.01ms
step:1197/1680 train_time:107746ms step_avg:90.01ms
step:1198/1680 train_time:107836ms step_avg:90.01ms
step:1199/1680 train_time:107927ms step_avg:90.01ms
step:1200/1680 train_time:108017ms step_avg:90.01ms
step:1201/1680 train_time:108107ms step_avg:90.01ms
step:1202/1680 train_time:108198ms step_avg:90.02ms
step:1203/1680 train_time:108288ms step_avg:90.02ms
step:1204/1680 train_time:108379ms step_avg:90.02ms
step:1205/1680 train_time:108470ms step_avg:90.02ms
step:1206/1680 train_time:108561ms step_avg:90.02ms
step:1207/1680 train_time:108652ms step_avg:90.02ms
step:1208/1680 train_time:108743ms step_avg:90.02ms
step:1209/1680 train_time:108833ms step_avg:90.02ms
step:1210/1680 train_time:108924ms step_avg:90.02ms
step:1211/1680 train_time:109014ms step_avg:90.02ms
step:1212/1680 train_time:109105ms step_avg:90.02ms
step:1213/1680 train_time:109196ms step_avg:90.02ms
step:1214/1680 train_time:109286ms step_avg:90.02ms
step:1215/1680 train_time:109376ms step_avg:90.02ms
step:1216/1680 train_time:109467ms step_avg:90.02ms
step:1217/1680 train_time:109558ms step_avg:90.02ms
step:1218/1680 train_time:109648ms step_avg:90.02ms
step:1219/1680 train_time:109739ms step_avg:90.02ms
step:1220/1680 train_time:109830ms step_avg:90.02ms
step:1221/1680 train_time:109920ms step_avg:90.02ms
step:1222/1680 train_time:110010ms step_avg:90.02ms
step:1223/1680 train_time:110101ms step_avg:90.03ms
step:1224/1680 train_time:110193ms step_avg:90.03ms
step:1225/1680 train_time:110284ms step_avg:90.03ms
step:1226/1680 train_time:110375ms step_avg:90.03ms
step:1227/1680 train_time:110466ms step_avg:90.03ms
step:1228/1680 train_time:110557ms step_avg:90.03ms
step:1229/1680 train_time:110648ms step_avg:90.03ms
step:1230/1680 train_time:110739ms step_avg:90.03ms
step:1231/1680 train_time:110829ms step_avg:90.03ms
step:1232/1680 train_time:110920ms step_avg:90.03ms
step:1233/1680 train_time:111011ms step_avg:90.03ms
step:1234/1680 train_time:111102ms step_avg:90.03ms
step:1235/1680 train_time:111192ms step_avg:90.03ms
step:1236/1680 train_time:111282ms step_avg:90.03ms
step:1237/1680 train_time:111374ms step_avg:90.04ms
step:1238/1680 train_time:111465ms step_avg:90.04ms
step:1239/1680 train_time:111555ms step_avg:90.04ms
step:1240/1680 train_time:111646ms step_avg:90.04ms
step:1241/1680 train_time:111736ms step_avg:90.04ms
step:1242/1680 train_time:111826ms step_avg:90.04ms
step:1243/1680 train_time:111918ms step_avg:90.04ms
step:1244/1680 train_time:112008ms step_avg:90.04ms
step:1245/1680 train_time:112099ms step_avg:90.04ms
step:1246/1680 train_time:112189ms step_avg:90.04ms
step:1247/1680 train_time:112280ms step_avg:90.04ms
step:1248/1680 train_time:112371ms step_avg:90.04ms
step:1249/1680 train_time:112461ms step_avg:90.04ms
step:1250/1680 train_time:112554ms step_avg:90.04ms
step:1250/1680 val_loss:3.3796 train_time:112647ms step_avg:90.12ms
step:1251/1680 train_time:112672ms step_avg:90.07ms
step:1252/1680 train_time:112742ms step_avg:90.05ms
step:1253/1680 train_time:112836ms step_avg:90.05ms
step:1254/1680 train_time:112927ms step_avg:90.05ms
step:1255/1680 train_time:113016ms step_avg:90.05ms
step:1256/1680 train_time:113106ms step_avg:90.05ms
step:1257/1680 train_time:113195ms step_avg:90.05ms
step:1258/1680 train_time:113284ms step_avg:90.05ms
step:1259/1680 train_time:113374ms step_avg:90.05ms
step:1260/1680 train_time:113464ms step_avg:90.05ms
step:1261/1680 train_time:113555ms step_avg:90.05ms
step:1262/1680 train_time:113648ms step_avg:90.05ms
step:1263/1680 train_time:113742ms step_avg:90.06ms
step:1264/1680 train_time:113834ms step_avg:90.06ms
step:1265/1680 train_time:113925ms step_avg:90.06ms
step:1266/1680 train_time:114015ms step_avg:90.06ms
step:1267/1680 train_time:114105ms step_avg:90.06ms
step:1268/1680 train_time:114195ms step_avg:90.06ms
step:1269/1680 train_time:114284ms step_avg:90.06ms
step:1270/1680 train_time:114375ms step_avg:90.06ms
step:1271/1680 train_time:114465ms step_avg:90.06ms
step:1272/1680 train_time:114555ms step_avg:90.06ms
step:1273/1680 train_time:114647ms step_avg:90.06ms
step:1274/1680 train_time:114739ms step_avg:90.06ms
step:1275/1680 train_time:114830ms step_avg:90.06ms
step:1276/1680 train_time:114922ms step_avg:90.06ms
step:1277/1680 train_time:115012ms step_avg:90.06ms
step:1278/1680 train_time:115102ms step_avg:90.06ms
step:1279/1680 train_time:115192ms step_avg:90.06ms
step:1280/1680 train_time:115283ms step_avg:90.06ms
step:1281/1680 train_time:115373ms step_avg:90.06ms
step:1282/1680 train_time:115463ms step_avg:90.06ms
step:1283/1680 train_time:115553ms step_avg:90.06ms
step:1284/1680 train_time:115645ms step_avg:90.07ms
step:1285/1680 train_time:115738ms step_avg:90.07ms
step:1286/1680 train_time:115829ms step_avg:90.07ms
step:1287/1680 train_time:115921ms step_avg:90.07ms
step:1288/1680 train_time:116012ms step_avg:90.07ms
step:1289/1680 train_time:116103ms step_avg:90.07ms
step:1290/1680 train_time:116193ms step_avg:90.07ms
step:1291/1680 train_time:116284ms step_avg:90.07ms
step:1292/1680 train_time:116374ms step_avg:90.07ms
step:1293/1680 train_time:116464ms step_avg:90.07ms
step:1294/1680 train_time:116554ms step_avg:90.07ms
step:1295/1680 train_time:116646ms step_avg:90.07ms
step:1296/1680 train_time:116737ms step_avg:90.07ms
step:1297/1680 train_time:116829ms step_avg:90.08ms
step:1298/1680 train_time:116921ms step_avg:90.08ms
step:1299/1680 train_time:117013ms step_avg:90.08ms
step:1300/1680 train_time:117103ms step_avg:90.08ms
step:1301/1680 train_time:117194ms step_avg:90.08ms
step:1302/1680 train_time:117284ms step_avg:90.08ms
step:1303/1680 train_time:117374ms step_avg:90.08ms
step:1304/1680 train_time:117465ms step_avg:90.08ms
step:1305/1680 train_time:117555ms step_avg:90.08ms
step:1306/1680 train_time:117646ms step_avg:90.08ms
step:1307/1680 train_time:117738ms step_avg:90.08ms
step:1308/1680 train_time:117829ms step_avg:90.08ms
step:1309/1680 train_time:117921ms step_avg:90.08ms
step:1310/1680 train_time:118014ms step_avg:90.09ms
step:1311/1680 train_time:118105ms step_avg:90.09ms
step:1312/1680 train_time:118195ms step_avg:90.09ms
step:1313/1680 train_time:118286ms step_avg:90.09ms
step:1314/1680 train_time:118376ms step_avg:90.09ms
step:1315/1680 train_time:118467ms step_avg:90.09ms
step:1316/1680 train_time:118557ms step_avg:90.09ms
step:1317/1680 train_time:118647ms step_avg:90.09ms
step:1318/1680 train_time:118743ms step_avg:90.09ms
step:1319/1680 train_time:118829ms step_avg:90.09ms
step:1320/1680 train_time:118920ms step_avg:90.09ms
step:1321/1680 train_time:119012ms step_avg:90.09ms
step:1322/1680 train_time:119103ms step_avg:90.09ms
step:1323/1680 train_time:119194ms step_avg:90.09ms
step:1324/1680 train_time:119284ms step_avg:90.09ms
step:1325/1680 train_time:119375ms step_avg:90.09ms
step:1326/1680 train_time:119466ms step_avg:90.10ms
step:1327/1680 train_time:119556ms step_avg:90.10ms
step:1328/1680 train_time:119647ms step_avg:90.10ms
step:1329/1680 train_time:119739ms step_avg:90.10ms
step:1330/1680 train_time:119829ms step_avg:90.10ms
step:1331/1680 train_time:119920ms step_avg:90.10ms
step:1332/1680 train_time:120012ms step_avg:90.10ms
step:1333/1680 train_time:120102ms step_avg:90.10ms
step:1334/1680 train_time:120192ms step_avg:90.10ms
step:1335/1680 train_time:120283ms step_avg:90.10ms
step:1336/1680 train_time:120374ms step_avg:90.10ms
step:1337/1680 train_time:120465ms step_avg:90.10ms
step:1338/1680 train_time:120555ms step_avg:90.10ms
step:1339/1680 train_time:120645ms step_avg:90.10ms
step:1340/1680 train_time:120735ms step_avg:90.10ms
step:1341/1680 train_time:120826ms step_avg:90.10ms
step:1342/1680 train_time:120917ms step_avg:90.10ms
step:1343/1680 train_time:121009ms step_avg:90.10ms
step:1344/1680 train_time:121100ms step_avg:90.10ms
step:1345/1680 train_time:121191ms step_avg:90.10ms
step:1346/1680 train_time:121281ms step_avg:90.10ms
step:1347/1680 train_time:121372ms step_avg:90.11ms
step:1348/1680 train_time:121463ms step_avg:90.11ms
step:1349/1680 train_time:121557ms step_avg:90.11ms
step:1350/1680 train_time:121644ms step_avg:90.11ms
step:1351/1680 train_time:121735ms step_avg:90.11ms
step:1352/1680 train_time:121826ms step_avg:90.11ms
step:1353/1680 train_time:121917ms step_avg:90.11ms
step:1354/1680 train_time:122008ms step_avg:90.11ms
step:1355/1680 train_time:122099ms step_avg:90.11ms
step:1356/1680 train_time:122190ms step_avg:90.11ms
step:1357/1680 train_time:122281ms step_avg:90.11ms
step:1358/1680 train_time:122372ms step_avg:90.11ms
step:1359/1680 train_time:122462ms step_avg:90.11ms
step:1360/1680 train_time:122552ms step_avg:90.11ms
step:1361/1680 train_time:122643ms step_avg:90.11ms
step:1362/1680 train_time:122733ms step_avg:90.11ms
step:1363/1680 train_time:122825ms step_avg:90.11ms
step:1364/1680 train_time:122917ms step_avg:90.11ms
step:1365/1680 train_time:123007ms step_avg:90.12ms
step:1366/1680 train_time:123099ms step_avg:90.12ms
step:1367/1680 train_time:123189ms step_avg:90.12ms
step:1368/1680 train_time:123280ms step_avg:90.12ms
step:1369/1680 train_time:123372ms step_avg:90.12ms
step:1370/1680 train_time:123462ms step_avg:90.12ms
step:1371/1680 train_time:123553ms step_avg:90.12ms
step:1372/1680 train_time:123643ms step_avg:90.12ms
step:1373/1680 train_time:123734ms step_avg:90.12ms
step:1374/1680 train_time:123824ms step_avg:90.12ms
step:1375/1680 train_time:123915ms step_avg:90.12ms
step:1375/1680 val_loss:3.3448 train_time:124007ms step_avg:90.19ms
step:1376/1680 train_time:124032ms step_avg:90.14ms
step:1377/1680 train_time:124104ms step_avg:90.13ms
step:1378/1680 train_time:124200ms step_avg:90.13ms
step:1379/1680 train_time:124291ms step_avg:90.13ms
step:1380/1680 train_time:124381ms step_avg:90.13ms
step:1381/1680 train_time:124471ms step_avg:90.13ms
step:1382/1680 train_time:124560ms step_avg:90.13ms
step:1383/1680 train_time:124650ms step_avg:90.13ms
step:1384/1680 train_time:124740ms step_avg:90.13ms
step:1385/1680 train_time:124829ms step_avg:90.13ms
step:1386/1680 train_time:124919ms step_avg:90.13ms
step:1387/1680 train_time:125010ms step_avg:90.13ms
step:1388/1680 train_time:125105ms step_avg:90.13ms
step:1389/1680 train_time:125200ms step_avg:90.14ms
step:1390/1680 train_time:125296ms step_avg:90.14ms
step:1391/1680 train_time:125383ms step_avg:90.14ms
step:1392/1680 train_time:125473ms step_avg:90.14ms
step:1393/1680 train_time:125563ms step_avg:90.14ms
step:1394/1680 train_time:125653ms step_avg:90.14ms
step:1395/1680 train_time:125742ms step_avg:90.14ms
step:1396/1680 train_time:125832ms step_avg:90.14ms
step:1397/1680 train_time:125922ms step_avg:90.14ms
step:1398/1680 train_time:126013ms step_avg:90.14ms
step:1399/1680 train_time:126105ms step_avg:90.14ms
step:1400/1680 train_time:126197ms step_avg:90.14ms
step:1401/1680 train_time:126290ms step_avg:90.14ms
step:1402/1680 train_time:126381ms step_avg:90.14ms
step:1403/1680 train_time:126473ms step_avg:90.14ms
step:1404/1680 train_time:126563ms step_avg:90.14ms
step:1405/1680 train_time:126654ms step_avg:90.15ms
step:1406/1680 train_time:126743ms step_avg:90.14ms
step:1407/1680 train_time:126832ms step_avg:90.14ms
step:1408/1680 train_time:126923ms step_avg:90.14ms
step:1409/1680 train_time:127014ms step_avg:90.14ms
step:1410/1680 train_time:127106ms step_avg:90.15ms
step:1411/1680 train_time:127198ms step_avg:90.15ms
step:1412/1680 train_time:127291ms step_avg:90.15ms
step:1413/1680 train_time:127383ms step_avg:90.15ms
step:1414/1680 train_time:127474ms step_avg:90.15ms
step:1415/1680 train_time:127564ms step_avg:90.15ms
step:1416/1680 train_time:127655ms step_avg:90.15ms
step:1417/1680 train_time:127745ms step_avg:90.15ms
step:1418/1680 train_time:127835ms step_avg:90.15ms
step:1419/1680 train_time:127925ms step_avg:90.15ms
step:1420/1680 train_time:128015ms step_avg:90.15ms
step:1421/1680 train_time:128108ms step_avg:90.15ms
step:1422/1680 train_time:128200ms step_avg:90.15ms
step:1423/1680 train_time:128291ms step_avg:90.16ms
step:1424/1680 train_time:128382ms step_avg:90.16ms
step:1425/1680 train_time:128474ms step_avg:90.16ms
step:1426/1680 train_time:128564ms step_avg:90.16ms
step:1427/1680 train_time:128656ms step_avg:90.16ms
step:1428/1680 train_time:128746ms step_avg:90.16ms
step:1429/1680 train_time:128837ms step_avg:90.16ms
step:1430/1680 train_time:128928ms step_avg:90.16ms
step:1431/1680 train_time:129018ms step_avg:90.16ms
step:1432/1680 train_time:129109ms step_avg:90.16ms
step:1433/1680 train_time:129201ms step_avg:90.16ms
step:1434/1680 train_time:129293ms step_avg:90.16ms
step:1435/1680 train_time:129385ms step_avg:90.16ms
step:1436/1680 train_time:129477ms step_avg:90.17ms
step:1437/1680 train_time:129567ms step_avg:90.17ms
step:1438/1680 train_time:129658ms step_avg:90.17ms
step:1439/1680 train_time:129748ms step_avg:90.17ms
step:1440/1680 train_time:129839ms step_avg:90.17ms
step:1441/1680 train_time:129930ms step_avg:90.17ms
step:1442/1680 train_time:130021ms step_avg:90.17ms
step:1443/1680 train_time:130112ms step_avg:90.17ms
step:1444/1680 train_time:130203ms step_avg:90.17ms
step:1445/1680 train_time:130294ms step_avg:90.17ms
step:1446/1680 train_time:130386ms step_avg:90.17ms
step:1447/1680 train_time:130478ms step_avg:90.17ms
step:1448/1680 train_time:130568ms step_avg:90.17ms
step:1449/1680 train_time:130660ms step_avg:90.17ms
step:1450/1680 train_time:130751ms step_avg:90.17ms
step:1451/1680 train_time:130841ms step_avg:90.17ms
step:1452/1680 train_time:130931ms step_avg:90.17ms
step:1453/1680 train_time:131022ms step_avg:90.17ms
step:1454/1680 train_time:131112ms step_avg:90.17ms
step:1455/1680 train_time:131203ms step_avg:90.17ms
step:1456/1680 train_time:131294ms step_avg:90.17ms
step:1457/1680 train_time:131386ms step_avg:90.18ms
step:1458/1680 train_time:131477ms step_avg:90.18ms
step:1459/1680 train_time:131568ms step_avg:90.18ms
step:1460/1680 train_time:131660ms step_avg:90.18ms
step:1461/1680 train_time:131750ms step_avg:90.18ms
step:1462/1680 train_time:131841ms step_avg:90.18ms
step:1463/1680 train_time:131932ms step_avg:90.18ms
step:1464/1680 train_time:132023ms step_avg:90.18ms
step:1465/1680 train_time:132115ms step_avg:90.18ms
step:1466/1680 train_time:132205ms step_avg:90.18ms
step:1467/1680 train_time:132297ms step_avg:90.18ms
step:1468/1680 train_time:132387ms step_avg:90.18ms
step:1469/1680 train_time:132479ms step_avg:90.18ms
step:1470/1680 train_time:132570ms step_avg:90.18ms
step:1471/1680 train_time:132661ms step_avg:90.18ms
step:1472/1680 train_time:132752ms step_avg:90.18ms
step:1473/1680 train_time:132842ms step_avg:90.18ms
step:1474/1680 train_time:132933ms step_avg:90.19ms
step:1475/1680 train_time:133024ms step_avg:90.19ms
step:1476/1680 train_time:133114ms step_avg:90.19ms
step:1477/1680 train_time:133205ms step_avg:90.19ms
step:1478/1680 train_time:133296ms step_avg:90.19ms
step:1479/1680 train_time:133387ms step_avg:90.19ms
step:1480/1680 train_time:133479ms step_avg:90.19ms
step:1481/1680 train_time:133570ms step_avg:90.19ms
step:1482/1680 train_time:133662ms step_avg:90.19ms
step:1483/1680 train_time:133752ms step_avg:90.19ms
step:1484/1680 train_time:133843ms step_avg:90.19ms
step:1485/1680 train_time:133935ms step_avg:90.19ms
step:1486/1680 train_time:134025ms step_avg:90.19ms
step:1487/1680 train_time:134116ms step_avg:90.19ms
step:1488/1680 train_time:134206ms step_avg:90.19ms
step:1489/1680 train_time:134296ms step_avg:90.19ms
step:1490/1680 train_time:134387ms step_avg:90.19ms
step:1491/1680 train_time:134478ms step_avg:90.19ms
step:1492/1680 train_time:134569ms step_avg:90.19ms
step:1493/1680 train_time:134660ms step_avg:90.19ms
step:1494/1680 train_time:134750ms step_avg:90.19ms
step:1495/1680 train_time:134841ms step_avg:90.19ms
step:1496/1680 train_time:134932ms step_avg:90.19ms
step:1497/1680 train_time:135023ms step_avg:90.20ms
step:1498/1680 train_time:135113ms step_avg:90.20ms
step:1499/1680 train_time:135204ms step_avg:90.20ms
step:1500/1680 train_time:135295ms step_avg:90.20ms
step:1500/1680 val_loss:3.3150 train_time:135386ms step_avg:90.26ms
step:1501/1680 train_time:135411ms step_avg:90.21ms
step:1502/1680 train_time:135482ms step_avg:90.20ms
step:1503/1680 train_time:135580ms step_avg:90.21ms
step:1504/1680 train_time:135671ms step_avg:90.21ms
step:1505/1680 train_time:135761ms step_avg:90.21ms
step:1506/1680 train_time:135851ms step_avg:90.21ms
step:1507/1680 train_time:135941ms step_avg:90.21ms
step:1508/1680 train_time:136033ms step_avg:90.21ms
step:1509/1680 train_time:136119ms step_avg:90.20ms
step:1510/1680 train_time:136209ms step_avg:90.20ms
step:1511/1680 train_time:136299ms step_avg:90.20ms
step:1512/1680 train_time:136392ms step_avg:90.21ms
step:1513/1680 train_time:136485ms step_avg:90.21ms
step:1514/1680 train_time:136578ms step_avg:90.21ms
step:1515/1680 train_time:136669ms step_avg:90.21ms
step:1516/1680 train_time:136759ms step_avg:90.21ms
step:1517/1680 train_time:136850ms step_avg:90.21ms
step:1518/1680 train_time:136940ms step_avg:90.21ms
step:1519/1680 train_time:137029ms step_avg:90.21ms
step:1520/1680 train_time:137119ms step_avg:90.21ms
step:1521/1680 train_time:137209ms step_avg:90.21ms
step:1522/1680 train_time:137299ms step_avg:90.21ms
step:1523/1680 train_time:137390ms step_avg:90.21ms
step:1524/1680 train_time:137482ms step_avg:90.21ms
step:1525/1680 train_time:137575ms step_avg:90.21ms
step:1526/1680 train_time:137667ms step_avg:90.21ms
step:1527/1680 train_time:137759ms step_avg:90.22ms
step:1528/1680 train_time:137849ms step_avg:90.22ms
step:1529/1680 train_time:137940ms step_avg:90.22ms
step:1530/1680 train_time:138030ms step_avg:90.22ms
step:1531/1680 train_time:138120ms step_avg:90.22ms
step:1532/1680 train_time:138210ms step_avg:90.22ms
step:1533/1680 train_time:138301ms step_avg:90.22ms
step:1534/1680 train_time:138392ms step_avg:90.22ms
step:1535/1680 train_time:138483ms step_avg:90.22ms
step:1536/1680 train_time:138576ms step_avg:90.22ms
step:1537/1680 train_time:138668ms step_avg:90.22ms
step:1538/1680 train_time:138759ms step_avg:90.22ms
step:1539/1680 train_time:138851ms step_avg:90.22ms
step:1540/1680 train_time:138943ms step_avg:90.22ms
step:1541/1680 train_time:139031ms step_avg:90.22ms
step:1542/1680 train_time:139121ms step_avg:90.22ms
step:1543/1680 train_time:139211ms step_avg:90.22ms
step:1544/1680 train_time:139302ms step_avg:90.22ms
step:1545/1680 train_time:139393ms step_avg:90.22ms
step:1546/1680 train_time:139484ms step_avg:90.22ms
step:1547/1680 train_time:139575ms step_avg:90.22ms
step:1548/1680 train_time:139667ms step_avg:90.22ms
step:1549/1680 train_time:139759ms step_avg:90.23ms
step:1550/1680 train_time:139849ms step_avg:90.23ms
step:1551/1680 train_time:139939ms step_avg:90.23ms
step:1552/1680 train_time:140030ms step_avg:90.23ms
step:1553/1680 train_time:140120ms step_avg:90.23ms
step:1554/1680 train_time:140211ms step_avg:90.23ms
step:1555/1680 train_time:140302ms step_avg:90.23ms
step:1556/1680 train_time:140394ms step_avg:90.23ms
step:1557/1680 train_time:140484ms step_avg:90.23ms
step:1558/1680 train_time:140575ms step_avg:90.23ms
step:1559/1680 train_time:140668ms step_avg:90.23ms
step:1560/1680 train_time:140760ms step_avg:90.23ms
step:1561/1680 train_time:140850ms step_avg:90.23ms
step:1562/1680 train_time:140941ms step_avg:90.23ms
step:1563/1680 train_time:141031ms step_avg:90.23ms
step:1564/1680 train_time:141123ms step_avg:90.23ms
step:1565/1680 train_time:141213ms step_avg:90.23ms
step:1566/1680 train_time:141305ms step_avg:90.23ms
step:1567/1680 train_time:141396ms step_avg:90.23ms
step:1568/1680 train_time:141486ms step_avg:90.23ms
step:1569/1680 train_time:141579ms step_avg:90.24ms
step:1570/1680 train_time:141670ms step_avg:90.24ms
step:1571/1680 train_time:141761ms step_avg:90.24ms
step:1572/1680 train_time:141852ms step_avg:90.24ms
step:1573/1680 train_time:141942ms step_avg:90.24ms
step:1574/1680 train_time:142033ms step_avg:90.24ms
step:1575/1680 train_time:142124ms step_avg:90.24ms
step:1576/1680 train_time:142215ms step_avg:90.24ms
step:1577/1680 train_time:142305ms step_avg:90.24ms
step:1578/1680 train_time:142397ms step_avg:90.24ms
step:1579/1680 train_time:142489ms step_avg:90.24ms
step:1580/1680 train_time:142580ms step_avg:90.24ms
step:1581/1680 train_time:142671ms step_avg:90.24ms
step:1582/1680 train_time:142762ms step_avg:90.24ms
step:1583/1680 train_time:142853ms step_avg:90.24ms
step:1584/1680 train_time:142943ms step_avg:90.24ms
step:1585/1680 train_time:143033ms step_avg:90.24ms
step:1586/1680 train_time:143124ms step_avg:90.24ms
step:1587/1680 train_time:143215ms step_avg:90.24ms
step:1588/1680 train_time:143305ms step_avg:90.24ms
step:1589/1680 train_time:143396ms step_avg:90.24ms
step:1590/1680 train_time:143487ms step_avg:90.24ms
step:1591/1680 train_time:143579ms step_avg:90.24ms
step:1592/1680 train_time:143669ms step_avg:90.24ms
step:1593/1680 train_time:143760ms step_avg:90.24ms
step:1594/1680 train_time:143851ms step_avg:90.25ms
step:1595/1680 train_time:143941ms step_avg:90.25ms
step:1596/1680 train_time:144031ms step_avg:90.25ms
step:1597/1680 train_time:144122ms step_avg:90.25ms
step:1598/1680 train_time:144212ms step_avg:90.25ms
step:1599/1680 train_time:144302ms step_avg:90.24ms
step:1600/1680 train_time:144392ms step_avg:90.25ms
step:1601/1680 train_time:144483ms step_avg:90.25ms
step:1602/1680 train_time:144574ms step_avg:90.25ms
step:1603/1680 train_time:144664ms step_avg:90.25ms
step:1604/1680 train_time:144755ms step_avg:90.25ms
step:1605/1680 train_time:144846ms step_avg:90.25ms
step:1606/1680 train_time:144938ms step_avg:90.25ms
step:1607/1680 train_time:145029ms step_avg:90.25ms
step:1608/1680 train_time:145120ms step_avg:90.25ms
step:1609/1680 train_time:145211ms step_avg:90.25ms
step:1610/1680 train_time:145302ms step_avg:90.25ms
step:1611/1680 train_time:145392ms step_avg:90.25ms
step:1612/1680 train_time:145483ms step_avg:90.25ms
step:1613/1680 train_time:145574ms step_avg:90.25ms
step:1614/1680 train_time:145665ms step_avg:90.25ms
step:1615/1680 train_time:145756ms step_avg:90.25ms
step:1616/1680 train_time:145847ms step_avg:90.25ms
step:1617/1680 train_time:145940ms step_avg:90.25ms
step:1618/1680 train_time:146035ms step_avg:90.26ms
step:1619/1680 train_time:146121ms step_avg:90.25ms
step:1620/1680 train_time:146211ms step_avg:90.25ms
step:1621/1680 train_time:146302ms step_avg:90.25ms
step:1622/1680 train_time:146393ms step_avg:90.25ms
step:1623/1680 train_time:146483ms step_avg:90.25ms
step:1624/1680 train_time:146575ms step_avg:90.26ms
step:1625/1680 train_time:146666ms step_avg:90.26ms
step:1625/1680 val_loss:3.2913 train_time:146759ms step_avg:90.31ms
step:1626/1680 train_time:146783ms step_avg:90.27ms
step:1627/1680 train_time:146854ms step_avg:90.26ms
step:1628/1680 train_time:146948ms step_avg:90.26ms
step:1629/1680 train_time:147040ms step_avg:90.26ms
step:1630/1680 train_time:147130ms step_avg:90.26ms
step:1631/1680 train_time:147220ms step_avg:90.26ms
step:1632/1680 train_time:147310ms step_avg:90.26ms
step:1633/1680 train_time:147400ms step_avg:90.26ms
step:1634/1680 train_time:147491ms step_avg:90.26ms
step:1635/1680 train_time:147580ms step_avg:90.26ms
step:1636/1680 train_time:147671ms step_avg:90.26ms
step:1637/1680 train_time:147764ms step_avg:90.27ms
step:1638/1680 train_time:147856ms step_avg:90.27ms
step:1639/1680 train_time:147949ms step_avg:90.27ms
step:1640/1680 train_time:148042ms step_avg:90.27ms
step:1641/1680 train_time:148133ms step_avg:90.27ms
step:1642/1680 train_time:148224ms step_avg:90.27ms
step:1643/1680 train_time:148314ms step_avg:90.27ms
step:1644/1680 train_time:148404ms step_avg:90.27ms
step:1645/1680 train_time:148495ms step_avg:90.27ms
step:1646/1680 train_time:148585ms step_avg:90.27ms
step:1647/1680 train_time:148676ms step_avg:90.27ms
step:1648/1680 train_time:148768ms step_avg:90.27ms
step:1649/1680 train_time:148859ms step_avg:90.27ms
step:1650/1680 train_time:148951ms step_avg:90.27ms
step:1651/1680 train_time:149042ms step_avg:90.27ms
step:1652/1680 train_time:149133ms step_avg:90.27ms
step:1653/1680 train_time:149224ms step_avg:90.27ms
step:1654/1680 train_time:149314ms step_avg:90.27ms
step:1655/1680 train_time:149404ms step_avg:90.27ms
step:1656/1680 train_time:149495ms step_avg:90.27ms
step:1657/1680 train_time:149586ms step_avg:90.28ms
step:1658/1680 train_time:149677ms step_avg:90.28ms
step:1659/1680 train_time:149768ms step_avg:90.28ms
step:1660/1680 train_time:149860ms step_avg:90.28ms
step:1661/1680 train_time:149951ms step_avg:90.28ms
step:1662/1680 train_time:150042ms step_avg:90.28ms
step:1663/1680 train_time:150133ms step_avg:90.28ms
step:1664/1680 train_time:150223ms step_avg:90.28ms
step:1665/1680 train_time:150313ms step_avg:90.28ms
step:1666/1680 train_time:150404ms step_avg:90.28ms
step:1667/1680 train_time:150493ms step_avg:90.28ms
step:1668/1680 train_time:150585ms step_avg:90.28ms
step:1669/1680 train_time:150679ms step_avg:90.28ms
step:1670/1680 train_time:150768ms step_avg:90.28ms
step:1671/1680 train_time:150860ms step_avg:90.28ms
step:1672/1680 train_time:150951ms step_avg:90.28ms
step:1673/1680 train_time:151042ms step_avg:90.28ms
step:1674/1680 train_time:151133ms step_avg:90.28ms
step:1675/1680 train_time:151223ms step_avg:90.28ms
step:1676/1680 train_time:151315ms step_avg:90.28ms
step:1677/1680 train_time:151405ms step_avg:90.28ms
step:1678/1680 train_time:151496ms step_avg:90.28ms
step:1679/1680 train_time:151587ms step_avg:90.28ms
step:1680/1680 train_time:151678ms step_avg:90.28ms
step:1680/1680 val_loss:3.2801 train_time:151771ms step_avg:90.34ms
peak memory allocated: 31255 MiB reserved: 46654 MiB

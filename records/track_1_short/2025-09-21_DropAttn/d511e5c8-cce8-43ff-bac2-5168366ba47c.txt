import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 and layer_idx !=0 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        first_bm = 1 * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, final_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"v1/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer shows no degradation with context length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
smear_gate_params = [p for n, p in model.named_parameters() if "smear" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params+smear_gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate, args.ws_validate_final_layer
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250724+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sun Sep 21 23:21:14 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   45C    P0            128W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   33C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   40C    P0            125W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   41C    P0            126W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   33C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   40C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           84409      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A           84410      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           84411      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           84412      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           84413      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           84414      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           84415      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           84416      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           84410      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A           84411      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A           84412      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A           84413      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A           84414      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A           84415      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A           84416      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/1680 train_time:152ms step_avg:152.49ms
step:2/1680 train_time:177ms step_avg:88.30ms
step:3/1680 train_time:238ms step_avg:79.35ms
step:4/1680 train_time:326ms step_avg:81.41ms
step:5/1680 train_time:413ms step_avg:82.67ms
step:6/1680 train_time:501ms step_avg:83.56ms
step:7/1680 train_time:603ms step_avg:86.20ms
step:8/1680 train_time:693ms step_avg:86.57ms
step:9/1680 train_time:781ms step_avg:86.78ms
step:10/1680 train_time:869ms step_avg:86.92ms
step:11/1680 train_time:957ms step_avg:87.00ms
step:12/1680 train_time:1045ms step_avg:87.12ms
step:13/1680 train_time:1135ms step_avg:87.29ms
step:14/1680 train_time:1225ms step_avg:87.51ms
step:15/1680 train_time:1314ms step_avg:87.61ms
step:16/1680 train_time:1404ms step_avg:87.77ms
step:17/1680 train_time:1494ms step_avg:87.88ms
step:18/1680 train_time:1584ms step_avg:88.00ms
step:19/1680 train_time:1674ms step_avg:88.11ms
step:20/1680 train_time:1764ms step_avg:88.21ms
step:21/1680 train_time:1853ms step_avg:88.25ms
step:22/1680 train_time:1942ms step_avg:88.29ms
step:23/1680 train_time:2031ms step_avg:88.32ms
step:24/1680 train_time:2120ms step_avg:88.34ms
step:25/1680 train_time:2210ms step_avg:88.38ms
step:26/1680 train_time:2300ms step_avg:88.45ms
step:27/1680 train_time:2388ms step_avg:88.46ms
step:28/1680 train_time:2478ms step_avg:88.51ms
step:29/1680 train_time:2567ms step_avg:88.53ms
step:30/1680 train_time:2657ms step_avg:88.58ms
step:31/1680 train_time:2747ms step_avg:88.60ms
step:32/1680 train_time:2836ms step_avg:88.61ms
step:33/1680 train_time:2925ms step_avg:88.62ms
step:34/1680 train_time:3013ms step_avg:88.62ms
step:35/1680 train_time:3102ms step_avg:88.62ms
step:36/1680 train_time:3191ms step_avg:88.63ms
step:37/1680 train_time:3280ms step_avg:88.64ms
step:38/1680 train_time:3369ms step_avg:88.66ms
step:39/1680 train_time:3459ms step_avg:88.68ms
step:40/1680 train_time:3547ms step_avg:88.69ms
step:41/1680 train_time:3638ms step_avg:88.73ms
step:42/1680 train_time:3727ms step_avg:88.75ms
step:43/1680 train_time:3816ms step_avg:88.75ms
step:44/1680 train_time:3906ms step_avg:88.77ms
step:45/1680 train_time:3995ms step_avg:88.77ms
step:46/1680 train_time:4084ms step_avg:88.78ms
step:47/1680 train_time:4173ms step_avg:88.79ms
step:48/1680 train_time:4263ms step_avg:88.81ms
step:49/1680 train_time:4352ms step_avg:88.82ms
step:50/1680 train_time:4442ms step_avg:88.83ms
step:51/1680 train_time:4531ms step_avg:88.84ms
step:52/1680 train_time:4620ms step_avg:88.85ms
step:53/1680 train_time:4710ms step_avg:88.86ms
step:54/1680 train_time:4800ms step_avg:88.88ms
step:55/1680 train_time:4889ms step_avg:88.89ms
step:56/1680 train_time:4977ms step_avg:88.88ms
step:57/1680 train_time:5066ms step_avg:88.87ms
step:58/1680 train_time:5155ms step_avg:88.87ms
step:59/1680 train_time:5244ms step_avg:88.88ms
step:60/1680 train_time:5333ms step_avg:88.88ms
step:61/1680 train_time:5423ms step_avg:88.90ms
step:62/1680 train_time:5512ms step_avg:88.91ms
step:63/1680 train_time:5603ms step_avg:88.93ms
step:64/1680 train_time:5692ms step_avg:88.93ms
step:65/1680 train_time:5781ms step_avg:88.94ms
step:66/1680 train_time:5870ms step_avg:88.94ms
step:67/1680 train_time:5960ms step_avg:88.95ms
step:68/1680 train_time:6050ms step_avg:88.97ms
step:69/1680 train_time:6139ms step_avg:88.97ms
step:70/1680 train_time:6228ms step_avg:88.97ms
step:71/1680 train_time:6317ms step_avg:88.97ms
step:72/1680 train_time:6406ms step_avg:88.97ms
step:73/1680 train_time:6496ms step_avg:88.98ms
step:74/1680 train_time:6585ms step_avg:88.99ms
step:75/1680 train_time:6674ms step_avg:88.99ms
step:76/1680 train_time:6764ms step_avg:89.00ms
step:77/1680 train_time:6853ms step_avg:89.00ms
step:78/1680 train_time:6943ms step_avg:89.01ms
step:79/1680 train_time:7031ms step_avg:89.00ms
step:80/1680 train_time:7120ms step_avg:89.00ms
step:81/1680 train_time:7209ms step_avg:89.00ms
step:82/1680 train_time:7298ms step_avg:89.00ms
step:83/1680 train_time:7387ms step_avg:89.00ms
step:84/1680 train_time:7476ms step_avg:89.00ms
step:85/1680 train_time:7566ms step_avg:89.01ms
step:86/1680 train_time:7655ms step_avg:89.01ms
step:87/1680 train_time:7745ms step_avg:89.03ms
step:88/1680 train_time:7834ms step_avg:89.02ms
step:89/1680 train_time:7923ms step_avg:89.02ms
step:90/1680 train_time:8012ms step_avg:89.02ms
step:91/1680 train_time:8101ms step_avg:89.02ms
step:92/1680 train_time:8190ms step_avg:89.03ms
step:93/1680 train_time:8279ms step_avg:89.02ms
step:94/1680 train_time:8368ms step_avg:89.03ms
step:95/1680 train_time:8457ms step_avg:89.03ms
step:96/1680 train_time:8547ms step_avg:89.03ms
step:97/1680 train_time:8636ms step_avg:89.03ms
step:98/1680 train_time:8725ms step_avg:89.03ms
step:99/1680 train_time:8814ms step_avg:89.03ms
step:100/1680 train_time:8903ms step_avg:89.03ms
step:101/1680 train_time:8992ms step_avg:89.03ms
step:102/1680 train_time:9081ms step_avg:89.03ms
step:103/1680 train_time:9170ms step_avg:89.03ms
step:104/1680 train_time:9258ms step_avg:89.02ms
step:105/1680 train_time:9348ms step_avg:89.03ms
step:106/1680 train_time:9437ms step_avg:89.03ms
step:107/1680 train_time:9526ms step_avg:89.03ms
step:108/1680 train_time:9614ms step_avg:89.02ms
step:109/1680 train_time:9704ms step_avg:89.03ms
step:110/1680 train_time:9793ms step_avg:89.03ms
step:111/1680 train_time:9882ms step_avg:89.03ms
step:112/1680 train_time:9971ms step_avg:89.02ms
step:113/1680 train_time:10059ms step_avg:89.02ms
step:114/1680 train_time:10148ms step_avg:89.02ms
step:115/1680 train_time:10237ms step_avg:89.01ms
step:116/1680 train_time:10326ms step_avg:89.02ms
step:117/1680 train_time:10414ms step_avg:89.01ms
step:118/1680 train_time:10503ms step_avg:89.01ms
step:119/1680 train_time:10593ms step_avg:89.01ms
step:120/1680 train_time:10681ms step_avg:89.01ms
step:121/1680 train_time:10771ms step_avg:89.01ms
step:122/1680 train_time:10860ms step_avg:89.01ms
step:123/1680 train_time:10949ms step_avg:89.02ms
step:124/1680 train_time:11038ms step_avg:89.02ms
step:125/1680 train_time:11127ms step_avg:89.01ms
step:125/1680 val_loss:4.3123 train_time:11216ms step_avg:89.73ms
step:126/1680 train_time:11239ms step_avg:89.20ms
step:127/1680 train_time:11305ms step_avg:89.02ms
step:128/1680 train_time:11402ms step_avg:89.08ms
step:129/1680 train_time:11497ms step_avg:89.13ms
step:130/1680 train_time:11588ms step_avg:89.14ms
step:131/1680 train_time:11676ms step_avg:89.13ms
step:132/1680 train_time:11763ms step_avg:89.12ms
step:133/1680 train_time:11851ms step_avg:89.11ms
step:134/1680 train_time:11939ms step_avg:89.09ms
step:135/1680 train_time:12027ms step_avg:89.09ms
step:136/1680 train_time:12114ms step_avg:89.07ms
step:137/1680 train_time:12202ms step_avg:89.06ms
step:138/1680 train_time:12291ms step_avg:89.06ms
step:139/1680 train_time:12381ms step_avg:89.07ms
step:140/1680 train_time:12472ms step_avg:89.08ms
step:141/1680 train_time:12562ms step_avg:89.09ms
step:142/1680 train_time:12652ms step_avg:89.10ms
step:143/1680 train_time:12740ms step_avg:89.09ms
step:144/1680 train_time:12828ms step_avg:89.08ms
step:145/1680 train_time:12916ms step_avg:89.08ms
step:146/1680 train_time:13004ms step_avg:89.07ms
step:147/1680 train_time:13092ms step_avg:89.06ms
step:148/1680 train_time:13180ms step_avg:89.05ms
step:149/1680 train_time:13269ms step_avg:89.06ms
step:150/1680 train_time:13358ms step_avg:89.05ms
step:151/1680 train_time:13448ms step_avg:89.06ms
step:152/1680 train_time:13537ms step_avg:89.06ms
step:153/1680 train_time:13626ms step_avg:89.06ms
step:154/1680 train_time:13716ms step_avg:89.06ms
step:155/1680 train_time:13804ms step_avg:89.06ms
step:156/1680 train_time:13893ms step_avg:89.06ms
step:157/1680 train_time:13981ms step_avg:89.05ms
step:158/1680 train_time:14069ms step_avg:89.04ms
step:159/1680 train_time:14157ms step_avg:89.04ms
step:160/1680 train_time:14246ms step_avg:89.04ms
step:161/1680 train_time:14335ms step_avg:89.04ms
step:162/1680 train_time:14425ms step_avg:89.04ms
step:163/1680 train_time:14514ms step_avg:89.04ms
step:164/1680 train_time:14603ms step_avg:89.04ms
step:165/1680 train_time:14692ms step_avg:89.04ms
step:166/1680 train_time:14780ms step_avg:89.04ms
step:167/1680 train_time:14869ms step_avg:89.04ms
step:168/1680 train_time:14958ms step_avg:89.03ms
step:169/1680 train_time:15047ms step_avg:89.03ms
step:170/1680 train_time:15135ms step_avg:89.03ms
step:171/1680 train_time:15224ms step_avg:89.03ms
step:172/1680 train_time:15312ms step_avg:89.02ms
step:173/1680 train_time:15400ms step_avg:89.02ms
step:174/1680 train_time:15489ms step_avg:89.02ms
step:175/1680 train_time:15578ms step_avg:89.02ms
step:176/1680 train_time:15667ms step_avg:89.02ms
step:177/1680 train_time:15757ms step_avg:89.02ms
step:178/1680 train_time:15846ms step_avg:89.02ms
step:179/1680 train_time:15934ms step_avg:89.02ms
step:180/1680 train_time:16022ms step_avg:89.01ms
step:181/1680 train_time:16111ms step_avg:89.01ms
step:182/1680 train_time:16200ms step_avg:89.01ms
step:183/1680 train_time:16289ms step_avg:89.01ms
step:184/1680 train_time:16377ms step_avg:89.01ms
step:185/1680 train_time:16465ms step_avg:89.00ms
step:186/1680 train_time:16555ms step_avg:89.00ms
step:187/1680 train_time:16644ms step_avg:89.01ms
step:188/1680 train_time:16733ms step_avg:89.01ms
step:189/1680 train_time:16822ms step_avg:89.00ms
step:190/1680 train_time:16911ms step_avg:89.01ms
step:191/1680 train_time:17000ms step_avg:89.01ms
step:192/1680 train_time:17089ms step_avg:89.00ms
step:193/1680 train_time:17177ms step_avg:89.00ms
step:194/1680 train_time:17266ms step_avg:89.00ms
step:195/1680 train_time:17355ms step_avg:89.00ms
step:196/1680 train_time:17443ms step_avg:89.00ms
step:197/1680 train_time:17533ms step_avg:89.00ms
step:198/1680 train_time:17621ms step_avg:89.00ms
step:199/1680 train_time:17710ms step_avg:89.00ms
step:200/1680 train_time:17799ms step_avg:89.00ms
step:201/1680 train_time:17887ms step_avg:88.99ms
step:202/1680 train_time:17976ms step_avg:88.99ms
step:203/1680 train_time:18064ms step_avg:88.99ms
step:204/1680 train_time:18153ms step_avg:88.99ms
step:205/1680 train_time:18242ms step_avg:88.99ms
step:206/1680 train_time:18332ms step_avg:88.99ms
step:207/1680 train_time:18420ms step_avg:88.99ms
step:208/1680 train_time:18509ms step_avg:88.98ms
step:209/1680 train_time:18599ms step_avg:88.99ms
step:210/1680 train_time:18688ms step_avg:88.99ms
step:211/1680 train_time:18777ms step_avg:88.99ms
step:212/1680 train_time:18865ms step_avg:88.99ms
step:213/1680 train_time:18954ms step_avg:88.99ms
step:214/1680 train_time:19042ms step_avg:88.98ms
step:215/1680 train_time:19131ms step_avg:88.98ms
step:216/1680 train_time:19219ms step_avg:88.98ms
step:217/1680 train_time:19308ms step_avg:88.98ms
step:218/1680 train_time:19396ms step_avg:88.97ms
step:219/1680 train_time:19485ms step_avg:88.97ms
step:220/1680 train_time:19574ms step_avg:88.97ms
step:221/1680 train_time:19663ms step_avg:88.97ms
step:222/1680 train_time:19752ms step_avg:88.97ms
step:223/1680 train_time:19840ms step_avg:88.97ms
step:224/1680 train_time:19930ms step_avg:88.97ms
step:225/1680 train_time:20018ms step_avg:88.97ms
step:226/1680 train_time:20108ms step_avg:88.97ms
step:227/1680 train_time:20196ms step_avg:88.97ms
step:228/1680 train_time:20285ms step_avg:88.97ms
step:229/1680 train_time:20374ms step_avg:88.97ms
step:230/1680 train_time:20462ms step_avg:88.97ms
step:231/1680 train_time:20552ms step_avg:88.97ms
step:232/1680 train_time:20640ms step_avg:88.96ms
step:233/1680 train_time:20728ms step_avg:88.96ms
step:234/1680 train_time:20817ms step_avg:88.96ms
step:235/1680 train_time:20906ms step_avg:88.96ms
step:236/1680 train_time:20995ms step_avg:88.96ms
step:237/1680 train_time:21084ms step_avg:88.96ms
step:238/1680 train_time:21173ms step_avg:88.96ms
step:239/1680 train_time:21261ms step_avg:88.96ms
step:240/1680 train_time:21350ms step_avg:88.96ms
step:241/1680 train_time:21438ms step_avg:88.96ms
step:242/1680 train_time:21527ms step_avg:88.96ms
step:243/1680 train_time:21616ms step_avg:88.96ms
step:244/1680 train_time:21705ms step_avg:88.95ms
step:245/1680 train_time:21794ms step_avg:88.96ms
step:246/1680 train_time:21883ms step_avg:88.96ms
step:247/1680 train_time:21972ms step_avg:88.96ms
step:248/1680 train_time:22061ms step_avg:88.95ms
step:249/1680 train_time:22149ms step_avg:88.95ms
step:250/1680 train_time:22238ms step_avg:88.95ms
step:250/1680 val_loss:3.9734 train_time:22328ms step_avg:89.31ms
step:251/1680 train_time:22351ms step_avg:89.05ms
step:252/1680 train_time:22422ms step_avg:88.98ms
step:253/1680 train_time:22516ms step_avg:89.00ms
step:254/1680 train_time:22606ms step_avg:89.00ms
step:255/1680 train_time:22695ms step_avg:89.00ms
step:256/1680 train_time:22782ms step_avg:88.99ms
step:257/1680 train_time:22870ms step_avg:88.99ms
step:258/1680 train_time:22959ms step_avg:88.99ms
step:259/1680 train_time:23047ms step_avg:88.98ms
step:260/1680 train_time:23134ms step_avg:88.98ms
step:261/1680 train_time:23222ms step_avg:88.97ms
step:262/1680 train_time:23312ms step_avg:88.98ms
step:263/1680 train_time:23403ms step_avg:88.98ms
step:264/1680 train_time:23494ms step_avg:88.99ms
step:265/1680 train_time:23584ms step_avg:89.00ms
step:266/1680 train_time:23674ms step_avg:89.00ms
step:267/1680 train_time:23761ms step_avg:88.99ms
step:268/1680 train_time:23850ms step_avg:88.99ms
step:269/1680 train_time:23938ms step_avg:88.99ms
step:270/1680 train_time:24026ms step_avg:88.99ms
step:271/1680 train_time:24114ms step_avg:88.98ms
step:272/1680 train_time:24202ms step_avg:88.98ms
step:273/1680 train_time:24292ms step_avg:88.98ms
step:274/1680 train_time:24382ms step_avg:88.99ms
step:275/1680 train_time:24472ms step_avg:88.99ms
step:276/1680 train_time:24561ms step_avg:88.99ms
step:277/1680 train_time:24651ms step_avg:88.99ms
step:278/1680 train_time:24739ms step_avg:88.99ms
step:279/1680 train_time:24828ms step_avg:88.99ms
step:280/1680 train_time:24916ms step_avg:88.99ms
step:281/1680 train_time:25006ms step_avg:88.99ms
step:282/1680 train_time:25094ms step_avg:88.99ms
step:283/1680 train_time:25182ms step_avg:88.98ms
step:284/1680 train_time:25272ms step_avg:88.99ms
step:285/1680 train_time:25361ms step_avg:88.99ms
step:286/1680 train_time:25451ms step_avg:88.99ms
step:287/1680 train_time:25540ms step_avg:88.99ms
step:288/1680 train_time:25630ms step_avg:88.99ms
step:289/1680 train_time:25719ms step_avg:88.99ms
step:290/1680 train_time:25808ms step_avg:88.99ms
step:291/1680 train_time:25897ms step_avg:88.99ms
step:292/1680 train_time:25985ms step_avg:88.99ms
step:293/1680 train_time:26073ms step_avg:88.99ms
step:294/1680 train_time:26161ms step_avg:88.98ms
step:295/1680 train_time:26250ms step_avg:88.98ms
step:296/1680 train_time:26338ms step_avg:88.98ms
step:297/1680 train_time:26427ms step_avg:88.98ms
step:298/1680 train_time:26516ms step_avg:88.98ms
step:299/1680 train_time:26605ms step_avg:88.98ms
step:300/1680 train_time:26694ms step_avg:88.98ms
step:301/1680 train_time:26783ms step_avg:88.98ms
step:302/1680 train_time:26872ms step_avg:88.98ms
step:303/1680 train_time:26960ms step_avg:88.98ms
step:304/1680 train_time:27049ms step_avg:88.98ms
step:305/1680 train_time:27137ms step_avg:88.97ms
step:306/1680 train_time:27226ms step_avg:88.97ms
step:307/1680 train_time:27315ms step_avg:88.97ms
step:308/1680 train_time:27403ms step_avg:88.97ms
step:309/1680 train_time:27491ms step_avg:88.97ms
step:310/1680 train_time:27580ms step_avg:88.97ms
step:311/1680 train_time:27669ms step_avg:88.97ms
step:312/1680 train_time:27758ms step_avg:88.97ms
step:313/1680 train_time:27847ms step_avg:88.97ms
step:314/1680 train_time:27936ms step_avg:88.97ms
step:315/1680 train_time:28024ms step_avg:88.97ms
step:316/1680 train_time:28114ms step_avg:88.97ms
step:317/1680 train_time:28202ms step_avg:88.96ms
step:318/1680 train_time:28291ms step_avg:88.97ms
step:319/1680 train_time:28380ms step_avg:88.96ms
step:320/1680 train_time:28469ms step_avg:88.97ms
step:321/1680 train_time:28559ms step_avg:88.97ms
step:322/1680 train_time:28647ms step_avg:88.97ms
step:323/1680 train_time:28736ms step_avg:88.97ms
step:324/1680 train_time:28825ms step_avg:88.97ms
step:325/1680 train_time:28914ms step_avg:88.97ms
step:326/1680 train_time:29002ms step_avg:88.96ms
step:327/1680 train_time:29092ms step_avg:88.97ms
step:328/1680 train_time:29180ms step_avg:88.96ms
step:329/1680 train_time:29269ms step_avg:88.96ms
step:330/1680 train_time:29358ms step_avg:88.96ms
step:331/1680 train_time:29446ms step_avg:88.96ms
step:332/1680 train_time:29535ms step_avg:88.96ms
step:333/1680 train_time:29624ms step_avg:88.96ms
step:334/1680 train_time:29713ms step_avg:88.96ms
step:335/1680 train_time:29802ms step_avg:88.96ms
step:336/1680 train_time:29891ms step_avg:88.96ms
step:337/1680 train_time:29979ms step_avg:88.96ms
step:338/1680 train_time:30068ms step_avg:88.96ms
step:339/1680 train_time:30157ms step_avg:88.96ms
step:340/1680 train_time:30246ms step_avg:88.96ms
step:341/1680 train_time:30336ms step_avg:88.96ms
step:342/1680 train_time:30425ms step_avg:88.96ms
step:343/1680 train_time:30514ms step_avg:88.96ms
step:344/1680 train_time:30603ms step_avg:88.96ms
step:345/1680 train_time:30692ms step_avg:88.96ms
step:346/1680 train_time:30781ms step_avg:88.96ms
step:347/1680 train_time:30870ms step_avg:88.96ms
step:348/1680 train_time:30959ms step_avg:88.96ms
step:349/1680 train_time:31048ms step_avg:88.96ms
step:350/1680 train_time:31139ms step_avg:88.97ms
step:351/1680 train_time:31225ms step_avg:88.96ms
step:352/1680 train_time:31314ms step_avg:88.96ms
step:353/1680 train_time:31403ms step_avg:88.96ms
step:354/1680 train_time:31493ms step_avg:88.96ms
step:355/1680 train_time:31581ms step_avg:88.96ms
step:356/1680 train_time:31670ms step_avg:88.96ms
step:357/1680 train_time:31759ms step_avg:88.96ms
step:358/1680 train_time:31848ms step_avg:88.96ms
step:359/1680 train_time:31936ms step_avg:88.96ms
step:360/1680 train_time:32025ms step_avg:88.96ms
step:361/1680 train_time:32115ms step_avg:88.96ms
step:362/1680 train_time:32203ms step_avg:88.96ms
step:363/1680 train_time:32292ms step_avg:88.96ms
step:364/1680 train_time:32381ms step_avg:88.96ms
step:365/1680 train_time:32469ms step_avg:88.96ms
step:366/1680 train_time:32558ms step_avg:88.96ms
step:367/1680 train_time:32648ms step_avg:88.96ms
step:368/1680 train_time:32737ms step_avg:88.96ms
step:369/1680 train_time:32826ms step_avg:88.96ms
step:370/1680 train_time:32915ms step_avg:88.96ms
step:371/1680 train_time:33003ms step_avg:88.96ms
step:372/1680 train_time:33092ms step_avg:88.96ms
step:373/1680 train_time:33181ms step_avg:88.96ms
step:374/1680 train_time:33270ms step_avg:88.96ms
step:375/1680 train_time:33359ms step_avg:88.96ms
step:375/1680 val_loss:3.8267 train_time:33449ms step_avg:89.20ms
step:376/1680 train_time:33472ms step_avg:89.02ms
step:377/1680 train_time:33543ms step_avg:88.97ms
step:378/1680 train_time:33640ms step_avg:88.99ms
step:379/1680 train_time:33729ms step_avg:88.99ms
step:380/1680 train_time:33818ms step_avg:88.99ms
step:381/1680 train_time:33905ms step_avg:88.99ms
step:382/1680 train_time:33993ms step_avg:88.99ms
step:383/1680 train_time:34081ms step_avg:88.99ms
step:384/1680 train_time:34170ms step_avg:88.98ms
step:385/1680 train_time:34258ms step_avg:88.98ms
step:386/1680 train_time:34346ms step_avg:88.98ms
step:387/1680 train_time:34435ms step_avg:88.98ms
step:388/1680 train_time:34528ms step_avg:88.99ms
step:389/1680 train_time:34619ms step_avg:88.99ms
step:390/1680 train_time:34708ms step_avg:89.00ms
step:391/1680 train_time:34798ms step_avg:89.00ms
step:392/1680 train_time:34887ms step_avg:89.00ms
step:393/1680 train_time:34975ms step_avg:89.00ms
step:394/1680 train_time:35063ms step_avg:88.99ms
step:395/1680 train_time:35152ms step_avg:88.99ms
step:396/1680 train_time:35240ms step_avg:88.99ms
step:397/1680 train_time:35328ms step_avg:88.99ms
step:398/1680 train_time:35417ms step_avg:88.99ms
step:399/1680 train_time:35507ms step_avg:88.99ms
step:400/1680 train_time:35598ms step_avg:89.00ms
step:401/1680 train_time:35688ms step_avg:89.00ms
step:402/1680 train_time:35777ms step_avg:89.00ms
step:403/1680 train_time:35867ms step_avg:89.00ms
step:404/1680 train_time:35957ms step_avg:89.00ms
step:405/1680 train_time:36045ms step_avg:89.00ms
step:406/1680 train_time:36134ms step_avg:89.00ms
step:407/1680 train_time:36222ms step_avg:89.00ms
step:408/1680 train_time:36310ms step_avg:88.99ms
step:409/1680 train_time:36398ms step_avg:88.99ms
step:410/1680 train_time:36487ms step_avg:88.99ms
step:411/1680 train_time:36577ms step_avg:88.99ms
step:412/1680 train_time:36666ms step_avg:89.00ms
step:413/1680 train_time:36756ms step_avg:89.00ms
step:414/1680 train_time:36845ms step_avg:89.00ms
step:415/1680 train_time:36934ms step_avg:89.00ms
step:416/1680 train_time:37023ms step_avg:89.00ms
step:417/1680 train_time:37111ms step_avg:89.00ms
step:418/1680 train_time:37200ms step_avg:88.99ms
step:419/1680 train_time:37288ms step_avg:88.99ms
step:420/1680 train_time:37377ms step_avg:88.99ms
step:421/1680 train_time:37466ms step_avg:88.99ms
step:422/1680 train_time:37555ms step_avg:88.99ms
step:423/1680 train_time:37645ms step_avg:88.99ms
step:424/1680 train_time:37735ms step_avg:89.00ms
step:425/1680 train_time:37824ms step_avg:89.00ms
step:426/1680 train_time:37913ms step_avg:89.00ms
step:427/1680 train_time:38003ms step_avg:89.00ms
step:428/1680 train_time:38092ms step_avg:89.00ms
step:429/1680 train_time:38181ms step_avg:89.00ms
step:430/1680 train_time:38270ms step_avg:89.00ms
step:431/1680 train_time:38360ms step_avg:89.00ms
step:432/1680 train_time:38449ms step_avg:89.00ms
step:433/1680 train_time:38538ms step_avg:89.00ms
step:434/1680 train_time:38627ms step_avg:89.00ms
step:435/1680 train_time:38716ms step_avg:89.00ms
step:436/1680 train_time:38806ms step_avg:89.00ms
step:437/1680 train_time:38895ms step_avg:89.01ms
step:438/1680 train_time:38990ms step_avg:89.02ms
step:439/1680 train_time:39074ms step_avg:89.01ms
step:440/1680 train_time:39163ms step_avg:89.01ms
step:441/1680 train_time:39252ms step_avg:89.01ms
step:442/1680 train_time:39341ms step_avg:89.01ms
step:443/1680 train_time:39430ms step_avg:89.01ms
step:444/1680 train_time:39519ms step_avg:89.01ms
step:445/1680 train_time:39607ms step_avg:89.00ms
step:446/1680 train_time:39696ms step_avg:89.00ms
step:447/1680 train_time:39785ms step_avg:89.00ms
step:448/1680 train_time:39874ms step_avg:89.01ms
step:449/1680 train_time:39964ms step_avg:89.01ms
step:450/1680 train_time:40053ms step_avg:89.01ms
step:451/1680 train_time:40142ms step_avg:89.01ms
step:452/1680 train_time:40231ms step_avg:89.01ms
step:453/1680 train_time:40320ms step_avg:89.01ms
step:454/1680 train_time:40409ms step_avg:89.01ms
step:455/1680 train_time:40500ms step_avg:89.01ms
step:456/1680 train_time:40586ms step_avg:89.01ms
step:457/1680 train_time:40675ms step_avg:89.00ms
step:458/1680 train_time:40764ms step_avg:89.00ms
step:459/1680 train_time:40854ms step_avg:89.01ms
step:460/1680 train_time:40942ms step_avg:89.01ms
step:461/1680 train_time:41031ms step_avg:89.01ms
step:462/1680 train_time:41121ms step_avg:89.01ms
step:463/1680 train_time:41210ms step_avg:89.01ms
step:464/1680 train_time:41300ms step_avg:89.01ms
step:465/1680 train_time:41389ms step_avg:89.01ms
step:466/1680 train_time:41479ms step_avg:89.01ms
step:467/1680 train_time:41567ms step_avg:89.01ms
step:468/1680 train_time:41656ms step_avg:89.01ms
step:469/1680 train_time:41744ms step_avg:89.01ms
step:470/1680 train_time:41833ms step_avg:89.01ms
step:471/1680 train_time:41922ms step_avg:89.01ms
step:472/1680 train_time:42011ms step_avg:89.01ms
step:473/1680 train_time:42101ms step_avg:89.01ms
step:474/1680 train_time:42190ms step_avg:89.01ms
step:475/1680 train_time:42279ms step_avg:89.01ms
step:476/1680 train_time:42368ms step_avg:89.01ms
step:477/1680 train_time:42457ms step_avg:89.01ms
step:478/1680 train_time:42546ms step_avg:89.01ms
step:479/1680 train_time:42635ms step_avg:89.01ms
step:480/1680 train_time:42724ms step_avg:89.01ms
step:481/1680 train_time:42813ms step_avg:89.01ms
step:482/1680 train_time:42902ms step_avg:89.01ms
step:483/1680 train_time:42991ms step_avg:89.01ms
step:484/1680 train_time:43080ms step_avg:89.01ms
step:485/1680 train_time:43169ms step_avg:89.01ms
step:486/1680 train_time:43259ms step_avg:89.01ms
step:487/1680 train_time:43347ms step_avg:89.01ms
step:488/1680 train_time:43437ms step_avg:89.01ms
step:489/1680 train_time:43525ms step_avg:89.01ms
step:490/1680 train_time:43614ms step_avg:89.01ms
step:491/1680 train_time:43704ms step_avg:89.01ms
step:492/1680 train_time:43793ms step_avg:89.01ms
step:493/1680 train_time:43882ms step_avg:89.01ms
step:494/1680 train_time:43971ms step_avg:89.01ms
step:495/1680 train_time:44061ms step_avg:89.01ms
step:496/1680 train_time:44150ms step_avg:89.01ms
step:497/1680 train_time:44239ms step_avg:89.01ms
step:498/1680 train_time:44328ms step_avg:89.01ms
step:499/1680 train_time:44417ms step_avg:89.01ms
step:500/1680 train_time:44506ms step_avg:89.01ms
step:500/1680 val_loss:3.7202 train_time:44596ms step_avg:89.19ms
step:501/1680 train_time:44619ms step_avg:89.06ms
step:502/1680 train_time:44688ms step_avg:89.02ms
step:503/1680 train_time:44781ms step_avg:89.03ms
step:504/1680 train_time:44872ms step_avg:89.03ms
step:505/1680 train_time:44961ms step_avg:89.03ms
step:506/1680 train_time:45049ms step_avg:89.03ms
step:507/1680 train_time:45137ms step_avg:89.03ms
step:508/1680 train_time:45225ms step_avg:89.03ms
step:509/1680 train_time:45313ms step_avg:89.02ms
step:510/1680 train_time:45401ms step_avg:89.02ms
step:511/1680 train_time:45489ms step_avg:89.02ms
step:512/1680 train_time:45579ms step_avg:89.02ms
step:513/1680 train_time:45668ms step_avg:89.02ms
step:514/1680 train_time:45759ms step_avg:89.03ms
step:515/1680 train_time:45850ms step_avg:89.03ms
step:516/1680 train_time:45939ms step_avg:89.03ms
step:517/1680 train_time:46029ms step_avg:89.03ms
step:518/1680 train_time:46117ms step_avg:89.03ms
step:519/1680 train_time:46206ms step_avg:89.03ms
step:520/1680 train_time:46295ms step_avg:89.03ms
step:521/1680 train_time:46384ms step_avg:89.03ms
step:522/1680 train_time:46472ms step_avg:89.03ms
step:523/1680 train_time:46561ms step_avg:89.03ms
step:524/1680 train_time:46651ms step_avg:89.03ms
step:525/1680 train_time:46741ms step_avg:89.03ms
step:526/1680 train_time:46830ms step_avg:89.03ms
step:527/1680 train_time:46920ms step_avg:89.03ms
step:528/1680 train_time:47009ms step_avg:89.03ms
step:529/1680 train_time:47098ms step_avg:89.03ms
step:530/1680 train_time:47187ms step_avg:89.03ms
step:531/1680 train_time:47275ms step_avg:89.03ms
step:532/1680 train_time:47364ms step_avg:89.03ms
step:533/1680 train_time:47453ms step_avg:89.03ms
step:534/1680 train_time:47541ms step_avg:89.03ms
step:535/1680 train_time:47631ms step_avg:89.03ms
step:536/1680 train_time:47720ms step_avg:89.03ms
step:537/1680 train_time:47809ms step_avg:89.03ms
step:538/1680 train_time:47898ms step_avg:89.03ms
step:539/1680 train_time:47988ms step_avg:89.03ms
step:540/1680 train_time:48077ms step_avg:89.03ms
step:541/1680 train_time:48165ms step_avg:89.03ms
step:542/1680 train_time:48254ms step_avg:89.03ms
step:543/1680 train_time:48342ms step_avg:89.03ms
step:544/1680 train_time:48432ms step_avg:89.03ms
step:545/1680 train_time:48521ms step_avg:89.03ms
step:546/1680 train_time:48611ms step_avg:89.03ms
step:547/1680 train_time:48700ms step_avg:89.03ms
step:548/1680 train_time:48789ms step_avg:89.03ms
step:549/1680 train_time:48879ms step_avg:89.03ms
step:550/1680 train_time:48970ms step_avg:89.04ms
step:551/1680 train_time:49061ms step_avg:89.04ms
step:552/1680 train_time:49151ms step_avg:89.04ms
step:553/1680 train_time:49240ms step_avg:89.04ms
step:554/1680 train_time:49331ms step_avg:89.04ms
step:555/1680 train_time:49421ms step_avg:89.05ms
step:556/1680 train_time:49512ms step_avg:89.05ms
step:557/1680 train_time:49602ms step_avg:89.05ms
step:558/1680 train_time:49692ms step_avg:89.05ms
step:559/1680 train_time:49782ms step_avg:89.06ms
step:560/1680 train_time:49872ms step_avg:89.06ms
step:561/1680 train_time:49963ms step_avg:89.06ms
step:562/1680 train_time:50056ms step_avg:89.07ms
step:563/1680 train_time:50144ms step_avg:89.07ms
step:564/1680 train_time:50234ms step_avg:89.07ms
step:565/1680 train_time:50324ms step_avg:89.07ms
step:566/1680 train_time:50416ms step_avg:89.07ms
step:567/1680 train_time:50506ms step_avg:89.08ms
step:568/1680 train_time:50596ms step_avg:89.08ms
step:569/1680 train_time:50687ms step_avg:89.08ms
step:570/1680 train_time:50776ms step_avg:89.08ms
step:571/1680 train_time:50867ms step_avg:89.08ms
step:572/1680 train_time:50958ms step_avg:89.09ms
step:573/1680 train_time:51049ms step_avg:89.09ms
step:574/1680 train_time:51139ms step_avg:89.09ms
step:575/1680 train_time:51230ms step_avg:89.10ms
step:576/1680 train_time:51320ms step_avg:89.10ms
step:577/1680 train_time:51411ms step_avg:89.10ms
step:578/1680 train_time:51501ms step_avg:89.10ms
step:579/1680 train_time:51591ms step_avg:89.10ms
step:580/1680 train_time:51682ms step_avg:89.11ms
step:581/1680 train_time:51772ms step_avg:89.11ms
step:582/1680 train_time:51862ms step_avg:89.11ms
step:583/1680 train_time:51953ms step_avg:89.11ms
step:584/1680 train_time:52043ms step_avg:89.11ms
step:585/1680 train_time:52133ms step_avg:89.12ms
step:586/1680 train_time:52224ms step_avg:89.12ms
step:587/1680 train_time:52315ms step_avg:89.12ms
step:588/1680 train_time:52404ms step_avg:89.12ms
step:589/1680 train_time:52495ms step_avg:89.13ms
step:590/1680 train_time:52586ms step_avg:89.13ms
step:591/1680 train_time:52676ms step_avg:89.13ms
step:592/1680 train_time:52767ms step_avg:89.13ms
step:593/1680 train_time:52857ms step_avg:89.14ms
step:594/1680 train_time:52947ms step_avg:89.14ms
step:595/1680 train_time:53038ms step_avg:89.14ms
step:596/1680 train_time:53128ms step_avg:89.14ms
step:597/1680 train_time:53219ms step_avg:89.14ms
step:598/1680 train_time:53309ms step_avg:89.15ms
step:599/1680 train_time:53399ms step_avg:89.15ms
step:600/1680 train_time:53489ms step_avg:89.15ms
step:601/1680 train_time:53579ms step_avg:89.15ms
step:602/1680 train_time:53669ms step_avg:89.15ms
step:603/1680 train_time:53760ms step_avg:89.15ms
step:604/1680 train_time:53850ms step_avg:89.16ms
step:605/1680 train_time:53939ms step_avg:89.16ms
step:606/1680 train_time:54031ms step_avg:89.16ms
step:607/1680 train_time:54121ms step_avg:89.16ms
step:608/1680 train_time:54211ms step_avg:89.16ms
step:609/1680 train_time:54301ms step_avg:89.16ms
step:610/1680 train_time:54391ms step_avg:89.17ms
step:611/1680 train_time:54481ms step_avg:89.17ms
step:612/1680 train_time:54572ms step_avg:89.17ms
step:613/1680 train_time:54662ms step_avg:89.17ms
step:614/1680 train_time:54754ms step_avg:89.18ms
step:615/1680 train_time:54844ms step_avg:89.18ms
step:616/1680 train_time:54935ms step_avg:89.18ms
step:617/1680 train_time:55025ms step_avg:89.18ms
step:618/1680 train_time:55116ms step_avg:89.18ms
step:619/1680 train_time:55206ms step_avg:89.19ms
step:620/1680 train_time:55297ms step_avg:89.19ms
step:621/1680 train_time:55386ms step_avg:89.19ms
step:622/1680 train_time:55477ms step_avg:89.19ms
step:623/1680 train_time:55566ms step_avg:89.19ms
step:624/1680 train_time:55657ms step_avg:89.19ms
step:625/1680 train_time:55748ms step_avg:89.20ms
step:625/1680 val_loss:3.6196 train_time:55839ms step_avg:89.34ms
step:626/1680 train_time:55862ms step_avg:89.24ms
step:627/1680 train_time:55931ms step_avg:89.20ms
step:628/1680 train_time:56031ms step_avg:89.22ms
step:629/1680 train_time:56122ms step_avg:89.22ms
step:630/1680 train_time:56211ms step_avg:89.22ms
step:631/1680 train_time:56301ms step_avg:89.22ms
step:632/1680 train_time:56390ms step_avg:89.23ms
step:633/1680 train_time:56480ms step_avg:89.23ms
step:634/1680 train_time:56568ms step_avg:89.22ms
step:635/1680 train_time:56657ms step_avg:89.22ms
step:636/1680 train_time:56747ms step_avg:89.22ms
step:637/1680 train_time:56843ms step_avg:89.24ms
step:638/1680 train_time:56936ms step_avg:89.24ms
step:639/1680 train_time:57027ms step_avg:89.24ms
step:640/1680 train_time:57117ms step_avg:89.25ms
step:641/1680 train_time:57207ms step_avg:89.25ms
step:642/1680 train_time:57297ms step_avg:89.25ms
step:643/1680 train_time:57387ms step_avg:89.25ms
step:644/1680 train_time:57476ms step_avg:89.25ms
step:645/1680 train_time:57565ms step_avg:89.25ms
step:646/1680 train_time:57654ms step_avg:89.25ms
step:647/1680 train_time:57744ms step_avg:89.25ms
step:648/1680 train_time:57836ms step_avg:89.25ms
step:649/1680 train_time:57927ms step_avg:89.26ms
step:650/1680 train_time:58018ms step_avg:89.26ms
step:651/1680 train_time:58108ms step_avg:89.26ms
step:652/1680 train_time:58199ms step_avg:89.26ms
step:653/1680 train_time:58289ms step_avg:89.26ms
step:654/1680 train_time:58379ms step_avg:89.26ms
step:655/1680 train_time:58468ms step_avg:89.26ms
step:656/1680 train_time:58557ms step_avg:89.26ms
step:657/1680 train_time:58646ms step_avg:89.26ms
step:658/1680 train_time:58738ms step_avg:89.27ms
step:659/1680 train_time:58826ms step_avg:89.27ms
step:660/1680 train_time:58918ms step_avg:89.27ms
step:661/1680 train_time:59008ms step_avg:89.27ms
step:662/1680 train_time:59098ms step_avg:89.27ms
step:663/1680 train_time:59189ms step_avg:89.27ms
step:664/1680 train_time:59279ms step_avg:89.28ms
step:665/1680 train_time:59369ms step_avg:89.28ms
step:666/1680 train_time:59459ms step_avg:89.28ms
step:667/1680 train_time:59549ms step_avg:89.28ms
step:668/1680 train_time:59639ms step_avg:89.28ms
step:669/1680 train_time:59729ms step_avg:89.28ms
step:670/1680 train_time:59819ms step_avg:89.28ms
step:671/1680 train_time:59909ms step_avg:89.28ms
step:672/1680 train_time:60000ms step_avg:89.29ms
step:673/1680 train_time:60090ms step_avg:89.29ms
step:674/1680 train_time:60181ms step_avg:89.29ms
step:675/1680 train_time:60271ms step_avg:89.29ms
step:676/1680 train_time:60361ms step_avg:89.29ms
step:677/1680 train_time:60451ms step_avg:89.29ms
step:678/1680 train_time:60542ms step_avg:89.29ms
step:679/1680 train_time:60632ms step_avg:89.30ms
step:680/1680 train_time:60723ms step_avg:89.30ms
step:681/1680 train_time:60813ms step_avg:89.30ms
step:682/1680 train_time:60904ms step_avg:89.30ms
step:683/1680 train_time:60994ms step_avg:89.30ms
step:684/1680 train_time:61083ms step_avg:89.30ms
step:685/1680 train_time:61175ms step_avg:89.31ms
step:686/1680 train_time:61265ms step_avg:89.31ms
step:687/1680 train_time:61355ms step_avg:89.31ms
step:688/1680 train_time:61445ms step_avg:89.31ms
step:689/1680 train_time:61535ms step_avg:89.31ms
step:690/1680 train_time:61625ms step_avg:89.31ms
step:691/1680 train_time:61715ms step_avg:89.31ms
step:692/1680 train_time:61805ms step_avg:89.31ms
step:693/1680 train_time:61894ms step_avg:89.31ms
step:694/1680 train_time:61984ms step_avg:89.31ms
step:695/1680 train_time:62075ms step_avg:89.32ms
step:696/1680 train_time:62166ms step_avg:89.32ms
step:697/1680 train_time:62255ms step_avg:89.32ms
step:698/1680 train_time:62346ms step_avg:89.32ms
step:699/1680 train_time:62436ms step_avg:89.32ms
step:700/1680 train_time:62526ms step_avg:89.32ms
step:701/1680 train_time:62616ms step_avg:89.32ms
step:702/1680 train_time:62707ms step_avg:89.33ms
step:703/1680 train_time:62797ms step_avg:89.33ms
step:704/1680 train_time:62887ms step_avg:89.33ms
step:705/1680 train_time:62977ms step_avg:89.33ms
step:706/1680 train_time:63067ms step_avg:89.33ms
step:707/1680 train_time:63157ms step_avg:89.33ms
step:708/1680 train_time:63248ms step_avg:89.33ms
step:709/1680 train_time:63339ms step_avg:89.34ms
step:710/1680 train_time:63428ms step_avg:89.34ms
step:711/1680 train_time:63519ms step_avg:89.34ms
step:712/1680 train_time:63609ms step_avg:89.34ms
step:713/1680 train_time:63699ms step_avg:89.34ms
step:714/1680 train_time:63788ms step_avg:89.34ms
step:715/1680 train_time:63878ms step_avg:89.34ms
step:716/1680 train_time:63969ms step_avg:89.34ms
step:717/1680 train_time:64059ms step_avg:89.34ms
step:718/1680 train_time:64149ms step_avg:89.34ms
step:719/1680 train_time:64241ms step_avg:89.35ms
step:720/1680 train_time:64331ms step_avg:89.35ms
step:721/1680 train_time:64421ms step_avg:89.35ms
step:722/1680 train_time:64512ms step_avg:89.35ms
step:723/1680 train_time:64603ms step_avg:89.35ms
step:724/1680 train_time:64693ms step_avg:89.36ms
step:725/1680 train_time:64783ms step_avg:89.36ms
step:726/1680 train_time:64872ms step_avg:89.36ms
step:727/1680 train_time:64963ms step_avg:89.36ms
step:728/1680 train_time:65053ms step_avg:89.36ms
step:729/1680 train_time:65143ms step_avg:89.36ms
step:730/1680 train_time:65234ms step_avg:89.36ms
step:731/1680 train_time:65324ms step_avg:89.36ms
step:732/1680 train_time:65415ms step_avg:89.36ms
step:733/1680 train_time:65505ms step_avg:89.37ms
step:734/1680 train_time:65595ms step_avg:89.37ms
step:735/1680 train_time:65685ms step_avg:89.37ms
step:736/1680 train_time:65775ms step_avg:89.37ms
step:737/1680 train_time:65865ms step_avg:89.37ms
step:738/1680 train_time:65954ms step_avg:89.37ms
step:739/1680 train_time:66045ms step_avg:89.37ms
step:740/1680 train_time:66136ms step_avg:89.37ms
step:741/1680 train_time:66226ms step_avg:89.37ms
step:742/1680 train_time:66317ms step_avg:89.38ms
step:743/1680 train_time:66407ms step_avg:89.38ms
step:744/1680 train_time:66497ms step_avg:89.38ms
step:745/1680 train_time:66588ms step_avg:89.38ms
step:746/1680 train_time:66678ms step_avg:89.38ms
step:747/1680 train_time:66768ms step_avg:89.38ms
step:748/1680 train_time:66858ms step_avg:89.38ms
step:749/1680 train_time:66948ms step_avg:89.38ms
step:750/1680 train_time:67039ms step_avg:89.39ms
step:750/1680 val_loss:3.5657 train_time:67129ms step_avg:89.51ms
step:751/1680 train_time:67152ms step_avg:89.42ms
step:752/1680 train_time:67224ms step_avg:89.39ms
step:753/1680 train_time:67322ms step_avg:89.40ms
step:754/1680 train_time:67412ms step_avg:89.41ms
step:755/1680 train_time:67502ms step_avg:89.41ms
step:756/1680 train_time:67591ms step_avg:89.41ms
step:757/1680 train_time:67680ms step_avg:89.41ms
step:758/1680 train_time:67769ms step_avg:89.40ms
step:759/1680 train_time:67858ms step_avg:89.40ms
step:760/1680 train_time:67948ms step_avg:89.40ms
step:761/1680 train_time:68036ms step_avg:89.40ms
step:762/1680 train_time:68127ms step_avg:89.41ms
step:763/1680 train_time:68220ms step_avg:89.41ms
step:764/1680 train_time:68312ms step_avg:89.41ms
step:765/1680 train_time:68404ms step_avg:89.42ms
step:766/1680 train_time:68494ms step_avg:89.42ms
step:767/1680 train_time:68584ms step_avg:89.42ms
step:768/1680 train_time:68674ms step_avg:89.42ms
step:769/1680 train_time:68763ms step_avg:89.42ms
step:770/1680 train_time:68852ms step_avg:89.42ms
step:771/1680 train_time:68942ms step_avg:89.42ms
step:772/1680 train_time:69031ms step_avg:89.42ms
step:773/1680 train_time:69122ms step_avg:89.42ms
step:774/1680 train_time:69212ms step_avg:89.42ms
step:775/1680 train_time:69304ms step_avg:89.43ms
step:776/1680 train_time:69396ms step_avg:89.43ms
step:777/1680 train_time:69487ms step_avg:89.43ms
step:778/1680 train_time:69578ms step_avg:89.43ms
step:779/1680 train_time:69668ms step_avg:89.43ms
step:780/1680 train_time:69758ms step_avg:89.43ms
step:781/1680 train_time:69847ms step_avg:89.43ms
step:782/1680 train_time:69937ms step_avg:89.43ms
step:783/1680 train_time:70027ms step_avg:89.43ms
step:784/1680 train_time:70117ms step_avg:89.44ms
step:785/1680 train_time:70207ms step_avg:89.44ms
step:786/1680 train_time:70298ms step_avg:89.44ms
step:787/1680 train_time:70389ms step_avg:89.44ms
step:788/1680 train_time:70480ms step_avg:89.44ms
step:789/1680 train_time:70570ms step_avg:89.44ms
step:790/1680 train_time:70661ms step_avg:89.44ms
step:791/1680 train_time:70750ms step_avg:89.44ms
step:792/1680 train_time:70841ms step_avg:89.45ms
step:793/1680 train_time:70931ms step_avg:89.45ms
step:794/1680 train_time:71021ms step_avg:89.45ms
step:795/1680 train_time:71110ms step_avg:89.45ms
step:796/1680 train_time:71201ms step_avg:89.45ms
step:797/1680 train_time:71292ms step_avg:89.45ms
step:798/1680 train_time:71384ms step_avg:89.45ms
step:799/1680 train_time:71475ms step_avg:89.46ms
step:800/1680 train_time:71565ms step_avg:89.46ms
step:801/1680 train_time:71656ms step_avg:89.46ms
step:802/1680 train_time:71746ms step_avg:89.46ms
step:803/1680 train_time:71836ms step_avg:89.46ms
step:804/1680 train_time:71926ms step_avg:89.46ms
step:805/1680 train_time:72015ms step_avg:89.46ms
step:806/1680 train_time:72105ms step_avg:89.46ms
step:807/1680 train_time:72194ms step_avg:89.46ms
step:808/1680 train_time:72285ms step_avg:89.46ms
step:809/1680 train_time:72376ms step_avg:89.46ms
step:810/1680 train_time:72467ms step_avg:89.46ms
step:811/1680 train_time:72557ms step_avg:89.47ms
step:812/1680 train_time:72648ms step_avg:89.47ms
step:813/1680 train_time:72739ms step_avg:89.47ms
step:814/1680 train_time:72828ms step_avg:89.47ms
step:815/1680 train_time:72918ms step_avg:89.47ms
step:816/1680 train_time:73008ms step_avg:89.47ms
step:817/1680 train_time:73100ms step_avg:89.47ms
step:818/1680 train_time:73188ms step_avg:89.47ms
step:819/1680 train_time:73278ms step_avg:89.47ms
step:820/1680 train_time:73368ms step_avg:89.47ms
step:821/1680 train_time:73459ms step_avg:89.48ms
step:822/1680 train_time:73549ms step_avg:89.48ms
step:823/1680 train_time:73640ms step_avg:89.48ms
step:824/1680 train_time:73730ms step_avg:89.48ms
step:825/1680 train_time:73820ms step_avg:89.48ms
step:826/1680 train_time:73910ms step_avg:89.48ms
step:827/1680 train_time:73999ms step_avg:89.48ms
step:828/1680 train_time:74089ms step_avg:89.48ms
step:829/1680 train_time:74179ms step_avg:89.48ms
step:830/1680 train_time:74269ms step_avg:89.48ms
step:831/1680 train_time:74360ms step_avg:89.48ms
step:832/1680 train_time:74450ms step_avg:89.48ms
step:833/1680 train_time:74541ms step_avg:89.49ms
step:834/1680 train_time:74631ms step_avg:89.49ms
step:835/1680 train_time:74722ms step_avg:89.49ms
step:836/1680 train_time:74812ms step_avg:89.49ms
step:837/1680 train_time:74902ms step_avg:89.49ms
step:838/1680 train_time:74991ms step_avg:89.49ms
step:839/1680 train_time:75082ms step_avg:89.49ms
step:840/1680 train_time:75171ms step_avg:89.49ms
step:841/1680 train_time:75262ms step_avg:89.49ms
step:842/1680 train_time:75352ms step_avg:89.49ms
step:843/1680 train_time:75442ms step_avg:89.49ms
step:844/1680 train_time:75532ms step_avg:89.49ms
step:845/1680 train_time:75624ms step_avg:89.50ms
step:846/1680 train_time:75715ms step_avg:89.50ms
step:847/1680 train_time:75805ms step_avg:89.50ms
step:848/1680 train_time:75896ms step_avg:89.50ms
step:849/1680 train_time:75986ms step_avg:89.50ms
step:850/1680 train_time:76076ms step_avg:89.50ms
step:851/1680 train_time:76165ms step_avg:89.50ms
step:852/1680 train_time:76256ms step_avg:89.50ms
step:853/1680 train_time:76347ms step_avg:89.50ms
step:854/1680 train_time:76437ms step_avg:89.51ms
step:855/1680 train_time:76528ms step_avg:89.51ms
step:856/1680 train_time:76619ms step_avg:89.51ms
step:857/1680 train_time:76709ms step_avg:89.51ms
step:858/1680 train_time:76799ms step_avg:89.51ms
step:859/1680 train_time:76889ms step_avg:89.51ms
step:860/1680 train_time:76980ms step_avg:89.51ms
step:861/1680 train_time:77070ms step_avg:89.51ms
step:862/1680 train_time:77160ms step_avg:89.51ms
step:863/1680 train_time:77250ms step_avg:89.51ms
step:864/1680 train_time:77340ms step_avg:89.51ms
step:865/1680 train_time:77430ms step_avg:89.51ms
step:866/1680 train_time:77521ms step_avg:89.52ms
step:867/1680 train_time:77611ms step_avg:89.52ms
step:868/1680 train_time:77702ms step_avg:89.52ms
step:869/1680 train_time:77792ms step_avg:89.52ms
step:870/1680 train_time:77883ms step_avg:89.52ms
step:871/1680 train_time:77973ms step_avg:89.52ms
step:872/1680 train_time:78064ms step_avg:89.52ms
step:873/1680 train_time:78154ms step_avg:89.52ms
step:874/1680 train_time:78244ms step_avg:89.52ms
step:875/1680 train_time:78335ms step_avg:89.53ms
step:875/1680 val_loss:3.5196 train_time:78426ms step_avg:89.63ms
step:876/1680 train_time:78450ms step_avg:89.55ms
step:877/1680 train_time:78521ms step_avg:89.53ms
step:878/1680 train_time:78620ms step_avg:89.54ms
step:879/1680 train_time:78712ms step_avg:89.55ms
step:880/1680 train_time:78800ms step_avg:89.55ms
step:881/1680 train_time:78889ms step_avg:89.55ms
step:882/1680 train_time:78978ms step_avg:89.54ms
step:883/1680 train_time:79067ms step_avg:89.54ms
step:884/1680 train_time:79156ms step_avg:89.54ms
step:885/1680 train_time:79244ms step_avg:89.54ms
step:886/1680 train_time:79334ms step_avg:89.54ms
step:887/1680 train_time:79424ms step_avg:89.54ms
step:888/1680 train_time:79517ms step_avg:89.55ms
step:889/1680 train_time:79611ms step_avg:89.55ms
step:890/1680 train_time:79704ms step_avg:89.55ms
step:891/1680 train_time:79795ms step_avg:89.56ms
step:892/1680 train_time:79885ms step_avg:89.56ms
step:893/1680 train_time:79974ms step_avg:89.56ms
step:894/1680 train_time:80062ms step_avg:89.56ms
step:895/1680 train_time:80152ms step_avg:89.56ms
step:896/1680 train_time:80241ms step_avg:89.55ms
step:897/1680 train_time:80330ms step_avg:89.55ms
step:898/1680 train_time:80420ms step_avg:89.55ms
step:899/1680 train_time:80512ms step_avg:89.56ms
step:900/1680 train_time:80604ms step_avg:89.56ms
step:901/1680 train_time:80698ms step_avg:89.56ms
step:902/1680 train_time:80789ms step_avg:89.57ms
step:903/1680 train_time:80879ms step_avg:89.57ms
step:904/1680 train_time:80970ms step_avg:89.57ms
step:905/1680 train_time:81060ms step_avg:89.57ms
step:906/1680 train_time:81149ms step_avg:89.57ms
step:907/1680 train_time:81240ms step_avg:89.57ms
step:908/1680 train_time:81329ms step_avg:89.57ms
step:909/1680 train_time:81419ms step_avg:89.57ms
step:910/1680 train_time:81510ms step_avg:89.57ms
step:911/1680 train_time:81601ms step_avg:89.57ms
step:912/1680 train_time:81692ms step_avg:89.58ms
step:913/1680 train_time:81784ms step_avg:89.58ms
step:914/1680 train_time:81874ms step_avg:89.58ms
step:915/1680 train_time:81965ms step_avg:89.58ms
step:916/1680 train_time:82054ms step_avg:89.58ms
step:917/1680 train_time:82144ms step_avg:89.58ms
step:918/1680 train_time:82234ms step_avg:89.58ms
step:919/1680 train_time:82323ms step_avg:89.58ms
step:920/1680 train_time:82413ms step_avg:89.58ms
step:921/1680 train_time:82503ms step_avg:89.58ms
step:922/1680 train_time:82595ms step_avg:89.58ms
step:923/1680 train_time:82685ms step_avg:89.58ms
step:924/1680 train_time:82776ms step_avg:89.58ms
step:925/1680 train_time:82867ms step_avg:89.59ms
step:926/1680 train_time:82957ms step_avg:89.59ms
step:927/1680 train_time:83047ms step_avg:89.59ms
step:928/1680 train_time:83138ms step_avg:89.59ms
step:929/1680 train_time:83228ms step_avg:89.59ms
step:930/1680 train_time:83317ms step_avg:89.59ms
step:931/1680 train_time:83408ms step_avg:89.59ms
step:932/1680 train_time:83498ms step_avg:89.59ms
step:933/1680 train_time:83588ms step_avg:89.59ms
step:934/1680 train_time:83679ms step_avg:89.59ms
step:935/1680 train_time:83770ms step_avg:89.59ms
step:936/1680 train_time:83861ms step_avg:89.60ms
step:937/1680 train_time:83951ms step_avg:89.60ms
step:938/1680 train_time:84042ms step_avg:89.60ms
step:939/1680 train_time:84132ms step_avg:89.60ms
step:940/1680 train_time:84221ms step_avg:89.60ms
step:941/1680 train_time:84311ms step_avg:89.60ms
step:942/1680 train_time:84401ms step_avg:89.60ms
step:943/1680 train_time:84490ms step_avg:89.60ms
step:944/1680 train_time:84580ms step_avg:89.60ms
step:945/1680 train_time:84671ms step_avg:89.60ms
step:946/1680 train_time:84761ms step_avg:89.60ms
step:947/1680 train_time:84851ms step_avg:89.60ms
step:948/1680 train_time:84941ms step_avg:89.60ms
step:949/1680 train_time:85032ms step_avg:89.60ms
step:950/1680 train_time:85122ms step_avg:89.60ms
step:951/1680 train_time:85213ms step_avg:89.60ms
step:952/1680 train_time:85303ms step_avg:89.60ms
step:953/1680 train_time:85393ms step_avg:89.60ms
step:954/1680 train_time:85483ms step_avg:89.60ms
step:955/1680 train_time:85573ms step_avg:89.61ms
step:956/1680 train_time:85663ms step_avg:89.61ms
step:957/1680 train_time:85753ms step_avg:89.61ms
step:958/1680 train_time:85843ms step_avg:89.61ms
step:959/1680 train_time:85934ms step_avg:89.61ms
step:960/1680 train_time:86023ms step_avg:89.61ms
step:961/1680 train_time:86114ms step_avg:89.61ms
step:962/1680 train_time:86204ms step_avg:89.61ms
step:963/1680 train_time:86295ms step_avg:89.61ms
step:964/1680 train_time:86385ms step_avg:89.61ms
step:965/1680 train_time:86475ms step_avg:89.61ms
step:966/1680 train_time:86566ms step_avg:89.61ms
step:967/1680 train_time:86655ms step_avg:89.61ms
step:968/1680 train_time:86745ms step_avg:89.61ms
step:969/1680 train_time:86839ms step_avg:89.62ms
step:970/1680 train_time:86927ms step_avg:89.62ms
step:971/1680 train_time:87017ms step_avg:89.62ms
step:972/1680 train_time:87109ms step_avg:89.62ms
step:973/1680 train_time:87198ms step_avg:89.62ms
step:974/1680 train_time:87289ms step_avg:89.62ms
step:975/1680 train_time:87378ms step_avg:89.62ms
step:976/1680 train_time:87468ms step_avg:89.62ms
step:977/1680 train_time:87558ms step_avg:89.62ms
step:978/1680 train_time:87648ms step_avg:89.62ms
step:979/1680 train_time:87739ms step_avg:89.62ms
step:980/1680 train_time:87830ms step_avg:89.62ms
step:981/1680 train_time:87920ms step_avg:89.62ms
step:982/1680 train_time:88011ms step_avg:89.62ms
step:983/1680 train_time:88101ms step_avg:89.62ms
step:984/1680 train_time:88192ms step_avg:89.63ms
step:985/1680 train_time:88282ms step_avg:89.63ms
step:986/1680 train_time:88372ms step_avg:89.63ms
step:987/1680 train_time:88462ms step_avg:89.63ms
step:988/1680 train_time:88552ms step_avg:89.63ms
step:989/1680 train_time:88642ms step_avg:89.63ms
step:990/1680 train_time:88732ms step_avg:89.63ms
step:991/1680 train_time:88822ms step_avg:89.63ms
step:992/1680 train_time:88913ms step_avg:89.63ms
step:993/1680 train_time:89003ms step_avg:89.63ms
step:994/1680 train_time:89094ms step_avg:89.63ms
step:995/1680 train_time:89183ms step_avg:89.63ms
step:996/1680 train_time:89274ms step_avg:89.63ms
step:997/1680 train_time:89364ms step_avg:89.63ms
step:998/1680 train_time:89455ms step_avg:89.63ms
step:999/1680 train_time:89544ms step_avg:89.63ms
step:1000/1680 train_time:89638ms step_avg:89.64ms
step:1000/1680 val_loss:3.4689 train_time:89726ms step_avg:89.73ms
step:1001/1680 train_time:89750ms step_avg:89.66ms
step:1002/1680 train_time:89819ms step_avg:89.64ms
step:1003/1680 train_time:89916ms step_avg:89.65ms
step:1004/1680 train_time:90007ms step_avg:89.65ms
step:1005/1680 train_time:90097ms step_avg:89.65ms
step:1006/1680 train_time:90186ms step_avg:89.65ms
step:1007/1680 train_time:90277ms step_avg:89.65ms
step:1008/1680 train_time:90366ms step_avg:89.65ms
step:1009/1680 train_time:90455ms step_avg:89.65ms
step:1010/1680 train_time:90544ms step_avg:89.65ms
step:1011/1680 train_time:90633ms step_avg:89.65ms
step:1012/1680 train_time:90724ms step_avg:89.65ms
step:1013/1680 train_time:90816ms step_avg:89.65ms
step:1014/1680 train_time:90908ms step_avg:89.65ms
step:1015/1680 train_time:91000ms step_avg:89.66ms
step:1016/1680 train_time:91091ms step_avg:89.66ms
step:1017/1680 train_time:91181ms step_avg:89.66ms
step:1018/1680 train_time:91271ms step_avg:89.66ms
step:1019/1680 train_time:91361ms step_avg:89.66ms
step:1020/1680 train_time:91450ms step_avg:89.66ms
step:1021/1680 train_time:91540ms step_avg:89.66ms
step:1022/1680 train_time:91629ms step_avg:89.66ms
step:1023/1680 train_time:91719ms step_avg:89.66ms
step:1024/1680 train_time:91811ms step_avg:89.66ms
step:1025/1680 train_time:91902ms step_avg:89.66ms
step:1026/1680 train_time:91993ms step_avg:89.66ms
step:1027/1680 train_time:92082ms step_avg:89.66ms
step:1028/1680 train_time:92173ms step_avg:89.66ms
step:1029/1680 train_time:92262ms step_avg:89.66ms
step:1030/1680 train_time:92352ms step_avg:89.66ms
step:1031/1680 train_time:92442ms step_avg:89.66ms
step:1032/1680 train_time:92531ms step_avg:89.66ms
step:1033/1680 train_time:92620ms step_avg:89.66ms
step:1034/1680 train_time:92710ms step_avg:89.66ms
step:1035/1680 train_time:92801ms step_avg:89.66ms
step:1036/1680 train_time:92892ms step_avg:89.66ms
step:1037/1680 train_time:92983ms step_avg:89.67ms
step:1038/1680 train_time:93074ms step_avg:89.67ms
step:1039/1680 train_time:93164ms step_avg:89.67ms
step:1040/1680 train_time:93254ms step_avg:89.67ms
step:1041/1680 train_time:93343ms step_avg:89.67ms
step:1042/1680 train_time:93434ms step_avg:89.67ms
step:1043/1680 train_time:93523ms step_avg:89.67ms
step:1044/1680 train_time:93613ms step_avg:89.67ms
step:1045/1680 train_time:93702ms step_avg:89.67ms
step:1046/1680 train_time:93793ms step_avg:89.67ms
step:1047/1680 train_time:93883ms step_avg:89.67ms
step:1048/1680 train_time:93974ms step_avg:89.67ms
step:1049/1680 train_time:94064ms step_avg:89.67ms
step:1050/1680 train_time:94155ms step_avg:89.67ms
step:1051/1680 train_time:94244ms step_avg:89.67ms
step:1052/1680 train_time:94334ms step_avg:89.67ms
step:1053/1680 train_time:94424ms step_avg:89.67ms
step:1054/1680 train_time:94514ms step_avg:89.67ms
step:1055/1680 train_time:94603ms step_avg:89.67ms
step:1056/1680 train_time:94694ms step_avg:89.67ms
step:1057/1680 train_time:94783ms step_avg:89.67ms
step:1058/1680 train_time:94874ms step_avg:89.67ms
step:1059/1680 train_time:94963ms step_avg:89.67ms
step:1060/1680 train_time:95055ms step_avg:89.67ms
step:1061/1680 train_time:95146ms step_avg:89.68ms
step:1062/1680 train_time:95238ms step_avg:89.68ms
step:1063/1680 train_time:95328ms step_avg:89.68ms
step:1064/1680 train_time:95419ms step_avg:89.68ms
step:1065/1680 train_time:95509ms step_avg:89.68ms
step:1066/1680 train_time:95599ms step_avg:89.68ms
step:1067/1680 train_time:95689ms step_avg:89.68ms
step:1068/1680 train_time:95779ms step_avg:89.68ms
step:1069/1680 train_time:95869ms step_avg:89.68ms
step:1070/1680 train_time:95960ms step_avg:89.68ms
step:1071/1680 train_time:96050ms step_avg:89.68ms
step:1072/1680 train_time:96140ms step_avg:89.68ms
step:1073/1680 train_time:96231ms step_avg:89.68ms
step:1074/1680 train_time:96321ms step_avg:89.68ms
step:1075/1680 train_time:96411ms step_avg:89.69ms
step:1076/1680 train_time:96501ms step_avg:89.69ms
step:1077/1680 train_time:96591ms step_avg:89.69ms
step:1078/1680 train_time:96682ms step_avg:89.69ms
step:1079/1680 train_time:96772ms step_avg:89.69ms
step:1080/1680 train_time:96862ms step_avg:89.69ms
step:1081/1680 train_time:96953ms step_avg:89.69ms
step:1082/1680 train_time:97043ms step_avg:89.69ms
step:1083/1680 train_time:97134ms step_avg:89.69ms
step:1084/1680 train_time:97224ms step_avg:89.69ms
step:1085/1680 train_time:97313ms step_avg:89.69ms
step:1086/1680 train_time:97404ms step_avg:89.69ms
step:1087/1680 train_time:97494ms step_avg:89.69ms
step:1088/1680 train_time:97585ms step_avg:89.69ms
step:1089/1680 train_time:97675ms step_avg:89.69ms
step:1090/1680 train_time:97764ms step_avg:89.69ms
step:1091/1680 train_time:97855ms step_avg:89.69ms
step:1092/1680 train_time:97945ms step_avg:89.69ms
step:1093/1680 train_time:98037ms step_avg:89.70ms
step:1094/1680 train_time:98126ms step_avg:89.69ms
step:1095/1680 train_time:98217ms step_avg:89.70ms
step:1096/1680 train_time:98307ms step_avg:89.70ms
step:1097/1680 train_time:98399ms step_avg:89.70ms
step:1098/1680 train_time:98489ms step_avg:89.70ms
step:1099/1680 train_time:98580ms step_avg:89.70ms
step:1100/1680 train_time:98670ms step_avg:89.70ms
step:1101/1680 train_time:98761ms step_avg:89.70ms
step:1102/1680 train_time:98853ms step_avg:89.70ms
step:1103/1680 train_time:98943ms step_avg:89.70ms
step:1104/1680 train_time:99034ms step_avg:89.70ms
step:1105/1680 train_time:99125ms step_avg:89.71ms
step:1106/1680 train_time:99215ms step_avg:89.71ms
step:1107/1680 train_time:99307ms step_avg:89.71ms
step:1108/1680 train_time:99398ms step_avg:89.71ms
step:1109/1680 train_time:99489ms step_avg:89.71ms
step:1110/1680 train_time:99580ms step_avg:89.71ms
step:1111/1680 train_time:99671ms step_avg:89.71ms
step:1112/1680 train_time:99762ms step_avg:89.71ms
step:1113/1680 train_time:99853ms step_avg:89.71ms
step:1114/1680 train_time:99943ms step_avg:89.72ms
step:1115/1680 train_time:100035ms step_avg:89.72ms
step:1116/1680 train_time:100125ms step_avg:89.72ms
step:1117/1680 train_time:100216ms step_avg:89.72ms
step:1118/1680 train_time:100307ms step_avg:89.72ms
step:1119/1680 train_time:100398ms step_avg:89.72ms
step:1120/1680 train_time:100489ms step_avg:89.72ms
step:1121/1680 train_time:100580ms step_avg:89.72ms
step:1122/1680 train_time:100672ms step_avg:89.73ms
step:1123/1680 train_time:100762ms step_avg:89.73ms
step:1124/1680 train_time:100854ms step_avg:89.73ms
step:1125/1680 train_time:100944ms step_avg:89.73ms
step:1125/1680 val_loss:3.4153 train_time:101037ms step_avg:89.81ms
step:1126/1680 train_time:101060ms step_avg:89.75ms
step:1127/1680 train_time:101131ms step_avg:89.74ms
step:1128/1680 train_time:101230ms step_avg:89.74ms
step:1129/1680 train_time:101324ms step_avg:89.75ms
step:1130/1680 train_time:101414ms step_avg:89.75ms
step:1131/1680 train_time:101504ms step_avg:89.75ms
step:1132/1680 train_time:101594ms step_avg:89.75ms
step:1133/1680 train_time:101684ms step_avg:89.75ms
step:1134/1680 train_time:101773ms step_avg:89.75ms
step:1135/1680 train_time:101863ms step_avg:89.75ms
step:1136/1680 train_time:101952ms step_avg:89.75ms
step:1137/1680 train_time:102043ms step_avg:89.75ms
step:1138/1680 train_time:102135ms step_avg:89.75ms
step:1139/1680 train_time:102229ms step_avg:89.75ms
step:1140/1680 train_time:102323ms step_avg:89.76ms
step:1141/1680 train_time:102414ms step_avg:89.76ms
step:1142/1680 train_time:102505ms step_avg:89.76ms
step:1143/1680 train_time:102595ms step_avg:89.76ms
step:1144/1680 train_time:102684ms step_avg:89.76ms
step:1145/1680 train_time:102774ms step_avg:89.76ms
step:1146/1680 train_time:102864ms step_avg:89.76ms
step:1147/1680 train_time:102954ms step_avg:89.76ms
step:1148/1680 train_time:103046ms step_avg:89.76ms
step:1149/1680 train_time:103138ms step_avg:89.76ms
step:1150/1680 train_time:103230ms step_avg:89.77ms
step:1151/1680 train_time:103324ms step_avg:89.77ms
step:1152/1680 train_time:103415ms step_avg:89.77ms
step:1153/1680 train_time:103506ms step_avg:89.77ms
step:1154/1680 train_time:103596ms step_avg:89.77ms
step:1155/1680 train_time:103687ms step_avg:89.77ms
step:1156/1680 train_time:103776ms step_avg:89.77ms
step:1157/1680 train_time:103866ms step_avg:89.77ms
step:1158/1680 train_time:103956ms step_avg:89.77ms
step:1159/1680 train_time:104047ms step_avg:89.77ms
step:1160/1680 train_time:104138ms step_avg:89.77ms
step:1161/1680 train_time:104229ms step_avg:89.78ms
step:1162/1680 train_time:104322ms step_avg:89.78ms
step:1163/1680 train_time:104413ms step_avg:89.78ms
step:1164/1680 train_time:104504ms step_avg:89.78ms
step:1165/1680 train_time:104595ms step_avg:89.78ms
step:1166/1680 train_time:104685ms step_avg:89.78ms
step:1167/1680 train_time:104775ms step_avg:89.78ms
step:1168/1680 train_time:104866ms step_avg:89.78ms
step:1169/1680 train_time:104957ms step_avg:89.78ms
step:1170/1680 train_time:105047ms step_avg:89.78ms
step:1171/1680 train_time:105138ms step_avg:89.79ms
step:1172/1680 train_time:105229ms step_avg:89.79ms
step:1173/1680 train_time:105322ms step_avg:89.79ms
step:1174/1680 train_time:105414ms step_avg:89.79ms
step:1175/1680 train_time:105504ms step_avg:89.79ms
step:1176/1680 train_time:105595ms step_avg:89.79ms
step:1177/1680 train_time:105685ms step_avg:89.79ms
step:1178/1680 train_time:105775ms step_avg:89.79ms
step:1179/1680 train_time:105866ms step_avg:89.79ms
step:1180/1680 train_time:105956ms step_avg:89.79ms
step:1181/1680 train_time:106046ms step_avg:89.79ms
step:1182/1680 train_time:106138ms step_avg:89.79ms
step:1183/1680 train_time:106228ms step_avg:89.80ms
step:1184/1680 train_time:106320ms step_avg:89.80ms
step:1185/1680 train_time:106411ms step_avg:89.80ms
step:1186/1680 train_time:106502ms step_avg:89.80ms
step:1187/1680 train_time:106592ms step_avg:89.80ms
step:1188/1680 train_time:106682ms step_avg:89.80ms
step:1189/1680 train_time:106773ms step_avg:89.80ms
step:1190/1680 train_time:106864ms step_avg:89.80ms
step:1191/1680 train_time:106954ms step_avg:89.80ms
step:1192/1680 train_time:107046ms step_avg:89.80ms
step:1193/1680 train_time:107138ms step_avg:89.81ms
step:1194/1680 train_time:107229ms step_avg:89.81ms
step:1195/1680 train_time:107320ms step_avg:89.81ms
step:1196/1680 train_time:107410ms step_avg:89.81ms
step:1197/1680 train_time:107501ms step_avg:89.81ms
step:1198/1680 train_time:107591ms step_avg:89.81ms
step:1199/1680 train_time:107682ms step_avg:89.81ms
step:1200/1680 train_time:107773ms step_avg:89.81ms
step:1201/1680 train_time:107863ms step_avg:89.81ms
step:1202/1680 train_time:107953ms step_avg:89.81ms
step:1203/1680 train_time:108045ms step_avg:89.81ms
step:1204/1680 train_time:108136ms step_avg:89.81ms
step:1205/1680 train_time:108227ms step_avg:89.82ms
step:1206/1680 train_time:108319ms step_avg:89.82ms
step:1207/1680 train_time:108409ms step_avg:89.82ms
step:1208/1680 train_time:108500ms step_avg:89.82ms
step:1209/1680 train_time:108590ms step_avg:89.82ms
step:1210/1680 train_time:108681ms step_avg:89.82ms
step:1211/1680 train_time:108772ms step_avg:89.82ms
step:1212/1680 train_time:108863ms step_avg:89.82ms
step:1213/1680 train_time:108953ms step_avg:89.82ms
step:1214/1680 train_time:109045ms step_avg:89.82ms
step:1215/1680 train_time:109136ms step_avg:89.82ms
step:1216/1680 train_time:109227ms step_avg:89.83ms
step:1217/1680 train_time:109319ms step_avg:89.83ms
step:1218/1680 train_time:109410ms step_avg:89.83ms
step:1219/1680 train_time:109500ms step_avg:89.83ms
step:1220/1680 train_time:109590ms step_avg:89.83ms
step:1221/1680 train_time:109680ms step_avg:89.83ms
step:1222/1680 train_time:109771ms step_avg:89.83ms
step:1223/1680 train_time:109862ms step_avg:89.83ms
step:1224/1680 train_time:109952ms step_avg:89.83ms
step:1225/1680 train_time:110043ms step_avg:89.83ms
step:1226/1680 train_time:110134ms step_avg:89.83ms
step:1227/1680 train_time:110225ms step_avg:89.83ms
step:1228/1680 train_time:110318ms step_avg:89.84ms
step:1229/1680 train_time:110408ms step_avg:89.84ms
step:1230/1680 train_time:110499ms step_avg:89.84ms
step:1231/1680 train_time:110590ms step_avg:89.84ms
step:1232/1680 train_time:110681ms step_avg:89.84ms
step:1233/1680 train_time:110771ms step_avg:89.84ms
step:1234/1680 train_time:110861ms step_avg:89.84ms
step:1235/1680 train_time:110951ms step_avg:89.84ms
step:1236/1680 train_time:111043ms step_avg:89.84ms
step:1237/1680 train_time:111133ms step_avg:89.84ms
step:1238/1680 train_time:111225ms step_avg:89.84ms
step:1239/1680 train_time:111316ms step_avg:89.84ms
step:1240/1680 train_time:111407ms step_avg:89.84ms
step:1241/1680 train_time:111497ms step_avg:89.84ms
step:1242/1680 train_time:111588ms step_avg:89.85ms
step:1243/1680 train_time:111679ms step_avg:89.85ms
step:1244/1680 train_time:111770ms step_avg:89.85ms
step:1245/1680 train_time:111861ms step_avg:89.85ms
step:1246/1680 train_time:111954ms step_avg:89.85ms
step:1247/1680 train_time:112043ms step_avg:89.85ms
step:1248/1680 train_time:112133ms step_avg:89.85ms
step:1249/1680 train_time:112224ms step_avg:89.85ms
step:1250/1680 train_time:112315ms step_avg:89.85ms
step:1250/1680 val_loss:3.3770 train_time:112407ms step_avg:89.93ms
step:1251/1680 train_time:112430ms step_avg:89.87ms
step:1252/1680 train_time:112503ms step_avg:89.86ms
step:1253/1680 train_time:112601ms step_avg:89.87ms
step:1254/1680 train_time:112693ms step_avg:89.87ms
step:1255/1680 train_time:112783ms step_avg:89.87ms
step:1256/1680 train_time:112873ms step_avg:89.87ms
step:1257/1680 train_time:112962ms step_avg:89.87ms
step:1258/1680 train_time:113052ms step_avg:89.87ms
step:1259/1680 train_time:113142ms step_avg:89.87ms
step:1260/1680 train_time:113231ms step_avg:89.87ms
step:1261/1680 train_time:113321ms step_avg:89.87ms
step:1262/1680 train_time:113413ms step_avg:89.87ms
step:1263/1680 train_time:113506ms step_avg:89.87ms
step:1264/1680 train_time:113600ms step_avg:89.87ms
step:1265/1680 train_time:113692ms step_avg:89.88ms
step:1266/1680 train_time:113783ms step_avg:89.88ms
step:1267/1680 train_time:113880ms step_avg:89.88ms
step:1268/1680 train_time:113964ms step_avg:89.88ms
step:1269/1680 train_time:114054ms step_avg:89.88ms
step:1270/1680 train_time:114143ms step_avg:89.88ms
step:1271/1680 train_time:114233ms step_avg:89.88ms
step:1272/1680 train_time:114323ms step_avg:89.88ms
step:1273/1680 train_time:114414ms step_avg:89.88ms
step:1274/1680 train_time:114507ms step_avg:89.88ms
step:1275/1680 train_time:114599ms step_avg:89.88ms
step:1276/1680 train_time:114691ms step_avg:89.88ms
step:1277/1680 train_time:114783ms step_avg:89.88ms
step:1278/1680 train_time:114874ms step_avg:89.89ms
step:1279/1680 train_time:114964ms step_avg:89.89ms
step:1280/1680 train_time:115055ms step_avg:89.89ms
step:1281/1680 train_time:115145ms step_avg:89.89ms
step:1282/1680 train_time:115235ms step_avg:89.89ms
step:1283/1680 train_time:115325ms step_avg:89.89ms
step:1284/1680 train_time:115417ms step_avg:89.89ms
step:1285/1680 train_time:115508ms step_avg:89.89ms
step:1286/1680 train_time:115599ms step_avg:89.89ms
step:1287/1680 train_time:115690ms step_avg:89.89ms
step:1288/1680 train_time:115783ms step_avg:89.89ms
step:1289/1680 train_time:115875ms step_avg:89.90ms
step:1290/1680 train_time:115965ms step_avg:89.90ms
step:1291/1680 train_time:116056ms step_avg:89.90ms
step:1292/1680 train_time:116146ms step_avg:89.90ms
step:1293/1680 train_time:116235ms step_avg:89.90ms
step:1294/1680 train_time:116326ms step_avg:89.90ms
step:1295/1680 train_time:116417ms step_avg:89.90ms
step:1296/1680 train_time:116509ms step_avg:89.90ms
step:1297/1680 train_time:116600ms step_avg:89.90ms
step:1298/1680 train_time:116691ms step_avg:89.90ms
step:1299/1680 train_time:116784ms step_avg:89.90ms
step:1300/1680 train_time:116875ms step_avg:89.90ms
step:1301/1680 train_time:116966ms step_avg:89.90ms
step:1302/1680 train_time:117057ms step_avg:89.91ms
step:1303/1680 train_time:117147ms step_avg:89.91ms
step:1304/1680 train_time:117239ms step_avg:89.91ms
step:1305/1680 train_time:117329ms step_avg:89.91ms
step:1306/1680 train_time:117420ms step_avg:89.91ms
step:1307/1680 train_time:117510ms step_avg:89.91ms
step:1308/1680 train_time:117601ms step_avg:89.91ms
step:1309/1680 train_time:117693ms step_avg:89.91ms
step:1310/1680 train_time:117784ms step_avg:89.91ms
step:1311/1680 train_time:117875ms step_avg:89.91ms
step:1312/1680 train_time:117966ms step_avg:89.91ms
step:1313/1680 train_time:118058ms step_avg:89.91ms
step:1314/1680 train_time:118148ms step_avg:89.92ms
step:1315/1680 train_time:118239ms step_avg:89.92ms
step:1316/1680 train_time:118330ms step_avg:89.92ms
step:1317/1680 train_time:118420ms step_avg:89.92ms
step:1318/1680 train_time:118511ms step_avg:89.92ms
step:1319/1680 train_time:118602ms step_avg:89.92ms
step:1320/1680 train_time:118692ms step_avg:89.92ms
step:1321/1680 train_time:118784ms step_avg:89.92ms
step:1322/1680 train_time:118876ms step_avg:89.92ms
step:1323/1680 train_time:118966ms step_avg:89.92ms
step:1324/1680 train_time:119057ms step_avg:89.92ms
step:1325/1680 train_time:119147ms step_avg:89.92ms
step:1326/1680 train_time:119237ms step_avg:89.92ms
step:1327/1680 train_time:119327ms step_avg:89.92ms
step:1328/1680 train_time:119418ms step_avg:89.92ms
step:1329/1680 train_time:119509ms step_avg:89.92ms
step:1330/1680 train_time:119600ms step_avg:89.92ms
step:1331/1680 train_time:119690ms step_avg:89.93ms
step:1332/1680 train_time:119782ms step_avg:89.93ms
step:1333/1680 train_time:119872ms step_avg:89.93ms
step:1334/1680 train_time:119964ms step_avg:89.93ms
step:1335/1680 train_time:120054ms step_avg:89.93ms
step:1336/1680 train_time:120145ms step_avg:89.93ms
step:1337/1680 train_time:120235ms step_avg:89.93ms
step:1338/1680 train_time:120325ms step_avg:89.93ms
step:1339/1680 train_time:120415ms step_avg:89.93ms
step:1340/1680 train_time:120507ms step_avg:89.93ms
step:1341/1680 train_time:120597ms step_avg:89.93ms
step:1342/1680 train_time:120687ms step_avg:89.93ms
step:1343/1680 train_time:120778ms step_avg:89.93ms
step:1344/1680 train_time:120869ms step_avg:89.93ms
step:1345/1680 train_time:120961ms step_avg:89.93ms
step:1346/1680 train_time:121052ms step_avg:89.93ms
step:1347/1680 train_time:121144ms step_avg:89.94ms
step:1348/1680 train_time:121234ms step_avg:89.94ms
step:1349/1680 train_time:121325ms step_avg:89.94ms
step:1350/1680 train_time:121416ms step_avg:89.94ms
step:1351/1680 train_time:121507ms step_avg:89.94ms
step:1352/1680 train_time:121598ms step_avg:89.94ms
step:1353/1680 train_time:121689ms step_avg:89.94ms
step:1354/1680 train_time:121780ms step_avg:89.94ms
step:1355/1680 train_time:121871ms step_avg:89.94ms
step:1356/1680 train_time:121962ms step_avg:89.94ms
step:1357/1680 train_time:122053ms step_avg:89.94ms
step:1358/1680 train_time:122144ms step_avg:89.94ms
step:1359/1680 train_time:122236ms step_avg:89.95ms
step:1360/1680 train_time:122326ms step_avg:89.95ms
step:1361/1680 train_time:122417ms step_avg:89.95ms
step:1362/1680 train_time:122507ms step_avg:89.95ms
step:1363/1680 train_time:122598ms step_avg:89.95ms
step:1364/1680 train_time:122688ms step_avg:89.95ms
step:1365/1680 train_time:122779ms step_avg:89.95ms
step:1366/1680 train_time:122869ms step_avg:89.95ms
step:1367/1680 train_time:122961ms step_avg:89.95ms
step:1368/1680 train_time:123053ms step_avg:89.95ms
step:1369/1680 train_time:123143ms step_avg:89.95ms
step:1370/1680 train_time:123234ms step_avg:89.95ms
step:1371/1680 train_time:123324ms step_avg:89.95ms
step:1372/1680 train_time:123415ms step_avg:89.95ms
step:1373/1680 train_time:123506ms step_avg:89.95ms
step:1374/1680 train_time:123597ms step_avg:89.95ms
step:1375/1680 train_time:123687ms step_avg:89.95ms
step:1375/1680 val_loss:3.3424 train_time:123779ms step_avg:90.02ms
step:1376/1680 train_time:123802ms step_avg:89.97ms
step:1377/1680 train_time:123875ms step_avg:89.96ms
step:1378/1680 train_time:123970ms step_avg:89.96ms
step:1379/1680 train_time:124061ms step_avg:89.96ms
step:1380/1680 train_time:124150ms step_avg:89.96ms
step:1381/1680 train_time:124240ms step_avg:89.96ms
step:1382/1680 train_time:124330ms step_avg:89.96ms
step:1383/1680 train_time:124419ms step_avg:89.96ms
step:1384/1680 train_time:124509ms step_avg:89.96ms
step:1385/1680 train_time:124600ms step_avg:89.96ms
step:1386/1680 train_time:124690ms step_avg:89.96ms
step:1387/1680 train_time:124782ms step_avg:89.97ms
step:1388/1680 train_time:124875ms step_avg:89.97ms
step:1389/1680 train_time:124968ms step_avg:89.97ms
step:1390/1680 train_time:125060ms step_avg:89.97ms
step:1391/1680 train_time:125151ms step_avg:89.97ms
step:1392/1680 train_time:125242ms step_avg:89.97ms
step:1393/1680 train_time:125331ms step_avg:89.97ms
step:1394/1680 train_time:125421ms step_avg:89.97ms
step:1395/1680 train_time:125510ms step_avg:89.97ms
step:1396/1680 train_time:125601ms step_avg:89.97ms
step:1397/1680 train_time:125692ms step_avg:89.97ms
step:1398/1680 train_time:125783ms step_avg:89.97ms
step:1399/1680 train_time:125876ms step_avg:89.98ms
step:1400/1680 train_time:125968ms step_avg:89.98ms
step:1401/1680 train_time:126061ms step_avg:89.98ms
step:1402/1680 train_time:126152ms step_avg:89.98ms
step:1403/1680 train_time:126243ms step_avg:89.98ms
step:1404/1680 train_time:126333ms step_avg:89.98ms
step:1405/1680 train_time:126423ms step_avg:89.98ms
step:1406/1680 train_time:126514ms step_avg:89.98ms
step:1407/1680 train_time:126604ms step_avg:89.98ms
step:1408/1680 train_time:126695ms step_avg:89.98ms
step:1409/1680 train_time:126786ms step_avg:89.98ms
step:1410/1680 train_time:126878ms step_avg:89.98ms
step:1411/1680 train_time:126969ms step_avg:89.99ms
step:1412/1680 train_time:127061ms step_avg:89.99ms
step:1413/1680 train_time:127151ms step_avg:89.99ms
step:1414/1680 train_time:127242ms step_avg:89.99ms
step:1415/1680 train_time:127332ms step_avg:89.99ms
step:1416/1680 train_time:127423ms step_avg:89.99ms
step:1417/1680 train_time:127513ms step_avg:89.99ms
step:1418/1680 train_time:127603ms step_avg:89.99ms
step:1419/1680 train_time:127694ms step_avg:89.99ms
step:1420/1680 train_time:127785ms step_avg:89.99ms
step:1421/1680 train_time:127877ms step_avg:89.99ms
step:1422/1680 train_time:127969ms step_avg:89.99ms
step:1423/1680 train_time:128060ms step_avg:89.99ms
step:1424/1680 train_time:128150ms step_avg:89.99ms
step:1425/1680 train_time:128242ms step_avg:89.99ms
step:1426/1680 train_time:128333ms step_avg:89.99ms
step:1427/1680 train_time:128424ms step_avg:90.00ms
step:1428/1680 train_time:128514ms step_avg:90.00ms
step:1429/1680 train_time:128605ms step_avg:90.00ms
step:1430/1680 train_time:128697ms step_avg:90.00ms
step:1431/1680 train_time:128787ms step_avg:90.00ms
step:1432/1680 train_time:128878ms step_avg:90.00ms
step:1433/1680 train_time:128970ms step_avg:90.00ms
step:1434/1680 train_time:129061ms step_avg:90.00ms
step:1435/1680 train_time:129152ms step_avg:90.00ms
step:1436/1680 train_time:129243ms step_avg:90.00ms
step:1437/1680 train_time:129334ms step_avg:90.00ms
step:1438/1680 train_time:129425ms step_avg:90.00ms
step:1439/1680 train_time:129515ms step_avg:90.00ms
step:1440/1680 train_time:129606ms step_avg:90.00ms
step:1441/1680 train_time:129697ms step_avg:90.01ms
step:1442/1680 train_time:129788ms step_avg:90.01ms
step:1443/1680 train_time:129878ms step_avg:90.01ms
step:1444/1680 train_time:129969ms step_avg:90.01ms
step:1445/1680 train_time:130061ms step_avg:90.01ms
step:1446/1680 train_time:130152ms step_avg:90.01ms
step:1447/1680 train_time:130243ms step_avg:90.01ms
step:1448/1680 train_time:130333ms step_avg:90.01ms
step:1449/1680 train_time:130425ms step_avg:90.01ms
step:1450/1680 train_time:130515ms step_avg:90.01ms
step:1451/1680 train_time:130606ms step_avg:90.01ms
step:1452/1680 train_time:130696ms step_avg:90.01ms
step:1453/1680 train_time:130786ms step_avg:90.01ms
step:1454/1680 train_time:130876ms step_avg:90.01ms
step:1455/1680 train_time:130967ms step_avg:90.01ms
step:1456/1680 train_time:131059ms step_avg:90.01ms
step:1457/1680 train_time:131150ms step_avg:90.01ms
step:1458/1680 train_time:131241ms step_avg:90.01ms
step:1459/1680 train_time:131332ms step_avg:90.02ms
step:1460/1680 train_time:131425ms step_avg:90.02ms
step:1461/1680 train_time:131516ms step_avg:90.02ms
step:1462/1680 train_time:131607ms step_avg:90.02ms
step:1463/1680 train_time:131698ms step_avg:90.02ms
step:1464/1680 train_time:131788ms step_avg:90.02ms
step:1465/1680 train_time:131879ms step_avg:90.02ms
step:1466/1680 train_time:131969ms step_avg:90.02ms
step:1467/1680 train_time:132060ms step_avg:90.02ms
step:1468/1680 train_time:132151ms step_avg:90.02ms
step:1469/1680 train_time:132243ms step_avg:90.02ms
step:1470/1680 train_time:132333ms step_avg:90.02ms
step:1471/1680 train_time:132424ms step_avg:90.02ms
step:1472/1680 train_time:132516ms step_avg:90.02ms
step:1473/1680 train_time:132606ms step_avg:90.02ms
step:1474/1680 train_time:132697ms step_avg:90.03ms
step:1475/1680 train_time:132788ms step_avg:90.03ms
step:1476/1680 train_time:132879ms step_avg:90.03ms
step:1477/1680 train_time:132969ms step_avg:90.03ms
step:1478/1680 train_time:133060ms step_avg:90.03ms
step:1479/1680 train_time:133151ms step_avg:90.03ms
step:1480/1680 train_time:133243ms step_avg:90.03ms
step:1481/1680 train_time:133333ms step_avg:90.03ms
step:1482/1680 train_time:133425ms step_avg:90.03ms
step:1483/1680 train_time:133517ms step_avg:90.03ms
step:1484/1680 train_time:133608ms step_avg:90.03ms
step:1485/1680 train_time:133699ms step_avg:90.03ms
step:1486/1680 train_time:133789ms step_avg:90.03ms
step:1487/1680 train_time:133881ms step_avg:90.03ms
step:1488/1680 train_time:133971ms step_avg:90.03ms
step:1489/1680 train_time:134062ms step_avg:90.03ms
step:1490/1680 train_time:134152ms step_avg:90.03ms
step:1491/1680 train_time:134243ms step_avg:90.04ms
step:1492/1680 train_time:134333ms step_avg:90.04ms
step:1493/1680 train_time:134424ms step_avg:90.04ms
step:1494/1680 train_time:134516ms step_avg:90.04ms
step:1495/1680 train_time:134607ms step_avg:90.04ms
step:1496/1680 train_time:134698ms step_avg:90.04ms
step:1497/1680 train_time:134788ms step_avg:90.04ms
step:1498/1680 train_time:134879ms step_avg:90.04ms
step:1499/1680 train_time:134969ms step_avg:90.04ms
step:1500/1680 train_time:135059ms step_avg:90.04ms
step:1500/1680 val_loss:3.3126 train_time:135152ms step_avg:90.10ms
step:1501/1680 train_time:135175ms step_avg:90.06ms
step:1502/1680 train_time:135245ms step_avg:90.04ms
step:1503/1680 train_time:135345ms step_avg:90.05ms
step:1504/1680 train_time:135437ms step_avg:90.05ms
step:1505/1680 train_time:135526ms step_avg:90.05ms
step:1506/1680 train_time:135616ms step_avg:90.05ms
step:1507/1680 train_time:135705ms step_avg:90.05ms
step:1508/1680 train_time:135795ms step_avg:90.05ms
step:1509/1680 train_time:135884ms step_avg:90.05ms
step:1510/1680 train_time:135975ms step_avg:90.05ms
step:1511/1680 train_time:136065ms step_avg:90.05ms
step:1512/1680 train_time:136159ms step_avg:90.05ms
step:1513/1680 train_time:136252ms step_avg:90.05ms
step:1514/1680 train_time:136345ms step_avg:90.06ms
step:1515/1680 train_time:136436ms step_avg:90.06ms
step:1516/1680 train_time:136527ms step_avg:90.06ms
step:1517/1680 train_time:136618ms step_avg:90.06ms
step:1518/1680 train_time:136707ms step_avg:90.06ms
step:1519/1680 train_time:136798ms step_avg:90.06ms
step:1520/1680 train_time:136887ms step_avg:90.06ms
step:1521/1680 train_time:136977ms step_avg:90.06ms
step:1522/1680 train_time:137067ms step_avg:90.06ms
step:1523/1680 train_time:137160ms step_avg:90.06ms
step:1524/1680 train_time:137252ms step_avg:90.06ms
step:1525/1680 train_time:137343ms step_avg:90.06ms
step:1526/1680 train_time:137434ms step_avg:90.06ms
step:1527/1680 train_time:137525ms step_avg:90.06ms
step:1528/1680 train_time:137616ms step_avg:90.06ms
step:1529/1680 train_time:137706ms step_avg:90.06ms
step:1530/1680 train_time:137797ms step_avg:90.06ms
step:1531/1680 train_time:137887ms step_avg:90.06ms
step:1532/1680 train_time:137978ms step_avg:90.06ms
step:1533/1680 train_time:138069ms step_avg:90.06ms
step:1534/1680 train_time:138161ms step_avg:90.07ms
step:1535/1680 train_time:138251ms step_avg:90.07ms
step:1536/1680 train_time:138343ms step_avg:90.07ms
step:1537/1680 train_time:138434ms step_avg:90.07ms
step:1538/1680 train_time:138524ms step_avg:90.07ms
step:1539/1680 train_time:138615ms step_avg:90.07ms
step:1540/1680 train_time:138705ms step_avg:90.07ms
step:1541/1680 train_time:138796ms step_avg:90.07ms
step:1542/1680 train_time:138886ms step_avg:90.07ms
step:1543/1680 train_time:138977ms step_avg:90.07ms
step:1544/1680 train_time:139070ms step_avg:90.07ms
step:1545/1680 train_time:139159ms step_avg:90.07ms
step:1546/1680 train_time:139250ms step_avg:90.07ms
step:1547/1680 train_time:139341ms step_avg:90.07ms
step:1548/1680 train_time:139433ms step_avg:90.07ms
step:1549/1680 train_time:139523ms step_avg:90.07ms
step:1550/1680 train_time:139614ms step_avg:90.07ms
step:1551/1680 train_time:139704ms step_avg:90.07ms
step:1552/1680 train_time:139795ms step_avg:90.07ms
step:1553/1680 train_time:139885ms step_avg:90.07ms
step:1554/1680 train_time:139976ms step_avg:90.07ms
step:1555/1680 train_time:140066ms step_avg:90.07ms
step:1556/1680 train_time:140158ms step_avg:90.08ms
step:1557/1680 train_time:140249ms step_avg:90.08ms
step:1558/1680 train_time:140339ms step_avg:90.08ms
step:1559/1680 train_time:140430ms step_avg:90.08ms
step:1560/1680 train_time:140521ms step_avg:90.08ms
step:1561/1680 train_time:140611ms step_avg:90.08ms
step:1562/1680 train_time:140702ms step_avg:90.08ms
step:1563/1680 train_time:140792ms step_avg:90.08ms
step:1564/1680 train_time:140882ms step_avg:90.08ms
step:1565/1680 train_time:140972ms step_avg:90.08ms
step:1566/1680 train_time:141064ms step_avg:90.08ms
step:1567/1680 train_time:141156ms step_avg:90.08ms
step:1568/1680 train_time:141247ms step_avg:90.08ms
step:1569/1680 train_time:141339ms step_avg:90.08ms
step:1570/1680 train_time:141430ms step_avg:90.08ms
step:1571/1680 train_time:141521ms step_avg:90.08ms
step:1572/1680 train_time:141612ms step_avg:90.08ms
step:1573/1680 train_time:141702ms step_avg:90.08ms
step:1574/1680 train_time:141793ms step_avg:90.08ms
step:1575/1680 train_time:141884ms step_avg:90.09ms
step:1576/1680 train_time:141974ms step_avg:90.09ms
step:1577/1680 train_time:142064ms step_avg:90.09ms
step:1578/1680 train_time:142156ms step_avg:90.09ms
step:1579/1680 train_time:142246ms step_avg:90.09ms
step:1580/1680 train_time:142338ms step_avg:90.09ms
step:1581/1680 train_time:142429ms step_avg:90.09ms
step:1582/1680 train_time:142521ms step_avg:90.09ms
step:1583/1680 train_time:142612ms step_avg:90.09ms
step:1584/1680 train_time:142702ms step_avg:90.09ms
step:1585/1680 train_time:142793ms step_avg:90.09ms
step:1586/1680 train_time:142883ms step_avg:90.09ms
step:1587/1680 train_time:142975ms step_avg:90.09ms
step:1588/1680 train_time:143065ms step_avg:90.09ms
step:1589/1680 train_time:143157ms step_avg:90.09ms
step:1590/1680 train_time:143247ms step_avg:90.09ms
step:1591/1680 train_time:143339ms step_avg:90.09ms
step:1592/1680 train_time:143430ms step_avg:90.09ms
step:1593/1680 train_time:143521ms step_avg:90.09ms
step:1594/1680 train_time:143612ms step_avg:90.10ms
step:1595/1680 train_time:143703ms step_avg:90.10ms
step:1596/1680 train_time:143793ms step_avg:90.10ms
step:1597/1680 train_time:143885ms step_avg:90.10ms
step:1598/1680 train_time:143975ms step_avg:90.10ms
step:1599/1680 train_time:144066ms step_avg:90.10ms
step:1600/1680 train_time:144157ms step_avg:90.10ms
step:1601/1680 train_time:144248ms step_avg:90.10ms
step:1602/1680 train_time:144339ms step_avg:90.10ms
step:1603/1680 train_time:144429ms step_avg:90.10ms
step:1604/1680 train_time:144520ms step_avg:90.10ms
step:1605/1680 train_time:144611ms step_avg:90.10ms
step:1606/1680 train_time:144701ms step_avg:90.10ms
step:1607/1680 train_time:144791ms step_avg:90.10ms
step:1608/1680 train_time:144883ms step_avg:90.10ms
step:1609/1680 train_time:144973ms step_avg:90.10ms
step:1610/1680 train_time:145068ms step_avg:90.10ms
step:1611/1680 train_time:145155ms step_avg:90.10ms
step:1612/1680 train_time:145245ms step_avg:90.10ms
step:1613/1680 train_time:145337ms step_avg:90.10ms
step:1614/1680 train_time:145427ms step_avg:90.10ms
step:1615/1680 train_time:145518ms step_avg:90.10ms
step:1616/1680 train_time:145609ms step_avg:90.10ms
step:1617/1680 train_time:145699ms step_avg:90.10ms
step:1618/1680 train_time:145790ms step_avg:90.11ms
step:1619/1680 train_time:145880ms step_avg:90.11ms
step:1620/1680 train_time:145971ms step_avg:90.11ms
step:1621/1680 train_time:146063ms step_avg:90.11ms
step:1622/1680 train_time:146154ms step_avg:90.11ms
step:1623/1680 train_time:146244ms step_avg:90.11ms
step:1624/1680 train_time:146336ms step_avg:90.11ms
step:1625/1680 train_time:146427ms step_avg:90.11ms
step:1625/1680 val_loss:3.2887 train_time:146520ms step_avg:90.17ms
step:1626/1680 train_time:146543ms step_avg:90.12ms
step:1627/1680 train_time:146613ms step_avg:90.11ms
step:1628/1680 train_time:146712ms step_avg:90.12ms
step:1629/1680 train_time:146808ms step_avg:90.12ms
step:1630/1680 train_time:146896ms step_avg:90.12ms
step:1631/1680 train_time:146986ms step_avg:90.12ms
step:1632/1680 train_time:147076ms step_avg:90.12ms
step:1633/1680 train_time:147165ms step_avg:90.12ms
step:1634/1680 train_time:147255ms step_avg:90.12ms
step:1635/1680 train_time:147345ms step_avg:90.12ms
step:1636/1680 train_time:147435ms step_avg:90.12ms
step:1637/1680 train_time:147525ms step_avg:90.12ms
step:1638/1680 train_time:147619ms step_avg:90.12ms
step:1639/1680 train_time:147712ms step_avg:90.12ms
step:1640/1680 train_time:147804ms step_avg:90.12ms
step:1641/1680 train_time:147896ms step_avg:90.13ms
step:1642/1680 train_time:147987ms step_avg:90.13ms
step:1643/1680 train_time:148077ms step_avg:90.13ms
step:1644/1680 train_time:148166ms step_avg:90.13ms
step:1645/1680 train_time:148256ms step_avg:90.13ms
step:1646/1680 train_time:148345ms step_avg:90.12ms
step:1647/1680 train_time:148435ms step_avg:90.12ms
step:1648/1680 train_time:148526ms step_avg:90.12ms
step:1649/1680 train_time:148618ms step_avg:90.13ms
step:1650/1680 train_time:148710ms step_avg:90.13ms
step:1651/1680 train_time:148801ms step_avg:90.13ms
step:1652/1680 train_time:148893ms step_avg:90.13ms
step:1653/1680 train_time:148983ms step_avg:90.13ms
step:1654/1680 train_time:149074ms step_avg:90.13ms
step:1655/1680 train_time:149164ms step_avg:90.13ms
step:1656/1680 train_time:149255ms step_avg:90.13ms
step:1657/1680 train_time:149345ms step_avg:90.13ms
step:1658/1680 train_time:149435ms step_avg:90.13ms
step:1659/1680 train_time:149525ms step_avg:90.13ms
step:1660/1680 train_time:149617ms step_avg:90.13ms
step:1661/1680 train_time:149709ms step_avg:90.13ms
step:1662/1680 train_time:149801ms step_avg:90.13ms
step:1663/1680 train_time:149892ms step_avg:90.13ms
step:1664/1680 train_time:149983ms step_avg:90.13ms
step:1665/1680 train_time:150074ms step_avg:90.13ms
step:1666/1680 train_time:150165ms step_avg:90.14ms
step:1667/1680 train_time:150255ms step_avg:90.14ms
step:1668/1680 train_time:150345ms step_avg:90.13ms
step:1669/1680 train_time:150435ms step_avg:90.14ms
step:1670/1680 train_time:150526ms step_avg:90.14ms
step:1671/1680 train_time:150617ms step_avg:90.14ms
step:1672/1680 train_time:150708ms step_avg:90.14ms
step:1673/1680 train_time:150800ms step_avg:90.14ms
step:1674/1680 train_time:150890ms step_avg:90.14ms
step:1675/1680 train_time:150981ms step_avg:90.14ms
step:1676/1680 train_time:151073ms step_avg:90.14ms
step:1677/1680 train_time:151163ms step_avg:90.14ms
step:1678/1680 train_time:151255ms step_avg:90.14ms
step:1679/1680 train_time:151344ms step_avg:90.14ms
step:1680/1680 train_time:151434ms step_avg:90.14ms
step:1680/1680 val_loss:3.2781 train_time:151527ms step_avg:90.19ms
peak memory allocated: 31255 MiB reserved: 46474 MiB

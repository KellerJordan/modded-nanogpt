import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 and layer_idx !=0 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        first_bm = 1 * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, final_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"v1/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer shows no degradation with context length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
smear_gate_params = [p for n, p in model.named_parameters() if "smear" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params+smear_gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate, args.ws_validate_final_layer
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250724+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sun Sep 21 23:25:01 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   43C    P0            126W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   39C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   40C    P0            126W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   39C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           86064      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A           86065      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           86066      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           86067      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           86068      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           86069      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           86070      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           86071      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           86065      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A           86066      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A           86067      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A           86068      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A           86069      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A           86070      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A           86071      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/1680 train_time:159ms step_avg:159.33ms
step:2/1680 train_time:181ms step_avg:90.71ms
step:3/1680 train_time:245ms step_avg:81.60ms
step:4/1680 train_time:332ms step_avg:82.89ms
step:5/1680 train_time:420ms step_avg:83.96ms
step:6/1680 train_time:508ms step_avg:84.63ms
step:7/1680 train_time:596ms step_avg:85.18ms
step:8/1680 train_time:684ms step_avg:85.56ms
step:9/1680 train_time:773ms step_avg:85.85ms
step:10/1680 train_time:861ms step_avg:86.06ms
step:11/1680 train_time:949ms step_avg:86.28ms
step:12/1680 train_time:1038ms step_avg:86.48ms
step:13/1680 train_time:1129ms step_avg:86.82ms
step:14/1680 train_time:1219ms step_avg:87.10ms
step:15/1680 train_time:1310ms step_avg:87.32ms
step:16/1680 train_time:1399ms step_avg:87.44ms
step:17/1680 train_time:1488ms step_avg:87.54ms
step:18/1680 train_time:1577ms step_avg:87.62ms
step:19/1680 train_time:1670ms step_avg:87.91ms
step:20/1680 train_time:1755ms step_avg:87.74ms
step:21/1680 train_time:1844ms step_avg:87.79ms
step:22/1680 train_time:1932ms step_avg:87.83ms
step:23/1680 train_time:2022ms step_avg:87.91ms
step:24/1680 train_time:2113ms step_avg:88.03ms
step:25/1680 train_time:2203ms step_avg:88.11ms
step:26/1680 train_time:2293ms step_avg:88.20ms
step:27/1680 train_time:2383ms step_avg:88.25ms
step:28/1680 train_time:2472ms step_avg:88.28ms
step:29/1680 train_time:2561ms step_avg:88.29ms
step:30/1680 train_time:2650ms step_avg:88.34ms
step:31/1680 train_time:2739ms step_avg:88.35ms
step:32/1680 train_time:2827ms step_avg:88.36ms
step:33/1680 train_time:2915ms step_avg:88.34ms
step:34/1680 train_time:3004ms step_avg:88.34ms
step:35/1680 train_time:3093ms step_avg:88.38ms
step:36/1680 train_time:3182ms step_avg:88.40ms
step:37/1680 train_time:3272ms step_avg:88.42ms
step:38/1680 train_time:3361ms step_avg:88.44ms
step:39/1680 train_time:3450ms step_avg:88.47ms
step:40/1680 train_time:3540ms step_avg:88.51ms
step:41/1680 train_time:3630ms step_avg:88.54ms
step:42/1680 train_time:3718ms step_avg:88.53ms
step:43/1680 train_time:3807ms step_avg:88.53ms
step:44/1680 train_time:3896ms step_avg:88.53ms
step:45/1680 train_time:3984ms step_avg:88.53ms
step:46/1680 train_time:4073ms step_avg:88.55ms
step:47/1680 train_time:4163ms step_avg:88.57ms
step:48/1680 train_time:4252ms step_avg:88.59ms
step:49/1680 train_time:4341ms step_avg:88.59ms
step:50/1680 train_time:4431ms step_avg:88.61ms
step:51/1680 train_time:4520ms step_avg:88.62ms
step:52/1680 train_time:4610ms step_avg:88.65ms
step:53/1680 train_time:4699ms step_avg:88.66ms
step:54/1680 train_time:4789ms step_avg:88.68ms
step:55/1680 train_time:4877ms step_avg:88.68ms
step:56/1680 train_time:4966ms step_avg:88.68ms
step:57/1680 train_time:5055ms step_avg:88.68ms
step:58/1680 train_time:5144ms step_avg:88.68ms
step:59/1680 train_time:5233ms step_avg:88.69ms
step:60/1680 train_time:5321ms step_avg:88.69ms
step:61/1680 train_time:5411ms step_avg:88.70ms
step:62/1680 train_time:5499ms step_avg:88.69ms
step:63/1680 train_time:5588ms step_avg:88.69ms
step:64/1680 train_time:5678ms step_avg:88.73ms
step:65/1680 train_time:5765ms step_avg:88.69ms
step:66/1680 train_time:5853ms step_avg:88.68ms
step:67/1680 train_time:5942ms step_avg:88.69ms
step:68/1680 train_time:6032ms step_avg:88.70ms
step:69/1680 train_time:6121ms step_avg:88.71ms
step:70/1680 train_time:6210ms step_avg:88.71ms
step:71/1680 train_time:6299ms step_avg:88.72ms
step:72/1680 train_time:6389ms step_avg:88.73ms
step:73/1680 train_time:6478ms step_avg:88.74ms
step:74/1680 train_time:6567ms step_avg:88.74ms
step:75/1680 train_time:6655ms step_avg:88.74ms
step:76/1680 train_time:6744ms step_avg:88.74ms
step:77/1680 train_time:6833ms step_avg:88.74ms
step:78/1680 train_time:6922ms step_avg:88.74ms
step:79/1680 train_time:7011ms step_avg:88.75ms
step:80/1680 train_time:7100ms step_avg:88.75ms
step:81/1680 train_time:7189ms step_avg:88.76ms
step:82/1680 train_time:7278ms step_avg:88.76ms
step:83/1680 train_time:7367ms step_avg:88.76ms
step:84/1680 train_time:7457ms step_avg:88.77ms
step:85/1680 train_time:7545ms step_avg:88.77ms
step:86/1680 train_time:7634ms step_avg:88.77ms
step:87/1680 train_time:7723ms step_avg:88.77ms
step:88/1680 train_time:7812ms step_avg:88.77ms
step:89/1680 train_time:7901ms step_avg:88.77ms
step:90/1680 train_time:7989ms step_avg:88.77ms
step:91/1680 train_time:8079ms step_avg:88.78ms
step:92/1680 train_time:8168ms step_avg:88.78ms
step:93/1680 train_time:8256ms step_avg:88.78ms
step:94/1680 train_time:8345ms step_avg:88.78ms
step:95/1680 train_time:8434ms step_avg:88.78ms
step:96/1680 train_time:8522ms step_avg:88.77ms
step:97/1680 train_time:8612ms step_avg:88.78ms
step:98/1680 train_time:8700ms step_avg:88.78ms
step:99/1680 train_time:8789ms step_avg:88.78ms
step:100/1680 train_time:8878ms step_avg:88.78ms
step:101/1680 train_time:8971ms step_avg:88.82ms
step:102/1680 train_time:9056ms step_avg:88.78ms
step:103/1680 train_time:9145ms step_avg:88.79ms
step:104/1680 train_time:9234ms step_avg:88.79ms
step:105/1680 train_time:9323ms step_avg:88.79ms
step:106/1680 train_time:9411ms step_avg:88.79ms
step:107/1680 train_time:9501ms step_avg:88.79ms
step:108/1680 train_time:9590ms step_avg:88.80ms
step:109/1680 train_time:9679ms step_avg:88.80ms
step:110/1680 train_time:9768ms step_avg:88.80ms
step:111/1680 train_time:9857ms step_avg:88.80ms
step:112/1680 train_time:9945ms step_avg:88.80ms
step:113/1680 train_time:10034ms step_avg:88.80ms
step:114/1680 train_time:10122ms step_avg:88.79ms
step:115/1680 train_time:10212ms step_avg:88.80ms
step:116/1680 train_time:10301ms step_avg:88.80ms
step:117/1680 train_time:10390ms step_avg:88.80ms
step:118/1680 train_time:10478ms step_avg:88.80ms
step:119/1680 train_time:10567ms step_avg:88.80ms
step:120/1680 train_time:10656ms step_avg:88.80ms
step:121/1680 train_time:10745ms step_avg:88.80ms
step:122/1680 train_time:10834ms step_avg:88.81ms
step:123/1680 train_time:10923ms step_avg:88.80ms
step:124/1680 train_time:11012ms step_avg:88.80ms
step:125/1680 train_time:11100ms step_avg:88.80ms
step:125/1680 val_loss:4.3203 train_time:11190ms step_avg:89.52ms
step:126/1680 train_time:11212ms step_avg:88.99ms
step:127/1680 train_time:11282ms step_avg:88.84ms
step:128/1680 train_time:11377ms step_avg:88.88ms
step:129/1680 train_time:11470ms step_avg:88.91ms
step:130/1680 train_time:11559ms step_avg:88.91ms
step:131/1680 train_time:11647ms step_avg:88.91ms
step:132/1680 train_time:11735ms step_avg:88.90ms
step:133/1680 train_time:11823ms step_avg:88.89ms
step:134/1680 train_time:11911ms step_avg:88.89ms
step:135/1680 train_time:11999ms step_avg:88.88ms
step:136/1680 train_time:12087ms step_avg:88.87ms
step:137/1680 train_time:12175ms step_avg:88.87ms
step:138/1680 train_time:12265ms step_avg:88.88ms
step:139/1680 train_time:12355ms step_avg:88.89ms
step:140/1680 train_time:12446ms step_avg:88.90ms
step:141/1680 train_time:12535ms step_avg:88.90ms
step:142/1680 train_time:12625ms step_avg:88.91ms
step:143/1680 train_time:12713ms step_avg:88.90ms
step:144/1680 train_time:12802ms step_avg:88.91ms
step:145/1680 train_time:12891ms step_avg:88.90ms
step:146/1680 train_time:12979ms step_avg:88.90ms
step:147/1680 train_time:13067ms step_avg:88.89ms
step:148/1680 train_time:13156ms step_avg:88.89ms
step:149/1680 train_time:13247ms step_avg:88.90ms
step:150/1680 train_time:13337ms step_avg:88.91ms
step:151/1680 train_time:13426ms step_avg:88.92ms
step:152/1680 train_time:13516ms step_avg:88.92ms
step:153/1680 train_time:13606ms step_avg:88.93ms
step:154/1680 train_time:13694ms step_avg:88.92ms
step:155/1680 train_time:13784ms step_avg:88.93ms
step:156/1680 train_time:13871ms step_avg:88.92ms
step:157/1680 train_time:13960ms step_avg:88.92ms
step:158/1680 train_time:14048ms step_avg:88.91ms
step:159/1680 train_time:14137ms step_avg:88.91ms
step:160/1680 train_time:14226ms step_avg:88.91ms
step:161/1680 train_time:14315ms step_avg:88.91ms
step:162/1680 train_time:14404ms step_avg:88.92ms
step:163/1680 train_time:14493ms step_avg:88.92ms
step:164/1680 train_time:14583ms step_avg:88.92ms
step:165/1680 train_time:14671ms step_avg:88.92ms
step:166/1680 train_time:14760ms step_avg:88.92ms
step:167/1680 train_time:14848ms step_avg:88.91ms
step:168/1680 train_time:14937ms step_avg:88.91ms
step:169/1680 train_time:15025ms step_avg:88.90ms
step:170/1680 train_time:15113ms step_avg:88.90ms
step:171/1680 train_time:15203ms step_avg:88.90ms
step:172/1680 train_time:15292ms step_avg:88.91ms
step:173/1680 train_time:15381ms step_avg:88.91ms
step:174/1680 train_time:15470ms step_avg:88.91ms
step:175/1680 train_time:15560ms step_avg:88.91ms
step:176/1680 train_time:15649ms step_avg:88.92ms
step:177/1680 train_time:15738ms step_avg:88.92ms
step:178/1680 train_time:15827ms step_avg:88.92ms
step:179/1680 train_time:15916ms step_avg:88.91ms
step:180/1680 train_time:16004ms step_avg:88.91ms
step:181/1680 train_time:16092ms step_avg:88.91ms
step:182/1680 train_time:16181ms step_avg:88.91ms
step:183/1680 train_time:16270ms step_avg:88.91ms
step:184/1680 train_time:16360ms step_avg:88.91ms
step:185/1680 train_time:16449ms step_avg:88.91ms
step:186/1680 train_time:16537ms step_avg:88.91ms
step:187/1680 train_time:16627ms step_avg:88.91ms
step:188/1680 train_time:16716ms step_avg:88.92ms
step:189/1680 train_time:16806ms step_avg:88.92ms
step:190/1680 train_time:16893ms step_avg:88.91ms
step:191/1680 train_time:16983ms step_avg:88.92ms
step:192/1680 train_time:17071ms step_avg:88.91ms
step:193/1680 train_time:17159ms step_avg:88.91ms
step:194/1680 train_time:17248ms step_avg:88.91ms
step:195/1680 train_time:17338ms step_avg:88.91ms
step:196/1680 train_time:17428ms step_avg:88.92ms
step:197/1680 train_time:17517ms step_avg:88.92ms
step:198/1680 train_time:17606ms step_avg:88.92ms
step:199/1680 train_time:17695ms step_avg:88.92ms
step:200/1680 train_time:17785ms step_avg:88.93ms
step:201/1680 train_time:17873ms step_avg:88.92ms
step:202/1680 train_time:17963ms step_avg:88.92ms
step:203/1680 train_time:18051ms step_avg:88.92ms
step:204/1680 train_time:18139ms step_avg:88.92ms
step:205/1680 train_time:18227ms step_avg:88.91ms
step:206/1680 train_time:18317ms step_avg:88.92ms
step:207/1680 train_time:18406ms step_avg:88.92ms
step:208/1680 train_time:18494ms step_avg:88.92ms
step:209/1680 train_time:18583ms step_avg:88.92ms
step:210/1680 train_time:18672ms step_avg:88.91ms
step:211/1680 train_time:18762ms step_avg:88.92ms
step:212/1680 train_time:18851ms step_avg:88.92ms
step:213/1680 train_time:18940ms step_avg:88.92ms
step:214/1680 train_time:19029ms step_avg:88.92ms
step:215/1680 train_time:19118ms step_avg:88.92ms
step:216/1680 train_time:19207ms step_avg:88.92ms
step:217/1680 train_time:19296ms step_avg:88.92ms
step:218/1680 train_time:19385ms step_avg:88.92ms
step:219/1680 train_time:19473ms step_avg:88.92ms
step:220/1680 train_time:19563ms step_avg:88.92ms
step:221/1680 train_time:19652ms step_avg:88.92ms
step:222/1680 train_time:19741ms step_avg:88.92ms
step:223/1680 train_time:19830ms step_avg:88.92ms
step:224/1680 train_time:19920ms step_avg:88.93ms
step:225/1680 train_time:20009ms step_avg:88.93ms
step:226/1680 train_time:20098ms step_avg:88.93ms
step:227/1680 train_time:20187ms step_avg:88.93ms
step:228/1680 train_time:20275ms step_avg:88.93ms
step:229/1680 train_time:20365ms step_avg:88.93ms
step:230/1680 train_time:20454ms step_avg:88.93ms
step:231/1680 train_time:20544ms step_avg:88.93ms
step:232/1680 train_time:20633ms step_avg:88.93ms
step:233/1680 train_time:20722ms step_avg:88.94ms
step:234/1680 train_time:20812ms step_avg:88.94ms
step:235/1680 train_time:20901ms step_avg:88.94ms
step:236/1680 train_time:20990ms step_avg:88.94ms
step:237/1680 train_time:21079ms step_avg:88.94ms
step:238/1680 train_time:21168ms step_avg:88.94ms
step:239/1680 train_time:21257ms step_avg:88.94ms
step:240/1680 train_time:21346ms step_avg:88.94ms
step:241/1680 train_time:21435ms step_avg:88.94ms
step:242/1680 train_time:21524ms step_avg:88.94ms
step:243/1680 train_time:21613ms step_avg:88.94ms
step:244/1680 train_time:21702ms step_avg:88.94ms
step:245/1680 train_time:21791ms step_avg:88.94ms
step:246/1680 train_time:21880ms step_avg:88.94ms
step:247/1680 train_time:21969ms step_avg:88.94ms
step:248/1680 train_time:22059ms step_avg:88.95ms
step:249/1680 train_time:22148ms step_avg:88.95ms
step:250/1680 train_time:22237ms step_avg:88.95ms
step:250/1680 val_loss:3.9720 train_time:22327ms step_avg:89.31ms
step:251/1680 train_time:22349ms step_avg:89.04ms
step:252/1680 train_time:22418ms step_avg:88.96ms
step:253/1680 train_time:22511ms step_avg:88.98ms
step:254/1680 train_time:22604ms step_avg:88.99ms
step:255/1680 train_time:22692ms step_avg:88.99ms
step:256/1680 train_time:22779ms step_avg:88.98ms
step:257/1680 train_time:22867ms step_avg:88.98ms
step:258/1680 train_time:22955ms step_avg:88.97ms
step:259/1680 train_time:23043ms step_avg:88.97ms
step:260/1680 train_time:23131ms step_avg:88.96ms
step:261/1680 train_time:23219ms step_avg:88.96ms
step:262/1680 train_time:23308ms step_avg:88.96ms
step:263/1680 train_time:23400ms step_avg:88.97ms
step:264/1680 train_time:23490ms step_avg:88.98ms
step:265/1680 train_time:23580ms step_avg:88.98ms
step:266/1680 train_time:23669ms step_avg:88.98ms
step:267/1680 train_time:23759ms step_avg:88.98ms
step:268/1680 train_time:23847ms step_avg:88.98ms
step:269/1680 train_time:23935ms step_avg:88.98ms
step:270/1680 train_time:24024ms step_avg:88.98ms
step:271/1680 train_time:24112ms step_avg:88.97ms
step:272/1680 train_time:24200ms step_avg:88.97ms
step:273/1680 train_time:24289ms step_avg:88.97ms
step:274/1680 train_time:24379ms step_avg:88.98ms
step:275/1680 train_time:24469ms step_avg:88.98ms
step:276/1680 train_time:24559ms step_avg:88.98ms
step:277/1680 train_time:24648ms step_avg:88.98ms
step:278/1680 train_time:24737ms step_avg:88.98ms
step:279/1680 train_time:24826ms step_avg:88.98ms
step:280/1680 train_time:24914ms step_avg:88.98ms
step:281/1680 train_time:25002ms step_avg:88.98ms
step:282/1680 train_time:25090ms step_avg:88.97ms
step:283/1680 train_time:25179ms step_avg:88.97ms
step:284/1680 train_time:25267ms step_avg:88.97ms
step:285/1680 train_time:25357ms step_avg:88.97ms
step:286/1680 train_time:25448ms step_avg:88.98ms
step:287/1680 train_time:25537ms step_avg:88.98ms
step:288/1680 train_time:25626ms step_avg:88.98ms
step:289/1680 train_time:25716ms step_avg:88.98ms
step:290/1680 train_time:25805ms step_avg:88.98ms
step:291/1680 train_time:25894ms step_avg:88.98ms
step:292/1680 train_time:25983ms step_avg:88.98ms
step:293/1680 train_time:26072ms step_avg:88.98ms
step:294/1680 train_time:26161ms step_avg:88.98ms
step:295/1680 train_time:26249ms step_avg:88.98ms
step:296/1680 train_time:26338ms step_avg:88.98ms
step:297/1680 train_time:26427ms step_avg:88.98ms
step:298/1680 train_time:26517ms step_avg:88.98ms
step:299/1680 train_time:26606ms step_avg:88.98ms
step:300/1680 train_time:26698ms step_avg:88.99ms
step:301/1680 train_time:26785ms step_avg:88.99ms
step:302/1680 train_time:26873ms step_avg:88.98ms
step:303/1680 train_time:26963ms step_avg:88.99ms
step:304/1680 train_time:27051ms step_avg:88.98ms
step:305/1680 train_time:27140ms step_avg:88.98ms
step:306/1680 train_time:27228ms step_avg:88.98ms
step:307/1680 train_time:27317ms step_avg:88.98ms
step:308/1680 train_time:27406ms step_avg:88.98ms
step:309/1680 train_time:27496ms step_avg:88.98ms
step:310/1680 train_time:27585ms step_avg:88.98ms
step:311/1680 train_time:27675ms step_avg:88.99ms
step:312/1680 train_time:27764ms step_avg:88.99ms
step:313/1680 train_time:27852ms step_avg:88.98ms
step:314/1680 train_time:27941ms step_avg:88.98ms
step:315/1680 train_time:28029ms step_avg:88.98ms
step:316/1680 train_time:28119ms step_avg:88.98ms
step:317/1680 train_time:28207ms step_avg:88.98ms
step:318/1680 train_time:28295ms step_avg:88.98ms
step:319/1680 train_time:28385ms step_avg:88.98ms
step:320/1680 train_time:28474ms step_avg:88.98ms
step:321/1680 train_time:28563ms step_avg:88.98ms
step:322/1680 train_time:28652ms step_avg:88.98ms
step:323/1680 train_time:28741ms step_avg:88.98ms
step:324/1680 train_time:28830ms step_avg:88.98ms
step:325/1680 train_time:28920ms step_avg:88.98ms
step:326/1680 train_time:29008ms step_avg:88.98ms
step:327/1680 train_time:29097ms step_avg:88.98ms
step:328/1680 train_time:29187ms step_avg:88.98ms
step:329/1680 train_time:29275ms step_avg:88.98ms
step:330/1680 train_time:29365ms step_avg:88.98ms
step:331/1680 train_time:29454ms step_avg:88.98ms
step:332/1680 train_time:29544ms step_avg:88.99ms
step:333/1680 train_time:29634ms step_avg:88.99ms
step:334/1680 train_time:29723ms step_avg:88.99ms
step:335/1680 train_time:29812ms step_avg:88.99ms
step:336/1680 train_time:29901ms step_avg:88.99ms
step:337/1680 train_time:29989ms step_avg:88.99ms
step:338/1680 train_time:30078ms step_avg:88.99ms
step:339/1680 train_time:30167ms step_avg:88.99ms
step:340/1680 train_time:30257ms step_avg:88.99ms
step:341/1680 train_time:30347ms step_avg:88.99ms
step:342/1680 train_time:30435ms step_avg:88.99ms
step:343/1680 train_time:30525ms step_avg:88.99ms
step:344/1680 train_time:30614ms step_avg:88.99ms
step:345/1680 train_time:30703ms step_avg:88.99ms
step:346/1680 train_time:30791ms step_avg:88.99ms
step:347/1680 train_time:30881ms step_avg:88.99ms
step:348/1680 train_time:30969ms step_avg:88.99ms
step:349/1680 train_time:31058ms step_avg:88.99ms
step:350/1680 train_time:31147ms step_avg:88.99ms
step:351/1680 train_time:31236ms step_avg:88.99ms
step:352/1680 train_time:31325ms step_avg:88.99ms
step:353/1680 train_time:31414ms step_avg:88.99ms
step:354/1680 train_time:31504ms step_avg:88.99ms
step:355/1680 train_time:31593ms step_avg:89.00ms
step:356/1680 train_time:31683ms step_avg:89.00ms
step:357/1680 train_time:31772ms step_avg:89.00ms
step:358/1680 train_time:31862ms step_avg:89.00ms
step:359/1680 train_time:31951ms step_avg:89.00ms
step:360/1680 train_time:32040ms step_avg:89.00ms
step:361/1680 train_time:32128ms step_avg:89.00ms
step:362/1680 train_time:32218ms step_avg:89.00ms
step:363/1680 train_time:32306ms step_avg:89.00ms
step:364/1680 train_time:32395ms step_avg:89.00ms
step:365/1680 train_time:32484ms step_avg:89.00ms
step:366/1680 train_time:32573ms step_avg:89.00ms
step:367/1680 train_time:32662ms step_avg:89.00ms
step:368/1680 train_time:32750ms step_avg:88.99ms
step:369/1680 train_time:32840ms step_avg:89.00ms
step:370/1680 train_time:32928ms step_avg:88.99ms
step:371/1680 train_time:33017ms step_avg:88.99ms
step:372/1680 train_time:33105ms step_avg:88.99ms
step:373/1680 train_time:33195ms step_avg:89.00ms
step:374/1680 train_time:33282ms step_avg:88.99ms
step:375/1680 train_time:33372ms step_avg:88.99ms
step:375/1680 val_loss:3.8171 train_time:33463ms step_avg:89.23ms
step:376/1680 train_time:33484ms step_avg:89.05ms
step:377/1680 train_time:33553ms step_avg:89.00ms
step:378/1680 train_time:33650ms step_avg:89.02ms
step:379/1680 train_time:33740ms step_avg:89.02ms
step:380/1680 train_time:33828ms step_avg:89.02ms
step:381/1680 train_time:33916ms step_avg:89.02ms
step:382/1680 train_time:34004ms step_avg:89.02ms
step:383/1680 train_time:34093ms step_avg:89.02ms
step:384/1680 train_time:34181ms step_avg:89.01ms
step:385/1680 train_time:34269ms step_avg:89.01ms
step:386/1680 train_time:34357ms step_avg:89.01ms
step:387/1680 train_time:34445ms step_avg:89.01ms
step:388/1680 train_time:34536ms step_avg:89.01ms
step:389/1680 train_time:34628ms step_avg:89.02ms
step:390/1680 train_time:34718ms step_avg:89.02ms
step:391/1680 train_time:34807ms step_avg:89.02ms
step:392/1680 train_time:34896ms step_avg:89.02ms
step:393/1680 train_time:34984ms step_avg:89.02ms
step:394/1680 train_time:35073ms step_avg:89.02ms
step:395/1680 train_time:35162ms step_avg:89.02ms
step:396/1680 train_time:35249ms step_avg:89.01ms
step:397/1680 train_time:35338ms step_avg:89.01ms
step:398/1680 train_time:35426ms step_avg:89.01ms
step:399/1680 train_time:35515ms step_avg:89.01ms
step:400/1680 train_time:35605ms step_avg:89.01ms
step:401/1680 train_time:35695ms step_avg:89.01ms
step:402/1680 train_time:35784ms step_avg:89.01ms
step:403/1680 train_time:35872ms step_avg:89.01ms
step:404/1680 train_time:35962ms step_avg:89.01ms
step:405/1680 train_time:36050ms step_avg:89.01ms
step:406/1680 train_time:36139ms step_avg:89.01ms
step:407/1680 train_time:36227ms step_avg:89.01ms
step:408/1680 train_time:36315ms step_avg:89.01ms
step:409/1680 train_time:36404ms step_avg:89.01ms
step:410/1680 train_time:36493ms step_avg:89.01ms
step:411/1680 train_time:36582ms step_avg:89.01ms
step:412/1680 train_time:36673ms step_avg:89.01ms
step:413/1680 train_time:36763ms step_avg:89.01ms
step:414/1680 train_time:36851ms step_avg:89.01ms
step:415/1680 train_time:36940ms step_avg:89.01ms
step:416/1680 train_time:37029ms step_avg:89.01ms
step:417/1680 train_time:37117ms step_avg:89.01ms
step:418/1680 train_time:37206ms step_avg:89.01ms
step:419/1680 train_time:37295ms step_avg:89.01ms
step:420/1680 train_time:37383ms step_avg:89.01ms
step:421/1680 train_time:37472ms step_avg:89.01ms
step:422/1680 train_time:37561ms step_avg:89.01ms
step:423/1680 train_time:37649ms step_avg:89.01ms
step:424/1680 train_time:37740ms step_avg:89.01ms
step:425/1680 train_time:37828ms step_avg:89.01ms
step:426/1680 train_time:37916ms step_avg:89.01ms
step:427/1680 train_time:38005ms step_avg:89.01ms
step:428/1680 train_time:38095ms step_avg:89.01ms
step:429/1680 train_time:38183ms step_avg:89.01ms
step:430/1680 train_time:38272ms step_avg:89.01ms
step:431/1680 train_time:38361ms step_avg:89.00ms
step:432/1680 train_time:38450ms step_avg:89.00ms
step:433/1680 train_time:38539ms step_avg:89.01ms
step:434/1680 train_time:38628ms step_avg:89.00ms
step:435/1680 train_time:38717ms step_avg:89.00ms
step:436/1680 train_time:38807ms step_avg:89.01ms
step:437/1680 train_time:38896ms step_avg:89.01ms
step:438/1680 train_time:38985ms step_avg:89.01ms
step:439/1680 train_time:39074ms step_avg:89.01ms
step:440/1680 train_time:39163ms step_avg:89.01ms
step:441/1680 train_time:39251ms step_avg:89.00ms
step:442/1680 train_time:39341ms step_avg:89.01ms
step:443/1680 train_time:39429ms step_avg:89.00ms
step:444/1680 train_time:39518ms step_avg:89.00ms
step:445/1680 train_time:39607ms step_avg:89.00ms
step:446/1680 train_time:39696ms step_avg:89.00ms
step:447/1680 train_time:39784ms step_avg:89.00ms
step:448/1680 train_time:39873ms step_avg:89.00ms
step:449/1680 train_time:39962ms step_avg:89.00ms
step:450/1680 train_time:40051ms step_avg:89.00ms
step:451/1680 train_time:40141ms step_avg:89.00ms
step:452/1680 train_time:40230ms step_avg:89.00ms
step:453/1680 train_time:40320ms step_avg:89.01ms
step:454/1680 train_time:40408ms step_avg:89.00ms
step:455/1680 train_time:40497ms step_avg:89.00ms
step:456/1680 train_time:40586ms step_avg:89.00ms
step:457/1680 train_time:40674ms step_avg:89.00ms
step:458/1680 train_time:40762ms step_avg:89.00ms
step:459/1680 train_time:40851ms step_avg:89.00ms
step:460/1680 train_time:40941ms step_avg:89.00ms
step:461/1680 train_time:41029ms step_avg:89.00ms
step:462/1680 train_time:41118ms step_avg:89.00ms
step:463/1680 train_time:41208ms step_avg:89.00ms
step:464/1680 train_time:41297ms step_avg:89.00ms
step:465/1680 train_time:41385ms step_avg:89.00ms
step:466/1680 train_time:41474ms step_avg:89.00ms
step:467/1680 train_time:41563ms step_avg:89.00ms
step:468/1680 train_time:41652ms step_avg:89.00ms
step:469/1680 train_time:41741ms step_avg:89.00ms
step:470/1680 train_time:41829ms step_avg:89.00ms
step:471/1680 train_time:41918ms step_avg:89.00ms
step:472/1680 train_time:42007ms step_avg:89.00ms
step:473/1680 train_time:42096ms step_avg:89.00ms
step:474/1680 train_time:42185ms step_avg:89.00ms
step:475/1680 train_time:42274ms step_avg:89.00ms
step:476/1680 train_time:42363ms step_avg:89.00ms
step:477/1680 train_time:42455ms step_avg:89.00ms
step:478/1680 train_time:42541ms step_avg:89.00ms
step:479/1680 train_time:42629ms step_avg:89.00ms
step:480/1680 train_time:42719ms step_avg:89.00ms
step:481/1680 train_time:42808ms step_avg:89.00ms
step:482/1680 train_time:42897ms step_avg:89.00ms
step:483/1680 train_time:42985ms step_avg:89.00ms
step:484/1680 train_time:43074ms step_avg:89.00ms
step:485/1680 train_time:43163ms step_avg:89.00ms
step:486/1680 train_time:43252ms step_avg:89.00ms
step:487/1680 train_time:43340ms step_avg:88.99ms
step:488/1680 train_time:43429ms step_avg:88.99ms
step:489/1680 train_time:43519ms step_avg:89.00ms
step:490/1680 train_time:43607ms step_avg:88.99ms
step:491/1680 train_time:43696ms step_avg:88.99ms
step:492/1680 train_time:43785ms step_avg:88.99ms
step:493/1680 train_time:43874ms step_avg:88.99ms
step:494/1680 train_time:43963ms step_avg:88.99ms
step:495/1680 train_time:44052ms step_avg:88.99ms
step:496/1680 train_time:44141ms step_avg:88.99ms
step:497/1680 train_time:44229ms step_avg:88.99ms
step:498/1680 train_time:44318ms step_avg:88.99ms
step:499/1680 train_time:44407ms step_avg:88.99ms
step:500/1680 train_time:44496ms step_avg:88.99ms
step:500/1680 val_loss:3.7151 train_time:44586ms step_avg:89.17ms
step:501/1680 train_time:44608ms step_avg:89.04ms
step:502/1680 train_time:44678ms step_avg:89.00ms
step:503/1680 train_time:44772ms step_avg:89.01ms
step:504/1680 train_time:44863ms step_avg:89.01ms
step:505/1680 train_time:44952ms step_avg:89.01ms
step:506/1680 train_time:45041ms step_avg:89.01ms
step:507/1680 train_time:45129ms step_avg:89.01ms
step:508/1680 train_time:45217ms step_avg:89.01ms
step:509/1680 train_time:45305ms step_avg:89.01ms
step:510/1680 train_time:45393ms step_avg:89.01ms
step:511/1680 train_time:45481ms step_avg:89.00ms
step:512/1680 train_time:45570ms step_avg:89.00ms
step:513/1680 train_time:45661ms step_avg:89.01ms
step:514/1680 train_time:45752ms step_avg:89.01ms
step:515/1680 train_time:45842ms step_avg:89.01ms
step:516/1680 train_time:45931ms step_avg:89.01ms
step:517/1680 train_time:46019ms step_avg:89.01ms
step:518/1680 train_time:46108ms step_avg:89.01ms
step:519/1680 train_time:46197ms step_avg:89.01ms
step:520/1680 train_time:46285ms step_avg:89.01ms
step:521/1680 train_time:46374ms step_avg:89.01ms
step:522/1680 train_time:46462ms step_avg:89.01ms
step:523/1680 train_time:46551ms step_avg:89.01ms
step:524/1680 train_time:46640ms step_avg:89.01ms
step:525/1680 train_time:46730ms step_avg:89.01ms
step:526/1680 train_time:46820ms step_avg:89.01ms
step:527/1680 train_time:46908ms step_avg:89.01ms
step:528/1680 train_time:46997ms step_avg:89.01ms
step:529/1680 train_time:47086ms step_avg:89.01ms
step:530/1680 train_time:47175ms step_avg:89.01ms
step:531/1680 train_time:47263ms step_avg:89.01ms
step:532/1680 train_time:47351ms step_avg:89.01ms
step:533/1680 train_time:47439ms step_avg:89.00ms
step:534/1680 train_time:47528ms step_avg:89.00ms
step:535/1680 train_time:47617ms step_avg:89.00ms
step:536/1680 train_time:47706ms step_avg:89.00ms
step:537/1680 train_time:47795ms step_avg:89.00ms
step:538/1680 train_time:47885ms step_avg:89.01ms
step:539/1680 train_time:47974ms step_avg:89.01ms
step:540/1680 train_time:48063ms step_avg:89.00ms
step:541/1680 train_time:48151ms step_avg:89.00ms
step:542/1680 train_time:48240ms step_avg:89.00ms
step:543/1680 train_time:48328ms step_avg:89.00ms
step:544/1680 train_time:48417ms step_avg:89.00ms
step:545/1680 train_time:48506ms step_avg:89.00ms
step:546/1680 train_time:48596ms step_avg:89.00ms
step:547/1680 train_time:48685ms step_avg:89.00ms
step:548/1680 train_time:48775ms step_avg:89.01ms
step:549/1680 train_time:48865ms step_avg:89.01ms
step:550/1680 train_time:48956ms step_avg:89.01ms
step:551/1680 train_time:49046ms step_avg:89.01ms
step:552/1680 train_time:49136ms step_avg:89.01ms
step:553/1680 train_time:49227ms step_avg:89.02ms
step:554/1680 train_time:49318ms step_avg:89.02ms
step:555/1680 train_time:49407ms step_avg:89.02ms
step:556/1680 train_time:49497ms step_avg:89.02ms
step:557/1680 train_time:49587ms step_avg:89.02ms
step:558/1680 train_time:49677ms step_avg:89.03ms
step:559/1680 train_time:49767ms step_avg:89.03ms
step:560/1680 train_time:49858ms step_avg:89.03ms
step:561/1680 train_time:49948ms step_avg:89.03ms
step:562/1680 train_time:50039ms step_avg:89.04ms
step:563/1680 train_time:50129ms step_avg:89.04ms
step:564/1680 train_time:50219ms step_avg:89.04ms
step:565/1680 train_time:50309ms step_avg:89.04ms
step:566/1680 train_time:50399ms step_avg:89.04ms
step:567/1680 train_time:50489ms step_avg:89.05ms
step:568/1680 train_time:50579ms step_avg:89.05ms
step:569/1680 train_time:50669ms step_avg:89.05ms
step:570/1680 train_time:50759ms step_avg:89.05ms
step:571/1680 train_time:50851ms step_avg:89.06ms
step:572/1680 train_time:50940ms step_avg:89.06ms
step:573/1680 train_time:51030ms step_avg:89.06ms
step:574/1680 train_time:51120ms step_avg:89.06ms
step:575/1680 train_time:51210ms step_avg:89.06ms
step:576/1680 train_time:51300ms step_avg:89.06ms
step:577/1680 train_time:51391ms step_avg:89.07ms
step:578/1680 train_time:51481ms step_avg:89.07ms
step:579/1680 train_time:51571ms step_avg:89.07ms
step:580/1680 train_time:51661ms step_avg:89.07ms
step:581/1680 train_time:51752ms step_avg:89.07ms
step:582/1680 train_time:51842ms step_avg:89.08ms
step:583/1680 train_time:51934ms step_avg:89.08ms
step:584/1680 train_time:52024ms step_avg:89.08ms
step:585/1680 train_time:52115ms step_avg:89.09ms
step:586/1680 train_time:52205ms step_avg:89.09ms
step:587/1680 train_time:52294ms step_avg:89.09ms
step:588/1680 train_time:52384ms step_avg:89.09ms
step:589/1680 train_time:52475ms step_avg:89.09ms
step:590/1680 train_time:52565ms step_avg:89.09ms
step:591/1680 train_time:52655ms step_avg:89.09ms
step:592/1680 train_time:52744ms step_avg:89.10ms
step:593/1680 train_time:52836ms step_avg:89.10ms
step:594/1680 train_time:52927ms step_avg:89.10ms
step:595/1680 train_time:53020ms step_avg:89.11ms
step:596/1680 train_time:53109ms step_avg:89.11ms
step:597/1680 train_time:53199ms step_avg:89.11ms
step:598/1680 train_time:53289ms step_avg:89.11ms
step:599/1680 train_time:53379ms step_avg:89.11ms
step:600/1680 train_time:53468ms step_avg:89.11ms
step:601/1680 train_time:53559ms step_avg:89.12ms
step:602/1680 train_time:53648ms step_avg:89.12ms
step:603/1680 train_time:53739ms step_avg:89.12ms
step:604/1680 train_time:53829ms step_avg:89.12ms
step:605/1680 train_time:53919ms step_avg:89.12ms
step:606/1680 train_time:54009ms step_avg:89.12ms
step:607/1680 train_time:54100ms step_avg:89.13ms
step:608/1680 train_time:54191ms step_avg:89.13ms
step:609/1680 train_time:54280ms step_avg:89.13ms
step:610/1680 train_time:54370ms step_avg:89.13ms
step:611/1680 train_time:54460ms step_avg:89.13ms
step:612/1680 train_time:54550ms step_avg:89.13ms
step:613/1680 train_time:54640ms step_avg:89.13ms
step:614/1680 train_time:54730ms step_avg:89.14ms
step:615/1680 train_time:54820ms step_avg:89.14ms
step:616/1680 train_time:54910ms step_avg:89.14ms
step:617/1680 train_time:55001ms step_avg:89.14ms
step:618/1680 train_time:55091ms step_avg:89.14ms
step:619/1680 train_time:55181ms step_avg:89.15ms
step:620/1680 train_time:55271ms step_avg:89.15ms
step:621/1680 train_time:55362ms step_avg:89.15ms
step:622/1680 train_time:55452ms step_avg:89.15ms
step:623/1680 train_time:55542ms step_avg:89.15ms
step:624/1680 train_time:55632ms step_avg:89.15ms
step:625/1680 train_time:55722ms step_avg:89.16ms
step:625/1680 val_loss:3.6139 train_time:55815ms step_avg:89.30ms
step:626/1680 train_time:55837ms step_avg:89.20ms
step:627/1680 train_time:55908ms step_avg:89.17ms
step:628/1680 train_time:56009ms step_avg:89.19ms
step:629/1680 train_time:56101ms step_avg:89.19ms
step:630/1680 train_time:56189ms step_avg:89.19ms
step:631/1680 train_time:56278ms step_avg:89.19ms
step:632/1680 train_time:56367ms step_avg:89.19ms
step:633/1680 train_time:56456ms step_avg:89.19ms
step:634/1680 train_time:56544ms step_avg:89.19ms
step:635/1680 train_time:56633ms step_avg:89.19ms
step:636/1680 train_time:56725ms step_avg:89.19ms
step:637/1680 train_time:56820ms step_avg:89.20ms
step:638/1680 train_time:56912ms step_avg:89.20ms
step:639/1680 train_time:57005ms step_avg:89.21ms
step:640/1680 train_time:57095ms step_avg:89.21ms
step:641/1680 train_time:57186ms step_avg:89.21ms
step:642/1680 train_time:57276ms step_avg:89.22ms
step:643/1680 train_time:57366ms step_avg:89.22ms
step:644/1680 train_time:57455ms step_avg:89.22ms
step:645/1680 train_time:57543ms step_avg:89.21ms
step:646/1680 train_time:57634ms step_avg:89.22ms
step:647/1680 train_time:57724ms step_avg:89.22ms
step:648/1680 train_time:57816ms step_avg:89.22ms
step:649/1680 train_time:57908ms step_avg:89.23ms
step:650/1680 train_time:57999ms step_avg:89.23ms
step:651/1680 train_time:58089ms step_avg:89.23ms
step:652/1680 train_time:58179ms step_avg:89.23ms
step:653/1680 train_time:58269ms step_avg:89.23ms
step:654/1680 train_time:58359ms step_avg:89.23ms
step:655/1680 train_time:58448ms step_avg:89.23ms
step:656/1680 train_time:58538ms step_avg:89.24ms
step:657/1680 train_time:58628ms step_avg:89.24ms
step:658/1680 train_time:58718ms step_avg:89.24ms
step:659/1680 train_time:58810ms step_avg:89.24ms
step:660/1680 train_time:58902ms step_avg:89.24ms
step:661/1680 train_time:58993ms step_avg:89.25ms
step:662/1680 train_time:59082ms step_avg:89.25ms
step:663/1680 train_time:59172ms step_avg:89.25ms
step:664/1680 train_time:59263ms step_avg:89.25ms
step:665/1680 train_time:59352ms step_avg:89.25ms
step:666/1680 train_time:59442ms step_avg:89.25ms
step:667/1680 train_time:59531ms step_avg:89.25ms
step:668/1680 train_time:59621ms step_avg:89.25ms
step:669/1680 train_time:59711ms step_avg:89.25ms
step:670/1680 train_time:59801ms step_avg:89.26ms
step:671/1680 train_time:59892ms step_avg:89.26ms
step:672/1680 train_time:59982ms step_avg:89.26ms
step:673/1680 train_time:60072ms step_avg:89.26ms
step:674/1680 train_time:60162ms step_avg:89.26ms
step:675/1680 train_time:60252ms step_avg:89.26ms
step:676/1680 train_time:60342ms step_avg:89.26ms
step:677/1680 train_time:60433ms step_avg:89.27ms
step:678/1680 train_time:60522ms step_avg:89.27ms
step:679/1680 train_time:60612ms step_avg:89.27ms
step:680/1680 train_time:60702ms step_avg:89.27ms
step:681/1680 train_time:60793ms step_avg:89.27ms
step:682/1680 train_time:60883ms step_avg:89.27ms
step:683/1680 train_time:60973ms step_avg:89.27ms
step:684/1680 train_time:61063ms step_avg:89.27ms
step:685/1680 train_time:61154ms step_avg:89.28ms
step:686/1680 train_time:61244ms step_avg:89.28ms
step:687/1680 train_time:61334ms step_avg:89.28ms
step:688/1680 train_time:61424ms step_avg:89.28ms
step:689/1680 train_time:61515ms step_avg:89.28ms
step:690/1680 train_time:61604ms step_avg:89.28ms
step:691/1680 train_time:61694ms step_avg:89.28ms
step:692/1680 train_time:61784ms step_avg:89.28ms
step:693/1680 train_time:61875ms step_avg:89.29ms
step:694/1680 train_time:61965ms step_avg:89.29ms
step:695/1680 train_time:62055ms step_avg:89.29ms
step:696/1680 train_time:62145ms step_avg:89.29ms
step:697/1680 train_time:62236ms step_avg:89.29ms
step:698/1680 train_time:62326ms step_avg:89.29ms
step:699/1680 train_time:62416ms step_avg:89.29ms
step:700/1680 train_time:62506ms step_avg:89.29ms
step:701/1680 train_time:62596ms step_avg:89.30ms
step:702/1680 train_time:62686ms step_avg:89.30ms
step:703/1680 train_time:62776ms step_avg:89.30ms
step:704/1680 train_time:62866ms step_avg:89.30ms
step:705/1680 train_time:62956ms step_avg:89.30ms
step:706/1680 train_time:63046ms step_avg:89.30ms
step:707/1680 train_time:63137ms step_avg:89.30ms
step:708/1680 train_time:63227ms step_avg:89.30ms
step:709/1680 train_time:63317ms step_avg:89.30ms
step:710/1680 train_time:63407ms step_avg:89.31ms
step:711/1680 train_time:63498ms step_avg:89.31ms
step:712/1680 train_time:63587ms step_avg:89.31ms
step:713/1680 train_time:63677ms step_avg:89.31ms
step:714/1680 train_time:63766ms step_avg:89.31ms
step:715/1680 train_time:63856ms step_avg:89.31ms
step:716/1680 train_time:63945ms step_avg:89.31ms
step:717/1680 train_time:64036ms step_avg:89.31ms
step:718/1680 train_time:64125ms step_avg:89.31ms
step:719/1680 train_time:64216ms step_avg:89.31ms
step:720/1680 train_time:64307ms step_avg:89.31ms
step:721/1680 train_time:64398ms step_avg:89.32ms
step:722/1680 train_time:64487ms step_avg:89.32ms
step:723/1680 train_time:64577ms step_avg:89.32ms
step:724/1680 train_time:64667ms step_avg:89.32ms
step:725/1680 train_time:64758ms step_avg:89.32ms
step:726/1680 train_time:64847ms step_avg:89.32ms
step:727/1680 train_time:64938ms step_avg:89.32ms
step:728/1680 train_time:65027ms step_avg:89.32ms
step:729/1680 train_time:65117ms step_avg:89.32ms
step:730/1680 train_time:65207ms step_avg:89.32ms
step:731/1680 train_time:65298ms step_avg:89.33ms
step:732/1680 train_time:65389ms step_avg:89.33ms
step:733/1680 train_time:65479ms step_avg:89.33ms
step:734/1680 train_time:65570ms step_avg:89.33ms
step:735/1680 train_time:65660ms step_avg:89.33ms
step:736/1680 train_time:65749ms step_avg:89.33ms
step:737/1680 train_time:65840ms step_avg:89.34ms
step:738/1680 train_time:65930ms step_avg:89.34ms
step:739/1680 train_time:66021ms step_avg:89.34ms
step:740/1680 train_time:66113ms step_avg:89.34ms
step:741/1680 train_time:66202ms step_avg:89.34ms
step:742/1680 train_time:66292ms step_avg:89.34ms
step:743/1680 train_time:66382ms step_avg:89.34ms
step:744/1680 train_time:66472ms step_avg:89.34ms
step:745/1680 train_time:66562ms step_avg:89.34ms
step:746/1680 train_time:66651ms step_avg:89.34ms
step:747/1680 train_time:66740ms step_avg:89.34ms
step:748/1680 train_time:66831ms step_avg:89.35ms
step:749/1680 train_time:66921ms step_avg:89.35ms
step:750/1680 train_time:67011ms step_avg:89.35ms
step:750/1680 val_loss:3.5641 train_time:67102ms step_avg:89.47ms
step:751/1680 train_time:67124ms step_avg:89.38ms
step:752/1680 train_time:67196ms step_avg:89.36ms
step:753/1680 train_time:67291ms step_avg:89.36ms
step:754/1680 train_time:67382ms step_avg:89.37ms
step:755/1680 train_time:67473ms step_avg:89.37ms
step:756/1680 train_time:67562ms step_avg:89.37ms
step:757/1680 train_time:67651ms step_avg:89.37ms
step:758/1680 train_time:67741ms step_avg:89.37ms
step:759/1680 train_time:67829ms step_avg:89.37ms
step:760/1680 train_time:67918ms step_avg:89.37ms
step:761/1680 train_time:68007ms step_avg:89.37ms
step:762/1680 train_time:68098ms step_avg:89.37ms
step:763/1680 train_time:68191ms step_avg:89.37ms
step:764/1680 train_time:68283ms step_avg:89.38ms
step:765/1680 train_time:68374ms step_avg:89.38ms
step:766/1680 train_time:68463ms step_avg:89.38ms
step:767/1680 train_time:68554ms step_avg:89.38ms
step:768/1680 train_time:68643ms step_avg:89.38ms
step:769/1680 train_time:68733ms step_avg:89.38ms
step:770/1680 train_time:68822ms step_avg:89.38ms
step:771/1680 train_time:68911ms step_avg:89.38ms
step:772/1680 train_time:69001ms step_avg:89.38ms
step:773/1680 train_time:69091ms step_avg:89.38ms
step:774/1680 train_time:69182ms step_avg:89.38ms
step:775/1680 train_time:69274ms step_avg:89.39ms
step:776/1680 train_time:69364ms step_avg:89.39ms
step:777/1680 train_time:69455ms step_avg:89.39ms
step:778/1680 train_time:69546ms step_avg:89.39ms
step:779/1680 train_time:69634ms step_avg:89.39ms
step:780/1680 train_time:69723ms step_avg:89.39ms
step:781/1680 train_time:69813ms step_avg:89.39ms
step:782/1680 train_time:69903ms step_avg:89.39ms
step:783/1680 train_time:69992ms step_avg:89.39ms
step:784/1680 train_time:70082ms step_avg:89.39ms
step:785/1680 train_time:70174ms step_avg:89.39ms
step:786/1680 train_time:70263ms step_avg:89.39ms
step:787/1680 train_time:70354ms step_avg:89.40ms
step:788/1680 train_time:70444ms step_avg:89.40ms
step:789/1680 train_time:70534ms step_avg:89.40ms
step:790/1680 train_time:70624ms step_avg:89.40ms
step:791/1680 train_time:70715ms step_avg:89.40ms
step:792/1680 train_time:70803ms step_avg:89.40ms
step:793/1680 train_time:70893ms step_avg:89.40ms
step:794/1680 train_time:70984ms step_avg:89.40ms
step:795/1680 train_time:71074ms step_avg:89.40ms
step:796/1680 train_time:71163ms step_avg:89.40ms
step:797/1680 train_time:71254ms step_avg:89.40ms
step:798/1680 train_time:71345ms step_avg:89.40ms
step:799/1680 train_time:71436ms step_avg:89.41ms
step:800/1680 train_time:71525ms step_avg:89.41ms
step:801/1680 train_time:71615ms step_avg:89.41ms
step:802/1680 train_time:71705ms step_avg:89.41ms
step:803/1680 train_time:71795ms step_avg:89.41ms
step:804/1680 train_time:71884ms step_avg:89.41ms
step:805/1680 train_time:71974ms step_avg:89.41ms
step:806/1680 train_time:72064ms step_avg:89.41ms
step:807/1680 train_time:72155ms step_avg:89.41ms
step:808/1680 train_time:72244ms step_avg:89.41ms
step:809/1680 train_time:72340ms step_avg:89.42ms
step:810/1680 train_time:72424ms step_avg:89.41ms
step:811/1680 train_time:72515ms step_avg:89.41ms
step:812/1680 train_time:72605ms step_avg:89.42ms
step:813/1680 train_time:72695ms step_avg:89.42ms
step:814/1680 train_time:72785ms step_avg:89.42ms
step:815/1680 train_time:72875ms step_avg:89.42ms
step:816/1680 train_time:72965ms step_avg:89.42ms
step:817/1680 train_time:73055ms step_avg:89.42ms
step:818/1680 train_time:73146ms step_avg:89.42ms
step:819/1680 train_time:73237ms step_avg:89.42ms
step:820/1680 train_time:73327ms step_avg:89.42ms
step:821/1680 train_time:73417ms step_avg:89.42ms
step:822/1680 train_time:73508ms step_avg:89.43ms
step:823/1680 train_time:73598ms step_avg:89.43ms
step:824/1680 train_time:73689ms step_avg:89.43ms
step:825/1680 train_time:73780ms step_avg:89.43ms
step:826/1680 train_time:73870ms step_avg:89.43ms
step:827/1680 train_time:73960ms step_avg:89.43ms
step:828/1680 train_time:74050ms step_avg:89.43ms
step:829/1680 train_time:74140ms step_avg:89.43ms
step:830/1680 train_time:74230ms step_avg:89.43ms
step:831/1680 train_time:74319ms step_avg:89.43ms
step:832/1680 train_time:74410ms step_avg:89.43ms
step:833/1680 train_time:74501ms step_avg:89.44ms
step:834/1680 train_time:74590ms step_avg:89.44ms
step:835/1680 train_time:74680ms step_avg:89.44ms
step:836/1680 train_time:74771ms step_avg:89.44ms
step:837/1680 train_time:74860ms step_avg:89.44ms
step:838/1680 train_time:74951ms step_avg:89.44ms
step:839/1680 train_time:75042ms step_avg:89.44ms
step:840/1680 train_time:75131ms step_avg:89.44ms
step:841/1680 train_time:75221ms step_avg:89.44ms
step:842/1680 train_time:75311ms step_avg:89.44ms
step:843/1680 train_time:75401ms step_avg:89.44ms
step:844/1680 train_time:75491ms step_avg:89.44ms
step:845/1680 train_time:75582ms step_avg:89.45ms
step:846/1680 train_time:75673ms step_avg:89.45ms
step:847/1680 train_time:75762ms step_avg:89.45ms
step:848/1680 train_time:75853ms step_avg:89.45ms
step:849/1680 train_time:75943ms step_avg:89.45ms
step:850/1680 train_time:76034ms step_avg:89.45ms
step:851/1680 train_time:76124ms step_avg:89.45ms
step:852/1680 train_time:76214ms step_avg:89.45ms
step:853/1680 train_time:76303ms step_avg:89.45ms
step:854/1680 train_time:76394ms step_avg:89.45ms
step:855/1680 train_time:76484ms step_avg:89.45ms
step:856/1680 train_time:76575ms step_avg:89.46ms
step:857/1680 train_time:76665ms step_avg:89.46ms
step:858/1680 train_time:76756ms step_avg:89.46ms
step:859/1680 train_time:76846ms step_avg:89.46ms
step:860/1680 train_time:76940ms step_avg:89.47ms
step:861/1680 train_time:77026ms step_avg:89.46ms
step:862/1680 train_time:77116ms step_avg:89.46ms
step:863/1680 train_time:77205ms step_avg:89.46ms
step:864/1680 train_time:77295ms step_avg:89.46ms
step:865/1680 train_time:77385ms step_avg:89.46ms
step:866/1680 train_time:77476ms step_avg:89.46ms
step:867/1680 train_time:77566ms step_avg:89.46ms
step:868/1680 train_time:77656ms step_avg:89.47ms
step:869/1680 train_time:77746ms step_avg:89.47ms
step:870/1680 train_time:77838ms step_avg:89.47ms
step:871/1680 train_time:77928ms step_avg:89.47ms
step:872/1680 train_time:78018ms step_avg:89.47ms
step:873/1680 train_time:78108ms step_avg:89.47ms
step:874/1680 train_time:78198ms step_avg:89.47ms
step:875/1680 train_time:78287ms step_avg:89.47ms
step:875/1680 val_loss:3.5174 train_time:78379ms step_avg:89.58ms
step:876/1680 train_time:78401ms step_avg:89.50ms
step:877/1680 train_time:78476ms step_avg:89.48ms
step:878/1680 train_time:78571ms step_avg:89.49ms
step:879/1680 train_time:78663ms step_avg:89.49ms
step:880/1680 train_time:78753ms step_avg:89.49ms
step:881/1680 train_time:78842ms step_avg:89.49ms
step:882/1680 train_time:78930ms step_avg:89.49ms
step:883/1680 train_time:79021ms step_avg:89.49ms
step:884/1680 train_time:79110ms step_avg:89.49ms
step:885/1680 train_time:79199ms step_avg:89.49ms
step:886/1680 train_time:79289ms step_avg:89.49ms
step:887/1680 train_time:79380ms step_avg:89.49ms
step:888/1680 train_time:79470ms step_avg:89.49ms
step:889/1680 train_time:79563ms step_avg:89.50ms
step:890/1680 train_time:79655ms step_avg:89.50ms
step:891/1680 train_time:79745ms step_avg:89.50ms
step:892/1680 train_time:79836ms step_avg:89.50ms
step:893/1680 train_time:79926ms step_avg:89.50ms
step:894/1680 train_time:80015ms step_avg:89.50ms
step:895/1680 train_time:80104ms step_avg:89.50ms
step:896/1680 train_time:80194ms step_avg:89.50ms
step:897/1680 train_time:80283ms step_avg:89.50ms
step:898/1680 train_time:80373ms step_avg:89.50ms
step:899/1680 train_time:80464ms step_avg:89.50ms
step:900/1680 train_time:80556ms step_avg:89.51ms
step:901/1680 train_time:80647ms step_avg:89.51ms
step:902/1680 train_time:80738ms step_avg:89.51ms
step:903/1680 train_time:80828ms step_avg:89.51ms
step:904/1680 train_time:80919ms step_avg:89.51ms
step:905/1680 train_time:81008ms step_avg:89.51ms
step:906/1680 train_time:81097ms step_avg:89.51ms
step:907/1680 train_time:81187ms step_avg:89.51ms
step:908/1680 train_time:81276ms step_avg:89.51ms
step:909/1680 train_time:81366ms step_avg:89.51ms
step:910/1680 train_time:81456ms step_avg:89.51ms
step:911/1680 train_time:81547ms step_avg:89.51ms
step:912/1680 train_time:81638ms step_avg:89.52ms
step:913/1680 train_time:81728ms step_avg:89.52ms
step:914/1680 train_time:81820ms step_avg:89.52ms
step:915/1680 train_time:81910ms step_avg:89.52ms
step:916/1680 train_time:82001ms step_avg:89.52ms
step:917/1680 train_time:82091ms step_avg:89.52ms
step:918/1680 train_time:82181ms step_avg:89.52ms
step:919/1680 train_time:82271ms step_avg:89.52ms
step:920/1680 train_time:82361ms step_avg:89.52ms
step:921/1680 train_time:82450ms step_avg:89.52ms
step:922/1680 train_time:82542ms step_avg:89.52ms
step:923/1680 train_time:82632ms step_avg:89.53ms
step:924/1680 train_time:82722ms step_avg:89.53ms
step:925/1680 train_time:82813ms step_avg:89.53ms
step:926/1680 train_time:82903ms step_avg:89.53ms
step:927/1680 train_time:82993ms step_avg:89.53ms
step:928/1680 train_time:83083ms step_avg:89.53ms
step:929/1680 train_time:83174ms step_avg:89.53ms
step:930/1680 train_time:83263ms step_avg:89.53ms
step:931/1680 train_time:83353ms step_avg:89.53ms
step:932/1680 train_time:83443ms step_avg:89.53ms
step:933/1680 train_time:83533ms step_avg:89.53ms
step:934/1680 train_time:83623ms step_avg:89.53ms
step:935/1680 train_time:83713ms step_avg:89.53ms
step:936/1680 train_time:83803ms step_avg:89.53ms
step:937/1680 train_time:83894ms step_avg:89.54ms
step:938/1680 train_time:83985ms step_avg:89.54ms
step:939/1680 train_time:84075ms step_avg:89.54ms
step:940/1680 train_time:84165ms step_avg:89.54ms
step:941/1680 train_time:84256ms step_avg:89.54ms
step:942/1680 train_time:84345ms step_avg:89.54ms
step:943/1680 train_time:84435ms step_avg:89.54ms
step:944/1680 train_time:84525ms step_avg:89.54ms
step:945/1680 train_time:84616ms step_avg:89.54ms
step:946/1680 train_time:84705ms step_avg:89.54ms
step:947/1680 train_time:84797ms step_avg:89.54ms
step:948/1680 train_time:84887ms step_avg:89.54ms
step:949/1680 train_time:84978ms step_avg:89.54ms
step:950/1680 train_time:85067ms step_avg:89.54ms
step:951/1680 train_time:85157ms step_avg:89.54ms
step:952/1680 train_time:85247ms step_avg:89.55ms
step:953/1680 train_time:85337ms step_avg:89.55ms
step:954/1680 train_time:85427ms step_avg:89.55ms
step:955/1680 train_time:85517ms step_avg:89.55ms
step:956/1680 train_time:85606ms step_avg:89.55ms
step:957/1680 train_time:85696ms step_avg:89.55ms
step:958/1680 train_time:85786ms step_avg:89.55ms
step:959/1680 train_time:85876ms step_avg:89.55ms
step:960/1680 train_time:85966ms step_avg:89.55ms
step:961/1680 train_time:86056ms step_avg:89.55ms
step:962/1680 train_time:86147ms step_avg:89.55ms
step:963/1680 train_time:86236ms step_avg:89.55ms
step:964/1680 train_time:86327ms step_avg:89.55ms
step:965/1680 train_time:86417ms step_avg:89.55ms
step:966/1680 train_time:86506ms step_avg:89.55ms
step:967/1680 train_time:86596ms step_avg:89.55ms
step:968/1680 train_time:86687ms step_avg:89.55ms
step:969/1680 train_time:86778ms step_avg:89.55ms
step:970/1680 train_time:86867ms step_avg:89.55ms
step:971/1680 train_time:86957ms step_avg:89.55ms
step:972/1680 train_time:87047ms step_avg:89.55ms
step:973/1680 train_time:87137ms step_avg:89.56ms
step:974/1680 train_time:87227ms step_avg:89.56ms
step:975/1680 train_time:87317ms step_avg:89.56ms
step:976/1680 train_time:87407ms step_avg:89.56ms
step:977/1680 train_time:87497ms step_avg:89.56ms
step:978/1680 train_time:87587ms step_avg:89.56ms
step:979/1680 train_time:87677ms step_avg:89.56ms
step:980/1680 train_time:87767ms step_avg:89.56ms
step:981/1680 train_time:87857ms step_avg:89.56ms
step:982/1680 train_time:87947ms step_avg:89.56ms
step:983/1680 train_time:88037ms step_avg:89.56ms
step:984/1680 train_time:88127ms step_avg:89.56ms
step:985/1680 train_time:88217ms step_avg:89.56ms
step:986/1680 train_time:88306ms step_avg:89.56ms
step:987/1680 train_time:88396ms step_avg:89.56ms
step:988/1680 train_time:88486ms step_avg:89.56ms
step:989/1680 train_time:88576ms step_avg:89.56ms
step:990/1680 train_time:88666ms step_avg:89.56ms
step:991/1680 train_time:88756ms step_avg:89.56ms
step:992/1680 train_time:88846ms step_avg:89.56ms
step:993/1680 train_time:88936ms step_avg:89.56ms
step:994/1680 train_time:89027ms step_avg:89.56ms
step:995/1680 train_time:89117ms step_avg:89.56ms
step:996/1680 train_time:89206ms step_avg:89.56ms
step:997/1680 train_time:89296ms step_avg:89.56ms
step:998/1680 train_time:89386ms step_avg:89.57ms
step:999/1680 train_time:89476ms step_avg:89.57ms
step:1000/1680 train_time:89566ms step_avg:89.57ms
step:1000/1680 val_loss:3.4674 train_time:89658ms step_avg:89.66ms
step:1001/1680 train_time:89680ms step_avg:89.59ms
step:1002/1680 train_time:89751ms step_avg:89.57ms
step:1003/1680 train_time:89845ms step_avg:89.58ms
step:1004/1680 train_time:89937ms step_avg:89.58ms
step:1005/1680 train_time:90027ms step_avg:89.58ms
step:1006/1680 train_time:90115ms step_avg:89.58ms
step:1007/1680 train_time:90204ms step_avg:89.58ms
step:1008/1680 train_time:90293ms step_avg:89.58ms
step:1009/1680 train_time:90381ms step_avg:89.58ms
step:1010/1680 train_time:90471ms step_avg:89.58ms
step:1011/1680 train_time:90560ms step_avg:89.58ms
step:1012/1680 train_time:90651ms step_avg:89.58ms
step:1013/1680 train_time:90743ms step_avg:89.58ms
step:1014/1680 train_time:90836ms step_avg:89.58ms
step:1015/1680 train_time:90928ms step_avg:89.58ms
step:1016/1680 train_time:91018ms step_avg:89.58ms
step:1017/1680 train_time:91107ms step_avg:89.58ms
step:1018/1680 train_time:91196ms step_avg:89.58ms
step:1019/1680 train_time:91286ms step_avg:89.58ms
step:1020/1680 train_time:91375ms step_avg:89.58ms
step:1021/1680 train_time:91464ms step_avg:89.58ms
step:1022/1680 train_time:91553ms step_avg:89.58ms
step:1023/1680 train_time:91644ms step_avg:89.58ms
step:1024/1680 train_time:91734ms step_avg:89.58ms
step:1025/1680 train_time:91825ms step_avg:89.59ms
step:1026/1680 train_time:91915ms step_avg:89.59ms
step:1027/1680 train_time:92006ms step_avg:89.59ms
step:1028/1680 train_time:92095ms step_avg:89.59ms
step:1029/1680 train_time:92185ms step_avg:89.59ms
step:1030/1680 train_time:92275ms step_avg:89.59ms
step:1031/1680 train_time:92365ms step_avg:89.59ms
step:1032/1680 train_time:92454ms step_avg:89.59ms
step:1033/1680 train_time:92544ms step_avg:89.59ms
step:1034/1680 train_time:92634ms step_avg:89.59ms
step:1035/1680 train_time:92725ms step_avg:89.59ms
step:1036/1680 train_time:92815ms step_avg:89.59ms
step:1037/1680 train_time:92906ms step_avg:89.59ms
step:1038/1680 train_time:92996ms step_avg:89.59ms
step:1039/1680 train_time:93087ms step_avg:89.59ms
step:1040/1680 train_time:93177ms step_avg:89.59ms
step:1041/1680 train_time:93267ms step_avg:89.59ms
step:1042/1680 train_time:93357ms step_avg:89.59ms
step:1043/1680 train_time:93447ms step_avg:89.59ms
step:1044/1680 train_time:93537ms step_avg:89.59ms
step:1045/1680 train_time:93627ms step_avg:89.60ms
step:1046/1680 train_time:93718ms step_avg:89.60ms
step:1047/1680 train_time:93809ms step_avg:89.60ms
step:1048/1680 train_time:93899ms step_avg:89.60ms
step:1049/1680 train_time:93989ms step_avg:89.60ms
step:1050/1680 train_time:94079ms step_avg:89.60ms
step:1051/1680 train_time:94169ms step_avg:89.60ms
step:1052/1680 train_time:94260ms step_avg:89.60ms
step:1053/1680 train_time:94350ms step_avg:89.60ms
step:1054/1680 train_time:94440ms step_avg:89.60ms
step:1055/1680 train_time:94530ms step_avg:89.60ms
step:1056/1680 train_time:94620ms step_avg:89.60ms
step:1057/1680 train_time:94710ms step_avg:89.60ms
step:1058/1680 train_time:94801ms step_avg:89.60ms
step:1059/1680 train_time:94891ms step_avg:89.60ms
step:1060/1680 train_time:94980ms step_avg:89.60ms
step:1061/1680 train_time:95072ms step_avg:89.61ms
step:1062/1680 train_time:95162ms step_avg:89.61ms
step:1063/1680 train_time:95252ms step_avg:89.61ms
step:1064/1680 train_time:95342ms step_avg:89.61ms
step:1065/1680 train_time:95432ms step_avg:89.61ms
step:1066/1680 train_time:95522ms step_avg:89.61ms
step:1067/1680 train_time:95613ms step_avg:89.61ms
step:1068/1680 train_time:95704ms step_avg:89.61ms
step:1069/1680 train_time:95794ms step_avg:89.61ms
step:1070/1680 train_time:95884ms step_avg:89.61ms
step:1071/1680 train_time:95974ms step_avg:89.61ms
step:1072/1680 train_time:96064ms step_avg:89.61ms
step:1073/1680 train_time:96154ms step_avg:89.61ms
step:1074/1680 train_time:96245ms step_avg:89.61ms
step:1075/1680 train_time:96335ms step_avg:89.61ms
step:1076/1680 train_time:96425ms step_avg:89.61ms
step:1077/1680 train_time:96515ms step_avg:89.61ms
step:1078/1680 train_time:96605ms step_avg:89.61ms
step:1079/1680 train_time:96694ms step_avg:89.61ms
step:1080/1680 train_time:96784ms step_avg:89.61ms
step:1081/1680 train_time:96873ms step_avg:89.61ms
step:1082/1680 train_time:96964ms step_avg:89.62ms
step:1083/1680 train_time:97053ms step_avg:89.62ms
step:1084/1680 train_time:97144ms step_avg:89.62ms
step:1085/1680 train_time:97238ms step_avg:89.62ms
step:1086/1680 train_time:97323ms step_avg:89.62ms
step:1087/1680 train_time:97412ms step_avg:89.62ms
step:1088/1680 train_time:97503ms step_avg:89.62ms
step:1089/1680 train_time:97592ms step_avg:89.62ms
step:1090/1680 train_time:97682ms step_avg:89.62ms
step:1091/1680 train_time:97772ms step_avg:89.62ms
step:1092/1680 train_time:97862ms step_avg:89.62ms
step:1093/1680 train_time:97952ms step_avg:89.62ms
step:1094/1680 train_time:98042ms step_avg:89.62ms
step:1095/1680 train_time:98133ms step_avg:89.62ms
step:1096/1680 train_time:98225ms step_avg:89.62ms
step:1097/1680 train_time:98315ms step_avg:89.62ms
step:1098/1680 train_time:98406ms step_avg:89.62ms
step:1099/1680 train_time:98497ms step_avg:89.62ms
step:1100/1680 train_time:98588ms step_avg:89.63ms
step:1101/1680 train_time:98680ms step_avg:89.63ms
step:1102/1680 train_time:98770ms step_avg:89.63ms
step:1103/1680 train_time:98861ms step_avg:89.63ms
step:1104/1680 train_time:98952ms step_avg:89.63ms
step:1105/1680 train_time:99043ms step_avg:89.63ms
step:1106/1680 train_time:99133ms step_avg:89.63ms
step:1107/1680 train_time:99223ms step_avg:89.63ms
step:1108/1680 train_time:99315ms step_avg:89.63ms
step:1109/1680 train_time:99406ms step_avg:89.64ms
step:1110/1680 train_time:99497ms step_avg:89.64ms
step:1111/1680 train_time:99588ms step_avg:89.64ms
step:1112/1680 train_time:99678ms step_avg:89.64ms
step:1113/1680 train_time:99770ms step_avg:89.64ms
step:1114/1680 train_time:99860ms step_avg:89.64ms
step:1115/1680 train_time:99951ms step_avg:89.64ms
step:1116/1680 train_time:100042ms step_avg:89.64ms
step:1117/1680 train_time:100137ms step_avg:89.65ms
step:1118/1680 train_time:100223ms step_avg:89.64ms
step:1119/1680 train_time:100314ms step_avg:89.65ms
step:1120/1680 train_time:100408ms step_avg:89.65ms
step:1121/1680 train_time:100495ms step_avg:89.65ms
step:1122/1680 train_time:100586ms step_avg:89.65ms
step:1123/1680 train_time:100676ms step_avg:89.65ms
step:1124/1680 train_time:100767ms step_avg:89.65ms
step:1125/1680 train_time:100858ms step_avg:89.65ms
step:1125/1680 val_loss:3.4146 train_time:100951ms step_avg:89.73ms
step:1126/1680 train_time:100972ms step_avg:89.67ms
step:1127/1680 train_time:101047ms step_avg:89.66ms
step:1128/1680 train_time:101147ms step_avg:89.67ms
step:1129/1680 train_time:101239ms step_avg:89.67ms
step:1130/1680 train_time:101329ms step_avg:89.67ms
step:1131/1680 train_time:101418ms step_avg:89.67ms
step:1132/1680 train_time:101508ms step_avg:89.67ms
step:1133/1680 train_time:101598ms step_avg:89.67ms
step:1134/1680 train_time:101688ms step_avg:89.67ms
step:1135/1680 train_time:101777ms step_avg:89.67ms
step:1136/1680 train_time:101868ms step_avg:89.67ms
step:1137/1680 train_time:101959ms step_avg:89.67ms
step:1138/1680 train_time:102053ms step_avg:89.68ms
step:1139/1680 train_time:102148ms step_avg:89.68ms
step:1140/1680 train_time:102239ms step_avg:89.68ms
step:1141/1680 train_time:102330ms step_avg:89.68ms
step:1142/1680 train_time:102421ms step_avg:89.69ms
step:1143/1680 train_time:102511ms step_avg:89.69ms
step:1144/1680 train_time:102600ms step_avg:89.69ms
step:1145/1680 train_time:102690ms step_avg:89.69ms
step:1146/1680 train_time:102780ms step_avg:89.69ms
step:1147/1680 train_time:102871ms step_avg:89.69ms
step:1148/1680 train_time:102962ms step_avg:89.69ms
step:1149/1680 train_time:103054ms step_avg:89.69ms
step:1150/1680 train_time:103147ms step_avg:89.69ms
step:1151/1680 train_time:103238ms step_avg:89.69ms
step:1152/1680 train_time:103331ms step_avg:89.70ms
step:1153/1680 train_time:103422ms step_avg:89.70ms
step:1154/1680 train_time:103513ms step_avg:89.70ms
step:1155/1680 train_time:103602ms step_avg:89.70ms
step:1156/1680 train_time:103692ms step_avg:89.70ms
step:1157/1680 train_time:103782ms step_avg:89.70ms
step:1158/1680 train_time:103873ms step_avg:89.70ms
step:1159/1680 train_time:103964ms step_avg:89.70ms
step:1160/1680 train_time:104055ms step_avg:89.70ms
step:1161/1680 train_time:104146ms step_avg:89.70ms
step:1162/1680 train_time:104238ms step_avg:89.71ms
step:1163/1680 train_time:104329ms step_avg:89.71ms
step:1164/1680 train_time:104420ms step_avg:89.71ms
step:1165/1680 train_time:104512ms step_avg:89.71ms
step:1166/1680 train_time:104602ms step_avg:89.71ms
step:1167/1680 train_time:104692ms step_avg:89.71ms
step:1168/1680 train_time:104782ms step_avg:89.71ms
step:1169/1680 train_time:104873ms step_avg:89.71ms
step:1170/1680 train_time:104964ms step_avg:89.71ms
step:1171/1680 train_time:105055ms step_avg:89.71ms
step:1172/1680 train_time:105146ms step_avg:89.71ms
step:1173/1680 train_time:105237ms step_avg:89.72ms
step:1174/1680 train_time:105329ms step_avg:89.72ms
step:1175/1680 train_time:105420ms step_avg:89.72ms
step:1176/1680 train_time:105512ms step_avg:89.72ms
step:1177/1680 train_time:105603ms step_avg:89.72ms
step:1178/1680 train_time:105694ms step_avg:89.72ms
step:1179/1680 train_time:105784ms step_avg:89.72ms
step:1180/1680 train_time:105874ms step_avg:89.72ms
step:1181/1680 train_time:105966ms step_avg:89.73ms
step:1182/1680 train_time:106056ms step_avg:89.73ms
step:1183/1680 train_time:106148ms step_avg:89.73ms
step:1184/1680 train_time:106239ms step_avg:89.73ms
step:1185/1680 train_time:106330ms step_avg:89.73ms
step:1186/1680 train_time:106422ms step_avg:89.73ms
step:1187/1680 train_time:106513ms step_avg:89.73ms
step:1188/1680 train_time:106603ms step_avg:89.73ms
step:1189/1680 train_time:106694ms step_avg:89.73ms
step:1190/1680 train_time:106784ms step_avg:89.73ms
step:1191/1680 train_time:106874ms step_avg:89.73ms
step:1192/1680 train_time:106965ms step_avg:89.74ms
step:1193/1680 train_time:107055ms step_avg:89.74ms
step:1194/1680 train_time:107146ms step_avg:89.74ms
step:1195/1680 train_time:107237ms step_avg:89.74ms
step:1196/1680 train_time:107328ms step_avg:89.74ms
step:1197/1680 train_time:107418ms step_avg:89.74ms
step:1198/1680 train_time:107510ms step_avg:89.74ms
step:1199/1680 train_time:107601ms step_avg:89.74ms
step:1200/1680 train_time:107691ms step_avg:89.74ms
step:1201/1680 train_time:107781ms step_avg:89.74ms
step:1202/1680 train_time:107872ms step_avg:89.74ms
step:1203/1680 train_time:107962ms step_avg:89.74ms
step:1204/1680 train_time:108053ms step_avg:89.75ms
step:1205/1680 train_time:108144ms step_avg:89.75ms
step:1206/1680 train_time:108235ms step_avg:89.75ms
step:1207/1680 train_time:108327ms step_avg:89.75ms
step:1208/1680 train_time:108418ms step_avg:89.75ms
step:1209/1680 train_time:108510ms step_avg:89.75ms
step:1210/1680 train_time:108602ms step_avg:89.75ms
step:1211/1680 train_time:108693ms step_avg:89.75ms
step:1212/1680 train_time:108784ms step_avg:89.76ms
step:1213/1680 train_time:108874ms step_avg:89.76ms
step:1214/1680 train_time:108965ms step_avg:89.76ms
step:1215/1680 train_time:109055ms step_avg:89.76ms
step:1216/1680 train_time:109146ms step_avg:89.76ms
step:1217/1680 train_time:109237ms step_avg:89.76ms
step:1218/1680 train_time:109329ms step_avg:89.76ms
step:1219/1680 train_time:109420ms step_avg:89.76ms
step:1220/1680 train_time:109512ms step_avg:89.76ms
step:1221/1680 train_time:109603ms step_avg:89.76ms
step:1222/1680 train_time:109693ms step_avg:89.77ms
step:1223/1680 train_time:109784ms step_avg:89.77ms
step:1224/1680 train_time:109874ms step_avg:89.77ms
step:1225/1680 train_time:109965ms step_avg:89.77ms
step:1226/1680 train_time:110055ms step_avg:89.77ms
step:1227/1680 train_time:110146ms step_avg:89.77ms
step:1228/1680 train_time:110237ms step_avg:89.77ms
step:1229/1680 train_time:110327ms step_avg:89.77ms
step:1230/1680 train_time:110417ms step_avg:89.77ms
step:1231/1680 train_time:110509ms step_avg:89.77ms
step:1232/1680 train_time:110600ms step_avg:89.77ms
step:1233/1680 train_time:110691ms step_avg:89.77ms
step:1234/1680 train_time:110782ms step_avg:89.77ms
step:1235/1680 train_time:110872ms step_avg:89.78ms
step:1236/1680 train_time:110963ms step_avg:89.78ms
step:1237/1680 train_time:111053ms step_avg:89.78ms
step:1238/1680 train_time:111143ms step_avg:89.78ms
step:1239/1680 train_time:111234ms step_avg:89.78ms
step:1240/1680 train_time:111325ms step_avg:89.78ms
step:1241/1680 train_time:111415ms step_avg:89.78ms
step:1242/1680 train_time:111506ms step_avg:89.78ms
step:1243/1680 train_time:111598ms step_avg:89.78ms
step:1244/1680 train_time:111689ms step_avg:89.78ms
step:1245/1680 train_time:111779ms step_avg:89.78ms
step:1246/1680 train_time:111870ms step_avg:89.78ms
step:1247/1680 train_time:111962ms step_avg:89.78ms
step:1248/1680 train_time:112052ms step_avg:89.79ms
step:1249/1680 train_time:112143ms step_avg:89.79ms
step:1250/1680 train_time:112234ms step_avg:89.79ms
step:1250/1680 val_loss:3.3761 train_time:112326ms step_avg:89.86ms
step:1251/1680 train_time:112348ms step_avg:89.81ms
step:1252/1680 train_time:112421ms step_avg:89.79ms
step:1253/1680 train_time:112518ms step_avg:89.80ms
step:1254/1680 train_time:112609ms step_avg:89.80ms
step:1255/1680 train_time:112698ms step_avg:89.80ms
step:1256/1680 train_time:112788ms step_avg:89.80ms
step:1257/1680 train_time:112878ms step_avg:89.80ms
step:1258/1680 train_time:112967ms step_avg:89.80ms
step:1259/1680 train_time:113057ms step_avg:89.80ms
step:1260/1680 train_time:113147ms step_avg:89.80ms
step:1261/1680 train_time:113237ms step_avg:89.80ms
step:1262/1680 train_time:113330ms step_avg:89.80ms
step:1263/1680 train_time:113422ms step_avg:89.80ms
step:1264/1680 train_time:113515ms step_avg:89.81ms
step:1265/1680 train_time:113606ms step_avg:89.81ms
step:1266/1680 train_time:113696ms step_avg:89.81ms
step:1267/1680 train_time:113786ms step_avg:89.81ms
step:1268/1680 train_time:113876ms step_avg:89.81ms
step:1269/1680 train_time:113966ms step_avg:89.81ms
step:1270/1680 train_time:114056ms step_avg:89.81ms
step:1271/1680 train_time:114151ms step_avg:89.81ms
step:1272/1680 train_time:114237ms step_avg:89.81ms
step:1273/1680 train_time:114329ms step_avg:89.81ms
step:1274/1680 train_time:114422ms step_avg:89.81ms
step:1275/1680 train_time:114513ms step_avg:89.81ms
step:1276/1680 train_time:114605ms step_avg:89.82ms
step:1277/1680 train_time:114696ms step_avg:89.82ms
step:1278/1680 train_time:114786ms step_avg:89.82ms
step:1279/1680 train_time:114877ms step_avg:89.82ms
step:1280/1680 train_time:114967ms step_avg:89.82ms
step:1281/1680 train_time:115057ms step_avg:89.82ms
step:1282/1680 train_time:115148ms step_avg:89.82ms
step:1283/1680 train_time:115239ms step_avg:89.82ms
step:1284/1680 train_time:115330ms step_avg:89.82ms
step:1285/1680 train_time:115422ms step_avg:89.82ms
step:1286/1680 train_time:115514ms step_avg:89.82ms
step:1287/1680 train_time:115605ms step_avg:89.83ms
step:1288/1680 train_time:115697ms step_avg:89.83ms
step:1289/1680 train_time:115788ms step_avg:89.83ms
step:1290/1680 train_time:115879ms step_avg:89.83ms
step:1291/1680 train_time:115969ms step_avg:89.83ms
step:1292/1680 train_time:116059ms step_avg:89.83ms
step:1293/1680 train_time:116150ms step_avg:89.83ms
step:1294/1680 train_time:116239ms step_avg:89.83ms
step:1295/1680 train_time:116330ms step_avg:89.83ms
step:1296/1680 train_time:116421ms step_avg:89.83ms
step:1297/1680 train_time:116512ms step_avg:89.83ms
step:1298/1680 train_time:116603ms step_avg:89.83ms
step:1299/1680 train_time:116694ms step_avg:89.83ms
step:1300/1680 train_time:116784ms step_avg:89.83ms
step:1301/1680 train_time:116876ms step_avg:89.84ms
step:1302/1680 train_time:116967ms step_avg:89.84ms
step:1303/1680 train_time:117059ms step_avg:89.84ms
step:1304/1680 train_time:117150ms step_avg:89.84ms
step:1305/1680 train_time:117240ms step_avg:89.84ms
step:1306/1680 train_time:117330ms step_avg:89.84ms
step:1307/1680 train_time:117420ms step_avg:89.84ms
step:1308/1680 train_time:117512ms step_avg:89.84ms
step:1309/1680 train_time:117603ms step_avg:89.84ms
step:1310/1680 train_time:117694ms step_avg:89.84ms
step:1311/1680 train_time:117786ms step_avg:89.84ms
step:1312/1680 train_time:117877ms step_avg:89.85ms
step:1313/1680 train_time:117968ms step_avg:89.85ms
step:1314/1680 train_time:118060ms step_avg:89.85ms
step:1315/1680 train_time:118152ms step_avg:89.85ms
step:1316/1680 train_time:118243ms step_avg:89.85ms
step:1317/1680 train_time:118333ms step_avg:89.85ms
step:1318/1680 train_time:118423ms step_avg:89.85ms
step:1319/1680 train_time:118515ms step_avg:89.85ms
step:1320/1680 train_time:118605ms step_avg:89.85ms
step:1321/1680 train_time:118697ms step_avg:89.85ms
step:1322/1680 train_time:118788ms step_avg:89.85ms
step:1323/1680 train_time:118879ms step_avg:89.86ms
step:1324/1680 train_time:118969ms step_avg:89.86ms
step:1325/1680 train_time:119061ms step_avg:89.86ms
step:1326/1680 train_time:119152ms step_avg:89.86ms
step:1327/1680 train_time:119242ms step_avg:89.86ms
step:1328/1680 train_time:119333ms step_avg:89.86ms
step:1329/1680 train_time:119424ms step_avg:89.86ms
step:1330/1680 train_time:119515ms step_avg:89.86ms
step:1331/1680 train_time:119605ms step_avg:89.86ms
step:1332/1680 train_time:119696ms step_avg:89.86ms
step:1333/1680 train_time:119786ms step_avg:89.86ms
step:1334/1680 train_time:119877ms step_avg:89.86ms
step:1335/1680 train_time:119969ms step_avg:89.86ms
step:1336/1680 train_time:120059ms step_avg:89.86ms
step:1337/1680 train_time:120151ms step_avg:89.87ms
step:1338/1680 train_time:120241ms step_avg:89.87ms
step:1339/1680 train_time:120332ms step_avg:89.87ms
step:1340/1680 train_time:120422ms step_avg:89.87ms
step:1341/1680 train_time:120513ms step_avg:89.87ms
step:1342/1680 train_time:120603ms step_avg:89.87ms
step:1343/1680 train_time:120695ms step_avg:89.87ms
step:1344/1680 train_time:120785ms step_avg:89.87ms
step:1345/1680 train_time:120876ms step_avg:89.87ms
step:1346/1680 train_time:120966ms step_avg:89.87ms
step:1347/1680 train_time:121058ms step_avg:89.87ms
step:1348/1680 train_time:121150ms step_avg:89.87ms
step:1349/1680 train_time:121241ms step_avg:89.87ms
step:1350/1680 train_time:121331ms step_avg:89.88ms
step:1351/1680 train_time:121423ms step_avg:89.88ms
step:1352/1680 train_time:121514ms step_avg:89.88ms
step:1353/1680 train_time:121606ms step_avg:89.88ms
step:1354/1680 train_time:121696ms step_avg:89.88ms
step:1355/1680 train_time:121786ms step_avg:89.88ms
step:1356/1680 train_time:121878ms step_avg:89.88ms
step:1357/1680 train_time:121968ms step_avg:89.88ms
step:1358/1680 train_time:122059ms step_avg:89.88ms
step:1359/1680 train_time:122150ms step_avg:89.88ms
step:1360/1680 train_time:122240ms step_avg:89.88ms
step:1361/1680 train_time:122331ms step_avg:89.88ms
step:1362/1680 train_time:122421ms step_avg:89.88ms
step:1363/1680 train_time:122513ms step_avg:89.88ms
step:1364/1680 train_time:122604ms step_avg:89.89ms
step:1365/1680 train_time:122695ms step_avg:89.89ms
step:1366/1680 train_time:122786ms step_avg:89.89ms
step:1367/1680 train_time:122877ms step_avg:89.89ms
step:1368/1680 train_time:122968ms step_avg:89.89ms
step:1369/1680 train_time:123060ms step_avg:89.89ms
step:1370/1680 train_time:123151ms step_avg:89.89ms
step:1371/1680 train_time:123241ms step_avg:89.89ms
step:1372/1680 train_time:123334ms step_avg:89.89ms
step:1373/1680 train_time:123424ms step_avg:89.89ms
step:1374/1680 train_time:123516ms step_avg:89.90ms
step:1375/1680 train_time:123606ms step_avg:89.90ms
step:1375/1680 val_loss:3.3420 train_time:123699ms step_avg:89.96ms
step:1376/1680 train_time:123721ms step_avg:89.91ms
step:1377/1680 train_time:123793ms step_avg:89.90ms
step:1378/1680 train_time:123889ms step_avg:89.90ms
step:1379/1680 train_time:123982ms step_avg:89.91ms
step:1380/1680 train_time:124073ms step_avg:89.91ms
step:1381/1680 train_time:124163ms step_avg:89.91ms
step:1382/1680 train_time:124254ms step_avg:89.91ms
step:1383/1680 train_time:124343ms step_avg:89.91ms
step:1384/1680 train_time:124433ms step_avg:89.91ms
step:1385/1680 train_time:124522ms step_avg:89.91ms
step:1386/1680 train_time:124612ms step_avg:89.91ms
step:1387/1680 train_time:124703ms step_avg:89.91ms
step:1388/1680 train_time:124797ms step_avg:89.91ms
step:1389/1680 train_time:124890ms step_avg:89.91ms
step:1390/1680 train_time:124983ms step_avg:89.92ms
step:1391/1680 train_time:125075ms step_avg:89.92ms
step:1392/1680 train_time:125165ms step_avg:89.92ms
step:1393/1680 train_time:125256ms step_avg:89.92ms
step:1394/1680 train_time:125346ms step_avg:89.92ms
step:1395/1680 train_time:125436ms step_avg:89.92ms
step:1396/1680 train_time:125525ms step_avg:89.92ms
step:1397/1680 train_time:125615ms step_avg:89.92ms
step:1398/1680 train_time:125706ms step_avg:89.92ms
step:1399/1680 train_time:125798ms step_avg:89.92ms
step:1400/1680 train_time:125890ms step_avg:89.92ms
step:1401/1680 train_time:125984ms step_avg:89.92ms
step:1402/1680 train_time:126075ms step_avg:89.93ms
step:1403/1680 train_time:126166ms step_avg:89.93ms
step:1404/1680 train_time:126257ms step_avg:89.93ms
step:1405/1680 train_time:126347ms step_avg:89.93ms
step:1406/1680 train_time:126436ms step_avg:89.93ms
step:1407/1680 train_time:126526ms step_avg:89.93ms
step:1408/1680 train_time:126617ms step_avg:89.93ms
step:1409/1680 train_time:126708ms step_avg:89.93ms
step:1410/1680 train_time:126800ms step_avg:89.93ms
step:1411/1680 train_time:126892ms step_avg:89.93ms
step:1412/1680 train_time:126983ms step_avg:89.93ms
step:1413/1680 train_time:127074ms step_avg:89.93ms
step:1414/1680 train_time:127166ms step_avg:89.93ms
step:1415/1680 train_time:127256ms step_avg:89.93ms
step:1416/1680 train_time:127346ms step_avg:89.93ms
step:1417/1680 train_time:127436ms step_avg:89.93ms
step:1418/1680 train_time:127527ms step_avg:89.93ms
step:1419/1680 train_time:127617ms step_avg:89.93ms
step:1420/1680 train_time:127709ms step_avg:89.94ms
step:1421/1680 train_time:127800ms step_avg:89.94ms
step:1422/1680 train_time:127898ms step_avg:89.94ms
step:1423/1680 train_time:127984ms step_avg:89.94ms
step:1424/1680 train_time:128074ms step_avg:89.94ms
step:1425/1680 train_time:128165ms step_avg:89.94ms
step:1426/1680 train_time:128256ms step_avg:89.94ms
step:1427/1680 train_time:128346ms step_avg:89.94ms
step:1428/1680 train_time:128436ms step_avg:89.94ms
step:1429/1680 train_time:128527ms step_avg:89.94ms
step:1430/1680 train_time:128619ms step_avg:89.94ms
step:1431/1680 train_time:128708ms step_avg:89.94ms
step:1432/1680 train_time:128800ms step_avg:89.94ms
step:1433/1680 train_time:128891ms step_avg:89.94ms
step:1434/1680 train_time:128983ms step_avg:89.95ms
step:1435/1680 train_time:129073ms step_avg:89.95ms
step:1436/1680 train_time:129164ms step_avg:89.95ms
step:1437/1680 train_time:129254ms step_avg:89.95ms
step:1438/1680 train_time:129344ms step_avg:89.95ms
step:1439/1680 train_time:129435ms step_avg:89.95ms
step:1440/1680 train_time:129526ms step_avg:89.95ms
step:1441/1680 train_time:129617ms step_avg:89.95ms
step:1442/1680 train_time:129708ms step_avg:89.95ms
step:1443/1680 train_time:129799ms step_avg:89.95ms
step:1444/1680 train_time:129891ms step_avg:89.95ms
step:1445/1680 train_time:129984ms step_avg:89.95ms
step:1446/1680 train_time:130075ms step_avg:89.96ms
step:1447/1680 train_time:130165ms step_avg:89.95ms
step:1448/1680 train_time:130255ms step_avg:89.96ms
step:1449/1680 train_time:130346ms step_avg:89.96ms
step:1450/1680 train_time:130437ms step_avg:89.96ms
step:1451/1680 train_time:130527ms step_avg:89.96ms
step:1452/1680 train_time:130619ms step_avg:89.96ms
step:1453/1680 train_time:130709ms step_avg:89.96ms
step:1454/1680 train_time:130800ms step_avg:89.96ms
step:1455/1680 train_time:130891ms step_avg:89.96ms
step:1456/1680 train_time:130982ms step_avg:89.96ms
step:1457/1680 train_time:131073ms step_avg:89.96ms
step:1458/1680 train_time:131163ms step_avg:89.96ms
step:1459/1680 train_time:131254ms step_avg:89.96ms
step:1460/1680 train_time:131345ms step_avg:89.96ms
step:1461/1680 train_time:131435ms step_avg:89.96ms
step:1462/1680 train_time:131526ms step_avg:89.96ms
step:1463/1680 train_time:131618ms step_avg:89.96ms
step:1464/1680 train_time:131708ms step_avg:89.96ms
step:1465/1680 train_time:131801ms step_avg:89.97ms
step:1466/1680 train_time:131894ms step_avg:89.97ms
step:1467/1680 train_time:131984ms step_avg:89.97ms
step:1468/1680 train_time:132076ms step_avg:89.97ms
step:1469/1680 train_time:132166ms step_avg:89.97ms
step:1470/1680 train_time:132257ms step_avg:89.97ms
step:1471/1680 train_time:132346ms step_avg:89.97ms
step:1472/1680 train_time:132437ms step_avg:89.97ms
step:1473/1680 train_time:132529ms step_avg:89.97ms
step:1474/1680 train_time:132620ms step_avg:89.97ms
step:1475/1680 train_time:132710ms step_avg:89.97ms
step:1476/1680 train_time:132801ms step_avg:89.97ms
step:1477/1680 train_time:132893ms step_avg:89.97ms
step:1478/1680 train_time:132984ms step_avg:89.98ms
step:1479/1680 train_time:133075ms step_avg:89.98ms
step:1480/1680 train_time:133166ms step_avg:89.98ms
step:1481/1680 train_time:133258ms step_avg:89.98ms
step:1482/1680 train_time:133348ms step_avg:89.98ms
step:1483/1680 train_time:133438ms step_avg:89.98ms
step:1484/1680 train_time:133528ms step_avg:89.98ms
step:1485/1680 train_time:133619ms step_avg:89.98ms
step:1486/1680 train_time:133709ms step_avg:89.98ms
step:1487/1680 train_time:133801ms step_avg:89.98ms
step:1488/1680 train_time:133892ms step_avg:89.98ms
step:1489/1680 train_time:133983ms step_avg:89.98ms
step:1490/1680 train_time:134074ms step_avg:89.98ms
step:1491/1680 train_time:134164ms step_avg:89.98ms
step:1492/1680 train_time:134255ms step_avg:89.98ms
step:1493/1680 train_time:134345ms step_avg:89.98ms
step:1494/1680 train_time:134436ms step_avg:89.98ms
step:1495/1680 train_time:134526ms step_avg:89.98ms
step:1496/1680 train_time:134617ms step_avg:89.98ms
step:1497/1680 train_time:134707ms step_avg:89.98ms
step:1498/1680 train_time:134799ms step_avg:89.99ms
step:1499/1680 train_time:134891ms step_avg:89.99ms
step:1500/1680 train_time:134981ms step_avg:89.99ms
step:1500/1680 val_loss:3.3121 train_time:135073ms step_avg:90.05ms
step:1501/1680 train_time:135095ms step_avg:90.00ms
step:1502/1680 train_time:135166ms step_avg:89.99ms
step:1503/1680 train_time:135261ms step_avg:89.99ms
step:1504/1680 train_time:135351ms step_avg:89.99ms
step:1505/1680 train_time:135442ms step_avg:89.99ms
step:1506/1680 train_time:135532ms step_avg:89.99ms
step:1507/1680 train_time:135622ms step_avg:89.99ms
step:1508/1680 train_time:135712ms step_avg:89.99ms
step:1509/1680 train_time:135803ms step_avg:90.00ms
step:1510/1680 train_time:135893ms step_avg:90.00ms
step:1511/1680 train_time:135983ms step_avg:90.00ms
step:1512/1680 train_time:136076ms step_avg:90.00ms
step:1513/1680 train_time:136168ms step_avg:90.00ms
step:1514/1680 train_time:136261ms step_avg:90.00ms
step:1515/1680 train_time:136354ms step_avg:90.00ms
step:1516/1680 train_time:136444ms step_avg:90.00ms
step:1517/1680 train_time:136534ms step_avg:90.00ms
step:1518/1680 train_time:136623ms step_avg:90.00ms
step:1519/1680 train_time:136714ms step_avg:90.00ms
step:1520/1680 train_time:136804ms step_avg:90.00ms
step:1521/1680 train_time:136894ms step_avg:90.00ms
step:1522/1680 train_time:136986ms step_avg:90.00ms
step:1523/1680 train_time:137077ms step_avg:90.00ms
step:1524/1680 train_time:137169ms step_avg:90.01ms
step:1525/1680 train_time:137260ms step_avg:90.01ms
step:1526/1680 train_time:137353ms step_avg:90.01ms
step:1527/1680 train_time:137449ms step_avg:90.01ms
step:1528/1680 train_time:137535ms step_avg:90.01ms
step:1529/1680 train_time:137625ms step_avg:90.01ms
step:1530/1680 train_time:137715ms step_avg:90.01ms
step:1531/1680 train_time:137805ms step_avg:90.01ms
step:1532/1680 train_time:137896ms step_avg:90.01ms
step:1533/1680 train_time:137988ms step_avg:90.01ms
step:1534/1680 train_time:138078ms step_avg:90.01ms
step:1535/1680 train_time:138169ms step_avg:90.01ms
step:1536/1680 train_time:138260ms step_avg:90.01ms
step:1537/1680 train_time:138355ms step_avg:90.02ms
step:1538/1680 train_time:138447ms step_avg:90.02ms
step:1539/1680 train_time:138536ms step_avg:90.02ms
step:1540/1680 train_time:138627ms step_avg:90.02ms
step:1541/1680 train_time:138717ms step_avg:90.02ms
step:1542/1680 train_time:138807ms step_avg:90.02ms
step:1543/1680 train_time:138898ms step_avg:90.02ms
step:1544/1680 train_time:138989ms step_avg:90.02ms
step:1545/1680 train_time:139080ms step_avg:90.02ms
step:1546/1680 train_time:139171ms step_avg:90.02ms
step:1547/1680 train_time:139261ms step_avg:90.02ms
step:1548/1680 train_time:139354ms step_avg:90.02ms
step:1549/1680 train_time:139446ms step_avg:90.02ms
step:1550/1680 train_time:139536ms step_avg:90.02ms
step:1551/1680 train_time:139626ms step_avg:90.02ms
step:1552/1680 train_time:139716ms step_avg:90.02ms
step:1553/1680 train_time:139807ms step_avg:90.02ms
step:1554/1680 train_time:139897ms step_avg:90.02ms
step:1555/1680 train_time:139987ms step_avg:90.02ms
step:1556/1680 train_time:140078ms step_avg:90.02ms
step:1557/1680 train_time:140170ms step_avg:90.03ms
step:1558/1680 train_time:140261ms step_avg:90.03ms
step:1559/1680 train_time:140354ms step_avg:90.03ms
step:1560/1680 train_time:140446ms step_avg:90.03ms
step:1561/1680 train_time:140536ms step_avg:90.03ms
step:1562/1680 train_time:140627ms step_avg:90.03ms
step:1563/1680 train_time:140717ms step_avg:90.03ms
step:1564/1680 train_time:140807ms step_avg:90.03ms
step:1565/1680 train_time:140897ms step_avg:90.03ms
step:1566/1680 train_time:140988ms step_avg:90.03ms
step:1567/1680 train_time:141078ms step_avg:90.03ms
step:1568/1680 train_time:141169ms step_avg:90.03ms
step:1569/1680 train_time:141259ms step_avg:90.03ms
step:1570/1680 train_time:141351ms step_avg:90.03ms
step:1571/1680 train_time:141442ms step_avg:90.03ms
step:1572/1680 train_time:141533ms step_avg:90.03ms
step:1573/1680 train_time:141624ms step_avg:90.03ms
step:1574/1680 train_time:141715ms step_avg:90.03ms
step:1575/1680 train_time:141805ms step_avg:90.04ms
step:1576/1680 train_time:141896ms step_avg:90.04ms
step:1577/1680 train_time:141986ms step_avg:90.04ms
step:1578/1680 train_time:142078ms step_avg:90.04ms
step:1579/1680 train_time:142168ms step_avg:90.04ms
step:1580/1680 train_time:142260ms step_avg:90.04ms
step:1581/1680 train_time:142352ms step_avg:90.04ms
step:1582/1680 train_time:142443ms step_avg:90.04ms
step:1583/1680 train_time:142534ms step_avg:90.04ms
step:1584/1680 train_time:142625ms step_avg:90.04ms
step:1585/1680 train_time:142715ms step_avg:90.04ms
step:1586/1680 train_time:142806ms step_avg:90.04ms
step:1587/1680 train_time:142897ms step_avg:90.04ms
step:1588/1680 train_time:142987ms step_avg:90.04ms
step:1589/1680 train_time:143078ms step_avg:90.04ms
step:1590/1680 train_time:143168ms step_avg:90.04ms
step:1591/1680 train_time:143259ms step_avg:90.04ms
step:1592/1680 train_time:143350ms step_avg:90.04ms
step:1593/1680 train_time:143441ms step_avg:90.04ms
step:1594/1680 train_time:143532ms step_avg:90.05ms
step:1595/1680 train_time:143623ms step_avg:90.05ms
step:1596/1680 train_time:143714ms step_avg:90.05ms
step:1597/1680 train_time:143805ms step_avg:90.05ms
step:1598/1680 train_time:143895ms step_avg:90.05ms
step:1599/1680 train_time:143985ms step_avg:90.05ms
step:1600/1680 train_time:144076ms step_avg:90.05ms
step:1601/1680 train_time:144167ms step_avg:90.05ms
step:1602/1680 train_time:144258ms step_avg:90.05ms
step:1603/1680 train_time:144349ms step_avg:90.05ms
step:1604/1680 train_time:144439ms step_avg:90.05ms
step:1605/1680 train_time:144531ms step_avg:90.05ms
step:1606/1680 train_time:144622ms step_avg:90.05ms
step:1607/1680 train_time:144713ms step_avg:90.05ms
step:1608/1680 train_time:144803ms step_avg:90.05ms
step:1609/1680 train_time:144894ms step_avg:90.05ms
step:1610/1680 train_time:144986ms step_avg:90.05ms
step:1611/1680 train_time:145077ms step_avg:90.05ms
step:1612/1680 train_time:145168ms step_avg:90.05ms
step:1613/1680 train_time:145259ms step_avg:90.06ms
step:1614/1680 train_time:145351ms step_avg:90.06ms
step:1615/1680 train_time:145441ms step_avg:90.06ms
step:1616/1680 train_time:145532ms step_avg:90.06ms
step:1617/1680 train_time:145623ms step_avg:90.06ms
step:1618/1680 train_time:145715ms step_avg:90.06ms
step:1619/1680 train_time:145806ms step_avg:90.06ms
step:1620/1680 train_time:145897ms step_avg:90.06ms
step:1621/1680 train_time:145989ms step_avg:90.06ms
step:1622/1680 train_time:146079ms step_avg:90.06ms
step:1623/1680 train_time:146170ms step_avg:90.06ms
step:1624/1680 train_time:146260ms step_avg:90.06ms
step:1625/1680 train_time:146352ms step_avg:90.06ms
step:1625/1680 val_loss:3.2884 train_time:146442ms step_avg:90.12ms
step:1626/1680 train_time:146464ms step_avg:90.08ms
step:1627/1680 train_time:146536ms step_avg:90.07ms
step:1628/1680 train_time:146630ms step_avg:90.07ms
step:1629/1680 train_time:146722ms step_avg:90.07ms
step:1630/1680 train_time:146812ms step_avg:90.07ms
step:1631/1680 train_time:146903ms step_avg:90.07ms
step:1632/1680 train_time:146991ms step_avg:90.07ms
step:1633/1680 train_time:147080ms step_avg:90.07ms
step:1634/1680 train_time:147170ms step_avg:90.07ms
step:1635/1680 train_time:147260ms step_avg:90.07ms
step:1636/1680 train_time:147351ms step_avg:90.07ms
step:1637/1680 train_time:147444ms step_avg:90.07ms
step:1638/1680 train_time:147537ms step_avg:90.07ms
step:1639/1680 train_time:147628ms step_avg:90.07ms
step:1640/1680 train_time:147720ms step_avg:90.07ms
step:1641/1680 train_time:147810ms step_avg:90.07ms
step:1642/1680 train_time:147901ms step_avg:90.07ms
step:1643/1680 train_time:147991ms step_avg:90.07ms
step:1644/1680 train_time:148081ms step_avg:90.07ms
step:1645/1680 train_time:148170ms step_avg:90.07ms
step:1646/1680 train_time:148261ms step_avg:90.07ms
step:1647/1680 train_time:148351ms step_avg:90.07ms
step:1648/1680 train_time:148444ms step_avg:90.08ms
step:1649/1680 train_time:148536ms step_avg:90.08ms
step:1650/1680 train_time:148627ms step_avg:90.08ms
step:1651/1680 train_time:148718ms step_avg:90.08ms
step:1652/1680 train_time:148808ms step_avg:90.08ms
step:1653/1680 train_time:148899ms step_avg:90.08ms
step:1654/1680 train_time:148989ms step_avg:90.08ms
step:1655/1680 train_time:149081ms step_avg:90.08ms
step:1656/1680 train_time:149172ms step_avg:90.08ms
step:1657/1680 train_time:149262ms step_avg:90.08ms
step:1658/1680 train_time:149351ms step_avg:90.08ms
step:1659/1680 train_time:149443ms step_avg:90.08ms
step:1660/1680 train_time:149532ms step_avg:90.08ms
step:1661/1680 train_time:149624ms step_avg:90.08ms
step:1662/1680 train_time:149716ms step_avg:90.08ms
step:1663/1680 train_time:149806ms step_avg:90.08ms
step:1664/1680 train_time:149896ms step_avg:90.08ms
step:1665/1680 train_time:149987ms step_avg:90.08ms
step:1666/1680 train_time:150078ms step_avg:90.08ms
step:1667/1680 train_time:150168ms step_avg:90.08ms
step:1668/1680 train_time:150259ms step_avg:90.08ms
step:1669/1680 train_time:150349ms step_avg:90.08ms
step:1670/1680 train_time:150440ms step_avg:90.08ms
step:1671/1680 train_time:150531ms step_avg:90.08ms
step:1672/1680 train_time:150622ms step_avg:90.08ms
step:1673/1680 train_time:150713ms step_avg:90.09ms
step:1674/1680 train_time:150804ms step_avg:90.09ms
step:1675/1680 train_time:150895ms step_avg:90.09ms
step:1676/1680 train_time:150986ms step_avg:90.09ms
step:1677/1680 train_time:151075ms step_avg:90.09ms
step:1678/1680 train_time:151166ms step_avg:90.09ms
step:1679/1680 train_time:151256ms step_avg:90.09ms
step:1680/1680 train_time:151347ms step_avg:90.09ms
step:1680/1680 val_loss:3.2777 train_time:151440ms step_avg:90.14ms
peak memory allocated: 31255 MiB reserved: 46774 MiB

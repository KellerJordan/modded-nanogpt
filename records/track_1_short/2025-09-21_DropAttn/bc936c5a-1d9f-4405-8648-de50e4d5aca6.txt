import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 and layer_idx !=0 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        first_bm = 1 * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, final_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"v1/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer shows no degradation with context length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
smear_gate_params = [p for n, p in model.named_parameters() if "smear" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params+smear_gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate, args.ws_validate_final_layer
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250724+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sun Sep 21 22:41:29 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   44C    P0            126W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   39C    P0            125W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   40C    P0            127W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           65397      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A           65398      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           65399      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           65400      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           65401      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           65402      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           65403      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           65404      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           65398      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A           65399      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A           65400      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A           65401      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A           65402      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A           65403      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A           65404      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/1680 train_time:154ms step_avg:154.18ms
step:2/1680 train_time:180ms step_avg:89.95ms
step:3/1680 train_time:240ms step_avg:80.11ms
step:4/1680 train_time:327ms step_avg:81.83ms
step:5/1680 train_time:416ms step_avg:83.12ms
step:6/1680 train_time:504ms step_avg:83.96ms
step:7/1680 train_time:592ms step_avg:84.61ms
step:8/1680 train_time:680ms step_avg:85.05ms
step:9/1680 train_time:769ms step_avg:85.40ms
step:10/1680 train_time:857ms step_avg:85.67ms
step:11/1680 train_time:945ms step_avg:85.92ms
step:12/1680 train_time:1037ms step_avg:86.44ms
step:13/1680 train_time:1130ms step_avg:86.95ms
step:14/1680 train_time:1221ms step_avg:87.23ms
step:15/1680 train_time:1311ms step_avg:87.40ms
step:16/1680 train_time:1401ms step_avg:87.53ms
step:17/1680 train_time:1490ms step_avg:87.63ms
step:18/1680 train_time:1578ms step_avg:87.67ms
step:19/1680 train_time:1666ms step_avg:87.70ms
step:20/1680 train_time:1755ms step_avg:87.73ms
step:21/1680 train_time:1843ms step_avg:87.77ms
step:22/1680 train_time:1932ms step_avg:87.80ms
step:23/1680 train_time:2021ms step_avg:87.88ms
step:24/1680 train_time:2114ms step_avg:88.09ms
step:25/1680 train_time:2205ms step_avg:88.21ms
step:26/1680 train_time:2295ms step_avg:88.26ms
step:27/1680 train_time:2384ms step_avg:88.30ms
step:28/1680 train_time:2473ms step_avg:88.33ms
step:29/1680 train_time:2563ms step_avg:88.39ms
step:30/1680 train_time:2653ms step_avg:88.43ms
step:31/1680 train_time:2741ms step_avg:88.43ms
step:32/1680 train_time:2830ms step_avg:88.44ms
step:33/1680 train_time:2919ms step_avg:88.45ms
step:34/1680 train_time:3009ms step_avg:88.49ms
step:35/1680 train_time:3098ms step_avg:88.52ms
step:36/1680 train_time:3188ms step_avg:88.55ms
step:37/1680 train_time:3277ms step_avg:88.58ms
step:38/1680 train_time:3367ms step_avg:88.60ms
step:39/1680 train_time:3456ms step_avg:88.62ms
step:40/1680 train_time:3546ms step_avg:88.64ms
step:41/1680 train_time:3635ms step_avg:88.66ms
step:42/1680 train_time:3724ms step_avg:88.67ms
step:43/1680 train_time:3814ms step_avg:88.70ms
step:44/1680 train_time:3903ms step_avg:88.69ms
step:45/1680 train_time:3992ms step_avg:88.70ms
step:46/1680 train_time:4081ms step_avg:88.72ms
step:47/1680 train_time:4171ms step_avg:88.75ms
step:48/1680 train_time:4261ms step_avg:88.76ms
step:49/1680 train_time:4351ms step_avg:88.80ms
step:50/1680 train_time:4440ms step_avg:88.80ms
step:51/1680 train_time:4530ms step_avg:88.83ms
step:52/1680 train_time:4619ms step_avg:88.83ms
step:53/1680 train_time:4708ms step_avg:88.83ms
step:54/1680 train_time:4797ms step_avg:88.83ms
step:55/1680 train_time:4885ms step_avg:88.83ms
step:56/1680 train_time:4974ms step_avg:88.83ms
step:57/1680 train_time:5064ms step_avg:88.84ms
step:58/1680 train_time:5153ms step_avg:88.85ms
step:59/1680 train_time:5243ms step_avg:88.87ms
step:60/1680 train_time:5333ms step_avg:88.88ms
step:61/1680 train_time:5423ms step_avg:88.89ms
step:62/1680 train_time:5512ms step_avg:88.91ms
step:63/1680 train_time:5602ms step_avg:88.92ms
step:64/1680 train_time:5691ms step_avg:88.92ms
step:65/1680 train_time:5780ms step_avg:88.92ms
step:66/1680 train_time:5870ms step_avg:88.94ms
step:67/1680 train_time:5960ms step_avg:88.95ms
step:68/1680 train_time:6048ms step_avg:88.95ms
step:69/1680 train_time:6137ms step_avg:88.95ms
step:70/1680 train_time:6226ms step_avg:88.94ms
step:71/1680 train_time:6316ms step_avg:88.95ms
step:72/1680 train_time:6405ms step_avg:88.96ms
step:73/1680 train_time:6494ms step_avg:88.96ms
step:74/1680 train_time:6582ms step_avg:88.95ms
step:75/1680 train_time:6672ms step_avg:88.96ms
step:76/1680 train_time:6761ms step_avg:88.96ms
step:77/1680 train_time:6850ms step_avg:88.96ms
step:78/1680 train_time:6940ms step_avg:88.97ms
step:79/1680 train_time:7029ms step_avg:88.97ms
step:80/1680 train_time:7118ms step_avg:88.98ms
step:81/1680 train_time:7207ms step_avg:88.98ms
step:82/1680 train_time:7297ms step_avg:88.99ms
step:83/1680 train_time:7386ms step_avg:88.98ms
step:84/1680 train_time:7474ms step_avg:88.98ms
step:85/1680 train_time:7564ms step_avg:88.98ms
step:86/1680 train_time:7652ms step_avg:88.98ms
step:87/1680 train_time:7742ms step_avg:88.99ms
step:88/1680 train_time:7832ms step_avg:89.00ms
step:89/1680 train_time:7921ms step_avg:89.00ms
step:90/1680 train_time:8010ms step_avg:89.00ms
step:91/1680 train_time:8099ms step_avg:89.00ms
step:92/1680 train_time:8187ms step_avg:88.99ms
step:93/1680 train_time:8276ms step_avg:88.99ms
step:94/1680 train_time:8366ms step_avg:89.00ms
step:95/1680 train_time:8455ms step_avg:89.00ms
step:96/1680 train_time:8544ms step_avg:89.00ms
step:97/1680 train_time:8633ms step_avg:89.00ms
step:98/1680 train_time:8722ms step_avg:89.00ms
step:99/1680 train_time:8812ms step_avg:89.01ms
step:100/1680 train_time:8900ms step_avg:89.00ms
step:101/1680 train_time:8989ms step_avg:89.00ms
step:102/1680 train_time:9078ms step_avg:89.00ms
step:103/1680 train_time:9167ms step_avg:89.00ms
step:104/1680 train_time:9255ms step_avg:88.99ms
step:105/1680 train_time:9345ms step_avg:89.00ms
step:106/1680 train_time:9433ms step_avg:88.99ms
step:107/1680 train_time:9523ms step_avg:89.00ms
step:108/1680 train_time:9612ms step_avg:89.00ms
step:109/1680 train_time:9701ms step_avg:89.00ms
step:110/1680 train_time:9791ms step_avg:89.01ms
step:111/1680 train_time:9879ms step_avg:89.00ms
step:112/1680 train_time:9967ms step_avg:88.99ms
step:113/1680 train_time:10056ms step_avg:88.99ms
step:114/1680 train_time:10145ms step_avg:89.00ms
step:115/1680 train_time:10234ms step_avg:88.99ms
step:116/1680 train_time:10323ms step_avg:88.99ms
step:117/1680 train_time:10412ms step_avg:88.99ms
step:118/1680 train_time:10502ms step_avg:89.00ms
step:119/1680 train_time:10591ms step_avg:89.00ms
step:120/1680 train_time:10680ms step_avg:89.00ms
step:121/1680 train_time:10770ms step_avg:89.00ms
step:122/1680 train_time:10858ms step_avg:89.00ms
step:123/1680 train_time:10947ms step_avg:89.00ms
step:124/1680 train_time:11036ms step_avg:89.00ms
step:125/1680 train_time:11124ms step_avg:88.99ms
step:125/1680 val_loss:4.3230 train_time:11215ms step_avg:89.72ms
step:126/1680 train_time:11242ms step_avg:89.22ms
step:127/1680 train_time:11305ms step_avg:89.02ms
step:128/1680 train_time:11402ms step_avg:89.08ms
step:129/1680 train_time:11495ms step_avg:89.11ms
step:130/1680 train_time:11584ms step_avg:89.11ms
step:131/1680 train_time:11673ms step_avg:89.11ms
step:132/1680 train_time:11762ms step_avg:89.11ms
step:133/1680 train_time:11849ms step_avg:89.09ms
step:134/1680 train_time:11937ms step_avg:89.08ms
step:135/1680 train_time:12025ms step_avg:89.07ms
step:136/1680 train_time:12113ms step_avg:89.06ms
step:137/1680 train_time:12201ms step_avg:89.06ms
step:138/1680 train_time:12290ms step_avg:89.06ms
step:139/1680 train_time:12381ms step_avg:89.07ms
step:140/1680 train_time:12471ms step_avg:89.08ms
step:141/1680 train_time:12561ms step_avg:89.08ms
step:142/1680 train_time:12650ms step_avg:89.08ms
step:143/1680 train_time:12739ms step_avg:89.09ms
step:144/1680 train_time:12828ms step_avg:89.08ms
step:145/1680 train_time:12916ms step_avg:89.08ms
step:146/1680 train_time:13005ms step_avg:89.07ms
step:147/1680 train_time:13093ms step_avg:89.07ms
step:148/1680 train_time:13182ms step_avg:89.06ms
step:149/1680 train_time:13270ms step_avg:89.06ms
step:150/1680 train_time:13361ms step_avg:89.07ms
step:151/1680 train_time:13451ms step_avg:89.08ms
step:152/1680 train_time:13540ms step_avg:89.08ms
step:153/1680 train_time:13629ms step_avg:89.08ms
step:154/1680 train_time:13718ms step_avg:89.08ms
step:155/1680 train_time:13806ms step_avg:89.07ms
step:156/1680 train_time:13895ms step_avg:89.07ms
step:157/1680 train_time:13984ms step_avg:89.07ms
step:158/1680 train_time:14073ms step_avg:89.07ms
step:159/1680 train_time:14163ms step_avg:89.08ms
step:160/1680 train_time:14252ms step_avg:89.07ms
step:161/1680 train_time:14342ms step_avg:89.08ms
step:162/1680 train_time:14431ms step_avg:89.08ms
step:163/1680 train_time:14520ms step_avg:89.08ms
step:164/1680 train_time:14610ms step_avg:89.08ms
step:165/1680 train_time:14699ms step_avg:89.09ms
step:166/1680 train_time:14788ms step_avg:89.08ms
step:167/1680 train_time:14876ms step_avg:89.08ms
step:168/1680 train_time:14965ms step_avg:89.08ms
step:169/1680 train_time:15054ms step_avg:89.08ms
step:170/1680 train_time:15143ms step_avg:89.08ms
step:171/1680 train_time:15231ms step_avg:89.07ms
step:172/1680 train_time:15320ms step_avg:89.07ms
step:173/1680 train_time:15409ms step_avg:89.07ms
step:174/1680 train_time:15498ms step_avg:89.07ms
step:175/1680 train_time:15588ms step_avg:89.07ms
step:176/1680 train_time:15678ms step_avg:89.08ms
step:177/1680 train_time:15766ms step_avg:89.08ms
step:178/1680 train_time:15856ms step_avg:89.08ms
step:179/1680 train_time:15944ms step_avg:89.07ms
step:180/1680 train_time:16033ms step_avg:89.07ms
step:181/1680 train_time:16122ms step_avg:89.07ms
step:182/1680 train_time:16210ms step_avg:89.07ms
step:183/1680 train_time:16299ms step_avg:89.07ms
step:184/1680 train_time:16388ms step_avg:89.06ms
step:185/1680 train_time:16477ms step_avg:89.06ms
step:186/1680 train_time:16566ms step_avg:89.06ms
step:187/1680 train_time:16656ms step_avg:89.07ms
step:188/1680 train_time:16745ms step_avg:89.07ms
step:189/1680 train_time:16834ms step_avg:89.07ms
step:190/1680 train_time:16923ms step_avg:89.07ms
step:191/1680 train_time:17012ms step_avg:89.07ms
step:192/1680 train_time:17101ms step_avg:89.07ms
step:193/1680 train_time:17190ms step_avg:89.07ms
step:194/1680 train_time:17279ms step_avg:89.07ms
step:195/1680 train_time:17368ms step_avg:89.07ms
step:196/1680 train_time:17457ms step_avg:89.07ms
step:197/1680 train_time:17546ms step_avg:89.07ms
step:198/1680 train_time:17636ms step_avg:89.07ms
step:199/1680 train_time:17725ms step_avg:89.07ms
step:200/1680 train_time:17815ms step_avg:89.07ms
step:201/1680 train_time:17904ms step_avg:89.07ms
step:202/1680 train_time:17992ms step_avg:89.07ms
step:203/1680 train_time:18082ms step_avg:89.07ms
step:204/1680 train_time:18171ms step_avg:89.07ms
step:205/1680 train_time:18260ms step_avg:89.07ms
step:206/1680 train_time:18349ms step_avg:89.07ms
step:207/1680 train_time:18438ms step_avg:89.07ms
step:208/1680 train_time:18527ms step_avg:89.07ms
step:209/1680 train_time:18615ms step_avg:89.07ms
step:210/1680 train_time:18704ms step_avg:89.07ms
step:211/1680 train_time:18794ms step_avg:89.07ms
step:212/1680 train_time:18882ms step_avg:89.07ms
step:213/1680 train_time:18971ms step_avg:89.07ms
step:214/1680 train_time:19060ms step_avg:89.07ms
step:215/1680 train_time:19149ms step_avg:89.06ms
step:216/1680 train_time:19238ms step_avg:89.06ms
step:217/1680 train_time:19327ms step_avg:89.06ms
step:218/1680 train_time:19416ms step_avg:89.07ms
step:219/1680 train_time:19505ms step_avg:89.07ms
step:220/1680 train_time:19594ms step_avg:89.06ms
step:221/1680 train_time:19684ms step_avg:89.07ms
step:222/1680 train_time:19774ms step_avg:89.07ms
step:223/1680 train_time:19863ms step_avg:89.07ms
step:224/1680 train_time:19952ms step_avg:89.07ms
step:225/1680 train_time:20041ms step_avg:89.07ms
step:226/1680 train_time:20130ms step_avg:89.07ms
step:227/1680 train_time:20218ms step_avg:89.07ms
step:228/1680 train_time:20308ms step_avg:89.07ms
step:229/1680 train_time:20398ms step_avg:89.07ms
step:230/1680 train_time:20487ms step_avg:89.07ms
step:231/1680 train_time:20576ms step_avg:89.07ms
step:232/1680 train_time:20665ms step_avg:89.07ms
step:233/1680 train_time:20754ms step_avg:89.07ms
step:234/1680 train_time:20843ms step_avg:89.07ms
step:235/1680 train_time:20932ms step_avg:89.07ms
step:236/1680 train_time:21021ms step_avg:89.07ms
step:237/1680 train_time:21109ms step_avg:89.07ms
step:238/1680 train_time:21198ms step_avg:89.07ms
step:239/1680 train_time:21287ms step_avg:89.07ms
step:240/1680 train_time:21376ms step_avg:89.07ms
step:241/1680 train_time:21465ms step_avg:89.07ms
step:242/1680 train_time:21554ms step_avg:89.07ms
step:243/1680 train_time:21644ms step_avg:89.07ms
step:244/1680 train_time:21733ms step_avg:89.07ms
step:245/1680 train_time:21821ms step_avg:89.07ms
step:246/1680 train_time:21911ms step_avg:89.07ms
step:247/1680 train_time:22000ms step_avg:89.07ms
step:248/1680 train_time:22089ms step_avg:89.07ms
step:249/1680 train_time:22178ms step_avg:89.07ms
step:250/1680 train_time:22267ms step_avg:89.07ms
step:250/1680 val_loss:3.9675 train_time:22358ms step_avg:89.43ms
step:251/1680 train_time:22381ms step_avg:89.17ms
step:252/1680 train_time:22451ms step_avg:89.09ms
step:253/1680 train_time:22543ms step_avg:89.10ms
step:254/1680 train_time:22636ms step_avg:89.12ms
step:255/1680 train_time:22725ms step_avg:89.12ms
step:256/1680 train_time:22813ms step_avg:89.11ms
step:257/1680 train_time:22901ms step_avg:89.11ms
step:258/1680 train_time:22989ms step_avg:89.10ms
step:259/1680 train_time:23078ms step_avg:89.10ms
step:260/1680 train_time:23166ms step_avg:89.10ms
step:261/1680 train_time:23254ms step_avg:89.10ms
step:262/1680 train_time:23344ms step_avg:89.10ms
step:263/1680 train_time:23435ms step_avg:89.11ms
step:264/1680 train_time:23526ms step_avg:89.11ms
step:265/1680 train_time:23618ms step_avg:89.12ms
step:266/1680 train_time:23707ms step_avg:89.12ms
step:267/1680 train_time:23796ms step_avg:89.12ms
step:268/1680 train_time:23885ms step_avg:89.12ms
step:269/1680 train_time:23973ms step_avg:89.12ms
step:270/1680 train_time:24061ms step_avg:89.12ms
step:271/1680 train_time:24150ms step_avg:89.11ms
step:272/1680 train_time:24239ms step_avg:89.11ms
step:273/1680 train_time:24327ms step_avg:89.11ms
step:274/1680 train_time:24417ms step_avg:89.11ms
step:275/1680 train_time:24507ms step_avg:89.12ms
step:276/1680 train_time:24598ms step_avg:89.12ms
step:277/1680 train_time:24688ms step_avg:89.13ms
step:278/1680 train_time:24777ms step_avg:89.13ms
step:279/1680 train_time:24866ms step_avg:89.13ms
step:280/1680 train_time:24956ms step_avg:89.13ms
step:281/1680 train_time:25044ms step_avg:89.13ms
step:282/1680 train_time:25133ms step_avg:89.12ms
step:283/1680 train_time:25222ms step_avg:89.12ms
step:284/1680 train_time:25311ms step_avg:89.12ms
step:285/1680 train_time:25400ms step_avg:89.12ms
step:286/1680 train_time:25490ms step_avg:89.13ms
step:287/1680 train_time:25581ms step_avg:89.13ms
step:288/1680 train_time:25670ms step_avg:89.13ms
step:289/1680 train_time:25759ms step_avg:89.13ms
step:290/1680 train_time:25848ms step_avg:89.13ms
step:291/1680 train_time:25938ms step_avg:89.13ms
step:292/1680 train_time:26027ms step_avg:89.13ms
step:293/1680 train_time:26116ms step_avg:89.13ms
step:294/1680 train_time:26205ms step_avg:89.13ms
step:295/1680 train_time:26294ms step_avg:89.13ms
step:296/1680 train_time:26382ms step_avg:89.13ms
step:297/1680 train_time:26472ms step_avg:89.13ms
step:298/1680 train_time:26561ms step_avg:89.13ms
step:299/1680 train_time:26651ms step_avg:89.13ms
step:300/1680 train_time:26740ms step_avg:89.13ms
step:301/1680 train_time:26830ms step_avg:89.14ms
step:302/1680 train_time:26920ms step_avg:89.14ms
step:303/1680 train_time:27009ms step_avg:89.14ms
step:304/1680 train_time:27097ms step_avg:89.14ms
step:305/1680 train_time:27187ms step_avg:89.14ms
step:306/1680 train_time:27276ms step_avg:89.14ms
step:307/1680 train_time:27365ms step_avg:89.14ms
step:308/1680 train_time:27454ms step_avg:89.14ms
step:309/1680 train_time:27544ms step_avg:89.14ms
step:310/1680 train_time:27633ms step_avg:89.14ms
step:311/1680 train_time:27722ms step_avg:89.14ms
step:312/1680 train_time:27811ms step_avg:89.14ms
step:313/1680 train_time:27900ms step_avg:89.14ms
step:314/1680 train_time:27990ms step_avg:89.14ms
step:315/1680 train_time:28078ms step_avg:89.14ms
step:316/1680 train_time:28168ms step_avg:89.14ms
step:317/1680 train_time:28256ms step_avg:89.14ms
step:318/1680 train_time:28346ms step_avg:89.14ms
step:319/1680 train_time:28435ms step_avg:89.14ms
step:320/1680 train_time:28524ms step_avg:89.14ms
step:321/1680 train_time:28614ms step_avg:89.14ms
step:322/1680 train_time:28703ms step_avg:89.14ms
step:323/1680 train_time:28792ms step_avg:89.14ms
step:324/1680 train_time:28880ms step_avg:89.14ms
step:325/1680 train_time:28970ms step_avg:89.14ms
step:326/1680 train_time:29058ms step_avg:89.14ms
step:327/1680 train_time:29147ms step_avg:89.13ms
step:328/1680 train_time:29236ms step_avg:89.13ms
step:329/1680 train_time:29324ms step_avg:89.13ms
step:330/1680 train_time:29413ms step_avg:89.13ms
step:331/1680 train_time:29502ms step_avg:89.13ms
step:332/1680 train_time:29590ms step_avg:89.13ms
step:333/1680 train_time:29679ms step_avg:89.13ms
step:334/1680 train_time:29767ms step_avg:89.12ms
step:335/1680 train_time:29857ms step_avg:89.12ms
step:336/1680 train_time:29946ms step_avg:89.12ms
step:337/1680 train_time:30035ms step_avg:89.12ms
step:338/1680 train_time:30124ms step_avg:89.13ms
step:339/1680 train_time:30213ms step_avg:89.12ms
step:340/1680 train_time:30301ms step_avg:89.12ms
step:341/1680 train_time:30390ms step_avg:89.12ms
step:342/1680 train_time:30479ms step_avg:89.12ms
step:343/1680 train_time:30568ms step_avg:89.12ms
step:344/1680 train_time:30656ms step_avg:89.12ms
step:345/1680 train_time:30746ms step_avg:89.12ms
step:346/1680 train_time:30835ms step_avg:89.12ms
step:347/1680 train_time:30924ms step_avg:89.12ms
step:348/1680 train_time:31014ms step_avg:89.12ms
step:349/1680 train_time:31104ms step_avg:89.12ms
step:350/1680 train_time:31193ms step_avg:89.12ms
step:351/1680 train_time:31281ms step_avg:89.12ms
step:352/1680 train_time:31371ms step_avg:89.12ms
step:353/1680 train_time:31459ms step_avg:89.12ms
step:354/1680 train_time:31548ms step_avg:89.12ms
step:355/1680 train_time:31637ms step_avg:89.12ms
step:356/1680 train_time:31726ms step_avg:89.12ms
step:357/1680 train_time:31816ms step_avg:89.12ms
step:358/1680 train_time:31904ms step_avg:89.12ms
step:359/1680 train_time:31993ms step_avg:89.12ms
step:360/1680 train_time:32082ms step_avg:89.12ms
step:361/1680 train_time:32172ms step_avg:89.12ms
step:362/1680 train_time:32260ms step_avg:89.12ms
step:363/1680 train_time:32350ms step_avg:89.12ms
step:364/1680 train_time:32438ms step_avg:89.12ms
step:365/1680 train_time:32527ms step_avg:89.12ms
step:366/1680 train_time:32616ms step_avg:89.11ms
step:367/1680 train_time:32704ms step_avg:89.11ms
step:368/1680 train_time:32794ms step_avg:89.11ms
step:369/1680 train_time:32883ms step_avg:89.11ms
step:370/1680 train_time:32972ms step_avg:89.11ms
step:371/1680 train_time:33060ms step_avg:89.11ms
step:372/1680 train_time:33149ms step_avg:89.11ms
step:373/1680 train_time:33238ms step_avg:89.11ms
step:374/1680 train_time:33327ms step_avg:89.11ms
step:375/1680 train_time:33418ms step_avg:89.11ms
step:375/1680 val_loss:3.8208 train_time:33508ms step_avg:89.35ms
step:376/1680 train_time:33530ms step_avg:89.18ms
step:377/1680 train_time:33600ms step_avg:89.12ms
step:378/1680 train_time:33697ms step_avg:89.14ms
step:379/1680 train_time:33788ms step_avg:89.15ms
step:380/1680 train_time:33877ms step_avg:89.15ms
step:381/1680 train_time:33965ms step_avg:89.15ms
step:382/1680 train_time:34053ms step_avg:89.14ms
step:383/1680 train_time:34141ms step_avg:89.14ms
step:384/1680 train_time:34229ms step_avg:89.14ms
step:385/1680 train_time:34316ms step_avg:89.13ms
step:386/1680 train_time:34404ms step_avg:89.13ms
step:387/1680 train_time:34493ms step_avg:89.13ms
step:388/1680 train_time:34584ms step_avg:89.13ms
step:389/1680 train_time:34678ms step_avg:89.15ms
step:390/1680 train_time:34769ms step_avg:89.15ms
step:391/1680 train_time:34857ms step_avg:89.15ms
step:392/1680 train_time:34946ms step_avg:89.15ms
step:393/1680 train_time:35034ms step_avg:89.15ms
step:394/1680 train_time:35123ms step_avg:89.14ms
step:395/1680 train_time:35211ms step_avg:89.14ms
step:396/1680 train_time:35300ms step_avg:89.14ms
step:397/1680 train_time:35388ms step_avg:89.14ms
step:398/1680 train_time:35476ms step_avg:89.14ms
step:399/1680 train_time:35566ms step_avg:89.14ms
step:400/1680 train_time:35657ms step_avg:89.14ms
step:401/1680 train_time:35747ms step_avg:89.14ms
step:402/1680 train_time:35837ms step_avg:89.15ms
step:403/1680 train_time:35926ms step_avg:89.15ms
step:404/1680 train_time:36014ms step_avg:89.14ms
step:405/1680 train_time:36103ms step_avg:89.14ms
step:406/1680 train_time:36191ms step_avg:89.14ms
step:407/1680 train_time:36279ms step_avg:89.14ms
step:408/1680 train_time:36367ms step_avg:89.14ms
step:409/1680 train_time:36456ms step_avg:89.13ms
step:410/1680 train_time:36546ms step_avg:89.14ms
step:411/1680 train_time:36636ms step_avg:89.14ms
step:412/1680 train_time:36726ms step_avg:89.14ms
step:413/1680 train_time:36816ms step_avg:89.14ms
step:414/1680 train_time:36907ms step_avg:89.15ms
step:415/1680 train_time:36996ms step_avg:89.15ms
step:416/1680 train_time:37085ms step_avg:89.15ms
step:417/1680 train_time:37174ms step_avg:89.15ms
step:418/1680 train_time:37262ms step_avg:89.14ms
step:419/1680 train_time:37350ms step_avg:89.14ms
step:420/1680 train_time:37440ms step_avg:89.14ms
step:421/1680 train_time:37528ms step_avg:89.14ms
step:422/1680 train_time:37617ms step_avg:89.14ms
step:423/1680 train_time:37706ms step_avg:89.14ms
step:424/1680 train_time:37796ms step_avg:89.14ms
step:425/1680 train_time:37885ms step_avg:89.14ms
step:426/1680 train_time:37973ms step_avg:89.14ms
step:427/1680 train_time:38062ms step_avg:89.14ms
step:428/1680 train_time:38151ms step_avg:89.14ms
step:429/1680 train_time:38240ms step_avg:89.14ms
step:430/1680 train_time:38329ms step_avg:89.14ms
step:431/1680 train_time:38417ms step_avg:89.13ms
step:432/1680 train_time:38506ms step_avg:89.13ms
step:433/1680 train_time:38595ms step_avg:89.13ms
step:434/1680 train_time:38684ms step_avg:89.13ms
step:435/1680 train_time:38774ms step_avg:89.14ms
step:436/1680 train_time:38863ms step_avg:89.14ms
step:437/1680 train_time:38953ms step_avg:89.14ms
step:438/1680 train_time:39041ms step_avg:89.14ms
step:439/1680 train_time:39130ms step_avg:89.14ms
step:440/1680 train_time:39219ms step_avg:89.13ms
step:441/1680 train_time:39308ms step_avg:89.13ms
step:442/1680 train_time:39396ms step_avg:89.13ms
step:443/1680 train_time:39484ms step_avg:89.13ms
step:444/1680 train_time:39574ms step_avg:89.13ms
step:445/1680 train_time:39663ms step_avg:89.13ms
step:446/1680 train_time:39753ms step_avg:89.13ms
step:447/1680 train_time:39842ms step_avg:89.13ms
step:448/1680 train_time:39932ms step_avg:89.13ms
step:449/1680 train_time:40021ms step_avg:89.13ms
step:450/1680 train_time:40110ms step_avg:89.13ms
step:451/1680 train_time:40199ms step_avg:89.13ms
step:452/1680 train_time:40287ms step_avg:89.13ms
step:453/1680 train_time:40376ms step_avg:89.13ms
step:454/1680 train_time:40465ms step_avg:89.13ms
step:455/1680 train_time:40553ms step_avg:89.13ms
step:456/1680 train_time:40644ms step_avg:89.13ms
step:457/1680 train_time:40734ms step_avg:89.13ms
step:458/1680 train_time:40823ms step_avg:89.13ms
step:459/1680 train_time:40912ms step_avg:89.13ms
step:460/1680 train_time:41002ms step_avg:89.13ms
step:461/1680 train_time:41090ms step_avg:89.13ms
step:462/1680 train_time:41179ms step_avg:89.13ms
step:463/1680 train_time:41268ms step_avg:89.13ms
step:464/1680 train_time:41357ms step_avg:89.13ms
step:465/1680 train_time:41446ms step_avg:89.13ms
step:466/1680 train_time:41535ms step_avg:89.13ms
step:467/1680 train_time:41623ms step_avg:89.13ms
step:468/1680 train_time:41712ms step_avg:89.13ms
step:469/1680 train_time:41802ms step_avg:89.13ms
step:470/1680 train_time:41892ms step_avg:89.13ms
step:471/1680 train_time:41981ms step_avg:89.13ms
step:472/1680 train_time:42071ms step_avg:89.13ms
step:473/1680 train_time:42160ms step_avg:89.13ms
step:474/1680 train_time:42248ms step_avg:89.13ms
step:475/1680 train_time:42337ms step_avg:89.13ms
step:476/1680 train_time:42426ms step_avg:89.13ms
step:477/1680 train_time:42514ms step_avg:89.13ms
step:478/1680 train_time:42603ms step_avg:89.13ms
step:479/1680 train_time:42691ms step_avg:89.13ms
step:480/1680 train_time:42780ms step_avg:89.13ms
step:481/1680 train_time:42869ms step_avg:89.12ms
step:482/1680 train_time:42958ms step_avg:89.12ms
step:483/1680 train_time:43047ms step_avg:89.12ms
step:484/1680 train_time:43136ms step_avg:89.12ms
step:485/1680 train_time:43224ms step_avg:89.12ms
step:486/1680 train_time:43314ms step_avg:89.12ms
step:487/1680 train_time:43403ms step_avg:89.12ms
step:488/1680 train_time:43491ms step_avg:89.12ms
step:489/1680 train_time:43580ms step_avg:89.12ms
step:490/1680 train_time:43668ms step_avg:89.12ms
step:491/1680 train_time:43757ms step_avg:89.12ms
step:492/1680 train_time:43846ms step_avg:89.12ms
step:493/1680 train_time:43935ms step_avg:89.12ms
step:494/1680 train_time:44024ms step_avg:89.12ms
step:495/1680 train_time:44114ms step_avg:89.12ms
step:496/1680 train_time:44203ms step_avg:89.12ms
step:497/1680 train_time:44294ms step_avg:89.12ms
step:498/1680 train_time:44383ms step_avg:89.12ms
step:499/1680 train_time:44473ms step_avg:89.12ms
step:500/1680 train_time:44562ms step_avg:89.12ms
step:500/1680 val_loss:3.7171 train_time:44653ms step_avg:89.31ms
step:501/1680 train_time:44675ms step_avg:89.17ms
step:502/1680 train_time:44746ms step_avg:89.13ms
step:503/1680 train_time:44841ms step_avg:89.15ms
step:504/1680 train_time:44931ms step_avg:89.15ms
step:505/1680 train_time:45019ms step_avg:89.15ms
step:506/1680 train_time:45107ms step_avg:89.15ms
step:507/1680 train_time:45195ms step_avg:89.14ms
step:508/1680 train_time:45283ms step_avg:89.14ms
step:509/1680 train_time:45371ms step_avg:89.14ms
step:510/1680 train_time:45459ms step_avg:89.13ms
step:511/1680 train_time:45547ms step_avg:89.13ms
step:512/1680 train_time:45637ms step_avg:89.13ms
step:513/1680 train_time:45728ms step_avg:89.14ms
step:514/1680 train_time:45818ms step_avg:89.14ms
step:515/1680 train_time:45908ms step_avg:89.14ms
step:516/1680 train_time:45997ms step_avg:89.14ms
step:517/1680 train_time:46086ms step_avg:89.14ms
step:518/1680 train_time:46175ms step_avg:89.14ms
step:519/1680 train_time:46263ms step_avg:89.14ms
step:520/1680 train_time:46352ms step_avg:89.14ms
step:521/1680 train_time:46440ms step_avg:89.14ms
step:522/1680 train_time:46528ms step_avg:89.13ms
step:523/1680 train_time:46616ms step_avg:89.13ms
step:524/1680 train_time:46706ms step_avg:89.13ms
step:525/1680 train_time:46797ms step_avg:89.14ms
step:526/1680 train_time:46887ms step_avg:89.14ms
step:527/1680 train_time:46976ms step_avg:89.14ms
step:528/1680 train_time:47066ms step_avg:89.14ms
step:529/1680 train_time:47155ms step_avg:89.14ms
step:530/1680 train_time:47244ms step_avg:89.14ms
step:531/1680 train_time:47333ms step_avg:89.14ms
step:532/1680 train_time:47421ms step_avg:89.14ms
step:533/1680 train_time:47510ms step_avg:89.14ms
step:534/1680 train_time:47599ms step_avg:89.14ms
step:535/1680 train_time:47688ms step_avg:89.14ms
step:536/1680 train_time:47778ms step_avg:89.14ms
step:537/1680 train_time:47868ms step_avg:89.14ms
step:538/1680 train_time:47957ms step_avg:89.14ms
step:539/1680 train_time:48047ms step_avg:89.14ms
step:540/1680 train_time:48136ms step_avg:89.14ms
step:541/1680 train_time:48225ms step_avg:89.14ms
step:542/1680 train_time:48314ms step_avg:89.14ms
step:543/1680 train_time:48402ms step_avg:89.14ms
step:544/1680 train_time:48491ms step_avg:89.14ms
step:545/1680 train_time:48580ms step_avg:89.14ms
step:546/1680 train_time:48668ms step_avg:89.14ms
step:547/1680 train_time:48758ms step_avg:89.14ms
step:548/1680 train_time:48847ms step_avg:89.14ms
step:549/1680 train_time:48938ms step_avg:89.14ms
step:550/1680 train_time:49029ms step_avg:89.14ms
step:551/1680 train_time:49119ms step_avg:89.14ms
step:552/1680 train_time:49209ms step_avg:89.15ms
step:553/1680 train_time:49299ms step_avg:89.15ms
step:554/1680 train_time:49389ms step_avg:89.15ms
step:555/1680 train_time:49478ms step_avg:89.15ms
step:556/1680 train_time:49569ms step_avg:89.15ms
step:557/1680 train_time:49659ms step_avg:89.15ms
step:558/1680 train_time:49748ms step_avg:89.15ms
step:559/1680 train_time:49839ms step_avg:89.16ms
step:560/1680 train_time:49931ms step_avg:89.16ms
step:561/1680 train_time:50020ms step_avg:89.16ms
step:562/1680 train_time:50110ms step_avg:89.16ms
step:563/1680 train_time:50200ms step_avg:89.17ms
step:564/1680 train_time:50292ms step_avg:89.17ms
step:565/1680 train_time:50381ms step_avg:89.17ms
step:566/1680 train_time:50472ms step_avg:89.17ms
step:567/1680 train_time:50562ms step_avg:89.18ms
step:568/1680 train_time:50653ms step_avg:89.18ms
step:569/1680 train_time:50742ms step_avg:89.18ms
step:570/1680 train_time:50834ms step_avg:89.18ms
step:571/1680 train_time:50923ms step_avg:89.18ms
step:572/1680 train_time:51014ms step_avg:89.19ms
step:573/1680 train_time:51104ms step_avg:89.19ms
step:574/1680 train_time:51196ms step_avg:89.19ms
step:575/1680 train_time:51286ms step_avg:89.19ms
step:576/1680 train_time:51376ms step_avg:89.20ms
step:577/1680 train_time:51466ms step_avg:89.20ms
step:578/1680 train_time:51556ms step_avg:89.20ms
step:579/1680 train_time:51646ms step_avg:89.20ms
step:580/1680 train_time:51738ms step_avg:89.20ms
step:581/1680 train_time:51829ms step_avg:89.21ms
step:582/1680 train_time:51919ms step_avg:89.21ms
step:583/1680 train_time:52009ms step_avg:89.21ms
step:584/1680 train_time:52099ms step_avg:89.21ms
step:585/1680 train_time:52189ms step_avg:89.21ms
step:586/1680 train_time:52279ms step_avg:89.21ms
step:587/1680 train_time:52369ms step_avg:89.21ms
step:588/1680 train_time:52459ms step_avg:89.22ms
step:589/1680 train_time:52549ms step_avg:89.22ms
step:590/1680 train_time:52639ms step_avg:89.22ms
step:591/1680 train_time:52730ms step_avg:89.22ms
step:592/1680 train_time:52820ms step_avg:89.22ms
step:593/1680 train_time:52910ms step_avg:89.22ms
step:594/1680 train_time:53000ms step_avg:89.23ms
step:595/1680 train_time:53090ms step_avg:89.23ms
step:596/1680 train_time:53180ms step_avg:89.23ms
step:597/1680 train_time:53270ms step_avg:89.23ms
step:598/1680 train_time:53360ms step_avg:89.23ms
step:599/1680 train_time:53450ms step_avg:89.23ms
step:600/1680 train_time:53540ms step_avg:89.23ms
step:601/1680 train_time:53631ms step_avg:89.24ms
step:602/1680 train_time:53721ms step_avg:89.24ms
step:603/1680 train_time:53811ms step_avg:89.24ms
step:604/1680 train_time:53901ms step_avg:89.24ms
step:605/1680 train_time:53991ms step_avg:89.24ms
step:606/1680 train_time:54082ms step_avg:89.24ms
step:607/1680 train_time:54171ms step_avg:89.24ms
step:608/1680 train_time:54262ms step_avg:89.25ms
step:609/1680 train_time:54351ms step_avg:89.25ms
step:610/1680 train_time:54441ms step_avg:89.25ms
step:611/1680 train_time:54531ms step_avg:89.25ms
step:612/1680 train_time:54621ms step_avg:89.25ms
step:613/1680 train_time:54712ms step_avg:89.25ms
step:614/1680 train_time:54803ms step_avg:89.26ms
step:615/1680 train_time:54893ms step_avg:89.26ms
step:616/1680 train_time:54983ms step_avg:89.26ms
step:617/1680 train_time:55073ms step_avg:89.26ms
step:618/1680 train_time:55163ms step_avg:89.26ms
step:619/1680 train_time:55254ms step_avg:89.26ms
step:620/1680 train_time:55344ms step_avg:89.26ms
step:621/1680 train_time:55434ms step_avg:89.27ms
step:622/1680 train_time:55523ms step_avg:89.27ms
step:623/1680 train_time:55613ms step_avg:89.27ms
step:624/1680 train_time:55704ms step_avg:89.27ms
step:625/1680 train_time:55795ms step_avg:89.27ms
step:625/1680 val_loss:3.6153 train_time:55887ms step_avg:89.42ms
step:626/1680 train_time:55910ms step_avg:89.31ms
step:627/1680 train_time:55980ms step_avg:89.28ms
step:628/1680 train_time:56079ms step_avg:89.30ms
step:629/1680 train_time:56171ms step_avg:89.30ms
step:630/1680 train_time:56260ms step_avg:89.30ms
step:631/1680 train_time:56348ms step_avg:89.30ms
step:632/1680 train_time:56437ms step_avg:89.30ms
step:633/1680 train_time:56526ms step_avg:89.30ms
step:634/1680 train_time:56615ms step_avg:89.30ms
step:635/1680 train_time:56704ms step_avg:89.30ms
step:636/1680 train_time:56793ms step_avg:89.30ms
step:637/1680 train_time:56884ms step_avg:89.30ms
step:638/1680 train_time:56977ms step_avg:89.31ms
step:639/1680 train_time:57071ms step_avg:89.31ms
step:640/1680 train_time:57163ms step_avg:89.32ms
step:641/1680 train_time:57252ms step_avg:89.32ms
step:642/1680 train_time:57341ms step_avg:89.32ms
step:643/1680 train_time:57430ms step_avg:89.32ms
step:644/1680 train_time:57520ms step_avg:89.32ms
step:645/1680 train_time:57609ms step_avg:89.32ms
step:646/1680 train_time:57699ms step_avg:89.32ms
step:647/1680 train_time:57788ms step_avg:89.32ms
step:648/1680 train_time:57879ms step_avg:89.32ms
step:649/1680 train_time:57970ms step_avg:89.32ms
step:650/1680 train_time:58061ms step_avg:89.33ms
step:651/1680 train_time:58153ms step_avg:89.33ms
step:652/1680 train_time:58244ms step_avg:89.33ms
step:653/1680 train_time:58334ms step_avg:89.33ms
step:654/1680 train_time:58424ms step_avg:89.33ms
step:655/1680 train_time:58513ms step_avg:89.33ms
step:656/1680 train_time:58603ms step_avg:89.33ms
step:657/1680 train_time:58692ms step_avg:89.33ms
step:658/1680 train_time:58782ms step_avg:89.33ms
step:659/1680 train_time:58872ms step_avg:89.34ms
step:660/1680 train_time:58963ms step_avg:89.34ms
step:661/1680 train_time:59054ms step_avg:89.34ms
step:662/1680 train_time:59145ms step_avg:89.34ms
step:663/1680 train_time:59236ms step_avg:89.34ms
step:664/1680 train_time:59326ms step_avg:89.35ms
step:665/1680 train_time:59416ms step_avg:89.35ms
step:666/1680 train_time:59507ms step_avg:89.35ms
step:667/1680 train_time:59597ms step_avg:89.35ms
step:668/1680 train_time:59687ms step_avg:89.35ms
step:669/1680 train_time:59777ms step_avg:89.35ms
step:670/1680 train_time:59867ms step_avg:89.35ms
step:671/1680 train_time:59957ms step_avg:89.36ms
step:672/1680 train_time:60048ms step_avg:89.36ms
step:673/1680 train_time:60139ms step_avg:89.36ms
step:674/1680 train_time:60229ms step_avg:89.36ms
step:675/1680 train_time:60320ms step_avg:89.36ms
step:676/1680 train_time:60409ms step_avg:89.36ms
step:677/1680 train_time:60500ms step_avg:89.37ms
step:678/1680 train_time:60590ms step_avg:89.37ms
step:679/1680 train_time:60680ms step_avg:89.37ms
step:680/1680 train_time:60770ms step_avg:89.37ms
step:681/1680 train_time:60860ms step_avg:89.37ms
step:682/1680 train_time:60950ms step_avg:89.37ms
step:683/1680 train_time:61041ms step_avg:89.37ms
step:684/1680 train_time:61131ms step_avg:89.37ms
step:685/1680 train_time:61223ms step_avg:89.38ms
step:686/1680 train_time:61313ms step_avg:89.38ms
step:687/1680 train_time:61403ms step_avg:89.38ms
step:688/1680 train_time:61492ms step_avg:89.38ms
step:689/1680 train_time:61581ms step_avg:89.38ms
step:690/1680 train_time:61672ms step_avg:89.38ms
step:691/1680 train_time:61762ms step_avg:89.38ms
step:692/1680 train_time:61852ms step_avg:89.38ms
step:693/1680 train_time:61942ms step_avg:89.38ms
step:694/1680 train_time:62033ms step_avg:89.39ms
step:695/1680 train_time:62125ms step_avg:89.39ms
step:696/1680 train_time:62215ms step_avg:89.39ms
step:697/1680 train_time:62305ms step_avg:89.39ms
step:698/1680 train_time:62396ms step_avg:89.39ms
step:699/1680 train_time:62487ms step_avg:89.39ms
step:700/1680 train_time:62576ms step_avg:89.39ms
step:701/1680 train_time:62666ms step_avg:89.40ms
step:702/1680 train_time:62757ms step_avg:89.40ms
step:703/1680 train_time:62847ms step_avg:89.40ms
step:704/1680 train_time:62937ms step_avg:89.40ms
step:705/1680 train_time:63028ms step_avg:89.40ms
step:706/1680 train_time:63118ms step_avg:89.40ms
step:707/1680 train_time:63208ms step_avg:89.40ms
step:708/1680 train_time:63299ms step_avg:89.41ms
step:709/1680 train_time:63389ms step_avg:89.41ms
step:710/1680 train_time:63480ms step_avg:89.41ms
step:711/1680 train_time:63569ms step_avg:89.41ms
step:712/1680 train_time:63660ms step_avg:89.41ms
step:713/1680 train_time:63750ms step_avg:89.41ms
step:714/1680 train_time:63839ms step_avg:89.41ms
step:715/1680 train_time:63930ms step_avg:89.41ms
step:716/1680 train_time:64020ms step_avg:89.41ms
step:717/1680 train_time:64111ms step_avg:89.42ms
step:718/1680 train_time:64201ms step_avg:89.42ms
step:719/1680 train_time:64292ms step_avg:89.42ms
step:720/1680 train_time:64383ms step_avg:89.42ms
step:721/1680 train_time:64473ms step_avg:89.42ms
step:722/1680 train_time:64564ms step_avg:89.42ms
step:723/1680 train_time:64653ms step_avg:89.42ms
step:724/1680 train_time:64743ms step_avg:89.42ms
step:725/1680 train_time:64833ms step_avg:89.43ms
step:726/1680 train_time:64923ms step_avg:89.42ms
step:727/1680 train_time:65012ms step_avg:89.43ms
step:728/1680 train_time:65103ms step_avg:89.43ms
step:729/1680 train_time:65193ms step_avg:89.43ms
step:730/1680 train_time:65285ms step_avg:89.43ms
step:731/1680 train_time:65375ms step_avg:89.43ms
step:732/1680 train_time:65465ms step_avg:89.43ms
step:733/1680 train_time:65556ms step_avg:89.43ms
step:734/1680 train_time:65646ms step_avg:89.44ms
step:735/1680 train_time:65736ms step_avg:89.44ms
step:736/1680 train_time:65827ms step_avg:89.44ms
step:737/1680 train_time:65917ms step_avg:89.44ms
step:738/1680 train_time:66007ms step_avg:89.44ms
step:739/1680 train_time:66098ms step_avg:89.44ms
step:740/1680 train_time:66189ms step_avg:89.44ms
step:741/1680 train_time:66280ms step_avg:89.45ms
step:742/1680 train_time:66369ms step_avg:89.45ms
step:743/1680 train_time:66460ms step_avg:89.45ms
step:744/1680 train_time:66549ms step_avg:89.45ms
step:745/1680 train_time:66640ms step_avg:89.45ms
step:746/1680 train_time:66730ms step_avg:89.45ms
step:747/1680 train_time:66821ms step_avg:89.45ms
step:748/1680 train_time:66912ms step_avg:89.45ms
step:749/1680 train_time:67001ms step_avg:89.45ms
step:750/1680 train_time:67092ms step_avg:89.46ms
step:750/1680 val_loss:3.5667 train_time:67184ms step_avg:89.58ms
step:751/1680 train_time:67208ms step_avg:89.49ms
step:752/1680 train_time:67279ms step_avg:89.47ms
step:753/1680 train_time:67377ms step_avg:89.48ms
step:754/1680 train_time:67470ms step_avg:89.48ms
step:755/1680 train_time:67560ms step_avg:89.48ms
step:756/1680 train_time:67649ms step_avg:89.48ms
step:757/1680 train_time:67739ms step_avg:89.48ms
step:758/1680 train_time:67828ms step_avg:89.48ms
step:759/1680 train_time:67917ms step_avg:89.48ms
step:760/1680 train_time:68006ms step_avg:89.48ms
step:761/1680 train_time:68095ms step_avg:89.48ms
step:762/1680 train_time:68184ms step_avg:89.48ms
step:763/1680 train_time:68277ms step_avg:89.48ms
step:764/1680 train_time:68370ms step_avg:89.49ms
step:765/1680 train_time:68462ms step_avg:89.49ms
step:766/1680 train_time:68554ms step_avg:89.50ms
step:767/1680 train_time:68644ms step_avg:89.50ms
step:768/1680 train_time:68734ms step_avg:89.50ms
step:769/1680 train_time:68824ms step_avg:89.50ms
step:770/1680 train_time:68914ms step_avg:89.50ms
step:771/1680 train_time:69003ms step_avg:89.50ms
step:772/1680 train_time:69092ms step_avg:89.50ms
step:773/1680 train_time:69182ms step_avg:89.50ms
step:774/1680 train_time:69274ms step_avg:89.50ms
step:775/1680 train_time:69366ms step_avg:89.50ms
step:776/1680 train_time:69458ms step_avg:89.51ms
step:777/1680 train_time:69549ms step_avg:89.51ms
step:778/1680 train_time:69639ms step_avg:89.51ms
step:779/1680 train_time:69729ms step_avg:89.51ms
step:780/1680 train_time:69819ms step_avg:89.51ms
step:781/1680 train_time:69910ms step_avg:89.51ms
step:782/1680 train_time:69999ms step_avg:89.51ms
step:783/1680 train_time:70088ms step_avg:89.51ms
step:784/1680 train_time:70177ms step_avg:89.51ms
step:785/1680 train_time:70268ms step_avg:89.51ms
step:786/1680 train_time:70358ms step_avg:89.51ms
step:787/1680 train_time:70449ms step_avg:89.52ms
step:788/1680 train_time:70539ms step_avg:89.52ms
step:789/1680 train_time:70629ms step_avg:89.52ms
step:790/1680 train_time:70719ms step_avg:89.52ms
step:791/1680 train_time:70809ms step_avg:89.52ms
step:792/1680 train_time:70898ms step_avg:89.52ms
step:793/1680 train_time:70988ms step_avg:89.52ms
step:794/1680 train_time:71078ms step_avg:89.52ms
step:795/1680 train_time:71168ms step_avg:89.52ms
step:796/1680 train_time:71257ms step_avg:89.52ms
step:797/1680 train_time:71348ms step_avg:89.52ms
step:798/1680 train_time:71438ms step_avg:89.52ms
step:799/1680 train_time:71529ms step_avg:89.52ms
step:800/1680 train_time:71619ms step_avg:89.52ms
step:801/1680 train_time:71709ms step_avg:89.52ms
step:802/1680 train_time:71799ms step_avg:89.53ms
step:803/1680 train_time:71889ms step_avg:89.53ms
step:804/1680 train_time:71978ms step_avg:89.52ms
step:805/1680 train_time:72068ms step_avg:89.53ms
step:806/1680 train_time:72158ms step_avg:89.53ms
step:807/1680 train_time:72248ms step_avg:89.53ms
step:808/1680 train_time:72338ms step_avg:89.53ms
step:809/1680 train_time:72428ms step_avg:89.53ms
step:810/1680 train_time:72518ms step_avg:89.53ms
step:811/1680 train_time:72608ms step_avg:89.53ms
step:812/1680 train_time:72698ms step_avg:89.53ms
step:813/1680 train_time:72788ms step_avg:89.53ms
step:814/1680 train_time:72878ms step_avg:89.53ms
step:815/1680 train_time:72969ms step_avg:89.53ms
step:816/1680 train_time:73059ms step_avg:89.53ms
step:817/1680 train_time:73149ms step_avg:89.53ms
step:818/1680 train_time:73239ms step_avg:89.53ms
step:819/1680 train_time:73330ms step_avg:89.54ms
step:820/1680 train_time:73419ms step_avg:89.54ms
step:821/1680 train_time:73512ms step_avg:89.54ms
step:822/1680 train_time:73602ms step_avg:89.54ms
step:823/1680 train_time:73692ms step_avg:89.54ms
step:824/1680 train_time:73782ms step_avg:89.54ms
step:825/1680 train_time:73873ms step_avg:89.54ms
step:826/1680 train_time:73963ms step_avg:89.54ms
step:827/1680 train_time:74054ms step_avg:89.55ms
step:828/1680 train_time:74144ms step_avg:89.55ms
step:829/1680 train_time:74235ms step_avg:89.55ms
step:830/1680 train_time:74325ms step_avg:89.55ms
step:831/1680 train_time:74415ms step_avg:89.55ms
step:832/1680 train_time:74505ms step_avg:89.55ms
step:833/1680 train_time:74595ms step_avg:89.55ms
step:834/1680 train_time:74686ms step_avg:89.55ms
step:835/1680 train_time:74776ms step_avg:89.55ms
step:836/1680 train_time:74867ms step_avg:89.55ms
step:837/1680 train_time:74957ms step_avg:89.55ms
step:838/1680 train_time:75047ms step_avg:89.55ms
step:839/1680 train_time:75137ms step_avg:89.56ms
step:840/1680 train_time:75227ms step_avg:89.56ms
step:841/1680 train_time:75317ms step_avg:89.56ms
step:842/1680 train_time:75407ms step_avg:89.56ms
step:843/1680 train_time:75497ms step_avg:89.56ms
step:844/1680 train_time:75587ms step_avg:89.56ms
step:845/1680 train_time:75677ms step_avg:89.56ms
step:846/1680 train_time:75768ms step_avg:89.56ms
step:847/1680 train_time:75859ms step_avg:89.56ms
step:848/1680 train_time:75950ms step_avg:89.56ms
step:849/1680 train_time:76040ms step_avg:89.56ms
step:850/1680 train_time:76131ms step_avg:89.57ms
step:851/1680 train_time:76221ms step_avg:89.57ms
step:852/1680 train_time:76311ms step_avg:89.57ms
step:853/1680 train_time:76401ms step_avg:89.57ms
step:854/1680 train_time:76490ms step_avg:89.57ms
step:855/1680 train_time:76581ms step_avg:89.57ms
step:856/1680 train_time:76670ms step_avg:89.57ms
step:857/1680 train_time:76761ms step_avg:89.57ms
step:858/1680 train_time:76851ms step_avg:89.57ms
step:859/1680 train_time:76941ms step_avg:89.57ms
step:860/1680 train_time:77033ms step_avg:89.57ms
step:861/1680 train_time:77123ms step_avg:89.57ms
step:862/1680 train_time:77213ms step_avg:89.57ms
step:863/1680 train_time:77303ms step_avg:89.57ms
step:864/1680 train_time:77393ms step_avg:89.58ms
step:865/1680 train_time:77484ms step_avg:89.58ms
step:866/1680 train_time:77574ms step_avg:89.58ms
step:867/1680 train_time:77664ms step_avg:89.58ms
step:868/1680 train_time:77755ms step_avg:89.58ms
step:869/1680 train_time:77845ms step_avg:89.58ms
step:870/1680 train_time:77936ms step_avg:89.58ms
step:871/1680 train_time:78026ms step_avg:89.58ms
step:872/1680 train_time:78116ms step_avg:89.58ms
step:873/1680 train_time:78207ms step_avg:89.58ms
step:874/1680 train_time:78297ms step_avg:89.58ms
step:875/1680 train_time:78387ms step_avg:89.58ms
step:875/1680 val_loss:3.5187 train_time:78479ms step_avg:89.69ms
step:876/1680 train_time:78501ms step_avg:89.61ms
step:877/1680 train_time:78572ms step_avg:89.59ms
step:878/1680 train_time:78670ms step_avg:89.60ms
step:879/1680 train_time:78761ms step_avg:89.60ms
step:880/1680 train_time:78850ms step_avg:89.60ms
step:881/1680 train_time:78939ms step_avg:89.60ms
step:882/1680 train_time:79028ms step_avg:89.60ms
step:883/1680 train_time:79118ms step_avg:89.60ms
step:884/1680 train_time:79207ms step_avg:89.60ms
step:885/1680 train_time:79296ms step_avg:89.60ms
step:886/1680 train_time:79386ms step_avg:89.60ms
step:887/1680 train_time:79477ms step_avg:89.60ms
step:888/1680 train_time:79569ms step_avg:89.60ms
step:889/1680 train_time:79662ms step_avg:89.61ms
step:890/1680 train_time:79753ms step_avg:89.61ms
step:891/1680 train_time:79843ms step_avg:89.61ms
step:892/1680 train_time:79933ms step_avg:89.61ms
step:893/1680 train_time:80023ms step_avg:89.61ms
step:894/1680 train_time:80115ms step_avg:89.61ms
step:895/1680 train_time:80204ms step_avg:89.61ms
step:896/1680 train_time:80293ms step_avg:89.61ms
step:897/1680 train_time:80383ms step_avg:89.61ms
step:898/1680 train_time:80473ms step_avg:89.61ms
step:899/1680 train_time:80564ms step_avg:89.62ms
step:900/1680 train_time:80655ms step_avg:89.62ms
step:901/1680 train_time:80746ms step_avg:89.62ms
step:902/1680 train_time:80837ms step_avg:89.62ms
step:903/1680 train_time:80927ms step_avg:89.62ms
step:904/1680 train_time:81017ms step_avg:89.62ms
step:905/1680 train_time:81108ms step_avg:89.62ms
step:906/1680 train_time:81197ms step_avg:89.62ms
step:907/1680 train_time:81287ms step_avg:89.62ms
step:908/1680 train_time:81378ms step_avg:89.62ms
step:909/1680 train_time:81468ms step_avg:89.62ms
step:910/1680 train_time:81559ms step_avg:89.62ms
step:911/1680 train_time:81650ms step_avg:89.63ms
step:912/1680 train_time:81741ms step_avg:89.63ms
step:913/1680 train_time:81831ms step_avg:89.63ms
step:914/1680 train_time:81921ms step_avg:89.63ms
step:915/1680 train_time:82012ms step_avg:89.63ms
step:916/1680 train_time:82102ms step_avg:89.63ms
step:917/1680 train_time:82192ms step_avg:89.63ms
step:918/1680 train_time:82281ms step_avg:89.63ms
step:919/1680 train_time:82371ms step_avg:89.63ms
step:920/1680 train_time:82461ms step_avg:89.63ms
step:921/1680 train_time:82551ms step_avg:89.63ms
step:922/1680 train_time:82641ms step_avg:89.63ms
step:923/1680 train_time:82731ms step_avg:89.63ms
step:924/1680 train_time:82822ms step_avg:89.63ms
step:925/1680 train_time:82911ms step_avg:89.63ms
step:926/1680 train_time:83002ms step_avg:89.63ms
step:927/1680 train_time:83092ms step_avg:89.64ms
step:928/1680 train_time:83183ms step_avg:89.64ms
step:929/1680 train_time:83274ms step_avg:89.64ms
step:930/1680 train_time:83364ms step_avg:89.64ms
step:931/1680 train_time:83454ms step_avg:89.64ms
step:932/1680 train_time:83544ms step_avg:89.64ms
step:933/1680 train_time:83634ms step_avg:89.64ms
step:934/1680 train_time:83724ms step_avg:89.64ms
step:935/1680 train_time:83816ms step_avg:89.64ms
step:936/1680 train_time:83905ms step_avg:89.64ms
step:937/1680 train_time:83996ms step_avg:89.64ms
step:938/1680 train_time:84086ms step_avg:89.64ms
step:939/1680 train_time:84176ms step_avg:89.64ms
step:940/1680 train_time:84266ms step_avg:89.64ms
step:941/1680 train_time:84355ms step_avg:89.64ms
step:942/1680 train_time:84445ms step_avg:89.64ms
step:943/1680 train_time:84536ms step_avg:89.65ms
step:944/1680 train_time:84626ms step_avg:89.65ms
step:945/1680 train_time:84717ms step_avg:89.65ms
step:946/1680 train_time:84807ms step_avg:89.65ms
step:947/1680 train_time:84898ms step_avg:89.65ms
step:948/1680 train_time:84989ms step_avg:89.65ms
step:949/1680 train_time:85080ms step_avg:89.65ms
step:950/1680 train_time:85170ms step_avg:89.65ms
step:951/1680 train_time:85260ms step_avg:89.65ms
step:952/1680 train_time:85350ms step_avg:89.65ms
step:953/1680 train_time:85440ms step_avg:89.65ms
step:954/1680 train_time:85529ms step_avg:89.65ms
step:955/1680 train_time:85620ms step_avg:89.65ms
step:956/1680 train_time:85710ms step_avg:89.65ms
step:957/1680 train_time:85799ms step_avg:89.65ms
step:958/1680 train_time:85890ms step_avg:89.66ms
step:959/1680 train_time:85980ms step_avg:89.66ms
step:960/1680 train_time:86070ms step_avg:89.66ms
step:961/1680 train_time:86160ms step_avg:89.66ms
step:962/1680 train_time:86250ms step_avg:89.66ms
step:963/1680 train_time:86340ms step_avg:89.66ms
step:964/1680 train_time:86430ms step_avg:89.66ms
step:965/1680 train_time:86520ms step_avg:89.66ms
step:966/1680 train_time:86610ms step_avg:89.66ms
step:967/1680 train_time:86700ms step_avg:89.66ms
step:968/1680 train_time:86791ms step_avg:89.66ms
step:969/1680 train_time:86881ms step_avg:89.66ms
step:970/1680 train_time:86970ms step_avg:89.66ms
step:971/1680 train_time:87060ms step_avg:89.66ms
step:972/1680 train_time:87150ms step_avg:89.66ms
step:973/1680 train_time:87240ms step_avg:89.66ms
step:974/1680 train_time:87330ms step_avg:89.66ms
step:975/1680 train_time:87420ms step_avg:89.66ms
step:976/1680 train_time:87509ms step_avg:89.66ms
step:977/1680 train_time:87599ms step_avg:89.66ms
step:978/1680 train_time:87689ms step_avg:89.66ms
step:979/1680 train_time:87779ms step_avg:89.66ms
step:980/1680 train_time:87869ms step_avg:89.66ms
step:981/1680 train_time:87959ms step_avg:89.66ms
step:982/1680 train_time:88050ms step_avg:89.66ms
step:983/1680 train_time:88140ms step_avg:89.66ms
step:984/1680 train_time:88231ms step_avg:89.67ms
step:985/1680 train_time:88322ms step_avg:89.67ms
step:986/1680 train_time:88411ms step_avg:89.67ms
step:987/1680 train_time:88500ms step_avg:89.67ms
step:988/1680 train_time:88591ms step_avg:89.67ms
step:989/1680 train_time:88682ms step_avg:89.67ms
step:990/1680 train_time:88773ms step_avg:89.67ms
step:991/1680 train_time:88863ms step_avg:89.67ms
step:992/1680 train_time:88954ms step_avg:89.67ms
step:993/1680 train_time:89044ms step_avg:89.67ms
step:994/1680 train_time:89135ms step_avg:89.67ms
step:995/1680 train_time:89225ms step_avg:89.67ms
step:996/1680 train_time:89316ms step_avg:89.68ms
step:997/1680 train_time:89407ms step_avg:89.68ms
step:998/1680 train_time:89497ms step_avg:89.68ms
step:999/1680 train_time:89588ms step_avg:89.68ms
step:1000/1680 train_time:89678ms step_avg:89.68ms
step:1000/1680 val_loss:3.4681 train_time:89770ms step_avg:89.77ms
step:1001/1680 train_time:89793ms step_avg:89.70ms
step:1002/1680 train_time:89865ms step_avg:89.69ms
step:1003/1680 train_time:89962ms step_avg:89.69ms
step:1004/1680 train_time:90054ms step_avg:89.70ms
step:1005/1680 train_time:90143ms step_avg:89.69ms
step:1006/1680 train_time:90233ms step_avg:89.69ms
step:1007/1680 train_time:90322ms step_avg:89.69ms
step:1008/1680 train_time:90411ms step_avg:89.69ms
step:1009/1680 train_time:90499ms step_avg:89.69ms
step:1010/1680 train_time:90588ms step_avg:89.69ms
step:1011/1680 train_time:90678ms step_avg:89.69ms
step:1012/1680 train_time:90767ms step_avg:89.69ms
step:1013/1680 train_time:90860ms step_avg:89.69ms
step:1014/1680 train_time:90954ms step_avg:89.70ms
step:1015/1680 train_time:91045ms step_avg:89.70ms
step:1016/1680 train_time:91136ms step_avg:89.70ms
step:1017/1680 train_time:91226ms step_avg:89.70ms
step:1018/1680 train_time:91315ms step_avg:89.70ms
step:1019/1680 train_time:91405ms step_avg:89.70ms
step:1020/1680 train_time:91494ms step_avg:89.70ms
step:1021/1680 train_time:91583ms step_avg:89.70ms
step:1022/1680 train_time:91673ms step_avg:89.70ms
step:1023/1680 train_time:91763ms step_avg:89.70ms
step:1024/1680 train_time:91855ms step_avg:89.70ms
step:1025/1680 train_time:91947ms step_avg:89.70ms
step:1026/1680 train_time:92037ms step_avg:89.70ms
step:1027/1680 train_time:92128ms step_avg:89.71ms
step:1028/1680 train_time:92219ms step_avg:89.71ms
step:1029/1680 train_time:92309ms step_avg:89.71ms
step:1030/1680 train_time:92399ms step_avg:89.71ms
step:1031/1680 train_time:92489ms step_avg:89.71ms
step:1032/1680 train_time:92578ms step_avg:89.71ms
step:1033/1680 train_time:92668ms step_avg:89.71ms
step:1034/1680 train_time:92759ms step_avg:89.71ms
step:1035/1680 train_time:92850ms step_avg:89.71ms
step:1036/1680 train_time:92942ms step_avg:89.71ms
step:1037/1680 train_time:93034ms step_avg:89.71ms
step:1038/1680 train_time:93125ms step_avg:89.72ms
step:1039/1680 train_time:93215ms step_avg:89.72ms
step:1040/1680 train_time:93304ms step_avg:89.72ms
step:1041/1680 train_time:93394ms step_avg:89.72ms
step:1042/1680 train_time:93483ms step_avg:89.72ms
step:1043/1680 train_time:93573ms step_avg:89.71ms
step:1044/1680 train_time:93663ms step_avg:89.72ms
step:1045/1680 train_time:93753ms step_avg:89.72ms
step:1046/1680 train_time:93844ms step_avg:89.72ms
step:1047/1680 train_time:93935ms step_avg:89.72ms
step:1048/1680 train_time:94025ms step_avg:89.72ms
step:1049/1680 train_time:94117ms step_avg:89.72ms
step:1050/1680 train_time:94207ms step_avg:89.72ms
step:1051/1680 train_time:94298ms step_avg:89.72ms
step:1052/1680 train_time:94388ms step_avg:89.72ms
step:1053/1680 train_time:94477ms step_avg:89.72ms
step:1054/1680 train_time:94567ms step_avg:89.72ms
step:1055/1680 train_time:94656ms step_avg:89.72ms
step:1056/1680 train_time:94746ms step_avg:89.72ms
step:1057/1680 train_time:94837ms step_avg:89.72ms
step:1058/1680 train_time:94927ms step_avg:89.72ms
step:1059/1680 train_time:95019ms step_avg:89.73ms
step:1060/1680 train_time:95109ms step_avg:89.73ms
step:1061/1680 train_time:95200ms step_avg:89.73ms
step:1062/1680 train_time:95290ms step_avg:89.73ms
step:1063/1680 train_time:95380ms step_avg:89.73ms
step:1064/1680 train_time:95469ms step_avg:89.73ms
step:1065/1680 train_time:95559ms step_avg:89.73ms
step:1066/1680 train_time:95649ms step_avg:89.73ms
step:1067/1680 train_time:95739ms step_avg:89.73ms
step:1068/1680 train_time:95830ms step_avg:89.73ms
step:1069/1680 train_time:95922ms step_avg:89.73ms
step:1070/1680 train_time:96012ms step_avg:89.73ms
step:1071/1680 train_time:96102ms step_avg:89.73ms
step:1072/1680 train_time:96193ms step_avg:89.73ms
step:1073/1680 train_time:96283ms step_avg:89.73ms
step:1074/1680 train_time:96373ms step_avg:89.73ms
step:1075/1680 train_time:96463ms step_avg:89.73ms
step:1076/1680 train_time:96553ms step_avg:89.73ms
step:1077/1680 train_time:96642ms step_avg:89.73ms
step:1078/1680 train_time:96733ms step_avg:89.73ms
step:1079/1680 train_time:96823ms step_avg:89.73ms
step:1080/1680 train_time:96913ms step_avg:89.73ms
step:1081/1680 train_time:97003ms step_avg:89.73ms
step:1082/1680 train_time:97095ms step_avg:89.74ms
step:1083/1680 train_time:97184ms step_avg:89.74ms
step:1084/1680 train_time:97275ms step_avg:89.74ms
step:1085/1680 train_time:97365ms step_avg:89.74ms
step:1086/1680 train_time:97455ms step_avg:89.74ms
step:1087/1680 train_time:97545ms step_avg:89.74ms
step:1088/1680 train_time:97636ms step_avg:89.74ms
step:1089/1680 train_time:97726ms step_avg:89.74ms
step:1090/1680 train_time:97816ms step_avg:89.74ms
step:1091/1680 train_time:97906ms step_avg:89.74ms
step:1092/1680 train_time:97996ms step_avg:89.74ms
step:1093/1680 train_time:98086ms step_avg:89.74ms
step:1094/1680 train_time:98175ms step_avg:89.74ms
step:1095/1680 train_time:98266ms step_avg:89.74ms
step:1096/1680 train_time:98357ms step_avg:89.74ms
step:1097/1680 train_time:98448ms step_avg:89.74ms
step:1098/1680 train_time:98538ms step_avg:89.74ms
step:1099/1680 train_time:98630ms step_avg:89.75ms
step:1100/1680 train_time:98722ms step_avg:89.75ms
step:1101/1680 train_time:98813ms step_avg:89.75ms
step:1102/1680 train_time:98903ms step_avg:89.75ms
step:1103/1680 train_time:98994ms step_avg:89.75ms
step:1104/1680 train_time:99084ms step_avg:89.75ms
step:1105/1680 train_time:99175ms step_avg:89.75ms
step:1106/1680 train_time:99266ms step_avg:89.75ms
step:1107/1680 train_time:99357ms step_avg:89.75ms
step:1108/1680 train_time:99447ms step_avg:89.75ms
step:1109/1680 train_time:99538ms step_avg:89.75ms
step:1110/1680 train_time:99630ms step_avg:89.76ms
step:1111/1680 train_time:99721ms step_avg:89.76ms
step:1112/1680 train_time:99812ms step_avg:89.76ms
step:1113/1680 train_time:99902ms step_avg:89.76ms
step:1114/1680 train_time:99993ms step_avg:89.76ms
step:1115/1680 train_time:100084ms step_avg:89.76ms
step:1116/1680 train_time:100175ms step_avg:89.76ms
step:1117/1680 train_time:100267ms step_avg:89.76ms
step:1118/1680 train_time:100358ms step_avg:89.77ms
step:1119/1680 train_time:100448ms step_avg:89.77ms
step:1120/1680 train_time:100539ms step_avg:89.77ms
step:1121/1680 train_time:100630ms step_avg:89.77ms
step:1122/1680 train_time:100721ms step_avg:89.77ms
step:1123/1680 train_time:100812ms step_avg:89.77ms
step:1124/1680 train_time:100902ms step_avg:89.77ms
step:1125/1680 train_time:100994ms step_avg:89.77ms
step:1125/1680 val_loss:3.4154 train_time:101086ms step_avg:89.85ms
step:1126/1680 train_time:101108ms step_avg:89.79ms
step:1127/1680 train_time:101181ms step_avg:89.78ms
step:1128/1680 train_time:101277ms step_avg:89.78ms
step:1129/1680 train_time:101368ms step_avg:89.79ms
step:1130/1680 train_time:101458ms step_avg:89.79ms
step:1131/1680 train_time:101549ms step_avg:89.79ms
step:1132/1680 train_time:101639ms step_avg:89.79ms
step:1133/1680 train_time:101729ms step_avg:89.79ms
step:1134/1680 train_time:101818ms step_avg:89.79ms
step:1135/1680 train_time:101908ms step_avg:89.79ms
step:1136/1680 train_time:101999ms step_avg:89.79ms
step:1137/1680 train_time:102090ms step_avg:89.79ms
step:1138/1680 train_time:102186ms step_avg:89.79ms
step:1139/1680 train_time:102279ms step_avg:89.80ms
step:1140/1680 train_time:102371ms step_avg:89.80ms
step:1141/1680 train_time:102461ms step_avg:89.80ms
step:1142/1680 train_time:102552ms step_avg:89.80ms
step:1143/1680 train_time:102642ms step_avg:89.80ms
step:1144/1680 train_time:102733ms step_avg:89.80ms
step:1145/1680 train_time:102823ms step_avg:89.80ms
step:1146/1680 train_time:102914ms step_avg:89.80ms
step:1147/1680 train_time:103005ms step_avg:89.80ms
step:1148/1680 train_time:103097ms step_avg:89.81ms
step:1149/1680 train_time:103189ms step_avg:89.81ms
step:1150/1680 train_time:103281ms step_avg:89.81ms
step:1151/1680 train_time:103372ms step_avg:89.81ms
step:1152/1680 train_time:103462ms step_avg:89.81ms
step:1153/1680 train_time:103553ms step_avg:89.81ms
step:1154/1680 train_time:103644ms step_avg:89.81ms
step:1155/1680 train_time:103735ms step_avg:89.81ms
step:1156/1680 train_time:103825ms step_avg:89.81ms
step:1157/1680 train_time:103915ms step_avg:89.81ms
step:1158/1680 train_time:104007ms step_avg:89.82ms
step:1159/1680 train_time:104098ms step_avg:89.82ms
step:1160/1680 train_time:104191ms step_avg:89.82ms
step:1161/1680 train_time:104283ms step_avg:89.82ms
step:1162/1680 train_time:104375ms step_avg:89.82ms
step:1163/1680 train_time:104466ms step_avg:89.82ms
step:1164/1680 train_time:104556ms step_avg:89.83ms
step:1165/1680 train_time:104647ms step_avg:89.83ms
step:1166/1680 train_time:104737ms step_avg:89.83ms
step:1167/1680 train_time:104828ms step_avg:89.83ms
step:1168/1680 train_time:104918ms step_avg:89.83ms
step:1169/1680 train_time:105008ms step_avg:89.83ms
step:1170/1680 train_time:105099ms step_avg:89.83ms
step:1171/1680 train_time:105191ms step_avg:89.83ms
step:1172/1680 train_time:105282ms step_avg:89.83ms
step:1173/1680 train_time:105374ms step_avg:89.83ms
step:1174/1680 train_time:105466ms step_avg:89.83ms
step:1175/1680 train_time:105558ms step_avg:89.84ms
step:1176/1680 train_time:105648ms step_avg:89.84ms
step:1177/1680 train_time:105738ms step_avg:89.84ms
step:1178/1680 train_time:105829ms step_avg:89.84ms
step:1179/1680 train_time:105919ms step_avg:89.84ms
step:1180/1680 train_time:106010ms step_avg:89.84ms
step:1181/1680 train_time:106101ms step_avg:89.84ms
step:1182/1680 train_time:106192ms step_avg:89.84ms
step:1183/1680 train_time:106284ms step_avg:89.84ms
step:1184/1680 train_time:106376ms step_avg:89.84ms
step:1185/1680 train_time:106467ms step_avg:89.85ms
step:1186/1680 train_time:106558ms step_avg:89.85ms
step:1187/1680 train_time:106649ms step_avg:89.85ms
step:1188/1680 train_time:106739ms step_avg:89.85ms
step:1189/1680 train_time:106829ms step_avg:89.85ms
step:1190/1680 train_time:106919ms step_avg:89.85ms
step:1191/1680 train_time:107011ms step_avg:89.85ms
step:1192/1680 train_time:107102ms step_avg:89.85ms
step:1193/1680 train_time:107193ms step_avg:89.85ms
step:1194/1680 train_time:107286ms step_avg:89.85ms
step:1195/1680 train_time:107378ms step_avg:89.86ms
step:1196/1680 train_time:107469ms step_avg:89.86ms
step:1197/1680 train_time:107560ms step_avg:89.86ms
step:1198/1680 train_time:107651ms step_avg:89.86ms
step:1199/1680 train_time:107741ms step_avg:89.86ms
step:1200/1680 train_time:107831ms step_avg:89.86ms
step:1201/1680 train_time:107922ms step_avg:89.86ms
step:1202/1680 train_time:108014ms step_avg:89.86ms
step:1203/1680 train_time:108104ms step_avg:89.86ms
step:1204/1680 train_time:108194ms step_avg:89.86ms
step:1205/1680 train_time:108285ms step_avg:89.86ms
step:1206/1680 train_time:108377ms step_avg:89.86ms
step:1207/1680 train_time:108468ms step_avg:89.87ms
step:1208/1680 train_time:108560ms step_avg:89.87ms
step:1209/1680 train_time:108650ms step_avg:89.87ms
step:1210/1680 train_time:108741ms step_avg:89.87ms
step:1211/1680 train_time:108832ms step_avg:89.87ms
step:1212/1680 train_time:108924ms step_avg:89.87ms
step:1213/1680 train_time:109014ms step_avg:89.87ms
step:1214/1680 train_time:109105ms step_avg:89.87ms
step:1215/1680 train_time:109196ms step_avg:89.87ms
step:1216/1680 train_time:109288ms step_avg:89.87ms
step:1217/1680 train_time:109379ms step_avg:89.88ms
step:1218/1680 train_time:109470ms step_avg:89.88ms
step:1219/1680 train_time:109561ms step_avg:89.88ms
step:1220/1680 train_time:109651ms step_avg:89.88ms
step:1221/1680 train_time:109742ms step_avg:89.88ms
step:1222/1680 train_time:109833ms step_avg:89.88ms
step:1223/1680 train_time:109924ms step_avg:89.88ms
step:1224/1680 train_time:110015ms step_avg:89.88ms
step:1225/1680 train_time:110106ms step_avg:89.88ms
step:1226/1680 train_time:110197ms step_avg:89.88ms
step:1227/1680 train_time:110288ms step_avg:89.88ms
step:1228/1680 train_time:110379ms step_avg:89.89ms
step:1229/1680 train_time:110470ms step_avg:89.89ms
step:1230/1680 train_time:110560ms step_avg:89.89ms
step:1231/1680 train_time:110653ms step_avg:89.89ms
step:1232/1680 train_time:110744ms step_avg:89.89ms
step:1233/1680 train_time:110835ms step_avg:89.89ms
step:1234/1680 train_time:110926ms step_avg:89.89ms
step:1235/1680 train_time:111016ms step_avg:89.89ms
step:1236/1680 train_time:111108ms step_avg:89.89ms
step:1237/1680 train_time:111200ms step_avg:89.89ms
step:1238/1680 train_time:111290ms step_avg:89.90ms
step:1239/1680 train_time:111382ms step_avg:89.90ms
step:1240/1680 train_time:111472ms step_avg:89.90ms
step:1241/1680 train_time:111563ms step_avg:89.90ms
step:1242/1680 train_time:111654ms step_avg:89.90ms
step:1243/1680 train_time:111745ms step_avg:89.90ms
step:1244/1680 train_time:111835ms step_avg:89.90ms
step:1245/1680 train_time:111926ms step_avg:89.90ms
step:1246/1680 train_time:112016ms step_avg:89.90ms
step:1247/1680 train_time:112108ms step_avg:89.90ms
step:1248/1680 train_time:112199ms step_avg:89.90ms
step:1249/1680 train_time:112289ms step_avg:89.90ms
step:1250/1680 train_time:112381ms step_avg:89.91ms
step:1250/1680 val_loss:3.3768 train_time:112473ms step_avg:89.98ms
step:1251/1680 train_time:112496ms step_avg:89.92ms
step:1252/1680 train_time:112572ms step_avg:89.91ms
step:1253/1680 train_time:112667ms step_avg:89.92ms
step:1254/1680 train_time:112759ms step_avg:89.92ms
step:1255/1680 train_time:112850ms step_avg:89.92ms
step:1256/1680 train_time:112939ms step_avg:89.92ms
step:1257/1680 train_time:113028ms step_avg:89.92ms
step:1258/1680 train_time:113118ms step_avg:89.92ms
step:1259/1680 train_time:113208ms step_avg:89.92ms
step:1260/1680 train_time:113298ms step_avg:89.92ms
step:1261/1680 train_time:113388ms step_avg:89.92ms
step:1262/1680 train_time:113483ms step_avg:89.92ms
step:1263/1680 train_time:113577ms step_avg:89.93ms
step:1264/1680 train_time:113671ms step_avg:89.93ms
step:1265/1680 train_time:113762ms step_avg:89.93ms
step:1266/1680 train_time:113852ms step_avg:89.93ms
step:1267/1680 train_time:113943ms step_avg:89.93ms
step:1268/1680 train_time:114033ms step_avg:89.93ms
step:1269/1680 train_time:114122ms step_avg:89.93ms
step:1270/1680 train_time:114211ms step_avg:89.93ms
step:1271/1680 train_time:114301ms step_avg:89.93ms
step:1272/1680 train_time:114391ms step_avg:89.93ms
step:1273/1680 train_time:114483ms step_avg:89.93ms
step:1274/1680 train_time:114576ms step_avg:89.93ms
step:1275/1680 train_time:114669ms step_avg:89.94ms
step:1276/1680 train_time:114760ms step_avg:89.94ms
step:1277/1680 train_time:114851ms step_avg:89.94ms
step:1278/1680 train_time:114941ms step_avg:89.94ms
step:1279/1680 train_time:115032ms step_avg:89.94ms
step:1280/1680 train_time:115122ms step_avg:89.94ms
step:1281/1680 train_time:115212ms step_avg:89.94ms
step:1282/1680 train_time:115302ms step_avg:89.94ms
step:1283/1680 train_time:115393ms step_avg:89.94ms
step:1284/1680 train_time:115484ms step_avg:89.94ms
step:1285/1680 train_time:115576ms step_avg:89.94ms
step:1286/1680 train_time:115667ms step_avg:89.94ms
step:1287/1680 train_time:115759ms step_avg:89.94ms
step:1288/1680 train_time:115850ms step_avg:89.95ms
step:1289/1680 train_time:115941ms step_avg:89.95ms
step:1290/1680 train_time:116032ms step_avg:89.95ms
step:1291/1680 train_time:116122ms step_avg:89.95ms
step:1292/1680 train_time:116212ms step_avg:89.95ms
step:1293/1680 train_time:116302ms step_avg:89.95ms
step:1294/1680 train_time:116393ms step_avg:89.95ms
step:1295/1680 train_time:116484ms step_avg:89.95ms
step:1296/1680 train_time:116575ms step_avg:89.95ms
step:1297/1680 train_time:116666ms step_avg:89.95ms
step:1298/1680 train_time:116758ms step_avg:89.95ms
step:1299/1680 train_time:116849ms step_avg:89.95ms
step:1300/1680 train_time:116940ms step_avg:89.95ms
step:1301/1680 train_time:117030ms step_avg:89.95ms
step:1302/1680 train_time:117120ms step_avg:89.95ms
step:1303/1680 train_time:117210ms step_avg:89.95ms
step:1304/1680 train_time:117301ms step_avg:89.95ms
step:1305/1680 train_time:117392ms step_avg:89.96ms
step:1306/1680 train_time:117484ms step_avg:89.96ms
step:1307/1680 train_time:117574ms step_avg:89.96ms
step:1308/1680 train_time:117666ms step_avg:89.96ms
step:1309/1680 train_time:117757ms step_avg:89.96ms
step:1310/1680 train_time:117848ms step_avg:89.96ms
step:1311/1680 train_time:117940ms step_avg:89.96ms
step:1312/1680 train_time:118030ms step_avg:89.96ms
step:1313/1680 train_time:118121ms step_avg:89.96ms
step:1314/1680 train_time:118212ms step_avg:89.96ms
step:1315/1680 train_time:118301ms step_avg:89.96ms
step:1316/1680 train_time:118392ms step_avg:89.96ms
step:1317/1680 train_time:118483ms step_avg:89.96ms
step:1318/1680 train_time:118574ms step_avg:89.97ms
step:1319/1680 train_time:118665ms step_avg:89.97ms
step:1320/1680 train_time:118756ms step_avg:89.97ms
step:1321/1680 train_time:118847ms step_avg:89.97ms
step:1322/1680 train_time:118940ms step_avg:89.97ms
step:1323/1680 train_time:119032ms step_avg:89.97ms
step:1324/1680 train_time:119122ms step_avg:89.97ms
step:1325/1680 train_time:119212ms step_avg:89.97ms
step:1326/1680 train_time:119302ms step_avg:89.97ms
step:1327/1680 train_time:119393ms step_avg:89.97ms
step:1328/1680 train_time:119484ms step_avg:89.97ms
step:1329/1680 train_time:119574ms step_avg:89.97ms
step:1330/1680 train_time:119665ms step_avg:89.97ms
step:1331/1680 train_time:119756ms step_avg:89.97ms
step:1332/1680 train_time:119849ms step_avg:89.98ms
step:1333/1680 train_time:119940ms step_avg:89.98ms
step:1334/1680 train_time:120032ms step_avg:89.98ms
step:1335/1680 train_time:120122ms step_avg:89.98ms
step:1336/1680 train_time:120213ms step_avg:89.98ms
step:1337/1680 train_time:120303ms step_avg:89.98ms
step:1338/1680 train_time:120394ms step_avg:89.98ms
step:1339/1680 train_time:120484ms step_avg:89.98ms
step:1340/1680 train_time:120575ms step_avg:89.98ms
step:1341/1680 train_time:120666ms step_avg:89.98ms
step:1342/1680 train_time:120758ms step_avg:89.98ms
step:1343/1680 train_time:120849ms step_avg:89.98ms
step:1344/1680 train_time:120940ms step_avg:89.98ms
step:1345/1680 train_time:121031ms step_avg:89.99ms
step:1346/1680 train_time:121121ms step_avg:89.99ms
step:1347/1680 train_time:121212ms step_avg:89.99ms
step:1348/1680 train_time:121303ms step_avg:89.99ms
step:1349/1680 train_time:121393ms step_avg:89.99ms
step:1350/1680 train_time:121483ms step_avg:89.99ms
step:1351/1680 train_time:121574ms step_avg:89.99ms
step:1352/1680 train_time:121667ms step_avg:89.99ms
step:1353/1680 train_time:121759ms step_avg:89.99ms
step:1354/1680 train_time:121850ms step_avg:89.99ms
step:1355/1680 train_time:121941ms step_avg:89.99ms
step:1356/1680 train_time:122032ms step_avg:89.99ms
step:1357/1680 train_time:122123ms step_avg:89.99ms
step:1358/1680 train_time:122213ms step_avg:90.00ms
step:1359/1680 train_time:122304ms step_avg:90.00ms
step:1360/1680 train_time:122394ms step_avg:90.00ms
step:1361/1680 train_time:122485ms step_avg:90.00ms
step:1362/1680 train_time:122576ms step_avg:90.00ms
step:1363/1680 train_time:122668ms step_avg:90.00ms
step:1364/1680 train_time:122759ms step_avg:90.00ms
step:1365/1680 train_time:122850ms step_avg:90.00ms
step:1366/1680 train_time:122941ms step_avg:90.00ms
step:1367/1680 train_time:123032ms step_avg:90.00ms
step:1368/1680 train_time:123123ms step_avg:90.00ms
step:1369/1680 train_time:123215ms step_avg:90.00ms
step:1370/1680 train_time:123306ms step_avg:90.00ms
step:1371/1680 train_time:123397ms step_avg:90.01ms
step:1372/1680 train_time:123488ms step_avg:90.01ms
step:1373/1680 train_time:123579ms step_avg:90.01ms
step:1374/1680 train_time:123669ms step_avg:90.01ms
step:1375/1680 train_time:123759ms step_avg:90.01ms
step:1375/1680 val_loss:3.3426 train_time:123852ms step_avg:90.07ms
step:1376/1680 train_time:123874ms step_avg:90.02ms
step:1377/1680 train_time:123944ms step_avg:90.01ms
step:1378/1680 train_time:124039ms step_avg:90.01ms
step:1379/1680 train_time:124129ms step_avg:90.01ms
step:1380/1680 train_time:124219ms step_avg:90.01ms
step:1381/1680 train_time:124310ms step_avg:90.01ms
step:1382/1680 train_time:124401ms step_avg:90.01ms
step:1383/1680 train_time:124491ms step_avg:90.01ms
step:1384/1680 train_time:124581ms step_avg:90.01ms
step:1385/1680 train_time:124671ms step_avg:90.01ms
step:1386/1680 train_time:124761ms step_avg:90.02ms
step:1387/1680 train_time:124854ms step_avg:90.02ms
step:1388/1680 train_time:124946ms step_avg:90.02ms
step:1389/1680 train_time:125038ms step_avg:90.02ms
step:1390/1680 train_time:125129ms step_avg:90.02ms
step:1391/1680 train_time:125221ms step_avg:90.02ms
step:1392/1680 train_time:125311ms step_avg:90.02ms
step:1393/1680 train_time:125401ms step_avg:90.02ms
step:1394/1680 train_time:125491ms step_avg:90.02ms
step:1395/1680 train_time:125581ms step_avg:90.02ms
step:1396/1680 train_time:125671ms step_avg:90.02ms
step:1397/1680 train_time:125762ms step_avg:90.02ms
step:1398/1680 train_time:125853ms step_avg:90.02ms
step:1399/1680 train_time:125945ms step_avg:90.03ms
step:1400/1680 train_time:126038ms step_avg:90.03ms
step:1401/1680 train_time:126130ms step_avg:90.03ms
step:1402/1680 train_time:126222ms step_avg:90.03ms
step:1403/1680 train_time:126312ms step_avg:90.03ms
step:1404/1680 train_time:126403ms step_avg:90.03ms
step:1405/1680 train_time:126493ms step_avg:90.03ms
step:1406/1680 train_time:126582ms step_avg:90.03ms
step:1407/1680 train_time:126673ms step_avg:90.03ms
step:1408/1680 train_time:126763ms step_avg:90.03ms
step:1409/1680 train_time:126854ms step_avg:90.03ms
step:1410/1680 train_time:126945ms step_avg:90.03ms
step:1411/1680 train_time:127037ms step_avg:90.03ms
step:1412/1680 train_time:127129ms step_avg:90.03ms
step:1413/1680 train_time:127221ms step_avg:90.04ms
step:1414/1680 train_time:127312ms step_avg:90.04ms
step:1415/1680 train_time:127403ms step_avg:90.04ms
step:1416/1680 train_time:127493ms step_avg:90.04ms
step:1417/1680 train_time:127583ms step_avg:90.04ms
step:1418/1680 train_time:127674ms step_avg:90.04ms
step:1419/1680 train_time:127764ms step_avg:90.04ms
step:1420/1680 train_time:127855ms step_avg:90.04ms
step:1421/1680 train_time:127946ms step_avg:90.04ms
step:1422/1680 train_time:128037ms step_avg:90.04ms
step:1423/1680 train_time:128128ms step_avg:90.04ms
step:1424/1680 train_time:128220ms step_avg:90.04ms
step:1425/1680 train_time:128311ms step_avg:90.04ms
step:1426/1680 train_time:128403ms step_avg:90.04ms
step:1427/1680 train_time:128493ms step_avg:90.04ms
step:1428/1680 train_time:128586ms step_avg:90.05ms
step:1429/1680 train_time:128675ms step_avg:90.05ms
step:1430/1680 train_time:128765ms step_avg:90.05ms
step:1431/1680 train_time:128855ms step_avg:90.05ms
step:1432/1680 train_time:128946ms step_avg:90.05ms
step:1433/1680 train_time:129037ms step_avg:90.05ms
step:1434/1680 train_time:129128ms step_avg:90.05ms
step:1435/1680 train_time:129219ms step_avg:90.05ms
step:1436/1680 train_time:129311ms step_avg:90.05ms
step:1437/1680 train_time:129401ms step_avg:90.05ms
step:1438/1680 train_time:129492ms step_avg:90.05ms
step:1439/1680 train_time:129583ms step_avg:90.05ms
step:1440/1680 train_time:129674ms step_avg:90.05ms
step:1441/1680 train_time:129765ms step_avg:90.05ms
step:1442/1680 train_time:129855ms step_avg:90.05ms
step:1443/1680 train_time:129946ms step_avg:90.05ms
step:1444/1680 train_time:130037ms step_avg:90.05ms
step:1445/1680 train_time:130128ms step_avg:90.05ms
step:1446/1680 train_time:130220ms step_avg:90.06ms
step:1447/1680 train_time:130310ms step_avg:90.06ms
step:1448/1680 train_time:130401ms step_avg:90.06ms
step:1449/1680 train_time:130492ms step_avg:90.06ms
step:1450/1680 train_time:130583ms step_avg:90.06ms
step:1451/1680 train_time:130673ms step_avg:90.06ms
step:1452/1680 train_time:130764ms step_avg:90.06ms
step:1453/1680 train_time:130854ms step_avg:90.06ms
step:1454/1680 train_time:130945ms step_avg:90.06ms
step:1455/1680 train_time:131036ms step_avg:90.06ms
step:1456/1680 train_time:131126ms step_avg:90.06ms
step:1457/1680 train_time:131217ms step_avg:90.06ms
step:1458/1680 train_time:131309ms step_avg:90.06ms
step:1459/1680 train_time:131399ms step_avg:90.06ms
step:1460/1680 train_time:131490ms step_avg:90.06ms
step:1461/1680 train_time:131581ms step_avg:90.06ms
step:1462/1680 train_time:131672ms step_avg:90.06ms
step:1463/1680 train_time:131763ms step_avg:90.06ms
step:1464/1680 train_time:131854ms step_avg:90.06ms
step:1465/1680 train_time:131944ms step_avg:90.06ms
step:1466/1680 train_time:132035ms step_avg:90.06ms
step:1467/1680 train_time:132125ms step_avg:90.07ms
step:1468/1680 train_time:132217ms step_avg:90.07ms
step:1469/1680 train_time:132308ms step_avg:90.07ms
step:1470/1680 train_time:132399ms step_avg:90.07ms
step:1471/1680 train_time:132490ms step_avg:90.07ms
step:1472/1680 train_time:132581ms step_avg:90.07ms
step:1473/1680 train_time:132672ms step_avg:90.07ms
step:1474/1680 train_time:132763ms step_avg:90.07ms
step:1475/1680 train_time:132853ms step_avg:90.07ms
step:1476/1680 train_time:132944ms step_avg:90.07ms
step:1477/1680 train_time:133034ms step_avg:90.07ms
step:1478/1680 train_time:133125ms step_avg:90.07ms
step:1479/1680 train_time:133216ms step_avg:90.07ms
step:1480/1680 train_time:133307ms step_avg:90.07ms
step:1481/1680 train_time:133398ms step_avg:90.07ms
step:1482/1680 train_time:133490ms step_avg:90.07ms
step:1483/1680 train_time:133580ms step_avg:90.07ms
step:1484/1680 train_time:133671ms step_avg:90.08ms
step:1485/1680 train_time:133764ms step_avg:90.08ms
step:1486/1680 train_time:133854ms step_avg:90.08ms
step:1487/1680 train_time:133945ms step_avg:90.08ms
step:1488/1680 train_time:134035ms step_avg:90.08ms
step:1489/1680 train_time:134126ms step_avg:90.08ms
step:1490/1680 train_time:134217ms step_avg:90.08ms
step:1491/1680 train_time:134308ms step_avg:90.08ms
step:1492/1680 train_time:134399ms step_avg:90.08ms
step:1493/1680 train_time:134492ms step_avg:90.08ms
step:1494/1680 train_time:134583ms step_avg:90.08ms
step:1495/1680 train_time:134674ms step_avg:90.08ms
step:1496/1680 train_time:134765ms step_avg:90.08ms
step:1497/1680 train_time:134855ms step_avg:90.08ms
step:1498/1680 train_time:134947ms step_avg:90.08ms
step:1499/1680 train_time:135037ms step_avg:90.09ms
step:1500/1680 train_time:135128ms step_avg:90.09ms
step:1500/1680 val_loss:3.3125 train_time:135220ms step_avg:90.15ms
step:1501/1680 train_time:135243ms step_avg:90.10ms
step:1502/1680 train_time:135316ms step_avg:90.09ms
step:1503/1680 train_time:135411ms step_avg:90.09ms
step:1504/1680 train_time:135502ms step_avg:90.09ms
step:1505/1680 train_time:135591ms step_avg:90.09ms
step:1506/1680 train_time:135681ms step_avg:90.09ms
step:1507/1680 train_time:135771ms step_avg:90.09ms
step:1508/1680 train_time:135861ms step_avg:90.09ms
step:1509/1680 train_time:135951ms step_avg:90.09ms
step:1510/1680 train_time:136042ms step_avg:90.09ms
step:1511/1680 train_time:136133ms step_avg:90.09ms
step:1512/1680 train_time:136226ms step_avg:90.10ms
step:1513/1680 train_time:136320ms step_avg:90.10ms
step:1514/1680 train_time:136411ms step_avg:90.10ms
step:1515/1680 train_time:136503ms step_avg:90.10ms
step:1516/1680 train_time:136594ms step_avg:90.10ms
step:1517/1680 train_time:136685ms step_avg:90.10ms
step:1518/1680 train_time:136775ms step_avg:90.10ms
step:1519/1680 train_time:136864ms step_avg:90.10ms
step:1520/1680 train_time:136954ms step_avg:90.10ms
step:1521/1680 train_time:137045ms step_avg:90.10ms
step:1522/1680 train_time:137136ms step_avg:90.10ms
step:1523/1680 train_time:137228ms step_avg:90.10ms
step:1524/1680 train_time:137321ms step_avg:90.11ms
step:1525/1680 train_time:137412ms step_avg:90.11ms
step:1526/1680 train_time:137503ms step_avg:90.11ms
step:1527/1680 train_time:137595ms step_avg:90.11ms
step:1528/1680 train_time:137686ms step_avg:90.11ms
step:1529/1680 train_time:137777ms step_avg:90.11ms
step:1530/1680 train_time:137867ms step_avg:90.11ms
step:1531/1680 train_time:137957ms step_avg:90.11ms
step:1532/1680 train_time:138048ms step_avg:90.11ms
step:1533/1680 train_time:138138ms step_avg:90.11ms
step:1534/1680 train_time:138230ms step_avg:90.11ms
step:1535/1680 train_time:138323ms step_avg:90.11ms
step:1536/1680 train_time:138415ms step_avg:90.11ms
step:1537/1680 train_time:138505ms step_avg:90.11ms
step:1538/1680 train_time:138597ms step_avg:90.11ms
step:1539/1680 train_time:138687ms step_avg:90.12ms
step:1540/1680 train_time:138778ms step_avg:90.12ms
step:1541/1680 train_time:138867ms step_avg:90.12ms
step:1542/1680 train_time:138958ms step_avg:90.12ms
step:1543/1680 train_time:139049ms step_avg:90.12ms
step:1544/1680 train_time:139138ms step_avg:90.12ms
step:1545/1680 train_time:139230ms step_avg:90.12ms
step:1546/1680 train_time:139322ms step_avg:90.12ms
step:1547/1680 train_time:139412ms step_avg:90.12ms
step:1548/1680 train_time:139504ms step_avg:90.12ms
step:1549/1680 train_time:139596ms step_avg:90.12ms
step:1550/1680 train_time:139687ms step_avg:90.12ms
step:1551/1680 train_time:139778ms step_avg:90.12ms
step:1552/1680 train_time:139867ms step_avg:90.12ms
step:1553/1680 train_time:139958ms step_avg:90.12ms
step:1554/1680 train_time:140048ms step_avg:90.12ms
step:1555/1680 train_time:140139ms step_avg:90.12ms
step:1556/1680 train_time:140230ms step_avg:90.12ms
step:1557/1680 train_time:140322ms step_avg:90.12ms
step:1558/1680 train_time:140413ms step_avg:90.12ms
step:1559/1680 train_time:140505ms step_avg:90.12ms
step:1560/1680 train_time:140597ms step_avg:90.13ms
step:1561/1680 train_time:140688ms step_avg:90.13ms
step:1562/1680 train_time:140778ms step_avg:90.13ms
step:1563/1680 train_time:140868ms step_avg:90.13ms
step:1564/1680 train_time:140958ms step_avg:90.13ms
step:1565/1680 train_time:141048ms step_avg:90.13ms
step:1566/1680 train_time:141139ms step_avg:90.13ms
step:1567/1680 train_time:141230ms step_avg:90.13ms
step:1568/1680 train_time:141322ms step_avg:90.13ms
step:1569/1680 train_time:141413ms step_avg:90.13ms
step:1570/1680 train_time:141505ms step_avg:90.13ms
step:1571/1680 train_time:141597ms step_avg:90.13ms
step:1572/1680 train_time:141688ms step_avg:90.13ms
step:1573/1680 train_time:141779ms step_avg:90.13ms
step:1574/1680 train_time:141869ms step_avg:90.13ms
step:1575/1680 train_time:141959ms step_avg:90.13ms
step:1576/1680 train_time:142050ms step_avg:90.13ms
step:1577/1680 train_time:142140ms step_avg:90.13ms
step:1578/1680 train_time:142232ms step_avg:90.13ms
step:1579/1680 train_time:142323ms step_avg:90.13ms
step:1580/1680 train_time:142414ms step_avg:90.14ms
step:1581/1680 train_time:142506ms step_avg:90.14ms
step:1582/1680 train_time:142599ms step_avg:90.14ms
step:1583/1680 train_time:142690ms step_avg:90.14ms
step:1584/1680 train_time:142782ms step_avg:90.14ms
step:1585/1680 train_time:142873ms step_avg:90.14ms
step:1586/1680 train_time:142963ms step_avg:90.14ms
step:1587/1680 train_time:143054ms step_avg:90.14ms
step:1588/1680 train_time:143144ms step_avg:90.14ms
step:1589/1680 train_time:143235ms step_avg:90.14ms
step:1590/1680 train_time:143326ms step_avg:90.14ms
step:1591/1680 train_time:143418ms step_avg:90.14ms
step:1592/1680 train_time:143508ms step_avg:90.14ms
step:1593/1680 train_time:143600ms step_avg:90.14ms
step:1594/1680 train_time:143693ms step_avg:90.15ms
step:1595/1680 train_time:143785ms step_avg:90.15ms
step:1596/1680 train_time:143875ms step_avg:90.15ms
step:1597/1680 train_time:143966ms step_avg:90.15ms
step:1598/1680 train_time:144056ms step_avg:90.15ms
step:1599/1680 train_time:144147ms step_avg:90.15ms
step:1600/1680 train_time:144238ms step_avg:90.15ms
step:1601/1680 train_time:144328ms step_avg:90.15ms
step:1602/1680 train_time:144418ms step_avg:90.15ms
step:1603/1680 train_time:144508ms step_avg:90.15ms
step:1604/1680 train_time:144600ms step_avg:90.15ms
step:1605/1680 train_time:144691ms step_avg:90.15ms
step:1606/1680 train_time:144783ms step_avg:90.15ms
step:1607/1680 train_time:144874ms step_avg:90.15ms
step:1608/1680 train_time:144965ms step_avg:90.15ms
step:1609/1680 train_time:145056ms step_avg:90.15ms
step:1610/1680 train_time:145147ms step_avg:90.15ms
step:1611/1680 train_time:145237ms step_avg:90.15ms
step:1612/1680 train_time:145328ms step_avg:90.15ms
step:1613/1680 train_time:145418ms step_avg:90.15ms
step:1614/1680 train_time:145509ms step_avg:90.15ms
step:1615/1680 train_time:145600ms step_avg:90.15ms
step:1616/1680 train_time:145691ms step_avg:90.16ms
step:1617/1680 train_time:145783ms step_avg:90.16ms
step:1618/1680 train_time:145874ms step_avg:90.16ms
step:1619/1680 train_time:145965ms step_avg:90.16ms
step:1620/1680 train_time:146056ms step_avg:90.16ms
step:1621/1680 train_time:146147ms step_avg:90.16ms
step:1622/1680 train_time:146237ms step_avg:90.16ms
step:1623/1680 train_time:146327ms step_avg:90.16ms
step:1624/1680 train_time:146418ms step_avg:90.16ms
step:1625/1680 train_time:146508ms step_avg:90.16ms
step:1625/1680 val_loss:3.2885 train_time:146600ms step_avg:90.22ms
step:1626/1680 train_time:146622ms step_avg:90.17ms
step:1627/1680 train_time:146694ms step_avg:90.16ms
step:1628/1680 train_time:146793ms step_avg:90.17ms
step:1629/1680 train_time:146885ms step_avg:90.17ms
step:1630/1680 train_time:146975ms step_avg:90.17ms
step:1631/1680 train_time:147064ms step_avg:90.17ms
step:1632/1680 train_time:147153ms step_avg:90.17ms
step:1633/1680 train_time:147242ms step_avg:90.17ms
step:1634/1680 train_time:147332ms step_avg:90.17ms
step:1635/1680 train_time:147422ms step_avg:90.17ms
step:1636/1680 train_time:147511ms step_avg:90.17ms
step:1637/1680 train_time:147602ms step_avg:90.17ms
step:1638/1680 train_time:147696ms step_avg:90.17ms
step:1639/1680 train_time:147789ms step_avg:90.17ms
step:1640/1680 train_time:147881ms step_avg:90.17ms
step:1641/1680 train_time:147972ms step_avg:90.17ms
step:1642/1680 train_time:148062ms step_avg:90.17ms
step:1643/1680 train_time:148152ms step_avg:90.17ms
step:1644/1680 train_time:148242ms step_avg:90.17ms
step:1645/1680 train_time:148331ms step_avg:90.17ms
step:1646/1680 train_time:148421ms step_avg:90.17ms
step:1647/1680 train_time:148512ms step_avg:90.17ms
step:1648/1680 train_time:148603ms step_avg:90.17ms
step:1649/1680 train_time:148695ms step_avg:90.17ms
step:1650/1680 train_time:148787ms step_avg:90.17ms
step:1651/1680 train_time:148880ms step_avg:90.18ms
step:1652/1680 train_time:148971ms step_avg:90.18ms
step:1653/1680 train_time:149061ms step_avg:90.18ms
step:1654/1680 train_time:149151ms step_avg:90.18ms
step:1655/1680 train_time:149241ms step_avg:90.18ms
step:1656/1680 train_time:149331ms step_avg:90.18ms
step:1657/1680 train_time:149421ms step_avg:90.18ms
step:1658/1680 train_time:149512ms step_avg:90.18ms
step:1659/1680 train_time:149603ms step_avg:90.18ms
step:1660/1680 train_time:149694ms step_avg:90.18ms
step:1661/1680 train_time:149785ms step_avg:90.18ms
step:1662/1680 train_time:149877ms step_avg:90.18ms
step:1663/1680 train_time:149969ms step_avg:90.18ms
step:1664/1680 train_time:150060ms step_avg:90.18ms
step:1665/1680 train_time:150152ms step_avg:90.18ms
step:1666/1680 train_time:150242ms step_avg:90.18ms
step:1667/1680 train_time:150333ms step_avg:90.18ms
step:1668/1680 train_time:150423ms step_avg:90.18ms
step:1669/1680 train_time:150514ms step_avg:90.18ms
step:1670/1680 train_time:150605ms step_avg:90.18ms
step:1671/1680 train_time:150696ms step_avg:90.18ms
step:1672/1680 train_time:150787ms step_avg:90.18ms
step:1673/1680 train_time:150879ms step_avg:90.18ms
step:1674/1680 train_time:150971ms step_avg:90.19ms
step:1675/1680 train_time:151062ms step_avg:90.19ms
step:1676/1680 train_time:151152ms step_avg:90.19ms
step:1677/1680 train_time:151243ms step_avg:90.19ms
step:1678/1680 train_time:151333ms step_avg:90.19ms
step:1679/1680 train_time:151423ms step_avg:90.19ms
step:1680/1680 train_time:151514ms step_avg:90.19ms
step:1680/1680 val_loss:3.2778 train_time:151606ms step_avg:90.24ms
peak memory allocated: 31255 MiB reserved: 46494 MiB

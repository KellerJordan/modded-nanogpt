import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i ==7:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections[0]
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i ==4:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2185  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 18 21:10:26 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   36C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   37C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   38C    P0            126W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   36C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   33C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          140095      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          140096      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          140097      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          140098      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          140099      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          140100      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          140101      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          140102      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          140096      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          140097      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          140098      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          140099      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          140100      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          140101      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          140102      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2225 val_loss:10.8258 train_time:0ms step_avg:0.12ms
step:1/2225 train_time:135ms step_avg:134.91ms
step:2/2225 train_time:182ms step_avg:91.09ms
step:3/2225 train_time:205ms step_avg:68.31ms
step:4/2225 train_time:251ms step_avg:62.69ms
step:5/2225 train_time:309ms step_avg:61.86ms
step:6/2225 train_time:371ms step_avg:61.77ms
step:7/2225 train_time:427ms step_avg:61.02ms
step:8/2225 train_time:486ms step_avg:60.70ms
step:9/2225 train_time:546ms step_avg:60.66ms
step:10/2225 train_time:605ms step_avg:60.47ms
step:11/2225 train_time:665ms step_avg:60.43ms
step:12/2225 train_time:723ms step_avg:60.27ms
step:13/2225 train_time:783ms step_avg:60.24ms
step:14/2225 train_time:842ms step_avg:60.13ms
step:15/2225 train_time:902ms step_avg:60.16ms
step:16/2225 train_time:961ms step_avg:60.09ms
step:17/2225 train_time:1023ms step_avg:60.16ms
step:18/2225 train_time:1084ms step_avg:60.20ms
step:19/2225 train_time:1148ms step_avg:60.43ms
step:20/2225 train_time:1209ms step_avg:60.45ms
step:21/2225 train_time:1271ms step_avg:60.52ms
step:22/2225 train_time:1330ms step_avg:60.46ms
step:23/2225 train_time:1391ms step_avg:60.49ms
step:24/2225 train_time:1451ms step_avg:60.46ms
step:25/2225 train_time:1512ms step_avg:60.47ms
step:26/2225 train_time:1571ms step_avg:60.41ms
step:27/2225 train_time:1631ms step_avg:60.42ms
step:28/2225 train_time:1690ms step_avg:60.36ms
step:29/2225 train_time:1751ms step_avg:60.37ms
step:30/2225 train_time:1810ms step_avg:60.33ms
step:31/2225 train_time:1871ms step_avg:60.34ms
step:32/2225 train_time:1930ms step_avg:60.30ms
step:33/2225 train_time:1992ms step_avg:60.35ms
step:34/2225 train_time:2053ms step_avg:60.38ms
step:35/2225 train_time:2116ms step_avg:60.45ms
step:36/2225 train_time:2177ms step_avg:60.48ms
step:37/2225 train_time:2239ms step_avg:60.51ms
step:38/2225 train_time:2300ms step_avg:60.52ms
step:39/2225 train_time:2361ms step_avg:60.53ms
step:40/2225 train_time:2420ms step_avg:60.51ms
step:41/2225 train_time:2481ms step_avg:60.51ms
step:42/2225 train_time:2540ms step_avg:60.48ms
step:43/2225 train_time:2602ms step_avg:60.50ms
step:44/2225 train_time:2661ms step_avg:60.48ms
step:45/2225 train_time:2722ms step_avg:60.49ms
step:46/2225 train_time:2782ms step_avg:60.47ms
step:47/2225 train_time:2842ms step_avg:60.47ms
step:48/2225 train_time:2901ms step_avg:60.45ms
step:49/2225 train_time:2963ms step_avg:60.47ms
step:50/2225 train_time:3023ms step_avg:60.46ms
step:51/2225 train_time:3084ms step_avg:60.47ms
step:52/2225 train_time:3144ms step_avg:60.45ms
step:53/2225 train_time:3205ms step_avg:60.48ms
step:54/2225 train_time:3265ms step_avg:60.46ms
step:55/2225 train_time:3326ms step_avg:60.47ms
step:56/2225 train_time:3385ms step_avg:60.45ms
step:57/2225 train_time:3446ms step_avg:60.46ms
step:58/2225 train_time:3506ms step_avg:60.45ms
step:59/2225 train_time:3567ms step_avg:60.45ms
step:60/2225 train_time:3626ms step_avg:60.43ms
step:61/2225 train_time:3686ms step_avg:60.43ms
step:62/2225 train_time:3746ms step_avg:60.41ms
step:63/2225 train_time:3807ms step_avg:60.42ms
step:64/2225 train_time:3866ms step_avg:60.41ms
step:65/2225 train_time:3927ms step_avg:60.41ms
step:66/2225 train_time:3986ms step_avg:60.40ms
step:67/2225 train_time:4047ms step_avg:60.40ms
step:68/2225 train_time:4106ms step_avg:60.39ms
step:69/2225 train_time:4168ms step_avg:60.40ms
step:70/2225 train_time:4226ms step_avg:60.38ms
step:71/2225 train_time:4287ms step_avg:60.38ms
step:72/2225 train_time:4346ms step_avg:60.36ms
step:73/2225 train_time:4407ms step_avg:60.37ms
step:74/2225 train_time:4466ms step_avg:60.35ms
step:75/2225 train_time:4527ms step_avg:60.36ms
step:76/2225 train_time:4586ms step_avg:60.35ms
step:77/2225 train_time:4648ms step_avg:60.36ms
step:78/2225 train_time:4707ms step_avg:60.34ms
step:79/2225 train_time:4768ms step_avg:60.35ms
step:80/2225 train_time:4827ms step_avg:60.34ms
step:81/2225 train_time:4887ms step_avg:60.34ms
step:82/2225 train_time:4946ms step_avg:60.32ms
step:83/2225 train_time:5007ms step_avg:60.33ms
step:84/2225 train_time:5066ms step_avg:60.31ms
step:85/2225 train_time:5127ms step_avg:60.31ms
step:86/2225 train_time:5185ms step_avg:60.30ms
step:87/2225 train_time:5246ms step_avg:60.30ms
step:88/2225 train_time:5306ms step_avg:60.29ms
step:89/2225 train_time:5367ms step_avg:60.30ms
step:90/2225 train_time:5426ms step_avg:60.29ms
step:91/2225 train_time:5487ms step_avg:60.29ms
step:92/2225 train_time:5546ms step_avg:60.28ms
step:93/2225 train_time:5607ms step_avg:60.29ms
step:94/2225 train_time:5666ms step_avg:60.28ms
step:95/2225 train_time:5727ms step_avg:60.29ms
step:96/2225 train_time:5786ms step_avg:60.27ms
step:97/2225 train_time:5847ms step_avg:60.28ms
step:98/2225 train_time:5906ms step_avg:60.27ms
step:99/2225 train_time:5967ms step_avg:60.27ms
step:100/2225 train_time:6025ms step_avg:60.25ms
step:101/2225 train_time:6086ms step_avg:60.26ms
step:102/2225 train_time:6146ms step_avg:60.25ms
step:103/2225 train_time:6207ms step_avg:60.26ms
step:104/2225 train_time:6266ms step_avg:60.25ms
step:105/2225 train_time:6326ms step_avg:60.25ms
step:106/2225 train_time:6386ms step_avg:60.24ms
step:107/2225 train_time:6447ms step_avg:60.25ms
step:108/2225 train_time:6506ms step_avg:60.24ms
step:109/2225 train_time:6566ms step_avg:60.24ms
step:110/2225 train_time:6625ms step_avg:60.23ms
step:111/2225 train_time:6687ms step_avg:60.24ms
step:112/2225 train_time:6746ms step_avg:60.23ms
step:113/2225 train_time:6807ms step_avg:60.24ms
step:114/2225 train_time:6865ms step_avg:60.22ms
step:115/2225 train_time:6926ms step_avg:60.23ms
step:116/2225 train_time:6985ms step_avg:60.21ms
step:117/2225 train_time:7045ms step_avg:60.21ms
step:118/2225 train_time:7104ms step_avg:60.21ms
step:119/2225 train_time:7165ms step_avg:60.21ms
step:120/2225 train_time:7224ms step_avg:60.20ms
step:121/2225 train_time:7285ms step_avg:60.20ms
step:122/2225 train_time:7344ms step_avg:60.19ms
step:123/2225 train_time:7405ms step_avg:60.21ms
step:124/2225 train_time:7465ms step_avg:60.20ms
step:125/2225 train_time:7525ms step_avg:60.20ms
step:126/2225 train_time:7584ms step_avg:60.19ms
step:127/2225 train_time:7644ms step_avg:60.19ms
step:128/2225 train_time:7703ms step_avg:60.18ms
step:129/2225 train_time:7765ms step_avg:60.19ms
step:130/2225 train_time:7824ms step_avg:60.19ms
step:131/2225 train_time:7884ms step_avg:60.18ms
step:132/2225 train_time:7943ms step_avg:60.17ms
step:133/2225 train_time:8004ms step_avg:60.18ms
step:134/2225 train_time:8063ms step_avg:60.17ms
step:135/2225 train_time:8123ms step_avg:60.17ms
step:136/2225 train_time:8182ms step_avg:60.16ms
step:137/2225 train_time:8243ms step_avg:60.17ms
step:138/2225 train_time:8303ms step_avg:60.16ms
step:139/2225 train_time:8363ms step_avg:60.17ms
step:140/2225 train_time:8422ms step_avg:60.16ms
step:141/2225 train_time:8483ms step_avg:60.16ms
step:142/2225 train_time:8542ms step_avg:60.16ms
step:143/2225 train_time:8603ms step_avg:60.16ms
step:144/2225 train_time:8663ms step_avg:60.16ms
step:145/2225 train_time:8723ms step_avg:60.16ms
step:146/2225 train_time:8783ms step_avg:60.16ms
step:147/2225 train_time:8843ms step_avg:60.16ms
step:148/2225 train_time:8902ms step_avg:60.15ms
step:149/2225 train_time:8962ms step_avg:60.15ms
step:150/2225 train_time:9021ms step_avg:60.14ms
step:151/2225 train_time:9081ms step_avg:60.14ms
step:152/2225 train_time:9140ms step_avg:60.13ms
step:153/2225 train_time:9201ms step_avg:60.14ms
step:154/2225 train_time:9260ms step_avg:60.13ms
step:155/2225 train_time:9321ms step_avg:60.14ms
step:156/2225 train_time:9381ms step_avg:60.13ms
step:157/2225 train_time:9441ms step_avg:60.14ms
step:158/2225 train_time:9501ms step_avg:60.13ms
step:159/2225 train_time:9562ms step_avg:60.14ms
step:160/2225 train_time:9621ms step_avg:60.13ms
step:161/2225 train_time:9682ms step_avg:60.14ms
step:162/2225 train_time:9741ms step_avg:60.13ms
step:163/2225 train_time:9803ms step_avg:60.14ms
step:164/2225 train_time:9863ms step_avg:60.14ms
step:165/2225 train_time:9923ms step_avg:60.14ms
step:166/2225 train_time:9982ms step_avg:60.13ms
step:167/2225 train_time:10042ms step_avg:60.13ms
step:168/2225 train_time:10101ms step_avg:60.13ms
step:169/2225 train_time:10162ms step_avg:60.13ms
step:170/2225 train_time:10221ms step_avg:60.12ms
step:171/2225 train_time:10281ms step_avg:60.12ms
step:172/2225 train_time:10340ms step_avg:60.12ms
step:173/2225 train_time:10401ms step_avg:60.12ms
step:174/2225 train_time:10461ms step_avg:60.12ms
step:175/2225 train_time:10521ms step_avg:60.12ms
step:176/2225 train_time:10580ms step_avg:60.11ms
step:177/2225 train_time:10641ms step_avg:60.12ms
step:178/2225 train_time:10701ms step_avg:60.12ms
step:179/2225 train_time:10762ms step_avg:60.13ms
step:180/2225 train_time:10822ms step_avg:60.12ms
step:181/2225 train_time:10882ms step_avg:60.12ms
step:182/2225 train_time:10941ms step_avg:60.12ms
step:183/2225 train_time:11002ms step_avg:60.12ms
step:184/2225 train_time:11061ms step_avg:60.11ms
step:185/2225 train_time:11121ms step_avg:60.11ms
step:186/2225 train_time:11180ms step_avg:60.11ms
step:187/2225 train_time:11240ms step_avg:60.11ms
step:188/2225 train_time:11299ms step_avg:60.10ms
step:189/2225 train_time:11360ms step_avg:60.11ms
step:190/2225 train_time:11419ms step_avg:60.10ms
step:191/2225 train_time:11480ms step_avg:60.11ms
step:192/2225 train_time:11539ms step_avg:60.10ms
step:193/2225 train_time:11600ms step_avg:60.10ms
step:194/2225 train_time:11659ms step_avg:60.10ms
step:195/2225 train_time:11720ms step_avg:60.10ms
step:196/2225 train_time:11779ms step_avg:60.10ms
step:197/2225 train_time:11840ms step_avg:60.10ms
step:198/2225 train_time:11900ms step_avg:60.10ms
step:199/2225 train_time:11960ms step_avg:60.10ms
step:200/2225 train_time:12020ms step_avg:60.10ms
step:201/2225 train_time:12081ms step_avg:60.10ms
step:202/2225 train_time:12140ms step_avg:60.10ms
step:203/2225 train_time:12201ms step_avg:60.10ms
step:204/2225 train_time:12260ms step_avg:60.10ms
step:205/2225 train_time:12321ms step_avg:60.10ms
step:206/2225 train_time:12379ms step_avg:60.09ms
step:207/2225 train_time:12440ms step_avg:60.10ms
step:208/2225 train_time:12500ms step_avg:60.10ms
step:209/2225 train_time:12561ms step_avg:60.10ms
step:210/2225 train_time:12620ms step_avg:60.10ms
step:211/2225 train_time:12680ms step_avg:60.10ms
step:212/2225 train_time:12740ms step_avg:60.09ms
step:213/2225 train_time:12801ms step_avg:60.10ms
step:214/2225 train_time:12861ms step_avg:60.10ms
step:215/2225 train_time:12921ms step_avg:60.10ms
step:216/2225 train_time:12980ms step_avg:60.09ms
step:217/2225 train_time:13041ms step_avg:60.10ms
step:218/2225 train_time:13100ms step_avg:60.09ms
step:219/2225 train_time:13161ms step_avg:60.10ms
step:220/2225 train_time:13220ms step_avg:60.09ms
step:221/2225 train_time:13281ms step_avg:60.09ms
step:222/2225 train_time:13340ms step_avg:60.09ms
step:223/2225 train_time:13400ms step_avg:60.09ms
step:224/2225 train_time:13459ms step_avg:60.09ms
step:225/2225 train_time:13520ms step_avg:60.09ms
step:226/2225 train_time:13579ms step_avg:60.08ms
step:227/2225 train_time:13640ms step_avg:60.09ms
step:228/2225 train_time:13700ms step_avg:60.09ms
step:229/2225 train_time:13761ms step_avg:60.09ms
step:230/2225 train_time:13820ms step_avg:60.09ms
step:231/2225 train_time:13880ms step_avg:60.09ms
step:232/2225 train_time:13940ms step_avg:60.08ms
step:233/2225 train_time:14001ms step_avg:60.09ms
step:234/2225 train_time:14060ms step_avg:60.09ms
step:235/2225 train_time:14121ms step_avg:60.09ms
step:236/2225 train_time:14180ms step_avg:60.09ms
step:237/2225 train_time:14241ms step_avg:60.09ms
step:238/2225 train_time:14301ms step_avg:60.09ms
step:239/2225 train_time:14361ms step_avg:60.09ms
step:240/2225 train_time:14420ms step_avg:60.09ms
step:241/2225 train_time:14481ms step_avg:60.09ms
step:242/2225 train_time:14540ms step_avg:60.08ms
step:243/2225 train_time:14600ms step_avg:60.08ms
step:244/2225 train_time:14660ms step_avg:60.08ms
step:245/2225 train_time:14720ms step_avg:60.08ms
step:246/2225 train_time:14780ms step_avg:60.08ms
step:247/2225 train_time:14841ms step_avg:60.08ms
step:248/2225 train_time:14900ms step_avg:60.08ms
step:249/2225 train_time:14961ms step_avg:60.08ms
step:250/2225 train_time:15020ms step_avg:60.08ms
step:250/2225 val_loss:4.0780 train_time:15081ms step_avg:60.32ms
step:251/2225 train_time:15106ms step_avg:60.18ms
step:252/2225 train_time:15141ms step_avg:60.08ms
step:253/2225 train_time:15204ms step_avg:60.09ms
step:254/2225 train_time:15268ms step_avg:60.11ms
step:255/2225 train_time:15333ms step_avg:60.13ms
step:256/2225 train_time:15393ms step_avg:60.13ms
step:257/2225 train_time:15454ms step_avg:60.13ms
step:258/2225 train_time:15512ms step_avg:60.13ms
step:259/2225 train_time:15573ms step_avg:60.13ms
step:260/2225 train_time:15631ms step_avg:60.12ms
step:261/2225 train_time:15691ms step_avg:60.12ms
step:262/2225 train_time:15749ms step_avg:60.11ms
step:263/2225 train_time:15808ms step_avg:60.11ms
step:264/2225 train_time:15866ms step_avg:60.10ms
step:265/2225 train_time:15926ms step_avg:60.10ms
step:266/2225 train_time:15985ms step_avg:60.09ms
step:267/2225 train_time:16045ms step_avg:60.09ms
step:268/2225 train_time:16104ms step_avg:60.09ms
step:269/2225 train_time:16165ms step_avg:60.09ms
step:270/2225 train_time:16224ms step_avg:60.09ms
step:271/2225 train_time:16286ms step_avg:60.10ms
step:272/2225 train_time:16346ms step_avg:60.09ms
step:273/2225 train_time:16407ms step_avg:60.10ms
step:274/2225 train_time:16466ms step_avg:60.10ms
step:275/2225 train_time:16527ms step_avg:60.10ms
step:276/2225 train_time:16586ms step_avg:60.09ms
step:277/2225 train_time:16645ms step_avg:60.09ms
step:278/2225 train_time:16705ms step_avg:60.09ms
step:279/2225 train_time:16764ms step_avg:60.09ms
step:280/2225 train_time:16823ms step_avg:60.08ms
step:281/2225 train_time:16883ms step_avg:60.08ms
step:282/2225 train_time:16942ms step_avg:60.08ms
step:283/2225 train_time:17002ms step_avg:60.08ms
step:284/2225 train_time:17060ms step_avg:60.07ms
step:285/2225 train_time:17121ms step_avg:60.07ms
step:286/2225 train_time:17181ms step_avg:60.07ms
step:287/2225 train_time:17242ms step_avg:60.08ms
step:288/2225 train_time:17302ms step_avg:60.08ms
step:289/2225 train_time:17364ms step_avg:60.08ms
step:290/2225 train_time:17424ms step_avg:60.08ms
step:291/2225 train_time:17485ms step_avg:60.08ms
step:292/2225 train_time:17544ms step_avg:60.08ms
step:293/2225 train_time:17604ms step_avg:60.08ms
step:294/2225 train_time:17663ms step_avg:60.08ms
step:295/2225 train_time:17723ms step_avg:60.08ms
step:296/2225 train_time:17782ms step_avg:60.07ms
step:297/2225 train_time:17842ms step_avg:60.07ms
step:298/2225 train_time:17900ms step_avg:60.07ms
step:299/2225 train_time:17960ms step_avg:60.07ms
step:300/2225 train_time:18019ms step_avg:60.06ms
step:301/2225 train_time:18079ms step_avg:60.06ms
step:302/2225 train_time:18139ms step_avg:60.06ms
step:303/2225 train_time:18200ms step_avg:60.07ms
step:304/2225 train_time:18260ms step_avg:60.06ms
step:305/2225 train_time:18322ms step_avg:60.07ms
step:306/2225 train_time:18382ms step_avg:60.07ms
step:307/2225 train_time:18443ms step_avg:60.08ms
step:308/2225 train_time:18503ms step_avg:60.07ms
step:309/2225 train_time:18563ms step_avg:60.08ms
step:310/2225 train_time:18622ms step_avg:60.07ms
step:311/2225 train_time:18683ms step_avg:60.07ms
step:312/2225 train_time:18741ms step_avg:60.07ms
step:313/2225 train_time:18801ms step_avg:60.07ms
step:314/2225 train_time:18860ms step_avg:60.06ms
step:315/2225 train_time:18921ms step_avg:60.07ms
step:316/2225 train_time:18980ms step_avg:60.06ms
step:317/2225 train_time:19041ms step_avg:60.06ms
step:318/2225 train_time:19099ms step_avg:60.06ms
step:319/2225 train_time:19160ms step_avg:60.06ms
step:320/2225 train_time:19219ms step_avg:60.06ms
step:321/2225 train_time:19280ms step_avg:60.06ms
step:322/2225 train_time:19339ms step_avg:60.06ms
step:323/2225 train_time:19400ms step_avg:60.06ms
step:324/2225 train_time:19459ms step_avg:60.06ms
step:325/2225 train_time:19521ms step_avg:60.07ms
step:326/2225 train_time:19580ms step_avg:60.06ms
step:327/2225 train_time:19641ms step_avg:60.06ms
step:328/2225 train_time:19700ms step_avg:60.06ms
step:329/2225 train_time:19760ms step_avg:60.06ms
step:330/2225 train_time:19819ms step_avg:60.06ms
step:331/2225 train_time:19879ms step_avg:60.06ms
step:332/2225 train_time:19938ms step_avg:60.06ms
step:333/2225 train_time:19999ms step_avg:60.06ms
step:334/2225 train_time:20058ms step_avg:60.05ms
step:335/2225 train_time:20118ms step_avg:60.05ms
step:336/2225 train_time:20177ms step_avg:60.05ms
step:337/2225 train_time:20239ms step_avg:60.06ms
step:338/2225 train_time:20297ms step_avg:60.05ms
step:339/2225 train_time:20359ms step_avg:60.06ms
step:340/2225 train_time:20419ms step_avg:60.06ms
step:341/2225 train_time:20480ms step_avg:60.06ms
step:342/2225 train_time:20540ms step_avg:60.06ms
step:343/2225 train_time:20601ms step_avg:60.06ms
step:344/2225 train_time:20660ms step_avg:60.06ms
step:345/2225 train_time:20721ms step_avg:60.06ms
step:346/2225 train_time:20779ms step_avg:60.05ms
step:347/2225 train_time:20839ms step_avg:60.05ms
step:348/2225 train_time:20898ms step_avg:60.05ms
step:349/2225 train_time:20958ms step_avg:60.05ms
step:350/2225 train_time:21017ms step_avg:60.05ms
step:351/2225 train_time:21077ms step_avg:60.05ms
step:352/2225 train_time:21137ms step_avg:60.05ms
step:353/2225 train_time:21197ms step_avg:60.05ms
step:354/2225 train_time:21256ms step_avg:60.05ms
step:355/2225 train_time:21318ms step_avg:60.05ms
step:356/2225 train_time:21377ms step_avg:60.05ms
step:357/2225 train_time:21438ms step_avg:60.05ms
step:358/2225 train_time:21497ms step_avg:60.05ms
step:359/2225 train_time:21559ms step_avg:60.05ms
step:360/2225 train_time:21619ms step_avg:60.05ms
step:361/2225 train_time:21680ms step_avg:60.05ms
step:362/2225 train_time:21739ms step_avg:60.05ms
step:363/2225 train_time:21799ms step_avg:60.05ms
step:364/2225 train_time:21858ms step_avg:60.05ms
step:365/2225 train_time:21918ms step_avg:60.05ms
step:366/2225 train_time:21976ms step_avg:60.04ms
step:367/2225 train_time:22037ms step_avg:60.05ms
step:368/2225 train_time:22096ms step_avg:60.04ms
step:369/2225 train_time:22156ms step_avg:60.04ms
step:370/2225 train_time:22215ms step_avg:60.04ms
step:371/2225 train_time:22275ms step_avg:60.04ms
step:372/2225 train_time:22336ms step_avg:60.04ms
step:373/2225 train_time:22396ms step_avg:60.04ms
step:374/2225 train_time:22456ms step_avg:60.04ms
step:375/2225 train_time:22517ms step_avg:60.04ms
step:376/2225 train_time:22576ms step_avg:60.04ms
step:377/2225 train_time:22637ms step_avg:60.04ms
step:378/2225 train_time:22696ms step_avg:60.04ms
step:379/2225 train_time:22757ms step_avg:60.05ms
step:380/2225 train_time:22816ms step_avg:60.04ms
step:381/2225 train_time:22877ms step_avg:60.05ms
step:382/2225 train_time:22936ms step_avg:60.04ms
step:383/2225 train_time:22996ms step_avg:60.04ms
step:384/2225 train_time:23054ms step_avg:60.04ms
step:385/2225 train_time:23115ms step_avg:60.04ms
step:386/2225 train_time:23174ms step_avg:60.04ms
step:387/2225 train_time:23235ms step_avg:60.04ms
step:388/2225 train_time:23295ms step_avg:60.04ms
step:389/2225 train_time:23356ms step_avg:60.04ms
step:390/2225 train_time:23416ms step_avg:60.04ms
step:391/2225 train_time:23477ms step_avg:60.04ms
step:392/2225 train_time:23537ms step_avg:60.04ms
step:393/2225 train_time:23598ms step_avg:60.04ms
step:394/2225 train_time:23657ms step_avg:60.04ms
step:395/2225 train_time:23718ms step_avg:60.05ms
step:396/2225 train_time:23777ms step_avg:60.04ms
step:397/2225 train_time:23838ms step_avg:60.05ms
step:398/2225 train_time:23897ms step_avg:60.04ms
step:399/2225 train_time:23958ms step_avg:60.04ms
step:400/2225 train_time:24017ms step_avg:60.04ms
step:401/2225 train_time:24077ms step_avg:60.04ms
step:402/2225 train_time:24136ms step_avg:60.04ms
step:403/2225 train_time:24197ms step_avg:60.04ms
step:404/2225 train_time:24256ms step_avg:60.04ms
step:405/2225 train_time:24317ms step_avg:60.04ms
step:406/2225 train_time:24376ms step_avg:60.04ms
step:407/2225 train_time:24438ms step_avg:60.04ms
step:408/2225 train_time:24497ms step_avg:60.04ms
step:409/2225 train_time:24558ms step_avg:60.04ms
step:410/2225 train_time:24617ms step_avg:60.04ms
step:411/2225 train_time:24678ms step_avg:60.04ms
step:412/2225 train_time:24737ms step_avg:60.04ms
step:413/2225 train_time:24798ms step_avg:60.04ms
step:414/2225 train_time:24857ms step_avg:60.04ms
step:415/2225 train_time:24918ms step_avg:60.04ms
step:416/2225 train_time:24977ms step_avg:60.04ms
step:417/2225 train_time:25038ms step_avg:60.04ms
step:418/2225 train_time:25096ms step_avg:60.04ms
step:419/2225 train_time:25156ms step_avg:60.04ms
step:420/2225 train_time:25216ms step_avg:60.04ms
step:421/2225 train_time:25276ms step_avg:60.04ms
step:422/2225 train_time:25335ms step_avg:60.04ms
step:423/2225 train_time:25395ms step_avg:60.04ms
step:424/2225 train_time:25455ms step_avg:60.03ms
step:425/2225 train_time:25515ms step_avg:60.04ms
step:426/2225 train_time:25575ms step_avg:60.04ms
step:427/2225 train_time:25636ms step_avg:60.04ms
step:428/2225 train_time:25695ms step_avg:60.04ms
step:429/2225 train_time:25756ms step_avg:60.04ms
step:430/2225 train_time:25815ms step_avg:60.04ms
step:431/2225 train_time:25876ms step_avg:60.04ms
step:432/2225 train_time:25935ms step_avg:60.03ms
step:433/2225 train_time:25996ms step_avg:60.04ms
step:434/2225 train_time:26054ms step_avg:60.03ms
step:435/2225 train_time:26115ms step_avg:60.04ms
step:436/2225 train_time:26174ms step_avg:60.03ms
step:437/2225 train_time:26234ms step_avg:60.03ms
step:438/2225 train_time:26293ms step_avg:60.03ms
step:439/2225 train_time:26354ms step_avg:60.03ms
step:440/2225 train_time:26412ms step_avg:60.03ms
step:441/2225 train_time:26473ms step_avg:60.03ms
step:442/2225 train_time:26532ms step_avg:60.03ms
step:443/2225 train_time:26593ms step_avg:60.03ms
step:444/2225 train_time:26651ms step_avg:60.03ms
step:445/2225 train_time:26712ms step_avg:60.03ms
step:446/2225 train_time:26771ms step_avg:60.02ms
step:447/2225 train_time:26832ms step_avg:60.03ms
step:448/2225 train_time:26890ms step_avg:60.02ms
step:449/2225 train_time:26950ms step_avg:60.02ms
step:450/2225 train_time:27008ms step_avg:60.02ms
step:451/2225 train_time:27069ms step_avg:60.02ms
step:452/2225 train_time:27127ms step_avg:60.02ms
step:453/2225 train_time:27188ms step_avg:60.02ms
step:454/2225 train_time:27247ms step_avg:60.02ms
step:455/2225 train_time:27307ms step_avg:60.02ms
step:456/2225 train_time:27366ms step_avg:60.01ms
step:457/2225 train_time:27426ms step_avg:60.01ms
step:458/2225 train_time:27486ms step_avg:60.01ms
step:459/2225 train_time:27546ms step_avg:60.01ms
step:460/2225 train_time:27605ms step_avg:60.01ms
step:461/2225 train_time:27665ms step_avg:60.01ms
step:462/2225 train_time:27724ms step_avg:60.01ms
step:463/2225 train_time:27784ms step_avg:60.01ms
step:464/2225 train_time:27843ms step_avg:60.01ms
step:465/2225 train_time:27903ms step_avg:60.01ms
step:466/2225 train_time:27961ms step_avg:60.00ms
step:467/2225 train_time:28022ms step_avg:60.01ms
step:468/2225 train_time:28082ms step_avg:60.00ms
step:469/2225 train_time:28143ms step_avg:60.01ms
step:470/2225 train_time:28201ms step_avg:60.00ms
step:471/2225 train_time:28262ms step_avg:60.00ms
step:472/2225 train_time:28321ms step_avg:60.00ms
step:473/2225 train_time:28382ms step_avg:60.00ms
step:474/2225 train_time:28442ms step_avg:60.00ms
step:475/2225 train_time:28502ms step_avg:60.00ms
step:476/2225 train_time:28561ms step_avg:60.00ms
step:477/2225 train_time:28621ms step_avg:60.00ms
step:478/2225 train_time:28680ms step_avg:60.00ms
step:479/2225 train_time:28740ms step_avg:60.00ms
step:480/2225 train_time:28799ms step_avg:60.00ms
step:481/2225 train_time:28860ms step_avg:60.00ms
step:482/2225 train_time:28919ms step_avg:60.00ms
step:483/2225 train_time:28980ms step_avg:60.00ms
step:484/2225 train_time:29038ms step_avg:60.00ms
step:485/2225 train_time:29099ms step_avg:60.00ms
step:486/2225 train_time:29158ms step_avg:60.00ms
step:487/2225 train_time:29220ms step_avg:60.00ms
step:488/2225 train_time:29279ms step_avg:60.00ms
step:489/2225 train_time:29340ms step_avg:60.00ms
step:490/2225 train_time:29399ms step_avg:60.00ms
step:491/2225 train_time:29459ms step_avg:60.00ms
step:492/2225 train_time:29518ms step_avg:60.00ms
step:493/2225 train_time:29579ms step_avg:60.00ms
step:494/2225 train_time:29638ms step_avg:60.00ms
step:495/2225 train_time:29698ms step_avg:60.00ms
step:496/2225 train_time:29757ms step_avg:59.99ms
step:497/2225 train_time:29818ms step_avg:60.00ms
step:498/2225 train_time:29878ms step_avg:60.00ms
step:499/2225 train_time:29939ms step_avg:60.00ms
step:500/2225 train_time:29997ms step_avg:59.99ms
step:500/2225 val_loss:3.8259 train_time:30058ms step_avg:60.12ms
step:501/2225 train_time:30081ms step_avg:60.04ms
step:502/2225 train_time:30118ms step_avg:60.00ms
step:503/2225 train_time:30181ms step_avg:60.00ms
step:504/2225 train_time:30243ms step_avg:60.00ms
step:505/2225 train_time:30304ms step_avg:60.01ms
step:506/2225 train_time:30363ms step_avg:60.01ms
step:507/2225 train_time:30423ms step_avg:60.01ms
step:508/2225 train_time:30481ms step_avg:60.00ms
step:509/2225 train_time:30541ms step_avg:60.00ms
step:510/2225 train_time:30600ms step_avg:60.00ms
step:511/2225 train_time:30660ms step_avg:60.00ms
step:512/2225 train_time:30718ms step_avg:60.00ms
step:513/2225 train_time:30777ms step_avg:59.99ms
step:514/2225 train_time:30835ms step_avg:59.99ms
step:515/2225 train_time:30895ms step_avg:59.99ms
step:516/2225 train_time:30955ms step_avg:59.99ms
step:517/2225 train_time:31017ms step_avg:59.99ms
step:518/2225 train_time:31077ms step_avg:59.99ms
step:519/2225 train_time:31138ms step_avg:60.00ms
step:520/2225 train_time:31198ms step_avg:60.00ms
step:521/2225 train_time:31259ms step_avg:60.00ms
step:522/2225 train_time:31320ms step_avg:60.00ms
step:523/2225 train_time:31381ms step_avg:60.00ms
step:524/2225 train_time:31440ms step_avg:60.00ms
step:525/2225 train_time:31500ms step_avg:60.00ms
step:526/2225 train_time:31559ms step_avg:60.00ms
step:527/2225 train_time:31619ms step_avg:60.00ms
step:528/2225 train_time:31678ms step_avg:60.00ms
step:529/2225 train_time:31737ms step_avg:59.99ms
step:530/2225 train_time:31795ms step_avg:59.99ms
step:531/2225 train_time:31855ms step_avg:59.99ms
step:532/2225 train_time:31914ms step_avg:59.99ms
step:533/2225 train_time:31974ms step_avg:59.99ms
step:534/2225 train_time:32034ms step_avg:59.99ms
step:535/2225 train_time:32095ms step_avg:59.99ms
step:536/2225 train_time:32155ms step_avg:59.99ms
step:537/2225 train_time:32217ms step_avg:59.99ms
step:538/2225 train_time:32277ms step_avg:59.99ms
step:539/2225 train_time:32338ms step_avg:60.00ms
step:540/2225 train_time:32397ms step_avg:59.99ms
step:541/2225 train_time:32458ms step_avg:60.00ms
step:542/2225 train_time:32517ms step_avg:59.99ms
step:543/2225 train_time:32577ms step_avg:59.99ms
step:544/2225 train_time:32635ms step_avg:59.99ms
step:545/2225 train_time:32695ms step_avg:59.99ms
step:546/2225 train_time:32754ms step_avg:59.99ms
step:547/2225 train_time:32814ms step_avg:59.99ms
step:548/2225 train_time:32873ms step_avg:59.99ms
step:549/2225 train_time:32934ms step_avg:59.99ms
step:550/2225 train_time:32993ms step_avg:59.99ms
step:551/2225 train_time:33055ms step_avg:59.99ms
step:552/2225 train_time:33115ms step_avg:59.99ms
step:553/2225 train_time:33176ms step_avg:59.99ms
step:554/2225 train_time:33236ms step_avg:59.99ms
step:555/2225 train_time:33297ms step_avg:60.00ms
step:556/2225 train_time:33358ms step_avg:60.00ms
step:557/2225 train_time:33418ms step_avg:60.00ms
step:558/2225 train_time:33477ms step_avg:59.99ms
step:559/2225 train_time:33537ms step_avg:59.99ms
step:560/2225 train_time:33596ms step_avg:59.99ms
step:561/2225 train_time:33656ms step_avg:59.99ms
step:562/2225 train_time:33715ms step_avg:59.99ms
step:563/2225 train_time:33774ms step_avg:59.99ms
step:564/2225 train_time:33833ms step_avg:59.99ms
step:565/2225 train_time:33893ms step_avg:59.99ms
step:566/2225 train_time:33952ms step_avg:59.99ms
step:567/2225 train_time:34013ms step_avg:59.99ms
step:568/2225 train_time:34073ms step_avg:59.99ms
step:569/2225 train_time:34134ms step_avg:59.99ms
step:570/2225 train_time:34194ms step_avg:59.99ms
step:571/2225 train_time:34257ms step_avg:59.99ms
step:572/2225 train_time:34316ms step_avg:59.99ms
step:573/2225 train_time:34377ms step_avg:60.00ms
step:574/2225 train_time:34436ms step_avg:59.99ms
step:575/2225 train_time:34496ms step_avg:59.99ms
step:576/2225 train_time:34555ms step_avg:59.99ms
step:577/2225 train_time:34616ms step_avg:59.99ms
step:578/2225 train_time:34675ms step_avg:59.99ms
step:579/2225 train_time:34735ms step_avg:59.99ms
step:580/2225 train_time:34794ms step_avg:59.99ms
step:581/2225 train_time:34855ms step_avg:59.99ms
step:582/2225 train_time:34914ms step_avg:59.99ms
step:583/2225 train_time:34974ms step_avg:59.99ms
step:584/2225 train_time:35033ms step_avg:59.99ms
step:585/2225 train_time:35094ms step_avg:59.99ms
step:586/2225 train_time:35154ms step_avg:59.99ms
step:587/2225 train_time:35216ms step_avg:59.99ms
step:588/2225 train_time:35275ms step_avg:59.99ms
step:589/2225 train_time:35335ms step_avg:59.99ms
step:590/2225 train_time:35395ms step_avg:59.99ms
step:591/2225 train_time:35456ms step_avg:59.99ms
step:592/2225 train_time:35515ms step_avg:59.99ms
step:593/2225 train_time:35575ms step_avg:59.99ms
step:594/2225 train_time:35634ms step_avg:59.99ms
step:595/2225 train_time:35695ms step_avg:59.99ms
step:596/2225 train_time:35754ms step_avg:59.99ms
step:597/2225 train_time:35815ms step_avg:59.99ms
step:598/2225 train_time:35874ms step_avg:59.99ms
step:599/2225 train_time:35934ms step_avg:59.99ms
step:600/2225 train_time:35993ms step_avg:59.99ms
step:601/2225 train_time:36054ms step_avg:59.99ms
step:602/2225 train_time:36113ms step_avg:59.99ms
step:603/2225 train_time:36174ms step_avg:59.99ms
step:604/2225 train_time:36234ms step_avg:59.99ms
step:605/2225 train_time:36295ms step_avg:59.99ms
step:606/2225 train_time:36354ms step_avg:59.99ms
step:607/2225 train_time:36415ms step_avg:59.99ms
step:608/2225 train_time:36475ms step_avg:59.99ms
step:609/2225 train_time:36535ms step_avg:59.99ms
step:610/2225 train_time:36595ms step_avg:59.99ms
step:611/2225 train_time:36656ms step_avg:59.99ms
step:612/2225 train_time:36715ms step_avg:59.99ms
step:613/2225 train_time:36775ms step_avg:59.99ms
step:614/2225 train_time:36834ms step_avg:59.99ms
step:615/2225 train_time:36894ms step_avg:59.99ms
step:616/2225 train_time:36953ms step_avg:59.99ms
step:617/2225 train_time:37014ms step_avg:59.99ms
step:618/2225 train_time:37073ms step_avg:59.99ms
step:619/2225 train_time:37133ms step_avg:59.99ms
step:620/2225 train_time:37193ms step_avg:59.99ms
step:621/2225 train_time:37255ms step_avg:59.99ms
step:622/2225 train_time:37315ms step_avg:59.99ms
step:623/2225 train_time:37376ms step_avg:59.99ms
step:624/2225 train_time:37435ms step_avg:59.99ms
step:625/2225 train_time:37496ms step_avg:59.99ms
step:626/2225 train_time:37556ms step_avg:59.99ms
step:627/2225 train_time:37616ms step_avg:59.99ms
step:628/2225 train_time:37675ms step_avg:59.99ms
step:629/2225 train_time:37735ms step_avg:59.99ms
step:630/2225 train_time:37794ms step_avg:59.99ms
step:631/2225 train_time:37855ms step_avg:59.99ms
step:632/2225 train_time:37915ms step_avg:59.99ms
step:633/2225 train_time:37976ms step_avg:59.99ms
step:634/2225 train_time:38035ms step_avg:59.99ms
step:635/2225 train_time:38095ms step_avg:59.99ms
step:636/2225 train_time:38155ms step_avg:59.99ms
step:637/2225 train_time:38215ms step_avg:59.99ms
step:638/2225 train_time:38275ms step_avg:59.99ms
step:639/2225 train_time:38336ms step_avg:59.99ms
step:640/2225 train_time:38395ms step_avg:59.99ms
step:641/2225 train_time:38457ms step_avg:60.00ms
step:642/2225 train_time:38516ms step_avg:59.99ms
step:643/2225 train_time:38576ms step_avg:59.99ms
step:644/2225 train_time:38635ms step_avg:59.99ms
step:645/2225 train_time:38696ms step_avg:59.99ms
step:646/2225 train_time:38756ms step_avg:59.99ms
step:647/2225 train_time:38816ms step_avg:59.99ms
step:648/2225 train_time:38875ms step_avg:59.99ms
step:649/2225 train_time:38935ms step_avg:59.99ms
step:650/2225 train_time:38994ms step_avg:59.99ms
step:651/2225 train_time:39056ms step_avg:59.99ms
step:652/2225 train_time:39115ms step_avg:59.99ms
step:653/2225 train_time:39176ms step_avg:59.99ms
step:654/2225 train_time:39236ms step_avg:59.99ms
step:655/2225 train_time:39297ms step_avg:60.00ms
step:656/2225 train_time:39357ms step_avg:59.99ms
step:657/2225 train_time:39418ms step_avg:60.00ms
step:658/2225 train_time:39476ms step_avg:59.99ms
step:659/2225 train_time:39536ms step_avg:59.99ms
step:660/2225 train_time:39596ms step_avg:59.99ms
step:661/2225 train_time:39657ms step_avg:59.99ms
step:662/2225 train_time:39716ms step_avg:59.99ms
step:663/2225 train_time:39776ms step_avg:59.99ms
step:664/2225 train_time:39835ms step_avg:59.99ms
step:665/2225 train_time:39895ms step_avg:59.99ms
step:666/2225 train_time:39954ms step_avg:59.99ms
step:667/2225 train_time:40015ms step_avg:59.99ms
step:668/2225 train_time:40073ms step_avg:59.99ms
step:669/2225 train_time:40134ms step_avg:59.99ms
step:670/2225 train_time:40193ms step_avg:59.99ms
step:671/2225 train_time:40254ms step_avg:59.99ms
step:672/2225 train_time:40314ms step_avg:59.99ms
step:673/2225 train_time:40375ms step_avg:59.99ms
step:674/2225 train_time:40434ms step_avg:59.99ms
step:675/2225 train_time:40495ms step_avg:59.99ms
step:676/2225 train_time:40554ms step_avg:59.99ms
step:677/2225 train_time:40615ms step_avg:59.99ms
step:678/2225 train_time:40674ms step_avg:59.99ms
step:679/2225 train_time:40734ms step_avg:59.99ms
step:680/2225 train_time:40793ms step_avg:59.99ms
step:681/2225 train_time:40854ms step_avg:59.99ms
step:682/2225 train_time:40913ms step_avg:59.99ms
step:683/2225 train_time:40974ms step_avg:59.99ms
step:684/2225 train_time:41033ms step_avg:59.99ms
step:685/2225 train_time:41094ms step_avg:59.99ms
step:686/2225 train_time:41154ms step_avg:59.99ms
step:687/2225 train_time:41215ms step_avg:59.99ms
step:688/2225 train_time:41274ms step_avg:59.99ms
step:689/2225 train_time:41335ms step_avg:59.99ms
step:690/2225 train_time:41395ms step_avg:59.99ms
step:691/2225 train_time:41456ms step_avg:59.99ms
step:692/2225 train_time:41516ms step_avg:59.99ms
step:693/2225 train_time:41577ms step_avg:59.99ms
step:694/2225 train_time:41635ms step_avg:59.99ms
step:695/2225 train_time:41696ms step_avg:59.99ms
step:696/2225 train_time:41755ms step_avg:59.99ms
step:697/2225 train_time:41815ms step_avg:59.99ms
step:698/2225 train_time:41874ms step_avg:59.99ms
step:699/2225 train_time:41934ms step_avg:59.99ms
step:700/2225 train_time:41994ms step_avg:59.99ms
step:701/2225 train_time:42055ms step_avg:59.99ms
step:702/2225 train_time:42115ms step_avg:59.99ms
step:703/2225 train_time:42176ms step_avg:59.99ms
step:704/2225 train_time:42235ms step_avg:59.99ms
step:705/2225 train_time:42296ms step_avg:59.99ms
step:706/2225 train_time:42356ms step_avg:59.99ms
step:707/2225 train_time:42417ms step_avg:60.00ms
step:708/2225 train_time:42476ms step_avg:59.99ms
step:709/2225 train_time:42537ms step_avg:60.00ms
step:710/2225 train_time:42596ms step_avg:59.99ms
step:711/2225 train_time:42657ms step_avg:60.00ms
step:712/2225 train_time:42717ms step_avg:60.00ms
step:713/2225 train_time:42777ms step_avg:60.00ms
step:714/2225 train_time:42835ms step_avg:59.99ms
step:715/2225 train_time:42896ms step_avg:59.99ms
step:716/2225 train_time:42955ms step_avg:59.99ms
step:717/2225 train_time:43016ms step_avg:59.99ms
step:718/2225 train_time:43076ms step_avg:59.99ms
step:719/2225 train_time:43136ms step_avg:59.99ms
step:720/2225 train_time:43195ms step_avg:59.99ms
step:721/2225 train_time:43256ms step_avg:60.00ms
step:722/2225 train_time:43316ms step_avg:59.99ms
step:723/2225 train_time:43376ms step_avg:59.99ms
step:724/2225 train_time:43435ms step_avg:59.99ms
step:725/2225 train_time:43495ms step_avg:59.99ms
step:726/2225 train_time:43555ms step_avg:59.99ms
step:727/2225 train_time:43616ms step_avg:59.99ms
step:728/2225 train_time:43675ms step_avg:59.99ms
step:729/2225 train_time:43735ms step_avg:59.99ms
step:730/2225 train_time:43794ms step_avg:59.99ms
step:731/2225 train_time:43856ms step_avg:59.99ms
step:732/2225 train_time:43916ms step_avg:59.99ms
step:733/2225 train_time:43977ms step_avg:60.00ms
step:734/2225 train_time:44037ms step_avg:60.00ms
step:735/2225 train_time:44099ms step_avg:60.00ms
step:736/2225 train_time:44159ms step_avg:60.00ms
step:737/2225 train_time:44221ms step_avg:60.00ms
step:738/2225 train_time:44280ms step_avg:60.00ms
step:739/2225 train_time:44342ms step_avg:60.00ms
step:740/2225 train_time:44401ms step_avg:60.00ms
step:741/2225 train_time:44462ms step_avg:60.00ms
step:742/2225 train_time:44522ms step_avg:60.00ms
step:743/2225 train_time:44583ms step_avg:60.00ms
step:744/2225 train_time:44643ms step_avg:60.00ms
step:745/2225 train_time:44704ms step_avg:60.01ms
step:746/2225 train_time:44764ms step_avg:60.01ms
step:747/2225 train_time:44826ms step_avg:60.01ms
step:748/2225 train_time:44885ms step_avg:60.01ms
step:749/2225 train_time:44947ms step_avg:60.01ms
step:750/2225 train_time:45007ms step_avg:60.01ms
step:750/2225 val_loss:3.6645 train_time:45070ms step_avg:60.09ms
step:751/2225 train_time:45094ms step_avg:60.04ms
step:752/2225 train_time:45131ms step_avg:60.02ms
step:753/2225 train_time:45192ms step_avg:60.02ms
step:754/2225 train_time:45252ms step_avg:60.02ms
step:755/2225 train_time:45315ms step_avg:60.02ms
step:756/2225 train_time:45375ms step_avg:60.02ms
step:757/2225 train_time:45435ms step_avg:60.02ms
step:758/2225 train_time:45493ms step_avg:60.02ms
step:759/2225 train_time:45553ms step_avg:60.02ms
step:760/2225 train_time:45611ms step_avg:60.02ms
step:761/2225 train_time:45672ms step_avg:60.02ms
step:762/2225 train_time:45730ms step_avg:60.01ms
step:763/2225 train_time:45790ms step_avg:60.01ms
step:764/2225 train_time:45851ms step_avg:60.01ms
step:765/2225 train_time:45912ms step_avg:60.02ms
step:766/2225 train_time:45973ms step_avg:60.02ms
step:767/2225 train_time:46041ms step_avg:60.03ms
step:768/2225 train_time:46103ms step_avg:60.03ms
step:769/2225 train_time:46165ms step_avg:60.03ms
step:770/2225 train_time:46225ms step_avg:60.03ms
step:771/2225 train_time:46287ms step_avg:60.03ms
step:772/2225 train_time:46347ms step_avg:60.03ms
step:773/2225 train_time:46408ms step_avg:60.04ms
step:774/2225 train_time:46468ms step_avg:60.04ms
step:775/2225 train_time:46528ms step_avg:60.04ms
step:776/2225 train_time:46588ms step_avg:60.04ms
step:777/2225 train_time:46648ms step_avg:60.04ms
step:778/2225 train_time:46708ms step_avg:60.04ms
step:779/2225 train_time:46769ms step_avg:60.04ms
step:780/2225 train_time:46829ms step_avg:60.04ms
step:781/2225 train_time:46889ms step_avg:60.04ms
step:782/2225 train_time:46950ms step_avg:60.04ms
step:783/2225 train_time:47012ms step_avg:60.04ms
step:784/2225 train_time:47073ms step_avg:60.04ms
step:785/2225 train_time:47135ms step_avg:60.05ms
step:786/2225 train_time:47195ms step_avg:60.04ms
step:787/2225 train_time:47257ms step_avg:60.05ms
step:788/2225 train_time:47316ms step_avg:60.05ms
step:789/2225 train_time:47378ms step_avg:60.05ms
step:790/2225 train_time:47437ms step_avg:60.05ms
step:791/2225 train_time:47498ms step_avg:60.05ms
step:792/2225 train_time:47558ms step_avg:60.05ms
step:793/2225 train_time:47619ms step_avg:60.05ms
step:794/2225 train_time:47678ms step_avg:60.05ms
step:795/2225 train_time:47739ms step_avg:60.05ms
step:796/2225 train_time:47799ms step_avg:60.05ms
step:797/2225 train_time:47861ms step_avg:60.05ms
step:798/2225 train_time:47921ms step_avg:60.05ms
step:799/2225 train_time:47983ms step_avg:60.05ms
step:800/2225 train_time:48044ms step_avg:60.06ms
step:801/2225 train_time:48107ms step_avg:60.06ms
step:802/2225 train_time:48167ms step_avg:60.06ms
step:803/2225 train_time:48228ms step_avg:60.06ms
step:804/2225 train_time:48288ms step_avg:60.06ms
step:805/2225 train_time:48350ms step_avg:60.06ms
step:806/2225 train_time:48409ms step_avg:60.06ms
step:807/2225 train_time:48471ms step_avg:60.06ms
step:808/2225 train_time:48530ms step_avg:60.06ms
step:809/2225 train_time:48591ms step_avg:60.06ms
step:810/2225 train_time:48651ms step_avg:60.06ms
step:811/2225 train_time:48711ms step_avg:60.06ms
step:812/2225 train_time:48771ms step_avg:60.06ms
step:813/2225 train_time:48833ms step_avg:60.06ms
step:814/2225 train_time:48892ms step_avg:60.06ms
step:815/2225 train_time:48953ms step_avg:60.07ms
step:816/2225 train_time:49013ms step_avg:60.07ms
step:817/2225 train_time:49075ms step_avg:60.07ms
step:818/2225 train_time:49134ms step_avg:60.07ms
step:819/2225 train_time:49196ms step_avg:60.07ms
step:820/2225 train_time:49256ms step_avg:60.07ms
step:821/2225 train_time:49317ms step_avg:60.07ms
step:822/2225 train_time:49377ms step_avg:60.07ms
step:823/2225 train_time:49438ms step_avg:60.07ms
step:824/2225 train_time:49498ms step_avg:60.07ms
step:825/2225 train_time:49559ms step_avg:60.07ms
step:826/2225 train_time:49618ms step_avg:60.07ms
step:827/2225 train_time:49680ms step_avg:60.07ms
step:828/2225 train_time:49739ms step_avg:60.07ms
step:829/2225 train_time:49800ms step_avg:60.07ms
step:830/2225 train_time:49860ms step_avg:60.07ms
step:831/2225 train_time:49922ms step_avg:60.07ms
step:832/2225 train_time:49982ms step_avg:60.07ms
step:833/2225 train_time:50044ms step_avg:60.08ms
step:834/2225 train_time:50105ms step_avg:60.08ms
step:835/2225 train_time:50167ms step_avg:60.08ms
step:836/2225 train_time:50227ms step_avg:60.08ms
step:837/2225 train_time:50289ms step_avg:60.08ms
step:838/2225 train_time:50348ms step_avg:60.08ms
step:839/2225 train_time:50409ms step_avg:60.08ms
step:840/2225 train_time:50469ms step_avg:60.08ms
step:841/2225 train_time:50530ms step_avg:60.08ms
step:842/2225 train_time:50589ms step_avg:60.08ms
step:843/2225 train_time:50650ms step_avg:60.08ms
step:844/2225 train_time:50711ms step_avg:60.08ms
step:845/2225 train_time:50772ms step_avg:60.09ms
step:846/2225 train_time:50832ms step_avg:60.09ms
step:847/2225 train_time:50893ms step_avg:60.09ms
step:848/2225 train_time:50953ms step_avg:60.09ms
step:849/2225 train_time:51014ms step_avg:60.09ms
step:850/2225 train_time:51073ms step_avg:60.09ms
step:851/2225 train_time:51135ms step_avg:60.09ms
step:852/2225 train_time:51195ms step_avg:60.09ms
step:853/2225 train_time:51257ms step_avg:60.09ms
step:854/2225 train_time:51317ms step_avg:60.09ms
step:855/2225 train_time:51379ms step_avg:60.09ms
step:856/2225 train_time:51439ms step_avg:60.09ms
step:857/2225 train_time:51500ms step_avg:60.09ms
step:858/2225 train_time:51559ms step_avg:60.09ms
step:859/2225 train_time:51620ms step_avg:60.09ms
step:860/2225 train_time:51680ms step_avg:60.09ms
step:861/2225 train_time:51741ms step_avg:60.09ms
step:862/2225 train_time:51801ms step_avg:60.09ms
step:863/2225 train_time:51863ms step_avg:60.10ms
step:864/2225 train_time:51923ms step_avg:60.10ms
step:865/2225 train_time:51985ms step_avg:60.10ms
step:866/2225 train_time:52045ms step_avg:60.10ms
step:867/2225 train_time:52108ms step_avg:60.10ms
step:868/2225 train_time:52167ms step_avg:60.10ms
step:869/2225 train_time:52230ms step_avg:60.10ms
step:870/2225 train_time:52289ms step_avg:60.10ms
step:871/2225 train_time:52350ms step_avg:60.10ms
step:872/2225 train_time:52410ms step_avg:60.10ms
step:873/2225 train_time:52471ms step_avg:60.10ms
step:874/2225 train_time:52531ms step_avg:60.10ms
step:875/2225 train_time:52591ms step_avg:60.10ms
step:876/2225 train_time:52651ms step_avg:60.10ms
step:877/2225 train_time:52713ms step_avg:60.11ms
step:878/2225 train_time:52773ms step_avg:60.11ms
step:879/2225 train_time:52833ms step_avg:60.11ms
step:880/2225 train_time:52893ms step_avg:60.11ms
step:881/2225 train_time:52954ms step_avg:60.11ms
step:882/2225 train_time:53014ms step_avg:60.11ms
step:883/2225 train_time:53075ms step_avg:60.11ms
step:884/2225 train_time:53135ms step_avg:60.11ms
step:885/2225 train_time:53197ms step_avg:60.11ms
step:886/2225 train_time:53257ms step_avg:60.11ms
step:887/2225 train_time:53318ms step_avg:60.11ms
step:888/2225 train_time:53378ms step_avg:60.11ms
step:889/2225 train_time:53439ms step_avg:60.11ms
step:890/2225 train_time:53498ms step_avg:60.11ms
step:891/2225 train_time:53560ms step_avg:60.11ms
step:892/2225 train_time:53619ms step_avg:60.11ms
step:893/2225 train_time:53680ms step_avg:60.11ms
step:894/2225 train_time:53740ms step_avg:60.11ms
step:895/2225 train_time:53801ms step_avg:60.11ms
step:896/2225 train_time:53862ms step_avg:60.11ms
step:897/2225 train_time:53923ms step_avg:60.11ms
step:898/2225 train_time:53983ms step_avg:60.11ms
step:899/2225 train_time:54045ms step_avg:60.12ms
step:900/2225 train_time:54106ms step_avg:60.12ms
step:901/2225 train_time:54168ms step_avg:60.12ms
step:902/2225 train_time:54228ms step_avg:60.12ms
step:903/2225 train_time:54289ms step_avg:60.12ms
step:904/2225 train_time:54349ms step_avg:60.12ms
step:905/2225 train_time:54411ms step_avg:60.12ms
step:906/2225 train_time:54472ms step_avg:60.12ms
step:907/2225 train_time:54533ms step_avg:60.13ms
step:908/2225 train_time:54593ms step_avg:60.12ms
step:909/2225 train_time:54653ms step_avg:60.12ms
step:910/2225 train_time:54713ms step_avg:60.12ms
step:911/2225 train_time:54774ms step_avg:60.13ms
step:912/2225 train_time:54834ms step_avg:60.12ms
step:913/2225 train_time:54895ms step_avg:60.13ms
step:914/2225 train_time:54955ms step_avg:60.13ms
step:915/2225 train_time:55016ms step_avg:60.13ms
step:916/2225 train_time:55076ms step_avg:60.13ms
step:917/2225 train_time:55138ms step_avg:60.13ms
step:918/2225 train_time:55198ms step_avg:60.13ms
step:919/2225 train_time:55260ms step_avg:60.13ms
step:920/2225 train_time:55320ms step_avg:60.13ms
step:921/2225 train_time:55381ms step_avg:60.13ms
step:922/2225 train_time:55442ms step_avg:60.13ms
step:923/2225 train_time:55504ms step_avg:60.13ms
step:924/2225 train_time:55564ms step_avg:60.13ms
step:925/2225 train_time:55625ms step_avg:60.14ms
step:926/2225 train_time:55685ms step_avg:60.14ms
step:927/2225 train_time:55747ms step_avg:60.14ms
step:928/2225 train_time:55807ms step_avg:60.14ms
step:929/2225 train_time:55869ms step_avg:60.14ms
step:930/2225 train_time:55928ms step_avg:60.14ms
step:931/2225 train_time:55990ms step_avg:60.14ms
step:932/2225 train_time:56051ms step_avg:60.14ms
step:933/2225 train_time:56111ms step_avg:60.14ms
step:934/2225 train_time:56171ms step_avg:60.14ms
step:935/2225 train_time:56232ms step_avg:60.14ms
step:936/2225 train_time:56292ms step_avg:60.14ms
step:937/2225 train_time:56353ms step_avg:60.14ms
step:938/2225 train_time:56413ms step_avg:60.14ms
step:939/2225 train_time:56474ms step_avg:60.14ms
step:940/2225 train_time:56534ms step_avg:60.14ms
step:941/2225 train_time:56595ms step_avg:60.14ms
step:942/2225 train_time:56654ms step_avg:60.14ms
step:943/2225 train_time:56715ms step_avg:60.14ms
step:944/2225 train_time:56775ms step_avg:60.14ms
step:945/2225 train_time:56836ms step_avg:60.14ms
step:946/2225 train_time:56896ms step_avg:60.14ms
step:947/2225 train_time:56957ms step_avg:60.14ms
step:948/2225 train_time:57017ms step_avg:60.14ms
step:949/2225 train_time:57079ms step_avg:60.15ms
step:950/2225 train_time:57138ms step_avg:60.15ms
step:951/2225 train_time:57200ms step_avg:60.15ms
step:952/2225 train_time:57260ms step_avg:60.15ms
step:953/2225 train_time:57323ms step_avg:60.15ms
step:954/2225 train_time:57383ms step_avg:60.15ms
step:955/2225 train_time:57445ms step_avg:60.15ms
step:956/2225 train_time:57506ms step_avg:60.15ms
step:957/2225 train_time:57569ms step_avg:60.16ms
step:958/2225 train_time:57629ms step_avg:60.16ms
step:959/2225 train_time:57690ms step_avg:60.16ms
step:960/2225 train_time:57750ms step_avg:60.16ms
step:961/2225 train_time:57811ms step_avg:60.16ms
step:962/2225 train_time:57872ms step_avg:60.16ms
step:963/2225 train_time:57932ms step_avg:60.16ms
step:964/2225 train_time:57992ms step_avg:60.16ms
step:965/2225 train_time:58053ms step_avg:60.16ms
step:966/2225 train_time:58112ms step_avg:60.16ms
step:967/2225 train_time:58173ms step_avg:60.16ms
step:968/2225 train_time:58233ms step_avg:60.16ms
step:969/2225 train_time:58295ms step_avg:60.16ms
step:970/2225 train_time:58355ms step_avg:60.16ms
step:971/2225 train_time:58416ms step_avg:60.16ms
step:972/2225 train_time:58477ms step_avg:60.16ms
step:973/2225 train_time:58538ms step_avg:60.16ms
step:974/2225 train_time:58599ms step_avg:60.16ms
step:975/2225 train_time:58660ms step_avg:60.16ms
step:976/2225 train_time:58720ms step_avg:60.16ms
step:977/2225 train_time:58781ms step_avg:60.16ms
step:978/2225 train_time:58841ms step_avg:60.16ms
step:979/2225 train_time:58903ms step_avg:60.17ms
step:980/2225 train_time:58962ms step_avg:60.17ms
step:981/2225 train_time:59024ms step_avg:60.17ms
step:982/2225 train_time:59085ms step_avg:60.17ms
step:983/2225 train_time:59146ms step_avg:60.17ms
step:984/2225 train_time:59207ms step_avg:60.17ms
step:985/2225 train_time:59269ms step_avg:60.17ms
step:986/2225 train_time:59329ms step_avg:60.17ms
step:987/2225 train_time:59390ms step_avg:60.17ms
step:988/2225 train_time:59450ms step_avg:60.17ms
step:989/2225 train_time:59511ms step_avg:60.17ms
step:990/2225 train_time:59571ms step_avg:60.17ms
step:991/2225 train_time:59632ms step_avg:60.17ms
step:992/2225 train_time:59692ms step_avg:60.17ms
step:993/2225 train_time:59754ms step_avg:60.17ms
step:994/2225 train_time:59813ms step_avg:60.17ms
step:995/2225 train_time:59874ms step_avg:60.18ms
step:996/2225 train_time:59933ms step_avg:60.17ms
step:997/2225 train_time:59995ms step_avg:60.18ms
step:998/2225 train_time:60054ms step_avg:60.17ms
step:999/2225 train_time:60116ms step_avg:60.18ms
step:1000/2225 train_time:60175ms step_avg:60.18ms
step:1000/2225 val_loss:3.5893 train_time:60237ms step_avg:60.24ms
step:1001/2225 train_time:60261ms step_avg:60.20ms
step:1002/2225 train_time:60300ms step_avg:60.18ms
step:1003/2225 train_time:60365ms step_avg:60.18ms
step:1004/2225 train_time:60427ms step_avg:60.19ms
step:1005/2225 train_time:60488ms step_avg:60.19ms
step:1006/2225 train_time:60548ms step_avg:60.19ms
step:1007/2225 train_time:60609ms step_avg:60.19ms
step:1008/2225 train_time:60668ms step_avg:60.19ms
step:1009/2225 train_time:60729ms step_avg:60.19ms
step:1010/2225 train_time:60788ms step_avg:60.19ms
step:1011/2225 train_time:60848ms step_avg:60.19ms
step:1012/2225 train_time:60907ms step_avg:60.18ms
step:1013/2225 train_time:60968ms step_avg:60.19ms
step:1014/2225 train_time:61027ms step_avg:60.18ms
step:1015/2225 train_time:61087ms step_avg:60.18ms
step:1016/2225 train_time:61148ms step_avg:60.18ms
step:1017/2225 train_time:61211ms step_avg:60.19ms
step:1018/2225 train_time:61272ms step_avg:60.19ms
step:1019/2225 train_time:61335ms step_avg:60.19ms
step:1020/2225 train_time:61396ms step_avg:60.19ms
step:1021/2225 train_time:61459ms step_avg:60.20ms
step:1022/2225 train_time:61521ms step_avg:60.20ms
step:1023/2225 train_time:61582ms step_avg:60.20ms
step:1024/2225 train_time:61642ms step_avg:60.20ms
step:1025/2225 train_time:61703ms step_avg:60.20ms
step:1026/2225 train_time:61762ms step_avg:60.20ms
step:1027/2225 train_time:61822ms step_avg:60.20ms
step:1028/2225 train_time:61882ms step_avg:60.20ms
step:1029/2225 train_time:61943ms step_avg:60.20ms
step:1030/2225 train_time:62003ms step_avg:60.20ms
step:1031/2225 train_time:62063ms step_avg:60.20ms
step:1032/2225 train_time:62122ms step_avg:60.20ms
step:1033/2225 train_time:62184ms step_avg:60.20ms
step:1034/2225 train_time:62245ms step_avg:60.20ms
step:1035/2225 train_time:62307ms step_avg:60.20ms
step:1036/2225 train_time:62369ms step_avg:60.20ms
step:1037/2225 train_time:62431ms step_avg:60.20ms
step:1038/2225 train_time:62492ms step_avg:60.20ms
step:1039/2225 train_time:62555ms step_avg:60.21ms
step:1040/2225 train_time:62614ms step_avg:60.21ms
step:1041/2225 train_time:62676ms step_avg:60.21ms
step:1042/2225 train_time:62737ms step_avg:60.21ms
step:1043/2225 train_time:62799ms step_avg:60.21ms
step:1044/2225 train_time:62858ms step_avg:60.21ms
step:1045/2225 train_time:62919ms step_avg:60.21ms
step:1046/2225 train_time:62979ms step_avg:60.21ms
step:1047/2225 train_time:63040ms step_avg:60.21ms
step:1048/2225 train_time:63100ms step_avg:60.21ms
step:1049/2225 train_time:63161ms step_avg:60.21ms
step:1050/2225 train_time:63220ms step_avg:60.21ms
step:1051/2225 train_time:63282ms step_avg:60.21ms
step:1052/2225 train_time:63343ms step_avg:60.21ms
step:1053/2225 train_time:63404ms step_avg:60.21ms
step:1054/2225 train_time:63465ms step_avg:60.21ms
step:1055/2225 train_time:63526ms step_avg:60.21ms
step:1056/2225 train_time:63587ms step_avg:60.21ms
step:1057/2225 train_time:63649ms step_avg:60.22ms
step:1058/2225 train_time:63708ms step_avg:60.22ms
step:1059/2225 train_time:63769ms step_avg:60.22ms
step:1060/2225 train_time:63829ms step_avg:60.22ms
step:1061/2225 train_time:63891ms step_avg:60.22ms
step:1062/2225 train_time:63951ms step_avg:60.22ms
step:1063/2225 train_time:64012ms step_avg:60.22ms
step:1064/2225 train_time:64072ms step_avg:60.22ms
step:1065/2225 train_time:64133ms step_avg:60.22ms
step:1066/2225 train_time:64194ms step_avg:60.22ms
step:1067/2225 train_time:64256ms step_avg:60.22ms
step:1068/2225 train_time:64316ms step_avg:60.22ms
step:1069/2225 train_time:64379ms step_avg:60.22ms
step:1070/2225 train_time:64440ms step_avg:60.22ms
step:1071/2225 train_time:64501ms step_avg:60.22ms
step:1072/2225 train_time:64561ms step_avg:60.22ms
step:1073/2225 train_time:64623ms step_avg:60.23ms
step:1074/2225 train_time:64683ms step_avg:60.23ms
step:1075/2225 train_time:64744ms step_avg:60.23ms
step:1076/2225 train_time:64803ms step_avg:60.23ms
step:1077/2225 train_time:64864ms step_avg:60.23ms
step:1078/2225 train_time:64923ms step_avg:60.23ms
step:1079/2225 train_time:64984ms step_avg:60.23ms
step:1080/2225 train_time:65044ms step_avg:60.23ms
step:1081/2225 train_time:65105ms step_avg:60.23ms
step:1082/2225 train_time:65165ms step_avg:60.23ms
step:1083/2225 train_time:65227ms step_avg:60.23ms
step:1084/2225 train_time:65287ms step_avg:60.23ms
step:1085/2225 train_time:65349ms step_avg:60.23ms
step:1086/2225 train_time:65410ms step_avg:60.23ms
step:1087/2225 train_time:65471ms step_avg:60.23ms
step:1088/2225 train_time:65531ms step_avg:60.23ms
step:1089/2225 train_time:65593ms step_avg:60.23ms
step:1090/2225 train_time:65654ms step_avg:60.23ms
step:1091/2225 train_time:65715ms step_avg:60.23ms
step:1092/2225 train_time:65776ms step_avg:60.23ms
step:1093/2225 train_time:65838ms step_avg:60.24ms
step:1094/2225 train_time:65898ms step_avg:60.24ms
step:1095/2225 train_time:65959ms step_avg:60.24ms
step:1096/2225 train_time:66019ms step_avg:60.24ms
step:1097/2225 train_time:66080ms step_avg:60.24ms
step:1098/2225 train_time:66140ms step_avg:60.24ms
step:1099/2225 train_time:66201ms step_avg:60.24ms
step:1100/2225 train_time:66261ms step_avg:60.24ms
step:1101/2225 train_time:66323ms step_avg:60.24ms
step:1102/2225 train_time:66383ms step_avg:60.24ms
step:1103/2225 train_time:66444ms step_avg:60.24ms
step:1104/2225 train_time:66504ms step_avg:60.24ms
step:1105/2225 train_time:66565ms step_avg:60.24ms
step:1106/2225 train_time:66625ms step_avg:60.24ms
step:1107/2225 train_time:66687ms step_avg:60.24ms
step:1108/2225 train_time:66746ms step_avg:60.24ms
step:1109/2225 train_time:66808ms step_avg:60.24ms
step:1110/2225 train_time:66867ms step_avg:60.24ms
step:1111/2225 train_time:66928ms step_avg:60.24ms
step:1112/2225 train_time:66988ms step_avg:60.24ms
step:1113/2225 train_time:67049ms step_avg:60.24ms
step:1114/2225 train_time:67108ms step_avg:60.24ms
step:1115/2225 train_time:67170ms step_avg:60.24ms
step:1116/2225 train_time:67230ms step_avg:60.24ms
step:1117/2225 train_time:67291ms step_avg:60.24ms
step:1118/2225 train_time:67351ms step_avg:60.24ms
step:1119/2225 train_time:67413ms step_avg:60.24ms
step:1120/2225 train_time:67474ms step_avg:60.24ms
step:1121/2225 train_time:67536ms step_avg:60.25ms
step:1122/2225 train_time:67596ms step_avg:60.25ms
step:1123/2225 train_time:67658ms step_avg:60.25ms
step:1124/2225 train_time:67718ms step_avg:60.25ms
step:1125/2225 train_time:67779ms step_avg:60.25ms
step:1126/2225 train_time:67840ms step_avg:60.25ms
step:1127/2225 train_time:67901ms step_avg:60.25ms
step:1128/2225 train_time:67961ms step_avg:60.25ms
step:1129/2225 train_time:68021ms step_avg:60.25ms
step:1130/2225 train_time:68080ms step_avg:60.25ms
step:1131/2225 train_time:68142ms step_avg:60.25ms
step:1132/2225 train_time:68202ms step_avg:60.25ms
step:1133/2225 train_time:68263ms step_avg:60.25ms
step:1134/2225 train_time:68323ms step_avg:60.25ms
step:1135/2225 train_time:68384ms step_avg:60.25ms
step:1136/2225 train_time:68444ms step_avg:60.25ms
step:1137/2225 train_time:68505ms step_avg:60.25ms
step:1138/2225 train_time:68565ms step_avg:60.25ms
step:1139/2225 train_time:68627ms step_avg:60.25ms
step:1140/2225 train_time:68686ms step_avg:60.25ms
step:1141/2225 train_time:68748ms step_avg:60.25ms
step:1142/2225 train_time:68808ms step_avg:60.25ms
step:1143/2225 train_time:68870ms step_avg:60.25ms
step:1144/2225 train_time:68929ms step_avg:60.25ms
step:1145/2225 train_time:68990ms step_avg:60.25ms
step:1146/2225 train_time:69050ms step_avg:60.25ms
step:1147/2225 train_time:69111ms step_avg:60.25ms
step:1148/2225 train_time:69171ms step_avg:60.25ms
step:1149/2225 train_time:69233ms step_avg:60.26ms
step:1150/2225 train_time:69294ms step_avg:60.26ms
step:1151/2225 train_time:69356ms step_avg:60.26ms
step:1152/2225 train_time:69417ms step_avg:60.26ms
step:1153/2225 train_time:69479ms step_avg:60.26ms
step:1154/2225 train_time:69539ms step_avg:60.26ms
step:1155/2225 train_time:69600ms step_avg:60.26ms
step:1156/2225 train_time:69660ms step_avg:60.26ms
step:1157/2225 train_time:69721ms step_avg:60.26ms
step:1158/2225 train_time:69781ms step_avg:60.26ms
step:1159/2225 train_time:69843ms step_avg:60.26ms
step:1160/2225 train_time:69903ms step_avg:60.26ms
step:1161/2225 train_time:69964ms step_avg:60.26ms
step:1162/2225 train_time:70023ms step_avg:60.26ms
step:1163/2225 train_time:70084ms step_avg:60.26ms
step:1164/2225 train_time:70144ms step_avg:60.26ms
step:1165/2225 train_time:70205ms step_avg:60.26ms
step:1166/2225 train_time:70265ms step_avg:60.26ms
step:1167/2225 train_time:70326ms step_avg:60.26ms
step:1168/2225 train_time:70386ms step_avg:60.26ms
step:1169/2225 train_time:70448ms step_avg:60.26ms
step:1170/2225 train_time:70507ms step_avg:60.26ms
step:1171/2225 train_time:70568ms step_avg:60.26ms
step:1172/2225 train_time:70628ms step_avg:60.26ms
step:1173/2225 train_time:70690ms step_avg:60.26ms
step:1174/2225 train_time:70750ms step_avg:60.26ms
step:1175/2225 train_time:70811ms step_avg:60.26ms
step:1176/2225 train_time:70871ms step_avg:60.26ms
step:1177/2225 train_time:70933ms step_avg:60.27ms
step:1178/2225 train_time:70992ms step_avg:60.27ms
step:1179/2225 train_time:71054ms step_avg:60.27ms
step:1180/2225 train_time:71114ms step_avg:60.27ms
step:1181/2225 train_time:71176ms step_avg:60.27ms
step:1182/2225 train_time:71236ms step_avg:60.27ms
step:1183/2225 train_time:71298ms step_avg:60.27ms
step:1184/2225 train_time:71359ms step_avg:60.27ms
step:1185/2225 train_time:71420ms step_avg:60.27ms
step:1186/2225 train_time:71480ms step_avg:60.27ms
step:1187/2225 train_time:71542ms step_avg:60.27ms
step:1188/2225 train_time:71602ms step_avg:60.27ms
step:1189/2225 train_time:71662ms step_avg:60.27ms
step:1190/2225 train_time:71722ms step_avg:60.27ms
step:1191/2225 train_time:71783ms step_avg:60.27ms
step:1192/2225 train_time:71843ms step_avg:60.27ms
step:1193/2225 train_time:71904ms step_avg:60.27ms
step:1194/2225 train_time:71963ms step_avg:60.27ms
step:1195/2225 train_time:72024ms step_avg:60.27ms
step:1196/2225 train_time:72084ms step_avg:60.27ms
step:1197/2225 train_time:72146ms step_avg:60.27ms
step:1198/2225 train_time:72206ms step_avg:60.27ms
step:1199/2225 train_time:72268ms step_avg:60.27ms
step:1200/2225 train_time:72328ms step_avg:60.27ms
step:1201/2225 train_time:72390ms step_avg:60.27ms
step:1202/2225 train_time:72450ms step_avg:60.27ms
step:1203/2225 train_time:72512ms step_avg:60.28ms
step:1204/2225 train_time:72573ms step_avg:60.28ms
step:1205/2225 train_time:72635ms step_avg:60.28ms
step:1206/2225 train_time:72695ms step_avg:60.28ms
step:1207/2225 train_time:72758ms step_avg:60.28ms
step:1208/2225 train_time:72818ms step_avg:60.28ms
step:1209/2225 train_time:72879ms step_avg:60.28ms
step:1210/2225 train_time:72939ms step_avg:60.28ms
step:1211/2225 train_time:73000ms step_avg:60.28ms
step:1212/2225 train_time:73060ms step_avg:60.28ms
step:1213/2225 train_time:73122ms step_avg:60.28ms
step:1214/2225 train_time:73182ms step_avg:60.28ms
step:1215/2225 train_time:73243ms step_avg:60.28ms
step:1216/2225 train_time:73303ms step_avg:60.28ms
step:1217/2225 train_time:73364ms step_avg:60.28ms
step:1218/2225 train_time:73423ms step_avg:60.28ms
step:1219/2225 train_time:73485ms step_avg:60.28ms
step:1220/2225 train_time:73545ms step_avg:60.28ms
step:1221/2225 train_time:73606ms step_avg:60.28ms
step:1222/2225 train_time:73665ms step_avg:60.28ms
step:1223/2225 train_time:73727ms step_avg:60.28ms
step:1224/2225 train_time:73787ms step_avg:60.28ms
step:1225/2225 train_time:73848ms step_avg:60.28ms
step:1226/2225 train_time:73908ms step_avg:60.28ms
step:1227/2225 train_time:73969ms step_avg:60.28ms
step:1228/2225 train_time:74029ms step_avg:60.28ms
step:1229/2225 train_time:74091ms step_avg:60.29ms
step:1230/2225 train_time:74152ms step_avg:60.29ms
step:1231/2225 train_time:74213ms step_avg:60.29ms
step:1232/2225 train_time:74273ms step_avg:60.29ms
step:1233/2225 train_time:74335ms step_avg:60.29ms
step:1234/2225 train_time:74395ms step_avg:60.29ms
step:1235/2225 train_time:74457ms step_avg:60.29ms
step:1236/2225 train_time:74517ms step_avg:60.29ms
step:1237/2225 train_time:74579ms step_avg:60.29ms
step:1238/2225 train_time:74640ms step_avg:60.29ms
step:1239/2225 train_time:74700ms step_avg:60.29ms
step:1240/2225 train_time:74760ms step_avg:60.29ms
step:1241/2225 train_time:74821ms step_avg:60.29ms
step:1242/2225 train_time:74881ms step_avg:60.29ms
step:1243/2225 train_time:74943ms step_avg:60.29ms
step:1244/2225 train_time:75003ms step_avg:60.29ms
step:1245/2225 train_time:75063ms step_avg:60.29ms
step:1246/2225 train_time:75123ms step_avg:60.29ms
step:1247/2225 train_time:75184ms step_avg:60.29ms
step:1248/2225 train_time:75243ms step_avg:60.29ms
step:1249/2225 train_time:75305ms step_avg:60.29ms
step:1250/2225 train_time:75364ms step_avg:60.29ms
step:1250/2225 val_loss:3.5190 train_time:75426ms step_avg:60.34ms
step:1251/2225 train_time:75449ms step_avg:60.31ms
step:1252/2225 train_time:75491ms step_avg:60.30ms
step:1253/2225 train_time:75554ms step_avg:60.30ms
step:1254/2225 train_time:75618ms step_avg:60.30ms
step:1255/2225 train_time:75680ms step_avg:60.30ms
step:1256/2225 train_time:75740ms step_avg:60.30ms
step:1257/2225 train_time:75800ms step_avg:60.30ms
step:1258/2225 train_time:75860ms step_avg:60.30ms
step:1259/2225 train_time:75920ms step_avg:60.30ms
step:1260/2225 train_time:75979ms step_avg:60.30ms
step:1261/2225 train_time:76040ms step_avg:60.30ms
step:1262/2225 train_time:76100ms step_avg:60.30ms
step:1263/2225 train_time:76160ms step_avg:60.30ms
step:1264/2225 train_time:76220ms step_avg:60.30ms
step:1265/2225 train_time:76281ms step_avg:60.30ms
step:1266/2225 train_time:76341ms step_avg:60.30ms
step:1267/2225 train_time:76404ms step_avg:60.30ms
step:1268/2225 train_time:76465ms step_avg:60.30ms
step:1269/2225 train_time:76528ms step_avg:60.31ms
step:1270/2225 train_time:76591ms step_avg:60.31ms
step:1271/2225 train_time:76654ms step_avg:60.31ms
step:1272/2225 train_time:76713ms step_avg:60.31ms
step:1273/2225 train_time:76775ms step_avg:60.31ms
step:1274/2225 train_time:76834ms step_avg:60.31ms
step:1275/2225 train_time:76895ms step_avg:60.31ms
step:1276/2225 train_time:76954ms step_avg:60.31ms
step:1277/2225 train_time:77015ms step_avg:60.31ms
step:1278/2225 train_time:77074ms step_avg:60.31ms
step:1279/2225 train_time:77134ms step_avg:60.31ms
step:1280/2225 train_time:77194ms step_avg:60.31ms
step:1281/2225 train_time:77255ms step_avg:60.31ms
step:1282/2225 train_time:77315ms step_avg:60.31ms
step:1283/2225 train_time:77377ms step_avg:60.31ms
step:1284/2225 train_time:77437ms step_avg:60.31ms
step:1285/2225 train_time:77500ms step_avg:60.31ms
step:1286/2225 train_time:77560ms step_avg:60.31ms
step:1287/2225 train_time:77622ms step_avg:60.31ms
step:1288/2225 train_time:77683ms step_avg:60.31ms
step:1289/2225 train_time:77744ms step_avg:60.31ms
step:1290/2225 train_time:77805ms step_avg:60.31ms
step:1291/2225 train_time:77867ms step_avg:60.32ms
step:1292/2225 train_time:77927ms step_avg:60.31ms
step:1293/2225 train_time:77989ms step_avg:60.32ms
step:1294/2225 train_time:78048ms step_avg:60.32ms
step:1295/2225 train_time:78109ms step_avg:60.32ms
step:1296/2225 train_time:78169ms step_avg:60.32ms
step:1297/2225 train_time:78231ms step_avg:60.32ms
step:1298/2225 train_time:78291ms step_avg:60.32ms
step:1299/2225 train_time:78352ms step_avg:60.32ms
step:1300/2225 train_time:78412ms step_avg:60.32ms
step:1301/2225 train_time:78474ms step_avg:60.32ms
step:1302/2225 train_time:78535ms step_avg:60.32ms
step:1303/2225 train_time:78597ms step_avg:60.32ms
step:1304/2225 train_time:78657ms step_avg:60.32ms
step:1305/2225 train_time:78718ms step_avg:60.32ms
step:1306/2225 train_time:78778ms step_avg:60.32ms
step:1307/2225 train_time:78839ms step_avg:60.32ms
step:1308/2225 train_time:78898ms step_avg:60.32ms
step:1309/2225 train_time:78960ms step_avg:60.32ms
step:1310/2225 train_time:79019ms step_avg:60.32ms
step:1311/2225 train_time:79080ms step_avg:60.32ms
step:1312/2225 train_time:79140ms step_avg:60.32ms
step:1313/2225 train_time:79200ms step_avg:60.32ms
step:1314/2225 train_time:79260ms step_avg:60.32ms
step:1315/2225 train_time:79322ms step_avg:60.32ms
step:1316/2225 train_time:79382ms step_avg:60.32ms
step:1317/2225 train_time:79443ms step_avg:60.32ms
step:1318/2225 train_time:79504ms step_avg:60.32ms
step:1319/2225 train_time:79566ms step_avg:60.32ms
step:1320/2225 train_time:79626ms step_avg:60.32ms
step:1321/2225 train_time:79688ms step_avg:60.32ms
step:1322/2225 train_time:79748ms step_avg:60.32ms
step:1323/2225 train_time:79811ms step_avg:60.33ms
step:1324/2225 train_time:79871ms step_avg:60.33ms
step:1325/2225 train_time:79933ms step_avg:60.33ms
step:1326/2225 train_time:79993ms step_avg:60.33ms
step:1327/2225 train_time:80053ms step_avg:60.33ms
step:1328/2225 train_time:80113ms step_avg:60.33ms
step:1329/2225 train_time:80174ms step_avg:60.33ms
step:1330/2225 train_time:80234ms step_avg:60.33ms
step:1331/2225 train_time:80295ms step_avg:60.33ms
step:1332/2225 train_time:80354ms step_avg:60.33ms
step:1333/2225 train_time:80415ms step_avg:60.33ms
step:1334/2225 train_time:80475ms step_avg:60.33ms
step:1335/2225 train_time:80536ms step_avg:60.33ms
step:1336/2225 train_time:80596ms step_avg:60.33ms
step:1337/2225 train_time:80657ms step_avg:60.33ms
step:1338/2225 train_time:80717ms step_avg:60.33ms
step:1339/2225 train_time:80779ms step_avg:60.33ms
step:1340/2225 train_time:80839ms step_avg:60.33ms
step:1341/2225 train_time:80900ms step_avg:60.33ms
step:1342/2225 train_time:80960ms step_avg:60.33ms
step:1343/2225 train_time:81021ms step_avg:60.33ms
step:1344/2225 train_time:81080ms step_avg:60.33ms
step:1345/2225 train_time:81141ms step_avg:60.33ms
step:1346/2225 train_time:81201ms step_avg:60.33ms
step:1347/2225 train_time:81262ms step_avg:60.33ms
step:1348/2225 train_time:81322ms step_avg:60.33ms
step:1349/2225 train_time:81384ms step_avg:60.33ms
step:1350/2225 train_time:81444ms step_avg:60.33ms
step:1351/2225 train_time:81505ms step_avg:60.33ms
step:1352/2225 train_time:81566ms step_avg:60.33ms
step:1353/2225 train_time:81628ms step_avg:60.33ms
step:1354/2225 train_time:81689ms step_avg:60.33ms
step:1355/2225 train_time:81751ms step_avg:60.33ms
step:1356/2225 train_time:81810ms step_avg:60.33ms
step:1357/2225 train_time:81872ms step_avg:60.33ms
step:1358/2225 train_time:81932ms step_avg:60.33ms
step:1359/2225 train_time:81993ms step_avg:60.33ms
step:1360/2225 train_time:82053ms step_avg:60.33ms
step:1361/2225 train_time:82114ms step_avg:60.33ms
step:1362/2225 train_time:82174ms step_avg:60.33ms
step:1363/2225 train_time:82235ms step_avg:60.33ms
step:1364/2225 train_time:82295ms step_avg:60.33ms
step:1365/2225 train_time:82356ms step_avg:60.33ms
step:1366/2225 train_time:82415ms step_avg:60.33ms
step:1367/2225 train_time:82477ms step_avg:60.33ms
step:1368/2225 train_time:82536ms step_avg:60.33ms
step:1369/2225 train_time:82598ms step_avg:60.33ms
step:1370/2225 train_time:82657ms step_avg:60.33ms
step:1371/2225 train_time:82720ms step_avg:60.34ms
step:1372/2225 train_time:82780ms step_avg:60.34ms
step:1373/2225 train_time:82842ms step_avg:60.34ms
step:1374/2225 train_time:82902ms step_avg:60.34ms
step:1375/2225 train_time:82964ms step_avg:60.34ms
step:1376/2225 train_time:83024ms step_avg:60.34ms
step:1377/2225 train_time:83085ms step_avg:60.34ms
step:1378/2225 train_time:83145ms step_avg:60.34ms
step:1379/2225 train_time:83207ms step_avg:60.34ms
step:1380/2225 train_time:83267ms step_avg:60.34ms
step:1381/2225 train_time:83329ms step_avg:60.34ms
step:1382/2225 train_time:83389ms step_avg:60.34ms
step:1383/2225 train_time:83451ms step_avg:60.34ms
step:1384/2225 train_time:83511ms step_avg:60.34ms
step:1385/2225 train_time:83573ms step_avg:60.34ms
step:1386/2225 train_time:83633ms step_avg:60.34ms
step:1387/2225 train_time:83694ms step_avg:60.34ms
step:1388/2225 train_time:83754ms step_avg:60.34ms
step:1389/2225 train_time:83814ms step_avg:60.34ms
step:1390/2225 train_time:83874ms step_avg:60.34ms
step:1391/2225 train_time:83936ms step_avg:60.34ms
step:1392/2225 train_time:83996ms step_avg:60.34ms
step:1393/2225 train_time:84056ms step_avg:60.34ms
step:1394/2225 train_time:84116ms step_avg:60.34ms
step:1395/2225 train_time:84177ms step_avg:60.34ms
step:1396/2225 train_time:84237ms step_avg:60.34ms
step:1397/2225 train_time:84299ms step_avg:60.34ms
step:1398/2225 train_time:84358ms step_avg:60.34ms
step:1399/2225 train_time:84420ms step_avg:60.34ms
step:1400/2225 train_time:84480ms step_avg:60.34ms
step:1401/2225 train_time:84541ms step_avg:60.34ms
step:1402/2225 train_time:84601ms step_avg:60.34ms
step:1403/2225 train_time:84662ms step_avg:60.34ms
step:1404/2225 train_time:84722ms step_avg:60.34ms
step:1405/2225 train_time:84784ms step_avg:60.34ms
step:1406/2225 train_time:84844ms step_avg:60.34ms
step:1407/2225 train_time:84906ms step_avg:60.35ms
step:1408/2225 train_time:84967ms step_avg:60.35ms
step:1409/2225 train_time:85029ms step_avg:60.35ms
step:1410/2225 train_time:85089ms step_avg:60.35ms
step:1411/2225 train_time:85150ms step_avg:60.35ms
step:1412/2225 train_time:85210ms step_avg:60.35ms
step:1413/2225 train_time:85272ms step_avg:60.35ms
step:1414/2225 train_time:85332ms step_avg:60.35ms
step:1415/2225 train_time:85393ms step_avg:60.35ms
step:1416/2225 train_time:85453ms step_avg:60.35ms
step:1417/2225 train_time:85514ms step_avg:60.35ms
step:1418/2225 train_time:85575ms step_avg:60.35ms
step:1419/2225 train_time:85636ms step_avg:60.35ms
step:1420/2225 train_time:85696ms step_avg:60.35ms
step:1421/2225 train_time:85757ms step_avg:60.35ms
step:1422/2225 train_time:85817ms step_avg:60.35ms
step:1423/2225 train_time:85878ms step_avg:60.35ms
step:1424/2225 train_time:85937ms step_avg:60.35ms
step:1425/2225 train_time:85999ms step_avg:60.35ms
step:1426/2225 train_time:86058ms step_avg:60.35ms
step:1427/2225 train_time:86120ms step_avg:60.35ms
step:1428/2225 train_time:86180ms step_avg:60.35ms
step:1429/2225 train_time:86242ms step_avg:60.35ms
step:1430/2225 train_time:86301ms step_avg:60.35ms
step:1431/2225 train_time:86363ms step_avg:60.35ms
step:1432/2225 train_time:86423ms step_avg:60.35ms
step:1433/2225 train_time:86484ms step_avg:60.35ms
step:1434/2225 train_time:86544ms step_avg:60.35ms
step:1435/2225 train_time:86606ms step_avg:60.35ms
step:1436/2225 train_time:86667ms step_avg:60.35ms
step:1437/2225 train_time:86729ms step_avg:60.35ms
step:1438/2225 train_time:86789ms step_avg:60.35ms
step:1439/2225 train_time:86851ms step_avg:60.35ms
step:1440/2225 train_time:86911ms step_avg:60.35ms
step:1441/2225 train_time:86972ms step_avg:60.36ms
step:1442/2225 train_time:87032ms step_avg:60.36ms
step:1443/2225 train_time:87093ms step_avg:60.36ms
step:1444/2225 train_time:87154ms step_avg:60.36ms
step:1445/2225 train_time:87215ms step_avg:60.36ms
step:1446/2225 train_time:87275ms step_avg:60.36ms
step:1447/2225 train_time:87336ms step_avg:60.36ms
step:1448/2225 train_time:87396ms step_avg:60.36ms
step:1449/2225 train_time:87457ms step_avg:60.36ms
step:1450/2225 train_time:87517ms step_avg:60.36ms
step:1451/2225 train_time:87578ms step_avg:60.36ms
step:1452/2225 train_time:87637ms step_avg:60.36ms
step:1453/2225 train_time:87699ms step_avg:60.36ms
step:1454/2225 train_time:87759ms step_avg:60.36ms
step:1455/2225 train_time:87821ms step_avg:60.36ms
step:1456/2225 train_time:87881ms step_avg:60.36ms
step:1457/2225 train_time:87942ms step_avg:60.36ms
step:1458/2225 train_time:88001ms step_avg:60.36ms
step:1459/2225 train_time:88064ms step_avg:60.36ms
step:1460/2225 train_time:88125ms step_avg:60.36ms
step:1461/2225 train_time:88187ms step_avg:60.36ms
step:1462/2225 train_time:88247ms step_avg:60.36ms
step:1463/2225 train_time:88310ms step_avg:60.36ms
step:1464/2225 train_time:88371ms step_avg:60.36ms
step:1465/2225 train_time:88433ms step_avg:60.36ms
step:1466/2225 train_time:88494ms step_avg:60.36ms
step:1467/2225 train_time:88555ms step_avg:60.36ms
step:1468/2225 train_time:88615ms step_avg:60.36ms
step:1469/2225 train_time:88676ms step_avg:60.37ms
step:1470/2225 train_time:88737ms step_avg:60.37ms
step:1471/2225 train_time:88799ms step_avg:60.37ms
step:1472/2225 train_time:88859ms step_avg:60.37ms
step:1473/2225 train_time:88921ms step_avg:60.37ms
step:1474/2225 train_time:88981ms step_avg:60.37ms
step:1475/2225 train_time:89044ms step_avg:60.37ms
step:1476/2225 train_time:89103ms step_avg:60.37ms
step:1477/2225 train_time:89166ms step_avg:60.37ms
step:1478/2225 train_time:89226ms step_avg:60.37ms
step:1479/2225 train_time:89289ms step_avg:60.37ms
step:1480/2225 train_time:89349ms step_avg:60.37ms
step:1481/2225 train_time:89411ms step_avg:60.37ms
step:1482/2225 train_time:89471ms step_avg:60.37ms
step:1483/2225 train_time:89533ms step_avg:60.37ms
step:1484/2225 train_time:89593ms step_avg:60.37ms
step:1485/2225 train_time:89655ms step_avg:60.37ms
step:1486/2225 train_time:89715ms step_avg:60.37ms
step:1487/2225 train_time:89776ms step_avg:60.37ms
step:1488/2225 train_time:89837ms step_avg:60.37ms
step:1489/2225 train_time:89898ms step_avg:60.38ms
step:1490/2225 train_time:89958ms step_avg:60.37ms
step:1491/2225 train_time:90021ms step_avg:60.38ms
step:1492/2225 train_time:90081ms step_avg:60.38ms
step:1493/2225 train_time:90143ms step_avg:60.38ms
step:1494/2225 train_time:90203ms step_avg:60.38ms
step:1495/2225 train_time:90265ms step_avg:60.38ms
step:1496/2225 train_time:90326ms step_avg:60.38ms
step:1497/2225 train_time:90387ms step_avg:60.38ms
step:1498/2225 train_time:90448ms step_avg:60.38ms
step:1499/2225 train_time:90511ms step_avg:60.38ms
step:1500/2225 train_time:90572ms step_avg:60.38ms
step:1500/2225 val_loss:3.4381 train_time:90633ms step_avg:60.42ms
step:1501/2225 train_time:90658ms step_avg:60.40ms
step:1502/2225 train_time:90695ms step_avg:60.38ms
step:1503/2225 train_time:90756ms step_avg:60.38ms
step:1504/2225 train_time:90817ms step_avg:60.38ms
step:1505/2225 train_time:90880ms step_avg:60.39ms
step:1506/2225 train_time:90943ms step_avg:60.39ms
step:1507/2225 train_time:91004ms step_avg:60.39ms
step:1508/2225 train_time:91063ms step_avg:60.39ms
step:1509/2225 train_time:91124ms step_avg:60.39ms
step:1510/2225 train_time:91184ms step_avg:60.39ms
step:1511/2225 train_time:91245ms step_avg:60.39ms
step:1512/2225 train_time:91304ms step_avg:60.39ms
step:1513/2225 train_time:91365ms step_avg:60.39ms
step:1514/2225 train_time:91425ms step_avg:60.39ms
step:1515/2225 train_time:91488ms step_avg:60.39ms
step:1516/2225 train_time:91549ms step_avg:60.39ms
step:1517/2225 train_time:91613ms step_avg:60.39ms
step:1518/2225 train_time:91674ms step_avg:60.39ms
step:1519/2225 train_time:91736ms step_avg:60.39ms
step:1520/2225 train_time:91797ms step_avg:60.39ms
step:1521/2225 train_time:91860ms step_avg:60.39ms
step:1522/2225 train_time:91920ms step_avg:60.39ms
step:1523/2225 train_time:91982ms step_avg:60.40ms
step:1524/2225 train_time:92042ms step_avg:60.39ms
step:1525/2225 train_time:92104ms step_avg:60.40ms
step:1526/2225 train_time:92164ms step_avg:60.40ms
step:1527/2225 train_time:92225ms step_avg:60.40ms
step:1528/2225 train_time:92285ms step_avg:60.40ms
step:1529/2225 train_time:92346ms step_avg:60.40ms
step:1530/2225 train_time:92407ms step_avg:60.40ms
step:1531/2225 train_time:92469ms step_avg:60.40ms
step:1532/2225 train_time:92530ms step_avg:60.40ms
step:1533/2225 train_time:92592ms step_avg:60.40ms
step:1534/2225 train_time:92654ms step_avg:60.40ms
step:1535/2225 train_time:92716ms step_avg:60.40ms
step:1536/2225 train_time:92776ms step_avg:60.40ms
step:1537/2225 train_time:92838ms step_avg:60.40ms
step:1538/2225 train_time:92898ms step_avg:60.40ms
step:1539/2225 train_time:92961ms step_avg:60.40ms
step:1540/2225 train_time:93021ms step_avg:60.40ms
step:1541/2225 train_time:93082ms step_avg:60.40ms
step:1542/2225 train_time:93142ms step_avg:60.40ms
step:1543/2225 train_time:93203ms step_avg:60.40ms
step:1544/2225 train_time:93263ms step_avg:60.40ms
step:1545/2225 train_time:93324ms step_avg:60.40ms
step:1546/2225 train_time:93384ms step_avg:60.40ms
step:1547/2225 train_time:93446ms step_avg:60.40ms
step:1548/2225 train_time:93507ms step_avg:60.41ms
step:1549/2225 train_time:93570ms step_avg:60.41ms
step:1550/2225 train_time:93630ms step_avg:60.41ms
step:1551/2225 train_time:93692ms step_avg:60.41ms
step:1552/2225 train_time:93753ms step_avg:60.41ms
step:1553/2225 train_time:93815ms step_avg:60.41ms
step:1554/2225 train_time:93875ms step_avg:60.41ms
step:1555/2225 train_time:93936ms step_avg:60.41ms
step:1556/2225 train_time:93997ms step_avg:60.41ms
step:1557/2225 train_time:94059ms step_avg:60.41ms
step:1558/2225 train_time:94120ms step_avg:60.41ms
step:1559/2225 train_time:94181ms step_avg:60.41ms
step:1560/2225 train_time:94241ms step_avg:60.41ms
step:1561/2225 train_time:94303ms step_avg:60.41ms
step:1562/2225 train_time:94363ms step_avg:60.41ms
step:1563/2225 train_time:94424ms step_avg:60.41ms
step:1564/2225 train_time:94485ms step_avg:60.41ms
step:1565/2225 train_time:94548ms step_avg:60.41ms
step:1566/2225 train_time:94609ms step_avg:60.41ms
step:1567/2225 train_time:94671ms step_avg:60.42ms
step:1568/2225 train_time:94732ms step_avg:60.42ms
step:1569/2225 train_time:94794ms step_avg:60.42ms
step:1570/2225 train_time:94854ms step_avg:60.42ms
step:1571/2225 train_time:94916ms step_avg:60.42ms
step:1572/2225 train_time:94976ms step_avg:60.42ms
step:1573/2225 train_time:95038ms step_avg:60.42ms
step:1574/2225 train_time:95098ms step_avg:60.42ms
step:1575/2225 train_time:95159ms step_avg:60.42ms
step:1576/2225 train_time:95220ms step_avg:60.42ms
step:1577/2225 train_time:95281ms step_avg:60.42ms
step:1578/2225 train_time:95341ms step_avg:60.42ms
step:1579/2225 train_time:95403ms step_avg:60.42ms
step:1580/2225 train_time:95464ms step_avg:60.42ms
step:1581/2225 train_time:95525ms step_avg:60.42ms
step:1582/2225 train_time:95586ms step_avg:60.42ms
step:1583/2225 train_time:95649ms step_avg:60.42ms
step:1584/2225 train_time:95711ms step_avg:60.42ms
step:1585/2225 train_time:95773ms step_avg:60.42ms
step:1586/2225 train_time:95833ms step_avg:60.42ms
step:1587/2225 train_time:95894ms step_avg:60.42ms
step:1588/2225 train_time:95955ms step_avg:60.43ms
step:1589/2225 train_time:96017ms step_avg:60.43ms
step:1590/2225 train_time:96076ms step_avg:60.43ms
step:1591/2225 train_time:96137ms step_avg:60.43ms
step:1592/2225 train_time:96198ms step_avg:60.43ms
step:1593/2225 train_time:96260ms step_avg:60.43ms
step:1594/2225 train_time:96321ms step_avg:60.43ms
step:1595/2225 train_time:96383ms step_avg:60.43ms
step:1596/2225 train_time:96443ms step_avg:60.43ms
step:1597/2225 train_time:96505ms step_avg:60.43ms
step:1598/2225 train_time:96566ms step_avg:60.43ms
step:1599/2225 train_time:96628ms step_avg:60.43ms
step:1600/2225 train_time:96689ms step_avg:60.43ms
step:1601/2225 train_time:96752ms step_avg:60.43ms
step:1602/2225 train_time:96812ms step_avg:60.43ms
step:1603/2225 train_time:96873ms step_avg:60.43ms
step:1604/2225 train_time:96934ms step_avg:60.43ms
step:1605/2225 train_time:96996ms step_avg:60.43ms
step:1606/2225 train_time:97055ms step_avg:60.43ms
step:1607/2225 train_time:97117ms step_avg:60.43ms
step:1608/2225 train_time:97177ms step_avg:60.43ms
step:1609/2225 train_time:97239ms step_avg:60.43ms
step:1610/2225 train_time:97300ms step_avg:60.43ms
step:1611/2225 train_time:97362ms step_avg:60.44ms
step:1612/2225 train_time:97422ms step_avg:60.44ms
step:1613/2225 train_time:97484ms step_avg:60.44ms
step:1614/2225 train_time:97544ms step_avg:60.44ms
step:1615/2225 train_time:97606ms step_avg:60.44ms
step:1616/2225 train_time:97667ms step_avg:60.44ms
step:1617/2225 train_time:97729ms step_avg:60.44ms
step:1618/2225 train_time:97790ms step_avg:60.44ms
step:1619/2225 train_time:97852ms step_avg:60.44ms
step:1620/2225 train_time:97913ms step_avg:60.44ms
step:1621/2225 train_time:97974ms step_avg:60.44ms
step:1622/2225 train_time:98034ms step_avg:60.44ms
step:1623/2225 train_time:98096ms step_avg:60.44ms
step:1624/2225 train_time:98156ms step_avg:60.44ms
step:1625/2225 train_time:98217ms step_avg:60.44ms
step:1626/2225 train_time:98278ms step_avg:60.44ms
step:1627/2225 train_time:98340ms step_avg:60.44ms
step:1628/2225 train_time:98400ms step_avg:60.44ms
step:1629/2225 train_time:98463ms step_avg:60.44ms
step:1630/2225 train_time:98523ms step_avg:60.44ms
step:1631/2225 train_time:98585ms step_avg:60.44ms
step:1632/2225 train_time:98646ms step_avg:60.44ms
step:1633/2225 train_time:98708ms step_avg:60.45ms
step:1634/2225 train_time:98769ms step_avg:60.45ms
step:1635/2225 train_time:98831ms step_avg:60.45ms
step:1636/2225 train_time:98891ms step_avg:60.45ms
step:1637/2225 train_time:98953ms step_avg:60.45ms
step:1638/2225 train_time:99013ms step_avg:60.45ms
step:1639/2225 train_time:99075ms step_avg:60.45ms
step:1640/2225 train_time:99135ms step_avg:60.45ms
step:1641/2225 train_time:99198ms step_avg:60.45ms
step:1642/2225 train_time:99258ms step_avg:60.45ms
step:1643/2225 train_time:99320ms step_avg:60.45ms
step:1644/2225 train_time:99380ms step_avg:60.45ms
step:1645/2225 train_time:99442ms step_avg:60.45ms
step:1646/2225 train_time:99502ms step_avg:60.45ms
step:1647/2225 train_time:99564ms step_avg:60.45ms
step:1648/2225 train_time:99624ms step_avg:60.45ms
step:1649/2225 train_time:99687ms step_avg:60.45ms
step:1650/2225 train_time:99747ms step_avg:60.45ms
step:1651/2225 train_time:99810ms step_avg:60.45ms
step:1652/2225 train_time:99871ms step_avg:60.45ms
step:1653/2225 train_time:99933ms step_avg:60.46ms
step:1654/2225 train_time:99993ms step_avg:60.46ms
step:1655/2225 train_time:100054ms step_avg:60.46ms
step:1656/2225 train_time:100114ms step_avg:60.46ms
step:1657/2225 train_time:100176ms step_avg:60.46ms
step:1658/2225 train_time:100236ms step_avg:60.46ms
step:1659/2225 train_time:100297ms step_avg:60.46ms
step:1660/2225 train_time:100357ms step_avg:60.46ms
step:1661/2225 train_time:100419ms step_avg:60.46ms
step:1662/2225 train_time:100479ms step_avg:60.46ms
step:1663/2225 train_time:100542ms step_avg:60.46ms
step:1664/2225 train_time:100603ms step_avg:60.46ms
step:1665/2225 train_time:100665ms step_avg:60.46ms
step:1666/2225 train_time:100725ms step_avg:60.46ms
step:1667/2225 train_time:100788ms step_avg:60.46ms
step:1668/2225 train_time:100849ms step_avg:60.46ms
step:1669/2225 train_time:100911ms step_avg:60.46ms
step:1670/2225 train_time:100971ms step_avg:60.46ms
step:1671/2225 train_time:101033ms step_avg:60.46ms
step:1672/2225 train_time:101094ms step_avg:60.46ms
step:1673/2225 train_time:101156ms step_avg:60.46ms
step:1674/2225 train_time:101216ms step_avg:60.46ms
step:1675/2225 train_time:101277ms step_avg:60.46ms
step:1676/2225 train_time:101337ms step_avg:60.46ms
step:1677/2225 train_time:101398ms step_avg:60.46ms
step:1678/2225 train_time:101459ms step_avg:60.46ms
step:1679/2225 train_time:101522ms step_avg:60.47ms
step:1680/2225 train_time:101582ms step_avg:60.47ms
step:1681/2225 train_time:101644ms step_avg:60.47ms
step:1682/2225 train_time:101704ms step_avg:60.47ms
step:1683/2225 train_time:101767ms step_avg:60.47ms
step:1684/2225 train_time:101828ms step_avg:60.47ms
step:1685/2225 train_time:101891ms step_avg:60.47ms
step:1686/2225 train_time:101951ms step_avg:60.47ms
step:1687/2225 train_time:102013ms step_avg:60.47ms
step:1688/2225 train_time:102073ms step_avg:60.47ms
step:1689/2225 train_time:102135ms step_avg:60.47ms
step:1690/2225 train_time:102195ms step_avg:60.47ms
step:1691/2225 train_time:102257ms step_avg:60.47ms
step:1692/2225 train_time:102318ms step_avg:60.47ms
step:1693/2225 train_time:102379ms step_avg:60.47ms
step:1694/2225 train_time:102439ms step_avg:60.47ms
step:1695/2225 train_time:102501ms step_avg:60.47ms
step:1696/2225 train_time:102562ms step_avg:60.47ms
step:1697/2225 train_time:102624ms step_avg:60.47ms
step:1698/2225 train_time:102684ms step_avg:60.47ms
step:1699/2225 train_time:102746ms step_avg:60.47ms
step:1700/2225 train_time:102808ms step_avg:60.48ms
step:1701/2225 train_time:102870ms step_avg:60.48ms
step:1702/2225 train_time:102930ms step_avg:60.48ms
step:1703/2225 train_time:102992ms step_avg:60.48ms
step:1704/2225 train_time:103052ms step_avg:60.48ms
step:1705/2225 train_time:103113ms step_avg:60.48ms
step:1706/2225 train_time:103174ms step_avg:60.48ms
step:1707/2225 train_time:103236ms step_avg:60.48ms
step:1708/2225 train_time:103296ms step_avg:60.48ms
step:1709/2225 train_time:103358ms step_avg:60.48ms
step:1710/2225 train_time:103418ms step_avg:60.48ms
step:1711/2225 train_time:103479ms step_avg:60.48ms
step:1712/2225 train_time:103539ms step_avg:60.48ms
step:1713/2225 train_time:103601ms step_avg:60.48ms
step:1714/2225 train_time:103662ms step_avg:60.48ms
step:1715/2225 train_time:103725ms step_avg:60.48ms
step:1716/2225 train_time:103785ms step_avg:60.48ms
step:1717/2225 train_time:103847ms step_avg:60.48ms
step:1718/2225 train_time:103907ms step_avg:60.48ms
step:1719/2225 train_time:103970ms step_avg:60.48ms
step:1720/2225 train_time:104030ms step_avg:60.48ms
step:1721/2225 train_time:104092ms step_avg:60.48ms
step:1722/2225 train_time:104153ms step_avg:60.48ms
step:1723/2225 train_time:104215ms step_avg:60.48ms
step:1724/2225 train_time:104275ms step_avg:60.48ms
step:1725/2225 train_time:104336ms step_avg:60.48ms
step:1726/2225 train_time:104396ms step_avg:60.48ms
step:1727/2225 train_time:104458ms step_avg:60.49ms
step:1728/2225 train_time:104519ms step_avg:60.49ms
step:1729/2225 train_time:104580ms step_avg:60.49ms
step:1730/2225 train_time:104640ms step_avg:60.49ms
step:1731/2225 train_time:104703ms step_avg:60.49ms
step:1732/2225 train_time:104764ms step_avg:60.49ms
step:1733/2225 train_time:104826ms step_avg:60.49ms
step:1734/2225 train_time:104886ms step_avg:60.49ms
step:1735/2225 train_time:104949ms step_avg:60.49ms
step:1736/2225 train_time:105009ms step_avg:60.49ms
step:1737/2225 train_time:105071ms step_avg:60.49ms
step:1738/2225 train_time:105131ms step_avg:60.49ms
step:1739/2225 train_time:105192ms step_avg:60.49ms
step:1740/2225 train_time:105253ms step_avg:60.49ms
step:1741/2225 train_time:105315ms step_avg:60.49ms
step:1742/2225 train_time:105375ms step_avg:60.49ms
step:1743/2225 train_time:105436ms step_avg:60.49ms
step:1744/2225 train_time:105496ms step_avg:60.49ms
step:1745/2225 train_time:105557ms step_avg:60.49ms
step:1746/2225 train_time:105618ms step_avg:60.49ms
step:1747/2225 train_time:105681ms step_avg:60.49ms
step:1748/2225 train_time:105742ms step_avg:60.49ms
step:1749/2225 train_time:105804ms step_avg:60.49ms
step:1750/2225 train_time:105864ms step_avg:60.49ms
step:1750/2225 val_loss:3.3744 train_time:105926ms step_avg:60.53ms
step:1751/2225 train_time:105950ms step_avg:60.51ms
step:1752/2225 train_time:105987ms step_avg:60.49ms
step:1753/2225 train_time:106052ms step_avg:60.50ms
step:1754/2225 train_time:106116ms step_avg:60.50ms
step:1755/2225 train_time:106179ms step_avg:60.50ms
step:1756/2225 train_time:106239ms step_avg:60.50ms
step:1757/2225 train_time:106300ms step_avg:60.50ms
step:1758/2225 train_time:106360ms step_avg:60.50ms
step:1759/2225 train_time:106421ms step_avg:60.50ms
step:1760/2225 train_time:106480ms step_avg:60.50ms
step:1761/2225 train_time:106542ms step_avg:60.50ms
step:1762/2225 train_time:106601ms step_avg:60.50ms
step:1763/2225 train_time:106662ms step_avg:60.50ms
step:1764/2225 train_time:106721ms step_avg:60.50ms
step:1765/2225 train_time:106783ms step_avg:60.50ms
step:1766/2225 train_time:106843ms step_avg:60.50ms
step:1767/2225 train_time:106906ms step_avg:60.50ms
step:1768/2225 train_time:106967ms step_avg:60.50ms
step:1769/2225 train_time:107031ms step_avg:60.50ms
step:1770/2225 train_time:107092ms step_avg:60.50ms
step:1771/2225 train_time:107154ms step_avg:60.50ms
step:1772/2225 train_time:107215ms step_avg:60.50ms
step:1773/2225 train_time:107277ms step_avg:60.51ms
step:1774/2225 train_time:107337ms step_avg:60.51ms
step:1775/2225 train_time:107398ms step_avg:60.51ms
step:1776/2225 train_time:107457ms step_avg:60.51ms
step:1777/2225 train_time:107519ms step_avg:60.51ms
step:1778/2225 train_time:107579ms step_avg:60.51ms
step:1779/2225 train_time:107640ms step_avg:60.51ms
step:1780/2225 train_time:107699ms step_avg:60.51ms
step:1781/2225 train_time:107760ms step_avg:60.51ms
step:1782/2225 train_time:107820ms step_avg:60.51ms
step:1783/2225 train_time:107882ms step_avg:60.51ms
step:1784/2225 train_time:107944ms step_avg:60.51ms
step:1785/2225 train_time:108008ms step_avg:60.51ms
step:1786/2225 train_time:108069ms step_avg:60.51ms
step:1787/2225 train_time:108132ms step_avg:60.51ms
step:1788/2225 train_time:108192ms step_avg:60.51ms
step:1789/2225 train_time:108253ms step_avg:60.51ms
step:1790/2225 train_time:108313ms step_avg:60.51ms
step:1791/2225 train_time:108375ms step_avg:60.51ms
step:1792/2225 train_time:108435ms step_avg:60.51ms
step:1793/2225 train_time:108496ms step_avg:60.51ms
step:1794/2225 train_time:108556ms step_avg:60.51ms
step:1795/2225 train_time:108618ms step_avg:60.51ms
step:1796/2225 train_time:108677ms step_avg:60.51ms
step:1797/2225 train_time:108738ms step_avg:60.51ms
step:1798/2225 train_time:108799ms step_avg:60.51ms
step:1799/2225 train_time:108861ms step_avg:60.51ms
step:1800/2225 train_time:108923ms step_avg:60.51ms
step:1801/2225 train_time:108986ms step_avg:60.51ms
step:1802/2225 train_time:109047ms step_avg:60.51ms
step:1803/2225 train_time:109109ms step_avg:60.52ms
step:1804/2225 train_time:109170ms step_avg:60.52ms
step:1805/2225 train_time:109232ms step_avg:60.52ms
step:1806/2225 train_time:109292ms step_avg:60.52ms
step:1807/2225 train_time:109354ms step_avg:60.52ms
step:1808/2225 train_time:109413ms step_avg:60.52ms
step:1809/2225 train_time:109475ms step_avg:60.52ms
step:1810/2225 train_time:109535ms step_avg:60.52ms
step:1811/2225 train_time:109596ms step_avg:60.52ms
step:1812/2225 train_time:109656ms step_avg:60.52ms
step:1813/2225 train_time:109717ms step_avg:60.52ms
step:1814/2225 train_time:109777ms step_avg:60.52ms
step:1815/2225 train_time:109839ms step_avg:60.52ms
step:1816/2225 train_time:109900ms step_avg:60.52ms
step:1817/2225 train_time:109963ms step_avg:60.52ms
step:1818/2225 train_time:110024ms step_avg:60.52ms
step:1819/2225 train_time:110087ms step_avg:60.52ms
step:1820/2225 train_time:110147ms step_avg:60.52ms
step:1821/2225 train_time:110209ms step_avg:60.52ms
step:1822/2225 train_time:110270ms step_avg:60.52ms
step:1823/2225 train_time:110332ms step_avg:60.52ms
step:1824/2225 train_time:110392ms step_avg:60.52ms
step:1825/2225 train_time:110453ms step_avg:60.52ms
step:1826/2225 train_time:110513ms step_avg:60.52ms
step:1827/2225 train_time:110575ms step_avg:60.52ms
step:1828/2225 train_time:110635ms step_avg:60.52ms
step:1829/2225 train_time:110697ms step_avg:60.52ms
step:1830/2225 train_time:110756ms step_avg:60.52ms
step:1831/2225 train_time:110819ms step_avg:60.52ms
step:1832/2225 train_time:110879ms step_avg:60.52ms
step:1833/2225 train_time:110941ms step_avg:60.52ms
step:1834/2225 train_time:111001ms step_avg:60.52ms
step:1835/2225 train_time:111064ms step_avg:60.53ms
step:1836/2225 train_time:111124ms step_avg:60.53ms
step:1837/2225 train_time:111187ms step_avg:60.53ms
step:1838/2225 train_time:111248ms step_avg:60.53ms
step:1839/2225 train_time:111309ms step_avg:60.53ms
step:1840/2225 train_time:111370ms step_avg:60.53ms
step:1841/2225 train_time:111432ms step_avg:60.53ms
step:1842/2225 train_time:111492ms step_avg:60.53ms
step:1843/2225 train_time:111554ms step_avg:60.53ms
step:1844/2225 train_time:111614ms step_avg:60.53ms
step:1845/2225 train_time:111675ms step_avg:60.53ms
step:1846/2225 train_time:111735ms step_avg:60.53ms
step:1847/2225 train_time:111798ms step_avg:60.53ms
step:1848/2225 train_time:111859ms step_avg:60.53ms
step:1849/2225 train_time:111922ms step_avg:60.53ms
step:1850/2225 train_time:111982ms step_avg:60.53ms
step:1851/2225 train_time:112044ms step_avg:60.53ms
step:1852/2225 train_time:112104ms step_avg:60.53ms
step:1853/2225 train_time:112167ms step_avg:60.53ms
step:1854/2225 train_time:112227ms step_avg:60.53ms
step:1855/2225 train_time:112289ms step_avg:60.53ms
step:1856/2225 train_time:112350ms step_avg:60.53ms
step:1857/2225 train_time:112411ms step_avg:60.53ms
step:1858/2225 train_time:112471ms step_avg:60.53ms
step:1859/2225 train_time:112533ms step_avg:60.53ms
step:1860/2225 train_time:112593ms step_avg:60.53ms
step:1861/2225 train_time:112654ms step_avg:60.53ms
step:1862/2225 train_time:112715ms step_avg:60.53ms
step:1863/2225 train_time:112776ms step_avg:60.53ms
step:1864/2225 train_time:112837ms step_avg:60.53ms
step:1865/2225 train_time:112899ms step_avg:60.54ms
step:1866/2225 train_time:112959ms step_avg:60.54ms
step:1867/2225 train_time:113022ms step_avg:60.54ms
step:1868/2225 train_time:113082ms step_avg:60.54ms
step:1869/2225 train_time:113144ms step_avg:60.54ms
step:1870/2225 train_time:113205ms step_avg:60.54ms
step:1871/2225 train_time:113269ms step_avg:60.54ms
step:1872/2225 train_time:113329ms step_avg:60.54ms
step:1873/2225 train_time:113390ms step_avg:60.54ms
step:1874/2225 train_time:113451ms step_avg:60.54ms
step:1875/2225 train_time:113511ms step_avg:60.54ms
step:1876/2225 train_time:113572ms step_avg:60.54ms
step:1877/2225 train_time:113633ms step_avg:60.54ms
step:1878/2225 train_time:113693ms step_avg:60.54ms
step:1879/2225 train_time:113755ms step_avg:60.54ms
step:1880/2225 train_time:113815ms step_avg:60.54ms
step:1881/2225 train_time:113877ms step_avg:60.54ms
step:1882/2225 train_time:113938ms step_avg:60.54ms
step:1883/2225 train_time:114000ms step_avg:60.54ms
step:1884/2225 train_time:114061ms step_avg:60.54ms
step:1885/2225 train_time:114123ms step_avg:60.54ms
step:1886/2225 train_time:114184ms step_avg:60.54ms
step:1887/2225 train_time:114245ms step_avg:60.54ms
step:1888/2225 train_time:114306ms step_avg:60.54ms
step:1889/2225 train_time:114369ms step_avg:60.54ms
step:1890/2225 train_time:114428ms step_avg:60.54ms
step:1891/2225 train_time:114490ms step_avg:60.54ms
step:1892/2225 train_time:114550ms step_avg:60.54ms
step:1893/2225 train_time:114611ms step_avg:60.54ms
step:1894/2225 train_time:114672ms step_avg:60.54ms
step:1895/2225 train_time:114734ms step_avg:60.55ms
step:1896/2225 train_time:114794ms step_avg:60.55ms
step:1897/2225 train_time:114855ms step_avg:60.55ms
step:1898/2225 train_time:114916ms step_avg:60.55ms
step:1899/2225 train_time:114979ms step_avg:60.55ms
step:1900/2225 train_time:115039ms step_avg:60.55ms
step:1901/2225 train_time:115101ms step_avg:60.55ms
step:1902/2225 train_time:115162ms step_avg:60.55ms
step:1903/2225 train_time:115223ms step_avg:60.55ms
step:1904/2225 train_time:115284ms step_avg:60.55ms
step:1905/2225 train_time:115347ms step_avg:60.55ms
step:1906/2225 train_time:115407ms step_avg:60.55ms
step:1907/2225 train_time:115469ms step_avg:60.55ms
step:1908/2225 train_time:115528ms step_avg:60.55ms
step:1909/2225 train_time:115590ms step_avg:60.55ms
step:1910/2225 train_time:115650ms step_avg:60.55ms
step:1911/2225 train_time:115712ms step_avg:60.55ms
step:1912/2225 train_time:115773ms step_avg:60.55ms
step:1913/2225 train_time:115834ms step_avg:60.55ms
step:1914/2225 train_time:115895ms step_avg:60.55ms
step:1915/2225 train_time:115956ms step_avg:60.55ms
step:1916/2225 train_time:116017ms step_avg:60.55ms
step:1917/2225 train_time:116079ms step_avg:60.55ms
step:1918/2225 train_time:116139ms step_avg:60.55ms
step:1919/2225 train_time:116201ms step_avg:60.55ms
step:1920/2225 train_time:116262ms step_avg:60.55ms
step:1921/2225 train_time:116324ms step_avg:60.55ms
step:1922/2225 train_time:116384ms step_avg:60.55ms
step:1923/2225 train_time:116447ms step_avg:60.55ms
step:1924/2225 train_time:116507ms step_avg:60.55ms
step:1925/2225 train_time:116569ms step_avg:60.56ms
step:1926/2225 train_time:116629ms step_avg:60.56ms
step:1927/2225 train_time:116691ms step_avg:60.56ms
step:1928/2225 train_time:116751ms step_avg:60.56ms
step:1929/2225 train_time:116813ms step_avg:60.56ms
step:1930/2225 train_time:116873ms step_avg:60.56ms
step:1931/2225 train_time:116935ms step_avg:60.56ms
step:1932/2225 train_time:116995ms step_avg:60.56ms
step:1933/2225 train_time:117057ms step_avg:60.56ms
step:1934/2225 train_time:117118ms step_avg:60.56ms
step:1935/2225 train_time:117181ms step_avg:60.56ms
step:1936/2225 train_time:117241ms step_avg:60.56ms
step:1937/2225 train_time:117303ms step_avg:60.56ms
step:1938/2225 train_time:117363ms step_avg:60.56ms
step:1939/2225 train_time:117426ms step_avg:60.56ms
step:1940/2225 train_time:117486ms step_avg:60.56ms
step:1941/2225 train_time:117548ms step_avg:60.56ms
step:1942/2225 train_time:117608ms step_avg:60.56ms
step:1943/2225 train_time:117670ms step_avg:60.56ms
step:1944/2225 train_time:117730ms step_avg:60.56ms
step:1945/2225 train_time:117792ms step_avg:60.56ms
step:1946/2225 train_time:117852ms step_avg:60.56ms
step:1947/2225 train_time:117913ms step_avg:60.56ms
step:1948/2225 train_time:117974ms step_avg:60.56ms
step:1949/2225 train_time:118036ms step_avg:60.56ms
step:1950/2225 train_time:118096ms step_avg:60.56ms
step:1951/2225 train_time:118158ms step_avg:60.56ms
step:1952/2225 train_time:118219ms step_avg:60.56ms
step:1953/2225 train_time:118282ms step_avg:60.56ms
step:1954/2225 train_time:118342ms step_avg:60.56ms
step:1955/2225 train_time:118404ms step_avg:60.56ms
step:1956/2225 train_time:118464ms step_avg:60.56ms
step:1957/2225 train_time:118527ms step_avg:60.57ms
step:1958/2225 train_time:118587ms step_avg:60.57ms
step:1959/2225 train_time:118649ms step_avg:60.57ms
step:1960/2225 train_time:118709ms step_avg:60.57ms
step:1961/2225 train_time:118771ms step_avg:60.57ms
step:1962/2225 train_time:118831ms step_avg:60.57ms
step:1963/2225 train_time:118892ms step_avg:60.57ms
step:1964/2225 train_time:118952ms step_avg:60.57ms
step:1965/2225 train_time:119014ms step_avg:60.57ms
step:1966/2225 train_time:119074ms step_avg:60.57ms
step:1967/2225 train_time:119136ms step_avg:60.57ms
step:1968/2225 train_time:119196ms step_avg:60.57ms
step:1969/2225 train_time:119259ms step_avg:60.57ms
step:1970/2225 train_time:119320ms step_avg:60.57ms
step:1971/2225 train_time:119382ms step_avg:60.57ms
step:1972/2225 train_time:119442ms step_avg:60.57ms
step:1973/2225 train_time:119504ms step_avg:60.57ms
step:1974/2225 train_time:119566ms step_avg:60.57ms
step:1975/2225 train_time:119628ms step_avg:60.57ms
step:1976/2225 train_time:119688ms step_avg:60.57ms
step:1977/2225 train_time:119749ms step_avg:60.57ms
step:1978/2225 train_time:119810ms step_avg:60.57ms
step:1979/2225 train_time:119872ms step_avg:60.57ms
step:1980/2225 train_time:119933ms step_avg:60.57ms
step:1981/2225 train_time:119994ms step_avg:60.57ms
step:1982/2225 train_time:120054ms step_avg:60.57ms
step:1983/2225 train_time:120115ms step_avg:60.57ms
step:1984/2225 train_time:120176ms step_avg:60.57ms
step:1985/2225 train_time:120238ms step_avg:60.57ms
step:1986/2225 train_time:120299ms step_avg:60.57ms
step:1987/2225 train_time:120361ms step_avg:60.57ms
step:1988/2225 train_time:120421ms step_avg:60.57ms
step:1989/2225 train_time:120483ms step_avg:60.57ms
step:1990/2225 train_time:120543ms step_avg:60.57ms
step:1991/2225 train_time:120605ms step_avg:60.58ms
step:1992/2225 train_time:120666ms step_avg:60.58ms
step:1993/2225 train_time:120728ms step_avg:60.58ms
step:1994/2225 train_time:120788ms step_avg:60.58ms
step:1995/2225 train_time:120850ms step_avg:60.58ms
step:1996/2225 train_time:120911ms step_avg:60.58ms
step:1997/2225 train_time:120972ms step_avg:60.58ms
step:1998/2225 train_time:121033ms step_avg:60.58ms
step:1999/2225 train_time:121094ms step_avg:60.58ms
step:2000/2225 train_time:121155ms step_avg:60.58ms
step:2000/2225 val_loss:3.3202 train_time:121217ms step_avg:60.61ms
step:2001/2225 train_time:121244ms step_avg:60.59ms
step:2002/2225 train_time:121279ms step_avg:60.58ms
step:2003/2225 train_time:121345ms step_avg:60.58ms
step:2004/2225 train_time:121408ms step_avg:60.58ms
step:2005/2225 train_time:121470ms step_avg:60.58ms
step:2006/2225 train_time:121531ms step_avg:60.58ms
step:2007/2225 train_time:121592ms step_avg:60.58ms
step:2008/2225 train_time:121653ms step_avg:60.58ms
step:2009/2225 train_time:121715ms step_avg:60.58ms
step:2010/2225 train_time:121774ms step_avg:60.58ms
step:2011/2225 train_time:121835ms step_avg:60.58ms
step:2012/2225 train_time:121894ms step_avg:60.58ms
step:2013/2225 train_time:121955ms step_avg:60.58ms
step:2014/2225 train_time:122015ms step_avg:60.58ms
step:2015/2225 train_time:122076ms step_avg:60.58ms
step:2016/2225 train_time:122137ms step_avg:60.58ms
step:2017/2225 train_time:122200ms step_avg:60.58ms
step:2018/2225 train_time:122262ms step_avg:60.59ms
step:2019/2225 train_time:122325ms step_avg:60.59ms
step:2020/2225 train_time:122386ms step_avg:60.59ms
step:2021/2225 train_time:122449ms step_avg:60.59ms
step:2022/2225 train_time:122510ms step_avg:60.59ms
step:2023/2225 train_time:122572ms step_avg:60.59ms
step:2024/2225 train_time:122633ms step_avg:60.59ms
step:2025/2225 train_time:122695ms step_avg:60.59ms
step:2026/2225 train_time:122754ms step_avg:60.59ms
step:2027/2225 train_time:122815ms step_avg:60.59ms
step:2028/2225 train_time:122874ms step_avg:60.59ms
step:2029/2225 train_time:122935ms step_avg:60.59ms
step:2030/2225 train_time:122995ms step_avg:60.59ms
step:2031/2225 train_time:123056ms step_avg:60.59ms
step:2032/2225 train_time:123117ms step_avg:60.59ms
step:2033/2225 train_time:123179ms step_avg:60.59ms
step:2034/2225 train_time:123240ms step_avg:60.59ms
step:2035/2225 train_time:123303ms step_avg:60.59ms
step:2036/2225 train_time:123363ms step_avg:60.59ms
step:2037/2225 train_time:123425ms step_avg:60.59ms
step:2038/2225 train_time:123486ms step_avg:60.59ms
step:2039/2225 train_time:123548ms step_avg:60.59ms
step:2040/2225 train_time:123609ms step_avg:60.59ms
step:2041/2225 train_time:123671ms step_avg:60.59ms
step:2042/2225 train_time:123732ms step_avg:60.59ms
step:2043/2225 train_time:123793ms step_avg:60.59ms
step:2044/2225 train_time:123853ms step_avg:60.59ms
step:2045/2225 train_time:123915ms step_avg:60.59ms
step:2046/2225 train_time:123974ms step_avg:60.59ms
step:2047/2225 train_time:124036ms step_avg:60.59ms
step:2048/2225 train_time:124096ms step_avg:60.59ms
step:2049/2225 train_time:124158ms step_avg:60.59ms
step:2050/2225 train_time:124219ms step_avg:60.59ms
step:2051/2225 train_time:124281ms step_avg:60.60ms
step:2052/2225 train_time:124342ms step_avg:60.60ms
step:2053/2225 train_time:124404ms step_avg:60.60ms
step:2054/2225 train_time:124465ms step_avg:60.60ms
step:2055/2225 train_time:124527ms step_avg:60.60ms
step:2056/2225 train_time:124588ms step_avg:60.60ms
step:2057/2225 train_time:124650ms step_avg:60.60ms
step:2058/2225 train_time:124710ms step_avg:60.60ms
step:2059/2225 train_time:124772ms step_avg:60.60ms
step:2060/2225 train_time:124832ms step_avg:60.60ms
step:2061/2225 train_time:124894ms step_avg:60.60ms
step:2062/2225 train_time:124954ms step_avg:60.60ms
step:2063/2225 train_time:125015ms step_avg:60.60ms
step:2064/2225 train_time:125075ms step_avg:60.60ms
step:2065/2225 train_time:125137ms step_avg:60.60ms
step:2066/2225 train_time:125198ms step_avg:60.60ms
step:2067/2225 train_time:125260ms step_avg:60.60ms
step:2068/2225 train_time:125320ms step_avg:60.60ms
step:2069/2225 train_time:125382ms step_avg:60.60ms
step:2070/2225 train_time:125442ms step_avg:60.60ms
step:2071/2225 train_time:125504ms step_avg:60.60ms
step:2072/2225 train_time:125564ms step_avg:60.60ms
step:2073/2225 train_time:125626ms step_avg:60.60ms
step:2074/2225 train_time:125687ms step_avg:60.60ms
step:2075/2225 train_time:125749ms step_avg:60.60ms
step:2076/2225 train_time:125809ms step_avg:60.60ms
step:2077/2225 train_time:125871ms step_avg:60.60ms
step:2078/2225 train_time:125931ms step_avg:60.60ms
step:2079/2225 train_time:125993ms step_avg:60.60ms
step:2080/2225 train_time:126054ms step_avg:60.60ms
step:2081/2225 train_time:126116ms step_avg:60.60ms
step:2082/2225 train_time:126176ms step_avg:60.60ms
step:2083/2225 train_time:126238ms step_avg:60.60ms
step:2084/2225 train_time:126298ms step_avg:60.60ms
step:2085/2225 train_time:126361ms step_avg:60.60ms
step:2086/2225 train_time:126421ms step_avg:60.60ms
step:2087/2225 train_time:126483ms step_avg:60.61ms
step:2088/2225 train_time:126543ms step_avg:60.61ms
step:2089/2225 train_time:126606ms step_avg:60.61ms
step:2090/2225 train_time:126667ms step_avg:60.61ms
step:2091/2225 train_time:126728ms step_avg:60.61ms
step:2092/2225 train_time:126789ms step_avg:60.61ms
step:2093/2225 train_time:126851ms step_avg:60.61ms
step:2094/2225 train_time:126911ms step_avg:60.61ms
step:2095/2225 train_time:126972ms step_avg:60.61ms
step:2096/2225 train_time:127033ms step_avg:60.61ms
step:2097/2225 train_time:127096ms step_avg:60.61ms
step:2098/2225 train_time:127157ms step_avg:60.61ms
step:2099/2225 train_time:127218ms step_avg:60.61ms
step:2100/2225 train_time:127279ms step_avg:60.61ms
step:2101/2225 train_time:127340ms step_avg:60.61ms
step:2102/2225 train_time:127401ms step_avg:60.61ms
step:2103/2225 train_time:127462ms step_avg:60.61ms
step:2104/2225 train_time:127522ms step_avg:60.61ms
step:2105/2225 train_time:127584ms step_avg:60.61ms
step:2106/2225 train_time:127646ms step_avg:60.61ms
step:2107/2225 train_time:127708ms step_avg:60.61ms
step:2108/2225 train_time:127768ms step_avg:60.61ms
step:2109/2225 train_time:127830ms step_avg:60.61ms
step:2110/2225 train_time:127890ms step_avg:60.61ms
step:2111/2225 train_time:127951ms step_avg:60.61ms
step:2112/2225 train_time:128012ms step_avg:60.61ms
step:2113/2225 train_time:128074ms step_avg:60.61ms
step:2114/2225 train_time:128134ms step_avg:60.61ms
step:2115/2225 train_time:128197ms step_avg:60.61ms
step:2116/2225 train_time:128257ms step_avg:60.61ms
step:2117/2225 train_time:128319ms step_avg:60.61ms
step:2118/2225 train_time:128380ms step_avg:60.61ms
step:2119/2225 train_time:128441ms step_avg:60.61ms
step:2120/2225 train_time:128502ms step_avg:60.61ms
step:2121/2225 train_time:128563ms step_avg:60.61ms
step:2122/2225 train_time:128624ms step_avg:60.61ms
step:2123/2225 train_time:128686ms step_avg:60.62ms
step:2124/2225 train_time:128747ms step_avg:60.62ms
step:2125/2225 train_time:128808ms step_avg:60.62ms
step:2126/2225 train_time:128868ms step_avg:60.62ms
step:2127/2225 train_time:128930ms step_avg:60.62ms
step:2128/2225 train_time:128991ms step_avg:60.62ms
step:2129/2225 train_time:129053ms step_avg:60.62ms
step:2130/2225 train_time:129114ms step_avg:60.62ms
step:2131/2225 train_time:129176ms step_avg:60.62ms
step:2132/2225 train_time:129237ms step_avg:60.62ms
step:2133/2225 train_time:129299ms step_avg:60.62ms
step:2134/2225 train_time:129360ms step_avg:60.62ms
step:2135/2225 train_time:129421ms step_avg:60.62ms
step:2136/2225 train_time:129481ms step_avg:60.62ms
step:2137/2225 train_time:129543ms step_avg:60.62ms
step:2138/2225 train_time:129603ms step_avg:60.62ms
step:2139/2225 train_time:129665ms step_avg:60.62ms
step:2140/2225 train_time:129725ms step_avg:60.62ms
step:2141/2225 train_time:129787ms step_avg:60.62ms
step:2142/2225 train_time:129848ms step_avg:60.62ms
step:2143/2225 train_time:129910ms step_avg:60.62ms
step:2144/2225 train_time:129970ms step_avg:60.62ms
step:2145/2225 train_time:130033ms step_avg:60.62ms
step:2146/2225 train_time:130094ms step_avg:60.62ms
step:2147/2225 train_time:130155ms step_avg:60.62ms
step:2148/2225 train_time:130215ms step_avg:60.62ms
step:2149/2225 train_time:130277ms step_avg:60.62ms
step:2150/2225 train_time:130337ms step_avg:60.62ms
step:2151/2225 train_time:130399ms step_avg:60.62ms
step:2152/2225 train_time:130460ms step_avg:60.62ms
step:2153/2225 train_time:130521ms step_avg:60.62ms
step:2154/2225 train_time:130581ms step_avg:60.62ms
step:2155/2225 train_time:130643ms step_avg:60.62ms
step:2156/2225 train_time:130703ms step_avg:60.62ms
step:2157/2225 train_time:130765ms step_avg:60.62ms
step:2158/2225 train_time:130825ms step_avg:60.62ms
step:2159/2225 train_time:130887ms step_avg:60.62ms
step:2160/2225 train_time:130948ms step_avg:60.62ms
step:2161/2225 train_time:131010ms step_avg:60.62ms
step:2162/2225 train_time:131071ms step_avg:60.62ms
step:2163/2225 train_time:131133ms step_avg:60.63ms
step:2164/2225 train_time:131194ms step_avg:60.63ms
step:2165/2225 train_time:131256ms step_avg:60.63ms
step:2166/2225 train_time:131317ms step_avg:60.63ms
step:2167/2225 train_time:131379ms step_avg:60.63ms
step:2168/2225 train_time:131439ms step_avg:60.63ms
step:2169/2225 train_time:131500ms step_avg:60.63ms
step:2170/2225 train_time:131560ms step_avg:60.63ms
step:2171/2225 train_time:131622ms step_avg:60.63ms
step:2172/2225 train_time:131682ms step_avg:60.63ms
step:2173/2225 train_time:131744ms step_avg:60.63ms
step:2174/2225 train_time:131805ms step_avg:60.63ms
step:2175/2225 train_time:131868ms step_avg:60.63ms
step:2176/2225 train_time:131928ms step_avg:60.63ms
step:2177/2225 train_time:131990ms step_avg:60.63ms
step:2178/2225 train_time:132051ms step_avg:60.63ms
step:2179/2225 train_time:132113ms step_avg:60.63ms
step:2180/2225 train_time:132174ms step_avg:60.63ms
step:2181/2225 train_time:132237ms step_avg:60.63ms
step:2182/2225 train_time:132297ms step_avg:60.63ms
step:2183/2225 train_time:132359ms step_avg:60.63ms
step:2184/2225 train_time:132419ms step_avg:60.63ms
step:2185/2225 train_time:132481ms step_avg:60.63ms
step:2186/2225 train_time:132541ms step_avg:60.63ms
step:2187/2225 train_time:132603ms step_avg:60.63ms
step:2188/2225 train_time:132664ms step_avg:60.63ms
step:2189/2225 train_time:132725ms step_avg:60.63ms
step:2190/2225 train_time:132786ms step_avg:60.63ms
step:2191/2225 train_time:132849ms step_avg:60.63ms
step:2192/2225 train_time:132909ms step_avg:60.63ms
step:2193/2225 train_time:132971ms step_avg:60.63ms
step:2194/2225 train_time:133031ms step_avg:60.63ms
step:2195/2225 train_time:133094ms step_avg:60.64ms
step:2196/2225 train_time:133155ms step_avg:60.64ms
step:2197/2225 train_time:133217ms step_avg:60.64ms
step:2198/2225 train_time:133278ms step_avg:60.64ms
step:2199/2225 train_time:133340ms step_avg:60.64ms
step:2200/2225 train_time:133401ms step_avg:60.64ms
step:2201/2225 train_time:133463ms step_avg:60.64ms
step:2202/2225 train_time:133523ms step_avg:60.64ms
step:2203/2225 train_time:133585ms step_avg:60.64ms
step:2204/2225 train_time:133646ms step_avg:60.64ms
step:2205/2225 train_time:133708ms step_avg:60.64ms
step:2206/2225 train_time:133769ms step_avg:60.64ms
step:2207/2225 train_time:133831ms step_avg:60.64ms
step:2208/2225 train_time:133891ms step_avg:60.64ms
step:2209/2225 train_time:133953ms step_avg:60.64ms
step:2210/2225 train_time:134013ms step_avg:60.64ms
step:2211/2225 train_time:134075ms step_avg:60.64ms
step:2212/2225 train_time:134135ms step_avg:60.64ms
step:2213/2225 train_time:134198ms step_avg:60.64ms
step:2214/2225 train_time:134258ms step_avg:60.64ms
step:2215/2225 train_time:134320ms step_avg:60.64ms
step:2216/2225 train_time:134381ms step_avg:60.64ms
step:2217/2225 train_time:134443ms step_avg:60.64ms
step:2218/2225 train_time:134503ms step_avg:60.64ms
step:2219/2225 train_time:134565ms step_avg:60.64ms
step:2220/2225 train_time:134625ms step_avg:60.64ms
step:2221/2225 train_time:134687ms step_avg:60.64ms
step:2222/2225 train_time:134748ms step_avg:60.64ms
step:2223/2225 train_time:134810ms step_avg:60.64ms
step:2224/2225 train_time:134870ms step_avg:60.64ms
step:2225/2225 train_time:134932ms step_avg:60.64ms
step:2225/2225 val_loss:3.2784 train_time:134993ms step_avg:60.67ms
peak memory allocated: 29249 MiB reserved: 47336 MiB

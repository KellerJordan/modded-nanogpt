import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i ==7:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections[0]
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i ==4:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2185  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 18 21:36:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   38C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   39C    P0            125W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   37C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   33C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          152885      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          152886      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          152887      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          152888      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          152889      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          152890      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          152891      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          152892      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          152886      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          152887      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          152888      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          152889      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          152890      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          152891      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          152892      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2225 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2225 train_time:132ms step_avg:131.64ms
step:2/2225 train_time:178ms step_avg:89.12ms
step:3/2225 train_time:203ms step_avg:67.63ms
step:4/2225 train_time:247ms step_avg:61.73ms
step:5/2225 train_time:305ms step_avg:61.09ms
step:6/2225 train_time:384ms step_avg:63.97ms
step:7/2225 train_time:423ms step_avg:60.49ms
step:8/2225 train_time:482ms step_avg:60.22ms
step:9/2225 train_time:542ms step_avg:60.20ms
step:10/2225 train_time:601ms step_avg:60.06ms
step:11/2225 train_time:660ms step_avg:60.01ms
step:12/2225 train_time:719ms step_avg:59.89ms
step:13/2225 train_time:779ms step_avg:59.94ms
step:14/2225 train_time:838ms step_avg:59.86ms
step:15/2225 train_time:899ms step_avg:59.91ms
step:16/2225 train_time:958ms step_avg:59.85ms
step:17/2225 train_time:1018ms step_avg:59.90ms
step:18/2225 train_time:1080ms step_avg:60.01ms
step:19/2225 train_time:1145ms step_avg:60.25ms
step:20/2225 train_time:1206ms step_avg:60.30ms
step:21/2225 train_time:1268ms step_avg:60.36ms
step:22/2225 train_time:1327ms step_avg:60.34ms
step:23/2225 train_time:1389ms step_avg:60.37ms
step:24/2225 train_time:1448ms step_avg:60.32ms
step:25/2225 train_time:1509ms step_avg:60.34ms
step:26/2225 train_time:1568ms step_avg:60.30ms
step:27/2225 train_time:1629ms step_avg:60.32ms
step:28/2225 train_time:1688ms step_avg:60.29ms
step:29/2225 train_time:1749ms step_avg:60.31ms
step:30/2225 train_time:1808ms step_avg:60.26ms
step:31/2225 train_time:1868ms step_avg:60.27ms
step:32/2225 train_time:1928ms step_avg:60.25ms
step:33/2225 train_time:1990ms step_avg:60.31ms
step:34/2225 train_time:2051ms step_avg:60.31ms
step:35/2225 train_time:2114ms step_avg:60.40ms
step:36/2225 train_time:2175ms step_avg:60.42ms
step:37/2225 train_time:2237ms step_avg:60.45ms
step:38/2225 train_time:2296ms step_avg:60.43ms
step:39/2225 train_time:2358ms step_avg:60.45ms
step:40/2225 train_time:2417ms step_avg:60.43ms
step:41/2225 train_time:2479ms step_avg:60.46ms
step:42/2225 train_time:2539ms step_avg:60.44ms
step:43/2225 train_time:2599ms step_avg:60.44ms
step:44/2225 train_time:2659ms step_avg:60.43ms
step:45/2225 train_time:2719ms step_avg:60.43ms
step:46/2225 train_time:2779ms step_avg:60.42ms
step:47/2225 train_time:2840ms step_avg:60.43ms
step:48/2225 train_time:2899ms step_avg:60.40ms
step:49/2225 train_time:2959ms step_avg:60.40ms
step:50/2225 train_time:3019ms step_avg:60.38ms
step:51/2225 train_time:3081ms step_avg:60.41ms
step:52/2225 train_time:3140ms step_avg:60.39ms
step:53/2225 train_time:3201ms step_avg:60.40ms
step:54/2225 train_time:3261ms step_avg:60.39ms
step:55/2225 train_time:3322ms step_avg:60.40ms
step:56/2225 train_time:3381ms step_avg:60.38ms
step:57/2225 train_time:3442ms step_avg:60.38ms
step:58/2225 train_time:3500ms step_avg:60.35ms
step:59/2225 train_time:3561ms step_avg:60.36ms
step:60/2225 train_time:3620ms step_avg:60.33ms
step:61/2225 train_time:3680ms step_avg:60.33ms
step:62/2225 train_time:3740ms step_avg:60.32ms
step:63/2225 train_time:3800ms step_avg:60.32ms
step:64/2225 train_time:3859ms step_avg:60.30ms
step:65/2225 train_time:3920ms step_avg:60.30ms
step:66/2225 train_time:3979ms step_avg:60.29ms
step:67/2225 train_time:4040ms step_avg:60.30ms
step:68/2225 train_time:4099ms step_avg:60.28ms
step:69/2225 train_time:4160ms step_avg:60.29ms
step:70/2225 train_time:4220ms step_avg:60.28ms
step:71/2225 train_time:4280ms step_avg:60.29ms
step:72/2225 train_time:4340ms step_avg:60.28ms
step:73/2225 train_time:4400ms step_avg:60.28ms
step:74/2225 train_time:4460ms step_avg:60.27ms
step:75/2225 train_time:4521ms step_avg:60.28ms
step:76/2225 train_time:4580ms step_avg:60.26ms
step:77/2225 train_time:4640ms step_avg:60.27ms
step:78/2225 train_time:4699ms step_avg:60.25ms
step:79/2225 train_time:4759ms step_avg:60.24ms
step:80/2225 train_time:4818ms step_avg:60.23ms
step:81/2225 train_time:4879ms step_avg:60.23ms
step:82/2225 train_time:4938ms step_avg:60.23ms
step:83/2225 train_time:4999ms step_avg:60.23ms
step:84/2225 train_time:5058ms step_avg:60.22ms
step:85/2225 train_time:5119ms step_avg:60.23ms
step:86/2225 train_time:5179ms step_avg:60.22ms
step:87/2225 train_time:5240ms step_avg:60.23ms
step:88/2225 train_time:5299ms step_avg:60.22ms
step:89/2225 train_time:5359ms step_avg:60.22ms
step:90/2225 train_time:5418ms step_avg:60.20ms
step:91/2225 train_time:5479ms step_avg:60.21ms
step:92/2225 train_time:5540ms step_avg:60.21ms
step:93/2225 train_time:5600ms step_avg:60.21ms
step:94/2225 train_time:5659ms step_avg:60.20ms
step:95/2225 train_time:5719ms step_avg:60.20ms
step:96/2225 train_time:5779ms step_avg:60.19ms
step:97/2225 train_time:5839ms step_avg:60.20ms
step:98/2225 train_time:5898ms step_avg:60.19ms
step:99/2225 train_time:5959ms step_avg:60.19ms
step:100/2225 train_time:6018ms step_avg:60.18ms
step:101/2225 train_time:6078ms step_avg:60.18ms
step:102/2225 train_time:6138ms step_avg:60.18ms
step:103/2225 train_time:6199ms step_avg:60.18ms
step:104/2225 train_time:6258ms step_avg:60.18ms
step:105/2225 train_time:6319ms step_avg:60.18ms
step:106/2225 train_time:6378ms step_avg:60.17ms
step:107/2225 train_time:6439ms step_avg:60.18ms
step:108/2225 train_time:6499ms step_avg:60.18ms
step:109/2225 train_time:6559ms step_avg:60.17ms
step:110/2225 train_time:6618ms step_avg:60.16ms
step:111/2225 train_time:6678ms step_avg:60.16ms
step:112/2225 train_time:6737ms step_avg:60.16ms
step:113/2225 train_time:6798ms step_avg:60.16ms
step:114/2225 train_time:6857ms step_avg:60.15ms
step:115/2225 train_time:6918ms step_avg:60.15ms
step:116/2225 train_time:6977ms step_avg:60.15ms
step:117/2225 train_time:7038ms step_avg:60.16ms
step:118/2225 train_time:7097ms step_avg:60.14ms
step:119/2225 train_time:7158ms step_avg:60.15ms
step:120/2225 train_time:7217ms step_avg:60.14ms
step:121/2225 train_time:7278ms step_avg:60.15ms
step:122/2225 train_time:7338ms step_avg:60.15ms
step:123/2225 train_time:7398ms step_avg:60.15ms
step:124/2225 train_time:7458ms step_avg:60.14ms
step:125/2225 train_time:7519ms step_avg:60.15ms
step:126/2225 train_time:7578ms step_avg:60.14ms
step:127/2225 train_time:7638ms step_avg:60.14ms
step:128/2225 train_time:7697ms step_avg:60.13ms
step:129/2225 train_time:7757ms step_avg:60.13ms
step:130/2225 train_time:7816ms step_avg:60.12ms
step:131/2225 train_time:7877ms step_avg:60.13ms
step:132/2225 train_time:7937ms step_avg:60.13ms
step:133/2225 train_time:7997ms step_avg:60.13ms
step:134/2225 train_time:8057ms step_avg:60.13ms
step:135/2225 train_time:8118ms step_avg:60.13ms
step:136/2225 train_time:8177ms step_avg:60.13ms
step:137/2225 train_time:8238ms step_avg:60.13ms
step:138/2225 train_time:8297ms step_avg:60.12ms
step:139/2225 train_time:8358ms step_avg:60.13ms
step:140/2225 train_time:8417ms step_avg:60.12ms
step:141/2225 train_time:8479ms step_avg:60.13ms
step:142/2225 train_time:8538ms step_avg:60.13ms
step:143/2225 train_time:8598ms step_avg:60.13ms
step:144/2225 train_time:8657ms step_avg:60.12ms
step:145/2225 train_time:8717ms step_avg:60.11ms
step:146/2225 train_time:8776ms step_avg:60.11ms
step:147/2225 train_time:8837ms step_avg:60.11ms
step:148/2225 train_time:8896ms step_avg:60.11ms
step:149/2225 train_time:8957ms step_avg:60.11ms
step:150/2225 train_time:9016ms step_avg:60.11ms
step:151/2225 train_time:9077ms step_avg:60.11ms
step:152/2225 train_time:9137ms step_avg:60.11ms
step:153/2225 train_time:9197ms step_avg:60.11ms
step:154/2225 train_time:9256ms step_avg:60.11ms
step:155/2225 train_time:9317ms step_avg:60.11ms
step:156/2225 train_time:9377ms step_avg:60.11ms
step:157/2225 train_time:9438ms step_avg:60.11ms
step:158/2225 train_time:9497ms step_avg:60.11ms
step:159/2225 train_time:9557ms step_avg:60.11ms
step:160/2225 train_time:9617ms step_avg:60.10ms
step:161/2225 train_time:9677ms step_avg:60.10ms
step:162/2225 train_time:9736ms step_avg:60.10ms
step:163/2225 train_time:9797ms step_avg:60.10ms
step:164/2225 train_time:9856ms step_avg:60.10ms
step:165/2225 train_time:9917ms step_avg:60.10ms
step:166/2225 train_time:9976ms step_avg:60.10ms
step:167/2225 train_time:10038ms step_avg:60.10ms
step:168/2225 train_time:10096ms step_avg:60.10ms
step:169/2225 train_time:10157ms step_avg:60.10ms
step:170/2225 train_time:10216ms step_avg:60.09ms
step:171/2225 train_time:10277ms step_avg:60.10ms
step:172/2225 train_time:10337ms step_avg:60.10ms
step:173/2225 train_time:10397ms step_avg:60.10ms
step:174/2225 train_time:10456ms step_avg:60.09ms
step:175/2225 train_time:10517ms step_avg:60.10ms
step:176/2225 train_time:10577ms step_avg:60.09ms
step:177/2225 train_time:10638ms step_avg:60.10ms
step:178/2225 train_time:10697ms step_avg:60.10ms
step:179/2225 train_time:10758ms step_avg:60.10ms
step:180/2225 train_time:10816ms step_avg:60.09ms
step:181/2225 train_time:10877ms step_avg:60.09ms
step:182/2225 train_time:10936ms step_avg:60.09ms
step:183/2225 train_time:10997ms step_avg:60.09ms
step:184/2225 train_time:11056ms step_avg:60.09ms
step:185/2225 train_time:11117ms step_avg:60.09ms
step:186/2225 train_time:11176ms step_avg:60.09ms
step:187/2225 train_time:11236ms step_avg:60.09ms
step:188/2225 train_time:11296ms step_avg:60.08ms
step:189/2225 train_time:11357ms step_avg:60.09ms
step:190/2225 train_time:11416ms step_avg:60.08ms
step:191/2225 train_time:11477ms step_avg:60.09ms
step:192/2225 train_time:11536ms step_avg:60.08ms
step:193/2225 train_time:11597ms step_avg:60.09ms
step:194/2225 train_time:11657ms step_avg:60.09ms
step:195/2225 train_time:11717ms step_avg:60.09ms
step:196/2225 train_time:11776ms step_avg:60.08ms
step:197/2225 train_time:11837ms step_avg:60.08ms
step:198/2225 train_time:11896ms step_avg:60.08ms
step:199/2225 train_time:11957ms step_avg:60.09ms
step:200/2225 train_time:12016ms step_avg:60.08ms
step:201/2225 train_time:12078ms step_avg:60.09ms
step:202/2225 train_time:12137ms step_avg:60.09ms
step:203/2225 train_time:12198ms step_avg:60.09ms
step:204/2225 train_time:12257ms step_avg:60.08ms
step:205/2225 train_time:12317ms step_avg:60.08ms
step:206/2225 train_time:12376ms step_avg:60.08ms
step:207/2225 train_time:12436ms step_avg:60.08ms
step:208/2225 train_time:12496ms step_avg:60.07ms
step:209/2225 train_time:12556ms step_avg:60.08ms
step:210/2225 train_time:12616ms step_avg:60.08ms
step:211/2225 train_time:12677ms step_avg:60.08ms
step:212/2225 train_time:12737ms step_avg:60.08ms
step:213/2225 train_time:12797ms step_avg:60.08ms
step:214/2225 train_time:12857ms step_avg:60.08ms
step:215/2225 train_time:12917ms step_avg:60.08ms
step:216/2225 train_time:12977ms step_avg:60.08ms
step:217/2225 train_time:13038ms step_avg:60.08ms
step:218/2225 train_time:13097ms step_avg:60.08ms
step:219/2225 train_time:13157ms step_avg:60.08ms
step:220/2225 train_time:13216ms step_avg:60.07ms
step:221/2225 train_time:13277ms step_avg:60.08ms
step:222/2225 train_time:13336ms step_avg:60.07ms
step:223/2225 train_time:13396ms step_avg:60.07ms
step:224/2225 train_time:13455ms step_avg:60.07ms
step:225/2225 train_time:13515ms step_avg:60.07ms
step:226/2225 train_time:13576ms step_avg:60.07ms
step:227/2225 train_time:13636ms step_avg:60.07ms
step:228/2225 train_time:13695ms step_avg:60.07ms
step:229/2225 train_time:13756ms step_avg:60.07ms
step:230/2225 train_time:13815ms step_avg:60.07ms
step:231/2225 train_time:13876ms step_avg:60.07ms
step:232/2225 train_time:13936ms step_avg:60.07ms
step:233/2225 train_time:13997ms step_avg:60.07ms
step:234/2225 train_time:14056ms step_avg:60.07ms
step:235/2225 train_time:14117ms step_avg:60.07ms
step:236/2225 train_time:14176ms step_avg:60.07ms
step:237/2225 train_time:14237ms step_avg:60.07ms
step:238/2225 train_time:14296ms step_avg:60.07ms
step:239/2225 train_time:14356ms step_avg:60.07ms
step:240/2225 train_time:14415ms step_avg:60.06ms
step:241/2225 train_time:14476ms step_avg:60.07ms
step:242/2225 train_time:14535ms step_avg:60.06ms
step:243/2225 train_time:14596ms step_avg:60.07ms
step:244/2225 train_time:14655ms step_avg:60.06ms
step:245/2225 train_time:14716ms step_avg:60.07ms
step:246/2225 train_time:14776ms step_avg:60.07ms
step:247/2225 train_time:14837ms step_avg:60.07ms
step:248/2225 train_time:14896ms step_avg:60.07ms
step:249/2225 train_time:14957ms step_avg:60.07ms
step:250/2225 train_time:15016ms step_avg:60.06ms
step:250/2225 val_loss:4.0881 train_time:15077ms step_avg:60.31ms
step:251/2225 train_time:15099ms step_avg:60.15ms
step:252/2225 train_time:15137ms step_avg:60.07ms
step:253/2225 train_time:15204ms step_avg:60.09ms
step:254/2225 train_time:15265ms step_avg:60.10ms
step:255/2225 train_time:15327ms step_avg:60.11ms
step:256/2225 train_time:15387ms step_avg:60.11ms
step:257/2225 train_time:15447ms step_avg:60.11ms
step:258/2225 train_time:15506ms step_avg:60.10ms
step:259/2225 train_time:15566ms step_avg:60.10ms
step:260/2225 train_time:15623ms step_avg:60.09ms
step:261/2225 train_time:15683ms step_avg:60.09ms
step:262/2225 train_time:15741ms step_avg:60.08ms
step:263/2225 train_time:15800ms step_avg:60.08ms
step:264/2225 train_time:15859ms step_avg:60.07ms
step:265/2225 train_time:15918ms step_avg:60.07ms
step:266/2225 train_time:15977ms step_avg:60.06ms
step:267/2225 train_time:16037ms step_avg:60.06ms
step:268/2225 train_time:16098ms step_avg:60.07ms
step:269/2225 train_time:16160ms step_avg:60.07ms
step:270/2225 train_time:16220ms step_avg:60.07ms
step:271/2225 train_time:16282ms step_avg:60.08ms
step:272/2225 train_time:16342ms step_avg:60.08ms
step:273/2225 train_time:16402ms step_avg:60.08ms
step:274/2225 train_time:16461ms step_avg:60.08ms
step:275/2225 train_time:16521ms step_avg:60.08ms
step:276/2225 train_time:16580ms step_avg:60.07ms
step:277/2225 train_time:16640ms step_avg:60.07ms
step:278/2225 train_time:16699ms step_avg:60.07ms
step:279/2225 train_time:16759ms step_avg:60.07ms
step:280/2225 train_time:16817ms step_avg:60.06ms
step:281/2225 train_time:16877ms step_avg:60.06ms
step:282/2225 train_time:16936ms step_avg:60.06ms
step:283/2225 train_time:16996ms step_avg:60.06ms
step:284/2225 train_time:17056ms step_avg:60.06ms
step:285/2225 train_time:17117ms step_avg:60.06ms
step:286/2225 train_time:17177ms step_avg:60.06ms
step:287/2225 train_time:17239ms step_avg:60.07ms
step:288/2225 train_time:17300ms step_avg:60.07ms
step:289/2225 train_time:17361ms step_avg:60.07ms
step:290/2225 train_time:17420ms step_avg:60.07ms
step:291/2225 train_time:17480ms step_avg:60.07ms
step:292/2225 train_time:17539ms step_avg:60.07ms
step:293/2225 train_time:17600ms step_avg:60.07ms
step:294/2225 train_time:17658ms step_avg:60.06ms
step:295/2225 train_time:17718ms step_avg:60.06ms
step:296/2225 train_time:17777ms step_avg:60.06ms
step:297/2225 train_time:17837ms step_avg:60.06ms
step:298/2225 train_time:17896ms step_avg:60.05ms
step:299/2225 train_time:17956ms step_avg:60.05ms
step:300/2225 train_time:18014ms step_avg:60.05ms
step:301/2225 train_time:18075ms step_avg:60.05ms
step:302/2225 train_time:18135ms step_avg:60.05ms
step:303/2225 train_time:18197ms step_avg:60.06ms
step:304/2225 train_time:18257ms step_avg:60.06ms
step:305/2225 train_time:18319ms step_avg:60.06ms
step:306/2225 train_time:18378ms step_avg:60.06ms
step:307/2225 train_time:18439ms step_avg:60.06ms
step:308/2225 train_time:18499ms step_avg:60.06ms
step:309/2225 train_time:18559ms step_avg:60.06ms
step:310/2225 train_time:18618ms step_avg:60.06ms
step:311/2225 train_time:18679ms step_avg:60.06ms
step:312/2225 train_time:18737ms step_avg:60.06ms
step:313/2225 train_time:18797ms step_avg:60.06ms
step:314/2225 train_time:18856ms step_avg:60.05ms
step:315/2225 train_time:18916ms step_avg:60.05ms
step:316/2225 train_time:18974ms step_avg:60.05ms
step:317/2225 train_time:19035ms step_avg:60.05ms
step:318/2225 train_time:19094ms step_avg:60.04ms
step:319/2225 train_time:19155ms step_avg:60.05ms
step:320/2225 train_time:19215ms step_avg:60.05ms
step:321/2225 train_time:19277ms step_avg:60.05ms
step:322/2225 train_time:19337ms step_avg:60.05ms
step:323/2225 train_time:19399ms step_avg:60.06ms
step:324/2225 train_time:19458ms step_avg:60.06ms
step:325/2225 train_time:19519ms step_avg:60.06ms
step:326/2225 train_time:19578ms step_avg:60.05ms
step:327/2225 train_time:19638ms step_avg:60.06ms
step:328/2225 train_time:19697ms step_avg:60.05ms
step:329/2225 train_time:19757ms step_avg:60.05ms
step:330/2225 train_time:19817ms step_avg:60.05ms
step:331/2225 train_time:19877ms step_avg:60.05ms
step:332/2225 train_time:19936ms step_avg:60.05ms
step:333/2225 train_time:19996ms step_avg:60.05ms
step:334/2225 train_time:20054ms step_avg:60.04ms
step:335/2225 train_time:20115ms step_avg:60.05ms
step:336/2225 train_time:20174ms step_avg:60.04ms
step:337/2225 train_time:20235ms step_avg:60.05ms
step:338/2225 train_time:20295ms step_avg:60.05ms
step:339/2225 train_time:20357ms step_avg:60.05ms
step:340/2225 train_time:20417ms step_avg:60.05ms
step:341/2225 train_time:20478ms step_avg:60.05ms
step:342/2225 train_time:20538ms step_avg:60.05ms
step:343/2225 train_time:20599ms step_avg:60.05ms
step:344/2225 train_time:20658ms step_avg:60.05ms
step:345/2225 train_time:20719ms step_avg:60.05ms
step:346/2225 train_time:20777ms step_avg:60.05ms
step:347/2225 train_time:20838ms step_avg:60.05ms
step:348/2225 train_time:20897ms step_avg:60.05ms
step:349/2225 train_time:20957ms step_avg:60.05ms
step:350/2225 train_time:21015ms step_avg:60.04ms
step:351/2225 train_time:21076ms step_avg:60.04ms
step:352/2225 train_time:21135ms step_avg:60.04ms
step:353/2225 train_time:21196ms step_avg:60.04ms
step:354/2225 train_time:21256ms step_avg:60.04ms
step:355/2225 train_time:21317ms step_avg:60.05ms
step:356/2225 train_time:21377ms step_avg:60.05ms
step:357/2225 train_time:21438ms step_avg:60.05ms
step:358/2225 train_time:21498ms step_avg:60.05ms
step:359/2225 train_time:21559ms step_avg:60.05ms
step:360/2225 train_time:21618ms step_avg:60.05ms
step:361/2225 train_time:21679ms step_avg:60.05ms
step:362/2225 train_time:21737ms step_avg:60.05ms
step:363/2225 train_time:21798ms step_avg:60.05ms
step:364/2225 train_time:21857ms step_avg:60.05ms
step:365/2225 train_time:21917ms step_avg:60.05ms
step:366/2225 train_time:21976ms step_avg:60.04ms
step:367/2225 train_time:22037ms step_avg:60.05ms
step:368/2225 train_time:22096ms step_avg:60.04ms
step:369/2225 train_time:22157ms step_avg:60.05ms
step:370/2225 train_time:22217ms step_avg:60.05ms
step:371/2225 train_time:22277ms step_avg:60.05ms
step:372/2225 train_time:22337ms step_avg:60.05ms
step:373/2225 train_time:22399ms step_avg:60.05ms
step:374/2225 train_time:22458ms step_avg:60.05ms
step:375/2225 train_time:22519ms step_avg:60.05ms
step:376/2225 train_time:22578ms step_avg:60.05ms
step:377/2225 train_time:22639ms step_avg:60.05ms
step:378/2225 train_time:22699ms step_avg:60.05ms
step:379/2225 train_time:22759ms step_avg:60.05ms
step:380/2225 train_time:22818ms step_avg:60.05ms
step:381/2225 train_time:22878ms step_avg:60.05ms
step:382/2225 train_time:22937ms step_avg:60.04ms
step:383/2225 train_time:22997ms step_avg:60.04ms
step:384/2225 train_time:23056ms step_avg:60.04ms
step:385/2225 train_time:23118ms step_avg:60.05ms
step:386/2225 train_time:23177ms step_avg:60.04ms
step:387/2225 train_time:23238ms step_avg:60.05ms
step:388/2225 train_time:23298ms step_avg:60.05ms
step:389/2225 train_time:23359ms step_avg:60.05ms
step:390/2225 train_time:23419ms step_avg:60.05ms
step:391/2225 train_time:23479ms step_avg:60.05ms
step:392/2225 train_time:23538ms step_avg:60.05ms
step:393/2225 train_time:23599ms step_avg:60.05ms
step:394/2225 train_time:23658ms step_avg:60.04ms
step:395/2225 train_time:23719ms step_avg:60.05ms
step:396/2225 train_time:23778ms step_avg:60.05ms
step:397/2225 train_time:23838ms step_avg:60.05ms
step:398/2225 train_time:23897ms step_avg:60.04ms
step:399/2225 train_time:23958ms step_avg:60.04ms
step:400/2225 train_time:24017ms step_avg:60.04ms
step:401/2225 train_time:24078ms step_avg:60.04ms
step:402/2225 train_time:24136ms step_avg:60.04ms
step:403/2225 train_time:24198ms step_avg:60.04ms
step:404/2225 train_time:24257ms step_avg:60.04ms
step:405/2225 train_time:24319ms step_avg:60.05ms
step:406/2225 train_time:24378ms step_avg:60.05ms
step:407/2225 train_time:24440ms step_avg:60.05ms
step:408/2225 train_time:24499ms step_avg:60.05ms
step:409/2225 train_time:24559ms step_avg:60.05ms
step:410/2225 train_time:24619ms step_avg:60.05ms
step:411/2225 train_time:24679ms step_avg:60.05ms
step:412/2225 train_time:24738ms step_avg:60.04ms
step:413/2225 train_time:24799ms step_avg:60.04ms
step:414/2225 train_time:24858ms step_avg:60.04ms
step:415/2225 train_time:24919ms step_avg:60.05ms
step:416/2225 train_time:24978ms step_avg:60.04ms
step:417/2225 train_time:25038ms step_avg:60.04ms
step:418/2225 train_time:25098ms step_avg:60.04ms
step:419/2225 train_time:25158ms step_avg:60.04ms
step:420/2225 train_time:25218ms step_avg:60.04ms
step:421/2225 train_time:25279ms step_avg:60.04ms
step:422/2225 train_time:25338ms step_avg:60.04ms
step:423/2225 train_time:25400ms step_avg:60.05ms
step:424/2225 train_time:25459ms step_avg:60.04ms
step:425/2225 train_time:25520ms step_avg:60.05ms
step:426/2225 train_time:25579ms step_avg:60.04ms
step:427/2225 train_time:25639ms step_avg:60.04ms
step:428/2225 train_time:25698ms step_avg:60.04ms
step:429/2225 train_time:25759ms step_avg:60.04ms
step:430/2225 train_time:25818ms step_avg:60.04ms
step:431/2225 train_time:25878ms step_avg:60.04ms
step:432/2225 train_time:25937ms step_avg:60.04ms
step:433/2225 train_time:25997ms step_avg:60.04ms
step:434/2225 train_time:26057ms step_avg:60.04ms
step:435/2225 train_time:26117ms step_avg:60.04ms
step:436/2225 train_time:26176ms step_avg:60.04ms
step:437/2225 train_time:26236ms step_avg:60.04ms
step:438/2225 train_time:26296ms step_avg:60.04ms
step:439/2225 train_time:26357ms step_avg:60.04ms
step:440/2225 train_time:26417ms step_avg:60.04ms
step:441/2225 train_time:26478ms step_avg:60.04ms
step:442/2225 train_time:26537ms step_avg:60.04ms
step:443/2225 train_time:26598ms step_avg:60.04ms
step:444/2225 train_time:26657ms step_avg:60.04ms
step:445/2225 train_time:26718ms step_avg:60.04ms
step:446/2225 train_time:26777ms step_avg:60.04ms
step:447/2225 train_time:26837ms step_avg:60.04ms
step:448/2225 train_time:26897ms step_avg:60.04ms
step:449/2225 train_time:26958ms step_avg:60.04ms
step:450/2225 train_time:27018ms step_avg:60.04ms
step:451/2225 train_time:27078ms step_avg:60.04ms
step:452/2225 train_time:27136ms step_avg:60.04ms
step:453/2225 train_time:27197ms step_avg:60.04ms
step:454/2225 train_time:27256ms step_avg:60.04ms
step:455/2225 train_time:27317ms step_avg:60.04ms
step:456/2225 train_time:27376ms step_avg:60.04ms
step:457/2225 train_time:27437ms step_avg:60.04ms
step:458/2225 train_time:27497ms step_avg:60.04ms
step:459/2225 train_time:27557ms step_avg:60.04ms
step:460/2225 train_time:27617ms step_avg:60.04ms
step:461/2225 train_time:27677ms step_avg:60.04ms
step:462/2225 train_time:27736ms step_avg:60.04ms
step:463/2225 train_time:27797ms step_avg:60.04ms
step:464/2225 train_time:27856ms step_avg:60.04ms
step:465/2225 train_time:27917ms step_avg:60.04ms
step:466/2225 train_time:27976ms step_avg:60.03ms
step:467/2225 train_time:28036ms step_avg:60.03ms
step:468/2225 train_time:28096ms step_avg:60.03ms
step:469/2225 train_time:28156ms step_avg:60.03ms
step:470/2225 train_time:28215ms step_avg:60.03ms
step:471/2225 train_time:28275ms step_avg:60.03ms
step:472/2225 train_time:28335ms step_avg:60.03ms
step:473/2225 train_time:28396ms step_avg:60.03ms
step:474/2225 train_time:28456ms step_avg:60.03ms
step:475/2225 train_time:28517ms step_avg:60.04ms
step:476/2225 train_time:28576ms step_avg:60.03ms
step:477/2225 train_time:28637ms step_avg:60.04ms
step:478/2225 train_time:28697ms step_avg:60.03ms
step:479/2225 train_time:28757ms step_avg:60.04ms
step:480/2225 train_time:28817ms step_avg:60.03ms
step:481/2225 train_time:28878ms step_avg:60.04ms
step:482/2225 train_time:28937ms step_avg:60.04ms
step:483/2225 train_time:28998ms step_avg:60.04ms
step:484/2225 train_time:29057ms step_avg:60.04ms
step:485/2225 train_time:29118ms step_avg:60.04ms
step:486/2225 train_time:29177ms step_avg:60.03ms
step:487/2225 train_time:29237ms step_avg:60.04ms
step:488/2225 train_time:29297ms step_avg:60.03ms
step:489/2225 train_time:29358ms step_avg:60.04ms
step:490/2225 train_time:29417ms step_avg:60.03ms
step:491/2225 train_time:29478ms step_avg:60.04ms
step:492/2225 train_time:29537ms step_avg:60.04ms
step:493/2225 train_time:29598ms step_avg:60.04ms
step:494/2225 train_time:29658ms step_avg:60.04ms
step:495/2225 train_time:29719ms step_avg:60.04ms
step:496/2225 train_time:29778ms step_avg:60.04ms
step:497/2225 train_time:29838ms step_avg:60.04ms
step:498/2225 train_time:29898ms step_avg:60.04ms
step:499/2225 train_time:29959ms step_avg:60.04ms
step:500/2225 train_time:30018ms step_avg:60.04ms
step:500/2225 val_loss:3.8233 train_time:30080ms step_avg:60.16ms
step:501/2225 train_time:30101ms step_avg:60.08ms
step:502/2225 train_time:30140ms step_avg:60.04ms
step:503/2225 train_time:30205ms step_avg:60.05ms
step:504/2225 train_time:30266ms step_avg:60.05ms
step:505/2225 train_time:30327ms step_avg:60.05ms
step:506/2225 train_time:30386ms step_avg:60.05ms
step:507/2225 train_time:30447ms step_avg:60.05ms
step:508/2225 train_time:30505ms step_avg:60.05ms
step:509/2225 train_time:30564ms step_avg:60.05ms
step:510/2225 train_time:30622ms step_avg:60.04ms
step:511/2225 train_time:30681ms step_avg:60.04ms
step:512/2225 train_time:30739ms step_avg:60.04ms
step:513/2225 train_time:30799ms step_avg:60.04ms
step:514/2225 train_time:30857ms step_avg:60.03ms
step:515/2225 train_time:30917ms step_avg:60.03ms
step:516/2225 train_time:30976ms step_avg:60.03ms
step:517/2225 train_time:31037ms step_avg:60.03ms
step:518/2225 train_time:31098ms step_avg:60.03ms
step:519/2225 train_time:31160ms step_avg:60.04ms
step:520/2225 train_time:31220ms step_avg:60.04ms
step:521/2225 train_time:31282ms step_avg:60.04ms
step:522/2225 train_time:31341ms step_avg:60.04ms
step:523/2225 train_time:31402ms step_avg:60.04ms
step:524/2225 train_time:31461ms step_avg:60.04ms
step:525/2225 train_time:31521ms step_avg:60.04ms
step:526/2225 train_time:31579ms step_avg:60.04ms
step:527/2225 train_time:31639ms step_avg:60.04ms
step:528/2225 train_time:31698ms step_avg:60.03ms
step:529/2225 train_time:31757ms step_avg:60.03ms
step:530/2225 train_time:31815ms step_avg:60.03ms
step:531/2225 train_time:31875ms step_avg:60.03ms
step:532/2225 train_time:31934ms step_avg:60.03ms
step:533/2225 train_time:31995ms step_avg:60.03ms
step:534/2225 train_time:32055ms step_avg:60.03ms
step:535/2225 train_time:32117ms step_avg:60.03ms
step:536/2225 train_time:32178ms step_avg:60.03ms
step:537/2225 train_time:32240ms step_avg:60.04ms
step:538/2225 train_time:32301ms step_avg:60.04ms
step:539/2225 train_time:32362ms step_avg:60.04ms
step:540/2225 train_time:32422ms step_avg:60.04ms
step:541/2225 train_time:32482ms step_avg:60.04ms
step:542/2225 train_time:32540ms step_avg:60.04ms
step:543/2225 train_time:32600ms step_avg:60.04ms
step:544/2225 train_time:32659ms step_avg:60.03ms
step:545/2225 train_time:32718ms step_avg:60.03ms
step:546/2225 train_time:32776ms step_avg:60.03ms
step:547/2225 train_time:32836ms step_avg:60.03ms
step:548/2225 train_time:32895ms step_avg:60.03ms
step:549/2225 train_time:32955ms step_avg:60.03ms
step:550/2225 train_time:33014ms step_avg:60.03ms
step:551/2225 train_time:33075ms step_avg:60.03ms
step:552/2225 train_time:33136ms step_avg:60.03ms
step:553/2225 train_time:33198ms step_avg:60.03ms
step:554/2225 train_time:33258ms step_avg:60.03ms
step:555/2225 train_time:33319ms step_avg:60.03ms
step:556/2225 train_time:33379ms step_avg:60.03ms
step:557/2225 train_time:33440ms step_avg:60.04ms
step:558/2225 train_time:33499ms step_avg:60.03ms
step:559/2225 train_time:33559ms step_avg:60.03ms
step:560/2225 train_time:33619ms step_avg:60.03ms
step:561/2225 train_time:33678ms step_avg:60.03ms
step:562/2225 train_time:33737ms step_avg:60.03ms
step:563/2225 train_time:33797ms step_avg:60.03ms
step:564/2225 train_time:33856ms step_avg:60.03ms
step:565/2225 train_time:33916ms step_avg:60.03ms
step:566/2225 train_time:33975ms step_avg:60.03ms
step:567/2225 train_time:34036ms step_avg:60.03ms
step:568/2225 train_time:34096ms step_avg:60.03ms
step:569/2225 train_time:34157ms step_avg:60.03ms
step:570/2225 train_time:34217ms step_avg:60.03ms
step:571/2225 train_time:34278ms step_avg:60.03ms
step:572/2225 train_time:34338ms step_avg:60.03ms
step:573/2225 train_time:34398ms step_avg:60.03ms
step:574/2225 train_time:34458ms step_avg:60.03ms
step:575/2225 train_time:34518ms step_avg:60.03ms
step:576/2225 train_time:34577ms step_avg:60.03ms
step:577/2225 train_time:34638ms step_avg:60.03ms
step:578/2225 train_time:34696ms step_avg:60.03ms
step:579/2225 train_time:34756ms step_avg:60.03ms
step:580/2225 train_time:34815ms step_avg:60.03ms
step:581/2225 train_time:34875ms step_avg:60.03ms
step:582/2225 train_time:34935ms step_avg:60.03ms
step:583/2225 train_time:34996ms step_avg:60.03ms
step:584/2225 train_time:35055ms step_avg:60.03ms
step:585/2225 train_time:35116ms step_avg:60.03ms
step:586/2225 train_time:35175ms step_avg:60.03ms
step:587/2225 train_time:35237ms step_avg:60.03ms
step:588/2225 train_time:35297ms step_avg:60.03ms
step:589/2225 train_time:35358ms step_avg:60.03ms
step:590/2225 train_time:35417ms step_avg:60.03ms
step:591/2225 train_time:35478ms step_avg:60.03ms
step:592/2225 train_time:35537ms step_avg:60.03ms
step:593/2225 train_time:35597ms step_avg:60.03ms
step:594/2225 train_time:35656ms step_avg:60.03ms
step:595/2225 train_time:35716ms step_avg:60.03ms
step:596/2225 train_time:35775ms step_avg:60.03ms
step:597/2225 train_time:35835ms step_avg:60.03ms
step:598/2225 train_time:35894ms step_avg:60.02ms
step:599/2225 train_time:35954ms step_avg:60.02ms
step:600/2225 train_time:36013ms step_avg:60.02ms
step:601/2225 train_time:36074ms step_avg:60.02ms
step:602/2225 train_time:36134ms step_avg:60.02ms
step:603/2225 train_time:36195ms step_avg:60.02ms
step:604/2225 train_time:36255ms step_avg:60.03ms
step:605/2225 train_time:36317ms step_avg:60.03ms
step:606/2225 train_time:36376ms step_avg:60.03ms
step:607/2225 train_time:36438ms step_avg:60.03ms
step:608/2225 train_time:36497ms step_avg:60.03ms
step:609/2225 train_time:36557ms step_avg:60.03ms
step:610/2225 train_time:36616ms step_avg:60.03ms
step:611/2225 train_time:36677ms step_avg:60.03ms
step:612/2225 train_time:36736ms step_avg:60.03ms
step:613/2225 train_time:36797ms step_avg:60.03ms
step:614/2225 train_time:36856ms step_avg:60.03ms
step:615/2225 train_time:36916ms step_avg:60.03ms
step:616/2225 train_time:36975ms step_avg:60.02ms
step:617/2225 train_time:37036ms step_avg:60.03ms
step:618/2225 train_time:37095ms step_avg:60.02ms
step:619/2225 train_time:37156ms step_avg:60.03ms
step:620/2225 train_time:37216ms step_avg:60.03ms
step:621/2225 train_time:37277ms step_avg:60.03ms
step:622/2225 train_time:37337ms step_avg:60.03ms
step:623/2225 train_time:37398ms step_avg:60.03ms
step:624/2225 train_time:37458ms step_avg:60.03ms
step:625/2225 train_time:37518ms step_avg:60.03ms
step:626/2225 train_time:37577ms step_avg:60.03ms
step:627/2225 train_time:37637ms step_avg:60.03ms
step:628/2225 train_time:37696ms step_avg:60.03ms
step:629/2225 train_time:37757ms step_avg:60.03ms
step:630/2225 train_time:37816ms step_avg:60.02ms
step:631/2225 train_time:37876ms step_avg:60.03ms
step:632/2225 train_time:37936ms step_avg:60.03ms
step:633/2225 train_time:37997ms step_avg:60.03ms
step:634/2225 train_time:38056ms step_avg:60.03ms
step:635/2225 train_time:38116ms step_avg:60.03ms
step:636/2225 train_time:38176ms step_avg:60.02ms
step:637/2225 train_time:38237ms step_avg:60.03ms
step:638/2225 train_time:38296ms step_avg:60.02ms
step:639/2225 train_time:38357ms step_avg:60.03ms
step:640/2225 train_time:38416ms step_avg:60.02ms
step:641/2225 train_time:38477ms step_avg:60.03ms
step:642/2225 train_time:38537ms step_avg:60.03ms
step:643/2225 train_time:38598ms step_avg:60.03ms
step:644/2225 train_time:38657ms step_avg:60.03ms
step:645/2225 train_time:38717ms step_avg:60.03ms
step:646/2225 train_time:38776ms step_avg:60.02ms
step:647/2225 train_time:38837ms step_avg:60.03ms
step:648/2225 train_time:38896ms step_avg:60.02ms
step:649/2225 train_time:38957ms step_avg:60.03ms
step:650/2225 train_time:39016ms step_avg:60.03ms
step:651/2225 train_time:39078ms step_avg:60.03ms
step:652/2225 train_time:39137ms step_avg:60.03ms
step:653/2225 train_time:39198ms step_avg:60.03ms
step:654/2225 train_time:39257ms step_avg:60.03ms
step:655/2225 train_time:39317ms step_avg:60.03ms
step:656/2225 train_time:39377ms step_avg:60.03ms
step:657/2225 train_time:39438ms step_avg:60.03ms
step:658/2225 train_time:39498ms step_avg:60.03ms
step:659/2225 train_time:39559ms step_avg:60.03ms
step:660/2225 train_time:39618ms step_avg:60.03ms
step:661/2225 train_time:39679ms step_avg:60.03ms
step:662/2225 train_time:39738ms step_avg:60.03ms
step:663/2225 train_time:39798ms step_avg:60.03ms
step:664/2225 train_time:39857ms step_avg:60.03ms
step:665/2225 train_time:39918ms step_avg:60.03ms
step:666/2225 train_time:39977ms step_avg:60.03ms
step:667/2225 train_time:40037ms step_avg:60.03ms
step:668/2225 train_time:40097ms step_avg:60.02ms
step:669/2225 train_time:40157ms step_avg:60.03ms
step:670/2225 train_time:40216ms step_avg:60.02ms
step:671/2225 train_time:40277ms step_avg:60.03ms
step:672/2225 train_time:40337ms step_avg:60.03ms
step:673/2225 train_time:40398ms step_avg:60.03ms
step:674/2225 train_time:40457ms step_avg:60.03ms
step:675/2225 train_time:40518ms step_avg:60.03ms
step:676/2225 train_time:40577ms step_avg:60.03ms
step:677/2225 train_time:40638ms step_avg:60.03ms
step:678/2225 train_time:40697ms step_avg:60.02ms
step:679/2225 train_time:40757ms step_avg:60.03ms
step:680/2225 train_time:40816ms step_avg:60.02ms
step:681/2225 train_time:40877ms step_avg:60.02ms
step:682/2225 train_time:40937ms step_avg:60.02ms
step:683/2225 train_time:40997ms step_avg:60.03ms
step:684/2225 train_time:41057ms step_avg:60.02ms
step:685/2225 train_time:41117ms step_avg:60.02ms
step:686/2225 train_time:41176ms step_avg:60.02ms
step:687/2225 train_time:41238ms step_avg:60.03ms
step:688/2225 train_time:41296ms step_avg:60.02ms
step:689/2225 train_time:41357ms step_avg:60.02ms
step:690/2225 train_time:41416ms step_avg:60.02ms
step:691/2225 train_time:41477ms step_avg:60.02ms
step:692/2225 train_time:41536ms step_avg:60.02ms
step:693/2225 train_time:41597ms step_avg:60.02ms
step:694/2225 train_time:41656ms step_avg:60.02ms
step:695/2225 train_time:41717ms step_avg:60.02ms
step:696/2225 train_time:41776ms step_avg:60.02ms
step:697/2225 train_time:41837ms step_avg:60.02ms
step:698/2225 train_time:41896ms step_avg:60.02ms
step:699/2225 train_time:41957ms step_avg:60.02ms
step:700/2225 train_time:42016ms step_avg:60.02ms
step:701/2225 train_time:42076ms step_avg:60.02ms
step:702/2225 train_time:42136ms step_avg:60.02ms
step:703/2225 train_time:42197ms step_avg:60.02ms
step:704/2225 train_time:42256ms step_avg:60.02ms
step:705/2225 train_time:42317ms step_avg:60.02ms
step:706/2225 train_time:42376ms step_avg:60.02ms
step:707/2225 train_time:42438ms step_avg:60.03ms
step:708/2225 train_time:42497ms step_avg:60.02ms
step:709/2225 train_time:42558ms step_avg:60.03ms
step:710/2225 train_time:42617ms step_avg:60.02ms
step:711/2225 train_time:42678ms step_avg:60.02ms
step:712/2225 train_time:42737ms step_avg:60.02ms
step:713/2225 train_time:42798ms step_avg:60.03ms
step:714/2225 train_time:42857ms step_avg:60.02ms
step:715/2225 train_time:42917ms step_avg:60.02ms
step:716/2225 train_time:42976ms step_avg:60.02ms
step:717/2225 train_time:43037ms step_avg:60.02ms
step:718/2225 train_time:43097ms step_avg:60.02ms
step:719/2225 train_time:43157ms step_avg:60.02ms
step:720/2225 train_time:43216ms step_avg:60.02ms
step:721/2225 train_time:43277ms step_avg:60.02ms
step:722/2225 train_time:43337ms step_avg:60.02ms
step:723/2225 train_time:43398ms step_avg:60.02ms
step:724/2225 train_time:43457ms step_avg:60.02ms
step:725/2225 train_time:43518ms step_avg:60.02ms
step:726/2225 train_time:43577ms step_avg:60.02ms
step:727/2225 train_time:43638ms step_avg:60.03ms
step:728/2225 train_time:43698ms step_avg:60.02ms
step:729/2225 train_time:43758ms step_avg:60.02ms
step:730/2225 train_time:43817ms step_avg:60.02ms
step:731/2225 train_time:43879ms step_avg:60.03ms
step:732/2225 train_time:43939ms step_avg:60.03ms
step:733/2225 train_time:44000ms step_avg:60.03ms
step:734/2225 train_time:44060ms step_avg:60.03ms
step:735/2225 train_time:44121ms step_avg:60.03ms
step:736/2225 train_time:44180ms step_avg:60.03ms
step:737/2225 train_time:44241ms step_avg:60.03ms
step:738/2225 train_time:44301ms step_avg:60.03ms
step:739/2225 train_time:44363ms step_avg:60.03ms
step:740/2225 train_time:44422ms step_avg:60.03ms
step:741/2225 train_time:44483ms step_avg:60.03ms
step:742/2225 train_time:44543ms step_avg:60.03ms
step:743/2225 train_time:44604ms step_avg:60.03ms
step:744/2225 train_time:44664ms step_avg:60.03ms
step:745/2225 train_time:44724ms step_avg:60.03ms
step:746/2225 train_time:44784ms step_avg:60.03ms
step:747/2225 train_time:44845ms step_avg:60.03ms
step:748/2225 train_time:44904ms step_avg:60.03ms
step:749/2225 train_time:44965ms step_avg:60.03ms
step:750/2225 train_time:45025ms step_avg:60.03ms
step:750/2225 val_loss:3.6664 train_time:45087ms step_avg:60.12ms
step:751/2225 train_time:45108ms step_avg:60.06ms
step:752/2225 train_time:45152ms step_avg:60.04ms
step:753/2225 train_time:45212ms step_avg:60.04ms
step:754/2225 train_time:45274ms step_avg:60.05ms
step:755/2225 train_time:45338ms step_avg:60.05ms
step:756/2225 train_time:45400ms step_avg:60.05ms
step:757/2225 train_time:45461ms step_avg:60.05ms
step:758/2225 train_time:45521ms step_avg:60.05ms
step:759/2225 train_time:45582ms step_avg:60.06ms
step:760/2225 train_time:45641ms step_avg:60.05ms
step:761/2225 train_time:45701ms step_avg:60.05ms
step:762/2225 train_time:45759ms step_avg:60.05ms
step:763/2225 train_time:45819ms step_avg:60.05ms
step:764/2225 train_time:45878ms step_avg:60.05ms
step:765/2225 train_time:45939ms step_avg:60.05ms
step:766/2225 train_time:46002ms step_avg:60.06ms
step:767/2225 train_time:46067ms step_avg:60.06ms
step:768/2225 train_time:46128ms step_avg:60.06ms
step:769/2225 train_time:46189ms step_avg:60.06ms
step:770/2225 train_time:46249ms step_avg:60.06ms
step:771/2225 train_time:46310ms step_avg:60.07ms
step:772/2225 train_time:46370ms step_avg:60.06ms
step:773/2225 train_time:46431ms step_avg:60.07ms
step:774/2225 train_time:46491ms step_avg:60.07ms
step:775/2225 train_time:46552ms step_avg:60.07ms
step:776/2225 train_time:46612ms step_avg:60.07ms
step:777/2225 train_time:46673ms step_avg:60.07ms
step:778/2225 train_time:46731ms step_avg:60.07ms
step:779/2225 train_time:46792ms step_avg:60.07ms
step:780/2225 train_time:46851ms step_avg:60.07ms
step:781/2225 train_time:46913ms step_avg:60.07ms
step:782/2225 train_time:46973ms step_avg:60.07ms
step:783/2225 train_time:47036ms step_avg:60.07ms
step:784/2225 train_time:47097ms step_avg:60.07ms
step:785/2225 train_time:47160ms step_avg:60.08ms
step:786/2225 train_time:47220ms step_avg:60.08ms
step:787/2225 train_time:47281ms step_avg:60.08ms
step:788/2225 train_time:47342ms step_avg:60.08ms
step:789/2225 train_time:47402ms step_avg:60.08ms
step:790/2225 train_time:47463ms step_avg:60.08ms
step:791/2225 train_time:47524ms step_avg:60.08ms
step:792/2225 train_time:47583ms step_avg:60.08ms
step:793/2225 train_time:47643ms step_avg:60.08ms
step:794/2225 train_time:47703ms step_avg:60.08ms
step:795/2225 train_time:47763ms step_avg:60.08ms
step:796/2225 train_time:47823ms step_avg:60.08ms
step:797/2225 train_time:47883ms step_avg:60.08ms
step:798/2225 train_time:47942ms step_avg:60.08ms
step:799/2225 train_time:48004ms step_avg:60.08ms
step:800/2225 train_time:48064ms step_avg:60.08ms
step:801/2225 train_time:48126ms step_avg:60.08ms
step:802/2225 train_time:48185ms step_avg:60.08ms
step:803/2225 train_time:48247ms step_avg:60.08ms
step:804/2225 train_time:48307ms step_avg:60.08ms
step:805/2225 train_time:48369ms step_avg:60.09ms
step:806/2225 train_time:48429ms step_avg:60.09ms
step:807/2225 train_time:48490ms step_avg:60.09ms
step:808/2225 train_time:48550ms step_avg:60.09ms
step:809/2225 train_time:48611ms step_avg:60.09ms
step:810/2225 train_time:48671ms step_avg:60.09ms
step:811/2225 train_time:48732ms step_avg:60.09ms
step:812/2225 train_time:48792ms step_avg:60.09ms
step:813/2225 train_time:48854ms step_avg:60.09ms
step:814/2225 train_time:48915ms step_avg:60.09ms
step:815/2225 train_time:48977ms step_avg:60.09ms
step:816/2225 train_time:49037ms step_avg:60.09ms
step:817/2225 train_time:49099ms step_avg:60.10ms
step:818/2225 train_time:49159ms step_avg:60.10ms
step:819/2225 train_time:49220ms step_avg:60.10ms
step:820/2225 train_time:49281ms step_avg:60.10ms
step:821/2225 train_time:49343ms step_avg:60.10ms
step:822/2225 train_time:49403ms step_avg:60.10ms
step:823/2225 train_time:49464ms step_avg:60.10ms
step:824/2225 train_time:49523ms step_avg:60.10ms
step:825/2225 train_time:49584ms step_avg:60.10ms
step:826/2225 train_time:49643ms step_avg:60.10ms
step:827/2225 train_time:49705ms step_avg:60.10ms
step:828/2225 train_time:49764ms step_avg:60.10ms
step:829/2225 train_time:49825ms step_avg:60.10ms
step:830/2225 train_time:49884ms step_avg:60.10ms
step:831/2225 train_time:49945ms step_avg:60.10ms
step:832/2225 train_time:50004ms step_avg:60.10ms
step:833/2225 train_time:50066ms step_avg:60.10ms
step:834/2225 train_time:50126ms step_avg:60.10ms
step:835/2225 train_time:50187ms step_avg:60.10ms
step:836/2225 train_time:50248ms step_avg:60.10ms
step:837/2225 train_time:50310ms step_avg:60.11ms
step:838/2225 train_time:50370ms step_avg:60.11ms
step:839/2225 train_time:50431ms step_avg:60.11ms
step:840/2225 train_time:50491ms step_avg:60.11ms
step:841/2225 train_time:50553ms step_avg:60.11ms
step:842/2225 train_time:50613ms step_avg:60.11ms
step:843/2225 train_time:50674ms step_avg:60.11ms
step:844/2225 train_time:50734ms step_avg:60.11ms
step:845/2225 train_time:50795ms step_avg:60.11ms
step:846/2225 train_time:50855ms step_avg:60.11ms
step:847/2225 train_time:50917ms step_avg:60.11ms
step:848/2225 train_time:50978ms step_avg:60.12ms
step:849/2225 train_time:51040ms step_avg:60.12ms
step:850/2225 train_time:51099ms step_avg:60.12ms
step:851/2225 train_time:51161ms step_avg:60.12ms
step:852/2225 train_time:51221ms step_avg:60.12ms
step:853/2225 train_time:51282ms step_avg:60.12ms
step:854/2225 train_time:51342ms step_avg:60.12ms
step:855/2225 train_time:51403ms step_avg:60.12ms
step:856/2225 train_time:51462ms step_avg:60.12ms
step:857/2225 train_time:51524ms step_avg:60.12ms
step:858/2225 train_time:51583ms step_avg:60.12ms
step:859/2225 train_time:51644ms step_avg:60.12ms
step:860/2225 train_time:51704ms step_avg:60.12ms
step:861/2225 train_time:51765ms step_avg:60.12ms
step:862/2225 train_time:51824ms step_avg:60.12ms
step:863/2225 train_time:51885ms step_avg:60.12ms
step:864/2225 train_time:51944ms step_avg:60.12ms
step:865/2225 train_time:52006ms step_avg:60.12ms
step:866/2225 train_time:52066ms step_avg:60.12ms
step:867/2225 train_time:52127ms step_avg:60.12ms
step:868/2225 train_time:52187ms step_avg:60.12ms
step:869/2225 train_time:52248ms step_avg:60.12ms
step:870/2225 train_time:52307ms step_avg:60.12ms
step:871/2225 train_time:52369ms step_avg:60.13ms
step:872/2225 train_time:52428ms step_avg:60.12ms
step:873/2225 train_time:52490ms step_avg:60.13ms
step:874/2225 train_time:52550ms step_avg:60.13ms
step:875/2225 train_time:52613ms step_avg:60.13ms
step:876/2225 train_time:52673ms step_avg:60.13ms
step:877/2225 train_time:52735ms step_avg:60.13ms
step:878/2225 train_time:52795ms step_avg:60.13ms
step:879/2225 train_time:52856ms step_avg:60.13ms
step:880/2225 train_time:52916ms step_avg:60.13ms
step:881/2225 train_time:52976ms step_avg:60.13ms
step:882/2225 train_time:53037ms step_avg:60.13ms
step:883/2225 train_time:53098ms step_avg:60.13ms
step:884/2225 train_time:53158ms step_avg:60.13ms
step:885/2225 train_time:53220ms step_avg:60.14ms
step:886/2225 train_time:53280ms step_avg:60.13ms
step:887/2225 train_time:53340ms step_avg:60.14ms
step:888/2225 train_time:53401ms step_avg:60.14ms
step:889/2225 train_time:53462ms step_avg:60.14ms
step:890/2225 train_time:53522ms step_avg:60.14ms
step:891/2225 train_time:53583ms step_avg:60.14ms
step:892/2225 train_time:53643ms step_avg:60.14ms
step:893/2225 train_time:53704ms step_avg:60.14ms
step:894/2225 train_time:53764ms step_avg:60.14ms
step:895/2225 train_time:53825ms step_avg:60.14ms
step:896/2225 train_time:53884ms step_avg:60.14ms
step:897/2225 train_time:53945ms step_avg:60.14ms
step:898/2225 train_time:54004ms step_avg:60.14ms
step:899/2225 train_time:54065ms step_avg:60.14ms
step:900/2225 train_time:54125ms step_avg:60.14ms
step:901/2225 train_time:54185ms step_avg:60.14ms
step:902/2225 train_time:54245ms step_avg:60.14ms
step:903/2225 train_time:54306ms step_avg:60.14ms
step:904/2225 train_time:54366ms step_avg:60.14ms
step:905/2225 train_time:54426ms step_avg:60.14ms
step:906/2225 train_time:54486ms step_avg:60.14ms
step:907/2225 train_time:54547ms step_avg:60.14ms
step:908/2225 train_time:54607ms step_avg:60.14ms
step:909/2225 train_time:54669ms step_avg:60.14ms
step:910/2225 train_time:54729ms step_avg:60.14ms
step:911/2225 train_time:54791ms step_avg:60.14ms
step:912/2225 train_time:54851ms step_avg:60.14ms
step:913/2225 train_time:54913ms step_avg:60.15ms
step:914/2225 train_time:54973ms step_avg:60.15ms
step:915/2225 train_time:55034ms step_avg:60.15ms
step:916/2225 train_time:55094ms step_avg:60.15ms
step:917/2225 train_time:55156ms step_avg:60.15ms
step:918/2225 train_time:55215ms step_avg:60.15ms
step:919/2225 train_time:55277ms step_avg:60.15ms
step:920/2225 train_time:55338ms step_avg:60.15ms
step:921/2225 train_time:55399ms step_avg:60.15ms
step:922/2225 train_time:55459ms step_avg:60.15ms
step:923/2225 train_time:55520ms step_avg:60.15ms
step:924/2225 train_time:55580ms step_avg:60.15ms
step:925/2225 train_time:55641ms step_avg:60.15ms
step:926/2225 train_time:55701ms step_avg:60.15ms
step:927/2225 train_time:55763ms step_avg:60.15ms
step:928/2225 train_time:55823ms step_avg:60.15ms
step:929/2225 train_time:55883ms step_avg:60.15ms
step:930/2225 train_time:55943ms step_avg:60.15ms
step:931/2225 train_time:56005ms step_avg:60.16ms
step:932/2225 train_time:56065ms step_avg:60.16ms
step:933/2225 train_time:56126ms step_avg:60.16ms
step:934/2225 train_time:56185ms step_avg:60.16ms
step:935/2225 train_time:56246ms step_avg:60.16ms
step:936/2225 train_time:56305ms step_avg:60.15ms
step:937/2225 train_time:56365ms step_avg:60.16ms
step:938/2225 train_time:56425ms step_avg:60.15ms
step:939/2225 train_time:56486ms step_avg:60.16ms
step:940/2225 train_time:56545ms step_avg:60.15ms
step:941/2225 train_time:56607ms step_avg:60.16ms
step:942/2225 train_time:56667ms step_avg:60.16ms
step:943/2225 train_time:56728ms step_avg:60.16ms
step:944/2225 train_time:56787ms step_avg:60.16ms
step:945/2225 train_time:56849ms step_avg:60.16ms
step:946/2225 train_time:56909ms step_avg:60.16ms
step:947/2225 train_time:56972ms step_avg:60.16ms
step:948/2225 train_time:57032ms step_avg:60.16ms
step:949/2225 train_time:57094ms step_avg:60.16ms
step:950/2225 train_time:57154ms step_avg:60.16ms
step:951/2225 train_time:57215ms step_avg:60.16ms
step:952/2225 train_time:57275ms step_avg:60.16ms
step:953/2225 train_time:57338ms step_avg:60.17ms
step:954/2225 train_time:57397ms step_avg:60.17ms
step:955/2225 train_time:57459ms step_avg:60.17ms
step:956/2225 train_time:57520ms step_avg:60.17ms
step:957/2225 train_time:57581ms step_avg:60.17ms
step:958/2225 train_time:57641ms step_avg:60.17ms
step:959/2225 train_time:57701ms step_avg:60.17ms
step:960/2225 train_time:57761ms step_avg:60.17ms
step:961/2225 train_time:57822ms step_avg:60.17ms
step:962/2225 train_time:57882ms step_avg:60.17ms
step:963/2225 train_time:57943ms step_avg:60.17ms
step:964/2225 train_time:58003ms step_avg:60.17ms
step:965/2225 train_time:58064ms step_avg:60.17ms
step:966/2225 train_time:58123ms step_avg:60.17ms
step:967/2225 train_time:58185ms step_avg:60.17ms
step:968/2225 train_time:58244ms step_avg:60.17ms
step:969/2225 train_time:58305ms step_avg:60.17ms
step:970/2225 train_time:58364ms step_avg:60.17ms
step:971/2225 train_time:58425ms step_avg:60.17ms
step:972/2225 train_time:58484ms step_avg:60.17ms
step:973/2225 train_time:58545ms step_avg:60.17ms
step:974/2225 train_time:58604ms step_avg:60.17ms
step:975/2225 train_time:58665ms step_avg:60.17ms
step:976/2225 train_time:58725ms step_avg:60.17ms
step:977/2225 train_time:58786ms step_avg:60.17ms
step:978/2225 train_time:58846ms step_avg:60.17ms
step:979/2225 train_time:58907ms step_avg:60.17ms
step:980/2225 train_time:58967ms step_avg:60.17ms
step:981/2225 train_time:59028ms step_avg:60.17ms
step:982/2225 train_time:59088ms step_avg:60.17ms
step:983/2225 train_time:59149ms step_avg:60.17ms
step:984/2225 train_time:59209ms step_avg:60.17ms
step:985/2225 train_time:59271ms step_avg:60.17ms
step:986/2225 train_time:59331ms step_avg:60.17ms
step:987/2225 train_time:59393ms step_avg:60.18ms
step:988/2225 train_time:59453ms step_avg:60.17ms
step:989/2225 train_time:59514ms step_avg:60.18ms
step:990/2225 train_time:59575ms step_avg:60.18ms
step:991/2225 train_time:59637ms step_avg:60.18ms
step:992/2225 train_time:59697ms step_avg:60.18ms
step:993/2225 train_time:59759ms step_avg:60.18ms
step:994/2225 train_time:59819ms step_avg:60.18ms
step:995/2225 train_time:59880ms step_avg:60.18ms
step:996/2225 train_time:59940ms step_avg:60.18ms
step:997/2225 train_time:60001ms step_avg:60.18ms
step:998/2225 train_time:60062ms step_avg:60.18ms
step:999/2225 train_time:60123ms step_avg:60.18ms
step:1000/2225 train_time:60182ms step_avg:60.18ms
step:1000/2225 val_loss:3.5959 train_time:60244ms step_avg:60.24ms
step:1001/2225 train_time:60266ms step_avg:60.21ms
step:1002/2225 train_time:60306ms step_avg:60.19ms
step:1003/2225 train_time:60370ms step_avg:60.19ms
step:1004/2225 train_time:60435ms step_avg:60.19ms
step:1005/2225 train_time:60498ms step_avg:60.20ms
step:1006/2225 train_time:60558ms step_avg:60.20ms
step:1007/2225 train_time:60618ms step_avg:60.20ms
step:1008/2225 train_time:60677ms step_avg:60.20ms
step:1009/2225 train_time:60738ms step_avg:60.20ms
step:1010/2225 train_time:60797ms step_avg:60.20ms
step:1011/2225 train_time:60858ms step_avg:60.20ms
step:1012/2225 train_time:60917ms step_avg:60.19ms
step:1013/2225 train_time:60977ms step_avg:60.19ms
step:1014/2225 train_time:61036ms step_avg:60.19ms
step:1015/2225 train_time:61096ms step_avg:60.19ms
step:1016/2225 train_time:61156ms step_avg:60.19ms
step:1017/2225 train_time:61218ms step_avg:60.19ms
step:1018/2225 train_time:61279ms step_avg:60.20ms
step:1019/2225 train_time:61342ms step_avg:60.20ms
step:1020/2225 train_time:61403ms step_avg:60.20ms
step:1021/2225 train_time:61465ms step_avg:60.20ms
step:1022/2225 train_time:61525ms step_avg:60.20ms
step:1023/2225 train_time:61587ms step_avg:60.20ms
step:1024/2225 train_time:61647ms step_avg:60.20ms
step:1025/2225 train_time:61709ms step_avg:60.20ms
step:1026/2225 train_time:61769ms step_avg:60.20ms
step:1027/2225 train_time:61830ms step_avg:60.20ms
step:1028/2225 train_time:61889ms step_avg:60.20ms
step:1029/2225 train_time:61951ms step_avg:60.21ms
step:1030/2225 train_time:62011ms step_avg:60.20ms
step:1031/2225 train_time:62072ms step_avg:60.21ms
step:1032/2225 train_time:62131ms step_avg:60.20ms
step:1033/2225 train_time:62193ms step_avg:60.21ms
step:1034/2225 train_time:62254ms step_avg:60.21ms
step:1035/2225 train_time:62316ms step_avg:60.21ms
step:1036/2225 train_time:62377ms step_avg:60.21ms
step:1037/2225 train_time:62439ms step_avg:60.21ms
step:1038/2225 train_time:62499ms step_avg:60.21ms
step:1039/2225 train_time:62560ms step_avg:60.21ms
step:1040/2225 train_time:62620ms step_avg:60.21ms
step:1041/2225 train_time:62681ms step_avg:60.21ms
step:1042/2225 train_time:62740ms step_avg:60.21ms
step:1043/2225 train_time:62801ms step_avg:60.21ms
step:1044/2225 train_time:62860ms step_avg:60.21ms
step:1045/2225 train_time:62921ms step_avg:60.21ms
step:1046/2225 train_time:62981ms step_avg:60.21ms
step:1047/2225 train_time:63042ms step_avg:60.21ms
step:1048/2225 train_time:63101ms step_avg:60.21ms
step:1049/2225 train_time:63163ms step_avg:60.21ms
step:1050/2225 train_time:63222ms step_avg:60.21ms
step:1051/2225 train_time:63284ms step_avg:60.21ms
step:1052/2225 train_time:63345ms step_avg:60.21ms
step:1053/2225 train_time:63407ms step_avg:60.22ms
step:1054/2225 train_time:63468ms step_avg:60.22ms
step:1055/2225 train_time:63529ms step_avg:60.22ms
step:1056/2225 train_time:63589ms step_avg:60.22ms
step:1057/2225 train_time:63651ms step_avg:60.22ms
step:1058/2225 train_time:63711ms step_avg:60.22ms
step:1059/2225 train_time:63774ms step_avg:60.22ms
step:1060/2225 train_time:63834ms step_avg:60.22ms
step:1061/2225 train_time:63894ms step_avg:60.22ms
step:1062/2225 train_time:63955ms step_avg:60.22ms
step:1063/2225 train_time:64015ms step_avg:60.22ms
step:1064/2225 train_time:64075ms step_avg:60.22ms
step:1065/2225 train_time:64136ms step_avg:60.22ms
step:1066/2225 train_time:64195ms step_avg:60.22ms
step:1067/2225 train_time:64256ms step_avg:60.22ms
step:1068/2225 train_time:64316ms step_avg:60.22ms
step:1069/2225 train_time:64377ms step_avg:60.22ms
step:1070/2225 train_time:64437ms step_avg:60.22ms
step:1071/2225 train_time:64499ms step_avg:60.22ms
step:1072/2225 train_time:64559ms step_avg:60.22ms
step:1073/2225 train_time:64619ms step_avg:60.22ms
step:1074/2225 train_time:64679ms step_avg:60.22ms
step:1075/2225 train_time:64741ms step_avg:60.22ms
step:1076/2225 train_time:64801ms step_avg:60.22ms
step:1077/2225 train_time:64861ms step_avg:60.22ms
step:1078/2225 train_time:64921ms step_avg:60.22ms
step:1079/2225 train_time:64982ms step_avg:60.22ms
step:1080/2225 train_time:65041ms step_avg:60.22ms
step:1081/2225 train_time:65103ms step_avg:60.22ms
step:1082/2225 train_time:65162ms step_avg:60.22ms
step:1083/2225 train_time:65223ms step_avg:60.22ms
step:1084/2225 train_time:65283ms step_avg:60.22ms
step:1085/2225 train_time:65345ms step_avg:60.23ms
step:1086/2225 train_time:65405ms step_avg:60.23ms
step:1087/2225 train_time:65467ms step_avg:60.23ms
step:1088/2225 train_time:65527ms step_avg:60.23ms
step:1089/2225 train_time:65589ms step_avg:60.23ms
step:1090/2225 train_time:65650ms step_avg:60.23ms
step:1091/2225 train_time:65712ms step_avg:60.23ms
step:1092/2225 train_time:65772ms step_avg:60.23ms
step:1093/2225 train_time:65834ms step_avg:60.23ms
step:1094/2225 train_time:65894ms step_avg:60.23ms
step:1095/2225 train_time:65955ms step_avg:60.23ms
step:1096/2225 train_time:66015ms step_avg:60.23ms
step:1097/2225 train_time:66077ms step_avg:60.23ms
step:1098/2225 train_time:66136ms step_avg:60.23ms
step:1099/2225 train_time:66197ms step_avg:60.23ms
step:1100/2225 train_time:66257ms step_avg:60.23ms
step:1101/2225 train_time:66318ms step_avg:60.23ms
step:1102/2225 train_time:66378ms step_avg:60.23ms
step:1103/2225 train_time:66439ms step_avg:60.24ms
step:1104/2225 train_time:66499ms step_avg:60.23ms
step:1105/2225 train_time:66560ms step_avg:60.24ms
step:1106/2225 train_time:66620ms step_avg:60.23ms
step:1107/2225 train_time:66682ms step_avg:60.24ms
step:1108/2225 train_time:66741ms step_avg:60.24ms
step:1109/2225 train_time:66803ms step_avg:60.24ms
step:1110/2225 train_time:66862ms step_avg:60.24ms
step:1111/2225 train_time:66923ms step_avg:60.24ms
step:1112/2225 train_time:66982ms step_avg:60.24ms
step:1113/2225 train_time:67044ms step_avg:60.24ms
step:1114/2225 train_time:67103ms step_avg:60.24ms
step:1115/2225 train_time:67164ms step_avg:60.24ms
step:1116/2225 train_time:67224ms step_avg:60.24ms
step:1117/2225 train_time:67286ms step_avg:60.24ms
step:1118/2225 train_time:67346ms step_avg:60.24ms
step:1119/2225 train_time:67408ms step_avg:60.24ms
step:1120/2225 train_time:67469ms step_avg:60.24ms
step:1121/2225 train_time:67531ms step_avg:60.24ms
step:1122/2225 train_time:67591ms step_avg:60.24ms
step:1123/2225 train_time:67653ms step_avg:60.24ms
step:1124/2225 train_time:67713ms step_avg:60.24ms
step:1125/2225 train_time:67774ms step_avg:60.24ms
step:1126/2225 train_time:67834ms step_avg:60.24ms
step:1127/2225 train_time:67896ms step_avg:60.24ms
step:1128/2225 train_time:67956ms step_avg:60.24ms
step:1129/2225 train_time:68017ms step_avg:60.25ms
step:1130/2225 train_time:68077ms step_avg:60.24ms
step:1131/2225 train_time:68137ms step_avg:60.25ms
step:1132/2225 train_time:68197ms step_avg:60.24ms
step:1133/2225 train_time:68258ms step_avg:60.25ms
step:1134/2225 train_time:68318ms step_avg:60.25ms
step:1135/2225 train_time:68379ms step_avg:60.25ms
step:1136/2225 train_time:68439ms step_avg:60.25ms
step:1137/2225 train_time:68500ms step_avg:60.25ms
step:1138/2225 train_time:68560ms step_avg:60.25ms
step:1139/2225 train_time:68621ms step_avg:60.25ms
step:1140/2225 train_time:68681ms step_avg:60.25ms
step:1141/2225 train_time:68742ms step_avg:60.25ms
step:1142/2225 train_time:68802ms step_avg:60.25ms
step:1143/2225 train_time:68863ms step_avg:60.25ms
step:1144/2225 train_time:68923ms step_avg:60.25ms
step:1145/2225 train_time:68984ms step_avg:60.25ms
step:1146/2225 train_time:69044ms step_avg:60.25ms
step:1147/2225 train_time:69105ms step_avg:60.25ms
step:1148/2225 train_time:69165ms step_avg:60.25ms
step:1149/2225 train_time:69227ms step_avg:60.25ms
step:1150/2225 train_time:69287ms step_avg:60.25ms
step:1151/2225 train_time:69350ms step_avg:60.25ms
step:1152/2225 train_time:69410ms step_avg:60.25ms
step:1153/2225 train_time:69471ms step_avg:60.25ms
step:1154/2225 train_time:69532ms step_avg:60.25ms
step:1155/2225 train_time:69593ms step_avg:60.25ms
step:1156/2225 train_time:69653ms step_avg:60.25ms
step:1157/2225 train_time:69715ms step_avg:60.25ms
step:1158/2225 train_time:69775ms step_avg:60.25ms
step:1159/2225 train_time:69836ms step_avg:60.26ms
step:1160/2225 train_time:69897ms step_avg:60.26ms
step:1161/2225 train_time:69958ms step_avg:60.26ms
step:1162/2225 train_time:70017ms step_avg:60.26ms
step:1163/2225 train_time:70078ms step_avg:60.26ms
step:1164/2225 train_time:70138ms step_avg:60.26ms
step:1165/2225 train_time:70199ms step_avg:60.26ms
step:1166/2225 train_time:70260ms step_avg:60.26ms
step:1167/2225 train_time:70320ms step_avg:60.26ms
step:1168/2225 train_time:70380ms step_avg:60.26ms
step:1169/2225 train_time:70441ms step_avg:60.26ms
step:1170/2225 train_time:70501ms step_avg:60.26ms
step:1171/2225 train_time:70562ms step_avg:60.26ms
step:1172/2225 train_time:70622ms step_avg:60.26ms
step:1173/2225 train_time:70683ms step_avg:60.26ms
step:1174/2225 train_time:70743ms step_avg:60.26ms
step:1175/2225 train_time:70805ms step_avg:60.26ms
step:1176/2225 train_time:70865ms step_avg:60.26ms
step:1177/2225 train_time:70926ms step_avg:60.26ms
step:1178/2225 train_time:70986ms step_avg:60.26ms
step:1179/2225 train_time:71047ms step_avg:60.26ms
step:1180/2225 train_time:71108ms step_avg:60.26ms
step:1181/2225 train_time:71170ms step_avg:60.26ms
step:1182/2225 train_time:71230ms step_avg:60.26ms
step:1183/2225 train_time:71293ms step_avg:60.26ms
step:1184/2225 train_time:71353ms step_avg:60.26ms
step:1185/2225 train_time:71415ms step_avg:60.27ms
step:1186/2225 train_time:71475ms step_avg:60.27ms
step:1187/2225 train_time:71536ms step_avg:60.27ms
step:1188/2225 train_time:71595ms step_avg:60.27ms
step:1189/2225 train_time:71656ms step_avg:60.27ms
step:1190/2225 train_time:71715ms step_avg:60.27ms
step:1191/2225 train_time:71776ms step_avg:60.27ms
step:1192/2225 train_time:71836ms step_avg:60.26ms
step:1193/2225 train_time:71897ms step_avg:60.27ms
step:1194/2225 train_time:71957ms step_avg:60.27ms
step:1195/2225 train_time:72018ms step_avg:60.27ms
step:1196/2225 train_time:72077ms step_avg:60.27ms
step:1197/2225 train_time:72138ms step_avg:60.27ms
step:1198/2225 train_time:72199ms step_avg:60.27ms
step:1199/2225 train_time:72259ms step_avg:60.27ms
step:1200/2225 train_time:72319ms step_avg:60.27ms
step:1201/2225 train_time:72379ms step_avg:60.27ms
step:1202/2225 train_time:72439ms step_avg:60.27ms
step:1203/2225 train_time:72500ms step_avg:60.27ms
step:1204/2225 train_time:72559ms step_avg:60.27ms
step:1205/2225 train_time:72620ms step_avg:60.27ms
step:1206/2225 train_time:72680ms step_avg:60.27ms
step:1207/2225 train_time:72741ms step_avg:60.27ms
step:1208/2225 train_time:72801ms step_avg:60.27ms
step:1209/2225 train_time:72862ms step_avg:60.27ms
step:1210/2225 train_time:72922ms step_avg:60.27ms
step:1211/2225 train_time:72984ms step_avg:60.27ms
step:1212/2225 train_time:73043ms step_avg:60.27ms
step:1213/2225 train_time:73105ms step_avg:60.27ms
step:1214/2225 train_time:73165ms step_avg:60.27ms
step:1215/2225 train_time:73226ms step_avg:60.27ms
step:1216/2225 train_time:73287ms step_avg:60.27ms
step:1217/2225 train_time:73349ms step_avg:60.27ms
step:1218/2225 train_time:73409ms step_avg:60.27ms
step:1219/2225 train_time:73471ms step_avg:60.27ms
step:1220/2225 train_time:73531ms step_avg:60.27ms
step:1221/2225 train_time:73593ms step_avg:60.27ms
step:1222/2225 train_time:73653ms step_avg:60.27ms
step:1223/2225 train_time:73715ms step_avg:60.27ms
step:1224/2225 train_time:73775ms step_avg:60.27ms
step:1225/2225 train_time:73836ms step_avg:60.27ms
step:1226/2225 train_time:73896ms step_avg:60.27ms
step:1227/2225 train_time:73957ms step_avg:60.27ms
step:1228/2225 train_time:74017ms step_avg:60.27ms
step:1229/2225 train_time:74078ms step_avg:60.28ms
step:1230/2225 train_time:74138ms step_avg:60.27ms
step:1231/2225 train_time:74199ms step_avg:60.28ms
step:1232/2225 train_time:74260ms step_avg:60.28ms
step:1233/2225 train_time:74321ms step_avg:60.28ms
step:1234/2225 train_time:74381ms step_avg:60.28ms
step:1235/2225 train_time:74442ms step_avg:60.28ms
step:1236/2225 train_time:74502ms step_avg:60.28ms
step:1237/2225 train_time:74562ms step_avg:60.28ms
step:1238/2225 train_time:74622ms step_avg:60.28ms
step:1239/2225 train_time:74683ms step_avg:60.28ms
step:1240/2225 train_time:74742ms step_avg:60.28ms
step:1241/2225 train_time:74804ms step_avg:60.28ms
step:1242/2225 train_time:74864ms step_avg:60.28ms
step:1243/2225 train_time:74926ms step_avg:60.28ms
step:1244/2225 train_time:74987ms step_avg:60.28ms
step:1245/2225 train_time:75048ms step_avg:60.28ms
step:1246/2225 train_time:75109ms step_avg:60.28ms
step:1247/2225 train_time:75171ms step_avg:60.28ms
step:1248/2225 train_time:75231ms step_avg:60.28ms
step:1249/2225 train_time:75293ms step_avg:60.28ms
step:1250/2225 train_time:75352ms step_avg:60.28ms
step:1250/2225 val_loss:3.5219 train_time:75414ms step_avg:60.33ms
step:1251/2225 train_time:75436ms step_avg:60.30ms
step:1252/2225 train_time:75476ms step_avg:60.28ms
step:1253/2225 train_time:75543ms step_avg:60.29ms
step:1254/2225 train_time:75609ms step_avg:60.29ms
step:1255/2225 train_time:75671ms step_avg:60.30ms
step:1256/2225 train_time:75730ms step_avg:60.29ms
step:1257/2225 train_time:75791ms step_avg:60.30ms
step:1258/2225 train_time:75850ms step_avg:60.29ms
step:1259/2225 train_time:75910ms step_avg:60.29ms
step:1260/2225 train_time:75969ms step_avg:60.29ms
step:1261/2225 train_time:76030ms step_avg:60.29ms
step:1262/2225 train_time:76089ms step_avg:60.29ms
step:1263/2225 train_time:76149ms step_avg:60.29ms
step:1264/2225 train_time:76209ms step_avg:60.29ms
step:1265/2225 train_time:76270ms step_avg:60.29ms
step:1266/2225 train_time:76330ms step_avg:60.29ms
step:1267/2225 train_time:76393ms step_avg:60.29ms
step:1268/2225 train_time:76454ms step_avg:60.29ms
step:1269/2225 train_time:76520ms step_avg:60.30ms
step:1270/2225 train_time:76582ms step_avg:60.30ms
step:1271/2225 train_time:76644ms step_avg:60.30ms
step:1272/2225 train_time:76704ms step_avg:60.30ms
step:1273/2225 train_time:76764ms step_avg:60.30ms
step:1274/2225 train_time:76824ms step_avg:60.30ms
step:1275/2225 train_time:76884ms step_avg:60.30ms
step:1276/2225 train_time:76943ms step_avg:60.30ms
step:1277/2225 train_time:77004ms step_avg:60.30ms
step:1278/2225 train_time:77064ms step_avg:60.30ms
step:1279/2225 train_time:77124ms step_avg:60.30ms
step:1280/2225 train_time:77184ms step_avg:60.30ms
step:1281/2225 train_time:77244ms step_avg:60.30ms
step:1282/2225 train_time:77304ms step_avg:60.30ms
step:1283/2225 train_time:77365ms step_avg:60.30ms
step:1284/2225 train_time:77426ms step_avg:60.30ms
step:1285/2225 train_time:77489ms step_avg:60.30ms
step:1286/2225 train_time:77549ms step_avg:60.30ms
step:1287/2225 train_time:77611ms step_avg:60.30ms
step:1288/2225 train_time:77671ms step_avg:60.30ms
step:1289/2225 train_time:77733ms step_avg:60.30ms
step:1290/2225 train_time:77793ms step_avg:60.30ms
step:1291/2225 train_time:77854ms step_avg:60.31ms
step:1292/2225 train_time:77914ms step_avg:60.31ms
step:1293/2225 train_time:77975ms step_avg:60.31ms
step:1294/2225 train_time:78035ms step_avg:60.31ms
step:1295/2225 train_time:78096ms step_avg:60.31ms
step:1296/2225 train_time:78156ms step_avg:60.31ms
step:1297/2225 train_time:78217ms step_avg:60.31ms
step:1298/2225 train_time:78277ms step_avg:60.31ms
step:1299/2225 train_time:78340ms step_avg:60.31ms
step:1300/2225 train_time:78400ms step_avg:60.31ms
step:1301/2225 train_time:78462ms step_avg:60.31ms
step:1302/2225 train_time:78523ms step_avg:60.31ms
step:1303/2225 train_time:78585ms step_avg:60.31ms
step:1304/2225 train_time:78644ms step_avg:60.31ms
step:1305/2225 train_time:78705ms step_avg:60.31ms
step:1306/2225 train_time:78765ms step_avg:60.31ms
step:1307/2225 train_time:78826ms step_avg:60.31ms
step:1308/2225 train_time:78885ms step_avg:60.31ms
step:1309/2225 train_time:78946ms step_avg:60.31ms
step:1310/2225 train_time:79006ms step_avg:60.31ms
step:1311/2225 train_time:79067ms step_avg:60.31ms
step:1312/2225 train_time:79126ms step_avg:60.31ms
step:1313/2225 train_time:79187ms step_avg:60.31ms
step:1314/2225 train_time:79247ms step_avg:60.31ms
step:1315/2225 train_time:79308ms step_avg:60.31ms
step:1316/2225 train_time:79368ms step_avg:60.31ms
step:1317/2225 train_time:79429ms step_avg:60.31ms
step:1318/2225 train_time:79489ms step_avg:60.31ms
step:1319/2225 train_time:79550ms step_avg:60.31ms
step:1320/2225 train_time:79611ms step_avg:60.31ms
step:1321/2225 train_time:79673ms step_avg:60.31ms
step:1322/2225 train_time:79734ms step_avg:60.31ms
step:1323/2225 train_time:79795ms step_avg:60.31ms
step:1324/2225 train_time:79855ms step_avg:60.31ms
step:1325/2225 train_time:79917ms step_avg:60.31ms
step:1326/2225 train_time:79977ms step_avg:60.31ms
step:1327/2225 train_time:80039ms step_avg:60.32ms
step:1328/2225 train_time:80099ms step_avg:60.32ms
step:1329/2225 train_time:80160ms step_avg:60.32ms
step:1330/2225 train_time:80220ms step_avg:60.32ms
step:1331/2225 train_time:80281ms step_avg:60.32ms
step:1332/2225 train_time:80341ms step_avg:60.32ms
step:1333/2225 train_time:80402ms step_avg:60.32ms
step:1334/2225 train_time:80462ms step_avg:60.32ms
step:1335/2225 train_time:80523ms step_avg:60.32ms
step:1336/2225 train_time:80584ms step_avg:60.32ms
step:1337/2225 train_time:80645ms step_avg:60.32ms
step:1338/2225 train_time:80705ms step_avg:60.32ms
step:1339/2225 train_time:80766ms step_avg:60.32ms
step:1340/2225 train_time:80826ms step_avg:60.32ms
step:1341/2225 train_time:80887ms step_avg:60.32ms
step:1342/2225 train_time:80946ms step_avg:60.32ms
step:1343/2225 train_time:81007ms step_avg:60.32ms
step:1344/2225 train_time:81066ms step_avg:60.32ms
step:1345/2225 train_time:81128ms step_avg:60.32ms
step:1346/2225 train_time:81187ms step_avg:60.32ms
step:1347/2225 train_time:81248ms step_avg:60.32ms
step:1348/2225 train_time:81307ms step_avg:60.32ms
step:1349/2225 train_time:81368ms step_avg:60.32ms
step:1350/2225 train_time:81427ms step_avg:60.32ms
step:1351/2225 train_time:81489ms step_avg:60.32ms
step:1352/2225 train_time:81548ms step_avg:60.32ms
step:1353/2225 train_time:81610ms step_avg:60.32ms
step:1354/2225 train_time:81670ms step_avg:60.32ms
step:1355/2225 train_time:81732ms step_avg:60.32ms
step:1356/2225 train_time:81792ms step_avg:60.32ms
step:1357/2225 train_time:81853ms step_avg:60.32ms
step:1358/2225 train_time:81914ms step_avg:60.32ms
step:1359/2225 train_time:81975ms step_avg:60.32ms
step:1360/2225 train_time:82035ms step_avg:60.32ms
step:1361/2225 train_time:82097ms step_avg:60.32ms
step:1362/2225 train_time:82157ms step_avg:60.32ms
step:1363/2225 train_time:82218ms step_avg:60.32ms
step:1364/2225 train_time:82279ms step_avg:60.32ms
step:1365/2225 train_time:82340ms step_avg:60.32ms
step:1366/2225 train_time:82400ms step_avg:60.32ms
step:1367/2225 train_time:82461ms step_avg:60.32ms
step:1368/2225 train_time:82522ms step_avg:60.32ms
step:1369/2225 train_time:82583ms step_avg:60.32ms
step:1370/2225 train_time:82644ms step_avg:60.32ms
step:1371/2225 train_time:82705ms step_avg:60.32ms
step:1372/2225 train_time:82765ms step_avg:60.32ms
step:1373/2225 train_time:82826ms step_avg:60.32ms
step:1374/2225 train_time:82886ms step_avg:60.32ms
step:1375/2225 train_time:82947ms step_avg:60.33ms
step:1376/2225 train_time:83007ms step_avg:60.32ms
step:1377/2225 train_time:83068ms step_avg:60.33ms
step:1378/2225 train_time:83127ms step_avg:60.32ms
step:1379/2225 train_time:83188ms step_avg:60.33ms
step:1380/2225 train_time:83248ms step_avg:60.32ms
step:1381/2225 train_time:83309ms step_avg:60.33ms
step:1382/2225 train_time:83369ms step_avg:60.32ms
step:1383/2225 train_time:83431ms step_avg:60.33ms
step:1384/2225 train_time:83491ms step_avg:60.33ms
step:1385/2225 train_time:83552ms step_avg:60.33ms
step:1386/2225 train_time:83612ms step_avg:60.33ms
step:1387/2225 train_time:83675ms step_avg:60.33ms
step:1388/2225 train_time:83735ms step_avg:60.33ms
step:1389/2225 train_time:83796ms step_avg:60.33ms
step:1390/2225 train_time:83857ms step_avg:60.33ms
step:1391/2225 train_time:83918ms step_avg:60.33ms
step:1392/2225 train_time:83979ms step_avg:60.33ms
step:1393/2225 train_time:84041ms step_avg:60.33ms
step:1394/2225 train_time:84100ms step_avg:60.33ms
step:1395/2225 train_time:84161ms step_avg:60.33ms
step:1396/2225 train_time:84221ms step_avg:60.33ms
step:1397/2225 train_time:84282ms step_avg:60.33ms
step:1398/2225 train_time:84342ms step_avg:60.33ms
step:1399/2225 train_time:84403ms step_avg:60.33ms
step:1400/2225 train_time:84463ms step_avg:60.33ms
step:1401/2225 train_time:84525ms step_avg:60.33ms
step:1402/2225 train_time:84585ms step_avg:60.33ms
step:1403/2225 train_time:84646ms step_avg:60.33ms
step:1404/2225 train_time:84705ms step_avg:60.33ms
step:1405/2225 train_time:84766ms step_avg:60.33ms
step:1406/2225 train_time:84826ms step_avg:60.33ms
step:1407/2225 train_time:84888ms step_avg:60.33ms
step:1408/2225 train_time:84947ms step_avg:60.33ms
step:1409/2225 train_time:85008ms step_avg:60.33ms
step:1410/2225 train_time:85067ms step_avg:60.33ms
step:1411/2225 train_time:85130ms step_avg:60.33ms
step:1412/2225 train_time:85189ms step_avg:60.33ms
step:1413/2225 train_time:85249ms step_avg:60.33ms
step:1414/2225 train_time:85308ms step_avg:60.33ms
step:1415/2225 train_time:85370ms step_avg:60.33ms
step:1416/2225 train_time:85430ms step_avg:60.33ms
step:1417/2225 train_time:85491ms step_avg:60.33ms
step:1418/2225 train_time:85551ms step_avg:60.33ms
step:1419/2225 train_time:85613ms step_avg:60.33ms
step:1420/2225 train_time:85673ms step_avg:60.33ms
step:1421/2225 train_time:85735ms step_avg:60.33ms
step:1422/2225 train_time:85796ms step_avg:60.33ms
step:1423/2225 train_time:85858ms step_avg:60.34ms
step:1424/2225 train_time:85919ms step_avg:60.34ms
step:1425/2225 train_time:85981ms step_avg:60.34ms
step:1426/2225 train_time:86041ms step_avg:60.34ms
step:1427/2225 train_time:86102ms step_avg:60.34ms
step:1428/2225 train_time:86162ms step_avg:60.34ms
step:1429/2225 train_time:86223ms step_avg:60.34ms
step:1430/2225 train_time:86283ms step_avg:60.34ms
step:1431/2225 train_time:86344ms step_avg:60.34ms
step:1432/2225 train_time:86403ms step_avg:60.34ms
step:1433/2225 train_time:86465ms step_avg:60.34ms
step:1434/2225 train_time:86524ms step_avg:60.34ms
step:1435/2225 train_time:86586ms step_avg:60.34ms
step:1436/2225 train_time:86645ms step_avg:60.34ms
step:1437/2225 train_time:86706ms step_avg:60.34ms
step:1438/2225 train_time:86767ms step_avg:60.34ms
step:1439/2225 train_time:86828ms step_avg:60.34ms
step:1440/2225 train_time:86888ms step_avg:60.34ms
step:1441/2225 train_time:86949ms step_avg:60.34ms
step:1442/2225 train_time:87008ms step_avg:60.34ms
step:1443/2225 train_time:87070ms step_avg:60.34ms
step:1444/2225 train_time:87129ms step_avg:60.34ms
step:1445/2225 train_time:87191ms step_avg:60.34ms
step:1446/2225 train_time:87250ms step_avg:60.34ms
step:1447/2225 train_time:87311ms step_avg:60.34ms
step:1448/2225 train_time:87371ms step_avg:60.34ms
step:1449/2225 train_time:87433ms step_avg:60.34ms
step:1450/2225 train_time:87492ms step_avg:60.34ms
step:1451/2225 train_time:87554ms step_avg:60.34ms
step:1452/2225 train_time:87614ms step_avg:60.34ms
step:1453/2225 train_time:87676ms step_avg:60.34ms
step:1454/2225 train_time:87737ms step_avg:60.34ms
step:1455/2225 train_time:87798ms step_avg:60.34ms
step:1456/2225 train_time:87858ms step_avg:60.34ms
step:1457/2225 train_time:87920ms step_avg:60.34ms
step:1458/2225 train_time:87980ms step_avg:60.34ms
step:1459/2225 train_time:88043ms step_avg:60.34ms
step:1460/2225 train_time:88103ms step_avg:60.34ms
step:1461/2225 train_time:88164ms step_avg:60.34ms
step:1462/2225 train_time:88224ms step_avg:60.34ms
step:1463/2225 train_time:88285ms step_avg:60.35ms
step:1464/2225 train_time:88346ms step_avg:60.35ms
step:1465/2225 train_time:88407ms step_avg:60.35ms
step:1466/2225 train_time:88467ms step_avg:60.35ms
step:1467/2225 train_time:88528ms step_avg:60.35ms
step:1468/2225 train_time:88588ms step_avg:60.35ms
step:1469/2225 train_time:88650ms step_avg:60.35ms
step:1470/2225 train_time:88710ms step_avg:60.35ms
step:1471/2225 train_time:88771ms step_avg:60.35ms
step:1472/2225 train_time:88831ms step_avg:60.35ms
step:1473/2225 train_time:88893ms step_avg:60.35ms
step:1474/2225 train_time:88954ms step_avg:60.35ms
step:1475/2225 train_time:89016ms step_avg:60.35ms
step:1476/2225 train_time:89077ms step_avg:60.35ms
step:1477/2225 train_time:89139ms step_avg:60.35ms
step:1478/2225 train_time:89200ms step_avg:60.35ms
step:1479/2225 train_time:89262ms step_avg:60.35ms
step:1480/2225 train_time:89322ms step_avg:60.35ms
step:1481/2225 train_time:89384ms step_avg:60.35ms
step:1482/2225 train_time:89444ms step_avg:60.35ms
step:1483/2225 train_time:89506ms step_avg:60.35ms
step:1484/2225 train_time:89566ms step_avg:60.35ms
step:1485/2225 train_time:89627ms step_avg:60.35ms
step:1486/2225 train_time:89687ms step_avg:60.35ms
step:1487/2225 train_time:89749ms step_avg:60.36ms
step:1488/2225 train_time:89809ms step_avg:60.36ms
step:1489/2225 train_time:89870ms step_avg:60.36ms
step:1490/2225 train_time:89930ms step_avg:60.36ms
step:1491/2225 train_time:89992ms step_avg:60.36ms
step:1492/2225 train_time:90053ms step_avg:60.36ms
step:1493/2225 train_time:90115ms step_avg:60.36ms
step:1494/2225 train_time:90176ms step_avg:60.36ms
step:1495/2225 train_time:90237ms step_avg:60.36ms
step:1496/2225 train_time:90298ms step_avg:60.36ms
step:1497/2225 train_time:90360ms step_avg:60.36ms
step:1498/2225 train_time:90420ms step_avg:60.36ms
step:1499/2225 train_time:90482ms step_avg:60.36ms
step:1500/2225 train_time:90543ms step_avg:60.36ms
step:1500/2225 val_loss:3.4405 train_time:90604ms step_avg:60.40ms
step:1501/2225 train_time:90626ms step_avg:60.38ms
step:1502/2225 train_time:90667ms step_avg:60.36ms
step:1503/2225 train_time:90730ms step_avg:60.37ms
step:1504/2225 train_time:90791ms step_avg:60.37ms
step:1505/2225 train_time:90854ms step_avg:60.37ms
step:1506/2225 train_time:90915ms step_avg:60.37ms
step:1507/2225 train_time:90975ms step_avg:60.37ms
step:1508/2225 train_time:91035ms step_avg:60.37ms
step:1509/2225 train_time:91095ms step_avg:60.37ms
step:1510/2225 train_time:91155ms step_avg:60.37ms
step:1511/2225 train_time:91215ms step_avg:60.37ms
step:1512/2225 train_time:91274ms step_avg:60.37ms
step:1513/2225 train_time:91335ms step_avg:60.37ms
step:1514/2225 train_time:91394ms step_avg:60.37ms
step:1515/2225 train_time:91456ms step_avg:60.37ms
step:1516/2225 train_time:91517ms step_avg:60.37ms
step:1517/2225 train_time:91583ms step_avg:60.37ms
step:1518/2225 train_time:91645ms step_avg:60.37ms
step:1519/2225 train_time:91707ms step_avg:60.37ms
step:1520/2225 train_time:91768ms step_avg:60.37ms
step:1521/2225 train_time:91831ms step_avg:60.38ms
step:1522/2225 train_time:91892ms step_avg:60.38ms
step:1523/2225 train_time:91953ms step_avg:60.38ms
step:1524/2225 train_time:92013ms step_avg:60.38ms
step:1525/2225 train_time:92074ms step_avg:60.38ms
step:1526/2225 train_time:92133ms step_avg:60.38ms
step:1527/2225 train_time:92194ms step_avg:60.38ms
step:1528/2225 train_time:92254ms step_avg:60.38ms
step:1529/2225 train_time:92314ms step_avg:60.38ms
step:1530/2225 train_time:92374ms step_avg:60.38ms
step:1531/2225 train_time:92436ms step_avg:60.38ms
step:1532/2225 train_time:92497ms step_avg:60.38ms
step:1533/2225 train_time:92561ms step_avg:60.38ms
step:1534/2225 train_time:92621ms step_avg:60.38ms
step:1535/2225 train_time:92683ms step_avg:60.38ms
step:1536/2225 train_time:92744ms step_avg:60.38ms
step:1537/2225 train_time:92806ms step_avg:60.38ms
step:1538/2225 train_time:92866ms step_avg:60.38ms
step:1539/2225 train_time:92929ms step_avg:60.38ms
step:1540/2225 train_time:92990ms step_avg:60.38ms
step:1541/2225 train_time:93052ms step_avg:60.38ms
step:1542/2225 train_time:93111ms step_avg:60.38ms
step:1543/2225 train_time:93172ms step_avg:60.38ms
step:1544/2225 train_time:93231ms step_avg:60.38ms
step:1545/2225 train_time:93292ms step_avg:60.38ms
step:1546/2225 train_time:93352ms step_avg:60.38ms
step:1547/2225 train_time:93413ms step_avg:60.38ms
step:1548/2225 train_time:93474ms step_avg:60.38ms
step:1549/2225 train_time:93537ms step_avg:60.39ms
step:1550/2225 train_time:93598ms step_avg:60.39ms
step:1551/2225 train_time:93660ms step_avg:60.39ms
step:1552/2225 train_time:93720ms step_avg:60.39ms
step:1553/2225 train_time:93782ms step_avg:60.39ms
step:1554/2225 train_time:93843ms step_avg:60.39ms
step:1555/2225 train_time:93905ms step_avg:60.39ms
step:1556/2225 train_time:93966ms step_avg:60.39ms
step:1557/2225 train_time:94028ms step_avg:60.39ms
step:1558/2225 train_time:94090ms step_avg:60.39ms
step:1559/2225 train_time:94151ms step_avg:60.39ms
step:1560/2225 train_time:94210ms step_avg:60.39ms
step:1561/2225 train_time:94272ms step_avg:60.39ms
step:1562/2225 train_time:94332ms step_avg:60.39ms
step:1563/2225 train_time:94393ms step_avg:60.39ms
step:1564/2225 train_time:94454ms step_avg:60.39ms
step:1565/2225 train_time:94516ms step_avg:60.39ms
step:1566/2225 train_time:94576ms step_avg:60.39ms
step:1567/2225 train_time:94638ms step_avg:60.39ms
step:1568/2225 train_time:94699ms step_avg:60.39ms
step:1569/2225 train_time:94761ms step_avg:60.40ms
step:1570/2225 train_time:94820ms step_avg:60.40ms
step:1571/2225 train_time:94882ms step_avg:60.40ms
step:1572/2225 train_time:94942ms step_avg:60.40ms
step:1573/2225 train_time:95005ms step_avg:60.40ms
step:1574/2225 train_time:95066ms step_avg:60.40ms
step:1575/2225 train_time:95128ms step_avg:60.40ms
step:1576/2225 train_time:95188ms step_avg:60.40ms
step:1577/2225 train_time:95251ms step_avg:60.40ms
step:1578/2225 train_time:95311ms step_avg:60.40ms
step:1579/2225 train_time:95373ms step_avg:60.40ms
step:1580/2225 train_time:95433ms step_avg:60.40ms
step:1581/2225 train_time:95494ms step_avg:60.40ms
step:1582/2225 train_time:95554ms step_avg:60.40ms
step:1583/2225 train_time:95616ms step_avg:60.40ms
step:1584/2225 train_time:95677ms step_avg:60.40ms
step:1585/2225 train_time:95739ms step_avg:60.40ms
step:1586/2225 train_time:95799ms step_avg:60.40ms
step:1587/2225 train_time:95861ms step_avg:60.40ms
step:1588/2225 train_time:95920ms step_avg:60.40ms
step:1589/2225 train_time:95982ms step_avg:60.40ms
step:1590/2225 train_time:96043ms step_avg:60.40ms
step:1591/2225 train_time:96105ms step_avg:60.41ms
step:1592/2225 train_time:96165ms step_avg:60.41ms
step:1593/2225 train_time:96228ms step_avg:60.41ms
step:1594/2225 train_time:96288ms step_avg:60.41ms
step:1595/2225 train_time:96350ms step_avg:60.41ms
step:1596/2225 train_time:96410ms step_avg:60.41ms
step:1597/2225 train_time:96472ms step_avg:60.41ms
step:1598/2225 train_time:96532ms step_avg:60.41ms
step:1599/2225 train_time:96594ms step_avg:60.41ms
step:1600/2225 train_time:96654ms step_avg:60.41ms
step:1601/2225 train_time:96716ms step_avg:60.41ms
step:1602/2225 train_time:96776ms step_avg:60.41ms
step:1603/2225 train_time:96838ms step_avg:60.41ms
step:1604/2225 train_time:96898ms step_avg:60.41ms
step:1605/2225 train_time:96960ms step_avg:60.41ms
step:1606/2225 train_time:97020ms step_avg:60.41ms
step:1607/2225 train_time:97081ms step_avg:60.41ms
step:1608/2225 train_time:97141ms step_avg:60.41ms
step:1609/2225 train_time:97203ms step_avg:60.41ms
step:1610/2225 train_time:97264ms step_avg:60.41ms
step:1611/2225 train_time:97325ms step_avg:60.41ms
step:1612/2225 train_time:97386ms step_avg:60.41ms
step:1613/2225 train_time:97449ms step_avg:60.41ms
step:1614/2225 train_time:97510ms step_avg:60.42ms
step:1615/2225 train_time:97572ms step_avg:60.42ms
step:1616/2225 train_time:97633ms step_avg:60.42ms
step:1617/2225 train_time:97696ms step_avg:60.42ms
step:1618/2225 train_time:97756ms step_avg:60.42ms
step:1619/2225 train_time:97817ms step_avg:60.42ms
step:1620/2225 train_time:97877ms step_avg:60.42ms
step:1621/2225 train_time:97939ms step_avg:60.42ms
step:1622/2225 train_time:97999ms step_avg:60.42ms
step:1623/2225 train_time:98060ms step_avg:60.42ms
step:1624/2225 train_time:98119ms step_avg:60.42ms
step:1625/2225 train_time:98181ms step_avg:60.42ms
step:1626/2225 train_time:98241ms step_avg:60.42ms
step:1627/2225 train_time:98304ms step_avg:60.42ms
step:1628/2225 train_time:98364ms step_avg:60.42ms
step:1629/2225 train_time:98427ms step_avg:60.42ms
step:1630/2225 train_time:98488ms step_avg:60.42ms
step:1631/2225 train_time:98551ms step_avg:60.42ms
step:1632/2225 train_time:98611ms step_avg:60.42ms
step:1633/2225 train_time:98673ms step_avg:60.42ms
step:1634/2225 train_time:98734ms step_avg:60.42ms
step:1635/2225 train_time:98796ms step_avg:60.43ms
step:1636/2225 train_time:98856ms step_avg:60.43ms
step:1637/2225 train_time:98917ms step_avg:60.43ms
step:1638/2225 train_time:98977ms step_avg:60.43ms
step:1639/2225 train_time:99039ms step_avg:60.43ms
step:1640/2225 train_time:99099ms step_avg:60.43ms
step:1641/2225 train_time:99161ms step_avg:60.43ms
step:1642/2225 train_time:99220ms step_avg:60.43ms
step:1643/2225 train_time:99282ms step_avg:60.43ms
step:1644/2225 train_time:99342ms step_avg:60.43ms
step:1645/2225 train_time:99404ms step_avg:60.43ms
step:1646/2225 train_time:99464ms step_avg:60.43ms
step:1647/2225 train_time:99527ms step_avg:60.43ms
step:1648/2225 train_time:99587ms step_avg:60.43ms
step:1649/2225 train_time:99649ms step_avg:60.43ms
step:1650/2225 train_time:99710ms step_avg:60.43ms
step:1651/2225 train_time:99772ms step_avg:60.43ms
step:1652/2225 train_time:99832ms step_avg:60.43ms
step:1653/2225 train_time:99894ms step_avg:60.43ms
step:1654/2225 train_time:99954ms step_avg:60.43ms
step:1655/2225 train_time:100016ms step_avg:60.43ms
step:1656/2225 train_time:100076ms step_avg:60.43ms
step:1657/2225 train_time:100138ms step_avg:60.43ms
step:1658/2225 train_time:100198ms step_avg:60.43ms
step:1659/2225 train_time:100260ms step_avg:60.43ms
step:1660/2225 train_time:100320ms step_avg:60.43ms
step:1661/2225 train_time:100382ms step_avg:60.43ms
step:1662/2225 train_time:100442ms step_avg:60.43ms
step:1663/2225 train_time:100504ms step_avg:60.44ms
step:1664/2225 train_time:100564ms step_avg:60.44ms
step:1665/2225 train_time:100627ms step_avg:60.44ms
step:1666/2225 train_time:100688ms step_avg:60.44ms
step:1667/2225 train_time:100750ms step_avg:60.44ms
step:1668/2225 train_time:100811ms step_avg:60.44ms
step:1669/2225 train_time:100873ms step_avg:60.44ms
step:1670/2225 train_time:100933ms step_avg:60.44ms
step:1671/2225 train_time:100994ms step_avg:60.44ms
step:1672/2225 train_time:101055ms step_avg:60.44ms
step:1673/2225 train_time:101116ms step_avg:60.44ms
step:1674/2225 train_time:101177ms step_avg:60.44ms
step:1675/2225 train_time:101239ms step_avg:60.44ms
step:1676/2225 train_time:101298ms step_avg:60.44ms
step:1677/2225 train_time:101360ms step_avg:60.44ms
step:1678/2225 train_time:101420ms step_avg:60.44ms
step:1679/2225 train_time:101482ms step_avg:60.44ms
step:1680/2225 train_time:101542ms step_avg:60.44ms
step:1681/2225 train_time:101604ms step_avg:60.44ms
step:1682/2225 train_time:101665ms step_avg:60.44ms
step:1683/2225 train_time:101728ms step_avg:60.44ms
step:1684/2225 train_time:101789ms step_avg:60.44ms
step:1685/2225 train_time:101851ms step_avg:60.45ms
step:1686/2225 train_time:101911ms step_avg:60.45ms
step:1687/2225 train_time:101973ms step_avg:60.45ms
step:1688/2225 train_time:102033ms step_avg:60.45ms
step:1689/2225 train_time:102095ms step_avg:60.45ms
step:1690/2225 train_time:102155ms step_avg:60.45ms
step:1691/2225 train_time:102217ms step_avg:60.45ms
step:1692/2225 train_time:102277ms step_avg:60.45ms
step:1693/2225 train_time:102339ms step_avg:60.45ms
step:1694/2225 train_time:102400ms step_avg:60.45ms
step:1695/2225 train_time:102460ms step_avg:60.45ms
step:1696/2225 train_time:102520ms step_avg:60.45ms
step:1697/2225 train_time:102582ms step_avg:60.45ms
step:1698/2225 train_time:102642ms step_avg:60.45ms
step:1699/2225 train_time:102704ms step_avg:60.45ms
step:1700/2225 train_time:102765ms step_avg:60.45ms
step:1701/2225 train_time:102828ms step_avg:60.45ms
step:1702/2225 train_time:102889ms step_avg:60.45ms
step:1703/2225 train_time:102952ms step_avg:60.45ms
step:1704/2225 train_time:103011ms step_avg:60.45ms
step:1705/2225 train_time:103073ms step_avg:60.45ms
step:1706/2225 train_time:103134ms step_avg:60.45ms
step:1707/2225 train_time:103195ms step_avg:60.45ms
step:1708/2225 train_time:103255ms step_avg:60.45ms
step:1709/2225 train_time:103317ms step_avg:60.45ms
step:1710/2225 train_time:103377ms step_avg:60.45ms
step:1711/2225 train_time:103438ms step_avg:60.45ms
step:1712/2225 train_time:103498ms step_avg:60.45ms
step:1713/2225 train_time:103560ms step_avg:60.46ms
step:1714/2225 train_time:103620ms step_avg:60.45ms
step:1715/2225 train_time:103681ms step_avg:60.46ms
step:1716/2225 train_time:103741ms step_avg:60.46ms
step:1717/2225 train_time:103804ms step_avg:60.46ms
step:1718/2225 train_time:103864ms step_avg:60.46ms
step:1719/2225 train_time:103928ms step_avg:60.46ms
step:1720/2225 train_time:103988ms step_avg:60.46ms
step:1721/2225 train_time:104051ms step_avg:60.46ms
step:1722/2225 train_time:104111ms step_avg:60.46ms
step:1723/2225 train_time:104173ms step_avg:60.46ms
step:1724/2225 train_time:104233ms step_avg:60.46ms
step:1725/2225 train_time:104294ms step_avg:60.46ms
step:1726/2225 train_time:104354ms step_avg:60.46ms
step:1727/2225 train_time:104415ms step_avg:60.46ms
step:1728/2225 train_time:104475ms step_avg:60.46ms
step:1729/2225 train_time:104537ms step_avg:60.46ms
step:1730/2225 train_time:104597ms step_avg:60.46ms
step:1731/2225 train_time:104659ms step_avg:60.46ms
step:1732/2225 train_time:104719ms step_avg:60.46ms
step:1733/2225 train_time:104781ms step_avg:60.46ms
step:1734/2225 train_time:104841ms step_avg:60.46ms
step:1735/2225 train_time:104903ms step_avg:60.46ms
step:1736/2225 train_time:104963ms step_avg:60.46ms
step:1737/2225 train_time:105026ms step_avg:60.46ms
step:1738/2225 train_time:105087ms step_avg:60.46ms
step:1739/2225 train_time:105150ms step_avg:60.47ms
step:1740/2225 train_time:105210ms step_avg:60.47ms
step:1741/2225 train_time:105271ms step_avg:60.47ms
step:1742/2225 train_time:105332ms step_avg:60.47ms
step:1743/2225 train_time:105393ms step_avg:60.47ms
step:1744/2225 train_time:105453ms step_avg:60.47ms
step:1745/2225 train_time:105515ms step_avg:60.47ms
step:1746/2225 train_time:105575ms step_avg:60.47ms
step:1747/2225 train_time:105637ms step_avg:60.47ms
step:1748/2225 train_time:105697ms step_avg:60.47ms
step:1749/2225 train_time:105758ms step_avg:60.47ms
step:1750/2225 train_time:105818ms step_avg:60.47ms
step:1750/2225 val_loss:3.3768 train_time:105881ms step_avg:60.50ms
step:1751/2225 train_time:105902ms step_avg:60.48ms
step:1752/2225 train_time:105944ms step_avg:60.47ms
step:1753/2225 train_time:106009ms step_avg:60.47ms
step:1754/2225 train_time:106070ms step_avg:60.47ms
step:1755/2225 train_time:106132ms step_avg:60.47ms
step:1756/2225 train_time:106192ms step_avg:60.47ms
step:1757/2225 train_time:106253ms step_avg:60.47ms
step:1758/2225 train_time:106313ms step_avg:60.47ms
step:1759/2225 train_time:106375ms step_avg:60.47ms
step:1760/2225 train_time:106434ms step_avg:60.47ms
step:1761/2225 train_time:106495ms step_avg:60.47ms
step:1762/2225 train_time:106554ms step_avg:60.47ms
step:1763/2225 train_time:106616ms step_avg:60.47ms
step:1764/2225 train_time:106676ms step_avg:60.47ms
step:1765/2225 train_time:106738ms step_avg:60.47ms
step:1766/2225 train_time:106798ms step_avg:60.47ms
step:1767/2225 train_time:106861ms step_avg:60.48ms
step:1768/2225 train_time:106923ms step_avg:60.48ms
step:1769/2225 train_time:106986ms step_avg:60.48ms
step:1770/2225 train_time:107046ms step_avg:60.48ms
step:1771/2225 train_time:107108ms step_avg:60.48ms
step:1772/2225 train_time:107168ms step_avg:60.48ms
step:1773/2225 train_time:107229ms step_avg:60.48ms
step:1774/2225 train_time:107289ms step_avg:60.48ms
step:1775/2225 train_time:107350ms step_avg:60.48ms
step:1776/2225 train_time:107409ms step_avg:60.48ms
step:1777/2225 train_time:107470ms step_avg:60.48ms
step:1778/2225 train_time:107530ms step_avg:60.48ms
step:1779/2225 train_time:107591ms step_avg:60.48ms
step:1780/2225 train_time:107650ms step_avg:60.48ms
step:1781/2225 train_time:107712ms step_avg:60.48ms
step:1782/2225 train_time:107772ms step_avg:60.48ms
step:1783/2225 train_time:107834ms step_avg:60.48ms
step:1784/2225 train_time:107895ms step_avg:60.48ms
step:1785/2225 train_time:107960ms step_avg:60.48ms
step:1786/2225 train_time:108021ms step_avg:60.48ms
step:1787/2225 train_time:108083ms step_avg:60.48ms
step:1788/2225 train_time:108143ms step_avg:60.48ms
step:1789/2225 train_time:108205ms step_avg:60.48ms
step:1790/2225 train_time:108265ms step_avg:60.48ms
step:1791/2225 train_time:108326ms step_avg:60.48ms
step:1792/2225 train_time:108386ms step_avg:60.48ms
step:1793/2225 train_time:108447ms step_avg:60.48ms
step:1794/2225 train_time:108507ms step_avg:60.48ms
step:1795/2225 train_time:108568ms step_avg:60.48ms
step:1796/2225 train_time:108628ms step_avg:60.48ms
step:1797/2225 train_time:108690ms step_avg:60.48ms
step:1798/2225 train_time:108749ms step_avg:60.48ms
step:1799/2225 train_time:108811ms step_avg:60.48ms
step:1800/2225 train_time:108871ms step_avg:60.48ms
step:1801/2225 train_time:108934ms step_avg:60.49ms
step:1802/2225 train_time:108995ms step_avg:60.49ms
step:1803/2225 train_time:109058ms step_avg:60.49ms
step:1804/2225 train_time:109119ms step_avg:60.49ms
step:1805/2225 train_time:109181ms step_avg:60.49ms
step:1806/2225 train_time:109241ms step_avg:60.49ms
step:1807/2225 train_time:109302ms step_avg:60.49ms
step:1808/2225 train_time:109362ms step_avg:60.49ms
step:1809/2225 train_time:109423ms step_avg:60.49ms
step:1810/2225 train_time:109484ms step_avg:60.49ms
step:1811/2225 train_time:109546ms step_avg:60.49ms
step:1812/2225 train_time:109605ms step_avg:60.49ms
step:1813/2225 train_time:109667ms step_avg:60.49ms
step:1814/2225 train_time:109727ms step_avg:60.49ms
step:1815/2225 train_time:109789ms step_avg:60.49ms
step:1816/2225 train_time:109849ms step_avg:60.49ms
step:1817/2225 train_time:109911ms step_avg:60.49ms
step:1818/2225 train_time:109971ms step_avg:60.49ms
step:1819/2225 train_time:110033ms step_avg:60.49ms
step:1820/2225 train_time:110094ms step_avg:60.49ms
step:1821/2225 train_time:110156ms step_avg:60.49ms
step:1822/2225 train_time:110217ms step_avg:60.49ms
step:1823/2225 train_time:110279ms step_avg:60.49ms
step:1824/2225 train_time:110339ms step_avg:60.49ms
step:1825/2225 train_time:110400ms step_avg:60.49ms
step:1826/2225 train_time:110461ms step_avg:60.49ms
step:1827/2225 train_time:110522ms step_avg:60.49ms
step:1828/2225 train_time:110583ms step_avg:60.49ms
step:1829/2225 train_time:110644ms step_avg:60.49ms
step:1830/2225 train_time:110705ms step_avg:60.49ms
step:1831/2225 train_time:110766ms step_avg:60.50ms
step:1832/2225 train_time:110827ms step_avg:60.49ms
step:1833/2225 train_time:110888ms step_avg:60.50ms
step:1834/2225 train_time:110948ms step_avg:60.50ms
step:1835/2225 train_time:111010ms step_avg:60.50ms
step:1836/2225 train_time:111071ms step_avg:60.50ms
step:1837/2225 train_time:111132ms step_avg:60.50ms
step:1838/2225 train_time:111192ms step_avg:60.50ms
step:1839/2225 train_time:111255ms step_avg:60.50ms
step:1840/2225 train_time:111315ms step_avg:60.50ms
step:1841/2225 train_time:111377ms step_avg:60.50ms
step:1842/2225 train_time:111438ms step_avg:60.50ms
step:1843/2225 train_time:111500ms step_avg:60.50ms
step:1844/2225 train_time:111561ms step_avg:60.50ms
step:1845/2225 train_time:111622ms step_avg:60.50ms
step:1846/2225 train_time:111683ms step_avg:60.50ms
step:1847/2225 train_time:111745ms step_avg:60.50ms
step:1848/2225 train_time:111804ms step_avg:60.50ms
step:1849/2225 train_time:111866ms step_avg:60.50ms
step:1850/2225 train_time:111927ms step_avg:60.50ms
step:1851/2225 train_time:111988ms step_avg:60.50ms
step:1852/2225 train_time:112048ms step_avg:60.50ms
step:1853/2225 train_time:112110ms step_avg:60.50ms
step:1854/2225 train_time:112170ms step_avg:60.50ms
step:1855/2225 train_time:112232ms step_avg:60.50ms
step:1856/2225 train_time:112292ms step_avg:60.50ms
step:1857/2225 train_time:112353ms step_avg:60.50ms
step:1858/2225 train_time:112414ms step_avg:60.50ms
step:1859/2225 train_time:112476ms step_avg:60.50ms
step:1860/2225 train_time:112537ms step_avg:60.50ms
step:1861/2225 train_time:112600ms step_avg:60.51ms
step:1862/2225 train_time:112660ms step_avg:60.50ms
step:1863/2225 train_time:112722ms step_avg:60.51ms
step:1864/2225 train_time:112782ms step_avg:60.51ms
step:1865/2225 train_time:112843ms step_avg:60.51ms
step:1866/2225 train_time:112903ms step_avg:60.51ms
step:1867/2225 train_time:112965ms step_avg:60.51ms
step:1868/2225 train_time:113025ms step_avg:60.51ms
step:1869/2225 train_time:113087ms step_avg:60.51ms
step:1870/2225 train_time:113148ms step_avg:60.51ms
step:1871/2225 train_time:113209ms step_avg:60.51ms
step:1872/2225 train_time:113269ms step_avg:60.51ms
step:1873/2225 train_time:113331ms step_avg:60.51ms
step:1874/2225 train_time:113390ms step_avg:60.51ms
step:1875/2225 train_time:113452ms step_avg:60.51ms
step:1876/2225 train_time:113512ms step_avg:60.51ms
step:1877/2225 train_time:113574ms step_avg:60.51ms
step:1878/2225 train_time:113635ms step_avg:60.51ms
step:1879/2225 train_time:113697ms step_avg:60.51ms
step:1880/2225 train_time:113758ms step_avg:60.51ms
step:1881/2225 train_time:113820ms step_avg:60.51ms
step:1882/2225 train_time:113880ms step_avg:60.51ms
step:1883/2225 train_time:113941ms step_avg:60.51ms
step:1884/2225 train_time:114002ms step_avg:60.51ms
step:1885/2225 train_time:114064ms step_avg:60.51ms
step:1886/2225 train_time:114124ms step_avg:60.51ms
step:1887/2225 train_time:114187ms step_avg:60.51ms
step:1888/2225 train_time:114246ms step_avg:60.51ms
step:1889/2225 train_time:114308ms step_avg:60.51ms
step:1890/2225 train_time:114368ms step_avg:60.51ms
step:1891/2225 train_time:114430ms step_avg:60.51ms
step:1892/2225 train_time:114489ms step_avg:60.51ms
step:1893/2225 train_time:114551ms step_avg:60.51ms
step:1894/2225 train_time:114611ms step_avg:60.51ms
step:1895/2225 train_time:114673ms step_avg:60.51ms
step:1896/2225 train_time:114733ms step_avg:60.51ms
step:1897/2225 train_time:114795ms step_avg:60.51ms
step:1898/2225 train_time:114855ms step_avg:60.51ms
step:1899/2225 train_time:114918ms step_avg:60.51ms
step:1900/2225 train_time:114979ms step_avg:60.52ms
step:1901/2225 train_time:115041ms step_avg:60.52ms
step:1902/2225 train_time:115102ms step_avg:60.52ms
step:1903/2225 train_time:115164ms step_avg:60.52ms
step:1904/2225 train_time:115224ms step_avg:60.52ms
step:1905/2225 train_time:115286ms step_avg:60.52ms
step:1906/2225 train_time:115346ms step_avg:60.52ms
step:1907/2225 train_time:115408ms step_avg:60.52ms
step:1908/2225 train_time:115468ms step_avg:60.52ms
step:1909/2225 train_time:115529ms step_avg:60.52ms
step:1910/2225 train_time:115589ms step_avg:60.52ms
step:1911/2225 train_time:115651ms step_avg:60.52ms
step:1912/2225 train_time:115710ms step_avg:60.52ms
step:1913/2225 train_time:115773ms step_avg:60.52ms
step:1914/2225 train_time:115833ms step_avg:60.52ms
step:1915/2225 train_time:115895ms step_avg:60.52ms
step:1916/2225 train_time:115955ms step_avg:60.52ms
step:1917/2225 train_time:116018ms step_avg:60.52ms
step:1918/2225 train_time:116079ms step_avg:60.52ms
step:1919/2225 train_time:116141ms step_avg:60.52ms
step:1920/2225 train_time:116202ms step_avg:60.52ms
step:1921/2225 train_time:116264ms step_avg:60.52ms
step:1922/2225 train_time:116325ms step_avg:60.52ms
step:1923/2225 train_time:116386ms step_avg:60.52ms
step:1924/2225 train_time:116446ms step_avg:60.52ms
step:1925/2225 train_time:116508ms step_avg:60.52ms
step:1926/2225 train_time:116567ms step_avg:60.52ms
step:1927/2225 train_time:116629ms step_avg:60.52ms
step:1928/2225 train_time:116688ms step_avg:60.52ms
step:1929/2225 train_time:116750ms step_avg:60.52ms
step:1930/2225 train_time:116810ms step_avg:60.52ms
step:1931/2225 train_time:116872ms step_avg:60.52ms
step:1932/2225 train_time:116932ms step_avg:60.52ms
step:1933/2225 train_time:116994ms step_avg:60.52ms
step:1934/2225 train_time:117055ms step_avg:60.52ms
step:1935/2225 train_time:117118ms step_avg:60.53ms
step:1936/2225 train_time:117178ms step_avg:60.53ms
step:1937/2225 train_time:117240ms step_avg:60.53ms
step:1938/2225 train_time:117301ms step_avg:60.53ms
step:1939/2225 train_time:117364ms step_avg:60.53ms
step:1940/2225 train_time:117425ms step_avg:60.53ms
step:1941/2225 train_time:117486ms step_avg:60.53ms
step:1942/2225 train_time:117546ms step_avg:60.53ms
step:1943/2225 train_time:117608ms step_avg:60.53ms
step:1944/2225 train_time:117668ms step_avg:60.53ms
step:1945/2225 train_time:117730ms step_avg:60.53ms
step:1946/2225 train_time:117790ms step_avg:60.53ms
step:1947/2225 train_time:117851ms step_avg:60.53ms
step:1948/2225 train_time:117911ms step_avg:60.53ms
step:1949/2225 train_time:117972ms step_avg:60.53ms
step:1950/2225 train_time:118033ms step_avg:60.53ms
step:1951/2225 train_time:118095ms step_avg:60.53ms
step:1952/2225 train_time:118155ms step_avg:60.53ms
step:1953/2225 train_time:118217ms step_avg:60.53ms
step:1954/2225 train_time:118279ms step_avg:60.53ms
step:1955/2225 train_time:118342ms step_avg:60.53ms
step:1956/2225 train_time:118402ms step_avg:60.53ms
step:1957/2225 train_time:118464ms step_avg:60.53ms
step:1958/2225 train_time:118524ms step_avg:60.53ms
step:1959/2225 train_time:118586ms step_avg:60.53ms
step:1960/2225 train_time:118646ms step_avg:60.53ms
step:1961/2225 train_time:118708ms step_avg:60.53ms
step:1962/2225 train_time:118768ms step_avg:60.53ms
step:1963/2225 train_time:118829ms step_avg:60.53ms
step:1964/2225 train_time:118889ms step_avg:60.53ms
step:1965/2225 train_time:118951ms step_avg:60.53ms
step:1966/2225 train_time:119011ms step_avg:60.53ms
step:1967/2225 train_time:119073ms step_avg:60.54ms
step:1968/2225 train_time:119134ms step_avg:60.54ms
step:1969/2225 train_time:119196ms step_avg:60.54ms
step:1970/2225 train_time:119256ms step_avg:60.54ms
step:1971/2225 train_time:119320ms step_avg:60.54ms
step:1972/2225 train_time:119380ms step_avg:60.54ms
step:1973/2225 train_time:119442ms step_avg:60.54ms
step:1974/2225 train_time:119503ms step_avg:60.54ms
step:1975/2225 train_time:119564ms step_avg:60.54ms
step:1976/2225 train_time:119625ms step_avg:60.54ms
step:1977/2225 train_time:119685ms step_avg:60.54ms
step:1978/2225 train_time:119745ms step_avg:60.54ms
step:1979/2225 train_time:119807ms step_avg:60.54ms
step:1980/2225 train_time:119868ms step_avg:60.54ms
step:1981/2225 train_time:119930ms step_avg:60.54ms
step:1982/2225 train_time:119990ms step_avg:60.54ms
step:1983/2225 train_time:120052ms step_avg:60.54ms
step:1984/2225 train_time:120111ms step_avg:60.54ms
step:1985/2225 train_time:120173ms step_avg:60.54ms
step:1986/2225 train_time:120233ms step_avg:60.54ms
step:1987/2225 train_time:120296ms step_avg:60.54ms
step:1988/2225 train_time:120357ms step_avg:60.54ms
step:1989/2225 train_time:120419ms step_avg:60.54ms
step:1990/2225 train_time:120479ms step_avg:60.54ms
step:1991/2225 train_time:120541ms step_avg:60.54ms
step:1992/2225 train_time:120601ms step_avg:60.54ms
step:1993/2225 train_time:120664ms step_avg:60.54ms
step:1994/2225 train_time:120724ms step_avg:60.54ms
step:1995/2225 train_time:120786ms step_avg:60.54ms
step:1996/2225 train_time:120846ms step_avg:60.54ms
step:1997/2225 train_time:120908ms step_avg:60.54ms
step:1998/2225 train_time:120968ms step_avg:60.54ms
step:1999/2225 train_time:121030ms step_avg:60.55ms
step:2000/2225 train_time:121090ms step_avg:60.54ms
step:2000/2225 val_loss:3.3220 train_time:121152ms step_avg:60.58ms
step:2001/2225 train_time:121173ms step_avg:60.56ms
step:2002/2225 train_time:121214ms step_avg:60.55ms
step:2003/2225 train_time:121281ms step_avg:60.55ms
step:2004/2225 train_time:121344ms step_avg:60.55ms
step:2005/2225 train_time:121406ms step_avg:60.55ms
step:2006/2225 train_time:121466ms step_avg:60.55ms
step:2007/2225 train_time:121527ms step_avg:60.55ms
step:2008/2225 train_time:121586ms step_avg:60.55ms
step:2009/2225 train_time:121648ms step_avg:60.55ms
step:2010/2225 train_time:121708ms step_avg:60.55ms
step:2011/2225 train_time:121768ms step_avg:60.55ms
step:2012/2225 train_time:121827ms step_avg:60.55ms
step:2013/2225 train_time:121888ms step_avg:60.55ms
step:2014/2225 train_time:121947ms step_avg:60.55ms
step:2015/2225 train_time:122008ms step_avg:60.55ms
step:2016/2225 train_time:122068ms step_avg:60.55ms
step:2017/2225 train_time:122132ms step_avg:60.55ms
step:2018/2225 train_time:122193ms step_avg:60.55ms
step:2019/2225 train_time:122256ms step_avg:60.55ms
step:2020/2225 train_time:122317ms step_avg:60.55ms
step:2021/2225 train_time:122379ms step_avg:60.55ms
step:2022/2225 train_time:122440ms step_avg:60.55ms
step:2023/2225 train_time:122502ms step_avg:60.55ms
step:2024/2225 train_time:122562ms step_avg:60.55ms
step:2025/2225 train_time:122624ms step_avg:60.55ms
step:2026/2225 train_time:122684ms step_avg:60.55ms
step:2027/2225 train_time:122745ms step_avg:60.56ms
step:2028/2225 train_time:122805ms step_avg:60.55ms
step:2029/2225 train_time:122865ms step_avg:60.55ms
step:2030/2225 train_time:122925ms step_avg:60.55ms
step:2031/2225 train_time:122986ms step_avg:60.55ms
step:2032/2225 train_time:123047ms step_avg:60.55ms
step:2033/2225 train_time:123109ms step_avg:60.56ms
step:2034/2225 train_time:123170ms step_avg:60.56ms
step:2035/2225 train_time:123234ms step_avg:60.56ms
step:2036/2225 train_time:123294ms step_avg:60.56ms
step:2037/2225 train_time:123356ms step_avg:60.56ms
step:2038/2225 train_time:123416ms step_avg:60.56ms
step:2039/2225 train_time:123478ms step_avg:60.56ms
step:2040/2225 train_time:123538ms step_avg:60.56ms
step:2041/2225 train_time:123601ms step_avg:60.56ms
step:2042/2225 train_time:123660ms step_avg:60.56ms
step:2043/2225 train_time:123723ms step_avg:60.56ms
step:2044/2225 train_time:123783ms step_avg:60.56ms
step:2045/2225 train_time:123844ms step_avg:60.56ms
step:2046/2225 train_time:123904ms step_avg:60.56ms
step:2047/2225 train_time:123966ms step_avg:60.56ms
step:2048/2225 train_time:124026ms step_avg:60.56ms
step:2049/2225 train_time:124088ms step_avg:60.56ms
step:2050/2225 train_time:124149ms step_avg:60.56ms
step:2051/2225 train_time:124211ms step_avg:60.56ms
step:2052/2225 train_time:124272ms step_avg:60.56ms
step:2053/2225 train_time:124334ms step_avg:60.56ms
step:2054/2225 train_time:124394ms step_avg:60.56ms
step:2055/2225 train_time:124456ms step_avg:60.56ms
step:2056/2225 train_time:124516ms step_avg:60.56ms
step:2057/2225 train_time:124578ms step_avg:60.56ms
step:2058/2225 train_time:124638ms step_avg:60.56ms
step:2059/2225 train_time:124701ms step_avg:60.56ms
step:2060/2225 train_time:124761ms step_avg:60.56ms
step:2061/2225 train_time:124822ms step_avg:60.56ms
step:2062/2225 train_time:124882ms step_avg:60.56ms
step:2063/2225 train_time:124945ms step_avg:60.56ms
step:2064/2225 train_time:125005ms step_avg:60.56ms
step:2065/2225 train_time:125066ms step_avg:60.56ms
step:2066/2225 train_time:125127ms step_avg:60.56ms
step:2067/2225 train_time:125190ms step_avg:60.57ms
step:2068/2225 train_time:125250ms step_avg:60.57ms
step:2069/2225 train_time:125312ms step_avg:60.57ms
step:2070/2225 train_time:125372ms step_avg:60.57ms
step:2071/2225 train_time:125434ms step_avg:60.57ms
step:2072/2225 train_time:125494ms step_avg:60.57ms
step:2073/2225 train_time:125556ms step_avg:60.57ms
step:2074/2225 train_time:125616ms step_avg:60.57ms
step:2075/2225 train_time:125677ms step_avg:60.57ms
step:2076/2225 train_time:125737ms step_avg:60.57ms
step:2077/2225 train_time:125799ms step_avg:60.57ms
step:2078/2225 train_time:125860ms step_avg:60.57ms
step:2079/2225 train_time:125922ms step_avg:60.57ms
step:2080/2225 train_time:125983ms step_avg:60.57ms
step:2081/2225 train_time:126045ms step_avg:60.57ms
step:2082/2225 train_time:126105ms step_avg:60.57ms
step:2083/2225 train_time:126168ms step_avg:60.57ms
step:2084/2225 train_time:126228ms step_avg:60.57ms
step:2085/2225 train_time:126290ms step_avg:60.57ms
step:2086/2225 train_time:126351ms step_avg:60.57ms
step:2087/2225 train_time:126412ms step_avg:60.57ms
step:2088/2225 train_time:126472ms step_avg:60.57ms
step:2089/2225 train_time:126534ms step_avg:60.57ms
step:2090/2225 train_time:126594ms step_avg:60.57ms
step:2091/2225 train_time:126655ms step_avg:60.57ms
step:2092/2225 train_time:126715ms step_avg:60.57ms
step:2093/2225 train_time:126777ms step_avg:60.57ms
step:2094/2225 train_time:126837ms step_avg:60.57ms
step:2095/2225 train_time:126899ms step_avg:60.57ms
step:2096/2225 train_time:126959ms step_avg:60.57ms
step:2097/2225 train_time:127021ms step_avg:60.57ms
step:2098/2225 train_time:127082ms step_avg:60.57ms
step:2099/2225 train_time:127145ms step_avg:60.57ms
step:2100/2225 train_time:127205ms step_avg:60.57ms
step:2101/2225 train_time:127267ms step_avg:60.57ms
step:2102/2225 train_time:127328ms step_avg:60.57ms
step:2103/2225 train_time:127390ms step_avg:60.58ms
step:2104/2225 train_time:127450ms step_avg:60.58ms
step:2105/2225 train_time:127511ms step_avg:60.58ms
step:2106/2225 train_time:127571ms step_avg:60.58ms
step:2107/2225 train_time:127633ms step_avg:60.58ms
step:2108/2225 train_time:127693ms step_avg:60.58ms
step:2109/2225 train_time:127755ms step_avg:60.58ms
step:2110/2225 train_time:127815ms step_avg:60.58ms
step:2111/2225 train_time:127876ms step_avg:60.58ms
step:2112/2225 train_time:127936ms step_avg:60.58ms
step:2113/2225 train_time:127998ms step_avg:60.58ms
step:2114/2225 train_time:128059ms step_avg:60.58ms
step:2115/2225 train_time:128122ms step_avg:60.58ms
step:2116/2225 train_time:128184ms step_avg:60.58ms
step:2117/2225 train_time:128246ms step_avg:60.58ms
step:2118/2225 train_time:128306ms step_avg:60.58ms
step:2119/2225 train_time:128368ms step_avg:60.58ms
step:2120/2225 train_time:128429ms step_avg:60.58ms
step:2121/2225 train_time:128490ms step_avg:60.58ms
step:2122/2225 train_time:128551ms step_avg:60.58ms
step:2123/2225 train_time:128612ms step_avg:60.58ms
step:2124/2225 train_time:128672ms step_avg:60.58ms
step:2125/2225 train_time:128733ms step_avg:60.58ms
step:2126/2225 train_time:128794ms step_avg:60.58ms
step:2127/2225 train_time:128855ms step_avg:60.58ms
step:2128/2225 train_time:128915ms step_avg:60.58ms
step:2129/2225 train_time:128977ms step_avg:60.58ms
step:2130/2225 train_time:129038ms step_avg:60.58ms
step:2131/2225 train_time:129101ms step_avg:60.58ms
step:2132/2225 train_time:129161ms step_avg:60.58ms
step:2133/2225 train_time:129225ms step_avg:60.58ms
step:2134/2225 train_time:129286ms step_avg:60.58ms
step:2135/2225 train_time:129348ms step_avg:60.58ms
step:2136/2225 train_time:129408ms step_avg:60.58ms
step:2137/2225 train_time:129470ms step_avg:60.59ms
step:2138/2225 train_time:129530ms step_avg:60.58ms
step:2139/2225 train_time:129591ms step_avg:60.59ms
step:2140/2225 train_time:129651ms step_avg:60.58ms
step:2141/2225 train_time:129712ms step_avg:60.58ms
step:2142/2225 train_time:129773ms step_avg:60.58ms
step:2143/2225 train_time:129834ms step_avg:60.59ms
step:2144/2225 train_time:129894ms step_avg:60.58ms
step:2145/2225 train_time:129956ms step_avg:60.59ms
step:2146/2225 train_time:130015ms step_avg:60.58ms
step:2147/2225 train_time:130078ms step_avg:60.59ms
step:2148/2225 train_time:130138ms step_avg:60.59ms
step:2149/2225 train_time:130201ms step_avg:60.59ms
step:2150/2225 train_time:130262ms step_avg:60.59ms
step:2151/2225 train_time:130324ms step_avg:60.59ms
step:2152/2225 train_time:130385ms step_avg:60.59ms
step:2153/2225 train_time:130447ms step_avg:60.59ms
step:2154/2225 train_time:130507ms step_avg:60.59ms
step:2155/2225 train_time:130568ms step_avg:60.59ms
step:2156/2225 train_time:130628ms step_avg:60.59ms
step:2157/2225 train_time:130691ms step_avg:60.59ms
step:2158/2225 train_time:130751ms step_avg:60.59ms
step:2159/2225 train_time:130812ms step_avg:60.59ms
step:2160/2225 train_time:130872ms step_avg:60.59ms
step:2161/2225 train_time:130934ms step_avg:60.59ms
step:2162/2225 train_time:130995ms step_avg:60.59ms
step:2163/2225 train_time:131056ms step_avg:60.59ms
step:2164/2225 train_time:131116ms step_avg:60.59ms
step:2165/2225 train_time:131178ms step_avg:60.59ms
step:2166/2225 train_time:131238ms step_avg:60.59ms
step:2167/2225 train_time:131302ms step_avg:60.59ms
step:2168/2225 train_time:131362ms step_avg:60.59ms
step:2169/2225 train_time:131425ms step_avg:60.59ms
step:2170/2225 train_time:131486ms step_avg:60.59ms
step:2171/2225 train_time:131547ms step_avg:60.59ms
step:2172/2225 train_time:131608ms step_avg:60.59ms
step:2173/2225 train_time:131669ms step_avg:60.59ms
step:2174/2225 train_time:131729ms step_avg:60.59ms
step:2175/2225 train_time:131791ms step_avg:60.59ms
step:2176/2225 train_time:131851ms step_avg:60.59ms
step:2177/2225 train_time:131913ms step_avg:60.59ms
step:2178/2225 train_time:131973ms step_avg:60.59ms
step:2179/2225 train_time:132035ms step_avg:60.59ms
step:2180/2225 train_time:132095ms step_avg:60.59ms
step:2181/2225 train_time:132157ms step_avg:60.59ms
step:2182/2225 train_time:132216ms step_avg:60.59ms
step:2183/2225 train_time:132279ms step_avg:60.59ms
step:2184/2225 train_time:132340ms step_avg:60.60ms
step:2185/2225 train_time:132402ms step_avg:60.60ms
step:2186/2225 train_time:132463ms step_avg:60.60ms
step:2187/2225 train_time:132525ms step_avg:60.60ms
step:2188/2225 train_time:132585ms step_avg:60.60ms
step:2189/2225 train_time:132648ms step_avg:60.60ms
step:2190/2225 train_time:132708ms step_avg:60.60ms
step:2191/2225 train_time:132770ms step_avg:60.60ms
step:2192/2225 train_time:132830ms step_avg:60.60ms
step:2193/2225 train_time:132892ms step_avg:60.60ms
step:2194/2225 train_time:132952ms step_avg:60.60ms
step:2195/2225 train_time:133014ms step_avg:60.60ms
step:2196/2225 train_time:133074ms step_avg:60.60ms
step:2197/2225 train_time:133136ms step_avg:60.60ms
step:2198/2225 train_time:133196ms step_avg:60.60ms
step:2199/2225 train_time:133258ms step_avg:60.60ms
step:2200/2225 train_time:133319ms step_avg:60.60ms
step:2201/2225 train_time:133382ms step_avg:60.60ms
step:2202/2225 train_time:133443ms step_avg:60.60ms
step:2203/2225 train_time:133505ms step_avg:60.60ms
step:2204/2225 train_time:133566ms step_avg:60.60ms
step:2205/2225 train_time:133628ms step_avg:60.60ms
step:2206/2225 train_time:133690ms step_avg:60.60ms
step:2207/2225 train_time:133751ms step_avg:60.60ms
step:2208/2225 train_time:133812ms step_avg:60.60ms
step:2209/2225 train_time:133874ms step_avg:60.60ms
step:2210/2225 train_time:133935ms step_avg:60.60ms
step:2211/2225 train_time:133996ms step_avg:60.60ms
step:2212/2225 train_time:134057ms step_avg:60.60ms
step:2213/2225 train_time:134119ms step_avg:60.60ms
step:2214/2225 train_time:134179ms step_avg:60.60ms
step:2215/2225 train_time:134241ms step_avg:60.61ms
step:2216/2225 train_time:134301ms step_avg:60.61ms
step:2217/2225 train_time:134364ms step_avg:60.61ms
step:2218/2225 train_time:134424ms step_avg:60.61ms
step:2219/2225 train_time:134486ms step_avg:60.61ms
step:2220/2225 train_time:134546ms step_avg:60.61ms
step:2221/2225 train_time:134608ms step_avg:60.61ms
step:2222/2225 train_time:134668ms step_avg:60.61ms
step:2223/2225 train_time:134729ms step_avg:60.61ms
step:2224/2225 train_time:134790ms step_avg:60.61ms
step:2225/2225 train_time:134852ms step_avg:60.61ms
step:2225/2225 val_loss:3.2804 train_time:134912ms step_avg:60.63ms
peak memory allocated: 29249 MiB reserved: 47336 MiB

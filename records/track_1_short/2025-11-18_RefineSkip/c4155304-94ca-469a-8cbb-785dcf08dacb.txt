import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i ==7:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections[0]
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i ==4:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2185  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 18 21:40:04 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   37C    P0            125W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   38C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   39C    P0            125W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   37C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   33C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          154679      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          154680      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          154681      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          154682      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          154683      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          154684      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          154685      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          154686      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          154680      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          154681      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          154682      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          154683      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          154684      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          154685      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          154686      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2225 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2225 train_time:134ms step_avg:133.88ms
step:2/2225 train_time:186ms step_avg:92.82ms
step:3/2225 train_time:215ms step_avg:71.56ms
step:4/2225 train_time:250ms step_avg:62.38ms
step:5/2225 train_time:308ms step_avg:61.58ms
step:6/2225 train_time:387ms step_avg:64.45ms
step:7/2225 train_time:426ms step_avg:60.83ms
step:8/2225 train_time:484ms step_avg:60.52ms
step:9/2225 train_time:544ms step_avg:60.43ms
step:10/2225 train_time:603ms step_avg:60.30ms
step:11/2225 train_time:663ms step_avg:60.26ms
step:12/2225 train_time:722ms step_avg:60.13ms
step:13/2225 train_time:782ms step_avg:60.16ms
step:14/2225 train_time:841ms step_avg:60.08ms
step:15/2225 train_time:901ms step_avg:60.09ms
step:16/2225 train_time:960ms step_avg:60.00ms
step:17/2225 train_time:1021ms step_avg:60.05ms
step:18/2225 train_time:1083ms step_avg:60.19ms
step:19/2225 train_time:1149ms step_avg:60.46ms
step:20/2225 train_time:1210ms step_avg:60.48ms
step:21/2225 train_time:1271ms step_avg:60.52ms
step:22/2225 train_time:1331ms step_avg:60.48ms
step:23/2225 train_time:1392ms step_avg:60.53ms
step:24/2225 train_time:1452ms step_avg:60.50ms
step:25/2225 train_time:1513ms step_avg:60.51ms
step:26/2225 train_time:1572ms step_avg:60.47ms
step:27/2225 train_time:1633ms step_avg:60.48ms
step:28/2225 train_time:1693ms step_avg:60.45ms
step:29/2225 train_time:1753ms step_avg:60.46ms
step:30/2225 train_time:1813ms step_avg:60.43ms
step:31/2225 train_time:1874ms step_avg:60.45ms
step:32/2225 train_time:1934ms step_avg:60.43ms
step:33/2225 train_time:1996ms step_avg:60.49ms
step:34/2225 train_time:2058ms step_avg:60.52ms
step:35/2225 train_time:2121ms step_avg:60.60ms
step:36/2225 train_time:2181ms step_avg:60.59ms
step:37/2225 train_time:2242ms step_avg:60.60ms
step:38/2225 train_time:2302ms step_avg:60.57ms
step:39/2225 train_time:2363ms step_avg:60.58ms
step:40/2225 train_time:2422ms step_avg:60.55ms
step:41/2225 train_time:2482ms step_avg:60.55ms
step:42/2225 train_time:2542ms step_avg:60.52ms
step:43/2225 train_time:2603ms step_avg:60.53ms
step:44/2225 train_time:2662ms step_avg:60.49ms
step:45/2225 train_time:2722ms step_avg:60.48ms
step:46/2225 train_time:2781ms step_avg:60.45ms
step:47/2225 train_time:2842ms step_avg:60.46ms
step:48/2225 train_time:2900ms step_avg:60.42ms
step:49/2225 train_time:2962ms step_avg:60.44ms
step:50/2225 train_time:3021ms step_avg:60.43ms
step:51/2225 train_time:3083ms step_avg:60.45ms
step:52/2225 train_time:3142ms step_avg:60.42ms
step:53/2225 train_time:3203ms step_avg:60.44ms
step:54/2225 train_time:3263ms step_avg:60.43ms
step:55/2225 train_time:3324ms step_avg:60.43ms
step:56/2225 train_time:3383ms step_avg:60.41ms
step:57/2225 train_time:3443ms step_avg:60.41ms
step:58/2225 train_time:3503ms step_avg:60.39ms
step:59/2225 train_time:3563ms step_avg:60.39ms
step:60/2225 train_time:3622ms step_avg:60.37ms
step:61/2225 train_time:3683ms step_avg:60.37ms
step:62/2225 train_time:3742ms step_avg:60.35ms
step:63/2225 train_time:3803ms step_avg:60.36ms
step:64/2225 train_time:3862ms step_avg:60.34ms
step:65/2225 train_time:3922ms step_avg:60.34ms
step:66/2225 train_time:3981ms step_avg:60.32ms
step:67/2225 train_time:4042ms step_avg:60.33ms
step:68/2225 train_time:4102ms step_avg:60.32ms
step:69/2225 train_time:4164ms step_avg:60.35ms
step:70/2225 train_time:4223ms step_avg:60.33ms
step:71/2225 train_time:4284ms step_avg:60.33ms
step:72/2225 train_time:4343ms step_avg:60.32ms
step:73/2225 train_time:4404ms step_avg:60.32ms
step:74/2225 train_time:4463ms step_avg:60.32ms
step:75/2225 train_time:4523ms step_avg:60.31ms
step:76/2225 train_time:4582ms step_avg:60.29ms
step:77/2225 train_time:4643ms step_avg:60.30ms
step:78/2225 train_time:4703ms step_avg:60.29ms
step:79/2225 train_time:4763ms step_avg:60.29ms
step:80/2225 train_time:4822ms step_avg:60.27ms
step:81/2225 train_time:4882ms step_avg:60.27ms
step:82/2225 train_time:4941ms step_avg:60.26ms
step:83/2225 train_time:5002ms step_avg:60.27ms
step:84/2225 train_time:5061ms step_avg:60.25ms
step:85/2225 train_time:5122ms step_avg:60.26ms
step:86/2225 train_time:5181ms step_avg:60.25ms
step:87/2225 train_time:5242ms step_avg:60.25ms
step:88/2225 train_time:5302ms step_avg:60.24ms
step:89/2225 train_time:5363ms step_avg:60.25ms
step:90/2225 train_time:5422ms step_avg:60.24ms
step:91/2225 train_time:5482ms step_avg:60.24ms
step:92/2225 train_time:5541ms step_avg:60.23ms
step:93/2225 train_time:5601ms step_avg:60.23ms
step:94/2225 train_time:5661ms step_avg:60.23ms
step:95/2225 train_time:5722ms step_avg:60.23ms
step:96/2225 train_time:5781ms step_avg:60.22ms
step:97/2225 train_time:5842ms step_avg:60.22ms
step:98/2225 train_time:5901ms step_avg:60.21ms
step:99/2225 train_time:5962ms step_avg:60.22ms
step:100/2225 train_time:6020ms step_avg:60.20ms
step:101/2225 train_time:6081ms step_avg:60.21ms
step:102/2225 train_time:6141ms step_avg:60.20ms
step:103/2225 train_time:6201ms step_avg:60.20ms
step:104/2225 train_time:6260ms step_avg:60.20ms
step:105/2225 train_time:6321ms step_avg:60.20ms
step:106/2225 train_time:6380ms step_avg:60.19ms
step:107/2225 train_time:6440ms step_avg:60.19ms
step:108/2225 train_time:6499ms step_avg:60.18ms
step:109/2225 train_time:6560ms step_avg:60.18ms
step:110/2225 train_time:6619ms step_avg:60.17ms
step:111/2225 train_time:6679ms step_avg:60.17ms
step:112/2225 train_time:6738ms step_avg:60.16ms
step:113/2225 train_time:6799ms step_avg:60.17ms
step:114/2225 train_time:6858ms step_avg:60.16ms
step:115/2225 train_time:6919ms step_avg:60.17ms
step:116/2225 train_time:6978ms step_avg:60.16ms
step:117/2225 train_time:7039ms step_avg:60.16ms
step:118/2225 train_time:7099ms step_avg:60.16ms
step:119/2225 train_time:7159ms step_avg:60.16ms
step:120/2225 train_time:7218ms step_avg:60.15ms
step:121/2225 train_time:7279ms step_avg:60.16ms
step:122/2225 train_time:7338ms step_avg:60.15ms
step:123/2225 train_time:7399ms step_avg:60.16ms
step:124/2225 train_time:7458ms step_avg:60.15ms
step:125/2225 train_time:7519ms step_avg:60.15ms
step:126/2225 train_time:7578ms step_avg:60.14ms
step:127/2225 train_time:7638ms step_avg:60.14ms
step:128/2225 train_time:7698ms step_avg:60.14ms
step:129/2225 train_time:7758ms step_avg:60.14ms
step:130/2225 train_time:7818ms step_avg:60.14ms
step:131/2225 train_time:7878ms step_avg:60.14ms
step:132/2225 train_time:7937ms step_avg:60.13ms
step:133/2225 train_time:7998ms step_avg:60.13ms
step:134/2225 train_time:8057ms step_avg:60.13ms
step:135/2225 train_time:8119ms step_avg:60.14ms
step:136/2225 train_time:8178ms step_avg:60.13ms
step:137/2225 train_time:8238ms step_avg:60.13ms
step:138/2225 train_time:8298ms step_avg:60.13ms
step:139/2225 train_time:8359ms step_avg:60.13ms
step:140/2225 train_time:8418ms step_avg:60.13ms
step:141/2225 train_time:8478ms step_avg:60.13ms
step:142/2225 train_time:8537ms step_avg:60.12ms
step:143/2225 train_time:8598ms step_avg:60.13ms
step:144/2225 train_time:8657ms step_avg:60.12ms
step:145/2225 train_time:8717ms step_avg:60.12ms
step:146/2225 train_time:8777ms step_avg:60.12ms
step:147/2225 train_time:8838ms step_avg:60.12ms
step:148/2225 train_time:8898ms step_avg:60.12ms
step:149/2225 train_time:8958ms step_avg:60.12ms
step:150/2225 train_time:9017ms step_avg:60.12ms
step:151/2225 train_time:9077ms step_avg:60.11ms
step:152/2225 train_time:9137ms step_avg:60.11ms
step:153/2225 train_time:9198ms step_avg:60.12ms
step:154/2225 train_time:9258ms step_avg:60.11ms
step:155/2225 train_time:9318ms step_avg:60.12ms
step:156/2225 train_time:9378ms step_avg:60.11ms
step:157/2225 train_time:9438ms step_avg:60.12ms
step:158/2225 train_time:9498ms step_avg:60.11ms
step:159/2225 train_time:9558ms step_avg:60.11ms
step:160/2225 train_time:9617ms step_avg:60.11ms
step:161/2225 train_time:9678ms step_avg:60.11ms
step:162/2225 train_time:9737ms step_avg:60.10ms
step:163/2225 train_time:9799ms step_avg:60.11ms
step:164/2225 train_time:9858ms step_avg:60.11ms
step:165/2225 train_time:9919ms step_avg:60.11ms
step:166/2225 train_time:9978ms step_avg:60.11ms
step:167/2225 train_time:10039ms step_avg:60.11ms
step:168/2225 train_time:10099ms step_avg:60.11ms
step:169/2225 train_time:10159ms step_avg:60.11ms
step:170/2225 train_time:10218ms step_avg:60.11ms
step:171/2225 train_time:10279ms step_avg:60.11ms
step:172/2225 train_time:10337ms step_avg:60.10ms
step:173/2225 train_time:10398ms step_avg:60.10ms
step:174/2225 train_time:10458ms step_avg:60.10ms
step:175/2225 train_time:10518ms step_avg:60.10ms
step:176/2225 train_time:10577ms step_avg:60.09ms
step:177/2225 train_time:10636ms step_avg:60.09ms
step:178/2225 train_time:10695ms step_avg:60.09ms
step:179/2225 train_time:10756ms step_avg:60.09ms
step:180/2225 train_time:10816ms step_avg:60.09ms
step:181/2225 train_time:10877ms step_avg:60.09ms
step:182/2225 train_time:10936ms step_avg:60.09ms
step:183/2225 train_time:10998ms step_avg:60.10ms
step:184/2225 train_time:11057ms step_avg:60.09ms
step:185/2225 train_time:11118ms step_avg:60.10ms
step:186/2225 train_time:11177ms step_avg:60.09ms
step:187/2225 train_time:11237ms step_avg:60.09ms
step:188/2225 train_time:11297ms step_avg:60.09ms
step:189/2225 train_time:11358ms step_avg:60.10ms
step:190/2225 train_time:11417ms step_avg:60.09ms
step:191/2225 train_time:11478ms step_avg:60.09ms
step:192/2225 train_time:11537ms step_avg:60.09ms
step:193/2225 train_time:11598ms step_avg:60.09ms
step:194/2225 train_time:11656ms step_avg:60.08ms
step:195/2225 train_time:11717ms step_avg:60.09ms
step:196/2225 train_time:11776ms step_avg:60.08ms
step:197/2225 train_time:11836ms step_avg:60.08ms
step:198/2225 train_time:11896ms step_avg:60.08ms
step:199/2225 train_time:11957ms step_avg:60.09ms
step:200/2225 train_time:12017ms step_avg:60.09ms
step:201/2225 train_time:12078ms step_avg:60.09ms
step:202/2225 train_time:12137ms step_avg:60.09ms
step:203/2225 train_time:12198ms step_avg:60.09ms
step:204/2225 train_time:12257ms step_avg:60.08ms
step:205/2225 train_time:12318ms step_avg:60.09ms
step:206/2225 train_time:12377ms step_avg:60.08ms
step:207/2225 train_time:12438ms step_avg:60.09ms
step:208/2225 train_time:12497ms step_avg:60.08ms
step:209/2225 train_time:12558ms step_avg:60.09ms
step:210/2225 train_time:12617ms step_avg:60.08ms
step:211/2225 train_time:12677ms step_avg:60.08ms
step:212/2225 train_time:12736ms step_avg:60.08ms
step:213/2225 train_time:12797ms step_avg:60.08ms
step:214/2225 train_time:12857ms step_avg:60.08ms
step:215/2225 train_time:12918ms step_avg:60.08ms
step:216/2225 train_time:12977ms step_avg:60.08ms
step:217/2225 train_time:13038ms step_avg:60.08ms
step:218/2225 train_time:13097ms step_avg:60.08ms
step:219/2225 train_time:13158ms step_avg:60.08ms
step:220/2225 train_time:13218ms step_avg:60.08ms
step:221/2225 train_time:13278ms step_avg:60.08ms
step:222/2225 train_time:13337ms step_avg:60.08ms
step:223/2225 train_time:13398ms step_avg:60.08ms
step:224/2225 train_time:13457ms step_avg:60.08ms
step:225/2225 train_time:13518ms step_avg:60.08ms
step:226/2225 train_time:13577ms step_avg:60.07ms
step:227/2225 train_time:13637ms step_avg:60.07ms
step:228/2225 train_time:13697ms step_avg:60.07ms
step:229/2225 train_time:13757ms step_avg:60.08ms
step:230/2225 train_time:13817ms step_avg:60.07ms
step:231/2225 train_time:13878ms step_avg:60.08ms
step:232/2225 train_time:13937ms step_avg:60.07ms
step:233/2225 train_time:13998ms step_avg:60.08ms
step:234/2225 train_time:14057ms step_avg:60.07ms
step:235/2225 train_time:14118ms step_avg:60.08ms
step:236/2225 train_time:14177ms step_avg:60.07ms
step:237/2225 train_time:14238ms step_avg:60.07ms
step:238/2225 train_time:14297ms step_avg:60.07ms
step:239/2225 train_time:14358ms step_avg:60.07ms
step:240/2225 train_time:14417ms step_avg:60.07ms
step:241/2225 train_time:14478ms step_avg:60.08ms
step:242/2225 train_time:14537ms step_avg:60.07ms
step:243/2225 train_time:14597ms step_avg:60.07ms
step:244/2225 train_time:14657ms step_avg:60.07ms
step:245/2225 train_time:14717ms step_avg:60.07ms
step:246/2225 train_time:14776ms step_avg:60.07ms
step:247/2225 train_time:14837ms step_avg:60.07ms
step:248/2225 train_time:14897ms step_avg:60.07ms
step:249/2225 train_time:14959ms step_avg:60.07ms
step:250/2225 train_time:15018ms step_avg:60.07ms
step:250/2225 val_loss:4.0822 train_time:15078ms step_avg:60.31ms
step:251/2225 train_time:15102ms step_avg:60.17ms
step:252/2225 train_time:15141ms step_avg:60.08ms
step:253/2225 train_time:15205ms step_avg:60.10ms
step:254/2225 train_time:15271ms step_avg:60.12ms
step:255/2225 train_time:15335ms step_avg:60.14ms
step:256/2225 train_time:15393ms step_avg:60.13ms
step:257/2225 train_time:15453ms step_avg:60.13ms
step:258/2225 train_time:15512ms step_avg:60.12ms
step:259/2225 train_time:15572ms step_avg:60.12ms
step:260/2225 train_time:15630ms step_avg:60.11ms
step:261/2225 train_time:15689ms step_avg:60.11ms
step:262/2225 train_time:15747ms step_avg:60.10ms
step:263/2225 train_time:15806ms step_avg:60.10ms
step:264/2225 train_time:15864ms step_avg:60.09ms
step:265/2225 train_time:15924ms step_avg:60.09ms
step:266/2225 train_time:15982ms step_avg:60.08ms
step:267/2225 train_time:16043ms step_avg:60.09ms
step:268/2225 train_time:16104ms step_avg:60.09ms
step:269/2225 train_time:16167ms step_avg:60.10ms
step:270/2225 train_time:16230ms step_avg:60.11ms
step:271/2225 train_time:16293ms step_avg:60.12ms
step:272/2225 train_time:16353ms step_avg:60.12ms
step:273/2225 train_time:16413ms step_avg:60.12ms
step:274/2225 train_time:16472ms step_avg:60.12ms
step:275/2225 train_time:16532ms step_avg:60.12ms
step:276/2225 train_time:16591ms step_avg:60.11ms
step:277/2225 train_time:16651ms step_avg:60.11ms
step:278/2225 train_time:16709ms step_avg:60.10ms
step:279/2225 train_time:16768ms step_avg:60.10ms
step:280/2225 train_time:16826ms step_avg:60.09ms
step:281/2225 train_time:16886ms step_avg:60.09ms
step:282/2225 train_time:16945ms step_avg:60.09ms
step:283/2225 train_time:17005ms step_avg:60.09ms
step:284/2225 train_time:17065ms step_avg:60.09ms
step:285/2225 train_time:17126ms step_avg:60.09ms
step:286/2225 train_time:17186ms step_avg:60.09ms
step:287/2225 train_time:17248ms step_avg:60.10ms
step:288/2225 train_time:17308ms step_avg:60.10ms
step:289/2225 train_time:17370ms step_avg:60.11ms
step:290/2225 train_time:17431ms step_avg:60.11ms
step:291/2225 train_time:17491ms step_avg:60.11ms
step:292/2225 train_time:17550ms step_avg:60.10ms
step:293/2225 train_time:17611ms step_avg:60.10ms
step:294/2225 train_time:17669ms step_avg:60.10ms
step:295/2225 train_time:17729ms step_avg:60.10ms
step:296/2225 train_time:17788ms step_avg:60.09ms
step:297/2225 train_time:17848ms step_avg:60.09ms
step:298/2225 train_time:17906ms step_avg:60.09ms
step:299/2225 train_time:17966ms step_avg:60.09ms
step:300/2225 train_time:18025ms step_avg:60.08ms
step:301/2225 train_time:18086ms step_avg:60.09ms
step:302/2225 train_time:18146ms step_avg:60.08ms
step:303/2225 train_time:18207ms step_avg:60.09ms
step:304/2225 train_time:18267ms step_avg:60.09ms
step:305/2225 train_time:18330ms step_avg:60.10ms
step:306/2225 train_time:18390ms step_avg:60.10ms
step:307/2225 train_time:18450ms step_avg:60.10ms
step:308/2225 train_time:18509ms step_avg:60.10ms
step:309/2225 train_time:18570ms step_avg:60.10ms
step:310/2225 train_time:18629ms step_avg:60.09ms
step:311/2225 train_time:18689ms step_avg:60.09ms
step:312/2225 train_time:18747ms step_avg:60.09ms
step:313/2225 train_time:18807ms step_avg:60.09ms
step:314/2225 train_time:18866ms step_avg:60.08ms
step:315/2225 train_time:18926ms step_avg:60.08ms
step:316/2225 train_time:18985ms step_avg:60.08ms
step:317/2225 train_time:19046ms step_avg:60.08ms
step:318/2225 train_time:19105ms step_avg:60.08ms
step:319/2225 train_time:19165ms step_avg:60.08ms
step:320/2225 train_time:19225ms step_avg:60.08ms
step:321/2225 train_time:19287ms step_avg:60.08ms
step:322/2225 train_time:19347ms step_avg:60.08ms
step:323/2225 train_time:19408ms step_avg:60.09ms
step:324/2225 train_time:19467ms step_avg:60.08ms
step:325/2225 train_time:19529ms step_avg:60.09ms
step:326/2225 train_time:19588ms step_avg:60.09ms
step:327/2225 train_time:19648ms step_avg:60.09ms
step:328/2225 train_time:19707ms step_avg:60.08ms
step:329/2225 train_time:19767ms step_avg:60.08ms
step:330/2225 train_time:19826ms step_avg:60.08ms
step:331/2225 train_time:19886ms step_avg:60.08ms
step:332/2225 train_time:19945ms step_avg:60.07ms
step:333/2225 train_time:20006ms step_avg:60.08ms
step:334/2225 train_time:20065ms step_avg:60.07ms
step:335/2225 train_time:20124ms step_avg:60.07ms
step:336/2225 train_time:20184ms step_avg:60.07ms
step:337/2225 train_time:20245ms step_avg:60.07ms
step:338/2225 train_time:20304ms step_avg:60.07ms
step:339/2225 train_time:20365ms step_avg:60.07ms
step:340/2225 train_time:20425ms step_avg:60.07ms
step:341/2225 train_time:20486ms step_avg:60.08ms
step:342/2225 train_time:20545ms step_avg:60.07ms
step:343/2225 train_time:20606ms step_avg:60.08ms
step:344/2225 train_time:20665ms step_avg:60.07ms
step:345/2225 train_time:20726ms step_avg:60.08ms
step:346/2225 train_time:20786ms step_avg:60.07ms
step:347/2225 train_time:20846ms step_avg:60.08ms
step:348/2225 train_time:20905ms step_avg:60.07ms
step:349/2225 train_time:20965ms step_avg:60.07ms
step:350/2225 train_time:21024ms step_avg:60.07ms
step:351/2225 train_time:21084ms step_avg:60.07ms
step:352/2225 train_time:21143ms step_avg:60.07ms
step:353/2225 train_time:21204ms step_avg:60.07ms
step:354/2225 train_time:21264ms step_avg:60.07ms
step:355/2225 train_time:21325ms step_avg:60.07ms
step:356/2225 train_time:21385ms step_avg:60.07ms
step:357/2225 train_time:21446ms step_avg:60.07ms
step:358/2225 train_time:21505ms step_avg:60.07ms
step:359/2225 train_time:21566ms step_avg:60.07ms
step:360/2225 train_time:21626ms step_avg:60.07ms
step:361/2225 train_time:21687ms step_avg:60.07ms
step:362/2225 train_time:21747ms step_avg:60.07ms
step:363/2225 train_time:21807ms step_avg:60.08ms
step:364/2225 train_time:21866ms step_avg:60.07ms
step:365/2225 train_time:21926ms step_avg:60.07ms
step:366/2225 train_time:21985ms step_avg:60.07ms
step:367/2225 train_time:22046ms step_avg:60.07ms
step:368/2225 train_time:22105ms step_avg:60.07ms
step:369/2225 train_time:22165ms step_avg:60.07ms
step:370/2225 train_time:22225ms step_avg:60.07ms
step:371/2225 train_time:22286ms step_avg:60.07ms
step:372/2225 train_time:22345ms step_avg:60.07ms
step:373/2225 train_time:22406ms step_avg:60.07ms
step:374/2225 train_time:22465ms step_avg:60.07ms
step:375/2225 train_time:22526ms step_avg:60.07ms
step:376/2225 train_time:22585ms step_avg:60.07ms
step:377/2225 train_time:22646ms step_avg:60.07ms
step:378/2225 train_time:22705ms step_avg:60.07ms
step:379/2225 train_time:22766ms step_avg:60.07ms
step:380/2225 train_time:22826ms step_avg:60.07ms
step:381/2225 train_time:22886ms step_avg:60.07ms
step:382/2225 train_time:22945ms step_avg:60.06ms
step:383/2225 train_time:23005ms step_avg:60.06ms
step:384/2225 train_time:23064ms step_avg:60.06ms
step:385/2225 train_time:23124ms step_avg:60.06ms
step:386/2225 train_time:23183ms step_avg:60.06ms
step:387/2225 train_time:23244ms step_avg:60.06ms
step:388/2225 train_time:23303ms step_avg:60.06ms
step:389/2225 train_time:23365ms step_avg:60.06ms
step:390/2225 train_time:23424ms step_avg:60.06ms
step:391/2225 train_time:23486ms step_avg:60.07ms
step:392/2225 train_time:23545ms step_avg:60.06ms
step:393/2225 train_time:23606ms step_avg:60.07ms
step:394/2225 train_time:23665ms step_avg:60.06ms
step:395/2225 train_time:23727ms step_avg:60.07ms
step:396/2225 train_time:23786ms step_avg:60.07ms
step:397/2225 train_time:23846ms step_avg:60.07ms
step:398/2225 train_time:23905ms step_avg:60.06ms
step:399/2225 train_time:23965ms step_avg:60.06ms
step:400/2225 train_time:24024ms step_avg:60.06ms
step:401/2225 train_time:24084ms step_avg:60.06ms
step:402/2225 train_time:24143ms step_avg:60.06ms
step:403/2225 train_time:24204ms step_avg:60.06ms
step:404/2225 train_time:24263ms step_avg:60.06ms
step:405/2225 train_time:24325ms step_avg:60.06ms
step:406/2225 train_time:24385ms step_avg:60.06ms
step:407/2225 train_time:24446ms step_avg:60.06ms
step:408/2225 train_time:24505ms step_avg:60.06ms
step:409/2225 train_time:24566ms step_avg:60.06ms
step:410/2225 train_time:24626ms step_avg:60.06ms
step:411/2225 train_time:24687ms step_avg:60.07ms
step:412/2225 train_time:24747ms step_avg:60.06ms
step:413/2225 train_time:24807ms step_avg:60.07ms
step:414/2225 train_time:24866ms step_avg:60.06ms
step:415/2225 train_time:24927ms step_avg:60.07ms
step:416/2225 train_time:24987ms step_avg:60.06ms
step:417/2225 train_time:25047ms step_avg:60.07ms
step:418/2225 train_time:25107ms step_avg:60.06ms
step:419/2225 train_time:25167ms step_avg:60.06ms
step:420/2225 train_time:25226ms step_avg:60.06ms
step:421/2225 train_time:25287ms step_avg:60.06ms
step:422/2225 train_time:25346ms step_avg:60.06ms
step:423/2225 train_time:25407ms step_avg:60.06ms
step:424/2225 train_time:25466ms step_avg:60.06ms
step:425/2225 train_time:25527ms step_avg:60.06ms
step:426/2225 train_time:25586ms step_avg:60.06ms
step:427/2225 train_time:25647ms step_avg:60.06ms
step:428/2225 train_time:25706ms step_avg:60.06ms
step:429/2225 train_time:25767ms step_avg:60.06ms
step:430/2225 train_time:25827ms step_avg:60.06ms
step:431/2225 train_time:25888ms step_avg:60.06ms
step:432/2225 train_time:25947ms step_avg:60.06ms
step:433/2225 train_time:26008ms step_avg:60.06ms
step:434/2225 train_time:26066ms step_avg:60.06ms
step:435/2225 train_time:26127ms step_avg:60.06ms
step:436/2225 train_time:26186ms step_avg:60.06ms
step:437/2225 train_time:26246ms step_avg:60.06ms
step:438/2225 train_time:26305ms step_avg:60.06ms
step:439/2225 train_time:26366ms step_avg:60.06ms
step:440/2225 train_time:26426ms step_avg:60.06ms
step:441/2225 train_time:26487ms step_avg:60.06ms
step:442/2225 train_time:26547ms step_avg:60.06ms
step:443/2225 train_time:26607ms step_avg:60.06ms
step:444/2225 train_time:26666ms step_avg:60.06ms
step:445/2225 train_time:26727ms step_avg:60.06ms
step:446/2225 train_time:26787ms step_avg:60.06ms
step:447/2225 train_time:26848ms step_avg:60.06ms
step:448/2225 train_time:26906ms step_avg:60.06ms
step:449/2225 train_time:26967ms step_avg:60.06ms
step:450/2225 train_time:27026ms step_avg:60.06ms
step:451/2225 train_time:27087ms step_avg:60.06ms
step:452/2225 train_time:27147ms step_avg:60.06ms
step:453/2225 train_time:27207ms step_avg:60.06ms
step:454/2225 train_time:27266ms step_avg:60.06ms
step:455/2225 train_time:27327ms step_avg:60.06ms
step:456/2225 train_time:27386ms step_avg:60.06ms
step:457/2225 train_time:27446ms step_avg:60.06ms
step:458/2225 train_time:27505ms step_avg:60.05ms
step:459/2225 train_time:27566ms step_avg:60.06ms
step:460/2225 train_time:27625ms step_avg:60.05ms
step:461/2225 train_time:27686ms step_avg:60.06ms
step:462/2225 train_time:27746ms step_avg:60.06ms
step:463/2225 train_time:27806ms step_avg:60.06ms
step:464/2225 train_time:27865ms step_avg:60.05ms
step:465/2225 train_time:27925ms step_avg:60.05ms
step:466/2225 train_time:27984ms step_avg:60.05ms
step:467/2225 train_time:28045ms step_avg:60.05ms
step:468/2225 train_time:28104ms step_avg:60.05ms
step:469/2225 train_time:28166ms step_avg:60.05ms
step:470/2225 train_time:28225ms step_avg:60.05ms
step:471/2225 train_time:28285ms step_avg:60.05ms
step:472/2225 train_time:28345ms step_avg:60.05ms
step:473/2225 train_time:28406ms step_avg:60.05ms
step:474/2225 train_time:28464ms step_avg:60.05ms
step:475/2225 train_time:28525ms step_avg:60.05ms
step:476/2225 train_time:28584ms step_avg:60.05ms
step:477/2225 train_time:28645ms step_avg:60.05ms
step:478/2225 train_time:28704ms step_avg:60.05ms
step:479/2225 train_time:28765ms step_avg:60.05ms
step:480/2225 train_time:28825ms step_avg:60.05ms
step:481/2225 train_time:28886ms step_avg:60.05ms
step:482/2225 train_time:28944ms step_avg:60.05ms
step:483/2225 train_time:29005ms step_avg:60.05ms
step:484/2225 train_time:29064ms step_avg:60.05ms
step:485/2225 train_time:29125ms step_avg:60.05ms
step:486/2225 train_time:29184ms step_avg:60.05ms
step:487/2225 train_time:29245ms step_avg:60.05ms
step:488/2225 train_time:29304ms step_avg:60.05ms
step:489/2225 train_time:29365ms step_avg:60.05ms
step:490/2225 train_time:29424ms step_avg:60.05ms
step:491/2225 train_time:29485ms step_avg:60.05ms
step:492/2225 train_time:29544ms step_avg:60.05ms
step:493/2225 train_time:29604ms step_avg:60.05ms
step:494/2225 train_time:29663ms step_avg:60.05ms
step:495/2225 train_time:29724ms step_avg:60.05ms
step:496/2225 train_time:29783ms step_avg:60.05ms
step:497/2225 train_time:29844ms step_avg:60.05ms
step:498/2225 train_time:29904ms step_avg:60.05ms
step:499/2225 train_time:29964ms step_avg:60.05ms
step:500/2225 train_time:30023ms step_avg:60.05ms
step:500/2225 val_loss:3.8186 train_time:30084ms step_avg:60.17ms
step:501/2225 train_time:30107ms step_avg:60.09ms
step:502/2225 train_time:30146ms step_avg:60.05ms
step:503/2225 train_time:30211ms step_avg:60.06ms
step:504/2225 train_time:30276ms step_avg:60.07ms
step:505/2225 train_time:30337ms step_avg:60.07ms
step:506/2225 train_time:30396ms step_avg:60.07ms
step:507/2225 train_time:30455ms step_avg:60.07ms
step:508/2225 train_time:30514ms step_avg:60.07ms
step:509/2225 train_time:30574ms step_avg:60.07ms
step:510/2225 train_time:30632ms step_avg:60.06ms
step:511/2225 train_time:30691ms step_avg:60.06ms
step:512/2225 train_time:30749ms step_avg:60.06ms
step:513/2225 train_time:30809ms step_avg:60.06ms
step:514/2225 train_time:30867ms step_avg:60.05ms
step:515/2225 train_time:30927ms step_avg:60.05ms
step:516/2225 train_time:30986ms step_avg:60.05ms
step:517/2225 train_time:31047ms step_avg:60.05ms
step:518/2225 train_time:31106ms step_avg:60.05ms
step:519/2225 train_time:31169ms step_avg:60.06ms
step:520/2225 train_time:31231ms step_avg:60.06ms
step:521/2225 train_time:31293ms step_avg:60.06ms
step:522/2225 train_time:31353ms step_avg:60.06ms
step:523/2225 train_time:31415ms step_avg:60.07ms
step:524/2225 train_time:31475ms step_avg:60.07ms
step:525/2225 train_time:31534ms step_avg:60.07ms
step:526/2225 train_time:31593ms step_avg:60.06ms
step:527/2225 train_time:31652ms step_avg:60.06ms
step:528/2225 train_time:31710ms step_avg:60.06ms
step:529/2225 train_time:31770ms step_avg:60.06ms
step:530/2225 train_time:31829ms step_avg:60.05ms
step:531/2225 train_time:31888ms step_avg:60.05ms
step:532/2225 train_time:31947ms step_avg:60.05ms
step:533/2225 train_time:32008ms step_avg:60.05ms
step:534/2225 train_time:32067ms step_avg:60.05ms
step:535/2225 train_time:32128ms step_avg:60.05ms
step:536/2225 train_time:32188ms step_avg:60.05ms
step:537/2225 train_time:32250ms step_avg:60.06ms
step:538/2225 train_time:32310ms step_avg:60.06ms
step:539/2225 train_time:32372ms step_avg:60.06ms
step:540/2225 train_time:32432ms step_avg:60.06ms
step:541/2225 train_time:32493ms step_avg:60.06ms
step:542/2225 train_time:32551ms step_avg:60.06ms
step:543/2225 train_time:32612ms step_avg:60.06ms
step:544/2225 train_time:32672ms step_avg:60.06ms
step:545/2225 train_time:32732ms step_avg:60.06ms
step:546/2225 train_time:32790ms step_avg:60.06ms
step:547/2225 train_time:32850ms step_avg:60.06ms
step:548/2225 train_time:32909ms step_avg:60.05ms
step:549/2225 train_time:32970ms step_avg:60.05ms
step:550/2225 train_time:33029ms step_avg:60.05ms
step:551/2225 train_time:33090ms step_avg:60.06ms
step:552/2225 train_time:33150ms step_avg:60.05ms
step:553/2225 train_time:33210ms step_avg:60.06ms
step:554/2225 train_time:33270ms step_avg:60.05ms
step:555/2225 train_time:33332ms step_avg:60.06ms
step:556/2225 train_time:33392ms step_avg:60.06ms
step:557/2225 train_time:33453ms step_avg:60.06ms
step:558/2225 train_time:33512ms step_avg:60.06ms
step:559/2225 train_time:33573ms step_avg:60.06ms
step:560/2225 train_time:33632ms step_avg:60.06ms
step:561/2225 train_time:33692ms step_avg:60.06ms
step:562/2225 train_time:33750ms step_avg:60.05ms
step:563/2225 train_time:33811ms step_avg:60.05ms
step:564/2225 train_time:33870ms step_avg:60.05ms
step:565/2225 train_time:33930ms step_avg:60.05ms
step:566/2225 train_time:33989ms step_avg:60.05ms
step:567/2225 train_time:34050ms step_avg:60.05ms
step:568/2225 train_time:34110ms step_avg:60.05ms
step:569/2225 train_time:34171ms step_avg:60.05ms
step:570/2225 train_time:34231ms step_avg:60.05ms
step:571/2225 train_time:34292ms step_avg:60.06ms
step:572/2225 train_time:34352ms step_avg:60.06ms
step:573/2225 train_time:34414ms step_avg:60.06ms
step:574/2225 train_time:34473ms step_avg:60.06ms
step:575/2225 train_time:34534ms step_avg:60.06ms
step:576/2225 train_time:34594ms step_avg:60.06ms
step:577/2225 train_time:34654ms step_avg:60.06ms
step:578/2225 train_time:34713ms step_avg:60.06ms
step:579/2225 train_time:34773ms step_avg:60.06ms
step:580/2225 train_time:34832ms step_avg:60.06ms
step:581/2225 train_time:34892ms step_avg:60.06ms
step:582/2225 train_time:34951ms step_avg:60.05ms
step:583/2225 train_time:35012ms step_avg:60.05ms
step:584/2225 train_time:35071ms step_avg:60.05ms
step:585/2225 train_time:35132ms step_avg:60.05ms
step:586/2225 train_time:35191ms step_avg:60.05ms
step:587/2225 train_time:35253ms step_avg:60.06ms
step:588/2225 train_time:35312ms step_avg:60.05ms
step:589/2225 train_time:35373ms step_avg:60.06ms
step:590/2225 train_time:35432ms step_avg:60.05ms
step:591/2225 train_time:35493ms step_avg:60.06ms
step:592/2225 train_time:35552ms step_avg:60.05ms
step:593/2225 train_time:35613ms step_avg:60.06ms
step:594/2225 train_time:35672ms step_avg:60.05ms
step:595/2225 train_time:35733ms step_avg:60.06ms
step:596/2225 train_time:35792ms step_avg:60.05ms
step:597/2225 train_time:35852ms step_avg:60.05ms
step:598/2225 train_time:35910ms step_avg:60.05ms
step:599/2225 train_time:35971ms step_avg:60.05ms
step:600/2225 train_time:36031ms step_avg:60.05ms
step:601/2225 train_time:36092ms step_avg:60.05ms
step:602/2225 train_time:36151ms step_avg:60.05ms
step:603/2225 train_time:36211ms step_avg:60.05ms
step:604/2225 train_time:36271ms step_avg:60.05ms
step:605/2225 train_time:36332ms step_avg:60.05ms
step:606/2225 train_time:36392ms step_avg:60.05ms
step:607/2225 train_time:36453ms step_avg:60.05ms
step:608/2225 train_time:36512ms step_avg:60.05ms
step:609/2225 train_time:36572ms step_avg:60.05ms
step:610/2225 train_time:36631ms step_avg:60.05ms
step:611/2225 train_time:36692ms step_avg:60.05ms
step:612/2225 train_time:36752ms step_avg:60.05ms
step:613/2225 train_time:36811ms step_avg:60.05ms
step:614/2225 train_time:36870ms step_avg:60.05ms
step:615/2225 train_time:36931ms step_avg:60.05ms
step:616/2225 train_time:36990ms step_avg:60.05ms
step:617/2225 train_time:37050ms step_avg:60.05ms
step:618/2225 train_time:37109ms step_avg:60.05ms
step:619/2225 train_time:37170ms step_avg:60.05ms
step:620/2225 train_time:37229ms step_avg:60.05ms
step:621/2225 train_time:37290ms step_avg:60.05ms
step:622/2225 train_time:37350ms step_avg:60.05ms
step:623/2225 train_time:37410ms step_avg:60.05ms
step:624/2225 train_time:37470ms step_avg:60.05ms
step:625/2225 train_time:37531ms step_avg:60.05ms
step:626/2225 train_time:37590ms step_avg:60.05ms
step:627/2225 train_time:37650ms step_avg:60.05ms
step:628/2225 train_time:37709ms step_avg:60.05ms
step:629/2225 train_time:37769ms step_avg:60.05ms
step:630/2225 train_time:37828ms step_avg:60.04ms
step:631/2225 train_time:37888ms step_avg:60.04ms
step:632/2225 train_time:37947ms step_avg:60.04ms
step:633/2225 train_time:38007ms step_avg:60.04ms
step:634/2225 train_time:38067ms step_avg:60.04ms
step:635/2225 train_time:38128ms step_avg:60.04ms
step:636/2225 train_time:38188ms step_avg:60.04ms
step:637/2225 train_time:38248ms step_avg:60.04ms
step:638/2225 train_time:38308ms step_avg:60.04ms
step:639/2225 train_time:38369ms step_avg:60.05ms
step:640/2225 train_time:38429ms step_avg:60.04ms
step:641/2225 train_time:38489ms step_avg:60.05ms
step:642/2225 train_time:38548ms step_avg:60.04ms
step:643/2225 train_time:38610ms step_avg:60.05ms
step:644/2225 train_time:38670ms step_avg:60.05ms
step:645/2225 train_time:38730ms step_avg:60.05ms
step:646/2225 train_time:38789ms step_avg:60.05ms
step:647/2225 train_time:38850ms step_avg:60.05ms
step:648/2225 train_time:38908ms step_avg:60.04ms
step:649/2225 train_time:38969ms step_avg:60.04ms
step:650/2225 train_time:39028ms step_avg:60.04ms
step:651/2225 train_time:39088ms step_avg:60.04ms
step:652/2225 train_time:39148ms step_avg:60.04ms
step:653/2225 train_time:39208ms step_avg:60.04ms
step:654/2225 train_time:39267ms step_avg:60.04ms
step:655/2225 train_time:39329ms step_avg:60.04ms
step:656/2225 train_time:39388ms step_avg:60.04ms
step:657/2225 train_time:39449ms step_avg:60.04ms
step:658/2225 train_time:39508ms step_avg:60.04ms
step:659/2225 train_time:39569ms step_avg:60.04ms
step:660/2225 train_time:39628ms step_avg:60.04ms
step:661/2225 train_time:39689ms step_avg:60.04ms
step:662/2225 train_time:39748ms step_avg:60.04ms
step:663/2225 train_time:39808ms step_avg:60.04ms
step:664/2225 train_time:39868ms step_avg:60.04ms
step:665/2225 train_time:39929ms step_avg:60.04ms
step:666/2225 train_time:39988ms step_avg:60.04ms
step:667/2225 train_time:40048ms step_avg:60.04ms
step:668/2225 train_time:40107ms step_avg:60.04ms
step:669/2225 train_time:40168ms step_avg:60.04ms
step:670/2225 train_time:40228ms step_avg:60.04ms
step:671/2225 train_time:40288ms step_avg:60.04ms
step:672/2225 train_time:40347ms step_avg:60.04ms
step:673/2225 train_time:40407ms step_avg:60.04ms
step:674/2225 train_time:40467ms step_avg:60.04ms
step:675/2225 train_time:40528ms step_avg:60.04ms
step:676/2225 train_time:40586ms step_avg:60.04ms
step:677/2225 train_time:40647ms step_avg:60.04ms
step:678/2225 train_time:40705ms step_avg:60.04ms
step:679/2225 train_time:40766ms step_avg:60.04ms
step:680/2225 train_time:40825ms step_avg:60.04ms
step:681/2225 train_time:40885ms step_avg:60.04ms
step:682/2225 train_time:40944ms step_avg:60.04ms
step:683/2225 train_time:41004ms step_avg:60.04ms
step:684/2225 train_time:41063ms step_avg:60.03ms
step:685/2225 train_time:41123ms step_avg:60.03ms
step:686/2225 train_time:41182ms step_avg:60.03ms
step:687/2225 train_time:41242ms step_avg:60.03ms
step:688/2225 train_time:41301ms step_avg:60.03ms
step:689/2225 train_time:41361ms step_avg:60.03ms
step:690/2225 train_time:41419ms step_avg:60.03ms
step:691/2225 train_time:41479ms step_avg:60.03ms
step:692/2225 train_time:41538ms step_avg:60.03ms
step:693/2225 train_time:41598ms step_avg:60.03ms
step:694/2225 train_time:41658ms step_avg:60.03ms
step:695/2225 train_time:41717ms step_avg:60.02ms
step:696/2225 train_time:41776ms step_avg:60.02ms
step:697/2225 train_time:41836ms step_avg:60.02ms
step:698/2225 train_time:41895ms step_avg:60.02ms
step:699/2225 train_time:41955ms step_avg:60.02ms
step:700/2225 train_time:42014ms step_avg:60.02ms
step:701/2225 train_time:42075ms step_avg:60.02ms
step:702/2225 train_time:42134ms step_avg:60.02ms
step:703/2225 train_time:42195ms step_avg:60.02ms
step:704/2225 train_time:42254ms step_avg:60.02ms
step:705/2225 train_time:42314ms step_avg:60.02ms
step:706/2225 train_time:42373ms step_avg:60.02ms
step:707/2225 train_time:42434ms step_avg:60.02ms
step:708/2225 train_time:42493ms step_avg:60.02ms
step:709/2225 train_time:42554ms step_avg:60.02ms
step:710/2225 train_time:42613ms step_avg:60.02ms
step:711/2225 train_time:42673ms step_avg:60.02ms
step:712/2225 train_time:42732ms step_avg:60.02ms
step:713/2225 train_time:42793ms step_avg:60.02ms
step:714/2225 train_time:42851ms step_avg:60.02ms
step:715/2225 train_time:42911ms step_avg:60.02ms
step:716/2225 train_time:42970ms step_avg:60.01ms
step:717/2225 train_time:43031ms step_avg:60.02ms
step:718/2225 train_time:43090ms step_avg:60.01ms
step:719/2225 train_time:43151ms step_avg:60.02ms
step:720/2225 train_time:43210ms step_avg:60.01ms
step:721/2225 train_time:43271ms step_avg:60.01ms
step:722/2225 train_time:43330ms step_avg:60.01ms
step:723/2225 train_time:43391ms step_avg:60.01ms
step:724/2225 train_time:43450ms step_avg:60.01ms
step:725/2225 train_time:43511ms step_avg:60.02ms
step:726/2225 train_time:43571ms step_avg:60.01ms
step:727/2225 train_time:43631ms step_avg:60.02ms
step:728/2225 train_time:43690ms step_avg:60.01ms
step:729/2225 train_time:43750ms step_avg:60.01ms
step:730/2225 train_time:43809ms step_avg:60.01ms
step:731/2225 train_time:43871ms step_avg:60.02ms
step:732/2225 train_time:43931ms step_avg:60.02ms
step:733/2225 train_time:43993ms step_avg:60.02ms
step:734/2225 train_time:44053ms step_avg:60.02ms
step:735/2225 train_time:44115ms step_avg:60.02ms
step:736/2225 train_time:44175ms step_avg:60.02ms
step:737/2225 train_time:44236ms step_avg:60.02ms
step:738/2225 train_time:44296ms step_avg:60.02ms
step:739/2225 train_time:44357ms step_avg:60.02ms
step:740/2225 train_time:44416ms step_avg:60.02ms
step:741/2225 train_time:44477ms step_avg:60.02ms
step:742/2225 train_time:44538ms step_avg:60.02ms
step:743/2225 train_time:44598ms step_avg:60.02ms
step:744/2225 train_time:44659ms step_avg:60.03ms
step:745/2225 train_time:44720ms step_avg:60.03ms
step:746/2225 train_time:44779ms step_avg:60.02ms
step:747/2225 train_time:44839ms step_avg:60.03ms
step:748/2225 train_time:44899ms step_avg:60.03ms
step:749/2225 train_time:44960ms step_avg:60.03ms
step:750/2225 train_time:45019ms step_avg:60.03ms
step:750/2225 val_loss:3.6681 train_time:45081ms step_avg:60.11ms
step:751/2225 train_time:45104ms step_avg:60.06ms
step:752/2225 train_time:45141ms step_avg:60.03ms
step:753/2225 train_time:45202ms step_avg:60.03ms
step:754/2225 train_time:45261ms step_avg:60.03ms
step:755/2225 train_time:45323ms step_avg:60.03ms
step:756/2225 train_time:45383ms step_avg:60.03ms
step:757/2225 train_time:45443ms step_avg:60.03ms
step:758/2225 train_time:45501ms step_avg:60.03ms
step:759/2225 train_time:45561ms step_avg:60.03ms
step:760/2225 train_time:45620ms step_avg:60.03ms
step:761/2225 train_time:45680ms step_avg:60.03ms
step:762/2225 train_time:45738ms step_avg:60.02ms
step:763/2225 train_time:45798ms step_avg:60.02ms
step:764/2225 train_time:45857ms step_avg:60.02ms
step:765/2225 train_time:45917ms step_avg:60.02ms
step:766/2225 train_time:45978ms step_avg:60.02ms
step:767/2225 train_time:46045ms step_avg:60.03ms
step:768/2225 train_time:46107ms step_avg:60.03ms
step:769/2225 train_time:46170ms step_avg:60.04ms
step:770/2225 train_time:46230ms step_avg:60.04ms
step:771/2225 train_time:46291ms step_avg:60.04ms
step:772/2225 train_time:46351ms step_avg:60.04ms
step:773/2225 train_time:46412ms step_avg:60.04ms
step:774/2225 train_time:46493ms step_avg:60.07ms
step:775/2225 train_time:46553ms step_avg:60.07ms
step:776/2225 train_time:46612ms step_avg:60.07ms
step:777/2225 train_time:46672ms step_avg:60.07ms
step:778/2225 train_time:46731ms step_avg:60.07ms
step:779/2225 train_time:46791ms step_avg:60.07ms
step:780/2225 train_time:46850ms step_avg:60.06ms
step:781/2225 train_time:46910ms step_avg:60.06ms
step:782/2225 train_time:46970ms step_avg:60.06ms
step:783/2225 train_time:47031ms step_avg:60.07ms
step:784/2225 train_time:47092ms step_avg:60.07ms
step:785/2225 train_time:47154ms step_avg:60.07ms
step:786/2225 train_time:47214ms step_avg:60.07ms
step:787/2225 train_time:47275ms step_avg:60.07ms
step:788/2225 train_time:47335ms step_avg:60.07ms
step:789/2225 train_time:47397ms step_avg:60.07ms
step:790/2225 train_time:47459ms step_avg:60.07ms
step:791/2225 train_time:47520ms step_avg:60.08ms
step:792/2225 train_time:47579ms step_avg:60.07ms
step:793/2225 train_time:47639ms step_avg:60.07ms
step:794/2225 train_time:47698ms step_avg:60.07ms
step:795/2225 train_time:47758ms step_avg:60.07ms
step:796/2225 train_time:47817ms step_avg:60.07ms
step:797/2225 train_time:47878ms step_avg:60.07ms
step:798/2225 train_time:47936ms step_avg:60.07ms
step:799/2225 train_time:47996ms step_avg:60.07ms
step:800/2225 train_time:48056ms step_avg:60.07ms
step:801/2225 train_time:48117ms step_avg:60.07ms
step:802/2225 train_time:48177ms step_avg:60.07ms
step:803/2225 train_time:48238ms step_avg:60.07ms
step:804/2225 train_time:48298ms step_avg:60.07ms
step:805/2225 train_time:48359ms step_avg:60.07ms
step:806/2225 train_time:48419ms step_avg:60.07ms
step:807/2225 train_time:48480ms step_avg:60.07ms
step:808/2225 train_time:48540ms step_avg:60.07ms
step:809/2225 train_time:48601ms step_avg:60.07ms
step:810/2225 train_time:48660ms step_avg:60.07ms
step:811/2225 train_time:48721ms step_avg:60.08ms
step:812/2225 train_time:48780ms step_avg:60.07ms
step:813/2225 train_time:48840ms step_avg:60.07ms
step:814/2225 train_time:48899ms step_avg:60.07ms
step:815/2225 train_time:48960ms step_avg:60.07ms
step:816/2225 train_time:49019ms step_avg:60.07ms
step:817/2225 train_time:49080ms step_avg:60.07ms
step:818/2225 train_time:49139ms step_avg:60.07ms
step:819/2225 train_time:49200ms step_avg:60.07ms
step:820/2225 train_time:49260ms step_avg:60.07ms
step:821/2225 train_time:49321ms step_avg:60.07ms
step:822/2225 train_time:49381ms step_avg:60.07ms
step:823/2225 train_time:49443ms step_avg:60.08ms
step:824/2225 train_time:49502ms step_avg:60.08ms
step:825/2225 train_time:49563ms step_avg:60.08ms
step:826/2225 train_time:49623ms step_avg:60.08ms
step:827/2225 train_time:49684ms step_avg:60.08ms
step:828/2225 train_time:49744ms step_avg:60.08ms
step:829/2225 train_time:49805ms step_avg:60.08ms
step:830/2225 train_time:49865ms step_avg:60.08ms
step:831/2225 train_time:49926ms step_avg:60.08ms
step:832/2225 train_time:49986ms step_avg:60.08ms
step:833/2225 train_time:50047ms step_avg:60.08ms
step:834/2225 train_time:50107ms step_avg:60.08ms
step:835/2225 train_time:50169ms step_avg:60.08ms
step:836/2225 train_time:50230ms step_avg:60.08ms
step:837/2225 train_time:50291ms step_avg:60.08ms
step:838/2225 train_time:50351ms step_avg:60.08ms
step:839/2225 train_time:50412ms step_avg:60.09ms
step:840/2225 train_time:50473ms step_avg:60.09ms
step:841/2225 train_time:50534ms step_avg:60.09ms
step:842/2225 train_time:50594ms step_avg:60.09ms
step:843/2225 train_time:50655ms step_avg:60.09ms
step:844/2225 train_time:50715ms step_avg:60.09ms
step:845/2225 train_time:50776ms step_avg:60.09ms
step:846/2225 train_time:50836ms step_avg:60.09ms
step:847/2225 train_time:50896ms step_avg:60.09ms
step:848/2225 train_time:50956ms step_avg:60.09ms
step:849/2225 train_time:51017ms step_avg:60.09ms
step:850/2225 train_time:51076ms step_avg:60.09ms
step:851/2225 train_time:51137ms step_avg:60.09ms
step:852/2225 train_time:51196ms step_avg:60.09ms
step:853/2225 train_time:51257ms step_avg:60.09ms
step:854/2225 train_time:51316ms step_avg:60.09ms
step:855/2225 train_time:51377ms step_avg:60.09ms
step:856/2225 train_time:51436ms step_avg:60.09ms
step:857/2225 train_time:51497ms step_avg:60.09ms
step:858/2225 train_time:51557ms step_avg:60.09ms
step:859/2225 train_time:51618ms step_avg:60.09ms
step:860/2225 train_time:51678ms step_avg:60.09ms
step:861/2225 train_time:51738ms step_avg:60.09ms
step:862/2225 train_time:51797ms step_avg:60.09ms
step:863/2225 train_time:51858ms step_avg:60.09ms
step:864/2225 train_time:51918ms step_avg:60.09ms
step:865/2225 train_time:51979ms step_avg:60.09ms
step:866/2225 train_time:52038ms step_avg:60.09ms
step:867/2225 train_time:52099ms step_avg:60.09ms
step:868/2225 train_time:52159ms step_avg:60.09ms
step:869/2225 train_time:52220ms step_avg:60.09ms
step:870/2225 train_time:52280ms step_avg:60.09ms
step:871/2225 train_time:52341ms step_avg:60.09ms
step:872/2225 train_time:52400ms step_avg:60.09ms
step:873/2225 train_time:52461ms step_avg:60.09ms
step:874/2225 train_time:52520ms step_avg:60.09ms
step:875/2225 train_time:52582ms step_avg:60.09ms
step:876/2225 train_time:52641ms step_avg:60.09ms
step:877/2225 train_time:52702ms step_avg:60.09ms
step:878/2225 train_time:52761ms step_avg:60.09ms
step:879/2225 train_time:52822ms step_avg:60.09ms
step:880/2225 train_time:52882ms step_avg:60.09ms
step:881/2225 train_time:52943ms step_avg:60.09ms
step:882/2225 train_time:53003ms step_avg:60.09ms
step:883/2225 train_time:53064ms step_avg:60.10ms
step:884/2225 train_time:53124ms step_avg:60.09ms
step:885/2225 train_time:53186ms step_avg:60.10ms
step:886/2225 train_time:53246ms step_avg:60.10ms
step:887/2225 train_time:53307ms step_avg:60.10ms
step:888/2225 train_time:53367ms step_avg:60.10ms
step:889/2225 train_time:53428ms step_avg:60.10ms
step:890/2225 train_time:53488ms step_avg:60.10ms
step:891/2225 train_time:53549ms step_avg:60.10ms
step:892/2225 train_time:53609ms step_avg:60.10ms
step:893/2225 train_time:53671ms step_avg:60.10ms
step:894/2225 train_time:53731ms step_avg:60.10ms
step:895/2225 train_time:53792ms step_avg:60.10ms
step:896/2225 train_time:53852ms step_avg:60.10ms
step:897/2225 train_time:53913ms step_avg:60.10ms
step:898/2225 train_time:53973ms step_avg:60.10ms
step:899/2225 train_time:54034ms step_avg:60.11ms
step:900/2225 train_time:54095ms step_avg:60.11ms
step:901/2225 train_time:54156ms step_avg:60.11ms
step:902/2225 train_time:54216ms step_avg:60.11ms
step:903/2225 train_time:54278ms step_avg:60.11ms
step:904/2225 train_time:54337ms step_avg:60.11ms
step:905/2225 train_time:54398ms step_avg:60.11ms
step:906/2225 train_time:54458ms step_avg:60.11ms
step:907/2225 train_time:54518ms step_avg:60.11ms
step:908/2225 train_time:54578ms step_avg:60.11ms
step:909/2225 train_time:54639ms step_avg:60.11ms
step:910/2225 train_time:54698ms step_avg:60.11ms
step:911/2225 train_time:54759ms step_avg:60.11ms
step:912/2225 train_time:54819ms step_avg:60.11ms
step:913/2225 train_time:54879ms step_avg:60.11ms
step:914/2225 train_time:54938ms step_avg:60.11ms
step:915/2225 train_time:54999ms step_avg:60.11ms
step:916/2225 train_time:55058ms step_avg:60.11ms
step:917/2225 train_time:55119ms step_avg:60.11ms
step:918/2225 train_time:55178ms step_avg:60.11ms
step:919/2225 train_time:55239ms step_avg:60.11ms
step:920/2225 train_time:55298ms step_avg:60.11ms
step:921/2225 train_time:55360ms step_avg:60.11ms
step:922/2225 train_time:55419ms step_avg:60.11ms
step:923/2225 train_time:55479ms step_avg:60.11ms
step:924/2225 train_time:55538ms step_avg:60.11ms
step:925/2225 train_time:55599ms step_avg:60.11ms
step:926/2225 train_time:55659ms step_avg:60.11ms
step:927/2225 train_time:55720ms step_avg:60.11ms
step:928/2225 train_time:55779ms step_avg:60.11ms
step:929/2225 train_time:55840ms step_avg:60.11ms
step:930/2225 train_time:55899ms step_avg:60.11ms
step:931/2225 train_time:55960ms step_avg:60.11ms
step:932/2225 train_time:56020ms step_avg:60.11ms
step:933/2225 train_time:56081ms step_avg:60.11ms
step:934/2225 train_time:56140ms step_avg:60.11ms
step:935/2225 train_time:56201ms step_avg:60.11ms
step:936/2225 train_time:56261ms step_avg:60.11ms
step:937/2225 train_time:56323ms step_avg:60.11ms
step:938/2225 train_time:56383ms step_avg:60.11ms
step:939/2225 train_time:56443ms step_avg:60.11ms
step:940/2225 train_time:56503ms step_avg:60.11ms
step:941/2225 train_time:56564ms step_avg:60.11ms
step:942/2225 train_time:56624ms step_avg:60.11ms
step:943/2225 train_time:56685ms step_avg:60.11ms
step:944/2225 train_time:56745ms step_avg:60.11ms
step:945/2225 train_time:56806ms step_avg:60.11ms
step:946/2225 train_time:56866ms step_avg:60.11ms
step:947/2225 train_time:56927ms step_avg:60.11ms
step:948/2225 train_time:56987ms step_avg:60.11ms
step:949/2225 train_time:57048ms step_avg:60.11ms
step:950/2225 train_time:57108ms step_avg:60.11ms
step:951/2225 train_time:57169ms step_avg:60.11ms
step:952/2225 train_time:57229ms step_avg:60.11ms
step:953/2225 train_time:57292ms step_avg:60.12ms
step:954/2225 train_time:57351ms step_avg:60.12ms
step:955/2225 train_time:57413ms step_avg:60.12ms
step:956/2225 train_time:57472ms step_avg:60.12ms
step:957/2225 train_time:57534ms step_avg:60.12ms
step:958/2225 train_time:57594ms step_avg:60.12ms
step:959/2225 train_time:57654ms step_avg:60.12ms
step:960/2225 train_time:57714ms step_avg:60.12ms
step:961/2225 train_time:57775ms step_avg:60.12ms
step:962/2225 train_time:57835ms step_avg:60.12ms
step:963/2225 train_time:57895ms step_avg:60.12ms
step:964/2225 train_time:57955ms step_avg:60.12ms
step:965/2225 train_time:58016ms step_avg:60.12ms
step:966/2225 train_time:58076ms step_avg:60.12ms
step:967/2225 train_time:58137ms step_avg:60.12ms
step:968/2225 train_time:58196ms step_avg:60.12ms
step:969/2225 train_time:58257ms step_avg:60.12ms
step:970/2225 train_time:58317ms step_avg:60.12ms
step:971/2225 train_time:58378ms step_avg:60.12ms
step:972/2225 train_time:58437ms step_avg:60.12ms
step:973/2225 train_time:58498ms step_avg:60.12ms
step:974/2225 train_time:58558ms step_avg:60.12ms
step:975/2225 train_time:58619ms step_avg:60.12ms
step:976/2225 train_time:58678ms step_avg:60.12ms
step:977/2225 train_time:58738ms step_avg:60.12ms
step:978/2225 train_time:58797ms step_avg:60.12ms
step:979/2225 train_time:58858ms step_avg:60.12ms
step:980/2225 train_time:58917ms step_avg:60.12ms
step:981/2225 train_time:58978ms step_avg:60.12ms
step:982/2225 train_time:59037ms step_avg:60.12ms
step:983/2225 train_time:59099ms step_avg:60.12ms
step:984/2225 train_time:59158ms step_avg:60.12ms
step:985/2225 train_time:59219ms step_avg:60.12ms
step:986/2225 train_time:59279ms step_avg:60.12ms
step:987/2225 train_time:59340ms step_avg:60.12ms
step:988/2225 train_time:59399ms step_avg:60.12ms
step:989/2225 train_time:59460ms step_avg:60.12ms
step:990/2225 train_time:59520ms step_avg:60.12ms
step:991/2225 train_time:59580ms step_avg:60.12ms
step:992/2225 train_time:59639ms step_avg:60.12ms
step:993/2225 train_time:59700ms step_avg:60.12ms
step:994/2225 train_time:59759ms step_avg:60.12ms
step:995/2225 train_time:59820ms step_avg:60.12ms
step:996/2225 train_time:59879ms step_avg:60.12ms
step:997/2225 train_time:59940ms step_avg:60.12ms
step:998/2225 train_time:60000ms step_avg:60.12ms
step:999/2225 train_time:60061ms step_avg:60.12ms
step:1000/2225 train_time:60121ms step_avg:60.12ms
step:1000/2225 val_loss:3.5931 train_time:60182ms step_avg:60.18ms
step:1001/2225 train_time:60206ms step_avg:60.15ms
step:1002/2225 train_time:60244ms step_avg:60.12ms
step:1003/2225 train_time:60311ms step_avg:60.13ms
step:1004/2225 train_time:60372ms step_avg:60.13ms
step:1005/2225 train_time:60433ms step_avg:60.13ms
step:1006/2225 train_time:60493ms step_avg:60.13ms
step:1007/2225 train_time:60554ms step_avg:60.13ms
step:1008/2225 train_time:60613ms step_avg:60.13ms
step:1009/2225 train_time:60674ms step_avg:60.13ms
step:1010/2225 train_time:60733ms step_avg:60.13ms
step:1011/2225 train_time:60794ms step_avg:60.13ms
step:1012/2225 train_time:60853ms step_avg:60.13ms
step:1013/2225 train_time:60914ms step_avg:60.13ms
step:1014/2225 train_time:60973ms step_avg:60.13ms
step:1015/2225 train_time:61034ms step_avg:60.13ms
step:1016/2225 train_time:61094ms step_avg:60.13ms
step:1017/2225 train_time:61156ms step_avg:60.13ms
step:1018/2225 train_time:61217ms step_avg:60.13ms
step:1019/2225 train_time:61282ms step_avg:60.14ms
step:1020/2225 train_time:61344ms step_avg:60.14ms
step:1021/2225 train_time:61405ms step_avg:60.14ms
step:1022/2225 train_time:61465ms step_avg:60.14ms
step:1023/2225 train_time:61526ms step_avg:60.14ms
step:1024/2225 train_time:61586ms step_avg:60.14ms
step:1025/2225 train_time:61647ms step_avg:60.14ms
step:1026/2225 train_time:61707ms step_avg:60.14ms
step:1027/2225 train_time:61767ms step_avg:60.14ms
step:1028/2225 train_time:61827ms step_avg:60.14ms
step:1029/2225 train_time:61887ms step_avg:60.14ms
step:1030/2225 train_time:61947ms step_avg:60.14ms
step:1031/2225 train_time:62008ms step_avg:60.14ms
step:1032/2225 train_time:62067ms step_avg:60.14ms
step:1033/2225 train_time:62128ms step_avg:60.14ms
step:1034/2225 train_time:62189ms step_avg:60.14ms
step:1035/2225 train_time:62251ms step_avg:60.15ms
step:1036/2225 train_time:62311ms step_avg:60.15ms
step:1037/2225 train_time:62372ms step_avg:60.15ms
step:1038/2225 train_time:62432ms step_avg:60.15ms
step:1039/2225 train_time:62493ms step_avg:60.15ms
step:1040/2225 train_time:62552ms step_avg:60.15ms
step:1041/2225 train_time:62613ms step_avg:60.15ms
step:1042/2225 train_time:62672ms step_avg:60.15ms
step:1043/2225 train_time:62733ms step_avg:60.15ms
step:1044/2225 train_time:62793ms step_avg:60.15ms
step:1045/2225 train_time:62854ms step_avg:60.15ms
step:1046/2225 train_time:62914ms step_avg:60.15ms
step:1047/2225 train_time:62975ms step_avg:60.15ms
step:1048/2225 train_time:63034ms step_avg:60.15ms
step:1049/2225 train_time:63096ms step_avg:60.15ms
step:1050/2225 train_time:63156ms step_avg:60.15ms
step:1051/2225 train_time:63218ms step_avg:60.15ms
step:1052/2225 train_time:63278ms step_avg:60.15ms
step:1053/2225 train_time:63341ms step_avg:60.15ms
step:1054/2225 train_time:63401ms step_avg:60.15ms
step:1055/2225 train_time:63463ms step_avg:60.15ms
step:1056/2225 train_time:63522ms step_avg:60.15ms
step:1057/2225 train_time:63584ms step_avg:60.16ms
step:1058/2225 train_time:63644ms step_avg:60.15ms
step:1059/2225 train_time:63705ms step_avg:60.16ms
step:1060/2225 train_time:63765ms step_avg:60.16ms
step:1061/2225 train_time:63826ms step_avg:60.16ms
step:1062/2225 train_time:63886ms step_avg:60.16ms
step:1063/2225 train_time:63947ms step_avg:60.16ms
step:1064/2225 train_time:64006ms step_avg:60.16ms
step:1065/2225 train_time:64067ms step_avg:60.16ms
step:1066/2225 train_time:64127ms step_avg:60.16ms
step:1067/2225 train_time:64188ms step_avg:60.16ms
step:1068/2225 train_time:64249ms step_avg:60.16ms
step:1069/2225 train_time:64310ms step_avg:60.16ms
step:1070/2225 train_time:64370ms step_avg:60.16ms
step:1071/2225 train_time:64431ms step_avg:60.16ms
step:1072/2225 train_time:64490ms step_avg:60.16ms
step:1073/2225 train_time:64551ms step_avg:60.16ms
step:1074/2225 train_time:64610ms step_avg:60.16ms
step:1075/2225 train_time:64671ms step_avg:60.16ms
step:1076/2225 train_time:64730ms step_avg:60.16ms
step:1077/2225 train_time:64790ms step_avg:60.16ms
step:1078/2225 train_time:64850ms step_avg:60.16ms
step:1079/2225 train_time:64910ms step_avg:60.16ms
step:1080/2225 train_time:64969ms step_avg:60.16ms
step:1081/2225 train_time:65030ms step_avg:60.16ms
step:1082/2225 train_time:65090ms step_avg:60.16ms
step:1083/2225 train_time:65151ms step_avg:60.16ms
step:1084/2225 train_time:65211ms step_avg:60.16ms
step:1085/2225 train_time:65272ms step_avg:60.16ms
step:1086/2225 train_time:65331ms step_avg:60.16ms
step:1087/2225 train_time:65392ms step_avg:60.16ms
step:1088/2225 train_time:65452ms step_avg:60.16ms
step:1089/2225 train_time:65513ms step_avg:60.16ms
step:1090/2225 train_time:65573ms step_avg:60.16ms
step:1091/2225 train_time:65634ms step_avg:60.16ms
step:1092/2225 train_time:65694ms step_avg:60.16ms
step:1093/2225 train_time:65755ms step_avg:60.16ms
step:1094/2225 train_time:65815ms step_avg:60.16ms
step:1095/2225 train_time:65877ms step_avg:60.16ms
step:1096/2225 train_time:65937ms step_avg:60.16ms
step:1097/2225 train_time:65998ms step_avg:60.16ms
step:1098/2225 train_time:66058ms step_avg:60.16ms
step:1099/2225 train_time:66120ms step_avg:60.16ms
step:1100/2225 train_time:66180ms step_avg:60.16ms
step:1101/2225 train_time:66241ms step_avg:60.16ms
step:1102/2225 train_time:66301ms step_avg:60.16ms
step:1103/2225 train_time:66363ms step_avg:60.17ms
step:1104/2225 train_time:66423ms step_avg:60.17ms
step:1105/2225 train_time:66484ms step_avg:60.17ms
step:1106/2225 train_time:66544ms step_avg:60.17ms
step:1107/2225 train_time:66605ms step_avg:60.17ms
step:1108/2225 train_time:66665ms step_avg:60.17ms
step:1109/2225 train_time:66726ms step_avg:60.17ms
step:1110/2225 train_time:66787ms step_avg:60.17ms
step:1111/2225 train_time:66848ms step_avg:60.17ms
step:1112/2225 train_time:66908ms step_avg:60.17ms
step:1113/2225 train_time:66968ms step_avg:60.17ms
step:1114/2225 train_time:67028ms step_avg:60.17ms
step:1115/2225 train_time:67089ms step_avg:60.17ms
step:1116/2225 train_time:67149ms step_avg:60.17ms
step:1117/2225 train_time:67210ms step_avg:60.17ms
step:1118/2225 train_time:67269ms step_avg:60.17ms
step:1119/2225 train_time:67330ms step_avg:60.17ms
step:1120/2225 train_time:67389ms step_avg:60.17ms
step:1121/2225 train_time:67451ms step_avg:60.17ms
step:1122/2225 train_time:67510ms step_avg:60.17ms
step:1123/2225 train_time:67570ms step_avg:60.17ms
step:1124/2225 train_time:67630ms step_avg:60.17ms
step:1125/2225 train_time:67690ms step_avg:60.17ms
step:1126/2225 train_time:67750ms step_avg:60.17ms
step:1127/2225 train_time:67811ms step_avg:60.17ms
step:1128/2225 train_time:67870ms step_avg:60.17ms
step:1129/2225 train_time:67931ms step_avg:60.17ms
step:1130/2225 train_time:67991ms step_avg:60.17ms
step:1131/2225 train_time:68052ms step_avg:60.17ms
step:1132/2225 train_time:68111ms step_avg:60.17ms
step:1133/2225 train_time:68172ms step_avg:60.17ms
step:1134/2225 train_time:68231ms step_avg:60.17ms
step:1135/2225 train_time:68292ms step_avg:60.17ms
step:1136/2225 train_time:68351ms step_avg:60.17ms
step:1137/2225 train_time:68412ms step_avg:60.17ms
step:1138/2225 train_time:68472ms step_avg:60.17ms
step:1139/2225 train_time:68533ms step_avg:60.17ms
step:1140/2225 train_time:68593ms step_avg:60.17ms
step:1141/2225 train_time:68654ms step_avg:60.17ms
step:1142/2225 train_time:68714ms step_avg:60.17ms
step:1143/2225 train_time:68775ms step_avg:60.17ms
step:1144/2225 train_time:68835ms step_avg:60.17ms
step:1145/2225 train_time:68897ms step_avg:60.17ms
step:1146/2225 train_time:68957ms step_avg:60.17ms
step:1147/2225 train_time:69019ms step_avg:60.17ms
step:1148/2225 train_time:69079ms step_avg:60.17ms
step:1149/2225 train_time:69141ms step_avg:60.18ms
step:1150/2225 train_time:69201ms step_avg:60.18ms
step:1151/2225 train_time:69263ms step_avg:60.18ms
step:1152/2225 train_time:69323ms step_avg:60.18ms
step:1153/2225 train_time:69384ms step_avg:60.18ms
step:1154/2225 train_time:69444ms step_avg:60.18ms
step:1155/2225 train_time:69505ms step_avg:60.18ms
step:1156/2225 train_time:69565ms step_avg:60.18ms
step:1157/2225 train_time:69626ms step_avg:60.18ms
step:1158/2225 train_time:69686ms step_avg:60.18ms
step:1159/2225 train_time:69747ms step_avg:60.18ms
step:1160/2225 train_time:69807ms step_avg:60.18ms
step:1161/2225 train_time:69868ms step_avg:60.18ms
step:1162/2225 train_time:69928ms step_avg:60.18ms
step:1163/2225 train_time:69989ms step_avg:60.18ms
step:1164/2225 train_time:70049ms step_avg:60.18ms
step:1165/2225 train_time:70110ms step_avg:60.18ms
step:1166/2225 train_time:70169ms step_avg:60.18ms
step:1167/2225 train_time:70230ms step_avg:60.18ms
step:1168/2225 train_time:70289ms step_avg:60.18ms
step:1169/2225 train_time:70350ms step_avg:60.18ms
step:1170/2225 train_time:70410ms step_avg:60.18ms
step:1171/2225 train_time:70470ms step_avg:60.18ms
step:1172/2225 train_time:70530ms step_avg:60.18ms
step:1173/2225 train_time:70590ms step_avg:60.18ms
step:1174/2225 train_time:70650ms step_avg:60.18ms
step:1175/2225 train_time:70711ms step_avg:60.18ms
step:1176/2225 train_time:70770ms step_avg:60.18ms
step:1177/2225 train_time:70831ms step_avg:60.18ms
step:1178/2225 train_time:70890ms step_avg:60.18ms
step:1179/2225 train_time:70951ms step_avg:60.18ms
step:1180/2225 train_time:71011ms step_avg:60.18ms
step:1181/2225 train_time:71072ms step_avg:60.18ms
step:1182/2225 train_time:71131ms step_avg:60.18ms
step:1183/2225 train_time:71192ms step_avg:60.18ms
step:1184/2225 train_time:71252ms step_avg:60.18ms
step:1185/2225 train_time:71313ms step_avg:60.18ms
step:1186/2225 train_time:71373ms step_avg:60.18ms
step:1187/2225 train_time:71434ms step_avg:60.18ms
step:1188/2225 train_time:71494ms step_avg:60.18ms
step:1189/2225 train_time:71555ms step_avg:60.18ms
step:1190/2225 train_time:71615ms step_avg:60.18ms
step:1191/2225 train_time:71676ms step_avg:60.18ms
step:1192/2225 train_time:71736ms step_avg:60.18ms
step:1193/2225 train_time:71798ms step_avg:60.18ms
step:1194/2225 train_time:71857ms step_avg:60.18ms
step:1195/2225 train_time:71919ms step_avg:60.18ms
step:1196/2225 train_time:71978ms step_avg:60.18ms
step:1197/2225 train_time:72040ms step_avg:60.18ms
step:1198/2225 train_time:72100ms step_avg:60.18ms
step:1199/2225 train_time:72161ms step_avg:60.18ms
step:1200/2225 train_time:72220ms step_avg:60.18ms
step:1201/2225 train_time:72282ms step_avg:60.18ms
step:1202/2225 train_time:72342ms step_avg:60.18ms
step:1203/2225 train_time:72403ms step_avg:60.19ms
step:1204/2225 train_time:72463ms step_avg:60.19ms
step:1205/2225 train_time:72524ms step_avg:60.19ms
step:1206/2225 train_time:72584ms step_avg:60.19ms
step:1207/2225 train_time:72645ms step_avg:60.19ms
step:1208/2225 train_time:72705ms step_avg:60.19ms
step:1209/2225 train_time:72766ms step_avg:60.19ms
step:1210/2225 train_time:72825ms step_avg:60.19ms
step:1211/2225 train_time:72887ms step_avg:60.19ms
step:1212/2225 train_time:72946ms step_avg:60.19ms
step:1213/2225 train_time:73008ms step_avg:60.19ms
step:1214/2225 train_time:73067ms step_avg:60.19ms
step:1215/2225 train_time:73128ms step_avg:60.19ms
step:1216/2225 train_time:73187ms step_avg:60.19ms
step:1217/2225 train_time:73248ms step_avg:60.19ms
step:1218/2225 train_time:73308ms step_avg:60.19ms
step:1219/2225 train_time:73369ms step_avg:60.19ms
step:1220/2225 train_time:73427ms step_avg:60.19ms
step:1221/2225 train_time:73488ms step_avg:60.19ms
step:1222/2225 train_time:73548ms step_avg:60.19ms
step:1223/2225 train_time:73609ms step_avg:60.19ms
step:1224/2225 train_time:73668ms step_avg:60.19ms
step:1225/2225 train_time:73728ms step_avg:60.19ms
step:1226/2225 train_time:73788ms step_avg:60.19ms
step:1227/2225 train_time:73849ms step_avg:60.19ms
step:1228/2225 train_time:73909ms step_avg:60.19ms
step:1229/2225 train_time:73969ms step_avg:60.19ms
step:1230/2225 train_time:74029ms step_avg:60.19ms
step:1231/2225 train_time:74090ms step_avg:60.19ms
step:1232/2225 train_time:74150ms step_avg:60.19ms
step:1233/2225 train_time:74211ms step_avg:60.19ms
step:1234/2225 train_time:74270ms step_avg:60.19ms
step:1235/2225 train_time:74330ms step_avg:60.19ms
step:1236/2225 train_time:74390ms step_avg:60.19ms
step:1237/2225 train_time:74451ms step_avg:60.19ms
step:1238/2225 train_time:74511ms step_avg:60.19ms
step:1239/2225 train_time:74572ms step_avg:60.19ms
step:1240/2225 train_time:74631ms step_avg:60.19ms
step:1241/2225 train_time:74692ms step_avg:60.19ms
step:1242/2225 train_time:74752ms step_avg:60.19ms
step:1243/2225 train_time:74813ms step_avg:60.19ms
step:1244/2225 train_time:74873ms step_avg:60.19ms
step:1245/2225 train_time:74934ms step_avg:60.19ms
step:1246/2225 train_time:74993ms step_avg:60.19ms
step:1247/2225 train_time:75055ms step_avg:60.19ms
step:1248/2225 train_time:75114ms step_avg:60.19ms
step:1249/2225 train_time:75176ms step_avg:60.19ms
step:1250/2225 train_time:75236ms step_avg:60.19ms
step:1250/2225 val_loss:3.5206 train_time:75297ms step_avg:60.24ms
step:1251/2225 train_time:75321ms step_avg:60.21ms
step:1252/2225 train_time:75359ms step_avg:60.19ms
step:1253/2225 train_time:75426ms step_avg:60.20ms
step:1254/2225 train_time:75488ms step_avg:60.20ms
step:1255/2225 train_time:75550ms step_avg:60.20ms
step:1256/2225 train_time:75610ms step_avg:60.20ms
step:1257/2225 train_time:75670ms step_avg:60.20ms
step:1258/2225 train_time:75730ms step_avg:60.20ms
step:1259/2225 train_time:75790ms step_avg:60.20ms
step:1260/2225 train_time:75849ms step_avg:60.20ms
step:1261/2225 train_time:75909ms step_avg:60.20ms
step:1262/2225 train_time:75968ms step_avg:60.20ms
step:1263/2225 train_time:76028ms step_avg:60.20ms
step:1264/2225 train_time:76088ms step_avg:60.20ms
step:1265/2225 train_time:76148ms step_avg:60.20ms
step:1266/2225 train_time:76208ms step_avg:60.20ms
step:1267/2225 train_time:76273ms step_avg:60.20ms
step:1268/2225 train_time:76334ms step_avg:60.20ms
step:1269/2225 train_time:76396ms step_avg:60.20ms
step:1270/2225 train_time:76457ms step_avg:60.20ms
step:1271/2225 train_time:76519ms step_avg:60.20ms
step:1272/2225 train_time:76579ms step_avg:60.20ms
step:1273/2225 train_time:76640ms step_avg:60.20ms
step:1274/2225 train_time:76699ms step_avg:60.20ms
step:1275/2225 train_time:76759ms step_avg:60.20ms
step:1276/2225 train_time:76818ms step_avg:60.20ms
step:1277/2225 train_time:76879ms step_avg:60.20ms
step:1278/2225 train_time:76938ms step_avg:60.20ms
step:1279/2225 train_time:76998ms step_avg:60.20ms
step:1280/2225 train_time:77057ms step_avg:60.20ms
step:1281/2225 train_time:77118ms step_avg:60.20ms
step:1282/2225 train_time:77177ms step_avg:60.20ms
step:1283/2225 train_time:77238ms step_avg:60.20ms
step:1284/2225 train_time:77298ms step_avg:60.20ms
step:1285/2225 train_time:77360ms step_avg:60.20ms
step:1286/2225 train_time:77420ms step_avg:60.20ms
step:1287/2225 train_time:77483ms step_avg:60.20ms
step:1288/2225 train_time:77543ms step_avg:60.20ms
step:1289/2225 train_time:77604ms step_avg:60.21ms
step:1290/2225 train_time:77665ms step_avg:60.21ms
step:1291/2225 train_time:77726ms step_avg:60.21ms
step:1292/2225 train_time:77786ms step_avg:60.21ms
step:1293/2225 train_time:77846ms step_avg:60.21ms
step:1294/2225 train_time:77906ms step_avg:60.21ms
step:1295/2225 train_time:77967ms step_avg:60.21ms
step:1296/2225 train_time:78027ms step_avg:60.21ms
step:1297/2225 train_time:78088ms step_avg:60.21ms
step:1298/2225 train_time:78147ms step_avg:60.21ms
step:1299/2225 train_time:78209ms step_avg:60.21ms
step:1300/2225 train_time:78270ms step_avg:60.21ms
step:1301/2225 train_time:78331ms step_avg:60.21ms
step:1302/2225 train_time:78392ms step_avg:60.21ms
step:1303/2225 train_time:78454ms step_avg:60.21ms
step:1304/2225 train_time:78513ms step_avg:60.21ms
step:1305/2225 train_time:78575ms step_avg:60.21ms
step:1306/2225 train_time:78634ms step_avg:60.21ms
step:1307/2225 train_time:78695ms step_avg:60.21ms
step:1308/2225 train_time:78754ms step_avg:60.21ms
step:1309/2225 train_time:78816ms step_avg:60.21ms
step:1310/2225 train_time:78875ms step_avg:60.21ms
step:1311/2225 train_time:78936ms step_avg:60.21ms
step:1312/2225 train_time:78995ms step_avg:60.21ms
step:1313/2225 train_time:79056ms step_avg:60.21ms
step:1314/2225 train_time:79115ms step_avg:60.21ms
step:1315/2225 train_time:79176ms step_avg:60.21ms
step:1316/2225 train_time:79235ms step_avg:60.21ms
step:1317/2225 train_time:79296ms step_avg:60.21ms
step:1318/2225 train_time:79356ms step_avg:60.21ms
step:1319/2225 train_time:79417ms step_avg:60.21ms
step:1320/2225 train_time:79477ms step_avg:60.21ms
step:1321/2225 train_time:79538ms step_avg:60.21ms
step:1322/2225 train_time:79598ms step_avg:60.21ms
step:1323/2225 train_time:79659ms step_avg:60.21ms
step:1324/2225 train_time:79718ms step_avg:60.21ms
step:1325/2225 train_time:79779ms step_avg:60.21ms
step:1326/2225 train_time:79839ms step_avg:60.21ms
step:1327/2225 train_time:79899ms step_avg:60.21ms
step:1328/2225 train_time:79959ms step_avg:60.21ms
step:1329/2225 train_time:80020ms step_avg:60.21ms
step:1330/2225 train_time:80079ms step_avg:60.21ms
step:1331/2225 train_time:80141ms step_avg:60.21ms
step:1332/2225 train_time:80200ms step_avg:60.21ms
step:1333/2225 train_time:80261ms step_avg:60.21ms
step:1334/2225 train_time:80321ms step_avg:60.21ms
step:1335/2225 train_time:80383ms step_avg:60.21ms
step:1336/2225 train_time:80443ms step_avg:60.21ms
step:1337/2225 train_time:80505ms step_avg:60.21ms
step:1338/2225 train_time:80564ms step_avg:60.21ms
step:1339/2225 train_time:80626ms step_avg:60.21ms
step:1340/2225 train_time:80686ms step_avg:60.21ms
step:1341/2225 train_time:80748ms step_avg:60.21ms
step:1342/2225 train_time:80808ms step_avg:60.21ms
step:1343/2225 train_time:80870ms step_avg:60.22ms
step:1344/2225 train_time:80930ms step_avg:60.22ms
step:1345/2225 train_time:80991ms step_avg:60.22ms
step:1346/2225 train_time:81051ms step_avg:60.22ms
step:1347/2225 train_time:81112ms step_avg:60.22ms
step:1348/2225 train_time:81172ms step_avg:60.22ms
step:1349/2225 train_time:81233ms step_avg:60.22ms
step:1350/2225 train_time:81292ms step_avg:60.22ms
step:1351/2225 train_time:81353ms step_avg:60.22ms
step:1352/2225 train_time:81413ms step_avg:60.22ms
step:1353/2225 train_time:81473ms step_avg:60.22ms
step:1354/2225 train_time:81533ms step_avg:60.22ms
step:1355/2225 train_time:81594ms step_avg:60.22ms
step:1356/2225 train_time:81653ms step_avg:60.22ms
step:1357/2225 train_time:81714ms step_avg:60.22ms
step:1358/2225 train_time:81774ms step_avg:60.22ms
step:1359/2225 train_time:81835ms step_avg:60.22ms
step:1360/2225 train_time:81894ms step_avg:60.22ms
step:1361/2225 train_time:81955ms step_avg:60.22ms
step:1362/2225 train_time:82014ms step_avg:60.22ms
step:1363/2225 train_time:82076ms step_avg:60.22ms
step:1364/2225 train_time:82136ms step_avg:60.22ms
step:1365/2225 train_time:82196ms step_avg:60.22ms
step:1366/2225 train_time:82255ms step_avg:60.22ms
step:1367/2225 train_time:82317ms step_avg:60.22ms
step:1368/2225 train_time:82376ms step_avg:60.22ms
step:1369/2225 train_time:82437ms step_avg:60.22ms
step:1370/2225 train_time:82497ms step_avg:60.22ms
step:1371/2225 train_time:82558ms step_avg:60.22ms
step:1372/2225 train_time:82618ms step_avg:60.22ms
step:1373/2225 train_time:82679ms step_avg:60.22ms
step:1374/2225 train_time:82738ms step_avg:60.22ms
step:1375/2225 train_time:82799ms step_avg:60.22ms
step:1376/2225 train_time:82859ms step_avg:60.22ms
step:1377/2225 train_time:82920ms step_avg:60.22ms
step:1378/2225 train_time:82980ms step_avg:60.22ms
step:1379/2225 train_time:83041ms step_avg:60.22ms
step:1380/2225 train_time:83101ms step_avg:60.22ms
step:1381/2225 train_time:83162ms step_avg:60.22ms
step:1382/2225 train_time:83222ms step_avg:60.22ms
step:1383/2225 train_time:83284ms step_avg:60.22ms
step:1384/2225 train_time:83344ms step_avg:60.22ms
step:1385/2225 train_time:83405ms step_avg:60.22ms
step:1386/2225 train_time:83465ms step_avg:60.22ms
step:1387/2225 train_time:83527ms step_avg:60.22ms
step:1388/2225 train_time:83586ms step_avg:60.22ms
step:1389/2225 train_time:83648ms step_avg:60.22ms
step:1390/2225 train_time:83708ms step_avg:60.22ms
step:1391/2225 train_time:83769ms step_avg:60.22ms
step:1392/2225 train_time:83829ms step_avg:60.22ms
step:1393/2225 train_time:83891ms step_avg:60.22ms
step:1394/2225 train_time:83951ms step_avg:60.22ms
step:1395/2225 train_time:84012ms step_avg:60.22ms
step:1396/2225 train_time:84071ms step_avg:60.22ms
step:1397/2225 train_time:84132ms step_avg:60.22ms
step:1398/2225 train_time:84192ms step_avg:60.22ms
step:1399/2225 train_time:84253ms step_avg:60.22ms
step:1400/2225 train_time:84313ms step_avg:60.22ms
step:1401/2225 train_time:84373ms step_avg:60.22ms
step:1402/2225 train_time:84433ms step_avg:60.22ms
step:1403/2225 train_time:84494ms step_avg:60.22ms
step:1404/2225 train_time:84554ms step_avg:60.22ms
step:1405/2225 train_time:84615ms step_avg:60.22ms
step:1406/2225 train_time:84674ms step_avg:60.22ms
step:1407/2225 train_time:84735ms step_avg:60.22ms
step:1408/2225 train_time:84794ms step_avg:60.22ms
step:1409/2225 train_time:84855ms step_avg:60.22ms
step:1410/2225 train_time:84915ms step_avg:60.22ms
step:1411/2225 train_time:84976ms step_avg:60.22ms
step:1412/2225 train_time:85035ms step_avg:60.22ms
step:1413/2225 train_time:85096ms step_avg:60.22ms
step:1414/2225 train_time:85155ms step_avg:60.22ms
step:1415/2225 train_time:85217ms step_avg:60.22ms
step:1416/2225 train_time:85276ms step_avg:60.22ms
step:1417/2225 train_time:85337ms step_avg:60.22ms
step:1418/2225 train_time:85396ms step_avg:60.22ms
step:1419/2225 train_time:85458ms step_avg:60.22ms
step:1420/2225 train_time:85518ms step_avg:60.22ms
step:1421/2225 train_time:85579ms step_avg:60.22ms
step:1422/2225 train_time:85639ms step_avg:60.22ms
step:1423/2225 train_time:85699ms step_avg:60.22ms
step:1424/2225 train_time:85758ms step_avg:60.22ms
step:1425/2225 train_time:85819ms step_avg:60.22ms
step:1426/2225 train_time:85879ms step_avg:60.22ms
step:1427/2225 train_time:85941ms step_avg:60.22ms
step:1428/2225 train_time:86000ms step_avg:60.22ms
step:1429/2225 train_time:86061ms step_avg:60.22ms
step:1430/2225 train_time:86121ms step_avg:60.22ms
step:1431/2225 train_time:86182ms step_avg:60.23ms
step:1432/2225 train_time:86242ms step_avg:60.23ms
step:1433/2225 train_time:86304ms step_avg:60.23ms
step:1434/2225 train_time:86364ms step_avg:60.23ms
step:1435/2225 train_time:86425ms step_avg:60.23ms
step:1436/2225 train_time:86485ms step_avg:60.23ms
step:1437/2225 train_time:86547ms step_avg:60.23ms
step:1438/2225 train_time:86607ms step_avg:60.23ms
step:1439/2225 train_time:86669ms step_avg:60.23ms
step:1440/2225 train_time:86729ms step_avg:60.23ms
step:1441/2225 train_time:86791ms step_avg:60.23ms
step:1442/2225 train_time:86850ms step_avg:60.23ms
step:1443/2225 train_time:86911ms step_avg:60.23ms
step:1444/2225 train_time:86971ms step_avg:60.23ms
step:1445/2225 train_time:87032ms step_avg:60.23ms
step:1446/2225 train_time:87092ms step_avg:60.23ms
step:1447/2225 train_time:87154ms step_avg:60.23ms
step:1448/2225 train_time:87214ms step_avg:60.23ms
step:1449/2225 train_time:87275ms step_avg:60.23ms
step:1450/2225 train_time:87334ms step_avg:60.23ms
step:1451/2225 train_time:87396ms step_avg:60.23ms
step:1452/2225 train_time:87455ms step_avg:60.23ms
step:1453/2225 train_time:87516ms step_avg:60.23ms
step:1454/2225 train_time:87576ms step_avg:60.23ms
step:1455/2225 train_time:87637ms step_avg:60.23ms
step:1456/2225 train_time:87696ms step_avg:60.23ms
step:1457/2225 train_time:87757ms step_avg:60.23ms
step:1458/2225 train_time:87817ms step_avg:60.23ms
step:1459/2225 train_time:87878ms step_avg:60.23ms
step:1460/2225 train_time:87938ms step_avg:60.23ms
step:1461/2225 train_time:88000ms step_avg:60.23ms
step:1462/2225 train_time:88060ms step_avg:60.23ms
step:1463/2225 train_time:88121ms step_avg:60.23ms
step:1464/2225 train_time:88182ms step_avg:60.23ms
step:1465/2225 train_time:88244ms step_avg:60.23ms
step:1466/2225 train_time:88305ms step_avg:60.24ms
step:1467/2225 train_time:88366ms step_avg:60.24ms
step:1468/2225 train_time:88427ms step_avg:60.24ms
step:1469/2225 train_time:88489ms step_avg:60.24ms
step:1470/2225 train_time:88549ms step_avg:60.24ms
step:1471/2225 train_time:88611ms step_avg:60.24ms
step:1472/2225 train_time:88671ms step_avg:60.24ms
step:1473/2225 train_time:88733ms step_avg:60.24ms
step:1474/2225 train_time:88793ms step_avg:60.24ms
step:1475/2225 train_time:88854ms step_avg:60.24ms
step:1476/2225 train_time:88915ms step_avg:60.24ms
step:1477/2225 train_time:88976ms step_avg:60.24ms
step:1478/2225 train_time:89036ms step_avg:60.24ms
step:1479/2225 train_time:89097ms step_avg:60.24ms
step:1480/2225 train_time:89157ms step_avg:60.24ms
step:1481/2225 train_time:89219ms step_avg:60.24ms
step:1482/2225 train_time:89279ms step_avg:60.24ms
step:1483/2225 train_time:89340ms step_avg:60.24ms
step:1484/2225 train_time:89400ms step_avg:60.24ms
step:1485/2225 train_time:89462ms step_avg:60.24ms
step:1486/2225 train_time:89522ms step_avg:60.24ms
step:1487/2225 train_time:89584ms step_avg:60.25ms
step:1488/2225 train_time:89645ms step_avg:60.25ms
step:1489/2225 train_time:89707ms step_avg:60.25ms
step:1490/2225 train_time:89767ms step_avg:60.25ms
step:1491/2225 train_time:89830ms step_avg:60.25ms
step:1492/2225 train_time:89890ms step_avg:60.25ms
step:1493/2225 train_time:89952ms step_avg:60.25ms
step:1494/2225 train_time:90012ms step_avg:60.25ms
step:1495/2225 train_time:90074ms step_avg:60.25ms
step:1496/2225 train_time:90134ms step_avg:60.25ms
step:1497/2225 train_time:90195ms step_avg:60.25ms
step:1498/2225 train_time:90256ms step_avg:60.25ms
step:1499/2225 train_time:90317ms step_avg:60.25ms
step:1500/2225 train_time:90377ms step_avg:60.25ms
step:1500/2225 val_loss:3.4402 train_time:90439ms step_avg:60.29ms
step:1501/2225 train_time:90463ms step_avg:60.27ms
step:1502/2225 train_time:90500ms step_avg:60.25ms
step:1503/2225 train_time:90562ms step_avg:60.25ms
step:1504/2225 train_time:90623ms step_avg:60.25ms
step:1505/2225 train_time:90686ms step_avg:60.26ms
step:1506/2225 train_time:90746ms step_avg:60.26ms
step:1507/2225 train_time:90807ms step_avg:60.26ms
step:1508/2225 train_time:90866ms step_avg:60.26ms
step:1509/2225 train_time:90926ms step_avg:60.26ms
step:1510/2225 train_time:90985ms step_avg:60.25ms
step:1511/2225 train_time:91045ms step_avg:60.26ms
step:1512/2225 train_time:91104ms step_avg:60.25ms
step:1513/2225 train_time:91165ms step_avg:60.25ms
step:1514/2225 train_time:91224ms step_avg:60.25ms
step:1515/2225 train_time:91285ms step_avg:60.25ms
step:1516/2225 train_time:91348ms step_avg:60.26ms
step:1517/2225 train_time:91414ms step_avg:60.26ms
step:1518/2225 train_time:91475ms step_avg:60.26ms
step:1519/2225 train_time:91538ms step_avg:60.26ms
step:1520/2225 train_time:91600ms step_avg:60.26ms
step:1521/2225 train_time:91663ms step_avg:60.26ms
step:1522/2225 train_time:91722ms step_avg:60.26ms
step:1523/2225 train_time:91784ms step_avg:60.27ms
step:1524/2225 train_time:91844ms step_avg:60.26ms
step:1525/2225 train_time:91905ms step_avg:60.27ms
step:1526/2225 train_time:91965ms step_avg:60.27ms
step:1527/2225 train_time:92025ms step_avg:60.27ms
step:1528/2225 train_time:92084ms step_avg:60.26ms
step:1529/2225 train_time:92145ms step_avg:60.26ms
step:1530/2225 train_time:92205ms step_avg:60.26ms
step:1531/2225 train_time:92266ms step_avg:60.27ms
step:1532/2225 train_time:92327ms step_avg:60.27ms
step:1533/2225 train_time:92391ms step_avg:60.27ms
step:1534/2225 train_time:92451ms step_avg:60.27ms
step:1535/2225 train_time:92513ms step_avg:60.27ms
step:1536/2225 train_time:92575ms step_avg:60.27ms
step:1537/2225 train_time:92636ms step_avg:60.27ms
step:1538/2225 train_time:92697ms step_avg:60.27ms
step:1539/2225 train_time:92760ms step_avg:60.27ms
step:1540/2225 train_time:92820ms step_avg:60.27ms
step:1541/2225 train_time:92882ms step_avg:60.27ms
step:1542/2225 train_time:92942ms step_avg:60.27ms
step:1543/2225 train_time:93003ms step_avg:60.27ms
step:1544/2225 train_time:93062ms step_avg:60.27ms
step:1545/2225 train_time:93123ms step_avg:60.27ms
step:1546/2225 train_time:93183ms step_avg:60.27ms
step:1547/2225 train_time:93245ms step_avg:60.27ms
step:1548/2225 train_time:93306ms step_avg:60.27ms
step:1549/2225 train_time:93367ms step_avg:60.28ms
step:1550/2225 train_time:93428ms step_avg:60.28ms
step:1551/2225 train_time:93490ms step_avg:60.28ms
step:1552/2225 train_time:93550ms step_avg:60.28ms
step:1553/2225 train_time:93612ms step_avg:60.28ms
step:1554/2225 train_time:93671ms step_avg:60.28ms
step:1555/2225 train_time:93735ms step_avg:60.28ms
step:1556/2225 train_time:93795ms step_avg:60.28ms
step:1557/2225 train_time:93857ms step_avg:60.28ms
step:1558/2225 train_time:93917ms step_avg:60.28ms
step:1559/2225 train_time:93978ms step_avg:60.28ms
step:1560/2225 train_time:94038ms step_avg:60.28ms
step:1561/2225 train_time:94100ms step_avg:60.28ms
step:1562/2225 train_time:94161ms step_avg:60.28ms
step:1563/2225 train_time:94222ms step_avg:60.28ms
step:1564/2225 train_time:94282ms step_avg:60.28ms
step:1565/2225 train_time:94344ms step_avg:60.28ms
step:1566/2225 train_time:94405ms step_avg:60.28ms
step:1567/2225 train_time:94467ms step_avg:60.29ms
step:1568/2225 train_time:94528ms step_avg:60.29ms
step:1569/2225 train_time:94590ms step_avg:60.29ms
step:1570/2225 train_time:94650ms step_avg:60.29ms
step:1571/2225 train_time:94711ms step_avg:60.29ms
step:1572/2225 train_time:94771ms step_avg:60.29ms
step:1573/2225 train_time:94833ms step_avg:60.29ms
step:1574/2225 train_time:94893ms step_avg:60.29ms
step:1575/2225 train_time:94954ms step_avg:60.29ms
step:1576/2225 train_time:95015ms step_avg:60.29ms
step:1577/2225 train_time:95077ms step_avg:60.29ms
step:1578/2225 train_time:95138ms step_avg:60.29ms
step:1579/2225 train_time:95200ms step_avg:60.29ms
step:1580/2225 train_time:95261ms step_avg:60.29ms
step:1581/2225 train_time:95322ms step_avg:60.29ms
step:1582/2225 train_time:95382ms step_avg:60.29ms
step:1583/2225 train_time:95444ms step_avg:60.29ms
step:1584/2225 train_time:95505ms step_avg:60.29ms
step:1585/2225 train_time:95566ms step_avg:60.29ms
step:1586/2225 train_time:95627ms step_avg:60.29ms
step:1587/2225 train_time:95688ms step_avg:60.29ms
step:1588/2225 train_time:95748ms step_avg:60.29ms
step:1589/2225 train_time:95810ms step_avg:60.30ms
step:1590/2225 train_time:95870ms step_avg:60.30ms
step:1591/2225 train_time:95931ms step_avg:60.30ms
step:1592/2225 train_time:95990ms step_avg:60.30ms
step:1593/2225 train_time:96052ms step_avg:60.30ms
step:1594/2225 train_time:96112ms step_avg:60.30ms
step:1595/2225 train_time:96174ms step_avg:60.30ms
step:1596/2225 train_time:96235ms step_avg:60.30ms
step:1597/2225 train_time:96297ms step_avg:60.30ms
step:1598/2225 train_time:96359ms step_avg:60.30ms
step:1599/2225 train_time:96421ms step_avg:60.30ms
step:1600/2225 train_time:96482ms step_avg:60.30ms
step:1601/2225 train_time:96544ms step_avg:60.30ms
step:1602/2225 train_time:96605ms step_avg:60.30ms
step:1603/2225 train_time:96666ms step_avg:60.30ms
step:1604/2225 train_time:96727ms step_avg:60.30ms
step:1605/2225 train_time:96788ms step_avg:60.30ms
step:1606/2225 train_time:96847ms step_avg:60.30ms
step:1607/2225 train_time:96909ms step_avg:60.30ms
step:1608/2225 train_time:96969ms step_avg:60.30ms
step:1609/2225 train_time:97030ms step_avg:60.30ms
step:1610/2225 train_time:97090ms step_avg:60.30ms
step:1611/2225 train_time:97151ms step_avg:60.30ms
step:1612/2225 train_time:97211ms step_avg:60.30ms
step:1613/2225 train_time:97274ms step_avg:60.31ms
step:1614/2225 train_time:97335ms step_avg:60.31ms
step:1615/2225 train_time:97398ms step_avg:60.31ms
step:1616/2225 train_time:97459ms step_avg:60.31ms
step:1617/2225 train_time:97521ms step_avg:60.31ms
step:1618/2225 train_time:97581ms step_avg:60.31ms
step:1619/2225 train_time:97642ms step_avg:60.31ms
step:1620/2225 train_time:97703ms step_avg:60.31ms
step:1621/2225 train_time:97764ms step_avg:60.31ms
step:1622/2225 train_time:97825ms step_avg:60.31ms
step:1623/2225 train_time:97886ms step_avg:60.31ms
step:1624/2225 train_time:97945ms step_avg:60.31ms
step:1625/2225 train_time:98007ms step_avg:60.31ms
step:1626/2225 train_time:98067ms step_avg:60.31ms
step:1627/2225 train_time:98129ms step_avg:60.31ms
step:1628/2225 train_time:98189ms step_avg:60.31ms
step:1629/2225 train_time:98250ms step_avg:60.31ms
step:1630/2225 train_time:98311ms step_avg:60.31ms
step:1631/2225 train_time:98372ms step_avg:60.31ms
step:1632/2225 train_time:98432ms step_avg:60.31ms
step:1633/2225 train_time:98494ms step_avg:60.32ms
step:1634/2225 train_time:98555ms step_avg:60.32ms
step:1635/2225 train_time:98617ms step_avg:60.32ms
step:1636/2225 train_time:98678ms step_avg:60.32ms
step:1637/2225 train_time:98741ms step_avg:60.32ms
step:1638/2225 train_time:98800ms step_avg:60.32ms
step:1639/2225 train_time:98862ms step_avg:60.32ms
step:1640/2225 train_time:98922ms step_avg:60.32ms
step:1641/2225 train_time:98984ms step_avg:60.32ms
step:1642/2225 train_time:99044ms step_avg:60.32ms
step:1643/2225 train_time:99106ms step_avg:60.32ms
step:1644/2225 train_time:99166ms step_avg:60.32ms
step:1645/2225 train_time:99227ms step_avg:60.32ms
step:1646/2225 train_time:99287ms step_avg:60.32ms
step:1647/2225 train_time:99349ms step_avg:60.32ms
step:1648/2225 train_time:99409ms step_avg:60.32ms
step:1649/2225 train_time:99470ms step_avg:60.32ms
step:1650/2225 train_time:99530ms step_avg:60.32ms
step:1651/2225 train_time:99592ms step_avg:60.32ms
step:1652/2225 train_time:99652ms step_avg:60.32ms
step:1653/2225 train_time:99714ms step_avg:60.32ms
step:1654/2225 train_time:99775ms step_avg:60.32ms
step:1655/2225 train_time:99838ms step_avg:60.32ms
step:1656/2225 train_time:99898ms step_avg:60.32ms
step:1657/2225 train_time:99960ms step_avg:60.33ms
step:1658/2225 train_time:100021ms step_avg:60.33ms
step:1659/2225 train_time:100082ms step_avg:60.33ms
step:1660/2225 train_time:100142ms step_avg:60.33ms
step:1661/2225 train_time:100203ms step_avg:60.33ms
step:1662/2225 train_time:100263ms step_avg:60.33ms
step:1663/2225 train_time:100325ms step_avg:60.33ms
step:1664/2225 train_time:100385ms step_avg:60.33ms
step:1665/2225 train_time:100447ms step_avg:60.33ms
step:1666/2225 train_time:100507ms step_avg:60.33ms
step:1667/2225 train_time:100569ms step_avg:60.33ms
step:1668/2225 train_time:100629ms step_avg:60.33ms
step:1669/2225 train_time:100690ms step_avg:60.33ms
step:1670/2225 train_time:100750ms step_avg:60.33ms
step:1671/2225 train_time:100812ms step_avg:60.33ms
step:1672/2225 train_time:100872ms step_avg:60.33ms
step:1673/2225 train_time:100934ms step_avg:60.33ms
step:1674/2225 train_time:100995ms step_avg:60.33ms
step:1675/2225 train_time:101057ms step_avg:60.33ms
step:1676/2225 train_time:101118ms step_avg:60.33ms
step:1677/2225 train_time:101181ms step_avg:60.33ms
step:1678/2225 train_time:101241ms step_avg:60.33ms
step:1679/2225 train_time:101302ms step_avg:60.33ms
step:1680/2225 train_time:101363ms step_avg:60.33ms
step:1681/2225 train_time:101425ms step_avg:60.34ms
step:1682/2225 train_time:101485ms step_avg:60.34ms
step:1683/2225 train_time:101547ms step_avg:60.34ms
step:1684/2225 train_time:101606ms step_avg:60.34ms
step:1685/2225 train_time:101668ms step_avg:60.34ms
step:1686/2225 train_time:101728ms step_avg:60.34ms
step:1687/2225 train_time:101790ms step_avg:60.34ms
step:1688/2225 train_time:101850ms step_avg:60.34ms
step:1689/2225 train_time:101912ms step_avg:60.34ms
step:1690/2225 train_time:101971ms step_avg:60.34ms
step:1691/2225 train_time:102034ms step_avg:60.34ms
step:1692/2225 train_time:102094ms step_avg:60.34ms
step:1693/2225 train_time:102157ms step_avg:60.34ms
step:1694/2225 train_time:102218ms step_avg:60.34ms
step:1695/2225 train_time:102280ms step_avg:60.34ms
step:1696/2225 train_time:102340ms step_avg:60.34ms
step:1697/2225 train_time:102402ms step_avg:60.34ms
step:1698/2225 train_time:102462ms step_avg:60.34ms
step:1699/2225 train_time:102525ms step_avg:60.34ms
step:1700/2225 train_time:102584ms step_avg:60.34ms
step:1701/2225 train_time:102646ms step_avg:60.34ms
step:1702/2225 train_time:102707ms step_avg:60.34ms
step:1703/2225 train_time:102767ms step_avg:60.34ms
step:1704/2225 train_time:102827ms step_avg:60.34ms
step:1705/2225 train_time:102889ms step_avg:60.35ms
step:1706/2225 train_time:102948ms step_avg:60.34ms
step:1707/2225 train_time:103011ms step_avg:60.35ms
step:1708/2225 train_time:103071ms step_avg:60.35ms
step:1709/2225 train_time:103132ms step_avg:60.35ms
step:1710/2225 train_time:103192ms step_avg:60.35ms
step:1711/2225 train_time:103254ms step_avg:60.35ms
step:1712/2225 train_time:103316ms step_avg:60.35ms
step:1713/2225 train_time:103378ms step_avg:60.35ms
step:1714/2225 train_time:103439ms step_avg:60.35ms
step:1715/2225 train_time:103502ms step_avg:60.35ms
step:1716/2225 train_time:103562ms step_avg:60.35ms
step:1717/2225 train_time:103625ms step_avg:60.35ms
step:1718/2225 train_time:103684ms step_avg:60.35ms
step:1719/2225 train_time:103746ms step_avg:60.35ms
step:1720/2225 train_time:103806ms step_avg:60.35ms
step:1721/2225 train_time:103868ms step_avg:60.35ms
step:1722/2225 train_time:103928ms step_avg:60.35ms
step:1723/2225 train_time:103990ms step_avg:60.35ms
step:1724/2225 train_time:104049ms step_avg:60.35ms
step:1725/2225 train_time:104111ms step_avg:60.35ms
step:1726/2225 train_time:104171ms step_avg:60.35ms
step:1727/2225 train_time:104233ms step_avg:60.35ms
step:1728/2225 train_time:104293ms step_avg:60.35ms
step:1729/2225 train_time:104355ms step_avg:60.36ms
step:1730/2225 train_time:104417ms step_avg:60.36ms
step:1731/2225 train_time:104479ms step_avg:60.36ms
step:1732/2225 train_time:104539ms step_avg:60.36ms
step:1733/2225 train_time:104602ms step_avg:60.36ms
step:1734/2225 train_time:104662ms step_avg:60.36ms
step:1735/2225 train_time:104723ms step_avg:60.36ms
step:1736/2225 train_time:104783ms step_avg:60.36ms
step:1737/2225 train_time:104845ms step_avg:60.36ms
step:1738/2225 train_time:104905ms step_avg:60.36ms
step:1739/2225 train_time:104967ms step_avg:60.36ms
step:1740/2225 train_time:105027ms step_avg:60.36ms
step:1741/2225 train_time:105089ms step_avg:60.36ms
step:1742/2225 train_time:105149ms step_avg:60.36ms
step:1743/2225 train_time:105210ms step_avg:60.36ms
step:1744/2225 train_time:105270ms step_avg:60.36ms
step:1745/2225 train_time:105332ms step_avg:60.36ms
step:1746/2225 train_time:105392ms step_avg:60.36ms
step:1747/2225 train_time:105454ms step_avg:60.36ms
step:1748/2225 train_time:105514ms step_avg:60.36ms
step:1749/2225 train_time:105577ms step_avg:60.36ms
step:1750/2225 train_time:105637ms step_avg:60.36ms
step:1750/2225 val_loss:3.3761 train_time:105700ms step_avg:60.40ms
step:1751/2225 train_time:105724ms step_avg:60.38ms
step:1752/2225 train_time:105762ms step_avg:60.37ms
step:1753/2225 train_time:105829ms step_avg:60.37ms
step:1754/2225 train_time:105890ms step_avg:60.37ms
step:1755/2225 train_time:105951ms step_avg:60.37ms
step:1756/2225 train_time:106011ms step_avg:60.37ms
step:1757/2225 train_time:106072ms step_avg:60.37ms
step:1758/2225 train_time:106131ms step_avg:60.37ms
step:1759/2225 train_time:106193ms step_avg:60.37ms
step:1760/2225 train_time:106253ms step_avg:60.37ms
step:1761/2225 train_time:106313ms step_avg:60.37ms
step:1762/2225 train_time:106372ms step_avg:60.37ms
step:1763/2225 train_time:106433ms step_avg:60.37ms
step:1764/2225 train_time:106492ms step_avg:60.37ms
step:1765/2225 train_time:106553ms step_avg:60.37ms
step:1766/2225 train_time:106614ms step_avg:60.37ms
step:1767/2225 train_time:106678ms step_avg:60.37ms
step:1768/2225 train_time:106742ms step_avg:60.37ms
step:1769/2225 train_time:106806ms step_avg:60.38ms
step:1770/2225 train_time:106866ms step_avg:60.38ms
step:1771/2225 train_time:106928ms step_avg:60.38ms
step:1772/2225 train_time:106988ms step_avg:60.38ms
step:1773/2225 train_time:107050ms step_avg:60.38ms
step:1774/2225 train_time:107110ms step_avg:60.38ms
step:1775/2225 train_time:107171ms step_avg:60.38ms
step:1776/2225 train_time:107230ms step_avg:60.38ms
step:1777/2225 train_time:107291ms step_avg:60.38ms
step:1778/2225 train_time:107351ms step_avg:60.38ms
step:1779/2225 train_time:107411ms step_avg:60.38ms
step:1780/2225 train_time:107470ms step_avg:60.38ms
step:1781/2225 train_time:107531ms step_avg:60.38ms
step:1782/2225 train_time:107591ms step_avg:60.38ms
step:1783/2225 train_time:107653ms step_avg:60.38ms
step:1784/2225 train_time:107713ms step_avg:60.38ms
step:1785/2225 train_time:107777ms step_avg:60.38ms
step:1786/2225 train_time:107839ms step_avg:60.38ms
step:1787/2225 train_time:107901ms step_avg:60.38ms
step:1788/2225 train_time:107961ms step_avg:60.38ms
step:1789/2225 train_time:108024ms step_avg:60.38ms
step:1790/2225 train_time:108085ms step_avg:60.38ms
step:1791/2225 train_time:108146ms step_avg:60.38ms
step:1792/2225 train_time:108206ms step_avg:60.38ms
step:1793/2225 train_time:108267ms step_avg:60.38ms
step:1794/2225 train_time:108327ms step_avg:60.38ms
step:1795/2225 train_time:108388ms step_avg:60.38ms
step:1796/2225 train_time:108447ms step_avg:60.38ms
step:1797/2225 train_time:108509ms step_avg:60.38ms
step:1798/2225 train_time:108569ms step_avg:60.38ms
step:1799/2225 train_time:108630ms step_avg:60.38ms
step:1800/2225 train_time:108690ms step_avg:60.38ms
step:1801/2225 train_time:108753ms step_avg:60.38ms
step:1802/2225 train_time:108813ms step_avg:60.38ms
step:1803/2225 train_time:108875ms step_avg:60.39ms
step:1804/2225 train_time:108935ms step_avg:60.39ms
step:1805/2225 train_time:108998ms step_avg:60.39ms
step:1806/2225 train_time:109059ms step_avg:60.39ms
step:1807/2225 train_time:109121ms step_avg:60.39ms
step:1808/2225 train_time:109182ms step_avg:60.39ms
step:1809/2225 train_time:109244ms step_avg:60.39ms
step:1810/2225 train_time:109304ms step_avg:60.39ms
step:1811/2225 train_time:109365ms step_avg:60.39ms
step:1812/2225 train_time:109425ms step_avg:60.39ms
step:1813/2225 train_time:109486ms step_avg:60.39ms
step:1814/2225 train_time:109547ms step_avg:60.39ms
step:1815/2225 train_time:109608ms step_avg:60.39ms
step:1816/2225 train_time:109669ms step_avg:60.39ms
step:1817/2225 train_time:109731ms step_avg:60.39ms
step:1818/2225 train_time:109791ms step_avg:60.39ms
step:1819/2225 train_time:109853ms step_avg:60.39ms
step:1820/2225 train_time:109912ms step_avg:60.39ms
step:1821/2225 train_time:109975ms step_avg:60.39ms
step:1822/2225 train_time:110034ms step_avg:60.39ms
step:1823/2225 train_time:110096ms step_avg:60.39ms
step:1824/2225 train_time:110157ms step_avg:60.39ms
step:1825/2225 train_time:110219ms step_avg:60.39ms
step:1826/2225 train_time:110280ms step_avg:60.39ms
step:1827/2225 train_time:110342ms step_avg:60.40ms
step:1828/2225 train_time:110402ms step_avg:60.39ms
step:1829/2225 train_time:110464ms step_avg:60.40ms
step:1830/2225 train_time:110524ms step_avg:60.40ms
step:1831/2225 train_time:110586ms step_avg:60.40ms
step:1832/2225 train_time:110646ms step_avg:60.40ms
step:1833/2225 train_time:110708ms step_avg:60.40ms
step:1834/2225 train_time:110768ms step_avg:60.40ms
step:1835/2225 train_time:110830ms step_avg:60.40ms
step:1836/2225 train_time:110890ms step_avg:60.40ms
step:1837/2225 train_time:110952ms step_avg:60.40ms
step:1838/2225 train_time:111011ms step_avg:60.40ms
step:1839/2225 train_time:111073ms step_avg:60.40ms
step:1840/2225 train_time:111133ms step_avg:60.40ms
step:1841/2225 train_time:111195ms step_avg:60.40ms
step:1842/2225 train_time:111255ms step_avg:60.40ms
step:1843/2225 train_time:111317ms step_avg:60.40ms
step:1844/2225 train_time:111378ms step_avg:60.40ms
step:1845/2225 train_time:111440ms step_avg:60.40ms
step:1846/2225 train_time:111500ms step_avg:60.40ms
step:1847/2225 train_time:111562ms step_avg:60.40ms
step:1848/2225 train_time:111623ms step_avg:60.40ms
step:1849/2225 train_time:111685ms step_avg:60.40ms
step:1850/2225 train_time:111746ms step_avg:60.40ms
step:1851/2225 train_time:111807ms step_avg:60.40ms
step:1852/2225 train_time:111867ms step_avg:60.40ms
step:1853/2225 train_time:111929ms step_avg:60.40ms
step:1854/2225 train_time:111988ms step_avg:60.40ms
step:1855/2225 train_time:112051ms step_avg:60.40ms
step:1856/2225 train_time:112110ms step_avg:60.40ms
step:1857/2225 train_time:112173ms step_avg:60.41ms
step:1858/2225 train_time:112232ms step_avg:60.40ms
step:1859/2225 train_time:112294ms step_avg:60.41ms
step:1860/2225 train_time:112354ms step_avg:60.41ms
step:1861/2225 train_time:112416ms step_avg:60.41ms
step:1862/2225 train_time:112476ms step_avg:60.41ms
step:1863/2225 train_time:112538ms step_avg:60.41ms
step:1864/2225 train_time:112599ms step_avg:60.41ms
step:1865/2225 train_time:112661ms step_avg:60.41ms
step:1866/2225 train_time:112722ms step_avg:60.41ms
step:1867/2225 train_time:112785ms step_avg:60.41ms
step:1868/2225 train_time:112845ms step_avg:60.41ms
step:1869/2225 train_time:112906ms step_avg:60.41ms
step:1870/2225 train_time:112967ms step_avg:60.41ms
step:1871/2225 train_time:113029ms step_avg:60.41ms
step:1872/2225 train_time:113089ms step_avg:60.41ms
step:1873/2225 train_time:113151ms step_avg:60.41ms
step:1874/2225 train_time:113210ms step_avg:60.41ms
step:1875/2225 train_time:113271ms step_avg:60.41ms
step:1876/2225 train_time:113331ms step_avg:60.41ms
step:1877/2225 train_time:113392ms step_avg:60.41ms
step:1878/2225 train_time:113452ms step_avg:60.41ms
step:1879/2225 train_time:113514ms step_avg:60.41ms
step:1880/2225 train_time:113574ms step_avg:60.41ms
step:1881/2225 train_time:113637ms step_avg:60.41ms
step:1882/2225 train_time:113697ms step_avg:60.41ms
step:1883/2225 train_time:113760ms step_avg:60.41ms
step:1884/2225 train_time:113820ms step_avg:60.41ms
step:1885/2225 train_time:113883ms step_avg:60.42ms
step:1886/2225 train_time:113943ms step_avg:60.42ms
step:1887/2225 train_time:114005ms step_avg:60.42ms
step:1888/2225 train_time:114065ms step_avg:60.42ms
step:1889/2225 train_time:114127ms step_avg:60.42ms
step:1890/2225 train_time:114188ms step_avg:60.42ms
step:1891/2225 train_time:114249ms step_avg:60.42ms
step:1892/2225 train_time:114308ms step_avg:60.42ms
step:1893/2225 train_time:114370ms step_avg:60.42ms
step:1894/2225 train_time:114430ms step_avg:60.42ms
step:1895/2225 train_time:114492ms step_avg:60.42ms
step:1896/2225 train_time:114552ms step_avg:60.42ms
step:1897/2225 train_time:114613ms step_avg:60.42ms
step:1898/2225 train_time:114673ms step_avg:60.42ms
step:1899/2225 train_time:114736ms step_avg:60.42ms
step:1900/2225 train_time:114797ms step_avg:60.42ms
step:1901/2225 train_time:114860ms step_avg:60.42ms
step:1902/2225 train_time:114921ms step_avg:60.42ms
step:1903/2225 train_time:114984ms step_avg:60.42ms
step:1904/2225 train_time:115044ms step_avg:60.42ms
step:1905/2225 train_time:115106ms step_avg:60.42ms
step:1906/2225 train_time:115166ms step_avg:60.42ms
step:1907/2225 train_time:115227ms step_avg:60.42ms
step:1908/2225 train_time:115287ms step_avg:60.42ms
step:1909/2225 train_time:115348ms step_avg:60.42ms
step:1910/2225 train_time:115409ms step_avg:60.42ms
step:1911/2225 train_time:115471ms step_avg:60.42ms
step:1912/2225 train_time:115530ms step_avg:60.42ms
step:1913/2225 train_time:115592ms step_avg:60.42ms
step:1914/2225 train_time:115652ms step_avg:60.42ms
step:1915/2225 train_time:115713ms step_avg:60.42ms
step:1916/2225 train_time:115773ms step_avg:60.42ms
step:1917/2225 train_time:115835ms step_avg:60.43ms
step:1918/2225 train_time:115896ms step_avg:60.43ms
step:1919/2225 train_time:115958ms step_avg:60.43ms
step:1920/2225 train_time:116019ms step_avg:60.43ms
step:1921/2225 train_time:116082ms step_avg:60.43ms
step:1922/2225 train_time:116142ms step_avg:60.43ms
step:1923/2225 train_time:116204ms step_avg:60.43ms
step:1924/2225 train_time:116265ms step_avg:60.43ms
step:1925/2225 train_time:116326ms step_avg:60.43ms
step:1926/2225 train_time:116387ms step_avg:60.43ms
step:1927/2225 train_time:116449ms step_avg:60.43ms
step:1928/2225 train_time:116508ms step_avg:60.43ms
step:1929/2225 train_time:116570ms step_avg:60.43ms
step:1930/2225 train_time:116629ms step_avg:60.43ms
step:1931/2225 train_time:116692ms step_avg:60.43ms
step:1932/2225 train_time:116753ms step_avg:60.43ms
step:1933/2225 train_time:116814ms step_avg:60.43ms
step:1934/2225 train_time:116874ms step_avg:60.43ms
step:1935/2225 train_time:116935ms step_avg:60.43ms
step:1936/2225 train_time:116995ms step_avg:60.43ms
step:1937/2225 train_time:117057ms step_avg:60.43ms
step:1938/2225 train_time:117118ms step_avg:60.43ms
step:1939/2225 train_time:117181ms step_avg:60.43ms
step:1940/2225 train_time:117241ms step_avg:60.43ms
step:1941/2225 train_time:117303ms step_avg:60.43ms
step:1942/2225 train_time:117363ms step_avg:60.43ms
step:1943/2225 train_time:117425ms step_avg:60.43ms
step:1944/2225 train_time:117486ms step_avg:60.44ms
step:1945/2225 train_time:117547ms step_avg:60.44ms
step:1946/2225 train_time:117607ms step_avg:60.44ms
step:1947/2225 train_time:117669ms step_avg:60.44ms
step:1948/2225 train_time:117729ms step_avg:60.44ms
step:1949/2225 train_time:117791ms step_avg:60.44ms
step:1950/2225 train_time:117851ms step_avg:60.44ms
step:1951/2225 train_time:117912ms step_avg:60.44ms
step:1952/2225 train_time:117972ms step_avg:60.44ms
step:1953/2225 train_time:118033ms step_avg:60.44ms
step:1954/2225 train_time:118093ms step_avg:60.44ms
step:1955/2225 train_time:118156ms step_avg:60.44ms
step:1956/2225 train_time:118216ms step_avg:60.44ms
step:1957/2225 train_time:118279ms step_avg:60.44ms
step:1958/2225 train_time:118340ms step_avg:60.44ms
step:1959/2225 train_time:118403ms step_avg:60.44ms
step:1960/2225 train_time:118462ms step_avg:60.44ms
step:1961/2225 train_time:118524ms step_avg:60.44ms
step:1962/2225 train_time:118584ms step_avg:60.44ms
step:1963/2225 train_time:118646ms step_avg:60.44ms
step:1964/2225 train_time:118706ms step_avg:60.44ms
step:1965/2225 train_time:118768ms step_avg:60.44ms
step:1966/2225 train_time:118828ms step_avg:60.44ms
step:1967/2225 train_time:118889ms step_avg:60.44ms
step:1968/2225 train_time:118949ms step_avg:60.44ms
step:1969/2225 train_time:119011ms step_avg:60.44ms
step:1970/2225 train_time:119071ms step_avg:60.44ms
step:1971/2225 train_time:119132ms step_avg:60.44ms
step:1972/2225 train_time:119192ms step_avg:60.44ms
step:1973/2225 train_time:119254ms step_avg:60.44ms
step:1974/2225 train_time:119314ms step_avg:60.44ms
step:1975/2225 train_time:119376ms step_avg:60.44ms
step:1976/2225 train_time:119437ms step_avg:60.44ms
step:1977/2225 train_time:119499ms step_avg:60.44ms
step:1978/2225 train_time:119559ms step_avg:60.44ms
step:1979/2225 train_time:119622ms step_avg:60.45ms
step:1980/2225 train_time:119683ms step_avg:60.45ms
step:1981/2225 train_time:119745ms step_avg:60.45ms
step:1982/2225 train_time:119806ms step_avg:60.45ms
step:1983/2225 train_time:119868ms step_avg:60.45ms
step:1984/2225 train_time:119927ms step_avg:60.45ms
step:1985/2225 train_time:119989ms step_avg:60.45ms
step:1986/2225 train_time:120049ms step_avg:60.45ms
step:1987/2225 train_time:120110ms step_avg:60.45ms
step:1988/2225 train_time:120170ms step_avg:60.45ms
step:1989/2225 train_time:120232ms step_avg:60.45ms
step:1990/2225 train_time:120292ms step_avg:60.45ms
step:1991/2225 train_time:120354ms step_avg:60.45ms
step:1992/2225 train_time:120414ms step_avg:60.45ms
step:1993/2225 train_time:120476ms step_avg:60.45ms
step:1994/2225 train_time:120537ms step_avg:60.45ms
step:1995/2225 train_time:120600ms step_avg:60.45ms
step:1996/2225 train_time:120661ms step_avg:60.45ms
step:1997/2225 train_time:120723ms step_avg:60.45ms
step:1998/2225 train_time:120784ms step_avg:60.45ms
step:1999/2225 train_time:120845ms step_avg:60.45ms
step:2000/2225 train_time:120905ms step_avg:60.45ms
step:2000/2225 val_loss:3.3216 train_time:120967ms step_avg:60.48ms
step:2001/2225 train_time:120990ms step_avg:60.47ms
step:2002/2225 train_time:121032ms step_avg:60.46ms
step:2003/2225 train_time:121097ms step_avg:60.46ms
step:2004/2225 train_time:121160ms step_avg:60.46ms
step:2005/2225 train_time:121222ms step_avg:60.46ms
step:2006/2225 train_time:121283ms step_avg:60.46ms
step:2007/2225 train_time:121345ms step_avg:60.46ms
step:2008/2225 train_time:121405ms step_avg:60.46ms
step:2009/2225 train_time:121467ms step_avg:60.46ms
step:2010/2225 train_time:121526ms step_avg:60.46ms
step:2011/2225 train_time:121587ms step_avg:60.46ms
step:2012/2225 train_time:121647ms step_avg:60.46ms
step:2013/2225 train_time:121708ms step_avg:60.46ms
step:2014/2225 train_time:121767ms step_avg:60.46ms
step:2015/2225 train_time:121827ms step_avg:60.46ms
step:2016/2225 train_time:121887ms step_avg:60.46ms
step:2017/2225 train_time:121950ms step_avg:60.46ms
step:2018/2225 train_time:122012ms step_avg:60.46ms
step:2019/2225 train_time:122077ms step_avg:60.46ms
step:2020/2225 train_time:122137ms step_avg:60.46ms
step:2021/2225 train_time:122200ms step_avg:60.46ms
step:2022/2225 train_time:122260ms step_avg:60.46ms
step:2023/2225 train_time:122322ms step_avg:60.47ms
step:2024/2225 train_time:122382ms step_avg:60.47ms
step:2025/2225 train_time:122444ms step_avg:60.47ms
step:2026/2225 train_time:122504ms step_avg:60.47ms
step:2027/2225 train_time:122566ms step_avg:60.47ms
step:2028/2225 train_time:122626ms step_avg:60.47ms
step:2029/2225 train_time:122686ms step_avg:60.47ms
step:2030/2225 train_time:122746ms step_avg:60.47ms
step:2031/2225 train_time:122807ms step_avg:60.47ms
step:2032/2225 train_time:122867ms step_avg:60.47ms
step:2033/2225 train_time:122929ms step_avg:60.47ms
step:2034/2225 train_time:122990ms step_avg:60.47ms
step:2035/2225 train_time:123053ms step_avg:60.47ms
step:2036/2225 train_time:123114ms step_avg:60.47ms
step:2037/2225 train_time:123176ms step_avg:60.47ms
step:2038/2225 train_time:123236ms step_avg:60.47ms
step:2039/2225 train_time:123298ms step_avg:60.47ms
step:2040/2225 train_time:123358ms step_avg:60.47ms
step:2041/2225 train_time:123420ms step_avg:60.47ms
step:2042/2225 train_time:123480ms step_avg:60.47ms
step:2043/2225 train_time:123542ms step_avg:60.47ms
step:2044/2225 train_time:123602ms step_avg:60.47ms
step:2045/2225 train_time:123664ms step_avg:60.47ms
step:2046/2225 train_time:123723ms step_avg:60.47ms
step:2047/2225 train_time:123784ms step_avg:60.47ms
step:2048/2225 train_time:123844ms step_avg:60.47ms
step:2049/2225 train_time:123907ms step_avg:60.47ms
step:2050/2225 train_time:123968ms step_avg:60.47ms
step:2051/2225 train_time:124031ms step_avg:60.47ms
step:2052/2225 train_time:124092ms step_avg:60.47ms
step:2053/2225 train_time:124153ms step_avg:60.47ms
step:2054/2225 train_time:124214ms step_avg:60.47ms
step:2055/2225 train_time:124276ms step_avg:60.47ms
step:2056/2225 train_time:124335ms step_avg:60.47ms
step:2057/2225 train_time:124397ms step_avg:60.47ms
step:2058/2225 train_time:124456ms step_avg:60.47ms
step:2059/2225 train_time:124518ms step_avg:60.47ms
step:2060/2225 train_time:124577ms step_avg:60.47ms
step:2061/2225 train_time:124639ms step_avg:60.47ms
step:2062/2225 train_time:124698ms step_avg:60.47ms
step:2063/2225 train_time:124760ms step_avg:60.48ms
step:2064/2225 train_time:124820ms step_avg:60.47ms
step:2065/2225 train_time:124883ms step_avg:60.48ms
step:2066/2225 train_time:124944ms step_avg:60.48ms
step:2067/2225 train_time:125008ms step_avg:60.48ms
step:2068/2225 train_time:125069ms step_avg:60.48ms
step:2069/2225 train_time:125131ms step_avg:60.48ms
step:2070/2225 train_time:125191ms step_avg:60.48ms
step:2071/2225 train_time:125253ms step_avg:60.48ms
step:2072/2225 train_time:125314ms step_avg:60.48ms
step:2073/2225 train_time:125375ms step_avg:60.48ms
step:2074/2225 train_time:125435ms step_avg:60.48ms
step:2075/2225 train_time:125496ms step_avg:60.48ms
step:2076/2225 train_time:125555ms step_avg:60.48ms
step:2077/2225 train_time:125616ms step_avg:60.48ms
step:2078/2225 train_time:125676ms step_avg:60.48ms
step:2079/2225 train_time:125737ms step_avg:60.48ms
step:2080/2225 train_time:125797ms step_avg:60.48ms
step:2081/2225 train_time:125861ms step_avg:60.48ms
step:2082/2225 train_time:125921ms step_avg:60.48ms
step:2083/2225 train_time:125984ms step_avg:60.48ms
step:2084/2225 train_time:126045ms step_avg:60.48ms
step:2085/2225 train_time:126108ms step_avg:60.48ms
step:2086/2225 train_time:126168ms step_avg:60.48ms
step:2087/2225 train_time:126230ms step_avg:60.48ms
step:2088/2225 train_time:126291ms step_avg:60.48ms
step:2089/2225 train_time:126352ms step_avg:60.48ms
step:2090/2225 train_time:126412ms step_avg:60.48ms
step:2091/2225 train_time:126474ms step_avg:60.48ms
step:2092/2225 train_time:126533ms step_avg:60.48ms
step:2093/2225 train_time:126594ms step_avg:60.48ms
step:2094/2225 train_time:126655ms step_avg:60.48ms
step:2095/2225 train_time:126717ms step_avg:60.49ms
step:2096/2225 train_time:126777ms step_avg:60.49ms
step:2097/2225 train_time:126838ms step_avg:60.49ms
step:2098/2225 train_time:126898ms step_avg:60.49ms
step:2099/2225 train_time:126959ms step_avg:60.49ms
step:2100/2225 train_time:127020ms step_avg:60.49ms
step:2101/2225 train_time:127083ms step_avg:60.49ms
step:2102/2225 train_time:127144ms step_avg:60.49ms
step:2103/2225 train_time:127206ms step_avg:60.49ms
step:2104/2225 train_time:127267ms step_avg:60.49ms
step:2105/2225 train_time:127329ms step_avg:60.49ms
step:2106/2225 train_time:127389ms step_avg:60.49ms
step:2107/2225 train_time:127450ms step_avg:60.49ms
step:2108/2225 train_time:127511ms step_avg:60.49ms
step:2109/2225 train_time:127572ms step_avg:60.49ms
step:2110/2225 train_time:127632ms step_avg:60.49ms
step:2111/2225 train_time:127694ms step_avg:60.49ms
step:2112/2225 train_time:127754ms step_avg:60.49ms
step:2113/2225 train_time:127815ms step_avg:60.49ms
step:2114/2225 train_time:127875ms step_avg:60.49ms
step:2115/2225 train_time:127936ms step_avg:60.49ms
step:2116/2225 train_time:127996ms step_avg:60.49ms
step:2117/2225 train_time:128058ms step_avg:60.49ms
step:2118/2225 train_time:128118ms step_avg:60.49ms
step:2119/2225 train_time:128181ms step_avg:60.49ms
step:2120/2225 train_time:128242ms step_avg:60.49ms
step:2121/2225 train_time:128305ms step_avg:60.49ms
step:2122/2225 train_time:128366ms step_avg:60.49ms
step:2123/2225 train_time:128428ms step_avg:60.49ms
step:2124/2225 train_time:128488ms step_avg:60.49ms
step:2125/2225 train_time:128550ms step_avg:60.49ms
step:2126/2225 train_time:128610ms step_avg:60.49ms
step:2127/2225 train_time:128672ms step_avg:60.49ms
step:2128/2225 train_time:128733ms step_avg:60.49ms
step:2129/2225 train_time:128794ms step_avg:60.50ms
step:2130/2225 train_time:128854ms step_avg:60.49ms
step:2131/2225 train_time:128916ms step_avg:60.50ms
step:2132/2225 train_time:128976ms step_avg:60.50ms
step:2133/2225 train_time:129037ms step_avg:60.50ms
step:2134/2225 train_time:129097ms step_avg:60.50ms
step:2135/2225 train_time:129159ms step_avg:60.50ms
step:2136/2225 train_time:129220ms step_avg:60.50ms
step:2137/2225 train_time:129282ms step_avg:60.50ms
step:2138/2225 train_time:129343ms step_avg:60.50ms
step:2139/2225 train_time:129406ms step_avg:60.50ms
step:2140/2225 train_time:129466ms step_avg:60.50ms
step:2141/2225 train_time:129529ms step_avg:60.50ms
step:2142/2225 train_time:129589ms step_avg:60.50ms
step:2143/2225 train_time:129650ms step_avg:60.50ms
step:2144/2225 train_time:129711ms step_avg:60.50ms
step:2145/2225 train_time:129772ms step_avg:60.50ms
step:2146/2225 train_time:129832ms step_avg:60.50ms
step:2147/2225 train_time:129894ms step_avg:60.50ms
step:2148/2225 train_time:129954ms step_avg:60.50ms
step:2149/2225 train_time:130015ms step_avg:60.50ms
step:2150/2225 train_time:130076ms step_avg:60.50ms
step:2151/2225 train_time:130137ms step_avg:60.50ms
step:2152/2225 train_time:130197ms step_avg:60.50ms
step:2153/2225 train_time:130259ms step_avg:60.50ms
step:2154/2225 train_time:130319ms step_avg:60.50ms
step:2155/2225 train_time:130420ms step_avg:60.52ms
step:2156/2225 train_time:130443ms step_avg:60.50ms
step:2157/2225 train_time:130505ms step_avg:60.50ms
step:2158/2225 train_time:130566ms step_avg:60.50ms
step:2159/2225 train_time:130629ms step_avg:60.50ms
step:2160/2225 train_time:130688ms step_avg:60.50ms
step:2161/2225 train_time:130750ms step_avg:60.50ms
step:2162/2225 train_time:130810ms step_avg:60.50ms
step:2163/2225 train_time:130873ms step_avg:60.51ms
step:2164/2225 train_time:130933ms step_avg:60.51ms
step:2165/2225 train_time:130994ms step_avg:60.51ms
step:2166/2225 train_time:131054ms step_avg:60.51ms
step:2167/2225 train_time:131116ms step_avg:60.51ms
step:2168/2225 train_time:131176ms step_avg:60.51ms
step:2169/2225 train_time:131237ms step_avg:60.51ms
step:2170/2225 train_time:131297ms step_avg:60.51ms
step:2171/2225 train_time:131358ms step_avg:60.51ms
step:2172/2225 train_time:131418ms step_avg:60.51ms
step:2173/2225 train_time:131481ms step_avg:60.51ms
step:2174/2225 train_time:131542ms step_avg:60.51ms
step:2175/2225 train_time:131604ms step_avg:60.51ms
step:2176/2225 train_time:131665ms step_avg:60.51ms
step:2177/2225 train_time:131728ms step_avg:60.51ms
step:2178/2225 train_time:131788ms step_avg:60.51ms
step:2179/2225 train_time:131849ms step_avg:60.51ms
step:2180/2225 train_time:131910ms step_avg:60.51ms
step:2181/2225 train_time:131972ms step_avg:60.51ms
step:2182/2225 train_time:132032ms step_avg:60.51ms
step:2183/2225 train_time:132094ms step_avg:60.51ms
step:2184/2225 train_time:132154ms step_avg:60.51ms
step:2185/2225 train_time:132216ms step_avg:60.51ms
step:2186/2225 train_time:132276ms step_avg:60.51ms
step:2187/2225 train_time:132337ms step_avg:60.51ms
step:2188/2225 train_time:132397ms step_avg:60.51ms
step:2189/2225 train_time:132459ms step_avg:60.51ms
step:2190/2225 train_time:132519ms step_avg:60.51ms
step:2191/2225 train_time:132581ms step_avg:60.51ms
step:2192/2225 train_time:132643ms step_avg:60.51ms
step:2193/2225 train_time:132706ms step_avg:60.51ms
step:2194/2225 train_time:132767ms step_avg:60.51ms
step:2195/2225 train_time:132829ms step_avg:60.51ms
step:2196/2225 train_time:132889ms step_avg:60.51ms
step:2197/2225 train_time:132951ms step_avg:60.51ms
step:2198/2225 train_time:133012ms step_avg:60.51ms
step:2199/2225 train_time:133073ms step_avg:60.52ms
step:2200/2225 train_time:133133ms step_avg:60.52ms
step:2201/2225 train_time:133195ms step_avg:60.52ms
step:2202/2225 train_time:133255ms step_avg:60.52ms
step:2203/2225 train_time:133316ms step_avg:60.52ms
step:2204/2225 train_time:133377ms step_avg:60.52ms
step:2205/2225 train_time:133438ms step_avg:60.52ms
step:2206/2225 train_time:133498ms step_avg:60.52ms
step:2207/2225 train_time:133560ms step_avg:60.52ms
step:2208/2225 train_time:133621ms step_avg:60.52ms
step:2209/2225 train_time:133684ms step_avg:60.52ms
step:2210/2225 train_time:133744ms step_avg:60.52ms
step:2211/2225 train_time:133807ms step_avg:60.52ms
step:2212/2225 train_time:133867ms step_avg:60.52ms
step:2213/2225 train_time:133930ms step_avg:60.52ms
step:2214/2225 train_time:133991ms step_avg:60.52ms
step:2215/2225 train_time:134053ms step_avg:60.52ms
step:2216/2225 train_time:134113ms step_avg:60.52ms
step:2217/2225 train_time:134174ms step_avg:60.52ms
step:2218/2225 train_time:134234ms step_avg:60.52ms
step:2219/2225 train_time:134296ms step_avg:60.52ms
step:2220/2225 train_time:134356ms step_avg:60.52ms
step:2221/2225 train_time:134418ms step_avg:60.52ms
step:2222/2225 train_time:134478ms step_avg:60.52ms
step:2223/2225 train_time:134539ms step_avg:60.52ms
step:2224/2225 train_time:134599ms step_avg:60.52ms
step:2225/2225 train_time:134661ms step_avg:60.52ms
step:2225/2225 val_loss:3.2800 train_time:134722ms step_avg:60.55ms
peak memory allocated: 29249 MiB reserved: 47336 MiB

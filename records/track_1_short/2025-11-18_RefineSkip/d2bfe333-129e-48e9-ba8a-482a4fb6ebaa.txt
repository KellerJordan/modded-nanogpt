import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i ==7:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections[0]
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i ==4:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2185  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 18 21:25:16 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   36C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   37C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   38C    P0            126W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   36C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   33C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          147435      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          147436      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          147437      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          147438      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          147439      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          147440      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          147441      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          147442      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          147436      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          147437      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          147438      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          147439      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          147440      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          147441      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          147442      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2225 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2225 train_time:135ms step_avg:135.42ms
step:2/2225 train_time:182ms step_avg:90.79ms
step:3/2225 train_time:206ms step_avg:68.76ms
step:4/2225 train_time:252ms step_avg:63.07ms
step:5/2225 train_time:310ms step_avg:62.10ms
step:6/2225 train_time:506ms step_avg:84.31ms
step:7/2225 train_time:544ms step_avg:77.72ms
step:8/2225 train_time:602ms step_avg:75.27ms
step:9/2225 train_time:662ms step_avg:73.53ms
step:10/2225 train_time:721ms step_avg:72.06ms
step:11/2225 train_time:781ms step_avg:70.99ms
step:12/2225 train_time:840ms step_avg:69.99ms
step:13/2225 train_time:900ms step_avg:69.24ms
step:14/2225 train_time:959ms step_avg:68.50ms
step:15/2225 train_time:1019ms step_avg:67.93ms
step:16/2225 train_time:1078ms step_avg:67.35ms
step:17/2225 train_time:1138ms step_avg:66.94ms
step:18/2225 train_time:1197ms step_avg:66.50ms
step:19/2225 train_time:1257ms step_avg:66.16ms
step:20/2225 train_time:1316ms step_avg:65.80ms
step:21/2225 train_time:1377ms step_avg:65.59ms
step:22/2225 train_time:1442ms step_avg:65.53ms
step:23/2225 train_time:1510ms step_avg:65.65ms
step:24/2225 train_time:1572ms step_avg:65.49ms
step:25/2225 train_time:1634ms step_avg:65.35ms
step:26/2225 train_time:1693ms step_avg:65.13ms
step:27/2225 train_time:1755ms step_avg:64.98ms
step:28/2225 train_time:1814ms step_avg:64.77ms
step:29/2225 train_time:1874ms step_avg:64.63ms
step:30/2225 train_time:1933ms step_avg:64.43ms
step:31/2225 train_time:1993ms step_avg:64.29ms
step:32/2225 train_time:2052ms step_avg:64.12ms
step:33/2225 train_time:2113ms step_avg:64.02ms
step:34/2225 train_time:2171ms step_avg:63.86ms
step:35/2225 train_time:2232ms step_avg:63.76ms
step:36/2225 train_time:2291ms step_avg:63.63ms
step:37/2225 train_time:2352ms step_avg:63.58ms
step:38/2225 train_time:2414ms step_avg:63.51ms
step:39/2225 train_time:2477ms step_avg:63.52ms
step:40/2225 train_time:2538ms step_avg:63.45ms
step:41/2225 train_time:2600ms step_avg:63.42ms
step:42/2225 train_time:2660ms step_avg:63.33ms
step:43/2225 train_time:2721ms step_avg:63.29ms
step:44/2225 train_time:2781ms step_avg:63.20ms
step:45/2225 train_time:2842ms step_avg:63.17ms
step:46/2225 train_time:2902ms step_avg:63.08ms
step:47/2225 train_time:2963ms step_avg:63.03ms
step:48/2225 train_time:3022ms step_avg:62.95ms
step:49/2225 train_time:3083ms step_avg:62.91ms
step:50/2225 train_time:3142ms step_avg:62.85ms
step:51/2225 train_time:3203ms step_avg:62.80ms
step:52/2225 train_time:3262ms step_avg:62.74ms
step:53/2225 train_time:3323ms step_avg:62.70ms
step:54/2225 train_time:3383ms step_avg:62.65ms
step:55/2225 train_time:3445ms step_avg:62.64ms
step:56/2225 train_time:3505ms step_avg:62.59ms
step:57/2225 train_time:3566ms step_avg:62.57ms
step:58/2225 train_time:3626ms step_avg:62.51ms
step:59/2225 train_time:3686ms step_avg:62.48ms
step:60/2225 train_time:3746ms step_avg:62.43ms
step:61/2225 train_time:3807ms step_avg:62.41ms
step:62/2225 train_time:3866ms step_avg:62.35ms
step:63/2225 train_time:3927ms step_avg:62.33ms
step:64/2225 train_time:3986ms step_avg:62.27ms
step:65/2225 train_time:4046ms step_avg:62.25ms
step:66/2225 train_time:4106ms step_avg:62.21ms
step:67/2225 train_time:4166ms step_avg:62.18ms
step:68/2225 train_time:4225ms step_avg:62.14ms
step:69/2225 train_time:4285ms step_avg:62.11ms
step:70/2225 train_time:4345ms step_avg:62.07ms
step:71/2225 train_time:4407ms step_avg:62.07ms
step:72/2225 train_time:4467ms step_avg:62.04ms
step:73/2225 train_time:4527ms step_avg:62.02ms
step:74/2225 train_time:4587ms step_avg:61.98ms
step:75/2225 train_time:4648ms step_avg:61.97ms
step:76/2225 train_time:4707ms step_avg:61.94ms
step:77/2225 train_time:4768ms step_avg:61.92ms
step:78/2225 train_time:4827ms step_avg:61.88ms
step:79/2225 train_time:4887ms step_avg:61.86ms
step:80/2225 train_time:4946ms step_avg:61.82ms
step:81/2225 train_time:5006ms step_avg:61.81ms
step:82/2225 train_time:5065ms step_avg:61.77ms
step:83/2225 train_time:5125ms step_avg:61.75ms
step:84/2225 train_time:5184ms step_avg:61.71ms
step:85/2225 train_time:5245ms step_avg:61.70ms
step:86/2225 train_time:5304ms step_avg:61.68ms
step:87/2225 train_time:5365ms step_avg:61.67ms
step:88/2225 train_time:5425ms step_avg:61.65ms
step:89/2225 train_time:5486ms step_avg:61.64ms
step:90/2225 train_time:5545ms step_avg:61.61ms
step:91/2225 train_time:5606ms step_avg:61.60ms
step:92/2225 train_time:5665ms step_avg:61.58ms
step:93/2225 train_time:5726ms step_avg:61.57ms
step:94/2225 train_time:5785ms step_avg:61.54ms
step:95/2225 train_time:5846ms step_avg:61.53ms
step:96/2225 train_time:5906ms step_avg:61.52ms
step:97/2225 train_time:5966ms step_avg:61.50ms
step:98/2225 train_time:6025ms step_avg:61.48ms
step:99/2225 train_time:6085ms step_avg:61.46ms
step:100/2225 train_time:6144ms step_avg:61.44ms
step:101/2225 train_time:6205ms step_avg:61.43ms
step:102/2225 train_time:6264ms step_avg:61.41ms
step:103/2225 train_time:6324ms step_avg:61.40ms
step:104/2225 train_time:6383ms step_avg:61.38ms
step:105/2225 train_time:6445ms step_avg:61.38ms
step:106/2225 train_time:6504ms step_avg:61.36ms
step:107/2225 train_time:6565ms step_avg:61.35ms
step:108/2225 train_time:6625ms step_avg:61.34ms
step:109/2225 train_time:6685ms step_avg:61.33ms
step:110/2225 train_time:6744ms step_avg:61.31ms
step:111/2225 train_time:6805ms step_avg:61.31ms
step:112/2225 train_time:6864ms step_avg:61.29ms
step:113/2225 train_time:6925ms step_avg:61.28ms
step:114/2225 train_time:6984ms step_avg:61.26ms
step:115/2225 train_time:7045ms step_avg:61.26ms
step:116/2225 train_time:7104ms step_avg:61.24ms
step:117/2225 train_time:7165ms step_avg:61.24ms
step:118/2225 train_time:7224ms step_avg:61.22ms
step:119/2225 train_time:7284ms step_avg:61.21ms
step:120/2225 train_time:7343ms step_avg:61.19ms
step:121/2225 train_time:7404ms step_avg:61.19ms
step:122/2225 train_time:7463ms step_avg:61.17ms
step:123/2225 train_time:7524ms step_avg:61.17ms
step:124/2225 train_time:7583ms step_avg:61.16ms
step:125/2225 train_time:7645ms step_avg:61.16ms
step:126/2225 train_time:7704ms step_avg:61.15ms
step:127/2225 train_time:7765ms step_avg:61.14ms
step:128/2225 train_time:7824ms step_avg:61.12ms
step:129/2225 train_time:7885ms step_avg:61.12ms
step:130/2225 train_time:7944ms step_avg:61.11ms
step:131/2225 train_time:8004ms step_avg:61.10ms
step:132/2225 train_time:8063ms step_avg:61.09ms
step:133/2225 train_time:8124ms step_avg:61.08ms
step:134/2225 train_time:8183ms step_avg:61.07ms
step:135/2225 train_time:8244ms step_avg:61.06ms
step:136/2225 train_time:8303ms step_avg:61.06ms
step:137/2225 train_time:8364ms step_avg:61.05ms
step:138/2225 train_time:8423ms step_avg:61.04ms
step:139/2225 train_time:8484ms step_avg:61.04ms
step:140/2225 train_time:8544ms step_avg:61.03ms
step:141/2225 train_time:8605ms step_avg:61.03ms
step:142/2225 train_time:8664ms step_avg:61.01ms
step:143/2225 train_time:8724ms step_avg:61.01ms
step:144/2225 train_time:8783ms step_avg:60.99ms
step:145/2225 train_time:8844ms step_avg:60.99ms
step:146/2225 train_time:8904ms step_avg:60.99ms
step:147/2225 train_time:8964ms step_avg:60.98ms
step:148/2225 train_time:9023ms step_avg:60.97ms
step:149/2225 train_time:9084ms step_avg:60.97ms
step:150/2225 train_time:9144ms step_avg:60.96ms
step:151/2225 train_time:9204ms step_avg:60.96ms
step:152/2225 train_time:9263ms step_avg:60.94ms
step:153/2225 train_time:9323ms step_avg:60.94ms
step:154/2225 train_time:9382ms step_avg:60.92ms
step:155/2225 train_time:9442ms step_avg:60.92ms
step:156/2225 train_time:9502ms step_avg:60.91ms
step:157/2225 train_time:9563ms step_avg:60.91ms
step:158/2225 train_time:9622ms step_avg:60.90ms
step:159/2225 train_time:9683ms step_avg:60.90ms
step:160/2225 train_time:9742ms step_avg:60.89ms
step:161/2225 train_time:9803ms step_avg:60.89ms
step:162/2225 train_time:9863ms step_avg:60.88ms
step:163/2225 train_time:9923ms step_avg:60.88ms
step:164/2225 train_time:9983ms step_avg:60.87ms
step:165/2225 train_time:10044ms step_avg:60.87ms
step:166/2225 train_time:10104ms step_avg:60.86ms
step:167/2225 train_time:10165ms step_avg:60.87ms
step:168/2225 train_time:10224ms step_avg:60.86ms
step:169/2225 train_time:10284ms step_avg:60.85ms
step:170/2225 train_time:10343ms step_avg:60.84ms
step:171/2225 train_time:10404ms step_avg:60.84ms
step:172/2225 train_time:10463ms step_avg:60.83ms
step:173/2225 train_time:10524ms step_avg:60.83ms
step:174/2225 train_time:10583ms step_avg:60.82ms
step:175/2225 train_time:10645ms step_avg:60.83ms
step:176/2225 train_time:10705ms step_avg:60.82ms
step:177/2225 train_time:10765ms step_avg:60.82ms
step:178/2225 train_time:10824ms step_avg:60.81ms
step:179/2225 train_time:10884ms step_avg:60.80ms
step:180/2225 train_time:10943ms step_avg:60.79ms
step:181/2225 train_time:11004ms step_avg:60.79ms
step:182/2225 train_time:11063ms step_avg:60.79ms
step:183/2225 train_time:11124ms step_avg:60.79ms
step:184/2225 train_time:11183ms step_avg:60.78ms
step:185/2225 train_time:11244ms step_avg:60.78ms
step:186/2225 train_time:11303ms step_avg:60.77ms
step:187/2225 train_time:11364ms step_avg:60.77ms
step:188/2225 train_time:11423ms step_avg:60.76ms
step:189/2225 train_time:11483ms step_avg:60.76ms
step:190/2225 train_time:11543ms step_avg:60.75ms
step:191/2225 train_time:11604ms step_avg:60.75ms
step:192/2225 train_time:11663ms step_avg:60.75ms
step:193/2225 train_time:11724ms step_avg:60.74ms
step:194/2225 train_time:11783ms step_avg:60.74ms
step:195/2225 train_time:11844ms step_avg:60.74ms
step:196/2225 train_time:11904ms step_avg:60.73ms
step:197/2225 train_time:11963ms step_avg:60.73ms
step:198/2225 train_time:12022ms step_avg:60.72ms
step:199/2225 train_time:12083ms step_avg:60.72ms
step:200/2225 train_time:12143ms step_avg:60.72ms
step:201/2225 train_time:12204ms step_avg:60.72ms
step:202/2225 train_time:12263ms step_avg:60.71ms
step:203/2225 train_time:12323ms step_avg:60.71ms
step:204/2225 train_time:12382ms step_avg:60.70ms
step:205/2225 train_time:12443ms step_avg:60.70ms
step:206/2225 train_time:12503ms step_avg:60.69ms
step:207/2225 train_time:12563ms step_avg:60.69ms
step:208/2225 train_time:12623ms step_avg:60.69ms
step:209/2225 train_time:12683ms step_avg:60.69ms
step:210/2225 train_time:12743ms step_avg:60.68ms
step:211/2225 train_time:12804ms step_avg:60.68ms
step:212/2225 train_time:12863ms step_avg:60.67ms
step:213/2225 train_time:12924ms step_avg:60.67ms
step:214/2225 train_time:12983ms step_avg:60.67ms
step:215/2225 train_time:13043ms step_avg:60.67ms
step:216/2225 train_time:13102ms step_avg:60.66ms
step:217/2225 train_time:13163ms step_avg:60.66ms
step:218/2225 train_time:13223ms step_avg:60.65ms
step:219/2225 train_time:13283ms step_avg:60.65ms
step:220/2225 train_time:13343ms step_avg:60.65ms
step:221/2225 train_time:13404ms step_avg:60.65ms
step:222/2225 train_time:13463ms step_avg:60.65ms
step:223/2225 train_time:13524ms step_avg:60.64ms
step:224/2225 train_time:13583ms step_avg:60.64ms
step:225/2225 train_time:13644ms step_avg:60.64ms
step:226/2225 train_time:13704ms step_avg:60.64ms
step:227/2225 train_time:13764ms step_avg:60.64ms
step:228/2225 train_time:13823ms step_avg:60.63ms
step:229/2225 train_time:13884ms step_avg:60.63ms
step:230/2225 train_time:13944ms step_avg:60.62ms
step:231/2225 train_time:14004ms step_avg:60.62ms
step:232/2225 train_time:14064ms step_avg:60.62ms
step:233/2225 train_time:14124ms step_avg:60.62ms
step:234/2225 train_time:14183ms step_avg:60.61ms
step:235/2225 train_time:14244ms step_avg:60.61ms
step:236/2225 train_time:14303ms step_avg:60.61ms
step:237/2225 train_time:14363ms step_avg:60.61ms
step:238/2225 train_time:14423ms step_avg:60.60ms
step:239/2225 train_time:14484ms step_avg:60.60ms
step:240/2225 train_time:14543ms step_avg:60.60ms
step:241/2225 train_time:14604ms step_avg:60.60ms
step:242/2225 train_time:14663ms step_avg:60.59ms
step:243/2225 train_time:14723ms step_avg:60.59ms
step:244/2225 train_time:14783ms step_avg:60.59ms
step:245/2225 train_time:14844ms step_avg:60.59ms
step:246/2225 train_time:14903ms step_avg:60.58ms
step:247/2225 train_time:14963ms step_avg:60.58ms
step:248/2225 train_time:15023ms step_avg:60.58ms
step:249/2225 train_time:15083ms step_avg:60.57ms
step:250/2225 train_time:15143ms step_avg:60.57ms
step:250/2225 val_loss:4.0811 train_time:15203ms step_avg:60.81ms
step:251/2225 train_time:15225ms step_avg:60.66ms
step:252/2225 train_time:15263ms step_avg:60.57ms
step:253/2225 train_time:15327ms step_avg:60.58ms
step:254/2225 train_time:15392ms step_avg:60.60ms
step:255/2225 train_time:15457ms step_avg:60.62ms
step:256/2225 train_time:15515ms step_avg:60.61ms
step:257/2225 train_time:15575ms step_avg:60.60ms
step:258/2225 train_time:15634ms step_avg:60.60ms
step:259/2225 train_time:15694ms step_avg:60.59ms
step:260/2225 train_time:15752ms step_avg:60.59ms
step:261/2225 train_time:15812ms step_avg:60.58ms
step:262/2225 train_time:15871ms step_avg:60.58ms
step:263/2225 train_time:15930ms step_avg:60.57ms
step:264/2225 train_time:15988ms step_avg:60.56ms
step:265/2225 train_time:16048ms step_avg:60.56ms
step:266/2225 train_time:16107ms step_avg:60.55ms
step:267/2225 train_time:16169ms step_avg:60.56ms
step:268/2225 train_time:16228ms step_avg:60.55ms
step:269/2225 train_time:16290ms step_avg:60.56ms
step:270/2225 train_time:16351ms step_avg:60.56ms
step:271/2225 train_time:16414ms step_avg:60.57ms
step:272/2225 train_time:16474ms step_avg:60.57ms
step:273/2225 train_time:16535ms step_avg:60.57ms
step:274/2225 train_time:16594ms step_avg:60.56ms
step:275/2225 train_time:16654ms step_avg:60.56ms
step:276/2225 train_time:16712ms step_avg:60.55ms
step:277/2225 train_time:16773ms step_avg:60.55ms
step:278/2225 train_time:16831ms step_avg:60.54ms
step:279/2225 train_time:16891ms step_avg:60.54ms
step:280/2225 train_time:16949ms step_avg:60.53ms
step:281/2225 train_time:17009ms step_avg:60.53ms
step:282/2225 train_time:17068ms step_avg:60.53ms
step:283/2225 train_time:17128ms step_avg:60.52ms
step:284/2225 train_time:17187ms step_avg:60.52ms
step:285/2225 train_time:17249ms step_avg:60.52ms
step:286/2225 train_time:17309ms step_avg:60.52ms
step:287/2225 train_time:17371ms step_avg:60.53ms
step:288/2225 train_time:17432ms step_avg:60.53ms
step:289/2225 train_time:17495ms step_avg:60.53ms
step:290/2225 train_time:17553ms step_avg:60.53ms
step:291/2225 train_time:17614ms step_avg:60.53ms
step:292/2225 train_time:17673ms step_avg:60.52ms
step:293/2225 train_time:17734ms step_avg:60.52ms
step:294/2225 train_time:17793ms step_avg:60.52ms
step:295/2225 train_time:17852ms step_avg:60.52ms
step:296/2225 train_time:17911ms step_avg:60.51ms
step:297/2225 train_time:17972ms step_avg:60.51ms
step:298/2225 train_time:18030ms step_avg:60.50ms
step:299/2225 train_time:18091ms step_avg:60.50ms
step:300/2225 train_time:18149ms step_avg:60.50ms
step:301/2225 train_time:18210ms step_avg:60.50ms
step:302/2225 train_time:18270ms step_avg:60.50ms
step:303/2225 train_time:18332ms step_avg:60.50ms
step:304/2225 train_time:18392ms step_avg:60.50ms
step:305/2225 train_time:18454ms step_avg:60.51ms
step:306/2225 train_time:18514ms step_avg:60.50ms
step:307/2225 train_time:18575ms step_avg:60.51ms
step:308/2225 train_time:18634ms step_avg:60.50ms
step:309/2225 train_time:18695ms step_avg:60.50ms
step:310/2225 train_time:18753ms step_avg:60.49ms
step:311/2225 train_time:18813ms step_avg:60.49ms
step:312/2225 train_time:18872ms step_avg:60.49ms
step:313/2225 train_time:18932ms step_avg:60.49ms
step:314/2225 train_time:18992ms step_avg:60.48ms
step:315/2225 train_time:19052ms step_avg:60.48ms
step:316/2225 train_time:19111ms step_avg:60.48ms
step:317/2225 train_time:19171ms step_avg:60.48ms
step:318/2225 train_time:19231ms step_avg:60.47ms
step:319/2225 train_time:19292ms step_avg:60.47ms
step:320/2225 train_time:19351ms step_avg:60.47ms
step:321/2225 train_time:19413ms step_avg:60.48ms
step:322/2225 train_time:19473ms step_avg:60.48ms
step:323/2225 train_time:19534ms step_avg:60.48ms
step:324/2225 train_time:19594ms step_avg:60.47ms
step:325/2225 train_time:19654ms step_avg:60.47ms
step:326/2225 train_time:19713ms step_avg:60.47ms
step:327/2225 train_time:19774ms step_avg:60.47ms
step:328/2225 train_time:19833ms step_avg:60.47ms
step:329/2225 train_time:19894ms step_avg:60.47ms
step:330/2225 train_time:19952ms step_avg:60.46ms
step:331/2225 train_time:20013ms step_avg:60.46ms
step:332/2225 train_time:20072ms step_avg:60.46ms
step:333/2225 train_time:20132ms step_avg:60.46ms
step:334/2225 train_time:20191ms step_avg:60.45ms
step:335/2225 train_time:20252ms step_avg:60.45ms
step:336/2225 train_time:20311ms step_avg:60.45ms
step:337/2225 train_time:20372ms step_avg:60.45ms
step:338/2225 train_time:20431ms step_avg:60.45ms
step:339/2225 train_time:20492ms step_avg:60.45ms
step:340/2225 train_time:20551ms step_avg:60.44ms
step:341/2225 train_time:20612ms step_avg:60.45ms
step:342/2225 train_time:20672ms step_avg:60.44ms
step:343/2225 train_time:20733ms step_avg:60.45ms
step:344/2225 train_time:20793ms step_avg:60.44ms
step:345/2225 train_time:20853ms step_avg:60.44ms
step:346/2225 train_time:20912ms step_avg:60.44ms
step:347/2225 train_time:20973ms step_avg:60.44ms
step:348/2225 train_time:21032ms step_avg:60.44ms
step:349/2225 train_time:21093ms step_avg:60.44ms
step:350/2225 train_time:21151ms step_avg:60.43ms
step:351/2225 train_time:21211ms step_avg:60.43ms
step:352/2225 train_time:21271ms step_avg:60.43ms
step:353/2225 train_time:21332ms step_avg:60.43ms
step:354/2225 train_time:21391ms step_avg:60.43ms
step:355/2225 train_time:21452ms step_avg:60.43ms
step:356/2225 train_time:21512ms step_avg:60.43ms
step:357/2225 train_time:21573ms step_avg:60.43ms
step:358/2225 train_time:21632ms step_avg:60.43ms
step:359/2225 train_time:21693ms step_avg:60.43ms
step:360/2225 train_time:21752ms step_avg:60.42ms
step:361/2225 train_time:21814ms step_avg:60.43ms
step:362/2225 train_time:21873ms step_avg:60.42ms
step:363/2225 train_time:21934ms step_avg:60.42ms
step:364/2225 train_time:21993ms step_avg:60.42ms
step:365/2225 train_time:22053ms step_avg:60.42ms
step:366/2225 train_time:22111ms step_avg:60.41ms
step:367/2225 train_time:22172ms step_avg:60.41ms
step:368/2225 train_time:22231ms step_avg:60.41ms
step:369/2225 train_time:22292ms step_avg:60.41ms
step:370/2225 train_time:22351ms step_avg:60.41ms
step:371/2225 train_time:22412ms step_avg:60.41ms
step:372/2225 train_time:22472ms step_avg:60.41ms
step:373/2225 train_time:22533ms step_avg:60.41ms
step:374/2225 train_time:22592ms step_avg:60.41ms
step:375/2225 train_time:22652ms step_avg:60.41ms
step:376/2225 train_time:22711ms step_avg:60.40ms
step:377/2225 train_time:22772ms step_avg:60.40ms
step:378/2225 train_time:22832ms step_avg:60.40ms
step:379/2225 train_time:22893ms step_avg:60.40ms
step:380/2225 train_time:22952ms step_avg:60.40ms
step:381/2225 train_time:23013ms step_avg:60.40ms
step:382/2225 train_time:23072ms step_avg:60.40ms
step:383/2225 train_time:23132ms step_avg:60.40ms
step:384/2225 train_time:23191ms step_avg:60.39ms
step:385/2225 train_time:23251ms step_avg:60.39ms
step:386/2225 train_time:23310ms step_avg:60.39ms
step:387/2225 train_time:23371ms step_avg:60.39ms
step:388/2225 train_time:23430ms step_avg:60.39ms
step:389/2225 train_time:23491ms step_avg:60.39ms
step:390/2225 train_time:23550ms step_avg:60.38ms
step:391/2225 train_time:23611ms step_avg:60.39ms
step:392/2225 train_time:23671ms step_avg:60.38ms
step:393/2225 train_time:23732ms step_avg:60.39ms
step:394/2225 train_time:23791ms step_avg:60.38ms
step:395/2225 train_time:23852ms step_avg:60.38ms
step:396/2225 train_time:23911ms step_avg:60.38ms
step:397/2225 train_time:23972ms step_avg:60.38ms
step:398/2225 train_time:24031ms step_avg:60.38ms
step:399/2225 train_time:24092ms step_avg:60.38ms
step:400/2225 train_time:24151ms step_avg:60.38ms
step:401/2225 train_time:24211ms step_avg:60.38ms
step:402/2225 train_time:24270ms step_avg:60.37ms
step:403/2225 train_time:24331ms step_avg:60.37ms
step:404/2225 train_time:24390ms step_avg:60.37ms
step:405/2225 train_time:24450ms step_avg:60.37ms
step:406/2225 train_time:24509ms step_avg:60.37ms
step:407/2225 train_time:24570ms step_avg:60.37ms
step:408/2225 train_time:24630ms step_avg:60.37ms
step:409/2225 train_time:24691ms step_avg:60.37ms
step:410/2225 train_time:24749ms step_avg:60.36ms
step:411/2225 train_time:24810ms step_avg:60.37ms
step:412/2225 train_time:24870ms step_avg:60.36ms
step:413/2225 train_time:24931ms step_avg:60.36ms
step:414/2225 train_time:24990ms step_avg:60.36ms
step:415/2225 train_time:25051ms step_avg:60.36ms
step:416/2225 train_time:25110ms step_avg:60.36ms
step:417/2225 train_time:25170ms step_avg:60.36ms
step:418/2225 train_time:25229ms step_avg:60.36ms
step:419/2225 train_time:25290ms step_avg:60.36ms
step:420/2225 train_time:25349ms step_avg:60.35ms
step:421/2225 train_time:25409ms step_avg:60.35ms
step:422/2225 train_time:25468ms step_avg:60.35ms
step:423/2225 train_time:25529ms step_avg:60.35ms
step:424/2225 train_time:25588ms step_avg:60.35ms
step:425/2225 train_time:25649ms step_avg:60.35ms
step:426/2225 train_time:25709ms step_avg:60.35ms
step:427/2225 train_time:25770ms step_avg:60.35ms
step:428/2225 train_time:25828ms step_avg:60.35ms
step:429/2225 train_time:25890ms step_avg:60.35ms
step:430/2225 train_time:25948ms step_avg:60.35ms
step:431/2225 train_time:26009ms step_avg:60.35ms
step:432/2225 train_time:26068ms step_avg:60.34ms
step:433/2225 train_time:26129ms step_avg:60.34ms
step:434/2225 train_time:26188ms step_avg:60.34ms
step:435/2225 train_time:26249ms step_avg:60.34ms
step:436/2225 train_time:26307ms step_avg:60.34ms
step:437/2225 train_time:26368ms step_avg:60.34ms
step:438/2225 train_time:26427ms step_avg:60.34ms
step:439/2225 train_time:26487ms step_avg:60.34ms
step:440/2225 train_time:26546ms step_avg:60.33ms
step:441/2225 train_time:26607ms step_avg:60.33ms
step:442/2225 train_time:26666ms step_avg:60.33ms
step:443/2225 train_time:26728ms step_avg:60.33ms
step:444/2225 train_time:26788ms step_avg:60.33ms
step:445/2225 train_time:26848ms step_avg:60.33ms
step:446/2225 train_time:26907ms step_avg:60.33ms
step:447/2225 train_time:26969ms step_avg:60.33ms
step:448/2225 train_time:27028ms step_avg:60.33ms
step:449/2225 train_time:27089ms step_avg:60.33ms
step:450/2225 train_time:27148ms step_avg:60.33ms
step:451/2225 train_time:27209ms step_avg:60.33ms
step:452/2225 train_time:27269ms step_avg:60.33ms
step:453/2225 train_time:27330ms step_avg:60.33ms
step:454/2225 train_time:27389ms step_avg:60.33ms
step:455/2225 train_time:27449ms step_avg:60.33ms
step:456/2225 train_time:27508ms step_avg:60.32ms
step:457/2225 train_time:27568ms step_avg:60.32ms
step:458/2225 train_time:27627ms step_avg:60.32ms
step:459/2225 train_time:27688ms step_avg:60.32ms
step:460/2225 train_time:27748ms step_avg:60.32ms
step:461/2225 train_time:27809ms step_avg:60.32ms
step:462/2225 train_time:27869ms step_avg:60.32ms
step:463/2225 train_time:27930ms step_avg:60.32ms
step:464/2225 train_time:27989ms step_avg:60.32ms
step:465/2225 train_time:28049ms step_avg:60.32ms
step:466/2225 train_time:28108ms step_avg:60.32ms
step:467/2225 train_time:28170ms step_avg:60.32ms
step:468/2225 train_time:28229ms step_avg:60.32ms
step:469/2225 train_time:28290ms step_avg:60.32ms
step:470/2225 train_time:28349ms step_avg:60.32ms
step:471/2225 train_time:28410ms step_avg:60.32ms
step:472/2225 train_time:28468ms step_avg:60.31ms
step:473/2225 train_time:28529ms step_avg:60.32ms
step:474/2225 train_time:28588ms step_avg:60.31ms
step:475/2225 train_time:28649ms step_avg:60.31ms
step:476/2225 train_time:28708ms step_avg:60.31ms
step:477/2225 train_time:28769ms step_avg:60.31ms
step:478/2225 train_time:28828ms step_avg:60.31ms
step:479/2225 train_time:28889ms step_avg:60.31ms
step:480/2225 train_time:28949ms step_avg:60.31ms
step:481/2225 train_time:29009ms step_avg:60.31ms
step:482/2225 train_time:29069ms step_avg:60.31ms
step:483/2225 train_time:29129ms step_avg:60.31ms
step:484/2225 train_time:29188ms step_avg:60.31ms
step:485/2225 train_time:29249ms step_avg:60.31ms
step:486/2225 train_time:29308ms step_avg:60.30ms
step:487/2225 train_time:29369ms step_avg:60.31ms
step:488/2225 train_time:29428ms step_avg:60.30ms
step:489/2225 train_time:29488ms step_avg:60.30ms
step:490/2225 train_time:29548ms step_avg:60.30ms
step:491/2225 train_time:29609ms step_avg:60.30ms
step:492/2225 train_time:29669ms step_avg:60.30ms
step:493/2225 train_time:29730ms step_avg:60.30ms
step:494/2225 train_time:29789ms step_avg:60.30ms
step:495/2225 train_time:29850ms step_avg:60.30ms
step:496/2225 train_time:29909ms step_avg:60.30ms
step:497/2225 train_time:29970ms step_avg:60.30ms
step:498/2225 train_time:30029ms step_avg:60.30ms
step:499/2225 train_time:30091ms step_avg:60.30ms
step:500/2225 train_time:30149ms step_avg:60.30ms
step:500/2225 val_loss:3.8213 train_time:30211ms step_avg:60.42ms
step:501/2225 train_time:30232ms step_avg:60.34ms
step:502/2225 train_time:30271ms step_avg:60.30ms
step:503/2225 train_time:30335ms step_avg:60.31ms
step:504/2225 train_time:30397ms step_avg:60.31ms
step:505/2225 train_time:30458ms step_avg:60.31ms
step:506/2225 train_time:30518ms step_avg:60.31ms
step:507/2225 train_time:30577ms step_avg:60.31ms
step:508/2225 train_time:30636ms step_avg:60.31ms
step:509/2225 train_time:30695ms step_avg:60.31ms
step:510/2225 train_time:30754ms step_avg:60.30ms
step:511/2225 train_time:30814ms step_avg:60.30ms
step:512/2225 train_time:30872ms step_avg:60.30ms
step:513/2225 train_time:30931ms step_avg:60.29ms
step:514/2225 train_time:30990ms step_avg:60.29ms
step:515/2225 train_time:31049ms step_avg:60.29ms
step:516/2225 train_time:31108ms step_avg:60.29ms
step:517/2225 train_time:31169ms step_avg:60.29ms
step:518/2225 train_time:31229ms step_avg:60.29ms
step:519/2225 train_time:31291ms step_avg:60.29ms
step:520/2225 train_time:31352ms step_avg:60.29ms
step:521/2225 train_time:31415ms step_avg:60.30ms
step:522/2225 train_time:31475ms step_avg:60.30ms
step:523/2225 train_time:31536ms step_avg:60.30ms
step:524/2225 train_time:31594ms step_avg:60.29ms
step:525/2225 train_time:31654ms step_avg:60.29ms
step:526/2225 train_time:31713ms step_avg:60.29ms
step:527/2225 train_time:31773ms step_avg:60.29ms
step:528/2225 train_time:31831ms step_avg:60.29ms
step:529/2225 train_time:31891ms step_avg:60.29ms
step:530/2225 train_time:31949ms step_avg:60.28ms
step:531/2225 train_time:32009ms step_avg:60.28ms
step:532/2225 train_time:32068ms step_avg:60.28ms
step:533/2225 train_time:32129ms step_avg:60.28ms
step:534/2225 train_time:32188ms step_avg:60.28ms
step:535/2225 train_time:32249ms step_avg:60.28ms
step:536/2225 train_time:32310ms step_avg:60.28ms
step:537/2225 train_time:32372ms step_avg:60.28ms
step:538/2225 train_time:32432ms step_avg:60.28ms
step:539/2225 train_time:32493ms step_avg:60.28ms
step:540/2225 train_time:32553ms step_avg:60.28ms
step:541/2225 train_time:32614ms step_avg:60.29ms
step:542/2225 train_time:32673ms step_avg:60.28ms
step:543/2225 train_time:32733ms step_avg:60.28ms
step:544/2225 train_time:32791ms step_avg:60.28ms
step:545/2225 train_time:32851ms step_avg:60.28ms
step:546/2225 train_time:32910ms step_avg:60.27ms
step:547/2225 train_time:32970ms step_avg:60.27ms
step:548/2225 train_time:33029ms step_avg:60.27ms
step:549/2225 train_time:33089ms step_avg:60.27ms
step:550/2225 train_time:33148ms step_avg:60.27ms
step:551/2225 train_time:33208ms step_avg:60.27ms
step:552/2225 train_time:33268ms step_avg:60.27ms
step:553/2225 train_time:33329ms step_avg:60.27ms
step:554/2225 train_time:33389ms step_avg:60.27ms
step:555/2225 train_time:33450ms step_avg:60.27ms
step:556/2225 train_time:33511ms step_avg:60.27ms
step:557/2225 train_time:33572ms step_avg:60.27ms
step:558/2225 train_time:33631ms step_avg:60.27ms
step:559/2225 train_time:33691ms step_avg:60.27ms
step:560/2225 train_time:33750ms step_avg:60.27ms
step:561/2225 train_time:33810ms step_avg:60.27ms
step:562/2225 train_time:33869ms step_avg:60.26ms
step:563/2225 train_time:33928ms step_avg:60.26ms
step:564/2225 train_time:33987ms step_avg:60.26ms
step:565/2225 train_time:34048ms step_avg:60.26ms
step:566/2225 train_time:34108ms step_avg:60.26ms
step:567/2225 train_time:34168ms step_avg:60.26ms
step:568/2225 train_time:34228ms step_avg:60.26ms
step:569/2225 train_time:34289ms step_avg:60.26ms
step:570/2225 train_time:34348ms step_avg:60.26ms
step:571/2225 train_time:34410ms step_avg:60.26ms
step:572/2225 train_time:34470ms step_avg:60.26ms
step:573/2225 train_time:34531ms step_avg:60.26ms
step:574/2225 train_time:34590ms step_avg:60.26ms
step:575/2225 train_time:34651ms step_avg:60.26ms
step:576/2225 train_time:34711ms step_avg:60.26ms
step:577/2225 train_time:34772ms step_avg:60.26ms
step:578/2225 train_time:34831ms step_avg:60.26ms
step:579/2225 train_time:34891ms step_avg:60.26ms
step:580/2225 train_time:34950ms step_avg:60.26ms
step:581/2225 train_time:35010ms step_avg:60.26ms
step:582/2225 train_time:35069ms step_avg:60.26ms
step:583/2225 train_time:35130ms step_avg:60.26ms
step:584/2225 train_time:35189ms step_avg:60.25ms
step:585/2225 train_time:35249ms step_avg:60.25ms
step:586/2225 train_time:35309ms step_avg:60.25ms
step:587/2225 train_time:35370ms step_avg:60.26ms
step:588/2225 train_time:35430ms step_avg:60.25ms
step:589/2225 train_time:35491ms step_avg:60.26ms
step:590/2225 train_time:35551ms step_avg:60.26ms
step:591/2225 train_time:35612ms step_avg:60.26ms
step:592/2225 train_time:35671ms step_avg:60.26ms
step:593/2225 train_time:35732ms step_avg:60.26ms
step:594/2225 train_time:35791ms step_avg:60.25ms
step:595/2225 train_time:35851ms step_avg:60.25ms
step:596/2225 train_time:35910ms step_avg:60.25ms
step:597/2225 train_time:35970ms step_avg:60.25ms
step:598/2225 train_time:36029ms step_avg:60.25ms
step:599/2225 train_time:36089ms step_avg:60.25ms
step:600/2225 train_time:36149ms step_avg:60.25ms
step:601/2225 train_time:36209ms step_avg:60.25ms
step:602/2225 train_time:36268ms step_avg:60.25ms
step:603/2225 train_time:36329ms step_avg:60.25ms
step:604/2225 train_time:36388ms step_avg:60.25ms
step:605/2225 train_time:36450ms step_avg:60.25ms
step:606/2225 train_time:36510ms step_avg:60.25ms
step:607/2225 train_time:36571ms step_avg:60.25ms
step:608/2225 train_time:36631ms step_avg:60.25ms
step:609/2225 train_time:36691ms step_avg:60.25ms
step:610/2225 train_time:36751ms step_avg:60.25ms
step:611/2225 train_time:36811ms step_avg:60.25ms
step:612/2225 train_time:36870ms step_avg:60.25ms
step:613/2225 train_time:36930ms step_avg:60.25ms
step:614/2225 train_time:36989ms step_avg:60.24ms
step:615/2225 train_time:37050ms step_avg:60.24ms
step:616/2225 train_time:37109ms step_avg:60.24ms
step:617/2225 train_time:37170ms step_avg:60.24ms
step:618/2225 train_time:37229ms step_avg:60.24ms
step:619/2225 train_time:37290ms step_avg:60.24ms
step:620/2225 train_time:37349ms step_avg:60.24ms
step:621/2225 train_time:37410ms step_avg:60.24ms
step:622/2225 train_time:37469ms step_avg:60.24ms
step:623/2225 train_time:37531ms step_avg:60.24ms
step:624/2225 train_time:37590ms step_avg:60.24ms
step:625/2225 train_time:37651ms step_avg:60.24ms
step:626/2225 train_time:37710ms step_avg:60.24ms
step:627/2225 train_time:37771ms step_avg:60.24ms
step:628/2225 train_time:37830ms step_avg:60.24ms
step:629/2225 train_time:37891ms step_avg:60.24ms
step:630/2225 train_time:37950ms step_avg:60.24ms
step:631/2225 train_time:38010ms step_avg:60.24ms
step:632/2225 train_time:38068ms step_avg:60.23ms
step:633/2225 train_time:38129ms step_avg:60.23ms
step:634/2225 train_time:38189ms step_avg:60.23ms
step:635/2225 train_time:38248ms step_avg:60.23ms
step:636/2225 train_time:38308ms step_avg:60.23ms
step:637/2225 train_time:38369ms step_avg:60.23ms
step:638/2225 train_time:38428ms step_avg:60.23ms
step:639/2225 train_time:38489ms step_avg:60.23ms
step:640/2225 train_time:38549ms step_avg:60.23ms
step:641/2225 train_time:38611ms step_avg:60.24ms
step:642/2225 train_time:38671ms step_avg:60.23ms
step:643/2225 train_time:38731ms step_avg:60.24ms
step:644/2225 train_time:38790ms step_avg:60.23ms
step:645/2225 train_time:38851ms step_avg:60.23ms
step:646/2225 train_time:38910ms step_avg:60.23ms
step:647/2225 train_time:38970ms step_avg:60.23ms
step:648/2225 train_time:39029ms step_avg:60.23ms
step:649/2225 train_time:39090ms step_avg:60.23ms
step:650/2225 train_time:39149ms step_avg:60.23ms
step:651/2225 train_time:39209ms step_avg:60.23ms
step:652/2225 train_time:39268ms step_avg:60.23ms
step:653/2225 train_time:39329ms step_avg:60.23ms
step:654/2225 train_time:39388ms step_avg:60.23ms
step:655/2225 train_time:39449ms step_avg:60.23ms
step:656/2225 train_time:39509ms step_avg:60.23ms
step:657/2225 train_time:39571ms step_avg:60.23ms
step:658/2225 train_time:39630ms step_avg:60.23ms
step:659/2225 train_time:39691ms step_avg:60.23ms
step:660/2225 train_time:39751ms step_avg:60.23ms
step:661/2225 train_time:39812ms step_avg:60.23ms
step:662/2225 train_time:39871ms step_avg:60.23ms
step:663/2225 train_time:39931ms step_avg:60.23ms
step:664/2225 train_time:39990ms step_avg:60.23ms
step:665/2225 train_time:40051ms step_avg:60.23ms
step:666/2225 train_time:40110ms step_avg:60.23ms
step:667/2225 train_time:40171ms step_avg:60.23ms
step:668/2225 train_time:40230ms step_avg:60.23ms
step:669/2225 train_time:40291ms step_avg:60.23ms
step:670/2225 train_time:40350ms step_avg:60.22ms
step:671/2225 train_time:40411ms step_avg:60.23ms
step:672/2225 train_time:40471ms step_avg:60.22ms
step:673/2225 train_time:40532ms step_avg:60.23ms
step:674/2225 train_time:40591ms step_avg:60.22ms
step:675/2225 train_time:40652ms step_avg:60.23ms
step:676/2225 train_time:40712ms step_avg:60.22ms
step:677/2225 train_time:40773ms step_avg:60.23ms
step:678/2225 train_time:40832ms step_avg:60.22ms
step:679/2225 train_time:40892ms step_avg:60.22ms
step:680/2225 train_time:40951ms step_avg:60.22ms
step:681/2225 train_time:41011ms step_avg:60.22ms
step:682/2225 train_time:41071ms step_avg:60.22ms
step:683/2225 train_time:41131ms step_avg:60.22ms
step:684/2225 train_time:41191ms step_avg:60.22ms
step:685/2225 train_time:41251ms step_avg:60.22ms
step:686/2225 train_time:41311ms step_avg:60.22ms
step:687/2225 train_time:41372ms step_avg:60.22ms
step:688/2225 train_time:41431ms step_avg:60.22ms
step:689/2225 train_time:41492ms step_avg:60.22ms
step:690/2225 train_time:41551ms step_avg:60.22ms
step:691/2225 train_time:41611ms step_avg:60.22ms
step:692/2225 train_time:41671ms step_avg:60.22ms
step:693/2225 train_time:41733ms step_avg:60.22ms
step:694/2225 train_time:41792ms step_avg:60.22ms
step:695/2225 train_time:41852ms step_avg:60.22ms
step:696/2225 train_time:41912ms step_avg:60.22ms
step:697/2225 train_time:41972ms step_avg:60.22ms
step:698/2225 train_time:42031ms step_avg:60.22ms
step:699/2225 train_time:42091ms step_avg:60.22ms
step:700/2225 train_time:42150ms step_avg:60.21ms
step:701/2225 train_time:42211ms step_avg:60.22ms
step:702/2225 train_time:42271ms step_avg:60.22ms
step:703/2225 train_time:42332ms step_avg:60.22ms
step:704/2225 train_time:42391ms step_avg:60.21ms
step:705/2225 train_time:42452ms step_avg:60.22ms
step:706/2225 train_time:42511ms step_avg:60.21ms
step:707/2225 train_time:42571ms step_avg:60.21ms
step:708/2225 train_time:42631ms step_avg:60.21ms
step:709/2225 train_time:42691ms step_avg:60.21ms
step:710/2225 train_time:42751ms step_avg:60.21ms
step:711/2225 train_time:42812ms step_avg:60.21ms
step:712/2225 train_time:42871ms step_avg:60.21ms
step:713/2225 train_time:42932ms step_avg:60.21ms
step:714/2225 train_time:42991ms step_avg:60.21ms
step:715/2225 train_time:43052ms step_avg:60.21ms
step:716/2225 train_time:43111ms step_avg:60.21ms
step:717/2225 train_time:43172ms step_avg:60.21ms
step:718/2225 train_time:43231ms step_avg:60.21ms
step:719/2225 train_time:43291ms step_avg:60.21ms
step:720/2225 train_time:43351ms step_avg:60.21ms
step:721/2225 train_time:43412ms step_avg:60.21ms
step:722/2225 train_time:43471ms step_avg:60.21ms
step:723/2225 train_time:43531ms step_avg:60.21ms
step:724/2225 train_time:43590ms step_avg:60.21ms
step:725/2225 train_time:43651ms step_avg:60.21ms
step:726/2225 train_time:43710ms step_avg:60.21ms
step:727/2225 train_time:43771ms step_avg:60.21ms
step:728/2225 train_time:43830ms step_avg:60.21ms
step:729/2225 train_time:43891ms step_avg:60.21ms
step:730/2225 train_time:43950ms step_avg:60.21ms
step:731/2225 train_time:44012ms step_avg:60.21ms
step:732/2225 train_time:44072ms step_avg:60.21ms
step:733/2225 train_time:44133ms step_avg:60.21ms
step:734/2225 train_time:44193ms step_avg:60.21ms
step:735/2225 train_time:44255ms step_avg:60.21ms
step:736/2225 train_time:44314ms step_avg:60.21ms
step:737/2225 train_time:44374ms step_avg:60.21ms
step:738/2225 train_time:44434ms step_avg:60.21ms
step:739/2225 train_time:44495ms step_avg:60.21ms
step:740/2225 train_time:44555ms step_avg:60.21ms
step:741/2225 train_time:44616ms step_avg:60.21ms
step:742/2225 train_time:44676ms step_avg:60.21ms
step:743/2225 train_time:44738ms step_avg:60.21ms
step:744/2225 train_time:44797ms step_avg:60.21ms
step:745/2225 train_time:44858ms step_avg:60.21ms
step:746/2225 train_time:44918ms step_avg:60.21ms
step:747/2225 train_time:44979ms step_avg:60.21ms
step:748/2225 train_time:45038ms step_avg:60.21ms
step:749/2225 train_time:45100ms step_avg:60.21ms
step:750/2225 train_time:45160ms step_avg:60.21ms
step:750/2225 val_loss:3.6683 train_time:45222ms step_avg:60.30ms
step:751/2225 train_time:45244ms step_avg:60.25ms
step:752/2225 train_time:45285ms step_avg:60.22ms
step:753/2225 train_time:45347ms step_avg:60.22ms
step:754/2225 train_time:45410ms step_avg:60.23ms
step:755/2225 train_time:45472ms step_avg:60.23ms
step:756/2225 train_time:45534ms step_avg:60.23ms
step:757/2225 train_time:45594ms step_avg:60.23ms
step:758/2225 train_time:45653ms step_avg:60.23ms
step:759/2225 train_time:45713ms step_avg:60.23ms
step:760/2225 train_time:45772ms step_avg:60.23ms
step:761/2225 train_time:45832ms step_avg:60.23ms
step:762/2225 train_time:45890ms step_avg:60.22ms
step:763/2225 train_time:45951ms step_avg:60.22ms
step:764/2225 train_time:46009ms step_avg:60.22ms
step:765/2225 train_time:46070ms step_avg:60.22ms
step:766/2225 train_time:46131ms step_avg:60.22ms
step:767/2225 train_time:46197ms step_avg:60.23ms
step:768/2225 train_time:46260ms step_avg:60.23ms
step:769/2225 train_time:46323ms step_avg:60.24ms
step:770/2225 train_time:46383ms step_avg:60.24ms
step:771/2225 train_time:46445ms step_avg:60.24ms
step:772/2225 train_time:46505ms step_avg:60.24ms
step:773/2225 train_time:46566ms step_avg:60.24ms
step:774/2225 train_time:46626ms step_avg:60.24ms
step:775/2225 train_time:46686ms step_avg:60.24ms
step:776/2225 train_time:46746ms step_avg:60.24ms
step:777/2225 train_time:46807ms step_avg:60.24ms
step:778/2225 train_time:46867ms step_avg:60.24ms
step:779/2225 train_time:46927ms step_avg:60.24ms
step:780/2225 train_time:46986ms step_avg:60.24ms
step:781/2225 train_time:47047ms step_avg:60.24ms
step:782/2225 train_time:47107ms step_avg:60.24ms
step:783/2225 train_time:47169ms step_avg:60.24ms
step:784/2225 train_time:47230ms step_avg:60.24ms
step:785/2225 train_time:47293ms step_avg:60.25ms
step:786/2225 train_time:47353ms step_avg:60.25ms
step:787/2225 train_time:47415ms step_avg:60.25ms
step:788/2225 train_time:47474ms step_avg:60.25ms
step:789/2225 train_time:47535ms step_avg:60.25ms
step:790/2225 train_time:47595ms step_avg:60.25ms
step:791/2225 train_time:47656ms step_avg:60.25ms
step:792/2225 train_time:47715ms step_avg:60.25ms
step:793/2225 train_time:47777ms step_avg:60.25ms
step:794/2225 train_time:47836ms step_avg:60.25ms
step:795/2225 train_time:47898ms step_avg:60.25ms
step:796/2225 train_time:47957ms step_avg:60.25ms
step:797/2225 train_time:48018ms step_avg:60.25ms
step:798/2225 train_time:48078ms step_avg:60.25ms
step:799/2225 train_time:48140ms step_avg:60.25ms
step:800/2225 train_time:48201ms step_avg:60.25ms
step:801/2225 train_time:48263ms step_avg:60.25ms
step:802/2225 train_time:48323ms step_avg:60.25ms
step:803/2225 train_time:48385ms step_avg:60.26ms
step:804/2225 train_time:48445ms step_avg:60.26ms
step:805/2225 train_time:48506ms step_avg:60.26ms
step:806/2225 train_time:48566ms step_avg:60.26ms
step:807/2225 train_time:48627ms step_avg:60.26ms
step:808/2225 train_time:48686ms step_avg:60.26ms
step:809/2225 train_time:48748ms step_avg:60.26ms
step:810/2225 train_time:48807ms step_avg:60.26ms
step:811/2225 train_time:48868ms step_avg:60.26ms
step:812/2225 train_time:48927ms step_avg:60.26ms
step:813/2225 train_time:48988ms step_avg:60.26ms
step:814/2225 train_time:49048ms step_avg:60.26ms
step:815/2225 train_time:49109ms step_avg:60.26ms
step:816/2225 train_time:49169ms step_avg:60.26ms
step:817/2225 train_time:49230ms step_avg:60.26ms
step:818/2225 train_time:49290ms step_avg:60.26ms
step:819/2225 train_time:49351ms step_avg:60.26ms
step:820/2225 train_time:49411ms step_avg:60.26ms
step:821/2225 train_time:49472ms step_avg:60.26ms
step:822/2225 train_time:49532ms step_avg:60.26ms
step:823/2225 train_time:49593ms step_avg:60.26ms
step:824/2225 train_time:49653ms step_avg:60.26ms
step:825/2225 train_time:49714ms step_avg:60.26ms
step:826/2225 train_time:49774ms step_avg:60.26ms
step:827/2225 train_time:49835ms step_avg:60.26ms
step:828/2225 train_time:49894ms step_avg:60.26ms
step:829/2225 train_time:49956ms step_avg:60.26ms
step:830/2225 train_time:50015ms step_avg:60.26ms
step:831/2225 train_time:50077ms step_avg:60.26ms
step:832/2225 train_time:50137ms step_avg:60.26ms
step:833/2225 train_time:50199ms step_avg:60.26ms
step:834/2225 train_time:50259ms step_avg:60.26ms
step:835/2225 train_time:50321ms step_avg:60.26ms
step:836/2225 train_time:50381ms step_avg:60.26ms
step:837/2225 train_time:50443ms step_avg:60.27ms
step:838/2225 train_time:50503ms step_avg:60.27ms
step:839/2225 train_time:50564ms step_avg:60.27ms
step:840/2225 train_time:50624ms step_avg:60.27ms
step:841/2225 train_time:50685ms step_avg:60.27ms
step:842/2225 train_time:50744ms step_avg:60.27ms
step:843/2225 train_time:50806ms step_avg:60.27ms
step:844/2225 train_time:50866ms step_avg:60.27ms
step:845/2225 train_time:50926ms step_avg:60.27ms
step:846/2225 train_time:50986ms step_avg:60.27ms
step:847/2225 train_time:51047ms step_avg:60.27ms
step:848/2225 train_time:51107ms step_avg:60.27ms
step:849/2225 train_time:51168ms step_avg:60.27ms
step:850/2225 train_time:51228ms step_avg:60.27ms
step:851/2225 train_time:51289ms step_avg:60.27ms
step:852/2225 train_time:51349ms step_avg:60.27ms
step:853/2225 train_time:51410ms step_avg:60.27ms
step:854/2225 train_time:51470ms step_avg:60.27ms
step:855/2225 train_time:51530ms step_avg:60.27ms
step:856/2225 train_time:51590ms step_avg:60.27ms
step:857/2225 train_time:51651ms step_avg:60.27ms
step:858/2225 train_time:51711ms step_avg:60.27ms
step:859/2225 train_time:51772ms step_avg:60.27ms
step:860/2225 train_time:51832ms step_avg:60.27ms
step:861/2225 train_time:51893ms step_avg:60.27ms
step:862/2225 train_time:51952ms step_avg:60.27ms
step:863/2225 train_time:52013ms step_avg:60.27ms
step:864/2225 train_time:52073ms step_avg:60.27ms
step:865/2225 train_time:52134ms step_avg:60.27ms
step:866/2225 train_time:52194ms step_avg:60.27ms
step:867/2225 train_time:52256ms step_avg:60.27ms
step:868/2225 train_time:52316ms step_avg:60.27ms
step:869/2225 train_time:52377ms step_avg:60.27ms
step:870/2225 train_time:52437ms step_avg:60.27ms
step:871/2225 train_time:52500ms step_avg:60.28ms
step:872/2225 train_time:52559ms step_avg:60.27ms
step:873/2225 train_time:52621ms step_avg:60.28ms
step:874/2225 train_time:52681ms step_avg:60.28ms
step:875/2225 train_time:52743ms step_avg:60.28ms
step:876/2225 train_time:52803ms step_avg:60.28ms
step:877/2225 train_time:52865ms step_avg:60.28ms
step:878/2225 train_time:52925ms step_avg:60.28ms
step:879/2225 train_time:52986ms step_avg:60.28ms
step:880/2225 train_time:53045ms step_avg:60.28ms
step:881/2225 train_time:53106ms step_avg:60.28ms
step:882/2225 train_time:53167ms step_avg:60.28ms
step:883/2225 train_time:53227ms step_avg:60.28ms
step:884/2225 train_time:53287ms step_avg:60.28ms
step:885/2225 train_time:53349ms step_avg:60.28ms
step:886/2225 train_time:53409ms step_avg:60.28ms
step:887/2225 train_time:53470ms step_avg:60.28ms
step:888/2225 train_time:53530ms step_avg:60.28ms
step:889/2225 train_time:53592ms step_avg:60.28ms
step:890/2225 train_time:53651ms step_avg:60.28ms
step:891/2225 train_time:53712ms step_avg:60.28ms
step:892/2225 train_time:53772ms step_avg:60.28ms
step:893/2225 train_time:53834ms step_avg:60.28ms
step:894/2225 train_time:53894ms step_avg:60.28ms
step:895/2225 train_time:53956ms step_avg:60.29ms
step:896/2225 train_time:54015ms step_avg:60.28ms
step:897/2225 train_time:54077ms step_avg:60.29ms
step:898/2225 train_time:54137ms step_avg:60.29ms
step:899/2225 train_time:54199ms step_avg:60.29ms
step:900/2225 train_time:54259ms step_avg:60.29ms
step:901/2225 train_time:54320ms step_avg:60.29ms
step:902/2225 train_time:54380ms step_avg:60.29ms
step:903/2225 train_time:54441ms step_avg:60.29ms
step:904/2225 train_time:54502ms step_avg:60.29ms
step:905/2225 train_time:54563ms step_avg:60.29ms
step:906/2225 train_time:54623ms step_avg:60.29ms
step:907/2225 train_time:54685ms step_avg:60.29ms
step:908/2225 train_time:54744ms step_avg:60.29ms
step:909/2225 train_time:54805ms step_avg:60.29ms
step:910/2225 train_time:54866ms step_avg:60.29ms
step:911/2225 train_time:54926ms step_avg:60.29ms
step:912/2225 train_time:54986ms step_avg:60.29ms
step:913/2225 train_time:55046ms step_avg:60.29ms
step:914/2225 train_time:55107ms step_avg:60.29ms
step:915/2225 train_time:55168ms step_avg:60.29ms
step:916/2225 train_time:55227ms step_avg:60.29ms
step:917/2225 train_time:55288ms step_avg:60.29ms
step:918/2225 train_time:55348ms step_avg:60.29ms
step:919/2225 train_time:55409ms step_avg:60.29ms
step:920/2225 train_time:55469ms step_avg:60.29ms
step:921/2225 train_time:55530ms step_avg:60.29ms
step:922/2225 train_time:55589ms step_avg:60.29ms
step:923/2225 train_time:55651ms step_avg:60.29ms
step:924/2225 train_time:55711ms step_avg:60.29ms
step:925/2225 train_time:55772ms step_avg:60.29ms
step:926/2225 train_time:55832ms step_avg:60.29ms
step:927/2225 train_time:55893ms step_avg:60.29ms
step:928/2225 train_time:55953ms step_avg:60.29ms
step:929/2225 train_time:56014ms step_avg:60.30ms
step:930/2225 train_time:56074ms step_avg:60.29ms
step:931/2225 train_time:56135ms step_avg:60.30ms
step:932/2225 train_time:56195ms step_avg:60.29ms
step:933/2225 train_time:56256ms step_avg:60.30ms
step:934/2225 train_time:56316ms step_avg:60.30ms
step:935/2225 train_time:56377ms step_avg:60.30ms
step:936/2225 train_time:56437ms step_avg:60.30ms
step:937/2225 train_time:56498ms step_avg:60.30ms
step:938/2225 train_time:56558ms step_avg:60.30ms
step:939/2225 train_time:56620ms step_avg:60.30ms
step:940/2225 train_time:56680ms step_avg:60.30ms
step:941/2225 train_time:56741ms step_avg:60.30ms
step:942/2225 train_time:56801ms step_avg:60.30ms
step:943/2225 train_time:56862ms step_avg:60.30ms
step:944/2225 train_time:56922ms step_avg:60.30ms
step:945/2225 train_time:56983ms step_avg:60.30ms
step:946/2225 train_time:57042ms step_avg:60.30ms
step:947/2225 train_time:57104ms step_avg:60.30ms
step:948/2225 train_time:57165ms step_avg:60.30ms
step:949/2225 train_time:57225ms step_avg:60.30ms
step:950/2225 train_time:57285ms step_avg:60.30ms
step:951/2225 train_time:57345ms step_avg:60.30ms
step:952/2225 train_time:57405ms step_avg:60.30ms
step:953/2225 train_time:57466ms step_avg:60.30ms
step:954/2225 train_time:57526ms step_avg:60.30ms
step:955/2225 train_time:57587ms step_avg:60.30ms
step:956/2225 train_time:57646ms step_avg:60.30ms
step:957/2225 train_time:57707ms step_avg:60.30ms
step:958/2225 train_time:57767ms step_avg:60.30ms
step:959/2225 train_time:57828ms step_avg:60.30ms
step:960/2225 train_time:57888ms step_avg:60.30ms
step:961/2225 train_time:57948ms step_avg:60.30ms
step:962/2225 train_time:58007ms step_avg:60.30ms
step:963/2225 train_time:58068ms step_avg:60.30ms
step:964/2225 train_time:58128ms step_avg:60.30ms
step:965/2225 train_time:58188ms step_avg:60.30ms
step:966/2225 train_time:58248ms step_avg:60.30ms
step:967/2225 train_time:58309ms step_avg:60.30ms
step:968/2225 train_time:58369ms step_avg:60.30ms
step:969/2225 train_time:58430ms step_avg:60.30ms
step:970/2225 train_time:58489ms step_avg:60.30ms
step:971/2225 train_time:58551ms step_avg:60.30ms
step:972/2225 train_time:58610ms step_avg:60.30ms
step:973/2225 train_time:58671ms step_avg:60.30ms
step:974/2225 train_time:58730ms step_avg:60.30ms
step:975/2225 train_time:58792ms step_avg:60.30ms
step:976/2225 train_time:58851ms step_avg:60.30ms
step:977/2225 train_time:58912ms step_avg:60.30ms
step:978/2225 train_time:58972ms step_avg:60.30ms
step:979/2225 train_time:59034ms step_avg:60.30ms
step:980/2225 train_time:59094ms step_avg:60.30ms
step:981/2225 train_time:59155ms step_avg:60.30ms
step:982/2225 train_time:59215ms step_avg:60.30ms
step:983/2225 train_time:59277ms step_avg:60.30ms
step:984/2225 train_time:59337ms step_avg:60.30ms
step:985/2225 train_time:59399ms step_avg:60.30ms
step:986/2225 train_time:59459ms step_avg:60.30ms
step:987/2225 train_time:59522ms step_avg:60.31ms
step:988/2225 train_time:59581ms step_avg:60.30ms
step:989/2225 train_time:59643ms step_avg:60.31ms
step:990/2225 train_time:59703ms step_avg:60.31ms
step:991/2225 train_time:59764ms step_avg:60.31ms
step:992/2225 train_time:59824ms step_avg:60.31ms
step:993/2225 train_time:59885ms step_avg:60.31ms
step:994/2225 train_time:59944ms step_avg:60.31ms
step:995/2225 train_time:60005ms step_avg:60.31ms
step:996/2225 train_time:60065ms step_avg:60.31ms
step:997/2225 train_time:60127ms step_avg:60.31ms
step:998/2225 train_time:60187ms step_avg:60.31ms
step:999/2225 train_time:60248ms step_avg:60.31ms
step:1000/2225 train_time:60307ms step_avg:60.31ms
step:1000/2225 val_loss:3.5900 train_time:60369ms step_avg:60.37ms
step:1001/2225 train_time:60391ms step_avg:60.33ms
step:1002/2225 train_time:60430ms step_avg:60.31ms
step:1003/2225 train_time:60497ms step_avg:60.32ms
step:1004/2225 train_time:60561ms step_avg:60.32ms
step:1005/2225 train_time:60622ms step_avg:60.32ms
step:1006/2225 train_time:60681ms step_avg:60.32ms
step:1007/2225 train_time:60742ms step_avg:60.32ms
step:1008/2225 train_time:60801ms step_avg:60.32ms
step:1009/2225 train_time:60861ms step_avg:60.32ms
step:1010/2225 train_time:60921ms step_avg:60.32ms
step:1011/2225 train_time:60981ms step_avg:60.32ms
step:1012/2225 train_time:61040ms step_avg:60.32ms
step:1013/2225 train_time:61101ms step_avg:60.32ms
step:1014/2225 train_time:61159ms step_avg:60.32ms
step:1015/2225 train_time:61220ms step_avg:60.31ms
step:1016/2225 train_time:61279ms step_avg:60.31ms
step:1017/2225 train_time:61342ms step_avg:60.32ms
step:1018/2225 train_time:61404ms step_avg:60.32ms
step:1019/2225 train_time:61468ms step_avg:60.32ms
step:1020/2225 train_time:61529ms step_avg:60.32ms
step:1021/2225 train_time:61591ms step_avg:60.32ms
step:1022/2225 train_time:61652ms step_avg:60.32ms
step:1023/2225 train_time:61713ms step_avg:60.33ms
step:1024/2225 train_time:61773ms step_avg:60.33ms
step:1025/2225 train_time:61834ms step_avg:60.33ms
step:1026/2225 train_time:61894ms step_avg:60.33ms
step:1027/2225 train_time:61955ms step_avg:60.33ms
step:1028/2225 train_time:62014ms step_avg:60.33ms
step:1029/2225 train_time:62074ms step_avg:60.32ms
step:1030/2225 train_time:62133ms step_avg:60.32ms
step:1031/2225 train_time:62194ms step_avg:60.32ms
step:1032/2225 train_time:62254ms step_avg:60.32ms
step:1033/2225 train_time:62315ms step_avg:60.32ms
step:1034/2225 train_time:62376ms step_avg:60.33ms
step:1035/2225 train_time:62439ms step_avg:60.33ms
step:1036/2225 train_time:62501ms step_avg:60.33ms
step:1037/2225 train_time:62563ms step_avg:60.33ms
step:1038/2225 train_time:62623ms step_avg:60.33ms
step:1039/2225 train_time:62684ms step_avg:60.33ms
step:1040/2225 train_time:62744ms step_avg:60.33ms
step:1041/2225 train_time:62805ms step_avg:60.33ms
step:1042/2225 train_time:62865ms step_avg:60.33ms
step:1043/2225 train_time:62926ms step_avg:60.33ms
step:1044/2225 train_time:62986ms step_avg:60.33ms
step:1045/2225 train_time:63047ms step_avg:60.33ms
step:1046/2225 train_time:63106ms step_avg:60.33ms
step:1047/2225 train_time:63168ms step_avg:60.33ms
step:1048/2225 train_time:63228ms step_avg:60.33ms
step:1049/2225 train_time:63290ms step_avg:60.33ms
step:1050/2225 train_time:63351ms step_avg:60.33ms
step:1051/2225 train_time:63413ms step_avg:60.34ms
step:1052/2225 train_time:63473ms step_avg:60.34ms
step:1053/2225 train_time:63535ms step_avg:60.34ms
step:1054/2225 train_time:63596ms step_avg:60.34ms
step:1055/2225 train_time:63657ms step_avg:60.34ms
step:1056/2225 train_time:63717ms step_avg:60.34ms
step:1057/2225 train_time:63778ms step_avg:60.34ms
step:1058/2225 train_time:63838ms step_avg:60.34ms
step:1059/2225 train_time:63898ms step_avg:60.34ms
step:1060/2225 train_time:63958ms step_avg:60.34ms
step:1061/2225 train_time:64019ms step_avg:60.34ms
step:1062/2225 train_time:64078ms step_avg:60.34ms
step:1063/2225 train_time:64140ms step_avg:60.34ms
step:1064/2225 train_time:64199ms step_avg:60.34ms
step:1065/2225 train_time:64260ms step_avg:60.34ms
step:1066/2225 train_time:64320ms step_avg:60.34ms
step:1067/2225 train_time:64382ms step_avg:60.34ms
step:1068/2225 train_time:64441ms step_avg:60.34ms
step:1069/2225 train_time:64503ms step_avg:60.34ms
step:1070/2225 train_time:64563ms step_avg:60.34ms
step:1071/2225 train_time:64624ms step_avg:60.34ms
step:1072/2225 train_time:64684ms step_avg:60.34ms
step:1073/2225 train_time:64745ms step_avg:60.34ms
step:1074/2225 train_time:64805ms step_avg:60.34ms
step:1075/2225 train_time:64866ms step_avg:60.34ms
step:1076/2225 train_time:64926ms step_avg:60.34ms
step:1077/2225 train_time:64987ms step_avg:60.34ms
step:1078/2225 train_time:65047ms step_avg:60.34ms
step:1079/2225 train_time:65107ms step_avg:60.34ms
step:1080/2225 train_time:65168ms step_avg:60.34ms
step:1081/2225 train_time:65230ms step_avg:60.34ms
step:1082/2225 train_time:65290ms step_avg:60.34ms
step:1083/2225 train_time:65352ms step_avg:60.34ms
step:1084/2225 train_time:65412ms step_avg:60.34ms
step:1085/2225 train_time:65474ms step_avg:60.34ms
step:1086/2225 train_time:65534ms step_avg:60.34ms
step:1087/2225 train_time:65596ms step_avg:60.35ms
step:1088/2225 train_time:65656ms step_avg:60.35ms
step:1089/2225 train_time:65717ms step_avg:60.35ms
step:1090/2225 train_time:65776ms step_avg:60.35ms
step:1091/2225 train_time:65838ms step_avg:60.35ms
step:1092/2225 train_time:65898ms step_avg:60.35ms
step:1093/2225 train_time:65959ms step_avg:60.35ms
step:1094/2225 train_time:66018ms step_avg:60.35ms
step:1095/2225 train_time:66079ms step_avg:60.35ms
step:1096/2225 train_time:66138ms step_avg:60.35ms
step:1097/2225 train_time:66200ms step_avg:60.35ms
step:1098/2225 train_time:66260ms step_avg:60.35ms
step:1099/2225 train_time:66321ms step_avg:60.35ms
step:1100/2225 train_time:66381ms step_avg:60.35ms
step:1101/2225 train_time:66443ms step_avg:60.35ms
step:1102/2225 train_time:66502ms step_avg:60.35ms
step:1103/2225 train_time:66564ms step_avg:60.35ms
step:1104/2225 train_time:66624ms step_avg:60.35ms
step:1105/2225 train_time:66686ms step_avg:60.35ms
step:1106/2225 train_time:66746ms step_avg:60.35ms
step:1107/2225 train_time:66807ms step_avg:60.35ms
step:1108/2225 train_time:66867ms step_avg:60.35ms
step:1109/2225 train_time:66929ms step_avg:60.35ms
step:1110/2225 train_time:66989ms step_avg:60.35ms
step:1111/2225 train_time:67051ms step_avg:60.35ms
step:1112/2225 train_time:67111ms step_avg:60.35ms
step:1113/2225 train_time:67172ms step_avg:60.35ms
step:1114/2225 train_time:67232ms step_avg:60.35ms
step:1115/2225 train_time:67294ms step_avg:60.35ms
step:1116/2225 train_time:67353ms step_avg:60.35ms
step:1117/2225 train_time:67415ms step_avg:60.35ms
step:1118/2225 train_time:67475ms step_avg:60.35ms
step:1119/2225 train_time:67536ms step_avg:60.35ms
step:1120/2225 train_time:67596ms step_avg:60.35ms
step:1121/2225 train_time:67657ms step_avg:60.35ms
step:1122/2225 train_time:67717ms step_avg:60.35ms
step:1123/2225 train_time:67778ms step_avg:60.35ms
step:1124/2225 train_time:67838ms step_avg:60.35ms
step:1125/2225 train_time:67899ms step_avg:60.35ms
step:1126/2225 train_time:67958ms step_avg:60.35ms
step:1127/2225 train_time:68020ms step_avg:60.35ms
step:1128/2225 train_time:68079ms step_avg:60.35ms
step:1129/2225 train_time:68140ms step_avg:60.35ms
step:1130/2225 train_time:68200ms step_avg:60.35ms
step:1131/2225 train_time:68262ms step_avg:60.36ms
step:1132/2225 train_time:68321ms step_avg:60.35ms
step:1133/2225 train_time:68383ms step_avg:60.36ms
step:1134/2225 train_time:68442ms step_avg:60.35ms
step:1135/2225 train_time:68503ms step_avg:60.36ms
step:1136/2225 train_time:68563ms step_avg:60.35ms
step:1137/2225 train_time:68625ms step_avg:60.36ms
step:1138/2225 train_time:68684ms step_avg:60.36ms
step:1139/2225 train_time:68746ms step_avg:60.36ms
step:1140/2225 train_time:68807ms step_avg:60.36ms
step:1141/2225 train_time:68869ms step_avg:60.36ms
step:1142/2225 train_time:68929ms step_avg:60.36ms
step:1143/2225 train_time:68990ms step_avg:60.36ms
step:1144/2225 train_time:69051ms step_avg:60.36ms
step:1145/2225 train_time:69111ms step_avg:60.36ms
step:1146/2225 train_time:69172ms step_avg:60.36ms
step:1147/2225 train_time:69235ms step_avg:60.36ms
step:1148/2225 train_time:69295ms step_avg:60.36ms
step:1149/2225 train_time:69356ms step_avg:60.36ms
step:1150/2225 train_time:69415ms step_avg:60.36ms
step:1151/2225 train_time:69476ms step_avg:60.36ms
step:1152/2225 train_time:69536ms step_avg:60.36ms
step:1153/2225 train_time:69598ms step_avg:60.36ms
step:1154/2225 train_time:69657ms step_avg:60.36ms
step:1155/2225 train_time:69718ms step_avg:60.36ms
step:1156/2225 train_time:69778ms step_avg:60.36ms
step:1157/2225 train_time:69840ms step_avg:60.36ms
step:1158/2225 train_time:69899ms step_avg:60.36ms
step:1159/2225 train_time:69960ms step_avg:60.36ms
step:1160/2225 train_time:70020ms step_avg:60.36ms
step:1161/2225 train_time:70082ms step_avg:60.36ms
step:1162/2225 train_time:70141ms step_avg:60.36ms
step:1163/2225 train_time:70203ms step_avg:60.36ms
step:1164/2225 train_time:70262ms step_avg:60.36ms
step:1165/2225 train_time:70324ms step_avg:60.36ms
step:1166/2225 train_time:70383ms step_avg:60.36ms
step:1167/2225 train_time:70445ms step_avg:60.36ms
step:1168/2225 train_time:70505ms step_avg:60.36ms
step:1169/2225 train_time:70566ms step_avg:60.36ms
step:1170/2225 train_time:70626ms step_avg:60.36ms
step:1171/2225 train_time:70688ms step_avg:60.37ms
step:1172/2225 train_time:70749ms step_avg:60.37ms
step:1173/2225 train_time:70809ms step_avg:60.37ms
step:1174/2225 train_time:70869ms step_avg:60.37ms
step:1175/2225 train_time:70930ms step_avg:60.37ms
step:1176/2225 train_time:70990ms step_avg:60.37ms
step:1177/2225 train_time:71052ms step_avg:60.37ms
step:1178/2225 train_time:71112ms step_avg:60.37ms
step:1179/2225 train_time:71173ms step_avg:60.37ms
step:1180/2225 train_time:71233ms step_avg:60.37ms
step:1181/2225 train_time:71295ms step_avg:60.37ms
step:1182/2225 train_time:71354ms step_avg:60.37ms
step:1183/2225 train_time:71415ms step_avg:60.37ms
step:1184/2225 train_time:71474ms step_avg:60.37ms
step:1185/2225 train_time:71535ms step_avg:60.37ms
step:1186/2225 train_time:71596ms step_avg:60.37ms
step:1187/2225 train_time:71656ms step_avg:60.37ms
step:1188/2225 train_time:71716ms step_avg:60.37ms
step:1189/2225 train_time:71777ms step_avg:60.37ms
step:1190/2225 train_time:71837ms step_avg:60.37ms
step:1191/2225 train_time:71898ms step_avg:60.37ms
step:1192/2225 train_time:71958ms step_avg:60.37ms
step:1193/2225 train_time:72019ms step_avg:60.37ms
step:1194/2225 train_time:72078ms step_avg:60.37ms
step:1195/2225 train_time:72140ms step_avg:60.37ms
step:1196/2225 train_time:72199ms step_avg:60.37ms
step:1197/2225 train_time:72260ms step_avg:60.37ms
step:1198/2225 train_time:72320ms step_avg:60.37ms
step:1199/2225 train_time:72381ms step_avg:60.37ms
step:1200/2225 train_time:72440ms step_avg:60.37ms
step:1201/2225 train_time:72502ms step_avg:60.37ms
step:1202/2225 train_time:72562ms step_avg:60.37ms
step:1203/2225 train_time:72624ms step_avg:60.37ms
step:1204/2225 train_time:72684ms step_avg:60.37ms
step:1205/2225 train_time:72745ms step_avg:60.37ms
step:1206/2225 train_time:72805ms step_avg:60.37ms
step:1207/2225 train_time:72867ms step_avg:60.37ms
step:1208/2225 train_time:72927ms step_avg:60.37ms
step:1209/2225 train_time:72988ms step_avg:60.37ms
step:1210/2225 train_time:73048ms step_avg:60.37ms
step:1211/2225 train_time:73110ms step_avg:60.37ms
step:1212/2225 train_time:73170ms step_avg:60.37ms
step:1213/2225 train_time:73231ms step_avg:60.37ms
step:1214/2225 train_time:73291ms step_avg:60.37ms
step:1215/2225 train_time:73353ms step_avg:60.37ms
step:1216/2225 train_time:73414ms step_avg:60.37ms
step:1217/2225 train_time:73475ms step_avg:60.37ms
step:1218/2225 train_time:73535ms step_avg:60.37ms
step:1219/2225 train_time:73595ms step_avg:60.37ms
step:1220/2225 train_time:73655ms step_avg:60.37ms
step:1221/2225 train_time:73716ms step_avg:60.37ms
step:1222/2225 train_time:73776ms step_avg:60.37ms
step:1223/2225 train_time:73837ms step_avg:60.37ms
step:1224/2225 train_time:73897ms step_avg:60.37ms
step:1225/2225 train_time:73958ms step_avg:60.37ms
step:1226/2225 train_time:74018ms step_avg:60.37ms
step:1227/2225 train_time:74079ms step_avg:60.37ms
step:1228/2225 train_time:74139ms step_avg:60.37ms
step:1229/2225 train_time:74200ms step_avg:60.37ms
step:1230/2225 train_time:74259ms step_avg:60.37ms
step:1231/2225 train_time:74320ms step_avg:60.37ms
step:1232/2225 train_time:74380ms step_avg:60.37ms
step:1233/2225 train_time:74441ms step_avg:60.37ms
step:1234/2225 train_time:74501ms step_avg:60.37ms
step:1235/2225 train_time:74563ms step_avg:60.37ms
step:1236/2225 train_time:74622ms step_avg:60.37ms
step:1237/2225 train_time:74684ms step_avg:60.38ms
step:1238/2225 train_time:74744ms step_avg:60.38ms
step:1239/2225 train_time:74806ms step_avg:60.38ms
step:1240/2225 train_time:74866ms step_avg:60.38ms
step:1241/2225 train_time:74927ms step_avg:60.38ms
step:1242/2225 train_time:74987ms step_avg:60.38ms
step:1243/2225 train_time:75049ms step_avg:60.38ms
step:1244/2225 train_time:75109ms step_avg:60.38ms
step:1245/2225 train_time:75172ms step_avg:60.38ms
step:1246/2225 train_time:75232ms step_avg:60.38ms
step:1247/2225 train_time:75294ms step_avg:60.38ms
step:1248/2225 train_time:75353ms step_avg:60.38ms
step:1249/2225 train_time:75415ms step_avg:60.38ms
step:1250/2225 train_time:75475ms step_avg:60.38ms
step:1250/2225 val_loss:3.5181 train_time:75536ms step_avg:60.43ms
step:1251/2225 train_time:75560ms step_avg:60.40ms
step:1252/2225 train_time:75598ms step_avg:60.38ms
step:1253/2225 train_time:75666ms step_avg:60.39ms
step:1254/2225 train_time:75729ms step_avg:60.39ms
step:1255/2225 train_time:75790ms step_avg:60.39ms
step:1256/2225 train_time:75849ms step_avg:60.39ms
step:1257/2225 train_time:75910ms step_avg:60.39ms
step:1258/2225 train_time:75969ms step_avg:60.39ms
step:1259/2225 train_time:76029ms step_avg:60.39ms
step:1260/2225 train_time:76088ms step_avg:60.39ms
step:1261/2225 train_time:76148ms step_avg:60.39ms
step:1262/2225 train_time:76207ms step_avg:60.39ms
step:1263/2225 train_time:76267ms step_avg:60.39ms
step:1264/2225 train_time:76327ms step_avg:60.39ms
step:1265/2225 train_time:76386ms step_avg:60.38ms
step:1266/2225 train_time:76446ms step_avg:60.38ms
step:1267/2225 train_time:76509ms step_avg:60.39ms
step:1268/2225 train_time:76571ms step_avg:60.39ms
step:1269/2225 train_time:76633ms step_avg:60.39ms
step:1270/2225 train_time:76694ms step_avg:60.39ms
step:1271/2225 train_time:76755ms step_avg:60.39ms
step:1272/2225 train_time:76815ms step_avg:60.39ms
step:1273/2225 train_time:76877ms step_avg:60.39ms
step:1274/2225 train_time:76936ms step_avg:60.39ms
step:1275/2225 train_time:76998ms step_avg:60.39ms
step:1276/2225 train_time:77057ms step_avg:60.39ms
step:1277/2225 train_time:77118ms step_avg:60.39ms
step:1278/2225 train_time:77178ms step_avg:60.39ms
step:1279/2225 train_time:77239ms step_avg:60.39ms
step:1280/2225 train_time:77298ms step_avg:60.39ms
step:1281/2225 train_time:77359ms step_avg:60.39ms
step:1282/2225 train_time:77419ms step_avg:60.39ms
step:1283/2225 train_time:77481ms step_avg:60.39ms
step:1284/2225 train_time:77541ms step_avg:60.39ms
step:1285/2225 train_time:77604ms step_avg:60.39ms
step:1286/2225 train_time:77666ms step_avg:60.39ms
step:1287/2225 train_time:77728ms step_avg:60.39ms
step:1288/2225 train_time:77787ms step_avg:60.39ms
step:1289/2225 train_time:77849ms step_avg:60.39ms
step:1290/2225 train_time:77909ms step_avg:60.39ms
step:1291/2225 train_time:77970ms step_avg:60.40ms
step:1292/2225 train_time:78030ms step_avg:60.39ms
step:1293/2225 train_time:78091ms step_avg:60.39ms
step:1294/2225 train_time:78150ms step_avg:60.39ms
step:1295/2225 train_time:78211ms step_avg:60.39ms
step:1296/2225 train_time:78270ms step_avg:60.39ms
step:1297/2225 train_time:78332ms step_avg:60.39ms
step:1298/2225 train_time:78391ms step_avg:60.39ms
step:1299/2225 train_time:78452ms step_avg:60.39ms
step:1300/2225 train_time:78512ms step_avg:60.39ms
step:1301/2225 train_time:78574ms step_avg:60.40ms
step:1302/2225 train_time:78634ms step_avg:60.39ms
step:1303/2225 train_time:78696ms step_avg:60.40ms
step:1304/2225 train_time:78755ms step_avg:60.40ms
step:1305/2225 train_time:78817ms step_avg:60.40ms
step:1306/2225 train_time:78877ms step_avg:60.40ms
step:1307/2225 train_time:78939ms step_avg:60.40ms
step:1308/2225 train_time:78998ms step_avg:60.40ms
step:1309/2225 train_time:79060ms step_avg:60.40ms
step:1310/2225 train_time:79120ms step_avg:60.40ms
step:1311/2225 train_time:79181ms step_avg:60.40ms
step:1312/2225 train_time:79240ms step_avg:60.40ms
step:1313/2225 train_time:79301ms step_avg:60.40ms
step:1314/2225 train_time:79361ms step_avg:60.40ms
step:1315/2225 train_time:79422ms step_avg:60.40ms
step:1316/2225 train_time:79482ms step_avg:60.40ms
step:1317/2225 train_time:79543ms step_avg:60.40ms
step:1318/2225 train_time:79603ms step_avg:60.40ms
step:1319/2225 train_time:79666ms step_avg:60.40ms
step:1320/2225 train_time:79726ms step_avg:60.40ms
step:1321/2225 train_time:79787ms step_avg:60.40ms
step:1322/2225 train_time:79847ms step_avg:60.40ms
step:1323/2225 train_time:79909ms step_avg:60.40ms
step:1324/2225 train_time:79969ms step_avg:60.40ms
step:1325/2225 train_time:80030ms step_avg:60.40ms
step:1326/2225 train_time:80090ms step_avg:60.40ms
step:1327/2225 train_time:80151ms step_avg:60.40ms
step:1328/2225 train_time:80210ms step_avg:60.40ms
step:1329/2225 train_time:80271ms step_avg:60.40ms
step:1330/2225 train_time:80330ms step_avg:60.40ms
step:1331/2225 train_time:80392ms step_avg:60.40ms
step:1332/2225 train_time:80451ms step_avg:60.40ms
step:1333/2225 train_time:80513ms step_avg:60.40ms
step:1334/2225 train_time:80573ms step_avg:60.40ms
step:1335/2225 train_time:80634ms step_avg:60.40ms
step:1336/2225 train_time:80693ms step_avg:60.40ms
step:1337/2225 train_time:80754ms step_avg:60.40ms
step:1338/2225 train_time:80815ms step_avg:60.40ms
step:1339/2225 train_time:80877ms step_avg:60.40ms
step:1340/2225 train_time:80937ms step_avg:60.40ms
step:1341/2225 train_time:80999ms step_avg:60.40ms
step:1342/2225 train_time:81059ms step_avg:60.40ms
step:1343/2225 train_time:81120ms step_avg:60.40ms
step:1344/2225 train_time:81179ms step_avg:60.40ms
step:1345/2225 train_time:81241ms step_avg:60.40ms
step:1346/2225 train_time:81301ms step_avg:60.40ms
step:1347/2225 train_time:81362ms step_avg:60.40ms
step:1348/2225 train_time:81423ms step_avg:60.40ms
step:1349/2225 train_time:81485ms step_avg:60.40ms
step:1350/2225 train_time:81544ms step_avg:60.40ms
step:1351/2225 train_time:81605ms step_avg:60.40ms
step:1352/2225 train_time:81666ms step_avg:60.40ms
step:1353/2225 train_time:81727ms step_avg:60.40ms
step:1354/2225 train_time:81788ms step_avg:60.40ms
step:1355/2225 train_time:81849ms step_avg:60.41ms
step:1356/2225 train_time:81909ms step_avg:60.40ms
step:1357/2225 train_time:81970ms step_avg:60.41ms
step:1358/2225 train_time:82029ms step_avg:60.40ms
step:1359/2225 train_time:82090ms step_avg:60.41ms
step:1360/2225 train_time:82150ms step_avg:60.40ms
step:1361/2225 train_time:82211ms step_avg:60.40ms
step:1362/2225 train_time:82271ms step_avg:60.40ms
step:1363/2225 train_time:82333ms step_avg:60.41ms
step:1364/2225 train_time:82393ms step_avg:60.41ms
step:1365/2225 train_time:82454ms step_avg:60.41ms
step:1366/2225 train_time:82514ms step_avg:60.41ms
step:1367/2225 train_time:82575ms step_avg:60.41ms
step:1368/2225 train_time:82635ms step_avg:60.41ms
step:1369/2225 train_time:82696ms step_avg:60.41ms
step:1370/2225 train_time:82756ms step_avg:60.41ms
step:1371/2225 train_time:82819ms step_avg:60.41ms
step:1372/2225 train_time:82878ms step_avg:60.41ms
step:1373/2225 train_time:82939ms step_avg:60.41ms
step:1374/2225 train_time:82999ms step_avg:60.41ms
step:1375/2225 train_time:83060ms step_avg:60.41ms
step:1376/2225 train_time:83120ms step_avg:60.41ms
step:1377/2225 train_time:83182ms step_avg:60.41ms
step:1378/2225 train_time:83242ms step_avg:60.41ms
step:1379/2225 train_time:83304ms step_avg:60.41ms
step:1380/2225 train_time:83364ms step_avg:60.41ms
step:1381/2225 train_time:83425ms step_avg:60.41ms
step:1382/2225 train_time:83485ms step_avg:60.41ms
step:1383/2225 train_time:83546ms step_avg:60.41ms
step:1384/2225 train_time:83606ms step_avg:60.41ms
step:1385/2225 train_time:83667ms step_avg:60.41ms
step:1386/2225 train_time:83728ms step_avg:60.41ms
step:1387/2225 train_time:83789ms step_avg:60.41ms
step:1388/2225 train_time:83849ms step_avg:60.41ms
step:1389/2225 train_time:83910ms step_avg:60.41ms
step:1390/2225 train_time:83970ms step_avg:60.41ms
step:1391/2225 train_time:84032ms step_avg:60.41ms
step:1392/2225 train_time:84091ms step_avg:60.41ms
step:1393/2225 train_time:84152ms step_avg:60.41ms
step:1394/2225 train_time:84212ms step_avg:60.41ms
step:1395/2225 train_time:84274ms step_avg:60.41ms
step:1396/2225 train_time:84333ms step_avg:60.41ms
step:1397/2225 train_time:84394ms step_avg:60.41ms
step:1398/2225 train_time:84453ms step_avg:60.41ms
step:1399/2225 train_time:84515ms step_avg:60.41ms
step:1400/2225 train_time:84574ms step_avg:60.41ms
step:1401/2225 train_time:84636ms step_avg:60.41ms
step:1402/2225 train_time:84695ms step_avg:60.41ms
step:1403/2225 train_time:84757ms step_avg:60.41ms
step:1404/2225 train_time:84817ms step_avg:60.41ms
step:1405/2225 train_time:84878ms step_avg:60.41ms
step:1406/2225 train_time:84937ms step_avg:60.41ms
step:1407/2225 train_time:84999ms step_avg:60.41ms
step:1408/2225 train_time:85058ms step_avg:60.41ms
step:1409/2225 train_time:85120ms step_avg:60.41ms
step:1410/2225 train_time:85180ms step_avg:60.41ms
step:1411/2225 train_time:85242ms step_avg:60.41ms
step:1412/2225 train_time:85303ms step_avg:60.41ms
step:1413/2225 train_time:85365ms step_avg:60.41ms
step:1414/2225 train_time:85425ms step_avg:60.41ms
step:1415/2225 train_time:85486ms step_avg:60.41ms
step:1416/2225 train_time:85546ms step_avg:60.41ms
step:1417/2225 train_time:85607ms step_avg:60.41ms
step:1418/2225 train_time:85667ms step_avg:60.41ms
step:1419/2225 train_time:85729ms step_avg:60.42ms
step:1420/2225 train_time:85789ms step_avg:60.41ms
step:1421/2225 train_time:85849ms step_avg:60.41ms
step:1422/2225 train_time:85909ms step_avg:60.41ms
step:1423/2225 train_time:85970ms step_avg:60.41ms
step:1424/2225 train_time:86029ms step_avg:60.41ms
step:1425/2225 train_time:86091ms step_avg:60.41ms
step:1426/2225 train_time:86150ms step_avg:60.41ms
step:1427/2225 train_time:86211ms step_avg:60.41ms
step:1428/2225 train_time:86272ms step_avg:60.41ms
step:1429/2225 train_time:86333ms step_avg:60.41ms
step:1430/2225 train_time:86392ms step_avg:60.41ms
step:1431/2225 train_time:86454ms step_avg:60.42ms
step:1432/2225 train_time:86514ms step_avg:60.41ms
step:1433/2225 train_time:86574ms step_avg:60.41ms
step:1434/2225 train_time:86634ms step_avg:60.41ms
step:1435/2225 train_time:86695ms step_avg:60.41ms
step:1436/2225 train_time:86755ms step_avg:60.41ms
step:1437/2225 train_time:86817ms step_avg:60.42ms
step:1438/2225 train_time:86876ms step_avg:60.41ms
step:1439/2225 train_time:86938ms step_avg:60.42ms
step:1440/2225 train_time:86998ms step_avg:60.42ms
step:1441/2225 train_time:87059ms step_avg:60.42ms
step:1442/2225 train_time:87119ms step_avg:60.42ms
step:1443/2225 train_time:87180ms step_avg:60.42ms
step:1444/2225 train_time:87240ms step_avg:60.42ms
step:1445/2225 train_time:87302ms step_avg:60.42ms
step:1446/2225 train_time:87362ms step_avg:60.42ms
step:1447/2225 train_time:87423ms step_avg:60.42ms
step:1448/2225 train_time:87484ms step_avg:60.42ms
step:1449/2225 train_time:87546ms step_avg:60.42ms
step:1450/2225 train_time:87606ms step_avg:60.42ms
step:1451/2225 train_time:87667ms step_avg:60.42ms
step:1452/2225 train_time:87727ms step_avg:60.42ms
step:1453/2225 train_time:87788ms step_avg:60.42ms
step:1454/2225 train_time:87848ms step_avg:60.42ms
step:1455/2225 train_time:87909ms step_avg:60.42ms
step:1456/2225 train_time:87969ms step_avg:60.42ms
step:1457/2225 train_time:88030ms step_avg:60.42ms
step:1458/2225 train_time:88089ms step_avg:60.42ms
step:1459/2225 train_time:88151ms step_avg:60.42ms
step:1460/2225 train_time:88211ms step_avg:60.42ms
step:1461/2225 train_time:88274ms step_avg:60.42ms
step:1462/2225 train_time:88334ms step_avg:60.42ms
step:1463/2225 train_time:88396ms step_avg:60.42ms
step:1464/2225 train_time:88456ms step_avg:60.42ms
step:1465/2225 train_time:88518ms step_avg:60.42ms
step:1466/2225 train_time:88578ms step_avg:60.42ms
step:1467/2225 train_time:88640ms step_avg:60.42ms
step:1468/2225 train_time:88701ms step_avg:60.42ms
step:1469/2225 train_time:88763ms step_avg:60.42ms
step:1470/2225 train_time:88823ms step_avg:60.42ms
step:1471/2225 train_time:88885ms step_avg:60.42ms
step:1472/2225 train_time:88945ms step_avg:60.42ms
step:1473/2225 train_time:89007ms step_avg:60.43ms
step:1474/2225 train_time:89067ms step_avg:60.43ms
step:1475/2225 train_time:89128ms step_avg:60.43ms
step:1476/2225 train_time:89189ms step_avg:60.43ms
step:1477/2225 train_time:89250ms step_avg:60.43ms
step:1478/2225 train_time:89310ms step_avg:60.43ms
step:1479/2225 train_time:89372ms step_avg:60.43ms
step:1480/2225 train_time:89432ms step_avg:60.43ms
step:1481/2225 train_time:89494ms step_avg:60.43ms
step:1482/2225 train_time:89554ms step_avg:60.43ms
step:1483/2225 train_time:89615ms step_avg:60.43ms
step:1484/2225 train_time:89675ms step_avg:60.43ms
step:1485/2225 train_time:89738ms step_avg:60.43ms
step:1486/2225 train_time:89798ms step_avg:60.43ms
step:1487/2225 train_time:89861ms step_avg:60.43ms
step:1488/2225 train_time:89922ms step_avg:60.43ms
step:1489/2225 train_time:89983ms step_avg:60.43ms
step:1490/2225 train_time:90043ms step_avg:60.43ms
step:1491/2225 train_time:90106ms step_avg:60.43ms
step:1492/2225 train_time:90166ms step_avg:60.43ms
step:1493/2225 train_time:90228ms step_avg:60.43ms
step:1494/2225 train_time:90288ms step_avg:60.43ms
step:1495/2225 train_time:90350ms step_avg:60.43ms
step:1496/2225 train_time:90411ms step_avg:60.43ms
step:1497/2225 train_time:90472ms step_avg:60.44ms
step:1498/2225 train_time:90533ms step_avg:60.44ms
step:1499/2225 train_time:90594ms step_avg:60.44ms
step:1500/2225 train_time:90655ms step_avg:60.44ms
step:1500/2225 val_loss:3.4388 train_time:90717ms step_avg:60.48ms
step:1501/2225 train_time:90738ms step_avg:60.45ms
step:1502/2225 train_time:90778ms step_avg:60.44ms
step:1503/2225 train_time:90839ms step_avg:60.44ms
step:1504/2225 train_time:90899ms step_avg:60.44ms
step:1505/2225 train_time:90962ms step_avg:60.44ms
step:1506/2225 train_time:91022ms step_avg:60.44ms
step:1507/2225 train_time:91083ms step_avg:60.44ms
step:1508/2225 train_time:91142ms step_avg:60.44ms
step:1509/2225 train_time:91203ms step_avg:60.44ms
step:1510/2225 train_time:91263ms step_avg:60.44ms
step:1511/2225 train_time:91324ms step_avg:60.44ms
step:1512/2225 train_time:91383ms step_avg:60.44ms
step:1513/2225 train_time:91445ms step_avg:60.44ms
step:1514/2225 train_time:91504ms step_avg:60.44ms
step:1515/2225 train_time:91565ms step_avg:60.44ms
step:1516/2225 train_time:91631ms step_avg:60.44ms
step:1517/2225 train_time:91697ms step_avg:60.45ms
step:1518/2225 train_time:91761ms step_avg:60.45ms
step:1519/2225 train_time:91824ms step_avg:60.45ms
step:1520/2225 train_time:91886ms step_avg:60.45ms
step:1521/2225 train_time:91949ms step_avg:60.45ms
step:1522/2225 train_time:92009ms step_avg:60.45ms
step:1523/2225 train_time:92070ms step_avg:60.45ms
step:1524/2225 train_time:92130ms step_avg:60.45ms
step:1525/2225 train_time:92190ms step_avg:60.45ms
step:1526/2225 train_time:92250ms step_avg:60.45ms
step:1527/2225 train_time:92311ms step_avg:60.45ms
step:1528/2225 train_time:92371ms step_avg:60.45ms
step:1529/2225 train_time:92431ms step_avg:60.45ms
step:1530/2225 train_time:92491ms step_avg:60.45ms
step:1531/2225 train_time:92553ms step_avg:60.45ms
step:1532/2225 train_time:92614ms step_avg:60.45ms
step:1533/2225 train_time:92678ms step_avg:60.46ms
step:1534/2225 train_time:92740ms step_avg:60.46ms
step:1535/2225 train_time:92802ms step_avg:60.46ms
step:1536/2225 train_time:92863ms step_avg:60.46ms
step:1537/2225 train_time:92926ms step_avg:60.46ms
step:1538/2225 train_time:92986ms step_avg:60.46ms
step:1539/2225 train_time:93048ms step_avg:60.46ms
step:1540/2225 train_time:93108ms step_avg:60.46ms
step:1541/2225 train_time:93169ms step_avg:60.46ms
step:1542/2225 train_time:93229ms step_avg:60.46ms
step:1543/2225 train_time:93290ms step_avg:60.46ms
step:1544/2225 train_time:93350ms step_avg:60.46ms
step:1545/2225 train_time:93411ms step_avg:60.46ms
step:1546/2225 train_time:93470ms step_avg:60.46ms
step:1547/2225 train_time:93533ms step_avg:60.46ms
step:1548/2225 train_time:93593ms step_avg:60.46ms
step:1549/2225 train_time:93656ms step_avg:60.46ms
step:1550/2225 train_time:93717ms step_avg:60.46ms
step:1551/2225 train_time:93779ms step_avg:60.46ms
step:1552/2225 train_time:93841ms step_avg:60.46ms
step:1553/2225 train_time:93903ms step_avg:60.47ms
step:1554/2225 train_time:93963ms step_avg:60.47ms
step:1555/2225 train_time:94026ms step_avg:60.47ms
step:1556/2225 train_time:94086ms step_avg:60.47ms
step:1557/2225 train_time:94147ms step_avg:60.47ms
step:1558/2225 train_time:94207ms step_avg:60.47ms
step:1559/2225 train_time:94269ms step_avg:60.47ms
step:1560/2225 train_time:94329ms step_avg:60.47ms
step:1561/2225 train_time:94390ms step_avg:60.47ms
step:1562/2225 train_time:94451ms step_avg:60.47ms
step:1563/2225 train_time:94512ms step_avg:60.47ms
step:1564/2225 train_time:94572ms step_avg:60.47ms
step:1565/2225 train_time:94635ms step_avg:60.47ms
step:1566/2225 train_time:94695ms step_avg:60.47ms
step:1567/2225 train_time:94757ms step_avg:60.47ms
step:1568/2225 train_time:94818ms step_avg:60.47ms
step:1569/2225 train_time:94880ms step_avg:60.47ms
step:1570/2225 train_time:94941ms step_avg:60.47ms
step:1571/2225 train_time:95003ms step_avg:60.47ms
step:1572/2225 train_time:95063ms step_avg:60.47ms
step:1573/2225 train_time:95125ms step_avg:60.47ms
step:1574/2225 train_time:95185ms step_avg:60.47ms
step:1575/2225 train_time:95247ms step_avg:60.47ms
step:1576/2225 train_time:95307ms step_avg:60.47ms
step:1577/2225 train_time:95369ms step_avg:60.48ms
step:1578/2225 train_time:95430ms step_avg:60.48ms
step:1579/2225 train_time:95491ms step_avg:60.48ms
step:1580/2225 train_time:95552ms step_avg:60.48ms
step:1581/2225 train_time:95614ms step_avg:60.48ms
step:1582/2225 train_time:95675ms step_avg:60.48ms
step:1583/2225 train_time:95737ms step_avg:60.48ms
step:1584/2225 train_time:95798ms step_avg:60.48ms
step:1585/2225 train_time:95859ms step_avg:60.48ms
step:1586/2225 train_time:95919ms step_avg:60.48ms
step:1587/2225 train_time:95981ms step_avg:60.48ms
step:1588/2225 train_time:96041ms step_avg:60.48ms
step:1589/2225 train_time:96102ms step_avg:60.48ms
step:1590/2225 train_time:96163ms step_avg:60.48ms
step:1591/2225 train_time:96225ms step_avg:60.48ms
step:1592/2225 train_time:96286ms step_avg:60.48ms
step:1593/2225 train_time:96348ms step_avg:60.48ms
step:1594/2225 train_time:96407ms step_avg:60.48ms
step:1595/2225 train_time:96470ms step_avg:60.48ms
step:1596/2225 train_time:96530ms step_avg:60.48ms
step:1597/2225 train_time:96592ms step_avg:60.48ms
step:1598/2225 train_time:96653ms step_avg:60.48ms
step:1599/2225 train_time:96715ms step_avg:60.48ms
step:1600/2225 train_time:96776ms step_avg:60.49ms
step:1601/2225 train_time:96838ms step_avg:60.49ms
step:1602/2225 train_time:96899ms step_avg:60.49ms
step:1603/2225 train_time:96959ms step_avg:60.49ms
step:1604/2225 train_time:97020ms step_avg:60.49ms
step:1605/2225 train_time:97081ms step_avg:60.49ms
step:1606/2225 train_time:97141ms step_avg:60.49ms
step:1607/2225 train_time:97203ms step_avg:60.49ms
step:1608/2225 train_time:97264ms step_avg:60.49ms
step:1609/2225 train_time:97326ms step_avg:60.49ms
step:1610/2225 train_time:97386ms step_avg:60.49ms
step:1611/2225 train_time:97448ms step_avg:60.49ms
step:1612/2225 train_time:97508ms step_avg:60.49ms
step:1613/2225 train_time:97570ms step_avg:60.49ms
step:1614/2225 train_time:97631ms step_avg:60.49ms
step:1615/2225 train_time:97693ms step_avg:60.49ms
step:1616/2225 train_time:97753ms step_avg:60.49ms
step:1617/2225 train_time:97815ms step_avg:60.49ms
step:1618/2225 train_time:97875ms step_avg:60.49ms
step:1619/2225 train_time:97937ms step_avg:60.49ms
step:1620/2225 train_time:97997ms step_avg:60.49ms
step:1621/2225 train_time:98059ms step_avg:60.49ms
step:1622/2225 train_time:98119ms step_avg:60.49ms
step:1623/2225 train_time:98181ms step_avg:60.49ms
step:1624/2225 train_time:98241ms step_avg:60.49ms
step:1625/2225 train_time:98303ms step_avg:60.49ms
step:1626/2225 train_time:98363ms step_avg:60.49ms
step:1627/2225 train_time:98425ms step_avg:60.50ms
step:1628/2225 train_time:98486ms step_avg:60.50ms
step:1629/2225 train_time:98549ms step_avg:60.50ms
step:1630/2225 train_time:98609ms step_avg:60.50ms
step:1631/2225 train_time:98671ms step_avg:60.50ms
step:1632/2225 train_time:98732ms step_avg:60.50ms
step:1633/2225 train_time:98794ms step_avg:60.50ms
step:1634/2225 train_time:98854ms step_avg:60.50ms
step:1635/2225 train_time:98916ms step_avg:60.50ms
step:1636/2225 train_time:98976ms step_avg:60.50ms
step:1637/2225 train_time:99038ms step_avg:60.50ms
step:1638/2225 train_time:99098ms step_avg:60.50ms
step:1639/2225 train_time:99160ms step_avg:60.50ms
step:1640/2225 train_time:99220ms step_avg:60.50ms
step:1641/2225 train_time:99282ms step_avg:60.50ms
step:1642/2225 train_time:99343ms step_avg:60.50ms
step:1643/2225 train_time:99405ms step_avg:60.50ms
step:1644/2225 train_time:99466ms step_avg:60.50ms
step:1645/2225 train_time:99529ms step_avg:60.50ms
step:1646/2225 train_time:99589ms step_avg:60.50ms
step:1647/2225 train_time:99651ms step_avg:60.50ms
step:1648/2225 train_time:99712ms step_avg:60.50ms
step:1649/2225 train_time:99773ms step_avg:60.51ms
step:1650/2225 train_time:99834ms step_avg:60.51ms
step:1651/2225 train_time:99896ms step_avg:60.51ms
step:1652/2225 train_time:99956ms step_avg:60.51ms
step:1653/2225 train_time:100017ms step_avg:60.51ms
step:1654/2225 train_time:100077ms step_avg:60.51ms
step:1655/2225 train_time:100139ms step_avg:60.51ms
step:1656/2225 train_time:100199ms step_avg:60.51ms
step:1657/2225 train_time:100260ms step_avg:60.51ms
step:1658/2225 train_time:100320ms step_avg:60.51ms
step:1659/2225 train_time:100382ms step_avg:60.51ms
step:1660/2225 train_time:100442ms step_avg:60.51ms
step:1661/2225 train_time:100505ms step_avg:60.51ms
step:1662/2225 train_time:100566ms step_avg:60.51ms
step:1663/2225 train_time:100628ms step_avg:60.51ms
step:1664/2225 train_time:100689ms step_avg:60.51ms
step:1665/2225 train_time:100752ms step_avg:60.51ms
step:1666/2225 train_time:100812ms step_avg:60.51ms
step:1667/2225 train_time:100873ms step_avg:60.51ms
step:1668/2225 train_time:100934ms step_avg:60.51ms
step:1669/2225 train_time:100995ms step_avg:60.51ms
step:1670/2225 train_time:101056ms step_avg:60.51ms
step:1671/2225 train_time:101117ms step_avg:60.51ms
step:1672/2225 train_time:101178ms step_avg:60.51ms
step:1673/2225 train_time:101239ms step_avg:60.51ms
step:1674/2225 train_time:101299ms step_avg:60.51ms
step:1675/2225 train_time:101360ms step_avg:60.51ms
step:1676/2225 train_time:101421ms step_avg:60.51ms
step:1677/2225 train_time:101483ms step_avg:60.51ms
step:1678/2225 train_time:101543ms step_avg:60.51ms
step:1679/2225 train_time:101605ms step_avg:60.52ms
step:1680/2225 train_time:101666ms step_avg:60.52ms
step:1681/2225 train_time:101729ms step_avg:60.52ms
step:1682/2225 train_time:101790ms step_avg:60.52ms
step:1683/2225 train_time:101852ms step_avg:60.52ms
step:1684/2225 train_time:101913ms step_avg:60.52ms
step:1685/2225 train_time:101975ms step_avg:60.52ms
step:1686/2225 train_time:102035ms step_avg:60.52ms
step:1687/2225 train_time:102096ms step_avg:60.52ms
step:1688/2225 train_time:102156ms step_avg:60.52ms
step:1689/2225 train_time:102217ms step_avg:60.52ms
step:1690/2225 train_time:102278ms step_avg:60.52ms
step:1691/2225 train_time:102340ms step_avg:60.52ms
step:1692/2225 train_time:102400ms step_avg:60.52ms
step:1693/2225 train_time:102463ms step_avg:60.52ms
step:1694/2225 train_time:102523ms step_avg:60.52ms
step:1695/2225 train_time:102586ms step_avg:60.52ms
step:1696/2225 train_time:102647ms step_avg:60.52ms
step:1697/2225 train_time:102708ms step_avg:60.52ms
step:1698/2225 train_time:102769ms step_avg:60.52ms
step:1699/2225 train_time:102831ms step_avg:60.52ms
step:1700/2225 train_time:102892ms step_avg:60.52ms
step:1701/2225 train_time:102953ms step_avg:60.53ms
step:1702/2225 train_time:103014ms step_avg:60.52ms
step:1703/2225 train_time:103076ms step_avg:60.53ms
step:1704/2225 train_time:103135ms step_avg:60.53ms
step:1705/2225 train_time:103197ms step_avg:60.53ms
step:1706/2225 train_time:103257ms step_avg:60.53ms
step:1707/2225 train_time:103319ms step_avg:60.53ms
step:1708/2225 train_time:103379ms step_avg:60.53ms
step:1709/2225 train_time:103440ms step_avg:60.53ms
step:1710/2225 train_time:103500ms step_avg:60.53ms
step:1711/2225 train_time:103562ms step_avg:60.53ms
step:1712/2225 train_time:103623ms step_avg:60.53ms
step:1713/2225 train_time:103686ms step_avg:60.53ms
step:1714/2225 train_time:103747ms step_avg:60.53ms
step:1715/2225 train_time:103809ms step_avg:60.53ms
step:1716/2225 train_time:103870ms step_avg:60.53ms
step:1717/2225 train_time:103932ms step_avg:60.53ms
step:1718/2225 train_time:103993ms step_avg:60.53ms
step:1719/2225 train_time:104055ms step_avg:60.53ms
step:1720/2225 train_time:104115ms step_avg:60.53ms
step:1721/2225 train_time:104176ms step_avg:60.53ms
step:1722/2225 train_time:104236ms step_avg:60.53ms
step:1723/2225 train_time:104298ms step_avg:60.53ms
step:1724/2225 train_time:104357ms step_avg:60.53ms
step:1725/2225 train_time:104418ms step_avg:60.53ms
step:1726/2225 train_time:104479ms step_avg:60.53ms
step:1727/2225 train_time:104541ms step_avg:60.53ms
step:1728/2225 train_time:104601ms step_avg:60.53ms
step:1729/2225 train_time:104663ms step_avg:60.53ms
step:1730/2225 train_time:104724ms step_avg:60.53ms
step:1731/2225 train_time:104786ms step_avg:60.54ms
step:1732/2225 train_time:104847ms step_avg:60.54ms
step:1733/2225 train_time:104910ms step_avg:60.54ms
step:1734/2225 train_time:104970ms step_avg:60.54ms
step:1735/2225 train_time:105031ms step_avg:60.54ms
step:1736/2225 train_time:105091ms step_avg:60.54ms
step:1737/2225 train_time:105153ms step_avg:60.54ms
step:1738/2225 train_time:105213ms step_avg:60.54ms
step:1739/2225 train_time:105274ms step_avg:60.54ms
step:1740/2225 train_time:105335ms step_avg:60.54ms
step:1741/2225 train_time:105396ms step_avg:60.54ms
step:1742/2225 train_time:105457ms step_avg:60.54ms
step:1743/2225 train_time:105519ms step_avg:60.54ms
step:1744/2225 train_time:105580ms step_avg:60.54ms
step:1745/2225 train_time:105641ms step_avg:60.54ms
step:1746/2225 train_time:105702ms step_avg:60.54ms
step:1747/2225 train_time:105765ms step_avg:60.54ms
step:1748/2225 train_time:105825ms step_avg:60.54ms
step:1749/2225 train_time:105888ms step_avg:60.54ms
step:1750/2225 train_time:105948ms step_avg:60.54ms
step:1750/2225 val_loss:3.3737 train_time:106011ms step_avg:60.58ms
step:1751/2225 train_time:106032ms step_avg:60.56ms
step:1752/2225 train_time:106073ms step_avg:60.54ms
step:1753/2225 train_time:106138ms step_avg:60.55ms
step:1754/2225 train_time:106202ms step_avg:60.55ms
step:1755/2225 train_time:106264ms step_avg:60.55ms
step:1756/2225 train_time:106325ms step_avg:60.55ms
step:1757/2225 train_time:106386ms step_avg:60.55ms
step:1758/2225 train_time:106445ms step_avg:60.55ms
step:1759/2225 train_time:106506ms step_avg:60.55ms
step:1760/2225 train_time:106565ms step_avg:60.55ms
step:1761/2225 train_time:106626ms step_avg:60.55ms
step:1762/2225 train_time:106685ms step_avg:60.55ms
step:1763/2225 train_time:106746ms step_avg:60.55ms
step:1764/2225 train_time:106805ms step_avg:60.55ms
step:1765/2225 train_time:106866ms step_avg:60.55ms
step:1766/2225 train_time:106926ms step_avg:60.55ms
step:1767/2225 train_time:106989ms step_avg:60.55ms
step:1768/2225 train_time:107051ms step_avg:60.55ms
step:1769/2225 train_time:107116ms step_avg:60.55ms
step:1770/2225 train_time:107177ms step_avg:60.55ms
step:1771/2225 train_time:107240ms step_avg:60.55ms
step:1772/2225 train_time:107300ms step_avg:60.55ms
step:1773/2225 train_time:107362ms step_avg:60.55ms
step:1774/2225 train_time:107422ms step_avg:60.55ms
step:1775/2225 train_time:107483ms step_avg:60.55ms
step:1776/2225 train_time:107542ms step_avg:60.55ms
step:1777/2225 train_time:107603ms step_avg:60.55ms
step:1778/2225 train_time:107662ms step_avg:60.55ms
step:1779/2225 train_time:107724ms step_avg:60.55ms
step:1780/2225 train_time:107782ms step_avg:60.55ms
step:1781/2225 train_time:107844ms step_avg:60.55ms
step:1782/2225 train_time:107904ms step_avg:60.55ms
step:1783/2225 train_time:107966ms step_avg:60.55ms
step:1784/2225 train_time:108027ms step_avg:60.55ms
step:1785/2225 train_time:108090ms step_avg:60.55ms
step:1786/2225 train_time:108152ms step_avg:60.56ms
step:1787/2225 train_time:108215ms step_avg:60.56ms
step:1788/2225 train_time:108276ms step_avg:60.56ms
step:1789/2225 train_time:108338ms step_avg:60.56ms
step:1790/2225 train_time:108398ms step_avg:60.56ms
step:1791/2225 train_time:108460ms step_avg:60.56ms
step:1792/2225 train_time:108520ms step_avg:60.56ms
step:1793/2225 train_time:108582ms step_avg:60.56ms
step:1794/2225 train_time:108641ms step_avg:60.56ms
step:1795/2225 train_time:108701ms step_avg:60.56ms
step:1796/2225 train_time:108761ms step_avg:60.56ms
step:1797/2225 train_time:108822ms step_avg:60.56ms
step:1798/2225 train_time:108882ms step_avg:60.56ms
step:1799/2225 train_time:108944ms step_avg:60.56ms
step:1800/2225 train_time:109005ms step_avg:60.56ms
step:1801/2225 train_time:109068ms step_avg:60.56ms
step:1802/2225 train_time:109129ms step_avg:60.56ms
step:1803/2225 train_time:109193ms step_avg:60.56ms
step:1804/2225 train_time:109253ms step_avg:60.56ms
step:1805/2225 train_time:109316ms step_avg:60.56ms
step:1806/2225 train_time:109377ms step_avg:60.56ms
step:1807/2225 train_time:109439ms step_avg:60.56ms
step:1808/2225 train_time:109499ms step_avg:60.56ms
step:1809/2225 train_time:109561ms step_avg:60.56ms
step:1810/2225 train_time:109621ms step_avg:60.56ms
step:1811/2225 train_time:109682ms step_avg:60.56ms
step:1812/2225 train_time:109741ms step_avg:60.56ms
step:1813/2225 train_time:109802ms step_avg:60.56ms
step:1814/2225 train_time:109862ms step_avg:60.56ms
step:1815/2225 train_time:109924ms step_avg:60.56ms
step:1816/2225 train_time:109984ms step_avg:60.56ms
step:1817/2225 train_time:110047ms step_avg:60.57ms
step:1818/2225 train_time:110108ms step_avg:60.57ms
step:1819/2225 train_time:110170ms step_avg:60.57ms
step:1820/2225 train_time:110230ms step_avg:60.57ms
step:1821/2225 train_time:110293ms step_avg:60.57ms
step:1822/2225 train_time:110354ms step_avg:60.57ms
step:1823/2225 train_time:110416ms step_avg:60.57ms
step:1824/2225 train_time:110476ms step_avg:60.57ms
step:1825/2225 train_time:110538ms step_avg:60.57ms
step:1826/2225 train_time:110598ms step_avg:60.57ms
step:1827/2225 train_time:110659ms step_avg:60.57ms
step:1828/2225 train_time:110721ms step_avg:60.57ms
step:1829/2225 train_time:110781ms step_avg:60.57ms
step:1830/2225 train_time:110841ms step_avg:60.57ms
step:1831/2225 train_time:110901ms step_avg:60.57ms
step:1832/2225 train_time:110961ms step_avg:60.57ms
step:1833/2225 train_time:111024ms step_avg:60.57ms
step:1834/2225 train_time:111084ms step_avg:60.57ms
step:1835/2225 train_time:111147ms step_avg:60.57ms
step:1836/2225 train_time:111207ms step_avg:60.57ms
step:1837/2225 train_time:111270ms step_avg:60.57ms
step:1838/2225 train_time:111331ms step_avg:60.57ms
step:1839/2225 train_time:111393ms step_avg:60.57ms
step:1840/2225 train_time:111455ms step_avg:60.57ms
step:1841/2225 train_time:111517ms step_avg:60.57ms
step:1842/2225 train_time:111577ms step_avg:60.57ms
step:1843/2225 train_time:111638ms step_avg:60.57ms
step:1844/2225 train_time:111698ms step_avg:60.57ms
step:1845/2225 train_time:111760ms step_avg:60.57ms
step:1846/2225 train_time:111820ms step_avg:60.57ms
step:1847/2225 train_time:111881ms step_avg:60.57ms
step:1848/2225 train_time:111941ms step_avg:60.57ms
step:1849/2225 train_time:112002ms step_avg:60.57ms
step:1850/2225 train_time:112063ms step_avg:60.57ms
step:1851/2225 train_time:112124ms step_avg:60.58ms
step:1852/2225 train_time:112185ms step_avg:60.57ms
step:1853/2225 train_time:112247ms step_avg:60.58ms
step:1854/2225 train_time:112308ms step_avg:60.58ms
step:1855/2225 train_time:112370ms step_avg:60.58ms
step:1856/2225 train_time:112431ms step_avg:60.58ms
step:1857/2225 train_time:112493ms step_avg:60.58ms
step:1858/2225 train_time:112553ms step_avg:60.58ms
step:1859/2225 train_time:112616ms step_avg:60.58ms
step:1860/2225 train_time:112676ms step_avg:60.58ms
step:1861/2225 train_time:112738ms step_avg:60.58ms
step:1862/2225 train_time:112798ms step_avg:60.58ms
step:1863/2225 train_time:112860ms step_avg:60.58ms
step:1864/2225 train_time:112920ms step_avg:60.58ms
step:1865/2225 train_time:112981ms step_avg:60.58ms
step:1866/2225 train_time:113041ms step_avg:60.58ms
step:1867/2225 train_time:113103ms step_avg:60.58ms
step:1868/2225 train_time:113164ms step_avg:60.58ms
step:1869/2225 train_time:113226ms step_avg:60.58ms
step:1870/2225 train_time:113286ms step_avg:60.58ms
step:1871/2225 train_time:113348ms step_avg:60.58ms
step:1872/2225 train_time:113408ms step_avg:60.58ms
step:1873/2225 train_time:113470ms step_avg:60.58ms
step:1874/2225 train_time:113530ms step_avg:60.58ms
step:1875/2225 train_time:113592ms step_avg:60.58ms
step:1876/2225 train_time:113653ms step_avg:60.58ms
step:1877/2225 train_time:113715ms step_avg:60.58ms
step:1878/2225 train_time:113776ms step_avg:60.58ms
step:1879/2225 train_time:113837ms step_avg:60.58ms
step:1880/2225 train_time:113897ms step_avg:60.58ms
step:1881/2225 train_time:113958ms step_avg:60.58ms
step:1882/2225 train_time:114019ms step_avg:60.58ms
step:1883/2225 train_time:114080ms step_avg:60.58ms
step:1884/2225 train_time:114141ms step_avg:60.58ms
step:1885/2225 train_time:114202ms step_avg:60.58ms
step:1886/2225 train_time:114263ms step_avg:60.58ms
step:1887/2225 train_time:114325ms step_avg:60.59ms
step:1888/2225 train_time:114385ms step_avg:60.59ms
step:1889/2225 train_time:114448ms step_avg:60.59ms
step:1890/2225 train_time:114508ms step_avg:60.59ms
step:1891/2225 train_time:114571ms step_avg:60.59ms
step:1892/2225 train_time:114631ms step_avg:60.59ms
step:1893/2225 train_time:114693ms step_avg:60.59ms
step:1894/2225 train_time:114754ms step_avg:60.59ms
step:1895/2225 train_time:114817ms step_avg:60.59ms
step:1896/2225 train_time:114877ms step_avg:60.59ms
step:1897/2225 train_time:114939ms step_avg:60.59ms
step:1898/2225 train_time:114998ms step_avg:60.59ms
step:1899/2225 train_time:115060ms step_avg:60.59ms
step:1900/2225 train_time:115121ms step_avg:60.59ms
step:1901/2225 train_time:115183ms step_avg:60.59ms
step:1902/2225 train_time:115242ms step_avg:60.59ms
step:1903/2225 train_time:115304ms step_avg:60.59ms
step:1904/2225 train_time:115364ms step_avg:60.59ms
step:1905/2225 train_time:115426ms step_avg:60.59ms
step:1906/2225 train_time:115486ms step_avg:60.59ms
step:1907/2225 train_time:115548ms step_avg:60.59ms
step:1908/2225 train_time:115607ms step_avg:60.59ms
step:1909/2225 train_time:115670ms step_avg:60.59ms
step:1910/2225 train_time:115730ms step_avg:60.59ms
step:1911/2225 train_time:115793ms step_avg:60.59ms
step:1912/2225 train_time:115853ms step_avg:60.59ms
step:1913/2225 train_time:115916ms step_avg:60.59ms
step:1914/2225 train_time:115976ms step_avg:60.59ms
step:1915/2225 train_time:116038ms step_avg:60.59ms
step:1916/2225 train_time:116098ms step_avg:60.59ms
step:1917/2225 train_time:116160ms step_avg:60.59ms
step:1918/2225 train_time:116221ms step_avg:60.59ms
step:1919/2225 train_time:116282ms step_avg:60.60ms
step:1920/2225 train_time:116343ms step_avg:60.60ms
step:1921/2225 train_time:116404ms step_avg:60.60ms
step:1922/2225 train_time:116465ms step_avg:60.60ms
step:1923/2225 train_time:116527ms step_avg:60.60ms
step:1924/2225 train_time:116586ms step_avg:60.60ms
step:1925/2225 train_time:116648ms step_avg:60.60ms
step:1926/2225 train_time:116708ms step_avg:60.60ms
step:1927/2225 train_time:116770ms step_avg:60.60ms
step:1928/2225 train_time:116830ms step_avg:60.60ms
step:1929/2225 train_time:116893ms step_avg:60.60ms
step:1930/2225 train_time:116954ms step_avg:60.60ms
step:1931/2225 train_time:117016ms step_avg:60.60ms
step:1932/2225 train_time:117076ms step_avg:60.60ms
step:1933/2225 train_time:117138ms step_avg:60.60ms
step:1934/2225 train_time:117198ms step_avg:60.60ms
step:1935/2225 train_time:117261ms step_avg:60.60ms
step:1936/2225 train_time:117322ms step_avg:60.60ms
step:1937/2225 train_time:117383ms step_avg:60.60ms
step:1938/2225 train_time:117443ms step_avg:60.60ms
step:1939/2225 train_time:117505ms step_avg:60.60ms
step:1940/2225 train_time:117565ms step_avg:60.60ms
step:1941/2225 train_time:117628ms step_avg:60.60ms
step:1942/2225 train_time:117687ms step_avg:60.60ms
step:1943/2225 train_time:117749ms step_avg:60.60ms
step:1944/2225 train_time:117810ms step_avg:60.60ms
step:1945/2225 train_time:117872ms step_avg:60.60ms
step:1946/2225 train_time:117932ms step_avg:60.60ms
step:1947/2225 train_time:117994ms step_avg:60.60ms
step:1948/2225 train_time:118056ms step_avg:60.60ms
step:1949/2225 train_time:118118ms step_avg:60.60ms
step:1950/2225 train_time:118178ms step_avg:60.60ms
step:1951/2225 train_time:118240ms step_avg:60.60ms
step:1952/2225 train_time:118300ms step_avg:60.60ms
step:1953/2225 train_time:118361ms step_avg:60.60ms
step:1954/2225 train_time:118422ms step_avg:60.60ms
step:1955/2225 train_time:118483ms step_avg:60.61ms
step:1956/2225 train_time:118543ms step_avg:60.60ms
step:1957/2225 train_time:118605ms step_avg:60.61ms
step:1958/2225 train_time:118665ms step_avg:60.61ms
step:1959/2225 train_time:118726ms step_avg:60.61ms
step:1960/2225 train_time:118786ms step_avg:60.61ms
step:1961/2225 train_time:118849ms step_avg:60.61ms
step:1962/2225 train_time:118911ms step_avg:60.61ms
step:1963/2225 train_time:118973ms step_avg:60.61ms
step:1964/2225 train_time:119033ms step_avg:60.61ms
step:1965/2225 train_time:119095ms step_avg:60.61ms
step:1966/2225 train_time:119156ms step_avg:60.61ms
step:1967/2225 train_time:119218ms step_avg:60.61ms
step:1968/2225 train_time:119278ms step_avg:60.61ms
step:1969/2225 train_time:119340ms step_avg:60.61ms
step:1970/2225 train_time:119400ms step_avg:60.61ms
step:1971/2225 train_time:119462ms step_avg:60.61ms
step:1972/2225 train_time:119522ms step_avg:60.61ms
step:1973/2225 train_time:119583ms step_avg:60.61ms
step:1974/2225 train_time:119643ms step_avg:60.61ms
step:1975/2225 train_time:119705ms step_avg:60.61ms
step:1976/2225 train_time:119765ms step_avg:60.61ms
step:1977/2225 train_time:119827ms step_avg:60.61ms
step:1978/2225 train_time:119887ms step_avg:60.61ms
step:1979/2225 train_time:119950ms step_avg:60.61ms
step:1980/2225 train_time:120011ms step_avg:60.61ms
step:1981/2225 train_time:120074ms step_avg:60.61ms
step:1982/2225 train_time:120134ms step_avg:60.61ms
step:1983/2225 train_time:120196ms step_avg:60.61ms
step:1984/2225 train_time:120257ms step_avg:60.61ms
step:1985/2225 train_time:120319ms step_avg:60.61ms
step:1986/2225 train_time:120379ms step_avg:60.61ms
step:1987/2225 train_time:120441ms step_avg:60.61ms
step:1988/2225 train_time:120501ms step_avg:60.61ms
step:1989/2225 train_time:120563ms step_avg:60.61ms
step:1990/2225 train_time:120623ms step_avg:60.61ms
step:1991/2225 train_time:120684ms step_avg:60.61ms
step:1992/2225 train_time:120744ms step_avg:60.61ms
step:1993/2225 train_time:120806ms step_avg:60.62ms
step:1994/2225 train_time:120866ms step_avg:60.61ms
step:1995/2225 train_time:120928ms step_avg:60.62ms
step:1996/2225 train_time:120989ms step_avg:60.62ms
step:1997/2225 train_time:121052ms step_avg:60.62ms
step:1998/2225 train_time:121112ms step_avg:60.62ms
step:1999/2225 train_time:121175ms step_avg:60.62ms
step:2000/2225 train_time:121235ms step_avg:60.62ms
step:2000/2225 val_loss:3.3193 train_time:121299ms step_avg:60.65ms
step:2001/2225 train_time:121320ms step_avg:60.63ms
step:2002/2225 train_time:121361ms step_avg:60.62ms
step:2003/2225 train_time:121426ms step_avg:60.62ms
step:2004/2225 train_time:121487ms step_avg:60.62ms
step:2005/2225 train_time:121549ms step_avg:60.62ms
step:2006/2225 train_time:121609ms step_avg:60.62ms
step:2007/2225 train_time:121670ms step_avg:60.62ms
step:2008/2225 train_time:121730ms step_avg:60.62ms
step:2009/2225 train_time:121792ms step_avg:60.62ms
step:2010/2225 train_time:121851ms step_avg:60.62ms
step:2011/2225 train_time:121913ms step_avg:60.62ms
step:2012/2225 train_time:121973ms step_avg:60.62ms
step:2013/2225 train_time:122033ms step_avg:60.62ms
step:2014/2225 train_time:122093ms step_avg:60.62ms
step:2015/2225 train_time:122154ms step_avg:60.62ms
step:2016/2225 train_time:122214ms step_avg:60.62ms
step:2017/2225 train_time:122278ms step_avg:60.62ms
step:2018/2225 train_time:122340ms step_avg:60.62ms
step:2019/2225 train_time:122403ms step_avg:60.63ms
step:2020/2225 train_time:122465ms step_avg:60.63ms
step:2021/2225 train_time:122527ms step_avg:60.63ms
step:2022/2225 train_time:122587ms step_avg:60.63ms
step:2023/2225 train_time:122649ms step_avg:60.63ms
step:2024/2225 train_time:122708ms step_avg:60.63ms
step:2025/2225 train_time:122770ms step_avg:60.63ms
step:2026/2225 train_time:122830ms step_avg:60.63ms
step:2027/2225 train_time:122891ms step_avg:60.63ms
step:2028/2225 train_time:122950ms step_avg:60.63ms
step:2029/2225 train_time:123012ms step_avg:60.63ms
step:2030/2225 train_time:123072ms step_avg:60.63ms
step:2031/2225 train_time:123134ms step_avg:60.63ms
step:2032/2225 train_time:123194ms step_avg:60.63ms
step:2033/2225 train_time:123256ms step_avg:60.63ms
step:2034/2225 train_time:123318ms step_avg:60.63ms
step:2035/2225 train_time:123380ms step_avg:60.63ms
step:2036/2225 train_time:123441ms step_avg:60.63ms
step:2037/2225 train_time:123503ms step_avg:60.63ms
step:2038/2225 train_time:123564ms step_avg:60.63ms
step:2039/2225 train_time:123625ms step_avg:60.63ms
step:2040/2225 train_time:123685ms step_avg:60.63ms
step:2041/2225 train_time:123746ms step_avg:60.63ms
step:2042/2225 train_time:123806ms step_avg:60.63ms
step:2043/2225 train_time:123868ms step_avg:60.63ms
step:2044/2225 train_time:123928ms step_avg:60.63ms
step:2045/2225 train_time:123989ms step_avg:60.63ms
step:2046/2225 train_time:124049ms step_avg:60.63ms
step:2047/2225 train_time:124109ms step_avg:60.63ms
step:2048/2225 train_time:124169ms step_avg:60.63ms
step:2049/2225 train_time:124233ms step_avg:60.63ms
step:2050/2225 train_time:124295ms step_avg:60.63ms
step:2051/2225 train_time:124358ms step_avg:60.63ms
step:2052/2225 train_time:124418ms step_avg:60.63ms
step:2053/2225 train_time:124481ms step_avg:60.63ms
step:2054/2225 train_time:124542ms step_avg:60.63ms
step:2055/2225 train_time:124603ms step_avg:60.63ms
step:2056/2225 train_time:124664ms step_avg:60.63ms
step:2057/2225 train_time:124725ms step_avg:60.63ms
step:2058/2225 train_time:124785ms step_avg:60.63ms
step:2059/2225 train_time:124847ms step_avg:60.63ms
step:2060/2225 train_time:124906ms step_avg:60.63ms
step:2061/2225 train_time:124968ms step_avg:60.63ms
step:2062/2225 train_time:125028ms step_avg:60.63ms
step:2063/2225 train_time:125089ms step_avg:60.63ms
step:2064/2225 train_time:125149ms step_avg:60.63ms
step:2065/2225 train_time:125211ms step_avg:60.63ms
step:2066/2225 train_time:125273ms step_avg:60.64ms
step:2067/2225 train_time:125336ms step_avg:60.64ms
step:2068/2225 train_time:125397ms step_avg:60.64ms
step:2069/2225 train_time:125459ms step_avg:60.64ms
step:2070/2225 train_time:125519ms step_avg:60.64ms
step:2071/2225 train_time:125582ms step_avg:60.64ms
step:2072/2225 train_time:125643ms step_avg:60.64ms
step:2073/2225 train_time:125704ms step_avg:60.64ms
step:2074/2225 train_time:125764ms step_avg:60.64ms
step:2075/2225 train_time:125826ms step_avg:60.64ms
step:2076/2225 train_time:125886ms step_avg:60.64ms
step:2077/2225 train_time:125947ms step_avg:60.64ms
step:2078/2225 train_time:126006ms step_avg:60.64ms
step:2079/2225 train_time:126068ms step_avg:60.64ms
step:2080/2225 train_time:126128ms step_avg:60.64ms
step:2081/2225 train_time:126191ms step_avg:60.64ms
step:2082/2225 train_time:126252ms step_avg:60.64ms
step:2083/2225 train_time:126315ms step_avg:60.64ms
step:2084/2225 train_time:126376ms step_avg:60.64ms
step:2085/2225 train_time:126439ms step_avg:60.64ms
step:2086/2225 train_time:126499ms step_avg:60.64ms
step:2087/2225 train_time:126561ms step_avg:60.64ms
step:2088/2225 train_time:126621ms step_avg:60.64ms
step:2089/2225 train_time:126683ms step_avg:60.64ms
step:2090/2225 train_time:126743ms step_avg:60.64ms
step:2091/2225 train_time:126804ms step_avg:60.64ms
step:2092/2225 train_time:126864ms step_avg:60.64ms
step:2093/2225 train_time:126926ms step_avg:60.64ms
step:2094/2225 train_time:126986ms step_avg:60.64ms
step:2095/2225 train_time:127048ms step_avg:60.64ms
step:2096/2225 train_time:127108ms step_avg:60.64ms
step:2097/2225 train_time:127170ms step_avg:60.64ms
step:2098/2225 train_time:127230ms step_avg:60.64ms
step:2099/2225 train_time:127293ms step_avg:60.64ms
step:2100/2225 train_time:127353ms step_avg:60.64ms
step:2101/2225 train_time:127415ms step_avg:60.65ms
step:2102/2225 train_time:127476ms step_avg:60.65ms
step:2103/2225 train_time:127538ms step_avg:60.65ms
step:2104/2225 train_time:127599ms step_avg:60.65ms
step:2105/2225 train_time:127660ms step_avg:60.65ms
step:2106/2225 train_time:127720ms step_avg:60.65ms
step:2107/2225 train_time:127782ms step_avg:60.65ms
step:2108/2225 train_time:127843ms step_avg:60.65ms
step:2109/2225 train_time:127904ms step_avg:60.65ms
step:2110/2225 train_time:127964ms step_avg:60.65ms
step:2111/2225 train_time:128025ms step_avg:60.65ms
step:2112/2225 train_time:128086ms step_avg:60.65ms
step:2113/2225 train_time:128148ms step_avg:60.65ms
step:2114/2225 train_time:128207ms step_avg:60.65ms
step:2115/2225 train_time:128270ms step_avg:60.65ms
step:2116/2225 train_time:128330ms step_avg:60.65ms
step:2117/2225 train_time:128393ms step_avg:60.65ms
step:2118/2225 train_time:128454ms step_avg:60.65ms
step:2119/2225 train_time:128516ms step_avg:60.65ms
step:2120/2225 train_time:128576ms step_avg:60.65ms
step:2121/2225 train_time:128639ms step_avg:60.65ms
step:2122/2225 train_time:128699ms step_avg:60.65ms
step:2123/2225 train_time:128761ms step_avg:60.65ms
step:2124/2225 train_time:128821ms step_avg:60.65ms
step:2125/2225 train_time:128883ms step_avg:60.65ms
step:2126/2225 train_time:128943ms step_avg:60.65ms
step:2127/2225 train_time:129004ms step_avg:60.65ms
step:2128/2225 train_time:129064ms step_avg:60.65ms
step:2129/2225 train_time:129126ms step_avg:60.65ms
step:2130/2225 train_time:129186ms step_avg:60.65ms
step:2131/2225 train_time:129248ms step_avg:60.65ms
step:2132/2225 train_time:129308ms step_avg:60.65ms
step:2133/2225 train_time:129371ms step_avg:60.65ms
step:2134/2225 train_time:129432ms step_avg:60.65ms
step:2135/2225 train_time:129494ms step_avg:60.65ms
step:2136/2225 train_time:129555ms step_avg:60.65ms
step:2137/2225 train_time:129617ms step_avg:60.65ms
step:2138/2225 train_time:129678ms step_avg:60.65ms
step:2139/2225 train_time:129740ms step_avg:60.65ms
step:2140/2225 train_time:129800ms step_avg:60.65ms
step:2141/2225 train_time:129861ms step_avg:60.65ms
step:2142/2225 train_time:129922ms step_avg:60.65ms
step:2143/2225 train_time:129984ms step_avg:60.66ms
step:2144/2225 train_time:130044ms step_avg:60.65ms
step:2145/2225 train_time:130105ms step_avg:60.66ms
step:2146/2225 train_time:130165ms step_avg:60.65ms
step:2147/2225 train_time:130228ms step_avg:60.66ms
step:2148/2225 train_time:130287ms step_avg:60.66ms
step:2149/2225 train_time:130349ms step_avg:60.66ms
step:2150/2225 train_time:130409ms step_avg:60.66ms
step:2151/2225 train_time:130472ms step_avg:60.66ms
step:2152/2225 train_time:130533ms step_avg:60.66ms
step:2153/2225 train_time:130595ms step_avg:60.66ms
step:2154/2225 train_time:130656ms step_avg:60.66ms
step:2155/2225 train_time:130717ms step_avg:60.66ms
step:2156/2225 train_time:130777ms step_avg:60.66ms
step:2157/2225 train_time:130839ms step_avg:60.66ms
step:2158/2225 train_time:130900ms step_avg:60.66ms
step:2159/2225 train_time:130962ms step_avg:60.66ms
step:2160/2225 train_time:131023ms step_avg:60.66ms
step:2161/2225 train_time:131084ms step_avg:60.66ms
step:2162/2225 train_time:131145ms step_avg:60.66ms
step:2163/2225 train_time:131206ms step_avg:60.66ms
step:2164/2225 train_time:131266ms step_avg:60.66ms
step:2165/2225 train_time:131328ms step_avg:60.66ms
step:2166/2225 train_time:131387ms step_avg:60.66ms
step:2167/2225 train_time:131449ms step_avg:60.66ms
step:2168/2225 train_time:131510ms step_avg:60.66ms
step:2169/2225 train_time:131573ms step_avg:60.66ms
step:2170/2225 train_time:131634ms step_avg:60.66ms
step:2171/2225 train_time:131696ms step_avg:60.66ms
step:2172/2225 train_time:131756ms step_avg:60.66ms
step:2173/2225 train_time:131819ms step_avg:60.66ms
step:2174/2225 train_time:131879ms step_avg:60.66ms
step:2175/2225 train_time:131941ms step_avg:60.66ms
step:2176/2225 train_time:132001ms step_avg:60.66ms
step:2177/2225 train_time:132063ms step_avg:60.66ms
step:2178/2225 train_time:132123ms step_avg:60.66ms
step:2179/2225 train_time:132184ms step_avg:60.66ms
step:2180/2225 train_time:132245ms step_avg:60.66ms
step:2181/2225 train_time:132306ms step_avg:60.66ms
step:2182/2225 train_time:132367ms step_avg:60.66ms
step:2183/2225 train_time:132429ms step_avg:60.66ms
step:2184/2225 train_time:132489ms step_avg:60.66ms
step:2185/2225 train_time:132551ms step_avg:60.66ms
step:2186/2225 train_time:132612ms step_avg:60.66ms
step:2187/2225 train_time:132675ms step_avg:60.67ms
step:2188/2225 train_time:132736ms step_avg:60.67ms
step:2189/2225 train_time:132798ms step_avg:60.67ms
step:2190/2225 train_time:132859ms step_avg:60.67ms
step:2191/2225 train_time:132921ms step_avg:60.67ms
step:2192/2225 train_time:132981ms step_avg:60.67ms
step:2193/2225 train_time:133043ms step_avg:60.67ms
step:2194/2225 train_time:133103ms step_avg:60.67ms
step:2195/2225 train_time:133164ms step_avg:60.67ms
step:2196/2225 train_time:133225ms step_avg:60.67ms
step:2197/2225 train_time:133286ms step_avg:60.67ms
step:2198/2225 train_time:133347ms step_avg:60.67ms
step:2199/2225 train_time:133408ms step_avg:60.67ms
step:2200/2225 train_time:133469ms step_avg:60.67ms
step:2201/2225 train_time:133531ms step_avg:60.67ms
step:2202/2225 train_time:133592ms step_avg:60.67ms
step:2203/2225 train_time:133655ms step_avg:60.67ms
step:2204/2225 train_time:133716ms step_avg:60.67ms
step:2205/2225 train_time:133779ms step_avg:60.67ms
step:2206/2225 train_time:133840ms step_avg:60.67ms
step:2207/2225 train_time:133900ms step_avg:60.67ms
step:2208/2225 train_time:133961ms step_avg:60.67ms
step:2209/2225 train_time:134023ms step_avg:60.67ms
step:2210/2225 train_time:134083ms step_avg:60.67ms
step:2211/2225 train_time:134145ms step_avg:60.67ms
step:2212/2225 train_time:134205ms step_avg:60.67ms
step:2213/2225 train_time:134266ms step_avg:60.67ms
step:2214/2225 train_time:134327ms step_avg:60.67ms
step:2215/2225 train_time:134390ms step_avg:60.67ms
step:2216/2225 train_time:134451ms step_avg:60.67ms
step:2217/2225 train_time:134513ms step_avg:60.67ms
step:2218/2225 train_time:134573ms step_avg:60.67ms
step:2219/2225 train_time:134635ms step_avg:60.67ms
step:2220/2225 train_time:134696ms step_avg:60.67ms
step:2221/2225 train_time:134758ms step_avg:60.67ms
step:2222/2225 train_time:134819ms step_avg:60.67ms
step:2223/2225 train_time:134881ms step_avg:60.68ms
step:2224/2225 train_time:134941ms step_avg:60.67ms
step:2225/2225 train_time:135003ms step_avg:60.68ms
step:2225/2225 val_loss:3.2780 train_time:135063ms step_avg:60.70ms
peak memory allocated: 29248 MiB reserved: 47336 MiB

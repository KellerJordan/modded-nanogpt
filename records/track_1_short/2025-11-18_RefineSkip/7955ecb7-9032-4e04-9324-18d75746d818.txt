import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i ==7:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections[0]
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i ==4:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2185  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 18 21:17:51 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   36C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   38C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   39C    P0            126W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   36C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   33C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          143797      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          143798      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          143799      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          143800      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          143801      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          143802      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          143803      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          143804      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          143798      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          143799      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          143800      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          143801      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          143802      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          143803      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          143804      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2225 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2225 train_time:134ms step_avg:133.64ms
step:2/2225 train_time:176ms step_avg:88.08ms
step:3/2225 train_time:198ms step_avg:65.93ms
step:4/2225 train_time:250ms step_avg:62.38ms
step:5/2225 train_time:308ms step_avg:61.63ms
step:6/2225 train_time:375ms step_avg:62.50ms
step:7/2225 train_time:446ms step_avg:63.73ms
step:8/2225 train_time:504ms step_avg:63.01ms
step:9/2225 train_time:564ms step_avg:62.68ms
step:10/2225 train_time:623ms step_avg:62.27ms
step:11/2225 train_time:683ms step_avg:62.08ms
step:12/2225 train_time:742ms step_avg:61.82ms
step:13/2225 train_time:802ms step_avg:61.70ms
step:14/2225 train_time:861ms step_avg:61.49ms
step:15/2225 train_time:921ms step_avg:61.40ms
step:16/2225 train_time:980ms step_avg:61.25ms
step:17/2225 train_time:1040ms step_avg:61.18ms
step:18/2225 train_time:1102ms step_avg:61.19ms
step:19/2225 train_time:1165ms step_avg:61.34ms
step:20/2225 train_time:1225ms step_avg:61.27ms
step:21/2225 train_time:1287ms step_avg:61.30ms
step:22/2225 train_time:1350ms step_avg:61.37ms
step:23/2225 train_time:1413ms step_avg:61.41ms
step:24/2225 train_time:1473ms step_avg:61.35ms
step:25/2225 train_time:1534ms step_avg:61.37ms
step:26/2225 train_time:1594ms step_avg:61.30ms
step:27/2225 train_time:1655ms step_avg:61.30ms
step:28/2225 train_time:1715ms step_avg:61.25ms
step:29/2225 train_time:1776ms step_avg:61.24ms
step:30/2225 train_time:1835ms step_avg:61.18ms
step:31/2225 train_time:1897ms step_avg:61.18ms
step:32/2225 train_time:1956ms step_avg:61.12ms
step:33/2225 train_time:2017ms step_avg:61.13ms
step:34/2225 train_time:2077ms step_avg:61.08ms
step:35/2225 train_time:2139ms step_avg:61.10ms
step:36/2225 train_time:2199ms step_avg:61.08ms
step:37/2225 train_time:2260ms step_avg:61.08ms
step:38/2225 train_time:2320ms step_avg:61.06ms
step:39/2225 train_time:2383ms step_avg:61.09ms
step:40/2225 train_time:2442ms step_avg:61.06ms
step:41/2225 train_time:2503ms step_avg:61.06ms
step:42/2225 train_time:2563ms step_avg:61.01ms
step:43/2225 train_time:2624ms step_avg:61.01ms
step:44/2225 train_time:2682ms step_avg:60.96ms
step:45/2225 train_time:2744ms step_avg:60.97ms
step:46/2225 train_time:2802ms step_avg:60.92ms
step:47/2225 train_time:2864ms step_avg:60.93ms
step:48/2225 train_time:2923ms step_avg:60.89ms
step:49/2225 train_time:2983ms step_avg:60.88ms
step:50/2225 train_time:3042ms step_avg:60.84ms
step:51/2225 train_time:3103ms step_avg:60.83ms
step:52/2225 train_time:3162ms step_avg:60.80ms
step:53/2225 train_time:3223ms step_avg:60.81ms
step:54/2225 train_time:3283ms step_avg:60.79ms
step:55/2225 train_time:3343ms step_avg:60.78ms
step:56/2225 train_time:3402ms step_avg:60.76ms
step:57/2225 train_time:3464ms step_avg:60.77ms
step:58/2225 train_time:3523ms step_avg:60.75ms
step:59/2225 train_time:3585ms step_avg:60.76ms
step:60/2225 train_time:3644ms step_avg:60.73ms
step:61/2225 train_time:3705ms step_avg:60.73ms
step:62/2225 train_time:3764ms step_avg:60.71ms
step:63/2225 train_time:3825ms step_avg:60.72ms
step:64/2225 train_time:3884ms step_avg:60.69ms
step:65/2225 train_time:3945ms step_avg:60.69ms
step:66/2225 train_time:4004ms step_avg:60.67ms
step:67/2225 train_time:4065ms step_avg:60.68ms
step:68/2225 train_time:4124ms step_avg:60.65ms
step:69/2225 train_time:4185ms step_avg:60.66ms
step:70/2225 train_time:4245ms step_avg:60.64ms
step:71/2225 train_time:4306ms step_avg:60.65ms
step:72/2225 train_time:4366ms step_avg:60.63ms
step:73/2225 train_time:4427ms step_avg:60.65ms
step:74/2225 train_time:4487ms step_avg:60.63ms
step:75/2225 train_time:4548ms step_avg:60.64ms
step:76/2225 train_time:4607ms step_avg:60.62ms
step:77/2225 train_time:4668ms step_avg:60.63ms
step:78/2225 train_time:4728ms step_avg:60.61ms
step:79/2225 train_time:4790ms step_avg:60.63ms
step:80/2225 train_time:4850ms step_avg:60.62ms
step:81/2225 train_time:4911ms step_avg:60.63ms
step:82/2225 train_time:4971ms step_avg:60.62ms
step:83/2225 train_time:5033ms step_avg:60.63ms
step:84/2225 train_time:5092ms step_avg:60.62ms
step:85/2225 train_time:5153ms step_avg:60.62ms
step:86/2225 train_time:5214ms step_avg:60.62ms
step:87/2225 train_time:5275ms step_avg:60.64ms
step:88/2225 train_time:5335ms step_avg:60.62ms
step:89/2225 train_time:5396ms step_avg:60.63ms
step:90/2225 train_time:5456ms step_avg:60.62ms
step:91/2225 train_time:5517ms step_avg:60.63ms
step:92/2225 train_time:5577ms step_avg:60.62ms
step:93/2225 train_time:5638ms step_avg:60.62ms
step:94/2225 train_time:5698ms step_avg:60.62ms
step:95/2225 train_time:5759ms step_avg:60.62ms
step:96/2225 train_time:5819ms step_avg:60.61ms
step:97/2225 train_time:5880ms step_avg:60.62ms
step:98/2225 train_time:5939ms step_avg:60.60ms
step:99/2225 train_time:5999ms step_avg:60.60ms
step:100/2225 train_time:6058ms step_avg:60.58ms
step:101/2225 train_time:6121ms step_avg:60.60ms
step:102/2225 train_time:6179ms step_avg:60.57ms
step:103/2225 train_time:6239ms step_avg:60.57ms
step:104/2225 train_time:6298ms step_avg:60.56ms
step:105/2225 train_time:6359ms step_avg:60.57ms
step:106/2225 train_time:6419ms step_avg:60.55ms
step:107/2225 train_time:6479ms step_avg:60.56ms
step:108/2225 train_time:6539ms step_avg:60.54ms
step:109/2225 train_time:6599ms step_avg:60.54ms
step:110/2225 train_time:6659ms step_avg:60.53ms
step:111/2225 train_time:6720ms step_avg:60.54ms
step:112/2225 train_time:6780ms step_avg:60.53ms
step:113/2225 train_time:6840ms step_avg:60.53ms
step:114/2225 train_time:6900ms step_avg:60.52ms
step:115/2225 train_time:6960ms step_avg:60.52ms
step:116/2225 train_time:7019ms step_avg:60.51ms
step:117/2225 train_time:7080ms step_avg:60.51ms
step:118/2225 train_time:7140ms step_avg:60.51ms
step:119/2225 train_time:7200ms step_avg:60.51ms
step:120/2225 train_time:7260ms step_avg:60.50ms
step:121/2225 train_time:7321ms step_avg:60.50ms
step:122/2225 train_time:7380ms step_avg:60.49ms
step:123/2225 train_time:7440ms step_avg:60.49ms
step:124/2225 train_time:7500ms step_avg:60.48ms
step:125/2225 train_time:7560ms step_avg:60.48ms
step:126/2225 train_time:7619ms step_avg:60.47ms
step:127/2225 train_time:7680ms step_avg:60.48ms
step:128/2225 train_time:7740ms step_avg:60.47ms
step:129/2225 train_time:7800ms step_avg:60.47ms
step:130/2225 train_time:7859ms step_avg:60.46ms
step:131/2225 train_time:7920ms step_avg:60.46ms
step:132/2225 train_time:7979ms step_avg:60.45ms
step:133/2225 train_time:8040ms step_avg:60.45ms
step:134/2225 train_time:8099ms step_avg:60.44ms
step:135/2225 train_time:8160ms step_avg:60.44ms
step:136/2225 train_time:8219ms step_avg:60.43ms
step:137/2225 train_time:8279ms step_avg:60.43ms
step:138/2225 train_time:8339ms step_avg:60.43ms
step:139/2225 train_time:8399ms step_avg:60.42ms
step:140/2225 train_time:8458ms step_avg:60.42ms
step:141/2225 train_time:8520ms step_avg:60.42ms
step:142/2225 train_time:8579ms step_avg:60.41ms
step:143/2225 train_time:8639ms step_avg:60.41ms
step:144/2225 train_time:8698ms step_avg:60.41ms
step:145/2225 train_time:8759ms step_avg:60.41ms
step:146/2225 train_time:8819ms step_avg:60.40ms
step:147/2225 train_time:8880ms step_avg:60.41ms
step:148/2225 train_time:8940ms step_avg:60.40ms
step:149/2225 train_time:9000ms step_avg:60.40ms
step:150/2225 train_time:9059ms step_avg:60.39ms
step:151/2225 train_time:9120ms step_avg:60.39ms
step:152/2225 train_time:9179ms step_avg:60.39ms
step:153/2225 train_time:9239ms step_avg:60.39ms
step:154/2225 train_time:9298ms step_avg:60.38ms
step:155/2225 train_time:9358ms step_avg:60.38ms
step:156/2225 train_time:9418ms step_avg:60.37ms
step:157/2225 train_time:9478ms step_avg:60.37ms
step:158/2225 train_time:9537ms step_avg:60.36ms
step:159/2225 train_time:9597ms step_avg:60.36ms
step:160/2225 train_time:9656ms step_avg:60.35ms
step:161/2225 train_time:9717ms step_avg:60.36ms
step:162/2225 train_time:9778ms step_avg:60.36ms
step:163/2225 train_time:9838ms step_avg:60.36ms
step:164/2225 train_time:9898ms step_avg:60.35ms
step:165/2225 train_time:9958ms step_avg:60.35ms
step:166/2225 train_time:10017ms step_avg:60.35ms
step:167/2225 train_time:10078ms step_avg:60.35ms
step:168/2225 train_time:10137ms step_avg:60.34ms
step:169/2225 train_time:10198ms step_avg:60.34ms
step:170/2225 train_time:10257ms step_avg:60.34ms
step:171/2225 train_time:10319ms step_avg:60.34ms
step:172/2225 train_time:10378ms step_avg:60.34ms
step:173/2225 train_time:10438ms step_avg:60.34ms
step:174/2225 train_time:10497ms step_avg:60.33ms
step:175/2225 train_time:10558ms step_avg:60.33ms
step:176/2225 train_time:10618ms step_avg:60.33ms
step:177/2225 train_time:10678ms step_avg:60.33ms
step:178/2225 train_time:10738ms step_avg:60.32ms
step:179/2225 train_time:10798ms step_avg:60.32ms
step:180/2225 train_time:10857ms step_avg:60.31ms
step:181/2225 train_time:10917ms step_avg:60.32ms
step:182/2225 train_time:10977ms step_avg:60.31ms
step:183/2225 train_time:11038ms step_avg:60.32ms
step:184/2225 train_time:11097ms step_avg:60.31ms
step:185/2225 train_time:11157ms step_avg:60.31ms
step:186/2225 train_time:11216ms step_avg:60.30ms
step:187/2225 train_time:11277ms step_avg:60.30ms
step:188/2225 train_time:11336ms step_avg:60.30ms
step:189/2225 train_time:11397ms step_avg:60.30ms
step:190/2225 train_time:11456ms step_avg:60.29ms
step:191/2225 train_time:11517ms step_avg:60.30ms
step:192/2225 train_time:11576ms step_avg:60.29ms
step:193/2225 train_time:11637ms step_avg:60.29ms
step:194/2225 train_time:11696ms step_avg:60.29ms
step:195/2225 train_time:11757ms step_avg:60.29ms
step:196/2225 train_time:11817ms step_avg:60.29ms
step:197/2225 train_time:11877ms step_avg:60.29ms
step:198/2225 train_time:11936ms step_avg:60.28ms
step:199/2225 train_time:11997ms step_avg:60.29ms
step:200/2225 train_time:12057ms step_avg:60.28ms
step:201/2225 train_time:12118ms step_avg:60.29ms
step:202/2225 train_time:12177ms step_avg:60.28ms
step:203/2225 train_time:12238ms step_avg:60.28ms
step:204/2225 train_time:12297ms step_avg:60.28ms
step:205/2225 train_time:12358ms step_avg:60.28ms
step:206/2225 train_time:12417ms step_avg:60.28ms
step:207/2225 train_time:12477ms step_avg:60.28ms
step:208/2225 train_time:12536ms step_avg:60.27ms
step:209/2225 train_time:12596ms step_avg:60.27ms
step:210/2225 train_time:12656ms step_avg:60.27ms
step:211/2225 train_time:12717ms step_avg:60.27ms
step:212/2225 train_time:12777ms step_avg:60.27ms
step:213/2225 train_time:12838ms step_avg:60.27ms
step:214/2225 train_time:12897ms step_avg:60.26ms
step:215/2225 train_time:12958ms step_avg:60.27ms
step:216/2225 train_time:13017ms step_avg:60.26ms
step:217/2225 train_time:13078ms step_avg:60.27ms
step:218/2225 train_time:13137ms step_avg:60.26ms
step:219/2225 train_time:13197ms step_avg:60.26ms
step:220/2225 train_time:13257ms step_avg:60.26ms
step:221/2225 train_time:13318ms step_avg:60.26ms
step:222/2225 train_time:13377ms step_avg:60.26ms
step:223/2225 train_time:13438ms step_avg:60.26ms
step:224/2225 train_time:13497ms step_avg:60.25ms
step:225/2225 train_time:13557ms step_avg:60.25ms
step:226/2225 train_time:13617ms step_avg:60.25ms
step:227/2225 train_time:13677ms step_avg:60.25ms
step:228/2225 train_time:13737ms step_avg:60.25ms
step:229/2225 train_time:13797ms step_avg:60.25ms
step:230/2225 train_time:13856ms step_avg:60.24ms
step:231/2225 train_time:13918ms step_avg:60.25ms
step:232/2225 train_time:13977ms step_avg:60.25ms
step:233/2225 train_time:14038ms step_avg:60.25ms
step:234/2225 train_time:14097ms step_avg:60.24ms
step:235/2225 train_time:14157ms step_avg:60.24ms
step:236/2225 train_time:14216ms step_avg:60.24ms
step:237/2225 train_time:14277ms step_avg:60.24ms
step:238/2225 train_time:14337ms step_avg:60.24ms
step:239/2225 train_time:14396ms step_avg:60.24ms
step:240/2225 train_time:14456ms step_avg:60.23ms
step:241/2225 train_time:14517ms step_avg:60.23ms
step:242/2225 train_time:14576ms step_avg:60.23ms
step:243/2225 train_time:14637ms step_avg:60.23ms
step:244/2225 train_time:14697ms step_avg:60.23ms
step:245/2225 train_time:14757ms step_avg:60.23ms
step:246/2225 train_time:14817ms step_avg:60.23ms
step:247/2225 train_time:14878ms step_avg:60.23ms
step:248/2225 train_time:14937ms step_avg:60.23ms
step:249/2225 train_time:14998ms step_avg:60.23ms
step:250/2225 train_time:15057ms step_avg:60.23ms
step:250/2225 val_loss:4.0861 train_time:15118ms step_avg:60.47ms
step:251/2225 train_time:15141ms step_avg:60.32ms
step:252/2225 train_time:15181ms step_avg:60.24ms
step:253/2225 train_time:15246ms step_avg:60.26ms
step:254/2225 train_time:15307ms step_avg:60.27ms
step:255/2225 train_time:15369ms step_avg:60.27ms
step:256/2225 train_time:15427ms step_avg:60.26ms
step:257/2225 train_time:15487ms step_avg:60.26ms
step:258/2225 train_time:15545ms step_avg:60.25ms
step:259/2225 train_time:15606ms step_avg:60.25ms
step:260/2225 train_time:15664ms step_avg:60.25ms
step:261/2225 train_time:15723ms step_avg:60.24ms
step:262/2225 train_time:15781ms step_avg:60.23ms
step:263/2225 train_time:15840ms step_avg:60.23ms
step:264/2225 train_time:15898ms step_avg:60.22ms
step:265/2225 train_time:15958ms step_avg:60.22ms
step:266/2225 train_time:16016ms step_avg:60.21ms
step:267/2225 train_time:16076ms step_avg:60.21ms
step:268/2225 train_time:16137ms step_avg:60.21ms
step:269/2225 train_time:16200ms step_avg:60.22ms
step:270/2225 train_time:16260ms step_avg:60.22ms
step:271/2225 train_time:16322ms step_avg:60.23ms
step:272/2225 train_time:16381ms step_avg:60.23ms
step:273/2225 train_time:16443ms step_avg:60.23ms
step:274/2225 train_time:16502ms step_avg:60.23ms
step:275/2225 train_time:16562ms step_avg:60.23ms
step:276/2225 train_time:16621ms step_avg:60.22ms
step:277/2225 train_time:16681ms step_avg:60.22ms
step:278/2225 train_time:16739ms step_avg:60.21ms
step:279/2225 train_time:16799ms step_avg:60.21ms
step:280/2225 train_time:16857ms step_avg:60.20ms
step:281/2225 train_time:16917ms step_avg:60.20ms
step:282/2225 train_time:16975ms step_avg:60.19ms
step:283/2225 train_time:17035ms step_avg:60.19ms
step:284/2225 train_time:17094ms step_avg:60.19ms
step:285/2225 train_time:17155ms step_avg:60.19ms
step:286/2225 train_time:17215ms step_avg:60.19ms
step:287/2225 train_time:17277ms step_avg:60.20ms
step:288/2225 train_time:17337ms step_avg:60.20ms
step:289/2225 train_time:17398ms step_avg:60.20ms
step:290/2225 train_time:17458ms step_avg:60.20ms
step:291/2225 train_time:17518ms step_avg:60.20ms
step:292/2225 train_time:17578ms step_avg:60.20ms
step:293/2225 train_time:17638ms step_avg:60.20ms
step:294/2225 train_time:17697ms step_avg:60.19ms
step:295/2225 train_time:17757ms step_avg:60.19ms
step:296/2225 train_time:17815ms step_avg:60.19ms
step:297/2225 train_time:17876ms step_avg:60.19ms
step:298/2225 train_time:17934ms step_avg:60.18ms
step:299/2225 train_time:17994ms step_avg:60.18ms
step:300/2225 train_time:18054ms step_avg:60.18ms
step:301/2225 train_time:18115ms step_avg:60.18ms
step:302/2225 train_time:18174ms step_avg:60.18ms
step:303/2225 train_time:18236ms step_avg:60.18ms
step:304/2225 train_time:18296ms step_avg:60.18ms
step:305/2225 train_time:18357ms step_avg:60.19ms
step:306/2225 train_time:18417ms step_avg:60.19ms
step:307/2225 train_time:18478ms step_avg:60.19ms
step:308/2225 train_time:18538ms step_avg:60.19ms
step:309/2225 train_time:18599ms step_avg:60.19ms
step:310/2225 train_time:18658ms step_avg:60.19ms
step:311/2225 train_time:18719ms step_avg:60.19ms
step:312/2225 train_time:18777ms step_avg:60.18ms
step:313/2225 train_time:18837ms step_avg:60.18ms
step:314/2225 train_time:18896ms step_avg:60.18ms
step:315/2225 train_time:18956ms step_avg:60.18ms
step:316/2225 train_time:19015ms step_avg:60.17ms
step:317/2225 train_time:19075ms step_avg:60.17ms
step:318/2225 train_time:19134ms step_avg:60.17ms
step:319/2225 train_time:19195ms step_avg:60.17ms
step:320/2225 train_time:19255ms step_avg:60.17ms
step:321/2225 train_time:19316ms step_avg:60.18ms
step:322/2225 train_time:19376ms step_avg:60.17ms
step:323/2225 train_time:19437ms step_avg:60.18ms
step:324/2225 train_time:19496ms step_avg:60.17ms
step:325/2225 train_time:19557ms step_avg:60.17ms
step:326/2225 train_time:19615ms step_avg:60.17ms
step:327/2225 train_time:19676ms step_avg:60.17ms
step:328/2225 train_time:19735ms step_avg:60.17ms
step:329/2225 train_time:19796ms step_avg:60.17ms
step:330/2225 train_time:19854ms step_avg:60.17ms
step:331/2225 train_time:19915ms step_avg:60.16ms
step:332/2225 train_time:19973ms step_avg:60.16ms
step:333/2225 train_time:20034ms step_avg:60.16ms
step:334/2225 train_time:20093ms step_avg:60.16ms
step:335/2225 train_time:20153ms step_avg:60.16ms
step:336/2225 train_time:20213ms step_avg:60.16ms
step:337/2225 train_time:20275ms step_avg:60.16ms
step:338/2225 train_time:20334ms step_avg:60.16ms
step:339/2225 train_time:20396ms step_avg:60.17ms
step:340/2225 train_time:20456ms step_avg:60.16ms
step:341/2225 train_time:20517ms step_avg:60.17ms
step:342/2225 train_time:20576ms step_avg:60.16ms
step:343/2225 train_time:20637ms step_avg:60.17ms
step:344/2225 train_time:20696ms step_avg:60.16ms
step:345/2225 train_time:20756ms step_avg:60.16ms
step:346/2225 train_time:20815ms step_avg:60.16ms
step:347/2225 train_time:20875ms step_avg:60.16ms
step:348/2225 train_time:20934ms step_avg:60.16ms
step:349/2225 train_time:20994ms step_avg:60.16ms
step:350/2225 train_time:21054ms step_avg:60.15ms
step:351/2225 train_time:21114ms step_avg:60.15ms
step:352/2225 train_time:21173ms step_avg:60.15ms
step:353/2225 train_time:21234ms step_avg:60.15ms
step:354/2225 train_time:21293ms step_avg:60.15ms
step:355/2225 train_time:21354ms step_avg:60.15ms
step:356/2225 train_time:21413ms step_avg:60.15ms
step:357/2225 train_time:21474ms step_avg:60.15ms
step:358/2225 train_time:21534ms step_avg:60.15ms
step:359/2225 train_time:21595ms step_avg:60.15ms
step:360/2225 train_time:21654ms step_avg:60.15ms
step:361/2225 train_time:21714ms step_avg:60.15ms
step:362/2225 train_time:21772ms step_avg:60.14ms
step:363/2225 train_time:21833ms step_avg:60.15ms
step:364/2225 train_time:21892ms step_avg:60.14ms
step:365/2225 train_time:21952ms step_avg:60.14ms
step:366/2225 train_time:22010ms step_avg:60.14ms
step:367/2225 train_time:22071ms step_avg:60.14ms
step:368/2225 train_time:22129ms step_avg:60.13ms
step:369/2225 train_time:22190ms step_avg:60.13ms
step:370/2225 train_time:22249ms step_avg:60.13ms
step:371/2225 train_time:22309ms step_avg:60.13ms
step:372/2225 train_time:22368ms step_avg:60.13ms
step:373/2225 train_time:22429ms step_avg:60.13ms
step:374/2225 train_time:22488ms step_avg:60.13ms
step:375/2225 train_time:22548ms step_avg:60.13ms
step:376/2225 train_time:22607ms step_avg:60.12ms
step:377/2225 train_time:22667ms step_avg:60.13ms
step:378/2225 train_time:22726ms step_avg:60.12ms
step:379/2225 train_time:22787ms step_avg:60.12ms
step:380/2225 train_time:22845ms step_avg:60.12ms
step:381/2225 train_time:22906ms step_avg:60.12ms
step:382/2225 train_time:22964ms step_avg:60.12ms
step:383/2225 train_time:23024ms step_avg:60.12ms
step:384/2225 train_time:23083ms step_avg:60.11ms
step:385/2225 train_time:23143ms step_avg:60.11ms
step:386/2225 train_time:23202ms step_avg:60.11ms
step:387/2225 train_time:23262ms step_avg:60.11ms
step:388/2225 train_time:23321ms step_avg:60.11ms
step:389/2225 train_time:23382ms step_avg:60.11ms
step:390/2225 train_time:23441ms step_avg:60.11ms
step:391/2225 train_time:23502ms step_avg:60.11ms
step:392/2225 train_time:23561ms step_avg:60.10ms
step:393/2225 train_time:23621ms step_avg:60.10ms
step:394/2225 train_time:23680ms step_avg:60.10ms
step:395/2225 train_time:23742ms step_avg:60.11ms
step:396/2225 train_time:23801ms step_avg:60.10ms
step:397/2225 train_time:23861ms step_avg:60.10ms
step:398/2225 train_time:23921ms step_avg:60.10ms
step:399/2225 train_time:23980ms step_avg:60.10ms
step:400/2225 train_time:24039ms step_avg:60.10ms
step:401/2225 train_time:24100ms step_avg:60.10ms
step:402/2225 train_time:24159ms step_avg:60.10ms
step:403/2225 train_time:24219ms step_avg:60.10ms
step:404/2225 train_time:24278ms step_avg:60.09ms
step:405/2225 train_time:24338ms step_avg:60.09ms
step:406/2225 train_time:24398ms step_avg:60.09ms
step:407/2225 train_time:24459ms step_avg:60.10ms
step:408/2225 train_time:24518ms step_avg:60.09ms
step:409/2225 train_time:24578ms step_avg:60.09ms
step:410/2225 train_time:24638ms step_avg:60.09ms
step:411/2225 train_time:24699ms step_avg:60.09ms
step:412/2225 train_time:24758ms step_avg:60.09ms
step:413/2225 train_time:24818ms step_avg:60.09ms
step:414/2225 train_time:24877ms step_avg:60.09ms
step:415/2225 train_time:24938ms step_avg:60.09ms
step:416/2225 train_time:24997ms step_avg:60.09ms
step:417/2225 train_time:25058ms step_avg:60.09ms
step:418/2225 train_time:25117ms step_avg:60.09ms
step:419/2225 train_time:25177ms step_avg:60.09ms
step:420/2225 train_time:25236ms step_avg:60.09ms
step:421/2225 train_time:25297ms step_avg:60.09ms
step:422/2225 train_time:25356ms step_avg:60.09ms
step:423/2225 train_time:25417ms step_avg:60.09ms
step:424/2225 train_time:25476ms step_avg:60.09ms
step:425/2225 train_time:25537ms step_avg:60.09ms
step:426/2225 train_time:25597ms step_avg:60.09ms
step:427/2225 train_time:25657ms step_avg:60.09ms
step:428/2225 train_time:25716ms step_avg:60.08ms
step:429/2225 train_time:25777ms step_avg:60.09ms
step:430/2225 train_time:25836ms step_avg:60.08ms
step:431/2225 train_time:25897ms step_avg:60.08ms
step:432/2225 train_time:25956ms step_avg:60.08ms
step:433/2225 train_time:26016ms step_avg:60.08ms
step:434/2225 train_time:26075ms step_avg:60.08ms
step:435/2225 train_time:26136ms step_avg:60.08ms
step:436/2225 train_time:26195ms step_avg:60.08ms
step:437/2225 train_time:26255ms step_avg:60.08ms
step:438/2225 train_time:26314ms step_avg:60.08ms
step:439/2225 train_time:26375ms step_avg:60.08ms
step:440/2225 train_time:26434ms step_avg:60.08ms
step:441/2225 train_time:26496ms step_avg:60.08ms
step:442/2225 train_time:26555ms step_avg:60.08ms
step:443/2225 train_time:26616ms step_avg:60.08ms
step:444/2225 train_time:26675ms step_avg:60.08ms
step:445/2225 train_time:26736ms step_avg:60.08ms
step:446/2225 train_time:26795ms step_avg:60.08ms
step:447/2225 train_time:26856ms step_avg:60.08ms
step:448/2225 train_time:26914ms step_avg:60.08ms
step:449/2225 train_time:26975ms step_avg:60.08ms
step:450/2225 train_time:27034ms step_avg:60.07ms
step:451/2225 train_time:27094ms step_avg:60.08ms
step:452/2225 train_time:27153ms step_avg:60.07ms
step:453/2225 train_time:27214ms step_avg:60.07ms
step:454/2225 train_time:27273ms step_avg:60.07ms
step:455/2225 train_time:27334ms step_avg:60.07ms
step:456/2225 train_time:27393ms step_avg:60.07ms
step:457/2225 train_time:27454ms step_avg:60.07ms
step:458/2225 train_time:27513ms step_avg:60.07ms
step:459/2225 train_time:27573ms step_avg:60.07ms
step:460/2225 train_time:27632ms step_avg:60.07ms
step:461/2225 train_time:27693ms step_avg:60.07ms
step:462/2225 train_time:27752ms step_avg:60.07ms
step:463/2225 train_time:27813ms step_avg:60.07ms
step:464/2225 train_time:27872ms step_avg:60.07ms
step:465/2225 train_time:27932ms step_avg:60.07ms
step:466/2225 train_time:27991ms step_avg:60.07ms
step:467/2225 train_time:28052ms step_avg:60.07ms
step:468/2225 train_time:28111ms step_avg:60.07ms
step:469/2225 train_time:28172ms step_avg:60.07ms
step:470/2225 train_time:28231ms step_avg:60.07ms
step:471/2225 train_time:28291ms step_avg:60.07ms
step:472/2225 train_time:28350ms step_avg:60.06ms
step:473/2225 train_time:28411ms step_avg:60.06ms
step:474/2225 train_time:28469ms step_avg:60.06ms
step:475/2225 train_time:28529ms step_avg:60.06ms
step:476/2225 train_time:28588ms step_avg:60.06ms
step:477/2225 train_time:28648ms step_avg:60.06ms
step:478/2225 train_time:28707ms step_avg:60.06ms
step:479/2225 train_time:28768ms step_avg:60.06ms
step:480/2225 train_time:28826ms step_avg:60.06ms
step:481/2225 train_time:28887ms step_avg:60.06ms
step:482/2225 train_time:28946ms step_avg:60.05ms
step:483/2225 train_time:29006ms step_avg:60.05ms
step:484/2225 train_time:29065ms step_avg:60.05ms
step:485/2225 train_time:29125ms step_avg:60.05ms
step:486/2225 train_time:29184ms step_avg:60.05ms
step:487/2225 train_time:29244ms step_avg:60.05ms
step:488/2225 train_time:29303ms step_avg:60.05ms
step:489/2225 train_time:29363ms step_avg:60.05ms
step:490/2225 train_time:29422ms step_avg:60.04ms
step:491/2225 train_time:29482ms step_avg:60.05ms
step:492/2225 train_time:29541ms step_avg:60.04ms
step:493/2225 train_time:29601ms step_avg:60.04ms
step:494/2225 train_time:29661ms step_avg:60.04ms
step:495/2225 train_time:29721ms step_avg:60.04ms
step:496/2225 train_time:29780ms step_avg:60.04ms
step:497/2225 train_time:29841ms step_avg:60.04ms
step:498/2225 train_time:29900ms step_avg:60.04ms
step:499/2225 train_time:29962ms step_avg:60.04ms
step:500/2225 train_time:30021ms step_avg:60.04ms
step:500/2225 val_loss:3.8200 train_time:30081ms step_avg:60.16ms
step:501/2225 train_time:30104ms step_avg:60.09ms
step:502/2225 train_time:30145ms step_avg:60.05ms
step:503/2225 train_time:30208ms step_avg:60.06ms
step:504/2225 train_time:30270ms step_avg:60.06ms
step:505/2225 train_time:30332ms step_avg:60.06ms
step:506/2225 train_time:30391ms step_avg:60.06ms
step:507/2225 train_time:30451ms step_avg:60.06ms
step:508/2225 train_time:30509ms step_avg:60.06ms
step:509/2225 train_time:30569ms step_avg:60.06ms
step:510/2225 train_time:30627ms step_avg:60.05ms
step:511/2225 train_time:30686ms step_avg:60.05ms
step:512/2225 train_time:30745ms step_avg:60.05ms
step:513/2225 train_time:30804ms step_avg:60.05ms
step:514/2225 train_time:30862ms step_avg:60.04ms
step:515/2225 train_time:30921ms step_avg:60.04ms
step:516/2225 train_time:30980ms step_avg:60.04ms
step:517/2225 train_time:31042ms step_avg:60.04ms
step:518/2225 train_time:31102ms step_avg:60.04ms
step:519/2225 train_time:31164ms step_avg:60.05ms
step:520/2225 train_time:31225ms step_avg:60.05ms
step:521/2225 train_time:31287ms step_avg:60.05ms
step:522/2225 train_time:31347ms step_avg:60.05ms
step:523/2225 train_time:31407ms step_avg:60.05ms
step:524/2225 train_time:31466ms step_avg:60.05ms
step:525/2225 train_time:31526ms step_avg:60.05ms
step:526/2225 train_time:31585ms step_avg:60.05ms
step:527/2225 train_time:31644ms step_avg:60.05ms
step:528/2225 train_time:31703ms step_avg:60.04ms
step:529/2225 train_time:31762ms step_avg:60.04ms
step:530/2225 train_time:31820ms step_avg:60.04ms
step:531/2225 train_time:31880ms step_avg:60.04ms
step:532/2225 train_time:31938ms step_avg:60.03ms
step:533/2225 train_time:31998ms step_avg:60.03ms
step:534/2225 train_time:32058ms step_avg:60.03ms
step:535/2225 train_time:32119ms step_avg:60.04ms
step:536/2225 train_time:32180ms step_avg:60.04ms
step:537/2225 train_time:32242ms step_avg:60.04ms
step:538/2225 train_time:32302ms step_avg:60.04ms
step:539/2225 train_time:32364ms step_avg:60.04ms
step:540/2225 train_time:32423ms step_avg:60.04ms
step:541/2225 train_time:32484ms step_avg:60.04ms
step:542/2225 train_time:32543ms step_avg:60.04ms
step:543/2225 train_time:32603ms step_avg:60.04ms
step:544/2225 train_time:32662ms step_avg:60.04ms
step:545/2225 train_time:32721ms step_avg:60.04ms
step:546/2225 train_time:32780ms step_avg:60.04ms
step:547/2225 train_time:32840ms step_avg:60.04ms
step:548/2225 train_time:32899ms step_avg:60.03ms
step:549/2225 train_time:32959ms step_avg:60.03ms
step:550/2225 train_time:33018ms step_avg:60.03ms
step:551/2225 train_time:33079ms step_avg:60.03ms
step:552/2225 train_time:33139ms step_avg:60.03ms
step:553/2225 train_time:33201ms step_avg:60.04ms
step:554/2225 train_time:33261ms step_avg:60.04ms
step:555/2225 train_time:33322ms step_avg:60.04ms
step:556/2225 train_time:33381ms step_avg:60.04ms
step:557/2225 train_time:33442ms step_avg:60.04ms
step:558/2225 train_time:33502ms step_avg:60.04ms
step:559/2225 train_time:33563ms step_avg:60.04ms
step:560/2225 train_time:33622ms step_avg:60.04ms
step:561/2225 train_time:33682ms step_avg:60.04ms
step:562/2225 train_time:33741ms step_avg:60.04ms
step:563/2225 train_time:33801ms step_avg:60.04ms
step:564/2225 train_time:33860ms step_avg:60.04ms
step:565/2225 train_time:33921ms step_avg:60.04ms
step:566/2225 train_time:33980ms step_avg:60.03ms
step:567/2225 train_time:34040ms step_avg:60.04ms
step:568/2225 train_time:34100ms step_avg:60.04ms
step:569/2225 train_time:34161ms step_avg:60.04ms
step:570/2225 train_time:34221ms step_avg:60.04ms
step:571/2225 train_time:34283ms step_avg:60.04ms
step:572/2225 train_time:34343ms step_avg:60.04ms
step:573/2225 train_time:34404ms step_avg:60.04ms
step:574/2225 train_time:34463ms step_avg:60.04ms
step:575/2225 train_time:34523ms step_avg:60.04ms
step:576/2225 train_time:34582ms step_avg:60.04ms
step:577/2225 train_time:34643ms step_avg:60.04ms
step:578/2225 train_time:34702ms step_avg:60.04ms
step:579/2225 train_time:34762ms step_avg:60.04ms
step:580/2225 train_time:34821ms step_avg:60.04ms
step:581/2225 train_time:34880ms step_avg:60.04ms
step:582/2225 train_time:34939ms step_avg:60.03ms
step:583/2225 train_time:35000ms step_avg:60.03ms
step:584/2225 train_time:35059ms step_avg:60.03ms
step:585/2225 train_time:35120ms step_avg:60.03ms
step:586/2225 train_time:35180ms step_avg:60.03ms
step:587/2225 train_time:35241ms step_avg:60.04ms
step:588/2225 train_time:35301ms step_avg:60.04ms
step:589/2225 train_time:35362ms step_avg:60.04ms
step:590/2225 train_time:35422ms step_avg:60.04ms
step:591/2225 train_time:35484ms step_avg:60.04ms
step:592/2225 train_time:35543ms step_avg:60.04ms
step:593/2225 train_time:35603ms step_avg:60.04ms
step:594/2225 train_time:35662ms step_avg:60.04ms
step:595/2225 train_time:35722ms step_avg:60.04ms
step:596/2225 train_time:35781ms step_avg:60.04ms
step:597/2225 train_time:35842ms step_avg:60.04ms
step:598/2225 train_time:35901ms step_avg:60.03ms
step:599/2225 train_time:35961ms step_avg:60.03ms
step:600/2225 train_time:36020ms step_avg:60.03ms
step:601/2225 train_time:36081ms step_avg:60.03ms
step:602/2225 train_time:36141ms step_avg:60.03ms
step:603/2225 train_time:36202ms step_avg:60.04ms
step:604/2225 train_time:36261ms step_avg:60.03ms
step:605/2225 train_time:36322ms step_avg:60.04ms
step:606/2225 train_time:36382ms step_avg:60.04ms
step:607/2225 train_time:36443ms step_avg:60.04ms
step:608/2225 train_time:36502ms step_avg:60.04ms
step:609/2225 train_time:36563ms step_avg:60.04ms
step:610/2225 train_time:36622ms step_avg:60.04ms
step:611/2225 train_time:36683ms step_avg:60.04ms
step:612/2225 train_time:36742ms step_avg:60.04ms
step:613/2225 train_time:36802ms step_avg:60.04ms
step:614/2225 train_time:36861ms step_avg:60.03ms
step:615/2225 train_time:36921ms step_avg:60.03ms
step:616/2225 train_time:36979ms step_avg:60.03ms
step:617/2225 train_time:37040ms step_avg:60.03ms
step:618/2225 train_time:37099ms step_avg:60.03ms
step:619/2225 train_time:37159ms step_avg:60.03ms
step:620/2225 train_time:37219ms step_avg:60.03ms
step:621/2225 train_time:37280ms step_avg:60.03ms
step:622/2225 train_time:37339ms step_avg:60.03ms
step:623/2225 train_time:37401ms step_avg:60.03ms
step:624/2225 train_time:37460ms step_avg:60.03ms
step:625/2225 train_time:37520ms step_avg:60.03ms
step:626/2225 train_time:37580ms step_avg:60.03ms
step:627/2225 train_time:37641ms step_avg:60.03ms
step:628/2225 train_time:37700ms step_avg:60.03ms
step:629/2225 train_time:37760ms step_avg:60.03ms
step:630/2225 train_time:37819ms step_avg:60.03ms
step:631/2225 train_time:37879ms step_avg:60.03ms
step:632/2225 train_time:37938ms step_avg:60.03ms
step:633/2225 train_time:37999ms step_avg:60.03ms
step:634/2225 train_time:38058ms step_avg:60.03ms
step:635/2225 train_time:38118ms step_avg:60.03ms
step:636/2225 train_time:38177ms step_avg:60.03ms
step:637/2225 train_time:38238ms step_avg:60.03ms
step:638/2225 train_time:38297ms step_avg:60.03ms
step:639/2225 train_time:38358ms step_avg:60.03ms
step:640/2225 train_time:38417ms step_avg:60.03ms
step:641/2225 train_time:38478ms step_avg:60.03ms
step:642/2225 train_time:38538ms step_avg:60.03ms
step:643/2225 train_time:38599ms step_avg:60.03ms
step:644/2225 train_time:38658ms step_avg:60.03ms
step:645/2225 train_time:38719ms step_avg:60.03ms
step:646/2225 train_time:38779ms step_avg:60.03ms
step:647/2225 train_time:38839ms step_avg:60.03ms
step:648/2225 train_time:38898ms step_avg:60.03ms
step:649/2225 train_time:38959ms step_avg:60.03ms
step:650/2225 train_time:39018ms step_avg:60.03ms
step:651/2225 train_time:39078ms step_avg:60.03ms
step:652/2225 train_time:39137ms step_avg:60.03ms
step:653/2225 train_time:39198ms step_avg:60.03ms
step:654/2225 train_time:39257ms step_avg:60.03ms
step:655/2225 train_time:39317ms step_avg:60.03ms
step:656/2225 train_time:39377ms step_avg:60.03ms
step:657/2225 train_time:39438ms step_avg:60.03ms
step:658/2225 train_time:39497ms step_avg:60.03ms
step:659/2225 train_time:39558ms step_avg:60.03ms
step:660/2225 train_time:39617ms step_avg:60.03ms
step:661/2225 train_time:39678ms step_avg:60.03ms
step:662/2225 train_time:39737ms step_avg:60.03ms
step:663/2225 train_time:39798ms step_avg:60.03ms
step:664/2225 train_time:39857ms step_avg:60.02ms
step:665/2225 train_time:39917ms step_avg:60.03ms
step:666/2225 train_time:39976ms step_avg:60.02ms
step:667/2225 train_time:40037ms step_avg:60.02ms
step:668/2225 train_time:40096ms step_avg:60.02ms
step:669/2225 train_time:40157ms step_avg:60.03ms
step:670/2225 train_time:40216ms step_avg:60.02ms
step:671/2225 train_time:40277ms step_avg:60.02ms
step:672/2225 train_time:40336ms step_avg:60.02ms
step:673/2225 train_time:40397ms step_avg:60.03ms
step:674/2225 train_time:40456ms step_avg:60.02ms
step:675/2225 train_time:40517ms step_avg:60.02ms
step:676/2225 train_time:40576ms step_avg:60.02ms
step:677/2225 train_time:40637ms step_avg:60.03ms
step:678/2225 train_time:40697ms step_avg:60.02ms
step:679/2225 train_time:40758ms step_avg:60.03ms
step:680/2225 train_time:40817ms step_avg:60.03ms
step:681/2225 train_time:40878ms step_avg:60.03ms
step:682/2225 train_time:40937ms step_avg:60.03ms
step:683/2225 train_time:40998ms step_avg:60.03ms
step:684/2225 train_time:41057ms step_avg:60.03ms
step:685/2225 train_time:41118ms step_avg:60.03ms
step:686/2225 train_time:41178ms step_avg:60.03ms
step:687/2225 train_time:41238ms step_avg:60.03ms
step:688/2225 train_time:41297ms step_avg:60.02ms
step:689/2225 train_time:41358ms step_avg:60.03ms
step:690/2225 train_time:41417ms step_avg:60.02ms
step:691/2225 train_time:41478ms step_avg:60.03ms
step:692/2225 train_time:41537ms step_avg:60.02ms
step:693/2225 train_time:41598ms step_avg:60.03ms
step:694/2225 train_time:41657ms step_avg:60.02ms
step:695/2225 train_time:41718ms step_avg:60.03ms
step:696/2225 train_time:41777ms step_avg:60.02ms
step:697/2225 train_time:41838ms step_avg:60.03ms
step:698/2225 train_time:41896ms step_avg:60.02ms
step:699/2225 train_time:41957ms step_avg:60.02ms
step:700/2225 train_time:42016ms step_avg:60.02ms
step:701/2225 train_time:42077ms step_avg:60.02ms
step:702/2225 train_time:42137ms step_avg:60.02ms
step:703/2225 train_time:42198ms step_avg:60.03ms
step:704/2225 train_time:42257ms step_avg:60.02ms
step:705/2225 train_time:42318ms step_avg:60.03ms
step:706/2225 train_time:42377ms step_avg:60.02ms
step:707/2225 train_time:42438ms step_avg:60.03ms
step:708/2225 train_time:42497ms step_avg:60.02ms
step:709/2225 train_time:42558ms step_avg:60.03ms
step:710/2225 train_time:42617ms step_avg:60.02ms
step:711/2225 train_time:42678ms step_avg:60.03ms
step:712/2225 train_time:42737ms step_avg:60.02ms
step:713/2225 train_time:42798ms step_avg:60.03ms
step:714/2225 train_time:42857ms step_avg:60.02ms
step:715/2225 train_time:42917ms step_avg:60.02ms
step:716/2225 train_time:42977ms step_avg:60.02ms
step:717/2225 train_time:43037ms step_avg:60.02ms
step:718/2225 train_time:43097ms step_avg:60.02ms
step:719/2225 train_time:43158ms step_avg:60.02ms
step:720/2225 train_time:43217ms step_avg:60.02ms
step:721/2225 train_time:43277ms step_avg:60.02ms
step:722/2225 train_time:43337ms step_avg:60.02ms
step:723/2225 train_time:43397ms step_avg:60.02ms
step:724/2225 train_time:43457ms step_avg:60.02ms
step:725/2225 train_time:43517ms step_avg:60.02ms
step:726/2225 train_time:43576ms step_avg:60.02ms
step:727/2225 train_time:43637ms step_avg:60.02ms
step:728/2225 train_time:43697ms step_avg:60.02ms
step:729/2225 train_time:43758ms step_avg:60.02ms
step:730/2225 train_time:43817ms step_avg:60.02ms
step:731/2225 train_time:43879ms step_avg:60.03ms
step:732/2225 train_time:43939ms step_avg:60.03ms
step:733/2225 train_time:44001ms step_avg:60.03ms
step:734/2225 train_time:44061ms step_avg:60.03ms
step:735/2225 train_time:44122ms step_avg:60.03ms
step:736/2225 train_time:44183ms step_avg:60.03ms
step:737/2225 train_time:44244ms step_avg:60.03ms
step:738/2225 train_time:44304ms step_avg:60.03ms
step:739/2225 train_time:44365ms step_avg:60.03ms
step:740/2225 train_time:44425ms step_avg:60.03ms
step:741/2225 train_time:44486ms step_avg:60.04ms
step:742/2225 train_time:44547ms step_avg:60.04ms
step:743/2225 train_time:44608ms step_avg:60.04ms
step:744/2225 train_time:44668ms step_avg:60.04ms
step:745/2225 train_time:44729ms step_avg:60.04ms
step:746/2225 train_time:44789ms step_avg:60.04ms
step:747/2225 train_time:44850ms step_avg:60.04ms
step:748/2225 train_time:44909ms step_avg:60.04ms
step:749/2225 train_time:44970ms step_avg:60.04ms
step:750/2225 train_time:45029ms step_avg:60.04ms
step:750/2225 val_loss:3.6682 train_time:45091ms step_avg:60.12ms
step:751/2225 train_time:45113ms step_avg:60.07ms
step:752/2225 train_time:45152ms step_avg:60.04ms
step:753/2225 train_time:45213ms step_avg:60.04ms
step:754/2225 train_time:45274ms step_avg:60.05ms
step:755/2225 train_time:45336ms step_avg:60.05ms
step:756/2225 train_time:45398ms step_avg:60.05ms
step:757/2225 train_time:45459ms step_avg:60.05ms
step:758/2225 train_time:45518ms step_avg:60.05ms
step:759/2225 train_time:45579ms step_avg:60.05ms
step:760/2225 train_time:45638ms step_avg:60.05ms
step:761/2225 train_time:45698ms step_avg:60.05ms
step:762/2225 train_time:45757ms step_avg:60.05ms
step:763/2225 train_time:45817ms step_avg:60.05ms
step:764/2225 train_time:45876ms step_avg:60.05ms
step:765/2225 train_time:45937ms step_avg:60.05ms
step:766/2225 train_time:45999ms step_avg:60.05ms
step:767/2225 train_time:46064ms step_avg:60.06ms
step:768/2225 train_time:46126ms step_avg:60.06ms
step:769/2225 train_time:46188ms step_avg:60.06ms
step:770/2225 train_time:46249ms step_avg:60.06ms
step:771/2225 train_time:46310ms step_avg:60.06ms
step:772/2225 train_time:46371ms step_avg:60.07ms
step:773/2225 train_time:46433ms step_avg:60.07ms
step:774/2225 train_time:46493ms step_avg:60.07ms
step:775/2225 train_time:46554ms step_avg:60.07ms
step:776/2225 train_time:46613ms step_avg:60.07ms
step:777/2225 train_time:46674ms step_avg:60.07ms
step:778/2225 train_time:46733ms step_avg:60.07ms
step:779/2225 train_time:46794ms step_avg:60.07ms
step:780/2225 train_time:46853ms step_avg:60.07ms
step:781/2225 train_time:46914ms step_avg:60.07ms
step:782/2225 train_time:46975ms step_avg:60.07ms
step:783/2225 train_time:47036ms step_avg:60.07ms
step:784/2225 train_time:47097ms step_avg:60.07ms
step:785/2225 train_time:47159ms step_avg:60.08ms
step:786/2225 train_time:47219ms step_avg:60.08ms
step:787/2225 train_time:47281ms step_avg:60.08ms
step:788/2225 train_time:47341ms step_avg:60.08ms
step:789/2225 train_time:47403ms step_avg:60.08ms
step:790/2225 train_time:47462ms step_avg:60.08ms
step:791/2225 train_time:47523ms step_avg:60.08ms
step:792/2225 train_time:47582ms step_avg:60.08ms
step:793/2225 train_time:47644ms step_avg:60.08ms
step:794/2225 train_time:47703ms step_avg:60.08ms
step:795/2225 train_time:47764ms step_avg:60.08ms
step:796/2225 train_time:47823ms step_avg:60.08ms
step:797/2225 train_time:47884ms step_avg:60.08ms
step:798/2225 train_time:47944ms step_avg:60.08ms
step:799/2225 train_time:48007ms step_avg:60.08ms
step:800/2225 train_time:48067ms step_avg:60.08ms
step:801/2225 train_time:48130ms step_avg:60.09ms
step:802/2225 train_time:48190ms step_avg:60.09ms
step:803/2225 train_time:48252ms step_avg:60.09ms
step:804/2225 train_time:48312ms step_avg:60.09ms
step:805/2225 train_time:48374ms step_avg:60.09ms
step:806/2225 train_time:48434ms step_avg:60.09ms
step:807/2225 train_time:48495ms step_avg:60.09ms
step:808/2225 train_time:48554ms step_avg:60.09ms
step:809/2225 train_time:48615ms step_avg:60.09ms
step:810/2225 train_time:48675ms step_avg:60.09ms
step:811/2225 train_time:48735ms step_avg:60.09ms
step:812/2225 train_time:48795ms step_avg:60.09ms
step:813/2225 train_time:48856ms step_avg:60.09ms
step:814/2225 train_time:48917ms step_avg:60.09ms
step:815/2225 train_time:48978ms step_avg:60.10ms
step:816/2225 train_time:49037ms step_avg:60.09ms
step:817/2225 train_time:49099ms step_avg:60.10ms
step:818/2225 train_time:49158ms step_avg:60.10ms
step:819/2225 train_time:49219ms step_avg:60.10ms
step:820/2225 train_time:49279ms step_avg:60.10ms
step:821/2225 train_time:49341ms step_avg:60.10ms
step:822/2225 train_time:49401ms step_avg:60.10ms
step:823/2225 train_time:49462ms step_avg:60.10ms
step:824/2225 train_time:49522ms step_avg:60.10ms
step:825/2225 train_time:49582ms step_avg:60.10ms
step:826/2225 train_time:49642ms step_avg:60.10ms
step:827/2225 train_time:49703ms step_avg:60.10ms
step:828/2225 train_time:49762ms step_avg:60.10ms
step:829/2225 train_time:49824ms step_avg:60.10ms
step:830/2225 train_time:49884ms step_avg:60.10ms
step:831/2225 train_time:49946ms step_avg:60.10ms
step:832/2225 train_time:50006ms step_avg:60.10ms
step:833/2225 train_time:50068ms step_avg:60.11ms
step:834/2225 train_time:50129ms step_avg:60.11ms
step:835/2225 train_time:50191ms step_avg:60.11ms
step:836/2225 train_time:50251ms step_avg:60.11ms
step:837/2225 train_time:50312ms step_avg:60.11ms
step:838/2225 train_time:50372ms step_avg:60.11ms
step:839/2225 train_time:50435ms step_avg:60.11ms
step:840/2225 train_time:50495ms step_avg:60.11ms
step:841/2225 train_time:50556ms step_avg:60.11ms
step:842/2225 train_time:50616ms step_avg:60.11ms
step:843/2225 train_time:50677ms step_avg:60.11ms
step:844/2225 train_time:50737ms step_avg:60.11ms
step:845/2225 train_time:50798ms step_avg:60.12ms
step:846/2225 train_time:50858ms step_avg:60.12ms
step:847/2225 train_time:50918ms step_avg:60.12ms
step:848/2225 train_time:50978ms step_avg:60.12ms
step:849/2225 train_time:51039ms step_avg:60.12ms
step:850/2225 train_time:51099ms step_avg:60.12ms
step:851/2225 train_time:51160ms step_avg:60.12ms
step:852/2225 train_time:51221ms step_avg:60.12ms
step:853/2225 train_time:51282ms step_avg:60.12ms
step:854/2225 train_time:51342ms step_avg:60.12ms
step:855/2225 train_time:51404ms step_avg:60.12ms
step:856/2225 train_time:51464ms step_avg:60.12ms
step:857/2225 train_time:51525ms step_avg:60.12ms
step:858/2225 train_time:51585ms step_avg:60.12ms
step:859/2225 train_time:51648ms step_avg:60.13ms
step:860/2225 train_time:51708ms step_avg:60.13ms
step:861/2225 train_time:51769ms step_avg:60.13ms
step:862/2225 train_time:51829ms step_avg:60.13ms
step:863/2225 train_time:51891ms step_avg:60.13ms
step:864/2225 train_time:51951ms step_avg:60.13ms
step:865/2225 train_time:52012ms step_avg:60.13ms
step:866/2225 train_time:52072ms step_avg:60.13ms
step:867/2225 train_time:52135ms step_avg:60.13ms
step:868/2225 train_time:52194ms step_avg:60.13ms
step:869/2225 train_time:52256ms step_avg:60.13ms
step:870/2225 train_time:52315ms step_avg:60.13ms
step:871/2225 train_time:52376ms step_avg:60.13ms
step:872/2225 train_time:52435ms step_avg:60.13ms
step:873/2225 train_time:52497ms step_avg:60.13ms
step:874/2225 train_time:52557ms step_avg:60.13ms
step:875/2225 train_time:52618ms step_avg:60.13ms
step:876/2225 train_time:52677ms step_avg:60.13ms
step:877/2225 train_time:52738ms step_avg:60.14ms
step:878/2225 train_time:52798ms step_avg:60.13ms
step:879/2225 train_time:52859ms step_avg:60.14ms
step:880/2225 train_time:52919ms step_avg:60.13ms
step:881/2225 train_time:52980ms step_avg:60.14ms
step:882/2225 train_time:53040ms step_avg:60.14ms
step:883/2225 train_time:53101ms step_avg:60.14ms
step:884/2225 train_time:53161ms step_avg:60.14ms
step:885/2225 train_time:53222ms step_avg:60.14ms
step:886/2225 train_time:53282ms step_avg:60.14ms
step:887/2225 train_time:53344ms step_avg:60.14ms
step:888/2225 train_time:53404ms step_avg:60.14ms
step:889/2225 train_time:53465ms step_avg:60.14ms
step:890/2225 train_time:53525ms step_avg:60.14ms
step:891/2225 train_time:53587ms step_avg:60.14ms
step:892/2225 train_time:53647ms step_avg:60.14ms
step:893/2225 train_time:53709ms step_avg:60.14ms
step:894/2225 train_time:53769ms step_avg:60.14ms
step:895/2225 train_time:53830ms step_avg:60.15ms
step:896/2225 train_time:53890ms step_avg:60.15ms
step:897/2225 train_time:53952ms step_avg:60.15ms
step:898/2225 train_time:54012ms step_avg:60.15ms
step:899/2225 train_time:54073ms step_avg:60.15ms
step:900/2225 train_time:54134ms step_avg:60.15ms
step:901/2225 train_time:54195ms step_avg:60.15ms
step:902/2225 train_time:54255ms step_avg:60.15ms
step:903/2225 train_time:54316ms step_avg:60.15ms
step:904/2225 train_time:54376ms step_avg:60.15ms
step:905/2225 train_time:54437ms step_avg:60.15ms
step:906/2225 train_time:54497ms step_avg:60.15ms
step:907/2225 train_time:54558ms step_avg:60.15ms
step:908/2225 train_time:54618ms step_avg:60.15ms
step:909/2225 train_time:54679ms step_avg:60.15ms
step:910/2225 train_time:54739ms step_avg:60.15ms
step:911/2225 train_time:54800ms step_avg:60.15ms
step:912/2225 train_time:54859ms step_avg:60.15ms
step:913/2225 train_time:54920ms step_avg:60.15ms
step:914/2225 train_time:54980ms step_avg:60.15ms
step:915/2225 train_time:55041ms step_avg:60.15ms
step:916/2225 train_time:55101ms step_avg:60.15ms
step:917/2225 train_time:55162ms step_avg:60.15ms
step:918/2225 train_time:55222ms step_avg:60.15ms
step:919/2225 train_time:55283ms step_avg:60.16ms
step:920/2225 train_time:55343ms step_avg:60.16ms
step:921/2225 train_time:55404ms step_avg:60.16ms
step:922/2225 train_time:55463ms step_avg:60.16ms
step:923/2225 train_time:55525ms step_avg:60.16ms
step:924/2225 train_time:55585ms step_avg:60.16ms
step:925/2225 train_time:55647ms step_avg:60.16ms
step:926/2225 train_time:55707ms step_avg:60.16ms
step:927/2225 train_time:55769ms step_avg:60.16ms
step:928/2225 train_time:55829ms step_avg:60.16ms
step:929/2225 train_time:55890ms step_avg:60.16ms
step:930/2225 train_time:55952ms step_avg:60.16ms
step:931/2225 train_time:56014ms step_avg:60.17ms
step:932/2225 train_time:56074ms step_avg:60.17ms
step:933/2225 train_time:56135ms step_avg:60.17ms
step:934/2225 train_time:56196ms step_avg:60.17ms
step:935/2225 train_time:56256ms step_avg:60.17ms
step:936/2225 train_time:56316ms step_avg:60.17ms
step:937/2225 train_time:56376ms step_avg:60.17ms
step:938/2225 train_time:56436ms step_avg:60.17ms
step:939/2225 train_time:56497ms step_avg:60.17ms
step:940/2225 train_time:56557ms step_avg:60.17ms
step:941/2225 train_time:56618ms step_avg:60.17ms
step:942/2225 train_time:56678ms step_avg:60.17ms
step:943/2225 train_time:56740ms step_avg:60.17ms
step:944/2225 train_time:56800ms step_avg:60.17ms
step:945/2225 train_time:56861ms step_avg:60.17ms
step:946/2225 train_time:56921ms step_avg:60.17ms
step:947/2225 train_time:56982ms step_avg:60.17ms
step:948/2225 train_time:57043ms step_avg:60.17ms
step:949/2225 train_time:57104ms step_avg:60.17ms
step:950/2225 train_time:57164ms step_avg:60.17ms
step:951/2225 train_time:57225ms step_avg:60.17ms
step:952/2225 train_time:57286ms step_avg:60.17ms
step:953/2225 train_time:57348ms step_avg:60.18ms
step:954/2225 train_time:57408ms step_avg:60.18ms
step:955/2225 train_time:57470ms step_avg:60.18ms
step:956/2225 train_time:57530ms step_avg:60.18ms
step:957/2225 train_time:57593ms step_avg:60.18ms
step:958/2225 train_time:57653ms step_avg:60.18ms
step:959/2225 train_time:57714ms step_avg:60.18ms
step:960/2225 train_time:57774ms step_avg:60.18ms
step:961/2225 train_time:57835ms step_avg:60.18ms
step:962/2225 train_time:57895ms step_avg:60.18ms
step:963/2225 train_time:57956ms step_avg:60.18ms
step:964/2225 train_time:58016ms step_avg:60.18ms
step:965/2225 train_time:58077ms step_avg:60.18ms
step:966/2225 train_time:58137ms step_avg:60.18ms
step:967/2225 train_time:58198ms step_avg:60.18ms
step:968/2225 train_time:58258ms step_avg:60.18ms
step:969/2225 train_time:58320ms step_avg:60.19ms
step:970/2225 train_time:58380ms step_avg:60.19ms
step:971/2225 train_time:58441ms step_avg:60.19ms
step:972/2225 train_time:58501ms step_avg:60.19ms
step:973/2225 train_time:58562ms step_avg:60.19ms
step:974/2225 train_time:58622ms step_avg:60.19ms
step:975/2225 train_time:58684ms step_avg:60.19ms
step:976/2225 train_time:58744ms step_avg:60.19ms
step:977/2225 train_time:58805ms step_avg:60.19ms
step:978/2225 train_time:58864ms step_avg:60.19ms
step:979/2225 train_time:58926ms step_avg:60.19ms
step:980/2225 train_time:58987ms step_avg:60.19ms
step:981/2225 train_time:59049ms step_avg:60.19ms
step:982/2225 train_time:59109ms step_avg:60.19ms
step:983/2225 train_time:59170ms step_avg:60.19ms
step:984/2225 train_time:59230ms step_avg:60.19ms
step:985/2225 train_time:59292ms step_avg:60.19ms
step:986/2225 train_time:59352ms step_avg:60.19ms
step:987/2225 train_time:59413ms step_avg:60.20ms
step:988/2225 train_time:59473ms step_avg:60.20ms
step:989/2225 train_time:59535ms step_avg:60.20ms
step:990/2225 train_time:59595ms step_avg:60.20ms
step:991/2225 train_time:59656ms step_avg:60.20ms
step:992/2225 train_time:59715ms step_avg:60.20ms
step:993/2225 train_time:59777ms step_avg:60.20ms
step:994/2225 train_time:59837ms step_avg:60.20ms
step:995/2225 train_time:59898ms step_avg:60.20ms
step:996/2225 train_time:59958ms step_avg:60.20ms
step:997/2225 train_time:60018ms step_avg:60.20ms
step:998/2225 train_time:60079ms step_avg:60.20ms
step:999/2225 train_time:60140ms step_avg:60.20ms
step:1000/2225 train_time:60200ms step_avg:60.20ms
step:1000/2225 val_loss:3.5950 train_time:60261ms step_avg:60.26ms
step:1001/2225 train_time:60284ms step_avg:60.22ms
step:1002/2225 train_time:60322ms step_avg:60.20ms
step:1003/2225 train_time:60387ms step_avg:60.21ms
step:1004/2225 train_time:60450ms step_avg:60.21ms
step:1005/2225 train_time:60511ms step_avg:60.21ms
step:1006/2225 train_time:60571ms step_avg:60.21ms
step:1007/2225 train_time:60632ms step_avg:60.21ms
step:1008/2225 train_time:60690ms step_avg:60.21ms
step:1009/2225 train_time:60751ms step_avg:60.21ms
step:1010/2225 train_time:60809ms step_avg:60.21ms
step:1011/2225 train_time:60870ms step_avg:60.21ms
step:1012/2225 train_time:60928ms step_avg:60.21ms
step:1013/2225 train_time:60988ms step_avg:60.21ms
step:1014/2225 train_time:61048ms step_avg:60.21ms
step:1015/2225 train_time:61109ms step_avg:60.21ms
step:1016/2225 train_time:61169ms step_avg:60.21ms
step:1017/2225 train_time:61231ms step_avg:60.21ms
step:1018/2225 train_time:61292ms step_avg:60.21ms
step:1019/2225 train_time:61354ms step_avg:60.21ms
step:1020/2225 train_time:61416ms step_avg:60.21ms
step:1021/2225 train_time:61478ms step_avg:60.21ms
step:1022/2225 train_time:61539ms step_avg:60.21ms
step:1023/2225 train_time:61602ms step_avg:60.22ms
step:1024/2225 train_time:61662ms step_avg:60.22ms
step:1025/2225 train_time:61724ms step_avg:60.22ms
step:1026/2225 train_time:61784ms step_avg:60.22ms
step:1027/2225 train_time:61844ms step_avg:60.22ms
step:1028/2225 train_time:61904ms step_avg:60.22ms
step:1029/2225 train_time:61965ms step_avg:60.22ms
step:1030/2225 train_time:62024ms step_avg:60.22ms
step:1031/2225 train_time:62085ms step_avg:60.22ms
step:1032/2225 train_time:62145ms step_avg:60.22ms
step:1033/2225 train_time:62207ms step_avg:60.22ms
step:1034/2225 train_time:62267ms step_avg:60.22ms
step:1035/2225 train_time:62329ms step_avg:60.22ms
step:1036/2225 train_time:62389ms step_avg:60.22ms
step:1037/2225 train_time:62451ms step_avg:60.22ms
step:1038/2225 train_time:62511ms step_avg:60.22ms
step:1039/2225 train_time:62573ms step_avg:60.22ms
step:1040/2225 train_time:62633ms step_avg:60.22ms
step:1041/2225 train_time:62694ms step_avg:60.23ms
step:1042/2225 train_time:62755ms step_avg:60.23ms
step:1043/2225 train_time:62816ms step_avg:60.23ms
step:1044/2225 train_time:62875ms step_avg:60.23ms
step:1045/2225 train_time:62936ms step_avg:60.23ms
step:1046/2225 train_time:62995ms step_avg:60.22ms
step:1047/2225 train_time:63057ms step_avg:60.23ms
step:1048/2225 train_time:63116ms step_avg:60.23ms
step:1049/2225 train_time:63178ms step_avg:60.23ms
step:1050/2225 train_time:63238ms step_avg:60.23ms
step:1051/2225 train_time:63301ms step_avg:60.23ms
step:1052/2225 train_time:63362ms step_avg:60.23ms
step:1053/2225 train_time:63424ms step_avg:60.23ms
step:1054/2225 train_time:63484ms step_avg:60.23ms
step:1055/2225 train_time:63546ms step_avg:60.23ms
step:1056/2225 train_time:63606ms step_avg:60.23ms
step:1057/2225 train_time:63667ms step_avg:60.23ms
step:1058/2225 train_time:63727ms step_avg:60.23ms
step:1059/2225 train_time:63788ms step_avg:60.23ms
step:1060/2225 train_time:63848ms step_avg:60.23ms
step:1061/2225 train_time:63909ms step_avg:60.23ms
step:1062/2225 train_time:63968ms step_avg:60.23ms
step:1063/2225 train_time:64029ms step_avg:60.23ms
step:1064/2225 train_time:64089ms step_avg:60.23ms
step:1065/2225 train_time:64150ms step_avg:60.23ms
step:1066/2225 train_time:64210ms step_avg:60.23ms
step:1067/2225 train_time:64271ms step_avg:60.24ms
step:1068/2225 train_time:64332ms step_avg:60.24ms
step:1069/2225 train_time:64393ms step_avg:60.24ms
step:1070/2225 train_time:64453ms step_avg:60.24ms
step:1071/2225 train_time:64515ms step_avg:60.24ms
step:1072/2225 train_time:64575ms step_avg:60.24ms
step:1073/2225 train_time:64637ms step_avg:60.24ms
step:1074/2225 train_time:64697ms step_avg:60.24ms
step:1075/2225 train_time:64759ms step_avg:60.24ms
step:1076/2225 train_time:64819ms step_avg:60.24ms
step:1077/2225 train_time:64880ms step_avg:60.24ms
step:1078/2225 train_time:64940ms step_avg:60.24ms
step:1079/2225 train_time:65002ms step_avg:60.24ms
step:1080/2225 train_time:65062ms step_avg:60.24ms
step:1081/2225 train_time:65124ms step_avg:60.24ms
step:1082/2225 train_time:65184ms step_avg:60.24ms
step:1083/2225 train_time:65246ms step_avg:60.25ms
step:1084/2225 train_time:65305ms step_avg:60.24ms
step:1085/2225 train_time:65367ms step_avg:60.25ms
step:1086/2225 train_time:65427ms step_avg:60.25ms
step:1087/2225 train_time:65488ms step_avg:60.25ms
step:1088/2225 train_time:65548ms step_avg:60.25ms
step:1089/2225 train_time:65609ms step_avg:60.25ms
step:1090/2225 train_time:65669ms step_avg:60.25ms
step:1091/2225 train_time:65730ms step_avg:60.25ms
step:1092/2225 train_time:65790ms step_avg:60.25ms
step:1093/2225 train_time:65851ms step_avg:60.25ms
step:1094/2225 train_time:65911ms step_avg:60.25ms
step:1095/2225 train_time:65972ms step_avg:60.25ms
step:1096/2225 train_time:66032ms step_avg:60.25ms
step:1097/2225 train_time:66093ms step_avg:60.25ms
step:1098/2225 train_time:66153ms step_avg:60.25ms
step:1099/2225 train_time:66213ms step_avg:60.25ms
step:1100/2225 train_time:66274ms step_avg:60.25ms
step:1101/2225 train_time:66335ms step_avg:60.25ms
step:1102/2225 train_time:66395ms step_avg:60.25ms
step:1103/2225 train_time:66456ms step_avg:60.25ms
step:1104/2225 train_time:66516ms step_avg:60.25ms
step:1105/2225 train_time:66578ms step_avg:60.25ms
step:1106/2225 train_time:66638ms step_avg:60.25ms
step:1107/2225 train_time:66699ms step_avg:60.25ms
step:1108/2225 train_time:66760ms step_avg:60.25ms
step:1109/2225 train_time:66822ms step_avg:60.25ms
step:1110/2225 train_time:66882ms step_avg:60.25ms
step:1111/2225 train_time:66943ms step_avg:60.26ms
step:1112/2225 train_time:67003ms step_avg:60.25ms
step:1113/2225 train_time:67065ms step_avg:60.26ms
step:1114/2225 train_time:67125ms step_avg:60.26ms
step:1115/2225 train_time:67186ms step_avg:60.26ms
step:1116/2225 train_time:67246ms step_avg:60.26ms
step:1117/2225 train_time:67307ms step_avg:60.26ms
step:1118/2225 train_time:67367ms step_avg:60.26ms
step:1119/2225 train_time:67429ms step_avg:60.26ms
step:1120/2225 train_time:67488ms step_avg:60.26ms
step:1121/2225 train_time:67549ms step_avg:60.26ms
step:1122/2225 train_time:67609ms step_avg:60.26ms
step:1123/2225 train_time:67670ms step_avg:60.26ms
step:1124/2225 train_time:67730ms step_avg:60.26ms
step:1125/2225 train_time:67791ms step_avg:60.26ms
step:1126/2225 train_time:67850ms step_avg:60.26ms
step:1127/2225 train_time:67912ms step_avg:60.26ms
step:1128/2225 train_time:67971ms step_avg:60.26ms
step:1129/2225 train_time:68033ms step_avg:60.26ms
step:1130/2225 train_time:68093ms step_avg:60.26ms
step:1131/2225 train_time:68155ms step_avg:60.26ms
step:1132/2225 train_time:68215ms step_avg:60.26ms
step:1133/2225 train_time:68277ms step_avg:60.26ms
step:1134/2225 train_time:68337ms step_avg:60.26ms
step:1135/2225 train_time:68398ms step_avg:60.26ms
step:1136/2225 train_time:68458ms step_avg:60.26ms
step:1137/2225 train_time:68520ms step_avg:60.26ms
step:1138/2225 train_time:68580ms step_avg:60.26ms
step:1139/2225 train_time:68642ms step_avg:60.27ms
step:1140/2225 train_time:68702ms step_avg:60.27ms
step:1141/2225 train_time:68765ms step_avg:60.27ms
step:1142/2225 train_time:68824ms step_avg:60.27ms
step:1143/2225 train_time:68886ms step_avg:60.27ms
step:1144/2225 train_time:68946ms step_avg:60.27ms
step:1145/2225 train_time:69007ms step_avg:60.27ms
step:1146/2225 train_time:69067ms step_avg:60.27ms
step:1147/2225 train_time:69128ms step_avg:60.27ms
step:1148/2225 train_time:69188ms step_avg:60.27ms
step:1149/2225 train_time:69249ms step_avg:60.27ms
step:1150/2225 train_time:69309ms step_avg:60.27ms
step:1151/2225 train_time:69371ms step_avg:60.27ms
step:1152/2225 train_time:69430ms step_avg:60.27ms
step:1153/2225 train_time:69491ms step_avg:60.27ms
step:1154/2225 train_time:69551ms step_avg:60.27ms
step:1155/2225 train_time:69612ms step_avg:60.27ms
step:1156/2225 train_time:69672ms step_avg:60.27ms
step:1157/2225 train_time:69734ms step_avg:60.27ms
step:1158/2225 train_time:69794ms step_avg:60.27ms
step:1159/2225 train_time:69856ms step_avg:60.27ms
step:1160/2225 train_time:69916ms step_avg:60.27ms
step:1161/2225 train_time:69978ms step_avg:60.27ms
step:1162/2225 train_time:70038ms step_avg:60.27ms
step:1163/2225 train_time:70099ms step_avg:60.27ms
step:1164/2225 train_time:70160ms step_avg:60.27ms
step:1165/2225 train_time:70222ms step_avg:60.28ms
step:1166/2225 train_time:70282ms step_avg:60.28ms
step:1167/2225 train_time:70343ms step_avg:60.28ms
step:1168/2225 train_time:70404ms step_avg:60.28ms
step:1169/2225 train_time:70466ms step_avg:60.28ms
step:1170/2225 train_time:70526ms step_avg:60.28ms
step:1171/2225 train_time:70587ms step_avg:60.28ms
step:1172/2225 train_time:70647ms step_avg:60.28ms
step:1173/2225 train_time:70708ms step_avg:60.28ms
step:1174/2225 train_time:70768ms step_avg:60.28ms
step:1175/2225 train_time:70829ms step_avg:60.28ms
step:1176/2225 train_time:70889ms step_avg:60.28ms
step:1177/2225 train_time:70950ms step_avg:60.28ms
step:1178/2225 train_time:71010ms step_avg:60.28ms
step:1179/2225 train_time:71071ms step_avg:60.28ms
step:1180/2225 train_time:71130ms step_avg:60.28ms
step:1181/2225 train_time:71191ms step_avg:60.28ms
step:1182/2225 train_time:71251ms step_avg:60.28ms
step:1183/2225 train_time:71313ms step_avg:60.28ms
step:1184/2225 train_time:71373ms step_avg:60.28ms
step:1185/2225 train_time:71435ms step_avg:60.28ms
step:1186/2225 train_time:71495ms step_avg:60.28ms
step:1187/2225 train_time:71557ms step_avg:60.28ms
step:1188/2225 train_time:71616ms step_avg:60.28ms
step:1189/2225 train_time:71678ms step_avg:60.28ms
step:1190/2225 train_time:71739ms step_avg:60.28ms
step:1191/2225 train_time:71800ms step_avg:60.29ms
step:1192/2225 train_time:71861ms step_avg:60.29ms
step:1193/2225 train_time:71923ms step_avg:60.29ms
step:1194/2225 train_time:71983ms step_avg:60.29ms
step:1195/2225 train_time:72045ms step_avg:60.29ms
step:1196/2225 train_time:72105ms step_avg:60.29ms
step:1197/2225 train_time:72167ms step_avg:60.29ms
step:1198/2225 train_time:72227ms step_avg:60.29ms
step:1199/2225 train_time:72288ms step_avg:60.29ms
step:1200/2225 train_time:72348ms step_avg:60.29ms
step:1201/2225 train_time:72409ms step_avg:60.29ms
step:1202/2225 train_time:72469ms step_avg:60.29ms
step:1203/2225 train_time:72530ms step_avg:60.29ms
step:1204/2225 train_time:72589ms step_avg:60.29ms
step:1205/2225 train_time:72650ms step_avg:60.29ms
step:1206/2225 train_time:72710ms step_avg:60.29ms
step:1207/2225 train_time:72772ms step_avg:60.29ms
step:1208/2225 train_time:72831ms step_avg:60.29ms
step:1209/2225 train_time:72893ms step_avg:60.29ms
step:1210/2225 train_time:72952ms step_avg:60.29ms
step:1211/2225 train_time:73014ms step_avg:60.29ms
step:1212/2225 train_time:73074ms step_avg:60.29ms
step:1213/2225 train_time:73136ms step_avg:60.29ms
step:1214/2225 train_time:73196ms step_avg:60.29ms
step:1215/2225 train_time:73258ms step_avg:60.29ms
step:1216/2225 train_time:73318ms step_avg:60.29ms
step:1217/2225 train_time:73379ms step_avg:60.30ms
step:1218/2225 train_time:73440ms step_avg:60.30ms
step:1219/2225 train_time:73501ms step_avg:60.30ms
step:1220/2225 train_time:73561ms step_avg:60.30ms
step:1221/2225 train_time:73623ms step_avg:60.30ms
step:1222/2225 train_time:73683ms step_avg:60.30ms
step:1223/2225 train_time:73744ms step_avg:60.30ms
step:1224/2225 train_time:73804ms step_avg:60.30ms
step:1225/2225 train_time:73866ms step_avg:60.30ms
step:1226/2225 train_time:73927ms step_avg:60.30ms
step:1227/2225 train_time:73988ms step_avg:60.30ms
step:1228/2225 train_time:74048ms step_avg:60.30ms
step:1229/2225 train_time:74109ms step_avg:60.30ms
step:1230/2225 train_time:74168ms step_avg:60.30ms
step:1231/2225 train_time:74230ms step_avg:60.30ms
step:1232/2225 train_time:74289ms step_avg:60.30ms
step:1233/2225 train_time:74351ms step_avg:60.30ms
step:1234/2225 train_time:74410ms step_avg:60.30ms
step:1235/2225 train_time:74472ms step_avg:60.30ms
step:1236/2225 train_time:74531ms step_avg:60.30ms
step:1237/2225 train_time:74593ms step_avg:60.30ms
step:1238/2225 train_time:74653ms step_avg:60.30ms
step:1239/2225 train_time:74715ms step_avg:60.30ms
step:1240/2225 train_time:74775ms step_avg:60.30ms
step:1241/2225 train_time:74837ms step_avg:60.30ms
step:1242/2225 train_time:74896ms step_avg:60.30ms
step:1243/2225 train_time:74958ms step_avg:60.30ms
step:1244/2225 train_time:75019ms step_avg:60.30ms
step:1245/2225 train_time:75081ms step_avg:60.31ms
step:1246/2225 train_time:75140ms step_avg:60.31ms
step:1247/2225 train_time:75202ms step_avg:60.31ms
step:1248/2225 train_time:75263ms step_avg:60.31ms
step:1249/2225 train_time:75325ms step_avg:60.31ms
step:1250/2225 train_time:75385ms step_avg:60.31ms
step:1250/2225 val_loss:3.5198 train_time:75447ms step_avg:60.36ms
step:1251/2225 train_time:75469ms step_avg:60.33ms
step:1252/2225 train_time:75510ms step_avg:60.31ms
step:1253/2225 train_time:75573ms step_avg:60.31ms
step:1254/2225 train_time:75638ms step_avg:60.32ms
step:1255/2225 train_time:75702ms step_avg:60.32ms
step:1256/2225 train_time:75762ms step_avg:60.32ms
step:1257/2225 train_time:75822ms step_avg:60.32ms
step:1258/2225 train_time:75881ms step_avg:60.32ms
step:1259/2225 train_time:75942ms step_avg:60.32ms
step:1260/2225 train_time:76001ms step_avg:60.32ms
step:1261/2225 train_time:76062ms step_avg:60.32ms
step:1262/2225 train_time:76121ms step_avg:60.32ms
step:1263/2225 train_time:76182ms step_avg:60.32ms
step:1264/2225 train_time:76241ms step_avg:60.32ms
step:1265/2225 train_time:76301ms step_avg:60.32ms
step:1266/2225 train_time:76360ms step_avg:60.32ms
step:1267/2225 train_time:76423ms step_avg:60.32ms
step:1268/2225 train_time:76484ms step_avg:60.32ms
step:1269/2225 train_time:76546ms step_avg:60.32ms
step:1270/2225 train_time:76607ms step_avg:60.32ms
step:1271/2225 train_time:76669ms step_avg:60.32ms
step:1272/2225 train_time:76729ms step_avg:60.32ms
step:1273/2225 train_time:76791ms step_avg:60.32ms
step:1274/2225 train_time:76851ms step_avg:60.32ms
step:1275/2225 train_time:76912ms step_avg:60.32ms
step:1276/2225 train_time:76972ms step_avg:60.32ms
step:1277/2225 train_time:77033ms step_avg:60.32ms
step:1278/2225 train_time:77092ms step_avg:60.32ms
step:1279/2225 train_time:77153ms step_avg:60.32ms
step:1280/2225 train_time:77213ms step_avg:60.32ms
step:1281/2225 train_time:77274ms step_avg:60.32ms
step:1282/2225 train_time:77333ms step_avg:60.32ms
step:1283/2225 train_time:77395ms step_avg:60.32ms
step:1284/2225 train_time:77455ms step_avg:60.32ms
step:1285/2225 train_time:77518ms step_avg:60.33ms
step:1286/2225 train_time:77578ms step_avg:60.33ms
step:1287/2225 train_time:77642ms step_avg:60.33ms
step:1288/2225 train_time:77703ms step_avg:60.33ms
step:1289/2225 train_time:77764ms step_avg:60.33ms
step:1290/2225 train_time:77824ms step_avg:60.33ms
step:1291/2225 train_time:77885ms step_avg:60.33ms
step:1292/2225 train_time:77945ms step_avg:60.33ms
step:1293/2225 train_time:78005ms step_avg:60.33ms
step:1294/2225 train_time:78065ms step_avg:60.33ms
step:1295/2225 train_time:78125ms step_avg:60.33ms
step:1296/2225 train_time:78185ms step_avg:60.33ms
step:1297/2225 train_time:78246ms step_avg:60.33ms
step:1298/2225 train_time:78305ms step_avg:60.33ms
step:1299/2225 train_time:78366ms step_avg:60.33ms
step:1300/2225 train_time:78426ms step_avg:60.33ms
step:1301/2225 train_time:78488ms step_avg:60.33ms
step:1302/2225 train_time:78548ms step_avg:60.33ms
step:1303/2225 train_time:78610ms step_avg:60.33ms
step:1304/2225 train_time:78670ms step_avg:60.33ms
step:1305/2225 train_time:78732ms step_avg:60.33ms
step:1306/2225 train_time:78792ms step_avg:60.33ms
step:1307/2225 train_time:78853ms step_avg:60.33ms
step:1308/2225 train_time:78913ms step_avg:60.33ms
step:1309/2225 train_time:78975ms step_avg:60.33ms
step:1310/2225 train_time:79035ms step_avg:60.33ms
step:1311/2225 train_time:79097ms step_avg:60.33ms
step:1312/2225 train_time:79156ms step_avg:60.33ms
step:1313/2225 train_time:79217ms step_avg:60.33ms
step:1314/2225 train_time:79277ms step_avg:60.33ms
step:1315/2225 train_time:79338ms step_avg:60.33ms
step:1316/2225 train_time:79397ms step_avg:60.33ms
step:1317/2225 train_time:79459ms step_avg:60.33ms
step:1318/2225 train_time:79519ms step_avg:60.33ms
step:1319/2225 train_time:79581ms step_avg:60.33ms
step:1320/2225 train_time:79642ms step_avg:60.33ms
step:1321/2225 train_time:79703ms step_avg:60.34ms
step:1322/2225 train_time:79763ms step_avg:60.34ms
step:1323/2225 train_time:79825ms step_avg:60.34ms
step:1324/2225 train_time:79884ms step_avg:60.34ms
step:1325/2225 train_time:79945ms step_avg:60.34ms
step:1326/2225 train_time:80005ms step_avg:60.34ms
step:1327/2225 train_time:80066ms step_avg:60.34ms
step:1328/2225 train_time:80126ms step_avg:60.34ms
step:1329/2225 train_time:80186ms step_avg:60.34ms
step:1330/2225 train_time:80245ms step_avg:60.33ms
step:1331/2225 train_time:80307ms step_avg:60.34ms
step:1332/2225 train_time:80366ms step_avg:60.33ms
step:1333/2225 train_time:80427ms step_avg:60.34ms
step:1334/2225 train_time:80487ms step_avg:60.33ms
step:1335/2225 train_time:80549ms step_avg:60.34ms
step:1336/2225 train_time:80608ms step_avg:60.34ms
step:1337/2225 train_time:80670ms step_avg:60.34ms
step:1338/2225 train_time:80729ms step_avg:60.34ms
step:1339/2225 train_time:80791ms step_avg:60.34ms
step:1340/2225 train_time:80851ms step_avg:60.34ms
step:1341/2225 train_time:80912ms step_avg:60.34ms
step:1342/2225 train_time:80972ms step_avg:60.34ms
step:1343/2225 train_time:81033ms step_avg:60.34ms
step:1344/2225 train_time:81093ms step_avg:60.34ms
step:1345/2225 train_time:81155ms step_avg:60.34ms
step:1346/2225 train_time:81215ms step_avg:60.34ms
step:1347/2225 train_time:81276ms step_avg:60.34ms
step:1348/2225 train_time:81336ms step_avg:60.34ms
step:1349/2225 train_time:81397ms step_avg:60.34ms
step:1350/2225 train_time:81458ms step_avg:60.34ms
step:1351/2225 train_time:81519ms step_avg:60.34ms
step:1352/2225 train_time:81578ms step_avg:60.34ms
step:1353/2225 train_time:81640ms step_avg:60.34ms
step:1354/2225 train_time:81701ms step_avg:60.34ms
step:1355/2225 train_time:81763ms step_avg:60.34ms
step:1356/2225 train_time:81823ms step_avg:60.34ms
step:1357/2225 train_time:81885ms step_avg:60.34ms
step:1358/2225 train_time:81945ms step_avg:60.34ms
step:1359/2225 train_time:82006ms step_avg:60.34ms
step:1360/2225 train_time:82066ms step_avg:60.34ms
step:1361/2225 train_time:82127ms step_avg:60.34ms
step:1362/2225 train_time:82187ms step_avg:60.34ms
step:1363/2225 train_time:82248ms step_avg:60.34ms
step:1364/2225 train_time:82308ms step_avg:60.34ms
step:1365/2225 train_time:82369ms step_avg:60.34ms
step:1366/2225 train_time:82429ms step_avg:60.34ms
step:1367/2225 train_time:82490ms step_avg:60.34ms
step:1368/2225 train_time:82550ms step_avg:60.34ms
step:1369/2225 train_time:82612ms step_avg:60.34ms
step:1370/2225 train_time:82672ms step_avg:60.34ms
step:1371/2225 train_time:82733ms step_avg:60.35ms
step:1372/2225 train_time:82794ms step_avg:60.35ms
step:1373/2225 train_time:82856ms step_avg:60.35ms
step:1374/2225 train_time:82915ms step_avg:60.35ms
step:1375/2225 train_time:82977ms step_avg:60.35ms
step:1376/2225 train_time:83038ms step_avg:60.35ms
step:1377/2225 train_time:83100ms step_avg:60.35ms
step:1378/2225 train_time:83160ms step_avg:60.35ms
step:1379/2225 train_time:83221ms step_avg:60.35ms
step:1380/2225 train_time:83281ms step_avg:60.35ms
step:1381/2225 train_time:83342ms step_avg:60.35ms
step:1382/2225 train_time:83402ms step_avg:60.35ms
step:1383/2225 train_time:83463ms step_avg:60.35ms
step:1384/2225 train_time:83523ms step_avg:60.35ms
step:1385/2225 train_time:83584ms step_avg:60.35ms
step:1386/2225 train_time:83644ms step_avg:60.35ms
step:1387/2225 train_time:83705ms step_avg:60.35ms
step:1388/2225 train_time:83766ms step_avg:60.35ms
step:1389/2225 train_time:83826ms step_avg:60.35ms
step:1390/2225 train_time:83886ms step_avg:60.35ms
step:1391/2225 train_time:83947ms step_avg:60.35ms
step:1392/2225 train_time:84007ms step_avg:60.35ms
step:1393/2225 train_time:84068ms step_avg:60.35ms
step:1394/2225 train_time:84128ms step_avg:60.35ms
step:1395/2225 train_time:84189ms step_avg:60.35ms
step:1396/2225 train_time:84248ms step_avg:60.35ms
step:1397/2225 train_time:84310ms step_avg:60.35ms
step:1398/2225 train_time:84369ms step_avg:60.35ms
step:1399/2225 train_time:84430ms step_avg:60.35ms
step:1400/2225 train_time:84490ms step_avg:60.35ms
step:1401/2225 train_time:84552ms step_avg:60.35ms
step:1402/2225 train_time:84612ms step_avg:60.35ms
step:1403/2225 train_time:84673ms step_avg:60.35ms
step:1404/2225 train_time:84732ms step_avg:60.35ms
step:1405/2225 train_time:84794ms step_avg:60.35ms
step:1406/2225 train_time:84853ms step_avg:60.35ms
step:1407/2225 train_time:84916ms step_avg:60.35ms
step:1408/2225 train_time:84975ms step_avg:60.35ms
step:1409/2225 train_time:85037ms step_avg:60.35ms
step:1410/2225 train_time:85097ms step_avg:60.35ms
step:1411/2225 train_time:85159ms step_avg:60.35ms
step:1412/2225 train_time:85218ms step_avg:60.35ms
step:1413/2225 train_time:85279ms step_avg:60.35ms
step:1414/2225 train_time:85340ms step_avg:60.35ms
step:1415/2225 train_time:85402ms step_avg:60.35ms
step:1416/2225 train_time:85462ms step_avg:60.35ms
step:1417/2225 train_time:85523ms step_avg:60.35ms
step:1418/2225 train_time:85583ms step_avg:60.35ms
step:1419/2225 train_time:85644ms step_avg:60.36ms
step:1420/2225 train_time:85705ms step_avg:60.36ms
step:1421/2225 train_time:85766ms step_avg:60.36ms
step:1422/2225 train_time:85825ms step_avg:60.36ms
step:1423/2225 train_time:85886ms step_avg:60.36ms
step:1424/2225 train_time:85946ms step_avg:60.36ms
step:1425/2225 train_time:86008ms step_avg:60.36ms
step:1426/2225 train_time:86067ms step_avg:60.36ms
step:1427/2225 train_time:86129ms step_avg:60.36ms
step:1428/2225 train_time:86189ms step_avg:60.36ms
step:1429/2225 train_time:86250ms step_avg:60.36ms
step:1430/2225 train_time:86310ms step_avg:60.36ms
step:1431/2225 train_time:86372ms step_avg:60.36ms
step:1432/2225 train_time:86432ms step_avg:60.36ms
step:1433/2225 train_time:86494ms step_avg:60.36ms
step:1434/2225 train_time:86554ms step_avg:60.36ms
step:1435/2225 train_time:86616ms step_avg:60.36ms
step:1436/2225 train_time:86676ms step_avg:60.36ms
step:1437/2225 train_time:86737ms step_avg:60.36ms
step:1438/2225 train_time:86797ms step_avg:60.36ms
step:1439/2225 train_time:86858ms step_avg:60.36ms
step:1440/2225 train_time:86919ms step_avg:60.36ms
step:1441/2225 train_time:86981ms step_avg:60.36ms
step:1442/2225 train_time:87041ms step_avg:60.36ms
step:1443/2225 train_time:87102ms step_avg:60.36ms
step:1444/2225 train_time:87163ms step_avg:60.36ms
step:1445/2225 train_time:87225ms step_avg:60.36ms
step:1446/2225 train_time:87285ms step_avg:60.36ms
step:1447/2225 train_time:87346ms step_avg:60.36ms
step:1448/2225 train_time:87405ms step_avg:60.36ms
step:1449/2225 train_time:87466ms step_avg:60.36ms
step:1450/2225 train_time:87526ms step_avg:60.36ms
step:1451/2225 train_time:87588ms step_avg:60.36ms
step:1452/2225 train_time:87647ms step_avg:60.36ms
step:1453/2225 train_time:87709ms step_avg:60.36ms
step:1454/2225 train_time:87768ms step_avg:60.36ms
step:1455/2225 train_time:87830ms step_avg:60.36ms
step:1456/2225 train_time:87889ms step_avg:60.36ms
step:1457/2225 train_time:87951ms step_avg:60.36ms
step:1458/2225 train_time:88011ms step_avg:60.36ms
step:1459/2225 train_time:88074ms step_avg:60.37ms
step:1460/2225 train_time:88134ms step_avg:60.37ms
step:1461/2225 train_time:88196ms step_avg:60.37ms
step:1462/2225 train_time:88257ms step_avg:60.37ms
step:1463/2225 train_time:88320ms step_avg:60.37ms
step:1464/2225 train_time:88380ms step_avg:60.37ms
step:1465/2225 train_time:88442ms step_avg:60.37ms
step:1466/2225 train_time:88502ms step_avg:60.37ms
step:1467/2225 train_time:88564ms step_avg:60.37ms
step:1468/2225 train_time:88623ms step_avg:60.37ms
step:1469/2225 train_time:88685ms step_avg:60.37ms
step:1470/2225 train_time:88745ms step_avg:60.37ms
step:1471/2225 train_time:88806ms step_avg:60.37ms
step:1472/2225 train_time:88866ms step_avg:60.37ms
step:1473/2225 train_time:88928ms step_avg:60.37ms
step:1474/2225 train_time:88987ms step_avg:60.37ms
step:1475/2225 train_time:89049ms step_avg:60.37ms
step:1476/2225 train_time:89109ms step_avg:60.37ms
step:1477/2225 train_time:89171ms step_avg:60.37ms
step:1478/2225 train_time:89232ms step_avg:60.37ms
step:1479/2225 train_time:89294ms step_avg:60.37ms
step:1480/2225 train_time:89354ms step_avg:60.37ms
step:1481/2225 train_time:89416ms step_avg:60.38ms
step:1482/2225 train_time:89476ms step_avg:60.38ms
step:1483/2225 train_time:89538ms step_avg:60.38ms
step:1484/2225 train_time:89599ms step_avg:60.38ms
step:1485/2225 train_time:89661ms step_avg:60.38ms
step:1486/2225 train_time:89721ms step_avg:60.38ms
step:1487/2225 train_time:89783ms step_avg:60.38ms
step:1488/2225 train_time:89844ms step_avg:60.38ms
step:1489/2225 train_time:89906ms step_avg:60.38ms
step:1490/2225 train_time:89966ms step_avg:60.38ms
step:1491/2225 train_time:90027ms step_avg:60.38ms
step:1492/2225 train_time:90087ms step_avg:60.38ms
step:1493/2225 train_time:90149ms step_avg:60.38ms
step:1494/2225 train_time:90209ms step_avg:60.38ms
step:1495/2225 train_time:90271ms step_avg:60.38ms
step:1496/2225 train_time:90330ms step_avg:60.38ms
step:1497/2225 train_time:90392ms step_avg:60.38ms
step:1498/2225 train_time:90453ms step_avg:60.38ms
step:1499/2225 train_time:90515ms step_avg:60.38ms
step:1500/2225 train_time:90576ms step_avg:60.38ms
step:1500/2225 val_loss:3.4391 train_time:90638ms step_avg:60.43ms
step:1501/2225 train_time:90660ms step_avg:60.40ms
step:1502/2225 train_time:90700ms step_avg:60.39ms
step:1503/2225 train_time:90761ms step_avg:60.39ms
step:1504/2225 train_time:90822ms step_avg:60.39ms
step:1505/2225 train_time:90884ms step_avg:60.39ms
step:1506/2225 train_time:90944ms step_avg:60.39ms
step:1507/2225 train_time:91005ms step_avg:60.39ms
step:1508/2225 train_time:91064ms step_avg:60.39ms
step:1509/2225 train_time:91125ms step_avg:60.39ms
step:1510/2225 train_time:91184ms step_avg:60.39ms
step:1511/2225 train_time:91245ms step_avg:60.39ms
step:1512/2225 train_time:91304ms step_avg:60.39ms
step:1513/2225 train_time:91365ms step_avg:60.39ms
step:1514/2225 train_time:91425ms step_avg:60.39ms
step:1515/2225 train_time:91487ms step_avg:60.39ms
step:1516/2225 train_time:91550ms step_avg:60.39ms
step:1517/2225 train_time:91619ms step_avg:60.39ms
step:1518/2225 train_time:91682ms step_avg:60.40ms
step:1519/2225 train_time:91745ms step_avg:60.40ms
step:1520/2225 train_time:91805ms step_avg:60.40ms
step:1521/2225 train_time:91867ms step_avg:60.40ms
step:1522/2225 train_time:91928ms step_avg:60.40ms
step:1523/2225 train_time:91990ms step_avg:60.40ms
step:1524/2225 train_time:92050ms step_avg:60.40ms
step:1525/2225 train_time:92111ms step_avg:60.40ms
step:1526/2225 train_time:92171ms step_avg:60.40ms
step:1527/2225 train_time:92233ms step_avg:60.40ms
step:1528/2225 train_time:92293ms step_avg:60.40ms
step:1529/2225 train_time:92354ms step_avg:60.40ms
step:1530/2225 train_time:92414ms step_avg:60.40ms
step:1531/2225 train_time:92476ms step_avg:60.40ms
step:1532/2225 train_time:92537ms step_avg:60.40ms
step:1533/2225 train_time:92599ms step_avg:60.40ms
step:1534/2225 train_time:92660ms step_avg:60.40ms
step:1535/2225 train_time:92723ms step_avg:60.41ms
step:1536/2225 train_time:92783ms step_avg:60.41ms
step:1537/2225 train_time:92844ms step_avg:60.41ms
step:1538/2225 train_time:92904ms step_avg:60.41ms
step:1539/2225 train_time:92966ms step_avg:60.41ms
step:1540/2225 train_time:93026ms step_avg:60.41ms
step:1541/2225 train_time:93087ms step_avg:60.41ms
step:1542/2225 train_time:93146ms step_avg:60.41ms
step:1543/2225 train_time:93208ms step_avg:60.41ms
step:1544/2225 train_time:93268ms step_avg:60.41ms
step:1545/2225 train_time:93329ms step_avg:60.41ms
step:1546/2225 train_time:93389ms step_avg:60.41ms
step:1547/2225 train_time:93451ms step_avg:60.41ms
step:1548/2225 train_time:93511ms step_avg:60.41ms
step:1549/2225 train_time:93575ms step_avg:60.41ms
step:1550/2225 train_time:93636ms step_avg:60.41ms
step:1551/2225 train_time:93698ms step_avg:60.41ms
step:1552/2225 train_time:93758ms step_avg:60.41ms
step:1553/2225 train_time:93821ms step_avg:60.41ms
step:1554/2225 train_time:93881ms step_avg:60.41ms
step:1555/2225 train_time:93942ms step_avg:60.41ms
step:1556/2225 train_time:94002ms step_avg:60.41ms
step:1557/2225 train_time:94064ms step_avg:60.41ms
step:1558/2225 train_time:94124ms step_avg:60.41ms
step:1559/2225 train_time:94185ms step_avg:60.41ms
step:1560/2225 train_time:94244ms step_avg:60.41ms
step:1561/2225 train_time:94307ms step_avg:60.41ms
step:1562/2225 train_time:94367ms step_avg:60.41ms
step:1563/2225 train_time:94429ms step_avg:60.42ms
step:1564/2225 train_time:94489ms step_avg:60.41ms
step:1565/2225 train_time:94551ms step_avg:60.42ms
step:1566/2225 train_time:94613ms step_avg:60.42ms
step:1567/2225 train_time:94675ms step_avg:60.42ms
step:1568/2225 train_time:94736ms step_avg:60.42ms
step:1569/2225 train_time:94797ms step_avg:60.42ms
step:1570/2225 train_time:94857ms step_avg:60.42ms
step:1571/2225 train_time:94919ms step_avg:60.42ms
step:1572/2225 train_time:94980ms step_avg:60.42ms
step:1573/2225 train_time:95041ms step_avg:60.42ms
step:1574/2225 train_time:95100ms step_avg:60.42ms
step:1575/2225 train_time:95161ms step_avg:60.42ms
step:1576/2225 train_time:95221ms step_avg:60.42ms
step:1577/2225 train_time:95284ms step_avg:60.42ms
step:1578/2225 train_time:95344ms step_avg:60.42ms
step:1579/2225 train_time:95406ms step_avg:60.42ms
step:1580/2225 train_time:95466ms step_avg:60.42ms
step:1581/2225 train_time:95528ms step_avg:60.42ms
step:1582/2225 train_time:95588ms step_avg:60.42ms
step:1583/2225 train_time:95651ms step_avg:60.42ms
step:1584/2225 train_time:95712ms step_avg:60.42ms
step:1585/2225 train_time:95775ms step_avg:60.43ms
step:1586/2225 train_time:95836ms step_avg:60.43ms
step:1587/2225 train_time:95898ms step_avg:60.43ms
step:1588/2225 train_time:95958ms step_avg:60.43ms
step:1589/2225 train_time:96020ms step_avg:60.43ms
step:1590/2225 train_time:96080ms step_avg:60.43ms
step:1591/2225 train_time:96141ms step_avg:60.43ms
step:1592/2225 train_time:96201ms step_avg:60.43ms
step:1593/2225 train_time:96262ms step_avg:60.43ms
step:1594/2225 train_time:96323ms step_avg:60.43ms
step:1595/2225 train_time:96384ms step_avg:60.43ms
step:1596/2225 train_time:96445ms step_avg:60.43ms
step:1597/2225 train_time:96508ms step_avg:60.43ms
step:1598/2225 train_time:96569ms step_avg:60.43ms
step:1599/2225 train_time:96631ms step_avg:60.43ms
step:1600/2225 train_time:96691ms step_avg:60.43ms
step:1601/2225 train_time:96753ms step_avg:60.43ms
step:1602/2225 train_time:96815ms step_avg:60.43ms
step:1603/2225 train_time:96877ms step_avg:60.43ms
step:1604/2225 train_time:96938ms step_avg:60.43ms
step:1605/2225 train_time:96999ms step_avg:60.44ms
step:1606/2225 train_time:97058ms step_avg:60.43ms
step:1607/2225 train_time:97120ms step_avg:60.44ms
step:1608/2225 train_time:97180ms step_avg:60.44ms
step:1609/2225 train_time:97242ms step_avg:60.44ms
step:1610/2225 train_time:97302ms step_avg:60.44ms
step:1611/2225 train_time:97364ms step_avg:60.44ms
step:1612/2225 train_time:97424ms step_avg:60.44ms
step:1613/2225 train_time:97486ms step_avg:60.44ms
step:1614/2225 train_time:97546ms step_avg:60.44ms
step:1615/2225 train_time:97609ms step_avg:60.44ms
step:1616/2225 train_time:97670ms step_avg:60.44ms
step:1617/2225 train_time:97732ms step_avg:60.44ms
step:1618/2225 train_time:97792ms step_avg:60.44ms
step:1619/2225 train_time:97855ms step_avg:60.44ms
step:1620/2225 train_time:97916ms step_avg:60.44ms
step:1621/2225 train_time:97978ms step_avg:60.44ms
step:1622/2225 train_time:98038ms step_avg:60.44ms
step:1623/2225 train_time:98099ms step_avg:60.44ms
step:1624/2225 train_time:98159ms step_avg:60.44ms
step:1625/2225 train_time:98220ms step_avg:60.44ms
step:1626/2225 train_time:98281ms step_avg:60.44ms
step:1627/2225 train_time:98342ms step_avg:60.44ms
step:1628/2225 train_time:98402ms step_avg:60.44ms
step:1629/2225 train_time:98464ms step_avg:60.44ms
step:1630/2225 train_time:98525ms step_avg:60.44ms
step:1631/2225 train_time:98587ms step_avg:60.45ms
step:1632/2225 train_time:98647ms step_avg:60.45ms
step:1633/2225 train_time:98709ms step_avg:60.45ms
step:1634/2225 train_time:98770ms step_avg:60.45ms
step:1635/2225 train_time:98834ms step_avg:60.45ms
step:1636/2225 train_time:98894ms step_avg:60.45ms
step:1637/2225 train_time:98957ms step_avg:60.45ms
step:1638/2225 train_time:99017ms step_avg:60.45ms
step:1639/2225 train_time:99078ms step_avg:60.45ms
step:1640/2225 train_time:99139ms step_avg:60.45ms
step:1641/2225 train_time:99200ms step_avg:60.45ms
step:1642/2225 train_time:99260ms step_avg:60.45ms
step:1643/2225 train_time:99321ms step_avg:60.45ms
step:1644/2225 train_time:99382ms step_avg:60.45ms
step:1645/2225 train_time:99443ms step_avg:60.45ms
step:1646/2225 train_time:99503ms step_avg:60.45ms
step:1647/2225 train_time:99565ms step_avg:60.45ms
step:1648/2225 train_time:99625ms step_avg:60.45ms
step:1649/2225 train_time:99687ms step_avg:60.45ms
step:1650/2225 train_time:99747ms step_avg:60.45ms
step:1651/2225 train_time:99810ms step_avg:60.45ms
step:1652/2225 train_time:99870ms step_avg:60.45ms
step:1653/2225 train_time:99932ms step_avg:60.46ms
step:1654/2225 train_time:99993ms step_avg:60.46ms
step:1655/2225 train_time:100056ms step_avg:60.46ms
step:1656/2225 train_time:100116ms step_avg:60.46ms
step:1657/2225 train_time:100179ms step_avg:60.46ms
step:1658/2225 train_time:100238ms step_avg:60.46ms
step:1659/2225 train_time:100300ms step_avg:60.46ms
step:1660/2225 train_time:100360ms step_avg:60.46ms
step:1661/2225 train_time:100421ms step_avg:60.46ms
step:1662/2225 train_time:100481ms step_avg:60.46ms
step:1663/2225 train_time:100542ms step_avg:60.46ms
step:1664/2225 train_time:100603ms step_avg:60.46ms
step:1665/2225 train_time:100665ms step_avg:60.46ms
step:1666/2225 train_time:100725ms step_avg:60.46ms
step:1667/2225 train_time:100786ms step_avg:60.46ms
step:1668/2225 train_time:100846ms step_avg:60.46ms
step:1669/2225 train_time:100909ms step_avg:60.46ms
step:1670/2225 train_time:100970ms step_avg:60.46ms
step:1671/2225 train_time:101032ms step_avg:60.46ms
step:1672/2225 train_time:101093ms step_avg:60.46ms
step:1673/2225 train_time:101156ms step_avg:60.46ms
step:1674/2225 train_time:101217ms step_avg:60.46ms
step:1675/2225 train_time:101278ms step_avg:60.46ms
step:1676/2225 train_time:101338ms step_avg:60.46ms
step:1677/2225 train_time:101400ms step_avg:60.46ms
step:1678/2225 train_time:101460ms step_avg:60.46ms
step:1679/2225 train_time:101521ms step_avg:60.47ms
step:1680/2225 train_time:101581ms step_avg:60.47ms
step:1681/2225 train_time:101643ms step_avg:60.47ms
step:1682/2225 train_time:101704ms step_avg:60.47ms
step:1683/2225 train_time:101766ms step_avg:60.47ms
step:1684/2225 train_time:101826ms step_avg:60.47ms
step:1685/2225 train_time:101888ms step_avg:60.47ms
step:1686/2225 train_time:101948ms step_avg:60.47ms
step:1687/2225 train_time:102011ms step_avg:60.47ms
step:1688/2225 train_time:102071ms step_avg:60.47ms
step:1689/2225 train_time:102133ms step_avg:60.47ms
step:1690/2225 train_time:102194ms step_avg:60.47ms
step:1691/2225 train_time:102257ms step_avg:60.47ms
step:1692/2225 train_time:102318ms step_avg:60.47ms
step:1693/2225 train_time:102379ms step_avg:60.47ms
step:1694/2225 train_time:102440ms step_avg:60.47ms
step:1695/2225 train_time:102502ms step_avg:60.47ms
step:1696/2225 train_time:102561ms step_avg:60.47ms
step:1697/2225 train_time:102623ms step_avg:60.47ms
step:1698/2225 train_time:102684ms step_avg:60.47ms
step:1699/2225 train_time:102745ms step_avg:60.47ms
step:1700/2225 train_time:102805ms step_avg:60.47ms
step:1701/2225 train_time:102867ms step_avg:60.47ms
step:1702/2225 train_time:102928ms step_avg:60.47ms
step:1703/2225 train_time:102990ms step_avg:60.48ms
step:1704/2225 train_time:103050ms step_avg:60.48ms
step:1705/2225 train_time:103113ms step_avg:60.48ms
step:1706/2225 train_time:103173ms step_avg:60.48ms
step:1707/2225 train_time:103236ms step_avg:60.48ms
step:1708/2225 train_time:103296ms step_avg:60.48ms
step:1709/2225 train_time:103358ms step_avg:60.48ms
step:1710/2225 train_time:103418ms step_avg:60.48ms
step:1711/2225 train_time:103480ms step_avg:60.48ms
step:1712/2225 train_time:103541ms step_avg:60.48ms
step:1713/2225 train_time:103602ms step_avg:60.48ms
step:1714/2225 train_time:103662ms step_avg:60.48ms
step:1715/2225 train_time:103723ms step_avg:60.48ms
step:1716/2225 train_time:103783ms step_avg:60.48ms
step:1717/2225 train_time:103844ms step_avg:60.48ms
step:1718/2225 train_time:103905ms step_avg:60.48ms
step:1719/2225 train_time:103968ms step_avg:60.48ms
step:1720/2225 train_time:104028ms step_avg:60.48ms
step:1721/2225 train_time:104091ms step_avg:60.48ms
step:1722/2225 train_time:104151ms step_avg:60.48ms
step:1723/2225 train_time:104214ms step_avg:60.48ms
step:1724/2225 train_time:104275ms step_avg:60.48ms
step:1725/2225 train_time:104337ms step_avg:60.49ms
step:1726/2225 train_time:104398ms step_avg:60.49ms
step:1727/2225 train_time:104459ms step_avg:60.49ms
step:1728/2225 train_time:104519ms step_avg:60.49ms
step:1729/2225 train_time:104581ms step_avg:60.49ms
step:1730/2225 train_time:104641ms step_avg:60.49ms
step:1731/2225 train_time:104702ms step_avg:60.49ms
step:1732/2225 train_time:104762ms step_avg:60.49ms
step:1733/2225 train_time:104823ms step_avg:60.49ms
step:1734/2225 train_time:104884ms step_avg:60.49ms
step:1735/2225 train_time:104946ms step_avg:60.49ms
step:1736/2225 train_time:105006ms step_avg:60.49ms
step:1737/2225 train_time:105068ms step_avg:60.49ms
step:1738/2225 train_time:105128ms step_avg:60.49ms
step:1739/2225 train_time:105191ms step_avg:60.49ms
step:1740/2225 train_time:105251ms step_avg:60.49ms
step:1741/2225 train_time:105314ms step_avg:60.49ms
step:1742/2225 train_time:105374ms step_avg:60.49ms
step:1743/2225 train_time:105437ms step_avg:60.49ms
step:1744/2225 train_time:105497ms step_avg:60.49ms
step:1745/2225 train_time:105558ms step_avg:60.49ms
step:1746/2225 train_time:105619ms step_avg:60.49ms
step:1747/2225 train_time:105681ms step_avg:60.49ms
step:1748/2225 train_time:105741ms step_avg:60.49ms
step:1749/2225 train_time:105803ms step_avg:60.49ms
step:1750/2225 train_time:105863ms step_avg:60.49ms
step:1750/2225 val_loss:3.3739 train_time:105925ms step_avg:60.53ms
step:1751/2225 train_time:105947ms step_avg:60.51ms
step:1752/2225 train_time:105986ms step_avg:60.49ms
step:1753/2225 train_time:106052ms step_avg:60.50ms
step:1754/2225 train_time:106117ms step_avg:60.50ms
step:1755/2225 train_time:106180ms step_avg:60.50ms
step:1756/2225 train_time:106240ms step_avg:60.50ms
step:1757/2225 train_time:106302ms step_avg:60.50ms
step:1758/2225 train_time:106362ms step_avg:60.50ms
step:1759/2225 train_time:106423ms step_avg:60.50ms
step:1760/2225 train_time:106483ms step_avg:60.50ms
step:1761/2225 train_time:106543ms step_avg:60.50ms
step:1762/2225 train_time:106603ms step_avg:60.50ms
step:1763/2225 train_time:106664ms step_avg:60.50ms
step:1764/2225 train_time:106723ms step_avg:60.50ms
step:1765/2225 train_time:106783ms step_avg:60.50ms
step:1766/2225 train_time:106844ms step_avg:60.50ms
step:1767/2225 train_time:106906ms step_avg:60.50ms
step:1768/2225 train_time:106968ms step_avg:60.50ms
step:1769/2225 train_time:107031ms step_avg:60.50ms
step:1770/2225 train_time:107093ms step_avg:60.50ms
step:1771/2225 train_time:107156ms step_avg:60.51ms
step:1772/2225 train_time:107216ms step_avg:60.51ms
step:1773/2225 train_time:107278ms step_avg:60.51ms
step:1774/2225 train_time:107338ms step_avg:60.51ms
step:1775/2225 train_time:107399ms step_avg:60.51ms
step:1776/2225 train_time:107459ms step_avg:60.51ms
step:1777/2225 train_time:107521ms step_avg:60.51ms
step:1778/2225 train_time:107580ms step_avg:60.51ms
step:1779/2225 train_time:107642ms step_avg:60.51ms
step:1780/2225 train_time:107703ms step_avg:60.51ms
step:1781/2225 train_time:107763ms step_avg:60.51ms
step:1782/2225 train_time:107824ms step_avg:60.51ms
step:1783/2225 train_time:107885ms step_avg:60.51ms
step:1784/2225 train_time:107946ms step_avg:60.51ms
step:1785/2225 train_time:108009ms step_avg:60.51ms
step:1786/2225 train_time:108069ms step_avg:60.51ms
step:1787/2225 train_time:108132ms step_avg:60.51ms
step:1788/2225 train_time:108193ms step_avg:60.51ms
step:1789/2225 train_time:108255ms step_avg:60.51ms
step:1790/2225 train_time:108315ms step_avg:60.51ms
step:1791/2225 train_time:108377ms step_avg:60.51ms
step:1792/2225 train_time:108436ms step_avg:60.51ms
step:1793/2225 train_time:108498ms step_avg:60.51ms
step:1794/2225 train_time:108558ms step_avg:60.51ms
step:1795/2225 train_time:108620ms step_avg:60.51ms
step:1796/2225 train_time:108680ms step_avg:60.51ms
step:1797/2225 train_time:108742ms step_avg:60.51ms
step:1798/2225 train_time:108802ms step_avg:60.51ms
step:1799/2225 train_time:108863ms step_avg:60.51ms
step:1800/2225 train_time:108925ms step_avg:60.51ms
step:1801/2225 train_time:108987ms step_avg:60.51ms
step:1802/2225 train_time:109048ms step_avg:60.51ms
step:1803/2225 train_time:109110ms step_avg:60.52ms
step:1804/2225 train_time:109170ms step_avg:60.52ms
step:1805/2225 train_time:109232ms step_avg:60.52ms
step:1806/2225 train_time:109292ms step_avg:60.52ms
step:1807/2225 train_time:109355ms step_avg:60.52ms
step:1808/2225 train_time:109414ms step_avg:60.52ms
step:1809/2225 train_time:109477ms step_avg:60.52ms
step:1810/2225 train_time:109537ms step_avg:60.52ms
step:1811/2225 train_time:109599ms step_avg:60.52ms
step:1812/2225 train_time:109659ms step_avg:60.52ms
step:1813/2225 train_time:109720ms step_avg:60.52ms
step:1814/2225 train_time:109781ms step_avg:60.52ms
step:1815/2225 train_time:109843ms step_avg:60.52ms
step:1816/2225 train_time:109904ms step_avg:60.52ms
step:1817/2225 train_time:109965ms step_avg:60.52ms
step:1818/2225 train_time:110026ms step_avg:60.52ms
step:1819/2225 train_time:110087ms step_avg:60.52ms
step:1820/2225 train_time:110148ms step_avg:60.52ms
step:1821/2225 train_time:110209ms step_avg:60.52ms
step:1822/2225 train_time:110270ms step_avg:60.52ms
step:1823/2225 train_time:110331ms step_avg:60.52ms
step:1824/2225 train_time:110392ms step_avg:60.52ms
step:1825/2225 train_time:110453ms step_avg:60.52ms
step:1826/2225 train_time:110513ms step_avg:60.52ms
step:1827/2225 train_time:110575ms step_avg:60.52ms
step:1828/2225 train_time:110636ms step_avg:60.52ms
step:1829/2225 train_time:110698ms step_avg:60.52ms
step:1830/2225 train_time:110759ms step_avg:60.52ms
step:1831/2225 train_time:110821ms step_avg:60.52ms
step:1832/2225 train_time:110882ms step_avg:60.52ms
step:1833/2225 train_time:110944ms step_avg:60.53ms
step:1834/2225 train_time:111005ms step_avg:60.53ms
step:1835/2225 train_time:111066ms step_avg:60.53ms
step:1836/2225 train_time:111127ms step_avg:60.53ms
step:1837/2225 train_time:111188ms step_avg:60.53ms
step:1838/2225 train_time:111248ms step_avg:60.53ms
step:1839/2225 train_time:111310ms step_avg:60.53ms
step:1840/2225 train_time:111371ms step_avg:60.53ms
step:1841/2225 train_time:111432ms step_avg:60.53ms
step:1842/2225 train_time:111493ms step_avg:60.53ms
step:1843/2225 train_time:111554ms step_avg:60.53ms
step:1844/2225 train_time:111615ms step_avg:60.53ms
step:1845/2225 train_time:111677ms step_avg:60.53ms
step:1846/2225 train_time:111737ms step_avg:60.53ms
step:1847/2225 train_time:111799ms step_avg:60.53ms
step:1848/2225 train_time:111859ms step_avg:60.53ms
step:1849/2225 train_time:111922ms step_avg:60.53ms
step:1850/2225 train_time:111983ms step_avg:60.53ms
step:1851/2225 train_time:112045ms step_avg:60.53ms
step:1852/2225 train_time:112106ms step_avg:60.53ms
step:1853/2225 train_time:112167ms step_avg:60.53ms
step:1854/2225 train_time:112227ms step_avg:60.53ms
step:1855/2225 train_time:112289ms step_avg:60.53ms
step:1856/2225 train_time:112349ms step_avg:60.53ms
step:1857/2225 train_time:112411ms step_avg:60.53ms
step:1858/2225 train_time:112471ms step_avg:60.53ms
step:1859/2225 train_time:112533ms step_avg:60.53ms
step:1860/2225 train_time:112592ms step_avg:60.53ms
step:1861/2225 train_time:112655ms step_avg:60.53ms
step:1862/2225 train_time:112716ms step_avg:60.53ms
step:1863/2225 train_time:112777ms step_avg:60.54ms
step:1864/2225 train_time:112838ms step_avg:60.54ms
step:1865/2225 train_time:112900ms step_avg:60.54ms
step:1866/2225 train_time:112961ms step_avg:60.54ms
step:1867/2225 train_time:113023ms step_avg:60.54ms
step:1868/2225 train_time:113083ms step_avg:60.54ms
step:1869/2225 train_time:113145ms step_avg:60.54ms
step:1870/2225 train_time:113206ms step_avg:60.54ms
step:1871/2225 train_time:113267ms step_avg:60.54ms
step:1872/2225 train_time:113328ms step_avg:60.54ms
step:1873/2225 train_time:113389ms step_avg:60.54ms
step:1874/2225 train_time:113449ms step_avg:60.54ms
step:1875/2225 train_time:113511ms step_avg:60.54ms
step:1876/2225 train_time:113572ms step_avg:60.54ms
step:1877/2225 train_time:113634ms step_avg:60.54ms
step:1878/2225 train_time:113695ms step_avg:60.54ms
step:1879/2225 train_time:113757ms step_avg:60.54ms
step:1880/2225 train_time:113817ms step_avg:60.54ms
step:1881/2225 train_time:113879ms step_avg:60.54ms
step:1882/2225 train_time:113940ms step_avg:60.54ms
step:1883/2225 train_time:114003ms step_avg:60.54ms
step:1884/2225 train_time:114063ms step_avg:60.54ms
step:1885/2225 train_time:114125ms step_avg:60.54ms
step:1886/2225 train_time:114184ms step_avg:60.54ms
step:1887/2225 train_time:114246ms step_avg:60.54ms
step:1888/2225 train_time:114306ms step_avg:60.54ms
step:1889/2225 train_time:114368ms step_avg:60.54ms
step:1890/2225 train_time:114429ms step_avg:60.54ms
step:1891/2225 train_time:114490ms step_avg:60.54ms
step:1892/2225 train_time:114550ms step_avg:60.54ms
step:1893/2225 train_time:114612ms step_avg:60.55ms
step:1894/2225 train_time:114672ms step_avg:60.55ms
step:1895/2225 train_time:114735ms step_avg:60.55ms
step:1896/2225 train_time:114796ms step_avg:60.55ms
step:1897/2225 train_time:114858ms step_avg:60.55ms
step:1898/2225 train_time:114919ms step_avg:60.55ms
step:1899/2225 train_time:114981ms step_avg:60.55ms
step:1900/2225 train_time:115042ms step_avg:60.55ms
step:1901/2225 train_time:115104ms step_avg:60.55ms
step:1902/2225 train_time:115164ms step_avg:60.55ms
step:1903/2225 train_time:115225ms step_avg:60.55ms
step:1904/2225 train_time:115285ms step_avg:60.55ms
step:1905/2225 train_time:115348ms step_avg:60.55ms
step:1906/2225 train_time:115408ms step_avg:60.55ms
step:1907/2225 train_time:115470ms step_avg:60.55ms
step:1908/2225 train_time:115530ms step_avg:60.55ms
step:1909/2225 train_time:115591ms step_avg:60.55ms
step:1910/2225 train_time:115652ms step_avg:60.55ms
step:1911/2225 train_time:115714ms step_avg:60.55ms
step:1912/2225 train_time:115774ms step_avg:60.55ms
step:1913/2225 train_time:115836ms step_avg:60.55ms
step:1914/2225 train_time:115897ms step_avg:60.55ms
step:1915/2225 train_time:115959ms step_avg:60.55ms
step:1916/2225 train_time:116019ms step_avg:60.55ms
step:1917/2225 train_time:116081ms step_avg:60.55ms
step:1918/2225 train_time:116142ms step_avg:60.55ms
step:1919/2225 train_time:116203ms step_avg:60.55ms
step:1920/2225 train_time:116264ms step_avg:60.55ms
step:1921/2225 train_time:116325ms step_avg:60.55ms
step:1922/2225 train_time:116386ms step_avg:60.55ms
step:1923/2225 train_time:116447ms step_avg:60.55ms
step:1924/2225 train_time:116508ms step_avg:60.56ms
step:1925/2225 train_time:116569ms step_avg:60.56ms
step:1926/2225 train_time:116629ms step_avg:60.56ms
step:1927/2225 train_time:116691ms step_avg:60.56ms
step:1928/2225 train_time:116753ms step_avg:60.56ms
step:1929/2225 train_time:116813ms step_avg:60.56ms
step:1930/2225 train_time:116873ms step_avg:60.56ms
step:1931/2225 train_time:116937ms step_avg:60.56ms
step:1932/2225 train_time:116997ms step_avg:60.56ms
step:1933/2225 train_time:117059ms step_avg:60.56ms
step:1934/2225 train_time:117120ms step_avg:60.56ms
step:1935/2225 train_time:117183ms step_avg:60.56ms
step:1936/2225 train_time:117243ms step_avg:60.56ms
step:1937/2225 train_time:117305ms step_avg:60.56ms
step:1938/2225 train_time:117365ms step_avg:60.56ms
step:1939/2225 train_time:117427ms step_avg:60.56ms
step:1940/2225 train_time:117487ms step_avg:60.56ms
step:1941/2225 train_time:117549ms step_avg:60.56ms
step:1942/2225 train_time:117609ms step_avg:60.56ms
step:1943/2225 train_time:117671ms step_avg:60.56ms
step:1944/2225 train_time:117730ms step_avg:60.56ms
step:1945/2225 train_time:117793ms step_avg:60.56ms
step:1946/2225 train_time:117853ms step_avg:60.56ms
step:1947/2225 train_time:117916ms step_avg:60.56ms
step:1948/2225 train_time:117976ms step_avg:60.56ms
step:1949/2225 train_time:118038ms step_avg:60.56ms
step:1950/2225 train_time:118099ms step_avg:60.56ms
step:1951/2225 train_time:118162ms step_avg:60.56ms
step:1952/2225 train_time:118222ms step_avg:60.56ms
step:1953/2225 train_time:118283ms step_avg:60.56ms
step:1954/2225 train_time:118343ms step_avg:60.56ms
step:1955/2225 train_time:118405ms step_avg:60.57ms
step:1956/2225 train_time:118466ms step_avg:60.57ms
step:1957/2225 train_time:118528ms step_avg:60.57ms
step:1958/2225 train_time:118589ms step_avg:60.57ms
step:1959/2225 train_time:118650ms step_avg:60.57ms
step:1960/2225 train_time:118710ms step_avg:60.57ms
step:1961/2225 train_time:118772ms step_avg:60.57ms
step:1962/2225 train_time:118833ms step_avg:60.57ms
step:1963/2225 train_time:118895ms step_avg:60.57ms
step:1964/2225 train_time:118956ms step_avg:60.57ms
step:1965/2225 train_time:119018ms step_avg:60.57ms
step:1966/2225 train_time:119078ms step_avg:60.57ms
step:1967/2225 train_time:119141ms step_avg:60.57ms
step:1968/2225 train_time:119202ms step_avg:60.57ms
step:1969/2225 train_time:119264ms step_avg:60.57ms
step:1970/2225 train_time:119324ms step_avg:60.57ms
step:1971/2225 train_time:119386ms step_avg:60.57ms
step:1972/2225 train_time:119446ms step_avg:60.57ms
step:1973/2225 train_time:119508ms step_avg:60.57ms
step:1974/2225 train_time:119567ms step_avg:60.57ms
step:1975/2225 train_time:119629ms step_avg:60.57ms
step:1976/2225 train_time:119689ms step_avg:60.57ms
step:1977/2225 train_time:119751ms step_avg:60.57ms
step:1978/2225 train_time:119811ms step_avg:60.57ms
step:1979/2225 train_time:119873ms step_avg:60.57ms
step:1980/2225 train_time:119933ms step_avg:60.57ms
step:1981/2225 train_time:119996ms step_avg:60.57ms
step:1982/2225 train_time:120057ms step_avg:60.57ms
step:1983/2225 train_time:120119ms step_avg:60.57ms
step:1984/2225 train_time:120179ms step_avg:60.57ms
step:1985/2225 train_time:120242ms step_avg:60.58ms
step:1986/2225 train_time:120302ms step_avg:60.58ms
step:1987/2225 train_time:120364ms step_avg:60.58ms
step:1988/2225 train_time:120424ms step_avg:60.58ms
step:1989/2225 train_time:120486ms step_avg:60.58ms
step:1990/2225 train_time:120546ms step_avg:60.58ms
step:1991/2225 train_time:120608ms step_avg:60.58ms
step:1992/2225 train_time:120668ms step_avg:60.58ms
step:1993/2225 train_time:120730ms step_avg:60.58ms
step:1994/2225 train_time:120791ms step_avg:60.58ms
step:1995/2225 train_time:120853ms step_avg:60.58ms
step:1996/2225 train_time:120913ms step_avg:60.58ms
step:1997/2225 train_time:120975ms step_avg:60.58ms
step:1998/2225 train_time:121035ms step_avg:60.58ms
step:1999/2225 train_time:121097ms step_avg:60.58ms
step:2000/2225 train_time:121158ms step_avg:60.58ms
step:2000/2225 val_loss:3.3201 train_time:121220ms step_avg:60.61ms
step:2001/2225 train_time:121242ms step_avg:60.59ms
step:2002/2225 train_time:121283ms step_avg:60.58ms
step:2003/2225 train_time:121349ms step_avg:60.58ms
step:2004/2225 train_time:121411ms step_avg:60.58ms
step:2005/2225 train_time:121474ms step_avg:60.59ms
step:2006/2225 train_time:121533ms step_avg:60.58ms
step:2007/2225 train_time:121594ms step_avg:60.59ms
step:2008/2225 train_time:121654ms step_avg:60.58ms
step:2009/2225 train_time:121716ms step_avg:60.59ms
step:2010/2225 train_time:121775ms step_avg:60.58ms
step:2011/2225 train_time:121837ms step_avg:60.59ms
step:2012/2225 train_time:121897ms step_avg:60.58ms
step:2013/2225 train_time:121958ms step_avg:60.59ms
step:2014/2225 train_time:122018ms step_avg:60.58ms
step:2015/2225 train_time:122079ms step_avg:60.59ms
step:2016/2225 train_time:122139ms step_avg:60.58ms
step:2017/2225 train_time:122203ms step_avg:60.59ms
step:2018/2225 train_time:122265ms step_avg:60.59ms
step:2019/2225 train_time:122329ms step_avg:60.59ms
step:2020/2225 train_time:122389ms step_avg:60.59ms
step:2021/2225 train_time:122451ms step_avg:60.59ms
step:2022/2225 train_time:122511ms step_avg:60.59ms
step:2023/2225 train_time:122573ms step_avg:60.59ms
step:2024/2225 train_time:122633ms step_avg:60.59ms
step:2025/2225 train_time:122694ms step_avg:60.59ms
step:2026/2225 train_time:122754ms step_avg:60.59ms
step:2027/2225 train_time:122816ms step_avg:60.59ms
step:2028/2225 train_time:122876ms step_avg:60.59ms
step:2029/2225 train_time:122937ms step_avg:60.59ms
step:2030/2225 train_time:122997ms step_avg:60.59ms
step:2031/2225 train_time:123059ms step_avg:60.59ms
step:2032/2225 train_time:123120ms step_avg:60.59ms
step:2033/2225 train_time:123184ms step_avg:60.59ms
step:2034/2225 train_time:123245ms step_avg:60.59ms
step:2035/2225 train_time:123308ms step_avg:60.59ms
step:2036/2225 train_time:123368ms step_avg:60.59ms
step:2037/2225 train_time:123430ms step_avg:60.59ms
step:2038/2225 train_time:123490ms step_avg:60.59ms
step:2039/2225 train_time:123551ms step_avg:60.59ms
step:2040/2225 train_time:123611ms step_avg:60.59ms
step:2041/2225 train_time:123673ms step_avg:60.59ms
step:2042/2225 train_time:123733ms step_avg:60.59ms
step:2043/2225 train_time:123794ms step_avg:60.59ms
step:2044/2225 train_time:123854ms step_avg:60.59ms
step:2045/2225 train_time:123916ms step_avg:60.59ms
step:2046/2225 train_time:123976ms step_avg:60.59ms
step:2047/2225 train_time:124038ms step_avg:60.60ms
step:2048/2225 train_time:124099ms step_avg:60.60ms
step:2049/2225 train_time:124162ms step_avg:60.60ms
step:2050/2225 train_time:124223ms step_avg:60.60ms
step:2051/2225 train_time:124286ms step_avg:60.60ms
step:2052/2225 train_time:124346ms step_avg:60.60ms
step:2053/2225 train_time:124408ms step_avg:60.60ms
step:2054/2225 train_time:124468ms step_avg:60.60ms
step:2055/2225 train_time:124529ms step_avg:60.60ms
step:2056/2225 train_time:124589ms step_avg:60.60ms
step:2057/2225 train_time:124650ms step_avg:60.60ms
step:2058/2225 train_time:124710ms step_avg:60.60ms
step:2059/2225 train_time:124772ms step_avg:60.60ms
step:2060/2225 train_time:124832ms step_avg:60.60ms
step:2061/2225 train_time:124893ms step_avg:60.60ms
step:2062/2225 train_time:124953ms step_avg:60.60ms
step:2063/2225 train_time:125017ms step_avg:60.60ms
step:2064/2225 train_time:125077ms step_avg:60.60ms
step:2065/2225 train_time:125140ms step_avg:60.60ms
step:2066/2225 train_time:125202ms step_avg:60.60ms
step:2067/2225 train_time:125264ms step_avg:60.60ms
step:2068/2225 train_time:125325ms step_avg:60.60ms
step:2069/2225 train_time:125387ms step_avg:60.60ms
step:2070/2225 train_time:125447ms step_avg:60.60ms
step:2071/2225 train_time:125508ms step_avg:60.60ms
step:2072/2225 train_time:125569ms step_avg:60.60ms
step:2073/2225 train_time:125631ms step_avg:60.60ms
step:2074/2225 train_time:125690ms step_avg:60.60ms
step:2075/2225 train_time:125752ms step_avg:60.60ms
step:2076/2225 train_time:125812ms step_avg:60.60ms
step:2077/2225 train_time:125873ms step_avg:60.60ms
step:2078/2225 train_time:125933ms step_avg:60.60ms
step:2079/2225 train_time:125996ms step_avg:60.60ms
step:2080/2225 train_time:126056ms step_avg:60.60ms
step:2081/2225 train_time:126119ms step_avg:60.60ms
step:2082/2225 train_time:126180ms step_avg:60.61ms
step:2083/2225 train_time:126242ms step_avg:60.61ms
step:2084/2225 train_time:126302ms step_avg:60.61ms
step:2085/2225 train_time:126365ms step_avg:60.61ms
step:2086/2225 train_time:126426ms step_avg:60.61ms
step:2087/2225 train_time:126487ms step_avg:60.61ms
step:2088/2225 train_time:126547ms step_avg:60.61ms
step:2089/2225 train_time:126608ms step_avg:60.61ms
step:2090/2225 train_time:126668ms step_avg:60.61ms
step:2091/2225 train_time:126730ms step_avg:60.61ms
step:2092/2225 train_time:126789ms step_avg:60.61ms
step:2093/2225 train_time:126851ms step_avg:60.61ms
step:2094/2225 train_time:126912ms step_avg:60.61ms
step:2095/2225 train_time:126974ms step_avg:60.61ms
step:2096/2225 train_time:127034ms step_avg:60.61ms
step:2097/2225 train_time:127097ms step_avg:60.61ms
step:2098/2225 train_time:127158ms step_avg:60.61ms
step:2099/2225 train_time:127221ms step_avg:60.61ms
step:2100/2225 train_time:127282ms step_avg:60.61ms
step:2101/2225 train_time:127344ms step_avg:60.61ms
step:2102/2225 train_time:127405ms step_avg:60.61ms
step:2103/2225 train_time:127466ms step_avg:60.61ms
step:2104/2225 train_time:127527ms step_avg:60.61ms
step:2105/2225 train_time:127588ms step_avg:60.61ms
step:2106/2225 train_time:127648ms step_avg:60.61ms
step:2107/2225 train_time:127710ms step_avg:60.61ms
step:2108/2225 train_time:127770ms step_avg:60.61ms
step:2109/2225 train_time:127831ms step_avg:60.61ms
step:2110/2225 train_time:127891ms step_avg:60.61ms
step:2111/2225 train_time:127954ms step_avg:60.61ms
step:2112/2225 train_time:128015ms step_avg:60.61ms
step:2113/2225 train_time:128078ms step_avg:60.61ms
step:2114/2225 train_time:128138ms step_avg:60.61ms
step:2115/2225 train_time:128200ms step_avg:60.61ms
step:2116/2225 train_time:128262ms step_avg:60.62ms
step:2117/2225 train_time:128323ms step_avg:60.62ms
step:2118/2225 train_time:128383ms step_avg:60.62ms
step:2119/2225 train_time:128445ms step_avg:60.62ms
step:2120/2225 train_time:128506ms step_avg:60.62ms
step:2121/2225 train_time:128568ms step_avg:60.62ms
step:2122/2225 train_time:128628ms step_avg:60.62ms
step:2123/2225 train_time:128690ms step_avg:60.62ms
step:2124/2225 train_time:128750ms step_avg:60.62ms
step:2125/2225 train_time:128812ms step_avg:60.62ms
step:2126/2225 train_time:128872ms step_avg:60.62ms
step:2127/2225 train_time:128934ms step_avg:60.62ms
step:2128/2225 train_time:128994ms step_avg:60.62ms
step:2129/2225 train_time:129057ms step_avg:60.62ms
step:2130/2225 train_time:129117ms step_avg:60.62ms
step:2131/2225 train_time:129179ms step_avg:60.62ms
step:2132/2225 train_time:129240ms step_avg:60.62ms
step:2133/2225 train_time:129302ms step_avg:60.62ms
step:2134/2225 train_time:129362ms step_avg:60.62ms
step:2135/2225 train_time:129424ms step_avg:60.62ms
step:2136/2225 train_time:129484ms step_avg:60.62ms
step:2137/2225 train_time:129546ms step_avg:60.62ms
step:2138/2225 train_time:129606ms step_avg:60.62ms
step:2139/2225 train_time:129668ms step_avg:60.62ms
step:2140/2225 train_time:129728ms step_avg:60.62ms
step:2141/2225 train_time:129789ms step_avg:60.62ms
step:2142/2225 train_time:129850ms step_avg:60.62ms
step:2143/2225 train_time:129911ms step_avg:60.62ms
step:2144/2225 train_time:129971ms step_avg:60.62ms
step:2145/2225 train_time:130033ms step_avg:60.62ms
step:2146/2225 train_time:130094ms step_avg:60.62ms
step:2147/2225 train_time:130157ms step_avg:60.62ms
step:2148/2225 train_time:130218ms step_avg:60.62ms
step:2149/2225 train_time:130280ms step_avg:60.62ms
step:2150/2225 train_time:130341ms step_avg:60.62ms
step:2151/2225 train_time:130403ms step_avg:60.62ms
step:2152/2225 train_time:130463ms step_avg:60.62ms
step:2153/2225 train_time:130525ms step_avg:60.62ms
step:2154/2225 train_time:130585ms step_avg:60.62ms
step:2155/2225 train_time:130647ms step_avg:60.62ms
step:2156/2225 train_time:130706ms step_avg:60.62ms
step:2157/2225 train_time:130768ms step_avg:60.62ms
step:2158/2225 train_time:130828ms step_avg:60.62ms
step:2159/2225 train_time:130890ms step_avg:60.63ms
step:2160/2225 train_time:130951ms step_avg:60.63ms
step:2161/2225 train_time:131013ms step_avg:60.63ms
step:2162/2225 train_time:131073ms step_avg:60.63ms
step:2163/2225 train_time:131136ms step_avg:60.63ms
step:2164/2225 train_time:131196ms step_avg:60.63ms
step:2165/2225 train_time:131259ms step_avg:60.63ms
step:2166/2225 train_time:131320ms step_avg:60.63ms
step:2167/2225 train_time:131382ms step_avg:60.63ms
step:2168/2225 train_time:131443ms step_avg:60.63ms
step:2169/2225 train_time:131505ms step_avg:60.63ms
step:2170/2225 train_time:131565ms step_avg:60.63ms
step:2171/2225 train_time:131627ms step_avg:60.63ms
step:2172/2225 train_time:131687ms step_avg:60.63ms
step:2173/2225 train_time:131750ms step_avg:60.63ms
step:2174/2225 train_time:131810ms step_avg:60.63ms
step:2175/2225 train_time:131871ms step_avg:60.63ms
step:2176/2225 train_time:131932ms step_avg:60.63ms
step:2177/2225 train_time:131994ms step_avg:60.63ms
step:2178/2225 train_time:132054ms step_avg:60.63ms
step:2179/2225 train_time:132116ms step_avg:60.63ms
step:2180/2225 train_time:132177ms step_avg:60.63ms
step:2181/2225 train_time:132239ms step_avg:60.63ms
step:2182/2225 train_time:132300ms step_avg:60.63ms
step:2183/2225 train_time:132362ms step_avg:60.63ms
step:2184/2225 train_time:132422ms step_avg:60.63ms
step:2185/2225 train_time:132484ms step_avg:60.63ms
step:2186/2225 train_time:132545ms step_avg:60.63ms
step:2187/2225 train_time:132608ms step_avg:60.63ms
step:2188/2225 train_time:132668ms step_avg:60.63ms
step:2189/2225 train_time:132730ms step_avg:60.64ms
step:2190/2225 train_time:132791ms step_avg:60.64ms
step:2191/2225 train_time:132853ms step_avg:60.64ms
step:2192/2225 train_time:132913ms step_avg:60.64ms
step:2193/2225 train_time:132975ms step_avg:60.64ms
step:2194/2225 train_time:133035ms step_avg:60.64ms
step:2195/2225 train_time:133097ms step_avg:60.64ms
step:2196/2225 train_time:133158ms step_avg:60.64ms
step:2197/2225 train_time:133220ms step_avg:60.64ms
step:2198/2225 train_time:133280ms step_avg:60.64ms
step:2199/2225 train_time:133342ms step_avg:60.64ms
step:2200/2225 train_time:133403ms step_avg:60.64ms
step:2201/2225 train_time:133465ms step_avg:60.64ms
step:2202/2225 train_time:133525ms step_avg:60.64ms
step:2203/2225 train_time:133587ms step_avg:60.64ms
step:2204/2225 train_time:133647ms step_avg:60.64ms
step:2205/2225 train_time:133709ms step_avg:60.64ms
step:2206/2225 train_time:133771ms step_avg:60.64ms
step:2207/2225 train_time:133832ms step_avg:60.64ms
step:2208/2225 train_time:133893ms step_avg:60.64ms
step:2209/2225 train_time:133956ms step_avg:60.64ms
step:2210/2225 train_time:134016ms step_avg:60.64ms
step:2211/2225 train_time:134078ms step_avg:60.64ms
step:2212/2225 train_time:134138ms step_avg:60.64ms
step:2213/2225 train_time:134200ms step_avg:60.64ms
step:2214/2225 train_time:134261ms step_avg:60.64ms
step:2215/2225 train_time:134324ms step_avg:60.64ms
step:2216/2225 train_time:134384ms step_avg:60.64ms
step:2217/2225 train_time:134447ms step_avg:60.64ms
step:2218/2225 train_time:134506ms step_avg:60.64ms
step:2219/2225 train_time:134569ms step_avg:60.64ms
step:2220/2225 train_time:134628ms step_avg:60.64ms
step:2221/2225 train_time:134690ms step_avg:60.64ms
step:2222/2225 train_time:134750ms step_avg:60.64ms
step:2223/2225 train_time:134812ms step_avg:60.64ms
step:2224/2225 train_time:134872ms step_avg:60.64ms
step:2225/2225 train_time:134934ms step_avg:60.64ms
step:2225/2225 val_loss:3.2783 train_time:134994ms step_avg:60.67ms
peak memory allocated: 29249 MiB reserved: 47336 MiB

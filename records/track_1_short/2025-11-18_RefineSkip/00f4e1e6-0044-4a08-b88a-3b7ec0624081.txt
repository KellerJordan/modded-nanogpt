import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i ==7:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections[0]
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i ==4:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2185  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 18 21:32:40 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   37C    P0            125W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   38C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   39C    P0            125W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   37C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   33C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          151098      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          151099      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          151100      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          151101      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          151102      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          151103      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          151104      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          151105      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          151099      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          151100      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          151101      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          151102      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          151103      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          151104      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          151105      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2225 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2225 train_time:134ms step_avg:133.98ms
step:2/2225 train_time:176ms step_avg:87.85ms
step:3/2225 train_time:197ms step_avg:65.63ms
step:4/2225 train_time:249ms step_avg:62.26ms
step:5/2225 train_time:307ms step_avg:61.46ms
step:6/2225 train_time:366ms step_avg:61.06ms
step:7/2225 train_time:436ms step_avg:62.33ms
step:8/2225 train_time:494ms step_avg:61.80ms
step:9/2225 train_time:554ms step_avg:61.56ms
step:10/2225 train_time:613ms step_avg:61.26ms
step:11/2225 train_time:673ms step_avg:61.15ms
step:12/2225 train_time:731ms step_avg:60.95ms
step:13/2225 train_time:792ms step_avg:60.90ms
step:14/2225 train_time:850ms step_avg:60.75ms
step:15/2225 train_time:911ms step_avg:60.71ms
step:16/2225 train_time:969ms step_avg:60.56ms
step:17/2225 train_time:1030ms step_avg:60.58ms
step:18/2225 train_time:1091ms step_avg:60.60ms
step:19/2225 train_time:1156ms step_avg:60.82ms
step:20/2225 train_time:1218ms step_avg:60.89ms
step:21/2225 train_time:1280ms step_avg:60.95ms
step:22/2225 train_time:1341ms step_avg:60.94ms
step:23/2225 train_time:1403ms step_avg:60.99ms
step:24/2225 train_time:1463ms step_avg:60.94ms
step:25/2225 train_time:1523ms step_avg:60.92ms
step:26/2225 train_time:1583ms step_avg:60.87ms
step:27/2225 train_time:1643ms step_avg:60.86ms
step:28/2225 train_time:1703ms step_avg:60.83ms
step:29/2225 train_time:1763ms step_avg:60.79ms
step:30/2225 train_time:1822ms step_avg:60.74ms
step:31/2225 train_time:1882ms step_avg:60.72ms
step:32/2225 train_time:1942ms step_avg:60.67ms
step:33/2225 train_time:2003ms step_avg:60.68ms
step:34/2225 train_time:2063ms step_avg:60.68ms
step:35/2225 train_time:2125ms step_avg:60.71ms
step:36/2225 train_time:2185ms step_avg:60.70ms
step:37/2225 train_time:2248ms step_avg:60.74ms
step:38/2225 train_time:2307ms step_avg:60.71ms
step:39/2225 train_time:2368ms step_avg:60.71ms
step:40/2225 train_time:2427ms step_avg:60.68ms
step:41/2225 train_time:2488ms step_avg:60.69ms
step:42/2225 train_time:2548ms step_avg:60.67ms
step:43/2225 train_time:2608ms step_avg:60.66ms
step:44/2225 train_time:2667ms step_avg:60.62ms
step:45/2225 train_time:2727ms step_avg:60.61ms
step:46/2225 train_time:2786ms step_avg:60.57ms
step:47/2225 train_time:2847ms step_avg:60.57ms
step:48/2225 train_time:2906ms step_avg:60.53ms
step:49/2225 train_time:2966ms step_avg:60.53ms
step:50/2225 train_time:3025ms step_avg:60.50ms
step:51/2225 train_time:3086ms step_avg:60.50ms
step:52/2225 train_time:3145ms step_avg:60.48ms
step:53/2225 train_time:3207ms step_avg:60.50ms
step:54/2225 train_time:3266ms step_avg:60.48ms
step:55/2225 train_time:3327ms step_avg:60.49ms
step:56/2225 train_time:3387ms step_avg:60.48ms
step:57/2225 train_time:3448ms step_avg:60.50ms
step:58/2225 train_time:3507ms step_avg:60.47ms
step:59/2225 train_time:3568ms step_avg:60.47ms
step:60/2225 train_time:3627ms step_avg:60.45ms
step:61/2225 train_time:3687ms step_avg:60.45ms
step:62/2225 train_time:3747ms step_avg:60.43ms
step:63/2225 train_time:3807ms step_avg:60.43ms
step:64/2225 train_time:3866ms step_avg:60.41ms
step:65/2225 train_time:3926ms step_avg:60.41ms
step:66/2225 train_time:3985ms step_avg:60.38ms
step:67/2225 train_time:4046ms step_avg:60.39ms
step:68/2225 train_time:4105ms step_avg:60.36ms
step:69/2225 train_time:4165ms step_avg:60.36ms
step:70/2225 train_time:4224ms step_avg:60.34ms
step:71/2225 train_time:4285ms step_avg:60.35ms
step:72/2225 train_time:4344ms step_avg:60.34ms
step:73/2225 train_time:4406ms step_avg:60.35ms
step:74/2225 train_time:4465ms step_avg:60.34ms
step:75/2225 train_time:4526ms step_avg:60.34ms
step:76/2225 train_time:4585ms step_avg:60.33ms
step:77/2225 train_time:4646ms step_avg:60.34ms
step:78/2225 train_time:4705ms step_avg:60.32ms
step:79/2225 train_time:4765ms step_avg:60.32ms
step:80/2225 train_time:4824ms step_avg:60.30ms
step:81/2225 train_time:4885ms step_avg:60.31ms
step:82/2225 train_time:4944ms step_avg:60.29ms
step:83/2225 train_time:5005ms step_avg:60.30ms
step:84/2225 train_time:5064ms step_avg:60.28ms
step:85/2225 train_time:5123ms step_avg:60.27ms
step:86/2225 train_time:5183ms step_avg:60.27ms
step:87/2225 train_time:5244ms step_avg:60.27ms
step:88/2225 train_time:5303ms step_avg:60.26ms
step:89/2225 train_time:5364ms step_avg:60.27ms
step:90/2225 train_time:5423ms step_avg:60.26ms
step:91/2225 train_time:5484ms step_avg:60.26ms
step:92/2225 train_time:5544ms step_avg:60.26ms
step:93/2225 train_time:5605ms step_avg:60.27ms
step:94/2225 train_time:5664ms step_avg:60.26ms
step:95/2225 train_time:5725ms step_avg:60.26ms
step:96/2225 train_time:5784ms step_avg:60.25ms
step:97/2225 train_time:5845ms step_avg:60.25ms
step:98/2225 train_time:5904ms step_avg:60.24ms
step:99/2225 train_time:5964ms step_avg:60.24ms
step:100/2225 train_time:6023ms step_avg:60.23ms
step:101/2225 train_time:6083ms step_avg:60.22ms
step:102/2225 train_time:6142ms step_avg:60.21ms
step:103/2225 train_time:6202ms step_avg:60.22ms
step:104/2225 train_time:6262ms step_avg:60.21ms
step:105/2225 train_time:6322ms step_avg:60.21ms
step:106/2225 train_time:6381ms step_avg:60.20ms
step:107/2225 train_time:6443ms step_avg:60.21ms
step:108/2225 train_time:6502ms step_avg:60.21ms
step:109/2225 train_time:6563ms step_avg:60.21ms
step:110/2225 train_time:6623ms step_avg:60.21ms
step:111/2225 train_time:6683ms step_avg:60.21ms
step:112/2225 train_time:6743ms step_avg:60.20ms
step:113/2225 train_time:6803ms step_avg:60.20ms
step:114/2225 train_time:6862ms step_avg:60.19ms
step:115/2225 train_time:6922ms step_avg:60.19ms
step:116/2225 train_time:6981ms step_avg:60.18ms
step:117/2225 train_time:7042ms step_avg:60.19ms
step:118/2225 train_time:7101ms step_avg:60.18ms
step:119/2225 train_time:7162ms step_avg:60.18ms
step:120/2225 train_time:7221ms step_avg:60.18ms
step:121/2225 train_time:7281ms step_avg:60.18ms
step:122/2225 train_time:7341ms step_avg:60.17ms
step:123/2225 train_time:7401ms step_avg:60.17ms
step:124/2225 train_time:7461ms step_avg:60.17ms
step:125/2225 train_time:7521ms step_avg:60.17ms
step:126/2225 train_time:7581ms step_avg:60.16ms
step:127/2225 train_time:7642ms step_avg:60.17ms
step:128/2225 train_time:7701ms step_avg:60.17ms
step:129/2225 train_time:7763ms step_avg:60.17ms
step:130/2225 train_time:7822ms step_avg:60.17ms
step:131/2225 train_time:7882ms step_avg:60.17ms
step:132/2225 train_time:7942ms step_avg:60.16ms
step:133/2225 train_time:8002ms step_avg:60.17ms
step:134/2225 train_time:8061ms step_avg:60.15ms
step:135/2225 train_time:8121ms step_avg:60.16ms
step:136/2225 train_time:8180ms step_avg:60.15ms
step:137/2225 train_time:8241ms step_avg:60.15ms
step:138/2225 train_time:8300ms step_avg:60.15ms
step:139/2225 train_time:8361ms step_avg:60.15ms
step:140/2225 train_time:8420ms step_avg:60.15ms
step:141/2225 train_time:8481ms step_avg:60.15ms
step:142/2225 train_time:8541ms step_avg:60.15ms
step:143/2225 train_time:8602ms step_avg:60.15ms
step:144/2225 train_time:8661ms step_avg:60.15ms
step:145/2225 train_time:8721ms step_avg:60.15ms
step:146/2225 train_time:8780ms step_avg:60.14ms
step:147/2225 train_time:8841ms step_avg:60.14ms
step:148/2225 train_time:8900ms step_avg:60.14ms
step:149/2225 train_time:8961ms step_avg:60.14ms
step:150/2225 train_time:9020ms step_avg:60.13ms
step:151/2225 train_time:9082ms step_avg:60.14ms
step:152/2225 train_time:9141ms step_avg:60.14ms
step:153/2225 train_time:9201ms step_avg:60.14ms
step:154/2225 train_time:9260ms step_avg:60.13ms
step:155/2225 train_time:9321ms step_avg:60.13ms
step:156/2225 train_time:9380ms step_avg:60.13ms
step:157/2225 train_time:9441ms step_avg:60.13ms
step:158/2225 train_time:9500ms step_avg:60.13ms
step:159/2225 train_time:9561ms step_avg:60.13ms
step:160/2225 train_time:9620ms step_avg:60.12ms
step:161/2225 train_time:9681ms step_avg:60.13ms
step:162/2225 train_time:9741ms step_avg:60.13ms
step:163/2225 train_time:9801ms step_avg:60.13ms
step:164/2225 train_time:9861ms step_avg:60.13ms
step:165/2225 train_time:9921ms step_avg:60.13ms
step:166/2225 train_time:9980ms step_avg:60.12ms
step:167/2225 train_time:10041ms step_avg:60.13ms
step:168/2225 train_time:10100ms step_avg:60.12ms
step:169/2225 train_time:10160ms step_avg:60.12ms
step:170/2225 train_time:10220ms step_avg:60.12ms
step:171/2225 train_time:10280ms step_avg:60.12ms
step:172/2225 train_time:10341ms step_avg:60.12ms
step:173/2225 train_time:10401ms step_avg:60.12ms
step:174/2225 train_time:10461ms step_avg:60.12ms
step:175/2225 train_time:10522ms step_avg:60.13ms
step:176/2225 train_time:10581ms step_avg:60.12ms
step:177/2225 train_time:10642ms step_avg:60.12ms
step:178/2225 train_time:10701ms step_avg:60.12ms
step:179/2225 train_time:10761ms step_avg:60.12ms
step:180/2225 train_time:10820ms step_avg:60.11ms
step:181/2225 train_time:10881ms step_avg:60.11ms
step:182/2225 train_time:10940ms step_avg:60.11ms
step:183/2225 train_time:11001ms step_avg:60.11ms
step:184/2225 train_time:11060ms step_avg:60.11ms
step:185/2225 train_time:11120ms step_avg:60.11ms
step:186/2225 train_time:11179ms step_avg:60.10ms
step:187/2225 train_time:11239ms step_avg:60.10ms
step:188/2225 train_time:11298ms step_avg:60.10ms
step:189/2225 train_time:11359ms step_avg:60.10ms
step:190/2225 train_time:11418ms step_avg:60.10ms
step:191/2225 train_time:11479ms step_avg:60.10ms
step:192/2225 train_time:11540ms step_avg:60.10ms
step:193/2225 train_time:11601ms step_avg:60.11ms
step:194/2225 train_time:11660ms step_avg:60.10ms
step:195/2225 train_time:11720ms step_avg:60.10ms
step:196/2225 train_time:11779ms step_avg:60.10ms
step:197/2225 train_time:11841ms step_avg:60.11ms
step:198/2225 train_time:11900ms step_avg:60.10ms
step:199/2225 train_time:11961ms step_avg:60.10ms
step:200/2225 train_time:12020ms step_avg:60.10ms
step:201/2225 train_time:12080ms step_avg:60.10ms
step:202/2225 train_time:12140ms step_avg:60.10ms
step:203/2225 train_time:12200ms step_avg:60.10ms
step:204/2225 train_time:12259ms step_avg:60.09ms
step:205/2225 train_time:12319ms step_avg:60.09ms
step:206/2225 train_time:12378ms step_avg:60.09ms
step:207/2225 train_time:12439ms step_avg:60.09ms
step:208/2225 train_time:12499ms step_avg:60.09ms
step:209/2225 train_time:12559ms step_avg:60.09ms
step:210/2225 train_time:12618ms step_avg:60.09ms
step:211/2225 train_time:12679ms step_avg:60.09ms
step:212/2225 train_time:12739ms step_avg:60.09ms
step:213/2225 train_time:12800ms step_avg:60.09ms
step:214/2225 train_time:12859ms step_avg:60.09ms
step:215/2225 train_time:12920ms step_avg:60.09ms
step:216/2225 train_time:12979ms step_avg:60.09ms
step:217/2225 train_time:13039ms step_avg:60.09ms
step:218/2225 train_time:13099ms step_avg:60.09ms
step:219/2225 train_time:13159ms step_avg:60.09ms
step:220/2225 train_time:13218ms step_avg:60.08ms
step:221/2225 train_time:13278ms step_avg:60.08ms
step:222/2225 train_time:13337ms step_avg:60.08ms
step:223/2225 train_time:13397ms step_avg:60.08ms
step:224/2225 train_time:13457ms step_avg:60.08ms
step:225/2225 train_time:13518ms step_avg:60.08ms
step:226/2225 train_time:13577ms step_avg:60.08ms
step:227/2225 train_time:13638ms step_avg:60.08ms
step:228/2225 train_time:13698ms step_avg:60.08ms
step:229/2225 train_time:13759ms step_avg:60.08ms
step:230/2225 train_time:13818ms step_avg:60.08ms
step:231/2225 train_time:13879ms step_avg:60.08ms
step:232/2225 train_time:13938ms step_avg:60.08ms
step:233/2225 train_time:13999ms step_avg:60.08ms
step:234/2225 train_time:14058ms step_avg:60.08ms
step:235/2225 train_time:14119ms step_avg:60.08ms
step:236/2225 train_time:14178ms step_avg:60.07ms
step:237/2225 train_time:14238ms step_avg:60.08ms
step:238/2225 train_time:14297ms step_avg:60.07ms
step:239/2225 train_time:14358ms step_avg:60.07ms
step:240/2225 train_time:14417ms step_avg:60.07ms
step:241/2225 train_time:14478ms step_avg:60.07ms
step:242/2225 train_time:14537ms step_avg:60.07ms
step:243/2225 train_time:14598ms step_avg:60.07ms
step:244/2225 train_time:14657ms step_avg:60.07ms
step:245/2225 train_time:14717ms step_avg:60.07ms
step:246/2225 train_time:14776ms step_avg:60.07ms
step:247/2225 train_time:14838ms step_avg:60.07ms
step:248/2225 train_time:14897ms step_avg:60.07ms
step:249/2225 train_time:14958ms step_avg:60.07ms
step:250/2225 train_time:15017ms step_avg:60.07ms
step:250/2225 val_loss:4.0964 train_time:15078ms step_avg:60.31ms
step:251/2225 train_time:15101ms step_avg:60.16ms
step:252/2225 train_time:15139ms step_avg:60.08ms
step:253/2225 train_time:15204ms step_avg:60.09ms
step:254/2225 train_time:15268ms step_avg:60.11ms
step:255/2225 train_time:15333ms step_avg:60.13ms
step:256/2225 train_time:15392ms step_avg:60.13ms
step:257/2225 train_time:15452ms step_avg:60.13ms
step:258/2225 train_time:15510ms step_avg:60.12ms
step:259/2225 train_time:15570ms step_avg:60.12ms
step:260/2225 train_time:15629ms step_avg:60.11ms
step:261/2225 train_time:15688ms step_avg:60.11ms
step:262/2225 train_time:15747ms step_avg:60.10ms
step:263/2225 train_time:15806ms step_avg:60.10ms
step:264/2225 train_time:15864ms step_avg:60.09ms
step:265/2225 train_time:15923ms step_avg:60.09ms
step:266/2225 train_time:15982ms step_avg:60.08ms
step:267/2225 train_time:16042ms step_avg:60.08ms
step:268/2225 train_time:16101ms step_avg:60.08ms
step:269/2225 train_time:16164ms step_avg:60.09ms
step:270/2225 train_time:16227ms step_avg:60.10ms
step:271/2225 train_time:16290ms step_avg:60.11ms
step:272/2225 train_time:16350ms step_avg:60.11ms
step:273/2225 train_time:16411ms step_avg:60.11ms
step:274/2225 train_time:16470ms step_avg:60.11ms
step:275/2225 train_time:16531ms step_avg:60.11ms
step:276/2225 train_time:16589ms step_avg:60.11ms
step:277/2225 train_time:16650ms step_avg:60.11ms
step:278/2225 train_time:16708ms step_avg:60.10ms
step:279/2225 train_time:16768ms step_avg:60.10ms
step:280/2225 train_time:16826ms step_avg:60.09ms
step:281/2225 train_time:16886ms step_avg:60.09ms
step:282/2225 train_time:16945ms step_avg:60.09ms
step:283/2225 train_time:17005ms step_avg:60.09ms
step:284/2225 train_time:17064ms step_avg:60.08ms
step:285/2225 train_time:17126ms step_avg:60.09ms
step:286/2225 train_time:17187ms step_avg:60.09ms
step:287/2225 train_time:17249ms step_avg:60.10ms
step:288/2225 train_time:17310ms step_avg:60.10ms
step:289/2225 train_time:17371ms step_avg:60.11ms
step:290/2225 train_time:17431ms step_avg:60.11ms
step:291/2225 train_time:17492ms step_avg:60.11ms
step:292/2225 train_time:17550ms step_avg:60.10ms
step:293/2225 train_time:17611ms step_avg:60.10ms
step:294/2225 train_time:17669ms step_avg:60.10ms
step:295/2225 train_time:17729ms step_avg:60.10ms
step:296/2225 train_time:17788ms step_avg:60.09ms
step:297/2225 train_time:17847ms step_avg:60.09ms
step:298/2225 train_time:17906ms step_avg:60.09ms
step:299/2225 train_time:17966ms step_avg:60.09ms
step:300/2225 train_time:18025ms step_avg:60.08ms
step:301/2225 train_time:18086ms step_avg:60.09ms
step:302/2225 train_time:18146ms step_avg:60.09ms
step:303/2225 train_time:18207ms step_avg:60.09ms
step:304/2225 train_time:18268ms step_avg:60.09ms
step:305/2225 train_time:18330ms step_avg:60.10ms
step:306/2225 train_time:18389ms step_avg:60.10ms
step:307/2225 train_time:18450ms step_avg:60.10ms
step:308/2225 train_time:18509ms step_avg:60.09ms
step:309/2225 train_time:18571ms step_avg:60.10ms
step:310/2225 train_time:18630ms step_avg:60.10ms
step:311/2225 train_time:18690ms step_avg:60.10ms
step:312/2225 train_time:18748ms step_avg:60.09ms
step:313/2225 train_time:18808ms step_avg:60.09ms
step:314/2225 train_time:18867ms step_avg:60.09ms
step:315/2225 train_time:18927ms step_avg:60.09ms
step:316/2225 train_time:18986ms step_avg:60.08ms
step:317/2225 train_time:19047ms step_avg:60.08ms
step:318/2225 train_time:19106ms step_avg:60.08ms
step:319/2225 train_time:19167ms step_avg:60.09ms
step:320/2225 train_time:19227ms step_avg:60.09ms
step:321/2225 train_time:19288ms step_avg:60.09ms
step:322/2225 train_time:19347ms step_avg:60.09ms
step:323/2225 train_time:19409ms step_avg:60.09ms
step:324/2225 train_time:19469ms step_avg:60.09ms
step:325/2225 train_time:19530ms step_avg:60.09ms
step:326/2225 train_time:19589ms step_avg:60.09ms
step:327/2225 train_time:19649ms step_avg:60.09ms
step:328/2225 train_time:19708ms step_avg:60.09ms
step:329/2225 train_time:19768ms step_avg:60.09ms
step:330/2225 train_time:19827ms step_avg:60.08ms
step:331/2225 train_time:19887ms step_avg:60.08ms
step:332/2225 train_time:19946ms step_avg:60.08ms
step:333/2225 train_time:20006ms step_avg:60.08ms
step:334/2225 train_time:20066ms step_avg:60.08ms
step:335/2225 train_time:20127ms step_avg:60.08ms
step:336/2225 train_time:20187ms step_avg:60.08ms
step:337/2225 train_time:20248ms step_avg:60.08ms
step:338/2225 train_time:20307ms step_avg:60.08ms
step:339/2225 train_time:20368ms step_avg:60.08ms
step:340/2225 train_time:20427ms step_avg:60.08ms
step:341/2225 train_time:20488ms step_avg:60.08ms
step:342/2225 train_time:20547ms step_avg:60.08ms
step:343/2225 train_time:20608ms step_avg:60.08ms
step:344/2225 train_time:20667ms step_avg:60.08ms
step:345/2225 train_time:20727ms step_avg:60.08ms
step:346/2225 train_time:20786ms step_avg:60.08ms
step:347/2225 train_time:20847ms step_avg:60.08ms
step:348/2225 train_time:20905ms step_avg:60.07ms
step:349/2225 train_time:20966ms step_avg:60.07ms
step:350/2225 train_time:21025ms step_avg:60.07ms
step:351/2225 train_time:21086ms step_avg:60.07ms
step:352/2225 train_time:21145ms step_avg:60.07ms
step:353/2225 train_time:21207ms step_avg:60.08ms
step:354/2225 train_time:21266ms step_avg:60.07ms
step:355/2225 train_time:21328ms step_avg:60.08ms
step:356/2225 train_time:21388ms step_avg:60.08ms
step:357/2225 train_time:21448ms step_avg:60.08ms
step:358/2225 train_time:21508ms step_avg:60.08ms
step:359/2225 train_time:21569ms step_avg:60.08ms
step:360/2225 train_time:21628ms step_avg:60.08ms
step:361/2225 train_time:21688ms step_avg:60.08ms
step:362/2225 train_time:21747ms step_avg:60.08ms
step:363/2225 train_time:21808ms step_avg:60.08ms
step:364/2225 train_time:21867ms step_avg:60.07ms
step:365/2225 train_time:21927ms step_avg:60.07ms
step:366/2225 train_time:21986ms step_avg:60.07ms
step:367/2225 train_time:22046ms step_avg:60.07ms
step:368/2225 train_time:22105ms step_avg:60.07ms
step:369/2225 train_time:22166ms step_avg:60.07ms
step:370/2225 train_time:22226ms step_avg:60.07ms
step:371/2225 train_time:22287ms step_avg:60.07ms
step:372/2225 train_time:22346ms step_avg:60.07ms
step:373/2225 train_time:22407ms step_avg:60.07ms
step:374/2225 train_time:22467ms step_avg:60.07ms
step:375/2225 train_time:22528ms step_avg:60.07ms
step:376/2225 train_time:22587ms step_avg:60.07ms
step:377/2225 train_time:22647ms step_avg:60.07ms
step:378/2225 train_time:22706ms step_avg:60.07ms
step:379/2225 train_time:22767ms step_avg:60.07ms
step:380/2225 train_time:22826ms step_avg:60.07ms
step:381/2225 train_time:22887ms step_avg:60.07ms
step:382/2225 train_time:22945ms step_avg:60.07ms
step:383/2225 train_time:23006ms step_avg:60.07ms
step:384/2225 train_time:23065ms step_avg:60.06ms
step:385/2225 train_time:23126ms step_avg:60.07ms
step:386/2225 train_time:23185ms step_avg:60.06ms
step:387/2225 train_time:23245ms step_avg:60.07ms
step:388/2225 train_time:23305ms step_avg:60.06ms
step:389/2225 train_time:23366ms step_avg:60.07ms
step:390/2225 train_time:23425ms step_avg:60.06ms
step:391/2225 train_time:23486ms step_avg:60.07ms
step:392/2225 train_time:23545ms step_avg:60.06ms
step:393/2225 train_time:23606ms step_avg:60.07ms
step:394/2225 train_time:23665ms step_avg:60.06ms
step:395/2225 train_time:23726ms step_avg:60.07ms
step:396/2225 train_time:23785ms step_avg:60.06ms
step:397/2225 train_time:23845ms step_avg:60.06ms
step:398/2225 train_time:23904ms step_avg:60.06ms
step:399/2225 train_time:23965ms step_avg:60.06ms
step:400/2225 train_time:24024ms step_avg:60.06ms
step:401/2225 train_time:24085ms step_avg:60.06ms
step:402/2225 train_time:24144ms step_avg:60.06ms
step:403/2225 train_time:24205ms step_avg:60.06ms
step:404/2225 train_time:24264ms step_avg:60.06ms
step:405/2225 train_time:24326ms step_avg:60.06ms
step:406/2225 train_time:24385ms step_avg:60.06ms
step:407/2225 train_time:24446ms step_avg:60.06ms
step:408/2225 train_time:24504ms step_avg:60.06ms
step:409/2225 train_time:24565ms step_avg:60.06ms
step:410/2225 train_time:24624ms step_avg:60.06ms
step:411/2225 train_time:24685ms step_avg:60.06ms
step:412/2225 train_time:24744ms step_avg:60.06ms
step:413/2225 train_time:24804ms step_avg:60.06ms
step:414/2225 train_time:24863ms step_avg:60.06ms
step:415/2225 train_time:24924ms step_avg:60.06ms
step:416/2225 train_time:24983ms step_avg:60.06ms
step:417/2225 train_time:25044ms step_avg:60.06ms
step:418/2225 train_time:25103ms step_avg:60.05ms
step:419/2225 train_time:25163ms step_avg:60.06ms
step:420/2225 train_time:25222ms step_avg:60.05ms
step:421/2225 train_time:25284ms step_avg:60.06ms
step:422/2225 train_time:25343ms step_avg:60.05ms
step:423/2225 train_time:25404ms step_avg:60.06ms
step:424/2225 train_time:25463ms step_avg:60.05ms
step:425/2225 train_time:25524ms step_avg:60.06ms
step:426/2225 train_time:25583ms step_avg:60.05ms
step:427/2225 train_time:25643ms step_avg:60.05ms
step:428/2225 train_time:25703ms step_avg:60.05ms
step:429/2225 train_time:25763ms step_avg:60.05ms
step:430/2225 train_time:25822ms step_avg:60.05ms
step:431/2225 train_time:25883ms step_avg:60.05ms
step:432/2225 train_time:25942ms step_avg:60.05ms
step:433/2225 train_time:26003ms step_avg:60.05ms
step:434/2225 train_time:26062ms step_avg:60.05ms
step:435/2225 train_time:26123ms step_avg:60.05ms
step:436/2225 train_time:26182ms step_avg:60.05ms
step:437/2225 train_time:26243ms step_avg:60.05ms
step:438/2225 train_time:26302ms step_avg:60.05ms
step:439/2225 train_time:26362ms step_avg:60.05ms
step:440/2225 train_time:26421ms step_avg:60.05ms
step:441/2225 train_time:26482ms step_avg:60.05ms
step:442/2225 train_time:26541ms step_avg:60.05ms
step:443/2225 train_time:26602ms step_avg:60.05ms
step:444/2225 train_time:26660ms step_avg:60.05ms
step:445/2225 train_time:26721ms step_avg:60.05ms
step:446/2225 train_time:26779ms step_avg:60.04ms
step:447/2225 train_time:26840ms step_avg:60.04ms
step:448/2225 train_time:26898ms step_avg:60.04ms
step:449/2225 train_time:26958ms step_avg:60.04ms
step:450/2225 train_time:27017ms step_avg:60.04ms
step:451/2225 train_time:27077ms step_avg:60.04ms
step:452/2225 train_time:27136ms step_avg:60.03ms
step:453/2225 train_time:27196ms step_avg:60.03ms
step:454/2225 train_time:27255ms step_avg:60.03ms
step:455/2225 train_time:27315ms step_avg:60.03ms
step:456/2225 train_time:27374ms step_avg:60.03ms
step:457/2225 train_time:27434ms step_avg:60.03ms
step:458/2225 train_time:27493ms step_avg:60.03ms
step:459/2225 train_time:27553ms step_avg:60.03ms
step:460/2225 train_time:27612ms step_avg:60.03ms
step:461/2225 train_time:27673ms step_avg:60.03ms
step:462/2225 train_time:27732ms step_avg:60.03ms
step:463/2225 train_time:27793ms step_avg:60.03ms
step:464/2225 train_time:27851ms step_avg:60.02ms
step:465/2225 train_time:27912ms step_avg:60.03ms
step:466/2225 train_time:27971ms step_avg:60.02ms
step:467/2225 train_time:28032ms step_avg:60.03ms
step:468/2225 train_time:28091ms step_avg:60.02ms
step:469/2225 train_time:28152ms step_avg:60.03ms
step:470/2225 train_time:28211ms step_avg:60.02ms
step:471/2225 train_time:28272ms step_avg:60.03ms
step:472/2225 train_time:28331ms step_avg:60.02ms
step:473/2225 train_time:28392ms step_avg:60.03ms
step:474/2225 train_time:28452ms step_avg:60.02ms
step:475/2225 train_time:28513ms step_avg:60.03ms
step:476/2225 train_time:28572ms step_avg:60.03ms
step:477/2225 train_time:28633ms step_avg:60.03ms
step:478/2225 train_time:28692ms step_avg:60.03ms
step:479/2225 train_time:28753ms step_avg:60.03ms
step:480/2225 train_time:28812ms step_avg:60.02ms
step:481/2225 train_time:28873ms step_avg:60.03ms
step:482/2225 train_time:28932ms step_avg:60.03ms
step:483/2225 train_time:28993ms step_avg:60.03ms
step:484/2225 train_time:29052ms step_avg:60.02ms
step:485/2225 train_time:29112ms step_avg:60.03ms
step:486/2225 train_time:29172ms step_avg:60.02ms
step:487/2225 train_time:29233ms step_avg:60.03ms
step:488/2225 train_time:29292ms step_avg:60.02ms
step:489/2225 train_time:29353ms step_avg:60.03ms
step:490/2225 train_time:29411ms step_avg:60.02ms
step:491/2225 train_time:29472ms step_avg:60.03ms
step:492/2225 train_time:29532ms step_avg:60.02ms
step:493/2225 train_time:29592ms step_avg:60.03ms
step:494/2225 train_time:29652ms step_avg:60.02ms
step:495/2225 train_time:29713ms step_avg:60.03ms
step:496/2225 train_time:29772ms step_avg:60.02ms
step:497/2225 train_time:29832ms step_avg:60.02ms
step:498/2225 train_time:29891ms step_avg:60.02ms
step:499/2225 train_time:29951ms step_avg:60.02ms
step:500/2225 train_time:30010ms step_avg:60.02ms
step:500/2225 val_loss:3.8205 train_time:30070ms step_avg:60.14ms
step:501/2225 train_time:30093ms step_avg:60.07ms
step:502/2225 train_time:30130ms step_avg:60.02ms
step:503/2225 train_time:30194ms step_avg:60.03ms
step:504/2225 train_time:30257ms step_avg:60.03ms
step:505/2225 train_time:30319ms step_avg:60.04ms
step:506/2225 train_time:30378ms step_avg:60.04ms
step:507/2225 train_time:30438ms step_avg:60.04ms
step:508/2225 train_time:30497ms step_avg:60.03ms
step:509/2225 train_time:30556ms step_avg:60.03ms
step:510/2225 train_time:30615ms step_avg:60.03ms
step:511/2225 train_time:30675ms step_avg:60.03ms
step:512/2225 train_time:30734ms step_avg:60.03ms
step:513/2225 train_time:30793ms step_avg:60.03ms
step:514/2225 train_time:30851ms step_avg:60.02ms
step:515/2225 train_time:30911ms step_avg:60.02ms
step:516/2225 train_time:30969ms step_avg:60.02ms
step:517/2225 train_time:31031ms step_avg:60.02ms
step:518/2225 train_time:31090ms step_avg:60.02ms
step:519/2225 train_time:31152ms step_avg:60.02ms
step:520/2225 train_time:31214ms step_avg:60.03ms
step:521/2225 train_time:31275ms step_avg:60.03ms
step:522/2225 train_time:31335ms step_avg:60.03ms
step:523/2225 train_time:31396ms step_avg:60.03ms
step:524/2225 train_time:31455ms step_avg:60.03ms
step:525/2225 train_time:31515ms step_avg:60.03ms
step:526/2225 train_time:31574ms step_avg:60.03ms
step:527/2225 train_time:31633ms step_avg:60.02ms
step:528/2225 train_time:31691ms step_avg:60.02ms
step:529/2225 train_time:31751ms step_avg:60.02ms
step:530/2225 train_time:31809ms step_avg:60.02ms
step:531/2225 train_time:31869ms step_avg:60.02ms
step:532/2225 train_time:31928ms step_avg:60.01ms
step:533/2225 train_time:31988ms step_avg:60.01ms
step:534/2225 train_time:32047ms step_avg:60.01ms
step:535/2225 train_time:32109ms step_avg:60.02ms
step:536/2225 train_time:32169ms step_avg:60.02ms
step:537/2225 train_time:32231ms step_avg:60.02ms
step:538/2225 train_time:32291ms step_avg:60.02ms
step:539/2225 train_time:32353ms step_avg:60.02ms
step:540/2225 train_time:32413ms step_avg:60.02ms
step:541/2225 train_time:32474ms step_avg:60.03ms
step:542/2225 train_time:32533ms step_avg:60.02ms
step:543/2225 train_time:32593ms step_avg:60.02ms
step:544/2225 train_time:32651ms step_avg:60.02ms
step:545/2225 train_time:32711ms step_avg:60.02ms
step:546/2225 train_time:32770ms step_avg:60.02ms
step:547/2225 train_time:32829ms step_avg:60.02ms
step:548/2225 train_time:32888ms step_avg:60.01ms
step:549/2225 train_time:32947ms step_avg:60.01ms
step:550/2225 train_time:33007ms step_avg:60.01ms
step:551/2225 train_time:33067ms step_avg:60.01ms
step:552/2225 train_time:33126ms step_avg:60.01ms
step:553/2225 train_time:33188ms step_avg:60.01ms
step:554/2225 train_time:33248ms step_avg:60.01ms
step:555/2225 train_time:33309ms step_avg:60.02ms
step:556/2225 train_time:33370ms step_avg:60.02ms
step:557/2225 train_time:33431ms step_avg:60.02ms
step:558/2225 train_time:33490ms step_avg:60.02ms
step:559/2225 train_time:33551ms step_avg:60.02ms
step:560/2225 train_time:33610ms step_avg:60.02ms
step:561/2225 train_time:33670ms step_avg:60.02ms
step:562/2225 train_time:33728ms step_avg:60.01ms
step:563/2225 train_time:33789ms step_avg:60.02ms
step:564/2225 train_time:33847ms step_avg:60.01ms
step:565/2225 train_time:33907ms step_avg:60.01ms
step:566/2225 train_time:33966ms step_avg:60.01ms
step:567/2225 train_time:34026ms step_avg:60.01ms
step:568/2225 train_time:34085ms step_avg:60.01ms
step:569/2225 train_time:34146ms step_avg:60.01ms
step:570/2225 train_time:34206ms step_avg:60.01ms
step:571/2225 train_time:34267ms step_avg:60.01ms
step:572/2225 train_time:34327ms step_avg:60.01ms
step:573/2225 train_time:34389ms step_avg:60.02ms
step:574/2225 train_time:34448ms step_avg:60.01ms
step:575/2225 train_time:34510ms step_avg:60.02ms
step:576/2225 train_time:34569ms step_avg:60.02ms
step:577/2225 train_time:34629ms step_avg:60.02ms
step:578/2225 train_time:34688ms step_avg:60.01ms
step:579/2225 train_time:34748ms step_avg:60.01ms
step:580/2225 train_time:34806ms step_avg:60.01ms
step:581/2225 train_time:34866ms step_avg:60.01ms
step:582/2225 train_time:34925ms step_avg:60.01ms
step:583/2225 train_time:34985ms step_avg:60.01ms
step:584/2225 train_time:35044ms step_avg:60.01ms
step:585/2225 train_time:35104ms step_avg:60.01ms
step:586/2225 train_time:35163ms step_avg:60.01ms
step:587/2225 train_time:35224ms step_avg:60.01ms
step:588/2225 train_time:35283ms step_avg:60.01ms
step:589/2225 train_time:35344ms step_avg:60.01ms
step:590/2225 train_time:35403ms step_avg:60.01ms
step:591/2225 train_time:35464ms step_avg:60.01ms
step:592/2225 train_time:35523ms step_avg:60.01ms
step:593/2225 train_time:35584ms step_avg:60.01ms
step:594/2225 train_time:35643ms step_avg:60.00ms
step:595/2225 train_time:35703ms step_avg:60.01ms
step:596/2225 train_time:35762ms step_avg:60.00ms
step:597/2225 train_time:35821ms step_avg:60.00ms
step:598/2225 train_time:35880ms step_avg:60.00ms
step:599/2225 train_time:35940ms step_avg:60.00ms
step:600/2225 train_time:35998ms step_avg:60.00ms
step:601/2225 train_time:36058ms step_avg:60.00ms
step:602/2225 train_time:36116ms step_avg:59.99ms
step:603/2225 train_time:36177ms step_avg:60.00ms
step:604/2225 train_time:36236ms step_avg:59.99ms
step:605/2225 train_time:36297ms step_avg:59.99ms
step:606/2225 train_time:36355ms step_avg:59.99ms
step:607/2225 train_time:36417ms step_avg:59.99ms
step:608/2225 train_time:36476ms step_avg:59.99ms
step:609/2225 train_time:36536ms step_avg:59.99ms
step:610/2225 train_time:36596ms step_avg:59.99ms
step:611/2225 train_time:36656ms step_avg:59.99ms
step:612/2225 train_time:36715ms step_avg:59.99ms
step:613/2225 train_time:36775ms step_avg:59.99ms
step:614/2225 train_time:36833ms step_avg:59.99ms
step:615/2225 train_time:36893ms step_avg:59.99ms
step:616/2225 train_time:36951ms step_avg:59.99ms
step:617/2225 train_time:37011ms step_avg:59.99ms
step:618/2225 train_time:37070ms step_avg:59.98ms
step:619/2225 train_time:37131ms step_avg:59.99ms
step:620/2225 train_time:37190ms step_avg:59.98ms
step:621/2225 train_time:37250ms step_avg:59.98ms
step:622/2225 train_time:37310ms step_avg:59.98ms
step:623/2225 train_time:37371ms step_avg:59.99ms
step:624/2225 train_time:37431ms step_avg:59.99ms
step:625/2225 train_time:37492ms step_avg:59.99ms
step:626/2225 train_time:37551ms step_avg:59.99ms
step:627/2225 train_time:37612ms step_avg:59.99ms
step:628/2225 train_time:37671ms step_avg:59.99ms
step:629/2225 train_time:37732ms step_avg:59.99ms
step:630/2225 train_time:37791ms step_avg:59.99ms
step:631/2225 train_time:37851ms step_avg:59.99ms
step:632/2225 train_time:37910ms step_avg:59.98ms
step:633/2225 train_time:37971ms step_avg:59.99ms
step:634/2225 train_time:38030ms step_avg:59.98ms
step:635/2225 train_time:38090ms step_avg:59.98ms
step:636/2225 train_time:38149ms step_avg:59.98ms
step:637/2225 train_time:38210ms step_avg:59.98ms
step:638/2225 train_time:38269ms step_avg:59.98ms
step:639/2225 train_time:38330ms step_avg:59.98ms
step:640/2225 train_time:38389ms step_avg:59.98ms
step:641/2225 train_time:38450ms step_avg:59.98ms
step:642/2225 train_time:38510ms step_avg:59.98ms
step:643/2225 train_time:38571ms step_avg:59.99ms
step:644/2225 train_time:38630ms step_avg:59.98ms
step:645/2225 train_time:38691ms step_avg:59.99ms
step:646/2225 train_time:38750ms step_avg:59.98ms
step:647/2225 train_time:38811ms step_avg:59.99ms
step:648/2225 train_time:38870ms step_avg:59.98ms
step:649/2225 train_time:38930ms step_avg:59.98ms
step:650/2225 train_time:38989ms step_avg:59.98ms
step:651/2225 train_time:39049ms step_avg:59.98ms
step:652/2225 train_time:39108ms step_avg:59.98ms
step:653/2225 train_time:39168ms step_avg:59.98ms
step:654/2225 train_time:39228ms step_avg:59.98ms
step:655/2225 train_time:39288ms step_avg:59.98ms
step:656/2225 train_time:39347ms step_avg:59.98ms
step:657/2225 train_time:39408ms step_avg:59.98ms
step:658/2225 train_time:39468ms step_avg:59.98ms
step:659/2225 train_time:39529ms step_avg:59.98ms
step:660/2225 train_time:39588ms step_avg:59.98ms
step:661/2225 train_time:39648ms step_avg:59.98ms
step:662/2225 train_time:39707ms step_avg:59.98ms
step:663/2225 train_time:39768ms step_avg:59.98ms
step:664/2225 train_time:39827ms step_avg:59.98ms
step:665/2225 train_time:39887ms step_avg:59.98ms
step:666/2225 train_time:39947ms step_avg:59.98ms
step:667/2225 train_time:40007ms step_avg:59.98ms
step:668/2225 train_time:40066ms step_avg:59.98ms
step:669/2225 train_time:40126ms step_avg:59.98ms
step:670/2225 train_time:40185ms step_avg:59.98ms
step:671/2225 train_time:40246ms step_avg:59.98ms
step:672/2225 train_time:40305ms step_avg:59.98ms
step:673/2225 train_time:40366ms step_avg:59.98ms
step:674/2225 train_time:40425ms step_avg:59.98ms
step:675/2225 train_time:40486ms step_avg:59.98ms
step:676/2225 train_time:40545ms step_avg:59.98ms
step:677/2225 train_time:40606ms step_avg:59.98ms
step:678/2225 train_time:40665ms step_avg:59.98ms
step:679/2225 train_time:40726ms step_avg:59.98ms
step:680/2225 train_time:40785ms step_avg:59.98ms
step:681/2225 train_time:40846ms step_avg:59.98ms
step:682/2225 train_time:40905ms step_avg:59.98ms
step:683/2225 train_time:40966ms step_avg:59.98ms
step:684/2225 train_time:41025ms step_avg:59.98ms
step:685/2225 train_time:41085ms step_avg:59.98ms
step:686/2225 train_time:41144ms step_avg:59.98ms
step:687/2225 train_time:41204ms step_avg:59.98ms
step:688/2225 train_time:41263ms step_avg:59.97ms
step:689/2225 train_time:41323ms step_avg:59.97ms
step:690/2225 train_time:41381ms step_avg:59.97ms
step:691/2225 train_time:41442ms step_avg:59.97ms
step:692/2225 train_time:41500ms step_avg:59.97ms
step:693/2225 train_time:41560ms step_avg:59.97ms
step:694/2225 train_time:41619ms step_avg:59.97ms
step:695/2225 train_time:41679ms step_avg:59.97ms
step:696/2225 train_time:41738ms step_avg:59.97ms
step:697/2225 train_time:41798ms step_avg:59.97ms
step:698/2225 train_time:41857ms step_avg:59.97ms
step:699/2225 train_time:41917ms step_avg:59.97ms
step:700/2225 train_time:41976ms step_avg:59.97ms
step:701/2225 train_time:42036ms step_avg:59.97ms
step:702/2225 train_time:42095ms step_avg:59.97ms
step:703/2225 train_time:42155ms step_avg:59.96ms
step:704/2225 train_time:42215ms step_avg:59.96ms
step:705/2225 train_time:42277ms step_avg:59.97ms
step:706/2225 train_time:42336ms step_avg:59.97ms
step:707/2225 train_time:42396ms step_avg:59.97ms
step:708/2225 train_time:42454ms step_avg:59.96ms
step:709/2225 train_time:42515ms step_avg:59.96ms
step:710/2225 train_time:42574ms step_avg:59.96ms
step:711/2225 train_time:42635ms step_avg:59.96ms
step:712/2225 train_time:42693ms step_avg:59.96ms
step:713/2225 train_time:42754ms step_avg:59.96ms
step:714/2225 train_time:42813ms step_avg:59.96ms
step:715/2225 train_time:42873ms step_avg:59.96ms
step:716/2225 train_time:42932ms step_avg:59.96ms
step:717/2225 train_time:42992ms step_avg:59.96ms
step:718/2225 train_time:43051ms step_avg:59.96ms
step:719/2225 train_time:43111ms step_avg:59.96ms
step:720/2225 train_time:43171ms step_avg:59.96ms
step:721/2225 train_time:43231ms step_avg:59.96ms
step:722/2225 train_time:43290ms step_avg:59.96ms
step:723/2225 train_time:43350ms step_avg:59.96ms
step:724/2225 train_time:43410ms step_avg:59.96ms
step:725/2225 train_time:43471ms step_avg:59.96ms
step:726/2225 train_time:43530ms step_avg:59.96ms
step:727/2225 train_time:43591ms step_avg:59.96ms
step:728/2225 train_time:43650ms step_avg:59.96ms
step:729/2225 train_time:43710ms step_avg:59.96ms
step:730/2225 train_time:43770ms step_avg:59.96ms
step:731/2225 train_time:43831ms step_avg:59.96ms
step:732/2225 train_time:43891ms step_avg:59.96ms
step:733/2225 train_time:43953ms step_avg:59.96ms
step:734/2225 train_time:44013ms step_avg:59.96ms
step:735/2225 train_time:44074ms step_avg:59.96ms
step:736/2225 train_time:44133ms step_avg:59.96ms
step:737/2225 train_time:44194ms step_avg:59.97ms
step:738/2225 train_time:44254ms step_avg:59.96ms
step:739/2225 train_time:44314ms step_avg:59.97ms
step:740/2225 train_time:44374ms step_avg:59.97ms
step:741/2225 train_time:44435ms step_avg:59.97ms
step:742/2225 train_time:44495ms step_avg:59.97ms
step:743/2225 train_time:44556ms step_avg:59.97ms
step:744/2225 train_time:44616ms step_avg:59.97ms
step:745/2225 train_time:44677ms step_avg:59.97ms
step:746/2225 train_time:44737ms step_avg:59.97ms
step:747/2225 train_time:44798ms step_avg:59.97ms
step:748/2225 train_time:44857ms step_avg:59.97ms
step:749/2225 train_time:44918ms step_avg:59.97ms
step:750/2225 train_time:44978ms step_avg:59.97ms
step:750/2225 val_loss:3.6674 train_time:45039ms step_avg:60.05ms
step:751/2225 train_time:45063ms step_avg:60.00ms
step:752/2225 train_time:45102ms step_avg:59.98ms
step:753/2225 train_time:45160ms step_avg:59.97ms
step:754/2225 train_time:45219ms step_avg:59.97ms
step:755/2225 train_time:45281ms step_avg:59.97ms
step:756/2225 train_time:45341ms step_avg:59.97ms
step:757/2225 train_time:45400ms step_avg:59.97ms
step:758/2225 train_time:45459ms step_avg:59.97ms
step:759/2225 train_time:45520ms step_avg:59.97ms
step:760/2225 train_time:45579ms step_avg:59.97ms
step:761/2225 train_time:45638ms step_avg:59.97ms
step:762/2225 train_time:45697ms step_avg:59.97ms
step:763/2225 train_time:45758ms step_avg:59.97ms
step:764/2225 train_time:45818ms step_avg:59.97ms
step:765/2225 train_time:45878ms step_avg:59.97ms
step:766/2225 train_time:45940ms step_avg:59.97ms
step:767/2225 train_time:46007ms step_avg:59.98ms
step:768/2225 train_time:46068ms step_avg:59.98ms
step:769/2225 train_time:46130ms step_avg:59.99ms
step:770/2225 train_time:46190ms step_avg:59.99ms
step:771/2225 train_time:46252ms step_avg:59.99ms
step:772/2225 train_time:46312ms step_avg:59.99ms
step:773/2225 train_time:46374ms step_avg:59.99ms
step:774/2225 train_time:46433ms step_avg:59.99ms
step:775/2225 train_time:46494ms step_avg:59.99ms
step:776/2225 train_time:46553ms step_avg:59.99ms
step:777/2225 train_time:46614ms step_avg:59.99ms
step:778/2225 train_time:46673ms step_avg:59.99ms
step:779/2225 train_time:46734ms step_avg:59.99ms
step:780/2225 train_time:46793ms step_avg:59.99ms
step:781/2225 train_time:46854ms step_avg:59.99ms
step:782/2225 train_time:46915ms step_avg:59.99ms
step:783/2225 train_time:46977ms step_avg:60.00ms
step:784/2225 train_time:47038ms step_avg:60.00ms
step:785/2225 train_time:47100ms step_avg:60.00ms
step:786/2225 train_time:47161ms step_avg:60.00ms
step:787/2225 train_time:47223ms step_avg:60.00ms
step:788/2225 train_time:47282ms step_avg:60.00ms
step:789/2225 train_time:47342ms step_avg:60.00ms
step:790/2225 train_time:47401ms step_avg:60.00ms
step:791/2225 train_time:47462ms step_avg:60.00ms
step:792/2225 train_time:47521ms step_avg:60.00ms
step:793/2225 train_time:47582ms step_avg:60.00ms
step:794/2225 train_time:47641ms step_avg:60.00ms
step:795/2225 train_time:47701ms step_avg:60.00ms
step:796/2225 train_time:47760ms step_avg:60.00ms
step:797/2225 train_time:47820ms step_avg:60.00ms
step:798/2225 train_time:47880ms step_avg:60.00ms
step:799/2225 train_time:47941ms step_avg:60.00ms
step:800/2225 train_time:48000ms step_avg:60.00ms
step:801/2225 train_time:48063ms step_avg:60.00ms
step:802/2225 train_time:48123ms step_avg:60.00ms
step:803/2225 train_time:48184ms step_avg:60.00ms
step:804/2225 train_time:48243ms step_avg:60.00ms
step:805/2225 train_time:48304ms step_avg:60.00ms
step:806/2225 train_time:48364ms step_avg:60.00ms
step:807/2225 train_time:48425ms step_avg:60.01ms
step:808/2225 train_time:48484ms step_avg:60.00ms
step:809/2225 train_time:48545ms step_avg:60.01ms
step:810/2225 train_time:48605ms step_avg:60.01ms
step:811/2225 train_time:48667ms step_avg:60.01ms
step:812/2225 train_time:48726ms step_avg:60.01ms
step:813/2225 train_time:48787ms step_avg:60.01ms
step:814/2225 train_time:48847ms step_avg:60.01ms
step:815/2225 train_time:48909ms step_avg:60.01ms
step:816/2225 train_time:48970ms step_avg:60.01ms
step:817/2225 train_time:49032ms step_avg:60.01ms
step:818/2225 train_time:49092ms step_avg:60.02ms
step:819/2225 train_time:49155ms step_avg:60.02ms
step:820/2225 train_time:49215ms step_avg:60.02ms
step:821/2225 train_time:49276ms step_avg:60.02ms
step:822/2225 train_time:49337ms step_avg:60.02ms
step:823/2225 train_time:49397ms step_avg:60.02ms
step:824/2225 train_time:49457ms step_avg:60.02ms
step:825/2225 train_time:49519ms step_avg:60.02ms
step:826/2225 train_time:49579ms step_avg:60.02ms
step:827/2225 train_time:49639ms step_avg:60.02ms
step:828/2225 train_time:49699ms step_avg:60.02ms
step:829/2225 train_time:49760ms step_avg:60.02ms
step:830/2225 train_time:49819ms step_avg:60.02ms
step:831/2225 train_time:49880ms step_avg:60.02ms
step:832/2225 train_time:49940ms step_avg:60.02ms
step:833/2225 train_time:50001ms step_avg:60.02ms
step:834/2225 train_time:50060ms step_avg:60.02ms
step:835/2225 train_time:50121ms step_avg:60.03ms
step:836/2225 train_time:50181ms step_avg:60.02ms
step:837/2225 train_time:50242ms step_avg:60.03ms
step:838/2225 train_time:50302ms step_avg:60.03ms
step:839/2225 train_time:50364ms step_avg:60.03ms
step:840/2225 train_time:50423ms step_avg:60.03ms
step:841/2225 train_time:50484ms step_avg:60.03ms
step:842/2225 train_time:50544ms step_avg:60.03ms
step:843/2225 train_time:50605ms step_avg:60.03ms
step:844/2225 train_time:50664ms step_avg:60.03ms
step:845/2225 train_time:50726ms step_avg:60.03ms
step:846/2225 train_time:50785ms step_avg:60.03ms
step:847/2225 train_time:50847ms step_avg:60.03ms
step:848/2225 train_time:50907ms step_avg:60.03ms
step:849/2225 train_time:50969ms step_avg:60.03ms
step:850/2225 train_time:51029ms step_avg:60.03ms
step:851/2225 train_time:51091ms step_avg:60.04ms
step:852/2225 train_time:51151ms step_avg:60.04ms
step:853/2225 train_time:51213ms step_avg:60.04ms
step:854/2225 train_time:51273ms step_avg:60.04ms
step:855/2225 train_time:51334ms step_avg:60.04ms
step:856/2225 train_time:51394ms step_avg:60.04ms
step:857/2225 train_time:51455ms step_avg:60.04ms
step:858/2225 train_time:51515ms step_avg:60.04ms
step:859/2225 train_time:51577ms step_avg:60.04ms
step:860/2225 train_time:51637ms step_avg:60.04ms
step:861/2225 train_time:51698ms step_avg:60.04ms
step:862/2225 train_time:51758ms step_avg:60.04ms
step:863/2225 train_time:51819ms step_avg:60.05ms
step:864/2225 train_time:51878ms step_avg:60.04ms
step:865/2225 train_time:51939ms step_avg:60.05ms
step:866/2225 train_time:51999ms step_avg:60.05ms
step:867/2225 train_time:52061ms step_avg:60.05ms
step:868/2225 train_time:52121ms step_avg:60.05ms
step:869/2225 train_time:52182ms step_avg:60.05ms
step:870/2225 train_time:52241ms step_avg:60.05ms
step:871/2225 train_time:52302ms step_avg:60.05ms
step:872/2225 train_time:52362ms step_avg:60.05ms
step:873/2225 train_time:52423ms step_avg:60.05ms
step:874/2225 train_time:52482ms step_avg:60.05ms
step:875/2225 train_time:52544ms step_avg:60.05ms
step:876/2225 train_time:52603ms step_avg:60.05ms
step:877/2225 train_time:52665ms step_avg:60.05ms
step:878/2225 train_time:52725ms step_avg:60.05ms
step:879/2225 train_time:52786ms step_avg:60.05ms
step:880/2225 train_time:52845ms step_avg:60.05ms
step:881/2225 train_time:52907ms step_avg:60.05ms
step:882/2225 train_time:52967ms step_avg:60.05ms
step:883/2225 train_time:53029ms step_avg:60.06ms
step:884/2225 train_time:53089ms step_avg:60.06ms
step:885/2225 train_time:53151ms step_avg:60.06ms
step:886/2225 train_time:53211ms step_avg:60.06ms
step:887/2225 train_time:53272ms step_avg:60.06ms
step:888/2225 train_time:53332ms step_avg:60.06ms
step:889/2225 train_time:53394ms step_avg:60.06ms
step:890/2225 train_time:53454ms step_avg:60.06ms
step:891/2225 train_time:53515ms step_avg:60.06ms
step:892/2225 train_time:53575ms step_avg:60.06ms
step:893/2225 train_time:53636ms step_avg:60.06ms
step:894/2225 train_time:53696ms step_avg:60.06ms
step:895/2225 train_time:53758ms step_avg:60.06ms
step:896/2225 train_time:53818ms step_avg:60.06ms
step:897/2225 train_time:53879ms step_avg:60.07ms
step:898/2225 train_time:53939ms step_avg:60.07ms
step:899/2225 train_time:54000ms step_avg:60.07ms
step:900/2225 train_time:54059ms step_avg:60.07ms
step:901/2225 train_time:54121ms step_avg:60.07ms
step:902/2225 train_time:54180ms step_avg:60.07ms
step:903/2225 train_time:54242ms step_avg:60.07ms
step:904/2225 train_time:54302ms step_avg:60.07ms
step:905/2225 train_time:54363ms step_avg:60.07ms
step:906/2225 train_time:54422ms step_avg:60.07ms
step:907/2225 train_time:54482ms step_avg:60.07ms
step:908/2225 train_time:54542ms step_avg:60.07ms
step:909/2225 train_time:54603ms step_avg:60.07ms
step:910/2225 train_time:54663ms step_avg:60.07ms
step:911/2225 train_time:54724ms step_avg:60.07ms
step:912/2225 train_time:54784ms step_avg:60.07ms
step:913/2225 train_time:54845ms step_avg:60.07ms
step:914/2225 train_time:54905ms step_avg:60.07ms
step:915/2225 train_time:54967ms step_avg:60.07ms
step:916/2225 train_time:55027ms step_avg:60.07ms
step:917/2225 train_time:55089ms step_avg:60.08ms
step:918/2225 train_time:55149ms step_avg:60.07ms
step:919/2225 train_time:55211ms step_avg:60.08ms
step:920/2225 train_time:55271ms step_avg:60.08ms
step:921/2225 train_time:55333ms step_avg:60.08ms
step:922/2225 train_time:55393ms step_avg:60.08ms
step:923/2225 train_time:55455ms step_avg:60.08ms
step:924/2225 train_time:55515ms step_avg:60.08ms
step:925/2225 train_time:55576ms step_avg:60.08ms
step:926/2225 train_time:55636ms step_avg:60.08ms
step:927/2225 train_time:55697ms step_avg:60.08ms
step:928/2225 train_time:55757ms step_avg:60.08ms
step:929/2225 train_time:55819ms step_avg:60.08ms
step:930/2225 train_time:55878ms step_avg:60.08ms
step:931/2225 train_time:55939ms step_avg:60.08ms
step:932/2225 train_time:55998ms step_avg:60.08ms
step:933/2225 train_time:56060ms step_avg:60.09ms
step:934/2225 train_time:56120ms step_avg:60.09ms
step:935/2225 train_time:56181ms step_avg:60.09ms
step:936/2225 train_time:56241ms step_avg:60.09ms
step:937/2225 train_time:56302ms step_avg:60.09ms
step:938/2225 train_time:56362ms step_avg:60.09ms
step:939/2225 train_time:56423ms step_avg:60.09ms
step:940/2225 train_time:56483ms step_avg:60.09ms
step:941/2225 train_time:56544ms step_avg:60.09ms
step:942/2225 train_time:56602ms step_avg:60.09ms
step:943/2225 train_time:56664ms step_avg:60.09ms
step:944/2225 train_time:56723ms step_avg:60.09ms
step:945/2225 train_time:56784ms step_avg:60.09ms
step:946/2225 train_time:56844ms step_avg:60.09ms
step:947/2225 train_time:56905ms step_avg:60.09ms
step:948/2225 train_time:56965ms step_avg:60.09ms
step:949/2225 train_time:57027ms step_avg:60.09ms
step:950/2225 train_time:57087ms step_avg:60.09ms
step:951/2225 train_time:57149ms step_avg:60.09ms
step:952/2225 train_time:57209ms step_avg:60.09ms
step:953/2225 train_time:57271ms step_avg:60.10ms
step:954/2225 train_time:57331ms step_avg:60.10ms
step:955/2225 train_time:57393ms step_avg:60.10ms
step:956/2225 train_time:57453ms step_avg:60.10ms
step:957/2225 train_time:57515ms step_avg:60.10ms
step:958/2225 train_time:57574ms step_avg:60.10ms
step:959/2225 train_time:57636ms step_avg:60.10ms
step:960/2225 train_time:57696ms step_avg:60.10ms
step:961/2225 train_time:57757ms step_avg:60.10ms
step:962/2225 train_time:57817ms step_avg:60.10ms
step:963/2225 train_time:57878ms step_avg:60.10ms
step:964/2225 train_time:57938ms step_avg:60.10ms
step:965/2225 train_time:57999ms step_avg:60.10ms
step:966/2225 train_time:58059ms step_avg:60.10ms
step:967/2225 train_time:58121ms step_avg:60.10ms
step:968/2225 train_time:58180ms step_avg:60.10ms
step:969/2225 train_time:58242ms step_avg:60.11ms
step:970/2225 train_time:58302ms step_avg:60.11ms
step:971/2225 train_time:58363ms step_avg:60.11ms
step:972/2225 train_time:58423ms step_avg:60.11ms
step:973/2225 train_time:58483ms step_avg:60.11ms
step:974/2225 train_time:58542ms step_avg:60.11ms
step:975/2225 train_time:58603ms step_avg:60.11ms
step:976/2225 train_time:58663ms step_avg:60.11ms
step:977/2225 train_time:58724ms step_avg:60.11ms
step:978/2225 train_time:58784ms step_avg:60.11ms
step:979/2225 train_time:58845ms step_avg:60.11ms
step:980/2225 train_time:58905ms step_avg:60.11ms
step:981/2225 train_time:58966ms step_avg:60.11ms
step:982/2225 train_time:59026ms step_avg:60.11ms
step:983/2225 train_time:59088ms step_avg:60.11ms
step:984/2225 train_time:59149ms step_avg:60.11ms
step:985/2225 train_time:59211ms step_avg:60.11ms
step:986/2225 train_time:59271ms step_avg:60.11ms
step:987/2225 train_time:59332ms step_avg:60.11ms
step:988/2225 train_time:59392ms step_avg:60.11ms
step:989/2225 train_time:59454ms step_avg:60.12ms
step:990/2225 train_time:59514ms step_avg:60.12ms
step:991/2225 train_time:59575ms step_avg:60.12ms
step:992/2225 train_time:59634ms step_avg:60.12ms
step:993/2225 train_time:59696ms step_avg:60.12ms
step:994/2225 train_time:59756ms step_avg:60.12ms
step:995/2225 train_time:59819ms step_avg:60.12ms
step:996/2225 train_time:59878ms step_avg:60.12ms
step:997/2225 train_time:59938ms step_avg:60.12ms
step:998/2225 train_time:59998ms step_avg:60.12ms
step:999/2225 train_time:60059ms step_avg:60.12ms
step:1000/2225 train_time:60119ms step_avg:60.12ms
step:1000/2225 val_loss:3.5911 train_time:60180ms step_avg:60.18ms
step:1001/2225 train_time:60203ms step_avg:60.14ms
step:1002/2225 train_time:60242ms step_avg:60.12ms
step:1003/2225 train_time:60308ms step_avg:60.13ms
step:1004/2225 train_time:60371ms step_avg:60.13ms
step:1005/2225 train_time:60433ms step_avg:60.13ms
step:1006/2225 train_time:60493ms step_avg:60.13ms
step:1007/2225 train_time:60555ms step_avg:60.13ms
step:1008/2225 train_time:60614ms step_avg:60.13ms
step:1009/2225 train_time:60676ms step_avg:60.13ms
step:1010/2225 train_time:60735ms step_avg:60.13ms
step:1011/2225 train_time:60796ms step_avg:60.13ms
step:1012/2225 train_time:60855ms step_avg:60.13ms
step:1013/2225 train_time:60916ms step_avg:60.13ms
step:1014/2225 train_time:60975ms step_avg:60.13ms
step:1015/2225 train_time:61035ms step_avg:60.13ms
step:1016/2225 train_time:61095ms step_avg:60.13ms
step:1017/2225 train_time:61158ms step_avg:60.14ms
step:1018/2225 train_time:61220ms step_avg:60.14ms
step:1019/2225 train_time:61285ms step_avg:60.14ms
step:1020/2225 train_time:61345ms step_avg:60.14ms
step:1021/2225 train_time:61407ms step_avg:60.14ms
step:1022/2225 train_time:61467ms step_avg:60.14ms
step:1023/2225 train_time:61528ms step_avg:60.14ms
step:1024/2225 train_time:61588ms step_avg:60.14ms
step:1025/2225 train_time:61649ms step_avg:60.14ms
step:1026/2225 train_time:61708ms step_avg:60.14ms
step:1027/2225 train_time:61769ms step_avg:60.15ms
step:1028/2225 train_time:61828ms step_avg:60.14ms
step:1029/2225 train_time:61889ms step_avg:60.14ms
step:1030/2225 train_time:61948ms step_avg:60.14ms
step:1031/2225 train_time:62009ms step_avg:60.14ms
step:1032/2225 train_time:62068ms step_avg:60.14ms
step:1033/2225 train_time:62131ms step_avg:60.15ms
step:1034/2225 train_time:62190ms step_avg:60.15ms
step:1035/2225 train_time:62253ms step_avg:60.15ms
step:1036/2225 train_time:62314ms step_avg:60.15ms
step:1037/2225 train_time:62376ms step_avg:60.15ms
step:1038/2225 train_time:62438ms step_avg:60.15ms
step:1039/2225 train_time:62500ms step_avg:60.15ms
step:1040/2225 train_time:62560ms step_avg:60.15ms
step:1041/2225 train_time:62622ms step_avg:60.16ms
step:1042/2225 train_time:62682ms step_avg:60.16ms
step:1043/2225 train_time:62743ms step_avg:60.16ms
step:1044/2225 train_time:62803ms step_avg:60.16ms
step:1045/2225 train_time:62864ms step_avg:60.16ms
step:1046/2225 train_time:62923ms step_avg:60.16ms
step:1047/2225 train_time:62984ms step_avg:60.16ms
step:1048/2225 train_time:63044ms step_avg:60.16ms
step:1049/2225 train_time:63104ms step_avg:60.16ms
step:1050/2225 train_time:63164ms step_avg:60.16ms
step:1051/2225 train_time:63226ms step_avg:60.16ms
step:1052/2225 train_time:63286ms step_avg:60.16ms
step:1053/2225 train_time:63348ms step_avg:60.16ms
step:1054/2225 train_time:63408ms step_avg:60.16ms
step:1055/2225 train_time:63470ms step_avg:60.16ms
step:1056/2225 train_time:63530ms step_avg:60.16ms
step:1057/2225 train_time:63591ms step_avg:60.16ms
step:1058/2225 train_time:63651ms step_avg:60.16ms
step:1059/2225 train_time:63712ms step_avg:60.16ms
step:1060/2225 train_time:63771ms step_avg:60.16ms
step:1061/2225 train_time:63832ms step_avg:60.16ms
step:1062/2225 train_time:63892ms step_avg:60.16ms
step:1063/2225 train_time:63952ms step_avg:60.16ms
step:1064/2225 train_time:64012ms step_avg:60.16ms
step:1065/2225 train_time:64074ms step_avg:60.16ms
step:1066/2225 train_time:64134ms step_avg:60.16ms
step:1067/2225 train_time:64196ms step_avg:60.17ms
step:1068/2225 train_time:64257ms step_avg:60.17ms
step:1069/2225 train_time:64319ms step_avg:60.17ms
step:1070/2225 train_time:64380ms step_avg:60.17ms
step:1071/2225 train_time:64442ms step_avg:60.17ms
step:1072/2225 train_time:64502ms step_avg:60.17ms
step:1073/2225 train_time:64563ms step_avg:60.17ms
step:1074/2225 train_time:64623ms step_avg:60.17ms
step:1075/2225 train_time:64685ms step_avg:60.17ms
step:1076/2225 train_time:64745ms step_avg:60.17ms
step:1077/2225 train_time:64806ms step_avg:60.17ms
step:1078/2225 train_time:64865ms step_avg:60.17ms
step:1079/2225 train_time:64926ms step_avg:60.17ms
step:1080/2225 train_time:64985ms step_avg:60.17ms
step:1081/2225 train_time:65046ms step_avg:60.17ms
step:1082/2225 train_time:65106ms step_avg:60.17ms
step:1083/2225 train_time:65167ms step_avg:60.17ms
step:1084/2225 train_time:65227ms step_avg:60.17ms
step:1085/2225 train_time:65288ms step_avg:60.17ms
step:1086/2225 train_time:65349ms step_avg:60.17ms
step:1087/2225 train_time:65410ms step_avg:60.17ms
step:1088/2225 train_time:65469ms step_avg:60.17ms
step:1089/2225 train_time:65530ms step_avg:60.17ms
step:1090/2225 train_time:65591ms step_avg:60.17ms
step:1091/2225 train_time:65652ms step_avg:60.18ms
step:1092/2225 train_time:65711ms step_avg:60.18ms
step:1093/2225 train_time:65773ms step_avg:60.18ms
step:1094/2225 train_time:65833ms step_avg:60.18ms
step:1095/2225 train_time:65895ms step_avg:60.18ms
step:1096/2225 train_time:65954ms step_avg:60.18ms
step:1097/2225 train_time:66016ms step_avg:60.18ms
step:1098/2225 train_time:66076ms step_avg:60.18ms
step:1099/2225 train_time:66138ms step_avg:60.18ms
step:1100/2225 train_time:66198ms step_avg:60.18ms
step:1101/2225 train_time:66258ms step_avg:60.18ms
step:1102/2225 train_time:66318ms step_avg:60.18ms
step:1103/2225 train_time:66380ms step_avg:60.18ms
step:1104/2225 train_time:66441ms step_avg:60.18ms
step:1105/2225 train_time:66502ms step_avg:60.18ms
step:1106/2225 train_time:66561ms step_avg:60.18ms
step:1107/2225 train_time:66623ms step_avg:60.18ms
step:1108/2225 train_time:66683ms step_avg:60.18ms
step:1109/2225 train_time:66746ms step_avg:60.19ms
step:1110/2225 train_time:66807ms step_avg:60.19ms
step:1111/2225 train_time:66867ms step_avg:60.19ms
step:1112/2225 train_time:66927ms step_avg:60.19ms
step:1113/2225 train_time:66988ms step_avg:60.19ms
step:1114/2225 train_time:67047ms step_avg:60.19ms
step:1115/2225 train_time:67108ms step_avg:60.19ms
step:1116/2225 train_time:67168ms step_avg:60.19ms
step:1117/2225 train_time:67229ms step_avg:60.19ms
step:1118/2225 train_time:67289ms step_avg:60.19ms
step:1119/2225 train_time:67350ms step_avg:60.19ms
step:1120/2225 train_time:67409ms step_avg:60.19ms
step:1121/2225 train_time:67471ms step_avg:60.19ms
step:1122/2225 train_time:67530ms step_avg:60.19ms
step:1123/2225 train_time:67592ms step_avg:60.19ms
step:1124/2225 train_time:67652ms step_avg:60.19ms
step:1125/2225 train_time:67713ms step_avg:60.19ms
step:1126/2225 train_time:67773ms step_avg:60.19ms
step:1127/2225 train_time:67835ms step_avg:60.19ms
step:1128/2225 train_time:67895ms step_avg:60.19ms
step:1129/2225 train_time:67956ms step_avg:60.19ms
step:1130/2225 train_time:68016ms step_avg:60.19ms
step:1131/2225 train_time:68077ms step_avg:60.19ms
step:1132/2225 train_time:68138ms step_avg:60.19ms
step:1133/2225 train_time:68200ms step_avg:60.19ms
step:1134/2225 train_time:68260ms step_avg:60.19ms
step:1135/2225 train_time:68322ms step_avg:60.20ms
step:1136/2225 train_time:68382ms step_avg:60.20ms
step:1137/2225 train_time:68443ms step_avg:60.20ms
step:1138/2225 train_time:68503ms step_avg:60.20ms
step:1139/2225 train_time:68564ms step_avg:60.20ms
step:1140/2225 train_time:68625ms step_avg:60.20ms
step:1141/2225 train_time:68687ms step_avg:60.20ms
step:1142/2225 train_time:68747ms step_avg:60.20ms
step:1143/2225 train_time:68808ms step_avg:60.20ms
step:1144/2225 train_time:68867ms step_avg:60.20ms
step:1145/2225 train_time:68928ms step_avg:60.20ms
step:1146/2225 train_time:68988ms step_avg:60.20ms
step:1147/2225 train_time:69049ms step_avg:60.20ms
step:1148/2225 train_time:69108ms step_avg:60.20ms
step:1149/2225 train_time:69170ms step_avg:60.20ms
step:1150/2225 train_time:69229ms step_avg:60.20ms
step:1151/2225 train_time:69290ms step_avg:60.20ms
step:1152/2225 train_time:69350ms step_avg:60.20ms
step:1153/2225 train_time:69411ms step_avg:60.20ms
step:1154/2225 train_time:69470ms step_avg:60.20ms
step:1155/2225 train_time:69531ms step_avg:60.20ms
step:1156/2225 train_time:69592ms step_avg:60.20ms
step:1157/2225 train_time:69654ms step_avg:60.20ms
step:1158/2225 train_time:69714ms step_avg:60.20ms
step:1159/2225 train_time:69776ms step_avg:60.20ms
step:1160/2225 train_time:69836ms step_avg:60.20ms
step:1161/2225 train_time:69897ms step_avg:60.20ms
step:1162/2225 train_time:69957ms step_avg:60.20ms
step:1163/2225 train_time:70019ms step_avg:60.21ms
step:1164/2225 train_time:70079ms step_avg:60.21ms
step:1165/2225 train_time:70141ms step_avg:60.21ms
step:1166/2225 train_time:70200ms step_avg:60.21ms
step:1167/2225 train_time:70262ms step_avg:60.21ms
step:1168/2225 train_time:70322ms step_avg:60.21ms
step:1169/2225 train_time:70383ms step_avg:60.21ms
step:1170/2225 train_time:70443ms step_avg:60.21ms
step:1171/2225 train_time:70505ms step_avg:60.21ms
step:1172/2225 train_time:70565ms step_avg:60.21ms
step:1173/2225 train_time:70626ms step_avg:60.21ms
step:1174/2225 train_time:70686ms step_avg:60.21ms
step:1175/2225 train_time:70747ms step_avg:60.21ms
step:1176/2225 train_time:70807ms step_avg:60.21ms
step:1177/2225 train_time:70867ms step_avg:60.21ms
step:1178/2225 train_time:70927ms step_avg:60.21ms
step:1179/2225 train_time:70988ms step_avg:60.21ms
step:1180/2225 train_time:71048ms step_avg:60.21ms
step:1181/2225 train_time:71109ms step_avg:60.21ms
step:1182/2225 train_time:71168ms step_avg:60.21ms
step:1183/2225 train_time:71229ms step_avg:60.21ms
step:1184/2225 train_time:71288ms step_avg:60.21ms
step:1185/2225 train_time:71349ms step_avg:60.21ms
step:1186/2225 train_time:71409ms step_avg:60.21ms
step:1187/2225 train_time:71470ms step_avg:60.21ms
step:1188/2225 train_time:71529ms step_avg:60.21ms
step:1189/2225 train_time:71591ms step_avg:60.21ms
step:1190/2225 train_time:71651ms step_avg:60.21ms
step:1191/2225 train_time:71712ms step_avg:60.21ms
step:1192/2225 train_time:71772ms step_avg:60.21ms
step:1193/2225 train_time:71834ms step_avg:60.21ms
step:1194/2225 train_time:71894ms step_avg:60.21ms
step:1195/2225 train_time:71955ms step_avg:60.21ms
step:1196/2225 train_time:72016ms step_avg:60.21ms
step:1197/2225 train_time:72078ms step_avg:60.22ms
step:1198/2225 train_time:72138ms step_avg:60.22ms
step:1199/2225 train_time:72200ms step_avg:60.22ms
step:1200/2225 train_time:72260ms step_avg:60.22ms
step:1201/2225 train_time:72322ms step_avg:60.22ms
step:1202/2225 train_time:72382ms step_avg:60.22ms
step:1203/2225 train_time:72443ms step_avg:60.22ms
step:1204/2225 train_time:72503ms step_avg:60.22ms
step:1205/2225 train_time:72564ms step_avg:60.22ms
step:1206/2225 train_time:72624ms step_avg:60.22ms
step:1207/2225 train_time:72685ms step_avg:60.22ms
step:1208/2225 train_time:72746ms step_avg:60.22ms
step:1209/2225 train_time:72807ms step_avg:60.22ms
step:1210/2225 train_time:72867ms step_avg:60.22ms
step:1211/2225 train_time:72928ms step_avg:60.22ms
step:1212/2225 train_time:72988ms step_avg:60.22ms
step:1213/2225 train_time:73050ms step_avg:60.22ms
step:1214/2225 train_time:73109ms step_avg:60.22ms
step:1215/2225 train_time:73170ms step_avg:60.22ms
step:1216/2225 train_time:73230ms step_avg:60.22ms
step:1217/2225 train_time:73291ms step_avg:60.22ms
step:1218/2225 train_time:73351ms step_avg:60.22ms
step:1219/2225 train_time:73412ms step_avg:60.22ms
step:1220/2225 train_time:73471ms step_avg:60.22ms
step:1221/2225 train_time:73533ms step_avg:60.22ms
step:1222/2225 train_time:73593ms step_avg:60.22ms
step:1223/2225 train_time:73655ms step_avg:60.22ms
step:1224/2225 train_time:73715ms step_avg:60.23ms
step:1225/2225 train_time:73777ms step_avg:60.23ms
step:1226/2225 train_time:73838ms step_avg:60.23ms
step:1227/2225 train_time:73900ms step_avg:60.23ms
step:1228/2225 train_time:73960ms step_avg:60.23ms
step:1229/2225 train_time:74022ms step_avg:60.23ms
step:1230/2225 train_time:74082ms step_avg:60.23ms
step:1231/2225 train_time:74143ms step_avg:60.23ms
step:1232/2225 train_time:74203ms step_avg:60.23ms
step:1233/2225 train_time:74264ms step_avg:60.23ms
step:1234/2225 train_time:74324ms step_avg:60.23ms
step:1235/2225 train_time:74386ms step_avg:60.23ms
step:1236/2225 train_time:74446ms step_avg:60.23ms
step:1237/2225 train_time:74507ms step_avg:60.23ms
step:1238/2225 train_time:74567ms step_avg:60.23ms
step:1239/2225 train_time:74627ms step_avg:60.23ms
step:1240/2225 train_time:74687ms step_avg:60.23ms
step:1241/2225 train_time:74748ms step_avg:60.23ms
step:1242/2225 train_time:74808ms step_avg:60.23ms
step:1243/2225 train_time:74870ms step_avg:60.23ms
step:1244/2225 train_time:74930ms step_avg:60.23ms
step:1245/2225 train_time:74991ms step_avg:60.23ms
step:1246/2225 train_time:75051ms step_avg:60.23ms
step:1247/2225 train_time:75112ms step_avg:60.23ms
step:1248/2225 train_time:75172ms step_avg:60.23ms
step:1249/2225 train_time:75233ms step_avg:60.23ms
step:1250/2225 train_time:75294ms step_avg:60.23ms
step:1250/2225 val_loss:3.5196 train_time:75356ms step_avg:60.28ms
step:1251/2225 train_time:75378ms step_avg:60.25ms
step:1252/2225 train_time:75419ms step_avg:60.24ms
step:1253/2225 train_time:75485ms step_avg:60.24ms
step:1254/2225 train_time:75550ms step_avg:60.25ms
step:1255/2225 train_time:75612ms step_avg:60.25ms
step:1256/2225 train_time:75672ms step_avg:60.25ms
step:1257/2225 train_time:75733ms step_avg:60.25ms
step:1258/2225 train_time:75791ms step_avg:60.25ms
step:1259/2225 train_time:75852ms step_avg:60.25ms
step:1260/2225 train_time:75911ms step_avg:60.25ms
step:1261/2225 train_time:75972ms step_avg:60.25ms
step:1262/2225 train_time:76031ms step_avg:60.25ms
step:1263/2225 train_time:76091ms step_avg:60.25ms
step:1264/2225 train_time:76150ms step_avg:60.25ms
step:1265/2225 train_time:76210ms step_avg:60.25ms
step:1266/2225 train_time:76271ms step_avg:60.25ms
step:1267/2225 train_time:76334ms step_avg:60.25ms
step:1268/2225 train_time:76397ms step_avg:60.25ms
step:1269/2225 train_time:76459ms step_avg:60.25ms
step:1270/2225 train_time:76521ms step_avg:60.25ms
step:1271/2225 train_time:76583ms step_avg:60.25ms
step:1272/2225 train_time:76643ms step_avg:60.25ms
step:1273/2225 train_time:76704ms step_avg:60.25ms
step:1274/2225 train_time:76763ms step_avg:60.25ms
step:1275/2225 train_time:76824ms step_avg:60.25ms
step:1276/2225 train_time:76883ms step_avg:60.25ms
step:1277/2225 train_time:76944ms step_avg:60.25ms
step:1278/2225 train_time:77003ms step_avg:60.25ms
step:1279/2225 train_time:77063ms step_avg:60.25ms
step:1280/2225 train_time:77122ms step_avg:60.25ms
step:1281/2225 train_time:77183ms step_avg:60.25ms
step:1282/2225 train_time:77242ms step_avg:60.25ms
step:1283/2225 train_time:77304ms step_avg:60.25ms
step:1284/2225 train_time:77365ms step_avg:60.25ms
step:1285/2225 train_time:77428ms step_avg:60.26ms
step:1286/2225 train_time:77489ms step_avg:60.26ms
step:1287/2225 train_time:77551ms step_avg:60.26ms
step:1288/2225 train_time:77612ms step_avg:60.26ms
step:1289/2225 train_time:77674ms step_avg:60.26ms
step:1290/2225 train_time:77733ms step_avg:60.26ms
step:1291/2225 train_time:77795ms step_avg:60.26ms
step:1292/2225 train_time:77854ms step_avg:60.26ms
step:1293/2225 train_time:77915ms step_avg:60.26ms
step:1294/2225 train_time:77975ms step_avg:60.26ms
step:1295/2225 train_time:78035ms step_avg:60.26ms
step:1296/2225 train_time:78095ms step_avg:60.26ms
step:1297/2225 train_time:78155ms step_avg:60.26ms
step:1298/2225 train_time:78215ms step_avg:60.26ms
step:1299/2225 train_time:78276ms step_avg:60.26ms
step:1300/2225 train_time:78337ms step_avg:60.26ms
step:1301/2225 train_time:78398ms step_avg:60.26ms
step:1302/2225 train_time:78457ms step_avg:60.26ms
step:1303/2225 train_time:78519ms step_avg:60.26ms
step:1304/2225 train_time:78579ms step_avg:60.26ms
step:1305/2225 train_time:78640ms step_avg:60.26ms
step:1306/2225 train_time:78699ms step_avg:60.26ms
step:1307/2225 train_time:78761ms step_avg:60.26ms
step:1308/2225 train_time:78820ms step_avg:60.26ms
step:1309/2225 train_time:78881ms step_avg:60.26ms
step:1310/2225 train_time:78940ms step_avg:60.26ms
step:1311/2225 train_time:79001ms step_avg:60.26ms
step:1312/2225 train_time:79060ms step_avg:60.26ms
step:1313/2225 train_time:79121ms step_avg:60.26ms
step:1314/2225 train_time:79180ms step_avg:60.26ms
step:1315/2225 train_time:79241ms step_avg:60.26ms
step:1316/2225 train_time:79301ms step_avg:60.26ms
step:1317/2225 train_time:79363ms step_avg:60.26ms
step:1318/2225 train_time:79423ms step_avg:60.26ms
step:1319/2225 train_time:79485ms step_avg:60.26ms
step:1320/2225 train_time:79545ms step_avg:60.26ms
step:1321/2225 train_time:79606ms step_avg:60.26ms
step:1322/2225 train_time:79666ms step_avg:60.26ms
step:1323/2225 train_time:79728ms step_avg:60.26ms
step:1324/2225 train_time:79787ms step_avg:60.26ms
step:1325/2225 train_time:79849ms step_avg:60.26ms
step:1326/2225 train_time:79909ms step_avg:60.26ms
step:1327/2225 train_time:79970ms step_avg:60.26ms
step:1328/2225 train_time:80030ms step_avg:60.26ms
step:1329/2225 train_time:80091ms step_avg:60.26ms
step:1330/2225 train_time:80150ms step_avg:60.26ms
step:1331/2225 train_time:80212ms step_avg:60.26ms
step:1332/2225 train_time:80272ms step_avg:60.26ms
step:1333/2225 train_time:80333ms step_avg:60.26ms
step:1334/2225 train_time:80393ms step_avg:60.26ms
step:1335/2225 train_time:80455ms step_avg:60.27ms
step:1336/2225 train_time:80514ms step_avg:60.27ms
step:1337/2225 train_time:80575ms step_avg:60.27ms
step:1338/2225 train_time:80636ms step_avg:60.27ms
step:1339/2225 train_time:80697ms step_avg:60.27ms
step:1340/2225 train_time:80756ms step_avg:60.27ms
step:1341/2225 train_time:80817ms step_avg:60.27ms
step:1342/2225 train_time:80877ms step_avg:60.27ms
step:1343/2225 train_time:80938ms step_avg:60.27ms
step:1344/2225 train_time:80997ms step_avg:60.27ms
step:1345/2225 train_time:81058ms step_avg:60.27ms
step:1346/2225 train_time:81117ms step_avg:60.27ms
step:1347/2225 train_time:81178ms step_avg:60.27ms
step:1348/2225 train_time:81237ms step_avg:60.26ms
step:1349/2225 train_time:81298ms step_avg:60.27ms
step:1350/2225 train_time:81358ms step_avg:60.27ms
step:1351/2225 train_time:81419ms step_avg:60.27ms
step:1352/2225 train_time:81478ms step_avg:60.26ms
step:1353/2225 train_time:81539ms step_avg:60.27ms
step:1354/2225 train_time:81598ms step_avg:60.26ms
step:1355/2225 train_time:81660ms step_avg:60.27ms
step:1356/2225 train_time:81719ms step_avg:60.26ms
step:1357/2225 train_time:81781ms step_avg:60.27ms
step:1358/2225 train_time:81840ms step_avg:60.26ms
step:1359/2225 train_time:81901ms step_avg:60.27ms
step:1360/2225 train_time:81961ms step_avg:60.27ms
step:1361/2225 train_time:82022ms step_avg:60.27ms
step:1362/2225 train_time:82081ms step_avg:60.27ms
step:1363/2225 train_time:82142ms step_avg:60.27ms
step:1364/2225 train_time:82202ms step_avg:60.27ms
step:1365/2225 train_time:82263ms step_avg:60.27ms
step:1366/2225 train_time:82322ms step_avg:60.27ms
step:1367/2225 train_time:82383ms step_avg:60.27ms
step:1368/2225 train_time:82442ms step_avg:60.26ms
step:1369/2225 train_time:82504ms step_avg:60.27ms
step:1370/2225 train_time:82564ms step_avg:60.27ms
step:1371/2225 train_time:82626ms step_avg:60.27ms
step:1372/2225 train_time:82686ms step_avg:60.27ms
step:1373/2225 train_time:82747ms step_avg:60.27ms
step:1374/2225 train_time:82807ms step_avg:60.27ms
step:1375/2225 train_time:82869ms step_avg:60.27ms
step:1376/2225 train_time:82929ms step_avg:60.27ms
step:1377/2225 train_time:82990ms step_avg:60.27ms
step:1378/2225 train_time:83050ms step_avg:60.27ms
step:1379/2225 train_time:83111ms step_avg:60.27ms
step:1380/2225 train_time:83171ms step_avg:60.27ms
step:1381/2225 train_time:83233ms step_avg:60.27ms
step:1382/2225 train_time:83293ms step_avg:60.27ms
step:1383/2225 train_time:83354ms step_avg:60.27ms
step:1384/2225 train_time:83414ms step_avg:60.27ms
step:1385/2225 train_time:83475ms step_avg:60.27ms
step:1386/2225 train_time:83535ms step_avg:60.27ms
step:1387/2225 train_time:83596ms step_avg:60.27ms
step:1388/2225 train_time:83655ms step_avg:60.27ms
step:1389/2225 train_time:83717ms step_avg:60.27ms
step:1390/2225 train_time:83777ms step_avg:60.27ms
step:1391/2225 train_time:83838ms step_avg:60.27ms
step:1392/2225 train_time:83898ms step_avg:60.27ms
step:1393/2225 train_time:83960ms step_avg:60.27ms
step:1394/2225 train_time:84019ms step_avg:60.27ms
step:1395/2225 train_time:84081ms step_avg:60.27ms
step:1396/2225 train_time:84141ms step_avg:60.27ms
step:1397/2225 train_time:84202ms step_avg:60.27ms
step:1398/2225 train_time:84261ms step_avg:60.27ms
step:1399/2225 train_time:84323ms step_avg:60.27ms
step:1400/2225 train_time:84382ms step_avg:60.27ms
step:1401/2225 train_time:84443ms step_avg:60.27ms
step:1402/2225 train_time:84503ms step_avg:60.27ms
step:1403/2225 train_time:84564ms step_avg:60.27ms
step:1404/2225 train_time:84624ms step_avg:60.27ms
step:1405/2225 train_time:84686ms step_avg:60.27ms
step:1406/2225 train_time:84746ms step_avg:60.27ms
step:1407/2225 train_time:84807ms step_avg:60.28ms
step:1408/2225 train_time:84867ms step_avg:60.27ms
step:1409/2225 train_time:84928ms step_avg:60.28ms
step:1410/2225 train_time:84988ms step_avg:60.28ms
step:1411/2225 train_time:85050ms step_avg:60.28ms
step:1412/2225 train_time:85112ms step_avg:60.28ms
step:1413/2225 train_time:85174ms step_avg:60.28ms
step:1414/2225 train_time:85232ms step_avg:60.28ms
step:1415/2225 train_time:85293ms step_avg:60.28ms
step:1416/2225 train_time:85353ms step_avg:60.28ms
step:1417/2225 train_time:85414ms step_avg:60.28ms
step:1418/2225 train_time:85474ms step_avg:60.28ms
step:1419/2225 train_time:85535ms step_avg:60.28ms
step:1420/2225 train_time:85595ms step_avg:60.28ms
step:1421/2225 train_time:85656ms step_avg:60.28ms
step:1422/2225 train_time:85717ms step_avg:60.28ms
step:1423/2225 train_time:85778ms step_avg:60.28ms
step:1424/2225 train_time:85838ms step_avg:60.28ms
step:1425/2225 train_time:85899ms step_avg:60.28ms
step:1426/2225 train_time:85960ms step_avg:60.28ms
step:1427/2225 train_time:86022ms step_avg:60.28ms
step:1428/2225 train_time:86081ms step_avg:60.28ms
step:1429/2225 train_time:86142ms step_avg:60.28ms
step:1430/2225 train_time:86201ms step_avg:60.28ms
step:1431/2225 train_time:86263ms step_avg:60.28ms
step:1432/2225 train_time:86322ms step_avg:60.28ms
step:1433/2225 train_time:86383ms step_avg:60.28ms
step:1434/2225 train_time:86442ms step_avg:60.28ms
step:1435/2225 train_time:86503ms step_avg:60.28ms
step:1436/2225 train_time:86562ms step_avg:60.28ms
step:1437/2225 train_time:86625ms step_avg:60.28ms
step:1438/2225 train_time:86685ms step_avg:60.28ms
step:1439/2225 train_time:86746ms step_avg:60.28ms
step:1440/2225 train_time:86806ms step_avg:60.28ms
step:1441/2225 train_time:86868ms step_avg:60.28ms
step:1442/2225 train_time:86928ms step_avg:60.28ms
step:1443/2225 train_time:86990ms step_avg:60.28ms
step:1444/2225 train_time:87050ms step_avg:60.28ms
step:1445/2225 train_time:87111ms step_avg:60.28ms
step:1446/2225 train_time:87171ms step_avg:60.28ms
step:1447/2225 train_time:87233ms step_avg:60.29ms
step:1448/2225 train_time:87292ms step_avg:60.28ms
step:1449/2225 train_time:87354ms step_avg:60.29ms
step:1450/2225 train_time:87413ms step_avg:60.29ms
step:1451/2225 train_time:87475ms step_avg:60.29ms
step:1452/2225 train_time:87536ms step_avg:60.29ms
step:1453/2225 train_time:87597ms step_avg:60.29ms
step:1454/2225 train_time:87657ms step_avg:60.29ms
step:1455/2225 train_time:87717ms step_avg:60.29ms
step:1456/2225 train_time:87777ms step_avg:60.29ms
step:1457/2225 train_time:87838ms step_avg:60.29ms
step:1458/2225 train_time:87898ms step_avg:60.29ms
step:1459/2225 train_time:87960ms step_avg:60.29ms
step:1460/2225 train_time:88020ms step_avg:60.29ms
step:1461/2225 train_time:88083ms step_avg:60.29ms
step:1462/2225 train_time:88142ms step_avg:60.29ms
step:1463/2225 train_time:88204ms step_avg:60.29ms
step:1464/2225 train_time:88265ms step_avg:60.29ms
step:1465/2225 train_time:88327ms step_avg:60.29ms
step:1466/2225 train_time:88386ms step_avg:60.29ms
step:1467/2225 train_time:88448ms step_avg:60.29ms
step:1468/2225 train_time:88509ms step_avg:60.29ms
step:1469/2225 train_time:88572ms step_avg:60.29ms
step:1470/2225 train_time:88632ms step_avg:60.29ms
step:1471/2225 train_time:88693ms step_avg:60.29ms
step:1472/2225 train_time:88753ms step_avg:60.29ms
step:1473/2225 train_time:88815ms step_avg:60.30ms
step:1474/2225 train_time:88876ms step_avg:60.30ms
step:1475/2225 train_time:88938ms step_avg:60.30ms
step:1476/2225 train_time:88998ms step_avg:60.30ms
step:1477/2225 train_time:89060ms step_avg:60.30ms
step:1478/2225 train_time:89120ms step_avg:60.30ms
step:1479/2225 train_time:89182ms step_avg:60.30ms
step:1480/2225 train_time:89241ms step_avg:60.30ms
step:1481/2225 train_time:89303ms step_avg:60.30ms
step:1482/2225 train_time:89362ms step_avg:60.30ms
step:1483/2225 train_time:89423ms step_avg:60.30ms
step:1484/2225 train_time:89484ms step_avg:60.30ms
step:1485/2225 train_time:89545ms step_avg:60.30ms
step:1486/2225 train_time:89605ms step_avg:60.30ms
step:1487/2225 train_time:89667ms step_avg:60.30ms
step:1488/2225 train_time:89727ms step_avg:60.30ms
step:1489/2225 train_time:89789ms step_avg:60.30ms
step:1490/2225 train_time:89850ms step_avg:60.30ms
step:1491/2225 train_time:89912ms step_avg:60.30ms
step:1492/2225 train_time:89972ms step_avg:60.30ms
step:1493/2225 train_time:90034ms step_avg:60.30ms
step:1494/2225 train_time:90094ms step_avg:60.30ms
step:1495/2225 train_time:90156ms step_avg:60.30ms
step:1496/2225 train_time:90216ms step_avg:60.31ms
step:1497/2225 train_time:90278ms step_avg:60.31ms
step:1498/2225 train_time:90338ms step_avg:60.31ms
step:1499/2225 train_time:90399ms step_avg:60.31ms
step:1500/2225 train_time:90459ms step_avg:60.31ms
step:1500/2225 val_loss:3.4380 train_time:90522ms step_avg:60.35ms
step:1501/2225 train_time:90544ms step_avg:60.32ms
step:1502/2225 train_time:90584ms step_avg:60.31ms
step:1503/2225 train_time:90644ms step_avg:60.31ms
step:1504/2225 train_time:90704ms step_avg:60.31ms
step:1505/2225 train_time:90767ms step_avg:60.31ms
step:1506/2225 train_time:90829ms step_avg:60.31ms
step:1507/2225 train_time:90890ms step_avg:60.31ms
step:1508/2225 train_time:90949ms step_avg:60.31ms
step:1509/2225 train_time:91010ms step_avg:60.31ms
step:1510/2225 train_time:91069ms step_avg:60.31ms
step:1511/2225 train_time:91130ms step_avg:60.31ms
step:1512/2225 train_time:91189ms step_avg:60.31ms
step:1513/2225 train_time:91250ms step_avg:60.31ms
step:1514/2225 train_time:91310ms step_avg:60.31ms
step:1515/2225 train_time:91371ms step_avg:60.31ms
step:1516/2225 train_time:91437ms step_avg:60.31ms
step:1517/2225 train_time:91504ms step_avg:60.32ms
step:1518/2225 train_time:91567ms step_avg:60.32ms
step:1519/2225 train_time:91629ms step_avg:60.32ms
step:1520/2225 train_time:91690ms step_avg:60.32ms
step:1521/2225 train_time:91752ms step_avg:60.32ms
step:1522/2225 train_time:91811ms step_avg:60.32ms
step:1523/2225 train_time:91872ms step_avg:60.32ms
step:1524/2225 train_time:91932ms step_avg:60.32ms
step:1525/2225 train_time:91993ms step_avg:60.32ms
step:1526/2225 train_time:92052ms step_avg:60.32ms
step:1527/2225 train_time:92114ms step_avg:60.32ms
step:1528/2225 train_time:92173ms step_avg:60.32ms
step:1529/2225 train_time:92234ms step_avg:60.32ms
step:1530/2225 train_time:92294ms step_avg:60.32ms
step:1531/2225 train_time:92356ms step_avg:60.32ms
step:1532/2225 train_time:92417ms step_avg:60.32ms
step:1533/2225 train_time:92480ms step_avg:60.33ms
step:1534/2225 train_time:92540ms step_avg:60.33ms
step:1535/2225 train_time:92603ms step_avg:60.33ms
step:1536/2225 train_time:92663ms step_avg:60.33ms
step:1537/2225 train_time:92725ms step_avg:60.33ms
step:1538/2225 train_time:92785ms step_avg:60.33ms
step:1539/2225 train_time:92847ms step_avg:60.33ms
step:1540/2225 train_time:92906ms step_avg:60.33ms
step:1541/2225 train_time:92968ms step_avg:60.33ms
step:1542/2225 train_time:93028ms step_avg:60.33ms
step:1543/2225 train_time:93089ms step_avg:60.33ms
step:1544/2225 train_time:93149ms step_avg:60.33ms
step:1545/2225 train_time:93210ms step_avg:60.33ms
step:1546/2225 train_time:93270ms step_avg:60.33ms
step:1547/2225 train_time:93332ms step_avg:60.33ms
step:1548/2225 train_time:93393ms step_avg:60.33ms
step:1549/2225 train_time:93456ms step_avg:60.33ms
step:1550/2225 train_time:93517ms step_avg:60.33ms
step:1551/2225 train_time:93579ms step_avg:60.33ms
step:1552/2225 train_time:93640ms step_avg:60.33ms
step:1553/2225 train_time:93702ms step_avg:60.34ms
step:1554/2225 train_time:93761ms step_avg:60.34ms
step:1555/2225 train_time:93822ms step_avg:60.34ms
step:1556/2225 train_time:93882ms step_avg:60.34ms
step:1557/2225 train_time:93943ms step_avg:60.34ms
step:1558/2225 train_time:94003ms step_avg:60.34ms
step:1559/2225 train_time:94064ms step_avg:60.34ms
step:1560/2225 train_time:94124ms step_avg:60.34ms
step:1561/2225 train_time:94186ms step_avg:60.34ms
step:1562/2225 train_time:94246ms step_avg:60.34ms
step:1563/2225 train_time:94308ms step_avg:60.34ms
step:1564/2225 train_time:94369ms step_avg:60.34ms
step:1565/2225 train_time:94432ms step_avg:60.34ms
step:1566/2225 train_time:94493ms step_avg:60.34ms
step:1567/2225 train_time:94555ms step_avg:60.34ms
step:1568/2225 train_time:94616ms step_avg:60.34ms
step:1569/2225 train_time:94677ms step_avg:60.34ms
step:1570/2225 train_time:94737ms step_avg:60.34ms
step:1571/2225 train_time:94799ms step_avg:60.34ms
step:1572/2225 train_time:94859ms step_avg:60.34ms
step:1573/2225 train_time:94920ms step_avg:60.34ms
step:1574/2225 train_time:94980ms step_avg:60.34ms
step:1575/2225 train_time:95042ms step_avg:60.34ms
step:1576/2225 train_time:95102ms step_avg:60.34ms
step:1577/2225 train_time:95163ms step_avg:60.34ms
step:1578/2225 train_time:95223ms step_avg:60.34ms
step:1579/2225 train_time:95284ms step_avg:60.34ms
step:1580/2225 train_time:95345ms step_avg:60.35ms
step:1581/2225 train_time:95408ms step_avg:60.35ms
step:1582/2225 train_time:95468ms step_avg:60.35ms
step:1583/2225 train_time:95531ms step_avg:60.35ms
step:1584/2225 train_time:95592ms step_avg:60.35ms
step:1585/2225 train_time:95654ms step_avg:60.35ms
step:1586/2225 train_time:95714ms step_avg:60.35ms
step:1587/2225 train_time:95776ms step_avg:60.35ms
step:1588/2225 train_time:95836ms step_avg:60.35ms
step:1589/2225 train_time:95898ms step_avg:60.35ms
step:1590/2225 train_time:95958ms step_avg:60.35ms
step:1591/2225 train_time:96019ms step_avg:60.35ms
step:1592/2225 train_time:96079ms step_avg:60.35ms
step:1593/2225 train_time:96141ms step_avg:60.35ms
step:1594/2225 train_time:96200ms step_avg:60.35ms
step:1595/2225 train_time:96262ms step_avg:60.35ms
step:1596/2225 train_time:96321ms step_avg:60.35ms
step:1597/2225 train_time:96383ms step_avg:60.35ms
step:1598/2225 train_time:96444ms step_avg:60.35ms
step:1599/2225 train_time:96506ms step_avg:60.35ms
step:1600/2225 train_time:96567ms step_avg:60.35ms
step:1601/2225 train_time:96630ms step_avg:60.36ms
step:1602/2225 train_time:96691ms step_avg:60.36ms
step:1603/2225 train_time:96753ms step_avg:60.36ms
step:1604/2225 train_time:96813ms step_avg:60.36ms
step:1605/2225 train_time:96875ms step_avg:60.36ms
step:1606/2225 train_time:96935ms step_avg:60.36ms
step:1607/2225 train_time:96998ms step_avg:60.36ms
step:1608/2225 train_time:97057ms step_avg:60.36ms
step:1609/2225 train_time:97118ms step_avg:60.36ms
step:1610/2225 train_time:97178ms step_avg:60.36ms
step:1611/2225 train_time:97241ms step_avg:60.36ms
step:1612/2225 train_time:97300ms step_avg:60.36ms
step:1613/2225 train_time:97362ms step_avg:60.36ms
step:1614/2225 train_time:97422ms step_avg:60.36ms
step:1615/2225 train_time:97484ms step_avg:60.36ms
step:1616/2225 train_time:97544ms step_avg:60.36ms
step:1617/2225 train_time:97606ms step_avg:60.36ms
step:1618/2225 train_time:97667ms step_avg:60.36ms
step:1619/2225 train_time:97729ms step_avg:60.36ms
step:1620/2225 train_time:97790ms step_avg:60.36ms
step:1621/2225 train_time:97852ms step_avg:60.37ms
step:1622/2225 train_time:97913ms step_avg:60.37ms
step:1623/2225 train_time:97974ms step_avg:60.37ms
step:1624/2225 train_time:98035ms step_avg:60.37ms
step:1625/2225 train_time:98096ms step_avg:60.37ms
step:1626/2225 train_time:98156ms step_avg:60.37ms
step:1627/2225 train_time:98217ms step_avg:60.37ms
step:1628/2225 train_time:98277ms step_avg:60.37ms
step:1629/2225 train_time:98339ms step_avg:60.37ms
step:1630/2225 train_time:98399ms step_avg:60.37ms
step:1631/2225 train_time:98461ms step_avg:60.37ms
step:1632/2225 train_time:98521ms step_avg:60.37ms
step:1633/2225 train_time:98584ms step_avg:60.37ms
step:1634/2225 train_time:98644ms step_avg:60.37ms
step:1635/2225 train_time:98705ms step_avg:60.37ms
step:1636/2225 train_time:98766ms step_avg:60.37ms
step:1637/2225 train_time:98827ms step_avg:60.37ms
step:1638/2225 train_time:98888ms step_avg:60.37ms
step:1639/2225 train_time:98951ms step_avg:60.37ms
step:1640/2225 train_time:99011ms step_avg:60.37ms
step:1641/2225 train_time:99073ms step_avg:60.37ms
step:1642/2225 train_time:99134ms step_avg:60.37ms
step:1643/2225 train_time:99196ms step_avg:60.37ms
step:1644/2225 train_time:99256ms step_avg:60.37ms
step:1645/2225 train_time:99317ms step_avg:60.38ms
step:1646/2225 train_time:99378ms step_avg:60.38ms
step:1647/2225 train_time:99439ms step_avg:60.38ms
step:1648/2225 train_time:99500ms step_avg:60.38ms
step:1649/2225 train_time:99561ms step_avg:60.38ms
step:1650/2225 train_time:99621ms step_avg:60.38ms
step:1651/2225 train_time:99683ms step_avg:60.38ms
step:1652/2225 train_time:99742ms step_avg:60.38ms
step:1653/2225 train_time:99804ms step_avg:60.38ms
step:1654/2225 train_time:99864ms step_avg:60.38ms
step:1655/2225 train_time:99926ms step_avg:60.38ms
step:1656/2225 train_time:99987ms step_avg:60.38ms
step:1657/2225 train_time:100050ms step_avg:60.38ms
step:1658/2225 train_time:100111ms step_avg:60.38ms
step:1659/2225 train_time:100173ms step_avg:60.38ms
step:1660/2225 train_time:100233ms step_avg:60.38ms
step:1661/2225 train_time:100294ms step_avg:60.38ms
step:1662/2225 train_time:100354ms step_avg:60.38ms
step:1663/2225 train_time:100416ms step_avg:60.38ms
step:1664/2225 train_time:100476ms step_avg:60.38ms
step:1665/2225 train_time:100538ms step_avg:60.38ms
step:1666/2225 train_time:100598ms step_avg:60.38ms
step:1667/2225 train_time:100660ms step_avg:60.38ms
step:1668/2225 train_time:100720ms step_avg:60.38ms
step:1669/2225 train_time:100782ms step_avg:60.38ms
step:1670/2225 train_time:100842ms step_avg:60.38ms
step:1671/2225 train_time:100903ms step_avg:60.39ms
step:1672/2225 train_time:100964ms step_avg:60.38ms
step:1673/2225 train_time:101026ms step_avg:60.39ms
step:1674/2225 train_time:101086ms step_avg:60.39ms
step:1675/2225 train_time:101149ms step_avg:60.39ms
step:1676/2225 train_time:101209ms step_avg:60.39ms
step:1677/2225 train_time:101272ms step_avg:60.39ms
step:1678/2225 train_time:101332ms step_avg:60.39ms
step:1679/2225 train_time:101394ms step_avg:60.39ms
step:1680/2225 train_time:101455ms step_avg:60.39ms
step:1681/2225 train_time:101516ms step_avg:60.39ms
step:1682/2225 train_time:101576ms step_avg:60.39ms
step:1683/2225 train_time:101638ms step_avg:60.39ms
step:1684/2225 train_time:101698ms step_avg:60.39ms
step:1685/2225 train_time:101760ms step_avg:60.39ms
step:1686/2225 train_time:101819ms step_avg:60.39ms
step:1687/2225 train_time:101881ms step_avg:60.39ms
step:1688/2225 train_time:101941ms step_avg:60.39ms
step:1689/2225 train_time:102003ms step_avg:60.39ms
step:1690/2225 train_time:102063ms step_avg:60.39ms
step:1691/2225 train_time:102125ms step_avg:60.39ms
step:1692/2225 train_time:102186ms step_avg:60.39ms
step:1693/2225 train_time:102249ms step_avg:60.39ms
step:1694/2225 train_time:102309ms step_avg:60.40ms
step:1695/2225 train_time:102373ms step_avg:60.40ms
step:1696/2225 train_time:102433ms step_avg:60.40ms
step:1697/2225 train_time:102495ms step_avg:60.40ms
step:1698/2225 train_time:102555ms step_avg:60.40ms
step:1699/2225 train_time:102617ms step_avg:60.40ms
step:1700/2225 train_time:102677ms step_avg:60.40ms
step:1701/2225 train_time:102738ms step_avg:60.40ms
step:1702/2225 train_time:102799ms step_avg:60.40ms
step:1703/2225 train_time:102861ms step_avg:60.40ms
step:1704/2225 train_time:102921ms step_avg:60.40ms
step:1705/2225 train_time:102982ms step_avg:60.40ms
step:1706/2225 train_time:103042ms step_avg:60.40ms
step:1707/2225 train_time:103104ms step_avg:60.40ms
step:1708/2225 train_time:103164ms step_avg:60.40ms
step:1709/2225 train_time:103226ms step_avg:60.40ms
step:1710/2225 train_time:103287ms step_avg:60.40ms
step:1711/2225 train_time:103349ms step_avg:60.40ms
step:1712/2225 train_time:103410ms step_avg:60.40ms
step:1713/2225 train_time:103471ms step_avg:60.40ms
step:1714/2225 train_time:103533ms step_avg:60.40ms
step:1715/2225 train_time:103594ms step_avg:60.40ms
step:1716/2225 train_time:103655ms step_avg:60.40ms
step:1717/2225 train_time:103716ms step_avg:60.41ms
step:1718/2225 train_time:103777ms step_avg:60.41ms
step:1719/2225 train_time:103838ms step_avg:60.41ms
step:1720/2225 train_time:103898ms step_avg:60.41ms
step:1721/2225 train_time:103960ms step_avg:60.41ms
step:1722/2225 train_time:104020ms step_avg:60.41ms
step:1723/2225 train_time:104082ms step_avg:60.41ms
step:1724/2225 train_time:104142ms step_avg:60.41ms
step:1725/2225 train_time:104203ms step_avg:60.41ms
step:1726/2225 train_time:104263ms step_avg:60.41ms
step:1727/2225 train_time:104326ms step_avg:60.41ms
step:1728/2225 train_time:104386ms step_avg:60.41ms
step:1729/2225 train_time:104449ms step_avg:60.41ms
step:1730/2225 train_time:104509ms step_avg:60.41ms
step:1731/2225 train_time:104572ms step_avg:60.41ms
step:1732/2225 train_time:104633ms step_avg:60.41ms
step:1733/2225 train_time:104694ms step_avg:60.41ms
step:1734/2225 train_time:104755ms step_avg:60.41ms
step:1735/2225 train_time:104817ms step_avg:60.41ms
step:1736/2225 train_time:104877ms step_avg:60.41ms
step:1737/2225 train_time:104938ms step_avg:60.41ms
step:1738/2225 train_time:104998ms step_avg:60.41ms
step:1739/2225 train_time:105059ms step_avg:60.41ms
step:1740/2225 train_time:105119ms step_avg:60.41ms
step:1741/2225 train_time:105182ms step_avg:60.41ms
step:1742/2225 train_time:105242ms step_avg:60.41ms
step:1743/2225 train_time:105304ms step_avg:60.42ms
step:1744/2225 train_time:105363ms step_avg:60.41ms
step:1745/2225 train_time:105426ms step_avg:60.42ms
step:1746/2225 train_time:105487ms step_avg:60.42ms
step:1747/2225 train_time:105550ms step_avg:60.42ms
step:1748/2225 train_time:105610ms step_avg:60.42ms
step:1749/2225 train_time:105673ms step_avg:60.42ms
step:1750/2225 train_time:105733ms step_avg:60.42ms
step:1750/2225 val_loss:3.3737 train_time:105796ms step_avg:60.45ms
step:1751/2225 train_time:105818ms step_avg:60.43ms
step:1752/2225 train_time:105859ms step_avg:60.42ms
step:1753/2225 train_time:105925ms step_avg:60.43ms
step:1754/2225 train_time:105986ms step_avg:60.43ms
step:1755/2225 train_time:106048ms step_avg:60.43ms
step:1756/2225 train_time:106109ms step_avg:60.43ms
step:1757/2225 train_time:106169ms step_avg:60.43ms
step:1758/2225 train_time:106229ms step_avg:60.43ms
step:1759/2225 train_time:106290ms step_avg:60.43ms
step:1760/2225 train_time:106348ms step_avg:60.43ms
step:1761/2225 train_time:106409ms step_avg:60.43ms
step:1762/2225 train_time:106468ms step_avg:60.42ms
step:1763/2225 train_time:106529ms step_avg:60.42ms
step:1764/2225 train_time:106589ms step_avg:60.42ms
step:1765/2225 train_time:106650ms step_avg:60.42ms
step:1766/2225 train_time:106712ms step_avg:60.43ms
step:1767/2225 train_time:106776ms step_avg:60.43ms
step:1768/2225 train_time:106836ms step_avg:60.43ms
step:1769/2225 train_time:106899ms step_avg:60.43ms
step:1770/2225 train_time:106961ms step_avg:60.43ms
step:1771/2225 train_time:107023ms step_avg:60.43ms
step:1772/2225 train_time:107083ms step_avg:60.43ms
step:1773/2225 train_time:107145ms step_avg:60.43ms
step:1774/2225 train_time:107205ms step_avg:60.43ms
step:1775/2225 train_time:107266ms step_avg:60.43ms
step:1776/2225 train_time:107325ms step_avg:60.43ms
step:1777/2225 train_time:107386ms step_avg:60.43ms
step:1778/2225 train_time:107446ms step_avg:60.43ms
step:1779/2225 train_time:107507ms step_avg:60.43ms
step:1780/2225 train_time:107567ms step_avg:60.43ms
step:1781/2225 train_time:107628ms step_avg:60.43ms
step:1782/2225 train_time:107688ms step_avg:60.43ms
step:1783/2225 train_time:107751ms step_avg:60.43ms
step:1784/2225 train_time:107811ms step_avg:60.43ms
step:1785/2225 train_time:107874ms step_avg:60.43ms
step:1786/2225 train_time:107934ms step_avg:60.43ms
step:1787/2225 train_time:107997ms step_avg:60.43ms
step:1788/2225 train_time:108058ms step_avg:60.43ms
step:1789/2225 train_time:108120ms step_avg:60.44ms
step:1790/2225 train_time:108179ms step_avg:60.44ms
step:1791/2225 train_time:108241ms step_avg:60.44ms
step:1792/2225 train_time:108300ms step_avg:60.44ms
step:1793/2225 train_time:108362ms step_avg:60.44ms
step:1794/2225 train_time:108422ms step_avg:60.44ms
step:1795/2225 train_time:108483ms step_avg:60.44ms
step:1796/2225 train_time:108543ms step_avg:60.44ms
step:1797/2225 train_time:108604ms step_avg:60.44ms
step:1798/2225 train_time:108664ms step_avg:60.44ms
step:1799/2225 train_time:108725ms step_avg:60.44ms
step:1800/2225 train_time:108787ms step_avg:60.44ms
step:1801/2225 train_time:108850ms step_avg:60.44ms
step:1802/2225 train_time:108910ms step_avg:60.44ms
step:1803/2225 train_time:108973ms step_avg:60.44ms
step:1804/2225 train_time:109033ms step_avg:60.44ms
step:1805/2225 train_time:109094ms step_avg:60.44ms
step:1806/2225 train_time:109154ms step_avg:60.44ms
step:1807/2225 train_time:109217ms step_avg:60.44ms
step:1808/2225 train_time:109277ms step_avg:60.44ms
step:1809/2225 train_time:109339ms step_avg:60.44ms
step:1810/2225 train_time:109399ms step_avg:60.44ms
step:1811/2225 train_time:109461ms step_avg:60.44ms
step:1812/2225 train_time:109521ms step_avg:60.44ms
step:1813/2225 train_time:109583ms step_avg:60.44ms
step:1814/2225 train_time:109643ms step_avg:60.44ms
step:1815/2225 train_time:109705ms step_avg:60.44ms
step:1816/2225 train_time:109766ms step_avg:60.44ms
step:1817/2225 train_time:109828ms step_avg:60.44ms
step:1818/2225 train_time:109888ms step_avg:60.44ms
step:1819/2225 train_time:109950ms step_avg:60.45ms
step:1820/2225 train_time:110011ms step_avg:60.45ms
step:1821/2225 train_time:110072ms step_avg:60.45ms
step:1822/2225 train_time:110132ms step_avg:60.45ms
step:1823/2225 train_time:110194ms step_avg:60.45ms
step:1824/2225 train_time:110253ms step_avg:60.45ms
step:1825/2225 train_time:110315ms step_avg:60.45ms
step:1826/2225 train_time:110375ms step_avg:60.45ms
step:1827/2225 train_time:110436ms step_avg:60.45ms
step:1828/2225 train_time:110496ms step_avg:60.45ms
step:1829/2225 train_time:110558ms step_avg:60.45ms
step:1830/2225 train_time:110619ms step_avg:60.45ms
step:1831/2225 train_time:110682ms step_avg:60.45ms
step:1832/2225 train_time:110742ms step_avg:60.45ms
step:1833/2225 train_time:110805ms step_avg:60.45ms
step:1834/2225 train_time:110865ms step_avg:60.45ms
step:1835/2225 train_time:110927ms step_avg:60.45ms
step:1836/2225 train_time:110988ms step_avg:60.45ms
step:1837/2225 train_time:111049ms step_avg:60.45ms
step:1838/2225 train_time:111109ms step_avg:60.45ms
step:1839/2225 train_time:111171ms step_avg:60.45ms
step:1840/2225 train_time:111231ms step_avg:60.45ms
step:1841/2225 train_time:111292ms step_avg:60.45ms
step:1842/2225 train_time:111353ms step_avg:60.45ms
step:1843/2225 train_time:111414ms step_avg:60.45ms
step:1844/2225 train_time:111473ms step_avg:60.45ms
step:1845/2225 train_time:111535ms step_avg:60.45ms
step:1846/2225 train_time:111595ms step_avg:60.45ms
step:1847/2225 train_time:111658ms step_avg:60.45ms
step:1848/2225 train_time:111718ms step_avg:60.45ms
step:1849/2225 train_time:111781ms step_avg:60.45ms
step:1850/2225 train_time:111841ms step_avg:60.45ms
step:1851/2225 train_time:111904ms step_avg:60.46ms
step:1852/2225 train_time:111964ms step_avg:60.46ms
step:1853/2225 train_time:112026ms step_avg:60.46ms
step:1854/2225 train_time:112086ms step_avg:60.46ms
step:1855/2225 train_time:112148ms step_avg:60.46ms
step:1856/2225 train_time:112207ms step_avg:60.46ms
step:1857/2225 train_time:112269ms step_avg:60.46ms
step:1858/2225 train_time:112329ms step_avg:60.46ms
step:1859/2225 train_time:112390ms step_avg:60.46ms
step:1860/2225 train_time:112450ms step_avg:60.46ms
step:1861/2225 train_time:112512ms step_avg:60.46ms
step:1862/2225 train_time:112573ms step_avg:60.46ms
step:1863/2225 train_time:112634ms step_avg:60.46ms
step:1864/2225 train_time:112695ms step_avg:60.46ms
step:1865/2225 train_time:112757ms step_avg:60.46ms
step:1866/2225 train_time:112817ms step_avg:60.46ms
step:1867/2225 train_time:112880ms step_avg:60.46ms
step:1868/2225 train_time:112940ms step_avg:60.46ms
step:1869/2225 train_time:113003ms step_avg:60.46ms
step:1870/2225 train_time:113064ms step_avg:60.46ms
step:1871/2225 train_time:113125ms step_avg:60.46ms
step:1872/2225 train_time:113185ms step_avg:60.46ms
step:1873/2225 train_time:113247ms step_avg:60.46ms
step:1874/2225 train_time:113308ms step_avg:60.46ms
step:1875/2225 train_time:113369ms step_avg:60.46ms
step:1876/2225 train_time:113429ms step_avg:60.46ms
step:1877/2225 train_time:113491ms step_avg:60.46ms
step:1878/2225 train_time:113551ms step_avg:60.46ms
step:1879/2225 train_time:113613ms step_avg:60.46ms
step:1880/2225 train_time:113672ms step_avg:60.46ms
step:1881/2225 train_time:113734ms step_avg:60.46ms
step:1882/2225 train_time:113794ms step_avg:60.46ms
step:1883/2225 train_time:113856ms step_avg:60.47ms
step:1884/2225 train_time:113917ms step_avg:60.47ms
step:1885/2225 train_time:113979ms step_avg:60.47ms
step:1886/2225 train_time:114040ms step_avg:60.47ms
step:1887/2225 train_time:114102ms step_avg:60.47ms
step:1888/2225 train_time:114162ms step_avg:60.47ms
step:1889/2225 train_time:114224ms step_avg:60.47ms
step:1890/2225 train_time:114285ms step_avg:60.47ms
step:1891/2225 train_time:114347ms step_avg:60.47ms
step:1892/2225 train_time:114407ms step_avg:60.47ms
step:1893/2225 train_time:114468ms step_avg:60.47ms
step:1894/2225 train_time:114529ms step_avg:60.47ms
step:1895/2225 train_time:114591ms step_avg:60.47ms
step:1896/2225 train_time:114652ms step_avg:60.47ms
step:1897/2225 train_time:114713ms step_avg:60.47ms
step:1898/2225 train_time:114772ms step_avg:60.47ms
step:1899/2225 train_time:114833ms step_avg:60.47ms
step:1900/2225 train_time:114893ms step_avg:60.47ms
step:1901/2225 train_time:114955ms step_avg:60.47ms
step:1902/2225 train_time:115015ms step_avg:60.47ms
step:1903/2225 train_time:115078ms step_avg:60.47ms
step:1904/2225 train_time:115139ms step_avg:60.47ms
step:1905/2225 train_time:115201ms step_avg:60.47ms
step:1906/2225 train_time:115263ms step_avg:60.47ms
step:1907/2225 train_time:115325ms step_avg:60.47ms
step:1908/2225 train_time:115385ms step_avg:60.47ms
step:1909/2225 train_time:115446ms step_avg:60.47ms
step:1910/2225 train_time:115506ms step_avg:60.47ms
step:1911/2225 train_time:115568ms step_avg:60.48ms
step:1912/2225 train_time:115628ms step_avg:60.47ms
step:1913/2225 train_time:115689ms step_avg:60.48ms
step:1914/2225 train_time:115749ms step_avg:60.47ms
step:1915/2225 train_time:115811ms step_avg:60.48ms
step:1916/2225 train_time:115871ms step_avg:60.48ms
step:1917/2225 train_time:115932ms step_avg:60.48ms
step:1918/2225 train_time:115992ms step_avg:60.48ms
step:1919/2225 train_time:116054ms step_avg:60.48ms
step:1920/2225 train_time:116115ms step_avg:60.48ms
step:1921/2225 train_time:116178ms step_avg:60.48ms
step:1922/2225 train_time:116239ms step_avg:60.48ms
step:1923/2225 train_time:116302ms step_avg:60.48ms
step:1924/2225 train_time:116362ms step_avg:60.48ms
step:1925/2225 train_time:116424ms step_avg:60.48ms
step:1926/2225 train_time:116484ms step_avg:60.48ms
step:1927/2225 train_time:116546ms step_avg:60.48ms
step:1928/2225 train_time:116606ms step_avg:60.48ms
step:1929/2225 train_time:116668ms step_avg:60.48ms
step:1930/2225 train_time:116728ms step_avg:60.48ms
step:1931/2225 train_time:116790ms step_avg:60.48ms
step:1932/2225 train_time:116850ms step_avg:60.48ms
step:1933/2225 train_time:116912ms step_avg:60.48ms
step:1934/2225 train_time:116972ms step_avg:60.48ms
step:1935/2225 train_time:117034ms step_avg:60.48ms
step:1936/2225 train_time:117094ms step_avg:60.48ms
step:1937/2225 train_time:117157ms step_avg:60.48ms
step:1938/2225 train_time:117217ms step_avg:60.48ms
step:1939/2225 train_time:117279ms step_avg:60.48ms
step:1940/2225 train_time:117339ms step_avg:60.48ms
step:1941/2225 train_time:117402ms step_avg:60.49ms
step:1942/2225 train_time:117462ms step_avg:60.49ms
step:1943/2225 train_time:117524ms step_avg:60.49ms
step:1944/2225 train_time:117584ms step_avg:60.49ms
step:1945/2225 train_time:117646ms step_avg:60.49ms
step:1946/2225 train_time:117707ms step_avg:60.49ms
step:1947/2225 train_time:117768ms step_avg:60.49ms
step:1948/2225 train_time:117829ms step_avg:60.49ms
step:1949/2225 train_time:117890ms step_avg:60.49ms
step:1950/2225 train_time:117950ms step_avg:60.49ms
step:1951/2225 train_time:118012ms step_avg:60.49ms
step:1952/2225 train_time:118073ms step_avg:60.49ms
step:1953/2225 train_time:118134ms step_avg:60.49ms
step:1954/2225 train_time:118195ms step_avg:60.49ms
step:1955/2225 train_time:118257ms step_avg:60.49ms
step:1956/2225 train_time:118318ms step_avg:60.49ms
step:1957/2225 train_time:118380ms step_avg:60.49ms
step:1958/2225 train_time:118441ms step_avg:60.49ms
step:1959/2225 train_time:118503ms step_avg:60.49ms
step:1960/2225 train_time:118564ms step_avg:60.49ms
step:1961/2225 train_time:118625ms step_avg:60.49ms
step:1962/2225 train_time:118686ms step_avg:60.49ms
step:1963/2225 train_time:118747ms step_avg:60.49ms
step:1964/2225 train_time:118807ms step_avg:60.49ms
step:1965/2225 train_time:118868ms step_avg:60.49ms
step:1966/2225 train_time:118928ms step_avg:60.49ms
step:1967/2225 train_time:118989ms step_avg:60.49ms
step:1968/2225 train_time:119050ms step_avg:60.49ms
step:1969/2225 train_time:119112ms step_avg:60.49ms
step:1970/2225 train_time:119172ms step_avg:60.49ms
step:1971/2225 train_time:119234ms step_avg:60.49ms
step:1972/2225 train_time:119294ms step_avg:60.49ms
step:1973/2225 train_time:119356ms step_avg:60.49ms
step:1974/2225 train_time:119417ms step_avg:60.50ms
step:1975/2225 train_time:119480ms step_avg:60.50ms
step:1976/2225 train_time:119540ms step_avg:60.50ms
step:1977/2225 train_time:119602ms step_avg:60.50ms
step:1978/2225 train_time:119662ms step_avg:60.50ms
step:1979/2225 train_time:119724ms step_avg:60.50ms
step:1980/2225 train_time:119785ms step_avg:60.50ms
step:1981/2225 train_time:119846ms step_avg:60.50ms
step:1982/2225 train_time:119907ms step_avg:60.50ms
step:1983/2225 train_time:119968ms step_avg:60.50ms
step:1984/2225 train_time:120028ms step_avg:60.50ms
step:1985/2225 train_time:120089ms step_avg:60.50ms
step:1986/2225 train_time:120150ms step_avg:60.50ms
step:1987/2225 train_time:120212ms step_avg:60.50ms
step:1988/2225 train_time:120272ms step_avg:60.50ms
step:1989/2225 train_time:120333ms step_avg:60.50ms
step:1990/2225 train_time:120393ms step_avg:60.50ms
step:1991/2225 train_time:120456ms step_avg:60.50ms
step:1992/2225 train_time:120516ms step_avg:60.50ms
step:1993/2225 train_time:120578ms step_avg:60.50ms
step:1994/2225 train_time:120640ms step_avg:60.50ms
step:1995/2225 train_time:120702ms step_avg:60.50ms
step:1996/2225 train_time:120763ms step_avg:60.50ms
step:1997/2225 train_time:120825ms step_avg:60.50ms
step:1998/2225 train_time:120885ms step_avg:60.50ms
step:1999/2225 train_time:120947ms step_avg:60.50ms
step:2000/2225 train_time:121007ms step_avg:60.50ms
step:2000/2225 val_loss:3.3195 train_time:121069ms step_avg:60.53ms
step:2001/2225 train_time:121091ms step_avg:60.52ms
step:2002/2225 train_time:121131ms step_avg:60.51ms
step:2003/2225 train_time:121195ms step_avg:60.51ms
step:2004/2225 train_time:121257ms step_avg:60.51ms
step:2005/2225 train_time:121319ms step_avg:60.51ms
step:2006/2225 train_time:121379ms step_avg:60.51ms
step:2007/2225 train_time:121440ms step_avg:60.51ms
step:2008/2225 train_time:121500ms step_avg:60.51ms
step:2009/2225 train_time:121560ms step_avg:60.51ms
step:2010/2225 train_time:121620ms step_avg:60.51ms
step:2011/2225 train_time:121682ms step_avg:60.51ms
step:2012/2225 train_time:121742ms step_avg:60.51ms
step:2013/2225 train_time:121803ms step_avg:60.51ms
step:2014/2225 train_time:121864ms step_avg:60.51ms
step:2015/2225 train_time:121925ms step_avg:60.51ms
step:2016/2225 train_time:121985ms step_avg:60.51ms
step:2017/2225 train_time:122048ms step_avg:60.51ms
step:2018/2225 train_time:122110ms step_avg:60.51ms
step:2019/2225 train_time:122173ms step_avg:60.51ms
step:2020/2225 train_time:122233ms step_avg:60.51ms
step:2021/2225 train_time:122295ms step_avg:60.51ms
step:2022/2225 train_time:122355ms step_avg:60.51ms
step:2023/2225 train_time:122417ms step_avg:60.51ms
step:2024/2225 train_time:122476ms step_avg:60.51ms
step:2025/2225 train_time:122537ms step_avg:60.51ms
step:2026/2225 train_time:122597ms step_avg:60.51ms
step:2027/2225 train_time:122658ms step_avg:60.51ms
step:2028/2225 train_time:122718ms step_avg:60.51ms
step:2029/2225 train_time:122779ms step_avg:60.51ms
step:2030/2225 train_time:122840ms step_avg:60.51ms
step:2031/2225 train_time:122903ms step_avg:60.51ms
step:2032/2225 train_time:122963ms step_avg:60.51ms
step:2033/2225 train_time:123026ms step_avg:60.51ms
step:2034/2225 train_time:123087ms step_avg:60.51ms
step:2035/2225 train_time:123150ms step_avg:60.52ms
step:2036/2225 train_time:123210ms step_avg:60.52ms
step:2037/2225 train_time:123272ms step_avg:60.52ms
step:2038/2225 train_time:123332ms step_avg:60.52ms
step:2039/2225 train_time:123394ms step_avg:60.52ms
step:2040/2225 train_time:123454ms step_avg:60.52ms
step:2041/2225 train_time:123515ms step_avg:60.52ms
step:2042/2225 train_time:123575ms step_avg:60.52ms
step:2043/2225 train_time:123636ms step_avg:60.52ms
step:2044/2225 train_time:123696ms step_avg:60.52ms
step:2045/2225 train_time:123758ms step_avg:60.52ms
step:2046/2225 train_time:123818ms step_avg:60.52ms
step:2047/2225 train_time:123881ms step_avg:60.52ms
step:2048/2225 train_time:123942ms step_avg:60.52ms
step:2049/2225 train_time:124005ms step_avg:60.52ms
step:2050/2225 train_time:124066ms step_avg:60.52ms
step:2051/2225 train_time:124128ms step_avg:60.52ms
step:2052/2225 train_time:124188ms step_avg:60.52ms
step:2053/2225 train_time:124250ms step_avg:60.52ms
step:2054/2225 train_time:124310ms step_avg:60.52ms
step:2055/2225 train_time:124371ms step_avg:60.52ms
step:2056/2225 train_time:124432ms step_avg:60.52ms
step:2057/2225 train_time:124493ms step_avg:60.52ms
step:2058/2225 train_time:124553ms step_avg:60.52ms
step:2059/2225 train_time:124614ms step_avg:60.52ms
step:2060/2225 train_time:124674ms step_avg:60.52ms
step:2061/2225 train_time:124735ms step_avg:60.52ms
step:2062/2225 train_time:124794ms step_avg:60.52ms
step:2063/2225 train_time:124857ms step_avg:60.52ms
step:2064/2225 train_time:124918ms step_avg:60.52ms
step:2065/2225 train_time:124981ms step_avg:60.52ms
step:2066/2225 train_time:125042ms step_avg:60.52ms
step:2067/2225 train_time:125105ms step_avg:60.53ms
step:2068/2225 train_time:125165ms step_avg:60.52ms
step:2069/2225 train_time:125227ms step_avg:60.53ms
step:2070/2225 train_time:125288ms step_avg:60.53ms
step:2071/2225 train_time:125349ms step_avg:60.53ms
step:2072/2225 train_time:125410ms step_avg:60.53ms
step:2073/2225 train_time:125471ms step_avg:60.53ms
step:2074/2225 train_time:125531ms step_avg:60.53ms
step:2075/2225 train_time:125592ms step_avg:60.53ms
step:2076/2225 train_time:125652ms step_avg:60.53ms
step:2077/2225 train_time:125713ms step_avg:60.53ms
step:2078/2225 train_time:125773ms step_avg:60.53ms
step:2079/2225 train_time:125835ms step_avg:60.53ms
step:2080/2225 train_time:125896ms step_avg:60.53ms
step:2081/2225 train_time:125958ms step_avg:60.53ms
step:2082/2225 train_time:126020ms step_avg:60.53ms
step:2083/2225 train_time:126083ms step_avg:60.53ms
step:2084/2225 train_time:126144ms step_avg:60.53ms
step:2085/2225 train_time:126207ms step_avg:60.53ms
step:2086/2225 train_time:126267ms step_avg:60.53ms
step:2087/2225 train_time:126328ms step_avg:60.53ms
step:2088/2225 train_time:126389ms step_avg:60.53ms
step:2089/2225 train_time:126450ms step_avg:60.53ms
step:2090/2225 train_time:126509ms step_avg:60.53ms
step:2091/2225 train_time:126571ms step_avg:60.53ms
step:2092/2225 train_time:126631ms step_avg:60.53ms
step:2093/2225 train_time:126692ms step_avg:60.53ms
step:2094/2225 train_time:126753ms step_avg:60.53ms
step:2095/2225 train_time:126814ms step_avg:60.53ms
step:2096/2225 train_time:126874ms step_avg:60.53ms
step:2097/2225 train_time:126936ms step_avg:60.53ms
step:2098/2225 train_time:126996ms step_avg:60.53ms
step:2099/2225 train_time:127057ms step_avg:60.53ms
step:2100/2225 train_time:127118ms step_avg:60.53ms
step:2101/2225 train_time:127181ms step_avg:60.53ms
step:2102/2225 train_time:127243ms step_avg:60.53ms
step:2103/2225 train_time:127305ms step_avg:60.54ms
step:2104/2225 train_time:127365ms step_avg:60.53ms
step:2105/2225 train_time:127427ms step_avg:60.54ms
step:2106/2225 train_time:127487ms step_avg:60.54ms
step:2107/2225 train_time:127548ms step_avg:60.54ms
step:2108/2225 train_time:127608ms step_avg:60.54ms
step:2109/2225 train_time:127669ms step_avg:60.54ms
step:2110/2225 train_time:127730ms step_avg:60.54ms
step:2111/2225 train_time:127792ms step_avg:60.54ms
step:2112/2225 train_time:127852ms step_avg:60.54ms
step:2113/2225 train_time:127914ms step_avg:60.54ms
step:2114/2225 train_time:127974ms step_avg:60.54ms
step:2115/2225 train_time:128036ms step_avg:60.54ms
step:2116/2225 train_time:128096ms step_avg:60.54ms
step:2117/2225 train_time:128158ms step_avg:60.54ms
step:2118/2225 train_time:128219ms step_avg:60.54ms
step:2119/2225 train_time:128281ms step_avg:60.54ms
step:2120/2225 train_time:128341ms step_avg:60.54ms
step:2121/2225 train_time:128404ms step_avg:60.54ms
step:2122/2225 train_time:128464ms step_avg:60.54ms
step:2123/2225 train_time:128526ms step_avg:60.54ms
step:2124/2225 train_time:128586ms step_avg:60.54ms
step:2125/2225 train_time:128648ms step_avg:60.54ms
step:2126/2225 train_time:128708ms step_avg:60.54ms
step:2127/2225 train_time:128770ms step_avg:60.54ms
step:2128/2225 train_time:128830ms step_avg:60.54ms
step:2129/2225 train_time:128892ms step_avg:60.54ms
step:2130/2225 train_time:128953ms step_avg:60.54ms
step:2131/2225 train_time:129014ms step_avg:60.54ms
step:2132/2225 train_time:129074ms step_avg:60.54ms
step:2133/2225 train_time:129136ms step_avg:60.54ms
step:2134/2225 train_time:129196ms step_avg:60.54ms
step:2135/2225 train_time:129258ms step_avg:60.54ms
step:2136/2225 train_time:129319ms step_avg:60.54ms
step:2137/2225 train_time:129382ms step_avg:60.54ms
step:2138/2225 train_time:129442ms step_avg:60.54ms
step:2139/2225 train_time:129505ms step_avg:60.54ms
step:2140/2225 train_time:129565ms step_avg:60.54ms
step:2141/2225 train_time:129626ms step_avg:60.54ms
step:2142/2225 train_time:129687ms step_avg:60.54ms
step:2143/2225 train_time:129749ms step_avg:60.55ms
step:2144/2225 train_time:129809ms step_avg:60.55ms
step:2145/2225 train_time:129871ms step_avg:60.55ms
step:2146/2225 train_time:129931ms step_avg:60.55ms
step:2147/2225 train_time:129992ms step_avg:60.55ms
step:2148/2225 train_time:130052ms step_avg:60.55ms
step:2149/2225 train_time:130114ms step_avg:60.55ms
step:2150/2225 train_time:130174ms step_avg:60.55ms
step:2151/2225 train_time:130236ms step_avg:60.55ms
step:2152/2225 train_time:130295ms step_avg:60.55ms
step:2153/2225 train_time:130357ms step_avg:60.55ms
step:2154/2225 train_time:130417ms step_avg:60.55ms
step:2155/2225 train_time:130479ms step_avg:60.55ms
step:2156/2225 train_time:130540ms step_avg:60.55ms
step:2157/2225 train_time:130603ms step_avg:60.55ms
step:2158/2225 train_time:130663ms step_avg:60.55ms
step:2159/2225 train_time:130726ms step_avg:60.55ms
step:2160/2225 train_time:130785ms step_avg:60.55ms
step:2161/2225 train_time:130847ms step_avg:60.55ms
step:2162/2225 train_time:130908ms step_avg:60.55ms
step:2163/2225 train_time:130969ms step_avg:60.55ms
step:2164/2225 train_time:131029ms step_avg:60.55ms
step:2165/2225 train_time:131091ms step_avg:60.55ms
step:2166/2225 train_time:131152ms step_avg:60.55ms
step:2167/2225 train_time:131214ms step_avg:60.55ms
step:2168/2225 train_time:131274ms step_avg:60.55ms
step:2169/2225 train_time:131336ms step_avg:60.55ms
step:2170/2225 train_time:131396ms step_avg:60.55ms
step:2171/2225 train_time:131458ms step_avg:60.55ms
step:2172/2225 train_time:131519ms step_avg:60.55ms
step:2173/2225 train_time:131581ms step_avg:60.55ms
step:2174/2225 train_time:131642ms step_avg:60.55ms
step:2175/2225 train_time:131704ms step_avg:60.55ms
step:2176/2225 train_time:131765ms step_avg:60.55ms
step:2177/2225 train_time:131827ms step_avg:60.55ms
step:2178/2225 train_time:131887ms step_avg:60.55ms
step:2179/2225 train_time:131949ms step_avg:60.55ms
step:2180/2225 train_time:132009ms step_avg:60.55ms
step:2181/2225 train_time:132070ms step_avg:60.56ms
step:2182/2225 train_time:132131ms step_avg:60.55ms
step:2183/2225 train_time:132192ms step_avg:60.56ms
step:2184/2225 train_time:132252ms step_avg:60.56ms
step:2185/2225 train_time:132314ms step_avg:60.56ms
step:2186/2225 train_time:132374ms step_avg:60.56ms
step:2187/2225 train_time:132437ms step_avg:60.56ms
step:2188/2225 train_time:132498ms step_avg:60.56ms
step:2189/2225 train_time:132561ms step_avg:60.56ms
step:2190/2225 train_time:132621ms step_avg:60.56ms
step:2191/2225 train_time:132683ms step_avg:60.56ms
step:2192/2225 train_time:132745ms step_avg:60.56ms
step:2193/2225 train_time:132807ms step_avg:60.56ms
step:2194/2225 train_time:132867ms step_avg:60.56ms
step:2195/2225 train_time:132929ms step_avg:60.56ms
step:2196/2225 train_time:132989ms step_avg:60.56ms
step:2197/2225 train_time:133051ms step_avg:60.56ms
step:2198/2225 train_time:133110ms step_avg:60.56ms
step:2199/2225 train_time:133172ms step_avg:60.56ms
step:2200/2225 train_time:133233ms step_avg:60.56ms
step:2201/2225 train_time:133295ms step_avg:60.56ms
step:2202/2225 train_time:133355ms step_avg:60.56ms
step:2203/2225 train_time:133416ms step_avg:60.56ms
step:2204/2225 train_time:133478ms step_avg:60.56ms
step:2205/2225 train_time:133540ms step_avg:60.56ms
step:2206/2225 train_time:133601ms step_avg:60.56ms
step:2207/2225 train_time:133664ms step_avg:60.56ms
step:2208/2225 train_time:133725ms step_avg:60.56ms
step:2209/2225 train_time:133788ms step_avg:60.56ms
step:2210/2225 train_time:133848ms step_avg:60.56ms
step:2211/2225 train_time:133910ms step_avg:60.57ms
step:2212/2225 train_time:133970ms step_avg:60.57ms
step:2213/2225 train_time:134032ms step_avg:60.57ms
step:2214/2225 train_time:134092ms step_avg:60.57ms
step:2215/2225 train_time:134154ms step_avg:60.57ms
step:2216/2225 train_time:134214ms step_avg:60.57ms
step:2217/2225 train_time:134276ms step_avg:60.57ms
step:2218/2225 train_time:134336ms step_avg:60.57ms
step:2219/2225 train_time:134397ms step_avg:60.57ms
step:2220/2225 train_time:134457ms step_avg:60.57ms
step:2221/2225 train_time:134520ms step_avg:60.57ms
step:2222/2225 train_time:134580ms step_avg:60.57ms
step:2223/2225 train_time:134643ms step_avg:60.57ms
step:2224/2225 train_time:134704ms step_avg:60.57ms
step:2225/2225 train_time:134765ms step_avg:60.57ms
step:2225/2225 val_loss:3.2777 train_time:134826ms step_avg:60.60ms
peak memory allocated: 29250 MiB reserved: 47336 MiB

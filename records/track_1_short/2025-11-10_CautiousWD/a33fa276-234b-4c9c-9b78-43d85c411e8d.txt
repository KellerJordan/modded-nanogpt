import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Nov 10 21:40:03 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            133W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   41C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   34C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   40C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2245 train_time:119ms step_avg:118.84ms
step:2/2245 train_time:141ms step_avg:70.36ms
step:3/2245 train_time:179ms step_avg:59.52ms
step:4/2245 train_time:235ms step_avg:58.75ms
step:5/2245 train_time:295ms step_avg:58.95ms
step:6/2245 train_time:353ms step_avg:58.87ms
step:7/2245 train_time:414ms step_avg:59.12ms
step:8/2245 train_time:472ms step_avg:59.05ms
step:9/2245 train_time:534ms step_avg:59.28ms
step:10/2245 train_time:593ms step_avg:59.27ms
step:11/2245 train_time:654ms step_avg:59.43ms
step:12/2245 train_time:712ms step_avg:59.36ms
step:13/2245 train_time:774ms step_avg:59.50ms
step:14/2245 train_time:832ms step_avg:59.45ms
step:15/2245 train_time:894ms step_avg:59.58ms
step:16/2245 train_time:953ms step_avg:59.54ms
step:17/2245 train_time:1017ms step_avg:59.83ms
step:18/2245 train_time:1080ms step_avg:59.99ms
step:19/2245 train_time:1144ms step_avg:60.21ms
step:20/2245 train_time:1204ms step_avg:60.20ms
step:21/2245 train_time:1266ms step_avg:60.29ms
step:22/2245 train_time:1326ms step_avg:60.28ms
step:23/2245 train_time:1388ms step_avg:60.35ms
step:24/2245 train_time:1447ms step_avg:60.29ms
step:25/2245 train_time:1509ms step_avg:60.35ms
step:26/2245 train_time:1568ms step_avg:60.29ms
step:27/2245 train_time:1629ms step_avg:60.33ms
step:28/2245 train_time:1689ms step_avg:60.31ms
step:29/2245 train_time:1751ms step_avg:60.37ms
step:30/2245 train_time:1810ms step_avg:60.32ms
step:31/2245 train_time:1871ms step_avg:60.35ms
step:32/2245 train_time:1930ms step_avg:60.30ms
step:33/2245 train_time:1993ms step_avg:60.40ms
step:34/2245 train_time:2053ms step_avg:60.37ms
step:35/2245 train_time:2115ms step_avg:60.42ms
step:36/2245 train_time:2174ms step_avg:60.40ms
step:37/2245 train_time:2237ms step_avg:60.47ms
step:38/2245 train_time:2298ms step_avg:60.47ms
step:39/2245 train_time:2360ms step_avg:60.51ms
step:40/2245 train_time:2419ms step_avg:60.48ms
step:41/2245 train_time:2481ms step_avg:60.52ms
step:42/2245 train_time:2541ms step_avg:60.50ms
step:43/2245 train_time:2603ms step_avg:60.53ms
step:44/2245 train_time:2662ms step_avg:60.50ms
step:45/2245 train_time:2724ms step_avg:60.54ms
step:46/2245 train_time:2785ms step_avg:60.54ms
step:47/2245 train_time:2847ms step_avg:60.58ms
step:48/2245 train_time:2907ms step_avg:60.56ms
step:49/2245 train_time:2969ms step_avg:60.59ms
step:50/2245 train_time:3028ms step_avg:60.57ms
step:51/2245 train_time:3090ms step_avg:60.59ms
step:52/2245 train_time:3149ms step_avg:60.56ms
step:53/2245 train_time:3211ms step_avg:60.59ms
step:54/2245 train_time:3271ms step_avg:60.57ms
step:55/2245 train_time:3332ms step_avg:60.59ms
step:56/2245 train_time:3392ms step_avg:60.56ms
step:57/2245 train_time:3454ms step_avg:60.59ms
step:58/2245 train_time:3513ms step_avg:60.57ms
step:59/2245 train_time:3575ms step_avg:60.60ms
step:60/2245 train_time:3635ms step_avg:60.58ms
step:61/2245 train_time:3697ms step_avg:60.60ms
step:62/2245 train_time:3756ms step_avg:60.59ms
step:63/2245 train_time:3820ms step_avg:60.63ms
step:64/2245 train_time:3879ms step_avg:60.61ms
step:65/2245 train_time:3941ms step_avg:60.63ms
step:66/2245 train_time:4001ms step_avg:60.62ms
step:67/2245 train_time:4063ms step_avg:60.64ms
step:68/2245 train_time:4123ms step_avg:60.63ms
step:69/2245 train_time:4185ms step_avg:60.65ms
step:70/2245 train_time:4244ms step_avg:60.63ms
step:71/2245 train_time:4306ms step_avg:60.65ms
step:72/2245 train_time:4366ms step_avg:60.64ms
step:73/2245 train_time:4428ms step_avg:60.65ms
step:74/2245 train_time:4488ms step_avg:60.64ms
step:75/2245 train_time:4550ms step_avg:60.67ms
step:76/2245 train_time:4609ms step_avg:60.64ms
step:77/2245 train_time:4671ms step_avg:60.66ms
step:78/2245 train_time:4730ms step_avg:60.64ms
step:79/2245 train_time:4793ms step_avg:60.67ms
step:80/2245 train_time:4852ms step_avg:60.66ms
step:81/2245 train_time:4914ms step_avg:60.66ms
step:82/2245 train_time:4973ms step_avg:60.65ms
step:83/2245 train_time:5035ms step_avg:60.66ms
step:84/2245 train_time:5094ms step_avg:60.64ms
step:85/2245 train_time:5156ms step_avg:60.65ms
step:86/2245 train_time:5215ms step_avg:60.64ms
step:87/2245 train_time:5277ms step_avg:60.66ms
step:88/2245 train_time:5337ms step_avg:60.64ms
step:89/2245 train_time:5398ms step_avg:60.66ms
step:90/2245 train_time:5457ms step_avg:60.64ms
step:91/2245 train_time:5519ms step_avg:60.65ms
step:92/2245 train_time:5579ms step_avg:60.64ms
step:93/2245 train_time:5641ms step_avg:60.65ms
step:94/2245 train_time:5700ms step_avg:60.64ms
step:95/2245 train_time:5762ms step_avg:60.65ms
step:96/2245 train_time:5822ms step_avg:60.64ms
step:97/2245 train_time:5884ms step_avg:60.66ms
step:98/2245 train_time:5944ms step_avg:60.65ms
step:99/2245 train_time:6006ms step_avg:60.67ms
step:100/2245 train_time:6065ms step_avg:60.65ms
step:101/2245 train_time:6127ms step_avg:60.66ms
step:102/2245 train_time:6188ms step_avg:60.67ms
step:103/2245 train_time:6248ms step_avg:60.66ms
step:104/2245 train_time:6307ms step_avg:60.65ms
step:105/2245 train_time:6370ms step_avg:60.67ms
step:106/2245 train_time:6429ms step_avg:60.65ms
step:107/2245 train_time:6491ms step_avg:60.66ms
step:108/2245 train_time:6550ms step_avg:60.65ms
step:109/2245 train_time:6612ms step_avg:60.66ms
step:110/2245 train_time:6671ms step_avg:60.65ms
step:111/2245 train_time:6732ms step_avg:60.65ms
step:112/2245 train_time:6792ms step_avg:60.64ms
step:113/2245 train_time:6853ms step_avg:60.65ms
step:114/2245 train_time:6912ms step_avg:60.63ms
step:115/2245 train_time:6973ms step_avg:60.64ms
step:116/2245 train_time:7032ms step_avg:60.62ms
step:117/2245 train_time:7094ms step_avg:60.63ms
step:118/2245 train_time:7153ms step_avg:60.62ms
step:119/2245 train_time:7214ms step_avg:60.62ms
step:120/2245 train_time:7274ms step_avg:60.61ms
step:121/2245 train_time:7335ms step_avg:60.62ms
step:122/2245 train_time:7395ms step_avg:60.62ms
step:123/2245 train_time:7457ms step_avg:60.63ms
step:124/2245 train_time:7516ms step_avg:60.61ms
step:125/2245 train_time:7578ms step_avg:60.62ms
step:126/2245 train_time:7637ms step_avg:60.61ms
step:127/2245 train_time:7698ms step_avg:60.61ms
step:128/2245 train_time:7757ms step_avg:60.60ms
step:129/2245 train_time:7819ms step_avg:60.61ms
step:130/2245 train_time:7877ms step_avg:60.60ms
step:131/2245 train_time:7940ms step_avg:60.61ms
step:132/2245 train_time:7999ms step_avg:60.60ms
step:133/2245 train_time:8061ms step_avg:60.61ms
step:134/2245 train_time:8120ms step_avg:60.60ms
step:135/2245 train_time:8181ms step_avg:60.60ms
step:136/2245 train_time:8241ms step_avg:60.59ms
step:137/2245 train_time:8303ms step_avg:60.60ms
step:138/2245 train_time:8363ms step_avg:60.60ms
step:139/2245 train_time:8426ms step_avg:60.62ms
step:140/2245 train_time:8485ms step_avg:60.61ms
step:141/2245 train_time:8547ms step_avg:60.62ms
step:142/2245 train_time:8606ms step_avg:60.61ms
step:143/2245 train_time:8669ms step_avg:60.62ms
step:144/2245 train_time:8728ms step_avg:60.61ms
step:145/2245 train_time:8789ms step_avg:60.61ms
step:146/2245 train_time:8848ms step_avg:60.60ms
step:147/2245 train_time:8909ms step_avg:60.61ms
step:148/2245 train_time:8968ms step_avg:60.60ms
step:149/2245 train_time:9030ms step_avg:60.60ms
step:150/2245 train_time:9089ms step_avg:60.59ms
step:151/2245 train_time:9151ms step_avg:60.60ms
step:152/2245 train_time:9210ms step_avg:60.59ms
step:153/2245 train_time:9272ms step_avg:60.60ms
step:154/2245 train_time:9330ms step_avg:60.59ms
step:155/2245 train_time:9392ms step_avg:60.59ms
step:156/2245 train_time:9451ms step_avg:60.58ms
step:157/2245 train_time:9512ms step_avg:60.59ms
step:158/2245 train_time:9571ms step_avg:60.58ms
step:159/2245 train_time:9633ms step_avg:60.58ms
step:160/2245 train_time:9692ms step_avg:60.58ms
step:161/2245 train_time:9754ms step_avg:60.58ms
step:162/2245 train_time:9813ms step_avg:60.57ms
step:163/2245 train_time:9875ms step_avg:60.58ms
step:164/2245 train_time:9934ms step_avg:60.57ms
step:165/2245 train_time:9995ms step_avg:60.58ms
step:166/2245 train_time:10054ms step_avg:60.57ms
step:167/2245 train_time:10115ms step_avg:60.57ms
step:168/2245 train_time:10174ms step_avg:60.56ms
step:169/2245 train_time:10235ms step_avg:60.56ms
step:170/2245 train_time:10294ms step_avg:60.55ms
step:171/2245 train_time:10355ms step_avg:60.56ms
step:172/2245 train_time:10414ms step_avg:60.54ms
step:173/2245 train_time:10475ms step_avg:60.55ms
step:174/2245 train_time:10534ms step_avg:60.54ms
step:175/2245 train_time:10596ms step_avg:60.55ms
step:176/2245 train_time:10654ms step_avg:60.54ms
step:177/2245 train_time:10716ms step_avg:60.54ms
step:178/2245 train_time:10776ms step_avg:60.54ms
step:179/2245 train_time:10838ms step_avg:60.55ms
step:180/2245 train_time:10897ms step_avg:60.54ms
step:181/2245 train_time:10959ms step_avg:60.54ms
step:182/2245 train_time:11018ms step_avg:60.54ms
step:183/2245 train_time:11079ms step_avg:60.54ms
step:184/2245 train_time:11138ms step_avg:60.53ms
step:185/2245 train_time:11199ms step_avg:60.54ms
step:186/2245 train_time:11258ms step_avg:60.53ms
step:187/2245 train_time:11319ms step_avg:60.53ms
step:188/2245 train_time:11378ms step_avg:60.52ms
step:189/2245 train_time:11440ms step_avg:60.53ms
step:190/2245 train_time:11499ms step_avg:60.52ms
step:191/2245 train_time:11562ms step_avg:60.53ms
step:192/2245 train_time:11621ms step_avg:60.53ms
step:193/2245 train_time:11683ms step_avg:60.53ms
step:194/2245 train_time:11742ms step_avg:60.53ms
step:195/2245 train_time:11804ms step_avg:60.53ms
step:196/2245 train_time:11863ms step_avg:60.53ms
step:197/2245 train_time:11925ms step_avg:60.53ms
step:198/2245 train_time:11984ms step_avg:60.52ms
step:199/2245 train_time:12045ms step_avg:60.53ms
step:200/2245 train_time:12104ms step_avg:60.52ms
step:201/2245 train_time:12166ms step_avg:60.52ms
step:202/2245 train_time:12224ms step_avg:60.52ms
step:203/2245 train_time:12286ms step_avg:60.52ms
step:204/2245 train_time:12345ms step_avg:60.52ms
step:205/2245 train_time:12407ms step_avg:60.52ms
step:206/2245 train_time:12466ms step_avg:60.52ms
step:207/2245 train_time:12528ms step_avg:60.52ms
step:208/2245 train_time:12587ms step_avg:60.52ms
step:209/2245 train_time:12649ms step_avg:60.52ms
step:210/2245 train_time:12708ms step_avg:60.51ms
step:211/2245 train_time:12769ms step_avg:60.52ms
step:212/2245 train_time:12827ms step_avg:60.51ms
step:213/2245 train_time:12889ms step_avg:60.51ms
step:214/2245 train_time:12948ms step_avg:60.50ms
step:215/2245 train_time:13009ms step_avg:60.51ms
step:216/2245 train_time:13068ms step_avg:60.50ms
step:217/2245 train_time:13129ms step_avg:60.50ms
step:218/2245 train_time:13188ms step_avg:60.49ms
step:219/2245 train_time:13249ms step_avg:60.50ms
step:220/2245 train_time:13308ms step_avg:60.49ms
step:221/2245 train_time:13370ms step_avg:60.50ms
step:222/2245 train_time:13429ms step_avg:60.49ms
step:223/2245 train_time:13491ms step_avg:60.50ms
step:224/2245 train_time:13550ms step_avg:60.49ms
step:225/2245 train_time:13612ms step_avg:60.50ms
step:226/2245 train_time:13671ms step_avg:60.49ms
step:227/2245 train_time:13732ms step_avg:60.49ms
step:228/2245 train_time:13791ms step_avg:60.49ms
step:229/2245 train_time:13852ms step_avg:60.49ms
step:230/2245 train_time:13911ms step_avg:60.48ms
step:231/2245 train_time:13972ms step_avg:60.49ms
step:232/2245 train_time:14031ms step_avg:60.48ms
step:233/2245 train_time:14093ms step_avg:60.48ms
step:234/2245 train_time:14152ms step_avg:60.48ms
step:235/2245 train_time:14214ms step_avg:60.48ms
step:236/2245 train_time:14273ms step_avg:60.48ms
step:237/2245 train_time:14335ms step_avg:60.49ms
step:238/2245 train_time:14394ms step_avg:60.48ms
step:239/2245 train_time:14456ms step_avg:60.49ms
step:240/2245 train_time:14515ms step_avg:60.48ms
step:241/2245 train_time:14576ms step_avg:60.48ms
step:242/2245 train_time:14635ms step_avg:60.48ms
step:243/2245 train_time:14698ms step_avg:60.48ms
step:244/2245 train_time:14756ms step_avg:60.48ms
step:245/2245 train_time:14818ms step_avg:60.48ms
step:246/2245 train_time:14876ms step_avg:60.47ms
step:247/2245 train_time:14937ms step_avg:60.47ms
step:248/2245 train_time:14996ms step_avg:60.47ms
step:249/2245 train_time:15058ms step_avg:60.47ms
step:250/2245 train_time:15117ms step_avg:60.47ms
step:250/2245 val_loss:4.0728 train_time:15180ms step_avg:60.72ms
step:251/2245 train_time:15199ms step_avg:60.55ms
step:252/2245 train_time:15241ms step_avg:60.48ms
step:253/2245 train_time:15309ms step_avg:60.51ms
step:254/2245 train_time:15373ms step_avg:60.52ms
step:255/2245 train_time:15436ms step_avg:60.53ms
step:256/2245 train_time:15495ms step_avg:60.53ms
step:257/2245 train_time:15555ms step_avg:60.53ms
step:258/2245 train_time:15614ms step_avg:60.52ms
step:259/2245 train_time:15675ms step_avg:60.52ms
step:260/2245 train_time:15734ms step_avg:60.52ms
step:261/2245 train_time:15795ms step_avg:60.52ms
step:262/2245 train_time:15853ms step_avg:60.51ms
step:263/2245 train_time:15913ms step_avg:60.51ms
step:264/2245 train_time:15972ms step_avg:60.50ms
step:265/2245 train_time:16032ms step_avg:60.50ms
step:266/2245 train_time:16091ms step_avg:60.49ms
step:267/2245 train_time:16153ms step_avg:60.50ms
step:268/2245 train_time:16213ms step_avg:60.50ms
step:269/2245 train_time:16277ms step_avg:60.51ms
step:270/2245 train_time:16338ms step_avg:60.51ms
step:271/2245 train_time:16401ms step_avg:60.52ms
step:272/2245 train_time:16460ms step_avg:60.52ms
step:273/2245 train_time:16522ms step_avg:60.52ms
step:274/2245 train_time:16581ms step_avg:60.51ms
step:275/2245 train_time:16642ms step_avg:60.52ms
step:276/2245 train_time:16701ms step_avg:60.51ms
step:277/2245 train_time:16762ms step_avg:60.51ms
step:278/2245 train_time:16821ms step_avg:60.51ms
step:279/2245 train_time:16882ms step_avg:60.51ms
step:280/2245 train_time:16941ms step_avg:60.50ms
step:281/2245 train_time:17002ms step_avg:60.51ms
step:282/2245 train_time:17061ms step_avg:60.50ms
step:283/2245 train_time:17122ms step_avg:60.50ms
step:284/2245 train_time:17182ms step_avg:60.50ms
step:285/2245 train_time:17243ms step_avg:60.50ms
step:286/2245 train_time:17303ms step_avg:60.50ms
step:287/2245 train_time:17365ms step_avg:60.51ms
step:288/2245 train_time:17425ms step_avg:60.50ms
step:289/2245 train_time:17487ms step_avg:60.51ms
step:290/2245 train_time:17547ms step_avg:60.51ms
step:291/2245 train_time:17608ms step_avg:60.51ms
step:292/2245 train_time:17667ms step_avg:60.50ms
step:293/2245 train_time:17728ms step_avg:60.51ms
step:294/2245 train_time:17787ms step_avg:60.50ms
step:295/2245 train_time:17848ms step_avg:60.50ms
step:296/2245 train_time:17906ms step_avg:60.49ms
step:297/2245 train_time:17968ms step_avg:60.50ms
step:298/2245 train_time:18028ms step_avg:60.50ms
step:299/2245 train_time:18089ms step_avg:60.50ms
step:300/2245 train_time:18147ms step_avg:60.49ms
step:301/2245 train_time:18209ms step_avg:60.50ms
step:302/2245 train_time:18270ms step_avg:60.50ms
step:303/2245 train_time:18332ms step_avg:60.50ms
step:304/2245 train_time:18392ms step_avg:60.50ms
step:305/2245 train_time:18454ms step_avg:60.50ms
step:306/2245 train_time:18513ms step_avg:60.50ms
step:307/2245 train_time:18575ms step_avg:60.50ms
step:308/2245 train_time:18634ms step_avg:60.50ms
step:309/2245 train_time:18695ms step_avg:60.50ms
step:310/2245 train_time:18754ms step_avg:60.50ms
step:311/2245 train_time:18815ms step_avg:60.50ms
step:312/2245 train_time:18873ms step_avg:60.49ms
step:313/2245 train_time:18934ms step_avg:60.49ms
step:314/2245 train_time:18993ms step_avg:60.49ms
step:315/2245 train_time:19054ms step_avg:60.49ms
step:316/2245 train_time:19112ms step_avg:60.48ms
step:317/2245 train_time:19174ms step_avg:60.48ms
step:318/2245 train_time:19233ms step_avg:60.48ms
step:319/2245 train_time:19295ms step_avg:60.49ms
step:320/2245 train_time:19354ms step_avg:60.48ms
step:321/2245 train_time:19415ms step_avg:60.48ms
step:322/2245 train_time:19474ms step_avg:60.48ms
step:323/2245 train_time:19536ms step_avg:60.48ms
step:324/2245 train_time:19596ms step_avg:60.48ms
step:325/2245 train_time:19657ms step_avg:60.48ms
step:326/2245 train_time:19715ms step_avg:60.48ms
step:327/2245 train_time:19777ms step_avg:60.48ms
step:328/2245 train_time:19835ms step_avg:60.47ms
step:329/2245 train_time:19897ms step_avg:60.48ms
step:330/2245 train_time:19956ms step_avg:60.47ms
step:331/2245 train_time:20017ms step_avg:60.48ms
step:332/2245 train_time:20076ms step_avg:60.47ms
step:333/2245 train_time:20137ms step_avg:60.47ms
step:334/2245 train_time:20196ms step_avg:60.47ms
step:335/2245 train_time:20258ms step_avg:60.47ms
step:336/2245 train_time:20317ms step_avg:60.47ms
step:337/2245 train_time:20378ms step_avg:60.47ms
step:338/2245 train_time:20437ms step_avg:60.47ms
step:339/2245 train_time:20499ms step_avg:60.47ms
step:340/2245 train_time:20558ms step_avg:60.46ms
step:341/2245 train_time:20619ms step_avg:60.47ms
step:342/2245 train_time:20678ms step_avg:60.46ms
step:343/2245 train_time:20739ms step_avg:60.46ms
step:344/2245 train_time:20798ms step_avg:60.46ms
step:345/2245 train_time:20860ms step_avg:60.46ms
step:346/2245 train_time:20918ms step_avg:60.46ms
step:347/2245 train_time:20980ms step_avg:60.46ms
step:348/2245 train_time:21039ms step_avg:60.46ms
step:349/2245 train_time:21101ms step_avg:60.46ms
step:350/2245 train_time:21159ms step_avg:60.45ms
step:351/2245 train_time:21220ms step_avg:60.46ms
step:352/2245 train_time:21279ms step_avg:60.45ms
step:353/2245 train_time:21341ms step_avg:60.46ms
step:354/2245 train_time:21400ms step_avg:60.45ms
step:355/2245 train_time:21462ms step_avg:60.45ms
step:356/2245 train_time:21521ms step_avg:60.45ms
step:357/2245 train_time:21582ms step_avg:60.45ms
step:358/2245 train_time:21641ms step_avg:60.45ms
step:359/2245 train_time:21703ms step_avg:60.45ms
step:360/2245 train_time:21761ms step_avg:60.45ms
step:361/2245 train_time:21823ms step_avg:60.45ms
step:362/2245 train_time:21881ms step_avg:60.45ms
step:363/2245 train_time:21942ms step_avg:60.45ms
step:364/2245 train_time:22001ms step_avg:60.44ms
step:365/2245 train_time:22062ms step_avg:60.44ms
step:366/2245 train_time:22121ms step_avg:60.44ms
step:367/2245 train_time:22183ms step_avg:60.44ms
step:368/2245 train_time:22241ms step_avg:60.44ms
step:369/2245 train_time:22303ms step_avg:60.44ms
step:370/2245 train_time:22362ms step_avg:60.44ms
step:371/2245 train_time:22424ms step_avg:60.44ms
step:372/2245 train_time:22482ms step_avg:60.44ms
step:373/2245 train_time:22544ms step_avg:60.44ms
step:374/2245 train_time:22603ms step_avg:60.44ms
step:375/2245 train_time:22665ms step_avg:60.44ms
step:376/2245 train_time:22723ms step_avg:60.43ms
step:377/2245 train_time:22785ms step_avg:60.44ms
step:378/2245 train_time:22844ms step_avg:60.43ms
step:379/2245 train_time:22905ms step_avg:60.44ms
step:380/2245 train_time:22964ms step_avg:60.43ms
step:381/2245 train_time:23026ms step_avg:60.43ms
step:382/2245 train_time:23085ms step_avg:60.43ms
step:383/2245 train_time:23147ms step_avg:60.44ms
step:384/2245 train_time:23206ms step_avg:60.43ms
step:385/2245 train_time:23268ms step_avg:60.44ms
step:386/2245 train_time:23329ms step_avg:60.44ms
step:387/2245 train_time:23391ms step_avg:60.44ms
step:388/2245 train_time:23450ms step_avg:60.44ms
step:389/2245 train_time:23512ms step_avg:60.44ms
step:390/2245 train_time:23571ms step_avg:60.44ms
step:391/2245 train_time:23633ms step_avg:60.44ms
step:392/2245 train_time:23692ms step_avg:60.44ms
step:393/2245 train_time:23754ms step_avg:60.44ms
step:394/2245 train_time:23813ms step_avg:60.44ms
step:395/2245 train_time:23875ms step_avg:60.44ms
step:396/2245 train_time:23933ms step_avg:60.44ms
step:397/2245 train_time:23996ms step_avg:60.44ms
step:398/2245 train_time:24055ms step_avg:60.44ms
step:399/2245 train_time:24117ms step_avg:60.44ms
step:400/2245 train_time:24175ms step_avg:60.44ms
step:401/2245 train_time:24236ms step_avg:60.44ms
step:402/2245 train_time:24296ms step_avg:60.44ms
step:403/2245 train_time:24357ms step_avg:60.44ms
step:404/2245 train_time:24416ms step_avg:60.44ms
step:405/2245 train_time:24477ms step_avg:60.44ms
step:406/2245 train_time:24536ms step_avg:60.43ms
step:407/2245 train_time:24598ms step_avg:60.44ms
step:408/2245 train_time:24656ms step_avg:60.43ms
step:409/2245 train_time:24718ms step_avg:60.44ms
step:410/2245 train_time:24777ms step_avg:60.43ms
step:411/2245 train_time:24838ms step_avg:60.43ms
step:412/2245 train_time:24897ms step_avg:60.43ms
step:413/2245 train_time:24959ms step_avg:60.43ms
step:414/2245 train_time:25018ms step_avg:60.43ms
step:415/2245 train_time:25079ms step_avg:60.43ms
step:416/2245 train_time:25137ms step_avg:60.43ms
step:417/2245 train_time:25199ms step_avg:60.43ms
step:418/2245 train_time:25258ms step_avg:60.43ms
step:419/2245 train_time:25320ms step_avg:60.43ms
step:420/2245 train_time:25379ms step_avg:60.43ms
step:421/2245 train_time:25440ms step_avg:60.43ms
step:422/2245 train_time:25499ms step_avg:60.42ms
step:423/2245 train_time:25561ms step_avg:60.43ms
step:424/2245 train_time:25619ms step_avg:60.42ms
step:425/2245 train_time:25681ms step_avg:60.43ms
step:426/2245 train_time:25740ms step_avg:60.42ms
step:427/2245 train_time:25802ms step_avg:60.43ms
step:428/2245 train_time:25860ms step_avg:60.42ms
step:429/2245 train_time:25922ms step_avg:60.42ms
step:430/2245 train_time:25981ms step_avg:60.42ms
step:431/2245 train_time:26042ms step_avg:60.42ms
step:432/2245 train_time:26101ms step_avg:60.42ms
step:433/2245 train_time:26163ms step_avg:60.42ms
step:434/2245 train_time:26221ms step_avg:60.42ms
step:435/2245 train_time:26283ms step_avg:60.42ms
step:436/2245 train_time:26342ms step_avg:60.42ms
step:437/2245 train_time:26404ms step_avg:60.42ms
step:438/2245 train_time:26463ms step_avg:60.42ms
step:439/2245 train_time:26524ms step_avg:60.42ms
step:440/2245 train_time:26583ms step_avg:60.42ms
step:441/2245 train_time:26646ms step_avg:60.42ms
step:442/2245 train_time:26705ms step_avg:60.42ms
step:443/2245 train_time:26766ms step_avg:60.42ms
step:444/2245 train_time:26826ms step_avg:60.42ms
step:445/2245 train_time:26887ms step_avg:60.42ms
step:446/2245 train_time:26946ms step_avg:60.42ms
step:447/2245 train_time:27008ms step_avg:60.42ms
step:448/2245 train_time:27067ms step_avg:60.42ms
step:449/2245 train_time:27128ms step_avg:60.42ms
step:450/2245 train_time:27188ms step_avg:60.42ms
step:451/2245 train_time:27250ms step_avg:60.42ms
step:452/2245 train_time:27309ms step_avg:60.42ms
step:453/2245 train_time:27371ms step_avg:60.42ms
step:454/2245 train_time:27431ms step_avg:60.42ms
step:455/2245 train_time:27492ms step_avg:60.42ms
step:456/2245 train_time:27552ms step_avg:60.42ms
step:457/2245 train_time:27613ms step_avg:60.42ms
step:458/2245 train_time:27673ms step_avg:60.42ms
step:459/2245 train_time:27735ms step_avg:60.42ms
step:460/2245 train_time:27794ms step_avg:60.42ms
step:461/2245 train_time:27856ms step_avg:60.42ms
step:462/2245 train_time:27915ms step_avg:60.42ms
step:463/2245 train_time:27977ms step_avg:60.43ms
step:464/2245 train_time:28036ms step_avg:60.42ms
step:465/2245 train_time:28097ms step_avg:60.42ms
step:466/2245 train_time:28155ms step_avg:60.42ms
step:467/2245 train_time:28216ms step_avg:60.42ms
step:468/2245 train_time:28275ms step_avg:60.42ms
step:469/2245 train_time:28336ms step_avg:60.42ms
step:470/2245 train_time:28395ms step_avg:60.42ms
step:471/2245 train_time:28457ms step_avg:60.42ms
step:472/2245 train_time:28515ms step_avg:60.41ms
step:473/2245 train_time:28577ms step_avg:60.42ms
step:474/2245 train_time:28636ms step_avg:60.41ms
step:475/2245 train_time:28697ms step_avg:60.41ms
step:476/2245 train_time:28756ms step_avg:60.41ms
step:477/2245 train_time:28817ms step_avg:60.41ms
step:478/2245 train_time:28876ms step_avg:60.41ms
step:479/2245 train_time:28937ms step_avg:60.41ms
step:480/2245 train_time:28996ms step_avg:60.41ms
step:481/2245 train_time:29058ms step_avg:60.41ms
step:482/2245 train_time:29116ms step_avg:60.41ms
step:483/2245 train_time:29178ms step_avg:60.41ms
step:484/2245 train_time:29237ms step_avg:60.41ms
step:485/2245 train_time:29298ms step_avg:60.41ms
step:486/2245 train_time:29357ms step_avg:60.40ms
step:487/2245 train_time:29418ms step_avg:60.41ms
step:488/2245 train_time:29477ms step_avg:60.40ms
step:489/2245 train_time:29539ms step_avg:60.41ms
step:490/2245 train_time:29598ms step_avg:60.40ms
step:491/2245 train_time:29660ms step_avg:60.41ms
step:492/2245 train_time:29719ms step_avg:60.40ms
step:493/2245 train_time:29781ms step_avg:60.41ms
step:494/2245 train_time:29841ms step_avg:60.41ms
step:495/2245 train_time:29902ms step_avg:60.41ms
step:496/2245 train_time:29961ms step_avg:60.40ms
step:497/2245 train_time:30022ms step_avg:60.41ms
step:498/2245 train_time:30081ms step_avg:60.40ms
step:499/2245 train_time:30143ms step_avg:60.41ms
step:500/2245 train_time:30202ms step_avg:60.40ms
step:500/2245 val_loss:3.8244 train_time:30264ms step_avg:60.53ms
step:501/2245 train_time:30283ms step_avg:60.44ms
step:502/2245 train_time:30325ms step_avg:60.41ms
step:503/2245 train_time:30389ms step_avg:60.42ms
step:504/2245 train_time:30450ms step_avg:60.42ms
step:505/2245 train_time:30511ms step_avg:60.42ms
step:506/2245 train_time:30571ms step_avg:60.42ms
step:507/2245 train_time:30633ms step_avg:60.42ms
step:508/2245 train_time:30691ms step_avg:60.42ms
step:509/2245 train_time:30753ms step_avg:60.42ms
step:510/2245 train_time:30812ms step_avg:60.42ms
step:511/2245 train_time:30873ms step_avg:60.42ms
step:512/2245 train_time:30932ms step_avg:60.41ms
step:513/2245 train_time:30993ms step_avg:60.41ms
step:514/2245 train_time:31052ms step_avg:60.41ms
step:515/2245 train_time:31113ms step_avg:60.41ms
step:516/2245 train_time:31172ms step_avg:60.41ms
step:517/2245 train_time:31235ms step_avg:60.42ms
step:518/2245 train_time:31296ms step_avg:60.42ms
step:519/2245 train_time:31360ms step_avg:60.42ms
step:520/2245 train_time:31420ms step_avg:60.42ms
step:521/2245 train_time:31482ms step_avg:60.43ms
step:522/2245 train_time:31540ms step_avg:60.42ms
step:523/2245 train_time:31602ms step_avg:60.42ms
step:524/2245 train_time:31661ms step_avg:60.42ms
step:525/2245 train_time:31722ms step_avg:60.42ms
step:526/2245 train_time:31781ms step_avg:60.42ms
step:527/2245 train_time:31842ms step_avg:60.42ms
step:528/2245 train_time:31900ms step_avg:60.42ms
step:529/2245 train_time:31962ms step_avg:60.42ms
step:530/2245 train_time:32021ms step_avg:60.42ms
step:531/2245 train_time:32082ms step_avg:60.42ms
step:532/2245 train_time:32142ms step_avg:60.42ms
step:533/2245 train_time:32203ms step_avg:60.42ms
step:534/2245 train_time:32262ms step_avg:60.42ms
step:535/2245 train_time:32324ms step_avg:60.42ms
step:536/2245 train_time:32384ms step_avg:60.42ms
step:537/2245 train_time:32446ms step_avg:60.42ms
step:538/2245 train_time:32506ms step_avg:60.42ms
step:539/2245 train_time:32567ms step_avg:60.42ms
step:540/2245 train_time:32626ms step_avg:60.42ms
step:541/2245 train_time:32688ms step_avg:60.42ms
step:542/2245 train_time:32746ms step_avg:60.42ms
step:543/2245 train_time:32807ms step_avg:60.42ms
step:544/2245 train_time:32866ms step_avg:60.42ms
step:545/2245 train_time:32928ms step_avg:60.42ms
step:546/2245 train_time:32986ms step_avg:60.41ms
step:547/2245 train_time:33050ms step_avg:60.42ms
step:548/2245 train_time:33107ms step_avg:60.41ms
step:549/2245 train_time:33168ms step_avg:60.42ms
step:550/2245 train_time:33228ms step_avg:60.41ms
step:551/2245 train_time:33290ms step_avg:60.42ms
step:552/2245 train_time:33349ms step_avg:60.41ms
step:553/2245 train_time:33411ms step_avg:60.42ms
step:554/2245 train_time:33470ms step_avg:60.41ms
step:555/2245 train_time:33531ms step_avg:60.42ms
step:556/2245 train_time:33591ms step_avg:60.42ms
step:557/2245 train_time:33653ms step_avg:60.42ms
step:558/2245 train_time:33712ms step_avg:60.42ms
step:559/2245 train_time:33774ms step_avg:60.42ms
step:560/2245 train_time:33833ms step_avg:60.42ms
step:561/2245 train_time:33895ms step_avg:60.42ms
step:562/2245 train_time:33955ms step_avg:60.42ms
step:563/2245 train_time:34017ms step_avg:60.42ms
step:564/2245 train_time:34077ms step_avg:60.42ms
step:565/2245 train_time:34139ms step_avg:60.42ms
step:566/2245 train_time:34199ms step_avg:60.42ms
step:567/2245 train_time:34261ms step_avg:60.42ms
step:568/2245 train_time:34320ms step_avg:60.42ms
step:569/2245 train_time:34382ms step_avg:60.43ms
step:570/2245 train_time:34441ms step_avg:60.42ms
step:571/2245 train_time:34502ms step_avg:60.42ms
step:572/2245 train_time:34562ms step_avg:60.42ms
step:573/2245 train_time:34624ms step_avg:60.43ms
step:574/2245 train_time:34683ms step_avg:60.42ms
step:575/2245 train_time:34744ms step_avg:60.42ms
step:576/2245 train_time:34803ms step_avg:60.42ms
step:577/2245 train_time:34864ms step_avg:60.42ms
step:578/2245 train_time:34923ms step_avg:60.42ms
step:579/2245 train_time:34984ms step_avg:60.42ms
step:580/2245 train_time:35043ms step_avg:60.42ms
step:581/2245 train_time:35104ms step_avg:60.42ms
step:582/2245 train_time:35162ms step_avg:60.42ms
step:583/2245 train_time:35224ms step_avg:60.42ms
step:584/2245 train_time:35283ms step_avg:60.42ms
step:585/2245 train_time:35344ms step_avg:60.42ms
step:586/2245 train_time:35403ms step_avg:60.41ms
step:587/2245 train_time:35464ms step_avg:60.42ms
step:588/2245 train_time:35523ms step_avg:60.41ms
step:589/2245 train_time:35584ms step_avg:60.42ms
step:590/2245 train_time:35644ms step_avg:60.41ms
step:591/2245 train_time:35705ms step_avg:60.41ms
step:592/2245 train_time:35764ms step_avg:60.41ms
step:593/2245 train_time:35825ms step_avg:60.41ms
step:594/2245 train_time:35884ms step_avg:60.41ms
step:595/2245 train_time:35945ms step_avg:60.41ms
step:596/2245 train_time:36004ms step_avg:60.41ms
step:597/2245 train_time:36065ms step_avg:60.41ms
step:598/2245 train_time:36124ms step_avg:60.41ms
step:599/2245 train_time:36185ms step_avg:60.41ms
step:600/2245 train_time:36244ms step_avg:60.41ms
step:601/2245 train_time:36306ms step_avg:60.41ms
step:602/2245 train_time:36365ms step_avg:60.41ms
step:603/2245 train_time:36427ms step_avg:60.41ms
step:604/2245 train_time:36486ms step_avg:60.41ms
step:605/2245 train_time:36547ms step_avg:60.41ms
step:606/2245 train_time:36606ms step_avg:60.41ms
step:607/2245 train_time:36668ms step_avg:60.41ms
step:608/2245 train_time:36727ms step_avg:60.41ms
step:609/2245 train_time:36788ms step_avg:60.41ms
step:610/2245 train_time:36847ms step_avg:60.40ms
step:611/2245 train_time:36908ms step_avg:60.41ms
step:612/2245 train_time:36967ms step_avg:60.40ms
step:613/2245 train_time:37028ms step_avg:60.40ms
step:614/2245 train_time:37087ms step_avg:60.40ms
step:615/2245 train_time:37149ms step_avg:60.40ms
step:616/2245 train_time:37208ms step_avg:60.40ms
step:617/2245 train_time:37269ms step_avg:60.40ms
step:618/2245 train_time:37328ms step_avg:60.40ms
step:619/2245 train_time:37391ms step_avg:60.40ms
step:620/2245 train_time:37449ms step_avg:60.40ms
step:621/2245 train_time:37511ms step_avg:60.40ms
step:622/2245 train_time:37570ms step_avg:60.40ms
step:623/2245 train_time:37631ms step_avg:60.40ms
step:624/2245 train_time:37690ms step_avg:60.40ms
step:625/2245 train_time:37753ms step_avg:60.40ms
step:626/2245 train_time:37813ms step_avg:60.40ms
step:627/2245 train_time:37874ms step_avg:60.40ms
step:628/2245 train_time:37933ms step_avg:60.40ms
step:629/2245 train_time:37995ms step_avg:60.41ms
step:630/2245 train_time:38055ms step_avg:60.40ms
step:631/2245 train_time:38117ms step_avg:60.41ms
step:632/2245 train_time:38176ms step_avg:60.41ms
step:633/2245 train_time:38238ms step_avg:60.41ms
step:634/2245 train_time:38298ms step_avg:60.41ms
step:635/2245 train_time:38360ms step_avg:60.41ms
step:636/2245 train_time:38419ms step_avg:60.41ms
step:637/2245 train_time:38480ms step_avg:60.41ms
step:638/2245 train_time:38540ms step_avg:60.41ms
step:639/2245 train_time:38601ms step_avg:60.41ms
step:640/2245 train_time:38659ms step_avg:60.41ms
step:641/2245 train_time:38721ms step_avg:60.41ms
step:642/2245 train_time:38780ms step_avg:60.40ms
step:643/2245 train_time:38841ms step_avg:60.41ms
step:644/2245 train_time:38900ms step_avg:60.40ms
step:645/2245 train_time:38962ms step_avg:60.41ms
step:646/2245 train_time:39021ms step_avg:60.40ms
step:647/2245 train_time:39083ms step_avg:60.41ms
step:648/2245 train_time:39142ms step_avg:60.40ms
step:649/2245 train_time:39203ms step_avg:60.41ms
step:650/2245 train_time:39262ms step_avg:60.40ms
step:651/2245 train_time:39324ms step_avg:60.41ms
step:652/2245 train_time:39383ms step_avg:60.40ms
step:653/2245 train_time:39444ms step_avg:60.40ms
step:654/2245 train_time:39503ms step_avg:60.40ms
step:655/2245 train_time:39564ms step_avg:60.40ms
step:656/2245 train_time:39623ms step_avg:60.40ms
step:657/2245 train_time:39684ms step_avg:60.40ms
step:658/2245 train_time:39744ms step_avg:60.40ms
step:659/2245 train_time:39805ms step_avg:60.40ms
step:660/2245 train_time:39864ms step_avg:60.40ms
step:661/2245 train_time:39925ms step_avg:60.40ms
step:662/2245 train_time:39984ms step_avg:60.40ms
step:663/2245 train_time:40046ms step_avg:60.40ms
step:664/2245 train_time:40105ms step_avg:60.40ms
step:665/2245 train_time:40166ms step_avg:60.40ms
step:666/2245 train_time:40225ms step_avg:60.40ms
step:667/2245 train_time:40286ms step_avg:60.40ms
step:668/2245 train_time:40345ms step_avg:60.40ms
step:669/2245 train_time:40407ms step_avg:60.40ms
step:670/2245 train_time:40466ms step_avg:60.40ms
step:671/2245 train_time:40527ms step_avg:60.40ms
step:672/2245 train_time:40585ms step_avg:60.40ms
step:673/2245 train_time:40647ms step_avg:60.40ms
step:674/2245 train_time:40707ms step_avg:60.40ms
step:675/2245 train_time:40768ms step_avg:60.40ms
step:676/2245 train_time:40827ms step_avg:60.40ms
step:677/2245 train_time:40889ms step_avg:60.40ms
step:678/2245 train_time:40948ms step_avg:60.40ms
step:679/2245 train_time:41009ms step_avg:60.40ms
step:680/2245 train_time:41069ms step_avg:60.40ms
step:681/2245 train_time:41130ms step_avg:60.40ms
step:682/2245 train_time:41190ms step_avg:60.40ms
step:683/2245 train_time:41251ms step_avg:60.40ms
step:684/2245 train_time:41310ms step_avg:60.40ms
step:685/2245 train_time:41372ms step_avg:60.40ms
step:686/2245 train_time:41431ms step_avg:60.39ms
step:687/2245 train_time:41492ms step_avg:60.40ms
step:688/2245 train_time:41551ms step_avg:60.39ms
step:689/2245 train_time:41613ms step_avg:60.40ms
step:690/2245 train_time:41673ms step_avg:60.40ms
step:691/2245 train_time:41734ms step_avg:60.40ms
step:692/2245 train_time:41794ms step_avg:60.40ms
step:693/2245 train_time:41855ms step_avg:60.40ms
step:694/2245 train_time:41915ms step_avg:60.40ms
step:695/2245 train_time:41977ms step_avg:60.40ms
step:696/2245 train_time:42036ms step_avg:60.40ms
step:697/2245 train_time:42098ms step_avg:60.40ms
step:698/2245 train_time:42158ms step_avg:60.40ms
step:699/2245 train_time:42220ms step_avg:60.40ms
step:700/2245 train_time:42279ms step_avg:60.40ms
step:701/2245 train_time:42341ms step_avg:60.40ms
step:702/2245 train_time:42400ms step_avg:60.40ms
step:703/2245 train_time:42461ms step_avg:60.40ms
step:704/2245 train_time:42520ms step_avg:60.40ms
step:705/2245 train_time:42582ms step_avg:60.40ms
step:706/2245 train_time:42641ms step_avg:60.40ms
step:707/2245 train_time:42703ms step_avg:60.40ms
step:708/2245 train_time:42762ms step_avg:60.40ms
step:709/2245 train_time:42823ms step_avg:60.40ms
step:710/2245 train_time:42882ms step_avg:60.40ms
step:711/2245 train_time:42944ms step_avg:60.40ms
step:712/2245 train_time:43003ms step_avg:60.40ms
step:713/2245 train_time:43063ms step_avg:60.40ms
step:714/2245 train_time:43122ms step_avg:60.40ms
step:715/2245 train_time:43183ms step_avg:60.40ms
step:716/2245 train_time:43242ms step_avg:60.39ms
step:717/2245 train_time:43303ms step_avg:60.39ms
step:718/2245 train_time:43362ms step_avg:60.39ms
step:719/2245 train_time:43423ms step_avg:60.39ms
step:720/2245 train_time:43867ms step_avg:60.93ms
step:721/2245 train_time:43926ms step_avg:60.92ms
step:722/2245 train_time:43985ms step_avg:60.92ms
step:723/2245 train_time:44046ms step_avg:60.92ms
step:724/2245 train_time:44104ms step_avg:60.92ms
step:725/2245 train_time:44164ms step_avg:60.92ms
step:726/2245 train_time:44222ms step_avg:60.91ms
step:727/2245 train_time:44283ms step_avg:60.91ms
step:728/2245 train_time:44341ms step_avg:60.91ms
step:729/2245 train_time:44402ms step_avg:60.91ms
step:730/2245 train_time:44460ms step_avg:60.90ms
step:731/2245 train_time:44520ms step_avg:60.90ms
step:732/2245 train_time:44579ms step_avg:60.90ms
step:733/2245 train_time:44639ms step_avg:60.90ms
step:734/2245 train_time:44698ms step_avg:60.90ms
step:735/2245 train_time:44762ms step_avg:60.90ms
step:736/2245 train_time:44826ms step_avg:60.90ms
step:737/2245 train_time:44890ms step_avg:60.91ms
step:738/2245 train_time:44950ms step_avg:60.91ms
step:739/2245 train_time:45014ms step_avg:60.91ms
step:740/2245 train_time:45073ms step_avg:60.91ms
step:741/2245 train_time:45135ms step_avg:60.91ms
step:742/2245 train_time:45195ms step_avg:60.91ms
step:743/2245 train_time:45257ms step_avg:60.91ms
step:744/2245 train_time:45317ms step_avg:60.91ms
step:745/2245 train_time:45379ms step_avg:60.91ms
step:746/2245 train_time:45438ms step_avg:60.91ms
step:747/2245 train_time:45499ms step_avg:60.91ms
step:748/2245 train_time:45558ms step_avg:60.91ms
step:749/2245 train_time:45620ms step_avg:60.91ms
step:750/2245 train_time:45680ms step_avg:60.91ms
step:750/2245 val_loss:3.6693 train_time:45744ms step_avg:60.99ms
step:751/2245 train_time:45764ms step_avg:60.94ms
step:752/2245 train_time:45806ms step_avg:60.91ms
step:753/2245 train_time:45868ms step_avg:60.91ms
step:754/2245 train_time:45929ms step_avg:60.91ms
step:755/2245 train_time:45993ms step_avg:60.92ms
step:756/2245 train_time:46052ms step_avg:60.91ms
step:757/2245 train_time:46113ms step_avg:60.92ms
step:758/2245 train_time:46173ms step_avg:60.91ms
step:759/2245 train_time:46234ms step_avg:60.91ms
step:760/2245 train_time:46293ms step_avg:60.91ms
step:761/2245 train_time:46354ms step_avg:60.91ms
step:762/2245 train_time:46413ms step_avg:60.91ms
step:763/2245 train_time:46474ms step_avg:60.91ms
step:764/2245 train_time:46533ms step_avg:60.91ms
step:765/2245 train_time:46595ms step_avg:60.91ms
step:766/2245 train_time:46661ms step_avg:60.92ms
step:767/2245 train_time:46728ms step_avg:60.92ms
step:768/2245 train_time:46790ms step_avg:60.93ms
step:769/2245 train_time:46853ms step_avg:60.93ms
step:770/2245 train_time:46914ms step_avg:60.93ms
step:771/2245 train_time:46976ms step_avg:60.93ms
step:772/2245 train_time:47035ms step_avg:60.93ms
step:773/2245 train_time:47097ms step_avg:60.93ms
step:774/2245 train_time:47157ms step_avg:60.93ms
step:775/2245 train_time:47218ms step_avg:60.93ms
step:776/2245 train_time:47278ms step_avg:60.92ms
step:777/2245 train_time:47339ms step_avg:60.92ms
step:778/2245 train_time:47398ms step_avg:60.92ms
step:779/2245 train_time:47459ms step_avg:60.92ms
step:780/2245 train_time:47518ms step_avg:60.92ms
step:781/2245 train_time:47581ms step_avg:60.92ms
step:782/2245 train_time:47642ms step_avg:60.92ms
step:783/2245 train_time:47707ms step_avg:60.93ms
step:784/2245 train_time:47768ms step_avg:60.93ms
step:785/2245 train_time:47831ms step_avg:60.93ms
step:786/2245 train_time:47892ms step_avg:60.93ms
step:787/2245 train_time:47955ms step_avg:60.93ms
step:788/2245 train_time:48015ms step_avg:60.93ms
step:789/2245 train_time:48077ms step_avg:60.93ms
step:790/2245 train_time:48137ms step_avg:60.93ms
step:791/2245 train_time:48199ms step_avg:60.93ms
step:792/2245 train_time:48258ms step_avg:60.93ms
step:793/2245 train_time:48320ms step_avg:60.93ms
step:794/2245 train_time:48379ms step_avg:60.93ms
step:795/2245 train_time:48440ms step_avg:60.93ms
step:796/2245 train_time:48500ms step_avg:60.93ms
step:797/2245 train_time:48562ms step_avg:60.93ms
step:798/2245 train_time:48622ms step_avg:60.93ms
step:799/2245 train_time:48684ms step_avg:60.93ms
step:800/2245 train_time:48744ms step_avg:60.93ms
step:801/2245 train_time:48806ms step_avg:60.93ms
step:802/2245 train_time:48867ms step_avg:60.93ms
step:803/2245 train_time:48929ms step_avg:60.93ms
step:804/2245 train_time:48990ms step_avg:60.93ms
step:805/2245 train_time:49052ms step_avg:60.93ms
step:806/2245 train_time:49112ms step_avg:60.93ms
step:807/2245 train_time:49175ms step_avg:60.94ms
step:808/2245 train_time:49235ms step_avg:60.93ms
step:809/2245 train_time:49297ms step_avg:60.94ms
step:810/2245 train_time:49357ms step_avg:60.93ms
step:811/2245 train_time:49419ms step_avg:60.94ms
step:812/2245 train_time:49478ms step_avg:60.93ms
step:813/2245 train_time:49541ms step_avg:60.94ms
step:814/2245 train_time:49600ms step_avg:60.93ms
step:815/2245 train_time:49663ms step_avg:60.94ms
step:816/2245 train_time:49722ms step_avg:60.93ms
step:817/2245 train_time:49785ms step_avg:60.94ms
step:818/2245 train_time:49845ms step_avg:60.93ms
step:819/2245 train_time:49907ms step_avg:60.94ms
step:820/2245 train_time:49967ms step_avg:60.94ms
step:821/2245 train_time:50029ms step_avg:60.94ms
step:822/2245 train_time:50089ms step_avg:60.94ms
step:823/2245 train_time:50152ms step_avg:60.94ms
step:824/2245 train_time:50212ms step_avg:60.94ms
step:825/2245 train_time:50274ms step_avg:60.94ms
step:826/2245 train_time:50334ms step_avg:60.94ms
step:827/2245 train_time:50396ms step_avg:60.94ms
step:828/2245 train_time:50456ms step_avg:60.94ms
step:829/2245 train_time:50518ms step_avg:60.94ms
step:830/2245 train_time:50579ms step_avg:60.94ms
step:831/2245 train_time:50641ms step_avg:60.94ms
step:832/2245 train_time:50701ms step_avg:60.94ms
step:833/2245 train_time:50763ms step_avg:60.94ms
step:834/2245 train_time:50822ms step_avg:60.94ms
step:835/2245 train_time:50885ms step_avg:60.94ms
step:836/2245 train_time:50945ms step_avg:60.94ms
step:837/2245 train_time:51007ms step_avg:60.94ms
step:838/2245 train_time:51067ms step_avg:60.94ms
step:839/2245 train_time:51129ms step_avg:60.94ms
step:840/2245 train_time:51189ms step_avg:60.94ms
step:841/2245 train_time:51252ms step_avg:60.94ms
step:842/2245 train_time:51312ms step_avg:60.94ms
step:843/2245 train_time:51374ms step_avg:60.94ms
step:844/2245 train_time:51434ms step_avg:60.94ms
step:845/2245 train_time:51496ms step_avg:60.94ms
step:846/2245 train_time:51557ms step_avg:60.94ms
step:847/2245 train_time:51620ms step_avg:60.94ms
step:848/2245 train_time:51681ms step_avg:60.94ms
step:849/2245 train_time:51743ms step_avg:60.95ms
step:850/2245 train_time:51803ms step_avg:60.94ms
step:851/2245 train_time:51865ms step_avg:60.95ms
step:852/2245 train_time:51924ms step_avg:60.94ms
step:853/2245 train_time:51987ms step_avg:60.95ms
step:854/2245 train_time:52047ms step_avg:60.94ms
step:855/2245 train_time:52108ms step_avg:60.95ms
step:856/2245 train_time:52168ms step_avg:60.94ms
step:857/2245 train_time:52231ms step_avg:60.95ms
step:858/2245 train_time:52291ms step_avg:60.95ms
step:859/2245 train_time:52354ms step_avg:60.95ms
step:860/2245 train_time:52414ms step_avg:60.95ms
step:861/2245 train_time:52476ms step_avg:60.95ms
step:862/2245 train_time:52537ms step_avg:60.95ms
step:863/2245 train_time:52600ms step_avg:60.95ms
step:864/2245 train_time:52659ms step_avg:60.95ms
step:865/2245 train_time:52722ms step_avg:60.95ms
step:866/2245 train_time:52782ms step_avg:60.95ms
step:867/2245 train_time:52844ms step_avg:60.95ms
step:868/2245 train_time:52904ms step_avg:60.95ms
step:869/2245 train_time:52966ms step_avg:60.95ms
step:870/2245 train_time:53026ms step_avg:60.95ms
step:871/2245 train_time:53088ms step_avg:60.95ms
step:872/2245 train_time:53148ms step_avg:60.95ms
step:873/2245 train_time:53211ms step_avg:60.95ms
step:874/2245 train_time:53271ms step_avg:60.95ms
step:875/2245 train_time:53333ms step_avg:60.95ms
step:876/2245 train_time:53392ms step_avg:60.95ms
step:877/2245 train_time:53455ms step_avg:60.95ms
step:878/2245 train_time:53516ms step_avg:60.95ms
step:879/2245 train_time:53578ms step_avg:60.95ms
step:880/2245 train_time:53639ms step_avg:60.95ms
step:881/2245 train_time:53702ms step_avg:60.96ms
step:882/2245 train_time:53762ms step_avg:60.95ms
step:883/2245 train_time:53825ms step_avg:60.96ms
step:884/2245 train_time:53885ms step_avg:60.96ms
step:885/2245 train_time:53947ms step_avg:60.96ms
step:886/2245 train_time:54007ms step_avg:60.96ms
step:887/2245 train_time:54069ms step_avg:60.96ms
step:888/2245 train_time:54129ms step_avg:60.96ms
step:889/2245 train_time:54191ms step_avg:60.96ms
step:890/2245 train_time:54251ms step_avg:60.96ms
step:891/2245 train_time:54314ms step_avg:60.96ms
step:892/2245 train_time:54374ms step_avg:60.96ms
step:893/2245 train_time:54436ms step_avg:60.96ms
step:894/2245 train_time:54496ms step_avg:60.96ms
step:895/2245 train_time:54559ms step_avg:60.96ms
step:896/2245 train_time:54620ms step_avg:60.96ms
step:897/2245 train_time:54682ms step_avg:60.96ms
step:898/2245 train_time:54742ms step_avg:60.96ms
step:899/2245 train_time:54804ms step_avg:60.96ms
step:900/2245 train_time:54864ms step_avg:60.96ms
step:901/2245 train_time:54926ms step_avg:60.96ms
step:902/2245 train_time:54986ms step_avg:60.96ms
step:903/2245 train_time:55047ms step_avg:60.96ms
step:904/2245 train_time:55107ms step_avg:60.96ms
step:905/2245 train_time:55169ms step_avg:60.96ms
step:906/2245 train_time:55229ms step_avg:60.96ms
step:907/2245 train_time:55291ms step_avg:60.96ms
step:908/2245 train_time:55350ms step_avg:60.96ms
step:909/2245 train_time:55413ms step_avg:60.96ms
step:910/2245 train_time:55474ms step_avg:60.96ms
step:911/2245 train_time:55537ms step_avg:60.96ms
step:912/2245 train_time:55598ms step_avg:60.96ms
step:913/2245 train_time:55660ms step_avg:60.96ms
step:914/2245 train_time:55720ms step_avg:60.96ms
step:915/2245 train_time:55783ms step_avg:60.96ms
step:916/2245 train_time:55842ms step_avg:60.96ms
step:917/2245 train_time:55904ms step_avg:60.96ms
step:918/2245 train_time:55964ms step_avg:60.96ms
step:919/2245 train_time:56026ms step_avg:60.96ms
step:920/2245 train_time:56086ms step_avg:60.96ms
step:921/2245 train_time:56148ms step_avg:60.96ms
step:922/2245 train_time:56207ms step_avg:60.96ms
step:923/2245 train_time:56270ms step_avg:60.96ms
step:924/2245 train_time:56330ms step_avg:60.96ms
step:925/2245 train_time:56393ms step_avg:60.97ms
step:926/2245 train_time:56453ms step_avg:60.96ms
step:927/2245 train_time:56515ms step_avg:60.97ms
step:928/2245 train_time:56576ms step_avg:60.97ms
step:929/2245 train_time:56638ms step_avg:60.97ms
step:930/2245 train_time:56699ms step_avg:60.97ms
step:931/2245 train_time:56762ms step_avg:60.97ms
step:932/2245 train_time:56821ms step_avg:60.97ms
step:933/2245 train_time:56883ms step_avg:60.97ms
step:934/2245 train_time:56943ms step_avg:60.97ms
step:935/2245 train_time:57005ms step_avg:60.97ms
step:936/2245 train_time:57065ms step_avg:60.97ms
step:937/2245 train_time:57126ms step_avg:60.97ms
step:938/2245 train_time:57186ms step_avg:60.97ms
step:939/2245 train_time:57248ms step_avg:60.97ms
step:940/2245 train_time:57307ms step_avg:60.97ms
step:941/2245 train_time:57370ms step_avg:60.97ms
step:942/2245 train_time:57430ms step_avg:60.97ms
step:943/2245 train_time:57493ms step_avg:60.97ms
step:944/2245 train_time:57553ms step_avg:60.97ms
step:945/2245 train_time:57616ms step_avg:60.97ms
step:946/2245 train_time:57678ms step_avg:60.97ms
step:947/2245 train_time:57741ms step_avg:60.97ms
step:948/2245 train_time:57801ms step_avg:60.97ms
step:949/2245 train_time:57863ms step_avg:60.97ms
step:950/2245 train_time:57923ms step_avg:60.97ms
step:951/2245 train_time:57985ms step_avg:60.97ms
step:952/2245 train_time:58045ms step_avg:60.97ms
step:953/2245 train_time:58107ms step_avg:60.97ms
step:954/2245 train_time:58167ms step_avg:60.97ms
step:955/2245 train_time:58228ms step_avg:60.97ms
step:956/2245 train_time:58288ms step_avg:60.97ms
step:957/2245 train_time:58351ms step_avg:60.97ms
step:958/2245 train_time:58411ms step_avg:60.97ms
step:959/2245 train_time:58473ms step_avg:60.97ms
step:960/2245 train_time:58534ms step_avg:60.97ms
step:961/2245 train_time:58596ms step_avg:60.97ms
step:962/2245 train_time:58657ms step_avg:60.97ms
step:963/2245 train_time:58720ms step_avg:60.98ms
step:964/2245 train_time:58780ms step_avg:60.98ms
step:965/2245 train_time:58843ms step_avg:60.98ms
step:966/2245 train_time:58903ms step_avg:60.98ms
step:967/2245 train_time:58965ms step_avg:60.98ms
step:968/2245 train_time:59024ms step_avg:60.97ms
step:969/2245 train_time:59086ms step_avg:60.98ms
step:970/2245 train_time:59145ms step_avg:60.97ms
step:971/2245 train_time:59207ms step_avg:60.98ms
step:972/2245 train_time:59266ms step_avg:60.97ms
step:973/2245 train_time:59329ms step_avg:60.98ms
step:974/2245 train_time:59389ms step_avg:60.97ms
step:975/2245 train_time:59452ms step_avg:60.98ms
step:976/2245 train_time:59512ms step_avg:60.98ms
step:977/2245 train_time:59575ms step_avg:60.98ms
step:978/2245 train_time:59636ms step_avg:60.98ms
step:979/2245 train_time:59698ms step_avg:60.98ms
step:980/2245 train_time:59759ms step_avg:60.98ms
step:981/2245 train_time:59822ms step_avg:60.98ms
step:982/2245 train_time:59882ms step_avg:60.98ms
step:983/2245 train_time:59944ms step_avg:60.98ms
step:984/2245 train_time:60003ms step_avg:60.98ms
step:985/2245 train_time:60065ms step_avg:60.98ms
step:986/2245 train_time:60124ms step_avg:60.98ms
step:987/2245 train_time:60186ms step_avg:60.98ms
step:988/2245 train_time:60246ms step_avg:60.98ms
step:989/2245 train_time:60309ms step_avg:60.98ms
step:990/2245 train_time:60368ms step_avg:60.98ms
step:991/2245 train_time:60431ms step_avg:60.98ms
step:992/2245 train_time:60491ms step_avg:60.98ms
step:993/2245 train_time:60554ms step_avg:60.98ms
step:994/2245 train_time:60614ms step_avg:60.98ms
step:995/2245 train_time:60677ms step_avg:60.98ms
step:996/2245 train_time:60737ms step_avg:60.98ms
step:997/2245 train_time:60800ms step_avg:60.98ms
step:998/2245 train_time:60860ms step_avg:60.98ms
step:999/2245 train_time:60922ms step_avg:60.98ms
step:1000/2245 train_time:60981ms step_avg:60.98ms
step:1000/2245 val_loss:3.5947 train_time:61044ms step_avg:61.04ms
step:1001/2245 train_time:61064ms step_avg:61.00ms
step:1002/2245 train_time:61107ms step_avg:60.98ms
step:1003/2245 train_time:61174ms step_avg:60.99ms
step:1004/2245 train_time:61237ms step_avg:60.99ms
step:1005/2245 train_time:61299ms step_avg:60.99ms
step:1006/2245 train_time:61358ms step_avg:60.99ms
step:1007/2245 train_time:61420ms step_avg:60.99ms
step:1008/2245 train_time:61479ms step_avg:60.99ms
step:1009/2245 train_time:61541ms step_avg:60.99ms
step:1010/2245 train_time:61600ms step_avg:60.99ms
step:1011/2245 train_time:61662ms step_avg:60.99ms
step:1012/2245 train_time:61721ms step_avg:60.99ms
step:1013/2245 train_time:61782ms step_avg:60.99ms
step:1014/2245 train_time:61842ms step_avg:60.99ms
step:1015/2245 train_time:61904ms step_avg:60.99ms
step:1016/2245 train_time:61965ms step_avg:60.99ms
step:1017/2245 train_time:62030ms step_avg:60.99ms
step:1018/2245 train_time:62092ms step_avg:60.99ms
step:1019/2245 train_time:62156ms step_avg:61.00ms
step:1020/2245 train_time:62217ms step_avg:61.00ms
step:1021/2245 train_time:62279ms step_avg:61.00ms
step:1022/2245 train_time:62338ms step_avg:61.00ms
step:1023/2245 train_time:62400ms step_avg:61.00ms
step:1024/2245 train_time:62459ms step_avg:61.00ms
step:1025/2245 train_time:62521ms step_avg:61.00ms
step:1026/2245 train_time:62580ms step_avg:60.99ms
step:1027/2245 train_time:62642ms step_avg:60.99ms
step:1028/2245 train_time:62701ms step_avg:60.99ms
step:1029/2245 train_time:62763ms step_avg:60.99ms
step:1030/2245 train_time:62823ms step_avg:60.99ms
step:1031/2245 train_time:62885ms step_avg:60.99ms
step:1032/2245 train_time:62945ms step_avg:60.99ms
step:1033/2245 train_time:63009ms step_avg:61.00ms
step:1034/2245 train_time:63070ms step_avg:61.00ms
step:1035/2245 train_time:63134ms step_avg:61.00ms
step:1036/2245 train_time:63195ms step_avg:61.00ms
step:1037/2245 train_time:63258ms step_avg:61.00ms
step:1038/2245 train_time:63318ms step_avg:61.00ms
step:1039/2245 train_time:63380ms step_avg:61.00ms
step:1040/2245 train_time:63440ms step_avg:61.00ms
step:1041/2245 train_time:63502ms step_avg:61.00ms
step:1042/2245 train_time:63562ms step_avg:61.00ms
step:1043/2245 train_time:63623ms step_avg:61.00ms
step:1044/2245 train_time:63683ms step_avg:61.00ms
step:1045/2245 train_time:63745ms step_avg:61.00ms
step:1046/2245 train_time:63805ms step_avg:61.00ms
step:1047/2245 train_time:63867ms step_avg:61.00ms
step:1048/2245 train_time:63928ms step_avg:61.00ms
step:1049/2245 train_time:63991ms step_avg:61.00ms
step:1050/2245 train_time:64052ms step_avg:61.00ms
step:1051/2245 train_time:64116ms step_avg:61.00ms
step:1052/2245 train_time:64176ms step_avg:61.00ms
step:1053/2245 train_time:64238ms step_avg:61.00ms
step:1054/2245 train_time:64298ms step_avg:61.00ms
step:1055/2245 train_time:64361ms step_avg:61.01ms
step:1056/2245 train_time:64420ms step_avg:61.00ms
step:1057/2245 train_time:64482ms step_avg:61.01ms
step:1058/2245 train_time:64542ms step_avg:61.00ms
step:1059/2245 train_time:64604ms step_avg:61.00ms
step:1060/2245 train_time:64663ms step_avg:61.00ms
step:1061/2245 train_time:64725ms step_avg:61.00ms
step:1062/2245 train_time:64785ms step_avg:61.00ms
step:1063/2245 train_time:64848ms step_avg:61.00ms
step:1064/2245 train_time:64909ms step_avg:61.00ms
step:1065/2245 train_time:64972ms step_avg:61.01ms
step:1066/2245 train_time:65032ms step_avg:61.01ms
step:1067/2245 train_time:65095ms step_avg:61.01ms
step:1068/2245 train_time:65156ms step_avg:61.01ms
step:1069/2245 train_time:65219ms step_avg:61.01ms
step:1070/2245 train_time:65278ms step_avg:61.01ms
step:1071/2245 train_time:65341ms step_avg:61.01ms
step:1072/2245 train_time:65400ms step_avg:61.01ms
step:1073/2245 train_time:65463ms step_avg:61.01ms
step:1074/2245 train_time:65523ms step_avg:61.01ms
step:1075/2245 train_time:65585ms step_avg:61.01ms
step:1076/2245 train_time:65645ms step_avg:61.01ms
step:1077/2245 train_time:65707ms step_avg:61.01ms
step:1078/2245 train_time:65767ms step_avg:61.01ms
step:1079/2245 train_time:65829ms step_avg:61.01ms
step:1080/2245 train_time:65889ms step_avg:61.01ms
step:1081/2245 train_time:65951ms step_avg:61.01ms
step:1082/2245 train_time:66012ms step_avg:61.01ms
step:1083/2245 train_time:66074ms step_avg:61.01ms
step:1084/2245 train_time:66134ms step_avg:61.01ms
step:1085/2245 train_time:66196ms step_avg:61.01ms
step:1086/2245 train_time:66256ms step_avg:61.01ms
step:1087/2245 train_time:66318ms step_avg:61.01ms
step:1088/2245 train_time:66378ms step_avg:61.01ms
step:1089/2245 train_time:66440ms step_avg:61.01ms
step:1090/2245 train_time:66500ms step_avg:61.01ms
step:1091/2245 train_time:66562ms step_avg:61.01ms
step:1092/2245 train_time:66622ms step_avg:61.01ms
step:1093/2245 train_time:66684ms step_avg:61.01ms
step:1094/2245 train_time:66744ms step_avg:61.01ms
step:1095/2245 train_time:66807ms step_avg:61.01ms
step:1096/2245 train_time:66868ms step_avg:61.01ms
step:1097/2245 train_time:66931ms step_avg:61.01ms
step:1098/2245 train_time:66991ms step_avg:61.01ms
step:1099/2245 train_time:67053ms step_avg:61.01ms
step:1100/2245 train_time:67113ms step_avg:61.01ms
step:1101/2245 train_time:67176ms step_avg:61.01ms
step:1102/2245 train_time:67236ms step_avg:61.01ms
step:1103/2245 train_time:67298ms step_avg:61.01ms
step:1104/2245 train_time:67357ms step_avg:61.01ms
step:1105/2245 train_time:67419ms step_avg:61.01ms
step:1106/2245 train_time:67479ms step_avg:61.01ms
step:1107/2245 train_time:67541ms step_avg:61.01ms
step:1108/2245 train_time:67600ms step_avg:61.01ms
step:1109/2245 train_time:67663ms step_avg:61.01ms
step:1110/2245 train_time:67723ms step_avg:61.01ms
step:1111/2245 train_time:67785ms step_avg:61.01ms
step:1112/2245 train_time:67846ms step_avg:61.01ms
step:1113/2245 train_time:67910ms step_avg:61.01ms
step:1114/2245 train_time:67969ms step_avg:61.01ms
step:1115/2245 train_time:68032ms step_avg:61.02ms
step:1116/2245 train_time:68092ms step_avg:61.01ms
step:1117/2245 train_time:68154ms step_avg:61.01ms
step:1118/2245 train_time:68214ms step_avg:61.01ms
step:1119/2245 train_time:68276ms step_avg:61.02ms
step:1120/2245 train_time:68336ms step_avg:61.01ms
step:1121/2245 train_time:68398ms step_avg:61.01ms
step:1122/2245 train_time:68458ms step_avg:61.01ms
step:1123/2245 train_time:68520ms step_avg:61.02ms
step:1124/2245 train_time:68581ms step_avg:61.01ms
step:1125/2245 train_time:68643ms step_avg:61.02ms
step:1126/2245 train_time:68703ms step_avg:61.02ms
step:1127/2245 train_time:68766ms step_avg:61.02ms
step:1128/2245 train_time:68826ms step_avg:61.02ms
step:1129/2245 train_time:68889ms step_avg:61.02ms
step:1130/2245 train_time:68949ms step_avg:61.02ms
step:1131/2245 train_time:69013ms step_avg:61.02ms
step:1132/2245 train_time:69072ms step_avg:61.02ms
step:1133/2245 train_time:69134ms step_avg:61.02ms
step:1134/2245 train_time:69194ms step_avg:61.02ms
step:1135/2245 train_time:69256ms step_avg:61.02ms
step:1136/2245 train_time:69315ms step_avg:61.02ms
step:1137/2245 train_time:69378ms step_avg:61.02ms
step:1138/2245 train_time:69437ms step_avg:61.02ms
step:1139/2245 train_time:69499ms step_avg:61.02ms
step:1140/2245 train_time:69559ms step_avg:61.02ms
step:1141/2245 train_time:69622ms step_avg:61.02ms
step:1142/2245 train_time:69682ms step_avg:61.02ms
step:1143/2245 train_time:69744ms step_avg:61.02ms
step:1144/2245 train_time:69806ms step_avg:61.02ms
step:1145/2245 train_time:69869ms step_avg:61.02ms
step:1146/2245 train_time:69929ms step_avg:61.02ms
step:1147/2245 train_time:69991ms step_avg:61.02ms
step:1148/2245 train_time:70051ms step_avg:61.02ms
step:1149/2245 train_time:70114ms step_avg:61.02ms
step:1150/2245 train_time:70174ms step_avg:61.02ms
step:1151/2245 train_time:70236ms step_avg:61.02ms
step:1152/2245 train_time:70296ms step_avg:61.02ms
step:1153/2245 train_time:70357ms step_avg:61.02ms
step:1154/2245 train_time:70417ms step_avg:61.02ms
step:1155/2245 train_time:70479ms step_avg:61.02ms
step:1156/2245 train_time:70539ms step_avg:61.02ms
step:1157/2245 train_time:70600ms step_avg:61.02ms
step:1158/2245 train_time:70660ms step_avg:61.02ms
step:1159/2245 train_time:70723ms step_avg:61.02ms
step:1160/2245 train_time:70783ms step_avg:61.02ms
step:1161/2245 train_time:70845ms step_avg:61.02ms
step:1162/2245 train_time:70906ms step_avg:61.02ms
step:1163/2245 train_time:70970ms step_avg:61.02ms
step:1164/2245 train_time:71030ms step_avg:61.02ms
step:1165/2245 train_time:71092ms step_avg:61.02ms
step:1166/2245 train_time:71152ms step_avg:61.02ms
step:1167/2245 train_time:71215ms step_avg:61.02ms
step:1168/2245 train_time:71275ms step_avg:61.02ms
step:1169/2245 train_time:71337ms step_avg:61.02ms
step:1170/2245 train_time:71397ms step_avg:61.02ms
step:1171/2245 train_time:71459ms step_avg:61.02ms
step:1172/2245 train_time:71519ms step_avg:61.02ms
step:1173/2245 train_time:71581ms step_avg:61.02ms
step:1174/2245 train_time:71641ms step_avg:61.02ms
step:1175/2245 train_time:71703ms step_avg:61.02ms
step:1176/2245 train_time:71764ms step_avg:61.02ms
step:1177/2245 train_time:71826ms step_avg:61.02ms
step:1178/2245 train_time:71886ms step_avg:61.02ms
step:1179/2245 train_time:71949ms step_avg:61.03ms
step:1180/2245 train_time:72009ms step_avg:61.02ms
step:1181/2245 train_time:72072ms step_avg:61.03ms
step:1182/2245 train_time:72131ms step_avg:61.02ms
step:1183/2245 train_time:72194ms step_avg:61.03ms
step:1184/2245 train_time:72253ms step_avg:61.02ms
step:1185/2245 train_time:72315ms step_avg:61.03ms
step:1186/2245 train_time:72375ms step_avg:61.02ms
step:1187/2245 train_time:72437ms step_avg:61.03ms
step:1188/2245 train_time:72497ms step_avg:61.02ms
step:1189/2245 train_time:72559ms step_avg:61.03ms
step:1190/2245 train_time:72619ms step_avg:61.02ms
step:1191/2245 train_time:72682ms step_avg:61.03ms
step:1192/2245 train_time:72742ms step_avg:61.02ms
step:1193/2245 train_time:72804ms step_avg:61.03ms
step:1194/2245 train_time:72864ms step_avg:61.03ms
step:1195/2245 train_time:72927ms step_avg:61.03ms
step:1196/2245 train_time:72987ms step_avg:61.03ms
step:1197/2245 train_time:73050ms step_avg:61.03ms
step:1198/2245 train_time:73110ms step_avg:61.03ms
step:1199/2245 train_time:73173ms step_avg:61.03ms
step:1200/2245 train_time:73233ms step_avg:61.03ms
step:1201/2245 train_time:73295ms step_avg:61.03ms
step:1202/2245 train_time:73355ms step_avg:61.03ms
step:1203/2245 train_time:73417ms step_avg:61.03ms
step:1204/2245 train_time:73477ms step_avg:61.03ms
step:1205/2245 train_time:73538ms step_avg:61.03ms
step:1206/2245 train_time:73598ms step_avg:61.03ms
step:1207/2245 train_time:73662ms step_avg:61.03ms
step:1208/2245 train_time:73722ms step_avg:61.03ms
step:1209/2245 train_time:73784ms step_avg:61.03ms
step:1210/2245 train_time:73844ms step_avg:61.03ms
step:1211/2245 train_time:73907ms step_avg:61.03ms
step:1212/2245 train_time:73968ms step_avg:61.03ms
step:1213/2245 train_time:74031ms step_avg:61.03ms
step:1214/2245 train_time:74091ms step_avg:61.03ms
step:1215/2245 train_time:74153ms step_avg:61.03ms
step:1216/2245 train_time:74213ms step_avg:61.03ms
step:1217/2245 train_time:74275ms step_avg:61.03ms
step:1218/2245 train_time:74335ms step_avg:61.03ms
step:1219/2245 train_time:74397ms step_avg:61.03ms
step:1220/2245 train_time:74457ms step_avg:61.03ms
step:1221/2245 train_time:74519ms step_avg:61.03ms
step:1222/2245 train_time:74579ms step_avg:61.03ms
step:1223/2245 train_time:74642ms step_avg:61.03ms
step:1224/2245 train_time:74702ms step_avg:61.03ms
step:1225/2245 train_time:74764ms step_avg:61.03ms
step:1226/2245 train_time:74824ms step_avg:61.03ms
step:1227/2245 train_time:74887ms step_avg:61.03ms
step:1228/2245 train_time:74947ms step_avg:61.03ms
step:1229/2245 train_time:75011ms step_avg:61.03ms
step:1230/2245 train_time:75071ms step_avg:61.03ms
step:1231/2245 train_time:75134ms step_avg:61.03ms
step:1232/2245 train_time:75194ms step_avg:61.03ms
step:1233/2245 train_time:75255ms step_avg:61.03ms
step:1234/2245 train_time:75316ms step_avg:61.03ms
step:1235/2245 train_time:75378ms step_avg:61.03ms
step:1236/2245 train_time:75438ms step_avg:61.03ms
step:1237/2245 train_time:75500ms step_avg:61.03ms
step:1238/2245 train_time:75559ms step_avg:61.03ms
step:1239/2245 train_time:75622ms step_avg:61.04ms
step:1240/2245 train_time:75682ms step_avg:61.03ms
step:1241/2245 train_time:75744ms step_avg:61.03ms
step:1242/2245 train_time:75804ms step_avg:61.03ms
step:1243/2245 train_time:75866ms step_avg:61.03ms
step:1244/2245 train_time:75927ms step_avg:61.03ms
step:1245/2245 train_time:75990ms step_avg:61.04ms
step:1246/2245 train_time:76050ms step_avg:61.04ms
step:1247/2245 train_time:76113ms step_avg:61.04ms
step:1248/2245 train_time:76173ms step_avg:61.04ms
step:1249/2245 train_time:76235ms step_avg:61.04ms
step:1250/2245 train_time:76295ms step_avg:61.04ms
step:1250/2245 val_loss:3.5259 train_time:76358ms step_avg:61.09ms
step:1251/2245 train_time:76376ms step_avg:61.05ms
step:1252/2245 train_time:76419ms step_avg:61.04ms
step:1253/2245 train_time:76487ms step_avg:61.04ms
step:1254/2245 train_time:76552ms step_avg:61.05ms
step:1255/2245 train_time:76614ms step_avg:61.05ms
step:1256/2245 train_time:76674ms step_avg:61.05ms
step:1257/2245 train_time:76736ms step_avg:61.05ms
step:1258/2245 train_time:76795ms step_avg:61.05ms
step:1259/2245 train_time:76856ms step_avg:61.05ms
step:1260/2245 train_time:76916ms step_avg:61.04ms
step:1261/2245 train_time:76977ms step_avg:61.04ms
step:1262/2245 train_time:77036ms step_avg:61.04ms
step:1263/2245 train_time:77098ms step_avg:61.04ms
step:1264/2245 train_time:77158ms step_avg:61.04ms
step:1265/2245 train_time:77220ms step_avg:61.04ms
step:1266/2245 train_time:77280ms step_avg:61.04ms
step:1267/2245 train_time:77344ms step_avg:61.04ms
step:1268/2245 train_time:77404ms step_avg:61.04ms
step:1269/2245 train_time:77468ms step_avg:61.05ms
step:1270/2245 train_time:77528ms step_avg:61.05ms
step:1271/2245 train_time:77591ms step_avg:61.05ms
step:1272/2245 train_time:77652ms step_avg:61.05ms
step:1273/2245 train_time:77714ms step_avg:61.05ms
step:1274/2245 train_time:77774ms step_avg:61.05ms
step:1275/2245 train_time:77836ms step_avg:61.05ms
step:1276/2245 train_time:77895ms step_avg:61.05ms
step:1277/2245 train_time:77957ms step_avg:61.05ms
step:1278/2245 train_time:78016ms step_avg:61.05ms
step:1279/2245 train_time:78078ms step_avg:61.05ms
step:1280/2245 train_time:78138ms step_avg:61.05ms
step:1281/2245 train_time:78200ms step_avg:61.05ms
step:1282/2245 train_time:78261ms step_avg:61.05ms
step:1283/2245 train_time:78325ms step_avg:61.05ms
step:1284/2245 train_time:78385ms step_avg:61.05ms
step:1285/2245 train_time:78448ms step_avg:61.05ms
step:1286/2245 train_time:78509ms step_avg:61.05ms
step:1287/2245 train_time:78572ms step_avg:61.05ms
step:1288/2245 train_time:78632ms step_avg:61.05ms
step:1289/2245 train_time:78695ms step_avg:61.05ms
step:1290/2245 train_time:78755ms step_avg:61.05ms
step:1291/2245 train_time:78817ms step_avg:61.05ms
step:1292/2245 train_time:78876ms step_avg:61.05ms
step:1293/2245 train_time:78938ms step_avg:61.05ms
step:1294/2245 train_time:78998ms step_avg:61.05ms
step:1295/2245 train_time:79060ms step_avg:61.05ms
step:1296/2245 train_time:79120ms step_avg:61.05ms
step:1297/2245 train_time:79182ms step_avg:61.05ms
step:1298/2245 train_time:79243ms step_avg:61.05ms
step:1299/2245 train_time:79305ms step_avg:61.05ms
step:1300/2245 train_time:79366ms step_avg:61.05ms
step:1301/2245 train_time:79429ms step_avg:61.05ms
step:1302/2245 train_time:79489ms step_avg:61.05ms
step:1303/2245 train_time:79552ms step_avg:61.05ms
step:1304/2245 train_time:79611ms step_avg:61.05ms
step:1305/2245 train_time:79674ms step_avg:61.05ms
step:1306/2245 train_time:79735ms step_avg:61.05ms
step:1307/2245 train_time:79797ms step_avg:61.05ms
step:1308/2245 train_time:79857ms step_avg:61.05ms
step:1309/2245 train_time:79919ms step_avg:61.05ms
step:1310/2245 train_time:79979ms step_avg:61.05ms
step:1311/2245 train_time:80041ms step_avg:61.05ms
step:1312/2245 train_time:80101ms step_avg:61.05ms
step:1313/2245 train_time:80163ms step_avg:61.05ms
step:1314/2245 train_time:80223ms step_avg:61.05ms
step:1315/2245 train_time:80286ms step_avg:61.05ms
step:1316/2245 train_time:80346ms step_avg:61.05ms
step:1317/2245 train_time:80408ms step_avg:61.05ms
step:1318/2245 train_time:80468ms step_avg:61.05ms
step:1319/2245 train_time:80530ms step_avg:61.05ms
step:1320/2245 train_time:80590ms step_avg:61.05ms
step:1321/2245 train_time:80653ms step_avg:61.05ms
step:1322/2245 train_time:80713ms step_avg:61.05ms
step:1323/2245 train_time:80776ms step_avg:61.05ms
step:1324/2245 train_time:80836ms step_avg:61.05ms
step:1325/2245 train_time:80898ms step_avg:61.06ms
step:1326/2245 train_time:80958ms step_avg:61.05ms
step:1327/2245 train_time:81020ms step_avg:61.06ms
step:1328/2245 train_time:81081ms step_avg:61.06ms
step:1329/2245 train_time:81144ms step_avg:61.06ms
step:1330/2245 train_time:81203ms step_avg:61.06ms
step:1331/2245 train_time:81266ms step_avg:61.06ms
step:1332/2245 train_time:81326ms step_avg:61.06ms
step:1333/2245 train_time:81389ms step_avg:61.06ms
step:1334/2245 train_time:81449ms step_avg:61.06ms
step:1335/2245 train_time:81510ms step_avg:61.06ms
step:1336/2245 train_time:81570ms step_avg:61.06ms
step:1337/2245 train_time:81632ms step_avg:61.06ms
step:1338/2245 train_time:81692ms step_avg:61.06ms
step:1339/2245 train_time:81754ms step_avg:61.06ms
step:1340/2245 train_time:81813ms step_avg:61.05ms
step:1341/2245 train_time:81876ms step_avg:61.06ms
step:1342/2245 train_time:81935ms step_avg:61.05ms
step:1343/2245 train_time:81998ms step_avg:61.06ms
step:1344/2245 train_time:82058ms step_avg:61.05ms
step:1345/2245 train_time:82120ms step_avg:61.06ms
step:1346/2245 train_time:82181ms step_avg:61.06ms
step:1347/2245 train_time:82244ms step_avg:61.06ms
step:1348/2245 train_time:82304ms step_avg:61.06ms
step:1349/2245 train_time:82366ms step_avg:61.06ms
step:1350/2245 train_time:82426ms step_avg:61.06ms
step:1351/2245 train_time:82488ms step_avg:61.06ms
step:1352/2245 train_time:82548ms step_avg:61.06ms
step:1353/2245 train_time:82611ms step_avg:61.06ms
step:1354/2245 train_time:82671ms step_avg:61.06ms
step:1355/2245 train_time:82733ms step_avg:61.06ms
step:1356/2245 train_time:82793ms step_avg:61.06ms
step:1357/2245 train_time:82855ms step_avg:61.06ms
step:1358/2245 train_time:82914ms step_avg:61.06ms
step:1359/2245 train_time:82977ms step_avg:61.06ms
step:1360/2245 train_time:83037ms step_avg:61.06ms
step:1361/2245 train_time:83099ms step_avg:61.06ms
step:1362/2245 train_time:83159ms step_avg:61.06ms
step:1363/2245 train_time:83222ms step_avg:61.06ms
step:1364/2245 train_time:83282ms step_avg:61.06ms
step:1365/2245 train_time:83345ms step_avg:61.06ms
step:1366/2245 train_time:83405ms step_avg:61.06ms
step:1367/2245 train_time:83467ms step_avg:61.06ms
step:1368/2245 train_time:83527ms step_avg:61.06ms
step:1369/2245 train_time:83590ms step_avg:61.06ms
step:1370/2245 train_time:83650ms step_avg:61.06ms
step:1371/2245 train_time:83712ms step_avg:61.06ms
step:1372/2245 train_time:83771ms step_avg:61.06ms
step:1373/2245 train_time:83834ms step_avg:61.06ms
step:1374/2245 train_time:83894ms step_avg:61.06ms
step:1375/2245 train_time:83957ms step_avg:61.06ms
step:1376/2245 train_time:84017ms step_avg:61.06ms
step:1377/2245 train_time:84079ms step_avg:61.06ms
step:1378/2245 train_time:84139ms step_avg:61.06ms
step:1379/2245 train_time:84202ms step_avg:61.06ms
step:1380/2245 train_time:84262ms step_avg:61.06ms
step:1381/2245 train_time:84325ms step_avg:61.06ms
step:1382/2245 train_time:84385ms step_avg:61.06ms
step:1383/2245 train_time:84448ms step_avg:61.06ms
step:1384/2245 train_time:84508ms step_avg:61.06ms
step:1385/2245 train_time:84570ms step_avg:61.06ms
step:1386/2245 train_time:84630ms step_avg:61.06ms
step:1387/2245 train_time:84693ms step_avg:61.06ms
step:1388/2245 train_time:84752ms step_avg:61.06ms
step:1389/2245 train_time:84814ms step_avg:61.06ms
step:1390/2245 train_time:84874ms step_avg:61.06ms
step:1391/2245 train_time:84937ms step_avg:61.06ms
step:1392/2245 train_time:84996ms step_avg:61.06ms
step:1393/2245 train_time:85059ms step_avg:61.06ms
step:1394/2245 train_time:85119ms step_avg:61.06ms
step:1395/2245 train_time:85182ms step_avg:61.06ms
step:1396/2245 train_time:85243ms step_avg:61.06ms
step:1397/2245 train_time:85305ms step_avg:61.06ms
step:1398/2245 train_time:85365ms step_avg:61.06ms
step:1399/2245 train_time:85429ms step_avg:61.06ms
step:1400/2245 train_time:85490ms step_avg:61.06ms
step:1401/2245 train_time:85552ms step_avg:61.06ms
step:1402/2245 train_time:85611ms step_avg:61.06ms
step:1403/2245 train_time:85673ms step_avg:61.06ms
step:1404/2245 train_time:85733ms step_avg:61.06ms
step:1405/2245 train_time:85796ms step_avg:61.06ms
step:1406/2245 train_time:85855ms step_avg:61.06ms
step:1407/2245 train_time:85917ms step_avg:61.06ms
step:1408/2245 train_time:85977ms step_avg:61.06ms
step:1409/2245 train_time:86039ms step_avg:61.06ms
step:1410/2245 train_time:86099ms step_avg:61.06ms
step:1411/2245 train_time:86161ms step_avg:61.06ms
step:1412/2245 train_time:86222ms step_avg:61.06ms
step:1413/2245 train_time:86285ms step_avg:61.07ms
step:1414/2245 train_time:86346ms step_avg:61.06ms
step:1415/2245 train_time:86408ms step_avg:61.07ms
step:1416/2245 train_time:86468ms step_avg:61.06ms
step:1417/2245 train_time:86529ms step_avg:61.07ms
step:1418/2245 train_time:86589ms step_avg:61.06ms
step:1419/2245 train_time:86652ms step_avg:61.07ms
step:1420/2245 train_time:86712ms step_avg:61.06ms
step:1421/2245 train_time:86774ms step_avg:61.07ms
step:1422/2245 train_time:86833ms step_avg:61.06ms
step:1423/2245 train_time:86896ms step_avg:61.07ms
step:1424/2245 train_time:86957ms step_avg:61.06ms
step:1425/2245 train_time:87019ms step_avg:61.07ms
step:1426/2245 train_time:87078ms step_avg:61.06ms
step:1427/2245 train_time:87141ms step_avg:61.07ms
step:1428/2245 train_time:87202ms step_avg:61.07ms
step:1429/2245 train_time:87265ms step_avg:61.07ms
step:1430/2245 train_time:87324ms step_avg:61.07ms
step:1431/2245 train_time:87387ms step_avg:61.07ms
step:1432/2245 train_time:87448ms step_avg:61.07ms
step:1433/2245 train_time:87510ms step_avg:61.07ms
step:1434/2245 train_time:87570ms step_avg:61.07ms
step:1435/2245 train_time:87632ms step_avg:61.07ms
step:1436/2245 train_time:87692ms step_avg:61.07ms
step:1437/2245 train_time:87754ms step_avg:61.07ms
step:1438/2245 train_time:87814ms step_avg:61.07ms
step:1439/2245 train_time:87876ms step_avg:61.07ms
step:1440/2245 train_time:87936ms step_avg:61.07ms
step:1441/2245 train_time:87998ms step_avg:61.07ms
step:1442/2245 train_time:88059ms step_avg:61.07ms
step:1443/2245 train_time:88121ms step_avg:61.07ms
step:1444/2245 train_time:88181ms step_avg:61.07ms
step:1445/2245 train_time:88245ms step_avg:61.07ms
step:1446/2245 train_time:88305ms step_avg:61.07ms
step:1447/2245 train_time:88367ms step_avg:61.07ms
step:1448/2245 train_time:88427ms step_avg:61.07ms
step:1449/2245 train_time:88489ms step_avg:61.07ms
step:1450/2245 train_time:88548ms step_avg:61.07ms
step:1451/2245 train_time:88610ms step_avg:61.07ms
step:1452/2245 train_time:88670ms step_avg:61.07ms
step:1453/2245 train_time:88732ms step_avg:61.07ms
step:1454/2245 train_time:88792ms step_avg:61.07ms
step:1455/2245 train_time:88855ms step_avg:61.07ms
step:1456/2245 train_time:88915ms step_avg:61.07ms
step:1457/2245 train_time:88977ms step_avg:61.07ms
step:1458/2245 train_time:89037ms step_avg:61.07ms
step:1459/2245 train_time:89100ms step_avg:61.07ms
step:1460/2245 train_time:89160ms step_avg:61.07ms
step:1461/2245 train_time:89222ms step_avg:61.07ms
step:1462/2245 train_time:89282ms step_avg:61.07ms
step:1463/2245 train_time:89345ms step_avg:61.07ms
step:1464/2245 train_time:89405ms step_avg:61.07ms
step:1465/2245 train_time:89468ms step_avg:61.07ms
step:1466/2245 train_time:89527ms step_avg:61.07ms
step:1467/2245 train_time:89590ms step_avg:61.07ms
step:1468/2245 train_time:89650ms step_avg:61.07ms
step:1469/2245 train_time:89712ms step_avg:61.07ms
step:1470/2245 train_time:89772ms step_avg:61.07ms
step:1471/2245 train_time:89834ms step_avg:61.07ms
step:1472/2245 train_time:89895ms step_avg:61.07ms
step:1473/2245 train_time:89958ms step_avg:61.07ms
step:1474/2245 train_time:90018ms step_avg:61.07ms
step:1475/2245 train_time:90080ms step_avg:61.07ms
step:1476/2245 train_time:90142ms step_avg:61.07ms
step:1477/2245 train_time:90205ms step_avg:61.07ms
step:1478/2245 train_time:90266ms step_avg:61.07ms
step:1479/2245 train_time:90329ms step_avg:61.07ms
step:1480/2245 train_time:90389ms step_avg:61.07ms
step:1481/2245 train_time:90452ms step_avg:61.07ms
step:1482/2245 train_time:90512ms step_avg:61.07ms
step:1483/2245 train_time:90576ms step_avg:61.08ms
step:1484/2245 train_time:90636ms step_avg:61.08ms
step:1485/2245 train_time:90699ms step_avg:61.08ms
step:1486/2245 train_time:90759ms step_avg:61.08ms
step:1487/2245 train_time:90823ms step_avg:61.08ms
step:1488/2245 train_time:90883ms step_avg:61.08ms
step:1489/2245 train_time:90946ms step_avg:61.08ms
step:1490/2245 train_time:91006ms step_avg:61.08ms
step:1491/2245 train_time:91069ms step_avg:61.08ms
step:1492/2245 train_time:91130ms step_avg:61.08ms
step:1493/2245 train_time:91193ms step_avg:61.08ms
step:1494/2245 train_time:91254ms step_avg:61.08ms
step:1495/2245 train_time:91317ms step_avg:61.08ms
step:1496/2245 train_time:91378ms step_avg:61.08ms
step:1497/2245 train_time:91442ms step_avg:61.08ms
step:1498/2245 train_time:91502ms step_avg:61.08ms
step:1499/2245 train_time:91565ms step_avg:61.08ms
step:1500/2245 train_time:91626ms step_avg:61.08ms
step:1500/2245 val_loss:3.4449 train_time:91689ms step_avg:61.13ms
step:1501/2245 train_time:91709ms step_avg:61.10ms
step:1502/2245 train_time:91749ms step_avg:61.08ms
step:1503/2245 train_time:91812ms step_avg:61.09ms
step:1504/2245 train_time:91873ms step_avg:61.09ms
step:1505/2245 train_time:91937ms step_avg:61.09ms
step:1506/2245 train_time:91997ms step_avg:61.09ms
step:1507/2245 train_time:92059ms step_avg:61.09ms
step:1508/2245 train_time:92118ms step_avg:61.09ms
step:1509/2245 train_time:92180ms step_avg:61.09ms
step:1510/2245 train_time:92240ms step_avg:61.09ms
step:1511/2245 train_time:92302ms step_avg:61.09ms
step:1512/2245 train_time:92361ms step_avg:61.09ms
step:1513/2245 train_time:92424ms step_avg:61.09ms
step:1514/2245 train_time:92484ms step_avg:61.09ms
step:1515/2245 train_time:92547ms step_avg:61.09ms
step:1516/2245 train_time:92609ms step_avg:61.09ms
step:1517/2245 train_time:92674ms step_avg:61.09ms
step:1518/2245 train_time:92735ms step_avg:61.09ms
step:1519/2245 train_time:92799ms step_avg:61.09ms
step:1520/2245 train_time:92860ms step_avg:61.09ms
step:1521/2245 train_time:92923ms step_avg:61.09ms
step:1522/2245 train_time:92984ms step_avg:61.09ms
step:1523/2245 train_time:93047ms step_avg:61.09ms
step:1524/2245 train_time:93108ms step_avg:61.09ms
step:1525/2245 train_time:93171ms step_avg:61.10ms
step:1526/2245 train_time:93230ms step_avg:61.09ms
step:1527/2245 train_time:93293ms step_avg:61.10ms
step:1528/2245 train_time:93353ms step_avg:61.09ms
step:1529/2245 train_time:93416ms step_avg:61.10ms
step:1530/2245 train_time:93476ms step_avg:61.10ms
step:1531/2245 train_time:93538ms step_avg:61.10ms
step:1532/2245 train_time:93599ms step_avg:61.10ms
step:1533/2245 train_time:93663ms step_avg:61.10ms
step:1534/2245 train_time:93724ms step_avg:61.10ms
step:1535/2245 train_time:93787ms step_avg:61.10ms
step:1536/2245 train_time:93849ms step_avg:61.10ms
step:1537/2245 train_time:93912ms step_avg:61.10ms
step:1538/2245 train_time:93972ms step_avg:61.10ms
step:1539/2245 train_time:94035ms step_avg:61.10ms
step:1540/2245 train_time:94096ms step_avg:61.10ms
step:1541/2245 train_time:94158ms step_avg:61.10ms
step:1542/2245 train_time:94218ms step_avg:61.10ms
step:1543/2245 train_time:94280ms step_avg:61.10ms
step:1544/2245 train_time:94340ms step_avg:61.10ms
step:1545/2245 train_time:94403ms step_avg:61.10ms
step:1546/2245 train_time:94463ms step_avg:61.10ms
step:1547/2245 train_time:94526ms step_avg:61.10ms
step:1548/2245 train_time:94587ms step_avg:61.10ms
step:1549/2245 train_time:94650ms step_avg:61.10ms
step:1550/2245 train_time:94710ms step_avg:61.10ms
step:1551/2245 train_time:94774ms step_avg:61.11ms
step:1552/2245 train_time:94834ms step_avg:61.10ms
step:1553/2245 train_time:94898ms step_avg:61.11ms
step:1554/2245 train_time:94958ms step_avg:61.11ms
step:1555/2245 train_time:95022ms step_avg:61.11ms
step:1556/2245 train_time:95083ms step_avg:61.11ms
step:1557/2245 train_time:95145ms step_avg:61.11ms
step:1558/2245 train_time:95206ms step_avg:61.11ms
step:1559/2245 train_time:95269ms step_avg:61.11ms
step:1560/2245 train_time:95329ms step_avg:61.11ms
step:1561/2245 train_time:95392ms step_avg:61.11ms
step:1562/2245 train_time:95452ms step_avg:61.11ms
step:1563/2245 train_time:95515ms step_avg:61.11ms
step:1564/2245 train_time:95575ms step_avg:61.11ms
step:1565/2245 train_time:95638ms step_avg:61.11ms
step:1566/2245 train_time:95698ms step_avg:61.11ms
step:1567/2245 train_time:95761ms step_avg:61.11ms
step:1568/2245 train_time:95821ms step_avg:61.11ms
step:1569/2245 train_time:95885ms step_avg:61.11ms
step:1570/2245 train_time:95946ms step_avg:61.11ms
step:1571/2245 train_time:96010ms step_avg:61.11ms
step:1572/2245 train_time:96071ms step_avg:61.11ms
step:1573/2245 train_time:96134ms step_avg:61.11ms
step:1574/2245 train_time:96194ms step_avg:61.11ms
step:1575/2245 train_time:96257ms step_avg:61.12ms
step:1576/2245 train_time:96317ms step_avg:61.12ms
step:1577/2245 train_time:96380ms step_avg:61.12ms
step:1578/2245 train_time:96440ms step_avg:61.12ms
step:1579/2245 train_time:96503ms step_avg:61.12ms
step:1580/2245 train_time:96564ms step_avg:61.12ms
step:1581/2245 train_time:96627ms step_avg:61.12ms
step:1582/2245 train_time:96688ms step_avg:61.12ms
step:1583/2245 train_time:96751ms step_avg:61.12ms
step:1584/2245 train_time:96812ms step_avg:61.12ms
step:1585/2245 train_time:96874ms step_avg:61.12ms
step:1586/2245 train_time:96934ms step_avg:61.12ms
step:1587/2245 train_time:96998ms step_avg:61.12ms
step:1588/2245 train_time:97058ms step_avg:61.12ms
step:1589/2245 train_time:97120ms step_avg:61.12ms
step:1590/2245 train_time:97180ms step_avg:61.12ms
step:1591/2245 train_time:97243ms step_avg:61.12ms
step:1592/2245 train_time:97304ms step_avg:61.12ms
step:1593/2245 train_time:97368ms step_avg:61.12ms
step:1594/2245 train_time:97429ms step_avg:61.12ms
step:1595/2245 train_time:97492ms step_avg:61.12ms
step:1596/2245 train_time:97552ms step_avg:61.12ms
step:1597/2245 train_time:97615ms step_avg:61.12ms
step:1598/2245 train_time:97675ms step_avg:61.12ms
step:1599/2245 train_time:97738ms step_avg:61.12ms
step:1600/2245 train_time:97799ms step_avg:61.12ms
step:1601/2245 train_time:97861ms step_avg:61.13ms
step:1602/2245 train_time:97922ms step_avg:61.12ms
step:1603/2245 train_time:97985ms step_avg:61.13ms
step:1604/2245 train_time:98047ms step_avg:61.13ms
step:1605/2245 train_time:98109ms step_avg:61.13ms
step:1606/2245 train_time:98170ms step_avg:61.13ms
step:1607/2245 train_time:98233ms step_avg:61.13ms
step:1608/2245 train_time:98293ms step_avg:61.13ms
step:1609/2245 train_time:98356ms step_avg:61.13ms
step:1610/2245 train_time:98416ms step_avg:61.13ms
step:1611/2245 train_time:98479ms step_avg:61.13ms
step:1612/2245 train_time:98539ms step_avg:61.13ms
step:1613/2245 train_time:98602ms step_avg:61.13ms
step:1614/2245 train_time:98662ms step_avg:61.13ms
step:1615/2245 train_time:98725ms step_avg:61.13ms
step:1616/2245 train_time:98785ms step_avg:61.13ms
step:1617/2245 train_time:98848ms step_avg:61.13ms
step:1618/2245 train_time:98909ms step_avg:61.13ms
step:1619/2245 train_time:98972ms step_avg:61.13ms
step:1620/2245 train_time:99033ms step_avg:61.13ms
step:1621/2245 train_time:99095ms step_avg:61.13ms
step:1622/2245 train_time:99155ms step_avg:61.13ms
step:1623/2245 train_time:99218ms step_avg:61.13ms
step:1624/2245 train_time:99279ms step_avg:61.13ms
step:1625/2245 train_time:99341ms step_avg:61.13ms
step:1626/2245 train_time:99402ms step_avg:61.13ms
step:1627/2245 train_time:99465ms step_avg:61.13ms
step:1628/2245 train_time:99526ms step_avg:61.13ms
step:1629/2245 train_time:99590ms step_avg:61.14ms
step:1630/2245 train_time:99650ms step_avg:61.14ms
step:1631/2245 train_time:99713ms step_avg:61.14ms
step:1632/2245 train_time:99773ms step_avg:61.14ms
step:1633/2245 train_time:99835ms step_avg:61.14ms
step:1634/2245 train_time:99895ms step_avg:61.14ms
step:1635/2245 train_time:99958ms step_avg:61.14ms
step:1636/2245 train_time:100018ms step_avg:61.14ms
step:1637/2245 train_time:100081ms step_avg:61.14ms
step:1638/2245 train_time:100141ms step_avg:61.14ms
step:1639/2245 train_time:100204ms step_avg:61.14ms
step:1640/2245 train_time:100265ms step_avg:61.14ms
step:1641/2245 train_time:100328ms step_avg:61.14ms
step:1642/2245 train_time:100389ms step_avg:61.14ms
step:1643/2245 train_time:100452ms step_avg:61.14ms
step:1644/2245 train_time:100513ms step_avg:61.14ms
step:1645/2245 train_time:100575ms step_avg:61.14ms
step:1646/2245 train_time:100635ms step_avg:61.14ms
step:1647/2245 train_time:100698ms step_avg:61.14ms
step:1648/2245 train_time:100758ms step_avg:61.14ms
step:1649/2245 train_time:100821ms step_avg:61.14ms
step:1650/2245 train_time:100881ms step_avg:61.14ms
step:1651/2245 train_time:100945ms step_avg:61.14ms
step:1652/2245 train_time:101006ms step_avg:61.14ms
step:1653/2245 train_time:101069ms step_avg:61.14ms
step:1654/2245 train_time:101130ms step_avg:61.14ms
step:1655/2245 train_time:101193ms step_avg:61.14ms
step:1656/2245 train_time:101253ms step_avg:61.14ms
step:1657/2245 train_time:101315ms step_avg:61.14ms
step:1658/2245 train_time:101376ms step_avg:61.14ms
step:1659/2245 train_time:101438ms step_avg:61.14ms
step:1660/2245 train_time:101499ms step_avg:61.14ms
step:1661/2245 train_time:101561ms step_avg:61.14ms
step:1662/2245 train_time:101622ms step_avg:61.14ms
step:1663/2245 train_time:101684ms step_avg:61.15ms
step:1664/2245 train_time:101745ms step_avg:61.14ms
step:1665/2245 train_time:101808ms step_avg:61.15ms
step:1666/2245 train_time:101868ms step_avg:61.15ms
step:1667/2245 train_time:101932ms step_avg:61.15ms
step:1668/2245 train_time:101993ms step_avg:61.15ms
step:1669/2245 train_time:102055ms step_avg:61.15ms
step:1670/2245 train_time:102115ms step_avg:61.15ms
step:1671/2245 train_time:102178ms step_avg:61.15ms
step:1672/2245 train_time:102238ms step_avg:61.15ms
step:1673/2245 train_time:102301ms step_avg:61.15ms
step:1674/2245 train_time:102361ms step_avg:61.15ms
step:1675/2245 train_time:102425ms step_avg:61.15ms
step:1676/2245 train_time:102485ms step_avg:61.15ms
step:1677/2245 train_time:102548ms step_avg:61.15ms
step:1678/2245 train_time:102609ms step_avg:61.15ms
step:1679/2245 train_time:102672ms step_avg:61.15ms
step:1680/2245 train_time:102732ms step_avg:61.15ms
step:1681/2245 train_time:102795ms step_avg:61.15ms
step:1682/2245 train_time:102856ms step_avg:61.15ms
step:1683/2245 train_time:102919ms step_avg:61.15ms
step:1684/2245 train_time:102980ms step_avg:61.15ms
step:1685/2245 train_time:103044ms step_avg:61.15ms
step:1686/2245 train_time:103104ms step_avg:61.15ms
step:1687/2245 train_time:103168ms step_avg:61.15ms
step:1688/2245 train_time:103230ms step_avg:61.15ms
step:1689/2245 train_time:103293ms step_avg:61.16ms
step:1690/2245 train_time:103353ms step_avg:61.16ms
step:1691/2245 train_time:103416ms step_avg:61.16ms
step:1692/2245 train_time:103476ms step_avg:61.16ms
step:1693/2245 train_time:103539ms step_avg:61.16ms
step:1694/2245 train_time:103599ms step_avg:61.16ms
step:1695/2245 train_time:103663ms step_avg:61.16ms
step:1696/2245 train_time:103723ms step_avg:61.16ms
step:1697/2245 train_time:103786ms step_avg:61.16ms
step:1698/2245 train_time:103846ms step_avg:61.16ms
step:1699/2245 train_time:103911ms step_avg:61.16ms
step:1700/2245 train_time:103972ms step_avg:61.16ms
step:1701/2245 train_time:104035ms step_avg:61.16ms
step:1702/2245 train_time:104096ms step_avg:61.16ms
step:1703/2245 train_time:104159ms step_avg:61.16ms
step:1704/2245 train_time:104219ms step_avg:61.16ms
step:1705/2245 train_time:104282ms step_avg:61.16ms
step:1706/2245 train_time:104344ms step_avg:61.16ms
step:1707/2245 train_time:104407ms step_avg:61.16ms
step:1708/2245 train_time:104469ms step_avg:61.16ms
step:1709/2245 train_time:104533ms step_avg:61.17ms
step:1710/2245 train_time:104593ms step_avg:61.17ms
step:1711/2245 train_time:104656ms step_avg:61.17ms
step:1712/2245 train_time:104716ms step_avg:61.17ms
step:1713/2245 train_time:104779ms step_avg:61.17ms
step:1714/2245 train_time:104839ms step_avg:61.17ms
step:1715/2245 train_time:104903ms step_avg:61.17ms
step:1716/2245 train_time:104963ms step_avg:61.17ms
step:1717/2245 train_time:105026ms step_avg:61.17ms
step:1718/2245 train_time:105087ms step_avg:61.17ms
step:1719/2245 train_time:105150ms step_avg:61.17ms
step:1720/2245 train_time:105211ms step_avg:61.17ms
step:1721/2245 train_time:105274ms step_avg:61.17ms
step:1722/2245 train_time:105334ms step_avg:61.17ms
step:1723/2245 train_time:105397ms step_avg:61.17ms
step:1724/2245 train_time:105457ms step_avg:61.17ms
step:1725/2245 train_time:105520ms step_avg:61.17ms
step:1726/2245 train_time:105580ms step_avg:61.17ms
step:1727/2245 train_time:105643ms step_avg:61.17ms
step:1728/2245 train_time:105703ms step_avg:61.17ms
step:1729/2245 train_time:105766ms step_avg:61.17ms
step:1730/2245 train_time:105827ms step_avg:61.17ms
step:1731/2245 train_time:105890ms step_avg:61.17ms
step:1732/2245 train_time:105950ms step_avg:61.17ms
step:1733/2245 train_time:106012ms step_avg:61.17ms
step:1734/2245 train_time:106073ms step_avg:61.17ms
step:1735/2245 train_time:106135ms step_avg:61.17ms
step:1736/2245 train_time:106195ms step_avg:61.17ms
step:1737/2245 train_time:106258ms step_avg:61.17ms
step:1738/2245 train_time:106318ms step_avg:61.17ms
step:1739/2245 train_time:106381ms step_avg:61.17ms
step:1740/2245 train_time:106442ms step_avg:61.17ms
step:1741/2245 train_time:106506ms step_avg:61.17ms
step:1742/2245 train_time:106567ms step_avg:61.18ms
step:1743/2245 train_time:106629ms step_avg:61.18ms
step:1744/2245 train_time:106690ms step_avg:61.18ms
step:1745/2245 train_time:106753ms step_avg:61.18ms
step:1746/2245 train_time:106813ms step_avg:61.18ms
step:1747/2245 train_time:106876ms step_avg:61.18ms
step:1748/2245 train_time:106937ms step_avg:61.18ms
step:1749/2245 train_time:107000ms step_avg:61.18ms
step:1750/2245 train_time:107060ms step_avg:61.18ms
step:1750/2245 val_loss:3.3807 train_time:107124ms step_avg:61.21ms
step:1751/2245 train_time:107143ms step_avg:61.19ms
step:1752/2245 train_time:107187ms step_avg:61.18ms
step:1753/2245 train_time:107253ms step_avg:61.18ms
step:1754/2245 train_time:107315ms step_avg:61.18ms
step:1755/2245 train_time:107378ms step_avg:61.18ms
step:1756/2245 train_time:107438ms step_avg:61.18ms
step:1757/2245 train_time:107500ms step_avg:61.18ms
step:1758/2245 train_time:107560ms step_avg:61.18ms
step:1759/2245 train_time:107622ms step_avg:61.18ms
step:1760/2245 train_time:107682ms step_avg:61.18ms
step:1761/2245 train_time:107744ms step_avg:61.18ms
step:1762/2245 train_time:107805ms step_avg:61.18ms
step:1763/2245 train_time:107867ms step_avg:61.18ms
step:1764/2245 train_time:107927ms step_avg:61.18ms
step:1765/2245 train_time:107990ms step_avg:61.18ms
step:1766/2245 train_time:108051ms step_avg:61.18ms
step:1767/2245 train_time:108114ms step_avg:61.19ms
step:1768/2245 train_time:108176ms step_avg:61.19ms
step:1769/2245 train_time:108240ms step_avg:61.19ms
step:1770/2245 train_time:108302ms step_avg:61.19ms
step:1771/2245 train_time:108365ms step_avg:61.19ms
step:1772/2245 train_time:108425ms step_avg:61.19ms
step:1773/2245 train_time:108489ms step_avg:61.19ms
step:1774/2245 train_time:108549ms step_avg:61.19ms
step:1775/2245 train_time:108612ms step_avg:61.19ms
step:1776/2245 train_time:108672ms step_avg:61.19ms
step:1777/2245 train_time:108735ms step_avg:61.19ms
step:1778/2245 train_time:108795ms step_avg:61.19ms
step:1779/2245 train_time:108858ms step_avg:61.19ms
step:1780/2245 train_time:108917ms step_avg:61.19ms
step:1781/2245 train_time:108979ms step_avg:61.19ms
step:1782/2245 train_time:109040ms step_avg:61.19ms
step:1783/2245 train_time:109103ms step_avg:61.19ms
step:1784/2245 train_time:109164ms step_avg:61.19ms
step:1785/2245 train_time:109228ms step_avg:61.19ms
step:1786/2245 train_time:109290ms step_avg:61.19ms
step:1787/2245 train_time:109353ms step_avg:61.19ms
step:1788/2245 train_time:109414ms step_avg:61.19ms
step:1789/2245 train_time:109476ms step_avg:61.19ms
step:1790/2245 train_time:109536ms step_avg:61.19ms
step:1791/2245 train_time:109598ms step_avg:61.19ms
step:1792/2245 train_time:109658ms step_avg:61.19ms
step:1793/2245 train_time:109721ms step_avg:61.19ms
step:1794/2245 train_time:109781ms step_avg:61.19ms
step:1795/2245 train_time:109843ms step_avg:61.19ms
step:1796/2245 train_time:109904ms step_avg:61.19ms
step:1797/2245 train_time:109966ms step_avg:61.19ms
step:1798/2245 train_time:110027ms step_avg:61.19ms
step:1799/2245 train_time:110090ms step_avg:61.20ms
step:1800/2245 train_time:110152ms step_avg:61.20ms
step:1801/2245 train_time:110215ms step_avg:61.20ms
step:1802/2245 train_time:110276ms step_avg:61.20ms
step:1803/2245 train_time:110339ms step_avg:61.20ms
step:1804/2245 train_time:110400ms step_avg:61.20ms
step:1805/2245 train_time:110463ms step_avg:61.20ms
step:1806/2245 train_time:110523ms step_avg:61.20ms
step:1807/2245 train_time:110586ms step_avg:61.20ms
step:1808/2245 train_time:110646ms step_avg:61.20ms
step:1809/2245 train_time:110709ms step_avg:61.20ms
step:1810/2245 train_time:110769ms step_avg:61.20ms
step:1811/2245 train_time:110832ms step_avg:61.20ms
step:1812/2245 train_time:110893ms step_avg:61.20ms
step:1813/2245 train_time:110955ms step_avg:61.20ms
step:1814/2245 train_time:111015ms step_avg:61.20ms
step:1815/2245 train_time:111078ms step_avg:61.20ms
step:1816/2245 train_time:111139ms step_avg:61.20ms
step:1817/2245 train_time:111202ms step_avg:61.20ms
step:1818/2245 train_time:111262ms step_avg:61.20ms
step:1819/2245 train_time:111326ms step_avg:61.20ms
step:1820/2245 train_time:111387ms step_avg:61.20ms
step:1821/2245 train_time:111451ms step_avg:61.20ms
step:1822/2245 train_time:111513ms step_avg:61.20ms
step:1823/2245 train_time:111576ms step_avg:61.20ms
step:1824/2245 train_time:111636ms step_avg:61.20ms
step:1825/2245 train_time:111699ms step_avg:61.20ms
step:1826/2245 train_time:111758ms step_avg:61.20ms
step:1827/2245 train_time:111821ms step_avg:61.20ms
step:1828/2245 train_time:111882ms step_avg:61.20ms
step:1829/2245 train_time:111944ms step_avg:61.21ms
step:1830/2245 train_time:112004ms step_avg:61.20ms
step:1831/2245 train_time:112067ms step_avg:61.21ms
step:1832/2245 train_time:112128ms step_avg:61.21ms
step:1833/2245 train_time:112191ms step_avg:61.21ms
step:1834/2245 train_time:112252ms step_avg:61.21ms
step:1835/2245 train_time:112316ms step_avg:61.21ms
step:1836/2245 train_time:112377ms step_avg:61.21ms
step:1837/2245 train_time:112439ms step_avg:61.21ms
step:1838/2245 train_time:112500ms step_avg:61.21ms
step:1839/2245 train_time:112562ms step_avg:61.21ms
step:1840/2245 train_time:112623ms step_avg:61.21ms
step:1841/2245 train_time:112686ms step_avg:61.21ms
step:1842/2245 train_time:112746ms step_avg:61.21ms
step:1843/2245 train_time:112810ms step_avg:61.21ms
step:1844/2245 train_time:112870ms step_avg:61.21ms
step:1845/2245 train_time:112933ms step_avg:61.21ms
step:1846/2245 train_time:112994ms step_avg:61.21ms
step:1847/2245 train_time:113056ms step_avg:61.21ms
step:1848/2245 train_time:113116ms step_avg:61.21ms
step:1849/2245 train_time:113179ms step_avg:61.21ms
step:1850/2245 train_time:113240ms step_avg:61.21ms
step:1851/2245 train_time:113304ms step_avg:61.21ms
step:1852/2245 train_time:113364ms step_avg:61.21ms
step:1853/2245 train_time:113427ms step_avg:61.21ms
step:1854/2245 train_time:113488ms step_avg:61.21ms
step:1855/2245 train_time:113551ms step_avg:61.21ms
step:1856/2245 train_time:113611ms step_avg:61.21ms
step:1857/2245 train_time:113674ms step_avg:61.21ms
step:1858/2245 train_time:113734ms step_avg:61.21ms
step:1859/2245 train_time:113797ms step_avg:61.21ms
step:1860/2245 train_time:113856ms step_avg:61.21ms
step:1861/2245 train_time:113919ms step_avg:61.21ms
step:1862/2245 train_time:113979ms step_avg:61.21ms
step:1863/2245 train_time:114041ms step_avg:61.21ms
step:1864/2245 train_time:114102ms step_avg:61.21ms
step:1865/2245 train_time:114165ms step_avg:61.21ms
step:1866/2245 train_time:114225ms step_avg:61.21ms
step:1867/2245 train_time:114288ms step_avg:61.21ms
step:1868/2245 train_time:114349ms step_avg:61.21ms
step:1869/2245 train_time:114412ms step_avg:61.22ms
step:1870/2245 train_time:114474ms step_avg:61.22ms
step:1871/2245 train_time:114537ms step_avg:61.22ms
step:1872/2245 train_time:114598ms step_avg:61.22ms
step:1873/2245 train_time:114661ms step_avg:61.22ms
step:1874/2245 train_time:114721ms step_avg:61.22ms
step:1875/2245 train_time:114783ms step_avg:61.22ms
step:1876/2245 train_time:114844ms step_avg:61.22ms
step:1877/2245 train_time:114908ms step_avg:61.22ms
step:1878/2245 train_time:114968ms step_avg:61.22ms
step:1879/2245 train_time:115031ms step_avg:61.22ms
step:1880/2245 train_time:115093ms step_avg:61.22ms
step:1881/2245 train_time:115156ms step_avg:61.22ms
step:1882/2245 train_time:115216ms step_avg:61.22ms
step:1883/2245 train_time:115279ms step_avg:61.22ms
step:1884/2245 train_time:115339ms step_avg:61.22ms
step:1885/2245 train_time:115401ms step_avg:61.22ms
step:1886/2245 train_time:115461ms step_avg:61.22ms
step:1887/2245 train_time:115523ms step_avg:61.22ms
step:1888/2245 train_time:115584ms step_avg:61.22ms
step:1889/2245 train_time:115647ms step_avg:61.22ms
step:1890/2245 train_time:115708ms step_avg:61.22ms
step:1891/2245 train_time:115771ms step_avg:61.22ms
step:1892/2245 train_time:115833ms step_avg:61.22ms
step:1893/2245 train_time:115897ms step_avg:61.22ms
step:1894/2245 train_time:115956ms step_avg:61.22ms
step:1895/2245 train_time:116019ms step_avg:61.22ms
step:1896/2245 train_time:116080ms step_avg:61.22ms
step:1897/2245 train_time:116142ms step_avg:61.22ms
step:1898/2245 train_time:116203ms step_avg:61.22ms
step:1899/2245 train_time:116265ms step_avg:61.22ms
step:1900/2245 train_time:116326ms step_avg:61.22ms
step:1901/2245 train_time:116389ms step_avg:61.23ms
step:1902/2245 train_time:116450ms step_avg:61.22ms
step:1903/2245 train_time:116513ms step_avg:61.23ms
step:1904/2245 train_time:116573ms step_avg:61.23ms
step:1905/2245 train_time:116636ms step_avg:61.23ms
step:1906/2245 train_time:116696ms step_avg:61.23ms
step:1907/2245 train_time:116759ms step_avg:61.23ms
step:1908/2245 train_time:116819ms step_avg:61.23ms
step:1909/2245 train_time:116881ms step_avg:61.23ms
step:1910/2245 train_time:116942ms step_avg:61.23ms
step:1911/2245 train_time:117005ms step_avg:61.23ms
step:1912/2245 train_time:117066ms step_avg:61.23ms
step:1913/2245 train_time:117129ms step_avg:61.23ms
step:1914/2245 train_time:117190ms step_avg:61.23ms
step:1915/2245 train_time:117253ms step_avg:61.23ms
step:1916/2245 train_time:117314ms step_avg:61.23ms
step:1917/2245 train_time:117377ms step_avg:61.23ms
step:1918/2245 train_time:117437ms step_avg:61.23ms
step:1919/2245 train_time:117499ms step_avg:61.23ms
step:1920/2245 train_time:117560ms step_avg:61.23ms
step:1921/2245 train_time:117622ms step_avg:61.23ms
step:1922/2245 train_time:117682ms step_avg:61.23ms
step:1923/2245 train_time:117746ms step_avg:61.23ms
step:1924/2245 train_time:117806ms step_avg:61.23ms
step:1925/2245 train_time:117869ms step_avg:61.23ms
step:1926/2245 train_time:117929ms step_avg:61.23ms
step:1927/2245 train_time:117993ms step_avg:61.23ms
step:1928/2245 train_time:118054ms step_avg:61.23ms
step:1929/2245 train_time:118117ms step_avg:61.23ms
step:1930/2245 train_time:118178ms step_avg:61.23ms
step:1931/2245 train_time:118240ms step_avg:61.23ms
step:1932/2245 train_time:118301ms step_avg:61.23ms
step:1933/2245 train_time:118363ms step_avg:61.23ms
step:1934/2245 train_time:118423ms step_avg:61.23ms
step:1935/2245 train_time:118486ms step_avg:61.23ms
step:1936/2245 train_time:118546ms step_avg:61.23ms
step:1937/2245 train_time:118609ms step_avg:61.23ms
step:1938/2245 train_time:118670ms step_avg:61.23ms
step:1939/2245 train_time:118732ms step_avg:61.23ms
step:1940/2245 train_time:118793ms step_avg:61.23ms
step:1941/2245 train_time:118857ms step_avg:61.23ms
step:1942/2245 train_time:118916ms step_avg:61.23ms
step:1943/2245 train_time:118979ms step_avg:61.23ms
step:1944/2245 train_time:119039ms step_avg:61.23ms
step:1945/2245 train_time:119102ms step_avg:61.24ms
step:1946/2245 train_time:119163ms step_avg:61.23ms
step:1947/2245 train_time:119225ms step_avg:61.24ms
step:1948/2245 train_time:119287ms step_avg:61.24ms
step:1949/2245 train_time:119350ms step_avg:61.24ms
step:1950/2245 train_time:119410ms step_avg:61.24ms
step:1951/2245 train_time:119472ms step_avg:61.24ms
step:1952/2245 train_time:119533ms step_avg:61.24ms
step:1953/2245 train_time:119596ms step_avg:61.24ms
step:1954/2245 train_time:119657ms step_avg:61.24ms
step:1955/2245 train_time:119719ms step_avg:61.24ms
step:1956/2245 train_time:119779ms step_avg:61.24ms
step:1957/2245 train_time:119842ms step_avg:61.24ms
step:1958/2245 train_time:119903ms step_avg:61.24ms
step:1959/2245 train_time:119965ms step_avg:61.24ms
step:1960/2245 train_time:120026ms step_avg:61.24ms
step:1961/2245 train_time:120088ms step_avg:61.24ms
step:1962/2245 train_time:120149ms step_avg:61.24ms
step:1963/2245 train_time:120212ms step_avg:61.24ms
step:1964/2245 train_time:120273ms step_avg:61.24ms
step:1965/2245 train_time:120336ms step_avg:61.24ms
step:1966/2245 train_time:120397ms step_avg:61.24ms
step:1967/2245 train_time:120460ms step_avg:61.24ms
step:1968/2245 train_time:120520ms step_avg:61.24ms
step:1969/2245 train_time:120582ms step_avg:61.24ms
step:1970/2245 train_time:120643ms step_avg:61.24ms
step:1971/2245 train_time:120705ms step_avg:61.24ms
step:1972/2245 train_time:120766ms step_avg:61.24ms
step:1973/2245 train_time:120829ms step_avg:61.24ms
step:1974/2245 train_time:120890ms step_avg:61.24ms
step:1975/2245 train_time:120953ms step_avg:61.24ms
step:1976/2245 train_time:121014ms step_avg:61.24ms
step:1977/2245 train_time:121077ms step_avg:61.24ms
step:1978/2245 train_time:121137ms step_avg:61.24ms
step:1979/2245 train_time:121200ms step_avg:61.24ms
step:1980/2245 train_time:121261ms step_avg:61.24ms
step:1981/2245 train_time:121323ms step_avg:61.24ms
step:1982/2245 train_time:121384ms step_avg:61.24ms
step:1983/2245 train_time:121447ms step_avg:61.24ms
step:1984/2245 train_time:121508ms step_avg:61.24ms
step:1985/2245 train_time:121571ms step_avg:61.24ms
step:1986/2245 train_time:121632ms step_avg:61.24ms
step:1987/2245 train_time:121694ms step_avg:61.25ms
step:1988/2245 train_time:121755ms step_avg:61.24ms
step:1989/2245 train_time:121818ms step_avg:61.25ms
step:1990/2245 train_time:121878ms step_avg:61.25ms
step:1991/2245 train_time:121941ms step_avg:61.25ms
step:1992/2245 train_time:122001ms step_avg:61.25ms
step:1993/2245 train_time:122064ms step_avg:61.25ms
step:1994/2245 train_time:122124ms step_avg:61.25ms
step:1995/2245 train_time:122187ms step_avg:61.25ms
step:1996/2245 train_time:122248ms step_avg:61.25ms
step:1997/2245 train_time:122311ms step_avg:61.25ms
step:1998/2245 train_time:122371ms step_avg:61.25ms
step:1999/2245 train_time:122435ms step_avg:61.25ms
step:2000/2245 train_time:122496ms step_avg:61.25ms
step:2000/2245 val_loss:3.3265 train_time:122559ms step_avg:61.28ms
step:2001/2245 train_time:122577ms step_avg:61.26ms
step:2002/2245 train_time:122624ms step_avg:61.25ms
step:2003/2245 train_time:122690ms step_avg:61.25ms
step:2004/2245 train_time:122751ms step_avg:61.25ms
step:2005/2245 train_time:122814ms step_avg:61.25ms
step:2006/2245 train_time:122874ms step_avg:61.25ms
step:2007/2245 train_time:122936ms step_avg:61.25ms
step:2008/2245 train_time:122996ms step_avg:61.25ms
step:2009/2245 train_time:123058ms step_avg:61.25ms
step:2010/2245 train_time:123117ms step_avg:61.25ms
step:2011/2245 train_time:123180ms step_avg:61.25ms
step:2012/2245 train_time:123240ms step_avg:61.25ms
step:2013/2245 train_time:123302ms step_avg:61.25ms
step:2014/2245 train_time:123362ms step_avg:61.25ms
step:2015/2245 train_time:123424ms step_avg:61.25ms
step:2016/2245 train_time:123485ms step_avg:61.25ms
step:2017/2245 train_time:123549ms step_avg:61.25ms
step:2018/2245 train_time:123611ms step_avg:61.25ms
step:2019/2245 train_time:123675ms step_avg:61.26ms
step:2020/2245 train_time:123737ms step_avg:61.26ms
step:2021/2245 train_time:123800ms step_avg:61.26ms
step:2022/2245 train_time:123861ms step_avg:61.26ms
step:2023/2245 train_time:123924ms step_avg:61.26ms
step:2024/2245 train_time:123983ms step_avg:61.26ms
step:2025/2245 train_time:124046ms step_avg:61.26ms
step:2026/2245 train_time:124106ms step_avg:61.26ms
step:2027/2245 train_time:124168ms step_avg:61.26ms
step:2028/2245 train_time:124228ms step_avg:61.26ms
step:2029/2245 train_time:124290ms step_avg:61.26ms
step:2030/2245 train_time:124350ms step_avg:61.26ms
step:2031/2245 train_time:124414ms step_avg:61.26ms
step:2032/2245 train_time:124474ms step_avg:61.26ms
step:2033/2245 train_time:124538ms step_avg:61.26ms
step:2034/2245 train_time:124600ms step_avg:61.26ms
step:2035/2245 train_time:124665ms step_avg:61.26ms
step:2036/2245 train_time:124726ms step_avg:61.26ms
step:2037/2245 train_time:124789ms step_avg:61.26ms
step:2038/2245 train_time:124849ms step_avg:61.26ms
step:2039/2245 train_time:124912ms step_avg:61.26ms
step:2040/2245 train_time:124972ms step_avg:61.26ms
step:2041/2245 train_time:125035ms step_avg:61.26ms
step:2042/2245 train_time:125095ms step_avg:61.26ms
step:2043/2245 train_time:125159ms step_avg:61.26ms
step:2044/2245 train_time:125220ms step_avg:61.26ms
step:2045/2245 train_time:125282ms step_avg:61.26ms
step:2046/2245 train_time:125343ms step_avg:61.26ms
step:2047/2245 train_time:125406ms step_avg:61.26ms
step:2048/2245 train_time:125466ms step_avg:61.26ms
step:2049/2245 train_time:125530ms step_avg:61.26ms
step:2050/2245 train_time:125590ms step_avg:61.26ms
step:2051/2245 train_time:125654ms step_avg:61.26ms
step:2052/2245 train_time:125715ms step_avg:61.26ms
step:2053/2245 train_time:125779ms step_avg:61.27ms
step:2054/2245 train_time:125840ms step_avg:61.27ms
step:2055/2245 train_time:125902ms step_avg:61.27ms
step:2056/2245 train_time:125963ms step_avg:61.27ms
step:2057/2245 train_time:126026ms step_avg:61.27ms
step:2058/2245 train_time:126085ms step_avg:61.27ms
step:2059/2245 train_time:126148ms step_avg:61.27ms
step:2060/2245 train_time:126208ms step_avg:61.27ms
step:2061/2245 train_time:126271ms step_avg:61.27ms
step:2062/2245 train_time:126331ms step_avg:61.27ms
step:2063/2245 train_time:126394ms step_avg:61.27ms
step:2064/2245 train_time:126456ms step_avg:61.27ms
step:2065/2245 train_time:126519ms step_avg:61.27ms
step:2066/2245 train_time:126581ms step_avg:61.27ms
step:2067/2245 train_time:126644ms step_avg:61.27ms
step:2068/2245 train_time:126704ms step_avg:61.27ms
step:2069/2245 train_time:126767ms step_avg:61.27ms
step:2070/2245 train_time:126828ms step_avg:61.27ms
step:2071/2245 train_time:126890ms step_avg:61.27ms
step:2072/2245 train_time:126951ms step_avg:61.27ms
step:2073/2245 train_time:127013ms step_avg:61.27ms
step:2074/2245 train_time:127074ms step_avg:61.27ms
step:2075/2245 train_time:127137ms step_avg:61.27ms
step:2076/2245 train_time:127198ms step_avg:61.27ms
step:2077/2245 train_time:127261ms step_avg:61.27ms
step:2078/2245 train_time:127321ms step_avg:61.27ms
step:2079/2245 train_time:127383ms step_avg:61.27ms
step:2080/2245 train_time:127444ms step_avg:61.27ms
step:2081/2245 train_time:127506ms step_avg:61.27ms
step:2082/2245 train_time:127567ms step_avg:61.27ms
step:2083/2245 train_time:127629ms step_avg:61.27ms
step:2084/2245 train_time:127690ms step_avg:61.27ms
step:2085/2245 train_time:127753ms step_avg:61.27ms
step:2086/2245 train_time:127814ms step_avg:61.27ms
step:2087/2245 train_time:127876ms step_avg:61.27ms
step:2088/2245 train_time:127937ms step_avg:61.27ms
step:2089/2245 train_time:128000ms step_avg:61.27ms
step:2090/2245 train_time:128061ms step_avg:61.27ms
step:2091/2245 train_time:128124ms step_avg:61.27ms
step:2092/2245 train_time:128184ms step_avg:61.27ms
step:2093/2245 train_time:128247ms step_avg:61.27ms
step:2094/2245 train_time:128307ms step_avg:61.27ms
step:2095/2245 train_time:128370ms step_avg:61.27ms
step:2096/2245 train_time:128430ms step_avg:61.27ms
step:2097/2245 train_time:128493ms step_avg:61.27ms
step:2098/2245 train_time:128554ms step_avg:61.27ms
step:2099/2245 train_time:128618ms step_avg:61.28ms
step:2100/2245 train_time:128679ms step_avg:61.28ms
step:2101/2245 train_time:128743ms step_avg:61.28ms
step:2102/2245 train_time:128803ms step_avg:61.28ms
step:2103/2245 train_time:128866ms step_avg:61.28ms
step:2104/2245 train_time:128927ms step_avg:61.28ms
step:2105/2245 train_time:128989ms step_avg:61.28ms
step:2106/2245 train_time:129050ms step_avg:61.28ms
step:2107/2245 train_time:129113ms step_avg:61.28ms
step:2108/2245 train_time:129174ms step_avg:61.28ms
step:2109/2245 train_time:129236ms step_avg:61.28ms
step:2110/2245 train_time:129297ms step_avg:61.28ms
step:2111/2245 train_time:129361ms step_avg:61.28ms
step:2112/2245 train_time:129422ms step_avg:61.28ms
step:2113/2245 train_time:129484ms step_avg:61.28ms
step:2114/2245 train_time:129545ms step_avg:61.28ms
step:2115/2245 train_time:129608ms step_avg:61.28ms
step:2116/2245 train_time:129669ms step_avg:61.28ms
step:2117/2245 train_time:129732ms step_avg:61.28ms
step:2118/2245 train_time:129793ms step_avg:61.28ms
step:2119/2245 train_time:129856ms step_avg:61.28ms
step:2120/2245 train_time:129917ms step_avg:61.28ms
step:2121/2245 train_time:129979ms step_avg:61.28ms
step:2122/2245 train_time:130040ms step_avg:61.28ms
step:2123/2245 train_time:130103ms step_avg:61.28ms
step:2124/2245 train_time:130163ms step_avg:61.28ms
step:2125/2245 train_time:130226ms step_avg:61.28ms
step:2126/2245 train_time:130286ms step_avg:61.28ms
step:2127/2245 train_time:130349ms step_avg:61.28ms
step:2128/2245 train_time:130409ms step_avg:61.28ms
step:2129/2245 train_time:130472ms step_avg:61.28ms
step:2130/2245 train_time:130533ms step_avg:61.28ms
step:2131/2245 train_time:130596ms step_avg:61.28ms
step:2132/2245 train_time:130658ms step_avg:61.28ms
step:2133/2245 train_time:130722ms step_avg:61.29ms
step:2134/2245 train_time:130782ms step_avg:61.28ms
step:2135/2245 train_time:130845ms step_avg:61.29ms
step:2136/2245 train_time:130905ms step_avg:61.29ms
step:2137/2245 train_time:130968ms step_avg:61.29ms
step:2138/2245 train_time:131028ms step_avg:61.29ms
step:2139/2245 train_time:131091ms step_avg:61.29ms
step:2140/2245 train_time:131152ms step_avg:61.29ms
step:2141/2245 train_time:131215ms step_avg:61.29ms
step:2142/2245 train_time:131275ms step_avg:61.29ms
step:2143/2245 train_time:131339ms step_avg:61.29ms
step:2144/2245 train_time:131400ms step_avg:61.29ms
step:2145/2245 train_time:131463ms step_avg:61.29ms
step:2146/2245 train_time:131524ms step_avg:61.29ms
step:2147/2245 train_time:131587ms step_avg:61.29ms
step:2148/2245 train_time:131648ms step_avg:61.29ms
step:2149/2245 train_time:131711ms step_avg:61.29ms
step:2150/2245 train_time:131771ms step_avg:61.29ms
step:2151/2245 train_time:131834ms step_avg:61.29ms
step:2152/2245 train_time:131895ms step_avg:61.29ms
step:2153/2245 train_time:131959ms step_avg:61.29ms
step:2154/2245 train_time:132020ms step_avg:61.29ms
step:2155/2245 train_time:132083ms step_avg:61.29ms
step:2156/2245 train_time:132144ms step_avg:61.29ms
step:2157/2245 train_time:132207ms step_avg:61.29ms
step:2158/2245 train_time:132267ms step_avg:61.29ms
step:2159/2245 train_time:132330ms step_avg:61.29ms
step:2160/2245 train_time:132390ms step_avg:61.29ms
step:2161/2245 train_time:132454ms step_avg:61.29ms
step:2162/2245 train_time:132514ms step_avg:61.29ms
step:2163/2245 train_time:132577ms step_avg:61.29ms
step:2164/2245 train_time:132638ms step_avg:61.29ms
step:2165/2245 train_time:132701ms step_avg:61.29ms
step:2166/2245 train_time:132762ms step_avg:61.29ms
step:2167/2245 train_time:132825ms step_avg:61.29ms
step:2168/2245 train_time:132885ms step_avg:61.29ms
step:2169/2245 train_time:132948ms step_avg:61.29ms
step:2170/2245 train_time:133008ms step_avg:61.29ms
step:2171/2245 train_time:133071ms step_avg:61.29ms
step:2172/2245 train_time:133131ms step_avg:61.29ms
step:2173/2245 train_time:133194ms step_avg:61.29ms
step:2174/2245 train_time:133255ms step_avg:61.29ms
step:2175/2245 train_time:133319ms step_avg:61.30ms
step:2176/2245 train_time:133380ms step_avg:61.30ms
step:2177/2245 train_time:133443ms step_avg:61.30ms
step:2178/2245 train_time:133503ms step_avg:61.30ms
step:2179/2245 train_time:133566ms step_avg:61.30ms
step:2180/2245 train_time:133627ms step_avg:61.30ms
step:2181/2245 train_time:133689ms step_avg:61.30ms
step:2182/2245 train_time:133750ms step_avg:61.30ms
step:2183/2245 train_time:133813ms step_avg:61.30ms
step:2184/2245 train_time:133873ms step_avg:61.30ms
step:2185/2245 train_time:133937ms step_avg:61.30ms
step:2186/2245 train_time:133997ms step_avg:61.30ms
step:2187/2245 train_time:134061ms step_avg:61.30ms
step:2188/2245 train_time:134122ms step_avg:61.30ms
step:2189/2245 train_time:134185ms step_avg:61.30ms
step:2190/2245 train_time:134245ms step_avg:61.30ms
step:2191/2245 train_time:134308ms step_avg:61.30ms
step:2192/2245 train_time:134368ms step_avg:61.30ms
step:2193/2245 train_time:134431ms step_avg:61.30ms
step:2194/2245 train_time:134491ms step_avg:61.30ms
step:2195/2245 train_time:134553ms step_avg:61.30ms
step:2196/2245 train_time:134614ms step_avg:61.30ms
step:2197/2245 train_time:134676ms step_avg:61.30ms
step:2198/2245 train_time:134737ms step_avg:61.30ms
step:2199/2245 train_time:134801ms step_avg:61.30ms
step:2200/2245 train_time:134862ms step_avg:61.30ms
step:2201/2245 train_time:134925ms step_avg:61.30ms
step:2202/2245 train_time:134985ms step_avg:61.30ms
step:2203/2245 train_time:135048ms step_avg:61.30ms
step:2204/2245 train_time:135108ms step_avg:61.30ms
step:2205/2245 train_time:135171ms step_avg:61.30ms
step:2206/2245 train_time:135232ms step_avg:61.30ms
step:2207/2245 train_time:135295ms step_avg:61.30ms
step:2208/2245 train_time:135356ms step_avg:61.30ms
step:2209/2245 train_time:135419ms step_avg:61.30ms
step:2210/2245 train_time:135480ms step_avg:61.30ms
step:2211/2245 train_time:135543ms step_avg:61.30ms
step:2212/2245 train_time:135604ms step_avg:61.30ms
step:2213/2245 train_time:135666ms step_avg:61.30ms
step:2214/2245 train_time:135727ms step_avg:61.30ms
step:2215/2245 train_time:135790ms step_avg:61.30ms
step:2216/2245 train_time:135851ms step_avg:61.30ms
step:2217/2245 train_time:135915ms step_avg:61.31ms
step:2218/2245 train_time:135977ms step_avg:61.31ms
step:2219/2245 train_time:136039ms step_avg:61.31ms
step:2220/2245 train_time:136101ms step_avg:61.31ms
step:2221/2245 train_time:136164ms step_avg:61.31ms
step:2222/2245 train_time:136224ms step_avg:61.31ms
step:2223/2245 train_time:136287ms step_avg:61.31ms
step:2224/2245 train_time:136347ms step_avg:61.31ms
step:2225/2245 train_time:136410ms step_avg:61.31ms
step:2226/2245 train_time:136470ms step_avg:61.31ms
step:2227/2245 train_time:136535ms step_avg:61.31ms
step:2228/2245 train_time:136595ms step_avg:61.31ms
step:2229/2245 train_time:136658ms step_avg:61.31ms
step:2230/2245 train_time:136720ms step_avg:61.31ms
step:2231/2245 train_time:136783ms step_avg:61.31ms
step:2232/2245 train_time:136844ms step_avg:61.31ms
step:2233/2245 train_time:136906ms step_avg:61.31ms
step:2234/2245 train_time:136967ms step_avg:61.31ms
step:2235/2245 train_time:137029ms step_avg:61.31ms
step:2236/2245 train_time:137090ms step_avg:61.31ms
step:2237/2245 train_time:137153ms step_avg:61.31ms
step:2238/2245 train_time:137213ms step_avg:61.31ms
step:2239/2245 train_time:137276ms step_avg:61.31ms
step:2240/2245 train_time:137336ms step_avg:61.31ms
step:2241/2245 train_time:137399ms step_avg:61.31ms
step:2242/2245 train_time:137461ms step_avg:61.31ms
step:2243/2245 train_time:137524ms step_avg:61.31ms
step:2244/2245 train_time:137585ms step_avg:61.31ms
step:2245/2245 train_time:137647ms step_avg:61.31ms
step:2245/2245 val_loss:3.2813 train_time:137708ms step_avg:61.34ms
peak memory allocated: 29249 MiB reserved: 50528 MiB

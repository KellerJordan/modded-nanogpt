import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Nov 10 21:53:41 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   35C    P0            126W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   34C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   34C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2245 train_time:118ms step_avg:117.70ms
step:2/2245 train_time:139ms step_avg:69.31ms
step:3/2245 train_time:178ms step_avg:59.30ms
step:4/2245 train_time:234ms step_avg:58.45ms
step:5/2245 train_time:294ms step_avg:58.71ms
step:6/2245 train_time:352ms step_avg:58.66ms
step:7/2245 train_time:413ms step_avg:58.98ms
step:8/2245 train_time:471ms step_avg:58.92ms
step:9/2245 train_time:532ms step_avg:59.17ms
step:10/2245 train_time:591ms step_avg:59.10ms
step:11/2245 train_time:652ms step_avg:59.27ms
step:12/2245 train_time:710ms step_avg:59.20ms
step:13/2245 train_time:772ms step_avg:59.38ms
step:14/2245 train_time:831ms step_avg:59.35ms
step:15/2245 train_time:894ms step_avg:59.58ms
step:16/2245 train_time:951ms step_avg:59.43ms
step:17/2245 train_time:1014ms step_avg:59.65ms
step:18/2245 train_time:1077ms step_avg:59.84ms
step:19/2245 train_time:1141ms step_avg:60.07ms
step:20/2245 train_time:1201ms step_avg:60.07ms
step:21/2245 train_time:1264ms step_avg:60.19ms
step:22/2245 train_time:1324ms step_avg:60.17ms
step:23/2245 train_time:1385ms step_avg:60.23ms
step:24/2245 train_time:1445ms step_avg:60.19ms
step:25/2245 train_time:1507ms step_avg:60.29ms
step:26/2245 train_time:1567ms step_avg:60.27ms
step:27/2245 train_time:1630ms step_avg:60.36ms
step:28/2245 train_time:1689ms step_avg:60.33ms
step:29/2245 train_time:1750ms step_avg:60.36ms
step:30/2245 train_time:1809ms step_avg:60.31ms
step:31/2245 train_time:1870ms step_avg:60.34ms
step:32/2245 train_time:1929ms step_avg:60.29ms
step:33/2245 train_time:1993ms step_avg:60.40ms
step:34/2245 train_time:2053ms step_avg:60.39ms
step:35/2245 train_time:2116ms step_avg:60.44ms
step:36/2245 train_time:2175ms step_avg:60.43ms
step:37/2245 train_time:2237ms step_avg:60.46ms
step:38/2245 train_time:2296ms step_avg:60.43ms
step:39/2245 train_time:2358ms step_avg:60.47ms
step:40/2245 train_time:2418ms step_avg:60.45ms
step:41/2245 train_time:2480ms step_avg:60.49ms
step:42/2245 train_time:2540ms step_avg:60.47ms
step:43/2245 train_time:2602ms step_avg:60.51ms
step:44/2245 train_time:2661ms step_avg:60.47ms
step:45/2245 train_time:2722ms step_avg:60.50ms
step:46/2245 train_time:2781ms step_avg:60.47ms
step:47/2245 train_time:2843ms step_avg:60.49ms
step:48/2245 train_time:2902ms step_avg:60.46ms
step:49/2245 train_time:2964ms step_avg:60.49ms
step:50/2245 train_time:3024ms step_avg:60.48ms
step:51/2245 train_time:3087ms step_avg:60.53ms
step:52/2245 train_time:3147ms step_avg:60.52ms
step:53/2245 train_time:3210ms step_avg:60.56ms
step:54/2245 train_time:3270ms step_avg:60.56ms
step:55/2245 train_time:3332ms step_avg:60.58ms
step:56/2245 train_time:3391ms step_avg:60.56ms
step:57/2245 train_time:3453ms step_avg:60.58ms
step:58/2245 train_time:3513ms step_avg:60.57ms
step:59/2245 train_time:3574ms step_avg:60.58ms
step:60/2245 train_time:3633ms step_avg:60.55ms
step:61/2245 train_time:3695ms step_avg:60.57ms
step:62/2245 train_time:3754ms step_avg:60.55ms
step:63/2245 train_time:3816ms step_avg:60.56ms
step:64/2245 train_time:3875ms step_avg:60.55ms
step:65/2245 train_time:3937ms step_avg:60.57ms
step:66/2245 train_time:3997ms step_avg:60.56ms
step:67/2245 train_time:4060ms step_avg:60.60ms
step:68/2245 train_time:4119ms step_avg:60.58ms
step:69/2245 train_time:4181ms step_avg:60.60ms
step:70/2245 train_time:4240ms step_avg:60.58ms
step:71/2245 train_time:4302ms step_avg:60.60ms
step:72/2245 train_time:4362ms step_avg:60.58ms
step:73/2245 train_time:4424ms step_avg:60.61ms
step:74/2245 train_time:4484ms step_avg:60.59ms
step:75/2245 train_time:4547ms step_avg:60.63ms
step:76/2245 train_time:4607ms step_avg:60.61ms
step:77/2245 train_time:4668ms step_avg:60.63ms
step:78/2245 train_time:4728ms step_avg:60.62ms
step:79/2245 train_time:4790ms step_avg:60.63ms
step:80/2245 train_time:4850ms step_avg:60.62ms
step:81/2245 train_time:4911ms step_avg:60.63ms
step:82/2245 train_time:4970ms step_avg:60.61ms
step:83/2245 train_time:5032ms step_avg:60.63ms
step:84/2245 train_time:5091ms step_avg:60.60ms
step:85/2245 train_time:5153ms step_avg:60.62ms
step:86/2245 train_time:5211ms step_avg:60.60ms
step:87/2245 train_time:5273ms step_avg:60.61ms
step:88/2245 train_time:5332ms step_avg:60.59ms
step:89/2245 train_time:5394ms step_avg:60.60ms
step:90/2245 train_time:5453ms step_avg:60.59ms
step:91/2245 train_time:5515ms step_avg:60.61ms
step:92/2245 train_time:5575ms step_avg:60.59ms
step:93/2245 train_time:5637ms step_avg:60.61ms
step:94/2245 train_time:5696ms step_avg:60.59ms
step:95/2245 train_time:5757ms step_avg:60.60ms
step:96/2245 train_time:5816ms step_avg:60.59ms
step:97/2245 train_time:5878ms step_avg:60.60ms
step:98/2245 train_time:5937ms step_avg:60.58ms
step:99/2245 train_time:5998ms step_avg:60.59ms
step:100/2245 train_time:6057ms step_avg:60.57ms
step:101/2245 train_time:6118ms step_avg:60.58ms
step:102/2245 train_time:6178ms step_avg:60.57ms
step:103/2245 train_time:6240ms step_avg:60.58ms
step:104/2245 train_time:6299ms step_avg:60.57ms
step:105/2245 train_time:6360ms step_avg:60.58ms
step:106/2245 train_time:6419ms step_avg:60.56ms
step:107/2245 train_time:6481ms step_avg:60.57ms
step:108/2245 train_time:6540ms step_avg:60.56ms
step:109/2245 train_time:6602ms step_avg:60.57ms
step:110/2245 train_time:6662ms step_avg:60.57ms
step:111/2245 train_time:6724ms step_avg:60.58ms
step:112/2245 train_time:6783ms step_avg:60.56ms
step:113/2245 train_time:6845ms step_avg:60.58ms
step:114/2245 train_time:6905ms step_avg:60.57ms
step:115/2245 train_time:6967ms step_avg:60.58ms
step:116/2245 train_time:7027ms step_avg:60.58ms
step:117/2245 train_time:7089ms step_avg:60.59ms
step:118/2245 train_time:7149ms step_avg:60.58ms
step:119/2245 train_time:7211ms step_avg:60.60ms
step:120/2245 train_time:7270ms step_avg:60.58ms
step:121/2245 train_time:7332ms step_avg:60.59ms
step:122/2245 train_time:7390ms step_avg:60.58ms
step:123/2245 train_time:7452ms step_avg:60.59ms
step:124/2245 train_time:7511ms step_avg:60.57ms
step:125/2245 train_time:7572ms step_avg:60.58ms
step:126/2245 train_time:7632ms step_avg:60.57ms
step:127/2245 train_time:7693ms step_avg:60.57ms
step:128/2245 train_time:7752ms step_avg:60.56ms
step:129/2245 train_time:7813ms step_avg:60.56ms
step:130/2245 train_time:7872ms step_avg:60.55ms
step:131/2245 train_time:7933ms step_avg:60.56ms
step:132/2245 train_time:7992ms step_avg:60.55ms
step:133/2245 train_time:8055ms step_avg:60.56ms
step:134/2245 train_time:8114ms step_avg:60.55ms
step:135/2245 train_time:8176ms step_avg:60.56ms
step:136/2245 train_time:8235ms step_avg:60.55ms
step:137/2245 train_time:8298ms step_avg:60.57ms
step:138/2245 train_time:8357ms step_avg:60.56ms
step:139/2245 train_time:8418ms step_avg:60.56ms
step:140/2245 train_time:8477ms step_avg:60.55ms
step:141/2245 train_time:8539ms step_avg:60.56ms
step:142/2245 train_time:8598ms step_avg:60.55ms
step:143/2245 train_time:8659ms step_avg:60.55ms
step:144/2245 train_time:8718ms step_avg:60.54ms
step:145/2245 train_time:8779ms step_avg:60.55ms
step:146/2245 train_time:8838ms step_avg:60.53ms
step:147/2245 train_time:8899ms step_avg:60.54ms
step:148/2245 train_time:8958ms step_avg:60.53ms
step:149/2245 train_time:9020ms step_avg:60.53ms
step:150/2245 train_time:9079ms step_avg:60.53ms
step:151/2245 train_time:9141ms step_avg:60.54ms
step:152/2245 train_time:9200ms step_avg:60.52ms
step:153/2245 train_time:9261ms step_avg:60.53ms
step:154/2245 train_time:9320ms step_avg:60.52ms
step:155/2245 train_time:9383ms step_avg:60.53ms
step:156/2245 train_time:9442ms step_avg:60.53ms
step:157/2245 train_time:9504ms step_avg:60.53ms
step:158/2245 train_time:9563ms step_avg:60.52ms
step:159/2245 train_time:9624ms step_avg:60.53ms
step:160/2245 train_time:9684ms step_avg:60.53ms
step:161/2245 train_time:9746ms step_avg:60.54ms
step:162/2245 train_time:9806ms step_avg:60.53ms
step:163/2245 train_time:9869ms step_avg:60.54ms
step:164/2245 train_time:9927ms step_avg:60.53ms
step:165/2245 train_time:9989ms step_avg:60.54ms
step:166/2245 train_time:10048ms step_avg:60.53ms
step:167/2245 train_time:10110ms step_avg:60.54ms
step:168/2245 train_time:10169ms step_avg:60.53ms
step:169/2245 train_time:10230ms step_avg:60.53ms
step:170/2245 train_time:10289ms step_avg:60.53ms
step:171/2245 train_time:10351ms step_avg:60.53ms
step:172/2245 train_time:10410ms step_avg:60.52ms
step:173/2245 train_time:10472ms step_avg:60.53ms
step:174/2245 train_time:10531ms step_avg:60.52ms
step:175/2245 train_time:10592ms step_avg:60.53ms
step:176/2245 train_time:10651ms step_avg:60.51ms
step:177/2245 train_time:10711ms step_avg:60.52ms
step:178/2245 train_time:10770ms step_avg:60.51ms
step:179/2245 train_time:10832ms step_avg:60.52ms
step:180/2245 train_time:10891ms step_avg:60.51ms
step:181/2245 train_time:10952ms step_avg:60.51ms
step:182/2245 train_time:11011ms step_avg:60.50ms
step:183/2245 train_time:11072ms step_avg:60.51ms
step:184/2245 train_time:11131ms step_avg:60.49ms
step:185/2245 train_time:11192ms step_avg:60.50ms
step:186/2245 train_time:11251ms step_avg:60.49ms
step:187/2245 train_time:11312ms step_avg:60.49ms
step:188/2245 train_time:11371ms step_avg:60.48ms
step:189/2245 train_time:11432ms step_avg:60.49ms
step:190/2245 train_time:11491ms step_avg:60.48ms
step:191/2245 train_time:11553ms step_avg:60.49ms
step:192/2245 train_time:11611ms step_avg:60.48ms
step:193/2245 train_time:11673ms step_avg:60.48ms
step:194/2245 train_time:11731ms step_avg:60.47ms
step:195/2245 train_time:11793ms step_avg:60.48ms
step:196/2245 train_time:11852ms step_avg:60.47ms
step:197/2245 train_time:11913ms step_avg:60.47ms
step:198/2245 train_time:11972ms step_avg:60.47ms
step:199/2245 train_time:12034ms step_avg:60.47ms
step:200/2245 train_time:12092ms step_avg:60.46ms
step:201/2245 train_time:12153ms step_avg:60.46ms
step:202/2245 train_time:12212ms step_avg:60.46ms
step:203/2245 train_time:12273ms step_avg:60.46ms
step:204/2245 train_time:12332ms step_avg:60.45ms
step:205/2245 train_time:12393ms step_avg:60.45ms
step:206/2245 train_time:12452ms step_avg:60.45ms
step:207/2245 train_time:12513ms step_avg:60.45ms
step:208/2245 train_time:12573ms step_avg:60.45ms
step:209/2245 train_time:12634ms step_avg:60.45ms
step:210/2245 train_time:12693ms step_avg:60.44ms
step:211/2245 train_time:12754ms step_avg:60.45ms
step:212/2245 train_time:12814ms step_avg:60.44ms
step:213/2245 train_time:12875ms step_avg:60.45ms
step:214/2245 train_time:12934ms step_avg:60.44ms
step:215/2245 train_time:12995ms step_avg:60.44ms
step:216/2245 train_time:13054ms step_avg:60.44ms
step:217/2245 train_time:13115ms step_avg:60.44ms
step:218/2245 train_time:13175ms step_avg:60.44ms
step:219/2245 train_time:13236ms step_avg:60.44ms
step:220/2245 train_time:13294ms step_avg:60.43ms
step:221/2245 train_time:13356ms step_avg:60.43ms
step:222/2245 train_time:13415ms step_avg:60.43ms
step:223/2245 train_time:13477ms step_avg:60.43ms
step:224/2245 train_time:13536ms step_avg:60.43ms
step:225/2245 train_time:13597ms step_avg:60.43ms
step:226/2245 train_time:13656ms step_avg:60.43ms
step:227/2245 train_time:13717ms step_avg:60.43ms
step:228/2245 train_time:13776ms step_avg:60.42ms
step:229/2245 train_time:13839ms step_avg:60.43ms
step:230/2245 train_time:13897ms step_avg:60.42ms
step:231/2245 train_time:13958ms step_avg:60.43ms
step:232/2245 train_time:14017ms step_avg:60.42ms
step:233/2245 train_time:14079ms step_avg:60.42ms
step:234/2245 train_time:14138ms step_avg:60.42ms
step:235/2245 train_time:14200ms step_avg:60.42ms
step:236/2245 train_time:14259ms step_avg:60.42ms
step:237/2245 train_time:14320ms step_avg:60.42ms
step:238/2245 train_time:14379ms step_avg:60.42ms
step:239/2245 train_time:14440ms step_avg:60.42ms
step:240/2245 train_time:14500ms step_avg:60.42ms
step:241/2245 train_time:14562ms step_avg:60.42ms
step:242/2245 train_time:14621ms step_avg:60.42ms
step:243/2245 train_time:14683ms step_avg:60.42ms
step:244/2245 train_time:14742ms step_avg:60.42ms
step:245/2245 train_time:14804ms step_avg:60.42ms
step:246/2245 train_time:14864ms step_avg:60.42ms
step:247/2245 train_time:14926ms step_avg:60.43ms
step:248/2245 train_time:14985ms step_avg:60.42ms
step:249/2245 train_time:15048ms step_avg:60.43ms
step:250/2245 train_time:15107ms step_avg:60.43ms
step:250/2245 val_loss:4.0823 train_time:15170ms step_avg:60.68ms
step:251/2245 train_time:15188ms step_avg:60.51ms
step:252/2245 train_time:15232ms step_avg:60.44ms
step:253/2245 train_time:15298ms step_avg:60.47ms
step:254/2245 train_time:15361ms step_avg:60.48ms
step:255/2245 train_time:15424ms step_avg:60.49ms
step:256/2245 train_time:15484ms step_avg:60.48ms
step:257/2245 train_time:15546ms step_avg:60.49ms
step:258/2245 train_time:15604ms step_avg:60.48ms
step:259/2245 train_time:15664ms step_avg:60.48ms
step:260/2245 train_time:15722ms step_avg:60.47ms
step:261/2245 train_time:15783ms step_avg:60.47ms
step:262/2245 train_time:15841ms step_avg:60.46ms
step:263/2245 train_time:15901ms step_avg:60.46ms
step:264/2245 train_time:15959ms step_avg:60.45ms
step:265/2245 train_time:16020ms step_avg:60.45ms
step:266/2245 train_time:16078ms step_avg:60.44ms
step:267/2245 train_time:16140ms step_avg:60.45ms
step:268/2245 train_time:16199ms step_avg:60.44ms
step:269/2245 train_time:16263ms step_avg:60.46ms
step:270/2245 train_time:16324ms step_avg:60.46ms
step:271/2245 train_time:16387ms step_avg:60.47ms
step:272/2245 train_time:16446ms step_avg:60.46ms
step:273/2245 train_time:16508ms step_avg:60.47ms
step:274/2245 train_time:16566ms step_avg:60.46ms
step:275/2245 train_time:16628ms step_avg:60.46ms
step:276/2245 train_time:16686ms step_avg:60.46ms
step:277/2245 train_time:16747ms step_avg:60.46ms
step:278/2245 train_time:16806ms step_avg:60.45ms
step:279/2245 train_time:16867ms step_avg:60.45ms
step:280/2245 train_time:16925ms step_avg:60.45ms
step:281/2245 train_time:16987ms step_avg:60.45ms
step:282/2245 train_time:17046ms step_avg:60.45ms
step:283/2245 train_time:17107ms step_avg:60.45ms
step:284/2245 train_time:17167ms step_avg:60.45ms
step:285/2245 train_time:17228ms step_avg:60.45ms
step:286/2245 train_time:17288ms step_avg:60.45ms
step:287/2245 train_time:17351ms step_avg:60.46ms
step:288/2245 train_time:17411ms step_avg:60.45ms
step:289/2245 train_time:17473ms step_avg:60.46ms
step:290/2245 train_time:17532ms step_avg:60.46ms
step:291/2245 train_time:17593ms step_avg:60.46ms
step:292/2245 train_time:17652ms step_avg:60.45ms
step:293/2245 train_time:17713ms step_avg:60.45ms
step:294/2245 train_time:17772ms step_avg:60.45ms
step:295/2245 train_time:17833ms step_avg:60.45ms
step:296/2245 train_time:17892ms step_avg:60.45ms
step:297/2245 train_time:17954ms step_avg:60.45ms
step:298/2245 train_time:18013ms step_avg:60.45ms
step:299/2245 train_time:18074ms step_avg:60.45ms
step:300/2245 train_time:18133ms step_avg:60.44ms
step:301/2245 train_time:18194ms step_avg:60.45ms
step:302/2245 train_time:18253ms step_avg:60.44ms
step:303/2245 train_time:18315ms step_avg:60.45ms
step:304/2245 train_time:18374ms step_avg:60.44ms
step:305/2245 train_time:18436ms step_avg:60.45ms
step:306/2245 train_time:18494ms step_avg:60.44ms
step:307/2245 train_time:18555ms step_avg:60.44ms
step:308/2245 train_time:18614ms step_avg:60.43ms
step:309/2245 train_time:18675ms step_avg:60.44ms
step:310/2245 train_time:18734ms step_avg:60.43ms
step:311/2245 train_time:18795ms step_avg:60.43ms
step:312/2245 train_time:18853ms step_avg:60.43ms
step:313/2245 train_time:18915ms step_avg:60.43ms
step:314/2245 train_time:18973ms step_avg:60.42ms
step:315/2245 train_time:19034ms step_avg:60.43ms
step:316/2245 train_time:19093ms step_avg:60.42ms
step:317/2245 train_time:19155ms step_avg:60.43ms
step:318/2245 train_time:19214ms step_avg:60.42ms
step:319/2245 train_time:19275ms step_avg:60.42ms
step:320/2245 train_time:19334ms step_avg:60.42ms
step:321/2245 train_time:19396ms step_avg:60.42ms
step:322/2245 train_time:19455ms step_avg:60.42ms
step:323/2245 train_time:19516ms step_avg:60.42ms
step:324/2245 train_time:19575ms step_avg:60.42ms
step:325/2245 train_time:19636ms step_avg:60.42ms
step:326/2245 train_time:19694ms step_avg:60.41ms
step:327/2245 train_time:19756ms step_avg:60.41ms
step:328/2245 train_time:19814ms step_avg:60.41ms
step:329/2245 train_time:19876ms step_avg:60.41ms
step:330/2245 train_time:19934ms step_avg:60.41ms
step:331/2245 train_time:19996ms step_avg:60.41ms
step:332/2245 train_time:20054ms step_avg:60.40ms
step:333/2245 train_time:20115ms step_avg:60.41ms
step:334/2245 train_time:20174ms step_avg:60.40ms
step:335/2245 train_time:20236ms step_avg:60.41ms
step:336/2245 train_time:20294ms step_avg:60.40ms
step:337/2245 train_time:20356ms step_avg:60.40ms
step:338/2245 train_time:20415ms step_avg:60.40ms
step:339/2245 train_time:20476ms step_avg:60.40ms
step:340/2245 train_time:20535ms step_avg:60.40ms
step:341/2245 train_time:20596ms step_avg:60.40ms
step:342/2245 train_time:20655ms step_avg:60.39ms
step:343/2245 train_time:20716ms step_avg:60.40ms
step:344/2245 train_time:20774ms step_avg:60.39ms
step:345/2245 train_time:20835ms step_avg:60.39ms
step:346/2245 train_time:20894ms step_avg:60.39ms
step:347/2245 train_time:20955ms step_avg:60.39ms
step:348/2245 train_time:21013ms step_avg:60.38ms
step:349/2245 train_time:21075ms step_avg:60.39ms
step:350/2245 train_time:21134ms step_avg:60.38ms
step:351/2245 train_time:21195ms step_avg:60.39ms
step:352/2245 train_time:21254ms step_avg:60.38ms
step:353/2245 train_time:21315ms step_avg:60.38ms
step:354/2245 train_time:21374ms step_avg:60.38ms
step:355/2245 train_time:21435ms step_avg:60.38ms
step:356/2245 train_time:21494ms step_avg:60.38ms
step:357/2245 train_time:21556ms step_avg:60.38ms
step:358/2245 train_time:21615ms step_avg:60.38ms
step:359/2245 train_time:21676ms step_avg:60.38ms
step:360/2245 train_time:21735ms step_avg:60.37ms
step:361/2245 train_time:21796ms step_avg:60.38ms
step:362/2245 train_time:21854ms step_avg:60.37ms
step:363/2245 train_time:21916ms step_avg:60.37ms
step:364/2245 train_time:21974ms step_avg:60.37ms
step:365/2245 train_time:22036ms step_avg:60.37ms
step:366/2245 train_time:22094ms step_avg:60.37ms
step:367/2245 train_time:22156ms step_avg:60.37ms
step:368/2245 train_time:22214ms step_avg:60.36ms
step:369/2245 train_time:22275ms step_avg:60.37ms
step:370/2245 train_time:22334ms step_avg:60.36ms
step:371/2245 train_time:22395ms step_avg:60.36ms
step:372/2245 train_time:22454ms step_avg:60.36ms
step:373/2245 train_time:22515ms step_avg:60.36ms
step:374/2245 train_time:22574ms step_avg:60.36ms
step:375/2245 train_time:22636ms step_avg:60.36ms
step:376/2245 train_time:22695ms step_avg:60.36ms
step:377/2245 train_time:22756ms step_avg:60.36ms
step:378/2245 train_time:22815ms step_avg:60.36ms
step:379/2245 train_time:22876ms step_avg:60.36ms
step:380/2245 train_time:22934ms step_avg:60.35ms
step:381/2245 train_time:22995ms step_avg:60.36ms
step:382/2245 train_time:23054ms step_avg:60.35ms
step:383/2245 train_time:23115ms step_avg:60.35ms
step:384/2245 train_time:23174ms step_avg:60.35ms
step:385/2245 train_time:23236ms step_avg:60.35ms
step:386/2245 train_time:23295ms step_avg:60.35ms
step:387/2245 train_time:23356ms step_avg:60.35ms
step:388/2245 train_time:23415ms step_avg:60.35ms
step:389/2245 train_time:23476ms step_avg:60.35ms
step:390/2245 train_time:23534ms step_avg:60.34ms
step:391/2245 train_time:23596ms step_avg:60.35ms
step:392/2245 train_time:23654ms step_avg:60.34ms
step:393/2245 train_time:23715ms step_avg:60.34ms
step:394/2245 train_time:23774ms step_avg:60.34ms
step:395/2245 train_time:23835ms step_avg:60.34ms
step:396/2245 train_time:23895ms step_avg:60.34ms
step:397/2245 train_time:23955ms step_avg:60.34ms
step:398/2245 train_time:24014ms step_avg:60.34ms
step:399/2245 train_time:24075ms step_avg:60.34ms
step:400/2245 train_time:24134ms step_avg:60.33ms
step:401/2245 train_time:24195ms step_avg:60.34ms
step:402/2245 train_time:24253ms step_avg:60.33ms
step:403/2245 train_time:24315ms step_avg:60.33ms
step:404/2245 train_time:24373ms step_avg:60.33ms
step:405/2245 train_time:24435ms step_avg:60.33ms
step:406/2245 train_time:24494ms step_avg:60.33ms
step:407/2245 train_time:24555ms step_avg:60.33ms
step:408/2245 train_time:24614ms step_avg:60.33ms
step:409/2245 train_time:24676ms step_avg:60.33ms
step:410/2245 train_time:24734ms step_avg:60.33ms
step:411/2245 train_time:24796ms step_avg:60.33ms
step:412/2245 train_time:24855ms step_avg:60.33ms
step:413/2245 train_time:24916ms step_avg:60.33ms
step:414/2245 train_time:24976ms step_avg:60.33ms
step:415/2245 train_time:25036ms step_avg:60.33ms
step:416/2245 train_time:25095ms step_avg:60.33ms
step:417/2245 train_time:25157ms step_avg:60.33ms
step:418/2245 train_time:25215ms step_avg:60.32ms
step:419/2245 train_time:25276ms step_avg:60.33ms
step:420/2245 train_time:25335ms step_avg:60.32ms
step:421/2245 train_time:25396ms step_avg:60.32ms
step:422/2245 train_time:25455ms step_avg:60.32ms
step:423/2245 train_time:25516ms step_avg:60.32ms
step:424/2245 train_time:25575ms step_avg:60.32ms
step:425/2245 train_time:25637ms step_avg:60.32ms
step:426/2245 train_time:25695ms step_avg:60.32ms
step:427/2245 train_time:25756ms step_avg:60.32ms
step:428/2245 train_time:25815ms step_avg:60.31ms
step:429/2245 train_time:25876ms step_avg:60.32ms
step:430/2245 train_time:25935ms step_avg:60.31ms
step:431/2245 train_time:25997ms step_avg:60.32ms
step:432/2245 train_time:26055ms step_avg:60.31ms
step:433/2245 train_time:26116ms step_avg:60.31ms
step:434/2245 train_time:26175ms step_avg:60.31ms
step:435/2245 train_time:26237ms step_avg:60.31ms
step:436/2245 train_time:26295ms step_avg:60.31ms
step:437/2245 train_time:26356ms step_avg:60.31ms
step:438/2245 train_time:26415ms step_avg:60.31ms
step:439/2245 train_time:26477ms step_avg:60.31ms
step:440/2245 train_time:26535ms step_avg:60.31ms
step:441/2245 train_time:26596ms step_avg:60.31ms
step:442/2245 train_time:26655ms step_avg:60.31ms
step:443/2245 train_time:26716ms step_avg:60.31ms
step:444/2245 train_time:26775ms step_avg:60.30ms
step:445/2245 train_time:26836ms step_avg:60.31ms
step:446/2245 train_time:26895ms step_avg:60.30ms
step:447/2245 train_time:26956ms step_avg:60.30ms
step:448/2245 train_time:27015ms step_avg:60.30ms
step:449/2245 train_time:27076ms step_avg:60.30ms
step:450/2245 train_time:27134ms step_avg:60.30ms
step:451/2245 train_time:27196ms step_avg:60.30ms
step:452/2245 train_time:27254ms step_avg:60.30ms
step:453/2245 train_time:27316ms step_avg:60.30ms
step:454/2245 train_time:27375ms step_avg:60.30ms
step:455/2245 train_time:27436ms step_avg:60.30ms
step:456/2245 train_time:27495ms step_avg:60.30ms
step:457/2245 train_time:27556ms step_avg:60.30ms
step:458/2245 train_time:27615ms step_avg:60.29ms
step:459/2245 train_time:27676ms step_avg:60.30ms
step:460/2245 train_time:27735ms step_avg:60.29ms
step:461/2245 train_time:27796ms step_avg:60.30ms
step:462/2245 train_time:27855ms step_avg:60.29ms
step:463/2245 train_time:27916ms step_avg:60.29ms
step:464/2245 train_time:27974ms step_avg:60.29ms
step:465/2245 train_time:28036ms step_avg:60.29ms
step:466/2245 train_time:28094ms step_avg:60.29ms
step:467/2245 train_time:28155ms step_avg:60.29ms
step:468/2245 train_time:28214ms step_avg:60.29ms
step:469/2245 train_time:28275ms step_avg:60.29ms
step:470/2245 train_time:28334ms step_avg:60.29ms
step:471/2245 train_time:28396ms step_avg:60.29ms
step:472/2245 train_time:28454ms step_avg:60.28ms
step:473/2245 train_time:28516ms step_avg:60.29ms
step:474/2245 train_time:28575ms step_avg:60.28ms
step:475/2245 train_time:28636ms step_avg:60.29ms
step:476/2245 train_time:28694ms step_avg:60.28ms
step:477/2245 train_time:28756ms step_avg:60.28ms
step:478/2245 train_time:28815ms step_avg:60.28ms
step:479/2245 train_time:28876ms step_avg:60.28ms
step:480/2245 train_time:28934ms step_avg:60.28ms
step:481/2245 train_time:28996ms step_avg:60.28ms
step:482/2245 train_time:29055ms step_avg:60.28ms
step:483/2245 train_time:29116ms step_avg:60.28ms
step:484/2245 train_time:29174ms step_avg:60.28ms
step:485/2245 train_time:29236ms step_avg:60.28ms
step:486/2245 train_time:29295ms step_avg:60.28ms
step:487/2245 train_time:29356ms step_avg:60.28ms
step:488/2245 train_time:29414ms step_avg:60.28ms
step:489/2245 train_time:29476ms step_avg:60.28ms
step:490/2245 train_time:29535ms step_avg:60.27ms
step:491/2245 train_time:29596ms step_avg:60.28ms
step:492/2245 train_time:29654ms step_avg:60.27ms
step:493/2245 train_time:29715ms step_avg:60.27ms
step:494/2245 train_time:29774ms step_avg:60.27ms
step:495/2245 train_time:29836ms step_avg:60.27ms
step:496/2245 train_time:29895ms step_avg:60.27ms
step:497/2245 train_time:29956ms step_avg:60.27ms
step:498/2245 train_time:30014ms step_avg:60.27ms
step:499/2245 train_time:30076ms step_avg:60.27ms
step:500/2245 train_time:30134ms step_avg:60.27ms
step:500/2245 val_loss:3.8249 train_time:30196ms step_avg:60.39ms
step:501/2245 train_time:30214ms step_avg:60.31ms
step:502/2245 train_time:30258ms step_avg:60.27ms
step:503/2245 train_time:30323ms step_avg:60.28ms
step:504/2245 train_time:30384ms step_avg:60.29ms
step:505/2245 train_time:30446ms step_avg:60.29ms
step:506/2245 train_time:30505ms step_avg:60.29ms
step:507/2245 train_time:30567ms step_avg:60.29ms
step:508/2245 train_time:30625ms step_avg:60.29ms
step:509/2245 train_time:30686ms step_avg:60.29ms
step:510/2245 train_time:30745ms step_avg:60.28ms
step:511/2245 train_time:30807ms step_avg:60.29ms
step:512/2245 train_time:30866ms step_avg:60.29ms
step:513/2245 train_time:30927ms step_avg:60.29ms
step:514/2245 train_time:30986ms step_avg:60.28ms
step:515/2245 train_time:31047ms step_avg:60.29ms
step:516/2245 train_time:31105ms step_avg:60.28ms
step:517/2245 train_time:31167ms step_avg:60.29ms
step:518/2245 train_time:31228ms step_avg:60.28ms
step:519/2245 train_time:31290ms step_avg:60.29ms
step:520/2245 train_time:31350ms step_avg:60.29ms
step:521/2245 train_time:31412ms step_avg:60.29ms
step:522/2245 train_time:31471ms step_avg:60.29ms
step:523/2245 train_time:31532ms step_avg:60.29ms
step:524/2245 train_time:31590ms step_avg:60.29ms
step:525/2245 train_time:31651ms step_avg:60.29ms
step:526/2245 train_time:31710ms step_avg:60.28ms
step:527/2245 train_time:31771ms step_avg:60.29ms
step:528/2245 train_time:31830ms step_avg:60.28ms
step:529/2245 train_time:31891ms step_avg:60.29ms
step:530/2245 train_time:31949ms step_avg:60.28ms
step:531/2245 train_time:32011ms step_avg:60.28ms
step:532/2245 train_time:32070ms step_avg:60.28ms
step:533/2245 train_time:32131ms step_avg:60.28ms
step:534/2245 train_time:32190ms step_avg:60.28ms
step:535/2245 train_time:32252ms step_avg:60.28ms
step:536/2245 train_time:32311ms step_avg:60.28ms
step:537/2245 train_time:32373ms step_avg:60.29ms
step:538/2245 train_time:32432ms step_avg:60.28ms
step:539/2245 train_time:32494ms step_avg:60.28ms
step:540/2245 train_time:32553ms step_avg:60.28ms
step:541/2245 train_time:32614ms step_avg:60.28ms
step:542/2245 train_time:32672ms step_avg:60.28ms
step:543/2245 train_time:32734ms step_avg:60.28ms
step:544/2245 train_time:32792ms step_avg:60.28ms
step:545/2245 train_time:32854ms step_avg:60.28ms
step:546/2245 train_time:32913ms step_avg:60.28ms
step:547/2245 train_time:32974ms step_avg:60.28ms
step:548/2245 train_time:33033ms step_avg:60.28ms
step:549/2245 train_time:33095ms step_avg:60.28ms
step:550/2245 train_time:33154ms step_avg:60.28ms
step:551/2245 train_time:33216ms step_avg:60.28ms
step:552/2245 train_time:33275ms step_avg:60.28ms
step:553/2245 train_time:33337ms step_avg:60.28ms
step:554/2245 train_time:33396ms step_avg:60.28ms
step:555/2245 train_time:33457ms step_avg:60.28ms
step:556/2245 train_time:33516ms step_avg:60.28ms
step:557/2245 train_time:33578ms step_avg:60.28ms
step:558/2245 train_time:33636ms step_avg:60.28ms
step:559/2245 train_time:33697ms step_avg:60.28ms
step:560/2245 train_time:33757ms step_avg:60.28ms
step:561/2245 train_time:33818ms step_avg:60.28ms
step:562/2245 train_time:33877ms step_avg:60.28ms
step:563/2245 train_time:33939ms step_avg:60.28ms
step:564/2245 train_time:33998ms step_avg:60.28ms
step:565/2245 train_time:34060ms step_avg:60.28ms
step:566/2245 train_time:34119ms step_avg:60.28ms
step:567/2245 train_time:34181ms step_avg:60.28ms
step:568/2245 train_time:34240ms step_avg:60.28ms
step:569/2245 train_time:34302ms step_avg:60.28ms
step:570/2245 train_time:34362ms step_avg:60.28ms
step:571/2245 train_time:34423ms step_avg:60.29ms
step:572/2245 train_time:34483ms step_avg:60.28ms
step:573/2245 train_time:34545ms step_avg:60.29ms
step:574/2245 train_time:34605ms step_avg:60.29ms
step:575/2245 train_time:34667ms step_avg:60.29ms
step:576/2245 train_time:34726ms step_avg:60.29ms
step:577/2245 train_time:34788ms step_avg:60.29ms
step:578/2245 train_time:34847ms step_avg:60.29ms
step:579/2245 train_time:34908ms step_avg:60.29ms
step:580/2245 train_time:34967ms step_avg:60.29ms
step:581/2245 train_time:35028ms step_avg:60.29ms
step:582/2245 train_time:35087ms step_avg:60.29ms
step:583/2245 train_time:35149ms step_avg:60.29ms
step:584/2245 train_time:35208ms step_avg:60.29ms
step:585/2245 train_time:35269ms step_avg:60.29ms
step:586/2245 train_time:35328ms step_avg:60.29ms
step:587/2245 train_time:35389ms step_avg:60.29ms
step:588/2245 train_time:35448ms step_avg:60.29ms
step:589/2245 train_time:35510ms step_avg:60.29ms
step:590/2245 train_time:35568ms step_avg:60.29ms
step:591/2245 train_time:35630ms step_avg:60.29ms
step:592/2245 train_time:35688ms step_avg:60.28ms
step:593/2245 train_time:35750ms step_avg:60.29ms
step:594/2245 train_time:35809ms step_avg:60.28ms
step:595/2245 train_time:35870ms step_avg:60.29ms
step:596/2245 train_time:35929ms step_avg:60.28ms
step:597/2245 train_time:35991ms step_avg:60.29ms
step:598/2245 train_time:36049ms step_avg:60.28ms
step:599/2245 train_time:36110ms step_avg:60.28ms
step:600/2245 train_time:36169ms step_avg:60.28ms
step:601/2245 train_time:36230ms step_avg:60.28ms
step:602/2245 train_time:36289ms step_avg:60.28ms
step:603/2245 train_time:36351ms step_avg:60.28ms
step:604/2245 train_time:36410ms step_avg:60.28ms
step:605/2245 train_time:36471ms step_avg:60.28ms
step:606/2245 train_time:36530ms step_avg:60.28ms
step:607/2245 train_time:36591ms step_avg:60.28ms
step:608/2245 train_time:36650ms step_avg:60.28ms
step:609/2245 train_time:36711ms step_avg:60.28ms
step:610/2245 train_time:36770ms step_avg:60.28ms
step:611/2245 train_time:36831ms step_avg:60.28ms
step:612/2245 train_time:36890ms step_avg:60.28ms
step:613/2245 train_time:36952ms step_avg:60.28ms
step:614/2245 train_time:37011ms step_avg:60.28ms
step:615/2245 train_time:37072ms step_avg:60.28ms
step:616/2245 train_time:37131ms step_avg:60.28ms
step:617/2245 train_time:37192ms step_avg:60.28ms
step:618/2245 train_time:37251ms step_avg:60.28ms
step:619/2245 train_time:37313ms step_avg:60.28ms
step:620/2245 train_time:37372ms step_avg:60.28ms
step:621/2245 train_time:37434ms step_avg:60.28ms
step:622/2245 train_time:37492ms step_avg:60.28ms
step:623/2245 train_time:37554ms step_avg:60.28ms
step:624/2245 train_time:37612ms step_avg:60.28ms
step:625/2245 train_time:37674ms step_avg:60.28ms
step:626/2245 train_time:37733ms step_avg:60.28ms
step:627/2245 train_time:37795ms step_avg:60.28ms
step:628/2245 train_time:37854ms step_avg:60.28ms
step:629/2245 train_time:37915ms step_avg:60.28ms
step:630/2245 train_time:37974ms step_avg:60.28ms
step:631/2245 train_time:38036ms step_avg:60.28ms
step:632/2245 train_time:38095ms step_avg:60.28ms
step:633/2245 train_time:38156ms step_avg:60.28ms
step:634/2245 train_time:38215ms step_avg:60.28ms
step:635/2245 train_time:38277ms step_avg:60.28ms
step:636/2245 train_time:38336ms step_avg:60.28ms
step:637/2245 train_time:38398ms step_avg:60.28ms
step:638/2245 train_time:38457ms step_avg:60.28ms
step:639/2245 train_time:38518ms step_avg:60.28ms
step:640/2245 train_time:38578ms step_avg:60.28ms
step:641/2245 train_time:38639ms step_avg:60.28ms
step:642/2245 train_time:38699ms step_avg:60.28ms
step:643/2245 train_time:38761ms step_avg:60.28ms
step:644/2245 train_time:38821ms step_avg:60.28ms
step:645/2245 train_time:38882ms step_avg:60.28ms
step:646/2245 train_time:38942ms step_avg:60.28ms
step:647/2245 train_time:39003ms step_avg:60.28ms
step:648/2245 train_time:39063ms step_avg:60.28ms
step:649/2245 train_time:39125ms step_avg:60.29ms
step:650/2245 train_time:39185ms step_avg:60.28ms
step:651/2245 train_time:39246ms step_avg:60.29ms
step:652/2245 train_time:39306ms step_avg:60.29ms
step:653/2245 train_time:39368ms step_avg:60.29ms
step:654/2245 train_time:39427ms step_avg:60.29ms
step:655/2245 train_time:39489ms step_avg:60.29ms
step:656/2245 train_time:39547ms step_avg:60.29ms
step:657/2245 train_time:39609ms step_avg:60.29ms
step:658/2245 train_time:39668ms step_avg:60.29ms
step:659/2245 train_time:39729ms step_avg:60.29ms
step:660/2245 train_time:39788ms step_avg:60.28ms
step:661/2245 train_time:39850ms step_avg:60.29ms
step:662/2245 train_time:39909ms step_avg:60.29ms
step:663/2245 train_time:39972ms step_avg:60.29ms
step:664/2245 train_time:40030ms step_avg:60.29ms
step:665/2245 train_time:40091ms step_avg:60.29ms
step:666/2245 train_time:40150ms step_avg:60.29ms
step:667/2245 train_time:40212ms step_avg:60.29ms
step:668/2245 train_time:40271ms step_avg:60.29ms
step:669/2245 train_time:40332ms step_avg:60.29ms
step:670/2245 train_time:40391ms step_avg:60.29ms
step:671/2245 train_time:40452ms step_avg:60.29ms
step:672/2245 train_time:40511ms step_avg:60.28ms
step:673/2245 train_time:40572ms step_avg:60.29ms
step:674/2245 train_time:40631ms step_avg:60.28ms
step:675/2245 train_time:40692ms step_avg:60.28ms
step:676/2245 train_time:40751ms step_avg:60.28ms
step:677/2245 train_time:40813ms step_avg:60.28ms
step:678/2245 train_time:40871ms step_avg:60.28ms
step:679/2245 train_time:40933ms step_avg:60.28ms
step:680/2245 train_time:40992ms step_avg:60.28ms
step:681/2245 train_time:41054ms step_avg:60.28ms
step:682/2245 train_time:41113ms step_avg:60.28ms
step:683/2245 train_time:41175ms step_avg:60.28ms
step:684/2245 train_time:41234ms step_avg:60.28ms
step:685/2245 train_time:41295ms step_avg:60.29ms
step:686/2245 train_time:41355ms step_avg:60.28ms
step:687/2245 train_time:41416ms step_avg:60.29ms
step:688/2245 train_time:41475ms step_avg:60.28ms
step:689/2245 train_time:41536ms step_avg:60.28ms
step:690/2245 train_time:41595ms step_avg:60.28ms
step:691/2245 train_time:41656ms step_avg:60.28ms
step:692/2245 train_time:41716ms step_avg:60.28ms
step:693/2245 train_time:41777ms step_avg:60.28ms
step:694/2245 train_time:41836ms step_avg:60.28ms
step:695/2245 train_time:41899ms step_avg:60.29ms
step:696/2245 train_time:41957ms step_avg:60.28ms
step:697/2245 train_time:42019ms step_avg:60.29ms
step:698/2245 train_time:42078ms step_avg:60.28ms
step:699/2245 train_time:42140ms step_avg:60.29ms
step:700/2245 train_time:42199ms step_avg:60.28ms
step:701/2245 train_time:42261ms step_avg:60.29ms
step:702/2245 train_time:42320ms step_avg:60.29ms
step:703/2245 train_time:42381ms step_avg:60.29ms
step:704/2245 train_time:42441ms step_avg:60.29ms
step:705/2245 train_time:42503ms step_avg:60.29ms
step:706/2245 train_time:42562ms step_avg:60.29ms
step:707/2245 train_time:42624ms step_avg:60.29ms
step:708/2245 train_time:42684ms step_avg:60.29ms
step:709/2245 train_time:42746ms step_avg:60.29ms
step:710/2245 train_time:42806ms step_avg:60.29ms
step:711/2245 train_time:42869ms step_avg:60.29ms
step:712/2245 train_time:42927ms step_avg:60.29ms
step:713/2245 train_time:42988ms step_avg:60.29ms
step:714/2245 train_time:43047ms step_avg:60.29ms
step:715/2245 train_time:43108ms step_avg:60.29ms
step:716/2245 train_time:43167ms step_avg:60.29ms
step:717/2245 train_time:43229ms step_avg:60.29ms
step:718/2245 train_time:43678ms step_avg:60.83ms
step:719/2245 train_time:43738ms step_avg:60.83ms
step:720/2245 train_time:43796ms step_avg:60.83ms
step:721/2245 train_time:43857ms step_avg:60.83ms
step:722/2245 train_time:43915ms step_avg:60.82ms
step:723/2245 train_time:43975ms step_avg:60.82ms
step:724/2245 train_time:44034ms step_avg:60.82ms
step:725/2245 train_time:44094ms step_avg:60.82ms
step:726/2245 train_time:44153ms step_avg:60.82ms
step:727/2245 train_time:44213ms step_avg:60.82ms
step:728/2245 train_time:44272ms step_avg:60.81ms
step:729/2245 train_time:44333ms step_avg:60.81ms
step:730/2245 train_time:44391ms step_avg:60.81ms
step:731/2245 train_time:44451ms step_avg:60.81ms
step:732/2245 train_time:44511ms step_avg:60.81ms
step:733/2245 train_time:44580ms step_avg:60.82ms
step:734/2245 train_time:44642ms step_avg:60.82ms
step:735/2245 train_time:44706ms step_avg:60.82ms
step:736/2245 train_time:44766ms step_avg:60.82ms
step:737/2245 train_time:44829ms step_avg:60.83ms
step:738/2245 train_time:44888ms step_avg:60.82ms
step:739/2245 train_time:44951ms step_avg:60.83ms
step:740/2245 train_time:45010ms step_avg:60.82ms
step:741/2245 train_time:45071ms step_avg:60.83ms
step:742/2245 train_time:45131ms step_avg:60.82ms
step:743/2245 train_time:45193ms step_avg:60.82ms
step:744/2245 train_time:45252ms step_avg:60.82ms
step:745/2245 train_time:45314ms step_avg:60.82ms
step:746/2245 train_time:45374ms step_avg:60.82ms
step:747/2245 train_time:45435ms step_avg:60.82ms
step:748/2245 train_time:45495ms step_avg:60.82ms
step:749/2245 train_time:45559ms step_avg:60.83ms
step:750/2245 train_time:45620ms step_avg:60.83ms
step:750/2245 val_loss:3.6714 train_time:45685ms step_avg:60.91ms
step:751/2245 train_time:45704ms step_avg:60.86ms
step:752/2245 train_time:45745ms step_avg:60.83ms
step:753/2245 train_time:45807ms step_avg:60.83ms
step:754/2245 train_time:45867ms step_avg:60.83ms
step:755/2245 train_time:45931ms step_avg:60.84ms
step:756/2245 train_time:45992ms step_avg:60.84ms
step:757/2245 train_time:46053ms step_avg:60.84ms
step:758/2245 train_time:46113ms step_avg:60.84ms
step:759/2245 train_time:46174ms step_avg:60.84ms
step:760/2245 train_time:46233ms step_avg:60.83ms
step:761/2245 train_time:46295ms step_avg:60.83ms
step:762/2245 train_time:46354ms step_avg:60.83ms
step:763/2245 train_time:46416ms step_avg:60.83ms
step:764/2245 train_time:46474ms step_avg:60.83ms
step:765/2245 train_time:46536ms step_avg:60.83ms
step:766/2245 train_time:46599ms step_avg:60.83ms
step:767/2245 train_time:46665ms step_avg:60.84ms
step:768/2245 train_time:46727ms step_avg:60.84ms
step:769/2245 train_time:46788ms step_avg:60.84ms
step:770/2245 train_time:46848ms step_avg:60.84ms
step:771/2245 train_time:46910ms step_avg:60.84ms
step:772/2245 train_time:46970ms step_avg:60.84ms
step:773/2245 train_time:47033ms step_avg:60.84ms
step:774/2245 train_time:47092ms step_avg:60.84ms
step:775/2245 train_time:47154ms step_avg:60.84ms
step:776/2245 train_time:47213ms step_avg:60.84ms
step:777/2245 train_time:47274ms step_avg:60.84ms
step:778/2245 train_time:47333ms step_avg:60.84ms
step:779/2245 train_time:47395ms step_avg:60.84ms
step:780/2245 train_time:47455ms step_avg:60.84ms
step:781/2245 train_time:47518ms step_avg:60.84ms
step:782/2245 train_time:47579ms step_avg:60.84ms
step:783/2245 train_time:47643ms step_avg:60.85ms
step:784/2245 train_time:47704ms step_avg:60.85ms
step:785/2245 train_time:47767ms step_avg:60.85ms
step:786/2245 train_time:47828ms step_avg:60.85ms
step:787/2245 train_time:47890ms step_avg:60.85ms
step:788/2245 train_time:47951ms step_avg:60.85ms
step:789/2245 train_time:48013ms step_avg:60.85ms
step:790/2245 train_time:48072ms step_avg:60.85ms
step:791/2245 train_time:48134ms step_avg:60.85ms
step:792/2245 train_time:48194ms step_avg:60.85ms
step:793/2245 train_time:48255ms step_avg:60.85ms
step:794/2245 train_time:48314ms step_avg:60.85ms
step:795/2245 train_time:48376ms step_avg:60.85ms
step:796/2245 train_time:48436ms step_avg:60.85ms
step:797/2245 train_time:48498ms step_avg:60.85ms
step:798/2245 train_time:48558ms step_avg:60.85ms
step:799/2245 train_time:48621ms step_avg:60.85ms
step:800/2245 train_time:48683ms step_avg:60.85ms
step:801/2245 train_time:48746ms step_avg:60.86ms
step:802/2245 train_time:48807ms step_avg:60.86ms
step:803/2245 train_time:48869ms step_avg:60.86ms
step:804/2245 train_time:48929ms step_avg:60.86ms
step:805/2245 train_time:48991ms step_avg:60.86ms
step:806/2245 train_time:49050ms step_avg:60.86ms
step:807/2245 train_time:49112ms step_avg:60.86ms
step:808/2245 train_time:49172ms step_avg:60.86ms
step:809/2245 train_time:49233ms step_avg:60.86ms
step:810/2245 train_time:49292ms step_avg:60.85ms
step:811/2245 train_time:49355ms step_avg:60.86ms
step:812/2245 train_time:49414ms step_avg:60.85ms
step:813/2245 train_time:49477ms step_avg:60.86ms
step:814/2245 train_time:49537ms step_avg:60.86ms
step:815/2245 train_time:49600ms step_avg:60.86ms
step:816/2245 train_time:49660ms step_avg:60.86ms
step:817/2245 train_time:49724ms step_avg:60.86ms
step:818/2245 train_time:49784ms step_avg:60.86ms
step:819/2245 train_time:49847ms step_avg:60.86ms
step:820/2245 train_time:49906ms step_avg:60.86ms
step:821/2245 train_time:49968ms step_avg:60.86ms
step:822/2245 train_time:50029ms step_avg:60.86ms
step:823/2245 train_time:50090ms step_avg:60.86ms
step:824/2245 train_time:50149ms step_avg:60.86ms
step:825/2245 train_time:50212ms step_avg:60.86ms
step:826/2245 train_time:50271ms step_avg:60.86ms
step:827/2245 train_time:50333ms step_avg:60.86ms
step:828/2245 train_time:50392ms step_avg:60.86ms
step:829/2245 train_time:50454ms step_avg:60.86ms
step:830/2245 train_time:50515ms step_avg:60.86ms
step:831/2245 train_time:50577ms step_avg:60.86ms
step:832/2245 train_time:50638ms step_avg:60.86ms
step:833/2245 train_time:50700ms step_avg:60.86ms
step:834/2245 train_time:50760ms step_avg:60.86ms
step:835/2245 train_time:50823ms step_avg:60.87ms
step:836/2245 train_time:50884ms step_avg:60.87ms
step:837/2245 train_time:50947ms step_avg:60.87ms
step:838/2245 train_time:51008ms step_avg:60.87ms
step:839/2245 train_time:51070ms step_avg:60.87ms
step:840/2245 train_time:51130ms step_avg:60.87ms
step:841/2245 train_time:51192ms step_avg:60.87ms
step:842/2245 train_time:51251ms step_avg:60.87ms
step:843/2245 train_time:51313ms step_avg:60.87ms
step:844/2245 train_time:51372ms step_avg:60.87ms
step:845/2245 train_time:51434ms step_avg:60.87ms
step:846/2245 train_time:51494ms step_avg:60.87ms
step:847/2245 train_time:51557ms step_avg:60.87ms
step:848/2245 train_time:51617ms step_avg:60.87ms
step:849/2245 train_time:51680ms step_avg:60.87ms
step:850/2245 train_time:51739ms step_avg:60.87ms
step:851/2245 train_time:51803ms step_avg:60.87ms
step:852/2245 train_time:51863ms step_avg:60.87ms
step:853/2245 train_time:51926ms step_avg:60.87ms
step:854/2245 train_time:51986ms step_avg:60.87ms
step:855/2245 train_time:52048ms step_avg:60.87ms
step:856/2245 train_time:52108ms step_avg:60.87ms
step:857/2245 train_time:52169ms step_avg:60.87ms
step:858/2245 train_time:52229ms step_avg:60.87ms
step:859/2245 train_time:52291ms step_avg:60.87ms
step:860/2245 train_time:52351ms step_avg:60.87ms
step:861/2245 train_time:52413ms step_avg:60.87ms
step:862/2245 train_time:52472ms step_avg:60.87ms
step:863/2245 train_time:52535ms step_avg:60.87ms
step:864/2245 train_time:52595ms step_avg:60.87ms
step:865/2245 train_time:52658ms step_avg:60.88ms
step:866/2245 train_time:52718ms step_avg:60.88ms
step:867/2245 train_time:52781ms step_avg:60.88ms
step:868/2245 train_time:52842ms step_avg:60.88ms
step:869/2245 train_time:52904ms step_avg:60.88ms
step:870/2245 train_time:52964ms step_avg:60.88ms
step:871/2245 train_time:53027ms step_avg:60.88ms
step:872/2245 train_time:53087ms step_avg:60.88ms
step:873/2245 train_time:53149ms step_avg:60.88ms
step:874/2245 train_time:53208ms step_avg:60.88ms
step:875/2245 train_time:53270ms step_avg:60.88ms
step:876/2245 train_time:53330ms step_avg:60.88ms
step:877/2245 train_time:53391ms step_avg:60.88ms
step:878/2245 train_time:53451ms step_avg:60.88ms
step:879/2245 train_time:53513ms step_avg:60.88ms
step:880/2245 train_time:53572ms step_avg:60.88ms
step:881/2245 train_time:53635ms step_avg:60.88ms
step:882/2245 train_time:53695ms step_avg:60.88ms
step:883/2245 train_time:53758ms step_avg:60.88ms
step:884/2245 train_time:53818ms step_avg:60.88ms
step:885/2245 train_time:53881ms step_avg:60.88ms
step:886/2245 train_time:53942ms step_avg:60.88ms
step:887/2245 train_time:54006ms step_avg:60.89ms
step:888/2245 train_time:54066ms step_avg:60.89ms
step:889/2245 train_time:54129ms step_avg:60.89ms
step:890/2245 train_time:54188ms step_avg:60.89ms
step:891/2245 train_time:54250ms step_avg:60.89ms
step:892/2245 train_time:54310ms step_avg:60.89ms
step:893/2245 train_time:54372ms step_avg:60.89ms
step:894/2245 train_time:54431ms step_avg:60.89ms
step:895/2245 train_time:54493ms step_avg:60.89ms
step:896/2245 train_time:54553ms step_avg:60.88ms
step:897/2245 train_time:54615ms step_avg:60.89ms
step:898/2245 train_time:54675ms step_avg:60.88ms
step:899/2245 train_time:54737ms step_avg:60.89ms
step:900/2245 train_time:54798ms step_avg:60.89ms
step:901/2245 train_time:54860ms step_avg:60.89ms
step:902/2245 train_time:54921ms step_avg:60.89ms
step:903/2245 train_time:54984ms step_avg:60.89ms
step:904/2245 train_time:55044ms step_avg:60.89ms
step:905/2245 train_time:55107ms step_avg:60.89ms
step:906/2245 train_time:55167ms step_avg:60.89ms
step:907/2245 train_time:55230ms step_avg:60.89ms
step:908/2245 train_time:55289ms step_avg:60.89ms
step:909/2245 train_time:55351ms step_avg:60.89ms
step:910/2245 train_time:55410ms step_avg:60.89ms
step:911/2245 train_time:55472ms step_avg:60.89ms
step:912/2245 train_time:55532ms step_avg:60.89ms
step:913/2245 train_time:55594ms step_avg:60.89ms
step:914/2245 train_time:55653ms step_avg:60.89ms
step:915/2245 train_time:55716ms step_avg:60.89ms
step:916/2245 train_time:55776ms step_avg:60.89ms
step:917/2245 train_time:55840ms step_avg:60.89ms
step:918/2245 train_time:55899ms step_avg:60.89ms
step:919/2245 train_time:55962ms step_avg:60.89ms
step:920/2245 train_time:56024ms step_avg:60.90ms
step:921/2245 train_time:56087ms step_avg:60.90ms
step:922/2245 train_time:56147ms step_avg:60.90ms
step:923/2245 train_time:56208ms step_avg:60.90ms
step:924/2245 train_time:56269ms step_avg:60.90ms
step:925/2245 train_time:56332ms step_avg:60.90ms
step:926/2245 train_time:56392ms step_avg:60.90ms
step:927/2245 train_time:56453ms step_avg:60.90ms
step:928/2245 train_time:56513ms step_avg:60.90ms
step:929/2245 train_time:56575ms step_avg:60.90ms
step:930/2245 train_time:56634ms step_avg:60.90ms
step:931/2245 train_time:56697ms step_avg:60.90ms
step:932/2245 train_time:56757ms step_avg:60.90ms
step:933/2245 train_time:56820ms step_avg:60.90ms
step:934/2245 train_time:56880ms step_avg:60.90ms
step:935/2245 train_time:56942ms step_avg:60.90ms
step:936/2245 train_time:57002ms step_avg:60.90ms
step:937/2245 train_time:57066ms step_avg:60.90ms
step:938/2245 train_time:57126ms step_avg:60.90ms
step:939/2245 train_time:57188ms step_avg:60.90ms
step:940/2245 train_time:57248ms step_avg:60.90ms
step:941/2245 train_time:57311ms step_avg:60.90ms
step:942/2245 train_time:57371ms step_avg:60.90ms
step:943/2245 train_time:57433ms step_avg:60.90ms
step:944/2245 train_time:57492ms step_avg:60.90ms
step:945/2245 train_time:57554ms step_avg:60.90ms
step:946/2245 train_time:57614ms step_avg:60.90ms
step:947/2245 train_time:57676ms step_avg:60.90ms
step:948/2245 train_time:57736ms step_avg:60.90ms
step:949/2245 train_time:57799ms step_avg:60.90ms
step:950/2245 train_time:57858ms step_avg:60.90ms
step:951/2245 train_time:57920ms step_avg:60.90ms
step:952/2245 train_time:57980ms step_avg:60.90ms
step:953/2245 train_time:58045ms step_avg:60.91ms
step:954/2245 train_time:58105ms step_avg:60.91ms
step:955/2245 train_time:58167ms step_avg:60.91ms
step:956/2245 train_time:58228ms step_avg:60.91ms
step:957/2245 train_time:58289ms step_avg:60.91ms
step:958/2245 train_time:58349ms step_avg:60.91ms
step:959/2245 train_time:58411ms step_avg:60.91ms
step:960/2245 train_time:58471ms step_avg:60.91ms
step:961/2245 train_time:58533ms step_avg:60.91ms
step:962/2245 train_time:58593ms step_avg:60.91ms
step:963/2245 train_time:58655ms step_avg:60.91ms
step:964/2245 train_time:58714ms step_avg:60.91ms
step:965/2245 train_time:58777ms step_avg:60.91ms
step:966/2245 train_time:58837ms step_avg:60.91ms
step:967/2245 train_time:58900ms step_avg:60.91ms
step:968/2245 train_time:58960ms step_avg:60.91ms
step:969/2245 train_time:59023ms step_avg:60.91ms
step:970/2245 train_time:59084ms step_avg:60.91ms
step:971/2245 train_time:59147ms step_avg:60.91ms
step:972/2245 train_time:59207ms step_avg:60.91ms
step:973/2245 train_time:59269ms step_avg:60.91ms
step:974/2245 train_time:59329ms step_avg:60.91ms
step:975/2245 train_time:59392ms step_avg:60.92ms
step:976/2245 train_time:59451ms step_avg:60.91ms
step:977/2245 train_time:59513ms step_avg:60.91ms
step:978/2245 train_time:59572ms step_avg:60.91ms
step:979/2245 train_time:59634ms step_avg:60.91ms
step:980/2245 train_time:59693ms step_avg:60.91ms
step:981/2245 train_time:59757ms step_avg:60.91ms
step:982/2245 train_time:59817ms step_avg:60.91ms
step:983/2245 train_time:59879ms step_avg:60.91ms
step:984/2245 train_time:59939ms step_avg:60.91ms
step:985/2245 train_time:60002ms step_avg:60.92ms
step:986/2245 train_time:60063ms step_avg:60.92ms
step:987/2245 train_time:60126ms step_avg:60.92ms
step:988/2245 train_time:60186ms step_avg:60.92ms
step:989/2245 train_time:60248ms step_avg:60.92ms
step:990/2245 train_time:60309ms step_avg:60.92ms
step:991/2245 train_time:60371ms step_avg:60.92ms
step:992/2245 train_time:60432ms step_avg:60.92ms
step:993/2245 train_time:60493ms step_avg:60.92ms
step:994/2245 train_time:60553ms step_avg:60.92ms
step:995/2245 train_time:60615ms step_avg:60.92ms
step:996/2245 train_time:60675ms step_avg:60.92ms
step:997/2245 train_time:60738ms step_avg:60.92ms
step:998/2245 train_time:60797ms step_avg:60.92ms
step:999/2245 train_time:60859ms step_avg:60.92ms
step:1000/2245 train_time:60920ms step_avg:60.92ms
step:1000/2245 val_loss:3.5950 train_time:60983ms step_avg:60.98ms
step:1001/2245 train_time:61002ms step_avg:60.94ms
step:1002/2245 train_time:61046ms step_avg:60.92ms
step:1003/2245 train_time:61110ms step_avg:60.93ms
step:1004/2245 train_time:61172ms step_avg:60.93ms
step:1005/2245 train_time:61234ms step_avg:60.93ms
step:1006/2245 train_time:61295ms step_avg:60.93ms
step:1007/2245 train_time:61357ms step_avg:60.93ms
step:1008/2245 train_time:61417ms step_avg:60.93ms
step:1009/2245 train_time:61479ms step_avg:60.93ms
step:1010/2245 train_time:61538ms step_avg:60.93ms
step:1011/2245 train_time:61600ms step_avg:60.93ms
step:1012/2245 train_time:61660ms step_avg:60.93ms
step:1013/2245 train_time:61721ms step_avg:60.93ms
step:1014/2245 train_time:61780ms step_avg:60.93ms
step:1015/2245 train_time:61841ms step_avg:60.93ms
step:1016/2245 train_time:61901ms step_avg:60.93ms
step:1017/2245 train_time:61965ms step_avg:60.93ms
step:1018/2245 train_time:62025ms step_avg:60.93ms
step:1019/2245 train_time:62089ms step_avg:60.93ms
step:1020/2245 train_time:62149ms step_avg:60.93ms
step:1021/2245 train_time:62212ms step_avg:60.93ms
step:1022/2245 train_time:62273ms step_avg:60.93ms
step:1023/2245 train_time:62335ms step_avg:60.93ms
step:1024/2245 train_time:62395ms step_avg:60.93ms
step:1025/2245 train_time:62458ms step_avg:60.93ms
step:1026/2245 train_time:62517ms step_avg:60.93ms
step:1027/2245 train_time:62579ms step_avg:60.93ms
step:1028/2245 train_time:62638ms step_avg:60.93ms
step:1029/2245 train_time:62700ms step_avg:60.93ms
step:1030/2245 train_time:62760ms step_avg:60.93ms
step:1031/2245 train_time:62821ms step_avg:60.93ms
step:1032/2245 train_time:62881ms step_avg:60.93ms
step:1033/2245 train_time:62944ms step_avg:60.93ms
step:1034/2245 train_time:63004ms step_avg:60.93ms
step:1035/2245 train_time:63067ms step_avg:60.93ms
step:1036/2245 train_time:63127ms step_avg:60.93ms
step:1037/2245 train_time:63190ms step_avg:60.93ms
step:1038/2245 train_time:63250ms step_avg:60.93ms
step:1039/2245 train_time:63312ms step_avg:60.94ms
step:1040/2245 train_time:63373ms step_avg:60.94ms
step:1041/2245 train_time:63436ms step_avg:60.94ms
step:1042/2245 train_time:63496ms step_avg:60.94ms
step:1043/2245 train_time:63558ms step_avg:60.94ms
step:1044/2245 train_time:63618ms step_avg:60.94ms
step:1045/2245 train_time:63680ms step_avg:60.94ms
step:1046/2245 train_time:63740ms step_avg:60.94ms
step:1047/2245 train_time:63802ms step_avg:60.94ms
step:1048/2245 train_time:63862ms step_avg:60.94ms
step:1049/2245 train_time:63924ms step_avg:60.94ms
step:1050/2245 train_time:63984ms step_avg:60.94ms
step:1051/2245 train_time:64046ms step_avg:60.94ms
step:1052/2245 train_time:64106ms step_avg:60.94ms
step:1053/2245 train_time:64169ms step_avg:60.94ms
step:1054/2245 train_time:64229ms step_avg:60.94ms
step:1055/2245 train_time:64292ms step_avg:60.94ms
step:1056/2245 train_time:64352ms step_avg:60.94ms
step:1057/2245 train_time:64415ms step_avg:60.94ms
step:1058/2245 train_time:64474ms step_avg:60.94ms
step:1059/2245 train_time:64536ms step_avg:60.94ms
step:1060/2245 train_time:64596ms step_avg:60.94ms
step:1061/2245 train_time:64659ms step_avg:60.94ms
step:1062/2245 train_time:64718ms step_avg:60.94ms
step:1063/2245 train_time:64781ms step_avg:60.94ms
step:1064/2245 train_time:64840ms step_avg:60.94ms
step:1065/2245 train_time:64902ms step_avg:60.94ms
step:1066/2245 train_time:64963ms step_avg:60.94ms
step:1067/2245 train_time:65025ms step_avg:60.94ms
step:1068/2245 train_time:65085ms step_avg:60.94ms
step:1069/2245 train_time:65147ms step_avg:60.94ms
step:1070/2245 train_time:65207ms step_avg:60.94ms
step:1071/2245 train_time:65271ms step_avg:60.94ms
step:1072/2245 train_time:65331ms step_avg:60.94ms
step:1073/2245 train_time:65394ms step_avg:60.94ms
step:1074/2245 train_time:65454ms step_avg:60.94ms
step:1075/2245 train_time:65517ms step_avg:60.95ms
step:1076/2245 train_time:65577ms step_avg:60.95ms
step:1077/2245 train_time:65640ms step_avg:60.95ms
step:1078/2245 train_time:65699ms step_avg:60.95ms
step:1079/2245 train_time:65762ms step_avg:60.95ms
step:1080/2245 train_time:65821ms step_avg:60.95ms
step:1081/2245 train_time:65883ms step_avg:60.95ms
step:1082/2245 train_time:65943ms step_avg:60.95ms
step:1083/2245 train_time:66005ms step_avg:60.95ms
step:1084/2245 train_time:66065ms step_avg:60.95ms
step:1085/2245 train_time:66127ms step_avg:60.95ms
step:1086/2245 train_time:66188ms step_avg:60.95ms
step:1087/2245 train_time:66250ms step_avg:60.95ms
step:1088/2245 train_time:66310ms step_avg:60.95ms
step:1089/2245 train_time:66373ms step_avg:60.95ms
step:1090/2245 train_time:66433ms step_avg:60.95ms
step:1091/2245 train_time:66495ms step_avg:60.95ms
step:1092/2245 train_time:66556ms step_avg:60.95ms
step:1093/2245 train_time:66618ms step_avg:60.95ms
step:1094/2245 train_time:66679ms step_avg:60.95ms
step:1095/2245 train_time:66741ms step_avg:60.95ms
step:1096/2245 train_time:66801ms step_avg:60.95ms
step:1097/2245 train_time:66864ms step_avg:60.95ms
step:1098/2245 train_time:66924ms step_avg:60.95ms
step:1099/2245 train_time:66986ms step_avg:60.95ms
step:1100/2245 train_time:67045ms step_avg:60.95ms
step:1101/2245 train_time:67108ms step_avg:60.95ms
step:1102/2245 train_time:67168ms step_avg:60.95ms
step:1103/2245 train_time:67230ms step_avg:60.95ms
step:1104/2245 train_time:67290ms step_avg:60.95ms
step:1105/2245 train_time:67353ms step_avg:60.95ms
step:1106/2245 train_time:67413ms step_avg:60.95ms
step:1107/2245 train_time:67476ms step_avg:60.95ms
step:1108/2245 train_time:67536ms step_avg:60.95ms
step:1109/2245 train_time:67599ms step_avg:60.95ms
step:1110/2245 train_time:67660ms step_avg:60.95ms
step:1111/2245 train_time:67722ms step_avg:60.96ms
step:1112/2245 train_time:67782ms step_avg:60.96ms
step:1113/2245 train_time:67845ms step_avg:60.96ms
step:1114/2245 train_time:67905ms step_avg:60.96ms
step:1115/2245 train_time:67967ms step_avg:60.96ms
step:1116/2245 train_time:68027ms step_avg:60.96ms
step:1117/2245 train_time:68089ms step_avg:60.96ms
step:1118/2245 train_time:68149ms step_avg:60.96ms
step:1119/2245 train_time:68211ms step_avg:60.96ms
step:1120/2245 train_time:68271ms step_avg:60.96ms
step:1121/2245 train_time:68333ms step_avg:60.96ms
step:1122/2245 train_time:68394ms step_avg:60.96ms
step:1123/2245 train_time:68456ms step_avg:60.96ms
step:1124/2245 train_time:68517ms step_avg:60.96ms
step:1125/2245 train_time:68579ms step_avg:60.96ms
step:1126/2245 train_time:68639ms step_avg:60.96ms
step:1127/2245 train_time:68701ms step_avg:60.96ms
step:1128/2245 train_time:68761ms step_avg:60.96ms
step:1129/2245 train_time:68824ms step_avg:60.96ms
step:1130/2245 train_time:68884ms step_avg:60.96ms
step:1131/2245 train_time:68946ms step_avg:60.96ms
step:1132/2245 train_time:69005ms step_avg:60.96ms
step:1133/2245 train_time:69068ms step_avg:60.96ms
step:1134/2245 train_time:69128ms step_avg:60.96ms
step:1135/2245 train_time:69190ms step_avg:60.96ms
step:1136/2245 train_time:69250ms step_avg:60.96ms
step:1137/2245 train_time:69312ms step_avg:60.96ms
step:1138/2245 train_time:69372ms step_avg:60.96ms
step:1139/2245 train_time:69434ms step_avg:60.96ms
step:1140/2245 train_time:69495ms step_avg:60.96ms
step:1141/2245 train_time:69559ms step_avg:60.96ms
step:1142/2245 train_time:69619ms step_avg:60.96ms
step:1143/2245 train_time:69681ms step_avg:60.96ms
step:1144/2245 train_time:69741ms step_avg:60.96ms
step:1145/2245 train_time:69804ms step_avg:60.96ms
step:1146/2245 train_time:69864ms step_avg:60.96ms
step:1147/2245 train_time:69926ms step_avg:60.96ms
step:1148/2245 train_time:69986ms step_avg:60.96ms
step:1149/2245 train_time:70049ms step_avg:60.97ms
step:1150/2245 train_time:70109ms step_avg:60.96ms
step:1151/2245 train_time:70172ms step_avg:60.97ms
step:1152/2245 train_time:70231ms step_avg:60.96ms
step:1153/2245 train_time:70294ms step_avg:60.97ms
step:1154/2245 train_time:70353ms step_avg:60.96ms
step:1155/2245 train_time:70416ms step_avg:60.97ms
step:1156/2245 train_time:70476ms step_avg:60.97ms
step:1157/2245 train_time:70539ms step_avg:60.97ms
step:1158/2245 train_time:70598ms step_avg:60.97ms
step:1159/2245 train_time:70661ms step_avg:60.97ms
step:1160/2245 train_time:70720ms step_avg:60.97ms
step:1161/2245 train_time:70783ms step_avg:60.97ms
step:1162/2245 train_time:70843ms step_avg:60.97ms
step:1163/2245 train_time:70905ms step_avg:60.97ms
step:1164/2245 train_time:70966ms step_avg:60.97ms
step:1165/2245 train_time:71028ms step_avg:60.97ms
step:1166/2245 train_time:71088ms step_avg:60.97ms
step:1167/2245 train_time:71150ms step_avg:60.97ms
step:1168/2245 train_time:71210ms step_avg:60.97ms
step:1169/2245 train_time:71273ms step_avg:60.97ms
step:1170/2245 train_time:71333ms step_avg:60.97ms
step:1171/2245 train_time:71395ms step_avg:60.97ms
step:1172/2245 train_time:71456ms step_avg:60.97ms
step:1173/2245 train_time:71519ms step_avg:60.97ms
step:1174/2245 train_time:71579ms step_avg:60.97ms
step:1175/2245 train_time:71641ms step_avg:60.97ms
step:1176/2245 train_time:71701ms step_avg:60.97ms
step:1177/2245 train_time:71764ms step_avg:60.97ms
step:1178/2245 train_time:71824ms step_avg:60.97ms
step:1179/2245 train_time:71886ms step_avg:60.97ms
step:1180/2245 train_time:71947ms step_avg:60.97ms
step:1181/2245 train_time:72009ms step_avg:60.97ms
step:1182/2245 train_time:72069ms step_avg:60.97ms
step:1183/2245 train_time:72131ms step_avg:60.97ms
step:1184/2245 train_time:72191ms step_avg:60.97ms
step:1185/2245 train_time:72254ms step_avg:60.97ms
step:1186/2245 train_time:72314ms step_avg:60.97ms
step:1187/2245 train_time:72376ms step_avg:60.97ms
step:1188/2245 train_time:72437ms step_avg:60.97ms
step:1189/2245 train_time:72499ms step_avg:60.98ms
step:1190/2245 train_time:72559ms step_avg:60.97ms
step:1191/2245 train_time:72622ms step_avg:60.98ms
step:1192/2245 train_time:72681ms step_avg:60.97ms
step:1193/2245 train_time:72744ms step_avg:60.98ms
step:1194/2245 train_time:72804ms step_avg:60.97ms
step:1195/2245 train_time:72866ms step_avg:60.98ms
step:1196/2245 train_time:72927ms step_avg:60.98ms
step:1197/2245 train_time:72988ms step_avg:60.98ms
step:1198/2245 train_time:73048ms step_avg:60.97ms
step:1199/2245 train_time:73110ms step_avg:60.98ms
step:1200/2245 train_time:73170ms step_avg:60.97ms
step:1201/2245 train_time:73232ms step_avg:60.98ms
step:1202/2245 train_time:73292ms step_avg:60.98ms
step:1203/2245 train_time:73355ms step_avg:60.98ms
step:1204/2245 train_time:73414ms step_avg:60.98ms
step:1205/2245 train_time:73478ms step_avg:60.98ms
step:1206/2245 train_time:73538ms step_avg:60.98ms
step:1207/2245 train_time:73601ms step_avg:60.98ms
step:1208/2245 train_time:73661ms step_avg:60.98ms
step:1209/2245 train_time:73724ms step_avg:60.98ms
step:1210/2245 train_time:73784ms step_avg:60.98ms
step:1211/2245 train_time:73847ms step_avg:60.98ms
step:1212/2245 train_time:73906ms step_avg:60.98ms
step:1213/2245 train_time:73968ms step_avg:60.98ms
step:1214/2245 train_time:74028ms step_avg:60.98ms
step:1215/2245 train_time:74091ms step_avg:60.98ms
step:1216/2245 train_time:74151ms step_avg:60.98ms
step:1217/2245 train_time:74214ms step_avg:60.98ms
step:1218/2245 train_time:74274ms step_avg:60.98ms
step:1219/2245 train_time:74337ms step_avg:60.98ms
step:1220/2245 train_time:74397ms step_avg:60.98ms
step:1221/2245 train_time:74460ms step_avg:60.98ms
step:1222/2245 train_time:74519ms step_avg:60.98ms
step:1223/2245 train_time:74581ms step_avg:60.98ms
step:1224/2245 train_time:74642ms step_avg:60.98ms
step:1225/2245 train_time:74704ms step_avg:60.98ms
step:1226/2245 train_time:74765ms step_avg:60.98ms
step:1227/2245 train_time:74827ms step_avg:60.98ms
step:1228/2245 train_time:74886ms step_avg:60.98ms
step:1229/2245 train_time:74949ms step_avg:60.98ms
step:1230/2245 train_time:75008ms step_avg:60.98ms
step:1231/2245 train_time:75071ms step_avg:60.98ms
step:1232/2245 train_time:75131ms step_avg:60.98ms
step:1233/2245 train_time:75194ms step_avg:60.98ms
step:1234/2245 train_time:75255ms step_avg:60.98ms
step:1235/2245 train_time:75318ms step_avg:60.99ms
step:1236/2245 train_time:75378ms step_avg:60.99ms
step:1237/2245 train_time:75440ms step_avg:60.99ms
step:1238/2245 train_time:75499ms step_avg:60.99ms
step:1239/2245 train_time:75561ms step_avg:60.99ms
step:1240/2245 train_time:75621ms step_avg:60.98ms
step:1241/2245 train_time:75684ms step_avg:60.99ms
step:1242/2245 train_time:75744ms step_avg:60.99ms
step:1243/2245 train_time:75806ms step_avg:60.99ms
step:1244/2245 train_time:75866ms step_avg:60.99ms
step:1245/2245 train_time:75928ms step_avg:60.99ms
step:1246/2245 train_time:75988ms step_avg:60.99ms
step:1247/2245 train_time:76050ms step_avg:60.99ms
step:1248/2245 train_time:76110ms step_avg:60.99ms
step:1249/2245 train_time:76173ms step_avg:60.99ms
step:1250/2245 train_time:76233ms step_avg:60.99ms
step:1250/2245 val_loss:3.5231 train_time:76297ms step_avg:61.04ms
step:1251/2245 train_time:76315ms step_avg:61.00ms
step:1252/2245 train_time:76358ms step_avg:60.99ms
step:1253/2245 train_time:76424ms step_avg:60.99ms
step:1254/2245 train_time:76486ms step_avg:60.99ms
step:1255/2245 train_time:76547ms step_avg:60.99ms
step:1256/2245 train_time:76606ms step_avg:60.99ms
step:1257/2245 train_time:76668ms step_avg:60.99ms
step:1258/2245 train_time:76727ms step_avg:60.99ms
step:1259/2245 train_time:76789ms step_avg:60.99ms
step:1260/2245 train_time:76848ms step_avg:60.99ms
step:1261/2245 train_time:76909ms step_avg:60.99ms
step:1262/2245 train_time:76968ms step_avg:60.99ms
step:1263/2245 train_time:77030ms step_avg:60.99ms
step:1264/2245 train_time:77090ms step_avg:60.99ms
step:1265/2245 train_time:77152ms step_avg:60.99ms
step:1266/2245 train_time:77214ms step_avg:60.99ms
step:1267/2245 train_time:77278ms step_avg:60.99ms
step:1268/2245 train_time:77339ms step_avg:60.99ms
step:1269/2245 train_time:77402ms step_avg:60.99ms
step:1270/2245 train_time:77464ms step_avg:61.00ms
step:1271/2245 train_time:77526ms step_avg:61.00ms
step:1272/2245 train_time:77587ms step_avg:61.00ms
step:1273/2245 train_time:77648ms step_avg:61.00ms
step:1274/2245 train_time:77707ms step_avg:60.99ms
step:1275/2245 train_time:77769ms step_avg:61.00ms
step:1276/2245 train_time:77828ms step_avg:60.99ms
step:1277/2245 train_time:77890ms step_avg:60.99ms
step:1278/2245 train_time:77949ms step_avg:60.99ms
step:1279/2245 train_time:78010ms step_avg:60.99ms
step:1280/2245 train_time:78070ms step_avg:60.99ms
step:1281/2245 train_time:78132ms step_avg:60.99ms
step:1282/2245 train_time:78192ms step_avg:60.99ms
step:1283/2245 train_time:78255ms step_avg:60.99ms
step:1284/2245 train_time:78316ms step_avg:60.99ms
step:1285/2245 train_time:78379ms step_avg:61.00ms
step:1286/2245 train_time:78439ms step_avg:60.99ms
step:1287/2245 train_time:78502ms step_avg:61.00ms
step:1288/2245 train_time:78562ms step_avg:61.00ms
step:1289/2245 train_time:78625ms step_avg:61.00ms
step:1290/2245 train_time:78686ms step_avg:61.00ms
step:1291/2245 train_time:78748ms step_avg:61.00ms
step:1292/2245 train_time:78807ms step_avg:61.00ms
step:1293/2245 train_time:78869ms step_avg:61.00ms
step:1294/2245 train_time:78929ms step_avg:61.00ms
step:1295/2245 train_time:78990ms step_avg:61.00ms
step:1296/2245 train_time:79049ms step_avg:60.99ms
step:1297/2245 train_time:79111ms step_avg:61.00ms
step:1298/2245 train_time:79171ms step_avg:60.99ms
step:1299/2245 train_time:79233ms step_avg:61.00ms
step:1300/2245 train_time:79293ms step_avg:60.99ms
step:1301/2245 train_time:79356ms step_avg:61.00ms
step:1302/2245 train_time:79416ms step_avg:61.00ms
step:1303/2245 train_time:79479ms step_avg:61.00ms
step:1304/2245 train_time:79539ms step_avg:61.00ms
step:1305/2245 train_time:79602ms step_avg:61.00ms
step:1306/2245 train_time:79662ms step_avg:61.00ms
step:1307/2245 train_time:79725ms step_avg:61.00ms
step:1308/2245 train_time:79785ms step_avg:61.00ms
step:1309/2245 train_time:79847ms step_avg:61.00ms
step:1310/2245 train_time:79907ms step_avg:61.00ms
step:1311/2245 train_time:79969ms step_avg:61.00ms
step:1312/2245 train_time:80028ms step_avg:61.00ms
step:1313/2245 train_time:80091ms step_avg:61.00ms
step:1314/2245 train_time:80151ms step_avg:61.00ms
step:1315/2245 train_time:80212ms step_avg:61.00ms
step:1316/2245 train_time:80272ms step_avg:61.00ms
step:1317/2245 train_time:80335ms step_avg:61.00ms
step:1318/2245 train_time:80395ms step_avg:61.00ms
step:1319/2245 train_time:80458ms step_avg:61.00ms
step:1320/2245 train_time:80518ms step_avg:61.00ms
step:1321/2245 train_time:80581ms step_avg:61.00ms
step:1322/2245 train_time:80642ms step_avg:61.00ms
step:1323/2245 train_time:80704ms step_avg:61.00ms
step:1324/2245 train_time:80764ms step_avg:61.00ms
step:1325/2245 train_time:80827ms step_avg:61.00ms
step:1326/2245 train_time:80887ms step_avg:61.00ms
step:1327/2245 train_time:80949ms step_avg:61.00ms
step:1328/2245 train_time:81009ms step_avg:61.00ms
step:1329/2245 train_time:81072ms step_avg:61.00ms
step:1330/2245 train_time:81131ms step_avg:61.00ms
step:1331/2245 train_time:81194ms step_avg:61.00ms
step:1332/2245 train_time:81253ms step_avg:61.00ms
step:1333/2245 train_time:81315ms step_avg:61.00ms
step:1334/2245 train_time:81376ms step_avg:61.00ms
step:1335/2245 train_time:81438ms step_avg:61.00ms
step:1336/2245 train_time:81498ms step_avg:61.00ms
step:1337/2245 train_time:81562ms step_avg:61.00ms
step:1338/2245 train_time:81622ms step_avg:61.00ms
step:1339/2245 train_time:81684ms step_avg:61.00ms
step:1340/2245 train_time:81745ms step_avg:61.00ms
step:1341/2245 train_time:81807ms step_avg:61.00ms
step:1342/2245 train_time:81867ms step_avg:61.00ms
step:1343/2245 train_time:81930ms step_avg:61.01ms
step:1344/2245 train_time:81990ms step_avg:61.00ms
step:1345/2245 train_time:82051ms step_avg:61.00ms
step:1346/2245 train_time:82111ms step_avg:61.00ms
step:1347/2245 train_time:82172ms step_avg:61.00ms
step:1348/2245 train_time:82232ms step_avg:61.00ms
step:1349/2245 train_time:82295ms step_avg:61.00ms
step:1350/2245 train_time:82354ms step_avg:61.00ms
step:1351/2245 train_time:82417ms step_avg:61.00ms
step:1352/2245 train_time:82477ms step_avg:61.00ms
step:1353/2245 train_time:82540ms step_avg:61.01ms
step:1354/2245 train_time:82600ms step_avg:61.00ms
step:1355/2245 train_time:82663ms step_avg:61.01ms
step:1356/2245 train_time:82722ms step_avg:61.00ms
step:1357/2245 train_time:82785ms step_avg:61.01ms
step:1358/2245 train_time:82845ms step_avg:61.00ms
step:1359/2245 train_time:82908ms step_avg:61.01ms
step:1360/2245 train_time:82968ms step_avg:61.01ms
step:1361/2245 train_time:83030ms step_avg:61.01ms
step:1362/2245 train_time:83092ms step_avg:61.01ms
step:1363/2245 train_time:83153ms step_avg:61.01ms
step:1364/2245 train_time:83213ms step_avg:61.01ms
step:1365/2245 train_time:83276ms step_avg:61.01ms
step:1366/2245 train_time:83335ms step_avg:61.01ms
step:1367/2245 train_time:83398ms step_avg:61.01ms
step:1368/2245 train_time:83457ms step_avg:61.01ms
step:1369/2245 train_time:83519ms step_avg:61.01ms
step:1370/2245 train_time:83579ms step_avg:61.01ms
step:1371/2245 train_time:83641ms step_avg:61.01ms
step:1372/2245 train_time:83702ms step_avg:61.01ms
step:1373/2245 train_time:83765ms step_avg:61.01ms
step:1374/2245 train_time:83826ms step_avg:61.01ms
step:1375/2245 train_time:83888ms step_avg:61.01ms
step:1376/2245 train_time:83948ms step_avg:61.01ms
step:1377/2245 train_time:84010ms step_avg:61.01ms
step:1378/2245 train_time:84070ms step_avg:61.01ms
step:1379/2245 train_time:84133ms step_avg:61.01ms
step:1380/2245 train_time:84193ms step_avg:61.01ms
step:1381/2245 train_time:84254ms step_avg:61.01ms
step:1382/2245 train_time:84314ms step_avg:61.01ms
step:1383/2245 train_time:84376ms step_avg:61.01ms
step:1384/2245 train_time:84437ms step_avg:61.01ms
step:1385/2245 train_time:84500ms step_avg:61.01ms
step:1386/2245 train_time:84560ms step_avg:61.01ms
step:1387/2245 train_time:84623ms step_avg:61.01ms
step:1388/2245 train_time:84683ms step_avg:61.01ms
step:1389/2245 train_time:84745ms step_avg:61.01ms
step:1390/2245 train_time:84805ms step_avg:61.01ms
step:1391/2245 train_time:84868ms step_avg:61.01ms
step:1392/2245 train_time:84927ms step_avg:61.01ms
step:1393/2245 train_time:84990ms step_avg:61.01ms
step:1394/2245 train_time:85050ms step_avg:61.01ms
step:1395/2245 train_time:85113ms step_avg:61.01ms
step:1396/2245 train_time:85172ms step_avg:61.01ms
step:1397/2245 train_time:85234ms step_avg:61.01ms
step:1398/2245 train_time:85294ms step_avg:61.01ms
step:1399/2245 train_time:85356ms step_avg:61.01ms
step:1400/2245 train_time:85416ms step_avg:61.01ms
step:1401/2245 train_time:85478ms step_avg:61.01ms
step:1402/2245 train_time:85538ms step_avg:61.01ms
step:1403/2245 train_time:85600ms step_avg:61.01ms
step:1404/2245 train_time:85660ms step_avg:61.01ms
step:1405/2245 train_time:85723ms step_avg:61.01ms
step:1406/2245 train_time:85784ms step_avg:61.01ms
step:1407/2245 train_time:85847ms step_avg:61.01ms
step:1408/2245 train_time:85907ms step_avg:61.01ms
step:1409/2245 train_time:85969ms step_avg:61.01ms
step:1410/2245 train_time:86029ms step_avg:61.01ms
step:1411/2245 train_time:86092ms step_avg:61.02ms
step:1412/2245 train_time:86152ms step_avg:61.01ms
step:1413/2245 train_time:86214ms step_avg:61.01ms
step:1414/2245 train_time:86273ms step_avg:61.01ms
step:1415/2245 train_time:86336ms step_avg:61.01ms
step:1416/2245 train_time:86395ms step_avg:61.01ms
step:1417/2245 train_time:86457ms step_avg:61.01ms
step:1418/2245 train_time:86517ms step_avg:61.01ms
step:1419/2245 train_time:86580ms step_avg:61.01ms
step:1420/2245 train_time:86640ms step_avg:61.01ms
step:1421/2245 train_time:86703ms step_avg:61.02ms
step:1422/2245 train_time:86763ms step_avg:61.01ms
step:1423/2245 train_time:86826ms step_avg:61.02ms
step:1424/2245 train_time:86886ms step_avg:61.02ms
step:1425/2245 train_time:86949ms step_avg:61.02ms
step:1426/2245 train_time:87009ms step_avg:61.02ms
step:1427/2245 train_time:87072ms step_avg:61.02ms
step:1428/2245 train_time:87133ms step_avg:61.02ms
step:1429/2245 train_time:87194ms step_avg:61.02ms
step:1430/2245 train_time:87254ms step_avg:61.02ms
step:1431/2245 train_time:87316ms step_avg:61.02ms
step:1432/2245 train_time:87376ms step_avg:61.02ms
step:1433/2245 train_time:87438ms step_avg:61.02ms
step:1434/2245 train_time:87498ms step_avg:61.02ms
step:1435/2245 train_time:87561ms step_avg:61.02ms
step:1436/2245 train_time:87621ms step_avg:61.02ms
step:1437/2245 train_time:87683ms step_avg:61.02ms
step:1438/2245 train_time:87743ms step_avg:61.02ms
step:1439/2245 train_time:87807ms step_avg:61.02ms
step:1440/2245 train_time:87867ms step_avg:61.02ms
step:1441/2245 train_time:87930ms step_avg:61.02ms
step:1442/2245 train_time:87991ms step_avg:61.02ms
step:1443/2245 train_time:88053ms step_avg:61.02ms
step:1444/2245 train_time:88113ms step_avg:61.02ms
step:1445/2245 train_time:88176ms step_avg:61.02ms
step:1446/2245 train_time:88236ms step_avg:61.02ms
step:1447/2245 train_time:88299ms step_avg:61.02ms
step:1448/2245 train_time:88358ms step_avg:61.02ms
step:1449/2245 train_time:88420ms step_avg:61.02ms
step:1450/2245 train_time:88480ms step_avg:61.02ms
step:1451/2245 train_time:88542ms step_avg:61.02ms
step:1452/2245 train_time:88602ms step_avg:61.02ms
step:1453/2245 train_time:88664ms step_avg:61.02ms
step:1454/2245 train_time:88724ms step_avg:61.02ms
step:1455/2245 train_time:88786ms step_avg:61.02ms
step:1456/2245 train_time:88847ms step_avg:61.02ms
step:1457/2245 train_time:88909ms step_avg:61.02ms
step:1458/2245 train_time:88969ms step_avg:61.02ms
step:1459/2245 train_time:89031ms step_avg:61.02ms
step:1460/2245 train_time:89092ms step_avg:61.02ms
step:1461/2245 train_time:89154ms step_avg:61.02ms
step:1462/2245 train_time:89213ms step_avg:61.02ms
step:1463/2245 train_time:89276ms step_avg:61.02ms
step:1464/2245 train_time:89335ms step_avg:61.02ms
step:1465/2245 train_time:89397ms step_avg:61.02ms
step:1466/2245 train_time:89457ms step_avg:61.02ms
step:1467/2245 train_time:89519ms step_avg:61.02ms
step:1468/2245 train_time:89579ms step_avg:61.02ms
step:1469/2245 train_time:89642ms step_avg:61.02ms
step:1470/2245 train_time:89702ms step_avg:61.02ms
step:1471/2245 train_time:89765ms step_avg:61.02ms
step:1472/2245 train_time:89826ms step_avg:61.02ms
step:1473/2245 train_time:89889ms step_avg:61.02ms
step:1474/2245 train_time:89949ms step_avg:61.02ms
step:1475/2245 train_time:90013ms step_avg:61.03ms
step:1476/2245 train_time:90074ms step_avg:61.03ms
step:1477/2245 train_time:90135ms step_avg:61.03ms
step:1478/2245 train_time:90196ms step_avg:61.03ms
step:1479/2245 train_time:90259ms step_avg:61.03ms
step:1480/2245 train_time:90319ms step_avg:61.03ms
step:1481/2245 train_time:90382ms step_avg:61.03ms
step:1482/2245 train_time:90442ms step_avg:61.03ms
step:1483/2245 train_time:90505ms step_avg:61.03ms
step:1484/2245 train_time:90565ms step_avg:61.03ms
step:1485/2245 train_time:90628ms step_avg:61.03ms
step:1486/2245 train_time:90688ms step_avg:61.03ms
step:1487/2245 train_time:90751ms step_avg:61.03ms
step:1488/2245 train_time:90813ms step_avg:61.03ms
step:1489/2245 train_time:90876ms step_avg:61.03ms
step:1490/2245 train_time:90936ms step_avg:61.03ms
step:1491/2245 train_time:91000ms step_avg:61.03ms
step:1492/2245 train_time:91060ms step_avg:61.03ms
step:1493/2245 train_time:91123ms step_avg:61.03ms
step:1494/2245 train_time:91185ms step_avg:61.03ms
step:1495/2245 train_time:91248ms step_avg:61.04ms
step:1496/2245 train_time:91308ms step_avg:61.03ms
step:1497/2245 train_time:91370ms step_avg:61.04ms
step:1498/2245 train_time:91431ms step_avg:61.04ms
step:1499/2245 train_time:91493ms step_avg:61.04ms
step:1500/2245 train_time:91553ms step_avg:61.04ms
step:1500/2245 val_loss:3.4424 train_time:91617ms step_avg:61.08ms
step:1501/2245 train_time:91635ms step_avg:61.05ms
step:1502/2245 train_time:91683ms step_avg:61.04ms
step:1503/2245 train_time:91744ms step_avg:61.04ms
step:1504/2245 train_time:91804ms step_avg:61.04ms
step:1505/2245 train_time:91867ms step_avg:61.04ms
step:1506/2245 train_time:91927ms step_avg:61.04ms
step:1507/2245 train_time:91989ms step_avg:61.04ms
step:1508/2245 train_time:92048ms step_avg:61.04ms
step:1509/2245 train_time:92110ms step_avg:61.04ms
step:1510/2245 train_time:92169ms step_avg:61.04ms
step:1511/2245 train_time:92231ms step_avg:61.04ms
step:1512/2245 train_time:92291ms step_avg:61.04ms
step:1513/2245 train_time:92353ms step_avg:61.04ms
step:1514/2245 train_time:92413ms step_avg:61.04ms
step:1515/2245 train_time:92476ms step_avg:61.04ms
step:1516/2245 train_time:92541ms step_avg:61.04ms
step:1517/2245 train_time:92608ms step_avg:61.05ms
step:1518/2245 train_time:92670ms step_avg:61.05ms
step:1519/2245 train_time:92733ms step_avg:61.05ms
step:1520/2245 train_time:92794ms step_avg:61.05ms
step:1521/2245 train_time:92857ms step_avg:61.05ms
step:1522/2245 train_time:92917ms step_avg:61.05ms
step:1523/2245 train_time:92979ms step_avg:61.05ms
step:1524/2245 train_time:93039ms step_avg:61.05ms
step:1525/2245 train_time:93102ms step_avg:61.05ms
step:1526/2245 train_time:93161ms step_avg:61.05ms
step:1527/2245 train_time:93224ms step_avg:61.05ms
step:1528/2245 train_time:93284ms step_avg:61.05ms
step:1529/2245 train_time:93346ms step_avg:61.05ms
step:1530/2245 train_time:93408ms step_avg:61.05ms
step:1531/2245 train_time:93472ms step_avg:61.05ms
step:1532/2245 train_time:93534ms step_avg:61.05ms
step:1533/2245 train_time:93598ms step_avg:61.06ms
step:1534/2245 train_time:93660ms step_avg:61.06ms
step:1535/2245 train_time:93723ms step_avg:61.06ms
step:1536/2245 train_time:93784ms step_avg:61.06ms
step:1537/2245 train_time:93848ms step_avg:61.06ms
step:1538/2245 train_time:93908ms step_avg:61.06ms
step:1539/2245 train_time:93970ms step_avg:61.06ms
step:1540/2245 train_time:94030ms step_avg:61.06ms
step:1541/2245 train_time:94092ms step_avg:61.06ms
step:1542/2245 train_time:94152ms step_avg:61.06ms
step:1543/2245 train_time:94214ms step_avg:61.06ms
step:1544/2245 train_time:94274ms step_avg:61.06ms
step:1545/2245 train_time:94336ms step_avg:61.06ms
step:1546/2245 train_time:94396ms step_avg:61.06ms
step:1547/2245 train_time:94459ms step_avg:61.06ms
step:1548/2245 train_time:94520ms step_avg:61.06ms
step:1549/2245 train_time:94584ms step_avg:61.06ms
step:1550/2245 train_time:94645ms step_avg:61.06ms
step:1551/2245 train_time:94709ms step_avg:61.06ms
step:1552/2245 train_time:94769ms step_avg:61.06ms
step:1553/2245 train_time:94832ms step_avg:61.06ms
step:1554/2245 train_time:94893ms step_avg:61.06ms
step:1555/2245 train_time:94956ms step_avg:61.06ms
step:1556/2245 train_time:95015ms step_avg:61.06ms
step:1557/2245 train_time:95078ms step_avg:61.06ms
step:1558/2245 train_time:95137ms step_avg:61.06ms
step:1559/2245 train_time:95200ms step_avg:61.06ms
step:1560/2245 train_time:95259ms step_avg:61.06ms
step:1561/2245 train_time:95323ms step_avg:61.07ms
step:1562/2245 train_time:95384ms step_avg:61.07ms
step:1563/2245 train_time:95446ms step_avg:61.07ms
step:1564/2245 train_time:95507ms step_avg:61.07ms
step:1565/2245 train_time:95571ms step_avg:61.07ms
step:1566/2245 train_time:95631ms step_avg:61.07ms
step:1567/2245 train_time:95695ms step_avg:61.07ms
step:1568/2245 train_time:95756ms step_avg:61.07ms
step:1569/2245 train_time:95819ms step_avg:61.07ms
step:1570/2245 train_time:95879ms step_avg:61.07ms
step:1571/2245 train_time:95943ms step_avg:61.07ms
step:1572/2245 train_time:96003ms step_avg:61.07ms
step:1573/2245 train_time:96066ms step_avg:61.07ms
step:1574/2245 train_time:96126ms step_avg:61.07ms
step:1575/2245 train_time:96190ms step_avg:61.07ms
step:1576/2245 train_time:96250ms step_avg:61.07ms
step:1577/2245 train_time:96312ms step_avg:61.07ms
step:1578/2245 train_time:96373ms step_avg:61.07ms
step:1579/2245 train_time:96435ms step_avg:61.07ms
step:1580/2245 train_time:96496ms step_avg:61.07ms
step:1581/2245 train_time:96559ms step_avg:61.07ms
step:1582/2245 train_time:96619ms step_avg:61.07ms
step:1583/2245 train_time:96683ms step_avg:61.08ms
step:1584/2245 train_time:96744ms step_avg:61.08ms
step:1585/2245 train_time:96808ms step_avg:61.08ms
step:1586/2245 train_time:96868ms step_avg:61.08ms
step:1587/2245 train_time:96931ms step_avg:61.08ms
step:1588/2245 train_time:96991ms step_avg:61.08ms
step:1589/2245 train_time:97053ms step_avg:61.08ms
step:1590/2245 train_time:97113ms step_avg:61.08ms
step:1591/2245 train_time:97176ms step_avg:61.08ms
step:1592/2245 train_time:97236ms step_avg:61.08ms
step:1593/2245 train_time:97299ms step_avg:61.08ms
step:1594/2245 train_time:97360ms step_avg:61.08ms
step:1595/2245 train_time:97423ms step_avg:61.08ms
step:1596/2245 train_time:97484ms step_avg:61.08ms
step:1597/2245 train_time:97548ms step_avg:61.08ms
step:1598/2245 train_time:97608ms step_avg:61.08ms
step:1599/2245 train_time:97671ms step_avg:61.08ms
step:1600/2245 train_time:97731ms step_avg:61.08ms
step:1601/2245 train_time:97794ms step_avg:61.08ms
step:1602/2245 train_time:97854ms step_avg:61.08ms
step:1603/2245 train_time:97916ms step_avg:61.08ms
step:1604/2245 train_time:97976ms step_avg:61.08ms
step:1605/2245 train_time:98039ms step_avg:61.08ms
step:1606/2245 train_time:98099ms step_avg:61.08ms
step:1607/2245 train_time:98161ms step_avg:61.08ms
step:1608/2245 train_time:98221ms step_avg:61.08ms
step:1609/2245 train_time:98284ms step_avg:61.08ms
step:1610/2245 train_time:98344ms step_avg:61.08ms
step:1611/2245 train_time:98407ms step_avg:61.08ms
step:1612/2245 train_time:98467ms step_avg:61.08ms
step:1613/2245 train_time:98531ms step_avg:61.09ms
step:1614/2245 train_time:98592ms step_avg:61.09ms
step:1615/2245 train_time:98654ms step_avg:61.09ms
step:1616/2245 train_time:98715ms step_avg:61.09ms
step:1617/2245 train_time:98779ms step_avg:61.09ms
step:1618/2245 train_time:98840ms step_avg:61.09ms
step:1619/2245 train_time:98903ms step_avg:61.09ms
step:1620/2245 train_time:98964ms step_avg:61.09ms
step:1621/2245 train_time:99026ms step_avg:61.09ms
step:1622/2245 train_time:99087ms step_avg:61.09ms
step:1623/2245 train_time:99150ms step_avg:61.09ms
step:1624/2245 train_time:99210ms step_avg:61.09ms
step:1625/2245 train_time:99272ms step_avg:61.09ms
step:1626/2245 train_time:99333ms step_avg:61.09ms
step:1627/2245 train_time:99395ms step_avg:61.09ms
step:1628/2245 train_time:99456ms step_avg:61.09ms
step:1629/2245 train_time:99519ms step_avg:61.09ms
step:1630/2245 train_time:99579ms step_avg:61.09ms
step:1631/2245 train_time:99641ms step_avg:61.09ms
step:1632/2245 train_time:99702ms step_avg:61.09ms
step:1633/2245 train_time:99766ms step_avg:61.09ms
step:1634/2245 train_time:99827ms step_avg:61.09ms
step:1635/2245 train_time:99890ms step_avg:61.09ms
step:1636/2245 train_time:99950ms step_avg:61.09ms
step:1637/2245 train_time:100013ms step_avg:61.10ms
step:1638/2245 train_time:100073ms step_avg:61.09ms
step:1639/2245 train_time:100135ms step_avg:61.10ms
step:1640/2245 train_time:100196ms step_avg:61.10ms
step:1641/2245 train_time:100259ms step_avg:61.10ms
step:1642/2245 train_time:100319ms step_avg:61.10ms
step:1643/2245 train_time:100382ms step_avg:61.10ms
step:1644/2245 train_time:100443ms step_avg:61.10ms
step:1645/2245 train_time:100505ms step_avg:61.10ms
step:1646/2245 train_time:100565ms step_avg:61.10ms
step:1647/2245 train_time:100629ms step_avg:61.10ms
step:1648/2245 train_time:100690ms step_avg:61.10ms
step:1649/2245 train_time:100753ms step_avg:61.10ms
step:1650/2245 train_time:100813ms step_avg:61.10ms
step:1651/2245 train_time:100875ms step_avg:61.10ms
step:1652/2245 train_time:100935ms step_avg:61.10ms
step:1653/2245 train_time:100998ms step_avg:61.10ms
step:1654/2245 train_time:101058ms step_avg:61.10ms
step:1655/2245 train_time:101121ms step_avg:61.10ms
step:1656/2245 train_time:101181ms step_avg:61.10ms
step:1657/2245 train_time:101244ms step_avg:61.10ms
step:1658/2245 train_time:101306ms step_avg:61.10ms
step:1659/2245 train_time:101369ms step_avg:61.10ms
step:1660/2245 train_time:101429ms step_avg:61.10ms
step:1661/2245 train_time:101492ms step_avg:61.10ms
step:1662/2245 train_time:101551ms step_avg:61.10ms
step:1663/2245 train_time:101614ms step_avg:61.10ms
step:1664/2245 train_time:101674ms step_avg:61.10ms
step:1665/2245 train_time:101738ms step_avg:61.10ms
step:1666/2245 train_time:101798ms step_avg:61.10ms
step:1667/2245 train_time:101861ms step_avg:61.10ms
step:1668/2245 train_time:101922ms step_avg:61.10ms
step:1669/2245 train_time:101986ms step_avg:61.11ms
step:1670/2245 train_time:102046ms step_avg:61.11ms
step:1671/2245 train_time:102109ms step_avg:61.11ms
step:1672/2245 train_time:102170ms step_avg:61.11ms
step:1673/2245 train_time:102233ms step_avg:61.11ms
step:1674/2245 train_time:102293ms step_avg:61.11ms
step:1675/2245 train_time:102356ms step_avg:61.11ms
step:1676/2245 train_time:102416ms step_avg:61.11ms
step:1677/2245 train_time:102478ms step_avg:61.11ms
step:1678/2245 train_time:102538ms step_avg:61.11ms
step:1679/2245 train_time:102601ms step_avg:61.11ms
step:1680/2245 train_time:102662ms step_avg:61.11ms
step:1681/2245 train_time:102726ms step_avg:61.11ms
step:1682/2245 train_time:102787ms step_avg:61.11ms
step:1683/2245 train_time:102850ms step_avg:61.11ms
step:1684/2245 train_time:102910ms step_avg:61.11ms
step:1685/2245 train_time:102973ms step_avg:61.11ms
step:1686/2245 train_time:103033ms step_avg:61.11ms
step:1687/2245 train_time:103096ms step_avg:61.11ms
step:1688/2245 train_time:103156ms step_avg:61.11ms
step:1689/2245 train_time:103219ms step_avg:61.11ms
step:1690/2245 train_time:103279ms step_avg:61.11ms
step:1691/2245 train_time:103342ms step_avg:61.11ms
step:1692/2245 train_time:103403ms step_avg:61.11ms
step:1693/2245 train_time:103466ms step_avg:61.11ms
step:1694/2245 train_time:103527ms step_avg:61.11ms
step:1695/2245 train_time:103590ms step_avg:61.12ms
step:1696/2245 train_time:103650ms step_avg:61.11ms
step:1697/2245 train_time:103713ms step_avg:61.12ms
step:1698/2245 train_time:103774ms step_avg:61.12ms
step:1699/2245 train_time:103837ms step_avg:61.12ms
step:1700/2245 train_time:103898ms step_avg:61.12ms
step:1701/2245 train_time:103961ms step_avg:61.12ms
step:1702/2245 train_time:104021ms step_avg:61.12ms
step:1703/2245 train_time:104085ms step_avg:61.12ms
step:1704/2245 train_time:104146ms step_avg:61.12ms
step:1705/2245 train_time:104209ms step_avg:61.12ms
step:1706/2245 train_time:104269ms step_avg:61.12ms
step:1707/2245 train_time:104332ms step_avg:61.12ms
step:1708/2245 train_time:104393ms step_avg:61.12ms
step:1709/2245 train_time:104456ms step_avg:61.12ms
step:1710/2245 train_time:104516ms step_avg:61.12ms
step:1711/2245 train_time:104579ms step_avg:61.12ms
step:1712/2245 train_time:104639ms step_avg:61.12ms
step:1713/2245 train_time:104702ms step_avg:61.12ms
step:1714/2245 train_time:104763ms step_avg:61.12ms
step:1715/2245 train_time:104827ms step_avg:61.12ms
step:1716/2245 train_time:104888ms step_avg:61.12ms
step:1717/2245 train_time:104950ms step_avg:61.12ms
step:1718/2245 train_time:105010ms step_avg:61.12ms
step:1719/2245 train_time:105073ms step_avg:61.12ms
step:1720/2245 train_time:105133ms step_avg:61.12ms
step:1721/2245 train_time:105195ms step_avg:61.12ms
step:1722/2245 train_time:105256ms step_avg:61.12ms
step:1723/2245 train_time:105318ms step_avg:61.12ms
step:1724/2245 train_time:105379ms step_avg:61.12ms
step:1725/2245 train_time:105442ms step_avg:61.13ms
step:1726/2245 train_time:105503ms step_avg:61.13ms
step:1727/2245 train_time:105567ms step_avg:61.13ms
step:1728/2245 train_time:105627ms step_avg:61.13ms
step:1729/2245 train_time:105691ms step_avg:61.13ms
step:1730/2245 train_time:105750ms step_avg:61.13ms
step:1731/2245 train_time:105813ms step_avg:61.13ms
step:1732/2245 train_time:105873ms step_avg:61.13ms
step:1733/2245 train_time:105936ms step_avg:61.13ms
step:1734/2245 train_time:105997ms step_avg:61.13ms
step:1735/2245 train_time:106059ms step_avg:61.13ms
step:1736/2245 train_time:106120ms step_avg:61.13ms
step:1737/2245 train_time:106183ms step_avg:61.13ms
step:1738/2245 train_time:106243ms step_avg:61.13ms
step:1739/2245 train_time:106306ms step_avg:61.13ms
step:1740/2245 train_time:106367ms step_avg:61.13ms
step:1741/2245 train_time:106430ms step_avg:61.13ms
step:1742/2245 train_time:106490ms step_avg:61.13ms
step:1743/2245 train_time:106553ms step_avg:61.13ms
step:1744/2245 train_time:106613ms step_avg:61.13ms
step:1745/2245 train_time:106676ms step_avg:61.13ms
step:1746/2245 train_time:106737ms step_avg:61.13ms
step:1747/2245 train_time:106800ms step_avg:61.13ms
step:1748/2245 train_time:106860ms step_avg:61.13ms
step:1749/2245 train_time:106924ms step_avg:61.13ms
step:1750/2245 train_time:106986ms step_avg:61.13ms
step:1750/2245 val_loss:3.3796 train_time:107050ms step_avg:61.17ms
step:1751/2245 train_time:107069ms step_avg:61.15ms
step:1752/2245 train_time:107112ms step_avg:61.14ms
step:1753/2245 train_time:107179ms step_avg:61.14ms
step:1754/2245 train_time:107244ms step_avg:61.14ms
step:1755/2245 train_time:107305ms step_avg:61.14ms
step:1756/2245 train_time:107366ms step_avg:61.14ms
step:1757/2245 train_time:107428ms step_avg:61.14ms
step:1758/2245 train_time:107487ms step_avg:61.14ms
step:1759/2245 train_time:107549ms step_avg:61.14ms
step:1760/2245 train_time:107610ms step_avg:61.14ms
step:1761/2245 train_time:107673ms step_avg:61.14ms
step:1762/2245 train_time:107733ms step_avg:61.14ms
step:1763/2245 train_time:107795ms step_avg:61.14ms
step:1764/2245 train_time:107856ms step_avg:61.14ms
step:1765/2245 train_time:107918ms step_avg:61.14ms
step:1766/2245 train_time:107978ms step_avg:61.14ms
step:1767/2245 train_time:108042ms step_avg:61.14ms
step:1768/2245 train_time:108104ms step_avg:61.14ms
step:1769/2245 train_time:108170ms step_avg:61.15ms
step:1770/2245 train_time:108232ms step_avg:61.15ms
step:1771/2245 train_time:108296ms step_avg:61.15ms
step:1772/2245 train_time:108356ms step_avg:61.15ms
step:1773/2245 train_time:108418ms step_avg:61.15ms
step:1774/2245 train_time:108478ms step_avg:61.15ms
step:1775/2245 train_time:108541ms step_avg:61.15ms
step:1776/2245 train_time:108601ms step_avg:61.15ms
step:1777/2245 train_time:108664ms step_avg:61.15ms
step:1778/2245 train_time:108725ms step_avg:61.15ms
step:1779/2245 train_time:108787ms step_avg:61.15ms
step:1780/2245 train_time:108848ms step_avg:61.15ms
step:1781/2245 train_time:108911ms step_avg:61.15ms
step:1782/2245 train_time:108972ms step_avg:61.15ms
step:1783/2245 train_time:109036ms step_avg:61.15ms
step:1784/2245 train_time:109097ms step_avg:61.15ms
step:1785/2245 train_time:109160ms step_avg:61.15ms
step:1786/2245 train_time:109222ms step_avg:61.15ms
step:1787/2245 train_time:109285ms step_avg:61.16ms
step:1788/2245 train_time:109346ms step_avg:61.16ms
step:1789/2245 train_time:109409ms step_avg:61.16ms
step:1790/2245 train_time:109469ms step_avg:61.16ms
step:1791/2245 train_time:109532ms step_avg:61.16ms
step:1792/2245 train_time:109592ms step_avg:61.16ms
step:1793/2245 train_time:109655ms step_avg:61.16ms
step:1794/2245 train_time:109715ms step_avg:61.16ms
step:1795/2245 train_time:109776ms step_avg:61.16ms
step:1796/2245 train_time:109836ms step_avg:61.16ms
step:1797/2245 train_time:109898ms step_avg:61.16ms
step:1798/2245 train_time:109959ms step_avg:61.16ms
step:1799/2245 train_time:110023ms step_avg:61.16ms
step:1800/2245 train_time:110083ms step_avg:61.16ms
step:1801/2245 train_time:110146ms step_avg:61.16ms
step:1802/2245 train_time:110207ms step_avg:61.16ms
step:1803/2245 train_time:110271ms step_avg:61.16ms
step:1804/2245 train_time:110332ms step_avg:61.16ms
step:1805/2245 train_time:110395ms step_avg:61.16ms
step:1806/2245 train_time:110456ms step_avg:61.16ms
step:1807/2245 train_time:110518ms step_avg:61.16ms
step:1808/2245 train_time:110578ms step_avg:61.16ms
step:1809/2245 train_time:110641ms step_avg:61.16ms
step:1810/2245 train_time:110701ms step_avg:61.16ms
step:1811/2245 train_time:110763ms step_avg:61.16ms
step:1812/2245 train_time:110823ms step_avg:61.16ms
step:1813/2245 train_time:110885ms step_avg:61.16ms
step:1814/2245 train_time:110946ms step_avg:61.16ms
step:1815/2245 train_time:111009ms step_avg:61.16ms
step:1816/2245 train_time:111070ms step_avg:61.16ms
step:1817/2245 train_time:111133ms step_avg:61.16ms
step:1818/2245 train_time:111194ms step_avg:61.16ms
step:1819/2245 train_time:111257ms step_avg:61.16ms
step:1820/2245 train_time:111318ms step_avg:61.16ms
step:1821/2245 train_time:111382ms step_avg:61.17ms
step:1822/2245 train_time:111442ms step_avg:61.16ms
step:1823/2245 train_time:111505ms step_avg:61.17ms
step:1824/2245 train_time:111565ms step_avg:61.17ms
step:1825/2245 train_time:111628ms step_avg:61.17ms
step:1826/2245 train_time:111689ms step_avg:61.17ms
step:1827/2245 train_time:111752ms step_avg:61.17ms
step:1828/2245 train_time:111813ms step_avg:61.17ms
step:1829/2245 train_time:111876ms step_avg:61.17ms
step:1830/2245 train_time:111936ms step_avg:61.17ms
step:1831/2245 train_time:111998ms step_avg:61.17ms
step:1832/2245 train_time:112058ms step_avg:61.17ms
step:1833/2245 train_time:112122ms step_avg:61.17ms
step:1834/2245 train_time:112182ms step_avg:61.17ms
step:1835/2245 train_time:112245ms step_avg:61.17ms
step:1836/2245 train_time:112306ms step_avg:61.17ms
step:1837/2245 train_time:112369ms step_avg:61.17ms
step:1838/2245 train_time:112430ms step_avg:61.17ms
step:1839/2245 train_time:112492ms step_avg:61.17ms
step:1840/2245 train_time:112552ms step_avg:61.17ms
step:1841/2245 train_time:112615ms step_avg:61.17ms
step:1842/2245 train_time:112676ms step_avg:61.17ms
step:1843/2245 train_time:112739ms step_avg:61.17ms
step:1844/2245 train_time:112799ms step_avg:61.17ms
step:1845/2245 train_time:112862ms step_avg:61.17ms
step:1846/2245 train_time:112923ms step_avg:61.17ms
step:1847/2245 train_time:112985ms step_avg:61.17ms
step:1848/2245 train_time:113045ms step_avg:61.17ms
step:1849/2245 train_time:113107ms step_avg:61.17ms
step:1850/2245 train_time:113168ms step_avg:61.17ms
step:1851/2245 train_time:113232ms step_avg:61.17ms
step:1852/2245 train_time:113292ms step_avg:61.17ms
step:1853/2245 train_time:113356ms step_avg:61.17ms
step:1854/2245 train_time:113417ms step_avg:61.17ms
step:1855/2245 train_time:113479ms step_avg:61.17ms
step:1856/2245 train_time:113540ms step_avg:61.17ms
step:1857/2245 train_time:113602ms step_avg:61.18ms
step:1858/2245 train_time:113663ms step_avg:61.17ms
step:1859/2245 train_time:113726ms step_avg:61.18ms
step:1860/2245 train_time:113786ms step_avg:61.18ms
step:1861/2245 train_time:113850ms step_avg:61.18ms
step:1862/2245 train_time:113911ms step_avg:61.18ms
step:1863/2245 train_time:113974ms step_avg:61.18ms
step:1864/2245 train_time:114035ms step_avg:61.18ms
step:1865/2245 train_time:114097ms step_avg:61.18ms
step:1866/2245 train_time:114157ms step_avg:61.18ms
step:1867/2245 train_time:114220ms step_avg:61.18ms
step:1868/2245 train_time:114281ms step_avg:61.18ms
step:1869/2245 train_time:114344ms step_avg:61.18ms
step:1870/2245 train_time:114404ms step_avg:61.18ms
step:1871/2245 train_time:114467ms step_avg:61.18ms
step:1872/2245 train_time:114528ms step_avg:61.18ms
step:1873/2245 train_time:114591ms step_avg:61.18ms
step:1874/2245 train_time:114651ms step_avg:61.18ms
step:1875/2245 train_time:114714ms step_avg:61.18ms
step:1876/2245 train_time:114775ms step_avg:61.18ms
step:1877/2245 train_time:114837ms step_avg:61.18ms
step:1878/2245 train_time:114897ms step_avg:61.18ms
step:1879/2245 train_time:114960ms step_avg:61.18ms
step:1880/2245 train_time:115020ms step_avg:61.18ms
step:1881/2245 train_time:115083ms step_avg:61.18ms
step:1882/2245 train_time:115143ms step_avg:61.18ms
step:1883/2245 train_time:115206ms step_avg:61.18ms
step:1884/2245 train_time:115266ms step_avg:61.18ms
step:1885/2245 train_time:115329ms step_avg:61.18ms
step:1886/2245 train_time:115390ms step_avg:61.18ms
step:1887/2245 train_time:115453ms step_avg:61.18ms
step:1888/2245 train_time:115514ms step_avg:61.18ms
step:1889/2245 train_time:115577ms step_avg:61.18ms
step:1890/2245 train_time:115637ms step_avg:61.18ms
step:1891/2245 train_time:115700ms step_avg:61.18ms
step:1892/2245 train_time:115760ms step_avg:61.18ms
step:1893/2245 train_time:115824ms step_avg:61.19ms
step:1894/2245 train_time:115884ms step_avg:61.19ms
step:1895/2245 train_time:115948ms step_avg:61.19ms
step:1896/2245 train_time:116008ms step_avg:61.19ms
step:1897/2245 train_time:116071ms step_avg:61.19ms
step:1898/2245 train_time:116132ms step_avg:61.19ms
step:1899/2245 train_time:116195ms step_avg:61.19ms
step:1900/2245 train_time:116255ms step_avg:61.19ms
step:1901/2245 train_time:116318ms step_avg:61.19ms
step:1902/2245 train_time:116378ms step_avg:61.19ms
step:1903/2245 train_time:116440ms step_avg:61.19ms
step:1904/2245 train_time:116501ms step_avg:61.19ms
step:1905/2245 train_time:116564ms step_avg:61.19ms
step:1906/2245 train_time:116625ms step_avg:61.19ms
step:1907/2245 train_time:116688ms step_avg:61.19ms
step:1908/2245 train_time:116749ms step_avg:61.19ms
step:1909/2245 train_time:116812ms step_avg:61.19ms
step:1910/2245 train_time:116873ms step_avg:61.19ms
step:1911/2245 train_time:116936ms step_avg:61.19ms
step:1912/2245 train_time:116996ms step_avg:61.19ms
step:1913/2245 train_time:117059ms step_avg:61.19ms
step:1914/2245 train_time:117119ms step_avg:61.19ms
step:1915/2245 train_time:117182ms step_avg:61.19ms
step:1916/2245 train_time:117242ms step_avg:61.19ms
step:1917/2245 train_time:117305ms step_avg:61.19ms
step:1918/2245 train_time:117366ms step_avg:61.19ms
step:1919/2245 train_time:117428ms step_avg:61.19ms
step:1920/2245 train_time:117489ms step_avg:61.19ms
step:1921/2245 train_time:117553ms step_avg:61.19ms
step:1922/2245 train_time:117614ms step_avg:61.19ms
step:1923/2245 train_time:117677ms step_avg:61.19ms
step:1924/2245 train_time:117737ms step_avg:61.19ms
step:1925/2245 train_time:117799ms step_avg:61.19ms
step:1926/2245 train_time:117859ms step_avg:61.19ms
step:1927/2245 train_time:117922ms step_avg:61.19ms
step:1928/2245 train_time:117983ms step_avg:61.19ms
step:1929/2245 train_time:118046ms step_avg:61.20ms
step:1930/2245 train_time:118106ms step_avg:61.19ms
step:1931/2245 train_time:118169ms step_avg:61.20ms
step:1932/2245 train_time:118230ms step_avg:61.20ms
step:1933/2245 train_time:118292ms step_avg:61.20ms
step:1934/2245 train_time:118353ms step_avg:61.20ms
step:1935/2245 train_time:118416ms step_avg:61.20ms
step:1936/2245 train_time:118476ms step_avg:61.20ms
step:1937/2245 train_time:118539ms step_avg:61.20ms
step:1938/2245 train_time:118599ms step_avg:61.20ms
step:1939/2245 train_time:118662ms step_avg:61.20ms
step:1940/2245 train_time:118723ms step_avg:61.20ms
step:1941/2245 train_time:118786ms step_avg:61.20ms
step:1942/2245 train_time:118847ms step_avg:61.20ms
step:1943/2245 train_time:118910ms step_avg:61.20ms
step:1944/2245 train_time:118970ms step_avg:61.20ms
step:1945/2245 train_time:119033ms step_avg:61.20ms
step:1946/2245 train_time:119094ms step_avg:61.20ms
step:1947/2245 train_time:119157ms step_avg:61.20ms
step:1948/2245 train_time:119218ms step_avg:61.20ms
step:1949/2245 train_time:119280ms step_avg:61.20ms
step:1950/2245 train_time:119340ms step_avg:61.20ms
step:1951/2245 train_time:119403ms step_avg:61.20ms
step:1952/2245 train_time:119463ms step_avg:61.20ms
step:1953/2245 train_time:119526ms step_avg:61.20ms
step:1954/2245 train_time:119586ms step_avg:61.20ms
step:1955/2245 train_time:119649ms step_avg:61.20ms
step:1956/2245 train_time:119710ms step_avg:61.20ms
step:1957/2245 train_time:119774ms step_avg:61.20ms
step:1958/2245 train_time:119834ms step_avg:61.20ms
step:1959/2245 train_time:119897ms step_avg:61.20ms
step:1960/2245 train_time:119957ms step_avg:61.20ms
step:1961/2245 train_time:120019ms step_avg:61.20ms
step:1962/2245 train_time:120079ms step_avg:61.20ms
step:1963/2245 train_time:120142ms step_avg:61.20ms
step:1964/2245 train_time:120202ms step_avg:61.20ms
step:1965/2245 train_time:120265ms step_avg:61.20ms
step:1966/2245 train_time:120326ms step_avg:61.20ms
step:1967/2245 train_time:120390ms step_avg:61.20ms
step:1968/2245 train_time:120451ms step_avg:61.20ms
step:1969/2245 train_time:120514ms step_avg:61.21ms
step:1970/2245 train_time:120575ms step_avg:61.21ms
step:1971/2245 train_time:120637ms step_avg:61.21ms
step:1972/2245 train_time:120697ms step_avg:61.21ms
step:1973/2245 train_time:120761ms step_avg:61.21ms
step:1974/2245 train_time:120822ms step_avg:61.21ms
step:1975/2245 train_time:120884ms step_avg:61.21ms
step:1976/2245 train_time:120945ms step_avg:61.21ms
step:1977/2245 train_time:121008ms step_avg:61.21ms
step:1978/2245 train_time:121069ms step_avg:61.21ms
step:1979/2245 train_time:121132ms step_avg:61.21ms
step:1980/2245 train_time:121193ms step_avg:61.21ms
step:1981/2245 train_time:121256ms step_avg:61.21ms
step:1982/2245 train_time:121316ms step_avg:61.21ms
step:1983/2245 train_time:121378ms step_avg:61.21ms
step:1984/2245 train_time:121438ms step_avg:61.21ms
step:1985/2245 train_time:121501ms step_avg:61.21ms
step:1986/2245 train_time:121561ms step_avg:61.21ms
step:1987/2245 train_time:121624ms step_avg:61.21ms
step:1988/2245 train_time:121684ms step_avg:61.21ms
step:1989/2245 train_time:121747ms step_avg:61.21ms
step:1990/2245 train_time:121807ms step_avg:61.21ms
step:1991/2245 train_time:121871ms step_avg:61.21ms
step:1992/2245 train_time:121932ms step_avg:61.21ms
step:1993/2245 train_time:121994ms step_avg:61.21ms
step:1994/2245 train_time:122055ms step_avg:61.21ms
step:1995/2245 train_time:122117ms step_avg:61.21ms
step:1996/2245 train_time:122177ms step_avg:61.21ms
step:1997/2245 train_time:122240ms step_avg:61.21ms
step:1998/2245 train_time:122300ms step_avg:61.21ms
step:1999/2245 train_time:122362ms step_avg:61.21ms
step:2000/2245 train_time:122422ms step_avg:61.21ms
step:2000/2245 val_loss:3.3248 train_time:122486ms step_avg:61.24ms
step:2001/2245 train_time:122507ms step_avg:61.22ms
step:2002/2245 train_time:122550ms step_avg:61.21ms
step:2003/2245 train_time:122615ms step_avg:61.22ms
step:2004/2245 train_time:122678ms step_avg:61.22ms
step:2005/2245 train_time:122742ms step_avg:61.22ms
step:2006/2245 train_time:122803ms step_avg:61.22ms
step:2007/2245 train_time:122865ms step_avg:61.22ms
step:2008/2245 train_time:122924ms step_avg:61.22ms
step:2009/2245 train_time:122986ms step_avg:61.22ms
step:2010/2245 train_time:123046ms step_avg:61.22ms
step:2011/2245 train_time:123108ms step_avg:61.22ms
step:2012/2245 train_time:123168ms step_avg:61.22ms
step:2013/2245 train_time:123230ms step_avg:61.22ms
step:2014/2245 train_time:123290ms step_avg:61.22ms
step:2015/2245 train_time:123352ms step_avg:61.22ms
step:2016/2245 train_time:123412ms step_avg:61.22ms
step:2017/2245 train_time:123475ms step_avg:61.22ms
step:2018/2245 train_time:123537ms step_avg:61.22ms
step:2019/2245 train_time:123602ms step_avg:61.22ms
step:2020/2245 train_time:123664ms step_avg:61.22ms
step:2021/2245 train_time:123728ms step_avg:61.22ms
step:2022/2245 train_time:123788ms step_avg:61.22ms
step:2023/2245 train_time:123851ms step_avg:61.22ms
step:2024/2245 train_time:123911ms step_avg:61.22ms
step:2025/2245 train_time:123974ms step_avg:61.22ms
step:2026/2245 train_time:124034ms step_avg:61.22ms
step:2027/2245 train_time:124097ms step_avg:61.22ms
step:2028/2245 train_time:124157ms step_avg:61.22ms
step:2029/2245 train_time:124219ms step_avg:61.22ms
step:2030/2245 train_time:124279ms step_avg:61.22ms
step:2031/2245 train_time:124342ms step_avg:61.22ms
step:2032/2245 train_time:124403ms step_avg:61.22ms
step:2033/2245 train_time:124466ms step_avg:61.22ms
step:2034/2245 train_time:124527ms step_avg:61.22ms
step:2035/2245 train_time:124590ms step_avg:61.22ms
step:2036/2245 train_time:124651ms step_avg:61.22ms
step:2037/2245 train_time:124715ms step_avg:61.22ms
step:2038/2245 train_time:124776ms step_avg:61.22ms
step:2039/2245 train_time:124840ms step_avg:61.23ms
step:2040/2245 train_time:124901ms step_avg:61.23ms
step:2041/2245 train_time:124964ms step_avg:61.23ms
step:2042/2245 train_time:125025ms step_avg:61.23ms
step:2043/2245 train_time:125087ms step_avg:61.23ms
step:2044/2245 train_time:125148ms step_avg:61.23ms
step:2045/2245 train_time:125209ms step_avg:61.23ms
step:2046/2245 train_time:125269ms step_avg:61.23ms
step:2047/2245 train_time:125332ms step_avg:61.23ms
step:2048/2245 train_time:125392ms step_avg:61.23ms
step:2049/2245 train_time:125455ms step_avg:61.23ms
step:2050/2245 train_time:125516ms step_avg:61.23ms
step:2051/2245 train_time:125579ms step_avg:61.23ms
step:2052/2245 train_time:125641ms step_avg:61.23ms
step:2053/2245 train_time:125705ms step_avg:61.23ms
step:2054/2245 train_time:125766ms step_avg:61.23ms
step:2055/2245 train_time:125829ms step_avg:61.23ms
step:2056/2245 train_time:125889ms step_avg:61.23ms
step:2057/2245 train_time:125952ms step_avg:61.23ms
step:2058/2245 train_time:126013ms step_avg:61.23ms
step:2059/2245 train_time:126075ms step_avg:61.23ms
step:2060/2245 train_time:126135ms step_avg:61.23ms
step:2061/2245 train_time:126198ms step_avg:61.23ms
step:2062/2245 train_time:126259ms step_avg:61.23ms
step:2063/2245 train_time:126323ms step_avg:61.23ms
step:2064/2245 train_time:126383ms step_avg:61.23ms
step:2065/2245 train_time:126447ms step_avg:61.23ms
step:2066/2245 train_time:126507ms step_avg:61.23ms
step:2067/2245 train_time:126570ms step_avg:61.23ms
step:2068/2245 train_time:126630ms step_avg:61.23ms
step:2069/2245 train_time:126693ms step_avg:61.23ms
step:2070/2245 train_time:126754ms step_avg:61.23ms
step:2071/2245 train_time:126817ms step_avg:61.23ms
step:2072/2245 train_time:126879ms step_avg:61.23ms
step:2073/2245 train_time:126943ms step_avg:61.24ms
step:2074/2245 train_time:127004ms step_avg:61.24ms
step:2075/2245 train_time:127067ms step_avg:61.24ms
step:2076/2245 train_time:127126ms step_avg:61.24ms
step:2077/2245 train_time:127188ms step_avg:61.24ms
step:2078/2245 train_time:127249ms step_avg:61.24ms
step:2079/2245 train_time:127311ms step_avg:61.24ms
step:2080/2245 train_time:127371ms step_avg:61.24ms
step:2081/2245 train_time:127434ms step_avg:61.24ms
step:2082/2245 train_time:127494ms step_avg:61.24ms
step:2083/2245 train_time:127558ms step_avg:61.24ms
step:2084/2245 train_time:127618ms step_avg:61.24ms
step:2085/2245 train_time:127683ms step_avg:61.24ms
step:2086/2245 train_time:127745ms step_avg:61.24ms
step:2087/2245 train_time:127807ms step_avg:61.24ms
step:2088/2245 train_time:127867ms step_avg:61.24ms
step:2089/2245 train_time:127930ms step_avg:61.24ms
step:2090/2245 train_time:127991ms step_avg:61.24ms
step:2091/2245 train_time:128053ms step_avg:61.24ms
step:2092/2245 train_time:128113ms step_avg:61.24ms
step:2093/2245 train_time:128177ms step_avg:61.24ms
step:2094/2245 train_time:128238ms step_avg:61.24ms
step:2095/2245 train_time:128300ms step_avg:61.24ms
step:2096/2245 train_time:128360ms step_avg:61.24ms
step:2097/2245 train_time:128423ms step_avg:61.24ms
step:2098/2245 train_time:128483ms step_avg:61.24ms
step:2099/2245 train_time:128547ms step_avg:61.24ms
step:2100/2245 train_time:128607ms step_avg:61.24ms
step:2101/2245 train_time:128670ms step_avg:61.24ms
step:2102/2245 train_time:128731ms step_avg:61.24ms
step:2103/2245 train_time:128794ms step_avg:61.24ms
step:2104/2245 train_time:128855ms step_avg:61.24ms
step:2105/2245 train_time:128919ms step_avg:61.24ms
step:2106/2245 train_time:128980ms step_avg:61.24ms
step:2107/2245 train_time:129043ms step_avg:61.25ms
step:2108/2245 train_time:129104ms step_avg:61.24ms
step:2109/2245 train_time:129167ms step_avg:61.25ms
step:2110/2245 train_time:129227ms step_avg:61.25ms
step:2111/2245 train_time:129289ms step_avg:61.25ms
step:2112/2245 train_time:129349ms step_avg:61.24ms
step:2113/2245 train_time:129412ms step_avg:61.25ms
step:2114/2245 train_time:129472ms step_avg:61.25ms
step:2115/2245 train_time:129534ms step_avg:61.25ms
step:2116/2245 train_time:129595ms step_avg:61.25ms
step:2117/2245 train_time:129659ms step_avg:61.25ms
step:2118/2245 train_time:129719ms step_avg:61.25ms
step:2119/2245 train_time:129783ms step_avg:61.25ms
step:2120/2245 train_time:129845ms step_avg:61.25ms
step:2121/2245 train_time:129907ms step_avg:61.25ms
step:2122/2245 train_time:129968ms step_avg:61.25ms
step:2123/2245 train_time:130031ms step_avg:61.25ms
step:2124/2245 train_time:130091ms step_avg:61.25ms
step:2125/2245 train_time:130155ms step_avg:61.25ms
step:2126/2245 train_time:130214ms step_avg:61.25ms
step:2127/2245 train_time:130277ms step_avg:61.25ms
step:2128/2245 train_time:130338ms step_avg:61.25ms
step:2129/2245 train_time:130401ms step_avg:61.25ms
step:2130/2245 train_time:130462ms step_avg:61.25ms
step:2131/2245 train_time:130525ms step_avg:61.25ms
step:2132/2245 train_time:130586ms step_avg:61.25ms
step:2133/2245 train_time:130649ms step_avg:61.25ms
step:2134/2245 train_time:130709ms step_avg:61.25ms
step:2135/2245 train_time:130772ms step_avg:61.25ms
step:2136/2245 train_time:130832ms step_avg:61.25ms
step:2137/2245 train_time:130895ms step_avg:61.25ms
step:2138/2245 train_time:130956ms step_avg:61.25ms
step:2139/2245 train_time:131019ms step_avg:61.25ms
step:2140/2245 train_time:131080ms step_avg:61.25ms
step:2141/2245 train_time:131143ms step_avg:61.25ms
step:2142/2245 train_time:131203ms step_avg:61.25ms
step:2143/2245 train_time:131266ms step_avg:61.25ms
step:2144/2245 train_time:131326ms step_avg:61.25ms
step:2145/2245 train_time:131388ms step_avg:61.25ms
step:2146/2245 train_time:131449ms step_avg:61.25ms
step:2147/2245 train_time:131512ms step_avg:61.25ms
step:2148/2245 train_time:131572ms step_avg:61.25ms
step:2149/2245 train_time:131635ms step_avg:61.25ms
step:2150/2245 train_time:131695ms step_avg:61.25ms
step:2151/2245 train_time:131760ms step_avg:61.26ms
step:2152/2245 train_time:131821ms step_avg:61.26ms
step:2153/2245 train_time:131885ms step_avg:61.26ms
step:2154/2245 train_time:131945ms step_avg:61.26ms
step:2155/2245 train_time:132008ms step_avg:61.26ms
step:2156/2245 train_time:132068ms step_avg:61.26ms
step:2157/2245 train_time:132132ms step_avg:61.26ms
step:2158/2245 train_time:132192ms step_avg:61.26ms
step:2159/2245 train_time:132256ms step_avg:61.26ms
step:2160/2245 train_time:132317ms step_avg:61.26ms
step:2161/2245 train_time:132380ms step_avg:61.26ms
step:2162/2245 train_time:132441ms step_avg:61.26ms
step:2163/2245 train_time:132504ms step_avg:61.26ms
step:2164/2245 train_time:132564ms step_avg:61.26ms
step:2165/2245 train_time:132627ms step_avg:61.26ms
step:2166/2245 train_time:132687ms step_avg:61.26ms
step:2167/2245 train_time:132750ms step_avg:61.26ms
step:2168/2245 train_time:132811ms step_avg:61.26ms
step:2169/2245 train_time:132873ms step_avg:61.26ms
step:2170/2245 train_time:132934ms step_avg:61.26ms
step:2171/2245 train_time:132997ms step_avg:61.26ms
step:2172/2245 train_time:133058ms step_avg:61.26ms
step:2173/2245 train_time:133123ms step_avg:61.26ms
step:2174/2245 train_time:133184ms step_avg:61.26ms
step:2175/2245 train_time:133247ms step_avg:61.26ms
step:2176/2245 train_time:133307ms step_avg:61.26ms
step:2177/2245 train_time:133369ms step_avg:61.26ms
step:2178/2245 train_time:133429ms step_avg:61.26ms
step:2179/2245 train_time:133492ms step_avg:61.26ms
step:2180/2245 train_time:133553ms step_avg:61.26ms
step:2181/2245 train_time:133615ms step_avg:61.26ms
step:2182/2245 train_time:133676ms step_avg:61.26ms
step:2183/2245 train_time:133739ms step_avg:61.26ms
step:2184/2245 train_time:133800ms step_avg:61.26ms
step:2185/2245 train_time:133864ms step_avg:61.26ms
step:2186/2245 train_time:133924ms step_avg:61.26ms
step:2187/2245 train_time:133988ms step_avg:61.27ms
step:2188/2245 train_time:134048ms step_avg:61.27ms
step:2189/2245 train_time:134111ms step_avg:61.27ms
step:2190/2245 train_time:134172ms step_avg:61.27ms
step:2191/2245 train_time:134234ms step_avg:61.27ms
step:2192/2245 train_time:134295ms step_avg:61.27ms
step:2193/2245 train_time:134358ms step_avg:61.27ms
step:2194/2245 train_time:134418ms step_avg:61.27ms
step:2195/2245 train_time:134481ms step_avg:61.27ms
step:2196/2245 train_time:134542ms step_avg:61.27ms
step:2197/2245 train_time:134605ms step_avg:61.27ms
step:2198/2245 train_time:134665ms step_avg:61.27ms
step:2199/2245 train_time:134728ms step_avg:61.27ms
step:2200/2245 train_time:134789ms step_avg:61.27ms
step:2201/2245 train_time:134852ms step_avg:61.27ms
step:2202/2245 train_time:134912ms step_avg:61.27ms
step:2203/2245 train_time:134975ms step_avg:61.27ms
step:2204/2245 train_time:135036ms step_avg:61.27ms
step:2205/2245 train_time:135100ms step_avg:61.27ms
step:2206/2245 train_time:135161ms step_avg:61.27ms
step:2207/2245 train_time:135224ms step_avg:61.27ms
step:2208/2245 train_time:135285ms step_avg:61.27ms
step:2209/2245 train_time:135348ms step_avg:61.27ms
step:2210/2245 train_time:135408ms step_avg:61.27ms
step:2211/2245 train_time:135471ms step_avg:61.27ms
step:2212/2245 train_time:135531ms step_avg:61.27ms
step:2213/2245 train_time:135594ms step_avg:61.27ms
step:2214/2245 train_time:135655ms step_avg:61.27ms
step:2215/2245 train_time:135718ms step_avg:61.27ms
step:2216/2245 train_time:135779ms step_avg:61.27ms
step:2217/2245 train_time:135843ms step_avg:61.27ms
step:2218/2245 train_time:135904ms step_avg:61.27ms
step:2219/2245 train_time:135967ms step_avg:61.27ms
step:2220/2245 train_time:136027ms step_avg:61.27ms
step:2221/2245 train_time:136090ms step_avg:61.27ms
step:2222/2245 train_time:136151ms step_avg:61.27ms
step:2223/2245 train_time:136213ms step_avg:61.27ms
step:2224/2245 train_time:136273ms step_avg:61.27ms
step:2225/2245 train_time:136337ms step_avg:61.27ms
step:2226/2245 train_time:136398ms step_avg:61.27ms
step:2227/2245 train_time:136461ms step_avg:61.28ms
step:2228/2245 train_time:136523ms step_avg:61.28ms
step:2229/2245 train_time:136586ms step_avg:61.28ms
step:2230/2245 train_time:136647ms step_avg:61.28ms
step:2231/2245 train_time:136709ms step_avg:61.28ms
step:2232/2245 train_time:136770ms step_avg:61.28ms
step:2233/2245 train_time:136833ms step_avg:61.28ms
step:2234/2245 train_time:136893ms step_avg:61.28ms
step:2235/2245 train_time:136957ms step_avg:61.28ms
step:2236/2245 train_time:137017ms step_avg:61.28ms
step:2237/2245 train_time:137081ms step_avg:61.28ms
step:2238/2245 train_time:137144ms step_avg:61.28ms
step:2239/2245 train_time:137207ms step_avg:61.28ms
step:2240/2245 train_time:137268ms step_avg:61.28ms
step:2241/2245 train_time:137331ms step_avg:61.28ms
step:2242/2245 train_time:137391ms step_avg:61.28ms
step:2243/2245 train_time:137453ms step_avg:61.28ms
step:2244/2245 train_time:137514ms step_avg:61.28ms
step:2245/2245 train_time:137578ms step_avg:61.28ms
step:2245/2245 val_loss:3.2794 train_time:137639ms step_avg:61.31ms
peak memory allocated: 29249 MiB reserved: 50528 MiB

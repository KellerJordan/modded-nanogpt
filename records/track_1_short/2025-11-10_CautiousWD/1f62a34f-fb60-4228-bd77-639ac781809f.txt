import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Nov 10 21:48:07 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   41C    P0            130W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   33C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   40C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   34C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   40C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   34C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2245 train_time:120ms step_avg:119.72ms
step:2/2245 train_time:141ms step_avg:70.45ms
step:3/2245 train_time:179ms step_avg:59.78ms
step:4/2245 train_time:236ms step_avg:58.95ms
step:5/2245 train_time:295ms step_avg:59.06ms
step:6/2245 train_time:354ms step_avg:58.94ms
step:7/2245 train_time:415ms step_avg:59.28ms
step:8/2245 train_time:474ms step_avg:59.19ms
step:9/2245 train_time:534ms step_avg:59.39ms
step:10/2245 train_time:593ms step_avg:59.32ms
step:11/2245 train_time:654ms step_avg:59.49ms
step:12/2245 train_time:713ms step_avg:59.43ms
step:13/2245 train_time:775ms step_avg:59.60ms
step:14/2245 train_time:833ms step_avg:59.53ms
step:15/2245 train_time:895ms step_avg:59.65ms
step:16/2245 train_time:954ms step_avg:59.63ms
step:17/2245 train_time:1019ms step_avg:59.94ms
step:18/2245 train_time:1084ms step_avg:60.21ms
step:19/2245 train_time:1150ms step_avg:60.50ms
step:20/2245 train_time:1209ms step_avg:60.44ms
step:21/2245 train_time:1272ms step_avg:60.56ms
step:22/2245 train_time:1331ms step_avg:60.51ms
step:23/2245 train_time:1393ms step_avg:60.56ms
step:24/2245 train_time:1452ms step_avg:60.49ms
step:25/2245 train_time:1513ms step_avg:60.53ms
step:26/2245 train_time:1573ms step_avg:60.49ms
step:27/2245 train_time:1634ms step_avg:60.51ms
step:28/2245 train_time:1692ms step_avg:60.44ms
step:29/2245 train_time:1754ms step_avg:60.48ms
step:30/2245 train_time:1813ms step_avg:60.43ms
step:31/2245 train_time:1874ms step_avg:60.46ms
step:32/2245 train_time:1934ms step_avg:60.44ms
step:33/2245 train_time:1997ms step_avg:60.52ms
step:34/2245 train_time:2059ms step_avg:60.56ms
step:35/2245 train_time:2123ms step_avg:60.67ms
step:36/2245 train_time:2184ms step_avg:60.66ms
step:37/2245 train_time:2248ms step_avg:60.77ms
step:38/2245 train_time:2306ms step_avg:60.69ms
step:39/2245 train_time:2368ms step_avg:60.72ms
step:40/2245 train_time:2427ms step_avg:60.69ms
step:41/2245 train_time:2489ms step_avg:60.71ms
step:42/2245 train_time:2548ms step_avg:60.67ms
step:43/2245 train_time:2609ms step_avg:60.68ms
step:44/2245 train_time:2668ms step_avg:60.63ms
step:45/2245 train_time:2730ms step_avg:60.66ms
step:46/2245 train_time:2788ms step_avg:60.62ms
step:47/2245 train_time:2850ms step_avg:60.64ms
step:48/2245 train_time:2909ms step_avg:60.61ms
step:49/2245 train_time:2972ms step_avg:60.65ms
step:50/2245 train_time:3032ms step_avg:60.63ms
step:51/2245 train_time:3094ms step_avg:60.67ms
step:52/2245 train_time:3154ms step_avg:60.66ms
step:53/2245 train_time:3217ms step_avg:60.70ms
step:54/2245 train_time:3277ms step_avg:60.68ms
step:55/2245 train_time:3339ms step_avg:60.71ms
step:56/2245 train_time:3398ms step_avg:60.68ms
step:57/2245 train_time:3461ms step_avg:60.71ms
step:58/2245 train_time:3520ms step_avg:60.69ms
step:59/2245 train_time:3582ms step_avg:60.71ms
step:60/2245 train_time:3641ms step_avg:60.69ms
step:61/2245 train_time:3703ms step_avg:60.71ms
step:62/2245 train_time:3762ms step_avg:60.68ms
step:63/2245 train_time:3825ms step_avg:60.72ms
step:64/2245 train_time:3885ms step_avg:60.70ms
step:65/2245 train_time:3947ms step_avg:60.72ms
step:66/2245 train_time:4006ms step_avg:60.69ms
step:67/2245 train_time:4068ms step_avg:60.72ms
step:68/2245 train_time:4129ms step_avg:60.71ms
step:69/2245 train_time:4190ms step_avg:60.73ms
step:70/2245 train_time:4250ms step_avg:60.71ms
step:71/2245 train_time:4311ms step_avg:60.72ms
step:72/2245 train_time:4370ms step_avg:60.70ms
step:73/2245 train_time:4434ms step_avg:60.74ms
step:74/2245 train_time:4493ms step_avg:60.71ms
step:75/2245 train_time:4555ms step_avg:60.73ms
step:76/2245 train_time:4614ms step_avg:60.71ms
step:77/2245 train_time:4676ms step_avg:60.73ms
step:78/2245 train_time:4735ms step_avg:60.71ms
step:79/2245 train_time:4798ms step_avg:60.74ms
step:80/2245 train_time:4858ms step_avg:60.73ms
step:81/2245 train_time:4920ms step_avg:60.73ms
step:82/2245 train_time:4979ms step_avg:60.72ms
step:83/2245 train_time:5042ms step_avg:60.75ms
step:84/2245 train_time:5102ms step_avg:60.74ms
step:85/2245 train_time:5165ms step_avg:60.77ms
step:86/2245 train_time:5223ms step_avg:60.73ms
step:87/2245 train_time:5285ms step_avg:60.75ms
step:88/2245 train_time:5345ms step_avg:60.74ms
step:89/2245 train_time:5407ms step_avg:60.75ms
step:90/2245 train_time:5466ms step_avg:60.73ms
step:91/2245 train_time:5528ms step_avg:60.74ms
step:92/2245 train_time:5586ms step_avg:60.72ms
step:93/2245 train_time:5648ms step_avg:60.73ms
step:94/2245 train_time:5706ms step_avg:60.71ms
step:95/2245 train_time:5768ms step_avg:60.72ms
step:96/2245 train_time:5827ms step_avg:60.70ms
step:97/2245 train_time:5889ms step_avg:60.71ms
step:98/2245 train_time:5948ms step_avg:60.70ms
step:99/2245 train_time:6010ms step_avg:60.71ms
step:100/2245 train_time:6070ms step_avg:60.70ms
step:101/2245 train_time:6132ms step_avg:60.72ms
step:102/2245 train_time:6192ms step_avg:60.71ms
step:103/2245 train_time:6254ms step_avg:60.72ms
step:104/2245 train_time:6313ms step_avg:60.71ms
step:105/2245 train_time:6375ms step_avg:60.72ms
step:106/2245 train_time:6435ms step_avg:60.71ms
step:107/2245 train_time:6498ms step_avg:60.72ms
step:108/2245 train_time:6557ms step_avg:60.71ms
step:109/2245 train_time:6619ms step_avg:60.72ms
step:110/2245 train_time:6678ms step_avg:60.71ms
step:111/2245 train_time:6740ms step_avg:60.72ms
step:112/2245 train_time:6799ms step_avg:60.70ms
step:113/2245 train_time:6860ms step_avg:60.71ms
step:114/2245 train_time:6919ms step_avg:60.69ms
step:115/2245 train_time:6982ms step_avg:60.71ms
step:116/2245 train_time:7042ms step_avg:60.71ms
step:117/2245 train_time:7104ms step_avg:60.72ms
step:118/2245 train_time:7164ms step_avg:60.71ms
step:119/2245 train_time:7225ms step_avg:60.72ms
step:120/2245 train_time:7284ms step_avg:60.70ms
step:121/2245 train_time:7346ms step_avg:60.71ms
step:122/2245 train_time:7406ms step_avg:60.70ms
step:123/2245 train_time:7467ms step_avg:60.71ms
step:124/2245 train_time:7527ms step_avg:60.70ms
step:125/2245 train_time:7588ms step_avg:60.71ms
step:126/2245 train_time:7647ms step_avg:60.69ms
step:127/2245 train_time:7708ms step_avg:60.70ms
step:128/2245 train_time:7767ms step_avg:60.68ms
step:129/2245 train_time:7828ms step_avg:60.69ms
step:130/2245 train_time:7887ms step_avg:60.67ms
step:131/2245 train_time:7949ms step_avg:60.68ms
step:132/2245 train_time:8009ms step_avg:60.67ms
step:133/2245 train_time:8071ms step_avg:60.68ms
step:134/2245 train_time:8131ms step_avg:60.68ms
step:135/2245 train_time:8193ms step_avg:60.69ms
step:136/2245 train_time:8252ms step_avg:60.68ms
step:137/2245 train_time:8315ms step_avg:60.69ms
step:138/2245 train_time:8374ms step_avg:60.68ms
step:139/2245 train_time:8436ms step_avg:60.69ms
step:140/2245 train_time:8496ms step_avg:60.68ms
step:141/2245 train_time:8558ms step_avg:60.69ms
step:142/2245 train_time:8617ms step_avg:60.69ms
step:143/2245 train_time:8680ms step_avg:60.70ms
step:144/2245 train_time:8740ms step_avg:60.69ms
step:145/2245 train_time:8802ms step_avg:60.70ms
step:146/2245 train_time:8861ms step_avg:60.69ms
step:147/2245 train_time:8922ms step_avg:60.69ms
step:148/2245 train_time:8982ms step_avg:60.69ms
step:149/2245 train_time:9044ms step_avg:60.70ms
step:150/2245 train_time:9104ms step_avg:60.69ms
step:151/2245 train_time:9165ms step_avg:60.70ms
step:152/2245 train_time:9225ms step_avg:60.69ms
step:153/2245 train_time:9286ms step_avg:60.70ms
step:154/2245 train_time:9346ms step_avg:60.69ms
step:155/2245 train_time:9408ms step_avg:60.70ms
step:156/2245 train_time:9467ms step_avg:60.68ms
step:157/2245 train_time:9528ms step_avg:60.69ms
step:158/2245 train_time:9586ms step_avg:60.67ms
step:159/2245 train_time:9647ms step_avg:60.68ms
step:160/2245 train_time:9707ms step_avg:60.67ms
step:161/2245 train_time:9768ms step_avg:60.67ms
step:162/2245 train_time:9827ms step_avg:60.66ms
step:163/2245 train_time:9889ms step_avg:60.67ms
step:164/2245 train_time:9948ms step_avg:60.66ms
step:165/2245 train_time:10010ms step_avg:60.67ms
step:166/2245 train_time:10070ms step_avg:60.66ms
step:167/2245 train_time:10131ms step_avg:60.67ms
step:168/2245 train_time:10191ms step_avg:60.66ms
step:169/2245 train_time:10253ms step_avg:60.67ms
step:170/2245 train_time:10312ms step_avg:60.66ms
step:171/2245 train_time:10374ms step_avg:60.66ms
step:172/2245 train_time:10433ms step_avg:60.65ms
step:173/2245 train_time:10494ms step_avg:60.66ms
step:174/2245 train_time:10553ms step_avg:60.65ms
step:175/2245 train_time:10614ms step_avg:60.65ms
step:176/2245 train_time:10673ms step_avg:60.64ms
step:177/2245 train_time:10734ms step_avg:60.65ms
step:178/2245 train_time:10793ms step_avg:60.64ms
step:179/2245 train_time:10856ms step_avg:60.65ms
step:180/2245 train_time:10915ms step_avg:60.64ms
step:181/2245 train_time:10977ms step_avg:60.65ms
step:182/2245 train_time:11037ms step_avg:60.64ms
step:183/2245 train_time:11099ms step_avg:60.65ms
step:184/2245 train_time:11159ms step_avg:60.65ms
step:185/2245 train_time:11221ms step_avg:60.65ms
step:186/2245 train_time:11280ms step_avg:60.65ms
step:187/2245 train_time:11343ms step_avg:60.66ms
step:188/2245 train_time:11402ms step_avg:60.65ms
step:189/2245 train_time:11464ms step_avg:60.66ms
step:190/2245 train_time:11523ms step_avg:60.65ms
step:191/2245 train_time:11586ms step_avg:60.66ms
step:192/2245 train_time:11644ms step_avg:60.65ms
step:193/2245 train_time:11706ms step_avg:60.65ms
step:194/2245 train_time:11765ms step_avg:60.64ms
step:195/2245 train_time:11826ms step_avg:60.64ms
step:196/2245 train_time:11885ms step_avg:60.64ms
step:197/2245 train_time:11946ms step_avg:60.64ms
step:198/2245 train_time:12005ms step_avg:60.63ms
step:199/2245 train_time:12067ms step_avg:60.64ms
step:200/2245 train_time:12126ms step_avg:60.63ms
step:201/2245 train_time:12188ms step_avg:60.64ms
step:202/2245 train_time:12247ms step_avg:60.63ms
step:203/2245 train_time:12308ms step_avg:60.63ms
step:204/2245 train_time:12367ms step_avg:60.62ms
step:205/2245 train_time:12429ms step_avg:60.63ms
step:206/2245 train_time:12488ms step_avg:60.62ms
step:207/2245 train_time:12549ms step_avg:60.63ms
step:208/2245 train_time:12608ms step_avg:60.62ms
step:209/2245 train_time:12670ms step_avg:60.62ms
step:210/2245 train_time:12729ms step_avg:60.61ms
step:211/2245 train_time:12790ms step_avg:60.62ms
step:212/2245 train_time:12849ms step_avg:60.61ms
step:213/2245 train_time:12910ms step_avg:60.61ms
step:214/2245 train_time:12969ms step_avg:60.60ms
step:215/2245 train_time:13030ms step_avg:60.60ms
step:216/2245 train_time:13089ms step_avg:60.60ms
step:217/2245 train_time:13151ms step_avg:60.60ms
step:218/2245 train_time:13209ms step_avg:60.59ms
step:219/2245 train_time:13270ms step_avg:60.60ms
step:220/2245 train_time:13330ms step_avg:60.59ms
step:221/2245 train_time:13392ms step_avg:60.60ms
step:222/2245 train_time:13451ms step_avg:60.59ms
step:223/2245 train_time:13513ms step_avg:60.59ms
step:224/2245 train_time:13572ms step_avg:60.59ms
step:225/2245 train_time:13634ms step_avg:60.60ms
step:226/2245 train_time:13693ms step_avg:60.59ms
step:227/2245 train_time:13755ms step_avg:60.59ms
step:228/2245 train_time:13814ms step_avg:60.59ms
step:229/2245 train_time:13875ms step_avg:60.59ms
step:230/2245 train_time:13934ms step_avg:60.58ms
step:231/2245 train_time:13996ms step_avg:60.59ms
step:232/2245 train_time:14055ms step_avg:60.58ms
step:233/2245 train_time:14117ms step_avg:60.59ms
step:234/2245 train_time:14177ms step_avg:60.58ms
step:235/2245 train_time:14238ms step_avg:60.59ms
step:236/2245 train_time:14298ms step_avg:60.59ms
step:237/2245 train_time:14360ms step_avg:60.59ms
step:238/2245 train_time:14420ms step_avg:60.59ms
step:239/2245 train_time:14482ms step_avg:60.59ms
step:240/2245 train_time:14541ms step_avg:60.59ms
step:241/2245 train_time:14603ms step_avg:60.59ms
step:242/2245 train_time:14662ms step_avg:60.59ms
step:243/2245 train_time:14724ms step_avg:60.59ms
step:244/2245 train_time:14783ms step_avg:60.59ms
step:245/2245 train_time:14845ms step_avg:60.59ms
step:246/2245 train_time:14905ms step_avg:60.59ms
step:247/2245 train_time:14966ms step_avg:60.59ms
step:248/2245 train_time:15025ms step_avg:60.58ms
step:249/2245 train_time:15087ms step_avg:60.59ms
step:250/2245 train_time:15146ms step_avg:60.59ms
step:250/2245 val_loss:4.0677 train_time:15208ms step_avg:60.83ms
step:251/2245 train_time:15227ms step_avg:60.67ms
step:252/2245 train_time:15269ms step_avg:60.59ms
step:253/2245 train_time:15336ms step_avg:60.61ms
step:254/2245 train_time:15397ms step_avg:60.62ms
step:255/2245 train_time:15460ms step_avg:60.63ms
step:256/2245 train_time:15519ms step_avg:60.62ms
step:257/2245 train_time:15580ms step_avg:60.62ms
step:258/2245 train_time:15638ms step_avg:60.61ms
step:259/2245 train_time:15699ms step_avg:60.61ms
step:260/2245 train_time:15758ms step_avg:60.61ms
step:261/2245 train_time:15819ms step_avg:60.61ms
step:262/2245 train_time:15877ms step_avg:60.60ms
step:263/2245 train_time:15937ms step_avg:60.60ms
step:264/2245 train_time:15995ms step_avg:60.59ms
step:265/2245 train_time:16056ms step_avg:60.59ms
step:266/2245 train_time:16116ms step_avg:60.59ms
step:267/2245 train_time:16179ms step_avg:60.60ms
step:268/2245 train_time:16240ms step_avg:60.60ms
step:269/2245 train_time:16303ms step_avg:60.61ms
step:270/2245 train_time:16363ms step_avg:60.61ms
step:271/2245 train_time:16426ms step_avg:60.61ms
step:272/2245 train_time:16486ms step_avg:60.61ms
step:273/2245 train_time:16547ms step_avg:60.61ms
step:274/2245 train_time:16605ms step_avg:60.60ms
step:275/2245 train_time:16666ms step_avg:60.61ms
step:276/2245 train_time:16725ms step_avg:60.60ms
step:277/2245 train_time:16786ms step_avg:60.60ms
step:278/2245 train_time:16845ms step_avg:60.59ms
step:279/2245 train_time:16906ms step_avg:60.59ms
step:280/2245 train_time:16964ms step_avg:60.59ms
step:281/2245 train_time:17026ms step_avg:60.59ms
step:282/2245 train_time:17086ms step_avg:60.59ms
step:283/2245 train_time:17149ms step_avg:60.60ms
step:284/2245 train_time:17208ms step_avg:60.59ms
step:285/2245 train_time:17271ms step_avg:60.60ms
step:286/2245 train_time:17330ms step_avg:60.59ms
step:287/2245 train_time:17392ms step_avg:60.60ms
step:288/2245 train_time:17451ms step_avg:60.59ms
step:289/2245 train_time:17512ms step_avg:60.60ms
step:290/2245 train_time:17571ms step_avg:60.59ms
step:291/2245 train_time:17632ms step_avg:60.59ms
step:292/2245 train_time:17691ms step_avg:60.58ms
step:293/2245 train_time:17752ms step_avg:60.59ms
step:294/2245 train_time:17811ms step_avg:60.58ms
step:295/2245 train_time:17873ms step_avg:60.59ms
step:296/2245 train_time:17932ms step_avg:60.58ms
step:297/2245 train_time:17993ms step_avg:60.58ms
step:298/2245 train_time:18053ms step_avg:60.58ms
step:299/2245 train_time:18114ms step_avg:60.58ms
step:300/2245 train_time:18173ms step_avg:60.58ms
step:301/2245 train_time:18235ms step_avg:60.58ms
step:302/2245 train_time:18294ms step_avg:60.58ms
step:303/2245 train_time:18356ms step_avg:60.58ms
step:304/2245 train_time:18415ms step_avg:60.58ms
step:305/2245 train_time:18476ms step_avg:60.58ms
step:306/2245 train_time:18534ms step_avg:60.57ms
step:307/2245 train_time:18596ms step_avg:60.57ms
step:308/2245 train_time:18655ms step_avg:60.57ms
step:309/2245 train_time:18717ms step_avg:60.57ms
step:310/2245 train_time:18776ms step_avg:60.57ms
step:311/2245 train_time:18837ms step_avg:60.57ms
step:312/2245 train_time:18895ms step_avg:60.56ms
step:313/2245 train_time:18957ms step_avg:60.56ms
step:314/2245 train_time:19016ms step_avg:60.56ms
step:315/2245 train_time:19077ms step_avg:60.56ms
step:316/2245 train_time:19136ms step_avg:60.56ms
step:317/2245 train_time:19198ms step_avg:60.56ms
step:318/2245 train_time:19257ms step_avg:60.56ms
step:319/2245 train_time:19319ms step_avg:60.56ms
step:320/2245 train_time:19378ms step_avg:60.55ms
step:321/2245 train_time:19439ms step_avg:60.56ms
step:322/2245 train_time:19498ms step_avg:60.55ms
step:323/2245 train_time:19560ms step_avg:60.56ms
step:324/2245 train_time:19620ms step_avg:60.56ms
step:325/2245 train_time:19681ms step_avg:60.56ms
step:326/2245 train_time:19740ms step_avg:60.55ms
step:327/2245 train_time:19801ms step_avg:60.55ms
step:328/2245 train_time:19860ms step_avg:60.55ms
step:329/2245 train_time:19922ms step_avg:60.55ms
step:330/2245 train_time:19981ms step_avg:60.55ms
step:331/2245 train_time:20043ms step_avg:60.55ms
step:332/2245 train_time:20102ms step_avg:60.55ms
step:333/2245 train_time:20164ms step_avg:60.55ms
step:334/2245 train_time:20223ms step_avg:60.55ms
step:335/2245 train_time:20284ms step_avg:60.55ms
step:336/2245 train_time:20343ms step_avg:60.54ms
step:337/2245 train_time:20405ms step_avg:60.55ms
step:338/2245 train_time:20465ms step_avg:60.55ms
step:339/2245 train_time:20526ms step_avg:60.55ms
step:340/2245 train_time:20585ms step_avg:60.55ms
step:341/2245 train_time:20647ms step_avg:60.55ms
step:342/2245 train_time:20707ms step_avg:60.55ms
step:343/2245 train_time:20768ms step_avg:60.55ms
step:344/2245 train_time:20828ms step_avg:60.55ms
step:345/2245 train_time:20889ms step_avg:60.55ms
step:346/2245 train_time:20949ms step_avg:60.55ms
step:347/2245 train_time:21010ms step_avg:60.55ms
step:348/2245 train_time:21069ms step_avg:60.54ms
step:349/2245 train_time:21130ms step_avg:60.54ms
step:350/2245 train_time:21188ms step_avg:60.54ms
step:351/2245 train_time:21250ms step_avg:60.54ms
step:352/2245 train_time:21308ms step_avg:60.54ms
step:353/2245 train_time:21370ms step_avg:60.54ms
step:354/2245 train_time:21429ms step_avg:60.53ms
step:355/2245 train_time:21490ms step_avg:60.54ms
step:356/2245 train_time:21549ms step_avg:60.53ms
step:357/2245 train_time:21610ms step_avg:60.53ms
step:358/2245 train_time:21669ms step_avg:60.53ms
step:359/2245 train_time:21730ms step_avg:60.53ms
step:360/2245 train_time:21789ms step_avg:60.52ms
step:361/2245 train_time:21851ms step_avg:60.53ms
step:362/2245 train_time:21909ms step_avg:60.52ms
step:363/2245 train_time:21970ms step_avg:60.52ms
step:364/2245 train_time:22029ms step_avg:60.52ms
step:365/2245 train_time:22091ms step_avg:60.52ms
step:366/2245 train_time:22150ms step_avg:60.52ms
step:367/2245 train_time:22211ms step_avg:60.52ms
step:368/2245 train_time:22270ms step_avg:60.52ms
step:369/2245 train_time:22332ms step_avg:60.52ms
step:370/2245 train_time:22391ms step_avg:60.52ms
step:371/2245 train_time:22452ms step_avg:60.52ms
step:372/2245 train_time:22511ms step_avg:60.51ms
step:373/2245 train_time:22572ms step_avg:60.51ms
step:374/2245 train_time:22630ms step_avg:60.51ms
step:375/2245 train_time:22692ms step_avg:60.51ms
step:376/2245 train_time:22750ms step_avg:60.51ms
step:377/2245 train_time:22812ms step_avg:60.51ms
step:378/2245 train_time:22871ms step_avg:60.51ms
step:379/2245 train_time:22932ms step_avg:60.51ms
step:380/2245 train_time:22991ms step_avg:60.50ms
step:381/2245 train_time:23053ms step_avg:60.51ms
step:382/2245 train_time:23112ms step_avg:60.50ms
step:383/2245 train_time:23173ms step_avg:60.50ms
step:384/2245 train_time:23232ms step_avg:60.50ms
step:385/2245 train_time:23293ms step_avg:60.50ms
step:386/2245 train_time:23352ms step_avg:60.50ms
step:387/2245 train_time:23413ms step_avg:60.50ms
step:388/2245 train_time:23472ms step_avg:60.50ms
step:389/2245 train_time:23533ms step_avg:60.50ms
step:390/2245 train_time:23593ms step_avg:60.49ms
step:391/2245 train_time:23654ms step_avg:60.50ms
step:392/2245 train_time:23712ms step_avg:60.49ms
step:393/2245 train_time:23774ms step_avg:60.49ms
step:394/2245 train_time:23832ms step_avg:60.49ms
step:395/2245 train_time:23894ms step_avg:60.49ms
step:396/2245 train_time:23953ms step_avg:60.49ms
step:397/2245 train_time:24015ms step_avg:60.49ms
step:398/2245 train_time:24074ms step_avg:60.49ms
step:399/2245 train_time:24136ms step_avg:60.49ms
step:400/2245 train_time:24194ms step_avg:60.49ms
step:401/2245 train_time:24256ms step_avg:60.49ms
step:402/2245 train_time:24315ms step_avg:60.48ms
step:403/2245 train_time:24376ms step_avg:60.49ms
step:404/2245 train_time:24434ms step_avg:60.48ms
step:405/2245 train_time:24495ms step_avg:60.48ms
step:406/2245 train_time:24554ms step_avg:60.48ms
step:407/2245 train_time:24615ms step_avg:60.48ms
step:408/2245 train_time:24674ms step_avg:60.48ms
step:409/2245 train_time:24735ms step_avg:60.48ms
step:410/2245 train_time:24794ms step_avg:60.47ms
step:411/2245 train_time:24856ms step_avg:60.48ms
step:412/2245 train_time:24914ms step_avg:60.47ms
step:413/2245 train_time:24975ms step_avg:60.47ms
step:414/2245 train_time:25035ms step_avg:60.47ms
step:415/2245 train_time:25096ms step_avg:60.47ms
step:416/2245 train_time:25155ms step_avg:60.47ms
step:417/2245 train_time:25217ms step_avg:60.47ms
step:418/2245 train_time:25276ms step_avg:60.47ms
step:419/2245 train_time:25338ms step_avg:60.47ms
step:420/2245 train_time:25396ms step_avg:60.47ms
step:421/2245 train_time:25457ms step_avg:60.47ms
step:422/2245 train_time:25516ms step_avg:60.47ms
step:423/2245 train_time:25578ms step_avg:60.47ms
step:424/2245 train_time:25637ms step_avg:60.46ms
step:425/2245 train_time:25698ms step_avg:60.47ms
step:426/2245 train_time:25757ms step_avg:60.46ms
step:427/2245 train_time:25819ms step_avg:60.46ms
step:428/2245 train_time:25877ms step_avg:60.46ms
step:429/2245 train_time:25939ms step_avg:60.46ms
step:430/2245 train_time:25997ms step_avg:60.46ms
step:431/2245 train_time:26059ms step_avg:60.46ms
step:432/2245 train_time:26118ms step_avg:60.46ms
step:433/2245 train_time:26180ms step_avg:60.46ms
step:434/2245 train_time:26238ms step_avg:60.46ms
step:435/2245 train_time:26300ms step_avg:60.46ms
step:436/2245 train_time:26359ms step_avg:60.46ms
step:437/2245 train_time:26421ms step_avg:60.46ms
step:438/2245 train_time:26480ms step_avg:60.46ms
step:439/2245 train_time:26542ms step_avg:60.46ms
step:440/2245 train_time:26601ms step_avg:60.46ms
step:441/2245 train_time:26663ms step_avg:60.46ms
step:442/2245 train_time:26723ms step_avg:60.46ms
step:443/2245 train_time:26784ms step_avg:60.46ms
step:444/2245 train_time:26842ms step_avg:60.46ms
step:445/2245 train_time:26905ms step_avg:60.46ms
step:446/2245 train_time:26965ms step_avg:60.46ms
step:447/2245 train_time:27027ms step_avg:60.46ms
step:448/2245 train_time:27087ms step_avg:60.46ms
step:449/2245 train_time:27148ms step_avg:60.46ms
step:450/2245 train_time:27208ms step_avg:60.46ms
step:451/2245 train_time:27271ms step_avg:60.47ms
step:452/2245 train_time:27330ms step_avg:60.46ms
step:453/2245 train_time:27391ms step_avg:60.47ms
step:454/2245 train_time:27450ms step_avg:60.46ms
step:455/2245 train_time:27511ms step_avg:60.46ms
step:456/2245 train_time:27570ms step_avg:60.46ms
step:457/2245 train_time:27631ms step_avg:60.46ms
step:458/2245 train_time:27690ms step_avg:60.46ms
step:459/2245 train_time:27751ms step_avg:60.46ms
step:460/2245 train_time:27810ms step_avg:60.46ms
step:461/2245 train_time:27871ms step_avg:60.46ms
step:462/2245 train_time:27931ms step_avg:60.46ms
step:463/2245 train_time:27992ms step_avg:60.46ms
step:464/2245 train_time:28051ms step_avg:60.45ms
step:465/2245 train_time:28112ms step_avg:60.46ms
step:466/2245 train_time:28171ms step_avg:60.45ms
step:467/2245 train_time:28233ms step_avg:60.46ms
step:468/2245 train_time:28292ms step_avg:60.45ms
step:469/2245 train_time:28353ms step_avg:60.45ms
step:470/2245 train_time:28412ms step_avg:60.45ms
step:471/2245 train_time:28473ms step_avg:60.45ms
step:472/2245 train_time:28532ms step_avg:60.45ms
step:473/2245 train_time:28593ms step_avg:60.45ms
step:474/2245 train_time:28652ms step_avg:60.45ms
step:475/2245 train_time:28714ms step_avg:60.45ms
step:476/2245 train_time:28773ms step_avg:60.45ms
step:477/2245 train_time:28834ms step_avg:60.45ms
step:478/2245 train_time:28893ms step_avg:60.45ms
step:479/2245 train_time:28955ms step_avg:60.45ms
step:480/2245 train_time:29014ms step_avg:60.45ms
step:481/2245 train_time:29076ms step_avg:60.45ms
step:482/2245 train_time:29135ms step_avg:60.45ms
step:483/2245 train_time:29197ms step_avg:60.45ms
step:484/2245 train_time:29256ms step_avg:60.45ms
step:485/2245 train_time:29318ms step_avg:60.45ms
step:486/2245 train_time:29377ms step_avg:60.45ms
step:487/2245 train_time:29438ms step_avg:60.45ms
step:488/2245 train_time:29500ms step_avg:60.45ms
step:489/2245 train_time:29559ms step_avg:60.45ms
step:490/2245 train_time:29618ms step_avg:60.45ms
step:491/2245 train_time:29680ms step_avg:60.45ms
step:492/2245 train_time:29740ms step_avg:60.45ms
step:493/2245 train_time:29801ms step_avg:60.45ms
step:494/2245 train_time:29860ms step_avg:60.45ms
step:495/2245 train_time:29922ms step_avg:60.45ms
step:496/2245 train_time:29981ms step_avg:60.45ms
step:497/2245 train_time:30043ms step_avg:60.45ms
step:498/2245 train_time:30103ms step_avg:60.45ms
step:499/2245 train_time:30165ms step_avg:60.45ms
step:500/2245 train_time:30224ms step_avg:60.45ms
step:500/2245 val_loss:3.8138 train_time:30287ms step_avg:60.57ms
step:501/2245 train_time:30306ms step_avg:60.49ms
step:502/2245 train_time:30347ms step_avg:60.45ms
step:503/2245 train_time:30411ms step_avg:60.46ms
step:504/2245 train_time:30473ms step_avg:60.46ms
step:505/2245 train_time:30535ms step_avg:60.47ms
step:506/2245 train_time:30593ms step_avg:60.46ms
step:507/2245 train_time:30654ms step_avg:60.46ms
step:508/2245 train_time:30712ms step_avg:60.46ms
step:509/2245 train_time:30773ms step_avg:60.46ms
step:510/2245 train_time:30831ms step_avg:60.45ms
step:511/2245 train_time:30892ms step_avg:60.45ms
step:512/2245 train_time:30951ms step_avg:60.45ms
step:513/2245 train_time:31011ms step_avg:60.45ms
step:514/2245 train_time:31070ms step_avg:60.45ms
step:515/2245 train_time:31131ms step_avg:60.45ms
step:516/2245 train_time:31190ms step_avg:60.45ms
step:517/2245 train_time:31252ms step_avg:60.45ms
step:518/2245 train_time:31313ms step_avg:60.45ms
step:519/2245 train_time:31376ms step_avg:60.46ms
step:520/2245 train_time:31438ms step_avg:60.46ms
step:521/2245 train_time:31497ms step_avg:60.46ms
step:522/2245 train_time:31556ms step_avg:60.45ms
step:523/2245 train_time:31618ms step_avg:60.45ms
step:524/2245 train_time:31676ms step_avg:60.45ms
step:525/2245 train_time:31737ms step_avg:60.45ms
step:526/2245 train_time:31796ms step_avg:60.45ms
step:527/2245 train_time:31858ms step_avg:60.45ms
step:528/2245 train_time:31917ms step_avg:60.45ms
step:529/2245 train_time:31978ms step_avg:60.45ms
step:530/2245 train_time:32037ms step_avg:60.45ms
step:531/2245 train_time:32098ms step_avg:60.45ms
step:532/2245 train_time:32158ms step_avg:60.45ms
step:533/2245 train_time:32219ms step_avg:60.45ms
step:534/2245 train_time:32279ms step_avg:60.45ms
step:535/2245 train_time:32342ms step_avg:60.45ms
step:536/2245 train_time:32401ms step_avg:60.45ms
step:537/2245 train_time:32462ms step_avg:60.45ms
step:538/2245 train_time:32522ms step_avg:60.45ms
step:539/2245 train_time:32583ms step_avg:60.45ms
step:540/2245 train_time:32643ms step_avg:60.45ms
step:541/2245 train_time:32704ms step_avg:60.45ms
step:542/2245 train_time:32763ms step_avg:60.45ms
step:543/2245 train_time:32825ms step_avg:60.45ms
step:544/2245 train_time:32883ms step_avg:60.45ms
step:545/2245 train_time:32946ms step_avg:60.45ms
step:546/2245 train_time:33005ms step_avg:60.45ms
step:547/2245 train_time:33067ms step_avg:60.45ms
step:548/2245 train_time:33127ms step_avg:60.45ms
step:549/2245 train_time:33188ms step_avg:60.45ms
step:550/2245 train_time:33248ms step_avg:60.45ms
step:551/2245 train_time:33310ms step_avg:60.45ms
step:552/2245 train_time:33369ms step_avg:60.45ms
step:553/2245 train_time:33431ms step_avg:60.45ms
step:554/2245 train_time:33490ms step_avg:60.45ms
step:555/2245 train_time:33552ms step_avg:60.45ms
step:556/2245 train_time:33611ms step_avg:60.45ms
step:557/2245 train_time:33674ms step_avg:60.46ms
step:558/2245 train_time:33732ms step_avg:60.45ms
step:559/2245 train_time:33794ms step_avg:60.45ms
step:560/2245 train_time:33853ms step_avg:60.45ms
step:561/2245 train_time:33914ms step_avg:60.45ms
step:562/2245 train_time:33973ms step_avg:60.45ms
step:563/2245 train_time:34034ms step_avg:60.45ms
step:564/2245 train_time:34093ms step_avg:60.45ms
step:565/2245 train_time:34154ms step_avg:60.45ms
step:566/2245 train_time:34213ms step_avg:60.45ms
step:567/2245 train_time:34274ms step_avg:60.45ms
step:568/2245 train_time:34333ms step_avg:60.45ms
step:569/2245 train_time:34395ms step_avg:60.45ms
step:570/2245 train_time:34454ms step_avg:60.45ms
step:571/2245 train_time:34516ms step_avg:60.45ms
step:572/2245 train_time:34574ms step_avg:60.44ms
step:573/2245 train_time:34636ms step_avg:60.45ms
step:574/2245 train_time:34695ms step_avg:60.44ms
step:575/2245 train_time:34757ms step_avg:60.45ms
step:576/2245 train_time:34816ms step_avg:60.44ms
step:577/2245 train_time:34877ms step_avg:60.45ms
step:578/2245 train_time:34936ms step_avg:60.44ms
step:579/2245 train_time:34997ms step_avg:60.44ms
step:580/2245 train_time:35056ms step_avg:60.44ms
step:581/2245 train_time:35117ms step_avg:60.44ms
step:582/2245 train_time:35176ms step_avg:60.44ms
step:583/2245 train_time:35238ms step_avg:60.44ms
step:584/2245 train_time:35297ms step_avg:60.44ms
step:585/2245 train_time:35358ms step_avg:60.44ms
step:586/2245 train_time:35418ms step_avg:60.44ms
step:587/2245 train_time:35479ms step_avg:60.44ms
step:588/2245 train_time:35539ms step_avg:60.44ms
step:589/2245 train_time:35600ms step_avg:60.44ms
step:590/2245 train_time:35659ms step_avg:60.44ms
step:591/2245 train_time:35721ms step_avg:60.44ms
step:592/2245 train_time:35780ms step_avg:60.44ms
step:593/2245 train_time:35841ms step_avg:60.44ms
step:594/2245 train_time:35901ms step_avg:60.44ms
step:595/2245 train_time:35963ms step_avg:60.44ms
step:596/2245 train_time:36021ms step_avg:60.44ms
step:597/2245 train_time:36083ms step_avg:60.44ms
step:598/2245 train_time:36142ms step_avg:60.44ms
step:599/2245 train_time:36204ms step_avg:60.44ms
step:600/2245 train_time:36263ms step_avg:60.44ms
step:601/2245 train_time:36325ms step_avg:60.44ms
step:602/2245 train_time:36384ms step_avg:60.44ms
step:603/2245 train_time:36446ms step_avg:60.44ms
step:604/2245 train_time:36506ms step_avg:60.44ms
step:605/2245 train_time:36569ms step_avg:60.44ms
step:606/2245 train_time:36629ms step_avg:60.44ms
step:607/2245 train_time:36690ms step_avg:60.45ms
step:608/2245 train_time:36750ms step_avg:60.44ms
step:609/2245 train_time:36812ms step_avg:60.45ms
step:610/2245 train_time:36872ms step_avg:60.45ms
step:611/2245 train_time:36933ms step_avg:60.45ms
step:612/2245 train_time:36992ms step_avg:60.45ms
step:613/2245 train_time:37054ms step_avg:60.45ms
step:614/2245 train_time:37113ms step_avg:60.45ms
step:615/2245 train_time:37175ms step_avg:60.45ms
step:616/2245 train_time:37233ms step_avg:60.44ms
step:617/2245 train_time:37295ms step_avg:60.45ms
step:618/2245 train_time:37354ms step_avg:60.44ms
step:619/2245 train_time:37416ms step_avg:60.45ms
step:620/2245 train_time:37475ms step_avg:60.44ms
step:621/2245 train_time:37537ms step_avg:60.45ms
step:622/2245 train_time:37596ms step_avg:60.44ms
step:623/2245 train_time:37658ms step_avg:60.45ms
step:624/2245 train_time:37717ms step_avg:60.44ms
step:625/2245 train_time:37779ms step_avg:60.45ms
step:626/2245 train_time:37838ms step_avg:60.44ms
step:627/2245 train_time:37900ms step_avg:60.45ms
step:628/2245 train_time:37959ms step_avg:60.44ms
step:629/2245 train_time:38022ms step_avg:60.45ms
step:630/2245 train_time:38080ms step_avg:60.44ms
step:631/2245 train_time:38142ms step_avg:60.45ms
step:632/2245 train_time:38201ms step_avg:60.44ms
step:633/2245 train_time:38262ms step_avg:60.45ms
step:634/2245 train_time:38322ms step_avg:60.44ms
step:635/2245 train_time:38384ms step_avg:60.45ms
step:636/2245 train_time:38443ms step_avg:60.45ms
step:637/2245 train_time:38505ms step_avg:60.45ms
step:638/2245 train_time:38564ms step_avg:60.45ms
step:639/2245 train_time:38626ms step_avg:60.45ms
step:640/2245 train_time:38686ms step_avg:60.45ms
step:641/2245 train_time:38748ms step_avg:60.45ms
step:642/2245 train_time:38808ms step_avg:60.45ms
step:643/2245 train_time:38870ms step_avg:60.45ms
step:644/2245 train_time:38929ms step_avg:60.45ms
step:645/2245 train_time:38991ms step_avg:60.45ms
step:646/2245 train_time:39050ms step_avg:60.45ms
step:647/2245 train_time:39112ms step_avg:60.45ms
step:648/2245 train_time:39171ms step_avg:60.45ms
step:649/2245 train_time:39235ms step_avg:60.45ms
step:650/2245 train_time:39292ms step_avg:60.45ms
step:651/2245 train_time:39354ms step_avg:60.45ms
step:652/2245 train_time:39414ms step_avg:60.45ms
step:653/2245 train_time:39475ms step_avg:60.45ms
step:654/2245 train_time:39534ms step_avg:60.45ms
step:655/2245 train_time:39596ms step_avg:60.45ms
step:656/2245 train_time:39655ms step_avg:60.45ms
step:657/2245 train_time:39717ms step_avg:60.45ms
step:658/2245 train_time:39775ms step_avg:60.45ms
step:659/2245 train_time:39837ms step_avg:60.45ms
step:660/2245 train_time:39896ms step_avg:60.45ms
step:661/2245 train_time:39959ms step_avg:60.45ms
step:662/2245 train_time:40018ms step_avg:60.45ms
step:663/2245 train_time:40079ms step_avg:60.45ms
step:664/2245 train_time:40138ms step_avg:60.45ms
step:665/2245 train_time:40200ms step_avg:60.45ms
step:666/2245 train_time:40259ms step_avg:60.45ms
step:667/2245 train_time:40321ms step_avg:60.45ms
step:668/2245 train_time:40379ms step_avg:60.45ms
step:669/2245 train_time:40440ms step_avg:60.45ms
step:670/2245 train_time:40500ms step_avg:60.45ms
step:671/2245 train_time:40561ms step_avg:60.45ms
step:672/2245 train_time:40621ms step_avg:60.45ms
step:673/2245 train_time:40682ms step_avg:60.45ms
step:674/2245 train_time:40741ms step_avg:60.45ms
step:675/2245 train_time:40802ms step_avg:60.45ms
step:676/2245 train_time:40861ms step_avg:60.45ms
step:677/2245 train_time:40924ms step_avg:60.45ms
step:678/2245 train_time:40983ms step_avg:60.45ms
step:679/2245 train_time:41045ms step_avg:60.45ms
step:680/2245 train_time:41105ms step_avg:60.45ms
step:681/2245 train_time:41167ms step_avg:60.45ms
step:682/2245 train_time:41226ms step_avg:60.45ms
step:683/2245 train_time:41288ms step_avg:60.45ms
step:684/2245 train_time:41347ms step_avg:60.45ms
step:685/2245 train_time:41409ms step_avg:60.45ms
step:686/2245 train_time:41468ms step_avg:60.45ms
step:687/2245 train_time:41530ms step_avg:60.45ms
step:688/2245 train_time:41589ms step_avg:60.45ms
step:689/2245 train_time:41651ms step_avg:60.45ms
step:690/2245 train_time:41711ms step_avg:60.45ms
step:691/2245 train_time:41772ms step_avg:60.45ms
step:692/2245 train_time:41831ms step_avg:60.45ms
step:693/2245 train_time:41893ms step_avg:60.45ms
step:694/2245 train_time:41952ms step_avg:60.45ms
step:695/2245 train_time:42014ms step_avg:60.45ms
step:696/2245 train_time:42073ms step_avg:60.45ms
step:697/2245 train_time:42135ms step_avg:60.45ms
step:698/2245 train_time:42194ms step_avg:60.45ms
step:699/2245 train_time:42255ms step_avg:60.45ms
step:700/2245 train_time:42314ms step_avg:60.45ms
step:701/2245 train_time:42375ms step_avg:60.45ms
step:702/2245 train_time:42434ms step_avg:60.45ms
step:703/2245 train_time:42495ms step_avg:60.45ms
step:704/2245 train_time:42554ms step_avg:60.45ms
step:705/2245 train_time:42615ms step_avg:60.45ms
step:706/2245 train_time:42674ms step_avg:60.44ms
step:707/2245 train_time:42736ms step_avg:60.45ms
step:708/2245 train_time:42795ms step_avg:60.44ms
step:709/2245 train_time:42857ms step_avg:60.45ms
step:710/2245 train_time:42916ms step_avg:60.45ms
step:711/2245 train_time:42978ms step_avg:60.45ms
step:712/2245 train_time:43037ms step_avg:60.44ms
step:713/2245 train_time:43098ms step_avg:60.45ms
step:714/2245 train_time:43157ms step_avg:60.44ms
step:715/2245 train_time:43219ms step_avg:60.45ms
step:716/2245 train_time:43278ms step_avg:60.44ms
step:717/2245 train_time:43340ms step_avg:60.45ms
step:718/2245 train_time:43399ms step_avg:60.44ms
step:719/2245 train_time:43461ms step_avg:60.45ms
step:720/2245 train_time:43520ms step_avg:60.44ms
step:721/2245 train_time:43582ms step_avg:60.45ms
step:722/2245 train_time:44038ms step_avg:61.00ms
step:723/2245 train_time:44098ms step_avg:60.99ms
step:724/2245 train_time:44156ms step_avg:60.99ms
step:725/2245 train_time:44217ms step_avg:60.99ms
step:726/2245 train_time:44275ms step_avg:60.98ms
step:727/2245 train_time:44335ms step_avg:60.98ms
step:728/2245 train_time:44393ms step_avg:60.98ms
step:729/2245 train_time:44454ms step_avg:60.98ms
step:730/2245 train_time:44512ms step_avg:60.98ms
step:731/2245 train_time:44573ms step_avg:60.98ms
step:732/2245 train_time:44631ms step_avg:60.97ms
step:733/2245 train_time:44692ms step_avg:60.97ms
step:734/2245 train_time:44750ms step_avg:60.97ms
step:735/2245 train_time:44811ms step_avg:60.97ms
step:736/2245 train_time:44870ms step_avg:60.97ms
step:737/2245 train_time:44938ms step_avg:60.97ms
step:738/2245 train_time:45002ms step_avg:60.98ms
step:739/2245 train_time:45067ms step_avg:60.98ms
step:740/2245 train_time:45128ms step_avg:60.98ms
step:741/2245 train_time:45190ms step_avg:60.99ms
step:742/2245 train_time:45250ms step_avg:60.98ms
step:743/2245 train_time:45313ms step_avg:60.99ms
step:744/2245 train_time:45373ms step_avg:60.98ms
step:745/2245 train_time:45434ms step_avg:60.99ms
step:746/2245 train_time:45493ms step_avg:60.98ms
step:747/2245 train_time:45554ms step_avg:60.98ms
step:748/2245 train_time:45613ms step_avg:60.98ms
step:749/2245 train_time:45674ms step_avg:60.98ms
step:750/2245 train_time:45733ms step_avg:60.98ms
step:750/2245 val_loss:3.6648 train_time:45796ms step_avg:61.06ms
step:751/2245 train_time:45816ms step_avg:61.01ms
step:752/2245 train_time:45857ms step_avg:60.98ms
step:753/2245 train_time:45919ms step_avg:60.98ms
step:754/2245 train_time:45979ms step_avg:60.98ms
step:755/2245 train_time:46042ms step_avg:60.98ms
step:756/2245 train_time:46101ms step_avg:60.98ms
step:757/2245 train_time:46162ms step_avg:60.98ms
step:758/2245 train_time:46221ms step_avg:60.98ms
step:759/2245 train_time:46283ms step_avg:60.98ms
step:760/2245 train_time:46342ms step_avg:60.98ms
step:761/2245 train_time:46403ms step_avg:60.98ms
step:762/2245 train_time:46462ms step_avg:60.97ms
step:763/2245 train_time:46523ms step_avg:60.97ms
step:764/2245 train_time:46582ms step_avg:60.97ms
step:765/2245 train_time:46643ms step_avg:60.97ms
step:766/2245 train_time:46708ms step_avg:60.98ms
step:767/2245 train_time:46775ms step_avg:60.98ms
step:768/2245 train_time:46836ms step_avg:60.98ms
step:769/2245 train_time:46899ms step_avg:60.99ms
step:770/2245 train_time:46959ms step_avg:60.99ms
step:771/2245 train_time:47021ms step_avg:60.99ms
step:772/2245 train_time:47081ms step_avg:60.99ms
step:773/2245 train_time:47142ms step_avg:60.99ms
step:774/2245 train_time:47201ms step_avg:60.98ms
step:775/2245 train_time:47263ms step_avg:60.98ms
step:776/2245 train_time:47322ms step_avg:60.98ms
step:777/2245 train_time:47384ms step_avg:60.98ms
step:778/2245 train_time:47443ms step_avg:60.98ms
step:779/2245 train_time:47504ms step_avg:60.98ms
step:780/2245 train_time:47564ms step_avg:60.98ms
step:781/2245 train_time:47626ms step_avg:60.98ms
step:782/2245 train_time:47689ms step_avg:60.98ms
step:783/2245 train_time:47753ms step_avg:60.99ms
step:784/2245 train_time:47815ms step_avg:60.99ms
step:785/2245 train_time:47877ms step_avg:60.99ms
step:786/2245 train_time:47938ms step_avg:60.99ms
step:787/2245 train_time:47999ms step_avg:60.99ms
step:788/2245 train_time:48059ms step_avg:60.99ms
step:789/2245 train_time:48120ms step_avg:60.99ms
step:790/2245 train_time:48180ms step_avg:60.99ms
step:791/2245 train_time:48241ms step_avg:60.99ms
step:792/2245 train_time:48300ms step_avg:60.98ms
step:793/2245 train_time:48361ms step_avg:60.98ms
step:794/2245 train_time:48420ms step_avg:60.98ms
step:795/2245 train_time:48482ms step_avg:60.98ms
step:796/2245 train_time:48541ms step_avg:60.98ms
step:797/2245 train_time:48604ms step_avg:60.98ms
step:798/2245 train_time:48665ms step_avg:60.98ms
step:799/2245 train_time:48729ms step_avg:60.99ms
step:800/2245 train_time:48790ms step_avg:60.99ms
step:801/2245 train_time:48853ms step_avg:60.99ms
step:802/2245 train_time:48913ms step_avg:60.99ms
step:803/2245 train_time:48976ms step_avg:60.99ms
step:804/2245 train_time:49036ms step_avg:60.99ms
step:805/2245 train_time:49097ms step_avg:60.99ms
step:806/2245 train_time:49157ms step_avg:60.99ms
step:807/2245 train_time:49219ms step_avg:60.99ms
step:808/2245 train_time:49278ms step_avg:60.99ms
step:809/2245 train_time:49340ms step_avg:60.99ms
step:810/2245 train_time:49399ms step_avg:60.99ms
step:811/2245 train_time:49461ms step_avg:60.99ms
step:812/2245 train_time:49521ms step_avg:60.99ms
step:813/2245 train_time:49584ms step_avg:60.99ms
step:814/2245 train_time:49644ms step_avg:60.99ms
step:815/2245 train_time:49707ms step_avg:60.99ms
step:816/2245 train_time:49768ms step_avg:60.99ms
step:817/2245 train_time:49831ms step_avg:60.99ms
step:818/2245 train_time:49891ms step_avg:60.99ms
step:819/2245 train_time:49954ms step_avg:60.99ms
step:820/2245 train_time:50014ms step_avg:60.99ms
step:821/2245 train_time:50076ms step_avg:60.99ms
step:822/2245 train_time:50135ms step_avg:60.99ms
step:823/2245 train_time:50197ms step_avg:60.99ms
step:824/2245 train_time:50257ms step_avg:60.99ms
step:825/2245 train_time:50319ms step_avg:60.99ms
step:826/2245 train_time:50379ms step_avg:60.99ms
step:827/2245 train_time:50440ms step_avg:60.99ms
step:828/2245 train_time:50500ms step_avg:60.99ms
step:829/2245 train_time:50562ms step_avg:60.99ms
step:830/2245 train_time:50622ms step_avg:60.99ms
step:831/2245 train_time:50685ms step_avg:60.99ms
step:832/2245 train_time:50745ms step_avg:60.99ms
step:833/2245 train_time:50808ms step_avg:60.99ms
step:834/2245 train_time:50868ms step_avg:60.99ms
step:835/2245 train_time:50931ms step_avg:61.00ms
step:836/2245 train_time:50991ms step_avg:60.99ms
step:837/2245 train_time:51053ms step_avg:61.00ms
step:838/2245 train_time:51114ms step_avg:60.99ms
step:839/2245 train_time:51176ms step_avg:61.00ms
step:840/2245 train_time:51236ms step_avg:60.99ms
step:841/2245 train_time:51298ms step_avg:61.00ms
step:842/2245 train_time:51357ms step_avg:60.99ms
step:843/2245 train_time:51419ms step_avg:61.00ms
step:844/2245 train_time:51479ms step_avg:60.99ms
step:845/2245 train_time:51541ms step_avg:61.00ms
step:846/2245 train_time:51601ms step_avg:60.99ms
step:847/2245 train_time:51664ms step_avg:61.00ms
step:848/2245 train_time:51724ms step_avg:60.99ms
step:849/2245 train_time:51786ms step_avg:61.00ms
step:850/2245 train_time:51846ms step_avg:61.00ms
step:851/2245 train_time:51909ms step_avg:61.00ms
step:852/2245 train_time:51970ms step_avg:61.00ms
step:853/2245 train_time:52032ms step_avg:61.00ms
step:854/2245 train_time:52092ms step_avg:61.00ms
step:855/2245 train_time:52155ms step_avg:61.00ms
step:856/2245 train_time:52215ms step_avg:61.00ms
step:857/2245 train_time:52277ms step_avg:61.00ms
step:858/2245 train_time:52337ms step_avg:61.00ms
step:859/2245 train_time:52398ms step_avg:61.00ms
step:860/2245 train_time:52458ms step_avg:61.00ms
step:861/2245 train_time:52520ms step_avg:61.00ms
step:862/2245 train_time:52580ms step_avg:61.00ms
step:863/2245 train_time:52642ms step_avg:61.00ms
step:864/2245 train_time:52702ms step_avg:61.00ms
step:865/2245 train_time:52765ms step_avg:61.00ms
step:866/2245 train_time:52825ms step_avg:61.00ms
step:867/2245 train_time:52888ms step_avg:61.00ms
step:868/2245 train_time:52948ms step_avg:61.00ms
step:869/2245 train_time:53010ms step_avg:61.00ms
step:870/2245 train_time:53072ms step_avg:61.00ms
step:871/2245 train_time:53134ms step_avg:61.00ms
step:872/2245 train_time:53194ms step_avg:61.00ms
step:873/2245 train_time:53256ms step_avg:61.00ms
step:874/2245 train_time:53316ms step_avg:61.00ms
step:875/2245 train_time:53378ms step_avg:61.00ms
step:876/2245 train_time:53438ms step_avg:61.00ms
step:877/2245 train_time:53499ms step_avg:61.00ms
step:878/2245 train_time:53559ms step_avg:61.00ms
step:879/2245 train_time:53621ms step_avg:61.00ms
step:880/2245 train_time:53681ms step_avg:61.00ms
step:881/2245 train_time:53743ms step_avg:61.00ms
step:882/2245 train_time:53804ms step_avg:61.00ms
step:883/2245 train_time:53867ms step_avg:61.00ms
step:884/2245 train_time:53927ms step_avg:61.00ms
step:885/2245 train_time:53990ms step_avg:61.01ms
step:886/2245 train_time:54051ms step_avg:61.01ms
step:887/2245 train_time:54114ms step_avg:61.01ms
step:888/2245 train_time:54174ms step_avg:61.01ms
step:889/2245 train_time:54235ms step_avg:61.01ms
step:890/2245 train_time:54295ms step_avg:61.01ms
step:891/2245 train_time:54357ms step_avg:61.01ms
step:892/2245 train_time:54417ms step_avg:61.01ms
step:893/2245 train_time:54479ms step_avg:61.01ms
step:894/2245 train_time:54539ms step_avg:61.01ms
step:895/2245 train_time:54601ms step_avg:61.01ms
step:896/2245 train_time:54661ms step_avg:61.01ms
step:897/2245 train_time:54723ms step_avg:61.01ms
step:898/2245 train_time:54784ms step_avg:61.01ms
step:899/2245 train_time:54847ms step_avg:61.01ms
step:900/2245 train_time:54906ms step_avg:61.01ms
step:901/2245 train_time:54969ms step_avg:61.01ms
step:902/2245 train_time:55029ms step_avg:61.01ms
step:903/2245 train_time:55092ms step_avg:61.01ms
step:904/2245 train_time:55151ms step_avg:61.01ms
step:905/2245 train_time:55214ms step_avg:61.01ms
step:906/2245 train_time:55274ms step_avg:61.01ms
step:907/2245 train_time:55336ms step_avg:61.01ms
step:908/2245 train_time:55395ms step_avg:61.01ms
step:909/2245 train_time:55458ms step_avg:61.01ms
step:910/2245 train_time:55517ms step_avg:61.01ms
step:911/2245 train_time:55579ms step_avg:61.01ms
step:912/2245 train_time:55639ms step_avg:61.01ms
step:913/2245 train_time:55701ms step_avg:61.01ms
step:914/2245 train_time:55761ms step_avg:61.01ms
step:915/2245 train_time:55823ms step_avg:61.01ms
step:916/2245 train_time:55884ms step_avg:61.01ms
step:917/2245 train_time:55947ms step_avg:61.01ms
step:918/2245 train_time:56007ms step_avg:61.01ms
step:919/2245 train_time:56069ms step_avg:61.01ms
step:920/2245 train_time:56129ms step_avg:61.01ms
step:921/2245 train_time:56192ms step_avg:61.01ms
step:922/2245 train_time:56253ms step_avg:61.01ms
step:923/2245 train_time:56315ms step_avg:61.01ms
step:924/2245 train_time:56376ms step_avg:61.01ms
step:925/2245 train_time:56438ms step_avg:61.01ms
step:926/2245 train_time:56497ms step_avg:61.01ms
step:927/2245 train_time:56559ms step_avg:61.01ms
step:928/2245 train_time:56619ms step_avg:61.01ms
step:929/2245 train_time:56681ms step_avg:61.01ms
step:930/2245 train_time:56740ms step_avg:61.01ms
step:931/2245 train_time:56802ms step_avg:61.01ms
step:932/2245 train_time:56863ms step_avg:61.01ms
step:933/2245 train_time:56925ms step_avg:61.01ms
step:934/2245 train_time:56985ms step_avg:61.01ms
step:935/2245 train_time:57048ms step_avg:61.01ms
step:936/2245 train_time:57108ms step_avg:61.01ms
step:937/2245 train_time:57171ms step_avg:61.01ms
step:938/2245 train_time:57231ms step_avg:61.01ms
step:939/2245 train_time:57294ms step_avg:61.02ms
step:940/2245 train_time:57355ms step_avg:61.02ms
step:941/2245 train_time:57417ms step_avg:61.02ms
step:942/2245 train_time:57477ms step_avg:61.02ms
step:943/2245 train_time:57539ms step_avg:61.02ms
step:944/2245 train_time:57599ms step_avg:61.02ms
step:945/2245 train_time:57661ms step_avg:61.02ms
step:946/2245 train_time:57720ms step_avg:61.01ms
step:947/2245 train_time:57782ms step_avg:61.02ms
step:948/2245 train_time:57842ms step_avg:61.02ms
step:949/2245 train_time:57905ms step_avg:61.02ms
step:950/2245 train_time:57965ms step_avg:61.02ms
step:951/2245 train_time:58028ms step_avg:61.02ms
step:952/2245 train_time:58087ms step_avg:61.02ms
step:953/2245 train_time:58150ms step_avg:61.02ms
step:954/2245 train_time:58211ms step_avg:61.02ms
step:955/2245 train_time:58275ms step_avg:61.02ms
step:956/2245 train_time:58335ms step_avg:61.02ms
step:957/2245 train_time:58396ms step_avg:61.02ms
step:958/2245 train_time:58457ms step_avg:61.02ms
step:959/2245 train_time:58519ms step_avg:61.02ms
step:960/2245 train_time:58580ms step_avg:61.02ms
step:961/2245 train_time:58642ms step_avg:61.02ms
step:962/2245 train_time:58701ms step_avg:61.02ms
step:963/2245 train_time:58763ms step_avg:61.02ms
step:964/2245 train_time:58824ms step_avg:61.02ms
step:965/2245 train_time:58886ms step_avg:61.02ms
step:966/2245 train_time:58945ms step_avg:61.02ms
step:967/2245 train_time:59007ms step_avg:61.02ms
step:968/2245 train_time:59067ms step_avg:61.02ms
step:969/2245 train_time:59129ms step_avg:61.02ms
step:970/2245 train_time:59191ms step_avg:61.02ms
step:971/2245 train_time:59254ms step_avg:61.02ms
step:972/2245 train_time:59314ms step_avg:61.02ms
step:973/2245 train_time:59376ms step_avg:61.02ms
step:974/2245 train_time:59436ms step_avg:61.02ms
step:975/2245 train_time:59499ms step_avg:61.02ms
step:976/2245 train_time:59559ms step_avg:61.02ms
step:977/2245 train_time:59621ms step_avg:61.02ms
step:978/2245 train_time:59681ms step_avg:61.02ms
step:979/2245 train_time:59743ms step_avg:61.02ms
step:980/2245 train_time:59802ms step_avg:61.02ms
step:981/2245 train_time:59864ms step_avg:61.02ms
step:982/2245 train_time:59924ms step_avg:61.02ms
step:983/2245 train_time:59987ms step_avg:61.02ms
step:984/2245 train_time:60046ms step_avg:61.02ms
step:985/2245 train_time:60108ms step_avg:61.02ms
step:986/2245 train_time:60170ms step_avg:61.02ms
step:987/2245 train_time:60233ms step_avg:61.03ms
step:988/2245 train_time:60294ms step_avg:61.03ms
step:989/2245 train_time:60357ms step_avg:61.03ms
step:990/2245 train_time:60417ms step_avg:61.03ms
step:991/2245 train_time:60479ms step_avg:61.03ms
step:992/2245 train_time:60539ms step_avg:61.03ms
step:993/2245 train_time:60601ms step_avg:61.03ms
step:994/2245 train_time:60660ms step_avg:61.03ms
step:995/2245 train_time:60722ms step_avg:61.03ms
step:996/2245 train_time:60782ms step_avg:61.03ms
step:997/2245 train_time:60844ms step_avg:61.03ms
step:998/2245 train_time:60903ms step_avg:61.03ms
step:999/2245 train_time:60966ms step_avg:61.03ms
step:1000/2245 train_time:61025ms step_avg:61.03ms
step:1000/2245 val_loss:3.5973 train_time:61089ms step_avg:61.09ms
step:1001/2245 train_time:61107ms step_avg:61.05ms
step:1002/2245 train_time:61149ms step_avg:61.03ms
step:1003/2245 train_time:61214ms step_avg:61.03ms
step:1004/2245 train_time:61274ms step_avg:61.03ms
step:1005/2245 train_time:61337ms step_avg:61.03ms
step:1006/2245 train_time:61398ms step_avg:61.03ms
step:1007/2245 train_time:61459ms step_avg:61.03ms
step:1008/2245 train_time:61519ms step_avg:61.03ms
step:1009/2245 train_time:61581ms step_avg:61.03ms
step:1010/2245 train_time:61640ms step_avg:61.03ms
step:1011/2245 train_time:61702ms step_avg:61.03ms
step:1012/2245 train_time:61761ms step_avg:61.03ms
step:1013/2245 train_time:61823ms step_avg:61.03ms
step:1014/2245 train_time:61882ms step_avg:61.03ms
step:1015/2245 train_time:61944ms step_avg:61.03ms
step:1016/2245 train_time:62004ms step_avg:61.03ms
step:1017/2245 train_time:62068ms step_avg:61.03ms
step:1018/2245 train_time:62129ms step_avg:61.03ms
step:1019/2245 train_time:62192ms step_avg:61.03ms
step:1020/2245 train_time:62252ms step_avg:61.03ms
step:1021/2245 train_time:62315ms step_avg:61.03ms
step:1022/2245 train_time:62375ms step_avg:61.03ms
step:1023/2245 train_time:62439ms step_avg:61.03ms
step:1024/2245 train_time:62497ms step_avg:61.03ms
step:1025/2245 train_time:62559ms step_avg:61.03ms
step:1026/2245 train_time:62619ms step_avg:61.03ms
step:1027/2245 train_time:62682ms step_avg:61.03ms
step:1028/2245 train_time:62742ms step_avg:61.03ms
step:1029/2245 train_time:62804ms step_avg:61.03ms
step:1030/2245 train_time:62863ms step_avg:61.03ms
step:1031/2245 train_time:62924ms step_avg:61.03ms
step:1032/2245 train_time:62984ms step_avg:61.03ms
step:1033/2245 train_time:63047ms step_avg:61.03ms
step:1034/2245 train_time:63107ms step_avg:61.03ms
step:1035/2245 train_time:63171ms step_avg:61.03ms
step:1036/2245 train_time:63230ms step_avg:61.03ms
step:1037/2245 train_time:63294ms step_avg:61.04ms
step:1038/2245 train_time:63354ms step_avg:61.03ms
step:1039/2245 train_time:63417ms step_avg:61.04ms
step:1040/2245 train_time:63477ms step_avg:61.04ms
step:1041/2245 train_time:63539ms step_avg:61.04ms
step:1042/2245 train_time:63599ms step_avg:61.04ms
step:1043/2245 train_time:63662ms step_avg:61.04ms
step:1044/2245 train_time:63722ms step_avg:61.04ms
step:1045/2245 train_time:63784ms step_avg:61.04ms
step:1046/2245 train_time:63844ms step_avg:61.04ms
step:1047/2245 train_time:63906ms step_avg:61.04ms
step:1048/2245 train_time:63965ms step_avg:61.04ms
step:1049/2245 train_time:64027ms step_avg:61.04ms
step:1050/2245 train_time:64087ms step_avg:61.04ms
step:1051/2245 train_time:64150ms step_avg:61.04ms
step:1052/2245 train_time:64210ms step_avg:61.04ms
step:1053/2245 train_time:64273ms step_avg:61.04ms
step:1054/2245 train_time:64333ms step_avg:61.04ms
step:1055/2245 train_time:64396ms step_avg:61.04ms
step:1056/2245 train_time:64456ms step_avg:61.04ms
step:1057/2245 train_time:64519ms step_avg:61.04ms
step:1058/2245 train_time:64578ms step_avg:61.04ms
step:1059/2245 train_time:64641ms step_avg:61.04ms
step:1060/2245 train_time:64701ms step_avg:61.04ms
step:1061/2245 train_time:64762ms step_avg:61.04ms
step:1062/2245 train_time:64823ms step_avg:61.04ms
step:1063/2245 train_time:64885ms step_avg:61.04ms
step:1064/2245 train_time:64944ms step_avg:61.04ms
step:1065/2245 train_time:65007ms step_avg:61.04ms
step:1066/2245 train_time:65066ms step_avg:61.04ms
step:1067/2245 train_time:65129ms step_avg:61.04ms
step:1068/2245 train_time:65189ms step_avg:61.04ms
step:1069/2245 train_time:65252ms step_avg:61.04ms
step:1070/2245 train_time:65312ms step_avg:61.04ms
step:1071/2245 train_time:65376ms step_avg:61.04ms
step:1072/2245 train_time:65435ms step_avg:61.04ms
step:1073/2245 train_time:65498ms step_avg:61.04ms
step:1074/2245 train_time:65558ms step_avg:61.04ms
step:1075/2245 train_time:65620ms step_avg:61.04ms
step:1076/2245 train_time:65680ms step_avg:61.04ms
step:1077/2245 train_time:65742ms step_avg:61.04ms
step:1078/2245 train_time:65802ms step_avg:61.04ms
step:1079/2245 train_time:65864ms step_avg:61.04ms
step:1080/2245 train_time:65924ms step_avg:61.04ms
step:1081/2245 train_time:65986ms step_avg:61.04ms
step:1082/2245 train_time:66046ms step_avg:61.04ms
step:1083/2245 train_time:66109ms step_avg:61.04ms
step:1084/2245 train_time:66169ms step_avg:61.04ms
step:1085/2245 train_time:66231ms step_avg:61.04ms
step:1086/2245 train_time:66292ms step_avg:61.04ms
step:1087/2245 train_time:66355ms step_avg:61.04ms
step:1088/2245 train_time:66415ms step_avg:61.04ms
step:1089/2245 train_time:66478ms step_avg:61.05ms
step:1090/2245 train_time:66538ms step_avg:61.04ms
step:1091/2245 train_time:66600ms step_avg:61.05ms
step:1092/2245 train_time:66660ms step_avg:61.04ms
step:1093/2245 train_time:66722ms step_avg:61.05ms
step:1094/2245 train_time:66782ms step_avg:61.04ms
step:1095/2245 train_time:66844ms step_avg:61.04ms
step:1096/2245 train_time:66903ms step_avg:61.04ms
step:1097/2245 train_time:66966ms step_avg:61.04ms
step:1098/2245 train_time:67025ms step_avg:61.04ms
step:1099/2245 train_time:67087ms step_avg:61.04ms
step:1100/2245 train_time:67149ms step_avg:61.04ms
step:1101/2245 train_time:67211ms step_avg:61.05ms
step:1102/2245 train_time:67271ms step_avg:61.04ms
step:1103/2245 train_time:67334ms step_avg:61.05ms
step:1104/2245 train_time:67395ms step_avg:61.05ms
step:1105/2245 train_time:67458ms step_avg:61.05ms
step:1106/2245 train_time:67517ms step_avg:61.05ms
step:1107/2245 train_time:67580ms step_avg:61.05ms
step:1108/2245 train_time:67640ms step_avg:61.05ms
step:1109/2245 train_time:67703ms step_avg:61.05ms
step:1110/2245 train_time:67763ms step_avg:61.05ms
step:1111/2245 train_time:67825ms step_avg:61.05ms
step:1112/2245 train_time:67885ms step_avg:61.05ms
step:1113/2245 train_time:67948ms step_avg:61.05ms
step:1114/2245 train_time:68007ms step_avg:61.05ms
step:1115/2245 train_time:68070ms step_avg:61.05ms
step:1116/2245 train_time:68130ms step_avg:61.05ms
step:1117/2245 train_time:68193ms step_avg:61.05ms
step:1118/2245 train_time:68253ms step_avg:61.05ms
step:1119/2245 train_time:68316ms step_avg:61.05ms
step:1120/2245 train_time:68376ms step_avg:61.05ms
step:1121/2245 train_time:68439ms step_avg:61.05ms
step:1122/2245 train_time:68499ms step_avg:61.05ms
step:1123/2245 train_time:68561ms step_avg:61.05ms
step:1124/2245 train_time:68622ms step_avg:61.05ms
step:1125/2245 train_time:68684ms step_avg:61.05ms
step:1126/2245 train_time:68744ms step_avg:61.05ms
step:1127/2245 train_time:68807ms step_avg:61.05ms
step:1128/2245 train_time:68867ms step_avg:61.05ms
step:1129/2245 train_time:68929ms step_avg:61.05ms
step:1130/2245 train_time:68989ms step_avg:61.05ms
step:1131/2245 train_time:69051ms step_avg:61.05ms
step:1132/2245 train_time:69110ms step_avg:61.05ms
step:1133/2245 train_time:69173ms step_avg:61.05ms
step:1134/2245 train_time:69234ms step_avg:61.05ms
step:1135/2245 train_time:69297ms step_avg:61.05ms
step:1136/2245 train_time:69357ms step_avg:61.05ms
step:1137/2245 train_time:69420ms step_avg:61.06ms
step:1138/2245 train_time:69480ms step_avg:61.05ms
step:1139/2245 train_time:69544ms step_avg:61.06ms
step:1140/2245 train_time:69604ms step_avg:61.06ms
step:1141/2245 train_time:69666ms step_avg:61.06ms
step:1142/2245 train_time:69726ms step_avg:61.06ms
step:1143/2245 train_time:69789ms step_avg:61.06ms
step:1144/2245 train_time:69849ms step_avg:61.06ms
step:1145/2245 train_time:69912ms step_avg:61.06ms
step:1146/2245 train_time:69972ms step_avg:61.06ms
step:1147/2245 train_time:70034ms step_avg:61.06ms
step:1148/2245 train_time:70093ms step_avg:61.06ms
step:1149/2245 train_time:70156ms step_avg:61.06ms
step:1150/2245 train_time:70215ms step_avg:61.06ms
step:1151/2245 train_time:70278ms step_avg:61.06ms
step:1152/2245 train_time:70337ms step_avg:61.06ms
step:1153/2245 train_time:70400ms step_avg:61.06ms
step:1154/2245 train_time:70459ms step_avg:61.06ms
step:1155/2245 train_time:70522ms step_avg:61.06ms
step:1156/2245 train_time:70582ms step_avg:61.06ms
step:1157/2245 train_time:70644ms step_avg:61.06ms
step:1158/2245 train_time:70704ms step_avg:61.06ms
step:1159/2245 train_time:70767ms step_avg:61.06ms
step:1160/2245 train_time:70827ms step_avg:61.06ms
step:1161/2245 train_time:70889ms step_avg:61.06ms
step:1162/2245 train_time:70949ms step_avg:61.06ms
step:1163/2245 train_time:71011ms step_avg:61.06ms
step:1164/2245 train_time:71071ms step_avg:61.06ms
step:1165/2245 train_time:71133ms step_avg:61.06ms
step:1166/2245 train_time:71194ms step_avg:61.06ms
step:1167/2245 train_time:71257ms step_avg:61.06ms
step:1168/2245 train_time:71317ms step_avg:61.06ms
step:1169/2245 train_time:71379ms step_avg:61.06ms
step:1170/2245 train_time:71439ms step_avg:61.06ms
step:1171/2245 train_time:71502ms step_avg:61.06ms
step:1172/2245 train_time:71561ms step_avg:61.06ms
step:1173/2245 train_time:71624ms step_avg:61.06ms
step:1174/2245 train_time:71683ms step_avg:61.06ms
step:1175/2245 train_time:71746ms step_avg:61.06ms
step:1176/2245 train_time:71806ms step_avg:61.06ms
step:1177/2245 train_time:71869ms step_avg:61.06ms
step:1178/2245 train_time:71929ms step_avg:61.06ms
step:1179/2245 train_time:71991ms step_avg:61.06ms
step:1180/2245 train_time:72051ms step_avg:61.06ms
step:1181/2245 train_time:72113ms step_avg:61.06ms
step:1182/2245 train_time:72173ms step_avg:61.06ms
step:1183/2245 train_time:72236ms step_avg:61.06ms
step:1184/2245 train_time:72297ms step_avg:61.06ms
step:1185/2245 train_time:72359ms step_avg:61.06ms
step:1186/2245 train_time:72419ms step_avg:61.06ms
step:1187/2245 train_time:72481ms step_avg:61.06ms
step:1188/2245 train_time:72541ms step_avg:61.06ms
step:1189/2245 train_time:72604ms step_avg:61.06ms
step:1190/2245 train_time:72664ms step_avg:61.06ms
step:1191/2245 train_time:72727ms step_avg:61.06ms
step:1192/2245 train_time:72786ms step_avg:61.06ms
step:1193/2245 train_time:72848ms step_avg:61.06ms
step:1194/2245 train_time:72909ms step_avg:61.06ms
step:1195/2245 train_time:72971ms step_avg:61.06ms
step:1196/2245 train_time:73031ms step_avg:61.06ms
step:1197/2245 train_time:73093ms step_avg:61.06ms
step:1198/2245 train_time:73154ms step_avg:61.06ms
step:1199/2245 train_time:73217ms step_avg:61.07ms
step:1200/2245 train_time:73277ms step_avg:61.06ms
step:1201/2245 train_time:73339ms step_avg:61.06ms
step:1202/2245 train_time:73398ms step_avg:61.06ms
step:1203/2245 train_time:73461ms step_avg:61.06ms
step:1204/2245 train_time:73521ms step_avg:61.06ms
step:1205/2245 train_time:73583ms step_avg:61.06ms
step:1206/2245 train_time:73643ms step_avg:61.06ms
step:1207/2245 train_time:73705ms step_avg:61.06ms
step:1208/2245 train_time:73764ms step_avg:61.06ms
step:1209/2245 train_time:73827ms step_avg:61.06ms
step:1210/2245 train_time:73887ms step_avg:61.06ms
step:1211/2245 train_time:73950ms step_avg:61.06ms
step:1212/2245 train_time:74010ms step_avg:61.06ms
step:1213/2245 train_time:74072ms step_avg:61.07ms
step:1214/2245 train_time:74133ms step_avg:61.06ms
step:1215/2245 train_time:74196ms step_avg:61.07ms
step:1216/2245 train_time:74256ms step_avg:61.07ms
step:1217/2245 train_time:74319ms step_avg:61.07ms
step:1218/2245 train_time:74378ms step_avg:61.07ms
step:1219/2245 train_time:74441ms step_avg:61.07ms
step:1220/2245 train_time:74500ms step_avg:61.07ms
step:1221/2245 train_time:74563ms step_avg:61.07ms
step:1222/2245 train_time:74622ms step_avg:61.07ms
step:1223/2245 train_time:74685ms step_avg:61.07ms
step:1224/2245 train_time:74745ms step_avg:61.07ms
step:1225/2245 train_time:74807ms step_avg:61.07ms
step:1226/2245 train_time:74866ms step_avg:61.07ms
step:1227/2245 train_time:74929ms step_avg:61.07ms
step:1228/2245 train_time:74989ms step_avg:61.07ms
step:1229/2245 train_time:75051ms step_avg:61.07ms
step:1230/2245 train_time:75112ms step_avg:61.07ms
step:1231/2245 train_time:75175ms step_avg:61.07ms
step:1232/2245 train_time:75235ms step_avg:61.07ms
step:1233/2245 train_time:75299ms step_avg:61.07ms
step:1234/2245 train_time:75359ms step_avg:61.07ms
step:1235/2245 train_time:75421ms step_avg:61.07ms
step:1236/2245 train_time:75481ms step_avg:61.07ms
step:1237/2245 train_time:75543ms step_avg:61.07ms
step:1238/2245 train_time:75603ms step_avg:61.07ms
step:1239/2245 train_time:75666ms step_avg:61.07ms
step:1240/2245 train_time:75725ms step_avg:61.07ms
step:1241/2245 train_time:75788ms step_avg:61.07ms
step:1242/2245 train_time:75847ms step_avg:61.07ms
step:1243/2245 train_time:75909ms step_avg:61.07ms
step:1244/2245 train_time:75969ms step_avg:61.07ms
step:1245/2245 train_time:76035ms step_avg:61.07ms
step:1246/2245 train_time:76092ms step_avg:61.07ms
step:1247/2245 train_time:76155ms step_avg:61.07ms
step:1248/2245 train_time:76215ms step_avg:61.07ms
step:1249/2245 train_time:76278ms step_avg:61.07ms
step:1250/2245 train_time:76338ms step_avg:61.07ms
step:1250/2245 val_loss:3.5218 train_time:76402ms step_avg:61.12ms
step:1251/2245 train_time:76420ms step_avg:61.09ms
step:1252/2245 train_time:76463ms step_avg:61.07ms
step:1253/2245 train_time:76530ms step_avg:61.08ms
step:1254/2245 train_time:76591ms step_avg:61.08ms
step:1255/2245 train_time:76652ms step_avg:61.08ms
step:1256/2245 train_time:76712ms step_avg:61.08ms
step:1257/2245 train_time:76775ms step_avg:61.08ms
step:1258/2245 train_time:76834ms step_avg:61.08ms
step:1259/2245 train_time:76896ms step_avg:61.08ms
step:1260/2245 train_time:76955ms step_avg:61.08ms
step:1261/2245 train_time:77017ms step_avg:61.08ms
step:1262/2245 train_time:77077ms step_avg:61.08ms
step:1263/2245 train_time:77137ms step_avg:61.07ms
step:1264/2245 train_time:77197ms step_avg:61.07ms
step:1265/2245 train_time:77259ms step_avg:61.07ms
step:1266/2245 train_time:77319ms step_avg:61.07ms
step:1267/2245 train_time:77383ms step_avg:61.08ms
step:1268/2245 train_time:77446ms step_avg:61.08ms
step:1269/2245 train_time:77510ms step_avg:61.08ms
step:1270/2245 train_time:77572ms step_avg:61.08ms
step:1271/2245 train_time:77634ms step_avg:61.08ms
step:1272/2245 train_time:77694ms step_avg:61.08ms
step:1273/2245 train_time:77757ms step_avg:61.08ms
step:1274/2245 train_time:77817ms step_avg:61.08ms
step:1275/2245 train_time:77879ms step_avg:61.08ms
step:1276/2245 train_time:77938ms step_avg:61.08ms
step:1277/2245 train_time:78000ms step_avg:61.08ms
step:1278/2245 train_time:78060ms step_avg:61.08ms
step:1279/2245 train_time:78122ms step_avg:61.08ms
step:1280/2245 train_time:78181ms step_avg:61.08ms
step:1281/2245 train_time:78243ms step_avg:61.08ms
step:1282/2245 train_time:78304ms step_avg:61.08ms
step:1283/2245 train_time:78368ms step_avg:61.08ms
step:1284/2245 train_time:78429ms step_avg:61.08ms
step:1285/2245 train_time:78491ms step_avg:61.08ms
step:1286/2245 train_time:78552ms step_avg:61.08ms
step:1287/2245 train_time:78614ms step_avg:61.08ms
step:1288/2245 train_time:78674ms step_avg:61.08ms
step:1289/2245 train_time:78737ms step_avg:61.08ms
step:1290/2245 train_time:78797ms step_avg:61.08ms
step:1291/2245 train_time:78860ms step_avg:61.08ms
step:1292/2245 train_time:78920ms step_avg:61.08ms
step:1293/2245 train_time:78982ms step_avg:61.08ms
step:1294/2245 train_time:79042ms step_avg:61.08ms
step:1295/2245 train_time:79103ms step_avg:61.08ms
step:1296/2245 train_time:79162ms step_avg:61.08ms
step:1297/2245 train_time:79225ms step_avg:61.08ms
step:1298/2245 train_time:79285ms step_avg:61.08ms
step:1299/2245 train_time:79348ms step_avg:61.08ms
step:1300/2245 train_time:79408ms step_avg:61.08ms
step:1301/2245 train_time:79472ms step_avg:61.09ms
step:1302/2245 train_time:79530ms step_avg:61.08ms
step:1303/2245 train_time:79593ms step_avg:61.08ms
step:1304/2245 train_time:79653ms step_avg:61.08ms
step:1305/2245 train_time:79715ms step_avg:61.08ms
step:1306/2245 train_time:79775ms step_avg:61.08ms
step:1307/2245 train_time:79838ms step_avg:61.08ms
step:1308/2245 train_time:79898ms step_avg:61.08ms
step:1309/2245 train_time:79959ms step_avg:61.08ms
step:1310/2245 train_time:80019ms step_avg:61.08ms
step:1311/2245 train_time:80081ms step_avg:61.08ms
step:1312/2245 train_time:80141ms step_avg:61.08ms
step:1313/2245 train_time:80203ms step_avg:61.08ms
step:1314/2245 train_time:80263ms step_avg:61.08ms
step:1315/2245 train_time:80326ms step_avg:61.08ms
step:1316/2245 train_time:80386ms step_avg:61.08ms
step:1317/2245 train_time:80449ms step_avg:61.08ms
step:1318/2245 train_time:80509ms step_avg:61.08ms
step:1319/2245 train_time:80572ms step_avg:61.09ms
step:1320/2245 train_time:80632ms step_avg:61.08ms
step:1321/2245 train_time:80694ms step_avg:61.09ms
step:1322/2245 train_time:80754ms step_avg:61.08ms
step:1323/2245 train_time:80818ms step_avg:61.09ms
step:1324/2245 train_time:80877ms step_avg:61.09ms
step:1325/2245 train_time:80940ms step_avg:61.09ms
step:1326/2245 train_time:81000ms step_avg:61.09ms
step:1327/2245 train_time:81062ms step_avg:61.09ms
step:1328/2245 train_time:81122ms step_avg:61.09ms
step:1329/2245 train_time:81185ms step_avg:61.09ms
step:1330/2245 train_time:81245ms step_avg:61.09ms
step:1331/2245 train_time:81307ms step_avg:61.09ms
step:1332/2245 train_time:81367ms step_avg:61.09ms
step:1333/2245 train_time:81430ms step_avg:61.09ms
step:1334/2245 train_time:81490ms step_avg:61.09ms
step:1335/2245 train_time:81552ms step_avg:61.09ms
step:1336/2245 train_time:81613ms step_avg:61.09ms
step:1337/2245 train_time:81675ms step_avg:61.09ms
step:1338/2245 train_time:81735ms step_avg:61.09ms
step:1339/2245 train_time:81798ms step_avg:61.09ms
step:1340/2245 train_time:81858ms step_avg:61.09ms
step:1341/2245 train_time:81920ms step_avg:61.09ms
step:1342/2245 train_time:81980ms step_avg:61.09ms
step:1343/2245 train_time:82043ms step_avg:61.09ms
step:1344/2245 train_time:82103ms step_avg:61.09ms
step:1345/2245 train_time:82164ms step_avg:61.09ms
step:1346/2245 train_time:82224ms step_avg:61.09ms
step:1347/2245 train_time:82286ms step_avg:61.09ms
step:1348/2245 train_time:82346ms step_avg:61.09ms
step:1349/2245 train_time:82409ms step_avg:61.09ms
step:1350/2245 train_time:82470ms step_avg:61.09ms
step:1351/2245 train_time:82532ms step_avg:61.09ms
step:1352/2245 train_time:82592ms step_avg:61.09ms
step:1353/2245 train_time:82653ms step_avg:61.09ms
step:1354/2245 train_time:82713ms step_avg:61.09ms
step:1355/2245 train_time:82776ms step_avg:61.09ms
step:1356/2245 train_time:82836ms step_avg:61.09ms
step:1357/2245 train_time:82898ms step_avg:61.09ms
step:1358/2245 train_time:82958ms step_avg:61.09ms
step:1359/2245 train_time:83021ms step_avg:61.09ms
step:1360/2245 train_time:83080ms step_avg:61.09ms
step:1361/2245 train_time:83143ms step_avg:61.09ms
step:1362/2245 train_time:83203ms step_avg:61.09ms
step:1363/2245 train_time:83266ms step_avg:61.09ms
step:1364/2245 train_time:83326ms step_avg:61.09ms
step:1365/2245 train_time:83388ms step_avg:61.09ms
step:1366/2245 train_time:83449ms step_avg:61.09ms
step:1367/2245 train_time:83511ms step_avg:61.09ms
step:1368/2245 train_time:83571ms step_avg:61.09ms
step:1369/2245 train_time:83633ms step_avg:61.09ms
step:1370/2245 train_time:83692ms step_avg:61.09ms
step:1371/2245 train_time:83754ms step_avg:61.09ms
step:1372/2245 train_time:83814ms step_avg:61.09ms
step:1373/2245 train_time:83876ms step_avg:61.09ms
step:1374/2245 train_time:83937ms step_avg:61.09ms
step:1375/2245 train_time:83999ms step_avg:61.09ms
step:1376/2245 train_time:84059ms step_avg:61.09ms
step:1377/2245 train_time:84121ms step_avg:61.09ms
step:1378/2245 train_time:84182ms step_avg:61.09ms
step:1379/2245 train_time:84245ms step_avg:61.09ms
step:1380/2245 train_time:84305ms step_avg:61.09ms
step:1381/2245 train_time:84367ms step_avg:61.09ms
step:1382/2245 train_time:84428ms step_avg:61.09ms
step:1383/2245 train_time:84490ms step_avg:61.09ms
step:1384/2245 train_time:84549ms step_avg:61.09ms
step:1385/2245 train_time:84612ms step_avg:61.09ms
step:1386/2245 train_time:84672ms step_avg:61.09ms
step:1387/2245 train_time:84734ms step_avg:61.09ms
step:1388/2245 train_time:84794ms step_avg:61.09ms
step:1389/2245 train_time:84857ms step_avg:61.09ms
step:1390/2245 train_time:84917ms step_avg:61.09ms
step:1391/2245 train_time:84979ms step_avg:61.09ms
step:1392/2245 train_time:85039ms step_avg:61.09ms
step:1393/2245 train_time:85101ms step_avg:61.09ms
step:1394/2245 train_time:85162ms step_avg:61.09ms
step:1395/2245 train_time:85225ms step_avg:61.09ms
step:1396/2245 train_time:85285ms step_avg:61.09ms
step:1397/2245 train_time:85348ms step_avg:61.09ms
step:1398/2245 train_time:85408ms step_avg:61.09ms
step:1399/2245 train_time:85471ms step_avg:61.09ms
step:1400/2245 train_time:85532ms step_avg:61.09ms
step:1401/2245 train_time:85593ms step_avg:61.09ms
step:1402/2245 train_time:85653ms step_avg:61.09ms
step:1403/2245 train_time:85715ms step_avg:61.09ms
step:1404/2245 train_time:85775ms step_avg:61.09ms
step:1405/2245 train_time:85838ms step_avg:61.09ms
step:1406/2245 train_time:85898ms step_avg:61.09ms
step:1407/2245 train_time:85960ms step_avg:61.09ms
step:1408/2245 train_time:86020ms step_avg:61.09ms
step:1409/2245 train_time:86081ms step_avg:61.09ms
step:1410/2245 train_time:86142ms step_avg:61.09ms
step:1411/2245 train_time:86205ms step_avg:61.09ms
step:1412/2245 train_time:86265ms step_avg:61.09ms
step:1413/2245 train_time:86328ms step_avg:61.10ms
step:1414/2245 train_time:86388ms step_avg:61.09ms
step:1415/2245 train_time:86450ms step_avg:61.10ms
step:1416/2245 train_time:86509ms step_avg:61.09ms
step:1417/2245 train_time:86571ms step_avg:61.09ms
step:1418/2245 train_time:86632ms step_avg:61.09ms
step:1419/2245 train_time:86693ms step_avg:61.09ms
step:1420/2245 train_time:86753ms step_avg:61.09ms
step:1421/2245 train_time:86815ms step_avg:61.09ms
step:1422/2245 train_time:86875ms step_avg:61.09ms
step:1423/2245 train_time:86938ms step_avg:61.09ms
step:1424/2245 train_time:86997ms step_avg:61.09ms
step:1425/2245 train_time:87060ms step_avg:61.10ms
step:1426/2245 train_time:87121ms step_avg:61.09ms
step:1427/2245 train_time:87183ms step_avg:61.10ms
step:1428/2245 train_time:87243ms step_avg:61.09ms
step:1429/2245 train_time:87306ms step_avg:61.10ms
step:1430/2245 train_time:87367ms step_avg:61.10ms
step:1431/2245 train_time:87429ms step_avg:61.10ms
step:1432/2245 train_time:87488ms step_avg:61.10ms
step:1433/2245 train_time:87550ms step_avg:61.10ms
step:1434/2245 train_time:87610ms step_avg:61.09ms
step:1435/2245 train_time:87672ms step_avg:61.10ms
step:1436/2245 train_time:87732ms step_avg:61.09ms
step:1437/2245 train_time:87794ms step_avg:61.10ms
step:1438/2245 train_time:87854ms step_avg:61.09ms
step:1439/2245 train_time:87916ms step_avg:61.10ms
step:1440/2245 train_time:87977ms step_avg:61.09ms
step:1441/2245 train_time:88039ms step_avg:61.10ms
step:1442/2245 train_time:88099ms step_avg:61.10ms
step:1443/2245 train_time:88162ms step_avg:61.10ms
step:1444/2245 train_time:88222ms step_avg:61.10ms
step:1445/2245 train_time:88285ms step_avg:61.10ms
step:1446/2245 train_time:88345ms step_avg:61.10ms
step:1447/2245 train_time:88408ms step_avg:61.10ms
step:1448/2245 train_time:88469ms step_avg:61.10ms
step:1449/2245 train_time:88531ms step_avg:61.10ms
step:1450/2245 train_time:88591ms step_avg:61.10ms
step:1451/2245 train_time:88653ms step_avg:61.10ms
step:1452/2245 train_time:88712ms step_avg:61.10ms
step:1453/2245 train_time:88774ms step_avg:61.10ms
step:1454/2245 train_time:88834ms step_avg:61.10ms
step:1455/2245 train_time:88897ms step_avg:61.10ms
step:1456/2245 train_time:88957ms step_avg:61.10ms
step:1457/2245 train_time:89019ms step_avg:61.10ms
step:1458/2245 train_time:89079ms step_avg:61.10ms
step:1459/2245 train_time:89141ms step_avg:61.10ms
step:1460/2245 train_time:89201ms step_avg:61.10ms
step:1461/2245 train_time:89264ms step_avg:61.10ms
step:1462/2245 train_time:89326ms step_avg:61.10ms
step:1463/2245 train_time:89388ms step_avg:61.10ms
step:1464/2245 train_time:89448ms step_avg:61.10ms
step:1465/2245 train_time:89511ms step_avg:61.10ms
step:1466/2245 train_time:89571ms step_avg:61.10ms
step:1467/2245 train_time:89633ms step_avg:61.10ms
step:1468/2245 train_time:89693ms step_avg:61.10ms
step:1469/2245 train_time:89755ms step_avg:61.10ms
step:1470/2245 train_time:89815ms step_avg:61.10ms
step:1471/2245 train_time:89877ms step_avg:61.10ms
step:1472/2245 train_time:89938ms step_avg:61.10ms
step:1473/2245 train_time:90001ms step_avg:61.10ms
step:1474/2245 train_time:90061ms step_avg:61.10ms
step:1475/2245 train_time:90125ms step_avg:61.10ms
step:1476/2245 train_time:90185ms step_avg:61.10ms
step:1477/2245 train_time:90248ms step_avg:61.10ms
step:1478/2245 train_time:90308ms step_avg:61.10ms
step:1479/2245 train_time:90371ms step_avg:61.10ms
step:1480/2245 train_time:90432ms step_avg:61.10ms
step:1481/2245 train_time:90495ms step_avg:61.10ms
step:1482/2245 train_time:90556ms step_avg:61.10ms
step:1483/2245 train_time:90618ms step_avg:61.10ms
step:1484/2245 train_time:90679ms step_avg:61.10ms
step:1485/2245 train_time:90741ms step_avg:61.11ms
step:1486/2245 train_time:90802ms step_avg:61.10ms
step:1487/2245 train_time:90865ms step_avg:61.11ms
step:1488/2245 train_time:90925ms step_avg:61.11ms
step:1489/2245 train_time:90988ms step_avg:61.11ms
step:1490/2245 train_time:91048ms step_avg:61.11ms
step:1491/2245 train_time:91111ms step_avg:61.11ms
step:1492/2245 train_time:91171ms step_avg:61.11ms
step:1493/2245 train_time:91234ms step_avg:61.11ms
step:1494/2245 train_time:91294ms step_avg:61.11ms
step:1495/2245 train_time:91357ms step_avg:61.11ms
step:1496/2245 train_time:91418ms step_avg:61.11ms
step:1497/2245 train_time:91482ms step_avg:61.11ms
step:1498/2245 train_time:91543ms step_avg:61.11ms
step:1499/2245 train_time:91605ms step_avg:61.11ms
step:1500/2245 train_time:91665ms step_avg:61.11ms
step:1500/2245 val_loss:3.4406 train_time:91728ms step_avg:61.15ms
step:1501/2245 train_time:91747ms step_avg:61.12ms
step:1502/2245 train_time:91791ms step_avg:61.11ms
step:1503/2245 train_time:91853ms step_avg:61.11ms
step:1504/2245 train_time:91915ms step_avg:61.11ms
step:1505/2245 train_time:91978ms step_avg:61.11ms
step:1506/2245 train_time:92037ms step_avg:61.11ms
step:1507/2245 train_time:92099ms step_avg:61.11ms
step:1508/2245 train_time:92159ms step_avg:61.11ms
step:1509/2245 train_time:92221ms step_avg:61.11ms
step:1510/2245 train_time:92280ms step_avg:61.11ms
step:1511/2245 train_time:92342ms step_avg:61.11ms
step:1512/2245 train_time:92402ms step_avg:61.11ms
step:1513/2245 train_time:92464ms step_avg:61.11ms
step:1514/2245 train_time:92524ms step_avg:61.11ms
step:1515/2245 train_time:92587ms step_avg:61.11ms
step:1516/2245 train_time:92648ms step_avg:61.11ms
step:1517/2245 train_time:92714ms step_avg:61.12ms
step:1518/2245 train_time:92776ms step_avg:61.12ms
step:1519/2245 train_time:92840ms step_avg:61.12ms
step:1520/2245 train_time:92901ms step_avg:61.12ms
step:1521/2245 train_time:92964ms step_avg:61.12ms
step:1522/2245 train_time:93024ms step_avg:61.12ms
step:1523/2245 train_time:93087ms step_avg:61.12ms
step:1524/2245 train_time:93148ms step_avg:61.12ms
step:1525/2245 train_time:93211ms step_avg:61.12ms
step:1526/2245 train_time:93271ms step_avg:61.12ms
step:1527/2245 train_time:93333ms step_avg:61.12ms
step:1528/2245 train_time:93394ms step_avg:61.12ms
step:1529/2245 train_time:93456ms step_avg:61.12ms
step:1530/2245 train_time:93516ms step_avg:61.12ms
step:1531/2245 train_time:93579ms step_avg:61.12ms
step:1532/2245 train_time:93640ms step_avg:61.12ms
step:1533/2245 train_time:93703ms step_avg:61.12ms
step:1534/2245 train_time:93765ms step_avg:61.12ms
step:1535/2245 train_time:93829ms step_avg:61.13ms
step:1536/2245 train_time:93890ms step_avg:61.13ms
step:1537/2245 train_time:93953ms step_avg:61.13ms
step:1538/2245 train_time:94013ms step_avg:61.13ms
step:1539/2245 train_time:94077ms step_avg:61.13ms
step:1540/2245 train_time:94137ms step_avg:61.13ms
step:1541/2245 train_time:94199ms step_avg:61.13ms
step:1542/2245 train_time:94259ms step_avg:61.13ms
step:1543/2245 train_time:94322ms step_avg:61.13ms
step:1544/2245 train_time:94382ms step_avg:61.13ms
step:1545/2245 train_time:94445ms step_avg:61.13ms
step:1546/2245 train_time:94505ms step_avg:61.13ms
step:1547/2245 train_time:94568ms step_avg:61.13ms
step:1548/2245 train_time:94630ms step_avg:61.13ms
step:1549/2245 train_time:94694ms step_avg:61.13ms
step:1550/2245 train_time:94755ms step_avg:61.13ms
step:1551/2245 train_time:94818ms step_avg:61.13ms
step:1552/2245 train_time:94878ms step_avg:61.13ms
step:1553/2245 train_time:94941ms step_avg:61.13ms
step:1554/2245 train_time:95001ms step_avg:61.13ms
step:1555/2245 train_time:95065ms step_avg:61.13ms
step:1556/2245 train_time:95125ms step_avg:61.13ms
step:1557/2245 train_time:95188ms step_avg:61.14ms
step:1558/2245 train_time:95249ms step_avg:61.14ms
step:1559/2245 train_time:95312ms step_avg:61.14ms
step:1560/2245 train_time:95372ms step_avg:61.14ms
step:1561/2245 train_time:95435ms step_avg:61.14ms
step:1562/2245 train_time:95496ms step_avg:61.14ms
step:1563/2245 train_time:95558ms step_avg:61.14ms
step:1564/2245 train_time:95619ms step_avg:61.14ms
step:1565/2245 train_time:95682ms step_avg:61.14ms
step:1566/2245 train_time:95743ms step_avg:61.14ms
step:1567/2245 train_time:95807ms step_avg:61.14ms
step:1568/2245 train_time:95868ms step_avg:61.14ms
step:1569/2245 train_time:95931ms step_avg:61.14ms
step:1570/2245 train_time:95992ms step_avg:61.14ms
step:1571/2245 train_time:96056ms step_avg:61.14ms
step:1572/2245 train_time:96116ms step_avg:61.14ms
step:1573/2245 train_time:96179ms step_avg:61.14ms
step:1574/2245 train_time:96240ms step_avg:61.14ms
step:1575/2245 train_time:96302ms step_avg:61.14ms
step:1576/2245 train_time:96363ms step_avg:61.14ms
step:1577/2245 train_time:96426ms step_avg:61.15ms
step:1578/2245 train_time:96486ms step_avg:61.14ms
step:1579/2245 train_time:96549ms step_avg:61.15ms
step:1580/2245 train_time:96610ms step_avg:61.15ms
step:1581/2245 train_time:96674ms step_avg:61.15ms
step:1582/2245 train_time:96735ms step_avg:61.15ms
step:1583/2245 train_time:96800ms step_avg:61.15ms
step:1584/2245 train_time:96859ms step_avg:61.15ms
step:1585/2245 train_time:96922ms step_avg:61.15ms
step:1586/2245 train_time:96982ms step_avg:61.15ms
step:1587/2245 train_time:97045ms step_avg:61.15ms
step:1588/2245 train_time:97106ms step_avg:61.15ms
step:1589/2245 train_time:97168ms step_avg:61.15ms
step:1590/2245 train_time:97229ms step_avg:61.15ms
step:1591/2245 train_time:97293ms step_avg:61.15ms
step:1592/2245 train_time:97353ms step_avg:61.15ms
step:1593/2245 train_time:97415ms step_avg:61.15ms
step:1594/2245 train_time:97476ms step_avg:61.15ms
step:1595/2245 train_time:97539ms step_avg:61.15ms
step:1596/2245 train_time:97599ms step_avg:61.15ms
step:1597/2245 train_time:97662ms step_avg:61.15ms
step:1598/2245 train_time:97722ms step_avg:61.15ms
step:1599/2245 train_time:97785ms step_avg:61.15ms
step:1600/2245 train_time:97845ms step_avg:61.15ms
step:1601/2245 train_time:97908ms step_avg:61.15ms
step:1602/2245 train_time:97969ms step_avg:61.15ms
step:1603/2245 train_time:98033ms step_avg:61.16ms
step:1604/2245 train_time:98094ms step_avg:61.16ms
step:1605/2245 train_time:98157ms step_avg:61.16ms
step:1606/2245 train_time:98217ms step_avg:61.16ms
step:1607/2245 train_time:98281ms step_avg:61.16ms
step:1608/2245 train_time:98341ms step_avg:61.16ms
step:1609/2245 train_time:98403ms step_avg:61.16ms
step:1610/2245 train_time:98464ms step_avg:61.16ms
step:1611/2245 train_time:98527ms step_avg:61.16ms
step:1612/2245 train_time:98587ms step_avg:61.16ms
step:1613/2245 train_time:98650ms step_avg:61.16ms
step:1614/2245 train_time:98711ms step_avg:61.16ms
step:1615/2245 train_time:98774ms step_avg:61.16ms
step:1616/2245 train_time:98835ms step_avg:61.16ms
step:1617/2245 train_time:98898ms step_avg:61.16ms
step:1618/2245 train_time:98958ms step_avg:61.16ms
step:1619/2245 train_time:99021ms step_avg:61.16ms
step:1620/2245 train_time:99081ms step_avg:61.16ms
step:1621/2245 train_time:99143ms step_avg:61.16ms
step:1622/2245 train_time:99203ms step_avg:61.16ms
step:1623/2245 train_time:99266ms step_avg:61.16ms
step:1624/2245 train_time:99327ms step_avg:61.16ms
step:1625/2245 train_time:99390ms step_avg:61.16ms
step:1626/2245 train_time:99451ms step_avg:61.16ms
step:1627/2245 train_time:99515ms step_avg:61.16ms
step:1628/2245 train_time:99575ms step_avg:61.16ms
step:1629/2245 train_time:99638ms step_avg:61.17ms
step:1630/2245 train_time:99698ms step_avg:61.16ms
step:1631/2245 train_time:99760ms step_avg:61.17ms
step:1632/2245 train_time:99821ms step_avg:61.16ms
step:1633/2245 train_time:99884ms step_avg:61.17ms
step:1634/2245 train_time:99944ms step_avg:61.17ms
step:1635/2245 train_time:100008ms step_avg:61.17ms
step:1636/2245 train_time:100068ms step_avg:61.17ms
step:1637/2245 train_time:100132ms step_avg:61.17ms
step:1638/2245 train_time:100192ms step_avg:61.17ms
step:1639/2245 train_time:100255ms step_avg:61.17ms
step:1640/2245 train_time:100316ms step_avg:61.17ms
step:1641/2245 train_time:100379ms step_avg:61.17ms
step:1642/2245 train_time:100439ms step_avg:61.17ms
step:1643/2245 train_time:100502ms step_avg:61.17ms
step:1644/2245 train_time:100562ms step_avg:61.17ms
step:1645/2245 train_time:100625ms step_avg:61.17ms
step:1646/2245 train_time:100685ms step_avg:61.17ms
step:1647/2245 train_time:100748ms step_avg:61.17ms
step:1648/2245 train_time:100809ms step_avg:61.17ms
step:1649/2245 train_time:100872ms step_avg:61.17ms
step:1650/2245 train_time:100934ms step_avg:61.17ms
step:1651/2245 train_time:100997ms step_avg:61.17ms
step:1652/2245 train_time:101058ms step_avg:61.17ms
step:1653/2245 train_time:101121ms step_avg:61.17ms
step:1654/2245 train_time:101181ms step_avg:61.17ms
step:1655/2245 train_time:101244ms step_avg:61.17ms
step:1656/2245 train_time:101304ms step_avg:61.17ms
step:1657/2245 train_time:101367ms step_avg:61.17ms
step:1658/2245 train_time:101427ms step_avg:61.17ms
step:1659/2245 train_time:101491ms step_avg:61.18ms
step:1660/2245 train_time:101552ms step_avg:61.18ms
step:1661/2245 train_time:101615ms step_avg:61.18ms
step:1662/2245 train_time:101675ms step_avg:61.18ms
step:1663/2245 train_time:101737ms step_avg:61.18ms
step:1664/2245 train_time:101797ms step_avg:61.18ms
step:1665/2245 train_time:101860ms step_avg:61.18ms
step:1666/2245 train_time:101920ms step_avg:61.18ms
step:1667/2245 train_time:101983ms step_avg:61.18ms
step:1668/2245 train_time:102043ms step_avg:61.18ms
step:1669/2245 train_time:102106ms step_avg:61.18ms
step:1670/2245 train_time:102166ms step_avg:61.18ms
step:1671/2245 train_time:102229ms step_avg:61.18ms
step:1672/2245 train_time:102291ms step_avg:61.18ms
step:1673/2245 train_time:102354ms step_avg:61.18ms
step:1674/2245 train_time:102415ms step_avg:61.18ms
step:1675/2245 train_time:102477ms step_avg:61.18ms
step:1676/2245 train_time:102538ms step_avg:61.18ms
step:1677/2245 train_time:102600ms step_avg:61.18ms
step:1678/2245 train_time:102660ms step_avg:61.18ms
step:1679/2245 train_time:102723ms step_avg:61.18ms
step:1680/2245 train_time:102783ms step_avg:61.18ms
step:1681/2245 train_time:102847ms step_avg:61.18ms
step:1682/2245 train_time:102907ms step_avg:61.18ms
step:1683/2245 train_time:102970ms step_avg:61.18ms
step:1684/2245 train_time:103033ms step_avg:61.18ms
step:1685/2245 train_time:103096ms step_avg:61.18ms
step:1686/2245 train_time:103156ms step_avg:61.18ms
step:1687/2245 train_time:103219ms step_avg:61.18ms
step:1688/2245 train_time:103280ms step_avg:61.18ms
step:1689/2245 train_time:103343ms step_avg:61.19ms
step:1690/2245 train_time:103403ms step_avg:61.19ms
step:1691/2245 train_time:103466ms step_avg:61.19ms
step:1692/2245 train_time:103527ms step_avg:61.19ms
step:1693/2245 train_time:103590ms step_avg:61.19ms
step:1694/2245 train_time:103651ms step_avg:61.19ms
step:1695/2245 train_time:103714ms step_avg:61.19ms
step:1696/2245 train_time:103775ms step_avg:61.19ms
step:1697/2245 train_time:103838ms step_avg:61.19ms
step:1698/2245 train_time:103898ms step_avg:61.19ms
step:1699/2245 train_time:103961ms step_avg:61.19ms
step:1700/2245 train_time:104021ms step_avg:61.19ms
step:1701/2245 train_time:104084ms step_avg:61.19ms
step:1702/2245 train_time:104145ms step_avg:61.19ms
step:1703/2245 train_time:104208ms step_avg:61.19ms
step:1704/2245 train_time:104269ms step_avg:61.19ms
step:1705/2245 train_time:104332ms step_avg:61.19ms
step:1706/2245 train_time:104393ms step_avg:61.19ms
step:1707/2245 train_time:104457ms step_avg:61.19ms
step:1708/2245 train_time:104517ms step_avg:61.19ms
step:1709/2245 train_time:104581ms step_avg:61.19ms
step:1710/2245 train_time:104641ms step_avg:61.19ms
step:1711/2245 train_time:104704ms step_avg:61.19ms
step:1712/2245 train_time:104764ms step_avg:61.19ms
step:1713/2245 train_time:104827ms step_avg:61.20ms
step:1714/2245 train_time:104888ms step_avg:61.19ms
step:1715/2245 train_time:104951ms step_avg:61.20ms
step:1716/2245 train_time:105013ms step_avg:61.20ms
step:1717/2245 train_time:105075ms step_avg:61.20ms
step:1718/2245 train_time:105135ms step_avg:61.20ms
step:1719/2245 train_time:105198ms step_avg:61.20ms
step:1720/2245 train_time:105258ms step_avg:61.20ms
step:1721/2245 train_time:105321ms step_avg:61.20ms
step:1722/2245 train_time:105381ms step_avg:61.20ms
step:1723/2245 train_time:105444ms step_avg:61.20ms
step:1724/2245 train_time:105504ms step_avg:61.20ms
step:1725/2245 train_time:105567ms step_avg:61.20ms
step:1726/2245 train_time:105628ms step_avg:61.20ms
step:1727/2245 train_time:105692ms step_avg:61.20ms
step:1728/2245 train_time:105752ms step_avg:61.20ms
step:1729/2245 train_time:105815ms step_avg:61.20ms
step:1730/2245 train_time:105875ms step_avg:61.20ms
step:1731/2245 train_time:105939ms step_avg:61.20ms
step:1732/2245 train_time:105999ms step_avg:61.20ms
step:1733/2245 train_time:106062ms step_avg:61.20ms
step:1734/2245 train_time:106122ms step_avg:61.20ms
step:1735/2245 train_time:106185ms step_avg:61.20ms
step:1736/2245 train_time:106245ms step_avg:61.20ms
step:1737/2245 train_time:106308ms step_avg:61.20ms
step:1738/2245 train_time:106368ms step_avg:61.20ms
step:1739/2245 train_time:106431ms step_avg:61.20ms
step:1740/2245 train_time:106492ms step_avg:61.20ms
step:1741/2245 train_time:106556ms step_avg:61.20ms
step:1742/2245 train_time:106616ms step_avg:61.20ms
step:1743/2245 train_time:106678ms step_avg:61.20ms
step:1744/2245 train_time:106739ms step_avg:61.20ms
step:1745/2245 train_time:106801ms step_avg:61.20ms
step:1746/2245 train_time:106862ms step_avg:61.20ms
step:1747/2245 train_time:106925ms step_avg:61.20ms
step:1748/2245 train_time:106985ms step_avg:61.20ms
step:1749/2245 train_time:107049ms step_avg:61.21ms
step:1750/2245 train_time:107110ms step_avg:61.21ms
step:1750/2245 val_loss:3.3766 train_time:107173ms step_avg:61.24ms
step:1751/2245 train_time:107192ms step_avg:61.22ms
step:1752/2245 train_time:107236ms step_avg:61.21ms
step:1753/2245 train_time:107302ms step_avg:61.21ms
step:1754/2245 train_time:107364ms step_avg:61.21ms
step:1755/2245 train_time:107427ms step_avg:61.21ms
step:1756/2245 train_time:107487ms step_avg:61.21ms
step:1757/2245 train_time:107549ms step_avg:61.21ms
step:1758/2245 train_time:107609ms step_avg:61.21ms
step:1759/2245 train_time:107671ms step_avg:61.21ms
step:1760/2245 train_time:107730ms step_avg:61.21ms
step:1761/2245 train_time:107793ms step_avg:61.21ms
step:1762/2245 train_time:107852ms step_avg:61.21ms
step:1763/2245 train_time:107915ms step_avg:61.21ms
step:1764/2245 train_time:107976ms step_avg:61.21ms
step:1765/2245 train_time:108038ms step_avg:61.21ms
step:1766/2245 train_time:108099ms step_avg:61.21ms
step:1767/2245 train_time:108163ms step_avg:61.21ms
step:1768/2245 train_time:108225ms step_avg:61.21ms
step:1769/2245 train_time:108289ms step_avg:61.21ms
step:1770/2245 train_time:108350ms step_avg:61.21ms
step:1771/2245 train_time:108414ms step_avg:61.22ms
step:1772/2245 train_time:108475ms step_avg:61.22ms
step:1773/2245 train_time:108539ms step_avg:61.22ms
step:1774/2245 train_time:108600ms step_avg:61.22ms
step:1775/2245 train_time:108662ms step_avg:61.22ms
step:1776/2245 train_time:108722ms step_avg:61.22ms
step:1777/2245 train_time:108785ms step_avg:61.22ms
step:1778/2245 train_time:108844ms step_avg:61.22ms
step:1779/2245 train_time:108907ms step_avg:61.22ms
step:1780/2245 train_time:108966ms step_avg:61.22ms
step:1781/2245 train_time:109029ms step_avg:61.22ms
step:1782/2245 train_time:109090ms step_avg:61.22ms
step:1783/2245 train_time:109153ms step_avg:61.22ms
step:1784/2245 train_time:109214ms step_avg:61.22ms
step:1785/2245 train_time:109278ms step_avg:61.22ms
step:1786/2245 train_time:109339ms step_avg:61.22ms
step:1787/2245 train_time:109403ms step_avg:61.22ms
step:1788/2245 train_time:109463ms step_avg:61.22ms
step:1789/2245 train_time:109526ms step_avg:61.22ms
step:1790/2245 train_time:109586ms step_avg:61.22ms
step:1791/2245 train_time:109648ms step_avg:61.22ms
step:1792/2245 train_time:109708ms step_avg:61.22ms
step:1793/2245 train_time:109771ms step_avg:61.22ms
step:1794/2245 train_time:109831ms step_avg:61.22ms
step:1795/2245 train_time:109893ms step_avg:61.22ms
step:1796/2245 train_time:109953ms step_avg:61.22ms
step:1797/2245 train_time:110016ms step_avg:61.22ms
step:1798/2245 train_time:110077ms step_avg:61.22ms
step:1799/2245 train_time:110140ms step_avg:61.22ms
step:1800/2245 train_time:110201ms step_avg:61.22ms
step:1801/2245 train_time:110265ms step_avg:61.22ms
step:1802/2245 train_time:110325ms step_avg:61.22ms
step:1803/2245 train_time:110389ms step_avg:61.23ms
step:1804/2245 train_time:110450ms step_avg:61.22ms
step:1805/2245 train_time:110512ms step_avg:61.23ms
step:1806/2245 train_time:110573ms step_avg:61.23ms
step:1807/2245 train_time:110636ms step_avg:61.23ms
step:1808/2245 train_time:110696ms step_avg:61.23ms
step:1809/2245 train_time:110758ms step_avg:61.23ms
step:1810/2245 train_time:110819ms step_avg:61.23ms
step:1811/2245 train_time:110881ms step_avg:61.23ms
step:1812/2245 train_time:110941ms step_avg:61.23ms
step:1813/2245 train_time:111004ms step_avg:61.23ms
step:1814/2245 train_time:111064ms step_avg:61.23ms
step:1815/2245 train_time:111127ms step_avg:61.23ms
step:1816/2245 train_time:111188ms step_avg:61.23ms
step:1817/2245 train_time:111250ms step_avg:61.23ms
step:1818/2245 train_time:111311ms step_avg:61.23ms
step:1819/2245 train_time:111374ms step_avg:61.23ms
step:1820/2245 train_time:111436ms step_avg:61.23ms
step:1821/2245 train_time:111500ms step_avg:61.23ms
step:1822/2245 train_time:111561ms step_avg:61.23ms
step:1823/2245 train_time:111623ms step_avg:61.23ms
step:1824/2245 train_time:111683ms step_avg:61.23ms
step:1825/2245 train_time:111745ms step_avg:61.23ms
step:1826/2245 train_time:111805ms step_avg:61.23ms
step:1827/2245 train_time:111868ms step_avg:61.23ms
step:1828/2245 train_time:111928ms step_avg:61.23ms
step:1829/2245 train_time:111991ms step_avg:61.23ms
step:1830/2245 train_time:112051ms step_avg:61.23ms
step:1831/2245 train_time:112114ms step_avg:61.23ms
step:1832/2245 train_time:112175ms step_avg:61.23ms
step:1833/2245 train_time:112238ms step_avg:61.23ms
step:1834/2245 train_time:112298ms step_avg:61.23ms
step:1835/2245 train_time:112362ms step_avg:61.23ms
step:1836/2245 train_time:112423ms step_avg:61.23ms
step:1837/2245 train_time:112486ms step_avg:61.23ms
step:1838/2245 train_time:112546ms step_avg:61.23ms
step:1839/2245 train_time:112609ms step_avg:61.23ms
step:1840/2245 train_time:112670ms step_avg:61.23ms
step:1841/2245 train_time:112732ms step_avg:61.23ms
step:1842/2245 train_time:112793ms step_avg:61.23ms
step:1843/2245 train_time:112856ms step_avg:61.23ms
step:1844/2245 train_time:112917ms step_avg:61.23ms
step:1845/2245 train_time:112980ms step_avg:61.24ms
step:1846/2245 train_time:113040ms step_avg:61.24ms
step:1847/2245 train_time:113103ms step_avg:61.24ms
step:1848/2245 train_time:113163ms step_avg:61.24ms
step:1849/2245 train_time:113226ms step_avg:61.24ms
step:1850/2245 train_time:113286ms step_avg:61.24ms
step:1851/2245 train_time:113348ms step_avg:61.24ms
step:1852/2245 train_time:113409ms step_avg:61.24ms
step:1853/2245 train_time:113472ms step_avg:61.24ms
step:1854/2245 train_time:113532ms step_avg:61.24ms
step:1855/2245 train_time:113596ms step_avg:61.24ms
step:1856/2245 train_time:113657ms step_avg:61.24ms
step:1857/2245 train_time:113723ms step_avg:61.24ms
step:1858/2245 train_time:113781ms step_avg:61.24ms
step:1859/2245 train_time:113843ms step_avg:61.24ms
step:1860/2245 train_time:113903ms step_avg:61.24ms
step:1861/2245 train_time:113965ms step_avg:61.24ms
step:1862/2245 train_time:114025ms step_avg:61.24ms
step:1863/2245 train_time:114088ms step_avg:61.24ms
step:1864/2245 train_time:114148ms step_avg:61.24ms
step:1865/2245 train_time:114211ms step_avg:61.24ms
step:1866/2245 train_time:114271ms step_avg:61.24ms
step:1867/2245 train_time:114334ms step_avg:61.24ms
step:1868/2245 train_time:114395ms step_avg:61.24ms
step:1869/2245 train_time:114459ms step_avg:61.24ms
step:1870/2245 train_time:114519ms step_avg:61.24ms
step:1871/2245 train_time:114582ms step_avg:61.24ms
step:1872/2245 train_time:114643ms step_avg:61.24ms
step:1873/2245 train_time:114705ms step_avg:61.24ms
step:1874/2245 train_time:114766ms step_avg:61.24ms
step:1875/2245 train_time:114828ms step_avg:61.24ms
step:1876/2245 train_time:114888ms step_avg:61.24ms
step:1877/2245 train_time:114951ms step_avg:61.24ms
step:1878/2245 train_time:115011ms step_avg:61.24ms
step:1879/2245 train_time:115075ms step_avg:61.24ms
step:1880/2245 train_time:115135ms step_avg:61.24ms
step:1881/2245 train_time:115198ms step_avg:61.24ms
step:1882/2245 train_time:115259ms step_avg:61.24ms
step:1883/2245 train_time:115322ms step_avg:61.24ms
step:1884/2245 train_time:115383ms step_avg:61.24ms
step:1885/2245 train_time:115446ms step_avg:61.24ms
step:1886/2245 train_time:115506ms step_avg:61.24ms
step:1887/2245 train_time:115568ms step_avg:61.24ms
step:1888/2245 train_time:115630ms step_avg:61.24ms
step:1889/2245 train_time:115691ms step_avg:61.24ms
step:1890/2245 train_time:115751ms step_avg:61.24ms
step:1891/2245 train_time:115815ms step_avg:61.25ms
step:1892/2245 train_time:115876ms step_avg:61.25ms
step:1893/2245 train_time:115939ms step_avg:61.25ms
step:1894/2245 train_time:116000ms step_avg:61.25ms
step:1895/2245 train_time:116063ms step_avg:61.25ms
step:1896/2245 train_time:116123ms step_avg:61.25ms
step:1897/2245 train_time:116185ms step_avg:61.25ms
step:1898/2245 train_time:116245ms step_avg:61.25ms
step:1899/2245 train_time:116308ms step_avg:61.25ms
step:1900/2245 train_time:116370ms step_avg:61.25ms
step:1901/2245 train_time:116434ms step_avg:61.25ms
step:1902/2245 train_time:116493ms step_avg:61.25ms
step:1903/2245 train_time:116556ms step_avg:61.25ms
step:1904/2245 train_time:116618ms step_avg:61.25ms
step:1905/2245 train_time:116680ms step_avg:61.25ms
step:1906/2245 train_time:116740ms step_avg:61.25ms
step:1907/2245 train_time:116803ms step_avg:61.25ms
step:1908/2245 train_time:116863ms step_avg:61.25ms
step:1909/2245 train_time:116925ms step_avg:61.25ms
step:1910/2245 train_time:116985ms step_avg:61.25ms
step:1911/2245 train_time:117048ms step_avg:61.25ms
step:1912/2245 train_time:117108ms step_avg:61.25ms
step:1913/2245 train_time:117172ms step_avg:61.25ms
step:1914/2245 train_time:117232ms step_avg:61.25ms
step:1915/2245 train_time:117296ms step_avg:61.25ms
step:1916/2245 train_time:117357ms step_avg:61.25ms
step:1917/2245 train_time:117420ms step_avg:61.25ms
step:1918/2245 train_time:117481ms step_avg:61.25ms
step:1919/2245 train_time:117543ms step_avg:61.25ms
step:1920/2245 train_time:117604ms step_avg:61.25ms
step:1921/2245 train_time:117666ms step_avg:61.25ms
step:1922/2245 train_time:117727ms step_avg:61.25ms
step:1923/2245 train_time:117791ms step_avg:61.25ms
step:1924/2245 train_time:117850ms step_avg:61.25ms
step:1925/2245 train_time:117914ms step_avg:61.25ms
step:1926/2245 train_time:117974ms step_avg:61.25ms
step:1927/2245 train_time:118038ms step_avg:61.25ms
step:1928/2245 train_time:118099ms step_avg:61.25ms
step:1929/2245 train_time:118161ms step_avg:61.26ms
step:1930/2245 train_time:118222ms step_avg:61.25ms
step:1931/2245 train_time:118285ms step_avg:61.26ms
step:1932/2245 train_time:118345ms step_avg:61.26ms
step:1933/2245 train_time:118408ms step_avg:61.26ms
step:1934/2245 train_time:118469ms step_avg:61.26ms
step:1935/2245 train_time:118531ms step_avg:61.26ms
step:1936/2245 train_time:118592ms step_avg:61.26ms
step:1937/2245 train_time:118655ms step_avg:61.26ms
step:1938/2245 train_time:118716ms step_avg:61.26ms
step:1939/2245 train_time:118779ms step_avg:61.26ms
step:1940/2245 train_time:118839ms step_avg:61.26ms
step:1941/2245 train_time:118902ms step_avg:61.26ms
step:1942/2245 train_time:118962ms step_avg:61.26ms
step:1943/2245 train_time:119025ms step_avg:61.26ms
step:1944/2245 train_time:119085ms step_avg:61.26ms
step:1945/2245 train_time:119148ms step_avg:61.26ms
step:1946/2245 train_time:119209ms step_avg:61.26ms
step:1947/2245 train_time:119271ms step_avg:61.26ms
step:1948/2245 train_time:119332ms step_avg:61.26ms
step:1949/2245 train_time:119394ms step_avg:61.26ms
step:1950/2245 train_time:119455ms step_avg:61.26ms
step:1951/2245 train_time:119517ms step_avg:61.26ms
step:1952/2245 train_time:119579ms step_avg:61.26ms
step:1953/2245 train_time:119642ms step_avg:61.26ms
step:1954/2245 train_time:119702ms step_avg:61.26ms
step:1955/2245 train_time:119764ms step_avg:61.26ms
step:1956/2245 train_time:119824ms step_avg:61.26ms
step:1957/2245 train_time:119887ms step_avg:61.26ms
step:1958/2245 train_time:119947ms step_avg:61.26ms
step:1959/2245 train_time:120010ms step_avg:61.26ms
step:1960/2245 train_time:120070ms step_avg:61.26ms
step:1961/2245 train_time:120133ms step_avg:61.26ms
step:1962/2245 train_time:120193ms step_avg:61.26ms
step:1963/2245 train_time:120257ms step_avg:61.26ms
step:1964/2245 train_time:120319ms step_avg:61.26ms
step:1965/2245 train_time:120382ms step_avg:61.26ms
step:1966/2245 train_time:120442ms step_avg:61.26ms
step:1967/2245 train_time:120505ms step_avg:61.26ms
step:1968/2245 train_time:120566ms step_avg:61.26ms
step:1969/2245 train_time:120628ms step_avg:61.26ms
step:1970/2245 train_time:120689ms step_avg:61.26ms
step:1971/2245 train_time:120752ms step_avg:61.26ms
step:1972/2245 train_time:120813ms step_avg:61.26ms
step:1973/2245 train_time:120876ms step_avg:61.27ms
step:1974/2245 train_time:120937ms step_avg:61.27ms
step:1975/2245 train_time:121000ms step_avg:61.27ms
step:1976/2245 train_time:121061ms step_avg:61.27ms
step:1977/2245 train_time:121125ms step_avg:61.27ms
step:1978/2245 train_time:121184ms step_avg:61.27ms
step:1979/2245 train_time:121247ms step_avg:61.27ms
step:1980/2245 train_time:121307ms step_avg:61.27ms
step:1981/2245 train_time:121370ms step_avg:61.27ms
step:1982/2245 train_time:121431ms step_avg:61.27ms
step:1983/2245 train_time:121494ms step_avg:61.27ms
step:1984/2245 train_time:121555ms step_avg:61.27ms
step:1985/2245 train_time:121618ms step_avg:61.27ms
step:1986/2245 train_time:121679ms step_avg:61.27ms
step:1987/2245 train_time:121741ms step_avg:61.27ms
step:1988/2245 train_time:121801ms step_avg:61.27ms
step:1989/2245 train_time:121864ms step_avg:61.27ms
step:1990/2245 train_time:121924ms step_avg:61.27ms
step:1991/2245 train_time:121986ms step_avg:61.27ms
step:1992/2245 train_time:122046ms step_avg:61.27ms
step:1993/2245 train_time:122109ms step_avg:61.27ms
step:1994/2245 train_time:122170ms step_avg:61.27ms
step:1995/2245 train_time:122233ms step_avg:61.27ms
step:1996/2245 train_time:122294ms step_avg:61.27ms
step:1997/2245 train_time:122357ms step_avg:61.27ms
step:1998/2245 train_time:122418ms step_avg:61.27ms
step:1999/2245 train_time:122480ms step_avg:61.27ms
step:2000/2245 train_time:122541ms step_avg:61.27ms
step:2000/2245 val_loss:3.3224 train_time:122605ms step_avg:61.30ms
step:2001/2245 train_time:122626ms step_avg:61.28ms
step:2002/2245 train_time:122666ms step_avg:61.27ms
step:2003/2245 train_time:122733ms step_avg:61.27ms
step:2004/2245 train_time:122796ms step_avg:61.28ms
step:2005/2245 train_time:122859ms step_avg:61.28ms
step:2006/2245 train_time:122920ms step_avg:61.28ms
step:2007/2245 train_time:122983ms step_avg:61.28ms
step:2008/2245 train_time:123042ms step_avg:61.28ms
step:2009/2245 train_time:123104ms step_avg:61.28ms
step:2010/2245 train_time:123163ms step_avg:61.28ms
step:2011/2245 train_time:123225ms step_avg:61.28ms
step:2012/2245 train_time:123285ms step_avg:61.27ms
step:2013/2245 train_time:123347ms step_avg:61.28ms
step:2014/2245 train_time:123407ms step_avg:61.27ms
step:2015/2245 train_time:123469ms step_avg:61.27ms
step:2016/2245 train_time:123529ms step_avg:61.27ms
step:2017/2245 train_time:123593ms step_avg:61.28ms
step:2018/2245 train_time:123654ms step_avg:61.28ms
step:2019/2245 train_time:123719ms step_avg:61.28ms
step:2020/2245 train_time:123781ms step_avg:61.28ms
step:2021/2245 train_time:123845ms step_avg:61.28ms
step:2022/2245 train_time:123905ms step_avg:61.28ms
step:2023/2245 train_time:123968ms step_avg:61.28ms
step:2024/2245 train_time:124029ms step_avg:61.28ms
step:2025/2245 train_time:124092ms step_avg:61.28ms
step:2026/2245 train_time:124152ms step_avg:61.28ms
step:2027/2245 train_time:124214ms step_avg:61.28ms
step:2028/2245 train_time:124275ms step_avg:61.28ms
step:2029/2245 train_time:124337ms step_avg:61.28ms
step:2030/2245 train_time:124398ms step_avg:61.28ms
step:2031/2245 train_time:124461ms step_avg:61.28ms
step:2032/2245 train_time:124522ms step_avg:61.28ms
step:2033/2245 train_time:124585ms step_avg:61.28ms
step:2034/2245 train_time:124646ms step_avg:61.28ms
step:2035/2245 train_time:124709ms step_avg:61.28ms
step:2036/2245 train_time:124770ms step_avg:61.28ms
step:2037/2245 train_time:124833ms step_avg:61.28ms
step:2038/2245 train_time:124894ms step_avg:61.28ms
step:2039/2245 train_time:124958ms step_avg:61.28ms
step:2040/2245 train_time:125018ms step_avg:61.28ms
step:2041/2245 train_time:125082ms step_avg:61.28ms
step:2042/2245 train_time:125142ms step_avg:61.28ms
step:2043/2245 train_time:125204ms step_avg:61.28ms
step:2044/2245 train_time:125265ms step_avg:61.28ms
step:2045/2245 train_time:125327ms step_avg:61.28ms
step:2046/2245 train_time:125387ms step_avg:61.28ms
step:2047/2245 train_time:125451ms step_avg:61.29ms
step:2048/2245 train_time:125511ms step_avg:61.28ms
step:2049/2245 train_time:125574ms step_avg:61.29ms
step:2050/2245 train_time:125634ms step_avg:61.29ms
step:2051/2245 train_time:125698ms step_avg:61.29ms
step:2052/2245 train_time:125759ms step_avg:61.29ms
step:2053/2245 train_time:125822ms step_avg:61.29ms
step:2054/2245 train_time:125883ms step_avg:61.29ms
step:2055/2245 train_time:125946ms step_avg:61.29ms
step:2056/2245 train_time:126007ms step_avg:61.29ms
step:2057/2245 train_time:126070ms step_avg:61.29ms
step:2058/2245 train_time:126130ms step_avg:61.29ms
step:2059/2245 train_time:126193ms step_avg:61.29ms
step:2060/2245 train_time:126253ms step_avg:61.29ms
step:2061/2245 train_time:126317ms step_avg:61.29ms
step:2062/2245 train_time:126378ms step_avg:61.29ms
step:2063/2245 train_time:126441ms step_avg:61.29ms
step:2064/2245 train_time:126502ms step_avg:61.29ms
step:2065/2245 train_time:126565ms step_avg:61.29ms
step:2066/2245 train_time:126625ms step_avg:61.29ms
step:2067/2245 train_time:126688ms step_avg:61.29ms
step:2068/2245 train_time:126748ms step_avg:61.29ms
step:2069/2245 train_time:126812ms step_avg:61.29ms
step:2070/2245 train_time:126873ms step_avg:61.29ms
step:2071/2245 train_time:126936ms step_avg:61.29ms
step:2072/2245 train_time:126998ms step_avg:61.29ms
step:2073/2245 train_time:127062ms step_avg:61.29ms
step:2074/2245 train_time:127122ms step_avg:61.29ms
step:2075/2245 train_time:127185ms step_avg:61.29ms
step:2076/2245 train_time:127245ms step_avg:61.29ms
step:2077/2245 train_time:127307ms step_avg:61.29ms
step:2078/2245 train_time:127367ms step_avg:61.29ms
step:2079/2245 train_time:127430ms step_avg:61.29ms
step:2080/2245 train_time:127491ms step_avg:61.29ms
step:2081/2245 train_time:127553ms step_avg:61.29ms
step:2082/2245 train_time:127614ms step_avg:61.29ms
step:2083/2245 train_time:127678ms step_avg:61.30ms
step:2084/2245 train_time:127739ms step_avg:61.30ms
step:2085/2245 train_time:127803ms step_avg:61.30ms
step:2086/2245 train_time:127863ms step_avg:61.30ms
step:2087/2245 train_time:127925ms step_avg:61.30ms
step:2088/2245 train_time:127986ms step_avg:61.30ms
step:2089/2245 train_time:128049ms step_avg:61.30ms
step:2090/2245 train_time:128109ms step_avg:61.30ms
step:2091/2245 train_time:128172ms step_avg:61.30ms
step:2092/2245 train_time:128233ms step_avg:61.30ms
step:2093/2245 train_time:128296ms step_avg:61.30ms
step:2094/2245 train_time:128357ms step_avg:61.30ms
step:2095/2245 train_time:128421ms step_avg:61.30ms
step:2096/2245 train_time:128481ms step_avg:61.30ms
step:2097/2245 train_time:128544ms step_avg:61.30ms
step:2098/2245 train_time:128603ms step_avg:61.30ms
step:2099/2245 train_time:128667ms step_avg:61.30ms
step:2100/2245 train_time:128727ms step_avg:61.30ms
step:2101/2245 train_time:128790ms step_avg:61.30ms
step:2102/2245 train_time:128851ms step_avg:61.30ms
step:2103/2245 train_time:128914ms step_avg:61.30ms
step:2104/2245 train_time:128976ms step_avg:61.30ms
step:2105/2245 train_time:129040ms step_avg:61.30ms
step:2106/2245 train_time:129101ms step_avg:61.30ms
step:2107/2245 train_time:129163ms step_avg:61.30ms
step:2108/2245 train_time:129223ms step_avg:61.30ms
step:2109/2245 train_time:129286ms step_avg:61.30ms
step:2110/2245 train_time:129347ms step_avg:61.30ms
step:2111/2245 train_time:129409ms step_avg:61.30ms
step:2112/2245 train_time:129469ms step_avg:61.30ms
step:2113/2245 train_time:129532ms step_avg:61.30ms
step:2114/2245 train_time:129593ms step_avg:61.30ms
step:2115/2245 train_time:129657ms step_avg:61.30ms
step:2116/2245 train_time:129718ms step_avg:61.30ms
step:2117/2245 train_time:129782ms step_avg:61.30ms
step:2118/2245 train_time:129842ms step_avg:61.30ms
step:2119/2245 train_time:129905ms step_avg:61.30ms
step:2120/2245 train_time:129965ms step_avg:61.30ms
step:2121/2245 train_time:130028ms step_avg:61.31ms
step:2122/2245 train_time:130090ms step_avg:61.31ms
step:2123/2245 train_time:130153ms step_avg:61.31ms
step:2124/2245 train_time:130213ms step_avg:61.31ms
step:2125/2245 train_time:130276ms step_avg:61.31ms
step:2126/2245 train_time:130336ms step_avg:61.31ms
step:2127/2245 train_time:130399ms step_avg:61.31ms
step:2128/2245 train_time:130459ms step_avg:61.31ms
step:2129/2245 train_time:130523ms step_avg:61.31ms
step:2130/2245 train_time:130583ms step_avg:61.31ms
step:2131/2245 train_time:130646ms step_avg:61.31ms
step:2132/2245 train_time:130706ms step_avg:61.31ms
step:2133/2245 train_time:130769ms step_avg:61.31ms
step:2134/2245 train_time:130829ms step_avg:61.31ms
step:2135/2245 train_time:130892ms step_avg:61.31ms
step:2136/2245 train_time:130953ms step_avg:61.31ms
step:2137/2245 train_time:131016ms step_avg:61.31ms
step:2138/2245 train_time:131077ms step_avg:61.31ms
step:2139/2245 train_time:131140ms step_avg:61.31ms
step:2140/2245 train_time:131201ms step_avg:61.31ms
step:2141/2245 train_time:131263ms step_avg:61.31ms
step:2142/2245 train_time:131323ms step_avg:61.31ms
step:2143/2245 train_time:131386ms step_avg:61.31ms
step:2144/2245 train_time:131447ms step_avg:61.31ms
step:2145/2245 train_time:131509ms step_avg:61.31ms
step:2146/2245 train_time:131569ms step_avg:61.31ms
step:2147/2245 train_time:131633ms step_avg:61.31ms
step:2148/2245 train_time:131694ms step_avg:61.31ms
step:2149/2245 train_time:131759ms step_avg:61.31ms
step:2150/2245 train_time:131819ms step_avg:61.31ms
step:2151/2245 train_time:131883ms step_avg:61.31ms
step:2152/2245 train_time:131944ms step_avg:61.31ms
step:2153/2245 train_time:132006ms step_avg:61.31ms
step:2154/2245 train_time:132067ms step_avg:61.31ms
step:2155/2245 train_time:132129ms step_avg:61.31ms
step:2156/2245 train_time:132189ms step_avg:61.31ms
step:2157/2245 train_time:132253ms step_avg:61.31ms
step:2158/2245 train_time:132313ms step_avg:61.31ms
step:2159/2245 train_time:132376ms step_avg:61.31ms
step:2160/2245 train_time:132437ms step_avg:61.31ms
step:2161/2245 train_time:132501ms step_avg:61.31ms
step:2162/2245 train_time:132561ms step_avg:61.31ms
step:2163/2245 train_time:132624ms step_avg:61.31ms
step:2164/2245 train_time:132684ms step_avg:61.31ms
step:2165/2245 train_time:132747ms step_avg:61.31ms
step:2166/2245 train_time:132807ms step_avg:61.31ms
step:2167/2245 train_time:132870ms step_avg:61.32ms
step:2168/2245 train_time:132930ms step_avg:61.31ms
step:2169/2245 train_time:132993ms step_avg:61.32ms
step:2170/2245 train_time:133053ms step_avg:61.31ms
step:2171/2245 train_time:133117ms step_avg:61.32ms
step:2172/2245 train_time:133178ms step_avg:61.32ms
step:2173/2245 train_time:133242ms step_avg:61.32ms
step:2174/2245 train_time:133302ms step_avg:61.32ms
step:2175/2245 train_time:133365ms step_avg:61.32ms
step:2176/2245 train_time:133426ms step_avg:61.32ms
step:2177/2245 train_time:133488ms step_avg:61.32ms
step:2178/2245 train_time:133548ms step_avg:61.32ms
step:2179/2245 train_time:133611ms step_avg:61.32ms
step:2180/2245 train_time:133672ms step_avg:61.32ms
step:2181/2245 train_time:133735ms step_avg:61.32ms
step:2182/2245 train_time:133796ms step_avg:61.32ms
step:2183/2245 train_time:133860ms step_avg:61.32ms
step:2184/2245 train_time:133921ms step_avg:61.32ms
step:2185/2245 train_time:133984ms step_avg:61.32ms
step:2186/2245 train_time:134044ms step_avg:61.32ms
step:2187/2245 train_time:134107ms step_avg:61.32ms
step:2188/2245 train_time:134168ms step_avg:61.32ms
step:2189/2245 train_time:134230ms step_avg:61.32ms
step:2190/2245 train_time:134290ms step_avg:61.32ms
step:2191/2245 train_time:134353ms step_avg:61.32ms
step:2192/2245 train_time:134413ms step_avg:61.32ms
step:2193/2245 train_time:134477ms step_avg:61.32ms
step:2194/2245 train_time:134538ms step_avg:61.32ms
step:2195/2245 train_time:134601ms step_avg:61.32ms
step:2196/2245 train_time:134661ms step_avg:61.32ms
step:2197/2245 train_time:134724ms step_avg:61.32ms
step:2198/2245 train_time:134784ms step_avg:61.32ms
step:2199/2245 train_time:134848ms step_avg:61.32ms
step:2200/2245 train_time:134908ms step_avg:61.32ms
step:2201/2245 train_time:134970ms step_avg:61.32ms
step:2202/2245 train_time:135031ms step_avg:61.32ms
step:2203/2245 train_time:135094ms step_avg:61.32ms
step:2204/2245 train_time:135154ms step_avg:61.32ms
step:2205/2245 train_time:135217ms step_avg:61.32ms
step:2206/2245 train_time:135278ms step_avg:61.32ms
step:2207/2245 train_time:135341ms step_avg:61.32ms
step:2208/2245 train_time:135402ms step_avg:61.32ms
step:2209/2245 train_time:135465ms step_avg:61.32ms
step:2210/2245 train_time:135525ms step_avg:61.32ms
step:2211/2245 train_time:135588ms step_avg:61.32ms
step:2212/2245 train_time:135649ms step_avg:61.32ms
step:2213/2245 train_time:135712ms step_avg:61.32ms
step:2214/2245 train_time:135772ms step_avg:61.32ms
step:2215/2245 train_time:135836ms step_avg:61.33ms
step:2216/2245 train_time:135897ms step_avg:61.33ms
step:2217/2245 train_time:135960ms step_avg:61.33ms
step:2218/2245 train_time:136021ms step_avg:61.33ms
step:2219/2245 train_time:136084ms step_avg:61.33ms
step:2220/2245 train_time:136144ms step_avg:61.33ms
step:2221/2245 train_time:136207ms step_avg:61.33ms
step:2222/2245 train_time:136268ms step_avg:61.33ms
step:2223/2245 train_time:136331ms step_avg:61.33ms
step:2224/2245 train_time:136392ms step_avg:61.33ms
step:2225/2245 train_time:136455ms step_avg:61.33ms
step:2226/2245 train_time:136515ms step_avg:61.33ms
step:2227/2245 train_time:136579ms step_avg:61.33ms
step:2228/2245 train_time:136640ms step_avg:61.33ms
step:2229/2245 train_time:136704ms step_avg:61.33ms
step:2230/2245 train_time:136764ms step_avg:61.33ms
step:2231/2245 train_time:136827ms step_avg:61.33ms
step:2232/2245 train_time:136887ms step_avg:61.33ms
step:2233/2245 train_time:136950ms step_avg:61.33ms
step:2234/2245 train_time:137011ms step_avg:61.33ms
step:2235/2245 train_time:137074ms step_avg:61.33ms
step:2236/2245 train_time:137134ms step_avg:61.33ms
step:2237/2245 train_time:137197ms step_avg:61.33ms
step:2238/2245 train_time:137258ms step_avg:61.33ms
step:2239/2245 train_time:137321ms step_avg:61.33ms
step:2240/2245 train_time:137382ms step_avg:61.33ms
step:2241/2245 train_time:137444ms step_avg:61.33ms
step:2242/2245 train_time:137504ms step_avg:61.33ms
step:2243/2245 train_time:137568ms step_avg:61.33ms
step:2244/2245 train_time:137629ms step_avg:61.33ms
step:2245/2245 train_time:137692ms step_avg:61.33ms
step:2245/2245 val_loss:3.2771 train_time:137753ms step_avg:61.36ms
peak memory allocated: 29626 MiB reserved: 50528 MiB

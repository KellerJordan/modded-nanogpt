import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Nov 10 21:31:59 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   36C    P0            125W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   35C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   35C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2245 train_time:119ms step_avg:119.48ms
step:2/2245 train_time:141ms step_avg:70.58ms
step:3/2245 train_time:179ms step_avg:59.77ms
step:4/2245 train_time:236ms step_avg:58.96ms
step:5/2245 train_time:296ms step_avg:59.13ms
step:6/2245 train_time:354ms step_avg:59.02ms
step:7/2245 train_time:415ms step_avg:59.30ms
step:8/2245 train_time:474ms step_avg:59.22ms
step:9/2245 train_time:535ms step_avg:59.46ms
step:10/2245 train_time:594ms step_avg:59.41ms
step:11/2245 train_time:655ms step_avg:59.57ms
step:12/2245 train_time:714ms step_avg:59.51ms
step:13/2245 train_time:775ms step_avg:59.61ms
step:14/2245 train_time:834ms step_avg:59.55ms
step:15/2245 train_time:895ms step_avg:59.67ms
step:16/2245 train_time:954ms step_avg:59.60ms
step:17/2245 train_time:1017ms step_avg:59.83ms
step:18/2245 train_time:1080ms step_avg:60.00ms
step:19/2245 train_time:1144ms step_avg:60.23ms
step:20/2245 train_time:1204ms step_avg:60.20ms
step:21/2245 train_time:1267ms step_avg:60.31ms
step:22/2245 train_time:1327ms step_avg:60.31ms
step:23/2245 train_time:1389ms step_avg:60.41ms
step:24/2245 train_time:1449ms step_avg:60.36ms
step:25/2245 train_time:1511ms step_avg:60.43ms
step:26/2245 train_time:1570ms step_avg:60.40ms
step:27/2245 train_time:1631ms step_avg:60.42ms
step:28/2245 train_time:1691ms step_avg:60.38ms
step:29/2245 train_time:1753ms step_avg:60.44ms
step:30/2245 train_time:1812ms step_avg:60.39ms
step:31/2245 train_time:1873ms step_avg:60.41ms
step:32/2245 train_time:1932ms step_avg:60.38ms
step:33/2245 train_time:1995ms step_avg:60.44ms
step:34/2245 train_time:2055ms step_avg:60.45ms
step:35/2245 train_time:2118ms step_avg:60.51ms
step:36/2245 train_time:2178ms step_avg:60.51ms
step:37/2245 train_time:2241ms step_avg:60.57ms
step:38/2245 train_time:2301ms step_avg:60.55ms
step:39/2245 train_time:2363ms step_avg:60.59ms
step:40/2245 train_time:2422ms step_avg:60.56ms
step:41/2245 train_time:2484ms step_avg:60.60ms
step:42/2245 train_time:2544ms step_avg:60.57ms
step:43/2245 train_time:2606ms step_avg:60.60ms
step:44/2245 train_time:2666ms step_avg:60.58ms
step:45/2245 train_time:2728ms step_avg:60.62ms
step:46/2245 train_time:2788ms step_avg:60.60ms
step:47/2245 train_time:2849ms step_avg:60.62ms
step:48/2245 train_time:2908ms step_avg:60.59ms
step:49/2245 train_time:2970ms step_avg:60.61ms
step:50/2245 train_time:3029ms step_avg:60.59ms
step:51/2245 train_time:3092ms step_avg:60.62ms
step:52/2245 train_time:3152ms step_avg:60.61ms
step:53/2245 train_time:3214ms step_avg:60.63ms
step:54/2245 train_time:3273ms step_avg:60.61ms
step:55/2245 train_time:3335ms step_avg:60.64ms
step:56/2245 train_time:3394ms step_avg:60.61ms
step:57/2245 train_time:3456ms step_avg:60.63ms
step:58/2245 train_time:3515ms step_avg:60.61ms
step:59/2245 train_time:3578ms step_avg:60.65ms
step:60/2245 train_time:3638ms step_avg:60.63ms
step:61/2245 train_time:3701ms step_avg:60.67ms
step:62/2245 train_time:3760ms step_avg:60.64ms
step:63/2245 train_time:3822ms step_avg:60.66ms
step:64/2245 train_time:3881ms step_avg:60.64ms
step:65/2245 train_time:3943ms step_avg:60.66ms
step:66/2245 train_time:4003ms step_avg:60.65ms
step:67/2245 train_time:4065ms step_avg:60.68ms
step:68/2245 train_time:4125ms step_avg:60.66ms
step:69/2245 train_time:4187ms step_avg:60.68ms
step:70/2245 train_time:4247ms step_avg:60.67ms
step:71/2245 train_time:4309ms step_avg:60.69ms
step:72/2245 train_time:4368ms step_avg:60.67ms
step:73/2245 train_time:4430ms step_avg:60.68ms
step:74/2245 train_time:4489ms step_avg:60.67ms
step:75/2245 train_time:4551ms step_avg:60.68ms
step:76/2245 train_time:4610ms step_avg:60.65ms
step:77/2245 train_time:4671ms step_avg:60.67ms
step:78/2245 train_time:4730ms step_avg:60.65ms
step:79/2245 train_time:4792ms step_avg:60.66ms
step:80/2245 train_time:4852ms step_avg:60.65ms
step:81/2245 train_time:4914ms step_avg:60.67ms
step:82/2245 train_time:4973ms step_avg:60.65ms
step:83/2245 train_time:5035ms step_avg:60.67ms
step:84/2245 train_time:5095ms step_avg:60.65ms
step:85/2245 train_time:5156ms step_avg:60.66ms
step:86/2245 train_time:5216ms step_avg:60.65ms
step:87/2245 train_time:5278ms step_avg:60.67ms
step:88/2245 train_time:5337ms step_avg:60.65ms
step:89/2245 train_time:5400ms step_avg:60.67ms
step:90/2245 train_time:5459ms step_avg:60.65ms
step:91/2245 train_time:5521ms step_avg:60.67ms
step:92/2245 train_time:5580ms step_avg:60.65ms
step:93/2245 train_time:5642ms step_avg:60.66ms
step:94/2245 train_time:5702ms step_avg:60.65ms
step:95/2245 train_time:5763ms step_avg:60.67ms
step:96/2245 train_time:5823ms step_avg:60.65ms
step:97/2245 train_time:5885ms step_avg:60.67ms
step:98/2245 train_time:5945ms step_avg:60.66ms
step:99/2245 train_time:6006ms step_avg:60.67ms
step:100/2245 train_time:6066ms step_avg:60.66ms
step:101/2245 train_time:6128ms step_avg:60.68ms
step:102/2245 train_time:6188ms step_avg:60.67ms
step:103/2245 train_time:6251ms step_avg:60.68ms
step:104/2245 train_time:6310ms step_avg:60.67ms
step:105/2245 train_time:6371ms step_avg:60.67ms
step:106/2245 train_time:6429ms step_avg:60.65ms
step:107/2245 train_time:6491ms step_avg:60.66ms
step:108/2245 train_time:6551ms step_avg:60.65ms
step:109/2245 train_time:6612ms step_avg:60.66ms
step:110/2245 train_time:6671ms step_avg:60.65ms
step:111/2245 train_time:6733ms step_avg:60.66ms
step:112/2245 train_time:6792ms step_avg:60.64ms
step:113/2245 train_time:6853ms step_avg:60.65ms
step:114/2245 train_time:6912ms step_avg:60.63ms
step:115/2245 train_time:6973ms step_avg:60.63ms
step:116/2245 train_time:7032ms step_avg:60.62ms
step:117/2245 train_time:7094ms step_avg:60.63ms
step:118/2245 train_time:7154ms step_avg:60.62ms
step:119/2245 train_time:7215ms step_avg:60.63ms
step:120/2245 train_time:7275ms step_avg:60.62ms
step:121/2245 train_time:7336ms step_avg:60.63ms
step:122/2245 train_time:7395ms step_avg:60.62ms
step:123/2245 train_time:7456ms step_avg:60.62ms
step:124/2245 train_time:7516ms step_avg:60.61ms
step:125/2245 train_time:7578ms step_avg:60.62ms
step:126/2245 train_time:7637ms step_avg:60.61ms
step:127/2245 train_time:7698ms step_avg:60.61ms
step:128/2245 train_time:7757ms step_avg:60.60ms
step:129/2245 train_time:7819ms step_avg:60.61ms
step:130/2245 train_time:7878ms step_avg:60.60ms
step:131/2245 train_time:7940ms step_avg:60.61ms
step:132/2245 train_time:7999ms step_avg:60.60ms
step:133/2245 train_time:8061ms step_avg:60.61ms
step:134/2245 train_time:8121ms step_avg:60.60ms
step:135/2245 train_time:8183ms step_avg:60.62ms
step:136/2245 train_time:8243ms step_avg:60.61ms
step:137/2245 train_time:8305ms step_avg:60.62ms
step:138/2245 train_time:8365ms step_avg:60.62ms
step:139/2245 train_time:8427ms step_avg:60.63ms
step:140/2245 train_time:8487ms step_avg:60.62ms
step:141/2245 train_time:8548ms step_avg:60.62ms
step:142/2245 train_time:8607ms step_avg:60.61ms
step:143/2245 train_time:8669ms step_avg:60.62ms
step:144/2245 train_time:8728ms step_avg:60.61ms
step:145/2245 train_time:8790ms step_avg:60.62ms
step:146/2245 train_time:8848ms step_avg:60.61ms
step:147/2245 train_time:8910ms step_avg:60.61ms
step:148/2245 train_time:8969ms step_avg:60.60ms
step:149/2245 train_time:9031ms step_avg:60.61ms
step:150/2245 train_time:9090ms step_avg:60.60ms
step:151/2245 train_time:9152ms step_avg:60.61ms
step:152/2245 train_time:9212ms step_avg:60.60ms
step:153/2245 train_time:9274ms step_avg:60.61ms
step:154/2245 train_time:9333ms step_avg:60.60ms
step:155/2245 train_time:9395ms step_avg:60.61ms
step:156/2245 train_time:9454ms step_avg:60.60ms
step:157/2245 train_time:9517ms step_avg:60.62ms
step:158/2245 train_time:9576ms step_avg:60.61ms
step:159/2245 train_time:9638ms step_avg:60.62ms
step:160/2245 train_time:9698ms step_avg:60.61ms
step:161/2245 train_time:9759ms step_avg:60.62ms
step:162/2245 train_time:9818ms step_avg:60.60ms
step:163/2245 train_time:9880ms step_avg:60.61ms
step:164/2245 train_time:9938ms step_avg:60.60ms
step:165/2245 train_time:9999ms step_avg:60.60ms
step:166/2245 train_time:10058ms step_avg:60.59ms
step:167/2245 train_time:10120ms step_avg:60.60ms
step:168/2245 train_time:10180ms step_avg:60.59ms
step:169/2245 train_time:10241ms step_avg:60.60ms
step:170/2245 train_time:10301ms step_avg:60.59ms
step:171/2245 train_time:10363ms step_avg:60.60ms
step:172/2245 train_time:10423ms step_avg:60.60ms
step:173/2245 train_time:10485ms step_avg:60.60ms
step:174/2245 train_time:10544ms step_avg:60.60ms
step:175/2245 train_time:10606ms step_avg:60.61ms
step:176/2245 train_time:10665ms step_avg:60.60ms
step:177/2245 train_time:10727ms step_avg:60.60ms
step:178/2245 train_time:10787ms step_avg:60.60ms
step:179/2245 train_time:10848ms step_avg:60.61ms
step:180/2245 train_time:10907ms step_avg:60.60ms
step:181/2245 train_time:10969ms step_avg:60.60ms
step:182/2245 train_time:11027ms step_avg:60.59ms
step:183/2245 train_time:11089ms step_avg:60.60ms
step:184/2245 train_time:11148ms step_avg:60.59ms
step:185/2245 train_time:11210ms step_avg:60.59ms
step:186/2245 train_time:11269ms step_avg:60.59ms
step:187/2245 train_time:11331ms step_avg:60.59ms
step:188/2245 train_time:11391ms step_avg:60.59ms
step:189/2245 train_time:11452ms step_avg:60.59ms
step:190/2245 train_time:11511ms step_avg:60.58ms
step:191/2245 train_time:11572ms step_avg:60.59ms
step:192/2245 train_time:11630ms step_avg:60.57ms
step:193/2245 train_time:11692ms step_avg:60.58ms
step:194/2245 train_time:11751ms step_avg:60.57ms
step:195/2245 train_time:11812ms step_avg:60.57ms
step:196/2245 train_time:11871ms step_avg:60.57ms
step:197/2245 train_time:11932ms step_avg:60.57ms
step:198/2245 train_time:11991ms step_avg:60.56ms
step:199/2245 train_time:12053ms step_avg:60.57ms
step:200/2245 train_time:12112ms step_avg:60.56ms
step:201/2245 train_time:12174ms step_avg:60.57ms
step:202/2245 train_time:12233ms step_avg:60.56ms
step:203/2245 train_time:12295ms step_avg:60.57ms
step:204/2245 train_time:12354ms step_avg:60.56ms
step:205/2245 train_time:12416ms step_avg:60.57ms
step:206/2245 train_time:12475ms step_avg:60.56ms
step:207/2245 train_time:12536ms step_avg:60.56ms
step:208/2245 train_time:12595ms step_avg:60.55ms
step:209/2245 train_time:12656ms step_avg:60.56ms
step:210/2245 train_time:12715ms step_avg:60.55ms
step:211/2245 train_time:12777ms step_avg:60.55ms
step:212/2245 train_time:12836ms step_avg:60.55ms
step:213/2245 train_time:12897ms step_avg:60.55ms
step:214/2245 train_time:12956ms step_avg:60.54ms
step:215/2245 train_time:13018ms step_avg:60.55ms
step:216/2245 train_time:13078ms step_avg:60.54ms
step:217/2245 train_time:13139ms step_avg:60.55ms
step:218/2245 train_time:13198ms step_avg:60.54ms
step:219/2245 train_time:13260ms step_avg:60.55ms
step:220/2245 train_time:13318ms step_avg:60.54ms
step:221/2245 train_time:13380ms step_avg:60.54ms
step:222/2245 train_time:13438ms step_avg:60.53ms
step:223/2245 train_time:13500ms step_avg:60.54ms
step:224/2245 train_time:13559ms step_avg:60.53ms
step:225/2245 train_time:13621ms step_avg:60.54ms
step:226/2245 train_time:13681ms step_avg:60.53ms
step:227/2245 train_time:13742ms step_avg:60.54ms
step:228/2245 train_time:13801ms step_avg:60.53ms
step:229/2245 train_time:13863ms step_avg:60.54ms
step:230/2245 train_time:13922ms step_avg:60.53ms
step:231/2245 train_time:13984ms step_avg:60.54ms
step:232/2245 train_time:14044ms step_avg:60.53ms
step:233/2245 train_time:14105ms step_avg:60.54ms
step:234/2245 train_time:14165ms step_avg:60.53ms
step:235/2245 train_time:14226ms step_avg:60.54ms
step:236/2245 train_time:14286ms step_avg:60.53ms
step:237/2245 train_time:14348ms step_avg:60.54ms
step:238/2245 train_time:14408ms step_avg:60.54ms
step:239/2245 train_time:14470ms step_avg:60.54ms
step:240/2245 train_time:14529ms step_avg:60.54ms
step:241/2245 train_time:14591ms step_avg:60.54ms
step:242/2245 train_time:14650ms step_avg:60.54ms
step:243/2245 train_time:14712ms step_avg:60.54ms
step:244/2245 train_time:14772ms step_avg:60.54ms
step:245/2245 train_time:14833ms step_avg:60.54ms
step:246/2245 train_time:14893ms step_avg:60.54ms
step:247/2245 train_time:14955ms step_avg:60.55ms
step:248/2245 train_time:15013ms step_avg:60.54ms
step:249/2245 train_time:15075ms step_avg:60.54ms
step:250/2245 train_time:15134ms step_avg:60.53ms
step:250/2245 val_loss:4.0721 train_time:15196ms step_avg:60.78ms
step:251/2245 train_time:15216ms step_avg:60.62ms
step:252/2245 train_time:15256ms step_avg:60.54ms
step:253/2245 train_time:15323ms step_avg:60.57ms
step:254/2245 train_time:15385ms step_avg:60.57ms
step:255/2245 train_time:15446ms step_avg:60.57ms
step:256/2245 train_time:15506ms step_avg:60.57ms
step:257/2245 train_time:15567ms step_avg:60.57ms
step:258/2245 train_time:15626ms step_avg:60.56ms
step:259/2245 train_time:15686ms step_avg:60.56ms
step:260/2245 train_time:15745ms step_avg:60.56ms
step:261/2245 train_time:15805ms step_avg:60.55ms
step:262/2245 train_time:15863ms step_avg:60.54ms
step:263/2245 train_time:15923ms step_avg:60.55ms
step:264/2245 train_time:15982ms step_avg:60.54ms
step:265/2245 train_time:16042ms step_avg:60.54ms
step:266/2245 train_time:16101ms step_avg:60.53ms
step:267/2245 train_time:16165ms step_avg:60.54ms
step:268/2245 train_time:16226ms step_avg:60.54ms
step:269/2245 train_time:16289ms step_avg:60.56ms
step:270/2245 train_time:16349ms step_avg:60.55ms
step:271/2245 train_time:16411ms step_avg:60.56ms
step:272/2245 train_time:16470ms step_avg:60.55ms
step:273/2245 train_time:16531ms step_avg:60.55ms
step:274/2245 train_time:16590ms step_avg:60.55ms
step:275/2245 train_time:16651ms step_avg:60.55ms
step:276/2245 train_time:16709ms step_avg:60.54ms
step:277/2245 train_time:16770ms step_avg:60.54ms
step:278/2245 train_time:16829ms step_avg:60.53ms
step:279/2245 train_time:16889ms step_avg:60.54ms
step:280/2245 train_time:16949ms step_avg:60.53ms
step:281/2245 train_time:17009ms step_avg:60.53ms
step:282/2245 train_time:17070ms step_avg:60.53ms
step:283/2245 train_time:17132ms step_avg:60.54ms
step:284/2245 train_time:17192ms step_avg:60.54ms
step:285/2245 train_time:17254ms step_avg:60.54ms
step:286/2245 train_time:17313ms step_avg:60.54ms
step:287/2245 train_time:17375ms step_avg:60.54ms
step:288/2245 train_time:17434ms step_avg:60.53ms
step:289/2245 train_time:17495ms step_avg:60.54ms
step:290/2245 train_time:17553ms step_avg:60.53ms
step:291/2245 train_time:17614ms step_avg:60.53ms
step:292/2245 train_time:17672ms step_avg:60.52ms
step:293/2245 train_time:17734ms step_avg:60.53ms
step:294/2245 train_time:17793ms step_avg:60.52ms
step:295/2245 train_time:17854ms step_avg:60.52ms
step:296/2245 train_time:17913ms step_avg:60.52ms
step:297/2245 train_time:17974ms step_avg:60.52ms
step:298/2245 train_time:18033ms step_avg:60.51ms
step:299/2245 train_time:18094ms step_avg:60.51ms
step:300/2245 train_time:18152ms step_avg:60.51ms
step:301/2245 train_time:18214ms step_avg:60.51ms
step:302/2245 train_time:18274ms step_avg:60.51ms
step:303/2245 train_time:18336ms step_avg:60.52ms
step:304/2245 train_time:18396ms step_avg:60.51ms
step:305/2245 train_time:18457ms step_avg:60.52ms
step:306/2245 train_time:18516ms step_avg:60.51ms
step:307/2245 train_time:18577ms step_avg:60.51ms
step:308/2245 train_time:18636ms step_avg:60.51ms
step:309/2245 train_time:18697ms step_avg:60.51ms
step:310/2245 train_time:18756ms step_avg:60.50ms
step:311/2245 train_time:18818ms step_avg:60.51ms
step:312/2245 train_time:18876ms step_avg:60.50ms
step:313/2245 train_time:18938ms step_avg:60.50ms
step:314/2245 train_time:18996ms step_avg:60.50ms
step:315/2245 train_time:19058ms step_avg:60.50ms
step:316/2245 train_time:19117ms step_avg:60.50ms
step:317/2245 train_time:19178ms step_avg:60.50ms
step:318/2245 train_time:19237ms step_avg:60.50ms
step:319/2245 train_time:19299ms step_avg:60.50ms
step:320/2245 train_time:19358ms step_avg:60.49ms
step:321/2245 train_time:19419ms step_avg:60.50ms
step:322/2245 train_time:19478ms step_avg:60.49ms
step:323/2245 train_time:19539ms step_avg:60.49ms
step:324/2245 train_time:19597ms step_avg:60.49ms
step:325/2245 train_time:19659ms step_avg:60.49ms
step:326/2245 train_time:19718ms step_avg:60.48ms
step:327/2245 train_time:19779ms step_avg:60.49ms
step:328/2245 train_time:19838ms step_avg:60.48ms
step:329/2245 train_time:19900ms step_avg:60.49ms
step:330/2245 train_time:19959ms step_avg:60.48ms
step:331/2245 train_time:20020ms step_avg:60.48ms
step:332/2245 train_time:20079ms step_avg:60.48ms
step:333/2245 train_time:20141ms step_avg:60.48ms
step:334/2245 train_time:20201ms step_avg:60.48ms
step:335/2245 train_time:20262ms step_avg:60.48ms
step:336/2245 train_time:20321ms step_avg:60.48ms
step:337/2245 train_time:20382ms step_avg:60.48ms
step:338/2245 train_time:20441ms step_avg:60.48ms
step:339/2245 train_time:20502ms step_avg:60.48ms
step:340/2245 train_time:20561ms step_avg:60.47ms
step:341/2245 train_time:20622ms step_avg:60.47ms
step:342/2245 train_time:20681ms step_avg:60.47ms
step:343/2245 train_time:20742ms step_avg:60.47ms
step:344/2245 train_time:20801ms step_avg:60.47ms
step:345/2245 train_time:20862ms step_avg:60.47ms
step:346/2245 train_time:20921ms step_avg:60.47ms
step:347/2245 train_time:20983ms step_avg:60.47ms
step:348/2245 train_time:21041ms step_avg:60.46ms
step:349/2245 train_time:21102ms step_avg:60.47ms
step:350/2245 train_time:21161ms step_avg:60.46ms
step:351/2245 train_time:21223ms step_avg:60.46ms
step:352/2245 train_time:21282ms step_avg:60.46ms
step:353/2245 train_time:21343ms step_avg:60.46ms
step:354/2245 train_time:21402ms step_avg:60.46ms
step:355/2245 train_time:21463ms step_avg:60.46ms
step:356/2245 train_time:21522ms step_avg:60.46ms
step:357/2245 train_time:21584ms step_avg:60.46ms
step:358/2245 train_time:21643ms step_avg:60.45ms
step:359/2245 train_time:21705ms step_avg:60.46ms
step:360/2245 train_time:21764ms step_avg:60.46ms
step:361/2245 train_time:21826ms step_avg:60.46ms
step:362/2245 train_time:21885ms step_avg:60.46ms
step:363/2245 train_time:21947ms step_avg:60.46ms
step:364/2245 train_time:22006ms step_avg:60.46ms
step:365/2245 train_time:22068ms step_avg:60.46ms
step:366/2245 train_time:22128ms step_avg:60.46ms
step:367/2245 train_time:22189ms step_avg:60.46ms
step:368/2245 train_time:22249ms step_avg:60.46ms
step:369/2245 train_time:22311ms step_avg:60.46ms
step:370/2245 train_time:22370ms step_avg:60.46ms
step:371/2245 train_time:22431ms step_avg:60.46ms
step:372/2245 train_time:22490ms step_avg:60.46ms
step:373/2245 train_time:22551ms step_avg:60.46ms
step:374/2245 train_time:22610ms step_avg:60.45ms
step:375/2245 train_time:22671ms step_avg:60.46ms
step:376/2245 train_time:22732ms step_avg:60.46ms
step:377/2245 train_time:22793ms step_avg:60.46ms
step:378/2245 train_time:22852ms step_avg:60.46ms
step:379/2245 train_time:22913ms step_avg:60.46ms
step:380/2245 train_time:22972ms step_avg:60.45ms
step:381/2245 train_time:23034ms step_avg:60.46ms
step:382/2245 train_time:23093ms step_avg:60.45ms
step:383/2245 train_time:23154ms step_avg:60.45ms
step:384/2245 train_time:23212ms step_avg:60.45ms
step:385/2245 train_time:23274ms step_avg:60.45ms
step:386/2245 train_time:23333ms step_avg:60.45ms
step:387/2245 train_time:23394ms step_avg:60.45ms
step:388/2245 train_time:23453ms step_avg:60.44ms
step:389/2245 train_time:23513ms step_avg:60.45ms
step:390/2245 train_time:23572ms step_avg:60.44ms
step:391/2245 train_time:23633ms step_avg:60.44ms
step:392/2245 train_time:23692ms step_avg:60.44ms
step:393/2245 train_time:23754ms step_avg:60.44ms
step:394/2245 train_time:23813ms step_avg:60.44ms
step:395/2245 train_time:23875ms step_avg:60.44ms
step:396/2245 train_time:23934ms step_avg:60.44ms
step:397/2245 train_time:23995ms step_avg:60.44ms
step:398/2245 train_time:24054ms step_avg:60.44ms
step:399/2245 train_time:24115ms step_avg:60.44ms
step:400/2245 train_time:24173ms step_avg:60.43ms
step:401/2245 train_time:24234ms step_avg:60.43ms
step:402/2245 train_time:24293ms step_avg:60.43ms
step:403/2245 train_time:24354ms step_avg:60.43ms
step:404/2245 train_time:24413ms step_avg:60.43ms
step:405/2245 train_time:24475ms step_avg:60.43ms
step:406/2245 train_time:24534ms step_avg:60.43ms
step:407/2245 train_time:24595ms step_avg:60.43ms
step:408/2245 train_time:24653ms step_avg:60.42ms
step:409/2245 train_time:24715ms step_avg:60.43ms
step:410/2245 train_time:24774ms step_avg:60.42ms
step:411/2245 train_time:24835ms step_avg:60.43ms
step:412/2245 train_time:24894ms step_avg:60.42ms
step:413/2245 train_time:24956ms step_avg:60.43ms
step:414/2245 train_time:25014ms step_avg:60.42ms
step:415/2245 train_time:25075ms step_avg:60.42ms
step:416/2245 train_time:25134ms step_avg:60.42ms
step:417/2245 train_time:25196ms step_avg:60.42ms
step:418/2245 train_time:25255ms step_avg:60.42ms
step:419/2245 train_time:25317ms step_avg:60.42ms
step:420/2245 train_time:25376ms step_avg:60.42ms
step:421/2245 train_time:25437ms step_avg:60.42ms
step:422/2245 train_time:25496ms step_avg:60.42ms
step:423/2245 train_time:25558ms step_avg:60.42ms
step:424/2245 train_time:25617ms step_avg:60.42ms
step:425/2245 train_time:25679ms step_avg:60.42ms
step:426/2245 train_time:25738ms step_avg:60.42ms
step:427/2245 train_time:25799ms step_avg:60.42ms
step:428/2245 train_time:25858ms step_avg:60.42ms
step:429/2245 train_time:25919ms step_avg:60.42ms
step:430/2245 train_time:25978ms step_avg:60.41ms
step:431/2245 train_time:26040ms step_avg:60.42ms
step:432/2245 train_time:26098ms step_avg:60.41ms
step:433/2245 train_time:26160ms step_avg:60.42ms
step:434/2245 train_time:26219ms step_avg:60.41ms
step:435/2245 train_time:26280ms step_avg:60.41ms
step:436/2245 train_time:26339ms step_avg:60.41ms
step:437/2245 train_time:26400ms step_avg:60.41ms
step:438/2245 train_time:26459ms step_avg:60.41ms
step:439/2245 train_time:26521ms step_avg:60.41ms
step:440/2245 train_time:26580ms step_avg:60.41ms
step:441/2245 train_time:26641ms step_avg:60.41ms
step:442/2245 train_time:26700ms step_avg:60.41ms
step:443/2245 train_time:26763ms step_avg:60.41ms
step:444/2245 train_time:26822ms step_avg:60.41ms
step:445/2245 train_time:26883ms step_avg:60.41ms
step:446/2245 train_time:26942ms step_avg:60.41ms
step:447/2245 train_time:27004ms step_avg:60.41ms
step:448/2245 train_time:27062ms step_avg:60.41ms
step:449/2245 train_time:27124ms step_avg:60.41ms
step:450/2245 train_time:27183ms step_avg:60.41ms
step:451/2245 train_time:27245ms step_avg:60.41ms
step:452/2245 train_time:27304ms step_avg:60.41ms
step:453/2245 train_time:27366ms step_avg:60.41ms
step:454/2245 train_time:27426ms step_avg:60.41ms
step:455/2245 train_time:27487ms step_avg:60.41ms
step:456/2245 train_time:27547ms step_avg:60.41ms
step:457/2245 train_time:27609ms step_avg:60.41ms
step:458/2245 train_time:27668ms step_avg:60.41ms
step:459/2245 train_time:27730ms step_avg:60.41ms
step:460/2245 train_time:27789ms step_avg:60.41ms
step:461/2245 train_time:27850ms step_avg:60.41ms
step:462/2245 train_time:27910ms step_avg:60.41ms
step:463/2245 train_time:27971ms step_avg:60.41ms
step:464/2245 train_time:28030ms step_avg:60.41ms
step:465/2245 train_time:28091ms step_avg:60.41ms
step:466/2245 train_time:28150ms step_avg:60.41ms
step:467/2245 train_time:28211ms step_avg:60.41ms
step:468/2245 train_time:28271ms step_avg:60.41ms
step:469/2245 train_time:28333ms step_avg:60.41ms
step:470/2245 train_time:28392ms step_avg:60.41ms
step:471/2245 train_time:28454ms step_avg:60.41ms
step:472/2245 train_time:28513ms step_avg:60.41ms
step:473/2245 train_time:28575ms step_avg:60.41ms
step:474/2245 train_time:28634ms step_avg:60.41ms
step:475/2245 train_time:28695ms step_avg:60.41ms
step:476/2245 train_time:28753ms step_avg:60.41ms
step:477/2245 train_time:28815ms step_avg:60.41ms
step:478/2245 train_time:28873ms step_avg:60.40ms
step:479/2245 train_time:28935ms step_avg:60.41ms
step:480/2245 train_time:28993ms step_avg:60.40ms
step:481/2245 train_time:29054ms step_avg:60.40ms
step:482/2245 train_time:29112ms step_avg:60.40ms
step:483/2245 train_time:29174ms step_avg:60.40ms
step:484/2245 train_time:29233ms step_avg:60.40ms
step:485/2245 train_time:29295ms step_avg:60.40ms
step:486/2245 train_time:29354ms step_avg:60.40ms
step:487/2245 train_time:29416ms step_avg:60.40ms
step:488/2245 train_time:29475ms step_avg:60.40ms
step:489/2245 train_time:29536ms step_avg:60.40ms
step:490/2245 train_time:29595ms step_avg:60.40ms
step:491/2245 train_time:29656ms step_avg:60.40ms
step:492/2245 train_time:29715ms step_avg:60.40ms
step:493/2245 train_time:29776ms step_avg:60.40ms
step:494/2245 train_time:29835ms step_avg:60.39ms
step:495/2245 train_time:29896ms step_avg:60.40ms
step:496/2245 train_time:29955ms step_avg:60.39ms
step:497/2245 train_time:30016ms step_avg:60.39ms
step:498/2245 train_time:30075ms step_avg:60.39ms
step:499/2245 train_time:30136ms step_avg:60.39ms
step:500/2245 train_time:30195ms step_avg:60.39ms
step:500/2245 val_loss:3.8156 train_time:30258ms step_avg:60.52ms
step:501/2245 train_time:30277ms step_avg:60.43ms
step:502/2245 train_time:30321ms step_avg:60.40ms
step:503/2245 train_time:30385ms step_avg:60.41ms
step:504/2245 train_time:30446ms step_avg:60.41ms
step:505/2245 train_time:30508ms step_avg:60.41ms
step:506/2245 train_time:30567ms step_avg:60.41ms
step:507/2245 train_time:30629ms step_avg:60.41ms
step:508/2245 train_time:30687ms step_avg:60.41ms
step:509/2245 train_time:30748ms step_avg:60.41ms
step:510/2245 train_time:30806ms step_avg:60.40ms
step:511/2245 train_time:30867ms step_avg:60.40ms
step:512/2245 train_time:30925ms step_avg:60.40ms
step:513/2245 train_time:30986ms step_avg:60.40ms
step:514/2245 train_time:31045ms step_avg:60.40ms
step:515/2245 train_time:31106ms step_avg:60.40ms
step:516/2245 train_time:31164ms step_avg:60.40ms
step:517/2245 train_time:31227ms step_avg:60.40ms
step:518/2245 train_time:31288ms step_avg:60.40ms
step:519/2245 train_time:31351ms step_avg:60.41ms
step:520/2245 train_time:31410ms step_avg:60.40ms
step:521/2245 train_time:31472ms step_avg:60.41ms
step:522/2245 train_time:31531ms step_avg:60.41ms
step:523/2245 train_time:31594ms step_avg:60.41ms
step:524/2245 train_time:31652ms step_avg:60.41ms
step:525/2245 train_time:31714ms step_avg:60.41ms
step:526/2245 train_time:31772ms step_avg:60.40ms
step:527/2245 train_time:31834ms step_avg:60.41ms
step:528/2245 train_time:31892ms step_avg:60.40ms
step:529/2245 train_time:31955ms step_avg:60.41ms
step:530/2245 train_time:32013ms step_avg:60.40ms
step:531/2245 train_time:32075ms step_avg:60.40ms
step:532/2245 train_time:32134ms step_avg:60.40ms
step:533/2245 train_time:32197ms step_avg:60.41ms
step:534/2245 train_time:32256ms step_avg:60.40ms
step:535/2245 train_time:32318ms step_avg:60.41ms
step:536/2245 train_time:32377ms step_avg:60.40ms
step:537/2245 train_time:32439ms step_avg:60.41ms
step:538/2245 train_time:32499ms step_avg:60.41ms
step:539/2245 train_time:32561ms step_avg:60.41ms
step:540/2245 train_time:32621ms step_avg:60.41ms
step:541/2245 train_time:32683ms step_avg:60.41ms
step:542/2245 train_time:32743ms step_avg:60.41ms
step:543/2245 train_time:32804ms step_avg:60.41ms
step:544/2245 train_time:32863ms step_avg:60.41ms
step:545/2245 train_time:32925ms step_avg:60.41ms
step:546/2245 train_time:32984ms step_avg:60.41ms
step:547/2245 train_time:33045ms step_avg:60.41ms
step:548/2245 train_time:33104ms step_avg:60.41ms
step:549/2245 train_time:33166ms step_avg:60.41ms
step:550/2245 train_time:33226ms step_avg:60.41ms
step:551/2245 train_time:33288ms step_avg:60.41ms
step:552/2245 train_time:33346ms step_avg:60.41ms
step:553/2245 train_time:33408ms step_avg:60.41ms
step:554/2245 train_time:33467ms step_avg:60.41ms
step:555/2245 train_time:33529ms step_avg:60.41ms
step:556/2245 train_time:33588ms step_avg:60.41ms
step:557/2245 train_time:33651ms step_avg:60.42ms
step:558/2245 train_time:33710ms step_avg:60.41ms
step:559/2245 train_time:33771ms step_avg:60.41ms
step:560/2245 train_time:33829ms step_avg:60.41ms
step:561/2245 train_time:33891ms step_avg:60.41ms
step:562/2245 train_time:33949ms step_avg:60.41ms
step:563/2245 train_time:34011ms step_avg:60.41ms
step:564/2245 train_time:34070ms step_avg:60.41ms
step:565/2245 train_time:34132ms step_avg:60.41ms
step:566/2245 train_time:34191ms step_avg:60.41ms
step:567/2245 train_time:34253ms step_avg:60.41ms
step:568/2245 train_time:34312ms step_avg:60.41ms
step:569/2245 train_time:34373ms step_avg:60.41ms
step:570/2245 train_time:34432ms step_avg:60.41ms
step:571/2245 train_time:34494ms step_avg:60.41ms
step:572/2245 train_time:34553ms step_avg:60.41ms
step:573/2245 train_time:34615ms step_avg:60.41ms
step:574/2245 train_time:34675ms step_avg:60.41ms
step:575/2245 train_time:34735ms step_avg:60.41ms
step:576/2245 train_time:34794ms step_avg:60.41ms
step:577/2245 train_time:34855ms step_avg:60.41ms
step:578/2245 train_time:34914ms step_avg:60.41ms
step:579/2245 train_time:34976ms step_avg:60.41ms
step:580/2245 train_time:35034ms step_avg:60.40ms
step:581/2245 train_time:35096ms step_avg:60.41ms
step:582/2245 train_time:35155ms step_avg:60.40ms
step:583/2245 train_time:35217ms step_avg:60.41ms
step:584/2245 train_time:35276ms step_avg:60.40ms
step:585/2245 train_time:35338ms step_avg:60.41ms
step:586/2245 train_time:35397ms step_avg:60.40ms
step:587/2245 train_time:35459ms step_avg:60.41ms
step:588/2245 train_time:35519ms step_avg:60.41ms
step:589/2245 train_time:35581ms step_avg:60.41ms
step:590/2245 train_time:35641ms step_avg:60.41ms
step:591/2245 train_time:35702ms step_avg:60.41ms
step:592/2245 train_time:35762ms step_avg:60.41ms
step:593/2245 train_time:35824ms step_avg:60.41ms
step:594/2245 train_time:35882ms step_avg:60.41ms
step:595/2245 train_time:35944ms step_avg:60.41ms
step:596/2245 train_time:36004ms step_avg:60.41ms
step:597/2245 train_time:36066ms step_avg:60.41ms
step:598/2245 train_time:36125ms step_avg:60.41ms
step:599/2245 train_time:36187ms step_avg:60.41ms
step:600/2245 train_time:36246ms step_avg:60.41ms
step:601/2245 train_time:36307ms step_avg:60.41ms
step:602/2245 train_time:36366ms step_avg:60.41ms
step:603/2245 train_time:36428ms step_avg:60.41ms
step:604/2245 train_time:36488ms step_avg:60.41ms
step:605/2245 train_time:36550ms step_avg:60.41ms
step:606/2245 train_time:36609ms step_avg:60.41ms
step:607/2245 train_time:36671ms step_avg:60.41ms
step:608/2245 train_time:36729ms step_avg:60.41ms
step:609/2245 train_time:36792ms step_avg:60.41ms
step:610/2245 train_time:36851ms step_avg:60.41ms
step:611/2245 train_time:36912ms step_avg:60.41ms
step:612/2245 train_time:36972ms step_avg:60.41ms
step:613/2245 train_time:37034ms step_avg:60.41ms
step:614/2245 train_time:37093ms step_avg:60.41ms
step:615/2245 train_time:37155ms step_avg:60.41ms
step:616/2245 train_time:37214ms step_avg:60.41ms
step:617/2245 train_time:37276ms step_avg:60.42ms
step:618/2245 train_time:37335ms step_avg:60.41ms
step:619/2245 train_time:37397ms step_avg:60.42ms
step:620/2245 train_time:37456ms step_avg:60.41ms
step:621/2245 train_time:37518ms step_avg:60.42ms
step:622/2245 train_time:37577ms step_avg:60.41ms
step:623/2245 train_time:37639ms step_avg:60.42ms
step:624/2245 train_time:37698ms step_avg:60.41ms
step:625/2245 train_time:37760ms step_avg:60.42ms
step:626/2245 train_time:37819ms step_avg:60.41ms
step:627/2245 train_time:37882ms step_avg:60.42ms
step:628/2245 train_time:37941ms step_avg:60.42ms
step:629/2245 train_time:38003ms step_avg:60.42ms
step:630/2245 train_time:38062ms step_avg:60.42ms
step:631/2245 train_time:38124ms step_avg:60.42ms
step:632/2245 train_time:38184ms step_avg:60.42ms
step:633/2245 train_time:38245ms step_avg:60.42ms
step:634/2245 train_time:38305ms step_avg:60.42ms
step:635/2245 train_time:38366ms step_avg:60.42ms
step:636/2245 train_time:38425ms step_avg:60.42ms
step:637/2245 train_time:38487ms step_avg:60.42ms
step:638/2245 train_time:38546ms step_avg:60.42ms
step:639/2245 train_time:38607ms step_avg:60.42ms
step:640/2245 train_time:38667ms step_avg:60.42ms
step:641/2245 train_time:38728ms step_avg:60.42ms
step:642/2245 train_time:38788ms step_avg:60.42ms
step:643/2245 train_time:38850ms step_avg:60.42ms
step:644/2245 train_time:38909ms step_avg:60.42ms
step:645/2245 train_time:38971ms step_avg:60.42ms
step:646/2245 train_time:39030ms step_avg:60.42ms
step:647/2245 train_time:39091ms step_avg:60.42ms
step:648/2245 train_time:39150ms step_avg:60.42ms
step:649/2245 train_time:39211ms step_avg:60.42ms
step:650/2245 train_time:39271ms step_avg:60.42ms
step:651/2245 train_time:39333ms step_avg:60.42ms
step:652/2245 train_time:39392ms step_avg:60.42ms
step:653/2245 train_time:39453ms step_avg:60.42ms
step:654/2245 train_time:39512ms step_avg:60.42ms
step:655/2245 train_time:39573ms step_avg:60.42ms
step:656/2245 train_time:39632ms step_avg:60.41ms
step:657/2245 train_time:39693ms step_avg:60.42ms
step:658/2245 train_time:39752ms step_avg:60.41ms
step:659/2245 train_time:39814ms step_avg:60.42ms
step:660/2245 train_time:39873ms step_avg:60.41ms
step:661/2245 train_time:39935ms step_avg:60.42ms
step:662/2245 train_time:39994ms step_avg:60.41ms
step:663/2245 train_time:40055ms step_avg:60.42ms
step:664/2245 train_time:40114ms step_avg:60.41ms
step:665/2245 train_time:40175ms step_avg:60.41ms
step:666/2245 train_time:40234ms step_avg:60.41ms
step:667/2245 train_time:40296ms step_avg:60.41ms
step:668/2245 train_time:40355ms step_avg:60.41ms
step:669/2245 train_time:40417ms step_avg:60.41ms
step:670/2245 train_time:40476ms step_avg:60.41ms
step:671/2245 train_time:40538ms step_avg:60.41ms
step:672/2245 train_time:40597ms step_avg:60.41ms
step:673/2245 train_time:40659ms step_avg:60.41ms
step:674/2245 train_time:40718ms step_avg:60.41ms
step:675/2245 train_time:40780ms step_avg:60.41ms
step:676/2245 train_time:40839ms step_avg:60.41ms
step:677/2245 train_time:40900ms step_avg:60.41ms
step:678/2245 train_time:40960ms step_avg:60.41ms
step:679/2245 train_time:41022ms step_avg:60.41ms
step:680/2245 train_time:41081ms step_avg:60.41ms
step:681/2245 train_time:41143ms step_avg:60.42ms
step:682/2245 train_time:41203ms step_avg:60.41ms
step:683/2245 train_time:41265ms step_avg:60.42ms
step:684/2245 train_time:41325ms step_avg:60.42ms
step:685/2245 train_time:41387ms step_avg:60.42ms
step:686/2245 train_time:41445ms step_avg:60.42ms
step:687/2245 train_time:41507ms step_avg:60.42ms
step:688/2245 train_time:41566ms step_avg:60.42ms
step:689/2245 train_time:41627ms step_avg:60.42ms
step:690/2245 train_time:41686ms step_avg:60.41ms
step:691/2245 train_time:41748ms step_avg:60.42ms
step:692/2245 train_time:41806ms step_avg:60.41ms
step:693/2245 train_time:41868ms step_avg:60.41ms
step:694/2245 train_time:41927ms step_avg:60.41ms
step:695/2245 train_time:41989ms step_avg:60.42ms
step:696/2245 train_time:42048ms step_avg:60.41ms
step:697/2245 train_time:42110ms step_avg:60.42ms
step:698/2245 train_time:42168ms step_avg:60.41ms
step:699/2245 train_time:42229ms step_avg:60.41ms
step:700/2245 train_time:42288ms step_avg:60.41ms
step:701/2245 train_time:42349ms step_avg:60.41ms
step:702/2245 train_time:42408ms step_avg:60.41ms
step:703/2245 train_time:42469ms step_avg:60.41ms
step:704/2245 train_time:42528ms step_avg:60.41ms
step:705/2245 train_time:42589ms step_avg:60.41ms
step:706/2245 train_time:42649ms step_avg:60.41ms
step:707/2245 train_time:42710ms step_avg:60.41ms
step:708/2245 train_time:42769ms step_avg:60.41ms
step:709/2245 train_time:42830ms step_avg:60.41ms
step:710/2245 train_time:42889ms step_avg:60.41ms
step:711/2245 train_time:42951ms step_avg:60.41ms
step:712/2245 train_time:43009ms step_avg:60.41ms
step:713/2245 train_time:43071ms step_avg:60.41ms
step:714/2245 train_time:43130ms step_avg:60.41ms
step:715/2245 train_time:43191ms step_avg:60.41ms
step:716/2245 train_time:43250ms step_avg:60.41ms
step:717/2245 train_time:43312ms step_avg:60.41ms
step:718/2245 train_time:43371ms step_avg:60.40ms
step:719/2245 train_time:43433ms step_avg:60.41ms
step:720/2245 train_time:43876ms step_avg:60.94ms
step:721/2245 train_time:43935ms step_avg:60.94ms
step:722/2245 train_time:43993ms step_avg:60.93ms
step:723/2245 train_time:44054ms step_avg:60.93ms
step:724/2245 train_time:44112ms step_avg:60.93ms
step:725/2245 train_time:44172ms step_avg:60.93ms
step:726/2245 train_time:44231ms step_avg:60.92ms
step:727/2245 train_time:44291ms step_avg:60.92ms
step:728/2245 train_time:44350ms step_avg:60.92ms
step:729/2245 train_time:44410ms step_avg:60.92ms
step:730/2245 train_time:44468ms step_avg:60.92ms
step:731/2245 train_time:44529ms step_avg:60.91ms
step:732/2245 train_time:44587ms step_avg:60.91ms
step:733/2245 train_time:44648ms step_avg:60.91ms
step:734/2245 train_time:44707ms step_avg:60.91ms
step:735/2245 train_time:44775ms step_avg:60.92ms
step:736/2245 train_time:44837ms step_avg:60.92ms
step:737/2245 train_time:44901ms step_avg:60.92ms
step:738/2245 train_time:44962ms step_avg:60.92ms
step:739/2245 train_time:45023ms step_avg:60.92ms
step:740/2245 train_time:45083ms step_avg:60.92ms
step:741/2245 train_time:45145ms step_avg:60.92ms
step:742/2245 train_time:45205ms step_avg:60.92ms
step:743/2245 train_time:45267ms step_avg:60.92ms
step:744/2245 train_time:45327ms step_avg:60.92ms
step:745/2245 train_time:45389ms step_avg:60.92ms
step:746/2245 train_time:45448ms step_avg:60.92ms
step:747/2245 train_time:45509ms step_avg:60.92ms
step:748/2245 train_time:45569ms step_avg:60.92ms
step:749/2245 train_time:45630ms step_avg:60.92ms
step:750/2245 train_time:45690ms step_avg:60.92ms
step:750/2245 val_loss:3.6674 train_time:45755ms step_avg:61.01ms
step:751/2245 train_time:45773ms step_avg:60.95ms
step:752/2245 train_time:45817ms step_avg:60.93ms
step:753/2245 train_time:45878ms step_avg:60.93ms
step:754/2245 train_time:45938ms step_avg:60.93ms
step:755/2245 train_time:46001ms step_avg:60.93ms
step:756/2245 train_time:46061ms step_avg:60.93ms
step:757/2245 train_time:46123ms step_avg:60.93ms
step:758/2245 train_time:46181ms step_avg:60.93ms
step:759/2245 train_time:46242ms step_avg:60.93ms
step:760/2245 train_time:46302ms step_avg:60.92ms
step:761/2245 train_time:46363ms step_avg:60.92ms
step:762/2245 train_time:46422ms step_avg:60.92ms
step:763/2245 train_time:46483ms step_avg:60.92ms
step:764/2245 train_time:46542ms step_avg:60.92ms
step:765/2245 train_time:46604ms step_avg:60.92ms
step:766/2245 train_time:46670ms step_avg:60.93ms
step:767/2245 train_time:46737ms step_avg:60.93ms
step:768/2245 train_time:46798ms step_avg:60.93ms
step:769/2245 train_time:46860ms step_avg:60.94ms
step:770/2245 train_time:46921ms step_avg:60.94ms
step:771/2245 train_time:46983ms step_avg:60.94ms
step:772/2245 train_time:47042ms step_avg:60.94ms
step:773/2245 train_time:47104ms step_avg:60.94ms
step:774/2245 train_time:47164ms step_avg:60.94ms
step:775/2245 train_time:47225ms step_avg:60.94ms
step:776/2245 train_time:47284ms step_avg:60.93ms
step:777/2245 train_time:47345ms step_avg:60.93ms
step:778/2245 train_time:47405ms step_avg:60.93ms
step:779/2245 train_time:47466ms step_avg:60.93ms
step:780/2245 train_time:47525ms step_avg:60.93ms
step:781/2245 train_time:47588ms step_avg:60.93ms
step:782/2245 train_time:47650ms step_avg:60.93ms
step:783/2245 train_time:47714ms step_avg:60.94ms
step:784/2245 train_time:47775ms step_avg:60.94ms
step:785/2245 train_time:47838ms step_avg:60.94ms
step:786/2245 train_time:47898ms step_avg:60.94ms
step:787/2245 train_time:47961ms step_avg:60.94ms
step:788/2245 train_time:48021ms step_avg:60.94ms
step:789/2245 train_time:48083ms step_avg:60.94ms
step:790/2245 train_time:48143ms step_avg:60.94ms
step:791/2245 train_time:48205ms step_avg:60.94ms
step:792/2245 train_time:48265ms step_avg:60.94ms
step:793/2245 train_time:48326ms step_avg:60.94ms
step:794/2245 train_time:48386ms step_avg:60.94ms
step:795/2245 train_time:48448ms step_avg:60.94ms
step:796/2245 train_time:48508ms step_avg:60.94ms
step:797/2245 train_time:48570ms step_avg:60.94ms
step:798/2245 train_time:48630ms step_avg:60.94ms
step:799/2245 train_time:48693ms step_avg:60.94ms
step:800/2245 train_time:48754ms step_avg:60.94ms
step:801/2245 train_time:48817ms step_avg:60.94ms
step:802/2245 train_time:48877ms step_avg:60.94ms
step:803/2245 train_time:48939ms step_avg:60.95ms
step:804/2245 train_time:48999ms step_avg:60.94ms
step:805/2245 train_time:49061ms step_avg:60.95ms
step:806/2245 train_time:49122ms step_avg:60.95ms
step:807/2245 train_time:49184ms step_avg:60.95ms
step:808/2245 train_time:49243ms step_avg:60.94ms
step:809/2245 train_time:49305ms step_avg:60.95ms
step:810/2245 train_time:49365ms step_avg:60.94ms
step:811/2245 train_time:49428ms step_avg:60.95ms
step:812/2245 train_time:49487ms step_avg:60.94ms
step:813/2245 train_time:49550ms step_avg:60.95ms
step:814/2245 train_time:49610ms step_avg:60.95ms
step:815/2245 train_time:49673ms step_avg:60.95ms
step:816/2245 train_time:49733ms step_avg:60.95ms
step:817/2245 train_time:49796ms step_avg:60.95ms
step:818/2245 train_time:49855ms step_avg:60.95ms
step:819/2245 train_time:49917ms step_avg:60.95ms
step:820/2245 train_time:49977ms step_avg:60.95ms
step:821/2245 train_time:50039ms step_avg:60.95ms
step:822/2245 train_time:50099ms step_avg:60.95ms
step:823/2245 train_time:50161ms step_avg:60.95ms
step:824/2245 train_time:50221ms step_avg:60.95ms
step:825/2245 train_time:50283ms step_avg:60.95ms
step:826/2245 train_time:50343ms step_avg:60.95ms
step:827/2245 train_time:50406ms step_avg:60.95ms
step:828/2245 train_time:50466ms step_avg:60.95ms
step:829/2245 train_time:50529ms step_avg:60.95ms
step:830/2245 train_time:50590ms step_avg:60.95ms
step:831/2245 train_time:50651ms step_avg:60.95ms
step:832/2245 train_time:50711ms step_avg:60.95ms
step:833/2245 train_time:50774ms step_avg:60.95ms
step:834/2245 train_time:50834ms step_avg:60.95ms
step:835/2245 train_time:50896ms step_avg:60.95ms
step:836/2245 train_time:50956ms step_avg:60.95ms
step:837/2245 train_time:51018ms step_avg:60.95ms
step:838/2245 train_time:51077ms step_avg:60.95ms
step:839/2245 train_time:51140ms step_avg:60.95ms
step:840/2245 train_time:51200ms step_avg:60.95ms
step:841/2245 train_time:51262ms step_avg:60.95ms
step:842/2245 train_time:51322ms step_avg:60.95ms
step:843/2245 train_time:51384ms step_avg:60.95ms
step:844/2245 train_time:51444ms step_avg:60.95ms
step:845/2245 train_time:51507ms step_avg:60.96ms
step:846/2245 train_time:51568ms step_avg:60.96ms
step:847/2245 train_time:51631ms step_avg:60.96ms
step:848/2245 train_time:51691ms step_avg:60.96ms
step:849/2245 train_time:51753ms step_avg:60.96ms
step:850/2245 train_time:51813ms step_avg:60.96ms
step:851/2245 train_time:51875ms step_avg:60.96ms
step:852/2245 train_time:51936ms step_avg:60.96ms
step:853/2245 train_time:51997ms step_avg:60.96ms
step:854/2245 train_time:52057ms step_avg:60.96ms
step:855/2245 train_time:52119ms step_avg:60.96ms
step:856/2245 train_time:52178ms step_avg:60.96ms
step:857/2245 train_time:52241ms step_avg:60.96ms
step:858/2245 train_time:52300ms step_avg:60.96ms
step:859/2245 train_time:52363ms step_avg:60.96ms
step:860/2245 train_time:52423ms step_avg:60.96ms
step:861/2245 train_time:52486ms step_avg:60.96ms
step:862/2245 train_time:52547ms step_avg:60.96ms
step:863/2245 train_time:52610ms step_avg:60.96ms
step:864/2245 train_time:52670ms step_avg:60.96ms
step:865/2245 train_time:52733ms step_avg:60.96ms
step:866/2245 train_time:52792ms step_avg:60.96ms
step:867/2245 train_time:52855ms step_avg:60.96ms
step:868/2245 train_time:52915ms step_avg:60.96ms
step:869/2245 train_time:52976ms step_avg:60.96ms
step:870/2245 train_time:53036ms step_avg:60.96ms
step:871/2245 train_time:53098ms step_avg:60.96ms
step:872/2245 train_time:53157ms step_avg:60.96ms
step:873/2245 train_time:53220ms step_avg:60.96ms
step:874/2245 train_time:53280ms step_avg:60.96ms
step:875/2245 train_time:53343ms step_avg:60.96ms
step:876/2245 train_time:53402ms step_avg:60.96ms
step:877/2245 train_time:53465ms step_avg:60.96ms
step:878/2245 train_time:53525ms step_avg:60.96ms
step:879/2245 train_time:53587ms step_avg:60.96ms
step:880/2245 train_time:53647ms step_avg:60.96ms
step:881/2245 train_time:53711ms step_avg:60.97ms
step:882/2245 train_time:53771ms step_avg:60.96ms
step:883/2245 train_time:53833ms step_avg:60.97ms
step:884/2245 train_time:53893ms step_avg:60.97ms
step:885/2245 train_time:53955ms step_avg:60.97ms
step:886/2245 train_time:54016ms step_avg:60.97ms
step:887/2245 train_time:54078ms step_avg:60.97ms
step:888/2245 train_time:54138ms step_avg:60.97ms
step:889/2245 train_time:54200ms step_avg:60.97ms
step:890/2245 train_time:54260ms step_avg:60.97ms
step:891/2245 train_time:54322ms step_avg:60.97ms
step:892/2245 train_time:54382ms step_avg:60.97ms
step:893/2245 train_time:54444ms step_avg:60.97ms
step:894/2245 train_time:54505ms step_avg:60.97ms
step:895/2245 train_time:54568ms step_avg:60.97ms
step:896/2245 train_time:54628ms step_avg:60.97ms
step:897/2245 train_time:54691ms step_avg:60.97ms
step:898/2245 train_time:54751ms step_avg:60.97ms
step:899/2245 train_time:54813ms step_avg:60.97ms
step:900/2245 train_time:54873ms step_avg:60.97ms
step:901/2245 train_time:54935ms step_avg:60.97ms
step:902/2245 train_time:54994ms step_avg:60.97ms
step:903/2245 train_time:55056ms step_avg:60.97ms
step:904/2245 train_time:55116ms step_avg:60.97ms
step:905/2245 train_time:55178ms step_avg:60.97ms
step:906/2245 train_time:55238ms step_avg:60.97ms
step:907/2245 train_time:55300ms step_avg:60.97ms
step:908/2245 train_time:55360ms step_avg:60.97ms
step:909/2245 train_time:55423ms step_avg:60.97ms
step:910/2245 train_time:55482ms step_avg:60.97ms
step:911/2245 train_time:55545ms step_avg:60.97ms
step:912/2245 train_time:55606ms step_avg:60.97ms
step:913/2245 train_time:55669ms step_avg:60.97ms
step:914/2245 train_time:55729ms step_avg:60.97ms
step:915/2245 train_time:55792ms step_avg:60.98ms
step:916/2245 train_time:55852ms step_avg:60.97ms
step:917/2245 train_time:55915ms step_avg:60.98ms
step:918/2245 train_time:55975ms step_avg:60.97ms
step:919/2245 train_time:56037ms step_avg:60.98ms
step:920/2245 train_time:56096ms step_avg:60.97ms
step:921/2245 train_time:56158ms step_avg:60.97ms
step:922/2245 train_time:56217ms step_avg:60.97ms
step:923/2245 train_time:56280ms step_avg:60.97ms
step:924/2245 train_time:56339ms step_avg:60.97ms
step:925/2245 train_time:56402ms step_avg:60.97ms
step:926/2245 train_time:56461ms step_avg:60.97ms
step:927/2245 train_time:56524ms step_avg:60.97ms
step:928/2245 train_time:56583ms step_avg:60.97ms
step:929/2245 train_time:56646ms step_avg:60.98ms
step:930/2245 train_time:56706ms step_avg:60.97ms
step:931/2245 train_time:56769ms step_avg:60.98ms
step:932/2245 train_time:56829ms step_avg:60.98ms
step:933/2245 train_time:56892ms step_avg:60.98ms
step:934/2245 train_time:56952ms step_avg:60.98ms
step:935/2245 train_time:57014ms step_avg:60.98ms
step:936/2245 train_time:57074ms step_avg:60.98ms
step:937/2245 train_time:57136ms step_avg:60.98ms
step:938/2245 train_time:57195ms step_avg:60.98ms
step:939/2245 train_time:57257ms step_avg:60.98ms
step:940/2245 train_time:57317ms step_avg:60.98ms
step:941/2245 train_time:57379ms step_avg:60.98ms
step:942/2245 train_time:57439ms step_avg:60.98ms
step:943/2245 train_time:57502ms step_avg:60.98ms
step:944/2245 train_time:57562ms step_avg:60.98ms
step:945/2245 train_time:57625ms step_avg:60.98ms
step:946/2245 train_time:57685ms step_avg:60.98ms
step:947/2245 train_time:57749ms step_avg:60.98ms
step:948/2245 train_time:57810ms step_avg:60.98ms
step:949/2245 train_time:57873ms step_avg:60.98ms
step:950/2245 train_time:57932ms step_avg:60.98ms
step:951/2245 train_time:57994ms step_avg:60.98ms
step:952/2245 train_time:58054ms step_avg:60.98ms
step:953/2245 train_time:58116ms step_avg:60.98ms
step:954/2245 train_time:58176ms step_avg:60.98ms
step:955/2245 train_time:58237ms step_avg:60.98ms
step:956/2245 train_time:58297ms step_avg:60.98ms
step:957/2245 train_time:58358ms step_avg:60.98ms
step:958/2245 train_time:58418ms step_avg:60.98ms
step:959/2245 train_time:58480ms step_avg:60.98ms
step:960/2245 train_time:58540ms step_avg:60.98ms
step:961/2245 train_time:58603ms step_avg:60.98ms
step:962/2245 train_time:58663ms step_avg:60.98ms
step:963/2245 train_time:58726ms step_avg:60.98ms
step:964/2245 train_time:58785ms step_avg:60.98ms
step:965/2245 train_time:58849ms step_avg:60.98ms
step:966/2245 train_time:58909ms step_avg:60.98ms
step:967/2245 train_time:58972ms step_avg:60.98ms
step:968/2245 train_time:59032ms step_avg:60.98ms
step:969/2245 train_time:59095ms step_avg:60.99ms
step:970/2245 train_time:59155ms step_avg:60.98ms
step:971/2245 train_time:59217ms step_avg:60.99ms
step:972/2245 train_time:59276ms step_avg:60.98ms
step:973/2245 train_time:59338ms step_avg:60.98ms
step:974/2245 train_time:59398ms step_avg:60.98ms
step:975/2245 train_time:59459ms step_avg:60.98ms
step:976/2245 train_time:59519ms step_avg:60.98ms
step:977/2245 train_time:59582ms step_avg:60.98ms
step:978/2245 train_time:59642ms step_avg:60.98ms
step:979/2245 train_time:59704ms step_avg:60.98ms
step:980/2245 train_time:59765ms step_avg:60.99ms
step:981/2245 train_time:59829ms step_avg:60.99ms
step:982/2245 train_time:59889ms step_avg:60.99ms
step:983/2245 train_time:59951ms step_avg:60.99ms
step:984/2245 train_time:60011ms step_avg:60.99ms
step:985/2245 train_time:60074ms step_avg:60.99ms
step:986/2245 train_time:60134ms step_avg:60.99ms
step:987/2245 train_time:60195ms step_avg:60.99ms
step:988/2245 train_time:60255ms step_avg:60.99ms
step:989/2245 train_time:60317ms step_avg:60.99ms
step:990/2245 train_time:60377ms step_avg:60.99ms
step:991/2245 train_time:60438ms step_avg:60.99ms
step:992/2245 train_time:60499ms step_avg:60.99ms
step:993/2245 train_time:60561ms step_avg:60.99ms
step:994/2245 train_time:60621ms step_avg:60.99ms
step:995/2245 train_time:60684ms step_avg:60.99ms
step:996/2245 train_time:60743ms step_avg:60.99ms
step:997/2245 train_time:60806ms step_avg:60.99ms
step:998/2245 train_time:60867ms step_avg:60.99ms
step:999/2245 train_time:60931ms step_avg:60.99ms
step:1000/2245 train_time:60991ms step_avg:60.99ms
step:1000/2245 val_loss:3.5961 train_time:61054ms step_avg:61.05ms
step:1001/2245 train_time:61073ms step_avg:61.01ms
step:1002/2245 train_time:61117ms step_avg:60.99ms
step:1003/2245 train_time:61183ms step_avg:61.00ms
step:1004/2245 train_time:61246ms step_avg:61.00ms
step:1005/2245 train_time:61309ms step_avg:61.00ms
step:1006/2245 train_time:61369ms step_avg:61.00ms
step:1007/2245 train_time:61430ms step_avg:61.00ms
step:1008/2245 train_time:61489ms step_avg:61.00ms
step:1009/2245 train_time:61551ms step_avg:61.00ms
step:1010/2245 train_time:61610ms step_avg:61.00ms
step:1011/2245 train_time:61671ms step_avg:61.00ms
step:1012/2245 train_time:61731ms step_avg:61.00ms
step:1013/2245 train_time:61793ms step_avg:61.00ms
step:1014/2245 train_time:61852ms step_avg:61.00ms
step:1015/2245 train_time:61914ms step_avg:61.00ms
step:1016/2245 train_time:61975ms step_avg:61.00ms
step:1017/2245 train_time:62039ms step_avg:61.00ms
step:1018/2245 train_time:62100ms step_avg:61.00ms
step:1019/2245 train_time:62164ms step_avg:61.00ms
step:1020/2245 train_time:62225ms step_avg:61.00ms
step:1021/2245 train_time:62289ms step_avg:61.01ms
step:1022/2245 train_time:62349ms step_avg:61.01ms
step:1023/2245 train_time:62411ms step_avg:61.01ms
step:1024/2245 train_time:62471ms step_avg:61.01ms
step:1025/2245 train_time:62533ms step_avg:61.01ms
step:1026/2245 train_time:62592ms step_avg:61.01ms
step:1027/2245 train_time:62654ms step_avg:61.01ms
step:1028/2245 train_time:62713ms step_avg:61.01ms
step:1029/2245 train_time:62775ms step_avg:61.01ms
step:1030/2245 train_time:62835ms step_avg:61.00ms
step:1031/2245 train_time:62896ms step_avg:61.01ms
step:1032/2245 train_time:62957ms step_avg:61.00ms
step:1033/2245 train_time:63020ms step_avg:61.01ms
step:1034/2245 train_time:63080ms step_avg:61.01ms
step:1035/2245 train_time:63144ms step_avg:61.01ms
step:1036/2245 train_time:63204ms step_avg:61.01ms
step:1037/2245 train_time:63268ms step_avg:61.01ms
step:1038/2245 train_time:63328ms step_avg:61.01ms
step:1039/2245 train_time:63391ms step_avg:61.01ms
step:1040/2245 train_time:63451ms step_avg:61.01ms
step:1041/2245 train_time:63513ms step_avg:61.01ms
step:1042/2245 train_time:63572ms step_avg:61.01ms
step:1043/2245 train_time:63634ms step_avg:61.01ms
step:1044/2245 train_time:63693ms step_avg:61.01ms
step:1045/2245 train_time:63755ms step_avg:61.01ms
step:1046/2245 train_time:63814ms step_avg:61.01ms
step:1047/2245 train_time:63876ms step_avg:61.01ms
step:1048/2245 train_time:63936ms step_avg:61.01ms
step:1049/2245 train_time:63999ms step_avg:61.01ms
step:1050/2245 train_time:64059ms step_avg:61.01ms
step:1051/2245 train_time:64122ms step_avg:61.01ms
step:1052/2245 train_time:64182ms step_avg:61.01ms
step:1053/2245 train_time:64245ms step_avg:61.01ms
step:1054/2245 train_time:64306ms step_avg:61.01ms
step:1055/2245 train_time:64369ms step_avg:61.01ms
step:1056/2245 train_time:64428ms step_avg:61.01ms
step:1057/2245 train_time:64491ms step_avg:61.01ms
step:1058/2245 train_time:64550ms step_avg:61.01ms
step:1059/2245 train_time:64612ms step_avg:61.01ms
step:1060/2245 train_time:64672ms step_avg:61.01ms
step:1061/2245 train_time:64734ms step_avg:61.01ms
step:1062/2245 train_time:64793ms step_avg:61.01ms
step:1063/2245 train_time:64856ms step_avg:61.01ms
step:1064/2245 train_time:64915ms step_avg:61.01ms
step:1065/2245 train_time:64978ms step_avg:61.01ms
step:1066/2245 train_time:65038ms step_avg:61.01ms
step:1067/2245 train_time:65101ms step_avg:61.01ms
step:1068/2245 train_time:65161ms step_avg:61.01ms
step:1069/2245 train_time:65224ms step_avg:61.01ms
step:1070/2245 train_time:65285ms step_avg:61.01ms
step:1071/2245 train_time:65348ms step_avg:61.02ms
step:1072/2245 train_time:65408ms step_avg:61.01ms
step:1073/2245 train_time:65471ms step_avg:61.02ms
step:1074/2245 train_time:65531ms step_avg:61.02ms
step:1075/2245 train_time:65593ms step_avg:61.02ms
step:1076/2245 train_time:65653ms step_avg:61.02ms
step:1077/2245 train_time:65715ms step_avg:61.02ms
step:1078/2245 train_time:65775ms step_avg:61.02ms
step:1079/2245 train_time:65837ms step_avg:61.02ms
step:1080/2245 train_time:65897ms step_avg:61.02ms
step:1081/2245 train_time:65960ms step_avg:61.02ms
step:1082/2245 train_time:66019ms step_avg:61.02ms
step:1083/2245 train_time:66082ms step_avg:61.02ms
step:1084/2245 train_time:66143ms step_avg:61.02ms
step:1085/2245 train_time:66206ms step_avg:61.02ms
step:1086/2245 train_time:66266ms step_avg:61.02ms
step:1087/2245 train_time:66328ms step_avg:61.02ms
step:1088/2245 train_time:66388ms step_avg:61.02ms
step:1089/2245 train_time:66451ms step_avg:61.02ms
step:1090/2245 train_time:66511ms step_avg:61.02ms
step:1091/2245 train_time:66574ms step_avg:61.02ms
step:1092/2245 train_time:66634ms step_avg:61.02ms
step:1093/2245 train_time:66696ms step_avg:61.02ms
step:1094/2245 train_time:66756ms step_avg:61.02ms
step:1095/2245 train_time:66818ms step_avg:61.02ms
step:1096/2245 train_time:66878ms step_avg:61.02ms
step:1097/2245 train_time:66940ms step_avg:61.02ms
step:1098/2245 train_time:67000ms step_avg:61.02ms
step:1099/2245 train_time:67063ms step_avg:61.02ms
step:1100/2245 train_time:67123ms step_avg:61.02ms
step:1101/2245 train_time:67187ms step_avg:61.02ms
step:1102/2245 train_time:67247ms step_avg:61.02ms
step:1103/2245 train_time:67310ms step_avg:61.02ms
step:1104/2245 train_time:67369ms step_avg:61.02ms
step:1105/2245 train_time:67432ms step_avg:61.02ms
step:1106/2245 train_time:67492ms step_avg:61.02ms
step:1107/2245 train_time:67555ms step_avg:61.03ms
step:1108/2245 train_time:67615ms step_avg:61.02ms
step:1109/2245 train_time:67677ms step_avg:61.03ms
step:1110/2245 train_time:67737ms step_avg:61.02ms
step:1111/2245 train_time:67800ms step_avg:61.03ms
step:1112/2245 train_time:67860ms step_avg:61.03ms
step:1113/2245 train_time:67922ms step_avg:61.03ms
step:1114/2245 train_time:67982ms step_avg:61.03ms
step:1115/2245 train_time:68045ms step_avg:61.03ms
step:1116/2245 train_time:68105ms step_avg:61.03ms
step:1117/2245 train_time:68168ms step_avg:61.03ms
step:1118/2245 train_time:68228ms step_avg:61.03ms
step:1119/2245 train_time:68289ms step_avg:61.03ms
step:1120/2245 train_time:68349ms step_avg:61.03ms
step:1121/2245 train_time:68411ms step_avg:61.03ms
step:1122/2245 train_time:68472ms step_avg:61.03ms
step:1123/2245 train_time:68534ms step_avg:61.03ms
step:1124/2245 train_time:68594ms step_avg:61.03ms
step:1125/2245 train_time:68657ms step_avg:61.03ms
step:1126/2245 train_time:68717ms step_avg:61.03ms
step:1127/2245 train_time:68779ms step_avg:61.03ms
step:1128/2245 train_time:68839ms step_avg:61.03ms
step:1129/2245 train_time:68902ms step_avg:61.03ms
step:1130/2245 train_time:68961ms step_avg:61.03ms
step:1131/2245 train_time:69024ms step_avg:61.03ms
step:1132/2245 train_time:69084ms step_avg:61.03ms
step:1133/2245 train_time:69147ms step_avg:61.03ms
step:1134/2245 train_time:69207ms step_avg:61.03ms
step:1135/2245 train_time:69269ms step_avg:61.03ms
step:1136/2245 train_time:69329ms step_avg:61.03ms
step:1137/2245 train_time:69392ms step_avg:61.03ms
step:1138/2245 train_time:69452ms step_avg:61.03ms
step:1139/2245 train_time:69514ms step_avg:61.03ms
step:1140/2245 train_time:69574ms step_avg:61.03ms
step:1141/2245 train_time:69636ms step_avg:61.03ms
step:1142/2245 train_time:69695ms step_avg:61.03ms
step:1143/2245 train_time:69759ms step_avg:61.03ms
step:1144/2245 train_time:69819ms step_avg:61.03ms
step:1145/2245 train_time:69881ms step_avg:61.03ms
step:1146/2245 train_time:69940ms step_avg:61.03ms
step:1147/2245 train_time:70003ms step_avg:61.03ms
step:1148/2245 train_time:70063ms step_avg:61.03ms
step:1149/2245 train_time:70126ms step_avg:61.03ms
step:1150/2245 train_time:70186ms step_avg:61.03ms
step:1151/2245 train_time:70249ms step_avg:61.03ms
step:1152/2245 train_time:70308ms step_avg:61.03ms
step:1153/2245 train_time:70371ms step_avg:61.03ms
step:1154/2245 train_time:70431ms step_avg:61.03ms
step:1155/2245 train_time:70493ms step_avg:61.03ms
step:1156/2245 train_time:70554ms step_avg:61.03ms
step:1157/2245 train_time:70616ms step_avg:61.03ms
step:1158/2245 train_time:70676ms step_avg:61.03ms
step:1159/2245 train_time:70738ms step_avg:61.03ms
step:1160/2245 train_time:70798ms step_avg:61.03ms
step:1161/2245 train_time:70860ms step_avg:61.03ms
step:1162/2245 train_time:70920ms step_avg:61.03ms
step:1163/2245 train_time:70982ms step_avg:61.03ms
step:1164/2245 train_time:71042ms step_avg:61.03ms
step:1165/2245 train_time:71105ms step_avg:61.03ms
step:1166/2245 train_time:71165ms step_avg:61.03ms
step:1167/2245 train_time:71228ms step_avg:61.04ms
step:1168/2245 train_time:71288ms step_avg:61.03ms
step:1169/2245 train_time:71350ms step_avg:61.04ms
step:1170/2245 train_time:71410ms step_avg:61.03ms
step:1171/2245 train_time:71473ms step_avg:61.04ms
step:1172/2245 train_time:71533ms step_avg:61.03ms
step:1173/2245 train_time:71595ms step_avg:61.04ms
step:1174/2245 train_time:71655ms step_avg:61.03ms
step:1175/2245 train_time:71717ms step_avg:61.04ms
step:1176/2245 train_time:71776ms step_avg:61.03ms
step:1177/2245 train_time:71838ms step_avg:61.04ms
step:1178/2245 train_time:71898ms step_avg:61.03ms
step:1179/2245 train_time:71960ms step_avg:61.03ms
step:1180/2245 train_time:72020ms step_avg:61.03ms
step:1181/2245 train_time:72084ms step_avg:61.04ms
step:1182/2245 train_time:72144ms step_avg:61.04ms
step:1183/2245 train_time:72208ms step_avg:61.04ms
step:1184/2245 train_time:72268ms step_avg:61.04ms
step:1185/2245 train_time:72330ms step_avg:61.04ms
step:1186/2245 train_time:72389ms step_avg:61.04ms
step:1187/2245 train_time:72452ms step_avg:61.04ms
step:1188/2245 train_time:72511ms step_avg:61.04ms
step:1189/2245 train_time:72574ms step_avg:61.04ms
step:1190/2245 train_time:72633ms step_avg:61.04ms
step:1191/2245 train_time:72695ms step_avg:61.04ms
step:1192/2245 train_time:72755ms step_avg:61.04ms
step:1193/2245 train_time:72816ms step_avg:61.04ms
step:1194/2245 train_time:72877ms step_avg:61.04ms
step:1195/2245 train_time:72939ms step_avg:61.04ms
step:1196/2245 train_time:72999ms step_avg:61.04ms
step:1197/2245 train_time:73062ms step_avg:61.04ms
step:1198/2245 train_time:73122ms step_avg:61.04ms
step:1199/2245 train_time:73185ms step_avg:61.04ms
step:1200/2245 train_time:73246ms step_avg:61.04ms
step:1201/2245 train_time:73309ms step_avg:61.04ms
step:1202/2245 train_time:73369ms step_avg:61.04ms
step:1203/2245 train_time:73431ms step_avg:61.04ms
step:1204/2245 train_time:73491ms step_avg:61.04ms
step:1205/2245 train_time:73554ms step_avg:61.04ms
step:1206/2245 train_time:73613ms step_avg:61.04ms
step:1207/2245 train_time:73675ms step_avg:61.04ms
step:1208/2245 train_time:73734ms step_avg:61.04ms
step:1209/2245 train_time:73796ms step_avg:61.04ms
step:1210/2245 train_time:73856ms step_avg:61.04ms
step:1211/2245 train_time:73919ms step_avg:61.04ms
step:1212/2245 train_time:73979ms step_avg:61.04ms
step:1213/2245 train_time:74042ms step_avg:61.04ms
step:1214/2245 train_time:74102ms step_avg:61.04ms
step:1215/2245 train_time:74165ms step_avg:61.04ms
step:1216/2245 train_time:74225ms step_avg:61.04ms
step:1217/2245 train_time:74288ms step_avg:61.04ms
step:1218/2245 train_time:74349ms step_avg:61.04ms
step:1219/2245 train_time:74411ms step_avg:61.04ms
step:1220/2245 train_time:74472ms step_avg:61.04ms
step:1221/2245 train_time:74533ms step_avg:61.04ms
step:1222/2245 train_time:74593ms step_avg:61.04ms
step:1223/2245 train_time:74656ms step_avg:61.04ms
step:1224/2245 train_time:74716ms step_avg:61.04ms
step:1225/2245 train_time:74778ms step_avg:61.04ms
step:1226/2245 train_time:74838ms step_avg:61.04ms
step:1227/2245 train_time:74901ms step_avg:61.04ms
step:1228/2245 train_time:74961ms step_avg:61.04ms
step:1229/2245 train_time:75023ms step_avg:61.04ms
step:1230/2245 train_time:75083ms step_avg:61.04ms
step:1231/2245 train_time:75146ms step_avg:61.04ms
step:1232/2245 train_time:75207ms step_avg:61.04ms
step:1233/2245 train_time:75269ms step_avg:61.05ms
step:1234/2245 train_time:75329ms step_avg:61.04ms
step:1235/2245 train_time:75391ms step_avg:61.05ms
step:1236/2245 train_time:75451ms step_avg:61.04ms
step:1237/2245 train_time:75513ms step_avg:61.05ms
step:1238/2245 train_time:75572ms step_avg:61.04ms
step:1239/2245 train_time:75635ms step_avg:61.04ms
step:1240/2245 train_time:75695ms step_avg:61.04ms
step:1241/2245 train_time:75756ms step_avg:61.04ms
step:1242/2245 train_time:75816ms step_avg:61.04ms
step:1243/2245 train_time:75878ms step_avg:61.04ms
step:1244/2245 train_time:75938ms step_avg:61.04ms
step:1245/2245 train_time:76001ms step_avg:61.04ms
step:1246/2245 train_time:76061ms step_avg:61.04ms
step:1247/2245 train_time:76124ms step_avg:61.05ms
step:1248/2245 train_time:76184ms step_avg:61.05ms
step:1249/2245 train_time:76247ms step_avg:61.05ms
step:1250/2245 train_time:76307ms step_avg:61.05ms
step:1250/2245 val_loss:3.5218 train_time:76371ms step_avg:61.10ms
step:1251/2245 train_time:76390ms step_avg:61.06ms
step:1252/2245 train_time:76433ms step_avg:61.05ms
step:1253/2245 train_time:76499ms step_avg:61.05ms
step:1254/2245 train_time:76561ms step_avg:61.05ms
step:1255/2245 train_time:76623ms step_avg:61.05ms
step:1256/2245 train_time:76683ms step_avg:61.05ms
step:1257/2245 train_time:76744ms step_avg:61.05ms
step:1258/2245 train_time:76803ms step_avg:61.05ms
step:1259/2245 train_time:76864ms step_avg:61.05ms
step:1260/2245 train_time:76924ms step_avg:61.05ms
step:1261/2245 train_time:76985ms step_avg:61.05ms
step:1262/2245 train_time:77044ms step_avg:61.05ms
step:1263/2245 train_time:77107ms step_avg:61.05ms
step:1264/2245 train_time:77166ms step_avg:61.05ms
step:1265/2245 train_time:77228ms step_avg:61.05ms
step:1266/2245 train_time:77288ms step_avg:61.05ms
step:1267/2245 train_time:77351ms step_avg:61.05ms
step:1268/2245 train_time:77413ms step_avg:61.05ms
step:1269/2245 train_time:77477ms step_avg:61.05ms
step:1270/2245 train_time:77538ms step_avg:61.05ms
step:1271/2245 train_time:77601ms step_avg:61.06ms
step:1272/2245 train_time:77662ms step_avg:61.06ms
step:1273/2245 train_time:77724ms step_avg:61.06ms
step:1274/2245 train_time:77783ms step_avg:61.05ms
step:1275/2245 train_time:77844ms step_avg:61.05ms
step:1276/2245 train_time:77903ms step_avg:61.05ms
step:1277/2245 train_time:77965ms step_avg:61.05ms
step:1278/2245 train_time:78024ms step_avg:61.05ms
step:1279/2245 train_time:78086ms step_avg:61.05ms
step:1280/2245 train_time:78145ms step_avg:61.05ms
step:1281/2245 train_time:78208ms step_avg:61.05ms
step:1282/2245 train_time:78268ms step_avg:61.05ms
step:1283/2245 train_time:78331ms step_avg:61.05ms
step:1284/2245 train_time:78391ms step_avg:61.05ms
step:1285/2245 train_time:78455ms step_avg:61.05ms
step:1286/2245 train_time:78515ms step_avg:61.05ms
step:1287/2245 train_time:78579ms step_avg:61.06ms
step:1288/2245 train_time:78639ms step_avg:61.06ms
step:1289/2245 train_time:78702ms step_avg:61.06ms
step:1290/2245 train_time:78762ms step_avg:61.06ms
step:1291/2245 train_time:78824ms step_avg:61.06ms
step:1292/2245 train_time:78884ms step_avg:61.06ms
step:1293/2245 train_time:78945ms step_avg:61.06ms
step:1294/2245 train_time:79004ms step_avg:61.05ms
step:1295/2245 train_time:79066ms step_avg:61.05ms
step:1296/2245 train_time:79126ms step_avg:61.05ms
step:1297/2245 train_time:79188ms step_avg:61.05ms
step:1298/2245 train_time:79247ms step_avg:61.05ms
step:1299/2245 train_time:79309ms step_avg:61.05ms
step:1300/2245 train_time:79369ms step_avg:61.05ms
step:1301/2245 train_time:79433ms step_avg:61.06ms
step:1302/2245 train_time:79493ms step_avg:61.05ms
step:1303/2245 train_time:79556ms step_avg:61.06ms
step:1304/2245 train_time:79617ms step_avg:61.06ms
step:1305/2245 train_time:79680ms step_avg:61.06ms
step:1306/2245 train_time:79741ms step_avg:61.06ms
step:1307/2245 train_time:79804ms step_avg:61.06ms
step:1308/2245 train_time:79863ms step_avg:61.06ms
step:1309/2245 train_time:79925ms step_avg:61.06ms
step:1310/2245 train_time:79985ms step_avg:61.06ms
step:1311/2245 train_time:80046ms step_avg:61.06ms
step:1312/2245 train_time:80106ms step_avg:61.06ms
step:1313/2245 train_time:80168ms step_avg:61.06ms
step:1314/2245 train_time:80228ms step_avg:61.06ms
step:1315/2245 train_time:80290ms step_avg:61.06ms
step:1316/2245 train_time:80350ms step_avg:61.06ms
step:1317/2245 train_time:80412ms step_avg:61.06ms
step:1318/2245 train_time:80472ms step_avg:61.06ms
step:1319/2245 train_time:80536ms step_avg:61.06ms
step:1320/2245 train_time:80596ms step_avg:61.06ms
step:1321/2245 train_time:80659ms step_avg:61.06ms
step:1322/2245 train_time:80719ms step_avg:61.06ms
step:1323/2245 train_time:80783ms step_avg:61.06ms
step:1324/2245 train_time:80843ms step_avg:61.06ms
step:1325/2245 train_time:80905ms step_avg:61.06ms
step:1326/2245 train_time:80965ms step_avg:61.06ms
step:1327/2245 train_time:81026ms step_avg:61.06ms
step:1328/2245 train_time:81086ms step_avg:61.06ms
step:1329/2245 train_time:81148ms step_avg:61.06ms
step:1330/2245 train_time:81208ms step_avg:61.06ms
step:1331/2245 train_time:81270ms step_avg:61.06ms
step:1332/2245 train_time:81329ms step_avg:61.06ms
step:1333/2245 train_time:81392ms step_avg:61.06ms
step:1334/2245 train_time:81451ms step_avg:61.06ms
step:1335/2245 train_time:81514ms step_avg:61.06ms
step:1336/2245 train_time:81574ms step_avg:61.06ms
step:1337/2245 train_time:81637ms step_avg:61.06ms
step:1338/2245 train_time:81698ms step_avg:61.06ms
step:1339/2245 train_time:81761ms step_avg:61.06ms
step:1340/2245 train_time:81821ms step_avg:61.06ms
step:1341/2245 train_time:81884ms step_avg:61.06ms
step:1342/2245 train_time:81944ms step_avg:61.06ms
step:1343/2245 train_time:82005ms step_avg:61.06ms
step:1344/2245 train_time:82065ms step_avg:61.06ms
step:1345/2245 train_time:82127ms step_avg:61.06ms
step:1346/2245 train_time:82186ms step_avg:61.06ms
step:1347/2245 train_time:82248ms step_avg:61.06ms
step:1348/2245 train_time:82307ms step_avg:61.06ms
step:1349/2245 train_time:82370ms step_avg:61.06ms
step:1350/2245 train_time:82430ms step_avg:61.06ms
step:1351/2245 train_time:82493ms step_avg:61.06ms
step:1352/2245 train_time:82553ms step_avg:61.06ms
step:1353/2245 train_time:82616ms step_avg:61.06ms
step:1354/2245 train_time:82677ms step_avg:61.06ms
step:1355/2245 train_time:82739ms step_avg:61.06ms
step:1356/2245 train_time:82800ms step_avg:61.06ms
step:1357/2245 train_time:82864ms step_avg:61.06ms
step:1358/2245 train_time:82923ms step_avg:61.06ms
step:1359/2245 train_time:82986ms step_avg:61.06ms
step:1360/2245 train_time:83045ms step_avg:61.06ms
step:1361/2245 train_time:83108ms step_avg:61.06ms
step:1362/2245 train_time:83168ms step_avg:61.06ms
step:1363/2245 train_time:83230ms step_avg:61.06ms
step:1364/2245 train_time:83290ms step_avg:61.06ms
step:1365/2245 train_time:83352ms step_avg:61.06ms
step:1366/2245 train_time:83411ms step_avg:61.06ms
step:1367/2245 train_time:83474ms step_avg:61.06ms
step:1368/2245 train_time:83534ms step_avg:61.06ms
step:1369/2245 train_time:83596ms step_avg:61.06ms
step:1370/2245 train_time:83656ms step_avg:61.06ms
step:1371/2245 train_time:83719ms step_avg:61.06ms
step:1372/2245 train_time:83781ms step_avg:61.06ms
step:1373/2245 train_time:83843ms step_avg:61.07ms
step:1374/2245 train_time:83903ms step_avg:61.06ms
step:1375/2245 train_time:83965ms step_avg:61.07ms
step:1376/2245 train_time:84024ms step_avg:61.06ms
step:1377/2245 train_time:84088ms step_avg:61.07ms
step:1378/2245 train_time:84147ms step_avg:61.06ms
step:1379/2245 train_time:84209ms step_avg:61.07ms
step:1380/2245 train_time:84268ms step_avg:61.06ms
step:1381/2245 train_time:84330ms step_avg:61.06ms
step:1382/2245 train_time:84390ms step_avg:61.06ms
step:1383/2245 train_time:84452ms step_avg:61.06ms
step:1384/2245 train_time:84512ms step_avg:61.06ms
step:1385/2245 train_time:84575ms step_avg:61.06ms
step:1386/2245 train_time:84635ms step_avg:61.06ms
step:1387/2245 train_time:84699ms step_avg:61.07ms
step:1388/2245 train_time:84759ms step_avg:61.07ms
step:1389/2245 train_time:84822ms step_avg:61.07ms
step:1390/2245 train_time:84883ms step_avg:61.07ms
step:1391/2245 train_time:84945ms step_avg:61.07ms
step:1392/2245 train_time:85005ms step_avg:61.07ms
step:1393/2245 train_time:85067ms step_avg:61.07ms
step:1394/2245 train_time:85127ms step_avg:61.07ms
step:1395/2245 train_time:85189ms step_avg:61.07ms
step:1396/2245 train_time:85248ms step_avg:61.07ms
step:1397/2245 train_time:85310ms step_avg:61.07ms
step:1398/2245 train_time:85370ms step_avg:61.07ms
step:1399/2245 train_time:85432ms step_avg:61.07ms
step:1400/2245 train_time:85491ms step_avg:61.07ms
step:1401/2245 train_time:85554ms step_avg:61.07ms
step:1402/2245 train_time:85614ms step_avg:61.07ms
step:1403/2245 train_time:85677ms step_avg:61.07ms
step:1404/2245 train_time:85737ms step_avg:61.07ms
step:1405/2245 train_time:85799ms step_avg:61.07ms
step:1406/2245 train_time:85859ms step_avg:61.07ms
step:1407/2245 train_time:85922ms step_avg:61.07ms
step:1408/2245 train_time:85982ms step_avg:61.07ms
step:1409/2245 train_time:86044ms step_avg:61.07ms
step:1410/2245 train_time:86105ms step_avg:61.07ms
step:1411/2245 train_time:86167ms step_avg:61.07ms
step:1412/2245 train_time:86227ms step_avg:61.07ms
step:1413/2245 train_time:86289ms step_avg:61.07ms
step:1414/2245 train_time:86349ms step_avg:61.07ms
step:1415/2245 train_time:86411ms step_avg:61.07ms
step:1416/2245 train_time:86470ms step_avg:61.07ms
step:1417/2245 train_time:86532ms step_avg:61.07ms
step:1418/2245 train_time:86592ms step_avg:61.07ms
step:1419/2245 train_time:86655ms step_avg:61.07ms
step:1420/2245 train_time:86715ms step_avg:61.07ms
step:1421/2245 train_time:86778ms step_avg:61.07ms
step:1422/2245 train_time:86839ms step_avg:61.07ms
step:1423/2245 train_time:86902ms step_avg:61.07ms
step:1424/2245 train_time:86962ms step_avg:61.07ms
step:1425/2245 train_time:87023ms step_avg:61.07ms
step:1426/2245 train_time:87083ms step_avg:61.07ms
step:1427/2245 train_time:87146ms step_avg:61.07ms
step:1428/2245 train_time:87206ms step_avg:61.07ms
step:1429/2245 train_time:87268ms step_avg:61.07ms
step:1430/2245 train_time:87328ms step_avg:61.07ms
step:1431/2245 train_time:87390ms step_avg:61.07ms
step:1432/2245 train_time:87450ms step_avg:61.07ms
step:1433/2245 train_time:87512ms step_avg:61.07ms
step:1434/2245 train_time:87572ms step_avg:61.07ms
step:1435/2245 train_time:87635ms step_avg:61.07ms
step:1436/2245 train_time:87695ms step_avg:61.07ms
step:1437/2245 train_time:87757ms step_avg:61.07ms
step:1438/2245 train_time:87818ms step_avg:61.07ms
step:1439/2245 train_time:87881ms step_avg:61.07ms
step:1440/2245 train_time:87941ms step_avg:61.07ms
step:1441/2245 train_time:88003ms step_avg:61.07ms
step:1442/2245 train_time:88063ms step_avg:61.07ms
step:1443/2245 train_time:88127ms step_avg:61.07ms
step:1444/2245 train_time:88187ms step_avg:61.07ms
step:1445/2245 train_time:88249ms step_avg:61.07ms
step:1446/2245 train_time:88309ms step_avg:61.07ms
step:1447/2245 train_time:88371ms step_avg:61.07ms
step:1448/2245 train_time:88430ms step_avg:61.07ms
step:1449/2245 train_time:88492ms step_avg:61.07ms
step:1450/2245 train_time:88552ms step_avg:61.07ms
step:1451/2245 train_time:88614ms step_avg:61.07ms
step:1452/2245 train_time:88674ms step_avg:61.07ms
step:1453/2245 train_time:88737ms step_avg:61.07ms
step:1454/2245 train_time:88798ms step_avg:61.07ms
step:1455/2245 train_time:88860ms step_avg:61.07ms
step:1456/2245 train_time:88920ms step_avg:61.07ms
step:1457/2245 train_time:88983ms step_avg:61.07ms
step:1458/2245 train_time:89043ms step_avg:61.07ms
step:1459/2245 train_time:89106ms step_avg:61.07ms
step:1460/2245 train_time:89166ms step_avg:61.07ms
step:1461/2245 train_time:89228ms step_avg:61.07ms
step:1462/2245 train_time:89288ms step_avg:61.07ms
step:1463/2245 train_time:89350ms step_avg:61.07ms
step:1464/2245 train_time:89409ms step_avg:61.07ms
step:1465/2245 train_time:89472ms step_avg:61.07ms
step:1466/2245 train_time:89531ms step_avg:61.07ms
step:1467/2245 train_time:89593ms step_avg:61.07ms
step:1468/2245 train_time:89653ms step_avg:61.07ms
step:1469/2245 train_time:89715ms step_avg:61.07ms
step:1470/2245 train_time:89775ms step_avg:61.07ms
step:1471/2245 train_time:89837ms step_avg:61.07ms
step:1472/2245 train_time:89898ms step_avg:61.07ms
step:1473/2245 train_time:89961ms step_avg:61.07ms
step:1474/2245 train_time:90022ms step_avg:61.07ms
step:1475/2245 train_time:90086ms step_avg:61.07ms
step:1476/2245 train_time:90146ms step_avg:61.07ms
step:1477/2245 train_time:90209ms step_avg:61.08ms
step:1478/2245 train_time:90270ms step_avg:61.08ms
step:1479/2245 train_time:90332ms step_avg:61.08ms
step:1480/2245 train_time:90393ms step_avg:61.08ms
step:1481/2245 train_time:90456ms step_avg:61.08ms
step:1482/2245 train_time:90516ms step_avg:61.08ms
step:1483/2245 train_time:90578ms step_avg:61.08ms
step:1484/2245 train_time:90639ms step_avg:61.08ms
step:1485/2245 train_time:90701ms step_avg:61.08ms
step:1486/2245 train_time:90762ms step_avg:61.08ms
step:1487/2245 train_time:90825ms step_avg:61.08ms
step:1488/2245 train_time:90885ms step_avg:61.08ms
step:1489/2245 train_time:90948ms step_avg:61.08ms
step:1490/2245 train_time:91009ms step_avg:61.08ms
step:1491/2245 train_time:91072ms step_avg:61.08ms
step:1492/2245 train_time:91133ms step_avg:61.08ms
step:1493/2245 train_time:91196ms step_avg:61.08ms
step:1494/2245 train_time:91257ms step_avg:61.08ms
step:1495/2245 train_time:91320ms step_avg:61.08ms
step:1496/2245 train_time:91382ms step_avg:61.08ms
step:1497/2245 train_time:91445ms step_avg:61.09ms
step:1498/2245 train_time:91505ms step_avg:61.08ms
step:1499/2245 train_time:91567ms step_avg:61.09ms
step:1500/2245 train_time:91627ms step_avg:61.08ms
step:1500/2245 val_loss:3.4418 train_time:91691ms step_avg:61.13ms
step:1501/2245 train_time:91710ms step_avg:61.10ms
step:1502/2245 train_time:91752ms step_avg:61.09ms
step:1503/2245 train_time:91814ms step_avg:61.09ms
step:1504/2245 train_time:91876ms step_avg:61.09ms
step:1505/2245 train_time:91941ms step_avg:61.09ms
step:1506/2245 train_time:92001ms step_avg:61.09ms
step:1507/2245 train_time:92063ms step_avg:61.09ms
step:1508/2245 train_time:92123ms step_avg:61.09ms
step:1509/2245 train_time:92185ms step_avg:61.09ms
step:1510/2245 train_time:92245ms step_avg:61.09ms
step:1511/2245 train_time:92307ms step_avg:61.09ms
step:1512/2245 train_time:92367ms step_avg:61.09ms
step:1513/2245 train_time:92430ms step_avg:61.09ms
step:1514/2245 train_time:92490ms step_avg:61.09ms
step:1515/2245 train_time:92552ms step_avg:61.09ms
step:1516/2245 train_time:92616ms step_avg:61.09ms
step:1517/2245 train_time:92680ms step_avg:61.09ms
step:1518/2245 train_time:92741ms step_avg:61.09ms
step:1519/2245 train_time:92806ms step_avg:61.10ms
step:1520/2245 train_time:92868ms step_avg:61.10ms
step:1521/2245 train_time:92931ms step_avg:61.10ms
step:1522/2245 train_time:92991ms step_avg:61.10ms
step:1523/2245 train_time:93054ms step_avg:61.10ms
step:1524/2245 train_time:93114ms step_avg:61.10ms
step:1525/2245 train_time:93176ms step_avg:61.10ms
step:1526/2245 train_time:93236ms step_avg:61.10ms
step:1527/2245 train_time:93299ms step_avg:61.10ms
step:1528/2245 train_time:93358ms step_avg:61.10ms
step:1529/2245 train_time:93421ms step_avg:61.10ms
step:1530/2245 train_time:93481ms step_avg:61.10ms
step:1531/2245 train_time:93546ms step_avg:61.10ms
step:1532/2245 train_time:93608ms step_avg:61.10ms
step:1533/2245 train_time:93671ms step_avg:61.10ms
step:1534/2245 train_time:93732ms step_avg:61.10ms
step:1535/2245 train_time:93795ms step_avg:61.10ms
step:1536/2245 train_time:93855ms step_avg:61.10ms
step:1537/2245 train_time:93919ms step_avg:61.11ms
step:1538/2245 train_time:93979ms step_avg:61.10ms
step:1539/2245 train_time:94042ms step_avg:61.11ms
step:1540/2245 train_time:94103ms step_avg:61.11ms
step:1541/2245 train_time:94166ms step_avg:61.11ms
step:1542/2245 train_time:94227ms step_avg:61.11ms
step:1543/2245 train_time:94289ms step_avg:61.11ms
step:1544/2245 train_time:94349ms step_avg:61.11ms
step:1545/2245 train_time:94412ms step_avg:61.11ms
step:1546/2245 train_time:94472ms step_avg:61.11ms
step:1547/2245 train_time:94535ms step_avg:61.11ms
step:1548/2245 train_time:94595ms step_avg:61.11ms
step:1549/2245 train_time:94658ms step_avg:61.11ms
step:1550/2245 train_time:94720ms step_avg:61.11ms
step:1551/2245 train_time:94784ms step_avg:61.11ms
step:1552/2245 train_time:94846ms step_avg:61.11ms
step:1553/2245 train_time:94908ms step_avg:61.11ms
step:1554/2245 train_time:94969ms step_avg:61.11ms
step:1555/2245 train_time:95032ms step_avg:61.11ms
step:1556/2245 train_time:95092ms step_avg:61.11ms
step:1557/2245 train_time:95155ms step_avg:61.11ms
step:1558/2245 train_time:95215ms step_avg:61.11ms
step:1559/2245 train_time:95277ms step_avg:61.11ms
step:1560/2245 train_time:95337ms step_avg:61.11ms
step:1561/2245 train_time:95400ms step_avg:61.11ms
step:1562/2245 train_time:95460ms step_avg:61.11ms
step:1563/2245 train_time:95524ms step_avg:61.12ms
step:1564/2245 train_time:95584ms step_avg:61.12ms
step:1565/2245 train_time:95648ms step_avg:61.12ms
step:1566/2245 train_time:95709ms step_avg:61.12ms
step:1567/2245 train_time:95772ms step_avg:61.12ms
step:1568/2245 train_time:95833ms step_avg:61.12ms
step:1569/2245 train_time:95895ms step_avg:61.12ms
step:1570/2245 train_time:95956ms step_avg:61.12ms
step:1571/2245 train_time:96019ms step_avg:61.12ms
step:1572/2245 train_time:96079ms step_avg:61.12ms
step:1573/2245 train_time:96143ms step_avg:61.12ms
step:1574/2245 train_time:96204ms step_avg:61.12ms
step:1575/2245 train_time:96268ms step_avg:61.12ms
step:1576/2245 train_time:96328ms step_avg:61.12ms
step:1577/2245 train_time:96390ms step_avg:61.12ms
step:1578/2245 train_time:96451ms step_avg:61.12ms
step:1579/2245 train_time:96513ms step_avg:61.12ms
step:1580/2245 train_time:96573ms step_avg:61.12ms
step:1581/2245 train_time:96637ms step_avg:61.12ms
step:1582/2245 train_time:96697ms step_avg:61.12ms
step:1583/2245 train_time:96760ms step_avg:61.12ms
step:1584/2245 train_time:96821ms step_avg:61.12ms
step:1585/2245 train_time:96884ms step_avg:61.13ms
step:1586/2245 train_time:96945ms step_avg:61.13ms
step:1587/2245 train_time:97008ms step_avg:61.13ms
step:1588/2245 train_time:97068ms step_avg:61.13ms
step:1589/2245 train_time:97131ms step_avg:61.13ms
step:1590/2245 train_time:97191ms step_avg:61.13ms
step:1591/2245 train_time:97254ms step_avg:61.13ms
step:1592/2245 train_time:97314ms step_avg:61.13ms
step:1593/2245 train_time:97376ms step_avg:61.13ms
step:1594/2245 train_time:97437ms step_avg:61.13ms
step:1595/2245 train_time:97499ms step_avg:61.13ms
step:1596/2245 train_time:97559ms step_avg:61.13ms
step:1597/2245 train_time:97623ms step_avg:61.13ms
step:1598/2245 train_time:97684ms step_avg:61.13ms
step:1599/2245 train_time:97747ms step_avg:61.13ms
step:1600/2245 train_time:97808ms step_avg:61.13ms
step:1601/2245 train_time:97872ms step_avg:61.13ms
step:1602/2245 train_time:97932ms step_avg:61.13ms
step:1603/2245 train_time:97995ms step_avg:61.13ms
step:1604/2245 train_time:98055ms step_avg:61.13ms
step:1605/2245 train_time:98118ms step_avg:61.13ms
step:1606/2245 train_time:98179ms step_avg:61.13ms
step:1607/2245 train_time:98242ms step_avg:61.13ms
step:1608/2245 train_time:98303ms step_avg:61.13ms
step:1609/2245 train_time:98367ms step_avg:61.14ms
step:1610/2245 train_time:98427ms step_avg:61.13ms
step:1611/2245 train_time:98489ms step_avg:61.14ms
step:1612/2245 train_time:98549ms step_avg:61.13ms
step:1613/2245 train_time:98612ms step_avg:61.14ms
step:1614/2245 train_time:98673ms step_avg:61.14ms
step:1615/2245 train_time:98736ms step_avg:61.14ms
step:1616/2245 train_time:98796ms step_avg:61.14ms
step:1617/2245 train_time:98859ms step_avg:61.14ms
step:1618/2245 train_time:98919ms step_avg:61.14ms
step:1619/2245 train_time:98983ms step_avg:61.14ms
step:1620/2245 train_time:99043ms step_avg:61.14ms
step:1621/2245 train_time:99106ms step_avg:61.14ms
step:1622/2245 train_time:99167ms step_avg:61.14ms
step:1623/2245 train_time:99230ms step_avg:61.14ms
step:1624/2245 train_time:99291ms step_avg:61.14ms
step:1625/2245 train_time:99353ms step_avg:61.14ms
step:1626/2245 train_time:99413ms step_avg:61.14ms
step:1627/2245 train_time:99476ms step_avg:61.14ms
step:1628/2245 train_time:99536ms step_avg:61.14ms
step:1629/2245 train_time:99598ms step_avg:61.14ms
step:1630/2245 train_time:99658ms step_avg:61.14ms
step:1631/2245 train_time:99722ms step_avg:61.14ms
step:1632/2245 train_time:99783ms step_avg:61.14ms
step:1633/2245 train_time:99846ms step_avg:61.14ms
step:1634/2245 train_time:99907ms step_avg:61.14ms
step:1635/2245 train_time:99970ms step_avg:61.14ms
step:1636/2245 train_time:100031ms step_avg:61.14ms
step:1637/2245 train_time:100094ms step_avg:61.14ms
step:1638/2245 train_time:100154ms step_avg:61.14ms
step:1639/2245 train_time:100217ms step_avg:61.15ms
step:1640/2245 train_time:100278ms step_avg:61.14ms
step:1641/2245 train_time:100341ms step_avg:61.15ms
step:1642/2245 train_time:100401ms step_avg:61.15ms
step:1643/2245 train_time:100464ms step_avg:61.15ms
step:1644/2245 train_time:100525ms step_avg:61.15ms
step:1645/2245 train_time:100587ms step_avg:61.15ms
step:1646/2245 train_time:100648ms step_avg:61.15ms
step:1647/2245 train_time:100711ms step_avg:61.15ms
step:1648/2245 train_time:100772ms step_avg:61.15ms
step:1649/2245 train_time:100834ms step_avg:61.15ms
step:1650/2245 train_time:100894ms step_avg:61.15ms
step:1651/2245 train_time:100958ms step_avg:61.15ms
step:1652/2245 train_time:101018ms step_avg:61.15ms
step:1653/2245 train_time:101081ms step_avg:61.15ms
step:1654/2245 train_time:101142ms step_avg:61.15ms
step:1655/2245 train_time:101206ms step_avg:61.15ms
step:1656/2245 train_time:101266ms step_avg:61.15ms
step:1657/2245 train_time:101329ms step_avg:61.15ms
step:1658/2245 train_time:101390ms step_avg:61.15ms
step:1659/2245 train_time:101453ms step_avg:61.15ms
step:1660/2245 train_time:101513ms step_avg:61.15ms
step:1661/2245 train_time:101575ms step_avg:61.15ms
step:1662/2245 train_time:101635ms step_avg:61.15ms
step:1663/2245 train_time:101698ms step_avg:61.15ms
step:1664/2245 train_time:101758ms step_avg:61.15ms
step:1665/2245 train_time:101821ms step_avg:61.15ms
step:1666/2245 train_time:101881ms step_avg:61.15ms
step:1667/2245 train_time:101945ms step_avg:61.15ms
step:1668/2245 train_time:102005ms step_avg:61.15ms
step:1669/2245 train_time:102068ms step_avg:61.16ms
step:1670/2245 train_time:102129ms step_avg:61.16ms
step:1671/2245 train_time:102192ms step_avg:61.16ms
step:1672/2245 train_time:102252ms step_avg:61.16ms
step:1673/2245 train_time:102315ms step_avg:61.16ms
step:1674/2245 train_time:102376ms step_avg:61.16ms
step:1675/2245 train_time:102438ms step_avg:61.16ms
step:1676/2245 train_time:102498ms step_avg:61.16ms
step:1677/2245 train_time:102561ms step_avg:61.16ms
step:1678/2245 train_time:102621ms step_avg:61.16ms
step:1679/2245 train_time:102684ms step_avg:61.16ms
step:1680/2245 train_time:102745ms step_avg:61.16ms
step:1681/2245 train_time:102808ms step_avg:61.16ms
step:1682/2245 train_time:102869ms step_avg:61.16ms
step:1683/2245 train_time:102932ms step_avg:61.16ms
step:1684/2245 train_time:102992ms step_avg:61.16ms
step:1685/2245 train_time:103055ms step_avg:61.16ms
step:1686/2245 train_time:103115ms step_avg:61.16ms
step:1687/2245 train_time:103177ms step_avg:61.16ms
step:1688/2245 train_time:103238ms step_avg:61.16ms
step:1689/2245 train_time:103301ms step_avg:61.16ms
step:1690/2245 train_time:103362ms step_avg:61.16ms
step:1691/2245 train_time:103425ms step_avg:61.16ms
step:1692/2245 train_time:103486ms step_avg:61.16ms
step:1693/2245 train_time:103548ms step_avg:61.16ms
step:1694/2245 train_time:103609ms step_avg:61.16ms
step:1695/2245 train_time:103672ms step_avg:61.16ms
step:1696/2245 train_time:103732ms step_avg:61.16ms
step:1697/2245 train_time:103795ms step_avg:61.16ms
step:1698/2245 train_time:103856ms step_avg:61.16ms
step:1699/2245 train_time:103919ms step_avg:61.16ms
step:1700/2245 train_time:103980ms step_avg:61.16ms
step:1701/2245 train_time:104042ms step_avg:61.17ms
step:1702/2245 train_time:104103ms step_avg:61.17ms
step:1703/2245 train_time:104166ms step_avg:61.17ms
step:1704/2245 train_time:104226ms step_avg:61.17ms
step:1705/2245 train_time:104290ms step_avg:61.17ms
step:1706/2245 train_time:104350ms step_avg:61.17ms
step:1707/2245 train_time:104412ms step_avg:61.17ms
step:1708/2245 train_time:104473ms step_avg:61.17ms
step:1709/2245 train_time:104536ms step_avg:61.17ms
step:1710/2245 train_time:104596ms step_avg:61.17ms
step:1711/2245 train_time:104659ms step_avg:61.17ms
step:1712/2245 train_time:104719ms step_avg:61.17ms
step:1713/2245 train_time:104782ms step_avg:61.17ms
step:1714/2245 train_time:104842ms step_avg:61.17ms
step:1715/2245 train_time:104906ms step_avg:61.17ms
step:1716/2245 train_time:104966ms step_avg:61.17ms
step:1717/2245 train_time:105029ms step_avg:61.17ms
step:1718/2245 train_time:105089ms step_avg:61.17ms
step:1719/2245 train_time:105152ms step_avg:61.17ms
step:1720/2245 train_time:105212ms step_avg:61.17ms
step:1721/2245 train_time:105275ms step_avg:61.17ms
step:1722/2245 train_time:105336ms step_avg:61.17ms
step:1723/2245 train_time:105398ms step_avg:61.17ms
step:1724/2245 train_time:105459ms step_avg:61.17ms
step:1725/2245 train_time:105523ms step_avg:61.17ms
step:1726/2245 train_time:105583ms step_avg:61.17ms
step:1727/2245 train_time:105647ms step_avg:61.17ms
step:1728/2245 train_time:105708ms step_avg:61.17ms
step:1729/2245 train_time:105771ms step_avg:61.17ms
step:1730/2245 train_time:105831ms step_avg:61.17ms
step:1731/2245 train_time:105894ms step_avg:61.17ms
step:1732/2245 train_time:105954ms step_avg:61.17ms
step:1733/2245 train_time:106017ms step_avg:61.18ms
step:1734/2245 train_time:106077ms step_avg:61.17ms
step:1735/2245 train_time:106140ms step_avg:61.18ms
step:1736/2245 train_time:106200ms step_avg:61.18ms
step:1737/2245 train_time:106264ms step_avg:61.18ms
step:1738/2245 train_time:106325ms step_avg:61.18ms
step:1739/2245 train_time:106387ms step_avg:61.18ms
step:1740/2245 train_time:106448ms step_avg:61.18ms
step:1741/2245 train_time:106511ms step_avg:61.18ms
step:1742/2245 train_time:106571ms step_avg:61.18ms
step:1743/2245 train_time:106634ms step_avg:61.18ms
step:1744/2245 train_time:106694ms step_avg:61.18ms
step:1745/2245 train_time:106757ms step_avg:61.18ms
step:1746/2245 train_time:106818ms step_avg:61.18ms
step:1747/2245 train_time:106880ms step_avg:61.18ms
step:1748/2245 train_time:106941ms step_avg:61.18ms
step:1749/2245 train_time:107004ms step_avg:61.18ms
step:1750/2245 train_time:107065ms step_avg:61.18ms
step:1750/2245 val_loss:3.3773 train_time:107129ms step_avg:61.22ms
step:1751/2245 train_time:107149ms step_avg:61.19ms
step:1752/2245 train_time:107192ms step_avg:61.18ms
step:1753/2245 train_time:107260ms step_avg:61.19ms
step:1754/2245 train_time:107322ms step_avg:61.19ms
step:1755/2245 train_time:107384ms step_avg:61.19ms
step:1756/2245 train_time:107444ms step_avg:61.19ms
step:1757/2245 train_time:107506ms step_avg:61.19ms
step:1758/2245 train_time:107566ms step_avg:61.19ms
step:1759/2245 train_time:107627ms step_avg:61.19ms
step:1760/2245 train_time:107687ms step_avg:61.19ms
step:1761/2245 train_time:107749ms step_avg:61.19ms
step:1762/2245 train_time:107809ms step_avg:61.19ms
step:1763/2245 train_time:107871ms step_avg:61.19ms
step:1764/2245 train_time:107931ms step_avg:61.19ms
step:1765/2245 train_time:107993ms step_avg:61.19ms
step:1766/2245 train_time:108054ms step_avg:61.19ms
step:1767/2245 train_time:108118ms step_avg:61.19ms
step:1768/2245 train_time:108181ms step_avg:61.19ms
step:1769/2245 train_time:108246ms step_avg:61.19ms
step:1770/2245 train_time:108307ms step_avg:61.19ms
step:1771/2245 train_time:108370ms step_avg:61.19ms
step:1772/2245 train_time:108430ms step_avg:61.19ms
step:1773/2245 train_time:108493ms step_avg:61.19ms
step:1774/2245 train_time:108553ms step_avg:61.19ms
step:1775/2245 train_time:108616ms step_avg:61.19ms
step:1776/2245 train_time:108676ms step_avg:61.19ms
step:1777/2245 train_time:108740ms step_avg:61.19ms
step:1778/2245 train_time:108800ms step_avg:61.19ms
step:1779/2245 train_time:108862ms step_avg:61.19ms
step:1780/2245 train_time:108922ms step_avg:61.19ms
step:1781/2245 train_time:108984ms step_avg:61.19ms
step:1782/2245 train_time:109044ms step_avg:61.19ms
step:1783/2245 train_time:109107ms step_avg:61.19ms
step:1784/2245 train_time:109169ms step_avg:61.19ms
step:1785/2245 train_time:109232ms step_avg:61.19ms
step:1786/2245 train_time:109294ms step_avg:61.19ms
step:1787/2245 train_time:109358ms step_avg:61.20ms
step:1788/2245 train_time:109418ms step_avg:61.20ms
step:1789/2245 train_time:109481ms step_avg:61.20ms
step:1790/2245 train_time:109542ms step_avg:61.20ms
step:1791/2245 train_time:109603ms step_avg:61.20ms
step:1792/2245 train_time:109664ms step_avg:61.20ms
step:1793/2245 train_time:109726ms step_avg:61.20ms
step:1794/2245 train_time:109786ms step_avg:61.20ms
step:1795/2245 train_time:109848ms step_avg:61.20ms
step:1796/2245 train_time:109907ms step_avg:61.20ms
step:1797/2245 train_time:109970ms step_avg:61.20ms
step:1798/2245 train_time:110030ms step_avg:61.20ms
step:1799/2245 train_time:110093ms step_avg:61.20ms
step:1800/2245 train_time:110154ms step_avg:61.20ms
step:1801/2245 train_time:110217ms step_avg:61.20ms
step:1802/2245 train_time:110279ms step_avg:61.20ms
step:1803/2245 train_time:110342ms step_avg:61.20ms
step:1804/2245 train_time:110403ms step_avg:61.20ms
step:1805/2245 train_time:110466ms step_avg:61.20ms
step:1806/2245 train_time:110526ms step_avg:61.20ms
step:1807/2245 train_time:110589ms step_avg:61.20ms
step:1808/2245 train_time:110650ms step_avg:61.20ms
step:1809/2245 train_time:110712ms step_avg:61.20ms
step:1810/2245 train_time:110773ms step_avg:61.20ms
step:1811/2245 train_time:110837ms step_avg:61.20ms
step:1812/2245 train_time:110897ms step_avg:61.20ms
step:1813/2245 train_time:110959ms step_avg:61.20ms
step:1814/2245 train_time:111019ms step_avg:61.20ms
step:1815/2245 train_time:111082ms step_avg:61.20ms
step:1816/2245 train_time:111143ms step_avg:61.20ms
step:1817/2245 train_time:111206ms step_avg:61.20ms
step:1818/2245 train_time:111266ms step_avg:61.20ms
step:1819/2245 train_time:111329ms step_avg:61.20ms
step:1820/2245 train_time:111390ms step_avg:61.20ms
step:1821/2245 train_time:111453ms step_avg:61.20ms
step:1822/2245 train_time:111514ms step_avg:61.20ms
step:1823/2245 train_time:111578ms step_avg:61.21ms
step:1824/2245 train_time:111639ms step_avg:61.21ms
step:1825/2245 train_time:111702ms step_avg:61.21ms
step:1826/2245 train_time:111762ms step_avg:61.21ms
step:1827/2245 train_time:111825ms step_avg:61.21ms
step:1828/2245 train_time:111885ms step_avg:61.21ms
step:1829/2245 train_time:111948ms step_avg:61.21ms
step:1830/2245 train_time:112008ms step_avg:61.21ms
step:1831/2245 train_time:112070ms step_avg:61.21ms
step:1832/2245 train_time:112131ms step_avg:61.21ms
step:1833/2245 train_time:112194ms step_avg:61.21ms
step:1834/2245 train_time:112254ms step_avg:61.21ms
step:1835/2245 train_time:112318ms step_avg:61.21ms
step:1836/2245 train_time:112379ms step_avg:61.21ms
step:1837/2245 train_time:112442ms step_avg:61.21ms
step:1838/2245 train_time:112503ms step_avg:61.21ms
step:1839/2245 train_time:112565ms step_avg:61.21ms
step:1840/2245 train_time:112626ms step_avg:61.21ms
step:1841/2245 train_time:112688ms step_avg:61.21ms
step:1842/2245 train_time:112749ms step_avg:61.21ms
step:1843/2245 train_time:112812ms step_avg:61.21ms
step:1844/2245 train_time:112873ms step_avg:61.21ms
step:1845/2245 train_time:112937ms step_avg:61.21ms
step:1846/2245 train_time:112997ms step_avg:61.21ms
step:1847/2245 train_time:113060ms step_avg:61.21ms
step:1848/2245 train_time:113120ms step_avg:61.21ms
step:1849/2245 train_time:113183ms step_avg:61.21ms
step:1850/2245 train_time:113244ms step_avg:61.21ms
step:1851/2245 train_time:113308ms step_avg:61.21ms
step:1852/2245 train_time:113367ms step_avg:61.21ms
step:1853/2245 train_time:113430ms step_avg:61.21ms
step:1854/2245 train_time:113490ms step_avg:61.21ms
step:1855/2245 train_time:113552ms step_avg:61.21ms
step:1856/2245 train_time:113613ms step_avg:61.21ms
step:1857/2245 train_time:113676ms step_avg:61.22ms
step:1858/2245 train_time:113737ms step_avg:61.21ms
step:1859/2245 train_time:113800ms step_avg:61.22ms
step:1860/2245 train_time:113861ms step_avg:61.22ms
step:1861/2245 train_time:113924ms step_avg:61.22ms
step:1862/2245 train_time:113984ms step_avg:61.22ms
step:1863/2245 train_time:114047ms step_avg:61.22ms
step:1864/2245 train_time:114108ms step_avg:61.22ms
step:1865/2245 train_time:114170ms step_avg:61.22ms
step:1866/2245 train_time:114230ms step_avg:61.22ms
step:1867/2245 train_time:114293ms step_avg:61.22ms
step:1868/2245 train_time:114354ms step_avg:61.22ms
step:1869/2245 train_time:114417ms step_avg:61.22ms
step:1870/2245 train_time:114478ms step_avg:61.22ms
step:1871/2245 train_time:114540ms step_avg:61.22ms
step:1872/2245 train_time:114601ms step_avg:61.22ms
step:1873/2245 train_time:114663ms step_avg:61.22ms
step:1874/2245 train_time:114724ms step_avg:61.22ms
step:1875/2245 train_time:114787ms step_avg:61.22ms
step:1876/2245 train_time:114848ms step_avg:61.22ms
step:1877/2245 train_time:114910ms step_avg:61.22ms
step:1878/2245 train_time:114971ms step_avg:61.22ms
step:1879/2245 train_time:115035ms step_avg:61.22ms
step:1880/2245 train_time:115096ms step_avg:61.22ms
step:1881/2245 train_time:115160ms step_avg:61.22ms
step:1882/2245 train_time:115220ms step_avg:61.22ms
step:1883/2245 train_time:115282ms step_avg:61.22ms
step:1884/2245 train_time:115342ms step_avg:61.22ms
step:1885/2245 train_time:115405ms step_avg:61.22ms
step:1886/2245 train_time:115465ms step_avg:61.22ms
step:1887/2245 train_time:115528ms step_avg:61.22ms
step:1888/2245 train_time:115587ms step_avg:61.22ms
step:1889/2245 train_time:115651ms step_avg:61.22ms
step:1890/2245 train_time:115711ms step_avg:61.22ms
step:1891/2245 train_time:115775ms step_avg:61.22ms
step:1892/2245 train_time:115836ms step_avg:61.22ms
step:1893/2245 train_time:115899ms step_avg:61.22ms
step:1894/2245 train_time:115960ms step_avg:61.22ms
step:1895/2245 train_time:116022ms step_avg:61.23ms
step:1896/2245 train_time:116082ms step_avg:61.22ms
step:1897/2245 train_time:116145ms step_avg:61.23ms
step:1898/2245 train_time:116206ms step_avg:61.23ms
step:1899/2245 train_time:116268ms step_avg:61.23ms
step:1900/2245 train_time:116328ms step_avg:61.23ms
step:1901/2245 train_time:116391ms step_avg:61.23ms
step:1902/2245 train_time:116452ms step_avg:61.23ms
step:1903/2245 train_time:116514ms step_avg:61.23ms
step:1904/2245 train_time:116575ms step_avg:61.23ms
step:1905/2245 train_time:116638ms step_avg:61.23ms
step:1906/2245 train_time:116698ms step_avg:61.23ms
step:1907/2245 train_time:116761ms step_avg:61.23ms
step:1908/2245 train_time:116822ms step_avg:61.23ms
step:1909/2245 train_time:116884ms step_avg:61.23ms
step:1910/2245 train_time:116945ms step_avg:61.23ms
step:1911/2245 train_time:117007ms step_avg:61.23ms
step:1912/2245 train_time:117068ms step_avg:61.23ms
step:1913/2245 train_time:117131ms step_avg:61.23ms
step:1914/2245 train_time:117191ms step_avg:61.23ms
step:1915/2245 train_time:117254ms step_avg:61.23ms
step:1916/2245 train_time:117315ms step_avg:61.23ms
step:1917/2245 train_time:117378ms step_avg:61.23ms
step:1918/2245 train_time:117439ms step_avg:61.23ms
step:1919/2245 train_time:117502ms step_avg:61.23ms
step:1920/2245 train_time:117562ms step_avg:61.23ms
step:1921/2245 train_time:117625ms step_avg:61.23ms
step:1922/2245 train_time:117685ms step_avg:61.23ms
step:1923/2245 train_time:117748ms step_avg:61.23ms
step:1924/2245 train_time:117808ms step_avg:61.23ms
step:1925/2245 train_time:117871ms step_avg:61.23ms
step:1926/2245 train_time:117932ms step_avg:61.23ms
step:1927/2245 train_time:117995ms step_avg:61.23ms
step:1928/2245 train_time:118057ms step_avg:61.23ms
step:1929/2245 train_time:118120ms step_avg:61.23ms
step:1930/2245 train_time:118181ms step_avg:61.23ms
step:1931/2245 train_time:118243ms step_avg:61.23ms
step:1932/2245 train_time:118303ms step_avg:61.23ms
step:1933/2245 train_time:118366ms step_avg:61.23ms
step:1934/2245 train_time:118426ms step_avg:61.23ms
step:1935/2245 train_time:118488ms step_avg:61.23ms
step:1936/2245 train_time:118550ms step_avg:61.23ms
step:1937/2245 train_time:118612ms step_avg:61.23ms
step:1938/2245 train_time:118672ms step_avg:61.23ms
step:1939/2245 train_time:118735ms step_avg:61.24ms
step:1940/2245 train_time:118795ms step_avg:61.23ms
step:1941/2245 train_time:118859ms step_avg:61.24ms
step:1942/2245 train_time:118918ms step_avg:61.23ms
step:1943/2245 train_time:118981ms step_avg:61.24ms
step:1944/2245 train_time:119041ms step_avg:61.24ms
step:1945/2245 train_time:119104ms step_avg:61.24ms
step:1946/2245 train_time:119164ms step_avg:61.24ms
step:1947/2245 train_time:119227ms step_avg:61.24ms
step:1948/2245 train_time:119287ms step_avg:61.24ms
step:1949/2245 train_time:119350ms step_avg:61.24ms
step:1950/2245 train_time:119411ms step_avg:61.24ms
step:1951/2245 train_time:119474ms step_avg:61.24ms
step:1952/2245 train_time:119535ms step_avg:61.24ms
step:1953/2245 train_time:119598ms step_avg:61.24ms
step:1954/2245 train_time:119658ms step_avg:61.24ms
step:1955/2245 train_time:119720ms step_avg:61.24ms
step:1956/2245 train_time:119781ms step_avg:61.24ms
step:1957/2245 train_time:119843ms step_avg:61.24ms
step:1958/2245 train_time:119903ms step_avg:61.24ms
step:1959/2245 train_time:119966ms step_avg:61.24ms
step:1960/2245 train_time:120026ms step_avg:61.24ms
step:1961/2245 train_time:120090ms step_avg:61.24ms
step:1962/2245 train_time:120150ms step_avg:61.24ms
step:1963/2245 train_time:120213ms step_avg:61.24ms
step:1964/2245 train_time:120273ms step_avg:61.24ms
step:1965/2245 train_time:120336ms step_avg:61.24ms
step:1966/2245 train_time:120397ms step_avg:61.24ms
step:1967/2245 train_time:120460ms step_avg:61.24ms
step:1968/2245 train_time:120520ms step_avg:61.24ms
step:1969/2245 train_time:120582ms step_avg:61.24ms
step:1970/2245 train_time:120643ms step_avg:61.24ms
step:1971/2245 train_time:120706ms step_avg:61.24ms
step:1972/2245 train_time:120766ms step_avg:61.24ms
step:1973/2245 train_time:120828ms step_avg:61.24ms
step:1974/2245 train_time:120889ms step_avg:61.24ms
step:1975/2245 train_time:120951ms step_avg:61.24ms
step:1976/2245 train_time:121011ms step_avg:61.24ms
step:1977/2245 train_time:121074ms step_avg:61.24ms
step:1978/2245 train_time:121135ms step_avg:61.24ms
step:1979/2245 train_time:121198ms step_avg:61.24ms
step:1980/2245 train_time:121259ms step_avg:61.24ms
step:1981/2245 train_time:121322ms step_avg:61.24ms
step:1982/2245 train_time:121382ms step_avg:61.24ms
step:1983/2245 train_time:121444ms step_avg:61.24ms
step:1984/2245 train_time:121504ms step_avg:61.24ms
step:1985/2245 train_time:121567ms step_avg:61.24ms
step:1986/2245 train_time:121628ms step_avg:61.24ms
step:1987/2245 train_time:121691ms step_avg:61.24ms
step:1988/2245 train_time:121752ms step_avg:61.24ms
step:1989/2245 train_time:121815ms step_avg:61.24ms
step:1990/2245 train_time:121875ms step_avg:61.24ms
step:1991/2245 train_time:121938ms step_avg:61.24ms
step:1992/2245 train_time:121999ms step_avg:61.24ms
step:1993/2245 train_time:122062ms step_avg:61.25ms
step:1994/2245 train_time:122122ms step_avg:61.24ms
step:1995/2245 train_time:122185ms step_avg:61.25ms
step:1996/2245 train_time:122246ms step_avg:61.25ms
step:1997/2245 train_time:122308ms step_avg:61.25ms
step:1998/2245 train_time:122369ms step_avg:61.25ms
step:1999/2245 train_time:122432ms step_avg:61.25ms
step:2000/2245 train_time:122492ms step_avg:61.25ms
step:2000/2245 val_loss:3.3233 train_time:122557ms step_avg:61.28ms
step:2001/2245 train_time:122575ms step_avg:61.26ms
step:2002/2245 train_time:122618ms step_avg:61.25ms
step:2003/2245 train_time:122682ms step_avg:61.25ms
step:2004/2245 train_time:122743ms step_avg:61.25ms
step:2005/2245 train_time:122806ms step_avg:61.25ms
step:2006/2245 train_time:122866ms step_avg:61.25ms
step:2007/2245 train_time:122928ms step_avg:61.25ms
step:2008/2245 train_time:122988ms step_avg:61.25ms
step:2009/2245 train_time:123050ms step_avg:61.25ms
step:2010/2245 train_time:123109ms step_avg:61.25ms
step:2011/2245 train_time:123173ms step_avg:61.25ms
step:2012/2245 train_time:123232ms step_avg:61.25ms
step:2013/2245 train_time:123295ms step_avg:61.25ms
step:2014/2245 train_time:123355ms step_avg:61.25ms
step:2015/2245 train_time:123417ms step_avg:61.25ms
step:2016/2245 train_time:123478ms step_avg:61.25ms
step:2017/2245 train_time:123542ms step_avg:61.25ms
step:2018/2245 train_time:123605ms step_avg:61.25ms
step:2019/2245 train_time:123668ms step_avg:61.25ms
step:2020/2245 train_time:123730ms step_avg:61.25ms
step:2021/2245 train_time:123793ms step_avg:61.25ms
step:2022/2245 train_time:123853ms step_avg:61.25ms
step:2023/2245 train_time:123916ms step_avg:61.25ms
step:2024/2245 train_time:123976ms step_avg:61.25ms
step:2025/2245 train_time:124039ms step_avg:61.25ms
step:2026/2245 train_time:124100ms step_avg:61.25ms
step:2027/2245 train_time:124163ms step_avg:61.25ms
step:2028/2245 train_time:124224ms step_avg:61.25ms
step:2029/2245 train_time:124287ms step_avg:61.26ms
step:2030/2245 train_time:124347ms step_avg:61.25ms
step:2031/2245 train_time:124409ms step_avg:61.25ms
step:2032/2245 train_time:124469ms step_avg:61.25ms
step:2033/2245 train_time:124532ms step_avg:61.26ms
step:2034/2245 train_time:124593ms step_avg:61.26ms
step:2035/2245 train_time:124656ms step_avg:61.26ms
step:2036/2245 train_time:124718ms step_avg:61.26ms
step:2037/2245 train_time:124781ms step_avg:61.26ms
step:2038/2245 train_time:124842ms step_avg:61.26ms
step:2039/2245 train_time:124905ms step_avg:61.26ms
step:2040/2245 train_time:124965ms step_avg:61.26ms
step:2041/2245 train_time:125028ms step_avg:61.26ms
step:2042/2245 train_time:125088ms step_avg:61.26ms
step:2043/2245 train_time:125151ms step_avg:61.26ms
step:2044/2245 train_time:125211ms step_avg:61.26ms
step:2045/2245 train_time:125274ms step_avg:61.26ms
step:2046/2245 train_time:125334ms step_avg:61.26ms
step:2047/2245 train_time:125398ms step_avg:61.26ms
step:2048/2245 train_time:125459ms step_avg:61.26ms
step:2049/2245 train_time:125521ms step_avg:61.26ms
step:2050/2245 train_time:125582ms step_avg:61.26ms
step:2051/2245 train_time:125647ms step_avg:61.26ms
step:2052/2245 train_time:125707ms step_avg:61.26ms
step:2053/2245 train_time:125770ms step_avg:61.26ms
step:2054/2245 train_time:125830ms step_avg:61.26ms
step:2055/2245 train_time:125893ms step_avg:61.26ms
step:2056/2245 train_time:125954ms step_avg:61.26ms
step:2057/2245 train_time:126017ms step_avg:61.26ms
step:2058/2245 train_time:126077ms step_avg:61.26ms
step:2059/2245 train_time:126140ms step_avg:61.26ms
step:2060/2245 train_time:126201ms step_avg:61.26ms
step:2061/2245 train_time:126264ms step_avg:61.26ms
step:2062/2245 train_time:126325ms step_avg:61.26ms
step:2063/2245 train_time:126387ms step_avg:61.26ms
step:2064/2245 train_time:126448ms step_avg:61.26ms
step:2065/2245 train_time:126511ms step_avg:61.26ms
step:2066/2245 train_time:126572ms step_avg:61.26ms
step:2067/2245 train_time:126634ms step_avg:61.26ms
step:2068/2245 train_time:126695ms step_avg:61.26ms
step:2069/2245 train_time:126759ms step_avg:61.27ms
step:2070/2245 train_time:126820ms step_avg:61.27ms
step:2071/2245 train_time:126884ms step_avg:61.27ms
step:2072/2245 train_time:126945ms step_avg:61.27ms
step:2073/2245 train_time:127007ms step_avg:61.27ms
step:2074/2245 train_time:127067ms step_avg:61.27ms
step:2075/2245 train_time:127130ms step_avg:61.27ms
step:2076/2245 train_time:127190ms step_avg:61.27ms
step:2077/2245 train_time:127253ms step_avg:61.27ms
step:2078/2245 train_time:127313ms step_avg:61.27ms
step:2079/2245 train_time:127376ms step_avg:61.27ms
step:2080/2245 train_time:127437ms step_avg:61.27ms
step:2081/2245 train_time:127499ms step_avg:61.27ms
step:2082/2245 train_time:127561ms step_avg:61.27ms
step:2083/2245 train_time:127624ms step_avg:61.27ms
step:2084/2245 train_time:127684ms step_avg:61.27ms
step:2085/2245 train_time:127748ms step_avg:61.27ms
step:2086/2245 train_time:127808ms step_avg:61.27ms
step:2087/2245 train_time:127871ms step_avg:61.27ms
step:2088/2245 train_time:127931ms step_avg:61.27ms
step:2089/2245 train_time:127995ms step_avg:61.27ms
step:2090/2245 train_time:128055ms step_avg:61.27ms
step:2091/2245 train_time:128119ms step_avg:61.27ms
step:2092/2245 train_time:128180ms step_avg:61.27ms
step:2093/2245 train_time:128244ms step_avg:61.27ms
step:2094/2245 train_time:128305ms step_avg:61.27ms
step:2095/2245 train_time:128367ms step_avg:61.27ms
step:2096/2245 train_time:128427ms step_avg:61.27ms
step:2097/2245 train_time:128490ms step_avg:61.27ms
step:2098/2245 train_time:128550ms step_avg:61.27ms
step:2099/2245 train_time:128613ms step_avg:61.27ms
step:2100/2245 train_time:128674ms step_avg:61.27ms
step:2101/2245 train_time:128737ms step_avg:61.27ms
step:2102/2245 train_time:128799ms step_avg:61.27ms
step:2103/2245 train_time:128862ms step_avg:61.28ms
step:2104/2245 train_time:128923ms step_avg:61.28ms
step:2105/2245 train_time:128986ms step_avg:61.28ms
step:2106/2245 train_time:129046ms step_avg:61.28ms
step:2107/2245 train_time:129109ms step_avg:61.28ms
step:2108/2245 train_time:129170ms step_avg:61.28ms
step:2109/2245 train_time:129233ms step_avg:61.28ms
step:2110/2245 train_time:129294ms step_avg:61.28ms
step:2111/2245 train_time:129357ms step_avg:61.28ms
step:2112/2245 train_time:129417ms step_avg:61.28ms
step:2113/2245 train_time:129480ms step_avg:61.28ms
step:2114/2245 train_time:129541ms step_avg:61.28ms
step:2115/2245 train_time:129604ms step_avg:61.28ms
step:2116/2245 train_time:129665ms step_avg:61.28ms
step:2117/2245 train_time:129727ms step_avg:61.28ms
step:2118/2245 train_time:129788ms step_avg:61.28ms
step:2119/2245 train_time:129851ms step_avg:61.28ms
step:2120/2245 train_time:129911ms step_avg:61.28ms
step:2121/2245 train_time:129974ms step_avg:61.28ms
step:2122/2245 train_time:130035ms step_avg:61.28ms
step:2123/2245 train_time:130098ms step_avg:61.28ms
step:2124/2245 train_time:130159ms step_avg:61.28ms
step:2125/2245 train_time:130222ms step_avg:61.28ms
step:2126/2245 train_time:130283ms step_avg:61.28ms
step:2127/2245 train_time:130345ms step_avg:61.28ms
step:2128/2245 train_time:130406ms step_avg:61.28ms
step:2129/2245 train_time:130469ms step_avg:61.28ms
step:2130/2245 train_time:130529ms step_avg:61.28ms
step:2131/2245 train_time:130592ms step_avg:61.28ms
step:2132/2245 train_time:130652ms step_avg:61.28ms
step:2133/2245 train_time:130715ms step_avg:61.28ms
step:2134/2245 train_time:130776ms step_avg:61.28ms
step:2135/2245 train_time:130839ms step_avg:61.28ms
step:2136/2245 train_time:130900ms step_avg:61.28ms
step:2137/2245 train_time:130963ms step_avg:61.28ms
step:2138/2245 train_time:131024ms step_avg:61.28ms
step:2139/2245 train_time:131086ms step_avg:61.28ms
step:2140/2245 train_time:131146ms step_avg:61.28ms
step:2141/2245 train_time:131209ms step_avg:61.28ms
step:2142/2245 train_time:131269ms step_avg:61.28ms
step:2143/2245 train_time:131332ms step_avg:61.28ms
step:2144/2245 train_time:131393ms step_avg:61.28ms
step:2145/2245 train_time:131456ms step_avg:61.28ms
step:2146/2245 train_time:131516ms step_avg:61.28ms
step:2147/2245 train_time:131579ms step_avg:61.28ms
step:2148/2245 train_time:131640ms step_avg:61.28ms
step:2149/2245 train_time:131703ms step_avg:61.29ms
step:2150/2245 train_time:131765ms step_avg:61.29ms
step:2151/2245 train_time:131827ms step_avg:61.29ms
step:2152/2245 train_time:131887ms step_avg:61.29ms
step:2153/2245 train_time:131951ms step_avg:61.29ms
step:2154/2245 train_time:132011ms step_avg:61.29ms
step:2155/2245 train_time:132075ms step_avg:61.29ms
step:2156/2245 train_time:132135ms step_avg:61.29ms
step:2157/2245 train_time:132199ms step_avg:61.29ms
step:2158/2245 train_time:132260ms step_avg:61.29ms
step:2159/2245 train_time:132323ms step_avg:61.29ms
step:2160/2245 train_time:132384ms step_avg:61.29ms
step:2161/2245 train_time:132447ms step_avg:61.29ms
step:2162/2245 train_time:132507ms step_avg:61.29ms
step:2163/2245 train_time:132571ms step_avg:61.29ms
step:2164/2245 train_time:132631ms step_avg:61.29ms
step:2165/2245 train_time:132694ms step_avg:61.29ms
step:2166/2245 train_time:132755ms step_avg:61.29ms
step:2167/2245 train_time:132818ms step_avg:61.29ms
step:2168/2245 train_time:132878ms step_avg:61.29ms
step:2169/2245 train_time:132942ms step_avg:61.29ms
step:2170/2245 train_time:133003ms step_avg:61.29ms
step:2171/2245 train_time:133067ms step_avg:61.29ms
step:2172/2245 train_time:133127ms step_avg:61.29ms
step:2173/2245 train_time:133189ms step_avg:61.29ms
step:2174/2245 train_time:133249ms step_avg:61.29ms
step:2175/2245 train_time:133312ms step_avg:61.29ms
step:2176/2245 train_time:133373ms step_avg:61.29ms
step:2177/2245 train_time:133436ms step_avg:61.29ms
step:2178/2245 train_time:133496ms step_avg:61.29ms
step:2179/2245 train_time:133559ms step_avg:61.29ms
step:2180/2245 train_time:133621ms step_avg:61.29ms
step:2181/2245 train_time:133684ms step_avg:61.29ms
step:2182/2245 train_time:133745ms step_avg:61.29ms
step:2183/2245 train_time:133808ms step_avg:61.30ms
step:2184/2245 train_time:133868ms step_avg:61.29ms
step:2185/2245 train_time:133931ms step_avg:61.30ms
step:2186/2245 train_time:133991ms step_avg:61.30ms
step:2187/2245 train_time:134054ms step_avg:61.30ms
step:2188/2245 train_time:134114ms step_avg:61.30ms
step:2189/2245 train_time:134177ms step_avg:61.30ms
step:2190/2245 train_time:134237ms step_avg:61.30ms
step:2191/2245 train_time:134300ms step_avg:61.30ms
step:2192/2245 train_time:134362ms step_avg:61.30ms
step:2193/2245 train_time:134425ms step_avg:61.30ms
step:2194/2245 train_time:134485ms step_avg:61.30ms
step:2195/2245 train_time:134547ms step_avg:61.30ms
step:2196/2245 train_time:134607ms step_avg:61.30ms
step:2197/2245 train_time:134669ms step_avg:61.30ms
step:2198/2245 train_time:134730ms step_avg:61.30ms
step:2199/2245 train_time:134793ms step_avg:61.30ms
step:2200/2245 train_time:134853ms step_avg:61.30ms
step:2201/2245 train_time:134917ms step_avg:61.30ms
step:2202/2245 train_time:134978ms step_avg:61.30ms
step:2203/2245 train_time:135042ms step_avg:61.30ms
step:2204/2245 train_time:135103ms step_avg:61.30ms
step:2205/2245 train_time:135167ms step_avg:61.30ms
step:2206/2245 train_time:135228ms step_avg:61.30ms
step:2207/2245 train_time:135291ms step_avg:61.30ms
step:2208/2245 train_time:135351ms step_avg:61.30ms
step:2209/2245 train_time:135415ms step_avg:61.30ms
step:2210/2245 train_time:135475ms step_avg:61.30ms
step:2211/2245 train_time:135538ms step_avg:61.30ms
step:2212/2245 train_time:135599ms step_avg:61.30ms
step:2213/2245 train_time:135663ms step_avg:61.30ms
step:2214/2245 train_time:135723ms step_avg:61.30ms
step:2215/2245 train_time:135787ms step_avg:61.30ms
step:2216/2245 train_time:135847ms step_avg:61.30ms
step:2217/2245 train_time:135910ms step_avg:61.30ms
step:2218/2245 train_time:135971ms step_avg:61.30ms
step:2219/2245 train_time:136033ms step_avg:61.30ms
step:2220/2245 train_time:136094ms step_avg:61.30ms
step:2221/2245 train_time:136158ms step_avg:61.30ms
step:2222/2245 train_time:136219ms step_avg:61.30ms
step:2223/2245 train_time:136282ms step_avg:61.31ms
step:2224/2245 train_time:136343ms step_avg:61.31ms
step:2225/2245 train_time:136406ms step_avg:61.31ms
step:2226/2245 train_time:136467ms step_avg:61.31ms
step:2227/2245 train_time:136530ms step_avg:61.31ms
step:2228/2245 train_time:136590ms step_avg:61.31ms
step:2229/2245 train_time:136653ms step_avg:61.31ms
step:2230/2245 train_time:136714ms step_avg:61.31ms
step:2231/2245 train_time:136778ms step_avg:61.31ms
step:2232/2245 train_time:136838ms step_avg:61.31ms
step:2233/2245 train_time:136902ms step_avg:61.31ms
step:2234/2245 train_time:136962ms step_avg:61.31ms
step:2235/2245 train_time:137026ms step_avg:61.31ms
step:2236/2245 train_time:137086ms step_avg:61.31ms
step:2237/2245 train_time:137149ms step_avg:61.31ms
step:2238/2245 train_time:137209ms step_avg:61.31ms
step:2239/2245 train_time:137273ms step_avg:61.31ms
step:2240/2245 train_time:137333ms step_avg:61.31ms
step:2241/2245 train_time:137396ms step_avg:61.31ms
step:2242/2245 train_time:137456ms step_avg:61.31ms
step:2243/2245 train_time:137520ms step_avg:61.31ms
step:2244/2245 train_time:137581ms step_avg:61.31ms
step:2245/2245 train_time:137644ms step_avg:61.31ms
step:2245/2245 val_loss:3.2783 train_time:137705ms step_avg:61.34ms
peak memory allocated: 29249 MiB reserved: 50528 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Nov 10 22:01:45 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            130W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   41C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   34C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   40C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2245 train_time:115ms step_avg:115.15ms
step:2/2245 train_time:136ms step_avg:68.22ms
step:3/2245 train_time:175ms step_avg:58.21ms
step:4/2245 train_time:231ms step_avg:57.75ms
step:5/2245 train_time:291ms step_avg:58.15ms
step:6/2245 train_time:350ms step_avg:58.32ms
step:7/2245 train_time:411ms step_avg:58.72ms
step:8/2245 train_time:469ms step_avg:58.67ms
step:9/2245 train_time:530ms step_avg:58.91ms
step:10/2245 train_time:589ms step_avg:58.89ms
step:11/2245 train_time:650ms step_avg:59.12ms
step:12/2245 train_time:709ms step_avg:59.06ms
step:13/2245 train_time:770ms step_avg:59.22ms
step:14/2245 train_time:828ms step_avg:59.16ms
step:15/2245 train_time:889ms step_avg:59.29ms
step:16/2245 train_time:949ms step_avg:59.30ms
step:17/2245 train_time:1013ms step_avg:59.62ms
step:18/2245 train_time:1077ms step_avg:59.84ms
step:19/2245 train_time:1142ms step_avg:60.13ms
step:20/2245 train_time:1203ms step_avg:60.13ms
step:21/2245 train_time:1265ms step_avg:60.24ms
step:22/2245 train_time:1324ms step_avg:60.18ms
step:23/2245 train_time:1386ms step_avg:60.25ms
step:24/2245 train_time:1445ms step_avg:60.20ms
step:25/2245 train_time:1506ms step_avg:60.24ms
step:26/2245 train_time:1565ms step_avg:60.20ms
step:27/2245 train_time:1627ms step_avg:60.24ms
step:28/2245 train_time:1686ms step_avg:60.20ms
step:29/2245 train_time:1747ms step_avg:60.24ms
step:30/2245 train_time:1806ms step_avg:60.19ms
step:31/2245 train_time:1867ms step_avg:60.23ms
step:32/2245 train_time:1926ms step_avg:60.19ms
step:33/2245 train_time:1989ms step_avg:60.29ms
step:34/2245 train_time:2050ms step_avg:60.31ms
step:35/2245 train_time:2115ms step_avg:60.41ms
step:36/2245 train_time:2175ms step_avg:60.40ms
step:37/2245 train_time:2237ms step_avg:60.45ms
step:38/2245 train_time:2297ms step_avg:60.44ms
step:39/2245 train_time:2360ms step_avg:60.50ms
step:40/2245 train_time:2419ms step_avg:60.47ms
step:41/2245 train_time:2481ms step_avg:60.51ms
step:42/2245 train_time:2540ms step_avg:60.48ms
step:43/2245 train_time:2602ms step_avg:60.51ms
step:44/2245 train_time:2661ms step_avg:60.48ms
step:45/2245 train_time:2723ms step_avg:60.51ms
step:46/2245 train_time:2781ms step_avg:60.47ms
step:47/2245 train_time:2843ms step_avg:60.49ms
step:48/2245 train_time:2902ms step_avg:60.46ms
step:49/2245 train_time:2963ms step_avg:60.47ms
step:50/2245 train_time:3022ms step_avg:60.44ms
step:51/2245 train_time:3085ms step_avg:60.49ms
step:52/2245 train_time:3145ms step_avg:60.49ms
step:53/2245 train_time:3209ms step_avg:60.54ms
step:54/2245 train_time:3268ms step_avg:60.52ms
step:55/2245 train_time:3330ms step_avg:60.55ms
step:56/2245 train_time:3390ms step_avg:60.53ms
step:57/2245 train_time:3452ms step_avg:60.55ms
step:58/2245 train_time:3511ms step_avg:60.54ms
step:59/2245 train_time:3573ms step_avg:60.57ms
step:60/2245 train_time:3632ms step_avg:60.54ms
step:61/2245 train_time:3695ms step_avg:60.57ms
step:62/2245 train_time:3755ms step_avg:60.57ms
step:63/2245 train_time:3818ms step_avg:60.60ms
step:64/2245 train_time:3877ms step_avg:60.58ms
step:65/2245 train_time:3939ms step_avg:60.60ms
step:66/2245 train_time:3998ms step_avg:60.58ms
step:67/2245 train_time:4062ms step_avg:60.62ms
step:68/2245 train_time:4120ms step_avg:60.59ms
step:69/2245 train_time:4182ms step_avg:60.61ms
step:70/2245 train_time:4242ms step_avg:60.60ms
step:71/2245 train_time:4303ms step_avg:60.61ms
step:72/2245 train_time:4363ms step_avg:60.60ms
step:73/2245 train_time:4424ms step_avg:60.60ms
step:74/2245 train_time:4483ms step_avg:60.58ms
step:75/2245 train_time:4544ms step_avg:60.59ms
step:76/2245 train_time:4604ms step_avg:60.57ms
step:77/2245 train_time:4665ms step_avg:60.59ms
step:78/2245 train_time:4724ms step_avg:60.57ms
step:79/2245 train_time:4787ms step_avg:60.59ms
step:80/2245 train_time:4846ms step_avg:60.58ms
step:81/2245 train_time:4909ms step_avg:60.60ms
step:82/2245 train_time:4969ms step_avg:60.59ms
step:83/2245 train_time:5031ms step_avg:60.61ms
step:84/2245 train_time:5090ms step_avg:60.60ms
step:85/2245 train_time:5152ms step_avg:60.62ms
step:86/2245 train_time:5212ms step_avg:60.61ms
step:87/2245 train_time:5275ms step_avg:60.63ms
step:88/2245 train_time:5334ms step_avg:60.62ms
step:89/2245 train_time:5396ms step_avg:60.63ms
step:90/2245 train_time:5456ms step_avg:60.62ms
step:91/2245 train_time:5518ms step_avg:60.63ms
step:92/2245 train_time:5577ms step_avg:60.62ms
step:93/2245 train_time:5638ms step_avg:60.63ms
step:94/2245 train_time:5698ms step_avg:60.62ms
step:95/2245 train_time:5761ms step_avg:60.64ms
step:96/2245 train_time:5820ms step_avg:60.62ms
step:97/2245 train_time:5882ms step_avg:60.63ms
step:98/2245 train_time:5940ms step_avg:60.61ms
step:99/2245 train_time:6001ms step_avg:60.62ms
step:100/2245 train_time:6060ms step_avg:60.60ms
step:101/2245 train_time:6122ms step_avg:60.61ms
step:102/2245 train_time:6181ms step_avg:60.59ms
step:103/2245 train_time:6242ms step_avg:60.61ms
step:104/2245 train_time:6301ms step_avg:60.59ms
step:105/2245 train_time:6363ms step_avg:60.60ms
step:106/2245 train_time:6422ms step_avg:60.58ms
step:107/2245 train_time:6483ms step_avg:60.59ms
step:108/2245 train_time:6542ms step_avg:60.58ms
step:109/2245 train_time:6605ms step_avg:60.59ms
step:110/2245 train_time:6664ms step_avg:60.58ms
step:111/2245 train_time:6726ms step_avg:60.59ms
step:112/2245 train_time:6785ms step_avg:60.58ms
step:113/2245 train_time:6847ms step_avg:60.59ms
step:114/2245 train_time:6906ms step_avg:60.58ms
step:115/2245 train_time:6967ms step_avg:60.58ms
step:116/2245 train_time:7026ms step_avg:60.57ms
step:117/2245 train_time:7088ms step_avg:60.58ms
step:118/2245 train_time:7147ms step_avg:60.57ms
step:119/2245 train_time:7209ms step_avg:60.58ms
step:120/2245 train_time:7268ms step_avg:60.56ms
step:121/2245 train_time:7330ms step_avg:60.58ms
step:122/2245 train_time:7389ms step_avg:60.57ms
step:123/2245 train_time:7451ms step_avg:60.58ms
step:124/2245 train_time:7510ms step_avg:60.57ms
step:125/2245 train_time:7572ms step_avg:60.58ms
step:126/2245 train_time:7631ms step_avg:60.57ms
step:127/2245 train_time:7693ms step_avg:60.58ms
step:128/2245 train_time:7753ms step_avg:60.57ms
step:129/2245 train_time:7815ms step_avg:60.58ms
step:130/2245 train_time:7874ms step_avg:60.57ms
step:131/2245 train_time:7936ms step_avg:60.58ms
step:132/2245 train_time:7995ms step_avg:60.57ms
step:133/2245 train_time:8057ms step_avg:60.58ms
step:134/2245 train_time:8117ms step_avg:60.57ms
step:135/2245 train_time:8179ms step_avg:60.59ms
step:136/2245 train_time:8238ms step_avg:60.57ms
step:137/2245 train_time:8300ms step_avg:60.58ms
step:138/2245 train_time:8359ms step_avg:60.57ms
step:139/2245 train_time:8420ms step_avg:60.58ms
step:140/2245 train_time:8479ms step_avg:60.56ms
step:141/2245 train_time:8540ms step_avg:60.57ms
step:142/2245 train_time:8599ms step_avg:60.56ms
step:143/2245 train_time:8661ms step_avg:60.57ms
step:144/2245 train_time:8720ms step_avg:60.56ms
step:145/2245 train_time:8782ms step_avg:60.56ms
step:146/2245 train_time:8840ms step_avg:60.55ms
step:147/2245 train_time:8902ms step_avg:60.56ms
step:148/2245 train_time:8961ms step_avg:60.54ms
step:149/2245 train_time:9022ms step_avg:60.55ms
step:150/2245 train_time:9081ms step_avg:60.54ms
step:151/2245 train_time:9142ms step_avg:60.54ms
step:152/2245 train_time:9201ms step_avg:60.53ms
step:153/2245 train_time:9262ms step_avg:60.54ms
step:154/2245 train_time:9321ms step_avg:60.52ms
step:155/2245 train_time:9382ms step_avg:60.53ms
step:156/2245 train_time:9441ms step_avg:60.52ms
step:157/2245 train_time:9502ms step_avg:60.52ms
step:158/2245 train_time:9561ms step_avg:60.51ms
step:159/2245 train_time:9622ms step_avg:60.52ms
step:160/2245 train_time:9681ms step_avg:60.50ms
step:161/2245 train_time:9742ms step_avg:60.51ms
step:162/2245 train_time:9800ms step_avg:60.50ms
step:163/2245 train_time:9862ms step_avg:60.50ms
step:164/2245 train_time:9920ms step_avg:60.49ms
step:165/2245 train_time:9981ms step_avg:60.49ms
step:166/2245 train_time:10040ms step_avg:60.48ms
step:167/2245 train_time:10101ms step_avg:60.49ms
step:168/2245 train_time:10161ms step_avg:60.48ms
step:169/2245 train_time:10222ms step_avg:60.49ms
step:170/2245 train_time:10281ms step_avg:60.47ms
step:171/2245 train_time:10342ms step_avg:60.48ms
step:172/2245 train_time:10401ms step_avg:60.47ms
step:173/2245 train_time:10462ms step_avg:60.47ms
step:174/2245 train_time:10520ms step_avg:60.46ms
step:175/2245 train_time:10582ms step_avg:60.47ms
step:176/2245 train_time:10641ms step_avg:60.46ms
step:177/2245 train_time:10702ms step_avg:60.46ms
step:178/2245 train_time:10761ms step_avg:60.46ms
step:179/2245 train_time:10822ms step_avg:60.46ms
step:180/2245 train_time:10881ms step_avg:60.45ms
step:181/2245 train_time:10942ms step_avg:60.45ms
step:182/2245 train_time:11001ms step_avg:60.44ms
step:183/2245 train_time:11062ms step_avg:60.45ms
step:184/2245 train_time:11121ms step_avg:60.44ms
step:185/2245 train_time:11182ms step_avg:60.44ms
step:186/2245 train_time:11240ms step_avg:60.43ms
step:187/2245 train_time:11301ms step_avg:60.43ms
step:188/2245 train_time:11360ms step_avg:60.42ms
step:189/2245 train_time:11421ms step_avg:60.43ms
step:190/2245 train_time:11480ms step_avg:60.42ms
step:191/2245 train_time:11541ms step_avg:60.43ms
step:192/2245 train_time:11600ms step_avg:60.42ms
step:193/2245 train_time:11661ms step_avg:60.42ms
step:194/2245 train_time:11719ms step_avg:60.41ms
step:195/2245 train_time:11781ms step_avg:60.41ms
step:196/2245 train_time:11839ms step_avg:60.40ms
step:197/2245 train_time:11901ms step_avg:60.41ms
step:198/2245 train_time:11959ms step_avg:60.40ms
step:199/2245 train_time:12021ms step_avg:60.41ms
step:200/2245 train_time:12080ms step_avg:60.40ms
step:201/2245 train_time:12141ms step_avg:60.40ms
step:202/2245 train_time:12200ms step_avg:60.39ms
step:203/2245 train_time:12261ms step_avg:60.40ms
step:204/2245 train_time:12319ms step_avg:60.39ms
step:205/2245 train_time:12381ms step_avg:60.39ms
step:206/2245 train_time:12440ms step_avg:60.39ms
step:207/2245 train_time:12501ms step_avg:60.39ms
step:208/2245 train_time:12560ms step_avg:60.38ms
step:209/2245 train_time:12622ms step_avg:60.39ms
step:210/2245 train_time:12680ms step_avg:60.38ms
step:211/2245 train_time:12742ms step_avg:60.39ms
step:212/2245 train_time:12800ms step_avg:60.38ms
step:213/2245 train_time:12861ms step_avg:60.38ms
step:214/2245 train_time:12919ms step_avg:60.37ms
step:215/2245 train_time:12981ms step_avg:60.38ms
step:216/2245 train_time:13039ms step_avg:60.37ms
step:217/2245 train_time:13100ms step_avg:60.37ms
step:218/2245 train_time:13159ms step_avg:60.36ms
step:219/2245 train_time:13221ms step_avg:60.37ms
step:220/2245 train_time:13279ms step_avg:60.36ms
step:221/2245 train_time:13340ms step_avg:60.36ms
step:222/2245 train_time:13399ms step_avg:60.36ms
step:223/2245 train_time:13461ms step_avg:60.36ms
step:224/2245 train_time:13520ms step_avg:60.36ms
step:225/2245 train_time:13581ms step_avg:60.36ms
step:226/2245 train_time:13640ms step_avg:60.35ms
step:227/2245 train_time:13702ms step_avg:60.36ms
step:228/2245 train_time:13760ms step_avg:60.35ms
step:229/2245 train_time:13821ms step_avg:60.36ms
step:230/2245 train_time:13880ms step_avg:60.35ms
step:231/2245 train_time:13941ms step_avg:60.35ms
step:232/2245 train_time:13999ms step_avg:60.34ms
step:233/2245 train_time:14061ms step_avg:60.35ms
step:234/2245 train_time:14120ms step_avg:60.34ms
step:235/2245 train_time:14181ms step_avg:60.35ms
step:236/2245 train_time:14240ms step_avg:60.34ms
step:237/2245 train_time:14301ms step_avg:60.34ms
step:238/2245 train_time:14360ms step_avg:60.34ms
step:239/2245 train_time:14421ms step_avg:60.34ms
step:240/2245 train_time:14480ms step_avg:60.33ms
step:241/2245 train_time:14542ms step_avg:60.34ms
step:242/2245 train_time:14600ms step_avg:60.33ms
step:243/2245 train_time:14662ms step_avg:60.34ms
step:244/2245 train_time:14720ms step_avg:60.33ms
step:245/2245 train_time:14781ms step_avg:60.33ms
step:246/2245 train_time:14840ms step_avg:60.33ms
step:247/2245 train_time:14901ms step_avg:60.33ms
step:248/2245 train_time:14959ms step_avg:60.32ms
step:249/2245 train_time:15021ms step_avg:60.33ms
step:250/2245 train_time:15080ms step_avg:60.32ms
step:250/2245 val_loss:4.0956 train_time:15142ms step_avg:60.57ms
step:251/2245 train_time:15161ms step_avg:60.40ms
step:252/2245 train_time:15203ms step_avg:60.33ms
step:253/2245 train_time:15269ms step_avg:60.35ms
step:254/2245 train_time:15331ms step_avg:60.36ms
step:255/2245 train_time:15394ms step_avg:60.37ms
step:256/2245 train_time:15454ms step_avg:60.37ms
step:257/2245 train_time:15514ms step_avg:60.37ms
step:258/2245 train_time:15573ms step_avg:60.36ms
step:259/2245 train_time:15634ms step_avg:60.36ms
step:260/2245 train_time:15692ms step_avg:60.35ms
step:261/2245 train_time:15752ms step_avg:60.35ms
step:262/2245 train_time:15811ms step_avg:60.35ms
step:263/2245 train_time:15871ms step_avg:60.35ms
step:264/2245 train_time:15929ms step_avg:60.34ms
step:265/2245 train_time:15990ms step_avg:60.34ms
step:266/2245 train_time:16048ms step_avg:60.33ms
step:267/2245 train_time:16109ms step_avg:60.33ms
step:268/2245 train_time:16169ms step_avg:60.33ms
step:269/2245 train_time:16233ms step_avg:60.35ms
step:270/2245 train_time:16294ms step_avg:60.35ms
step:271/2245 train_time:16357ms step_avg:60.36ms
step:272/2245 train_time:16416ms step_avg:60.35ms
step:273/2245 train_time:16478ms step_avg:60.36ms
step:274/2245 train_time:16536ms step_avg:60.35ms
step:275/2245 train_time:16598ms step_avg:60.35ms
step:276/2245 train_time:16656ms step_avg:60.35ms
step:277/2245 train_time:16717ms step_avg:60.35ms
step:278/2245 train_time:16776ms step_avg:60.34ms
step:279/2245 train_time:16837ms step_avg:60.35ms
step:280/2245 train_time:16896ms step_avg:60.34ms
step:281/2245 train_time:16957ms step_avg:60.35ms
step:282/2245 train_time:17017ms step_avg:60.34ms
step:283/2245 train_time:17078ms step_avg:60.35ms
step:284/2245 train_time:17138ms step_avg:60.34ms
step:285/2245 train_time:17200ms step_avg:60.35ms
step:286/2245 train_time:17260ms step_avg:60.35ms
step:287/2245 train_time:17322ms step_avg:60.35ms
step:288/2245 train_time:17380ms step_avg:60.35ms
step:289/2245 train_time:17442ms step_avg:60.35ms
step:290/2245 train_time:17501ms step_avg:60.35ms
step:291/2245 train_time:17562ms step_avg:60.35ms
step:292/2245 train_time:17621ms step_avg:60.34ms
step:293/2245 train_time:17682ms step_avg:60.35ms
step:294/2245 train_time:17741ms step_avg:60.34ms
step:295/2245 train_time:17802ms step_avg:60.34ms
step:296/2245 train_time:17860ms step_avg:60.34ms
step:297/2245 train_time:17922ms step_avg:60.34ms
step:298/2245 train_time:17981ms step_avg:60.34ms
step:299/2245 train_time:18042ms step_avg:60.34ms
step:300/2245 train_time:18102ms step_avg:60.34ms
step:301/2245 train_time:18163ms step_avg:60.34ms
step:302/2245 train_time:18223ms step_avg:60.34ms
step:303/2245 train_time:18285ms step_avg:60.35ms
step:304/2245 train_time:18343ms step_avg:60.34ms
step:305/2245 train_time:18405ms step_avg:60.34ms
step:306/2245 train_time:18463ms step_avg:60.34ms
step:307/2245 train_time:18524ms step_avg:60.34ms
step:308/2245 train_time:18583ms step_avg:60.33ms
step:309/2245 train_time:18644ms step_avg:60.34ms
step:310/2245 train_time:18703ms step_avg:60.33ms
step:311/2245 train_time:18763ms step_avg:60.33ms
step:312/2245 train_time:18822ms step_avg:60.33ms
step:313/2245 train_time:18884ms step_avg:60.33ms
step:314/2245 train_time:18942ms step_avg:60.32ms
step:315/2245 train_time:19004ms step_avg:60.33ms
step:316/2245 train_time:19063ms step_avg:60.33ms
step:317/2245 train_time:19125ms step_avg:60.33ms
step:318/2245 train_time:19184ms step_avg:60.33ms
step:319/2245 train_time:19246ms step_avg:60.33ms
step:320/2245 train_time:19305ms step_avg:60.33ms
step:321/2245 train_time:19366ms step_avg:60.33ms
step:322/2245 train_time:19424ms step_avg:60.32ms
step:323/2245 train_time:19485ms step_avg:60.33ms
step:324/2245 train_time:19544ms step_avg:60.32ms
step:325/2245 train_time:19605ms step_avg:60.32ms
step:326/2245 train_time:19663ms step_avg:60.32ms
step:327/2245 train_time:19724ms step_avg:60.32ms
step:328/2245 train_time:19783ms step_avg:60.31ms
step:329/2245 train_time:19844ms step_avg:60.32ms
step:330/2245 train_time:19903ms step_avg:60.31ms
step:331/2245 train_time:19964ms step_avg:60.32ms
step:332/2245 train_time:20023ms step_avg:60.31ms
step:333/2245 train_time:20084ms step_avg:60.31ms
step:334/2245 train_time:20143ms step_avg:60.31ms
step:335/2245 train_time:20205ms step_avg:60.31ms
step:336/2245 train_time:20263ms step_avg:60.31ms
step:337/2245 train_time:20325ms step_avg:60.31ms
step:338/2245 train_time:20384ms step_avg:60.31ms
step:339/2245 train_time:20445ms step_avg:60.31ms
step:340/2245 train_time:20503ms step_avg:60.30ms
step:341/2245 train_time:20565ms step_avg:60.31ms
step:342/2245 train_time:20623ms step_avg:60.30ms
step:343/2245 train_time:20684ms step_avg:60.30ms
step:344/2245 train_time:20742ms step_avg:60.30ms
step:345/2245 train_time:20804ms step_avg:60.30ms
step:346/2245 train_time:20863ms step_avg:60.30ms
step:347/2245 train_time:20924ms step_avg:60.30ms
step:348/2245 train_time:20982ms step_avg:60.29ms
step:349/2245 train_time:21044ms step_avg:60.30ms
step:350/2245 train_time:21103ms step_avg:60.29ms
step:351/2245 train_time:21164ms step_avg:60.30ms
step:352/2245 train_time:21223ms step_avg:60.29ms
step:353/2245 train_time:21285ms step_avg:60.30ms
step:354/2245 train_time:21343ms step_avg:60.29ms
step:355/2245 train_time:21404ms step_avg:60.29ms
step:356/2245 train_time:21463ms step_avg:60.29ms
step:357/2245 train_time:21524ms step_avg:60.29ms
step:358/2245 train_time:21583ms step_avg:60.29ms
step:359/2245 train_time:21644ms step_avg:60.29ms
step:360/2245 train_time:21702ms step_avg:60.28ms
step:361/2245 train_time:21764ms step_avg:60.29ms
step:362/2245 train_time:21823ms step_avg:60.28ms
step:363/2245 train_time:21884ms step_avg:60.29ms
step:364/2245 train_time:21942ms step_avg:60.28ms
step:365/2245 train_time:22004ms step_avg:60.29ms
step:366/2245 train_time:22063ms step_avg:60.28ms
step:367/2245 train_time:22124ms step_avg:60.28ms
step:368/2245 train_time:22183ms step_avg:60.28ms
step:369/2245 train_time:22244ms step_avg:60.28ms
step:370/2245 train_time:22303ms step_avg:60.28ms
step:371/2245 train_time:22364ms step_avg:60.28ms
step:372/2245 train_time:22423ms step_avg:60.28ms
step:373/2245 train_time:22484ms step_avg:60.28ms
step:374/2245 train_time:22543ms step_avg:60.28ms
step:375/2245 train_time:22604ms step_avg:60.28ms
step:376/2245 train_time:22663ms step_avg:60.27ms
step:377/2245 train_time:22724ms step_avg:60.28ms
step:378/2245 train_time:22783ms step_avg:60.27ms
step:379/2245 train_time:22844ms step_avg:60.28ms
step:380/2245 train_time:22903ms step_avg:60.27ms
step:381/2245 train_time:22964ms step_avg:60.27ms
step:382/2245 train_time:23023ms step_avg:60.27ms
step:383/2245 train_time:23084ms step_avg:60.27ms
step:384/2245 train_time:23143ms step_avg:60.27ms
step:385/2245 train_time:23205ms step_avg:60.27ms
step:386/2245 train_time:23264ms step_avg:60.27ms
step:387/2245 train_time:23325ms step_avg:60.27ms
step:388/2245 train_time:23384ms step_avg:60.27ms
step:389/2245 train_time:23445ms step_avg:60.27ms
step:390/2245 train_time:23504ms step_avg:60.27ms
step:391/2245 train_time:23565ms step_avg:60.27ms
step:392/2245 train_time:23623ms step_avg:60.26ms
step:393/2245 train_time:23684ms step_avg:60.27ms
step:394/2245 train_time:23743ms step_avg:60.26ms
step:395/2245 train_time:23804ms step_avg:60.26ms
step:396/2245 train_time:23863ms step_avg:60.26ms
step:397/2245 train_time:23924ms step_avg:60.26ms
step:398/2245 train_time:23983ms step_avg:60.26ms
step:399/2245 train_time:24045ms step_avg:60.26ms
step:400/2245 train_time:24104ms step_avg:60.26ms
step:401/2245 train_time:24165ms step_avg:60.26ms
step:402/2245 train_time:24223ms step_avg:60.26ms
step:403/2245 train_time:24285ms step_avg:60.26ms
step:404/2245 train_time:24344ms step_avg:60.26ms
step:405/2245 train_time:24405ms step_avg:60.26ms
step:406/2245 train_time:24463ms step_avg:60.25ms
step:407/2245 train_time:24525ms step_avg:60.26ms
step:408/2245 train_time:24584ms step_avg:60.25ms
step:409/2245 train_time:24645ms step_avg:60.26ms
step:410/2245 train_time:24704ms step_avg:60.25ms
step:411/2245 train_time:24765ms step_avg:60.26ms
step:412/2245 train_time:24823ms step_avg:60.25ms
step:413/2245 train_time:24885ms step_avg:60.25ms
step:414/2245 train_time:24943ms step_avg:60.25ms
step:415/2245 train_time:25005ms step_avg:60.25ms
step:416/2245 train_time:25063ms step_avg:60.25ms
step:417/2245 train_time:25125ms step_avg:60.25ms
step:418/2245 train_time:25184ms step_avg:60.25ms
step:419/2245 train_time:25246ms step_avg:60.25ms
step:420/2245 train_time:25304ms step_avg:60.25ms
step:421/2245 train_time:25365ms step_avg:60.25ms
step:422/2245 train_time:25424ms step_avg:60.25ms
step:423/2245 train_time:25485ms step_avg:60.25ms
step:424/2245 train_time:25543ms step_avg:60.24ms
step:425/2245 train_time:25604ms step_avg:60.25ms
step:426/2245 train_time:25663ms step_avg:60.24ms
step:427/2245 train_time:25724ms step_avg:60.24ms
step:428/2245 train_time:25783ms step_avg:60.24ms
step:429/2245 train_time:25844ms step_avg:60.24ms
step:430/2245 train_time:25903ms step_avg:60.24ms
step:431/2245 train_time:25964ms step_avg:60.24ms
step:432/2245 train_time:26023ms step_avg:60.24ms
step:433/2245 train_time:26084ms step_avg:60.24ms
step:434/2245 train_time:26144ms step_avg:60.24ms
step:435/2245 train_time:26205ms step_avg:60.24ms
step:436/2245 train_time:26263ms step_avg:60.24ms
step:437/2245 train_time:26325ms step_avg:60.24ms
step:438/2245 train_time:26383ms step_avg:60.24ms
step:439/2245 train_time:26444ms step_avg:60.24ms
step:440/2245 train_time:26503ms step_avg:60.23ms
step:441/2245 train_time:26564ms step_avg:60.24ms
step:442/2245 train_time:26623ms step_avg:60.23ms
step:443/2245 train_time:26684ms step_avg:60.23ms
step:444/2245 train_time:26743ms step_avg:60.23ms
step:445/2245 train_time:26805ms step_avg:60.24ms
step:446/2245 train_time:26863ms step_avg:60.23ms
step:447/2245 train_time:26924ms step_avg:60.23ms
step:448/2245 train_time:26983ms step_avg:60.23ms
step:449/2245 train_time:27044ms step_avg:60.23ms
step:450/2245 train_time:27103ms step_avg:60.23ms
step:451/2245 train_time:27165ms step_avg:60.23ms
step:452/2245 train_time:27223ms step_avg:60.23ms
step:453/2245 train_time:27285ms step_avg:60.23ms
step:454/2245 train_time:27343ms step_avg:60.23ms
step:455/2245 train_time:27405ms step_avg:60.23ms
step:456/2245 train_time:27463ms step_avg:60.23ms
step:457/2245 train_time:27524ms step_avg:60.23ms
step:458/2245 train_time:27583ms step_avg:60.22ms
step:459/2245 train_time:27644ms step_avg:60.23ms
step:460/2245 train_time:27703ms step_avg:60.22ms
step:461/2245 train_time:27765ms step_avg:60.23ms
step:462/2245 train_time:27823ms step_avg:60.22ms
step:463/2245 train_time:27884ms step_avg:60.23ms
step:464/2245 train_time:27943ms step_avg:60.22ms
step:465/2245 train_time:28004ms step_avg:60.22ms
step:466/2245 train_time:28063ms step_avg:60.22ms
step:467/2245 train_time:28124ms step_avg:60.22ms
step:468/2245 train_time:28183ms step_avg:60.22ms
step:469/2245 train_time:28245ms step_avg:60.22ms
step:470/2245 train_time:28304ms step_avg:60.22ms
step:471/2245 train_time:28365ms step_avg:60.22ms
step:472/2245 train_time:28424ms step_avg:60.22ms
step:473/2245 train_time:28485ms step_avg:60.22ms
step:474/2245 train_time:28543ms step_avg:60.22ms
step:475/2245 train_time:28605ms step_avg:60.22ms
step:476/2245 train_time:28663ms step_avg:60.22ms
step:477/2245 train_time:28725ms step_avg:60.22ms
step:478/2245 train_time:28784ms step_avg:60.22ms
step:479/2245 train_time:28845ms step_avg:60.22ms
step:480/2245 train_time:28904ms step_avg:60.22ms
step:481/2245 train_time:28966ms step_avg:60.22ms
step:482/2245 train_time:29024ms step_avg:60.22ms
step:483/2245 train_time:29085ms step_avg:60.22ms
step:484/2245 train_time:29144ms step_avg:60.21ms
step:485/2245 train_time:29205ms step_avg:60.22ms
step:486/2245 train_time:29264ms step_avg:60.21ms
step:487/2245 train_time:29325ms step_avg:60.22ms
step:488/2245 train_time:29384ms step_avg:60.21ms
step:489/2245 train_time:29445ms step_avg:60.21ms
step:490/2245 train_time:29504ms step_avg:60.21ms
step:491/2245 train_time:29565ms step_avg:60.21ms
step:492/2245 train_time:29623ms step_avg:60.21ms
step:493/2245 train_time:29684ms step_avg:60.21ms
step:494/2245 train_time:29743ms step_avg:60.21ms
step:495/2245 train_time:29805ms step_avg:60.21ms
step:496/2245 train_time:29864ms step_avg:60.21ms
step:497/2245 train_time:29925ms step_avg:60.21ms
step:498/2245 train_time:29984ms step_avg:60.21ms
step:499/2245 train_time:30045ms step_avg:60.21ms
step:500/2245 train_time:30104ms step_avg:60.21ms
step:500/2245 val_loss:3.8197 train_time:30166ms step_avg:60.33ms
step:501/2245 train_time:30184ms step_avg:60.25ms
step:502/2245 train_time:30227ms step_avg:60.21ms
step:503/2245 train_time:30293ms step_avg:60.22ms
step:504/2245 train_time:30353ms step_avg:60.22ms
step:505/2245 train_time:30414ms step_avg:60.23ms
step:506/2245 train_time:30473ms step_avg:60.22ms
step:507/2245 train_time:30534ms step_avg:60.22ms
step:508/2245 train_time:30592ms step_avg:60.22ms
step:509/2245 train_time:30653ms step_avg:60.22ms
step:510/2245 train_time:30711ms step_avg:60.22ms
step:511/2245 train_time:30771ms step_avg:60.22ms
step:512/2245 train_time:30829ms step_avg:60.21ms
step:513/2245 train_time:30890ms step_avg:60.21ms
step:514/2245 train_time:30949ms step_avg:60.21ms
step:515/2245 train_time:31009ms step_avg:60.21ms
step:516/2245 train_time:31068ms step_avg:60.21ms
step:517/2245 train_time:31130ms step_avg:60.21ms
step:518/2245 train_time:31190ms step_avg:60.21ms
step:519/2245 train_time:31253ms step_avg:60.22ms
step:520/2245 train_time:31313ms step_avg:60.22ms
step:521/2245 train_time:31374ms step_avg:60.22ms
step:522/2245 train_time:31433ms step_avg:60.22ms
step:523/2245 train_time:31495ms step_avg:60.22ms
step:524/2245 train_time:31554ms step_avg:60.22ms
step:525/2245 train_time:31615ms step_avg:60.22ms
step:526/2245 train_time:31673ms step_avg:60.22ms
step:527/2245 train_time:31734ms step_avg:60.22ms
step:528/2245 train_time:31792ms step_avg:60.21ms
step:529/2245 train_time:31854ms step_avg:60.21ms
step:530/2245 train_time:31912ms step_avg:60.21ms
step:531/2245 train_time:31973ms step_avg:60.21ms
step:532/2245 train_time:32032ms step_avg:60.21ms
step:533/2245 train_time:32094ms step_avg:60.21ms
step:534/2245 train_time:32154ms step_avg:60.21ms
step:535/2245 train_time:32216ms step_avg:60.22ms
step:536/2245 train_time:32275ms step_avg:60.22ms
step:537/2245 train_time:32338ms step_avg:60.22ms
step:538/2245 train_time:32397ms step_avg:60.22ms
step:539/2245 train_time:32459ms step_avg:60.22ms
step:540/2245 train_time:32518ms step_avg:60.22ms
step:541/2245 train_time:32579ms step_avg:60.22ms
step:542/2245 train_time:32638ms step_avg:60.22ms
step:543/2245 train_time:32699ms step_avg:60.22ms
step:544/2245 train_time:32758ms step_avg:60.22ms
step:545/2245 train_time:32819ms step_avg:60.22ms
step:546/2245 train_time:32877ms step_avg:60.21ms
step:547/2245 train_time:32939ms step_avg:60.22ms
step:548/2245 train_time:32998ms step_avg:60.21ms
step:549/2245 train_time:33060ms step_avg:60.22ms
step:550/2245 train_time:33119ms step_avg:60.22ms
step:551/2245 train_time:33181ms step_avg:60.22ms
step:552/2245 train_time:33241ms step_avg:60.22ms
step:553/2245 train_time:33303ms step_avg:60.22ms
step:554/2245 train_time:33361ms step_avg:60.22ms
step:555/2245 train_time:33423ms step_avg:60.22ms
step:556/2245 train_time:33482ms step_avg:60.22ms
step:557/2245 train_time:33544ms step_avg:60.22ms
step:558/2245 train_time:33604ms step_avg:60.22ms
step:559/2245 train_time:33666ms step_avg:60.23ms
step:560/2245 train_time:33725ms step_avg:60.22ms
step:561/2245 train_time:33787ms step_avg:60.23ms
step:562/2245 train_time:33846ms step_avg:60.22ms
step:563/2245 train_time:33908ms step_avg:60.23ms
step:564/2245 train_time:33967ms step_avg:60.23ms
step:565/2245 train_time:34030ms step_avg:60.23ms
step:566/2245 train_time:34089ms step_avg:60.23ms
step:567/2245 train_time:34150ms step_avg:60.23ms
step:568/2245 train_time:34209ms step_avg:60.23ms
step:569/2245 train_time:34270ms step_avg:60.23ms
step:570/2245 train_time:34329ms step_avg:60.23ms
step:571/2245 train_time:34391ms step_avg:60.23ms
step:572/2245 train_time:34450ms step_avg:60.23ms
step:573/2245 train_time:34512ms step_avg:60.23ms
step:574/2245 train_time:34571ms step_avg:60.23ms
step:575/2245 train_time:34632ms step_avg:60.23ms
step:576/2245 train_time:34691ms step_avg:60.23ms
step:577/2245 train_time:34752ms step_avg:60.23ms
step:578/2245 train_time:34810ms step_avg:60.23ms
step:579/2245 train_time:34872ms step_avg:60.23ms
step:580/2245 train_time:34930ms step_avg:60.22ms
step:581/2245 train_time:34992ms step_avg:60.23ms
step:582/2245 train_time:35051ms step_avg:60.22ms
step:583/2245 train_time:35112ms step_avg:60.23ms
step:584/2245 train_time:35171ms step_avg:60.22ms
step:585/2245 train_time:35232ms step_avg:60.23ms
step:586/2245 train_time:35291ms step_avg:60.22ms
step:587/2245 train_time:35352ms step_avg:60.23ms
step:588/2245 train_time:35411ms step_avg:60.22ms
step:589/2245 train_time:35472ms step_avg:60.22ms
step:590/2245 train_time:35531ms step_avg:60.22ms
step:591/2245 train_time:35593ms step_avg:60.22ms
step:592/2245 train_time:35651ms step_avg:60.22ms
step:593/2245 train_time:35712ms step_avg:60.22ms
step:594/2245 train_time:35771ms step_avg:60.22ms
step:595/2245 train_time:35832ms step_avg:60.22ms
step:596/2245 train_time:35891ms step_avg:60.22ms
step:597/2245 train_time:35952ms step_avg:60.22ms
step:598/2245 train_time:36011ms step_avg:60.22ms
step:599/2245 train_time:36072ms step_avg:60.22ms
step:600/2245 train_time:36131ms step_avg:60.22ms
step:601/2245 train_time:36193ms step_avg:60.22ms
step:602/2245 train_time:36252ms step_avg:60.22ms
step:603/2245 train_time:36313ms step_avg:60.22ms
step:604/2245 train_time:36372ms step_avg:60.22ms
step:605/2245 train_time:36433ms step_avg:60.22ms
step:606/2245 train_time:36492ms step_avg:60.22ms
step:607/2245 train_time:36554ms step_avg:60.22ms
step:608/2245 train_time:36613ms step_avg:60.22ms
step:609/2245 train_time:36674ms step_avg:60.22ms
step:610/2245 train_time:36733ms step_avg:60.22ms
step:611/2245 train_time:36795ms step_avg:60.22ms
step:612/2245 train_time:36855ms step_avg:60.22ms
step:613/2245 train_time:36916ms step_avg:60.22ms
step:614/2245 train_time:36976ms step_avg:60.22ms
step:615/2245 train_time:37038ms step_avg:60.22ms
step:616/2245 train_time:37097ms step_avg:60.22ms
step:617/2245 train_time:37159ms step_avg:60.22ms
step:618/2245 train_time:37218ms step_avg:60.22ms
step:619/2245 train_time:37280ms step_avg:60.23ms
step:620/2245 train_time:37339ms step_avg:60.22ms
step:621/2245 train_time:37401ms step_avg:60.23ms
step:622/2245 train_time:37460ms step_avg:60.23ms
step:623/2245 train_time:37522ms step_avg:60.23ms
step:624/2245 train_time:37581ms step_avg:60.23ms
step:625/2245 train_time:37642ms step_avg:60.23ms
step:626/2245 train_time:37701ms step_avg:60.23ms
step:627/2245 train_time:37763ms step_avg:60.23ms
step:628/2245 train_time:37823ms step_avg:60.23ms
step:629/2245 train_time:37885ms step_avg:60.23ms
step:630/2245 train_time:37945ms step_avg:60.23ms
step:631/2245 train_time:38008ms step_avg:60.23ms
step:632/2245 train_time:38067ms step_avg:60.23ms
step:633/2245 train_time:38129ms step_avg:60.23ms
step:634/2245 train_time:38189ms step_avg:60.23ms
step:635/2245 train_time:38251ms step_avg:60.24ms
step:636/2245 train_time:38309ms step_avg:60.23ms
step:637/2245 train_time:38371ms step_avg:60.24ms
step:638/2245 train_time:38430ms step_avg:60.24ms
step:639/2245 train_time:38492ms step_avg:60.24ms
step:640/2245 train_time:38550ms step_avg:60.23ms
step:641/2245 train_time:38611ms step_avg:60.24ms
step:642/2245 train_time:38669ms step_avg:60.23ms
step:643/2245 train_time:38730ms step_avg:60.23ms
step:644/2245 train_time:38789ms step_avg:60.23ms
step:645/2245 train_time:38851ms step_avg:60.23ms
step:646/2245 train_time:38910ms step_avg:60.23ms
step:647/2245 train_time:38971ms step_avg:60.23ms
step:648/2245 train_time:39030ms step_avg:60.23ms
step:649/2245 train_time:39091ms step_avg:60.23ms
step:650/2245 train_time:39150ms step_avg:60.23ms
step:651/2245 train_time:39211ms step_avg:60.23ms
step:652/2245 train_time:39270ms step_avg:60.23ms
step:653/2245 train_time:39331ms step_avg:60.23ms
step:654/2245 train_time:39390ms step_avg:60.23ms
step:655/2245 train_time:39451ms step_avg:60.23ms
step:656/2245 train_time:39510ms step_avg:60.23ms
step:657/2245 train_time:39571ms step_avg:60.23ms
step:658/2245 train_time:39630ms step_avg:60.23ms
step:659/2245 train_time:39691ms step_avg:60.23ms
step:660/2245 train_time:39750ms step_avg:60.23ms
step:661/2245 train_time:39812ms step_avg:60.23ms
step:662/2245 train_time:39870ms step_avg:60.23ms
step:663/2245 train_time:39932ms step_avg:60.23ms
step:664/2245 train_time:39990ms step_avg:60.23ms
step:665/2245 train_time:40052ms step_avg:60.23ms
step:666/2245 train_time:40111ms step_avg:60.23ms
step:667/2245 train_time:40172ms step_avg:60.23ms
step:668/2245 train_time:40231ms step_avg:60.23ms
step:669/2245 train_time:40292ms step_avg:60.23ms
step:670/2245 train_time:40351ms step_avg:60.22ms
step:671/2245 train_time:40412ms step_avg:60.23ms
step:672/2245 train_time:40471ms step_avg:60.22ms
step:673/2245 train_time:40532ms step_avg:60.23ms
step:674/2245 train_time:40590ms step_avg:60.22ms
step:675/2245 train_time:40652ms step_avg:60.23ms
step:676/2245 train_time:40710ms step_avg:60.22ms
step:677/2245 train_time:40772ms step_avg:60.22ms
step:678/2245 train_time:40830ms step_avg:60.22ms
step:679/2245 train_time:40892ms step_avg:60.22ms
step:680/2245 train_time:40950ms step_avg:60.22ms
step:681/2245 train_time:41012ms step_avg:60.22ms
step:682/2245 train_time:41071ms step_avg:60.22ms
step:683/2245 train_time:41132ms step_avg:60.22ms
step:684/2245 train_time:41191ms step_avg:60.22ms
step:685/2245 train_time:41253ms step_avg:60.22ms
step:686/2245 train_time:41312ms step_avg:60.22ms
step:687/2245 train_time:41374ms step_avg:60.22ms
step:688/2245 train_time:41433ms step_avg:60.22ms
step:689/2245 train_time:41494ms step_avg:60.22ms
step:690/2245 train_time:41553ms step_avg:60.22ms
step:691/2245 train_time:41615ms step_avg:60.22ms
step:692/2245 train_time:41673ms step_avg:60.22ms
step:693/2245 train_time:41735ms step_avg:60.22ms
step:694/2245 train_time:41794ms step_avg:60.22ms
step:695/2245 train_time:41856ms step_avg:60.22ms
step:696/2245 train_time:41914ms step_avg:60.22ms
step:697/2245 train_time:41976ms step_avg:60.22ms
step:698/2245 train_time:42035ms step_avg:60.22ms
step:699/2245 train_time:42097ms step_avg:60.22ms
step:700/2245 train_time:42155ms step_avg:60.22ms
step:701/2245 train_time:42217ms step_avg:60.22ms
step:702/2245 train_time:42276ms step_avg:60.22ms
step:703/2245 train_time:42338ms step_avg:60.22ms
step:704/2245 train_time:42397ms step_avg:60.22ms
step:705/2245 train_time:42459ms step_avg:60.23ms
step:706/2245 train_time:42518ms step_avg:60.22ms
step:707/2245 train_time:42579ms step_avg:60.23ms
step:708/2245 train_time:42638ms step_avg:60.22ms
step:709/2245 train_time:42700ms step_avg:60.23ms
step:710/2245 train_time:42759ms step_avg:60.22ms
step:711/2245 train_time:42822ms step_avg:60.23ms
step:712/2245 train_time:42881ms step_avg:60.23ms
step:713/2245 train_time:42943ms step_avg:60.23ms
step:714/2245 train_time:43003ms step_avg:60.23ms
step:715/2245 train_time:43064ms step_avg:60.23ms
step:716/2245 train_time:43123ms step_avg:60.23ms
step:717/2245 train_time:43186ms step_avg:60.23ms
step:718/2245 train_time:43246ms step_avg:60.23ms
step:719/2245 train_time:43308ms step_avg:60.23ms
step:720/2245 train_time:43367ms step_avg:60.23ms
step:721/2245 train_time:43429ms step_avg:60.23ms
step:722/2245 train_time:43896ms step_avg:60.80ms
step:723/2245 train_time:43955ms step_avg:60.80ms
step:724/2245 train_time:44013ms step_avg:60.79ms
step:725/2245 train_time:44074ms step_avg:60.79ms
step:726/2245 train_time:44132ms step_avg:60.79ms
step:727/2245 train_time:44192ms step_avg:60.79ms
step:728/2245 train_time:44250ms step_avg:60.78ms
step:729/2245 train_time:44311ms step_avg:60.78ms
step:730/2245 train_time:44369ms step_avg:60.78ms
step:731/2245 train_time:44430ms step_avg:60.78ms
step:732/2245 train_time:44488ms step_avg:60.78ms
step:733/2245 train_time:44549ms step_avg:60.78ms
step:734/2245 train_time:44608ms step_avg:60.77ms
step:735/2245 train_time:44668ms step_avg:60.77ms
step:736/2245 train_time:44727ms step_avg:60.77ms
step:737/2245 train_time:44793ms step_avg:60.78ms
step:738/2245 train_time:44858ms step_avg:60.78ms
step:739/2245 train_time:44923ms step_avg:60.79ms
step:740/2245 train_time:44983ms step_avg:60.79ms
step:741/2245 train_time:45047ms step_avg:60.79ms
step:742/2245 train_time:45107ms step_avg:60.79ms
step:743/2245 train_time:45170ms step_avg:60.79ms
step:744/2245 train_time:45229ms step_avg:60.79ms
step:745/2245 train_time:45290ms step_avg:60.79ms
step:746/2245 train_time:45349ms step_avg:60.79ms
step:747/2245 train_time:45411ms step_avg:60.79ms
step:748/2245 train_time:45470ms step_avg:60.79ms
step:749/2245 train_time:45531ms step_avg:60.79ms
step:750/2245 train_time:45590ms step_avg:60.79ms
step:750/2245 val_loss:3.6701 train_time:45653ms step_avg:60.87ms
step:751/2245 train_time:45672ms step_avg:60.82ms
step:752/2245 train_time:45714ms step_avg:60.79ms
step:753/2245 train_time:45775ms step_avg:60.79ms
step:754/2245 train_time:45835ms step_avg:60.79ms
step:755/2245 train_time:45898ms step_avg:60.79ms
step:756/2245 train_time:45958ms step_avg:60.79ms
step:757/2245 train_time:46019ms step_avg:60.79ms
step:758/2245 train_time:46077ms step_avg:60.79ms
step:759/2245 train_time:46138ms step_avg:60.79ms
step:760/2245 train_time:46197ms step_avg:60.79ms
step:761/2245 train_time:46258ms step_avg:60.79ms
step:762/2245 train_time:46317ms step_avg:60.78ms
step:763/2245 train_time:46378ms step_avg:60.78ms
step:764/2245 train_time:46437ms step_avg:60.78ms
step:765/2245 train_time:46498ms step_avg:60.78ms
step:766/2245 train_time:46561ms step_avg:60.79ms
step:767/2245 train_time:46628ms step_avg:60.79ms
step:768/2245 train_time:46689ms step_avg:60.79ms
step:769/2245 train_time:46751ms step_avg:60.79ms
step:770/2245 train_time:46812ms step_avg:60.79ms
step:771/2245 train_time:46873ms step_avg:60.80ms
step:772/2245 train_time:46933ms step_avg:60.79ms
step:773/2245 train_time:46995ms step_avg:60.80ms
step:774/2245 train_time:47054ms step_avg:60.79ms
step:775/2245 train_time:47115ms step_avg:60.79ms
step:776/2245 train_time:47175ms step_avg:60.79ms
step:777/2245 train_time:47237ms step_avg:60.79ms
step:778/2245 train_time:47296ms step_avg:60.79ms
step:779/2245 train_time:47357ms step_avg:60.79ms
step:780/2245 train_time:47416ms step_avg:60.79ms
step:781/2245 train_time:47478ms step_avg:60.79ms
step:782/2245 train_time:47540ms step_avg:60.79ms
step:783/2245 train_time:47603ms step_avg:60.80ms
step:784/2245 train_time:47663ms step_avg:60.79ms
step:785/2245 train_time:47726ms step_avg:60.80ms
step:786/2245 train_time:47786ms step_avg:60.80ms
step:787/2245 train_time:47849ms step_avg:60.80ms
step:788/2245 train_time:47909ms step_avg:60.80ms
step:789/2245 train_time:47971ms step_avg:60.80ms
step:790/2245 train_time:48031ms step_avg:60.80ms
step:791/2245 train_time:48093ms step_avg:60.80ms
step:792/2245 train_time:48153ms step_avg:60.80ms
step:793/2245 train_time:48215ms step_avg:60.80ms
step:794/2245 train_time:48275ms step_avg:60.80ms
step:795/2245 train_time:48336ms step_avg:60.80ms
step:796/2245 train_time:48396ms step_avg:60.80ms
step:797/2245 train_time:48458ms step_avg:60.80ms
step:798/2245 train_time:48517ms step_avg:60.80ms
step:799/2245 train_time:48580ms step_avg:60.80ms
step:800/2245 train_time:48640ms step_avg:60.80ms
step:801/2245 train_time:48703ms step_avg:60.80ms
step:802/2245 train_time:48762ms step_avg:60.80ms
step:803/2245 train_time:48825ms step_avg:60.80ms
step:804/2245 train_time:48885ms step_avg:60.80ms
step:805/2245 train_time:48948ms step_avg:60.80ms
step:806/2245 train_time:49008ms step_avg:60.80ms
step:807/2245 train_time:49070ms step_avg:60.81ms
step:808/2245 train_time:49130ms step_avg:60.80ms
step:809/2245 train_time:49192ms step_avg:60.81ms
step:810/2245 train_time:49252ms step_avg:60.80ms
step:811/2245 train_time:49314ms step_avg:60.81ms
step:812/2245 train_time:49373ms step_avg:60.80ms
step:813/2245 train_time:49436ms step_avg:60.81ms
step:814/2245 train_time:49497ms step_avg:60.81ms
step:815/2245 train_time:49558ms step_avg:60.81ms
step:816/2245 train_time:49618ms step_avg:60.81ms
step:817/2245 train_time:49680ms step_avg:60.81ms
step:818/2245 train_time:49740ms step_avg:60.81ms
step:819/2245 train_time:49802ms step_avg:60.81ms
step:820/2245 train_time:49862ms step_avg:60.81ms
step:821/2245 train_time:49924ms step_avg:60.81ms
step:822/2245 train_time:49984ms step_avg:60.81ms
step:823/2245 train_time:50048ms step_avg:60.81ms
step:824/2245 train_time:50108ms step_avg:60.81ms
step:825/2245 train_time:50170ms step_avg:60.81ms
step:826/2245 train_time:50230ms step_avg:60.81ms
step:827/2245 train_time:50292ms step_avg:60.81ms
step:828/2245 train_time:50351ms step_avg:60.81ms
step:829/2245 train_time:50414ms step_avg:60.81ms
step:830/2245 train_time:50474ms step_avg:60.81ms
step:831/2245 train_time:50537ms step_avg:60.81ms
step:832/2245 train_time:50597ms step_avg:60.81ms
step:833/2245 train_time:50659ms step_avg:60.82ms
step:834/2245 train_time:50719ms step_avg:60.81ms
step:835/2245 train_time:50781ms step_avg:60.82ms
step:836/2245 train_time:50841ms step_avg:60.81ms
step:837/2245 train_time:50903ms step_avg:60.82ms
step:838/2245 train_time:50963ms step_avg:60.81ms
step:839/2245 train_time:51025ms step_avg:60.82ms
step:840/2245 train_time:51086ms step_avg:60.82ms
step:841/2245 train_time:51148ms step_avg:60.82ms
step:842/2245 train_time:51208ms step_avg:60.82ms
step:843/2245 train_time:51271ms step_avg:60.82ms
step:844/2245 train_time:51331ms step_avg:60.82ms
step:845/2245 train_time:51393ms step_avg:60.82ms
step:846/2245 train_time:51453ms step_avg:60.82ms
step:847/2245 train_time:51516ms step_avg:60.82ms
step:848/2245 train_time:51576ms step_avg:60.82ms
step:849/2245 train_time:51639ms step_avg:60.82ms
step:850/2245 train_time:51698ms step_avg:60.82ms
step:851/2245 train_time:51760ms step_avg:60.82ms
step:852/2245 train_time:51821ms step_avg:60.82ms
step:853/2245 train_time:51882ms step_avg:60.82ms
step:854/2245 train_time:51942ms step_avg:60.82ms
step:855/2245 train_time:52004ms step_avg:60.82ms
step:856/2245 train_time:52063ms step_avg:60.82ms
step:857/2245 train_time:52126ms step_avg:60.82ms
step:858/2245 train_time:52185ms step_avg:60.82ms
step:859/2245 train_time:52248ms step_avg:60.82ms
step:860/2245 train_time:52308ms step_avg:60.82ms
step:861/2245 train_time:52371ms step_avg:60.83ms
step:862/2245 train_time:52431ms step_avg:60.83ms
step:863/2245 train_time:52494ms step_avg:60.83ms
step:864/2245 train_time:52554ms step_avg:60.83ms
step:865/2245 train_time:52617ms step_avg:60.83ms
step:866/2245 train_time:52676ms step_avg:60.83ms
step:867/2245 train_time:52739ms step_avg:60.83ms
step:868/2245 train_time:52799ms step_avg:60.83ms
step:869/2245 train_time:52861ms step_avg:60.83ms
step:870/2245 train_time:52921ms step_avg:60.83ms
step:871/2245 train_time:52983ms step_avg:60.83ms
step:872/2245 train_time:53043ms step_avg:60.83ms
step:873/2245 train_time:53105ms step_avg:60.83ms
step:874/2245 train_time:53165ms step_avg:60.83ms
step:875/2245 train_time:53227ms step_avg:60.83ms
step:876/2245 train_time:53287ms step_avg:60.83ms
step:877/2245 train_time:53351ms step_avg:60.83ms
step:878/2245 train_time:53411ms step_avg:60.83ms
step:879/2245 train_time:53473ms step_avg:60.83ms
step:880/2245 train_time:53533ms step_avg:60.83ms
step:881/2245 train_time:53596ms step_avg:60.84ms
step:882/2245 train_time:53656ms step_avg:60.83ms
step:883/2245 train_time:53718ms step_avg:60.84ms
step:884/2245 train_time:53778ms step_avg:60.83ms
step:885/2245 train_time:53840ms step_avg:60.84ms
step:886/2245 train_time:53900ms step_avg:60.84ms
step:887/2245 train_time:53962ms step_avg:60.84ms
step:888/2245 train_time:54022ms step_avg:60.84ms
step:889/2245 train_time:54083ms step_avg:60.84ms
step:890/2245 train_time:54143ms step_avg:60.83ms
step:891/2245 train_time:54206ms step_avg:60.84ms
step:892/2245 train_time:54265ms step_avg:60.84ms
step:893/2245 train_time:54328ms step_avg:60.84ms
step:894/2245 train_time:54387ms step_avg:60.84ms
step:895/2245 train_time:54450ms step_avg:60.84ms
step:896/2245 train_time:54510ms step_avg:60.84ms
step:897/2245 train_time:54572ms step_avg:60.84ms
step:898/2245 train_time:54633ms step_avg:60.84ms
step:899/2245 train_time:54696ms step_avg:60.84ms
step:900/2245 train_time:54756ms step_avg:60.84ms
step:901/2245 train_time:54818ms step_avg:60.84ms
step:902/2245 train_time:54878ms step_avg:60.84ms
step:903/2245 train_time:54940ms step_avg:60.84ms
step:904/2245 train_time:55000ms step_avg:60.84ms
step:905/2245 train_time:55062ms step_avg:60.84ms
step:906/2245 train_time:55122ms step_avg:60.84ms
step:907/2245 train_time:55183ms step_avg:60.84ms
step:908/2245 train_time:55243ms step_avg:60.84ms
step:909/2245 train_time:55306ms step_avg:60.84ms
step:910/2245 train_time:55366ms step_avg:60.84ms
step:911/2245 train_time:55429ms step_avg:60.84ms
step:912/2245 train_time:55488ms step_avg:60.84ms
step:913/2245 train_time:55551ms step_avg:60.84ms
step:914/2245 train_time:55611ms step_avg:60.84ms
step:915/2245 train_time:55673ms step_avg:60.85ms
step:916/2245 train_time:55734ms step_avg:60.84ms
step:917/2245 train_time:55796ms step_avg:60.85ms
step:918/2245 train_time:55856ms step_avg:60.85ms
step:919/2245 train_time:55918ms step_avg:60.85ms
step:920/2245 train_time:55978ms step_avg:60.85ms
step:921/2245 train_time:56040ms step_avg:60.85ms
step:922/2245 train_time:56100ms step_avg:60.85ms
step:923/2245 train_time:56162ms step_avg:60.85ms
step:924/2245 train_time:56222ms step_avg:60.85ms
step:925/2245 train_time:56284ms step_avg:60.85ms
step:926/2245 train_time:56344ms step_avg:60.85ms
step:927/2245 train_time:56406ms step_avg:60.85ms
step:928/2245 train_time:56467ms step_avg:60.85ms
step:929/2245 train_time:56530ms step_avg:60.85ms
step:930/2245 train_time:56590ms step_avg:60.85ms
step:931/2245 train_time:56652ms step_avg:60.85ms
step:932/2245 train_time:56713ms step_avg:60.85ms
step:933/2245 train_time:56775ms step_avg:60.85ms
step:934/2245 train_time:56835ms step_avg:60.85ms
step:935/2245 train_time:56898ms step_avg:60.85ms
step:936/2245 train_time:56958ms step_avg:60.85ms
step:937/2245 train_time:57021ms step_avg:60.85ms
step:938/2245 train_time:57080ms step_avg:60.85ms
step:939/2245 train_time:57142ms step_avg:60.85ms
step:940/2245 train_time:57203ms step_avg:60.85ms
step:941/2245 train_time:57264ms step_avg:60.85ms
step:942/2245 train_time:57324ms step_avg:60.85ms
step:943/2245 train_time:57387ms step_avg:60.86ms
step:944/2245 train_time:57447ms step_avg:60.85ms
step:945/2245 train_time:57510ms step_avg:60.86ms
step:946/2245 train_time:57569ms step_avg:60.86ms
step:947/2245 train_time:57631ms step_avg:60.86ms
step:948/2245 train_time:57692ms step_avg:60.86ms
step:949/2245 train_time:57754ms step_avg:60.86ms
step:950/2245 train_time:57815ms step_avg:60.86ms
step:951/2245 train_time:57877ms step_avg:60.86ms
step:952/2245 train_time:57937ms step_avg:60.86ms
step:953/2245 train_time:58000ms step_avg:60.86ms
step:954/2245 train_time:58059ms step_avg:60.86ms
step:955/2245 train_time:58121ms step_avg:60.86ms
step:956/2245 train_time:58181ms step_avg:60.86ms
step:957/2245 train_time:58243ms step_avg:60.86ms
step:958/2245 train_time:58303ms step_avg:60.86ms
step:959/2245 train_time:58365ms step_avg:60.86ms
step:960/2245 train_time:58425ms step_avg:60.86ms
step:961/2245 train_time:58488ms step_avg:60.86ms
step:962/2245 train_time:58548ms step_avg:60.86ms
step:963/2245 train_time:58611ms step_avg:60.86ms
step:964/2245 train_time:58670ms step_avg:60.86ms
step:965/2245 train_time:58733ms step_avg:60.86ms
step:966/2245 train_time:58795ms step_avg:60.86ms
step:967/2245 train_time:58857ms step_avg:60.87ms
step:968/2245 train_time:58917ms step_avg:60.86ms
step:969/2245 train_time:58980ms step_avg:60.87ms
step:970/2245 train_time:59040ms step_avg:60.87ms
step:971/2245 train_time:59102ms step_avg:60.87ms
step:972/2245 train_time:59162ms step_avg:60.87ms
step:973/2245 train_time:59224ms step_avg:60.87ms
step:974/2245 train_time:59284ms step_avg:60.87ms
step:975/2245 train_time:59346ms step_avg:60.87ms
step:976/2245 train_time:59406ms step_avg:60.87ms
step:977/2245 train_time:59468ms step_avg:60.87ms
step:978/2245 train_time:59528ms step_avg:60.87ms
step:979/2245 train_time:59590ms step_avg:60.87ms
step:980/2245 train_time:59650ms step_avg:60.87ms
step:981/2245 train_time:59712ms step_avg:60.87ms
step:982/2245 train_time:59772ms step_avg:60.87ms
step:983/2245 train_time:59835ms step_avg:60.87ms
step:984/2245 train_time:59896ms step_avg:60.87ms
step:985/2245 train_time:59958ms step_avg:60.87ms
step:986/2245 train_time:60017ms step_avg:60.87ms
step:987/2245 train_time:60079ms step_avg:60.87ms
step:988/2245 train_time:60139ms step_avg:60.87ms
step:989/2245 train_time:60201ms step_avg:60.87ms
step:990/2245 train_time:60261ms step_avg:60.87ms
step:991/2245 train_time:60323ms step_avg:60.87ms
step:992/2245 train_time:60382ms step_avg:60.87ms
step:993/2245 train_time:60445ms step_avg:60.87ms
step:994/2245 train_time:60505ms step_avg:60.87ms
step:995/2245 train_time:60568ms step_avg:60.87ms
step:996/2245 train_time:60627ms step_avg:60.87ms
step:997/2245 train_time:60690ms step_avg:60.87ms
step:998/2245 train_time:60750ms step_avg:60.87ms
step:999/2245 train_time:60813ms step_avg:60.87ms
step:1000/2245 train_time:60873ms step_avg:60.87ms
step:1000/2245 val_loss:3.5925 train_time:60937ms step_avg:60.94ms
step:1001/2245 train_time:60956ms step_avg:60.90ms
step:1002/2245 train_time:60999ms step_avg:60.88ms
step:1003/2245 train_time:61065ms step_avg:60.88ms
step:1004/2245 train_time:61128ms step_avg:60.88ms
step:1005/2245 train_time:61192ms step_avg:60.89ms
step:1006/2245 train_time:61253ms step_avg:60.89ms
step:1007/2245 train_time:61314ms step_avg:60.89ms
step:1008/2245 train_time:61373ms step_avg:60.89ms
step:1009/2245 train_time:61434ms step_avg:60.89ms
step:1010/2245 train_time:61493ms step_avg:60.88ms
step:1011/2245 train_time:61555ms step_avg:60.89ms
step:1012/2245 train_time:61614ms step_avg:60.88ms
step:1013/2245 train_time:61675ms step_avg:60.88ms
step:1014/2245 train_time:61734ms step_avg:60.88ms
step:1015/2245 train_time:61796ms step_avg:60.88ms
step:1016/2245 train_time:61855ms step_avg:60.88ms
step:1017/2245 train_time:61918ms step_avg:60.88ms
step:1018/2245 train_time:61980ms step_avg:60.88ms
step:1019/2245 train_time:62046ms step_avg:60.89ms
step:1020/2245 train_time:62106ms step_avg:60.89ms
step:1021/2245 train_time:62170ms step_avg:60.89ms
step:1022/2245 train_time:62231ms step_avg:60.89ms
step:1023/2245 train_time:62293ms step_avg:60.89ms
step:1024/2245 train_time:62352ms step_avg:60.89ms
step:1025/2245 train_time:62415ms step_avg:60.89ms
step:1026/2245 train_time:62474ms step_avg:60.89ms
step:1027/2245 train_time:62537ms step_avg:60.89ms
step:1028/2245 train_time:62596ms step_avg:60.89ms
step:1029/2245 train_time:62657ms step_avg:60.89ms
step:1030/2245 train_time:62716ms step_avg:60.89ms
step:1031/2245 train_time:62778ms step_avg:60.89ms
step:1032/2245 train_time:62838ms step_avg:60.89ms
step:1033/2245 train_time:62900ms step_avg:60.89ms
step:1034/2245 train_time:62961ms step_avg:60.89ms
step:1035/2245 train_time:63025ms step_avg:60.89ms
step:1036/2245 train_time:63085ms step_avg:60.89ms
step:1037/2245 train_time:63149ms step_avg:60.90ms
step:1038/2245 train_time:63209ms step_avg:60.90ms
step:1039/2245 train_time:63272ms step_avg:60.90ms
step:1040/2245 train_time:63332ms step_avg:60.90ms
step:1041/2245 train_time:63394ms step_avg:60.90ms
step:1042/2245 train_time:63454ms step_avg:60.90ms
step:1043/2245 train_time:63516ms step_avg:60.90ms
step:1044/2245 train_time:63575ms step_avg:60.90ms
step:1045/2245 train_time:63637ms step_avg:60.90ms
step:1046/2245 train_time:63696ms step_avg:60.89ms
step:1047/2245 train_time:63758ms step_avg:60.90ms
step:1048/2245 train_time:63817ms step_avg:60.89ms
step:1049/2245 train_time:63880ms step_avg:60.90ms
step:1050/2245 train_time:63940ms step_avg:60.90ms
step:1051/2245 train_time:64003ms step_avg:60.90ms
step:1052/2245 train_time:64063ms step_avg:60.90ms
step:1053/2245 train_time:64126ms step_avg:60.90ms
step:1054/2245 train_time:64186ms step_avg:60.90ms
step:1055/2245 train_time:64249ms step_avg:60.90ms
step:1056/2245 train_time:64309ms step_avg:60.90ms
step:1057/2245 train_time:64372ms step_avg:60.90ms
step:1058/2245 train_time:64432ms step_avg:60.90ms
step:1059/2245 train_time:64493ms step_avg:60.90ms
step:1060/2245 train_time:64553ms step_avg:60.90ms
step:1061/2245 train_time:64615ms step_avg:60.90ms
step:1062/2245 train_time:64675ms step_avg:60.90ms
step:1063/2245 train_time:64737ms step_avg:60.90ms
step:1064/2245 train_time:64796ms step_avg:60.90ms
step:1065/2245 train_time:64860ms step_avg:60.90ms
step:1066/2245 train_time:64920ms step_avg:60.90ms
step:1067/2245 train_time:64982ms step_avg:60.90ms
step:1068/2245 train_time:65043ms step_avg:60.90ms
step:1069/2245 train_time:65106ms step_avg:60.90ms
step:1070/2245 train_time:65167ms step_avg:60.90ms
step:1071/2245 train_time:65230ms step_avg:60.91ms
step:1072/2245 train_time:65289ms step_avg:60.90ms
step:1073/2245 train_time:65352ms step_avg:60.91ms
step:1074/2245 train_time:65412ms step_avg:60.90ms
step:1075/2245 train_time:65474ms step_avg:60.91ms
step:1076/2245 train_time:65534ms step_avg:60.91ms
step:1077/2245 train_time:65596ms step_avg:60.91ms
step:1078/2245 train_time:65655ms step_avg:60.90ms
step:1079/2245 train_time:65717ms step_avg:60.91ms
step:1080/2245 train_time:65776ms step_avg:60.90ms
step:1081/2245 train_time:65838ms step_avg:60.91ms
step:1082/2245 train_time:65898ms step_avg:60.90ms
step:1083/2245 train_time:65961ms step_avg:60.91ms
step:1084/2245 train_time:66021ms step_avg:60.91ms
step:1085/2245 train_time:66085ms step_avg:60.91ms
step:1086/2245 train_time:66145ms step_avg:60.91ms
step:1087/2245 train_time:66207ms step_avg:60.91ms
step:1088/2245 train_time:66268ms step_avg:60.91ms
step:1089/2245 train_time:66332ms step_avg:60.91ms
step:1090/2245 train_time:66391ms step_avg:60.91ms
step:1091/2245 train_time:66454ms step_avg:60.91ms
step:1092/2245 train_time:66513ms step_avg:60.91ms
step:1093/2245 train_time:66576ms step_avg:60.91ms
step:1094/2245 train_time:66636ms step_avg:60.91ms
step:1095/2245 train_time:66698ms step_avg:60.91ms
step:1096/2245 train_time:66757ms step_avg:60.91ms
step:1097/2245 train_time:66819ms step_avg:60.91ms
step:1098/2245 train_time:66879ms step_avg:60.91ms
step:1099/2245 train_time:66942ms step_avg:60.91ms
step:1100/2245 train_time:67001ms step_avg:60.91ms
step:1101/2245 train_time:67064ms step_avg:60.91ms
step:1102/2245 train_time:67125ms step_avg:60.91ms
step:1103/2245 train_time:67187ms step_avg:60.91ms
step:1104/2245 train_time:67247ms step_avg:60.91ms
step:1105/2245 train_time:67310ms step_avg:60.91ms
step:1106/2245 train_time:67370ms step_avg:60.91ms
step:1107/2245 train_time:67433ms step_avg:60.91ms
step:1108/2245 train_time:67492ms step_avg:60.91ms
step:1109/2245 train_time:67555ms step_avg:60.92ms
step:1110/2245 train_time:67615ms step_avg:60.91ms
step:1111/2245 train_time:67678ms step_avg:60.92ms
step:1112/2245 train_time:67738ms step_avg:60.92ms
step:1113/2245 train_time:67800ms step_avg:60.92ms
step:1114/2245 train_time:67859ms step_avg:60.91ms
step:1115/2245 train_time:67921ms step_avg:60.92ms
step:1116/2245 train_time:67981ms step_avg:60.91ms
step:1117/2245 train_time:68044ms step_avg:60.92ms
step:1118/2245 train_time:68104ms step_avg:60.92ms
step:1119/2245 train_time:68167ms step_avg:60.92ms
step:1120/2245 train_time:68228ms step_avg:60.92ms
step:1121/2245 train_time:68291ms step_avg:60.92ms
step:1122/2245 train_time:68351ms step_avg:60.92ms
step:1123/2245 train_time:68413ms step_avg:60.92ms
step:1124/2245 train_time:68474ms step_avg:60.92ms
step:1125/2245 train_time:68536ms step_avg:60.92ms
step:1126/2245 train_time:68596ms step_avg:60.92ms
step:1127/2245 train_time:68658ms step_avg:60.92ms
step:1128/2245 train_time:68718ms step_avg:60.92ms
step:1129/2245 train_time:68781ms step_avg:60.92ms
step:1130/2245 train_time:68840ms step_avg:60.92ms
step:1131/2245 train_time:68902ms step_avg:60.92ms
step:1132/2245 train_time:68961ms step_avg:60.92ms
step:1133/2245 train_time:69023ms step_avg:60.92ms
step:1134/2245 train_time:69083ms step_avg:60.92ms
step:1135/2245 train_time:69147ms step_avg:60.92ms
step:1136/2245 train_time:69207ms step_avg:60.92ms
step:1137/2245 train_time:69270ms step_avg:60.92ms
step:1138/2245 train_time:69330ms step_avg:60.92ms
step:1139/2245 train_time:69392ms step_avg:60.92ms
step:1140/2245 train_time:69453ms step_avg:60.92ms
step:1141/2245 train_time:69515ms step_avg:60.92ms
step:1142/2245 train_time:69575ms step_avg:60.92ms
step:1143/2245 train_time:69637ms step_avg:60.92ms
step:1144/2245 train_time:69697ms step_avg:60.92ms
step:1145/2245 train_time:69759ms step_avg:60.92ms
step:1146/2245 train_time:69818ms step_avg:60.92ms
step:1147/2245 train_time:69880ms step_avg:60.92ms
step:1148/2245 train_time:69940ms step_avg:60.92ms
step:1149/2245 train_time:70003ms step_avg:60.92ms
step:1150/2245 train_time:70063ms step_avg:60.92ms
step:1151/2245 train_time:70125ms step_avg:60.93ms
step:1152/2245 train_time:70186ms step_avg:60.93ms
step:1153/2245 train_time:70249ms step_avg:60.93ms
step:1154/2245 train_time:70309ms step_avg:60.93ms
step:1155/2245 train_time:70372ms step_avg:60.93ms
step:1156/2245 train_time:70433ms step_avg:60.93ms
step:1157/2245 train_time:70496ms step_avg:60.93ms
step:1158/2245 train_time:70556ms step_avg:60.93ms
step:1159/2245 train_time:70618ms step_avg:60.93ms
step:1160/2245 train_time:70677ms step_avg:60.93ms
step:1161/2245 train_time:70739ms step_avg:60.93ms
step:1162/2245 train_time:70799ms step_avg:60.93ms
step:1163/2245 train_time:70861ms step_avg:60.93ms
step:1164/2245 train_time:70920ms step_avg:60.93ms
step:1165/2245 train_time:70982ms step_avg:60.93ms
step:1166/2245 train_time:71043ms step_avg:60.93ms
step:1167/2245 train_time:71105ms step_avg:60.93ms
step:1168/2245 train_time:71165ms step_avg:60.93ms
step:1169/2245 train_time:71228ms step_avg:60.93ms
step:1170/2245 train_time:71289ms step_avg:60.93ms
step:1171/2245 train_time:71352ms step_avg:60.93ms
step:1172/2245 train_time:71412ms step_avg:60.93ms
step:1173/2245 train_time:71476ms step_avg:60.93ms
step:1174/2245 train_time:71536ms step_avg:60.93ms
step:1175/2245 train_time:71598ms step_avg:60.93ms
step:1176/2245 train_time:71658ms step_avg:60.93ms
step:1177/2245 train_time:71721ms step_avg:60.94ms
step:1178/2245 train_time:71780ms step_avg:60.93ms
step:1179/2245 train_time:71843ms step_avg:60.94ms
step:1180/2245 train_time:71902ms step_avg:60.93ms
step:1181/2245 train_time:71964ms step_avg:60.93ms
step:1182/2245 train_time:72024ms step_avg:60.93ms
step:1183/2245 train_time:72086ms step_avg:60.94ms
step:1184/2245 train_time:72146ms step_avg:60.93ms
step:1185/2245 train_time:72209ms step_avg:60.94ms
step:1186/2245 train_time:72270ms step_avg:60.94ms
step:1187/2245 train_time:72332ms step_avg:60.94ms
step:1188/2245 train_time:72392ms step_avg:60.94ms
step:1189/2245 train_time:72454ms step_avg:60.94ms
step:1190/2245 train_time:72514ms step_avg:60.94ms
step:1191/2245 train_time:72576ms step_avg:60.94ms
step:1192/2245 train_time:72637ms step_avg:60.94ms
step:1193/2245 train_time:72699ms step_avg:60.94ms
step:1194/2245 train_time:72758ms step_avg:60.94ms
step:1195/2245 train_time:72820ms step_avg:60.94ms
step:1196/2245 train_time:72880ms step_avg:60.94ms
step:1197/2245 train_time:72942ms step_avg:60.94ms
step:1198/2245 train_time:73002ms step_avg:60.94ms
step:1199/2245 train_time:73065ms step_avg:60.94ms
step:1200/2245 train_time:73125ms step_avg:60.94ms
step:1201/2245 train_time:73188ms step_avg:60.94ms
step:1202/2245 train_time:73249ms step_avg:60.94ms
step:1203/2245 train_time:73312ms step_avg:60.94ms
step:1204/2245 train_time:73371ms step_avg:60.94ms
step:1205/2245 train_time:73434ms step_avg:60.94ms
step:1206/2245 train_time:73494ms step_avg:60.94ms
step:1207/2245 train_time:73557ms step_avg:60.94ms
step:1208/2245 train_time:73616ms step_avg:60.94ms
step:1209/2245 train_time:73679ms step_avg:60.94ms
step:1210/2245 train_time:73739ms step_avg:60.94ms
step:1211/2245 train_time:73801ms step_avg:60.94ms
step:1212/2245 train_time:73860ms step_avg:60.94ms
step:1213/2245 train_time:73922ms step_avg:60.94ms
step:1214/2245 train_time:73982ms step_avg:60.94ms
step:1215/2245 train_time:74045ms step_avg:60.94ms
step:1216/2245 train_time:74105ms step_avg:60.94ms
step:1217/2245 train_time:74168ms step_avg:60.94ms
step:1218/2245 train_time:74229ms step_avg:60.94ms
step:1219/2245 train_time:74292ms step_avg:60.94ms
step:1220/2245 train_time:74352ms step_avg:60.94ms
step:1221/2245 train_time:74415ms step_avg:60.95ms
step:1222/2245 train_time:74475ms step_avg:60.94ms
step:1223/2245 train_time:74536ms step_avg:60.95ms
step:1224/2245 train_time:74596ms step_avg:60.94ms
step:1225/2245 train_time:74659ms step_avg:60.95ms
step:1226/2245 train_time:74718ms step_avg:60.94ms
step:1227/2245 train_time:74780ms step_avg:60.95ms
step:1228/2245 train_time:74839ms step_avg:60.94ms
step:1229/2245 train_time:74901ms step_avg:60.94ms
step:1230/2245 train_time:74961ms step_avg:60.94ms
step:1231/2245 train_time:75024ms step_avg:60.95ms
step:1232/2245 train_time:75083ms step_avg:60.94ms
step:1233/2245 train_time:75146ms step_avg:60.95ms
step:1234/2245 train_time:75206ms step_avg:60.94ms
step:1235/2245 train_time:75269ms step_avg:60.95ms
step:1236/2245 train_time:75330ms step_avg:60.95ms
step:1237/2245 train_time:75393ms step_avg:60.95ms
step:1238/2245 train_time:75453ms step_avg:60.95ms
step:1239/2245 train_time:75516ms step_avg:60.95ms
step:1240/2245 train_time:75576ms step_avg:60.95ms
step:1241/2245 train_time:75638ms step_avg:60.95ms
step:1242/2245 train_time:75698ms step_avg:60.95ms
step:1243/2245 train_time:75760ms step_avg:60.95ms
step:1244/2245 train_time:75820ms step_avg:60.95ms
step:1245/2245 train_time:75882ms step_avg:60.95ms
step:1246/2245 train_time:75942ms step_avg:60.95ms
step:1247/2245 train_time:76004ms step_avg:60.95ms
step:1248/2245 train_time:76064ms step_avg:60.95ms
step:1249/2245 train_time:76127ms step_avg:60.95ms
step:1250/2245 train_time:76187ms step_avg:60.95ms
step:1250/2245 val_loss:3.5232 train_time:76252ms step_avg:61.00ms
step:1251/2245 train_time:76271ms step_avg:60.97ms
step:1252/2245 train_time:76312ms step_avg:60.95ms
step:1253/2245 train_time:76379ms step_avg:60.96ms
step:1254/2245 train_time:76440ms step_avg:60.96ms
step:1255/2245 train_time:76502ms step_avg:60.96ms
step:1256/2245 train_time:76562ms step_avg:60.96ms
step:1257/2245 train_time:76625ms step_avg:60.96ms
step:1258/2245 train_time:76685ms step_avg:60.96ms
step:1259/2245 train_time:76747ms step_avg:60.96ms
step:1260/2245 train_time:76807ms step_avg:60.96ms
step:1261/2245 train_time:76869ms step_avg:60.96ms
step:1262/2245 train_time:76929ms step_avg:60.96ms
step:1263/2245 train_time:76991ms step_avg:60.96ms
step:1264/2245 train_time:77051ms step_avg:60.96ms
step:1265/2245 train_time:77112ms step_avg:60.96ms
step:1266/2245 train_time:77172ms step_avg:60.96ms
step:1267/2245 train_time:77235ms step_avg:60.96ms
step:1268/2245 train_time:77295ms step_avg:60.96ms
step:1269/2245 train_time:77358ms step_avg:60.96ms
step:1270/2245 train_time:77419ms step_avg:60.96ms
step:1271/2245 train_time:77482ms step_avg:60.96ms
step:1272/2245 train_time:77542ms step_avg:60.96ms
step:1273/2245 train_time:77604ms step_avg:60.96ms
step:1274/2245 train_time:77664ms step_avg:60.96ms
step:1275/2245 train_time:77726ms step_avg:60.96ms
step:1276/2245 train_time:77786ms step_avg:60.96ms
step:1277/2245 train_time:77848ms step_avg:60.96ms
step:1278/2245 train_time:77908ms step_avg:60.96ms
step:1279/2245 train_time:77970ms step_avg:60.96ms
step:1280/2245 train_time:78029ms step_avg:60.96ms
step:1281/2245 train_time:78092ms step_avg:60.96ms
step:1282/2245 train_time:78152ms step_avg:60.96ms
step:1283/2245 train_time:78215ms step_avg:60.96ms
step:1284/2245 train_time:78275ms step_avg:60.96ms
step:1285/2245 train_time:78337ms step_avg:60.96ms
step:1286/2245 train_time:78397ms step_avg:60.96ms
step:1287/2245 train_time:78459ms step_avg:60.96ms
step:1288/2245 train_time:78519ms step_avg:60.96ms
step:1289/2245 train_time:78581ms step_avg:60.96ms
step:1290/2245 train_time:78641ms step_avg:60.96ms
step:1291/2245 train_time:78703ms step_avg:60.96ms
step:1292/2245 train_time:78764ms step_avg:60.96ms
step:1293/2245 train_time:78826ms step_avg:60.96ms
step:1294/2245 train_time:78886ms step_avg:60.96ms
step:1295/2245 train_time:78949ms step_avg:60.96ms
step:1296/2245 train_time:79009ms step_avg:60.96ms
step:1297/2245 train_time:79072ms step_avg:60.97ms
step:1298/2245 train_time:79131ms step_avg:60.96ms
step:1299/2245 train_time:79193ms step_avg:60.96ms
step:1300/2245 train_time:79254ms step_avg:60.96ms
step:1301/2245 train_time:79316ms step_avg:60.97ms
step:1302/2245 train_time:79376ms step_avg:60.96ms
step:1303/2245 train_time:79438ms step_avg:60.97ms
step:1304/2245 train_time:79498ms step_avg:60.96ms
step:1305/2245 train_time:79562ms step_avg:60.97ms
step:1306/2245 train_time:79622ms step_avg:60.97ms
step:1307/2245 train_time:79685ms step_avg:60.97ms
step:1308/2245 train_time:79745ms step_avg:60.97ms
step:1309/2245 train_time:79807ms step_avg:60.97ms
step:1310/2245 train_time:79868ms step_avg:60.97ms
step:1311/2245 train_time:79930ms step_avg:60.97ms
step:1312/2245 train_time:79990ms step_avg:60.97ms
step:1313/2245 train_time:80053ms step_avg:60.97ms
step:1314/2245 train_time:80114ms step_avg:60.97ms
step:1315/2245 train_time:80176ms step_avg:60.97ms
step:1316/2245 train_time:80236ms step_avg:60.97ms
step:1317/2245 train_time:80298ms step_avg:60.97ms
step:1318/2245 train_time:80359ms step_avg:60.97ms
step:1319/2245 train_time:80421ms step_avg:60.97ms
step:1320/2245 train_time:80481ms step_avg:60.97ms
step:1321/2245 train_time:80543ms step_avg:60.97ms
step:1322/2245 train_time:80603ms step_avg:60.97ms
step:1323/2245 train_time:80666ms step_avg:60.97ms
step:1324/2245 train_time:80726ms step_avg:60.97ms
step:1325/2245 train_time:80788ms step_avg:60.97ms
step:1326/2245 train_time:80849ms step_avg:60.97ms
step:1327/2245 train_time:80911ms step_avg:60.97ms
step:1328/2245 train_time:80971ms step_avg:60.97ms
step:1329/2245 train_time:81034ms step_avg:60.97ms
step:1330/2245 train_time:81093ms step_avg:60.97ms
step:1331/2245 train_time:81155ms step_avg:60.97ms
step:1332/2245 train_time:81214ms step_avg:60.97ms
step:1333/2245 train_time:81276ms step_avg:60.97ms
step:1334/2245 train_time:81336ms step_avg:60.97ms
step:1335/2245 train_time:81398ms step_avg:60.97ms
step:1336/2245 train_time:81458ms step_avg:60.97ms
step:1337/2245 train_time:81520ms step_avg:60.97ms
step:1338/2245 train_time:81581ms step_avg:60.97ms
step:1339/2245 train_time:81644ms step_avg:60.97ms
step:1340/2245 train_time:81703ms step_avg:60.97ms
step:1341/2245 train_time:81766ms step_avg:60.97ms
step:1342/2245 train_time:81826ms step_avg:60.97ms
step:1343/2245 train_time:81888ms step_avg:60.97ms
step:1344/2245 train_time:81948ms step_avg:60.97ms
step:1345/2245 train_time:82011ms step_avg:60.97ms
step:1346/2245 train_time:82071ms step_avg:60.97ms
step:1347/2245 train_time:82133ms step_avg:60.97ms
step:1348/2245 train_time:82192ms step_avg:60.97ms
step:1349/2245 train_time:82255ms step_avg:60.97ms
step:1350/2245 train_time:82315ms step_avg:60.97ms
step:1351/2245 train_time:82377ms step_avg:60.97ms
step:1352/2245 train_time:82437ms step_avg:60.97ms
step:1353/2245 train_time:82499ms step_avg:60.97ms
step:1354/2245 train_time:82560ms step_avg:60.97ms
step:1355/2245 train_time:82623ms step_avg:60.98ms
step:1356/2245 train_time:82683ms step_avg:60.98ms
step:1357/2245 train_time:82746ms step_avg:60.98ms
step:1358/2245 train_time:82806ms step_avg:60.98ms
step:1359/2245 train_time:82869ms step_avg:60.98ms
step:1360/2245 train_time:82928ms step_avg:60.98ms
step:1361/2245 train_time:82991ms step_avg:60.98ms
step:1362/2245 train_time:83051ms step_avg:60.98ms
step:1363/2245 train_time:83113ms step_avg:60.98ms
step:1364/2245 train_time:83173ms step_avg:60.98ms
step:1365/2245 train_time:83235ms step_avg:60.98ms
step:1366/2245 train_time:83294ms step_avg:60.98ms
step:1367/2245 train_time:83356ms step_avg:60.98ms
step:1368/2245 train_time:83416ms step_avg:60.98ms
step:1369/2245 train_time:83478ms step_avg:60.98ms
step:1370/2245 train_time:83538ms step_avg:60.98ms
step:1371/2245 train_time:83602ms step_avg:60.98ms
step:1372/2245 train_time:83662ms step_avg:60.98ms
step:1373/2245 train_time:83725ms step_avg:60.98ms
step:1374/2245 train_time:83784ms step_avg:60.98ms
step:1375/2245 train_time:83847ms step_avg:60.98ms
step:1376/2245 train_time:83907ms step_avg:60.98ms
step:1377/2245 train_time:83970ms step_avg:60.98ms
step:1378/2245 train_time:84030ms step_avg:60.98ms
step:1379/2245 train_time:84092ms step_avg:60.98ms
step:1380/2245 train_time:84152ms step_avg:60.98ms
step:1381/2245 train_time:84214ms step_avg:60.98ms
step:1382/2245 train_time:84275ms step_avg:60.98ms
step:1383/2245 train_time:84337ms step_avg:60.98ms
step:1384/2245 train_time:84397ms step_avg:60.98ms
step:1385/2245 train_time:84460ms step_avg:60.98ms
step:1386/2245 train_time:84520ms step_avg:60.98ms
step:1387/2245 train_time:84583ms step_avg:60.98ms
step:1388/2245 train_time:84643ms step_avg:60.98ms
step:1389/2245 train_time:84705ms step_avg:60.98ms
step:1390/2245 train_time:84766ms step_avg:60.98ms
step:1391/2245 train_time:84828ms step_avg:60.98ms
step:1392/2245 train_time:84888ms step_avg:60.98ms
step:1393/2245 train_time:84950ms step_avg:60.98ms
step:1394/2245 train_time:85010ms step_avg:60.98ms
step:1395/2245 train_time:85072ms step_avg:60.98ms
step:1396/2245 train_time:85132ms step_avg:60.98ms
step:1397/2245 train_time:85194ms step_avg:60.98ms
step:1398/2245 train_time:85254ms step_avg:60.98ms
step:1399/2245 train_time:85316ms step_avg:60.98ms
step:1400/2245 train_time:85376ms step_avg:60.98ms
step:1401/2245 train_time:85438ms step_avg:60.98ms
step:1402/2245 train_time:85498ms step_avg:60.98ms
step:1403/2245 train_time:85562ms step_avg:60.99ms
step:1404/2245 train_time:85622ms step_avg:60.98ms
step:1405/2245 train_time:85685ms step_avg:60.99ms
step:1406/2245 train_time:85744ms step_avg:60.98ms
step:1407/2245 train_time:85807ms step_avg:60.99ms
step:1408/2245 train_time:85867ms step_avg:60.98ms
step:1409/2245 train_time:85929ms step_avg:60.99ms
step:1410/2245 train_time:85990ms step_avg:60.99ms
step:1411/2245 train_time:86052ms step_avg:60.99ms
step:1412/2245 train_time:86112ms step_avg:60.99ms
step:1413/2245 train_time:86174ms step_avg:60.99ms
step:1414/2245 train_time:86234ms step_avg:60.99ms
step:1415/2245 train_time:86297ms step_avg:60.99ms
step:1416/2245 train_time:86357ms step_avg:60.99ms
step:1417/2245 train_time:86419ms step_avg:60.99ms
step:1418/2245 train_time:86479ms step_avg:60.99ms
step:1419/2245 train_time:86541ms step_avg:60.99ms
step:1420/2245 train_time:86601ms step_avg:60.99ms
step:1421/2245 train_time:86664ms step_avg:60.99ms
step:1422/2245 train_time:86724ms step_avg:60.99ms
step:1423/2245 train_time:86786ms step_avg:60.99ms
step:1424/2245 train_time:86846ms step_avg:60.99ms
step:1425/2245 train_time:86908ms step_avg:60.99ms
step:1426/2245 train_time:86969ms step_avg:60.99ms
step:1427/2245 train_time:87031ms step_avg:60.99ms
step:1428/2245 train_time:87091ms step_avg:60.99ms
step:1429/2245 train_time:87153ms step_avg:60.99ms
step:1430/2245 train_time:87214ms step_avg:60.99ms
step:1431/2245 train_time:87277ms step_avg:60.99ms
step:1432/2245 train_time:87337ms step_avg:60.99ms
step:1433/2245 train_time:87399ms step_avg:60.99ms
step:1434/2245 train_time:87459ms step_avg:60.99ms
step:1435/2245 train_time:87521ms step_avg:60.99ms
step:1436/2245 train_time:87580ms step_avg:60.99ms
step:1437/2245 train_time:87643ms step_avg:60.99ms
step:1438/2245 train_time:87703ms step_avg:60.99ms
step:1439/2245 train_time:87766ms step_avg:60.99ms
step:1440/2245 train_time:87827ms step_avg:60.99ms
step:1441/2245 train_time:87889ms step_avg:60.99ms
step:1442/2245 train_time:87949ms step_avg:60.99ms
step:1443/2245 train_time:88011ms step_avg:60.99ms
step:1444/2245 train_time:88071ms step_avg:60.99ms
step:1445/2245 train_time:88133ms step_avg:60.99ms
step:1446/2245 train_time:88194ms step_avg:60.99ms
step:1447/2245 train_time:88256ms step_avg:60.99ms
step:1448/2245 train_time:88316ms step_avg:60.99ms
step:1449/2245 train_time:88378ms step_avg:60.99ms
step:1450/2245 train_time:88438ms step_avg:60.99ms
step:1451/2245 train_time:88500ms step_avg:60.99ms
step:1452/2245 train_time:88560ms step_avg:60.99ms
step:1453/2245 train_time:88623ms step_avg:60.99ms
step:1454/2245 train_time:88683ms step_avg:60.99ms
step:1455/2245 train_time:88747ms step_avg:60.99ms
step:1456/2245 train_time:88806ms step_avg:60.99ms
step:1457/2245 train_time:88869ms step_avg:60.99ms
step:1458/2245 train_time:88929ms step_avg:60.99ms
step:1459/2245 train_time:88992ms step_avg:61.00ms
step:1460/2245 train_time:89052ms step_avg:60.99ms
step:1461/2245 train_time:89114ms step_avg:61.00ms
step:1462/2245 train_time:89175ms step_avg:60.99ms
step:1463/2245 train_time:89237ms step_avg:61.00ms
step:1464/2245 train_time:89296ms step_avg:60.99ms
step:1465/2245 train_time:89359ms step_avg:61.00ms
step:1466/2245 train_time:89418ms step_avg:60.99ms
step:1467/2245 train_time:89481ms step_avg:61.00ms
step:1468/2245 train_time:89541ms step_avg:61.00ms
step:1469/2245 train_time:89603ms step_avg:61.00ms
step:1470/2245 train_time:89663ms step_avg:61.00ms
step:1471/2245 train_time:89726ms step_avg:61.00ms
step:1472/2245 train_time:89786ms step_avg:61.00ms
step:1473/2245 train_time:89850ms step_avg:61.00ms
step:1474/2245 train_time:89910ms step_avg:61.00ms
step:1475/2245 train_time:89974ms step_avg:61.00ms
step:1476/2245 train_time:90035ms step_avg:61.00ms
step:1477/2245 train_time:90097ms step_avg:61.00ms
step:1478/2245 train_time:90158ms step_avg:61.00ms
step:1479/2245 train_time:90220ms step_avg:61.00ms
step:1480/2245 train_time:90280ms step_avg:61.00ms
step:1481/2245 train_time:90342ms step_avg:61.00ms
step:1482/2245 train_time:90403ms step_avg:61.00ms
step:1483/2245 train_time:90465ms step_avg:61.00ms
step:1484/2245 train_time:90526ms step_avg:61.00ms
step:1485/2245 train_time:90589ms step_avg:61.00ms
step:1486/2245 train_time:90649ms step_avg:61.00ms
step:1487/2245 train_time:90712ms step_avg:61.00ms
step:1488/2245 train_time:90773ms step_avg:61.00ms
step:1489/2245 train_time:90835ms step_avg:61.00ms
step:1490/2245 train_time:90896ms step_avg:61.00ms
step:1491/2245 train_time:90959ms step_avg:61.01ms
step:1492/2245 train_time:91020ms step_avg:61.01ms
step:1493/2245 train_time:91083ms step_avg:61.01ms
step:1494/2245 train_time:91144ms step_avg:61.01ms
step:1495/2245 train_time:91208ms step_avg:61.01ms
step:1496/2245 train_time:91268ms step_avg:61.01ms
step:1497/2245 train_time:91331ms step_avg:61.01ms
step:1498/2245 train_time:91391ms step_avg:61.01ms
step:1499/2245 train_time:91453ms step_avg:61.01ms
step:1500/2245 train_time:91513ms step_avg:61.01ms
step:1500/2245 val_loss:3.4421 train_time:91576ms step_avg:61.05ms
step:1501/2245 train_time:91595ms step_avg:61.02ms
step:1502/2245 train_time:91637ms step_avg:61.01ms
step:1503/2245 train_time:91699ms step_avg:61.01ms
step:1504/2245 train_time:91758ms step_avg:61.01ms
step:1505/2245 train_time:91821ms step_avg:61.01ms
step:1506/2245 train_time:91881ms step_avg:61.01ms
step:1507/2245 train_time:91943ms step_avg:61.01ms
step:1508/2245 train_time:92003ms step_avg:61.01ms
step:1509/2245 train_time:92065ms step_avg:61.01ms
step:1510/2245 train_time:92125ms step_avg:61.01ms
step:1511/2245 train_time:92187ms step_avg:61.01ms
step:1512/2245 train_time:92247ms step_avg:61.01ms
step:1513/2245 train_time:92309ms step_avg:61.01ms
step:1514/2245 train_time:92369ms step_avg:61.01ms
step:1515/2245 train_time:92433ms step_avg:61.01ms
step:1516/2245 train_time:92500ms step_avg:61.02ms
step:1517/2245 train_time:92568ms step_avg:61.02ms
step:1518/2245 train_time:92631ms step_avg:61.02ms
step:1519/2245 train_time:92694ms step_avg:61.02ms
step:1520/2245 train_time:92754ms step_avg:61.02ms
step:1521/2245 train_time:92816ms step_avg:61.02ms
step:1522/2245 train_time:92876ms step_avg:61.02ms
step:1523/2245 train_time:92938ms step_avg:61.02ms
step:1524/2245 train_time:92998ms step_avg:61.02ms
step:1525/2245 train_time:93061ms step_avg:61.02ms
step:1526/2245 train_time:93121ms step_avg:61.02ms
step:1527/2245 train_time:93184ms step_avg:61.02ms
step:1528/2245 train_time:93243ms step_avg:61.02ms
step:1529/2245 train_time:93307ms step_avg:61.02ms
step:1530/2245 train_time:93367ms step_avg:61.02ms
step:1531/2245 train_time:93431ms step_avg:61.03ms
step:1532/2245 train_time:93492ms step_avg:61.03ms
step:1533/2245 train_time:93556ms step_avg:61.03ms
step:1534/2245 train_time:93618ms step_avg:61.03ms
step:1535/2245 train_time:93681ms step_avg:61.03ms
step:1536/2245 train_time:93741ms step_avg:61.03ms
step:1537/2245 train_time:93805ms step_avg:61.03ms
step:1538/2245 train_time:93866ms step_avg:61.03ms
step:1539/2245 train_time:93929ms step_avg:61.03ms
step:1540/2245 train_time:93989ms step_avg:61.03ms
step:1541/2245 train_time:94052ms step_avg:61.03ms
step:1542/2245 train_time:94112ms step_avg:61.03ms
step:1543/2245 train_time:94174ms step_avg:61.03ms
step:1544/2245 train_time:94234ms step_avg:61.03ms
step:1545/2245 train_time:94297ms step_avg:61.03ms
step:1546/2245 train_time:94358ms step_avg:61.03ms
step:1547/2245 train_time:94422ms step_avg:61.04ms
step:1548/2245 train_time:94483ms step_avg:61.04ms
step:1549/2245 train_time:94547ms step_avg:61.04ms
step:1550/2245 train_time:94609ms step_avg:61.04ms
step:1551/2245 train_time:94673ms step_avg:61.04ms
step:1552/2245 train_time:94733ms step_avg:61.04ms
step:1553/2245 train_time:94796ms step_avg:61.04ms
step:1554/2245 train_time:94856ms step_avg:61.04ms
step:1555/2245 train_time:94919ms step_avg:61.04ms
step:1556/2245 train_time:94978ms step_avg:61.04ms
step:1557/2245 train_time:95042ms step_avg:61.04ms
step:1558/2245 train_time:95102ms step_avg:61.04ms
step:1559/2245 train_time:95165ms step_avg:61.04ms
step:1560/2245 train_time:95225ms step_avg:61.04ms
step:1561/2245 train_time:95288ms step_avg:61.04ms
step:1562/2245 train_time:95350ms step_avg:61.04ms
step:1563/2245 train_time:95413ms step_avg:61.04ms
step:1564/2245 train_time:95472ms step_avg:61.04ms
step:1565/2245 train_time:95536ms step_avg:61.05ms
step:1566/2245 train_time:95596ms step_avg:61.05ms
step:1567/2245 train_time:95659ms step_avg:61.05ms
step:1568/2245 train_time:95720ms step_avg:61.05ms
step:1569/2245 train_time:95782ms step_avg:61.05ms
step:1570/2245 train_time:95843ms step_avg:61.05ms
step:1571/2245 train_time:95906ms step_avg:61.05ms
step:1572/2245 train_time:95966ms step_avg:61.05ms
step:1573/2245 train_time:96030ms step_avg:61.05ms
step:1574/2245 train_time:96090ms step_avg:61.05ms
step:1575/2245 train_time:96153ms step_avg:61.05ms
step:1576/2245 train_time:96213ms step_avg:61.05ms
step:1577/2245 train_time:96276ms step_avg:61.05ms
step:1578/2245 train_time:96336ms step_avg:61.05ms
step:1579/2245 train_time:96399ms step_avg:61.05ms
step:1580/2245 train_time:96460ms step_avg:61.05ms
step:1581/2245 train_time:96523ms step_avg:61.05ms
step:1582/2245 train_time:96583ms step_avg:61.05ms
step:1583/2245 train_time:96647ms step_avg:61.05ms
step:1584/2245 train_time:96710ms step_avg:61.05ms
step:1585/2245 train_time:96773ms step_avg:61.06ms
step:1586/2245 train_time:96833ms step_avg:61.05ms
step:1587/2245 train_time:96895ms step_avg:61.06ms
step:1588/2245 train_time:96955ms step_avg:61.05ms
step:1589/2245 train_time:97018ms step_avg:61.06ms
step:1590/2245 train_time:97078ms step_avg:61.06ms
step:1591/2245 train_time:97141ms step_avg:61.06ms
step:1592/2245 train_time:97201ms step_avg:61.06ms
step:1593/2245 train_time:97265ms step_avg:61.06ms
step:1594/2245 train_time:97326ms step_avg:61.06ms
step:1595/2245 train_time:97389ms step_avg:61.06ms
step:1596/2245 train_time:97451ms step_avg:61.06ms
step:1597/2245 train_time:97514ms step_avg:61.06ms
step:1598/2245 train_time:97573ms step_avg:61.06ms
step:1599/2245 train_time:97636ms step_avg:61.06ms
step:1600/2245 train_time:97697ms step_avg:61.06ms
step:1601/2245 train_time:97760ms step_avg:61.06ms
step:1602/2245 train_time:97820ms step_avg:61.06ms
step:1603/2245 train_time:97883ms step_avg:61.06ms
step:1604/2245 train_time:97943ms step_avg:61.06ms
step:1605/2245 train_time:98006ms step_avg:61.06ms
step:1606/2245 train_time:98066ms step_avg:61.06ms
step:1607/2245 train_time:98129ms step_avg:61.06ms
step:1608/2245 train_time:98190ms step_avg:61.06ms
step:1609/2245 train_time:98253ms step_avg:61.06ms
step:1610/2245 train_time:98313ms step_avg:61.06ms
step:1611/2245 train_time:98376ms step_avg:61.06ms
step:1612/2245 train_time:98436ms step_avg:61.06ms
step:1613/2245 train_time:98498ms step_avg:61.07ms
step:1614/2245 train_time:98558ms step_avg:61.06ms
step:1615/2245 train_time:98621ms step_avg:61.07ms
step:1616/2245 train_time:98681ms step_avg:61.07ms
step:1617/2245 train_time:98745ms step_avg:61.07ms
step:1618/2245 train_time:98806ms step_avg:61.07ms
step:1619/2245 train_time:98869ms step_avg:61.07ms
step:1620/2245 train_time:98929ms step_avg:61.07ms
step:1621/2245 train_time:98992ms step_avg:61.07ms
step:1622/2245 train_time:99052ms step_avg:61.07ms
step:1623/2245 train_time:99115ms step_avg:61.07ms
step:1624/2245 train_time:99174ms step_avg:61.07ms
step:1625/2245 train_time:99237ms step_avg:61.07ms
step:1626/2245 train_time:99298ms step_avg:61.07ms
step:1627/2245 train_time:99361ms step_avg:61.07ms
step:1628/2245 train_time:99421ms step_avg:61.07ms
step:1629/2245 train_time:99484ms step_avg:61.07ms
step:1630/2245 train_time:99545ms step_avg:61.07ms
step:1631/2245 train_time:99608ms step_avg:61.07ms
step:1632/2245 train_time:99668ms step_avg:61.07ms
step:1633/2245 train_time:99732ms step_avg:61.07ms
step:1634/2245 train_time:99792ms step_avg:61.07ms
step:1635/2245 train_time:99854ms step_avg:61.07ms
step:1636/2245 train_time:99914ms step_avg:61.07ms
step:1637/2245 train_time:99976ms step_avg:61.07ms
step:1638/2245 train_time:100036ms step_avg:61.07ms
step:1639/2245 train_time:100099ms step_avg:61.07ms
step:1640/2245 train_time:100159ms step_avg:61.07ms
step:1641/2245 train_time:100222ms step_avg:61.07ms
step:1642/2245 train_time:100282ms step_avg:61.07ms
step:1643/2245 train_time:100345ms step_avg:61.07ms
step:1644/2245 train_time:100406ms step_avg:61.07ms
step:1645/2245 train_time:100469ms step_avg:61.08ms
step:1646/2245 train_time:100530ms step_avg:61.08ms
step:1647/2245 train_time:100593ms step_avg:61.08ms
step:1648/2245 train_time:100653ms step_avg:61.08ms
step:1649/2245 train_time:100717ms step_avg:61.08ms
step:1650/2245 train_time:100777ms step_avg:61.08ms
step:1651/2245 train_time:100840ms step_avg:61.08ms
step:1652/2245 train_time:100900ms step_avg:61.08ms
step:1653/2245 train_time:100963ms step_avg:61.08ms
step:1654/2245 train_time:101024ms step_avg:61.08ms
step:1655/2245 train_time:101088ms step_avg:61.08ms
step:1656/2245 train_time:101149ms step_avg:61.08ms
step:1657/2245 train_time:101212ms step_avg:61.08ms
step:1658/2245 train_time:101272ms step_avg:61.08ms
step:1659/2245 train_time:101334ms step_avg:61.08ms
step:1660/2245 train_time:101394ms step_avg:61.08ms
step:1661/2245 train_time:101457ms step_avg:61.08ms
step:1662/2245 train_time:101518ms step_avg:61.08ms
step:1663/2245 train_time:101580ms step_avg:61.08ms
step:1664/2245 train_time:101640ms step_avg:61.08ms
step:1665/2245 train_time:101704ms step_avg:61.08ms
step:1666/2245 train_time:101765ms step_avg:61.08ms
step:1667/2245 train_time:101829ms step_avg:61.08ms
step:1668/2245 train_time:101889ms step_avg:61.08ms
step:1669/2245 train_time:101952ms step_avg:61.09ms
step:1670/2245 train_time:102013ms step_avg:61.09ms
step:1671/2245 train_time:102075ms step_avg:61.09ms
step:1672/2245 train_time:102136ms step_avg:61.09ms
step:1673/2245 train_time:102198ms step_avg:61.09ms
step:1674/2245 train_time:102259ms step_avg:61.09ms
step:1675/2245 train_time:102322ms step_avg:61.09ms
step:1676/2245 train_time:102382ms step_avg:61.09ms
step:1677/2245 train_time:102446ms step_avg:61.09ms
step:1678/2245 train_time:102507ms step_avg:61.09ms
step:1679/2245 train_time:102570ms step_avg:61.09ms
step:1680/2245 train_time:102630ms step_avg:61.09ms
step:1681/2245 train_time:102694ms step_avg:61.09ms
step:1682/2245 train_time:102753ms step_avg:61.09ms
step:1683/2245 train_time:102816ms step_avg:61.09ms
step:1684/2245 train_time:102876ms step_avg:61.09ms
step:1685/2245 train_time:102939ms step_avg:61.09ms
step:1686/2245 train_time:102999ms step_avg:61.09ms
step:1687/2245 train_time:103062ms step_avg:61.09ms
step:1688/2245 train_time:103123ms step_avg:61.09ms
step:1689/2245 train_time:103186ms step_avg:61.09ms
step:1690/2245 train_time:103247ms step_avg:61.09ms
step:1691/2245 train_time:103310ms step_avg:61.09ms
step:1692/2245 train_time:103371ms step_avg:61.09ms
step:1693/2245 train_time:103434ms step_avg:61.09ms
step:1694/2245 train_time:103494ms step_avg:61.09ms
step:1695/2245 train_time:103557ms step_avg:61.10ms
step:1696/2245 train_time:103617ms step_avg:61.10ms
step:1697/2245 train_time:103680ms step_avg:61.10ms
step:1698/2245 train_time:103740ms step_avg:61.10ms
step:1699/2245 train_time:103803ms step_avg:61.10ms
step:1700/2245 train_time:103863ms step_avg:61.10ms
step:1701/2245 train_time:103926ms step_avg:61.10ms
step:1702/2245 train_time:103987ms step_avg:61.10ms
step:1703/2245 train_time:104050ms step_avg:61.10ms
step:1704/2245 train_time:104110ms step_avg:61.10ms
step:1705/2245 train_time:104173ms step_avg:61.10ms
step:1706/2245 train_time:104233ms step_avg:61.10ms
step:1707/2245 train_time:104296ms step_avg:61.10ms
step:1708/2245 train_time:104356ms step_avg:61.10ms
step:1709/2245 train_time:104419ms step_avg:61.10ms
step:1710/2245 train_time:104479ms step_avg:61.10ms
step:1711/2245 train_time:104542ms step_avg:61.10ms
step:1712/2245 train_time:104602ms step_avg:61.10ms
step:1713/2245 train_time:104665ms step_avg:61.10ms
step:1714/2245 train_time:104725ms step_avg:61.10ms
step:1715/2245 train_time:104788ms step_avg:61.10ms
step:1716/2245 train_time:104849ms step_avg:61.10ms
step:1717/2245 train_time:104912ms step_avg:61.10ms
step:1718/2245 train_time:104972ms step_avg:61.10ms
step:1719/2245 train_time:105035ms step_avg:61.10ms
step:1720/2245 train_time:105095ms step_avg:61.10ms
step:1721/2245 train_time:105158ms step_avg:61.10ms
step:1722/2245 train_time:105218ms step_avg:61.10ms
step:1723/2245 train_time:105281ms step_avg:61.10ms
step:1724/2245 train_time:105341ms step_avg:61.10ms
step:1725/2245 train_time:105405ms step_avg:61.10ms
step:1726/2245 train_time:105466ms step_avg:61.10ms
step:1727/2245 train_time:105530ms step_avg:61.11ms
step:1728/2245 train_time:105590ms step_avg:61.11ms
step:1729/2245 train_time:105653ms step_avg:61.11ms
step:1730/2245 train_time:105712ms step_avg:61.11ms
step:1731/2245 train_time:105775ms step_avg:61.11ms
step:1732/2245 train_time:105836ms step_avg:61.11ms
step:1733/2245 train_time:105899ms step_avg:61.11ms
step:1734/2245 train_time:105959ms step_avg:61.11ms
step:1735/2245 train_time:106022ms step_avg:61.11ms
step:1736/2245 train_time:106082ms step_avg:61.11ms
step:1737/2245 train_time:106146ms step_avg:61.11ms
step:1738/2245 train_time:106207ms step_avg:61.11ms
step:1739/2245 train_time:106270ms step_avg:61.11ms
step:1740/2245 train_time:106331ms step_avg:61.11ms
step:1741/2245 train_time:106393ms step_avg:61.11ms
step:1742/2245 train_time:106453ms step_avg:61.11ms
step:1743/2245 train_time:106517ms step_avg:61.11ms
step:1744/2245 train_time:106577ms step_avg:61.11ms
step:1745/2245 train_time:106639ms step_avg:61.11ms
step:1746/2245 train_time:106699ms step_avg:61.11ms
step:1747/2245 train_time:106762ms step_avg:61.11ms
step:1748/2245 train_time:106823ms step_avg:61.11ms
step:1749/2245 train_time:106886ms step_avg:61.11ms
step:1750/2245 train_time:106947ms step_avg:61.11ms
step:1750/2245 val_loss:3.3784 train_time:107012ms step_avg:61.15ms
step:1751/2245 train_time:107030ms step_avg:61.13ms
step:1752/2245 train_time:107075ms step_avg:61.12ms
step:1753/2245 train_time:107141ms step_avg:61.12ms
step:1754/2245 train_time:107202ms step_avg:61.12ms
step:1755/2245 train_time:107266ms step_avg:61.12ms
step:1756/2245 train_time:107326ms step_avg:61.12ms
step:1757/2245 train_time:107388ms step_avg:61.12ms
step:1758/2245 train_time:107448ms step_avg:61.12ms
step:1759/2245 train_time:107510ms step_avg:61.12ms
step:1760/2245 train_time:107570ms step_avg:61.12ms
step:1761/2245 train_time:107633ms step_avg:61.12ms
step:1762/2245 train_time:107693ms step_avg:61.12ms
step:1763/2245 train_time:107755ms step_avg:61.12ms
step:1764/2245 train_time:107815ms step_avg:61.12ms
step:1765/2245 train_time:107877ms step_avg:61.12ms
step:1766/2245 train_time:107938ms step_avg:61.12ms
step:1767/2245 train_time:108002ms step_avg:61.12ms
step:1768/2245 train_time:108064ms step_avg:61.12ms
step:1769/2245 train_time:108129ms step_avg:61.12ms
step:1770/2245 train_time:108191ms step_avg:61.12ms
step:1771/2245 train_time:108255ms step_avg:61.13ms
step:1772/2245 train_time:108315ms step_avg:61.13ms
step:1773/2245 train_time:108377ms step_avg:61.13ms
step:1774/2245 train_time:108437ms step_avg:61.13ms
step:1775/2245 train_time:108499ms step_avg:61.13ms
step:1776/2245 train_time:108559ms step_avg:61.13ms
step:1777/2245 train_time:108622ms step_avg:61.13ms
step:1778/2245 train_time:108682ms step_avg:61.13ms
step:1779/2245 train_time:108745ms step_avg:61.13ms
step:1780/2245 train_time:108805ms step_avg:61.13ms
step:1781/2245 train_time:108868ms step_avg:61.13ms
step:1782/2245 train_time:108931ms step_avg:61.13ms
step:1783/2245 train_time:108995ms step_avg:61.13ms
step:1784/2245 train_time:109055ms step_avg:61.13ms
step:1785/2245 train_time:109119ms step_avg:61.13ms
step:1786/2245 train_time:109179ms step_avg:61.13ms
step:1787/2245 train_time:109242ms step_avg:61.13ms
step:1788/2245 train_time:109304ms step_avg:61.13ms
step:1789/2245 train_time:109367ms step_avg:61.13ms
step:1790/2245 train_time:109427ms step_avg:61.13ms
step:1791/2245 train_time:109490ms step_avg:61.13ms
step:1792/2245 train_time:109550ms step_avg:61.13ms
step:1793/2245 train_time:109614ms step_avg:61.13ms
step:1794/2245 train_time:109673ms step_avg:61.13ms
step:1795/2245 train_time:109735ms step_avg:61.13ms
step:1796/2245 train_time:109794ms step_avg:61.13ms
step:1797/2245 train_time:109857ms step_avg:61.13ms
step:1798/2245 train_time:109917ms step_avg:61.13ms
step:1799/2245 train_time:109980ms step_avg:61.13ms
step:1800/2245 train_time:110042ms step_avg:61.13ms
step:1801/2245 train_time:110105ms step_avg:61.14ms
step:1802/2245 train_time:110167ms step_avg:61.14ms
step:1803/2245 train_time:110230ms step_avg:61.14ms
step:1804/2245 train_time:110291ms step_avg:61.14ms
step:1805/2245 train_time:110354ms step_avg:61.14ms
step:1806/2245 train_time:110415ms step_avg:61.14ms
step:1807/2245 train_time:110477ms step_avg:61.14ms
step:1808/2245 train_time:110537ms step_avg:61.14ms
step:1809/2245 train_time:110600ms step_avg:61.14ms
step:1810/2245 train_time:110660ms step_avg:61.14ms
step:1811/2245 train_time:110723ms step_avg:61.14ms
step:1812/2245 train_time:110783ms step_avg:61.14ms
step:1813/2245 train_time:110846ms step_avg:61.14ms
step:1814/2245 train_time:110906ms step_avg:61.14ms
step:1815/2245 train_time:110970ms step_avg:61.14ms
step:1816/2245 train_time:111031ms step_avg:61.14ms
step:1817/2245 train_time:111095ms step_avg:61.14ms
step:1818/2245 train_time:111156ms step_avg:61.14ms
step:1819/2245 train_time:111218ms step_avg:61.14ms
step:1820/2245 train_time:111278ms step_avg:61.14ms
step:1821/2245 train_time:111342ms step_avg:61.14ms
step:1822/2245 train_time:111402ms step_avg:61.14ms
step:1823/2245 train_time:111464ms step_avg:61.14ms
step:1824/2245 train_time:111525ms step_avg:61.14ms
step:1825/2245 train_time:111588ms step_avg:61.14ms
step:1826/2245 train_time:111648ms step_avg:61.14ms
step:1827/2245 train_time:111711ms step_avg:61.14ms
step:1828/2245 train_time:111772ms step_avg:61.14ms
step:1829/2245 train_time:111834ms step_avg:61.15ms
step:1830/2245 train_time:111894ms step_avg:61.14ms
step:1831/2245 train_time:111957ms step_avg:61.15ms
step:1832/2245 train_time:112017ms step_avg:61.14ms
step:1833/2245 train_time:112080ms step_avg:61.15ms
step:1834/2245 train_time:112141ms step_avg:61.15ms
step:1835/2245 train_time:112204ms step_avg:61.15ms
step:1836/2245 train_time:112264ms step_avg:61.15ms
step:1837/2245 train_time:112327ms step_avg:61.15ms
step:1838/2245 train_time:112387ms step_avg:61.15ms
step:1839/2245 train_time:112450ms step_avg:61.15ms
step:1840/2245 train_time:112511ms step_avg:61.15ms
step:1841/2245 train_time:112575ms step_avg:61.15ms
step:1842/2245 train_time:112635ms step_avg:61.15ms
step:1843/2245 train_time:112698ms step_avg:61.15ms
step:1844/2245 train_time:112758ms step_avg:61.15ms
step:1845/2245 train_time:112820ms step_avg:61.15ms
step:1846/2245 train_time:112881ms step_avg:61.15ms
step:1847/2245 train_time:112944ms step_avg:61.15ms
step:1848/2245 train_time:113004ms step_avg:61.15ms
step:1849/2245 train_time:113067ms step_avg:61.15ms
step:1850/2245 train_time:113128ms step_avg:61.15ms
step:1851/2245 train_time:113191ms step_avg:61.15ms
step:1852/2245 train_time:113252ms step_avg:61.15ms
step:1853/2245 train_time:113314ms step_avg:61.15ms
step:1854/2245 train_time:113374ms step_avg:61.15ms
step:1855/2245 train_time:113437ms step_avg:61.15ms
step:1856/2245 train_time:113497ms step_avg:61.15ms
step:1857/2245 train_time:113560ms step_avg:61.15ms
step:1858/2245 train_time:113620ms step_avg:61.15ms
step:1859/2245 train_time:113683ms step_avg:61.15ms
step:1860/2245 train_time:113743ms step_avg:61.15ms
step:1861/2245 train_time:113807ms step_avg:61.15ms
step:1862/2245 train_time:113867ms step_avg:61.15ms
step:1863/2245 train_time:113931ms step_avg:61.15ms
step:1864/2245 train_time:113992ms step_avg:61.15ms
step:1865/2245 train_time:114055ms step_avg:61.16ms
step:1866/2245 train_time:114116ms step_avg:61.16ms
step:1867/2245 train_time:114179ms step_avg:61.16ms
step:1868/2245 train_time:114240ms step_avg:61.16ms
step:1869/2245 train_time:114302ms step_avg:61.16ms
step:1870/2245 train_time:114362ms step_avg:61.16ms
step:1871/2245 train_time:114426ms step_avg:61.16ms
step:1872/2245 train_time:114486ms step_avg:61.16ms
step:1873/2245 train_time:114549ms step_avg:61.16ms
step:1874/2245 train_time:114609ms step_avg:61.16ms
step:1875/2245 train_time:114672ms step_avg:61.16ms
step:1876/2245 train_time:114733ms step_avg:61.16ms
step:1877/2245 train_time:114795ms step_avg:61.16ms
step:1878/2245 train_time:114856ms step_avg:61.16ms
step:1879/2245 train_time:114918ms step_avg:61.16ms
step:1880/2245 train_time:114978ms step_avg:61.16ms
step:1881/2245 train_time:115041ms step_avg:61.16ms
step:1882/2245 train_time:115101ms step_avg:61.16ms
step:1883/2245 train_time:115165ms step_avg:61.16ms
step:1884/2245 train_time:115225ms step_avg:61.16ms
step:1885/2245 train_time:115288ms step_avg:61.16ms
step:1886/2245 train_time:115349ms step_avg:61.16ms
step:1887/2245 train_time:115411ms step_avg:61.16ms
step:1888/2245 train_time:115471ms step_avg:61.16ms
step:1889/2245 train_time:115534ms step_avg:61.16ms
step:1890/2245 train_time:115595ms step_avg:61.16ms
step:1891/2245 train_time:115657ms step_avg:61.16ms
step:1892/2245 train_time:115718ms step_avg:61.16ms
step:1893/2245 train_time:115781ms step_avg:61.16ms
step:1894/2245 train_time:115841ms step_avg:61.16ms
step:1895/2245 train_time:115903ms step_avg:61.16ms
step:1896/2245 train_time:115964ms step_avg:61.16ms
step:1897/2245 train_time:116028ms step_avg:61.16ms
step:1898/2245 train_time:116088ms step_avg:61.16ms
step:1899/2245 train_time:116152ms step_avg:61.16ms
step:1900/2245 train_time:116212ms step_avg:61.16ms
step:1901/2245 train_time:116276ms step_avg:61.17ms
step:1902/2245 train_time:116336ms step_avg:61.16ms
step:1903/2245 train_time:116398ms step_avg:61.17ms
step:1904/2245 train_time:116458ms step_avg:61.16ms
step:1905/2245 train_time:116521ms step_avg:61.17ms
step:1906/2245 train_time:116581ms step_avg:61.17ms
step:1907/2245 train_time:116644ms step_avg:61.17ms
step:1908/2245 train_time:116704ms step_avg:61.17ms
step:1909/2245 train_time:116767ms step_avg:61.17ms
step:1910/2245 train_time:116828ms step_avg:61.17ms
step:1911/2245 train_time:116891ms step_avg:61.17ms
step:1912/2245 train_time:116951ms step_avg:61.17ms
step:1913/2245 train_time:117015ms step_avg:61.17ms
step:1914/2245 train_time:117075ms step_avg:61.17ms
step:1915/2245 train_time:117137ms step_avg:61.17ms
step:1916/2245 train_time:117198ms step_avg:61.17ms
step:1917/2245 train_time:117260ms step_avg:61.17ms
step:1918/2245 train_time:117321ms step_avg:61.17ms
step:1919/2245 train_time:117384ms step_avg:61.17ms
step:1920/2245 train_time:117444ms step_avg:61.17ms
step:1921/2245 train_time:117507ms step_avg:61.17ms
step:1922/2245 train_time:117568ms step_avg:61.17ms
step:1923/2245 train_time:117632ms step_avg:61.17ms
step:1924/2245 train_time:117693ms step_avg:61.17ms
step:1925/2245 train_time:117755ms step_avg:61.17ms
step:1926/2245 train_time:117815ms step_avg:61.17ms
step:1927/2245 train_time:117878ms step_avg:61.17ms
step:1928/2245 train_time:117939ms step_avg:61.17ms
step:1929/2245 train_time:118001ms step_avg:61.17ms
step:1930/2245 train_time:118061ms step_avg:61.17ms
step:1931/2245 train_time:118124ms step_avg:61.17ms
step:1932/2245 train_time:118183ms step_avg:61.17ms
step:1933/2245 train_time:118247ms step_avg:61.17ms
step:1934/2245 train_time:118307ms step_avg:61.17ms
step:1935/2245 train_time:118370ms step_avg:61.17ms
step:1936/2245 train_time:118431ms step_avg:61.17ms
step:1937/2245 train_time:118494ms step_avg:61.17ms
step:1938/2245 train_time:118555ms step_avg:61.17ms
step:1939/2245 train_time:118618ms step_avg:61.17ms
step:1940/2245 train_time:118678ms step_avg:61.17ms
step:1941/2245 train_time:118741ms step_avg:61.18ms
step:1942/2245 train_time:118801ms step_avg:61.17ms
step:1943/2245 train_time:118863ms step_avg:61.18ms
step:1944/2245 train_time:118923ms step_avg:61.17ms
step:1945/2245 train_time:118986ms step_avg:61.18ms
step:1946/2245 train_time:119047ms step_avg:61.18ms
step:1947/2245 train_time:119110ms step_avg:61.18ms
step:1948/2245 train_time:119170ms step_avg:61.18ms
step:1949/2245 train_time:119233ms step_avg:61.18ms
step:1950/2245 train_time:119293ms step_avg:61.18ms
step:1951/2245 train_time:119356ms step_avg:61.18ms
step:1952/2245 train_time:119416ms step_avg:61.18ms
step:1953/2245 train_time:119478ms step_avg:61.18ms
step:1954/2245 train_time:119539ms step_avg:61.18ms
step:1955/2245 train_time:119601ms step_avg:61.18ms
step:1956/2245 train_time:119662ms step_avg:61.18ms
step:1957/2245 train_time:119725ms step_avg:61.18ms
step:1958/2245 train_time:119786ms step_avg:61.18ms
step:1959/2245 train_time:119849ms step_avg:61.18ms
step:1960/2245 train_time:119909ms step_avg:61.18ms
step:1961/2245 train_time:119971ms step_avg:61.18ms
step:1962/2245 train_time:120032ms step_avg:61.18ms
step:1963/2245 train_time:120095ms step_avg:61.18ms
step:1964/2245 train_time:120155ms step_avg:61.18ms
step:1965/2245 train_time:120218ms step_avg:61.18ms
step:1966/2245 train_time:120278ms step_avg:61.18ms
step:1967/2245 train_time:120341ms step_avg:61.18ms
step:1968/2245 train_time:120401ms step_avg:61.18ms
step:1969/2245 train_time:120465ms step_avg:61.18ms
step:1970/2245 train_time:120525ms step_avg:61.18ms
step:1971/2245 train_time:120587ms step_avg:61.18ms
step:1972/2245 train_time:120648ms step_avg:61.18ms
step:1973/2245 train_time:120711ms step_avg:61.18ms
step:1974/2245 train_time:120772ms step_avg:61.18ms
step:1975/2245 train_time:120836ms step_avg:61.18ms
step:1976/2245 train_time:120896ms step_avg:61.18ms
step:1977/2245 train_time:120958ms step_avg:61.18ms
step:1978/2245 train_time:121018ms step_avg:61.18ms
step:1979/2245 train_time:121081ms step_avg:61.18ms
step:1980/2245 train_time:121143ms step_avg:61.18ms
step:1981/2245 train_time:121206ms step_avg:61.18ms
step:1982/2245 train_time:121267ms step_avg:61.18ms
step:1983/2245 train_time:121330ms step_avg:61.19ms
step:1984/2245 train_time:121391ms step_avg:61.18ms
step:1985/2245 train_time:121454ms step_avg:61.19ms
step:1986/2245 train_time:121514ms step_avg:61.19ms
step:1987/2245 train_time:121576ms step_avg:61.19ms
step:1988/2245 train_time:121636ms step_avg:61.19ms
step:1989/2245 train_time:121699ms step_avg:61.19ms
step:1990/2245 train_time:121759ms step_avg:61.19ms
step:1991/2245 train_time:121821ms step_avg:61.19ms
step:1992/2245 train_time:121881ms step_avg:61.19ms
step:1993/2245 train_time:121944ms step_avg:61.19ms
step:1994/2245 train_time:122004ms step_avg:61.19ms
step:1995/2245 train_time:122067ms step_avg:61.19ms
step:1996/2245 train_time:122129ms step_avg:61.19ms
step:1997/2245 train_time:122191ms step_avg:61.19ms
step:1998/2245 train_time:122252ms step_avg:61.19ms
step:1999/2245 train_time:122314ms step_avg:61.19ms
step:2000/2245 train_time:122375ms step_avg:61.19ms
step:2000/2245 val_loss:3.3237 train_time:122438ms step_avg:61.22ms
step:2001/2245 train_time:122457ms step_avg:61.20ms
step:2002/2245 train_time:122501ms step_avg:61.19ms
step:2003/2245 train_time:122568ms step_avg:61.19ms
step:2004/2245 train_time:122630ms step_avg:61.19ms
step:2005/2245 train_time:122694ms step_avg:61.19ms
step:2006/2245 train_time:122754ms step_avg:61.19ms
step:2007/2245 train_time:122816ms step_avg:61.19ms
step:2008/2245 train_time:122876ms step_avg:61.19ms
step:2009/2245 train_time:122938ms step_avg:61.19ms
step:2010/2245 train_time:122997ms step_avg:61.19ms
step:2011/2245 train_time:123059ms step_avg:61.19ms
step:2012/2245 train_time:123119ms step_avg:61.19ms
step:2013/2245 train_time:123181ms step_avg:61.19ms
step:2014/2245 train_time:123240ms step_avg:61.19ms
step:2015/2245 train_time:123302ms step_avg:61.19ms
step:2016/2245 train_time:123362ms step_avg:61.19ms
step:2017/2245 train_time:123426ms step_avg:61.19ms
step:2018/2245 train_time:123488ms step_avg:61.19ms
step:2019/2245 train_time:123553ms step_avg:61.19ms
step:2020/2245 train_time:123614ms step_avg:61.20ms
step:2021/2245 train_time:123678ms step_avg:61.20ms
step:2022/2245 train_time:123739ms step_avg:61.20ms
step:2023/2245 train_time:123801ms step_avg:61.20ms
step:2024/2245 train_time:123861ms step_avg:61.20ms
step:2025/2245 train_time:123925ms step_avg:61.20ms
step:2026/2245 train_time:123985ms step_avg:61.20ms
step:2027/2245 train_time:124048ms step_avg:61.20ms
step:2028/2245 train_time:124108ms step_avg:61.20ms
step:2029/2245 train_time:124170ms step_avg:61.20ms
step:2030/2245 train_time:124231ms step_avg:61.20ms
step:2031/2245 train_time:124294ms step_avg:61.20ms
step:2032/2245 train_time:124355ms step_avg:61.20ms
step:2033/2245 train_time:124419ms step_avg:61.20ms
step:2034/2245 train_time:124479ms step_avg:61.20ms
step:2035/2245 train_time:124542ms step_avg:61.20ms
step:2036/2245 train_time:124603ms step_avg:61.20ms
step:2037/2245 train_time:124667ms step_avg:61.20ms
step:2038/2245 train_time:124728ms step_avg:61.20ms
step:2039/2245 train_time:124792ms step_avg:61.20ms
step:2040/2245 train_time:124854ms step_avg:61.20ms
step:2041/2245 train_time:124916ms step_avg:61.20ms
step:2042/2245 train_time:124976ms step_avg:61.20ms
step:2043/2245 train_time:125039ms step_avg:61.20ms
step:2044/2245 train_time:125099ms step_avg:61.20ms
step:2045/2245 train_time:125161ms step_avg:61.20ms
step:2046/2245 train_time:125221ms step_avg:61.20ms
step:2047/2245 train_time:125284ms step_avg:61.20ms
step:2048/2245 train_time:125344ms step_avg:61.20ms
step:2049/2245 train_time:125407ms step_avg:61.20ms
step:2050/2245 train_time:125469ms step_avg:61.20ms
step:2051/2245 train_time:125532ms step_avg:61.21ms
step:2052/2245 train_time:125594ms step_avg:61.21ms
step:2053/2245 train_time:125657ms step_avg:61.21ms
step:2054/2245 train_time:125718ms step_avg:61.21ms
step:2055/2245 train_time:125781ms step_avg:61.21ms
step:2056/2245 train_time:125842ms step_avg:61.21ms
step:2057/2245 train_time:125905ms step_avg:61.21ms
step:2058/2245 train_time:125965ms step_avg:61.21ms
step:2059/2245 train_time:126028ms step_avg:61.21ms
step:2060/2245 train_time:126087ms step_avg:61.21ms
step:2061/2245 train_time:126151ms step_avg:61.21ms
step:2062/2245 train_time:126212ms step_avg:61.21ms
step:2063/2245 train_time:126275ms step_avg:61.21ms
step:2064/2245 train_time:126336ms step_avg:61.21ms
step:2065/2245 train_time:126399ms step_avg:61.21ms
step:2066/2245 train_time:126460ms step_avg:61.21ms
step:2067/2245 train_time:126523ms step_avg:61.21ms
step:2068/2245 train_time:126583ms step_avg:61.21ms
step:2069/2245 train_time:126646ms step_avg:61.21ms
step:2070/2245 train_time:126707ms step_avg:61.21ms
step:2071/2245 train_time:126771ms step_avg:61.21ms
step:2072/2245 train_time:126832ms step_avg:61.21ms
step:2073/2245 train_time:126897ms step_avg:61.21ms
step:2074/2245 train_time:126957ms step_avg:61.21ms
step:2075/2245 train_time:127019ms step_avg:61.21ms
step:2076/2245 train_time:127079ms step_avg:61.21ms
step:2077/2245 train_time:127142ms step_avg:61.21ms
step:2078/2245 train_time:127202ms step_avg:61.21ms
step:2079/2245 train_time:127265ms step_avg:61.21ms
step:2080/2245 train_time:127325ms step_avg:61.21ms
step:2081/2245 train_time:127387ms step_avg:61.21ms
step:2082/2245 train_time:127448ms step_avg:61.21ms
step:2083/2245 train_time:127512ms step_avg:61.22ms
step:2084/2245 train_time:127572ms step_avg:61.22ms
step:2085/2245 train_time:127635ms step_avg:61.22ms
step:2086/2245 train_time:127697ms step_avg:61.22ms
step:2087/2245 train_time:127759ms step_avg:61.22ms
step:2088/2245 train_time:127820ms step_avg:61.22ms
step:2089/2245 train_time:127883ms step_avg:61.22ms
step:2090/2245 train_time:127943ms step_avg:61.22ms
step:2091/2245 train_time:128006ms step_avg:61.22ms
step:2092/2245 train_time:128067ms step_avg:61.22ms
step:2093/2245 train_time:128130ms step_avg:61.22ms
step:2094/2245 train_time:128191ms step_avg:61.22ms
step:2095/2245 train_time:128255ms step_avg:61.22ms
step:2096/2245 train_time:128315ms step_avg:61.22ms
step:2097/2245 train_time:128377ms step_avg:61.22ms
step:2098/2245 train_time:128437ms step_avg:61.22ms
step:2099/2245 train_time:128500ms step_avg:61.22ms
step:2100/2245 train_time:128560ms step_avg:61.22ms
step:2101/2245 train_time:128623ms step_avg:61.22ms
step:2102/2245 train_time:128683ms step_avg:61.22ms
step:2103/2245 train_time:128746ms step_avg:61.22ms
step:2104/2245 train_time:128806ms step_avg:61.22ms
step:2105/2245 train_time:128871ms step_avg:61.22ms
step:2106/2245 train_time:128932ms step_avg:61.22ms
step:2107/2245 train_time:128995ms step_avg:61.22ms
step:2108/2245 train_time:129056ms step_avg:61.22ms
step:2109/2245 train_time:129118ms step_avg:61.22ms
step:2110/2245 train_time:129178ms step_avg:61.22ms
step:2111/2245 train_time:129241ms step_avg:61.22ms
step:2112/2245 train_time:129301ms step_avg:61.22ms
step:2113/2245 train_time:129364ms step_avg:61.22ms
step:2114/2245 train_time:129423ms step_avg:61.22ms
step:2115/2245 train_time:129486ms step_avg:61.22ms
step:2116/2245 train_time:129546ms step_avg:61.22ms
step:2117/2245 train_time:129609ms step_avg:61.22ms
step:2118/2245 train_time:129670ms step_avg:61.22ms
step:2119/2245 train_time:129733ms step_avg:61.22ms
step:2120/2245 train_time:129794ms step_avg:61.22ms
step:2121/2245 train_time:129857ms step_avg:61.22ms
step:2122/2245 train_time:129917ms step_avg:61.22ms
step:2123/2245 train_time:129980ms step_avg:61.22ms
step:2124/2245 train_time:130041ms step_avg:61.22ms
step:2125/2245 train_time:130103ms step_avg:61.23ms
step:2126/2245 train_time:130163ms step_avg:61.22ms
step:2127/2245 train_time:130226ms step_avg:61.23ms
step:2128/2245 train_time:130286ms step_avg:61.22ms
step:2129/2245 train_time:130349ms step_avg:61.23ms
step:2130/2245 train_time:130410ms step_avg:61.23ms
step:2131/2245 train_time:130473ms step_avg:61.23ms
step:2132/2245 train_time:130534ms step_avg:61.23ms
step:2133/2245 train_time:130598ms step_avg:61.23ms
step:2134/2245 train_time:130658ms step_avg:61.23ms
step:2135/2245 train_time:130720ms step_avg:61.23ms
step:2136/2245 train_time:130780ms step_avg:61.23ms
step:2137/2245 train_time:130844ms step_avg:61.23ms
step:2138/2245 train_time:130905ms step_avg:61.23ms
step:2139/2245 train_time:130967ms step_avg:61.23ms
step:2140/2245 train_time:131028ms step_avg:61.23ms
step:2141/2245 train_time:131091ms step_avg:61.23ms
step:2142/2245 train_time:131152ms step_avg:61.23ms
step:2143/2245 train_time:131216ms step_avg:61.23ms
step:2144/2245 train_time:131276ms step_avg:61.23ms
step:2145/2245 train_time:131338ms step_avg:61.23ms
step:2146/2245 train_time:131398ms step_avg:61.23ms
step:2147/2245 train_time:131461ms step_avg:61.23ms
step:2148/2245 train_time:131521ms step_avg:61.23ms
step:2149/2245 train_time:131584ms step_avg:61.23ms
step:2150/2245 train_time:131645ms step_avg:61.23ms
step:2151/2245 train_time:131708ms step_avg:61.23ms
step:2152/2245 train_time:131769ms step_avg:61.23ms
step:2153/2245 train_time:131833ms step_avg:61.23ms
step:2154/2245 train_time:131894ms step_avg:61.23ms
step:2155/2245 train_time:131957ms step_avg:61.23ms
step:2156/2245 train_time:132016ms step_avg:61.23ms
step:2157/2245 train_time:132080ms step_avg:61.23ms
step:2158/2245 train_time:132140ms step_avg:61.23ms
step:2159/2245 train_time:132203ms step_avg:61.23ms
step:2160/2245 train_time:132263ms step_avg:61.23ms
step:2161/2245 train_time:132326ms step_avg:61.23ms
step:2162/2245 train_time:132386ms step_avg:61.23ms
step:2163/2245 train_time:132449ms step_avg:61.23ms
step:2164/2245 train_time:132510ms step_avg:61.23ms
step:2165/2245 train_time:132573ms step_avg:61.23ms
step:2166/2245 train_time:132635ms step_avg:61.23ms
step:2167/2245 train_time:132697ms step_avg:61.24ms
step:2168/2245 train_time:132757ms step_avg:61.23ms
step:2169/2245 train_time:132820ms step_avg:61.24ms
step:2170/2245 train_time:132880ms step_avg:61.24ms
step:2171/2245 train_time:132944ms step_avg:61.24ms
step:2172/2245 train_time:133005ms step_avg:61.24ms
step:2173/2245 train_time:133068ms step_avg:61.24ms
step:2174/2245 train_time:133128ms step_avg:61.24ms
step:2175/2245 train_time:133191ms step_avg:61.24ms
step:2176/2245 train_time:133252ms step_avg:61.24ms
step:2177/2245 train_time:133315ms step_avg:61.24ms
step:2178/2245 train_time:133375ms step_avg:61.24ms
step:2179/2245 train_time:133438ms step_avg:61.24ms
step:2180/2245 train_time:133498ms step_avg:61.24ms
step:2181/2245 train_time:133560ms step_avg:61.24ms
step:2182/2245 train_time:133620ms step_avg:61.24ms
step:2183/2245 train_time:133683ms step_avg:61.24ms
step:2184/2245 train_time:133744ms step_avg:61.24ms
step:2185/2245 train_time:133807ms step_avg:61.24ms
step:2186/2245 train_time:133868ms step_avg:61.24ms
step:2187/2245 train_time:133932ms step_avg:61.24ms
step:2188/2245 train_time:133993ms step_avg:61.24ms
step:2189/2245 train_time:134057ms step_avg:61.24ms
step:2190/2245 train_time:134117ms step_avg:61.24ms
step:2191/2245 train_time:134179ms step_avg:61.24ms
step:2192/2245 train_time:134239ms step_avg:61.24ms
step:2193/2245 train_time:134302ms step_avg:61.24ms
step:2194/2245 train_time:134363ms step_avg:61.24ms
step:2195/2245 train_time:134425ms step_avg:61.24ms
step:2196/2245 train_time:134486ms step_avg:61.24ms
step:2197/2245 train_time:134549ms step_avg:61.24ms
step:2198/2245 train_time:134610ms step_avg:61.24ms
step:2199/2245 train_time:134674ms step_avg:61.24ms
step:2200/2245 train_time:134734ms step_avg:61.24ms
step:2201/2245 train_time:134798ms step_avg:61.24ms
step:2202/2245 train_time:134858ms step_avg:61.24ms
step:2203/2245 train_time:134920ms step_avg:61.24ms
step:2204/2245 train_time:134981ms step_avg:61.24ms
step:2205/2245 train_time:135043ms step_avg:61.24ms
step:2206/2245 train_time:135104ms step_avg:61.24ms
step:2207/2245 train_time:135167ms step_avg:61.24ms
step:2208/2245 train_time:135227ms step_avg:61.24ms
step:2209/2245 train_time:135290ms step_avg:61.25ms
step:2210/2245 train_time:135352ms step_avg:61.25ms
step:2211/2245 train_time:135415ms step_avg:61.25ms
step:2212/2245 train_time:135475ms step_avg:61.25ms
step:2213/2245 train_time:135538ms step_avg:61.25ms
step:2214/2245 train_time:135599ms step_avg:61.25ms
step:2215/2245 train_time:135662ms step_avg:61.25ms
step:2216/2245 train_time:135722ms step_avg:61.25ms
step:2217/2245 train_time:135785ms step_avg:61.25ms
step:2218/2245 train_time:135845ms step_avg:61.25ms
step:2219/2245 train_time:135909ms step_avg:61.25ms
step:2220/2245 train_time:135970ms step_avg:61.25ms
step:2221/2245 train_time:136033ms step_avg:61.25ms
step:2222/2245 train_time:136094ms step_avg:61.25ms
step:2223/2245 train_time:136158ms step_avg:61.25ms
step:2224/2245 train_time:136217ms step_avg:61.25ms
step:2225/2245 train_time:136280ms step_avg:61.25ms
step:2226/2245 train_time:136341ms step_avg:61.25ms
step:2227/2245 train_time:136405ms step_avg:61.25ms
step:2228/2245 train_time:136466ms step_avg:61.25ms
step:2229/2245 train_time:136529ms step_avg:61.25ms
step:2230/2245 train_time:136589ms step_avg:61.25ms
step:2231/2245 train_time:136653ms step_avg:61.25ms
step:2232/2245 train_time:136713ms step_avg:61.25ms
step:2233/2245 train_time:136777ms step_avg:61.25ms
step:2234/2245 train_time:136837ms step_avg:61.25ms
step:2235/2245 train_time:136899ms step_avg:61.25ms
step:2236/2245 train_time:136960ms step_avg:61.25ms
step:2237/2245 train_time:137024ms step_avg:61.25ms
step:2238/2245 train_time:137085ms step_avg:61.25ms
step:2239/2245 train_time:137148ms step_avg:61.25ms
step:2240/2245 train_time:137210ms step_avg:61.25ms
step:2241/2245 train_time:137274ms step_avg:61.26ms
step:2242/2245 train_time:137335ms step_avg:61.26ms
step:2243/2245 train_time:137398ms step_avg:61.26ms
step:2244/2245 train_time:137459ms step_avg:61.26ms
step:2245/2245 train_time:137521ms step_avg:61.26ms
step:2245/2245 val_loss:3.2784 train_time:137582ms step_avg:61.28ms
peak memory allocated: 29248 MiB reserved: 50528 MiB

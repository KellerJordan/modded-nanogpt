import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Nov 10 21:44:05 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            132W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   41C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   34C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   40C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2245 train_time:123ms step_avg:122.63ms
step:2/2245 train_time:144ms step_avg:71.98ms
step:3/2245 train_time:182ms step_avg:60.74ms
step:4/2245 train_time:239ms step_avg:59.63ms
step:5/2245 train_time:298ms step_avg:59.57ms
step:6/2245 train_time:357ms step_avg:59.45ms
step:7/2245 train_time:441ms step_avg:63.00ms
step:8/2245 train_time:499ms step_avg:62.38ms
step:9/2245 train_time:560ms step_avg:62.23ms
step:10/2245 train_time:619ms step_avg:61.87ms
step:11/2245 train_time:680ms step_avg:61.82ms
step:12/2245 train_time:739ms step_avg:61.56ms
step:13/2245 train_time:800ms step_avg:61.55ms
step:14/2245 train_time:860ms step_avg:61.43ms
step:15/2245 train_time:921ms step_avg:61.40ms
step:16/2245 train_time:980ms step_avg:61.27ms
step:17/2245 train_time:1044ms step_avg:61.39ms
step:18/2245 train_time:1104ms step_avg:61.35ms
step:19/2245 train_time:1168ms step_avg:61.47ms
step:20/2245 train_time:1228ms step_avg:61.40ms
step:21/2245 train_time:1290ms step_avg:61.43ms
step:22/2245 train_time:1349ms step_avg:61.33ms
step:23/2245 train_time:1412ms step_avg:61.39ms
step:24/2245 train_time:1473ms step_avg:61.37ms
step:25/2245 train_time:1534ms step_avg:61.37ms
step:26/2245 train_time:1594ms step_avg:61.31ms
step:27/2245 train_time:1656ms step_avg:61.32ms
step:28/2245 train_time:1714ms step_avg:61.22ms
step:29/2245 train_time:1776ms step_avg:61.24ms
step:30/2245 train_time:1835ms step_avg:61.18ms
step:31/2245 train_time:1897ms step_avg:61.19ms
step:32/2245 train_time:1957ms step_avg:61.15ms
step:33/2245 train_time:2020ms step_avg:61.20ms
step:34/2245 train_time:2081ms step_avg:61.20ms
step:35/2245 train_time:2144ms step_avg:61.26ms
step:36/2245 train_time:2204ms step_avg:61.21ms
step:37/2245 train_time:2266ms step_avg:61.23ms
step:38/2245 train_time:2325ms step_avg:61.18ms
step:39/2245 train_time:2387ms step_avg:61.22ms
step:40/2245 train_time:2447ms step_avg:61.18ms
step:41/2245 train_time:2510ms step_avg:61.21ms
step:42/2245 train_time:2569ms step_avg:61.16ms
step:43/2245 train_time:2630ms step_avg:61.16ms
step:44/2245 train_time:2689ms step_avg:61.11ms
step:45/2245 train_time:2751ms step_avg:61.14ms
step:46/2245 train_time:2810ms step_avg:61.09ms
step:47/2245 train_time:2872ms step_avg:61.11ms
step:48/2245 train_time:2931ms step_avg:61.07ms
step:49/2245 train_time:2994ms step_avg:61.10ms
step:50/2245 train_time:3054ms step_avg:61.07ms
step:51/2245 train_time:3116ms step_avg:61.09ms
step:52/2245 train_time:3176ms step_avg:61.07ms
step:53/2245 train_time:3238ms step_avg:61.09ms
step:54/2245 train_time:3297ms step_avg:61.06ms
step:55/2245 train_time:3359ms step_avg:61.08ms
step:56/2245 train_time:3420ms step_avg:61.07ms
step:57/2245 train_time:3483ms step_avg:61.10ms
step:58/2245 train_time:3542ms step_avg:61.06ms
step:59/2245 train_time:3603ms step_avg:61.07ms
step:60/2245 train_time:3662ms step_avg:61.04ms
step:61/2245 train_time:3724ms step_avg:61.05ms
step:62/2245 train_time:3783ms step_avg:61.02ms
step:63/2245 train_time:3846ms step_avg:61.04ms
step:64/2245 train_time:3904ms step_avg:61.01ms
step:65/2245 train_time:3966ms step_avg:61.02ms
step:66/2245 train_time:4026ms step_avg:61.00ms
step:67/2245 train_time:4087ms step_avg:61.01ms
step:68/2245 train_time:4146ms step_avg:60.98ms
step:69/2245 train_time:4208ms step_avg:60.98ms
step:70/2245 train_time:4267ms step_avg:60.96ms
step:71/2245 train_time:4329ms step_avg:60.97ms
step:72/2245 train_time:4388ms step_avg:60.95ms
step:73/2245 train_time:4450ms step_avg:60.96ms
step:74/2245 train_time:4510ms step_avg:60.94ms
step:75/2245 train_time:4572ms step_avg:60.96ms
step:76/2245 train_time:4631ms step_avg:60.94ms
step:77/2245 train_time:4693ms step_avg:60.95ms
step:78/2245 train_time:4752ms step_avg:60.92ms
step:79/2245 train_time:4814ms step_avg:60.94ms
step:80/2245 train_time:4873ms step_avg:60.92ms
step:81/2245 train_time:4935ms step_avg:60.92ms
step:82/2245 train_time:4994ms step_avg:60.91ms
step:83/2245 train_time:5056ms step_avg:60.92ms
step:84/2245 train_time:5116ms step_avg:60.90ms
step:85/2245 train_time:5178ms step_avg:60.92ms
step:86/2245 train_time:5238ms step_avg:60.91ms
step:87/2245 train_time:5301ms step_avg:60.93ms
step:88/2245 train_time:5361ms step_avg:60.92ms
step:89/2245 train_time:5423ms step_avg:60.93ms
step:90/2245 train_time:5482ms step_avg:60.91ms
step:91/2245 train_time:5544ms step_avg:60.92ms
step:92/2245 train_time:5603ms step_avg:60.91ms
step:93/2245 train_time:5666ms step_avg:60.92ms
step:94/2245 train_time:5725ms step_avg:60.91ms
step:95/2245 train_time:5786ms step_avg:60.91ms
step:96/2245 train_time:5845ms step_avg:60.88ms
step:97/2245 train_time:5906ms step_avg:60.89ms
step:98/2245 train_time:5967ms step_avg:60.89ms
step:99/2245 train_time:6028ms step_avg:60.89ms
step:100/2245 train_time:6087ms step_avg:60.87ms
step:101/2245 train_time:6148ms step_avg:60.87ms
step:102/2245 train_time:6207ms step_avg:60.85ms
step:103/2245 train_time:6269ms step_avg:60.86ms
step:104/2245 train_time:6328ms step_avg:60.85ms
step:105/2245 train_time:6390ms step_avg:60.86ms
step:106/2245 train_time:6450ms step_avg:60.85ms
step:107/2245 train_time:6512ms step_avg:60.86ms
step:108/2245 train_time:6571ms step_avg:60.84ms
step:109/2245 train_time:6632ms step_avg:60.85ms
step:110/2245 train_time:6691ms step_avg:60.83ms
step:111/2245 train_time:6752ms step_avg:60.83ms
step:112/2245 train_time:6812ms step_avg:60.82ms
step:113/2245 train_time:6873ms step_avg:60.83ms
step:114/2245 train_time:6933ms step_avg:60.81ms
step:115/2245 train_time:6997ms step_avg:60.84ms
step:116/2245 train_time:7054ms step_avg:60.81ms
step:117/2245 train_time:7116ms step_avg:60.82ms
step:118/2245 train_time:7175ms step_avg:60.81ms
step:119/2245 train_time:7237ms step_avg:60.81ms
step:120/2245 train_time:7297ms step_avg:60.81ms
step:121/2245 train_time:7360ms step_avg:60.82ms
step:122/2245 train_time:7420ms step_avg:60.82ms
step:123/2245 train_time:7482ms step_avg:60.83ms
step:124/2245 train_time:7541ms step_avg:60.82ms
step:125/2245 train_time:7603ms step_avg:60.82ms
step:126/2245 train_time:7662ms step_avg:60.81ms
step:127/2245 train_time:7725ms step_avg:60.82ms
step:128/2245 train_time:7784ms step_avg:60.81ms
step:129/2245 train_time:7845ms step_avg:60.81ms
step:130/2245 train_time:7904ms step_avg:60.80ms
step:131/2245 train_time:7966ms step_avg:60.81ms
step:132/2245 train_time:8025ms step_avg:60.79ms
step:133/2245 train_time:8087ms step_avg:60.81ms
step:134/2245 train_time:8146ms step_avg:60.79ms
step:135/2245 train_time:8207ms step_avg:60.80ms
step:136/2245 train_time:8267ms step_avg:60.78ms
step:137/2245 train_time:8328ms step_avg:60.79ms
step:138/2245 train_time:8387ms step_avg:60.78ms
step:139/2245 train_time:8449ms step_avg:60.79ms
step:140/2245 train_time:8508ms step_avg:60.77ms
step:141/2245 train_time:8571ms step_avg:60.78ms
step:142/2245 train_time:8630ms step_avg:60.77ms
step:143/2245 train_time:8691ms step_avg:60.78ms
step:144/2245 train_time:8750ms step_avg:60.76ms
step:145/2245 train_time:8812ms step_avg:60.77ms
step:146/2245 train_time:8871ms step_avg:60.76ms
step:147/2245 train_time:8932ms step_avg:60.76ms
step:148/2245 train_time:8991ms step_avg:60.75ms
step:149/2245 train_time:9053ms step_avg:60.76ms
step:150/2245 train_time:9112ms step_avg:60.74ms
step:151/2245 train_time:9173ms step_avg:60.75ms
step:152/2245 train_time:9232ms step_avg:60.74ms
step:153/2245 train_time:9294ms step_avg:60.74ms
step:154/2245 train_time:9353ms step_avg:60.73ms
step:155/2245 train_time:9415ms step_avg:60.74ms
step:156/2245 train_time:9475ms step_avg:60.74ms
step:157/2245 train_time:9537ms step_avg:60.75ms
step:158/2245 train_time:9597ms step_avg:60.74ms
step:159/2245 train_time:9659ms step_avg:60.75ms
step:160/2245 train_time:9719ms step_avg:60.75ms
step:161/2245 train_time:9781ms step_avg:60.75ms
step:162/2245 train_time:9841ms step_avg:60.74ms
step:163/2245 train_time:9902ms step_avg:60.75ms
step:164/2245 train_time:9961ms step_avg:60.74ms
step:165/2245 train_time:10022ms step_avg:60.74ms
step:166/2245 train_time:10081ms step_avg:60.73ms
step:167/2245 train_time:10143ms step_avg:60.74ms
step:168/2245 train_time:10202ms step_avg:60.73ms
step:169/2245 train_time:10264ms step_avg:60.73ms
step:170/2245 train_time:10323ms step_avg:60.72ms
step:171/2245 train_time:10384ms step_avg:60.73ms
step:172/2245 train_time:10443ms step_avg:60.72ms
step:173/2245 train_time:10505ms step_avg:60.72ms
step:174/2245 train_time:10564ms step_avg:60.71ms
step:175/2245 train_time:10625ms step_avg:60.72ms
step:176/2245 train_time:10684ms step_avg:60.71ms
step:177/2245 train_time:10746ms step_avg:60.71ms
step:178/2245 train_time:10805ms step_avg:60.70ms
step:179/2245 train_time:10867ms step_avg:60.71ms
step:180/2245 train_time:10926ms step_avg:60.70ms
step:181/2245 train_time:10987ms step_avg:60.70ms
step:182/2245 train_time:11046ms step_avg:60.69ms
step:183/2245 train_time:11107ms step_avg:60.70ms
step:184/2245 train_time:11166ms step_avg:60.69ms
step:185/2245 train_time:11227ms step_avg:60.69ms
step:186/2245 train_time:11286ms step_avg:60.68ms
step:187/2245 train_time:11347ms step_avg:60.68ms
step:188/2245 train_time:11406ms step_avg:60.67ms
step:189/2245 train_time:11468ms step_avg:60.67ms
step:190/2245 train_time:11526ms step_avg:60.67ms
step:191/2245 train_time:11588ms step_avg:60.67ms
step:192/2245 train_time:11647ms step_avg:60.66ms
step:193/2245 train_time:11709ms step_avg:60.67ms
step:194/2245 train_time:11768ms step_avg:60.66ms
step:195/2245 train_time:11829ms step_avg:60.66ms
step:196/2245 train_time:11888ms step_avg:60.65ms
step:197/2245 train_time:11950ms step_avg:60.66ms
step:198/2245 train_time:12009ms step_avg:60.65ms
step:199/2245 train_time:12070ms step_avg:60.66ms
step:200/2245 train_time:12129ms step_avg:60.64ms
step:201/2245 train_time:12190ms step_avg:60.65ms
step:202/2245 train_time:12250ms step_avg:60.64ms
step:203/2245 train_time:12311ms step_avg:60.65ms
step:204/2245 train_time:12371ms step_avg:60.64ms
step:205/2245 train_time:12432ms step_avg:60.64ms
step:206/2245 train_time:12491ms step_avg:60.64ms
step:207/2245 train_time:12552ms step_avg:60.64ms
step:208/2245 train_time:12611ms step_avg:60.63ms
step:209/2245 train_time:12672ms step_avg:60.63ms
step:210/2245 train_time:12731ms step_avg:60.62ms
step:211/2245 train_time:12793ms step_avg:60.63ms
step:212/2245 train_time:12852ms step_avg:60.62ms
step:213/2245 train_time:12914ms step_avg:60.63ms
step:214/2245 train_time:12974ms step_avg:60.62ms
step:215/2245 train_time:13036ms step_avg:60.63ms
step:216/2245 train_time:13095ms step_avg:60.63ms
step:217/2245 train_time:13157ms step_avg:60.63ms
step:218/2245 train_time:13216ms step_avg:60.62ms
step:219/2245 train_time:13277ms step_avg:60.63ms
step:220/2245 train_time:13336ms step_avg:60.62ms
step:221/2245 train_time:13398ms step_avg:60.63ms
step:222/2245 train_time:13458ms step_avg:60.62ms
step:223/2245 train_time:13519ms step_avg:60.63ms
step:224/2245 train_time:13579ms step_avg:60.62ms
step:225/2245 train_time:13641ms step_avg:60.63ms
step:226/2245 train_time:13700ms step_avg:60.62ms
step:227/2245 train_time:13761ms step_avg:60.62ms
step:228/2245 train_time:13820ms step_avg:60.62ms
step:229/2245 train_time:13882ms step_avg:60.62ms
step:230/2245 train_time:13942ms step_avg:60.62ms
step:231/2245 train_time:14003ms step_avg:60.62ms
step:232/2245 train_time:14062ms step_avg:60.61ms
step:233/2245 train_time:14124ms step_avg:60.62ms
step:234/2245 train_time:14183ms step_avg:60.61ms
step:235/2245 train_time:14245ms step_avg:60.62ms
step:236/2245 train_time:14303ms step_avg:60.60ms
step:237/2245 train_time:14364ms step_avg:60.61ms
step:238/2245 train_time:14423ms step_avg:60.60ms
step:239/2245 train_time:14484ms step_avg:60.60ms
step:240/2245 train_time:14543ms step_avg:60.60ms
step:241/2245 train_time:14604ms step_avg:60.60ms
step:242/2245 train_time:14663ms step_avg:60.59ms
step:243/2245 train_time:14726ms step_avg:60.60ms
step:244/2245 train_time:14784ms step_avg:60.59ms
step:245/2245 train_time:14845ms step_avg:60.59ms
step:246/2245 train_time:14904ms step_avg:60.58ms
step:247/2245 train_time:14965ms step_avg:60.59ms
step:248/2245 train_time:15024ms step_avg:60.58ms
step:249/2245 train_time:15086ms step_avg:60.59ms
step:250/2245 train_time:15145ms step_avg:60.58ms
step:250/2245 val_loss:4.0752 train_time:15207ms step_avg:60.83ms
step:251/2245 train_time:15226ms step_avg:60.66ms
step:252/2245 train_time:15267ms step_avg:60.58ms
step:253/2245 train_time:15334ms step_avg:60.61ms
step:254/2245 train_time:15399ms step_avg:60.63ms
step:255/2245 train_time:15460ms step_avg:60.63ms
step:256/2245 train_time:15519ms step_avg:60.62ms
step:257/2245 train_time:15580ms step_avg:60.62ms
step:258/2245 train_time:15638ms step_avg:60.61ms
step:259/2245 train_time:15699ms step_avg:60.61ms
step:260/2245 train_time:15758ms step_avg:60.61ms
step:261/2245 train_time:15818ms step_avg:60.61ms
step:262/2245 train_time:15876ms step_avg:60.60ms
step:263/2245 train_time:15937ms step_avg:60.60ms
step:264/2245 train_time:15996ms step_avg:60.59ms
step:265/2245 train_time:16056ms step_avg:60.59ms
step:266/2245 train_time:16116ms step_avg:60.59ms
step:267/2245 train_time:16179ms step_avg:60.59ms
step:268/2245 train_time:16240ms step_avg:60.60ms
step:269/2245 train_time:16304ms step_avg:60.61ms
step:270/2245 train_time:16364ms step_avg:60.61ms
step:271/2245 train_time:16426ms step_avg:60.61ms
step:272/2245 train_time:16485ms step_avg:60.61ms
step:273/2245 train_time:16547ms step_avg:60.61ms
step:274/2245 train_time:16606ms step_avg:60.60ms
step:275/2245 train_time:16668ms step_avg:60.61ms
step:276/2245 train_time:16727ms step_avg:60.60ms
step:277/2245 train_time:16787ms step_avg:60.60ms
step:278/2245 train_time:16846ms step_avg:60.60ms
step:279/2245 train_time:16907ms step_avg:60.60ms
step:280/2245 train_time:16966ms step_avg:60.59ms
step:281/2245 train_time:17028ms step_avg:60.60ms
step:282/2245 train_time:17087ms step_avg:60.59ms
step:283/2245 train_time:17148ms step_avg:60.59ms
step:284/2245 train_time:17207ms step_avg:60.59ms
step:285/2245 train_time:17269ms step_avg:60.59ms
step:286/2245 train_time:17328ms step_avg:60.59ms
step:287/2245 train_time:17389ms step_avg:60.59ms
step:288/2245 train_time:17448ms step_avg:60.58ms
step:289/2245 train_time:17509ms step_avg:60.59ms
step:290/2245 train_time:17568ms step_avg:60.58ms
step:291/2245 train_time:17630ms step_avg:60.58ms
step:292/2245 train_time:17689ms step_avg:60.58ms
step:293/2245 train_time:17750ms step_avg:60.58ms
step:294/2245 train_time:17809ms step_avg:60.57ms
step:295/2245 train_time:17870ms step_avg:60.58ms
step:296/2245 train_time:17929ms step_avg:60.57ms
step:297/2245 train_time:17990ms step_avg:60.57ms
step:298/2245 train_time:18048ms step_avg:60.56ms
step:299/2245 train_time:18110ms step_avg:60.57ms
step:300/2245 train_time:18168ms step_avg:60.56ms
step:301/2245 train_time:18230ms step_avg:60.56ms
step:302/2245 train_time:18289ms step_avg:60.56ms
step:303/2245 train_time:18350ms step_avg:60.56ms
step:304/2245 train_time:18409ms step_avg:60.56ms
step:305/2245 train_time:18470ms step_avg:60.56ms
step:306/2245 train_time:18529ms step_avg:60.55ms
step:307/2245 train_time:18590ms step_avg:60.56ms
step:308/2245 train_time:18649ms step_avg:60.55ms
step:309/2245 train_time:18710ms step_avg:60.55ms
step:310/2245 train_time:18769ms step_avg:60.54ms
step:311/2245 train_time:18830ms step_avg:60.55ms
step:312/2245 train_time:18889ms step_avg:60.54ms
step:313/2245 train_time:18950ms step_avg:60.54ms
step:314/2245 train_time:19009ms step_avg:60.54ms
step:315/2245 train_time:19070ms step_avg:60.54ms
step:316/2245 train_time:19129ms step_avg:60.53ms
step:317/2245 train_time:19190ms step_avg:60.54ms
step:318/2245 train_time:19251ms step_avg:60.54ms
step:319/2245 train_time:19309ms step_avg:60.53ms
step:320/2245 train_time:19368ms step_avg:60.53ms
step:321/2245 train_time:19430ms step_avg:60.53ms
step:322/2245 train_time:19488ms step_avg:60.52ms
step:323/2245 train_time:19549ms step_avg:60.52ms
step:324/2245 train_time:19609ms step_avg:60.52ms
step:325/2245 train_time:19670ms step_avg:60.52ms
step:326/2245 train_time:19728ms step_avg:60.51ms
step:327/2245 train_time:19789ms step_avg:60.52ms
step:328/2245 train_time:19847ms step_avg:60.51ms
step:329/2245 train_time:19908ms step_avg:60.51ms
step:330/2245 train_time:19967ms step_avg:60.51ms
step:331/2245 train_time:20029ms step_avg:60.51ms
step:332/2245 train_time:20087ms step_avg:60.50ms
step:333/2245 train_time:20148ms step_avg:60.51ms
step:334/2245 train_time:20207ms step_avg:60.50ms
step:335/2245 train_time:20268ms step_avg:60.50ms
step:336/2245 train_time:20327ms step_avg:60.50ms
step:337/2245 train_time:20389ms step_avg:60.50ms
step:338/2245 train_time:20447ms step_avg:60.49ms
step:339/2245 train_time:20509ms step_avg:60.50ms
step:340/2245 train_time:20567ms step_avg:60.49ms
step:341/2245 train_time:20629ms step_avg:60.50ms
step:342/2245 train_time:20688ms step_avg:60.49ms
step:343/2245 train_time:20748ms step_avg:60.49ms
step:344/2245 train_time:20807ms step_avg:60.49ms
step:345/2245 train_time:20868ms step_avg:60.49ms
step:346/2245 train_time:20927ms step_avg:60.48ms
step:347/2245 train_time:20988ms step_avg:60.48ms
step:348/2245 train_time:21046ms step_avg:60.48ms
step:349/2245 train_time:21108ms step_avg:60.48ms
step:350/2245 train_time:21167ms step_avg:60.48ms
step:351/2245 train_time:21228ms step_avg:60.48ms
step:352/2245 train_time:21287ms step_avg:60.47ms
step:353/2245 train_time:21348ms step_avg:60.48ms
step:354/2245 train_time:21407ms step_avg:60.47ms
step:355/2245 train_time:21470ms step_avg:60.48ms
step:356/2245 train_time:21528ms step_avg:60.47ms
step:357/2245 train_time:21589ms step_avg:60.47ms
step:358/2245 train_time:21648ms step_avg:60.47ms
step:359/2245 train_time:21709ms step_avg:60.47ms
step:360/2245 train_time:21768ms step_avg:60.47ms
step:361/2245 train_time:21829ms step_avg:60.47ms
step:362/2245 train_time:21887ms step_avg:60.46ms
step:363/2245 train_time:21948ms step_avg:60.46ms
step:364/2245 train_time:22007ms step_avg:60.46ms
step:365/2245 train_time:22068ms step_avg:60.46ms
step:366/2245 train_time:22127ms step_avg:60.46ms
step:367/2245 train_time:22188ms step_avg:60.46ms
step:368/2245 train_time:22247ms step_avg:60.45ms
step:369/2245 train_time:22308ms step_avg:60.46ms
step:370/2245 train_time:22367ms step_avg:60.45ms
step:371/2245 train_time:22428ms step_avg:60.45ms
step:372/2245 train_time:22486ms step_avg:60.45ms
step:373/2245 train_time:22548ms step_avg:60.45ms
step:374/2245 train_time:22607ms step_avg:60.45ms
step:375/2245 train_time:22669ms step_avg:60.45ms
step:376/2245 train_time:22727ms step_avg:60.45ms
step:377/2245 train_time:22789ms step_avg:60.45ms
step:378/2245 train_time:22847ms step_avg:60.44ms
step:379/2245 train_time:22908ms step_avg:60.44ms
step:380/2245 train_time:22966ms step_avg:60.44ms
step:381/2245 train_time:23028ms step_avg:60.44ms
step:382/2245 train_time:23087ms step_avg:60.44ms
step:383/2245 train_time:23148ms step_avg:60.44ms
step:384/2245 train_time:23207ms step_avg:60.43ms
step:385/2245 train_time:23268ms step_avg:60.44ms
step:386/2245 train_time:23327ms step_avg:60.43ms
step:387/2245 train_time:23389ms step_avg:60.44ms
step:388/2245 train_time:23448ms step_avg:60.43ms
step:389/2245 train_time:23509ms step_avg:60.43ms
step:390/2245 train_time:23568ms step_avg:60.43ms
step:391/2245 train_time:23629ms step_avg:60.43ms
step:392/2245 train_time:23688ms step_avg:60.43ms
step:393/2245 train_time:23749ms step_avg:60.43ms
step:394/2245 train_time:23808ms step_avg:60.43ms
step:395/2245 train_time:23869ms step_avg:60.43ms
step:396/2245 train_time:23927ms step_avg:60.42ms
step:397/2245 train_time:23988ms step_avg:60.42ms
step:398/2245 train_time:24047ms step_avg:60.42ms
step:399/2245 train_time:24108ms step_avg:60.42ms
step:400/2245 train_time:24167ms step_avg:60.42ms
step:401/2245 train_time:24228ms step_avg:60.42ms
step:402/2245 train_time:24287ms step_avg:60.41ms
step:403/2245 train_time:24350ms step_avg:60.42ms
step:404/2245 train_time:24407ms step_avg:60.41ms
step:405/2245 train_time:24469ms step_avg:60.42ms
step:406/2245 train_time:24528ms step_avg:60.41ms
step:407/2245 train_time:24590ms step_avg:60.42ms
step:408/2245 train_time:24648ms step_avg:60.41ms
step:409/2245 train_time:24709ms step_avg:60.41ms
step:410/2245 train_time:24768ms step_avg:60.41ms
step:411/2245 train_time:24830ms step_avg:60.41ms
step:412/2245 train_time:24888ms step_avg:60.41ms
step:413/2245 train_time:24949ms step_avg:60.41ms
step:414/2245 train_time:25007ms step_avg:60.40ms
step:415/2245 train_time:25070ms step_avg:60.41ms
step:416/2245 train_time:25128ms step_avg:60.40ms
step:417/2245 train_time:25189ms step_avg:60.41ms
step:418/2245 train_time:25248ms step_avg:60.40ms
step:419/2245 train_time:25310ms step_avg:60.41ms
step:420/2245 train_time:25368ms step_avg:60.40ms
step:421/2245 train_time:25430ms step_avg:60.40ms
step:422/2245 train_time:25489ms step_avg:60.40ms
step:423/2245 train_time:25550ms step_avg:60.40ms
step:424/2245 train_time:25608ms step_avg:60.40ms
step:425/2245 train_time:25670ms step_avg:60.40ms
step:426/2245 train_time:25729ms step_avg:60.40ms
step:427/2245 train_time:25790ms step_avg:60.40ms
step:428/2245 train_time:25848ms step_avg:60.39ms
step:429/2245 train_time:25910ms step_avg:60.40ms
step:430/2245 train_time:25968ms step_avg:60.39ms
step:431/2245 train_time:26030ms step_avg:60.39ms
step:432/2245 train_time:26088ms step_avg:60.39ms
step:433/2245 train_time:26150ms step_avg:60.39ms
step:434/2245 train_time:26208ms step_avg:60.39ms
step:435/2245 train_time:26270ms step_avg:60.39ms
step:436/2245 train_time:26328ms step_avg:60.39ms
step:437/2245 train_time:26390ms step_avg:60.39ms
step:438/2245 train_time:26449ms step_avg:60.38ms
step:439/2245 train_time:26510ms step_avg:60.39ms
step:440/2245 train_time:26569ms step_avg:60.38ms
step:441/2245 train_time:26630ms step_avg:60.39ms
step:442/2245 train_time:26689ms step_avg:60.38ms
step:443/2245 train_time:26750ms step_avg:60.38ms
step:444/2245 train_time:26809ms step_avg:60.38ms
step:445/2245 train_time:26870ms step_avg:60.38ms
step:446/2245 train_time:26928ms step_avg:60.38ms
step:447/2245 train_time:26989ms step_avg:60.38ms
step:448/2245 train_time:27048ms step_avg:60.38ms
step:449/2245 train_time:27109ms step_avg:60.38ms
step:450/2245 train_time:27168ms step_avg:60.37ms
step:451/2245 train_time:27229ms step_avg:60.37ms
step:452/2245 train_time:27288ms step_avg:60.37ms
step:453/2245 train_time:27349ms step_avg:60.37ms
step:454/2245 train_time:27408ms step_avg:60.37ms
step:455/2245 train_time:27469ms step_avg:60.37ms
step:456/2245 train_time:27528ms step_avg:60.37ms
step:457/2245 train_time:27589ms step_avg:60.37ms
step:458/2245 train_time:27648ms step_avg:60.37ms
step:459/2245 train_time:27709ms step_avg:60.37ms
step:460/2245 train_time:27768ms step_avg:60.36ms
step:461/2245 train_time:27829ms step_avg:60.37ms
step:462/2245 train_time:27888ms step_avg:60.36ms
step:463/2245 train_time:27949ms step_avg:60.36ms
step:464/2245 train_time:28008ms step_avg:60.36ms
step:465/2245 train_time:28070ms step_avg:60.36ms
step:466/2245 train_time:28128ms step_avg:60.36ms
step:467/2245 train_time:28190ms step_avg:60.36ms
step:468/2245 train_time:28248ms step_avg:60.36ms
step:469/2245 train_time:28309ms step_avg:60.36ms
step:470/2245 train_time:28368ms step_avg:60.36ms
step:471/2245 train_time:28429ms step_avg:60.36ms
step:472/2245 train_time:28489ms step_avg:60.36ms
step:473/2245 train_time:28550ms step_avg:60.36ms
step:474/2245 train_time:28609ms step_avg:60.36ms
step:475/2245 train_time:28670ms step_avg:60.36ms
step:476/2245 train_time:28729ms step_avg:60.36ms
step:477/2245 train_time:28790ms step_avg:60.36ms
step:478/2245 train_time:28849ms step_avg:60.35ms
step:479/2245 train_time:28910ms step_avg:60.36ms
step:480/2245 train_time:28969ms step_avg:60.35ms
step:481/2245 train_time:29031ms step_avg:60.35ms
step:482/2245 train_time:29089ms step_avg:60.35ms
step:483/2245 train_time:29152ms step_avg:60.36ms
step:484/2245 train_time:29209ms step_avg:60.35ms
step:485/2245 train_time:29270ms step_avg:60.35ms
step:486/2245 train_time:29329ms step_avg:60.35ms
step:487/2245 train_time:29390ms step_avg:60.35ms
step:488/2245 train_time:29449ms step_avg:60.35ms
step:489/2245 train_time:29510ms step_avg:60.35ms
step:490/2245 train_time:29569ms step_avg:60.34ms
step:491/2245 train_time:29630ms step_avg:60.35ms
step:492/2245 train_time:29689ms step_avg:60.34ms
step:493/2245 train_time:29750ms step_avg:60.34ms
step:494/2245 train_time:29809ms step_avg:60.34ms
step:495/2245 train_time:29870ms step_avg:60.34ms
step:496/2245 train_time:29929ms step_avg:60.34ms
step:497/2245 train_time:29990ms step_avg:60.34ms
step:498/2245 train_time:30049ms step_avg:60.34ms
step:499/2245 train_time:30110ms step_avg:60.34ms
step:500/2245 train_time:30169ms step_avg:60.34ms
step:500/2245 val_loss:3.8231 train_time:30231ms step_avg:60.46ms
step:501/2245 train_time:30252ms step_avg:60.38ms
step:502/2245 train_time:30293ms step_avg:60.34ms
step:503/2245 train_time:30357ms step_avg:60.35ms
step:504/2245 train_time:30417ms step_avg:60.35ms
step:505/2245 train_time:30479ms step_avg:60.35ms
step:506/2245 train_time:30537ms step_avg:60.35ms
step:507/2245 train_time:30598ms step_avg:60.35ms
step:508/2245 train_time:30657ms step_avg:60.35ms
step:509/2245 train_time:30717ms step_avg:60.35ms
step:510/2245 train_time:30775ms step_avg:60.34ms
step:511/2245 train_time:30836ms step_avg:60.35ms
step:512/2245 train_time:30895ms step_avg:60.34ms
step:513/2245 train_time:30956ms step_avg:60.34ms
step:514/2245 train_time:31015ms step_avg:60.34ms
step:515/2245 train_time:31075ms step_avg:60.34ms
step:516/2245 train_time:31134ms step_avg:60.34ms
step:517/2245 train_time:31198ms step_avg:60.34ms
step:518/2245 train_time:31258ms step_avg:60.34ms
step:519/2245 train_time:31321ms step_avg:60.35ms
step:520/2245 train_time:31381ms step_avg:60.35ms
step:521/2245 train_time:31442ms step_avg:60.35ms
step:522/2245 train_time:31501ms step_avg:60.35ms
step:523/2245 train_time:31562ms step_avg:60.35ms
step:524/2245 train_time:31621ms step_avg:60.35ms
step:525/2245 train_time:31682ms step_avg:60.35ms
step:526/2245 train_time:31742ms step_avg:60.35ms
step:527/2245 train_time:31803ms step_avg:60.35ms
step:528/2245 train_time:31862ms step_avg:60.34ms
step:529/2245 train_time:31923ms step_avg:60.35ms
step:530/2245 train_time:31982ms step_avg:60.34ms
step:531/2245 train_time:32043ms step_avg:60.34ms
step:532/2245 train_time:32102ms step_avg:60.34ms
step:533/2245 train_time:32164ms step_avg:60.35ms
step:534/2245 train_time:32224ms step_avg:60.34ms
step:535/2245 train_time:32286ms step_avg:60.35ms
step:536/2245 train_time:32346ms step_avg:60.35ms
step:537/2245 train_time:32408ms step_avg:60.35ms
step:538/2245 train_time:32467ms step_avg:60.35ms
step:539/2245 train_time:32529ms step_avg:60.35ms
step:540/2245 train_time:32588ms step_avg:60.35ms
step:541/2245 train_time:32650ms step_avg:60.35ms
step:542/2245 train_time:32709ms step_avg:60.35ms
step:543/2245 train_time:32770ms step_avg:60.35ms
step:544/2245 train_time:32830ms step_avg:60.35ms
step:545/2245 train_time:32891ms step_avg:60.35ms
step:546/2245 train_time:32951ms step_avg:60.35ms
step:547/2245 train_time:33012ms step_avg:60.35ms
step:548/2245 train_time:33071ms step_avg:60.35ms
step:549/2245 train_time:33133ms step_avg:60.35ms
step:550/2245 train_time:33192ms step_avg:60.35ms
step:551/2245 train_time:33254ms step_avg:60.35ms
step:552/2245 train_time:33312ms step_avg:60.35ms
step:553/2245 train_time:33374ms step_avg:60.35ms
step:554/2245 train_time:33432ms step_avg:60.35ms
step:555/2245 train_time:33494ms step_avg:60.35ms
step:556/2245 train_time:33553ms step_avg:60.35ms
step:557/2245 train_time:33614ms step_avg:60.35ms
step:558/2245 train_time:33673ms step_avg:60.35ms
step:559/2245 train_time:33734ms step_avg:60.35ms
step:560/2245 train_time:33793ms step_avg:60.34ms
step:561/2245 train_time:33854ms step_avg:60.35ms
step:562/2245 train_time:33913ms step_avg:60.34ms
step:563/2245 train_time:33974ms step_avg:60.35ms
step:564/2245 train_time:34033ms step_avg:60.34ms
step:565/2245 train_time:34095ms step_avg:60.34ms
step:566/2245 train_time:34154ms step_avg:60.34ms
step:567/2245 train_time:34215ms step_avg:60.34ms
step:568/2245 train_time:34274ms step_avg:60.34ms
step:569/2245 train_time:34335ms step_avg:60.34ms
step:570/2245 train_time:34394ms step_avg:60.34ms
step:571/2245 train_time:34456ms step_avg:60.34ms
step:572/2245 train_time:34514ms step_avg:60.34ms
step:573/2245 train_time:34576ms step_avg:60.34ms
step:574/2245 train_time:34635ms step_avg:60.34ms
step:575/2245 train_time:34696ms step_avg:60.34ms
step:576/2245 train_time:34755ms step_avg:60.34ms
step:577/2245 train_time:34816ms step_avg:60.34ms
step:578/2245 train_time:34875ms step_avg:60.34ms
step:579/2245 train_time:34937ms step_avg:60.34ms
step:580/2245 train_time:34995ms step_avg:60.34ms
step:581/2245 train_time:35057ms step_avg:60.34ms
step:582/2245 train_time:35115ms step_avg:60.34ms
step:583/2245 train_time:35177ms step_avg:60.34ms
step:584/2245 train_time:35236ms step_avg:60.33ms
step:585/2245 train_time:35298ms step_avg:60.34ms
step:586/2245 train_time:35356ms step_avg:60.34ms
step:587/2245 train_time:35418ms step_avg:60.34ms
step:588/2245 train_time:35476ms step_avg:60.33ms
step:589/2245 train_time:35538ms step_avg:60.34ms
step:590/2245 train_time:35597ms step_avg:60.33ms
step:591/2245 train_time:35658ms step_avg:60.34ms
step:592/2245 train_time:35717ms step_avg:60.33ms
step:593/2245 train_time:35779ms step_avg:60.33ms
step:594/2245 train_time:35838ms step_avg:60.33ms
step:595/2245 train_time:35899ms step_avg:60.33ms
step:596/2245 train_time:35958ms step_avg:60.33ms
step:597/2245 train_time:36019ms step_avg:60.33ms
step:598/2245 train_time:36078ms step_avg:60.33ms
step:599/2245 train_time:36139ms step_avg:60.33ms
step:600/2245 train_time:36199ms step_avg:60.33ms
step:601/2245 train_time:36261ms step_avg:60.33ms
step:602/2245 train_time:36320ms step_avg:60.33ms
step:603/2245 train_time:36381ms step_avg:60.33ms
step:604/2245 train_time:36440ms step_avg:60.33ms
step:605/2245 train_time:36502ms step_avg:60.33ms
step:606/2245 train_time:36560ms step_avg:60.33ms
step:607/2245 train_time:36622ms step_avg:60.33ms
step:608/2245 train_time:36681ms step_avg:60.33ms
step:609/2245 train_time:36742ms step_avg:60.33ms
step:610/2245 train_time:36802ms step_avg:60.33ms
step:611/2245 train_time:36864ms step_avg:60.33ms
step:612/2245 train_time:36923ms step_avg:60.33ms
step:613/2245 train_time:36984ms step_avg:60.33ms
step:614/2245 train_time:37044ms step_avg:60.33ms
step:615/2245 train_time:37106ms step_avg:60.33ms
step:616/2245 train_time:37165ms step_avg:60.33ms
step:617/2245 train_time:37227ms step_avg:60.34ms
step:618/2245 train_time:37286ms step_avg:60.33ms
step:619/2245 train_time:37348ms step_avg:60.34ms
step:620/2245 train_time:37408ms step_avg:60.34ms
step:621/2245 train_time:37469ms step_avg:60.34ms
step:622/2245 train_time:37528ms step_avg:60.33ms
step:623/2245 train_time:37590ms step_avg:60.34ms
step:624/2245 train_time:37650ms step_avg:60.34ms
step:625/2245 train_time:37711ms step_avg:60.34ms
step:626/2245 train_time:37770ms step_avg:60.34ms
step:627/2245 train_time:37831ms step_avg:60.34ms
step:628/2245 train_time:37890ms step_avg:60.33ms
step:629/2245 train_time:37951ms step_avg:60.34ms
step:630/2245 train_time:38010ms step_avg:60.33ms
step:631/2245 train_time:38072ms step_avg:60.34ms
step:632/2245 train_time:38131ms step_avg:60.33ms
step:633/2245 train_time:38193ms step_avg:60.34ms
step:634/2245 train_time:38252ms step_avg:60.33ms
step:635/2245 train_time:38314ms step_avg:60.34ms
step:636/2245 train_time:38372ms step_avg:60.33ms
step:637/2245 train_time:38433ms step_avg:60.34ms
step:638/2245 train_time:38492ms step_avg:60.33ms
step:639/2245 train_time:38554ms step_avg:60.33ms
step:640/2245 train_time:38613ms step_avg:60.33ms
step:641/2245 train_time:38674ms step_avg:60.33ms
step:642/2245 train_time:38733ms step_avg:60.33ms
step:643/2245 train_time:38794ms step_avg:60.33ms
step:644/2245 train_time:38853ms step_avg:60.33ms
step:645/2245 train_time:38914ms step_avg:60.33ms
step:646/2245 train_time:38973ms step_avg:60.33ms
step:647/2245 train_time:39036ms step_avg:60.33ms
step:648/2245 train_time:39095ms step_avg:60.33ms
step:649/2245 train_time:39156ms step_avg:60.33ms
step:650/2245 train_time:39214ms step_avg:60.33ms
step:651/2245 train_time:39276ms step_avg:60.33ms
step:652/2245 train_time:39335ms step_avg:60.33ms
step:653/2245 train_time:39396ms step_avg:60.33ms
step:654/2245 train_time:39455ms step_avg:60.33ms
step:655/2245 train_time:39517ms step_avg:60.33ms
step:656/2245 train_time:39576ms step_avg:60.33ms
step:657/2245 train_time:39638ms step_avg:60.33ms
step:658/2245 train_time:39697ms step_avg:60.33ms
step:659/2245 train_time:39758ms step_avg:60.33ms
step:660/2245 train_time:39817ms step_avg:60.33ms
step:661/2245 train_time:39879ms step_avg:60.33ms
step:662/2245 train_time:39938ms step_avg:60.33ms
step:663/2245 train_time:39999ms step_avg:60.33ms
step:664/2245 train_time:40058ms step_avg:60.33ms
step:665/2245 train_time:40119ms step_avg:60.33ms
step:666/2245 train_time:40178ms step_avg:60.33ms
step:667/2245 train_time:40239ms step_avg:60.33ms
step:668/2245 train_time:40298ms step_avg:60.33ms
step:669/2245 train_time:40359ms step_avg:60.33ms
step:670/2245 train_time:40418ms step_avg:60.33ms
step:671/2245 train_time:40479ms step_avg:60.33ms
step:672/2245 train_time:40538ms step_avg:60.32ms
step:673/2245 train_time:40599ms step_avg:60.33ms
step:674/2245 train_time:40659ms step_avg:60.32ms
step:675/2245 train_time:40721ms step_avg:60.33ms
step:676/2245 train_time:40780ms step_avg:60.33ms
step:677/2245 train_time:40841ms step_avg:60.33ms
step:678/2245 train_time:40900ms step_avg:60.32ms
step:679/2245 train_time:40962ms step_avg:60.33ms
step:680/2245 train_time:41021ms step_avg:60.32ms
step:681/2245 train_time:41082ms step_avg:60.33ms
step:682/2245 train_time:41141ms step_avg:60.32ms
step:683/2245 train_time:41203ms step_avg:60.33ms
step:684/2245 train_time:41263ms step_avg:60.33ms
step:685/2245 train_time:41324ms step_avg:60.33ms
step:686/2245 train_time:41383ms step_avg:60.33ms
step:687/2245 train_time:41445ms step_avg:60.33ms
step:688/2245 train_time:41505ms step_avg:60.33ms
step:689/2245 train_time:41567ms step_avg:60.33ms
step:690/2245 train_time:41626ms step_avg:60.33ms
step:691/2245 train_time:41688ms step_avg:60.33ms
step:692/2245 train_time:41748ms step_avg:60.33ms
step:693/2245 train_time:41810ms step_avg:60.33ms
step:694/2245 train_time:41869ms step_avg:60.33ms
step:695/2245 train_time:41930ms step_avg:60.33ms
step:696/2245 train_time:41989ms step_avg:60.33ms
step:697/2245 train_time:42051ms step_avg:60.33ms
step:698/2245 train_time:42109ms step_avg:60.33ms
step:699/2245 train_time:42171ms step_avg:60.33ms
step:700/2245 train_time:42229ms step_avg:60.33ms
step:701/2245 train_time:42291ms step_avg:60.33ms
step:702/2245 train_time:42351ms step_avg:60.33ms
step:703/2245 train_time:42412ms step_avg:60.33ms
step:704/2245 train_time:42471ms step_avg:60.33ms
step:705/2245 train_time:42533ms step_avg:60.33ms
step:706/2245 train_time:42595ms step_avg:60.33ms
step:707/2245 train_time:42653ms step_avg:60.33ms
step:708/2245 train_time:42713ms step_avg:60.33ms
step:709/2245 train_time:42774ms step_avg:60.33ms
step:710/2245 train_time:42832ms step_avg:60.33ms
step:711/2245 train_time:42893ms step_avg:60.33ms
step:712/2245 train_time:42952ms step_avg:60.33ms
step:713/2245 train_time:43013ms step_avg:60.33ms
step:714/2245 train_time:43072ms step_avg:60.32ms
step:715/2245 train_time:43133ms step_avg:60.33ms
step:716/2245 train_time:43192ms step_avg:60.32ms
step:717/2245 train_time:43254ms step_avg:60.33ms
step:718/2245 train_time:43314ms step_avg:60.33ms
step:719/2245 train_time:43375ms step_avg:60.33ms
step:720/2245 train_time:43434ms step_avg:60.32ms
step:721/2245 train_time:44026ms step_avg:61.06ms
step:722/2245 train_time:44083ms step_avg:61.06ms
step:723/2245 train_time:44143ms step_avg:61.06ms
step:724/2245 train_time:44202ms step_avg:61.05ms
step:725/2245 train_time:44262ms step_avg:61.05ms
step:726/2245 train_time:44320ms step_avg:61.05ms
step:727/2245 train_time:44381ms step_avg:61.05ms
step:728/2245 train_time:44439ms step_avg:61.04ms
step:729/2245 train_time:44500ms step_avg:61.04ms
step:730/2245 train_time:44558ms step_avg:61.04ms
step:731/2245 train_time:44618ms step_avg:61.04ms
step:732/2245 train_time:44676ms step_avg:61.03ms
step:733/2245 train_time:44737ms step_avg:61.03ms
step:734/2245 train_time:44795ms step_avg:61.03ms
step:735/2245 train_time:44857ms step_avg:61.03ms
step:736/2245 train_time:44923ms step_avg:61.04ms
step:737/2245 train_time:44990ms step_avg:61.04ms
step:738/2245 train_time:45051ms step_avg:61.05ms
step:739/2245 train_time:45113ms step_avg:61.05ms
step:740/2245 train_time:45173ms step_avg:61.04ms
step:741/2245 train_time:45234ms step_avg:61.04ms
step:742/2245 train_time:45293ms step_avg:61.04ms
step:743/2245 train_time:45355ms step_avg:61.04ms
step:744/2245 train_time:45414ms step_avg:61.04ms
step:745/2245 train_time:45476ms step_avg:61.04ms
step:746/2245 train_time:45535ms step_avg:61.04ms
step:747/2245 train_time:45597ms step_avg:61.04ms
step:748/2245 train_time:45656ms step_avg:61.04ms
step:749/2245 train_time:45717ms step_avg:61.04ms
step:750/2245 train_time:45776ms step_avg:61.03ms
step:750/2245 val_loss:3.6687 train_time:45839ms step_avg:61.12ms
step:751/2245 train_time:45860ms step_avg:61.07ms
step:752/2245 train_time:45903ms step_avg:61.04ms
step:753/2245 train_time:45964ms step_avg:61.04ms
step:754/2245 train_time:46024ms step_avg:61.04ms
step:755/2245 train_time:46087ms step_avg:61.04ms
step:756/2245 train_time:46147ms step_avg:61.04ms
step:757/2245 train_time:46208ms step_avg:61.04ms
step:758/2245 train_time:46267ms step_avg:61.04ms
step:759/2245 train_time:46329ms step_avg:61.04ms
step:760/2245 train_time:46387ms step_avg:61.04ms
step:761/2245 train_time:46448ms step_avg:61.04ms
step:762/2245 train_time:46507ms step_avg:61.03ms
step:763/2245 train_time:46568ms step_avg:61.03ms
step:764/2245 train_time:46627ms step_avg:61.03ms
step:765/2245 train_time:46689ms step_avg:61.03ms
step:766/2245 train_time:46755ms step_avg:61.04ms
step:767/2245 train_time:46823ms step_avg:61.05ms
step:768/2245 train_time:46883ms step_avg:61.05ms
step:769/2245 train_time:46945ms step_avg:61.05ms
step:770/2245 train_time:47005ms step_avg:61.05ms
step:771/2245 train_time:47067ms step_avg:61.05ms
step:772/2245 train_time:47126ms step_avg:61.04ms
step:773/2245 train_time:47187ms step_avg:61.04ms
step:774/2245 train_time:47246ms step_avg:61.04ms
step:775/2245 train_time:47307ms step_avg:61.04ms
step:776/2245 train_time:47366ms step_avg:61.04ms
step:777/2245 train_time:47429ms step_avg:61.04ms
step:778/2245 train_time:47487ms step_avg:61.04ms
step:779/2245 train_time:47547ms step_avg:61.04ms
step:780/2245 train_time:47606ms step_avg:61.03ms
step:781/2245 train_time:47670ms step_avg:61.04ms
step:782/2245 train_time:47733ms step_avg:61.04ms
step:783/2245 train_time:47796ms step_avg:61.04ms
step:784/2245 train_time:47857ms step_avg:61.04ms
step:785/2245 train_time:47921ms step_avg:61.05ms
step:786/2245 train_time:47981ms step_avg:61.04ms
step:787/2245 train_time:48044ms step_avg:61.05ms
step:788/2245 train_time:48104ms step_avg:61.05ms
step:789/2245 train_time:48165ms step_avg:61.05ms
step:790/2245 train_time:48225ms step_avg:61.04ms
step:791/2245 train_time:48286ms step_avg:61.04ms
step:792/2245 train_time:48345ms step_avg:61.04ms
step:793/2245 train_time:48406ms step_avg:61.04ms
step:794/2245 train_time:48465ms step_avg:61.04ms
step:795/2245 train_time:48526ms step_avg:61.04ms
step:796/2245 train_time:48586ms step_avg:61.04ms
step:797/2245 train_time:48649ms step_avg:61.04ms
step:798/2245 train_time:48709ms step_avg:61.04ms
step:799/2245 train_time:48773ms step_avg:61.04ms
step:800/2245 train_time:48834ms step_avg:61.04ms
step:801/2245 train_time:48897ms step_avg:61.04ms
step:802/2245 train_time:48957ms step_avg:61.04ms
step:803/2245 train_time:49021ms step_avg:61.05ms
step:804/2245 train_time:49081ms step_avg:61.05ms
step:805/2245 train_time:49143ms step_avg:61.05ms
step:806/2245 train_time:49203ms step_avg:61.05ms
step:807/2245 train_time:49265ms step_avg:61.05ms
step:808/2245 train_time:49325ms step_avg:61.05ms
step:809/2245 train_time:49386ms step_avg:61.05ms
step:810/2245 train_time:49445ms step_avg:61.04ms
step:811/2245 train_time:49506ms step_avg:61.04ms
step:812/2245 train_time:49565ms step_avg:61.04ms
step:813/2245 train_time:49628ms step_avg:61.04ms
step:814/2245 train_time:49687ms step_avg:61.04ms
step:815/2245 train_time:49750ms step_avg:61.04ms
step:816/2245 train_time:49810ms step_avg:61.04ms
step:817/2245 train_time:49873ms step_avg:61.04ms
step:818/2245 train_time:49933ms step_avg:61.04ms
step:819/2245 train_time:49996ms step_avg:61.05ms
step:820/2245 train_time:50057ms step_avg:61.05ms
step:821/2245 train_time:50120ms step_avg:61.05ms
step:822/2245 train_time:50180ms step_avg:61.05ms
step:823/2245 train_time:50242ms step_avg:61.05ms
step:824/2245 train_time:50302ms step_avg:61.05ms
step:825/2245 train_time:50364ms step_avg:61.05ms
step:826/2245 train_time:50423ms step_avg:61.05ms
step:827/2245 train_time:50485ms step_avg:61.05ms
step:828/2245 train_time:50545ms step_avg:61.04ms
step:829/2245 train_time:50607ms step_avg:61.05ms
step:830/2245 train_time:50666ms step_avg:61.04ms
step:831/2245 train_time:50728ms step_avg:61.04ms
step:832/2245 train_time:50788ms step_avg:61.04ms
step:833/2245 train_time:50851ms step_avg:61.05ms
step:834/2245 train_time:50911ms step_avg:61.04ms
step:835/2245 train_time:50975ms step_avg:61.05ms
step:836/2245 train_time:51034ms step_avg:61.05ms
step:837/2245 train_time:51097ms step_avg:61.05ms
step:838/2245 train_time:51158ms step_avg:61.05ms
step:839/2245 train_time:51221ms step_avg:61.05ms
step:840/2245 train_time:51281ms step_avg:61.05ms
step:841/2245 train_time:51343ms step_avg:61.05ms
step:842/2245 train_time:51403ms step_avg:61.05ms
step:843/2245 train_time:51465ms step_avg:61.05ms
step:844/2245 train_time:51525ms step_avg:61.05ms
step:845/2245 train_time:51587ms step_avg:61.05ms
step:846/2245 train_time:51646ms step_avg:61.05ms
step:847/2245 train_time:51708ms step_avg:61.05ms
step:848/2245 train_time:51768ms step_avg:61.05ms
step:849/2245 train_time:51830ms step_avg:61.05ms
step:850/2245 train_time:51891ms step_avg:61.05ms
step:851/2245 train_time:51954ms step_avg:61.05ms
step:852/2245 train_time:52014ms step_avg:61.05ms
step:853/2245 train_time:52076ms step_avg:61.05ms
step:854/2245 train_time:52137ms step_avg:61.05ms
step:855/2245 train_time:52200ms step_avg:61.05ms
step:856/2245 train_time:52263ms step_avg:61.05ms
step:857/2245 train_time:52321ms step_avg:61.05ms
step:858/2245 train_time:52381ms step_avg:61.05ms
step:859/2245 train_time:52443ms step_avg:61.05ms
step:860/2245 train_time:52503ms step_avg:61.05ms
step:861/2245 train_time:52567ms step_avg:61.05ms
step:862/2245 train_time:52626ms step_avg:61.05ms
step:863/2245 train_time:52687ms step_avg:61.05ms
step:864/2245 train_time:52747ms step_avg:61.05ms
step:865/2245 train_time:52809ms step_avg:61.05ms
step:866/2245 train_time:52868ms step_avg:61.05ms
step:867/2245 train_time:52931ms step_avg:61.05ms
step:868/2245 train_time:52991ms step_avg:61.05ms
step:869/2245 train_time:53054ms step_avg:61.05ms
step:870/2245 train_time:53115ms step_avg:61.05ms
step:871/2245 train_time:53178ms step_avg:61.05ms
step:872/2245 train_time:53238ms step_avg:61.05ms
step:873/2245 train_time:53301ms step_avg:61.05ms
step:874/2245 train_time:53361ms step_avg:61.05ms
step:875/2245 train_time:53423ms step_avg:61.05ms
step:876/2245 train_time:53483ms step_avg:61.05ms
step:877/2245 train_time:53545ms step_avg:61.05ms
step:878/2245 train_time:53604ms step_avg:61.05ms
step:879/2245 train_time:53666ms step_avg:61.05ms
step:880/2245 train_time:53726ms step_avg:61.05ms
step:881/2245 train_time:53788ms step_avg:61.05ms
step:882/2245 train_time:53848ms step_avg:61.05ms
step:883/2245 train_time:53911ms step_avg:61.05ms
step:884/2245 train_time:53971ms step_avg:61.05ms
step:885/2245 train_time:54034ms step_avg:61.06ms
step:886/2245 train_time:54093ms step_avg:61.05ms
step:887/2245 train_time:54156ms step_avg:61.06ms
step:888/2245 train_time:54218ms step_avg:61.06ms
step:889/2245 train_time:54280ms step_avg:61.06ms
step:890/2245 train_time:54340ms step_avg:61.06ms
step:891/2245 train_time:54402ms step_avg:61.06ms
step:892/2245 train_time:54461ms step_avg:61.06ms
step:893/2245 train_time:54523ms step_avg:61.06ms
step:894/2245 train_time:54583ms step_avg:61.05ms
step:895/2245 train_time:54644ms step_avg:61.06ms
step:896/2245 train_time:54704ms step_avg:61.05ms
step:897/2245 train_time:54766ms step_avg:61.05ms
step:898/2245 train_time:54826ms step_avg:61.05ms
step:899/2245 train_time:54889ms step_avg:61.06ms
step:900/2245 train_time:54948ms step_avg:61.05ms
step:901/2245 train_time:55012ms step_avg:61.06ms
step:902/2245 train_time:55071ms step_avg:61.05ms
step:903/2245 train_time:55134ms step_avg:61.06ms
step:904/2245 train_time:55193ms step_avg:61.05ms
step:905/2245 train_time:55256ms step_avg:61.06ms
step:906/2245 train_time:55317ms step_avg:61.06ms
step:907/2245 train_time:55380ms step_avg:61.06ms
step:908/2245 train_time:55440ms step_avg:61.06ms
step:909/2245 train_time:55502ms step_avg:61.06ms
step:910/2245 train_time:55562ms step_avg:61.06ms
step:911/2245 train_time:55624ms step_avg:61.06ms
step:912/2245 train_time:55684ms step_avg:61.06ms
step:913/2245 train_time:55746ms step_avg:61.06ms
step:914/2245 train_time:55805ms step_avg:61.06ms
step:915/2245 train_time:55868ms step_avg:61.06ms
step:916/2245 train_time:55927ms step_avg:61.06ms
step:917/2245 train_time:55990ms step_avg:61.06ms
step:918/2245 train_time:56050ms step_avg:61.06ms
step:919/2245 train_time:56112ms step_avg:61.06ms
step:920/2245 train_time:56172ms step_avg:61.06ms
step:921/2245 train_time:56235ms step_avg:61.06ms
step:922/2245 train_time:56295ms step_avg:61.06ms
step:923/2245 train_time:56358ms step_avg:61.06ms
step:924/2245 train_time:56419ms step_avg:61.06ms
step:925/2245 train_time:56482ms step_avg:61.06ms
step:926/2245 train_time:56541ms step_avg:61.06ms
step:927/2245 train_time:56603ms step_avg:61.06ms
step:928/2245 train_time:56662ms step_avg:61.06ms
step:929/2245 train_time:56725ms step_avg:61.06ms
step:930/2245 train_time:56785ms step_avg:61.06ms
step:931/2245 train_time:56846ms step_avg:61.06ms
step:932/2245 train_time:56905ms step_avg:61.06ms
step:933/2245 train_time:56968ms step_avg:61.06ms
step:934/2245 train_time:57027ms step_avg:61.06ms
step:935/2245 train_time:57090ms step_avg:61.06ms
step:936/2245 train_time:57150ms step_avg:61.06ms
step:937/2245 train_time:57212ms step_avg:61.06ms
step:938/2245 train_time:57272ms step_avg:61.06ms
step:939/2245 train_time:57335ms step_avg:61.06ms
step:940/2245 train_time:57395ms step_avg:61.06ms
step:941/2245 train_time:57458ms step_avg:61.06ms
step:942/2245 train_time:57519ms step_avg:61.06ms
step:943/2245 train_time:57582ms step_avg:61.06ms
step:944/2245 train_time:57642ms step_avg:61.06ms
step:945/2245 train_time:57704ms step_avg:61.06ms
step:946/2245 train_time:57764ms step_avg:61.06ms
step:947/2245 train_time:57826ms step_avg:61.06ms
step:948/2245 train_time:57885ms step_avg:61.06ms
step:949/2245 train_time:57947ms step_avg:61.06ms
step:950/2245 train_time:58006ms step_avg:61.06ms
step:951/2245 train_time:58068ms step_avg:61.06ms
step:952/2245 train_time:58128ms step_avg:61.06ms
step:953/2245 train_time:58190ms step_avg:61.06ms
step:954/2245 train_time:58250ms step_avg:61.06ms
step:955/2245 train_time:58313ms step_avg:61.06ms
step:956/2245 train_time:58373ms step_avg:61.06ms
step:957/2245 train_time:58436ms step_avg:61.06ms
step:958/2245 train_time:58497ms step_avg:61.06ms
step:959/2245 train_time:58560ms step_avg:61.06ms
step:960/2245 train_time:58621ms step_avg:61.06ms
step:961/2245 train_time:58683ms step_avg:61.06ms
step:962/2245 train_time:58743ms step_avg:61.06ms
step:963/2245 train_time:58805ms step_avg:61.06ms
step:964/2245 train_time:58865ms step_avg:61.06ms
step:965/2245 train_time:58927ms step_avg:61.06ms
step:966/2245 train_time:58986ms step_avg:61.06ms
step:967/2245 train_time:59048ms step_avg:61.06ms
step:968/2245 train_time:59107ms step_avg:61.06ms
step:969/2245 train_time:59169ms step_avg:61.06ms
step:970/2245 train_time:59230ms step_avg:61.06ms
step:971/2245 train_time:59293ms step_avg:61.06ms
step:972/2245 train_time:59353ms step_avg:61.06ms
step:973/2245 train_time:59416ms step_avg:61.06ms
step:974/2245 train_time:59477ms step_avg:61.06ms
step:975/2245 train_time:59541ms step_avg:61.07ms
step:976/2245 train_time:59601ms step_avg:61.07ms
step:977/2245 train_time:59663ms step_avg:61.07ms
step:978/2245 train_time:59724ms step_avg:61.07ms
step:979/2245 train_time:59785ms step_avg:61.07ms
step:980/2245 train_time:59844ms step_avg:61.07ms
step:981/2245 train_time:59906ms step_avg:61.07ms
step:982/2245 train_time:59966ms step_avg:61.07ms
step:983/2245 train_time:60028ms step_avg:61.07ms
step:984/2245 train_time:60087ms step_avg:61.06ms
step:985/2245 train_time:60150ms step_avg:61.07ms
step:986/2245 train_time:60210ms step_avg:61.06ms
step:987/2245 train_time:60273ms step_avg:61.07ms
step:988/2245 train_time:60333ms step_avg:61.07ms
step:989/2245 train_time:60395ms step_avg:61.07ms
step:990/2245 train_time:60456ms step_avg:61.07ms
step:991/2245 train_time:60520ms step_avg:61.07ms
step:992/2245 train_time:60580ms step_avg:61.07ms
step:993/2245 train_time:60643ms step_avg:61.07ms
step:994/2245 train_time:60703ms step_avg:61.07ms
step:995/2245 train_time:60765ms step_avg:61.07ms
step:996/2245 train_time:60826ms step_avg:61.07ms
step:997/2245 train_time:60887ms step_avg:61.07ms
step:998/2245 train_time:60947ms step_avg:61.07ms
step:999/2245 train_time:61009ms step_avg:61.07ms
step:1000/2245 train_time:61068ms step_avg:61.07ms
step:1000/2245 val_loss:3.5916 train_time:61132ms step_avg:61.13ms
step:1001/2245 train_time:61151ms step_avg:61.09ms
step:1002/2245 train_time:61196ms step_avg:61.07ms
step:1003/2245 train_time:61262ms step_avg:61.08ms
step:1004/2245 train_time:61321ms step_avg:61.08ms
step:1005/2245 train_time:61383ms step_avg:61.08ms
step:1006/2245 train_time:61442ms step_avg:61.08ms
step:1007/2245 train_time:61503ms step_avg:61.08ms
step:1008/2245 train_time:61563ms step_avg:61.07ms
step:1009/2245 train_time:61626ms step_avg:61.08ms
step:1010/2245 train_time:61685ms step_avg:61.07ms
step:1011/2245 train_time:61747ms step_avg:61.08ms
step:1012/2245 train_time:61806ms step_avg:61.07ms
step:1013/2245 train_time:61868ms step_avg:61.07ms
step:1014/2245 train_time:61927ms step_avg:61.07ms
step:1015/2245 train_time:61989ms step_avg:61.07ms
step:1016/2245 train_time:62049ms step_avg:61.07ms
step:1017/2245 train_time:62113ms step_avg:61.07ms
step:1018/2245 train_time:62175ms step_avg:61.08ms
step:1019/2245 train_time:62239ms step_avg:61.08ms
step:1020/2245 train_time:62299ms step_avg:61.08ms
step:1021/2245 train_time:62362ms step_avg:61.08ms
step:1022/2245 train_time:62421ms step_avg:61.08ms
step:1023/2245 train_time:62484ms step_avg:61.08ms
step:1024/2245 train_time:62543ms step_avg:61.08ms
step:1025/2245 train_time:62605ms step_avg:61.08ms
step:1026/2245 train_time:62666ms step_avg:61.08ms
step:1027/2245 train_time:62727ms step_avg:61.08ms
step:1028/2245 train_time:62787ms step_avg:61.08ms
step:1029/2245 train_time:62848ms step_avg:61.08ms
step:1030/2245 train_time:62908ms step_avg:61.08ms
step:1031/2245 train_time:62970ms step_avg:61.08ms
step:1032/2245 train_time:63031ms step_avg:61.08ms
step:1033/2245 train_time:63094ms step_avg:61.08ms
step:1034/2245 train_time:63156ms step_avg:61.08ms
step:1035/2245 train_time:63219ms step_avg:61.08ms
step:1036/2245 train_time:63280ms step_avg:61.08ms
step:1037/2245 train_time:63342ms step_avg:61.08ms
step:1038/2245 train_time:63402ms step_avg:61.08ms
step:1039/2245 train_time:63464ms step_avg:61.08ms
step:1040/2245 train_time:63524ms step_avg:61.08ms
step:1041/2245 train_time:63586ms step_avg:61.08ms
step:1042/2245 train_time:63645ms step_avg:61.08ms
step:1043/2245 train_time:63707ms step_avg:61.08ms
step:1044/2245 train_time:63767ms step_avg:61.08ms
step:1045/2245 train_time:63829ms step_avg:61.08ms
step:1046/2245 train_time:63888ms step_avg:61.08ms
step:1047/2245 train_time:63950ms step_avg:61.08ms
step:1048/2245 train_time:64011ms step_avg:61.08ms
step:1049/2245 train_time:64072ms step_avg:61.08ms
step:1050/2245 train_time:64133ms step_avg:61.08ms
step:1051/2245 train_time:64196ms step_avg:61.08ms
step:1052/2245 train_time:64257ms step_avg:61.08ms
step:1053/2245 train_time:64319ms step_avg:61.08ms
step:1054/2245 train_time:64378ms step_avg:61.08ms
step:1055/2245 train_time:64441ms step_avg:61.08ms
step:1056/2245 train_time:64501ms step_avg:61.08ms
step:1057/2245 train_time:64563ms step_avg:61.08ms
step:1058/2245 train_time:64622ms step_avg:61.08ms
step:1059/2245 train_time:64684ms step_avg:61.08ms
step:1060/2245 train_time:64743ms step_avg:61.08ms
step:1061/2245 train_time:64807ms step_avg:61.08ms
step:1062/2245 train_time:64866ms step_avg:61.08ms
step:1063/2245 train_time:64928ms step_avg:61.08ms
step:1064/2245 train_time:64988ms step_avg:61.08ms
step:1065/2245 train_time:65051ms step_avg:61.08ms
step:1066/2245 train_time:65111ms step_avg:61.08ms
step:1067/2245 train_time:65174ms step_avg:61.08ms
step:1068/2245 train_time:65234ms step_avg:61.08ms
step:1069/2245 train_time:65298ms step_avg:61.08ms
step:1070/2245 train_time:65359ms step_avg:61.08ms
step:1071/2245 train_time:65421ms step_avg:61.08ms
step:1072/2245 train_time:65480ms step_avg:61.08ms
step:1073/2245 train_time:65542ms step_avg:61.08ms
step:1074/2245 train_time:65602ms step_avg:61.08ms
step:1075/2245 train_time:65664ms step_avg:61.08ms
step:1076/2245 train_time:65723ms step_avg:61.08ms
step:1077/2245 train_time:65785ms step_avg:61.08ms
step:1078/2245 train_time:65844ms step_avg:61.08ms
step:1079/2245 train_time:65906ms step_avg:61.08ms
step:1080/2245 train_time:65966ms step_avg:61.08ms
step:1081/2245 train_time:66029ms step_avg:61.08ms
step:1082/2245 train_time:66089ms step_avg:61.08ms
step:1083/2245 train_time:66152ms step_avg:61.08ms
step:1084/2245 train_time:66212ms step_avg:61.08ms
step:1085/2245 train_time:66275ms step_avg:61.08ms
step:1086/2245 train_time:66336ms step_avg:61.08ms
step:1087/2245 train_time:66399ms step_avg:61.08ms
step:1088/2245 train_time:66459ms step_avg:61.08ms
step:1089/2245 train_time:66521ms step_avg:61.08ms
step:1090/2245 train_time:66580ms step_avg:61.08ms
step:1091/2245 train_time:66643ms step_avg:61.08ms
step:1092/2245 train_time:66703ms step_avg:61.08ms
step:1093/2245 train_time:66764ms step_avg:61.08ms
step:1094/2245 train_time:66824ms step_avg:61.08ms
step:1095/2245 train_time:66885ms step_avg:61.08ms
step:1096/2245 train_time:66945ms step_avg:61.08ms
step:1097/2245 train_time:67007ms step_avg:61.08ms
step:1098/2245 train_time:67067ms step_avg:61.08ms
step:1099/2245 train_time:67130ms step_avg:61.08ms
step:1100/2245 train_time:67190ms step_avg:61.08ms
step:1101/2245 train_time:67253ms step_avg:61.08ms
step:1102/2245 train_time:67316ms step_avg:61.09ms
step:1103/2245 train_time:67378ms step_avg:61.09ms
step:1104/2245 train_time:67438ms step_avg:61.08ms
step:1105/2245 train_time:67500ms step_avg:61.09ms
step:1106/2245 train_time:67560ms step_avg:61.09ms
step:1107/2245 train_time:67622ms step_avg:61.09ms
step:1108/2245 train_time:67682ms step_avg:61.08ms
step:1109/2245 train_time:67743ms step_avg:61.09ms
step:1110/2245 train_time:67803ms step_avg:61.08ms
step:1111/2245 train_time:67866ms step_avg:61.09ms
step:1112/2245 train_time:67925ms step_avg:61.08ms
step:1113/2245 train_time:67987ms step_avg:61.08ms
step:1114/2245 train_time:68046ms step_avg:61.08ms
step:1115/2245 train_time:68109ms step_avg:61.08ms
step:1116/2245 train_time:68169ms step_avg:61.08ms
step:1117/2245 train_time:68232ms step_avg:61.09ms
step:1118/2245 train_time:68293ms step_avg:61.08ms
step:1119/2245 train_time:68356ms step_avg:61.09ms
step:1120/2245 train_time:68417ms step_avg:61.09ms
step:1121/2245 train_time:68480ms step_avg:61.09ms
step:1122/2245 train_time:68539ms step_avg:61.09ms
step:1123/2245 train_time:68601ms step_avg:61.09ms
step:1124/2245 train_time:68661ms step_avg:61.09ms
step:1125/2245 train_time:68723ms step_avg:61.09ms
step:1126/2245 train_time:68783ms step_avg:61.09ms
step:1127/2245 train_time:68845ms step_avg:61.09ms
step:1128/2245 train_time:68905ms step_avg:61.09ms
step:1129/2245 train_time:68967ms step_avg:61.09ms
step:1130/2245 train_time:69027ms step_avg:61.09ms
step:1131/2245 train_time:69089ms step_avg:61.09ms
step:1132/2245 train_time:69149ms step_avg:61.09ms
step:1133/2245 train_time:69213ms step_avg:61.09ms
step:1134/2245 train_time:69272ms step_avg:61.09ms
step:1135/2245 train_time:69335ms step_avg:61.09ms
step:1136/2245 train_time:69396ms step_avg:61.09ms
step:1137/2245 train_time:69459ms step_avg:61.09ms
step:1138/2245 train_time:69518ms step_avg:61.09ms
step:1139/2245 train_time:69580ms step_avg:61.09ms
step:1140/2245 train_time:69640ms step_avg:61.09ms
step:1141/2245 train_time:69702ms step_avg:61.09ms
step:1142/2245 train_time:69763ms step_avg:61.09ms
step:1143/2245 train_time:69824ms step_avg:61.09ms
step:1144/2245 train_time:69884ms step_avg:61.09ms
step:1145/2245 train_time:69945ms step_avg:61.09ms
step:1146/2245 train_time:70005ms step_avg:61.09ms
step:1147/2245 train_time:70067ms step_avg:61.09ms
step:1148/2245 train_time:70127ms step_avg:61.09ms
step:1149/2245 train_time:70189ms step_avg:61.09ms
step:1150/2245 train_time:70250ms step_avg:61.09ms
step:1151/2245 train_time:70313ms step_avg:61.09ms
step:1152/2245 train_time:70373ms step_avg:61.09ms
step:1153/2245 train_time:70436ms step_avg:61.09ms
step:1154/2245 train_time:70496ms step_avg:61.09ms
step:1155/2245 train_time:70559ms step_avg:61.09ms
step:1156/2245 train_time:70618ms step_avg:61.09ms
step:1157/2245 train_time:70681ms step_avg:61.09ms
step:1158/2245 train_time:70740ms step_avg:61.09ms
step:1159/2245 train_time:70802ms step_avg:61.09ms
step:1160/2245 train_time:70862ms step_avg:61.09ms
step:1161/2245 train_time:70925ms step_avg:61.09ms
step:1162/2245 train_time:70984ms step_avg:61.09ms
step:1163/2245 train_time:71046ms step_avg:61.09ms
step:1164/2245 train_time:71106ms step_avg:61.09ms
step:1165/2245 train_time:71170ms step_avg:61.09ms
step:1166/2245 train_time:71230ms step_avg:61.09ms
step:1167/2245 train_time:71292ms step_avg:61.09ms
step:1168/2245 train_time:71353ms step_avg:61.09ms
step:1169/2245 train_time:71415ms step_avg:61.09ms
step:1170/2245 train_time:71475ms step_avg:61.09ms
step:1171/2245 train_time:71538ms step_avg:61.09ms
step:1172/2245 train_time:71598ms step_avg:61.09ms
step:1173/2245 train_time:71660ms step_avg:61.09ms
step:1174/2245 train_time:71720ms step_avg:61.09ms
step:1175/2245 train_time:71782ms step_avg:61.09ms
step:1176/2245 train_time:71842ms step_avg:61.09ms
step:1177/2245 train_time:71904ms step_avg:61.09ms
step:1178/2245 train_time:71965ms step_avg:61.09ms
step:1179/2245 train_time:72027ms step_avg:61.09ms
step:1180/2245 train_time:72086ms step_avg:61.09ms
step:1181/2245 train_time:72148ms step_avg:61.09ms
step:1182/2245 train_time:72208ms step_avg:61.09ms
step:1183/2245 train_time:72271ms step_avg:61.09ms
step:1184/2245 train_time:72331ms step_avg:61.09ms
step:1185/2245 train_time:72393ms step_avg:61.09ms
step:1186/2245 train_time:72455ms step_avg:61.09ms
step:1187/2245 train_time:72519ms step_avg:61.09ms
step:1188/2245 train_time:72578ms step_avg:61.09ms
step:1189/2245 train_time:72641ms step_avg:61.09ms
step:1190/2245 train_time:72700ms step_avg:61.09ms
step:1191/2245 train_time:72763ms step_avg:61.09ms
step:1192/2245 train_time:72822ms step_avg:61.09ms
step:1193/2245 train_time:72885ms step_avg:61.09ms
step:1194/2245 train_time:72945ms step_avg:61.09ms
step:1195/2245 train_time:73006ms step_avg:61.09ms
step:1196/2245 train_time:73066ms step_avg:61.09ms
step:1197/2245 train_time:73128ms step_avg:61.09ms
step:1198/2245 train_time:73188ms step_avg:61.09ms
step:1199/2245 train_time:73251ms step_avg:61.09ms
step:1200/2245 train_time:73311ms step_avg:61.09ms
step:1201/2245 train_time:73374ms step_avg:61.09ms
step:1202/2245 train_time:73434ms step_avg:61.09ms
step:1203/2245 train_time:73498ms step_avg:61.10ms
step:1204/2245 train_time:73557ms step_avg:61.09ms
step:1205/2245 train_time:73619ms step_avg:61.09ms
step:1206/2245 train_time:73679ms step_avg:61.09ms
step:1207/2245 train_time:73741ms step_avg:61.09ms
step:1208/2245 train_time:73800ms step_avg:61.09ms
step:1209/2245 train_time:73863ms step_avg:61.09ms
step:1210/2245 train_time:73923ms step_avg:61.09ms
step:1211/2245 train_time:73984ms step_avg:61.09ms
step:1212/2245 train_time:74044ms step_avg:61.09ms
step:1213/2245 train_time:74106ms step_avg:61.09ms
step:1214/2245 train_time:74166ms step_avg:61.09ms
step:1215/2245 train_time:74229ms step_avg:61.09ms
step:1216/2245 train_time:74290ms step_avg:61.09ms
step:1217/2245 train_time:74353ms step_avg:61.09ms
step:1218/2245 train_time:74413ms step_avg:61.09ms
step:1219/2245 train_time:74476ms step_avg:61.10ms
step:1220/2245 train_time:74535ms step_avg:61.09ms
step:1221/2245 train_time:74598ms step_avg:61.10ms
step:1222/2245 train_time:74658ms step_avg:61.10ms
step:1223/2245 train_time:74720ms step_avg:61.10ms
step:1224/2245 train_time:74781ms step_avg:61.10ms
step:1225/2245 train_time:74843ms step_avg:61.10ms
step:1226/2245 train_time:74902ms step_avg:61.09ms
step:1227/2245 train_time:74964ms step_avg:61.10ms
step:1228/2245 train_time:75023ms step_avg:61.09ms
step:1229/2245 train_time:75085ms step_avg:61.09ms
step:1230/2245 train_time:75145ms step_avg:61.09ms
step:1231/2245 train_time:75208ms step_avg:61.09ms
step:1232/2245 train_time:75268ms step_avg:61.09ms
step:1233/2245 train_time:75331ms step_avg:61.10ms
step:1234/2245 train_time:75391ms step_avg:61.09ms
step:1235/2245 train_time:75454ms step_avg:61.10ms
step:1236/2245 train_time:75513ms step_avg:61.09ms
step:1237/2245 train_time:75577ms step_avg:61.10ms
step:1238/2245 train_time:75636ms step_avg:61.10ms
step:1239/2245 train_time:75699ms step_avg:61.10ms
step:1240/2245 train_time:75759ms step_avg:61.10ms
step:1241/2245 train_time:75822ms step_avg:61.10ms
step:1242/2245 train_time:75881ms step_avg:61.10ms
step:1243/2245 train_time:75943ms step_avg:61.10ms
step:1244/2245 train_time:76003ms step_avg:61.10ms
step:1245/2245 train_time:76065ms step_avg:61.10ms
step:1246/2245 train_time:76125ms step_avg:61.10ms
step:1247/2245 train_time:76187ms step_avg:61.10ms
step:1248/2245 train_time:76246ms step_avg:61.09ms
step:1249/2245 train_time:76310ms step_avg:61.10ms
step:1250/2245 train_time:76370ms step_avg:61.10ms
step:1250/2245 val_loss:3.5215 train_time:76434ms step_avg:61.15ms
step:1251/2245 train_time:76453ms step_avg:61.11ms
step:1252/2245 train_time:76496ms step_avg:61.10ms
step:1253/2245 train_time:76561ms step_avg:61.10ms
step:1254/2245 train_time:76620ms step_avg:61.10ms
step:1255/2245 train_time:76683ms step_avg:61.10ms
step:1256/2245 train_time:76744ms step_avg:61.10ms
step:1257/2245 train_time:76805ms step_avg:61.10ms
step:1258/2245 train_time:76866ms step_avg:61.10ms
step:1259/2245 train_time:76928ms step_avg:61.10ms
step:1260/2245 train_time:76988ms step_avg:61.10ms
step:1261/2245 train_time:77051ms step_avg:61.10ms
step:1262/2245 train_time:77110ms step_avg:61.10ms
step:1263/2245 train_time:77171ms step_avg:61.10ms
step:1264/2245 train_time:77231ms step_avg:61.10ms
step:1265/2245 train_time:77292ms step_avg:61.10ms
step:1266/2245 train_time:77352ms step_avg:61.10ms
step:1267/2245 train_time:77416ms step_avg:61.10ms
step:1268/2245 train_time:77477ms step_avg:61.10ms
step:1269/2245 train_time:77540ms step_avg:61.10ms
step:1270/2245 train_time:77601ms step_avg:61.10ms
step:1271/2245 train_time:77663ms step_avg:61.10ms
step:1272/2245 train_time:77723ms step_avg:61.10ms
step:1273/2245 train_time:77785ms step_avg:61.10ms
step:1274/2245 train_time:77845ms step_avg:61.10ms
step:1275/2245 train_time:77907ms step_avg:61.10ms
step:1276/2245 train_time:77966ms step_avg:61.10ms
step:1277/2245 train_time:78029ms step_avg:61.10ms
step:1278/2245 train_time:78089ms step_avg:61.10ms
step:1279/2245 train_time:78151ms step_avg:61.10ms
step:1280/2245 train_time:78211ms step_avg:61.10ms
step:1281/2245 train_time:78272ms step_avg:61.10ms
step:1282/2245 train_time:78333ms step_avg:61.10ms
step:1283/2245 train_time:78396ms step_avg:61.10ms
step:1284/2245 train_time:78456ms step_avg:61.10ms
step:1285/2245 train_time:78518ms step_avg:61.10ms
step:1286/2245 train_time:78578ms step_avg:61.10ms
step:1287/2245 train_time:78641ms step_avg:61.10ms
step:1288/2245 train_time:78701ms step_avg:61.10ms
step:1289/2245 train_time:78764ms step_avg:61.10ms
step:1290/2245 train_time:78824ms step_avg:61.10ms
step:1291/2245 train_time:78887ms step_avg:61.11ms
step:1292/2245 train_time:78946ms step_avg:61.10ms
step:1293/2245 train_time:79008ms step_avg:61.10ms
step:1294/2245 train_time:79068ms step_avg:61.10ms
step:1295/2245 train_time:79130ms step_avg:61.10ms
step:1296/2245 train_time:79190ms step_avg:61.10ms
step:1297/2245 train_time:79253ms step_avg:61.11ms
step:1298/2245 train_time:79312ms step_avg:61.10ms
step:1299/2245 train_time:79374ms step_avg:61.10ms
step:1300/2245 train_time:79435ms step_avg:61.10ms
step:1301/2245 train_time:79498ms step_avg:61.11ms
step:1302/2245 train_time:79557ms step_avg:61.10ms
step:1303/2245 train_time:79619ms step_avg:61.10ms
step:1304/2245 train_time:79679ms step_avg:61.10ms
step:1305/2245 train_time:79741ms step_avg:61.10ms
step:1306/2245 train_time:79802ms step_avg:61.10ms
step:1307/2245 train_time:79865ms step_avg:61.11ms
step:1308/2245 train_time:79925ms step_avg:61.10ms
step:1309/2245 train_time:79986ms step_avg:61.10ms
step:1310/2245 train_time:80047ms step_avg:61.10ms
step:1311/2245 train_time:80110ms step_avg:61.11ms
step:1312/2245 train_time:80170ms step_avg:61.10ms
step:1313/2245 train_time:80232ms step_avg:61.11ms
step:1314/2245 train_time:80292ms step_avg:61.11ms
step:1315/2245 train_time:80354ms step_avg:61.11ms
step:1316/2245 train_time:80414ms step_avg:61.11ms
step:1317/2245 train_time:80476ms step_avg:61.11ms
step:1318/2245 train_time:80536ms step_avg:61.10ms
step:1319/2245 train_time:80598ms step_avg:61.11ms
step:1320/2245 train_time:80658ms step_avg:61.10ms
step:1321/2245 train_time:80720ms step_avg:61.11ms
step:1322/2245 train_time:80780ms step_avg:61.10ms
step:1323/2245 train_time:80843ms step_avg:61.11ms
step:1324/2245 train_time:80904ms step_avg:61.11ms
step:1325/2245 train_time:80966ms step_avg:61.11ms
step:1326/2245 train_time:81026ms step_avg:61.11ms
step:1327/2245 train_time:81089ms step_avg:61.11ms
step:1328/2245 train_time:81149ms step_avg:61.11ms
step:1329/2245 train_time:81212ms step_avg:61.11ms
step:1330/2245 train_time:81271ms step_avg:61.11ms
step:1331/2245 train_time:81334ms step_avg:61.11ms
step:1332/2245 train_time:81395ms step_avg:61.11ms
step:1333/2245 train_time:81456ms step_avg:61.11ms
step:1334/2245 train_time:81516ms step_avg:61.11ms
step:1335/2245 train_time:81577ms step_avg:61.11ms
step:1336/2245 train_time:81637ms step_avg:61.11ms
step:1337/2245 train_time:81699ms step_avg:61.11ms
step:1338/2245 train_time:81759ms step_avg:61.11ms
step:1339/2245 train_time:81821ms step_avg:61.11ms
step:1340/2245 train_time:81881ms step_avg:61.11ms
step:1341/2245 train_time:81945ms step_avg:61.11ms
step:1342/2245 train_time:82005ms step_avg:61.11ms
step:1343/2245 train_time:82068ms step_avg:61.11ms
step:1344/2245 train_time:82128ms step_avg:61.11ms
step:1345/2245 train_time:82191ms step_avg:61.11ms
step:1346/2245 train_time:82251ms step_avg:61.11ms
step:1347/2245 train_time:82314ms step_avg:61.11ms
step:1348/2245 train_time:82374ms step_avg:61.11ms
step:1349/2245 train_time:82437ms step_avg:61.11ms
step:1350/2245 train_time:82498ms step_avg:61.11ms
step:1351/2245 train_time:82560ms step_avg:61.11ms
step:1352/2245 train_time:82619ms step_avg:61.11ms
step:1353/2245 train_time:82680ms step_avg:61.11ms
step:1354/2245 train_time:82740ms step_avg:61.11ms
step:1355/2245 train_time:82802ms step_avg:61.11ms
step:1356/2245 train_time:82862ms step_avg:61.11ms
step:1357/2245 train_time:82925ms step_avg:61.11ms
step:1358/2245 train_time:82985ms step_avg:61.11ms
step:1359/2245 train_time:83048ms step_avg:61.11ms
step:1360/2245 train_time:83108ms step_avg:61.11ms
step:1361/2245 train_time:83171ms step_avg:61.11ms
step:1362/2245 train_time:83231ms step_avg:61.11ms
step:1363/2245 train_time:83293ms step_avg:61.11ms
step:1364/2245 train_time:83353ms step_avg:61.11ms
step:1365/2245 train_time:83415ms step_avg:61.11ms
step:1366/2245 train_time:83476ms step_avg:61.11ms
step:1367/2245 train_time:83538ms step_avg:61.11ms
step:1368/2245 train_time:83597ms step_avg:61.11ms
step:1369/2245 train_time:83659ms step_avg:61.11ms
step:1370/2245 train_time:83719ms step_avg:61.11ms
step:1371/2245 train_time:83781ms step_avg:61.11ms
step:1372/2245 train_time:83841ms step_avg:61.11ms
step:1373/2245 train_time:83904ms step_avg:61.11ms
step:1374/2245 train_time:83964ms step_avg:61.11ms
step:1375/2245 train_time:84026ms step_avg:61.11ms
step:1376/2245 train_time:84087ms step_avg:61.11ms
step:1377/2245 train_time:84150ms step_avg:61.11ms
step:1378/2245 train_time:84210ms step_avg:61.11ms
step:1379/2245 train_time:84272ms step_avg:61.11ms
step:1380/2245 train_time:84331ms step_avg:61.11ms
step:1381/2245 train_time:84393ms step_avg:61.11ms
step:1382/2245 train_time:84453ms step_avg:61.11ms
step:1383/2245 train_time:84515ms step_avg:61.11ms
step:1384/2245 train_time:84574ms step_avg:61.11ms
step:1385/2245 train_time:84637ms step_avg:61.11ms
step:1386/2245 train_time:84696ms step_avg:61.11ms
step:1387/2245 train_time:84758ms step_avg:61.11ms
step:1388/2245 train_time:84818ms step_avg:61.11ms
step:1389/2245 train_time:84880ms step_avg:61.11ms
step:1390/2245 train_time:84940ms step_avg:61.11ms
step:1391/2245 train_time:85004ms step_avg:61.11ms
step:1392/2245 train_time:85064ms step_avg:61.11ms
step:1393/2245 train_time:85126ms step_avg:61.11ms
step:1394/2245 train_time:85187ms step_avg:61.11ms
step:1395/2245 train_time:85249ms step_avg:61.11ms
step:1396/2245 train_time:85310ms step_avg:61.11ms
step:1397/2245 train_time:85373ms step_avg:61.11ms
step:1398/2245 train_time:85432ms step_avg:61.11ms
step:1399/2245 train_time:85494ms step_avg:61.11ms
step:1400/2245 train_time:85554ms step_avg:61.11ms
step:1401/2245 train_time:85616ms step_avg:61.11ms
step:1402/2245 train_time:85675ms step_avg:61.11ms
step:1403/2245 train_time:85737ms step_avg:61.11ms
step:1404/2245 train_time:85798ms step_avg:61.11ms
step:1405/2245 train_time:85859ms step_avg:61.11ms
step:1406/2245 train_time:85919ms step_avg:61.11ms
step:1407/2245 train_time:85982ms step_avg:61.11ms
step:1408/2245 train_time:86042ms step_avg:61.11ms
step:1409/2245 train_time:86106ms step_avg:61.11ms
step:1410/2245 train_time:86165ms step_avg:61.11ms
step:1411/2245 train_time:86228ms step_avg:61.11ms
step:1412/2245 train_time:86290ms step_avg:61.11ms
step:1413/2245 train_time:86353ms step_avg:61.11ms
step:1414/2245 train_time:86412ms step_avg:61.11ms
step:1415/2245 train_time:86474ms step_avg:61.11ms
step:1416/2245 train_time:86534ms step_avg:61.11ms
step:1417/2245 train_time:86596ms step_avg:61.11ms
step:1418/2245 train_time:86656ms step_avg:61.11ms
step:1419/2245 train_time:86718ms step_avg:61.11ms
step:1420/2245 train_time:86777ms step_avg:61.11ms
step:1421/2245 train_time:86839ms step_avg:61.11ms
step:1422/2245 train_time:86900ms step_avg:61.11ms
step:1423/2245 train_time:86962ms step_avg:61.11ms
step:1424/2245 train_time:87022ms step_avg:61.11ms
step:1425/2245 train_time:87085ms step_avg:61.11ms
step:1426/2245 train_time:87145ms step_avg:61.11ms
step:1427/2245 train_time:87208ms step_avg:61.11ms
step:1428/2245 train_time:87269ms step_avg:61.11ms
step:1429/2245 train_time:87331ms step_avg:61.11ms
step:1430/2245 train_time:87391ms step_avg:61.11ms
step:1431/2245 train_time:87453ms step_avg:61.11ms
step:1432/2245 train_time:87513ms step_avg:61.11ms
step:1433/2245 train_time:87575ms step_avg:61.11ms
step:1434/2245 train_time:87634ms step_avg:61.11ms
step:1435/2245 train_time:87697ms step_avg:61.11ms
step:1436/2245 train_time:87757ms step_avg:61.11ms
step:1437/2245 train_time:87819ms step_avg:61.11ms
step:1438/2245 train_time:87878ms step_avg:61.11ms
step:1439/2245 train_time:87941ms step_avg:61.11ms
step:1440/2245 train_time:88001ms step_avg:61.11ms
step:1441/2245 train_time:88064ms step_avg:61.11ms
step:1442/2245 train_time:88124ms step_avg:61.11ms
step:1443/2245 train_time:88187ms step_avg:61.11ms
step:1444/2245 train_time:88248ms step_avg:61.11ms
step:1445/2245 train_time:88310ms step_avg:61.11ms
step:1446/2245 train_time:88370ms step_avg:61.11ms
step:1447/2245 train_time:88433ms step_avg:61.11ms
step:1448/2245 train_time:88493ms step_avg:61.11ms
step:1449/2245 train_time:88555ms step_avg:61.11ms
step:1450/2245 train_time:88615ms step_avg:61.11ms
step:1451/2245 train_time:88677ms step_avg:61.11ms
step:1452/2245 train_time:88737ms step_avg:61.11ms
step:1453/2245 train_time:88799ms step_avg:61.11ms
step:1454/2245 train_time:88859ms step_avg:61.11ms
step:1455/2245 train_time:88921ms step_avg:61.11ms
step:1456/2245 train_time:88981ms step_avg:61.11ms
step:1457/2245 train_time:89043ms step_avg:61.11ms
step:1458/2245 train_time:89104ms step_avg:61.11ms
step:1459/2245 train_time:89167ms step_avg:61.11ms
step:1460/2245 train_time:89227ms step_avg:61.11ms
step:1461/2245 train_time:89289ms step_avg:61.12ms
step:1462/2245 train_time:89350ms step_avg:61.11ms
step:1463/2245 train_time:89413ms step_avg:61.12ms
step:1464/2245 train_time:89473ms step_avg:61.12ms
step:1465/2245 train_time:89535ms step_avg:61.12ms
step:1466/2245 train_time:89595ms step_avg:61.12ms
step:1467/2245 train_time:89657ms step_avg:61.12ms
step:1468/2245 train_time:89717ms step_avg:61.12ms
step:1469/2245 train_time:89779ms step_avg:61.12ms
step:1470/2245 train_time:89838ms step_avg:61.11ms
step:1471/2245 train_time:89900ms step_avg:61.11ms
step:1472/2245 train_time:89960ms step_avg:61.11ms
step:1473/2245 train_time:90023ms step_avg:61.12ms
step:1474/2245 train_time:90083ms step_avg:61.11ms
step:1475/2245 train_time:90147ms step_avg:61.12ms
step:1476/2245 train_time:90208ms step_avg:61.12ms
step:1477/2245 train_time:90270ms step_avg:61.12ms
step:1478/2245 train_time:90330ms step_avg:61.12ms
step:1479/2245 train_time:90393ms step_avg:61.12ms
step:1480/2245 train_time:90453ms step_avg:61.12ms
step:1481/2245 train_time:90516ms step_avg:61.12ms
step:1482/2245 train_time:90576ms step_avg:61.12ms
step:1483/2245 train_time:90638ms step_avg:61.12ms
step:1484/2245 train_time:90699ms step_avg:61.12ms
step:1485/2245 train_time:90761ms step_avg:61.12ms
step:1486/2245 train_time:90821ms step_avg:61.12ms
step:1487/2245 train_time:90883ms step_avg:61.12ms
step:1488/2245 train_time:90944ms step_avg:61.12ms
step:1489/2245 train_time:91007ms step_avg:61.12ms
step:1490/2245 train_time:91067ms step_avg:61.12ms
step:1491/2245 train_time:91130ms step_avg:61.12ms
step:1492/2245 train_time:91190ms step_avg:61.12ms
step:1493/2245 train_time:91253ms step_avg:61.12ms
step:1494/2245 train_time:91312ms step_avg:61.12ms
step:1495/2245 train_time:91375ms step_avg:61.12ms
step:1496/2245 train_time:91436ms step_avg:61.12ms
step:1497/2245 train_time:91499ms step_avg:61.12ms
step:1498/2245 train_time:91560ms step_avg:61.12ms
step:1499/2245 train_time:91623ms step_avg:61.12ms
step:1500/2245 train_time:91684ms step_avg:61.12ms
step:1500/2245 val_loss:3.4415 train_time:91748ms step_avg:61.17ms
step:1501/2245 train_time:91768ms step_avg:61.14ms
step:1502/2245 train_time:91812ms step_avg:61.13ms
step:1503/2245 train_time:91874ms step_avg:61.13ms
step:1504/2245 train_time:91933ms step_avg:61.13ms
step:1505/2245 train_time:91996ms step_avg:61.13ms
step:1506/2245 train_time:92057ms step_avg:61.13ms
step:1507/2245 train_time:92119ms step_avg:61.13ms
step:1508/2245 train_time:92179ms step_avg:61.13ms
step:1509/2245 train_time:92240ms step_avg:61.13ms
step:1510/2245 train_time:92299ms step_avg:61.13ms
step:1511/2245 train_time:92361ms step_avg:61.13ms
step:1512/2245 train_time:92421ms step_avg:61.12ms
step:1513/2245 train_time:92483ms step_avg:61.13ms
step:1514/2245 train_time:92543ms step_avg:61.12ms
step:1515/2245 train_time:92606ms step_avg:61.13ms
step:1516/2245 train_time:92672ms step_avg:61.13ms
step:1517/2245 train_time:92739ms step_avg:61.13ms
step:1518/2245 train_time:92801ms step_avg:61.13ms
step:1519/2245 train_time:92864ms step_avg:61.13ms
step:1520/2245 train_time:92924ms step_avg:61.13ms
step:1521/2245 train_time:92987ms step_avg:61.14ms
step:1522/2245 train_time:93048ms step_avg:61.14ms
step:1523/2245 train_time:93110ms step_avg:61.14ms
step:1524/2245 train_time:93170ms step_avg:61.13ms
step:1525/2245 train_time:93232ms step_avg:61.14ms
step:1526/2245 train_time:93292ms step_avg:61.14ms
step:1527/2245 train_time:93354ms step_avg:61.14ms
step:1528/2245 train_time:93414ms step_avg:61.13ms
step:1529/2245 train_time:93476ms step_avg:61.14ms
step:1530/2245 train_time:93535ms step_avg:61.13ms
step:1531/2245 train_time:93598ms step_avg:61.14ms
step:1532/2245 train_time:93659ms step_avg:61.14ms
step:1533/2245 train_time:93724ms step_avg:61.14ms
step:1534/2245 train_time:93786ms step_avg:61.14ms
step:1535/2245 train_time:93850ms step_avg:61.14ms
step:1536/2245 train_time:93910ms step_avg:61.14ms
step:1537/2245 train_time:93973ms step_avg:61.14ms
step:1538/2245 train_time:94033ms step_avg:61.14ms
step:1539/2245 train_time:94095ms step_avg:61.14ms
step:1540/2245 train_time:94156ms step_avg:61.14ms
step:1541/2245 train_time:94218ms step_avg:61.14ms
step:1542/2245 train_time:94279ms step_avg:61.14ms
step:1543/2245 train_time:94340ms step_avg:61.14ms
step:1544/2245 train_time:94400ms step_avg:61.14ms
step:1545/2245 train_time:94462ms step_avg:61.14ms
step:1546/2245 train_time:94523ms step_avg:61.14ms
step:1547/2245 train_time:94586ms step_avg:61.14ms
step:1548/2245 train_time:94646ms step_avg:61.14ms
step:1549/2245 train_time:94710ms step_avg:61.14ms
step:1550/2245 train_time:94771ms step_avg:61.14ms
step:1551/2245 train_time:94834ms step_avg:61.14ms
step:1552/2245 train_time:94895ms step_avg:61.14ms
step:1553/2245 train_time:94957ms step_avg:61.14ms
step:1554/2245 train_time:95018ms step_avg:61.14ms
step:1555/2245 train_time:95080ms step_avg:61.14ms
step:1556/2245 train_time:95141ms step_avg:61.14ms
step:1557/2245 train_time:95204ms step_avg:61.15ms
step:1558/2245 train_time:95264ms step_avg:61.15ms
step:1559/2245 train_time:95326ms step_avg:61.15ms
step:1560/2245 train_time:95386ms step_avg:61.15ms
step:1561/2245 train_time:95449ms step_avg:61.15ms
step:1562/2245 train_time:95510ms step_avg:61.15ms
step:1563/2245 train_time:95573ms step_avg:61.15ms
step:1564/2245 train_time:95633ms step_avg:61.15ms
step:1565/2245 train_time:95696ms step_avg:61.15ms
step:1566/2245 train_time:95756ms step_avg:61.15ms
step:1567/2245 train_time:95819ms step_avg:61.15ms
step:1568/2245 train_time:95881ms step_avg:61.15ms
step:1569/2245 train_time:95944ms step_avg:61.15ms
step:1570/2245 train_time:96005ms step_avg:61.15ms
step:1571/2245 train_time:96069ms step_avg:61.15ms
step:1572/2245 train_time:96129ms step_avg:61.15ms
step:1573/2245 train_time:96192ms step_avg:61.15ms
step:1574/2245 train_time:96251ms step_avg:61.15ms
step:1575/2245 train_time:96313ms step_avg:61.15ms
step:1576/2245 train_time:96373ms step_avg:61.15ms
step:1577/2245 train_time:96436ms step_avg:61.15ms
step:1578/2245 train_time:96496ms step_avg:61.15ms
step:1579/2245 train_time:96558ms step_avg:61.15ms
step:1580/2245 train_time:96618ms step_avg:61.15ms
step:1581/2245 train_time:96681ms step_avg:61.15ms
step:1582/2245 train_time:96742ms step_avg:61.15ms
step:1583/2245 train_time:96805ms step_avg:61.15ms
step:1584/2245 train_time:96866ms step_avg:61.15ms
step:1585/2245 train_time:96929ms step_avg:61.15ms
step:1586/2245 train_time:96989ms step_avg:61.15ms
step:1587/2245 train_time:97054ms step_avg:61.16ms
step:1588/2245 train_time:97113ms step_avg:61.15ms
step:1589/2245 train_time:97175ms step_avg:61.15ms
step:1590/2245 train_time:97235ms step_avg:61.15ms
step:1591/2245 train_time:97297ms step_avg:61.15ms
step:1592/2245 train_time:97357ms step_avg:61.15ms
step:1593/2245 train_time:97420ms step_avg:61.15ms
step:1594/2245 train_time:97481ms step_avg:61.15ms
step:1595/2245 train_time:97544ms step_avg:61.16ms
step:1596/2245 train_time:97604ms step_avg:61.16ms
step:1597/2245 train_time:97666ms step_avg:61.16ms
step:1598/2245 train_time:97727ms step_avg:61.16ms
step:1599/2245 train_time:97790ms step_avg:61.16ms
step:1600/2245 train_time:97850ms step_avg:61.16ms
step:1601/2245 train_time:97913ms step_avg:61.16ms
step:1602/2245 train_time:97974ms step_avg:61.16ms
step:1603/2245 train_time:98037ms step_avg:61.16ms
step:1604/2245 train_time:98097ms step_avg:61.16ms
step:1605/2245 train_time:98160ms step_avg:61.16ms
step:1606/2245 train_time:98220ms step_avg:61.16ms
step:1607/2245 train_time:98282ms step_avg:61.16ms
step:1608/2245 train_time:98342ms step_avg:61.16ms
step:1609/2245 train_time:98405ms step_avg:61.16ms
step:1610/2245 train_time:98466ms step_avg:61.16ms
step:1611/2245 train_time:98529ms step_avg:61.16ms
step:1612/2245 train_time:98589ms step_avg:61.16ms
step:1613/2245 train_time:98652ms step_avg:61.16ms
step:1614/2245 train_time:98712ms step_avg:61.16ms
step:1615/2245 train_time:98775ms step_avg:61.16ms
step:1616/2245 train_time:98834ms step_avg:61.16ms
step:1617/2245 train_time:98897ms step_avg:61.16ms
step:1618/2245 train_time:98958ms step_avg:61.16ms
step:1619/2245 train_time:99020ms step_avg:61.16ms
step:1620/2245 train_time:99080ms step_avg:61.16ms
step:1621/2245 train_time:99143ms step_avg:61.16ms
step:1622/2245 train_time:99204ms step_avg:61.16ms
step:1623/2245 train_time:99266ms step_avg:61.16ms
step:1624/2245 train_time:99327ms step_avg:61.16ms
step:1625/2245 train_time:99389ms step_avg:61.16ms
step:1626/2245 train_time:99450ms step_avg:61.16ms
step:1627/2245 train_time:99512ms step_avg:61.16ms
step:1628/2245 train_time:99572ms step_avg:61.16ms
step:1629/2245 train_time:99635ms step_avg:61.16ms
step:1630/2245 train_time:99696ms step_avg:61.16ms
step:1631/2245 train_time:99758ms step_avg:61.16ms
step:1632/2245 train_time:99818ms step_avg:61.16ms
step:1633/2245 train_time:99881ms step_avg:61.16ms
step:1634/2245 train_time:99941ms step_avg:61.16ms
step:1635/2245 train_time:100004ms step_avg:61.16ms
step:1636/2245 train_time:100065ms step_avg:61.16ms
step:1637/2245 train_time:100129ms step_avg:61.17ms
step:1638/2245 train_time:100189ms step_avg:61.17ms
step:1639/2245 train_time:100251ms step_avg:61.17ms
step:1640/2245 train_time:100311ms step_avg:61.17ms
step:1641/2245 train_time:100373ms step_avg:61.17ms
step:1642/2245 train_time:100433ms step_avg:61.17ms
step:1643/2245 train_time:100496ms step_avg:61.17ms
step:1644/2245 train_time:100557ms step_avg:61.17ms
step:1645/2245 train_time:100619ms step_avg:61.17ms
step:1646/2245 train_time:100680ms step_avg:61.17ms
step:1647/2245 train_time:100743ms step_avg:61.17ms
step:1648/2245 train_time:100804ms step_avg:61.17ms
step:1649/2245 train_time:100868ms step_avg:61.17ms
step:1650/2245 train_time:100928ms step_avg:61.17ms
step:1651/2245 train_time:100991ms step_avg:61.17ms
step:1652/2245 train_time:101051ms step_avg:61.17ms
step:1653/2245 train_time:101113ms step_avg:61.17ms
step:1654/2245 train_time:101174ms step_avg:61.17ms
step:1655/2245 train_time:101236ms step_avg:61.17ms
step:1656/2245 train_time:101297ms step_avg:61.17ms
step:1657/2245 train_time:101359ms step_avg:61.17ms
step:1658/2245 train_time:101419ms step_avg:61.17ms
step:1659/2245 train_time:101483ms step_avg:61.17ms
step:1660/2245 train_time:101547ms step_avg:61.17ms
step:1661/2245 train_time:101607ms step_avg:61.17ms
step:1662/2245 train_time:101667ms step_avg:61.17ms
step:1663/2245 train_time:101730ms step_avg:61.17ms
step:1664/2245 train_time:101790ms step_avg:61.17ms
step:1665/2245 train_time:101853ms step_avg:61.17ms
step:1666/2245 train_time:101913ms step_avg:61.17ms
step:1667/2245 train_time:101977ms step_avg:61.17ms
step:1668/2245 train_time:102037ms step_avg:61.17ms
step:1669/2245 train_time:102100ms step_avg:61.17ms
step:1670/2245 train_time:102160ms step_avg:61.17ms
step:1671/2245 train_time:102224ms step_avg:61.18ms
step:1672/2245 train_time:102285ms step_avg:61.18ms
step:1673/2245 train_time:102348ms step_avg:61.18ms
step:1674/2245 train_time:102408ms step_avg:61.18ms
step:1675/2245 train_time:102471ms step_avg:61.18ms
step:1676/2245 train_time:102531ms step_avg:61.18ms
step:1677/2245 train_time:102594ms step_avg:61.18ms
step:1678/2245 train_time:102654ms step_avg:61.18ms
step:1679/2245 train_time:102716ms step_avg:61.18ms
step:1680/2245 train_time:102778ms step_avg:61.18ms
step:1681/2245 train_time:102840ms step_avg:61.18ms
step:1682/2245 train_time:102901ms step_avg:61.18ms
step:1683/2245 train_time:102964ms step_avg:61.18ms
step:1684/2245 train_time:103025ms step_avg:61.18ms
step:1685/2245 train_time:103089ms step_avg:61.18ms
step:1686/2245 train_time:103149ms step_avg:61.18ms
step:1687/2245 train_time:103212ms step_avg:61.18ms
step:1688/2245 train_time:103272ms step_avg:61.18ms
step:1689/2245 train_time:103335ms step_avg:61.18ms
step:1690/2245 train_time:103395ms step_avg:61.18ms
step:1691/2245 train_time:103457ms step_avg:61.18ms
step:1692/2245 train_time:103518ms step_avg:61.18ms
step:1693/2245 train_time:103581ms step_avg:61.18ms
step:1694/2245 train_time:103641ms step_avg:61.18ms
step:1695/2245 train_time:103705ms step_avg:61.18ms
step:1696/2245 train_time:103766ms step_avg:61.18ms
step:1697/2245 train_time:103829ms step_avg:61.18ms
step:1698/2245 train_time:103889ms step_avg:61.18ms
step:1699/2245 train_time:103952ms step_avg:61.18ms
step:1700/2245 train_time:104012ms step_avg:61.18ms
step:1701/2245 train_time:104074ms step_avg:61.18ms
step:1702/2245 train_time:104135ms step_avg:61.18ms
step:1703/2245 train_time:104198ms step_avg:61.18ms
step:1704/2245 train_time:104258ms step_avg:61.18ms
step:1705/2245 train_time:104321ms step_avg:61.19ms
step:1706/2245 train_time:104382ms step_avg:61.18ms
step:1707/2245 train_time:104444ms step_avg:61.19ms
step:1708/2245 train_time:104505ms step_avg:61.19ms
step:1709/2245 train_time:104569ms step_avg:61.19ms
step:1710/2245 train_time:104630ms step_avg:61.19ms
step:1711/2245 train_time:104693ms step_avg:61.19ms
step:1712/2245 train_time:104753ms step_avg:61.19ms
step:1713/2245 train_time:104816ms step_avg:61.19ms
step:1714/2245 train_time:104876ms step_avg:61.19ms
step:1715/2245 train_time:104939ms step_avg:61.19ms
step:1716/2245 train_time:105000ms step_avg:61.19ms
step:1717/2245 train_time:105063ms step_avg:61.19ms
step:1718/2245 train_time:105123ms step_avg:61.19ms
step:1719/2245 train_time:105186ms step_avg:61.19ms
step:1720/2245 train_time:105247ms step_avg:61.19ms
step:1721/2245 train_time:105310ms step_avg:61.19ms
step:1722/2245 train_time:105371ms step_avg:61.19ms
step:1723/2245 train_time:105433ms step_avg:61.19ms
step:1724/2245 train_time:105494ms step_avg:61.19ms
step:1725/2245 train_time:105557ms step_avg:61.19ms
step:1726/2245 train_time:105617ms step_avg:61.19ms
step:1727/2245 train_time:105680ms step_avg:61.19ms
step:1728/2245 train_time:105740ms step_avg:61.19ms
step:1729/2245 train_time:105803ms step_avg:61.19ms
step:1730/2245 train_time:105865ms step_avg:61.19ms
step:1731/2245 train_time:105927ms step_avg:61.19ms
step:1732/2245 train_time:105988ms step_avg:61.19ms
step:1733/2245 train_time:106050ms step_avg:61.19ms
step:1734/2245 train_time:106110ms step_avg:61.19ms
step:1735/2245 train_time:106173ms step_avg:61.19ms
step:1736/2245 train_time:106233ms step_avg:61.19ms
step:1737/2245 train_time:106295ms step_avg:61.19ms
step:1738/2245 train_time:106355ms step_avg:61.19ms
step:1739/2245 train_time:106418ms step_avg:61.19ms
step:1740/2245 train_time:106478ms step_avg:61.19ms
step:1741/2245 train_time:106542ms step_avg:61.20ms
step:1742/2245 train_time:106603ms step_avg:61.20ms
step:1743/2245 train_time:106666ms step_avg:61.20ms
step:1744/2245 train_time:106727ms step_avg:61.20ms
step:1745/2245 train_time:106790ms step_avg:61.20ms
step:1746/2245 train_time:106851ms step_avg:61.20ms
step:1747/2245 train_time:106913ms step_avg:61.20ms
step:1748/2245 train_time:106973ms step_avg:61.20ms
step:1749/2245 train_time:107036ms step_avg:61.20ms
step:1750/2245 train_time:107097ms step_avg:61.20ms
step:1750/2245 val_loss:3.3772 train_time:107160ms step_avg:61.23ms
step:1751/2245 train_time:107179ms step_avg:61.21ms
step:1752/2245 train_time:107222ms step_avg:61.20ms
step:1753/2245 train_time:107288ms step_avg:61.20ms
step:1754/2245 train_time:107349ms step_avg:61.20ms
step:1755/2245 train_time:107412ms step_avg:61.20ms
step:1756/2245 train_time:107474ms step_avg:61.20ms
step:1757/2245 train_time:107534ms step_avg:61.20ms
step:1758/2245 train_time:107594ms step_avg:61.20ms
step:1759/2245 train_time:107656ms step_avg:61.20ms
step:1760/2245 train_time:107715ms step_avg:61.20ms
step:1761/2245 train_time:107778ms step_avg:61.20ms
step:1762/2245 train_time:107837ms step_avg:61.20ms
step:1763/2245 train_time:107899ms step_avg:61.20ms
step:1764/2245 train_time:107959ms step_avg:61.20ms
step:1765/2245 train_time:108021ms step_avg:61.20ms
step:1766/2245 train_time:108081ms step_avg:61.20ms
step:1767/2245 train_time:108146ms step_avg:61.20ms
step:1768/2245 train_time:108207ms step_avg:61.20ms
step:1769/2245 train_time:108271ms step_avg:61.20ms
step:1770/2245 train_time:108332ms step_avg:61.20ms
step:1771/2245 train_time:108396ms step_avg:61.21ms
step:1772/2245 train_time:108456ms step_avg:61.21ms
step:1773/2245 train_time:108519ms step_avg:61.21ms
step:1774/2245 train_time:108579ms step_avg:61.21ms
step:1775/2245 train_time:108642ms step_avg:61.21ms
step:1776/2245 train_time:108702ms step_avg:61.21ms
step:1777/2245 train_time:108765ms step_avg:61.21ms
step:1778/2245 train_time:108825ms step_avg:61.21ms
step:1779/2245 train_time:108888ms step_avg:61.21ms
step:1780/2245 train_time:108949ms step_avg:61.21ms
step:1781/2245 train_time:109011ms step_avg:61.21ms
step:1782/2245 train_time:109073ms step_avg:61.21ms
step:1783/2245 train_time:109137ms step_avg:61.21ms
step:1784/2245 train_time:109197ms step_avg:61.21ms
step:1785/2245 train_time:109261ms step_avg:61.21ms
step:1786/2245 train_time:109321ms step_avg:61.21ms
step:1787/2245 train_time:109383ms step_avg:61.21ms
step:1788/2245 train_time:109444ms step_avg:61.21ms
step:1789/2245 train_time:109507ms step_avg:61.21ms
step:1790/2245 train_time:109571ms step_avg:61.21ms
step:1791/2245 train_time:109630ms step_avg:61.21ms
step:1792/2245 train_time:109691ms step_avg:61.21ms
step:1793/2245 train_time:109754ms step_avg:61.21ms
step:1794/2245 train_time:109814ms step_avg:61.21ms
step:1795/2245 train_time:109876ms step_avg:61.21ms
step:1796/2245 train_time:109935ms step_avg:61.21ms
step:1797/2245 train_time:109998ms step_avg:61.21ms
step:1798/2245 train_time:110059ms step_avg:61.21ms
step:1799/2245 train_time:110121ms step_avg:61.21ms
step:1800/2245 train_time:110182ms step_avg:61.21ms
step:1801/2245 train_time:110245ms step_avg:61.21ms
step:1802/2245 train_time:110305ms step_avg:61.21ms
step:1803/2245 train_time:110370ms step_avg:61.21ms
step:1804/2245 train_time:110429ms step_avg:61.21ms
step:1805/2245 train_time:110492ms step_avg:61.21ms
step:1806/2245 train_time:110552ms step_avg:61.21ms
step:1807/2245 train_time:110615ms step_avg:61.21ms
step:1808/2245 train_time:110675ms step_avg:61.21ms
step:1809/2245 train_time:110738ms step_avg:61.22ms
step:1810/2245 train_time:110799ms step_avg:61.21ms
step:1811/2245 train_time:110861ms step_avg:61.22ms
step:1812/2245 train_time:110921ms step_avg:61.21ms
step:1813/2245 train_time:110983ms step_avg:61.22ms
step:1814/2245 train_time:111043ms step_avg:61.21ms
step:1815/2245 train_time:111107ms step_avg:61.22ms
step:1816/2245 train_time:111168ms step_avg:61.22ms
step:1817/2245 train_time:111232ms step_avg:61.22ms
step:1818/2245 train_time:111293ms step_avg:61.22ms
step:1819/2245 train_time:111356ms step_avg:61.22ms
step:1820/2245 train_time:111415ms step_avg:61.22ms
step:1821/2245 train_time:111479ms step_avg:61.22ms
step:1822/2245 train_time:111539ms step_avg:61.22ms
step:1823/2245 train_time:111602ms step_avg:61.22ms
step:1824/2245 train_time:111662ms step_avg:61.22ms
step:1825/2245 train_time:111725ms step_avg:61.22ms
step:1826/2245 train_time:111785ms step_avg:61.22ms
step:1827/2245 train_time:111848ms step_avg:61.22ms
step:1828/2245 train_time:111909ms step_avg:61.22ms
step:1829/2245 train_time:111971ms step_avg:61.22ms
step:1830/2245 train_time:112032ms step_avg:61.22ms
step:1831/2245 train_time:112095ms step_avg:61.22ms
step:1832/2245 train_time:112155ms step_avg:61.22ms
step:1833/2245 train_time:112219ms step_avg:61.22ms
step:1834/2245 train_time:112279ms step_avg:61.22ms
step:1835/2245 train_time:112341ms step_avg:61.22ms
step:1836/2245 train_time:112402ms step_avg:61.22ms
step:1837/2245 train_time:112465ms step_avg:61.22ms
step:1838/2245 train_time:112525ms step_avg:61.22ms
step:1839/2245 train_time:112588ms step_avg:61.22ms
step:1840/2245 train_time:112649ms step_avg:61.22ms
step:1841/2245 train_time:112711ms step_avg:61.22ms
step:1842/2245 train_time:112772ms step_avg:61.22ms
step:1843/2245 train_time:112835ms step_avg:61.22ms
step:1844/2245 train_time:112895ms step_avg:61.22ms
step:1845/2245 train_time:112957ms step_avg:61.22ms
step:1846/2245 train_time:113018ms step_avg:61.22ms
step:1847/2245 train_time:113080ms step_avg:61.22ms
step:1848/2245 train_time:113141ms step_avg:61.22ms
step:1849/2245 train_time:113203ms step_avg:61.22ms
step:1850/2245 train_time:113264ms step_avg:61.22ms
step:1851/2245 train_time:113327ms step_avg:61.22ms
step:1852/2245 train_time:113387ms step_avg:61.22ms
step:1853/2245 train_time:113450ms step_avg:61.23ms
step:1854/2245 train_time:113511ms step_avg:61.22ms
step:1855/2245 train_time:113573ms step_avg:61.23ms
step:1856/2245 train_time:113633ms step_avg:61.22ms
step:1857/2245 train_time:113696ms step_avg:61.23ms
step:1858/2245 train_time:113756ms step_avg:61.23ms
step:1859/2245 train_time:113819ms step_avg:61.23ms
step:1860/2245 train_time:113880ms step_avg:61.23ms
step:1861/2245 train_time:113942ms step_avg:61.23ms
step:1862/2245 train_time:114003ms step_avg:61.23ms
step:1863/2245 train_time:114066ms step_avg:61.23ms
step:1864/2245 train_time:114127ms step_avg:61.23ms
step:1865/2245 train_time:114190ms step_avg:61.23ms
step:1866/2245 train_time:114251ms step_avg:61.23ms
step:1867/2245 train_time:114313ms step_avg:61.23ms
step:1868/2245 train_time:114374ms step_avg:61.23ms
step:1869/2245 train_time:114436ms step_avg:61.23ms
step:1870/2245 train_time:114496ms step_avg:61.23ms
step:1871/2245 train_time:114559ms step_avg:61.23ms
step:1872/2245 train_time:114619ms step_avg:61.23ms
step:1873/2245 train_time:114681ms step_avg:61.23ms
step:1874/2245 train_time:114742ms step_avg:61.23ms
step:1875/2245 train_time:114804ms step_avg:61.23ms
step:1876/2245 train_time:114864ms step_avg:61.23ms
step:1877/2245 train_time:114927ms step_avg:61.23ms
step:1878/2245 train_time:114988ms step_avg:61.23ms
step:1879/2245 train_time:115051ms step_avg:61.23ms
step:1880/2245 train_time:115112ms step_avg:61.23ms
step:1881/2245 train_time:115176ms step_avg:61.23ms
step:1882/2245 train_time:115236ms step_avg:61.23ms
step:1883/2245 train_time:115298ms step_avg:61.23ms
step:1884/2245 train_time:115359ms step_avg:61.23ms
step:1885/2245 train_time:115422ms step_avg:61.23ms
step:1886/2245 train_time:115481ms step_avg:61.23ms
step:1887/2245 train_time:115544ms step_avg:61.23ms
step:1888/2245 train_time:115604ms step_avg:61.23ms
step:1889/2245 train_time:115667ms step_avg:61.23ms
step:1890/2245 train_time:115728ms step_avg:61.23ms
step:1891/2245 train_time:115790ms step_avg:61.23ms
step:1892/2245 train_time:115851ms step_avg:61.23ms
step:1893/2245 train_time:115914ms step_avg:61.23ms
step:1894/2245 train_time:115974ms step_avg:61.23ms
step:1895/2245 train_time:116036ms step_avg:61.23ms
step:1896/2245 train_time:116097ms step_avg:61.23ms
step:1897/2245 train_time:116159ms step_avg:61.23ms
step:1898/2245 train_time:116219ms step_avg:61.23ms
step:1899/2245 train_time:116281ms step_avg:61.23ms
step:1900/2245 train_time:116342ms step_avg:61.23ms
step:1901/2245 train_time:116405ms step_avg:61.23ms
step:1902/2245 train_time:116466ms step_avg:61.23ms
step:1903/2245 train_time:116528ms step_avg:61.23ms
step:1904/2245 train_time:116589ms step_avg:61.23ms
step:1905/2245 train_time:116651ms step_avg:61.23ms
step:1906/2245 train_time:116711ms step_avg:61.23ms
step:1907/2245 train_time:116773ms step_avg:61.23ms
step:1908/2245 train_time:116834ms step_avg:61.23ms
step:1909/2245 train_time:116896ms step_avg:61.23ms
step:1910/2245 train_time:116956ms step_avg:61.23ms
step:1911/2245 train_time:117019ms step_avg:61.23ms
step:1912/2245 train_time:117079ms step_avg:61.23ms
step:1913/2245 train_time:117142ms step_avg:61.23ms
step:1914/2245 train_time:117203ms step_avg:61.23ms
step:1915/2245 train_time:117266ms step_avg:61.24ms
step:1916/2245 train_time:117326ms step_avg:61.23ms
step:1917/2245 train_time:117389ms step_avg:61.24ms
step:1918/2245 train_time:117450ms step_avg:61.24ms
step:1919/2245 train_time:117512ms step_avg:61.24ms
step:1920/2245 train_time:117573ms step_avg:61.24ms
step:1921/2245 train_time:117636ms step_avg:61.24ms
step:1922/2245 train_time:117696ms step_avg:61.24ms
step:1923/2245 train_time:117759ms step_avg:61.24ms
step:1924/2245 train_time:117819ms step_avg:61.24ms
step:1925/2245 train_time:117882ms step_avg:61.24ms
step:1926/2245 train_time:117941ms step_avg:61.24ms
step:1927/2245 train_time:118005ms step_avg:61.24ms
step:1928/2245 train_time:118065ms step_avg:61.24ms
step:1929/2245 train_time:118129ms step_avg:61.24ms
step:1930/2245 train_time:118189ms step_avg:61.24ms
step:1931/2245 train_time:118252ms step_avg:61.24ms
step:1932/2245 train_time:118312ms step_avg:61.24ms
step:1933/2245 train_time:118375ms step_avg:61.24ms
step:1934/2245 train_time:118435ms step_avg:61.24ms
step:1935/2245 train_time:118498ms step_avg:61.24ms
step:1936/2245 train_time:118558ms step_avg:61.24ms
step:1937/2245 train_time:118620ms step_avg:61.24ms
step:1938/2245 train_time:118681ms step_avg:61.24ms
step:1939/2245 train_time:118744ms step_avg:61.24ms
step:1940/2245 train_time:118804ms step_avg:61.24ms
step:1941/2245 train_time:118867ms step_avg:61.24ms
step:1942/2245 train_time:118927ms step_avg:61.24ms
step:1943/2245 train_time:118990ms step_avg:61.24ms
step:1944/2245 train_time:119051ms step_avg:61.24ms
step:1945/2245 train_time:119114ms step_avg:61.24ms
step:1946/2245 train_time:119175ms step_avg:61.24ms
step:1947/2245 train_time:119237ms step_avg:61.24ms
step:1948/2245 train_time:119297ms step_avg:61.24ms
step:1949/2245 train_time:119360ms step_avg:61.24ms
step:1950/2245 train_time:119420ms step_avg:61.24ms
step:1951/2245 train_time:119482ms step_avg:61.24ms
step:1952/2245 train_time:119542ms step_avg:61.24ms
step:1953/2245 train_time:119605ms step_avg:61.24ms
step:1954/2245 train_time:119665ms step_avg:61.24ms
step:1955/2245 train_time:119728ms step_avg:61.24ms
step:1956/2245 train_time:119789ms step_avg:61.24ms
step:1957/2245 train_time:119852ms step_avg:61.24ms
step:1958/2245 train_time:119912ms step_avg:61.24ms
step:1959/2245 train_time:119975ms step_avg:61.24ms
step:1960/2245 train_time:120035ms step_avg:61.24ms
step:1961/2245 train_time:120098ms step_avg:61.24ms
step:1962/2245 train_time:120158ms step_avg:61.24ms
step:1963/2245 train_time:120220ms step_avg:61.24ms
step:1964/2245 train_time:120281ms step_avg:61.24ms
step:1965/2245 train_time:120343ms step_avg:61.24ms
step:1966/2245 train_time:120404ms step_avg:61.24ms
step:1967/2245 train_time:120467ms step_avg:61.24ms
step:1968/2245 train_time:120528ms step_avg:61.24ms
step:1969/2245 train_time:120591ms step_avg:61.24ms
step:1970/2245 train_time:120652ms step_avg:61.24ms
step:1971/2245 train_time:120714ms step_avg:61.25ms
step:1972/2245 train_time:120774ms step_avg:61.24ms
step:1973/2245 train_time:120837ms step_avg:61.25ms
step:1974/2245 train_time:120898ms step_avg:61.24ms
step:1975/2245 train_time:120960ms step_avg:61.25ms
step:1976/2245 train_time:121021ms step_avg:61.25ms
step:1977/2245 train_time:121083ms step_avg:61.25ms
step:1978/2245 train_time:121144ms step_avg:61.25ms
step:1979/2245 train_time:121207ms step_avg:61.25ms
step:1980/2245 train_time:121268ms step_avg:61.25ms
step:1981/2245 train_time:121331ms step_avg:61.25ms
step:1982/2245 train_time:121393ms step_avg:61.25ms
step:1983/2245 train_time:121455ms step_avg:61.25ms
step:1984/2245 train_time:121515ms step_avg:61.25ms
step:1985/2245 train_time:121578ms step_avg:61.25ms
step:1986/2245 train_time:121638ms step_avg:61.25ms
step:1987/2245 train_time:121701ms step_avg:61.25ms
step:1988/2245 train_time:121761ms step_avg:61.25ms
step:1989/2245 train_time:121824ms step_avg:61.25ms
step:1990/2245 train_time:121884ms step_avg:61.25ms
step:1991/2245 train_time:121947ms step_avg:61.25ms
step:1992/2245 train_time:122008ms step_avg:61.25ms
step:1993/2245 train_time:122071ms step_avg:61.25ms
step:1994/2245 train_time:122131ms step_avg:61.25ms
step:1995/2245 train_time:122193ms step_avg:61.25ms
step:1996/2245 train_time:122254ms step_avg:61.25ms
step:1997/2245 train_time:122317ms step_avg:61.25ms
step:1998/2245 train_time:122377ms step_avg:61.25ms
step:1999/2245 train_time:122440ms step_avg:61.25ms
step:2000/2245 train_time:122500ms step_avg:61.25ms
step:2000/2245 val_loss:3.3229 train_time:122564ms step_avg:61.28ms
step:2001/2245 train_time:122583ms step_avg:61.26ms
step:2002/2245 train_time:122627ms step_avg:61.25ms
step:2003/2245 train_time:122692ms step_avg:61.25ms
step:2004/2245 train_time:122753ms step_avg:61.25ms
step:2005/2245 train_time:122816ms step_avg:61.25ms
step:2006/2245 train_time:122876ms step_avg:61.25ms
step:2007/2245 train_time:122938ms step_avg:61.25ms
step:2008/2245 train_time:122998ms step_avg:61.25ms
step:2009/2245 train_time:123060ms step_avg:61.25ms
step:2010/2245 train_time:123121ms step_avg:61.25ms
step:2011/2245 train_time:123182ms step_avg:61.25ms
step:2012/2245 train_time:123242ms step_avg:61.25ms
step:2013/2245 train_time:123304ms step_avg:61.25ms
step:2014/2245 train_time:123364ms step_avg:61.25ms
step:2015/2245 train_time:123426ms step_avg:61.25ms
step:2016/2245 train_time:123487ms step_avg:61.25ms
step:2017/2245 train_time:123552ms step_avg:61.26ms
step:2018/2245 train_time:123613ms step_avg:61.26ms
step:2019/2245 train_time:123677ms step_avg:61.26ms
step:2020/2245 train_time:123739ms step_avg:61.26ms
step:2021/2245 train_time:123803ms step_avg:61.26ms
step:2022/2245 train_time:123863ms step_avg:61.26ms
step:2023/2245 train_time:123925ms step_avg:61.26ms
step:2024/2245 train_time:123985ms step_avg:61.26ms
step:2025/2245 train_time:124048ms step_avg:61.26ms
step:2026/2245 train_time:124108ms step_avg:61.26ms
step:2027/2245 train_time:124170ms step_avg:61.26ms
step:2028/2245 train_time:124230ms step_avg:61.26ms
step:2029/2245 train_time:124292ms step_avg:61.26ms
step:2030/2245 train_time:124351ms step_avg:61.26ms
step:2031/2245 train_time:124414ms step_avg:61.26ms
step:2032/2245 train_time:124475ms step_avg:61.26ms
step:2033/2245 train_time:124538ms step_avg:61.26ms
step:2034/2245 train_time:124600ms step_avg:61.26ms
step:2035/2245 train_time:124663ms step_avg:61.26ms
step:2036/2245 train_time:124725ms step_avg:61.26ms
step:2037/2245 train_time:124788ms step_avg:61.26ms
step:2038/2245 train_time:124847ms step_avg:61.26ms
step:2039/2245 train_time:124910ms step_avg:61.26ms
step:2040/2245 train_time:124970ms step_avg:61.26ms
step:2041/2245 train_time:125034ms step_avg:61.26ms
step:2042/2245 train_time:125094ms step_avg:61.26ms
step:2043/2245 train_time:125157ms step_avg:61.26ms
step:2044/2245 train_time:125218ms step_avg:61.26ms
step:2045/2245 train_time:125281ms step_avg:61.26ms
step:2046/2245 train_time:125341ms step_avg:61.26ms
step:2047/2245 train_time:125404ms step_avg:61.26ms
step:2048/2245 train_time:125464ms step_avg:61.26ms
step:2049/2245 train_time:125528ms step_avg:61.26ms
step:2050/2245 train_time:125589ms step_avg:61.26ms
step:2051/2245 train_time:125652ms step_avg:61.26ms
step:2052/2245 train_time:125713ms step_avg:61.26ms
step:2053/2245 train_time:125776ms step_avg:61.26ms
step:2054/2245 train_time:125837ms step_avg:61.26ms
step:2055/2245 train_time:125900ms step_avg:61.27ms
step:2056/2245 train_time:125961ms step_avg:61.26ms
step:2057/2245 train_time:126023ms step_avg:61.27ms
step:2058/2245 train_time:126083ms step_avg:61.26ms
step:2059/2245 train_time:126145ms step_avg:61.27ms
step:2060/2245 train_time:126205ms step_avg:61.26ms
step:2061/2245 train_time:126268ms step_avg:61.27ms
step:2062/2245 train_time:126328ms step_avg:61.26ms
step:2063/2245 train_time:126391ms step_avg:61.27ms
step:2064/2245 train_time:126451ms step_avg:61.27ms
step:2065/2245 train_time:126514ms step_avg:61.27ms
step:2066/2245 train_time:126576ms step_avg:61.27ms
step:2067/2245 train_time:126639ms step_avg:61.27ms
step:2068/2245 train_time:126700ms step_avg:61.27ms
step:2069/2245 train_time:126763ms step_avg:61.27ms
step:2070/2245 train_time:126824ms step_avg:61.27ms
step:2071/2245 train_time:126887ms step_avg:61.27ms
step:2072/2245 train_time:126947ms step_avg:61.27ms
step:2073/2245 train_time:127011ms step_avg:61.27ms
step:2074/2245 train_time:127071ms step_avg:61.27ms
step:2075/2245 train_time:127134ms step_avg:61.27ms
step:2076/2245 train_time:127195ms step_avg:61.27ms
step:2077/2245 train_time:127257ms step_avg:61.27ms
step:2078/2245 train_time:127319ms step_avg:61.27ms
step:2079/2245 train_time:127381ms step_avg:61.27ms
step:2080/2245 train_time:127441ms step_avg:61.27ms
step:2081/2245 train_time:127504ms step_avg:61.27ms
step:2082/2245 train_time:127563ms step_avg:61.27ms
step:2083/2245 train_time:127626ms step_avg:61.27ms
step:2084/2245 train_time:127686ms step_avg:61.27ms
step:2085/2245 train_time:127749ms step_avg:61.27ms
step:2086/2245 train_time:127809ms step_avg:61.27ms
step:2087/2245 train_time:127872ms step_avg:61.27ms
step:2088/2245 train_time:127933ms step_avg:61.27ms
step:2089/2245 train_time:127997ms step_avg:61.27ms
step:2090/2245 train_time:128057ms step_avg:61.27ms
step:2091/2245 train_time:128120ms step_avg:61.27ms
step:2092/2245 train_time:128180ms step_avg:61.27ms
step:2093/2245 train_time:128243ms step_avg:61.27ms
step:2094/2245 train_time:128304ms step_avg:61.27ms
step:2095/2245 train_time:128366ms step_avg:61.27ms
step:2096/2245 train_time:128426ms step_avg:61.27ms
step:2097/2245 train_time:128488ms step_avg:61.27ms
step:2098/2245 train_time:128548ms step_avg:61.27ms
step:2099/2245 train_time:128611ms step_avg:61.27ms
step:2100/2245 train_time:128672ms step_avg:61.27ms
step:2101/2245 train_time:128735ms step_avg:61.27ms
step:2102/2245 train_time:128797ms step_avg:61.27ms
step:2103/2245 train_time:128860ms step_avg:61.27ms
step:2104/2245 train_time:128921ms step_avg:61.27ms
step:2105/2245 train_time:128984ms step_avg:61.28ms
step:2106/2245 train_time:129044ms step_avg:61.27ms
step:2107/2245 train_time:129107ms step_avg:61.28ms
step:2108/2245 train_time:129167ms step_avg:61.27ms
step:2109/2245 train_time:129229ms step_avg:61.28ms
step:2110/2245 train_time:129289ms step_avg:61.27ms
step:2111/2245 train_time:129352ms step_avg:61.28ms
step:2112/2245 train_time:129413ms step_avg:61.27ms
step:2113/2245 train_time:129476ms step_avg:61.28ms
step:2114/2245 train_time:129537ms step_avg:61.28ms
step:2115/2245 train_time:129599ms step_avg:61.28ms
step:2116/2245 train_time:129659ms step_avg:61.28ms
step:2117/2245 train_time:129722ms step_avg:61.28ms
step:2118/2245 train_time:129782ms step_avg:61.28ms
step:2119/2245 train_time:129845ms step_avg:61.28ms
step:2120/2245 train_time:129905ms step_avg:61.28ms
step:2121/2245 train_time:129968ms step_avg:61.28ms
step:2122/2245 train_time:130029ms step_avg:61.28ms
step:2123/2245 train_time:130091ms step_avg:61.28ms
step:2124/2245 train_time:130152ms step_avg:61.28ms
step:2125/2245 train_time:130215ms step_avg:61.28ms
step:2126/2245 train_time:130275ms step_avg:61.28ms
step:2127/2245 train_time:130337ms step_avg:61.28ms
step:2128/2245 train_time:130397ms step_avg:61.28ms
step:2129/2245 train_time:130460ms step_avg:61.28ms
step:2130/2245 train_time:130521ms step_avg:61.28ms
step:2131/2245 train_time:130583ms step_avg:61.28ms
step:2132/2245 train_time:130644ms step_avg:61.28ms
step:2133/2245 train_time:130707ms step_avg:61.28ms
step:2134/2245 train_time:130766ms step_avg:61.28ms
step:2135/2245 train_time:130829ms step_avg:61.28ms
step:2136/2245 train_time:130889ms step_avg:61.28ms
step:2137/2245 train_time:130952ms step_avg:61.28ms
step:2138/2245 train_time:131012ms step_avg:61.28ms
step:2139/2245 train_time:131075ms step_avg:61.28ms
step:2140/2245 train_time:131136ms step_avg:61.28ms
step:2141/2245 train_time:131199ms step_avg:61.28ms
step:2142/2245 train_time:131260ms step_avg:61.28ms
step:2143/2245 train_time:131323ms step_avg:61.28ms
step:2144/2245 train_time:131383ms step_avg:61.28ms
step:2145/2245 train_time:131446ms step_avg:61.28ms
step:2146/2245 train_time:131506ms step_avg:61.28ms
step:2147/2245 train_time:131568ms step_avg:61.28ms
step:2148/2245 train_time:131629ms step_avg:61.28ms
step:2149/2245 train_time:131691ms step_avg:61.28ms
step:2150/2245 train_time:131751ms step_avg:61.28ms
step:2151/2245 train_time:131814ms step_avg:61.28ms
step:2152/2245 train_time:131875ms step_avg:61.28ms
step:2153/2245 train_time:131939ms step_avg:61.28ms
step:2154/2245 train_time:131999ms step_avg:61.28ms
step:2155/2245 train_time:132062ms step_avg:61.28ms
step:2156/2245 train_time:132123ms step_avg:61.28ms
step:2157/2245 train_time:132185ms step_avg:61.28ms
step:2158/2245 train_time:132246ms step_avg:61.28ms
step:2159/2245 train_time:132309ms step_avg:61.28ms
step:2160/2245 train_time:132369ms step_avg:61.28ms
step:2161/2245 train_time:132432ms step_avg:61.28ms
step:2162/2245 train_time:132492ms step_avg:61.28ms
step:2163/2245 train_time:132555ms step_avg:61.28ms
step:2164/2245 train_time:132616ms step_avg:61.28ms
step:2165/2245 train_time:132679ms step_avg:61.28ms
step:2166/2245 train_time:132739ms step_avg:61.28ms
step:2167/2245 train_time:132802ms step_avg:61.28ms
step:2168/2245 train_time:132862ms step_avg:61.28ms
step:2169/2245 train_time:132925ms step_avg:61.28ms
step:2170/2245 train_time:132985ms step_avg:61.28ms
step:2171/2245 train_time:133048ms step_avg:61.28ms
step:2172/2245 train_time:133108ms step_avg:61.28ms
step:2173/2245 train_time:133171ms step_avg:61.28ms
step:2174/2245 train_time:133231ms step_avg:61.28ms
step:2175/2245 train_time:133294ms step_avg:61.28ms
step:2176/2245 train_time:133354ms step_avg:61.28ms
step:2177/2245 train_time:133417ms step_avg:61.28ms
step:2178/2245 train_time:133478ms step_avg:61.28ms
step:2179/2245 train_time:133541ms step_avg:61.29ms
step:2180/2245 train_time:133601ms step_avg:61.28ms
step:2181/2245 train_time:133664ms step_avg:61.29ms
step:2182/2245 train_time:133725ms step_avg:61.29ms
step:2183/2245 train_time:133787ms step_avg:61.29ms
step:2184/2245 train_time:133847ms step_avg:61.29ms
step:2185/2245 train_time:133909ms step_avg:61.29ms
step:2186/2245 train_time:133969ms step_avg:61.29ms
step:2187/2245 train_time:134032ms step_avg:61.29ms
step:2188/2245 train_time:134093ms step_avg:61.29ms
step:2189/2245 train_time:134156ms step_avg:61.29ms
step:2190/2245 train_time:134217ms step_avg:61.29ms
step:2191/2245 train_time:134280ms step_avg:61.29ms
step:2192/2245 train_time:134340ms step_avg:61.29ms
step:2193/2245 train_time:134402ms step_avg:61.29ms
step:2194/2245 train_time:134463ms step_avg:61.29ms
step:2195/2245 train_time:134525ms step_avg:61.29ms
step:2196/2245 train_time:134585ms step_avg:61.29ms
step:2197/2245 train_time:134648ms step_avg:61.29ms
step:2198/2245 train_time:134708ms step_avg:61.29ms
step:2199/2245 train_time:134771ms step_avg:61.29ms
step:2200/2245 train_time:134832ms step_avg:61.29ms
step:2201/2245 train_time:134895ms step_avg:61.29ms
step:2202/2245 train_time:134955ms step_avg:61.29ms
step:2203/2245 train_time:135018ms step_avg:61.29ms
step:2204/2245 train_time:135079ms step_avg:61.29ms
step:2205/2245 train_time:135142ms step_avg:61.29ms
step:2206/2245 train_time:135203ms step_avg:61.29ms
step:2207/2245 train_time:135266ms step_avg:61.29ms
step:2208/2245 train_time:135326ms step_avg:61.29ms
step:2209/2245 train_time:135389ms step_avg:61.29ms
step:2210/2245 train_time:135450ms step_avg:61.29ms
step:2211/2245 train_time:135513ms step_avg:61.29ms
step:2212/2245 train_time:135574ms step_avg:61.29ms
step:2213/2245 train_time:135637ms step_avg:61.29ms
step:2214/2245 train_time:135698ms step_avg:61.29ms
step:2215/2245 train_time:135762ms step_avg:61.29ms
step:2216/2245 train_time:135823ms step_avg:61.29ms
step:2217/2245 train_time:135885ms step_avg:61.29ms
step:2218/2245 train_time:135946ms step_avg:61.29ms
step:2219/2245 train_time:136008ms step_avg:61.29ms
step:2220/2245 train_time:136068ms step_avg:61.29ms
step:2221/2245 train_time:136132ms step_avg:61.29ms
step:2222/2245 train_time:136193ms step_avg:61.29ms
step:2223/2245 train_time:136257ms step_avg:61.29ms
step:2224/2245 train_time:136318ms step_avg:61.29ms
step:2225/2245 train_time:136381ms step_avg:61.29ms
step:2226/2245 train_time:136441ms step_avg:61.29ms
step:2227/2245 train_time:136503ms step_avg:61.29ms
step:2228/2245 train_time:136564ms step_avg:61.29ms
step:2229/2245 train_time:136626ms step_avg:61.29ms
step:2230/2245 train_time:136686ms step_avg:61.29ms
step:2231/2245 train_time:136749ms step_avg:61.29ms
step:2232/2245 train_time:136809ms step_avg:61.29ms
step:2233/2245 train_time:136872ms step_avg:61.30ms
step:2234/2245 train_time:136933ms step_avg:61.29ms
step:2235/2245 train_time:136996ms step_avg:61.30ms
step:2236/2245 train_time:137057ms step_avg:61.30ms
step:2237/2245 train_time:137120ms step_avg:61.30ms
step:2238/2245 train_time:137182ms step_avg:61.30ms
step:2239/2245 train_time:137244ms step_avg:61.30ms
step:2240/2245 train_time:137304ms step_avg:61.30ms
step:2241/2245 train_time:137368ms step_avg:61.30ms
step:2242/2245 train_time:137429ms step_avg:61.30ms
step:2243/2245 train_time:137492ms step_avg:61.30ms
step:2244/2245 train_time:137553ms step_avg:61.30ms
step:2245/2245 train_time:137617ms step_avg:61.30ms
step:2245/2245 val_loss:3.2772 train_time:137677ms step_avg:61.33ms
peak memory allocated: 29249 MiB reserved: 50528 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Nov 10 22:05:46 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            132W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   41C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   34C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   40C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2245 train_time:118ms step_avg:118.22ms
step:2/2245 train_time:140ms step_avg:70.03ms
step:3/2245 train_time:178ms step_avg:59.27ms
step:4/2245 train_time:234ms step_avg:58.52ms
step:5/2245 train_time:293ms step_avg:58.65ms
step:6/2245 train_time:352ms step_avg:58.63ms
step:7/2245 train_time:412ms step_avg:58.92ms
step:8/2245 train_time:471ms step_avg:58.88ms
step:9/2245 train_time:532ms step_avg:59.12ms
step:10/2245 train_time:591ms step_avg:59.07ms
step:11/2245 train_time:651ms step_avg:59.22ms
step:12/2245 train_time:710ms step_avg:59.17ms
step:13/2245 train_time:771ms step_avg:59.32ms
step:14/2245 train_time:830ms step_avg:59.27ms
step:15/2245 train_time:891ms step_avg:59.41ms
step:16/2245 train_time:950ms step_avg:59.39ms
step:17/2245 train_time:1013ms step_avg:59.58ms
step:18/2245 train_time:1076ms step_avg:59.77ms
step:19/2245 train_time:1140ms step_avg:59.99ms
step:20/2245 train_time:1200ms step_avg:59.98ms
step:21/2245 train_time:1262ms step_avg:60.08ms
step:22/2245 train_time:1321ms step_avg:60.05ms
step:23/2245 train_time:1383ms step_avg:60.14ms
step:24/2245 train_time:1442ms step_avg:60.10ms
step:25/2245 train_time:1504ms step_avg:60.16ms
step:26/2245 train_time:1564ms step_avg:60.14ms
step:27/2245 train_time:1625ms step_avg:60.19ms
step:28/2245 train_time:1684ms step_avg:60.16ms
step:29/2245 train_time:1746ms step_avg:60.22ms
step:30/2245 train_time:1806ms step_avg:60.19ms
step:31/2245 train_time:1868ms step_avg:60.24ms
step:32/2245 train_time:1927ms step_avg:60.23ms
step:33/2245 train_time:1991ms step_avg:60.33ms
step:34/2245 train_time:2052ms step_avg:60.35ms
step:35/2245 train_time:2115ms step_avg:60.42ms
step:36/2245 train_time:2174ms step_avg:60.40ms
step:37/2245 train_time:2236ms step_avg:60.44ms
step:38/2245 train_time:2295ms step_avg:60.40ms
step:39/2245 train_time:2357ms step_avg:60.44ms
step:40/2245 train_time:2416ms step_avg:60.40ms
step:41/2245 train_time:2478ms step_avg:60.43ms
step:42/2245 train_time:2537ms step_avg:60.40ms
step:43/2245 train_time:2598ms step_avg:60.43ms
step:44/2245 train_time:2658ms step_avg:60.41ms
step:45/2245 train_time:2720ms step_avg:60.44ms
step:46/2245 train_time:2779ms step_avg:60.41ms
step:47/2245 train_time:2841ms step_avg:60.45ms
step:48/2245 train_time:2901ms step_avg:60.43ms
step:49/2245 train_time:2964ms step_avg:60.48ms
step:50/2245 train_time:3024ms step_avg:60.48ms
step:51/2245 train_time:3087ms step_avg:60.53ms
step:52/2245 train_time:3146ms step_avg:60.50ms
step:53/2245 train_time:3208ms step_avg:60.52ms
step:54/2245 train_time:3268ms step_avg:60.52ms
step:55/2245 train_time:3330ms step_avg:60.55ms
step:56/2245 train_time:3390ms step_avg:60.54ms
step:57/2245 train_time:3452ms step_avg:60.56ms
step:58/2245 train_time:3511ms step_avg:60.54ms
step:59/2245 train_time:3573ms step_avg:60.57ms
step:60/2245 train_time:3632ms step_avg:60.54ms
step:61/2245 train_time:3694ms step_avg:60.56ms
step:62/2245 train_time:3754ms step_avg:60.54ms
step:63/2245 train_time:3815ms step_avg:60.56ms
step:64/2245 train_time:3874ms step_avg:60.53ms
step:65/2245 train_time:3936ms step_avg:60.55ms
step:66/2245 train_time:3995ms step_avg:60.53ms
step:67/2245 train_time:4058ms step_avg:60.57ms
step:68/2245 train_time:4117ms step_avg:60.54ms
step:69/2245 train_time:4180ms step_avg:60.58ms
step:70/2245 train_time:4239ms step_avg:60.56ms
step:71/2245 train_time:4302ms step_avg:60.59ms
step:72/2245 train_time:4361ms step_avg:60.57ms
step:73/2245 train_time:4423ms step_avg:60.59ms
step:74/2245 train_time:4483ms step_avg:60.58ms
step:75/2245 train_time:4545ms step_avg:60.60ms
step:76/2245 train_time:4604ms step_avg:60.58ms
step:77/2245 train_time:4666ms step_avg:60.60ms
step:78/2245 train_time:4726ms step_avg:60.58ms
step:79/2245 train_time:4787ms step_avg:60.60ms
step:80/2245 train_time:4847ms step_avg:60.58ms
step:81/2245 train_time:4909ms step_avg:60.60ms
step:82/2245 train_time:4969ms step_avg:60.59ms
step:83/2245 train_time:5031ms step_avg:60.61ms
step:84/2245 train_time:5090ms step_avg:60.60ms
step:85/2245 train_time:5153ms step_avg:60.62ms
step:86/2245 train_time:5212ms step_avg:60.60ms
step:87/2245 train_time:5274ms step_avg:60.62ms
step:88/2245 train_time:5332ms step_avg:60.59ms
step:89/2245 train_time:5394ms step_avg:60.60ms
step:90/2245 train_time:5452ms step_avg:60.58ms
step:91/2245 train_time:5514ms step_avg:60.60ms
step:92/2245 train_time:5573ms step_avg:60.58ms
step:93/2245 train_time:5634ms step_avg:60.59ms
step:94/2245 train_time:5693ms step_avg:60.56ms
step:95/2245 train_time:5755ms step_avg:60.58ms
step:96/2245 train_time:5814ms step_avg:60.56ms
step:97/2245 train_time:5875ms step_avg:60.57ms
step:98/2245 train_time:5934ms step_avg:60.55ms
step:99/2245 train_time:5996ms step_avg:60.57ms
step:100/2245 train_time:6056ms step_avg:60.56ms
step:101/2245 train_time:6117ms step_avg:60.57ms
step:102/2245 train_time:6176ms step_avg:60.55ms
step:103/2245 train_time:6238ms step_avg:60.56ms
step:104/2245 train_time:6297ms step_avg:60.55ms
step:105/2245 train_time:6360ms step_avg:60.57ms
step:106/2245 train_time:6419ms step_avg:60.55ms
step:107/2245 train_time:6480ms step_avg:60.56ms
step:108/2245 train_time:6539ms step_avg:60.55ms
step:109/2245 train_time:6601ms step_avg:60.56ms
step:110/2245 train_time:6661ms step_avg:60.55ms
step:111/2245 train_time:6722ms step_avg:60.56ms
step:112/2245 train_time:6781ms step_avg:60.54ms
step:113/2245 train_time:6843ms step_avg:60.56ms
step:114/2245 train_time:6902ms step_avg:60.54ms
step:115/2245 train_time:6964ms step_avg:60.55ms
step:116/2245 train_time:7023ms step_avg:60.54ms
step:117/2245 train_time:7085ms step_avg:60.56ms
step:118/2245 train_time:7145ms step_avg:60.55ms
step:119/2245 train_time:7207ms step_avg:60.56ms
step:120/2245 train_time:7266ms step_avg:60.55ms
step:121/2245 train_time:7328ms step_avg:60.56ms
step:122/2245 train_time:7388ms step_avg:60.56ms
step:123/2245 train_time:7449ms step_avg:60.56ms
step:124/2245 train_time:7509ms step_avg:60.56ms
step:125/2245 train_time:7571ms step_avg:60.57ms
step:126/2245 train_time:7631ms step_avg:60.56ms
step:127/2245 train_time:7693ms step_avg:60.57ms
step:128/2245 train_time:7751ms step_avg:60.56ms
step:129/2245 train_time:7813ms step_avg:60.56ms
step:130/2245 train_time:7871ms step_avg:60.55ms
step:131/2245 train_time:7933ms step_avg:60.55ms
step:132/2245 train_time:7991ms step_avg:60.54ms
step:133/2245 train_time:8053ms step_avg:60.55ms
step:134/2245 train_time:8112ms step_avg:60.54ms
step:135/2245 train_time:8174ms step_avg:60.55ms
step:136/2245 train_time:8233ms step_avg:60.54ms
step:137/2245 train_time:8294ms step_avg:60.54ms
step:138/2245 train_time:8353ms step_avg:60.53ms
step:139/2245 train_time:8415ms step_avg:60.54ms
step:140/2245 train_time:8474ms step_avg:60.53ms
step:141/2245 train_time:8535ms step_avg:60.53ms
step:142/2245 train_time:8593ms step_avg:60.52ms
step:143/2245 train_time:8655ms step_avg:60.52ms
step:144/2245 train_time:8713ms step_avg:60.51ms
step:145/2245 train_time:8775ms step_avg:60.52ms
step:146/2245 train_time:8834ms step_avg:60.50ms
step:147/2245 train_time:8895ms step_avg:60.51ms
step:148/2245 train_time:8953ms step_avg:60.49ms
step:149/2245 train_time:9015ms step_avg:60.50ms
step:150/2245 train_time:9073ms step_avg:60.49ms
step:151/2245 train_time:9134ms step_avg:60.49ms
step:152/2245 train_time:9192ms step_avg:60.48ms
step:153/2245 train_time:9254ms step_avg:60.48ms
step:154/2245 train_time:9312ms step_avg:60.47ms
step:155/2245 train_time:9374ms step_avg:60.48ms
step:156/2245 train_time:9433ms step_avg:60.46ms
step:157/2245 train_time:9494ms step_avg:60.47ms
step:158/2245 train_time:9552ms step_avg:60.46ms
step:159/2245 train_time:9614ms step_avg:60.46ms
step:160/2245 train_time:9673ms step_avg:60.45ms
step:161/2245 train_time:9733ms step_avg:60.46ms
step:162/2245 train_time:9792ms step_avg:60.44ms
step:163/2245 train_time:9853ms step_avg:60.45ms
step:164/2245 train_time:9912ms step_avg:60.44ms
step:165/2245 train_time:9973ms step_avg:60.45ms
step:166/2245 train_time:10032ms step_avg:60.43ms
step:167/2245 train_time:10094ms step_avg:60.44ms
step:168/2245 train_time:10152ms step_avg:60.43ms
step:169/2245 train_time:10214ms step_avg:60.44ms
step:170/2245 train_time:10274ms step_avg:60.43ms
step:171/2245 train_time:10335ms step_avg:60.44ms
step:172/2245 train_time:10393ms step_avg:60.43ms
step:173/2245 train_time:10455ms step_avg:60.43ms
step:174/2245 train_time:10514ms step_avg:60.42ms
step:175/2245 train_time:10576ms step_avg:60.43ms
step:176/2245 train_time:10634ms step_avg:60.42ms
step:177/2245 train_time:10695ms step_avg:60.43ms
step:178/2245 train_time:10754ms step_avg:60.42ms
step:179/2245 train_time:10815ms step_avg:60.42ms
step:180/2245 train_time:10874ms step_avg:60.41ms
step:181/2245 train_time:10935ms step_avg:60.42ms
step:182/2245 train_time:10994ms step_avg:60.41ms
step:183/2245 train_time:11055ms step_avg:60.41ms
step:184/2245 train_time:11114ms step_avg:60.40ms
step:185/2245 train_time:11176ms step_avg:60.41ms
step:186/2245 train_time:11234ms step_avg:60.40ms
step:187/2245 train_time:11296ms step_avg:60.41ms
step:188/2245 train_time:11355ms step_avg:60.40ms
step:189/2245 train_time:11416ms step_avg:60.40ms
step:190/2245 train_time:11475ms step_avg:60.39ms
step:191/2245 train_time:11536ms step_avg:60.40ms
step:192/2245 train_time:11595ms step_avg:60.39ms
step:193/2245 train_time:11657ms step_avg:60.40ms
step:194/2245 train_time:11716ms step_avg:60.39ms
step:195/2245 train_time:11777ms step_avg:60.39ms
step:196/2245 train_time:11835ms step_avg:60.38ms
step:197/2245 train_time:11897ms step_avg:60.39ms
step:198/2245 train_time:11956ms step_avg:60.38ms
step:199/2245 train_time:12017ms step_avg:60.39ms
step:200/2245 train_time:12076ms step_avg:60.38ms
step:201/2245 train_time:12138ms step_avg:60.39ms
step:202/2245 train_time:12196ms step_avg:60.38ms
step:203/2245 train_time:12257ms step_avg:60.38ms
step:204/2245 train_time:12316ms step_avg:60.37ms
step:205/2245 train_time:12377ms step_avg:60.38ms
step:206/2245 train_time:12436ms step_avg:60.37ms
step:207/2245 train_time:12497ms step_avg:60.37ms
step:208/2245 train_time:12557ms step_avg:60.37ms
step:209/2245 train_time:12618ms step_avg:60.37ms
step:210/2245 train_time:12678ms step_avg:60.37ms
step:211/2245 train_time:12739ms step_avg:60.37ms
step:212/2245 train_time:12798ms step_avg:60.37ms
step:213/2245 train_time:12859ms step_avg:60.37ms
step:214/2245 train_time:12918ms step_avg:60.36ms
step:215/2245 train_time:12979ms step_avg:60.37ms
step:216/2245 train_time:13037ms step_avg:60.36ms
step:217/2245 train_time:13099ms step_avg:60.36ms
step:218/2245 train_time:13158ms step_avg:60.36ms
step:219/2245 train_time:13219ms step_avg:60.36ms
step:220/2245 train_time:13278ms step_avg:60.36ms
step:221/2245 train_time:13340ms step_avg:60.36ms
step:222/2245 train_time:13398ms step_avg:60.35ms
step:223/2245 train_time:13461ms step_avg:60.36ms
step:224/2245 train_time:13519ms step_avg:60.35ms
step:225/2245 train_time:13581ms step_avg:60.36ms
step:226/2245 train_time:13640ms step_avg:60.35ms
step:227/2245 train_time:13702ms step_avg:60.36ms
step:228/2245 train_time:13761ms step_avg:60.35ms
step:229/2245 train_time:13822ms step_avg:60.36ms
step:230/2245 train_time:13881ms step_avg:60.35ms
step:231/2245 train_time:13943ms step_avg:60.36ms
step:232/2245 train_time:14001ms step_avg:60.35ms
step:233/2245 train_time:14063ms step_avg:60.35ms
step:234/2245 train_time:14122ms step_avg:60.35ms
step:235/2245 train_time:14184ms step_avg:60.36ms
step:236/2245 train_time:14243ms step_avg:60.35ms
step:237/2245 train_time:14305ms step_avg:60.36ms
step:238/2245 train_time:14365ms step_avg:60.36ms
step:239/2245 train_time:14427ms step_avg:60.37ms
step:240/2245 train_time:14487ms step_avg:60.36ms
step:241/2245 train_time:14548ms step_avg:60.37ms
step:242/2245 train_time:14609ms step_avg:60.37ms
step:243/2245 train_time:14671ms step_avg:60.38ms
step:244/2245 train_time:14730ms step_avg:60.37ms
step:245/2245 train_time:14792ms step_avg:60.37ms
step:246/2245 train_time:14851ms step_avg:60.37ms
step:247/2245 train_time:14912ms step_avg:60.37ms
step:248/2245 train_time:14972ms step_avg:60.37ms
step:249/2245 train_time:15033ms step_avg:60.37ms
step:250/2245 train_time:15092ms step_avg:60.37ms
step:250/2245 val_loss:4.0984 train_time:15153ms step_avg:60.61ms
step:251/2245 train_time:15172ms step_avg:60.45ms
step:252/2245 train_time:15217ms step_avg:60.38ms
step:253/2245 train_time:15281ms step_avg:60.40ms
step:254/2245 train_time:15346ms step_avg:60.42ms
step:255/2245 train_time:15408ms step_avg:60.42ms
step:256/2245 train_time:15467ms step_avg:60.42ms
step:257/2245 train_time:15528ms step_avg:60.42ms
step:258/2245 train_time:15586ms step_avg:60.41ms
step:259/2245 train_time:15647ms step_avg:60.41ms
step:260/2245 train_time:15705ms step_avg:60.40ms
step:261/2245 train_time:15765ms step_avg:60.40ms
step:262/2245 train_time:15823ms step_avg:60.39ms
step:263/2245 train_time:15884ms step_avg:60.40ms
step:264/2245 train_time:15942ms step_avg:60.39ms
step:265/2245 train_time:16002ms step_avg:60.39ms
step:266/2245 train_time:16060ms step_avg:60.38ms
step:267/2245 train_time:16122ms step_avg:60.38ms
step:268/2245 train_time:16181ms step_avg:60.38ms
step:269/2245 train_time:16244ms step_avg:60.39ms
step:270/2245 train_time:16305ms step_avg:60.39ms
step:271/2245 train_time:16367ms step_avg:60.40ms
step:272/2245 train_time:16427ms step_avg:60.39ms
step:273/2245 train_time:16489ms step_avg:60.40ms
step:274/2245 train_time:16547ms step_avg:60.39ms
step:275/2245 train_time:16609ms step_avg:60.40ms
step:276/2245 train_time:16667ms step_avg:60.39ms
step:277/2245 train_time:16728ms step_avg:60.39ms
step:278/2245 train_time:16787ms step_avg:60.38ms
step:279/2245 train_time:16848ms step_avg:60.39ms
step:280/2245 train_time:16906ms step_avg:60.38ms
step:281/2245 train_time:16966ms step_avg:60.38ms
step:282/2245 train_time:17024ms step_avg:60.37ms
step:283/2245 train_time:17085ms step_avg:60.37ms
step:284/2245 train_time:17144ms step_avg:60.36ms
step:285/2245 train_time:17205ms step_avg:60.37ms
step:286/2245 train_time:17265ms step_avg:60.37ms
step:287/2245 train_time:17327ms step_avg:60.37ms
step:288/2245 train_time:17387ms step_avg:60.37ms
step:289/2245 train_time:17449ms step_avg:60.38ms
step:290/2245 train_time:17508ms step_avg:60.37ms
step:291/2245 train_time:17569ms step_avg:60.38ms
step:292/2245 train_time:17628ms step_avg:60.37ms
step:293/2245 train_time:17689ms step_avg:60.37ms
step:294/2245 train_time:17747ms step_avg:60.36ms
step:295/2245 train_time:17809ms step_avg:60.37ms
step:296/2245 train_time:17867ms step_avg:60.36ms
step:297/2245 train_time:17928ms step_avg:60.36ms
step:298/2245 train_time:17986ms step_avg:60.36ms
step:299/2245 train_time:18047ms step_avg:60.36ms
step:300/2245 train_time:18106ms step_avg:60.35ms
step:301/2245 train_time:18167ms step_avg:60.36ms
step:302/2245 train_time:18227ms step_avg:60.35ms
step:303/2245 train_time:18288ms step_avg:60.36ms
step:304/2245 train_time:18348ms step_avg:60.36ms
step:305/2245 train_time:18411ms step_avg:60.36ms
step:306/2245 train_time:18470ms step_avg:60.36ms
step:307/2245 train_time:18531ms step_avg:60.36ms
step:308/2245 train_time:18589ms step_avg:60.35ms
step:309/2245 train_time:18650ms step_avg:60.36ms
step:310/2245 train_time:18709ms step_avg:60.35ms
step:311/2245 train_time:18770ms step_avg:60.36ms
step:312/2245 train_time:18830ms step_avg:60.35ms
step:313/2245 train_time:18891ms step_avg:60.36ms
step:314/2245 train_time:18950ms step_avg:60.35ms
step:315/2245 train_time:19011ms step_avg:60.35ms
step:316/2245 train_time:19070ms step_avg:60.35ms
step:317/2245 train_time:19133ms step_avg:60.36ms
step:318/2245 train_time:19193ms step_avg:60.35ms
step:319/2245 train_time:19254ms step_avg:60.36ms
step:320/2245 train_time:19314ms step_avg:60.36ms
step:321/2245 train_time:19376ms step_avg:60.36ms
step:322/2245 train_time:19434ms step_avg:60.35ms
step:323/2245 train_time:19495ms step_avg:60.36ms
step:324/2245 train_time:19554ms step_avg:60.35ms
step:325/2245 train_time:19616ms step_avg:60.36ms
step:326/2245 train_time:19675ms step_avg:60.35ms
step:327/2245 train_time:19736ms step_avg:60.35ms
step:328/2245 train_time:19795ms step_avg:60.35ms
step:329/2245 train_time:19856ms step_avg:60.35ms
step:330/2245 train_time:19916ms step_avg:60.35ms
step:331/2245 train_time:19978ms step_avg:60.36ms
step:332/2245 train_time:20036ms step_avg:60.35ms
step:333/2245 train_time:20098ms step_avg:60.35ms
step:334/2245 train_time:20156ms step_avg:60.35ms
step:335/2245 train_time:20218ms step_avg:60.35ms
step:336/2245 train_time:20278ms step_avg:60.35ms
step:337/2245 train_time:20339ms step_avg:60.35ms
step:338/2245 train_time:20398ms step_avg:60.35ms
step:339/2245 train_time:20459ms step_avg:60.35ms
step:340/2245 train_time:20518ms step_avg:60.35ms
step:341/2245 train_time:20579ms step_avg:60.35ms
step:342/2245 train_time:20637ms step_avg:60.34ms
step:343/2245 train_time:20699ms step_avg:60.35ms
step:344/2245 train_time:20757ms step_avg:60.34ms
step:345/2245 train_time:20819ms step_avg:60.34ms
step:346/2245 train_time:20878ms step_avg:60.34ms
step:347/2245 train_time:20939ms step_avg:60.34ms
step:348/2245 train_time:20998ms step_avg:60.34ms
step:349/2245 train_time:21060ms step_avg:60.34ms
step:350/2245 train_time:21118ms step_avg:60.34ms
step:351/2245 train_time:21180ms step_avg:60.34ms
step:352/2245 train_time:21239ms step_avg:60.34ms
step:353/2245 train_time:21300ms step_avg:60.34ms
step:354/2245 train_time:21358ms step_avg:60.33ms
step:355/2245 train_time:21420ms step_avg:60.34ms
step:356/2245 train_time:21478ms step_avg:60.33ms
step:357/2245 train_time:21540ms step_avg:60.33ms
step:358/2245 train_time:21598ms step_avg:60.33ms
step:359/2245 train_time:21659ms step_avg:60.33ms
step:360/2245 train_time:21718ms step_avg:60.33ms
step:361/2245 train_time:21779ms step_avg:60.33ms
step:362/2245 train_time:21837ms step_avg:60.32ms
step:363/2245 train_time:21898ms step_avg:60.33ms
step:364/2245 train_time:21957ms step_avg:60.32ms
step:365/2245 train_time:22019ms step_avg:60.32ms
step:366/2245 train_time:22078ms step_avg:60.32ms
step:367/2245 train_time:22139ms step_avg:60.33ms
step:368/2245 train_time:22198ms step_avg:60.32ms
step:369/2245 train_time:22259ms step_avg:60.32ms
step:370/2245 train_time:22318ms step_avg:60.32ms
step:371/2245 train_time:22380ms step_avg:60.32ms
step:372/2245 train_time:22438ms step_avg:60.32ms
step:373/2245 train_time:22499ms step_avg:60.32ms
step:374/2245 train_time:22558ms step_avg:60.32ms
step:375/2245 train_time:22619ms step_avg:60.32ms
step:376/2245 train_time:22678ms step_avg:60.31ms
step:377/2245 train_time:22739ms step_avg:60.31ms
step:378/2245 train_time:22798ms step_avg:60.31ms
step:379/2245 train_time:22859ms step_avg:60.31ms
step:380/2245 train_time:22918ms step_avg:60.31ms
step:381/2245 train_time:22979ms step_avg:60.31ms
step:382/2245 train_time:23038ms step_avg:60.31ms
step:383/2245 train_time:23099ms step_avg:60.31ms
step:384/2245 train_time:23158ms step_avg:60.31ms
step:385/2245 train_time:23220ms step_avg:60.31ms
step:386/2245 train_time:23278ms step_avg:60.31ms
step:387/2245 train_time:23340ms step_avg:60.31ms
step:388/2245 train_time:23399ms step_avg:60.31ms
step:389/2245 train_time:23460ms step_avg:60.31ms
step:390/2245 train_time:23518ms step_avg:60.30ms
step:391/2245 train_time:23580ms step_avg:60.31ms
step:392/2245 train_time:23638ms step_avg:60.30ms
step:393/2245 train_time:23699ms step_avg:60.30ms
step:394/2245 train_time:23758ms step_avg:60.30ms
step:395/2245 train_time:23819ms step_avg:60.30ms
step:396/2245 train_time:23878ms step_avg:60.30ms
step:397/2245 train_time:23939ms step_avg:60.30ms
step:398/2245 train_time:23998ms step_avg:60.30ms
step:399/2245 train_time:24060ms step_avg:60.30ms
step:400/2245 train_time:24119ms step_avg:60.30ms
step:401/2245 train_time:24180ms step_avg:60.30ms
step:402/2245 train_time:24239ms step_avg:60.30ms
step:403/2245 train_time:24301ms step_avg:60.30ms
step:404/2245 train_time:24359ms step_avg:60.30ms
step:405/2245 train_time:24420ms step_avg:60.30ms
step:406/2245 train_time:24479ms step_avg:60.29ms
step:407/2245 train_time:24540ms step_avg:60.29ms
step:408/2245 train_time:24599ms step_avg:60.29ms
step:409/2245 train_time:24660ms step_avg:60.29ms
step:410/2245 train_time:24719ms step_avg:60.29ms
step:411/2245 train_time:24780ms step_avg:60.29ms
step:412/2245 train_time:24838ms step_avg:60.29ms
step:413/2245 train_time:24900ms step_avg:60.29ms
step:414/2245 train_time:24959ms step_avg:60.29ms
step:415/2245 train_time:25020ms step_avg:60.29ms
step:416/2245 train_time:25079ms step_avg:60.29ms
step:417/2245 train_time:25140ms step_avg:60.29ms
step:418/2245 train_time:25199ms step_avg:60.29ms
step:419/2245 train_time:25261ms step_avg:60.29ms
step:420/2245 train_time:25321ms step_avg:60.29ms
step:421/2245 train_time:25382ms step_avg:60.29ms
step:422/2245 train_time:25440ms step_avg:60.29ms
step:423/2245 train_time:25502ms step_avg:60.29ms
step:424/2245 train_time:25560ms step_avg:60.28ms
step:425/2245 train_time:25621ms step_avg:60.28ms
step:426/2245 train_time:25679ms step_avg:60.28ms
step:427/2245 train_time:25740ms step_avg:60.28ms
step:428/2245 train_time:25799ms step_avg:60.28ms
step:429/2245 train_time:25860ms step_avg:60.28ms
step:430/2245 train_time:25919ms step_avg:60.28ms
step:431/2245 train_time:25980ms step_avg:60.28ms
step:432/2245 train_time:26039ms step_avg:60.28ms
step:433/2245 train_time:26100ms step_avg:60.28ms
step:434/2245 train_time:26159ms step_avg:60.27ms
step:435/2245 train_time:26220ms step_avg:60.28ms
step:436/2245 train_time:26279ms step_avg:60.27ms
step:437/2245 train_time:26340ms step_avg:60.28ms
step:438/2245 train_time:26400ms step_avg:60.27ms
step:439/2245 train_time:26461ms step_avg:60.28ms
step:440/2245 train_time:26520ms step_avg:60.27ms
step:441/2245 train_time:26580ms step_avg:60.27ms
step:442/2245 train_time:26639ms step_avg:60.27ms
step:443/2245 train_time:26700ms step_avg:60.27ms
step:444/2245 train_time:26758ms step_avg:60.27ms
step:445/2245 train_time:26820ms step_avg:60.27ms
step:446/2245 train_time:26878ms step_avg:60.26ms
step:447/2245 train_time:26939ms step_avg:60.27ms
step:448/2245 train_time:26998ms step_avg:60.26ms
step:449/2245 train_time:27060ms step_avg:60.27ms
step:450/2245 train_time:27118ms step_avg:60.26ms
step:451/2245 train_time:27179ms step_avg:60.26ms
step:452/2245 train_time:27238ms step_avg:60.26ms
step:453/2245 train_time:27299ms step_avg:60.26ms
step:454/2245 train_time:27358ms step_avg:60.26ms
step:455/2245 train_time:27420ms step_avg:60.26ms
step:456/2245 train_time:27479ms step_avg:60.26ms
step:457/2245 train_time:27540ms step_avg:60.26ms
step:458/2245 train_time:27599ms step_avg:60.26ms
step:459/2245 train_time:27660ms step_avg:60.26ms
step:460/2245 train_time:27719ms step_avg:60.26ms
step:461/2245 train_time:27780ms step_avg:60.26ms
step:462/2245 train_time:27838ms step_avg:60.26ms
step:463/2245 train_time:27900ms step_avg:60.26ms
step:464/2245 train_time:27958ms step_avg:60.25ms
step:465/2245 train_time:28020ms step_avg:60.26ms
step:466/2245 train_time:28078ms step_avg:60.25ms
step:467/2245 train_time:28139ms step_avg:60.26ms
step:468/2245 train_time:28198ms step_avg:60.25ms
step:469/2245 train_time:28259ms step_avg:60.25ms
step:470/2245 train_time:28319ms step_avg:60.25ms
step:471/2245 train_time:28380ms step_avg:60.26ms
step:472/2245 train_time:28439ms step_avg:60.25ms
step:473/2245 train_time:28500ms step_avg:60.25ms
step:474/2245 train_time:28558ms step_avg:60.25ms
step:475/2245 train_time:28620ms step_avg:60.25ms
step:476/2245 train_time:28678ms step_avg:60.25ms
step:477/2245 train_time:28740ms step_avg:60.25ms
step:478/2245 train_time:28798ms step_avg:60.25ms
step:479/2245 train_time:28860ms step_avg:60.25ms
step:480/2245 train_time:28918ms step_avg:60.25ms
step:481/2245 train_time:28980ms step_avg:60.25ms
step:482/2245 train_time:29038ms step_avg:60.24ms
step:483/2245 train_time:29099ms step_avg:60.25ms
step:484/2245 train_time:29158ms step_avg:60.24ms
step:485/2245 train_time:29220ms step_avg:60.25ms
step:486/2245 train_time:29278ms step_avg:60.24ms
step:487/2245 train_time:29339ms step_avg:60.25ms
step:488/2245 train_time:29399ms step_avg:60.24ms
step:489/2245 train_time:29460ms step_avg:60.25ms
step:490/2245 train_time:29519ms step_avg:60.24ms
step:491/2245 train_time:29580ms step_avg:60.24ms
step:492/2245 train_time:29639ms step_avg:60.24ms
step:493/2245 train_time:29700ms step_avg:60.24ms
step:494/2245 train_time:29758ms step_avg:60.24ms
step:495/2245 train_time:29820ms step_avg:60.24ms
step:496/2245 train_time:29878ms step_avg:60.24ms
step:497/2245 train_time:29940ms step_avg:60.24ms
step:498/2245 train_time:29998ms step_avg:60.24ms
step:499/2245 train_time:30060ms step_avg:60.24ms
step:500/2245 train_time:30118ms step_avg:60.24ms
step:500/2245 val_loss:3.8233 train_time:30181ms step_avg:60.36ms
step:501/2245 train_time:30201ms step_avg:60.28ms
step:502/2245 train_time:30242ms step_avg:60.24ms
step:503/2245 train_time:30309ms step_avg:60.26ms
step:504/2245 train_time:30370ms step_avg:60.26ms
step:505/2245 train_time:30431ms step_avg:60.26ms
step:506/2245 train_time:30490ms step_avg:60.26ms
step:507/2245 train_time:30550ms step_avg:60.26ms
step:508/2245 train_time:30609ms step_avg:60.25ms
step:509/2245 train_time:30670ms step_avg:60.25ms
step:510/2245 train_time:30727ms step_avg:60.25ms
step:511/2245 train_time:30788ms step_avg:60.25ms
step:512/2245 train_time:30846ms step_avg:60.25ms
step:513/2245 train_time:30907ms step_avg:60.25ms
step:514/2245 train_time:30966ms step_avg:60.25ms
step:515/2245 train_time:31027ms step_avg:60.25ms
step:516/2245 train_time:31085ms step_avg:60.24ms
step:517/2245 train_time:31147ms step_avg:60.25ms
step:518/2245 train_time:31207ms step_avg:60.24ms
step:519/2245 train_time:31270ms step_avg:60.25ms
step:520/2245 train_time:31329ms step_avg:60.25ms
step:521/2245 train_time:31392ms step_avg:60.25ms
step:522/2245 train_time:31451ms step_avg:60.25ms
step:523/2245 train_time:31512ms step_avg:60.25ms
step:524/2245 train_time:31571ms step_avg:60.25ms
step:525/2245 train_time:31632ms step_avg:60.25ms
step:526/2245 train_time:31690ms step_avg:60.25ms
step:527/2245 train_time:31751ms step_avg:60.25ms
step:528/2245 train_time:31809ms step_avg:60.25ms
step:529/2245 train_time:31871ms step_avg:60.25ms
step:530/2245 train_time:31929ms step_avg:60.24ms
step:531/2245 train_time:31990ms step_avg:60.24ms
step:532/2245 train_time:32048ms step_avg:60.24ms
step:533/2245 train_time:32110ms step_avg:60.24ms
step:534/2245 train_time:32170ms step_avg:60.24ms
step:535/2245 train_time:32233ms step_avg:60.25ms
step:536/2245 train_time:32292ms step_avg:60.25ms
step:537/2245 train_time:32354ms step_avg:60.25ms
step:538/2245 train_time:32413ms step_avg:60.25ms
step:539/2245 train_time:32475ms step_avg:60.25ms
step:540/2245 train_time:32534ms step_avg:60.25ms
step:541/2245 train_time:32595ms step_avg:60.25ms
step:542/2245 train_time:32654ms step_avg:60.25ms
step:543/2245 train_time:32715ms step_avg:60.25ms
step:544/2245 train_time:32773ms step_avg:60.25ms
step:545/2245 train_time:32834ms step_avg:60.25ms
step:546/2245 train_time:32893ms step_avg:60.24ms
step:547/2245 train_time:32955ms step_avg:60.25ms
step:548/2245 train_time:33014ms step_avg:60.24ms
step:549/2245 train_time:33076ms step_avg:60.25ms
step:550/2245 train_time:33135ms step_avg:60.25ms
step:551/2245 train_time:33197ms step_avg:60.25ms
step:552/2245 train_time:33257ms step_avg:60.25ms
step:553/2245 train_time:33319ms step_avg:60.25ms
step:554/2245 train_time:33378ms step_avg:60.25ms
step:555/2245 train_time:33440ms step_avg:60.25ms
step:556/2245 train_time:33500ms step_avg:60.25ms
step:557/2245 train_time:33561ms step_avg:60.25ms
step:558/2245 train_time:33621ms step_avg:60.25ms
step:559/2245 train_time:33683ms step_avg:60.26ms
step:560/2245 train_time:33742ms step_avg:60.25ms
step:561/2245 train_time:33804ms step_avg:60.26ms
step:562/2245 train_time:33864ms step_avg:60.26ms
step:563/2245 train_time:33926ms step_avg:60.26ms
step:564/2245 train_time:33985ms step_avg:60.26ms
step:565/2245 train_time:34047ms step_avg:60.26ms
step:566/2245 train_time:34106ms step_avg:60.26ms
step:567/2245 train_time:34168ms step_avg:60.26ms
step:568/2245 train_time:34226ms step_avg:60.26ms
step:569/2245 train_time:34288ms step_avg:60.26ms
step:570/2245 train_time:34347ms step_avg:60.26ms
step:571/2245 train_time:34409ms step_avg:60.26ms
step:572/2245 train_time:34468ms step_avg:60.26ms
step:573/2245 train_time:34529ms step_avg:60.26ms
step:574/2245 train_time:34588ms step_avg:60.26ms
step:575/2245 train_time:34649ms step_avg:60.26ms
step:576/2245 train_time:34707ms step_avg:60.26ms
step:577/2245 train_time:34768ms step_avg:60.26ms
step:578/2245 train_time:34827ms step_avg:60.25ms
step:579/2245 train_time:34889ms step_avg:60.26ms
step:580/2245 train_time:34948ms step_avg:60.25ms
step:581/2245 train_time:35009ms step_avg:60.26ms
step:582/2245 train_time:35068ms step_avg:60.25ms
step:583/2245 train_time:35130ms step_avg:60.26ms
step:584/2245 train_time:35188ms step_avg:60.25ms
step:585/2245 train_time:35249ms step_avg:60.26ms
step:586/2245 train_time:35308ms step_avg:60.25ms
step:587/2245 train_time:35369ms step_avg:60.25ms
step:588/2245 train_time:35427ms step_avg:60.25ms
step:589/2245 train_time:35489ms step_avg:60.25ms
step:590/2245 train_time:35547ms step_avg:60.25ms
step:591/2245 train_time:35609ms step_avg:60.25ms
step:592/2245 train_time:35667ms step_avg:60.25ms
step:593/2245 train_time:35729ms step_avg:60.25ms
step:594/2245 train_time:35787ms step_avg:60.25ms
step:595/2245 train_time:35849ms step_avg:60.25ms
step:596/2245 train_time:35908ms step_avg:60.25ms
step:597/2245 train_time:35969ms step_avg:60.25ms
step:598/2245 train_time:36028ms step_avg:60.25ms
step:599/2245 train_time:36089ms step_avg:60.25ms
step:600/2245 train_time:36148ms step_avg:60.25ms
step:601/2245 train_time:36209ms step_avg:60.25ms
step:602/2245 train_time:36269ms step_avg:60.25ms
step:603/2245 train_time:36330ms step_avg:60.25ms
step:604/2245 train_time:36388ms step_avg:60.25ms
step:605/2245 train_time:36449ms step_avg:60.25ms
step:606/2245 train_time:36507ms step_avg:60.24ms
step:607/2245 train_time:36569ms step_avg:60.25ms
step:608/2245 train_time:36627ms step_avg:60.24ms
step:609/2245 train_time:36689ms step_avg:60.24ms
step:610/2245 train_time:36748ms step_avg:60.24ms
step:611/2245 train_time:36810ms step_avg:60.24ms
step:612/2245 train_time:36869ms step_avg:60.24ms
step:613/2245 train_time:36930ms step_avg:60.25ms
step:614/2245 train_time:36989ms step_avg:60.24ms
step:615/2245 train_time:37050ms step_avg:60.24ms
step:616/2245 train_time:37108ms step_avg:60.24ms
step:617/2245 train_time:37169ms step_avg:60.24ms
step:618/2245 train_time:37228ms step_avg:60.24ms
step:619/2245 train_time:37290ms step_avg:60.24ms
step:620/2245 train_time:37348ms step_avg:60.24ms
step:621/2245 train_time:37410ms step_avg:60.24ms
step:622/2245 train_time:37468ms step_avg:60.24ms
step:623/2245 train_time:37530ms step_avg:60.24ms
step:624/2245 train_time:37588ms step_avg:60.24ms
step:625/2245 train_time:37649ms step_avg:60.24ms
step:626/2245 train_time:37708ms step_avg:60.24ms
step:627/2245 train_time:37770ms step_avg:60.24ms
step:628/2245 train_time:37828ms step_avg:60.24ms
step:629/2245 train_time:37889ms step_avg:60.24ms
step:630/2245 train_time:37948ms step_avg:60.24ms
step:631/2245 train_time:38010ms step_avg:60.24ms
step:632/2245 train_time:38069ms step_avg:60.24ms
step:633/2245 train_time:38130ms step_avg:60.24ms
step:634/2245 train_time:38190ms step_avg:60.24ms
step:635/2245 train_time:38251ms step_avg:60.24ms
step:636/2245 train_time:38310ms step_avg:60.24ms
step:637/2245 train_time:38371ms step_avg:60.24ms
step:638/2245 train_time:38430ms step_avg:60.23ms
step:639/2245 train_time:38491ms step_avg:60.24ms
step:640/2245 train_time:38550ms step_avg:60.23ms
step:641/2245 train_time:38611ms step_avg:60.24ms
step:642/2245 train_time:38670ms step_avg:60.23ms
step:643/2245 train_time:38731ms step_avg:60.24ms
step:644/2245 train_time:38790ms step_avg:60.23ms
step:645/2245 train_time:38852ms step_avg:60.24ms
step:646/2245 train_time:38910ms step_avg:60.23ms
step:647/2245 train_time:38972ms step_avg:60.24ms
step:648/2245 train_time:39031ms step_avg:60.23ms
step:649/2245 train_time:39092ms step_avg:60.23ms
step:650/2245 train_time:39151ms step_avg:60.23ms
step:651/2245 train_time:39213ms step_avg:60.23ms
step:652/2245 train_time:39272ms step_avg:60.23ms
step:653/2245 train_time:39333ms step_avg:60.23ms
step:654/2245 train_time:39392ms step_avg:60.23ms
step:655/2245 train_time:39454ms step_avg:60.23ms
step:656/2245 train_time:39513ms step_avg:60.23ms
step:657/2245 train_time:39574ms step_avg:60.23ms
step:658/2245 train_time:39632ms step_avg:60.23ms
step:659/2245 train_time:39694ms step_avg:60.23ms
step:660/2245 train_time:39753ms step_avg:60.23ms
step:661/2245 train_time:39814ms step_avg:60.23ms
step:662/2245 train_time:39873ms step_avg:60.23ms
step:663/2245 train_time:39935ms step_avg:60.23ms
step:664/2245 train_time:39994ms step_avg:60.23ms
step:665/2245 train_time:40056ms step_avg:60.23ms
step:666/2245 train_time:40115ms step_avg:60.23ms
step:667/2245 train_time:40176ms step_avg:60.23ms
step:668/2245 train_time:40235ms step_avg:60.23ms
step:669/2245 train_time:40296ms step_avg:60.23ms
step:670/2245 train_time:40355ms step_avg:60.23ms
step:671/2245 train_time:40417ms step_avg:60.23ms
step:672/2245 train_time:40476ms step_avg:60.23ms
step:673/2245 train_time:40537ms step_avg:60.23ms
step:674/2245 train_time:40596ms step_avg:60.23ms
step:675/2245 train_time:40657ms step_avg:60.23ms
step:676/2245 train_time:40716ms step_avg:60.23ms
step:677/2245 train_time:40778ms step_avg:60.23ms
step:678/2245 train_time:40837ms step_avg:60.23ms
step:679/2245 train_time:40899ms step_avg:60.23ms
step:680/2245 train_time:40959ms step_avg:60.23ms
step:681/2245 train_time:41020ms step_avg:60.24ms
step:682/2245 train_time:41080ms step_avg:60.23ms
step:683/2245 train_time:41141ms step_avg:60.24ms
step:684/2245 train_time:41201ms step_avg:60.24ms
step:685/2245 train_time:41263ms step_avg:60.24ms
step:686/2245 train_time:41323ms step_avg:60.24ms
step:687/2245 train_time:41384ms step_avg:60.24ms
step:688/2245 train_time:41443ms step_avg:60.24ms
step:689/2245 train_time:41505ms step_avg:60.24ms
step:690/2245 train_time:41565ms step_avg:60.24ms
step:691/2245 train_time:41627ms step_avg:60.24ms
step:692/2245 train_time:41686ms step_avg:60.24ms
step:693/2245 train_time:41747ms step_avg:60.24ms
step:694/2245 train_time:41806ms step_avg:60.24ms
step:695/2245 train_time:41867ms step_avg:60.24ms
step:696/2245 train_time:41926ms step_avg:60.24ms
step:697/2245 train_time:41988ms step_avg:60.24ms
step:698/2245 train_time:42046ms step_avg:60.24ms
step:699/2245 train_time:42108ms step_avg:60.24ms
step:700/2245 train_time:42167ms step_avg:60.24ms
step:701/2245 train_time:42228ms step_avg:60.24ms
step:702/2245 train_time:42287ms step_avg:60.24ms
step:703/2245 train_time:42348ms step_avg:60.24ms
step:704/2245 train_time:42407ms step_avg:60.24ms
step:705/2245 train_time:42468ms step_avg:60.24ms
step:706/2245 train_time:42527ms step_avg:60.24ms
step:707/2245 train_time:42588ms step_avg:60.24ms
step:708/2245 train_time:42647ms step_avg:60.24ms
step:709/2245 train_time:42708ms step_avg:60.24ms
step:710/2245 train_time:42767ms step_avg:60.23ms
step:711/2245 train_time:42828ms step_avg:60.24ms
step:712/2245 train_time:42887ms step_avg:60.23ms
step:713/2245 train_time:42949ms step_avg:60.24ms
step:714/2245 train_time:43007ms step_avg:60.23ms
step:715/2245 train_time:43069ms step_avg:60.24ms
step:716/2245 train_time:43128ms step_avg:60.23ms
step:717/2245 train_time:43189ms step_avg:60.24ms
step:718/2245 train_time:43248ms step_avg:60.23ms
step:719/2245 train_time:43309ms step_avg:60.24ms
step:720/2245 train_time:43368ms step_avg:60.23ms
step:721/2245 train_time:43429ms step_avg:60.23ms
step:722/2245 train_time:43871ms step_avg:60.76ms
step:723/2245 train_time:43931ms step_avg:60.76ms
step:724/2245 train_time:43989ms step_avg:60.76ms
step:725/2245 train_time:44049ms step_avg:60.76ms
step:726/2245 train_time:44107ms step_avg:60.75ms
step:727/2245 train_time:44168ms step_avg:60.75ms
step:728/2245 train_time:44226ms step_avg:60.75ms
step:729/2245 train_time:44286ms step_avg:60.75ms
step:730/2245 train_time:44345ms step_avg:60.75ms
step:731/2245 train_time:44405ms step_avg:60.75ms
step:732/2245 train_time:44463ms step_avg:60.74ms
step:733/2245 train_time:44524ms step_avg:60.74ms
step:734/2245 train_time:44582ms step_avg:60.74ms
step:735/2245 train_time:44643ms step_avg:60.74ms
step:736/2245 train_time:44702ms step_avg:60.74ms
step:737/2245 train_time:44768ms step_avg:60.74ms
step:738/2245 train_time:44832ms step_avg:60.75ms
step:739/2245 train_time:44896ms step_avg:60.75ms
step:740/2245 train_time:44957ms step_avg:60.75ms
step:741/2245 train_time:45020ms step_avg:60.76ms
step:742/2245 train_time:45081ms step_avg:60.76ms
step:743/2245 train_time:45143ms step_avg:60.76ms
step:744/2245 train_time:45202ms step_avg:60.76ms
step:745/2245 train_time:45264ms step_avg:60.76ms
step:746/2245 train_time:45324ms step_avg:60.76ms
step:747/2245 train_time:45386ms step_avg:60.76ms
step:748/2245 train_time:45445ms step_avg:60.75ms
step:749/2245 train_time:45506ms step_avg:60.76ms
step:750/2245 train_time:45566ms step_avg:60.75ms
step:750/2245 val_loss:3.6688 train_time:45628ms step_avg:60.84ms
step:751/2245 train_time:45648ms step_avg:60.78ms
step:752/2245 train_time:45688ms step_avg:60.76ms
step:753/2245 train_time:45750ms step_avg:60.76ms
step:754/2245 train_time:45810ms step_avg:60.76ms
step:755/2245 train_time:45873ms step_avg:60.76ms
step:756/2245 train_time:45932ms step_avg:60.76ms
step:757/2245 train_time:45994ms step_avg:60.76ms
step:758/2245 train_time:46052ms step_avg:60.76ms
step:759/2245 train_time:46113ms step_avg:60.76ms
step:760/2245 train_time:46172ms step_avg:60.75ms
step:761/2245 train_time:46233ms step_avg:60.75ms
step:762/2245 train_time:46292ms step_avg:60.75ms
step:763/2245 train_time:46353ms step_avg:60.75ms
step:764/2245 train_time:46412ms step_avg:60.75ms
step:765/2245 train_time:46472ms step_avg:60.75ms
step:766/2245 train_time:46536ms step_avg:60.75ms
step:767/2245 train_time:46604ms step_avg:60.76ms
step:768/2245 train_time:46664ms step_avg:60.76ms
step:769/2245 train_time:46727ms step_avg:60.76ms
step:770/2245 train_time:46787ms step_avg:60.76ms
step:771/2245 train_time:46849ms step_avg:60.76ms
step:772/2245 train_time:46908ms step_avg:60.76ms
step:773/2245 train_time:46971ms step_avg:60.76ms
step:774/2245 train_time:47030ms step_avg:60.76ms
step:775/2245 train_time:47092ms step_avg:60.76ms
step:776/2245 train_time:47151ms step_avg:60.76ms
step:777/2245 train_time:47212ms step_avg:60.76ms
step:778/2245 train_time:47271ms step_avg:60.76ms
step:779/2245 train_time:47331ms step_avg:60.76ms
step:780/2245 train_time:47390ms step_avg:60.76ms
step:781/2245 train_time:47452ms step_avg:60.76ms
step:782/2245 train_time:47513ms step_avg:60.76ms
step:783/2245 train_time:47576ms step_avg:60.76ms
step:784/2245 train_time:47637ms step_avg:60.76ms
step:785/2245 train_time:47700ms step_avg:60.76ms
step:786/2245 train_time:47760ms step_avg:60.76ms
step:787/2245 train_time:47823ms step_avg:60.77ms
step:788/2245 train_time:47883ms step_avg:60.76ms
step:789/2245 train_time:47945ms step_avg:60.77ms
step:790/2245 train_time:48005ms step_avg:60.77ms
step:791/2245 train_time:48068ms step_avg:60.77ms
step:792/2245 train_time:48128ms step_avg:60.77ms
step:793/2245 train_time:48189ms step_avg:60.77ms
step:794/2245 train_time:48249ms step_avg:60.77ms
step:795/2245 train_time:48311ms step_avg:60.77ms
step:796/2245 train_time:48370ms step_avg:60.77ms
step:797/2245 train_time:48432ms step_avg:60.77ms
step:798/2245 train_time:48491ms step_avg:60.77ms
step:799/2245 train_time:48554ms step_avg:60.77ms
step:800/2245 train_time:48614ms step_avg:60.77ms
step:801/2245 train_time:48676ms step_avg:60.77ms
step:802/2245 train_time:48736ms step_avg:60.77ms
step:803/2245 train_time:48798ms step_avg:60.77ms
step:804/2245 train_time:48859ms step_avg:60.77ms
step:805/2245 train_time:48921ms step_avg:60.77ms
step:806/2245 train_time:48981ms step_avg:60.77ms
step:807/2245 train_time:49044ms step_avg:60.77ms
step:808/2245 train_time:49104ms step_avg:60.77ms
step:809/2245 train_time:49166ms step_avg:60.77ms
step:810/2245 train_time:49226ms step_avg:60.77ms
step:811/2245 train_time:49288ms step_avg:60.77ms
step:812/2245 train_time:49348ms step_avg:60.77ms
step:813/2245 train_time:49411ms step_avg:60.78ms
step:814/2245 train_time:49470ms step_avg:60.77ms
step:815/2245 train_time:49532ms step_avg:60.78ms
step:816/2245 train_time:49591ms step_avg:60.77ms
step:817/2245 train_time:49654ms step_avg:60.78ms
step:818/2245 train_time:49714ms step_avg:60.77ms
step:819/2245 train_time:49775ms step_avg:60.78ms
step:820/2245 train_time:49835ms step_avg:60.77ms
step:821/2245 train_time:49898ms step_avg:60.78ms
step:822/2245 train_time:49958ms step_avg:60.78ms
step:823/2245 train_time:50021ms step_avg:60.78ms
step:824/2245 train_time:50081ms step_avg:60.78ms
step:825/2245 train_time:50144ms step_avg:60.78ms
step:826/2245 train_time:50203ms step_avg:60.78ms
step:827/2245 train_time:50266ms step_avg:60.78ms
step:828/2245 train_time:50326ms step_avg:60.78ms
step:829/2245 train_time:50388ms step_avg:60.78ms
step:830/2245 train_time:50449ms step_avg:60.78ms
step:831/2245 train_time:50512ms step_avg:60.78ms
step:832/2245 train_time:50571ms step_avg:60.78ms
step:833/2245 train_time:50634ms step_avg:60.78ms
step:834/2245 train_time:50693ms step_avg:60.78ms
step:835/2245 train_time:50755ms step_avg:60.78ms
step:836/2245 train_time:50815ms step_avg:60.78ms
step:837/2245 train_time:50877ms step_avg:60.78ms
step:838/2245 train_time:50937ms step_avg:60.78ms
step:839/2245 train_time:51000ms step_avg:60.79ms
step:840/2245 train_time:51060ms step_avg:60.79ms
step:841/2245 train_time:51123ms step_avg:60.79ms
step:842/2245 train_time:51182ms step_avg:60.79ms
step:843/2245 train_time:51244ms step_avg:60.79ms
step:844/2245 train_time:51305ms step_avg:60.79ms
step:845/2245 train_time:51367ms step_avg:60.79ms
step:846/2245 train_time:51427ms step_avg:60.79ms
step:847/2245 train_time:51490ms step_avg:60.79ms
step:848/2245 train_time:51550ms step_avg:60.79ms
step:849/2245 train_time:51613ms step_avg:60.79ms
step:850/2245 train_time:51673ms step_avg:60.79ms
step:851/2245 train_time:51735ms step_avg:60.79ms
step:852/2245 train_time:51795ms step_avg:60.79ms
step:853/2245 train_time:51857ms step_avg:60.79ms
step:854/2245 train_time:51916ms step_avg:60.79ms
step:855/2245 train_time:51979ms step_avg:60.79ms
step:856/2245 train_time:52038ms step_avg:60.79ms
step:857/2245 train_time:52102ms step_avg:60.80ms
step:858/2245 train_time:52162ms step_avg:60.79ms
step:859/2245 train_time:52224ms step_avg:60.80ms
step:860/2245 train_time:52284ms step_avg:60.79ms
step:861/2245 train_time:52346ms step_avg:60.80ms
step:862/2245 train_time:52406ms step_avg:60.80ms
step:863/2245 train_time:52469ms step_avg:60.80ms
step:864/2245 train_time:52529ms step_avg:60.80ms
step:865/2245 train_time:52592ms step_avg:60.80ms
step:866/2245 train_time:52651ms step_avg:60.80ms
step:867/2245 train_time:52715ms step_avg:60.80ms
step:868/2245 train_time:52774ms step_avg:60.80ms
step:869/2245 train_time:52836ms step_avg:60.80ms
step:870/2245 train_time:52896ms step_avg:60.80ms
step:871/2245 train_time:52957ms step_avg:60.80ms
step:872/2245 train_time:53017ms step_avg:60.80ms
step:873/2245 train_time:53079ms step_avg:60.80ms
step:874/2245 train_time:53139ms step_avg:60.80ms
step:875/2245 train_time:53203ms step_avg:60.80ms
step:876/2245 train_time:53263ms step_avg:60.80ms
step:877/2245 train_time:53325ms step_avg:60.80ms
step:878/2245 train_time:53385ms step_avg:60.80ms
step:879/2245 train_time:53448ms step_avg:60.81ms
step:880/2245 train_time:53509ms step_avg:60.81ms
step:881/2245 train_time:53572ms step_avg:60.81ms
step:882/2245 train_time:53632ms step_avg:60.81ms
step:883/2245 train_time:53695ms step_avg:60.81ms
step:884/2245 train_time:53755ms step_avg:60.81ms
step:885/2245 train_time:53817ms step_avg:60.81ms
step:886/2245 train_time:53877ms step_avg:60.81ms
step:887/2245 train_time:53939ms step_avg:60.81ms
step:888/2245 train_time:53998ms step_avg:60.81ms
step:889/2245 train_time:54061ms step_avg:60.81ms
step:890/2245 train_time:54120ms step_avg:60.81ms
step:891/2245 train_time:54182ms step_avg:60.81ms
step:892/2245 train_time:54241ms step_avg:60.81ms
step:893/2245 train_time:54304ms step_avg:60.81ms
step:894/2245 train_time:54364ms step_avg:60.81ms
step:895/2245 train_time:54426ms step_avg:60.81ms
step:896/2245 train_time:54487ms step_avg:60.81ms
step:897/2245 train_time:54550ms step_avg:60.81ms
step:898/2245 train_time:54610ms step_avg:60.81ms
step:899/2245 train_time:54673ms step_avg:60.82ms
step:900/2245 train_time:54733ms step_avg:60.81ms
step:901/2245 train_time:54795ms step_avg:60.82ms
step:902/2245 train_time:54855ms step_avg:60.81ms
step:903/2245 train_time:54917ms step_avg:60.82ms
step:904/2245 train_time:54977ms step_avg:60.81ms
step:905/2245 train_time:55038ms step_avg:60.82ms
step:906/2245 train_time:55098ms step_avg:60.81ms
step:907/2245 train_time:55161ms step_avg:60.82ms
step:908/2245 train_time:55221ms step_avg:60.82ms
step:909/2245 train_time:55284ms step_avg:60.82ms
step:910/2245 train_time:55344ms step_avg:60.82ms
step:911/2245 train_time:55406ms step_avg:60.82ms
step:912/2245 train_time:55465ms step_avg:60.82ms
step:913/2245 train_time:55528ms step_avg:60.82ms
step:914/2245 train_time:55588ms step_avg:60.82ms
step:915/2245 train_time:55652ms step_avg:60.82ms
step:916/2245 train_time:55712ms step_avg:60.82ms
step:917/2245 train_time:55774ms step_avg:60.82ms
step:918/2245 train_time:55834ms step_avg:60.82ms
step:919/2245 train_time:55896ms step_avg:60.82ms
step:920/2245 train_time:55956ms step_avg:60.82ms
step:921/2245 train_time:56018ms step_avg:60.82ms
step:922/2245 train_time:56077ms step_avg:60.82ms
step:923/2245 train_time:56139ms step_avg:60.82ms
step:924/2245 train_time:56199ms step_avg:60.82ms
step:925/2245 train_time:56262ms step_avg:60.82ms
step:926/2245 train_time:56321ms step_avg:60.82ms
step:927/2245 train_time:56384ms step_avg:60.82ms
step:928/2245 train_time:56443ms step_avg:60.82ms
step:929/2245 train_time:56506ms step_avg:60.82ms
step:930/2245 train_time:56567ms step_avg:60.82ms
step:931/2245 train_time:56630ms step_avg:60.83ms
step:932/2245 train_time:56690ms step_avg:60.83ms
step:933/2245 train_time:56752ms step_avg:60.83ms
step:934/2245 train_time:56813ms step_avg:60.83ms
step:935/2245 train_time:56875ms step_avg:60.83ms
step:936/2245 train_time:56936ms step_avg:60.83ms
step:937/2245 train_time:56998ms step_avg:60.83ms
step:938/2245 train_time:57057ms step_avg:60.83ms
step:939/2245 train_time:57119ms step_avg:60.83ms
step:940/2245 train_time:57178ms step_avg:60.83ms
step:941/2245 train_time:57241ms step_avg:60.83ms
step:942/2245 train_time:57300ms step_avg:60.83ms
step:943/2245 train_time:57363ms step_avg:60.83ms
step:944/2245 train_time:57423ms step_avg:60.83ms
step:945/2245 train_time:57485ms step_avg:60.83ms
step:946/2245 train_time:57546ms step_avg:60.83ms
step:947/2245 train_time:57609ms step_avg:60.83ms
step:948/2245 train_time:57669ms step_avg:60.83ms
step:949/2245 train_time:57731ms step_avg:60.83ms
step:950/2245 train_time:57791ms step_avg:60.83ms
step:951/2245 train_time:57854ms step_avg:60.84ms
step:952/2245 train_time:57915ms step_avg:60.83ms
step:953/2245 train_time:57976ms step_avg:60.84ms
step:954/2245 train_time:58036ms step_avg:60.83ms
step:955/2245 train_time:58097ms step_avg:60.83ms
step:956/2245 train_time:58156ms step_avg:60.83ms
step:957/2245 train_time:58218ms step_avg:60.83ms
step:958/2245 train_time:58277ms step_avg:60.83ms
step:959/2245 train_time:58340ms step_avg:60.83ms
step:960/2245 train_time:58400ms step_avg:60.83ms
step:961/2245 train_time:58463ms step_avg:60.84ms
step:962/2245 train_time:58523ms step_avg:60.84ms
step:963/2245 train_time:58586ms step_avg:60.84ms
step:964/2245 train_time:58647ms step_avg:60.84ms
step:965/2245 train_time:58710ms step_avg:60.84ms
step:966/2245 train_time:58770ms step_avg:60.84ms
step:967/2245 train_time:58833ms step_avg:60.84ms
step:968/2245 train_time:58893ms step_avg:60.84ms
step:969/2245 train_time:58955ms step_avg:60.84ms
step:970/2245 train_time:59014ms step_avg:60.84ms
step:971/2245 train_time:59076ms step_avg:60.84ms
step:972/2245 train_time:59135ms step_avg:60.84ms
step:973/2245 train_time:59197ms step_avg:60.84ms
step:974/2245 train_time:59256ms step_avg:60.84ms
step:975/2245 train_time:59318ms step_avg:60.84ms
step:976/2245 train_time:59378ms step_avg:60.84ms
step:977/2245 train_time:59441ms step_avg:60.84ms
step:978/2245 train_time:59501ms step_avg:60.84ms
step:979/2245 train_time:59564ms step_avg:60.84ms
step:980/2245 train_time:59624ms step_avg:60.84ms
step:981/2245 train_time:59687ms step_avg:60.84ms
step:982/2245 train_time:59748ms step_avg:60.84ms
step:983/2245 train_time:59811ms step_avg:60.85ms
step:984/2245 train_time:59871ms step_avg:60.84ms
step:985/2245 train_time:59933ms step_avg:60.85ms
step:986/2245 train_time:59993ms step_avg:60.85ms
step:987/2245 train_time:60055ms step_avg:60.85ms
step:988/2245 train_time:60115ms step_avg:60.85ms
step:989/2245 train_time:60177ms step_avg:60.85ms
step:990/2245 train_time:60237ms step_avg:60.85ms
step:991/2245 train_time:60299ms step_avg:60.85ms
step:992/2245 train_time:60359ms step_avg:60.85ms
step:993/2245 train_time:60421ms step_avg:60.85ms
step:994/2245 train_time:60481ms step_avg:60.85ms
step:995/2245 train_time:60544ms step_avg:60.85ms
step:996/2245 train_time:60604ms step_avg:60.85ms
step:997/2245 train_time:60666ms step_avg:60.85ms
step:998/2245 train_time:60728ms step_avg:60.85ms
step:999/2245 train_time:60792ms step_avg:60.85ms
step:1000/2245 train_time:60851ms step_avg:60.85ms
step:1000/2245 val_loss:3.5937 train_time:60914ms step_avg:60.91ms
step:1001/2245 train_time:60934ms step_avg:60.87ms
step:1002/2245 train_time:60976ms step_avg:60.85ms
step:1003/2245 train_time:61041ms step_avg:60.86ms
step:1004/2245 train_time:61105ms step_avg:60.86ms
step:1005/2245 train_time:61169ms step_avg:60.86ms
step:1006/2245 train_time:61228ms step_avg:60.86ms
step:1007/2245 train_time:61290ms step_avg:60.86ms
step:1008/2245 train_time:61349ms step_avg:60.86ms
step:1009/2245 train_time:61411ms step_avg:60.86ms
step:1010/2245 train_time:61470ms step_avg:60.86ms
step:1011/2245 train_time:61531ms step_avg:60.86ms
step:1012/2245 train_time:61590ms step_avg:60.86ms
step:1013/2245 train_time:61652ms step_avg:60.86ms
step:1014/2245 train_time:61711ms step_avg:60.86ms
step:1015/2245 train_time:61773ms step_avg:60.86ms
step:1016/2245 train_time:61832ms step_avg:60.86ms
step:1017/2245 train_time:61895ms step_avg:60.86ms
step:1018/2245 train_time:61955ms step_avg:60.86ms
step:1019/2245 train_time:62020ms step_avg:60.86ms
step:1020/2245 train_time:62081ms step_avg:60.86ms
step:1021/2245 train_time:62145ms step_avg:60.87ms
step:1022/2245 train_time:62206ms step_avg:60.87ms
step:1023/2245 train_time:62268ms step_avg:60.87ms
step:1024/2245 train_time:62327ms step_avg:60.87ms
step:1025/2245 train_time:62390ms step_avg:60.87ms
step:1026/2245 train_time:62450ms step_avg:60.87ms
step:1027/2245 train_time:62511ms step_avg:60.87ms
step:1028/2245 train_time:62570ms step_avg:60.87ms
step:1029/2245 train_time:62631ms step_avg:60.87ms
step:1030/2245 train_time:62690ms step_avg:60.86ms
step:1031/2245 train_time:62752ms step_avg:60.86ms
step:1032/2245 train_time:62811ms step_avg:60.86ms
step:1033/2245 train_time:62873ms step_avg:60.86ms
step:1034/2245 train_time:62933ms step_avg:60.86ms
step:1035/2245 train_time:62997ms step_avg:60.87ms
step:1036/2245 train_time:63058ms step_avg:60.87ms
step:1037/2245 train_time:63121ms step_avg:60.87ms
step:1038/2245 train_time:63181ms step_avg:60.87ms
step:1039/2245 train_time:63244ms step_avg:60.87ms
step:1040/2245 train_time:63304ms step_avg:60.87ms
step:1041/2245 train_time:63367ms step_avg:60.87ms
step:1042/2245 train_time:63426ms step_avg:60.87ms
step:1043/2245 train_time:63489ms step_avg:60.87ms
step:1044/2245 train_time:63548ms step_avg:60.87ms
step:1045/2245 train_time:63610ms step_avg:60.87ms
step:1046/2245 train_time:63669ms step_avg:60.87ms
step:1047/2245 train_time:63732ms step_avg:60.87ms
step:1048/2245 train_time:63791ms step_avg:60.87ms
step:1049/2245 train_time:63853ms step_avg:60.87ms
step:1050/2245 train_time:63914ms step_avg:60.87ms
step:1051/2245 train_time:63976ms step_avg:60.87ms
step:1052/2245 train_time:64036ms step_avg:60.87ms
step:1053/2245 train_time:64099ms step_avg:60.87ms
step:1054/2245 train_time:64159ms step_avg:60.87ms
step:1055/2245 train_time:64223ms step_avg:60.87ms
step:1056/2245 train_time:64282ms step_avg:60.87ms
step:1057/2245 train_time:64345ms step_avg:60.88ms
step:1058/2245 train_time:64405ms step_avg:60.87ms
step:1059/2245 train_time:64467ms step_avg:60.88ms
step:1060/2245 train_time:64527ms step_avg:60.87ms
step:1061/2245 train_time:64589ms step_avg:60.88ms
step:1062/2245 train_time:64649ms step_avg:60.87ms
step:1063/2245 train_time:64711ms step_avg:60.88ms
step:1064/2245 train_time:64770ms step_avg:60.87ms
step:1065/2245 train_time:64833ms step_avg:60.88ms
step:1066/2245 train_time:64893ms step_avg:60.87ms
step:1067/2245 train_time:64955ms step_avg:60.88ms
step:1068/2245 train_time:65014ms step_avg:60.87ms
step:1069/2245 train_time:65076ms step_avg:60.88ms
step:1070/2245 train_time:65137ms step_avg:60.88ms
step:1071/2245 train_time:65200ms step_avg:60.88ms
step:1072/2245 train_time:65260ms step_avg:60.88ms
step:1073/2245 train_time:65323ms step_avg:60.88ms
step:1074/2245 train_time:65382ms step_avg:60.88ms
step:1075/2245 train_time:65445ms step_avg:60.88ms
step:1076/2245 train_time:65506ms step_avg:60.88ms
step:1077/2245 train_time:65568ms step_avg:60.88ms
step:1078/2245 train_time:65628ms step_avg:60.88ms
step:1079/2245 train_time:65690ms step_avg:60.88ms
step:1080/2245 train_time:65751ms step_avg:60.88ms
step:1081/2245 train_time:65812ms step_avg:60.88ms
step:1082/2245 train_time:65872ms step_avg:60.88ms
step:1083/2245 train_time:65936ms step_avg:60.88ms
step:1084/2245 train_time:65995ms step_avg:60.88ms
step:1085/2245 train_time:66057ms step_avg:60.88ms
step:1086/2245 train_time:66117ms step_avg:60.88ms
step:1087/2245 train_time:66179ms step_avg:60.88ms
step:1088/2245 train_time:66239ms step_avg:60.88ms
step:1089/2245 train_time:66301ms step_avg:60.88ms
step:1090/2245 train_time:66362ms step_avg:60.88ms
step:1091/2245 train_time:66424ms step_avg:60.88ms
step:1092/2245 train_time:66484ms step_avg:60.88ms
step:1093/2245 train_time:66547ms step_avg:60.88ms
step:1094/2245 train_time:66607ms step_avg:60.88ms
step:1095/2245 train_time:66670ms step_avg:60.89ms
step:1096/2245 train_time:66730ms step_avg:60.89ms
step:1097/2245 train_time:66793ms step_avg:60.89ms
step:1098/2245 train_time:66852ms step_avg:60.89ms
step:1099/2245 train_time:66914ms step_avg:60.89ms
step:1100/2245 train_time:66974ms step_avg:60.89ms
step:1101/2245 train_time:67037ms step_avg:60.89ms
step:1102/2245 train_time:67097ms step_avg:60.89ms
step:1103/2245 train_time:67159ms step_avg:60.89ms
step:1104/2245 train_time:67219ms step_avg:60.89ms
step:1105/2245 train_time:67281ms step_avg:60.89ms
step:1106/2245 train_time:67341ms step_avg:60.89ms
step:1107/2245 train_time:67404ms step_avg:60.89ms
step:1108/2245 train_time:67464ms step_avg:60.89ms
step:1109/2245 train_time:67527ms step_avg:60.89ms
step:1110/2245 train_time:67588ms step_avg:60.89ms
step:1111/2245 train_time:67651ms step_avg:60.89ms
step:1112/2245 train_time:67711ms step_avg:60.89ms
step:1113/2245 train_time:67773ms step_avg:60.89ms
step:1114/2245 train_time:67832ms step_avg:60.89ms
step:1115/2245 train_time:67895ms step_avg:60.89ms
step:1116/2245 train_time:67955ms step_avg:60.89ms
step:1117/2245 train_time:68017ms step_avg:60.89ms
step:1118/2245 train_time:68077ms step_avg:60.89ms
step:1119/2245 train_time:68139ms step_avg:60.89ms
step:1120/2245 train_time:68198ms step_avg:60.89ms
step:1121/2245 train_time:68261ms step_avg:60.89ms
step:1122/2245 train_time:68321ms step_avg:60.89ms
step:1123/2245 train_time:68383ms step_avg:60.89ms
step:1124/2245 train_time:68444ms step_avg:60.89ms
step:1125/2245 train_time:68506ms step_avg:60.89ms
step:1126/2245 train_time:68566ms step_avg:60.89ms
step:1127/2245 train_time:68629ms step_avg:60.90ms
step:1128/2245 train_time:68689ms step_avg:60.89ms
step:1129/2245 train_time:68752ms step_avg:60.90ms
step:1130/2245 train_time:68811ms step_avg:60.89ms
step:1131/2245 train_time:68874ms step_avg:60.90ms
step:1132/2245 train_time:68933ms step_avg:60.90ms
step:1133/2245 train_time:68995ms step_avg:60.90ms
step:1134/2245 train_time:69055ms step_avg:60.90ms
step:1135/2245 train_time:69117ms step_avg:60.90ms
step:1136/2245 train_time:69176ms step_avg:60.89ms
step:1137/2245 train_time:69239ms step_avg:60.90ms
step:1138/2245 train_time:69299ms step_avg:60.90ms
step:1139/2245 train_time:69361ms step_avg:60.90ms
step:1140/2245 train_time:69421ms step_avg:60.90ms
step:1141/2245 train_time:69484ms step_avg:60.90ms
step:1142/2245 train_time:69544ms step_avg:60.90ms
step:1143/2245 train_time:69606ms step_avg:60.90ms
step:1144/2245 train_time:69667ms step_avg:60.90ms
step:1145/2245 train_time:69729ms step_avg:60.90ms
step:1146/2245 train_time:69789ms step_avg:60.90ms
step:1147/2245 train_time:69852ms step_avg:60.90ms
step:1148/2245 train_time:69912ms step_avg:60.90ms
step:1149/2245 train_time:69975ms step_avg:60.90ms
step:1150/2245 train_time:70034ms step_avg:60.90ms
step:1151/2245 train_time:70096ms step_avg:60.90ms
step:1152/2245 train_time:70156ms step_avg:60.90ms
step:1153/2245 train_time:70218ms step_avg:60.90ms
step:1154/2245 train_time:70278ms step_avg:60.90ms
step:1155/2245 train_time:70340ms step_avg:60.90ms
step:1156/2245 train_time:70400ms step_avg:60.90ms
step:1157/2245 train_time:70463ms step_avg:60.90ms
step:1158/2245 train_time:70522ms step_avg:60.90ms
step:1159/2245 train_time:70585ms step_avg:60.90ms
step:1160/2245 train_time:70645ms step_avg:60.90ms
step:1161/2245 train_time:70708ms step_avg:60.90ms
step:1162/2245 train_time:70768ms step_avg:60.90ms
step:1163/2245 train_time:70831ms step_avg:60.90ms
step:1164/2245 train_time:70891ms step_avg:60.90ms
step:1165/2245 train_time:70954ms step_avg:60.90ms
step:1166/2245 train_time:71013ms step_avg:60.90ms
step:1167/2245 train_time:71075ms step_avg:60.90ms
step:1168/2245 train_time:71135ms step_avg:60.90ms
step:1169/2245 train_time:71197ms step_avg:60.90ms
step:1170/2245 train_time:71257ms step_avg:60.90ms
step:1171/2245 train_time:71319ms step_avg:60.90ms
step:1172/2245 train_time:71378ms step_avg:60.90ms
step:1173/2245 train_time:71440ms step_avg:60.90ms
step:1174/2245 train_time:71501ms step_avg:60.90ms
step:1175/2245 train_time:71563ms step_avg:60.90ms
step:1176/2245 train_time:71623ms step_avg:60.90ms
step:1177/2245 train_time:71686ms step_avg:60.91ms
step:1178/2245 train_time:71746ms step_avg:60.91ms
step:1179/2245 train_time:71809ms step_avg:60.91ms
step:1180/2245 train_time:71870ms step_avg:60.91ms
step:1181/2245 train_time:71933ms step_avg:60.91ms
step:1182/2245 train_time:71993ms step_avg:60.91ms
step:1183/2245 train_time:72056ms step_avg:60.91ms
step:1184/2245 train_time:72115ms step_avg:60.91ms
step:1185/2245 train_time:72177ms step_avg:60.91ms
step:1186/2245 train_time:72236ms step_avg:60.91ms
step:1187/2245 train_time:72298ms step_avg:60.91ms
step:1188/2245 train_time:72358ms step_avg:60.91ms
step:1189/2245 train_time:72420ms step_avg:60.91ms
step:1190/2245 train_time:72480ms step_avg:60.91ms
step:1191/2245 train_time:72542ms step_avg:60.91ms
step:1192/2245 train_time:72602ms step_avg:60.91ms
step:1193/2245 train_time:72665ms step_avg:60.91ms
step:1194/2245 train_time:72725ms step_avg:60.91ms
step:1195/2245 train_time:72789ms step_avg:60.91ms
step:1196/2245 train_time:72850ms step_avg:60.91ms
step:1197/2245 train_time:72913ms step_avg:60.91ms
step:1198/2245 train_time:72973ms step_avg:60.91ms
step:1199/2245 train_time:73036ms step_avg:60.91ms
step:1200/2245 train_time:73095ms step_avg:60.91ms
step:1201/2245 train_time:73157ms step_avg:60.91ms
step:1202/2245 train_time:73217ms step_avg:60.91ms
step:1203/2245 train_time:73279ms step_avg:60.91ms
step:1204/2245 train_time:73338ms step_avg:60.91ms
step:1205/2245 train_time:73401ms step_avg:60.91ms
step:1206/2245 train_time:73460ms step_avg:60.91ms
step:1207/2245 train_time:73523ms step_avg:60.91ms
step:1208/2245 train_time:73583ms step_avg:60.91ms
step:1209/2245 train_time:73646ms step_avg:60.91ms
step:1210/2245 train_time:73706ms step_avg:60.91ms
step:1211/2245 train_time:73769ms step_avg:60.92ms
step:1212/2245 train_time:73829ms step_avg:60.92ms
step:1213/2245 train_time:73891ms step_avg:60.92ms
step:1214/2245 train_time:73952ms step_avg:60.92ms
step:1215/2245 train_time:74015ms step_avg:60.92ms
step:1216/2245 train_time:74074ms step_avg:60.92ms
step:1217/2245 train_time:74136ms step_avg:60.92ms
step:1218/2245 train_time:74196ms step_avg:60.92ms
step:1219/2245 train_time:74258ms step_avg:60.92ms
step:1220/2245 train_time:74317ms step_avg:60.92ms
step:1221/2245 train_time:74379ms step_avg:60.92ms
step:1222/2245 train_time:74439ms step_avg:60.92ms
step:1223/2245 train_time:74501ms step_avg:60.92ms
step:1224/2245 train_time:74560ms step_avg:60.92ms
step:1225/2245 train_time:74623ms step_avg:60.92ms
step:1226/2245 train_time:74683ms step_avg:60.92ms
step:1227/2245 train_time:74747ms step_avg:60.92ms
step:1228/2245 train_time:74807ms step_avg:60.92ms
step:1229/2245 train_time:74870ms step_avg:60.92ms
step:1230/2245 train_time:74930ms step_avg:60.92ms
step:1231/2245 train_time:74992ms step_avg:60.92ms
step:1232/2245 train_time:75052ms step_avg:60.92ms
step:1233/2245 train_time:75114ms step_avg:60.92ms
step:1234/2245 train_time:75174ms step_avg:60.92ms
step:1235/2245 train_time:75235ms step_avg:60.92ms
step:1236/2245 train_time:75295ms step_avg:60.92ms
step:1237/2245 train_time:75357ms step_avg:60.92ms
step:1238/2245 train_time:75416ms step_avg:60.92ms
step:1239/2245 train_time:75478ms step_avg:60.92ms
step:1240/2245 train_time:75538ms step_avg:60.92ms
step:1241/2245 train_time:75601ms step_avg:60.92ms
step:1242/2245 train_time:75661ms step_avg:60.92ms
step:1243/2245 train_time:75724ms step_avg:60.92ms
step:1244/2245 train_time:75785ms step_avg:60.92ms
step:1245/2245 train_time:75849ms step_avg:60.92ms
step:1246/2245 train_time:75909ms step_avg:60.92ms
step:1247/2245 train_time:75972ms step_avg:60.92ms
step:1248/2245 train_time:76031ms step_avg:60.92ms
step:1249/2245 train_time:76093ms step_avg:60.92ms
step:1250/2245 train_time:76153ms step_avg:60.92ms
step:1250/2245 val_loss:3.5225 train_time:76216ms step_avg:60.97ms
step:1251/2245 train_time:76240ms step_avg:60.94ms
step:1252/2245 train_time:76279ms step_avg:60.93ms
step:1253/2245 train_time:76346ms step_avg:60.93ms
step:1254/2245 train_time:76406ms step_avg:60.93ms
step:1255/2245 train_time:76468ms step_avg:60.93ms
step:1256/2245 train_time:76528ms step_avg:60.93ms
step:1257/2245 train_time:76589ms step_avg:60.93ms
step:1258/2245 train_time:76648ms step_avg:60.93ms
step:1259/2245 train_time:76709ms step_avg:60.93ms
step:1260/2245 train_time:76768ms step_avg:60.93ms
step:1261/2245 train_time:76830ms step_avg:60.93ms
step:1262/2245 train_time:76889ms step_avg:60.93ms
step:1263/2245 train_time:76951ms step_avg:60.93ms
step:1264/2245 train_time:77011ms step_avg:60.93ms
step:1265/2245 train_time:77073ms step_avg:60.93ms
step:1266/2245 train_time:77133ms step_avg:60.93ms
step:1267/2245 train_time:77198ms step_avg:60.93ms
step:1268/2245 train_time:77261ms step_avg:60.93ms
step:1269/2245 train_time:77325ms step_avg:60.93ms
step:1270/2245 train_time:77386ms step_avg:60.93ms
step:1271/2245 train_time:77449ms step_avg:60.94ms
step:1272/2245 train_time:77508ms step_avg:60.93ms
step:1273/2245 train_time:77570ms step_avg:60.93ms
step:1274/2245 train_time:77629ms step_avg:60.93ms
step:1275/2245 train_time:77691ms step_avg:60.93ms
step:1276/2245 train_time:77750ms step_avg:60.93ms
step:1277/2245 train_time:77812ms step_avg:60.93ms
step:1278/2245 train_time:77872ms step_avg:60.93ms
step:1279/2245 train_time:77934ms step_avg:60.93ms
step:1280/2245 train_time:77994ms step_avg:60.93ms
step:1281/2245 train_time:78057ms step_avg:60.93ms
step:1282/2245 train_time:78118ms step_avg:60.93ms
step:1283/2245 train_time:78181ms step_avg:60.94ms
step:1284/2245 train_time:78241ms step_avg:60.94ms
step:1285/2245 train_time:78304ms step_avg:60.94ms
step:1286/2245 train_time:78364ms step_avg:60.94ms
step:1287/2245 train_time:78426ms step_avg:60.94ms
step:1288/2245 train_time:78486ms step_avg:60.94ms
step:1289/2245 train_time:78548ms step_avg:60.94ms
step:1290/2245 train_time:78607ms step_avg:60.94ms
step:1291/2245 train_time:78669ms step_avg:60.94ms
step:1292/2245 train_time:78728ms step_avg:60.93ms
step:1293/2245 train_time:78790ms step_avg:60.94ms
step:1294/2245 train_time:78850ms step_avg:60.93ms
step:1295/2245 train_time:78912ms step_avg:60.94ms
step:1296/2245 train_time:78972ms step_avg:60.94ms
step:1297/2245 train_time:79034ms step_avg:60.94ms
step:1298/2245 train_time:79095ms step_avg:60.94ms
step:1299/2245 train_time:79159ms step_avg:60.94ms
step:1300/2245 train_time:79220ms step_avg:60.94ms
step:1301/2245 train_time:79283ms step_avg:60.94ms
step:1302/2245 train_time:79343ms step_avg:60.94ms
step:1303/2245 train_time:79405ms step_avg:60.94ms
step:1304/2245 train_time:79465ms step_avg:60.94ms
step:1305/2245 train_time:79527ms step_avg:60.94ms
step:1306/2245 train_time:79587ms step_avg:60.94ms
step:1307/2245 train_time:79649ms step_avg:60.94ms
step:1308/2245 train_time:79708ms step_avg:60.94ms
step:1309/2245 train_time:79770ms step_avg:60.94ms
step:1310/2245 train_time:79830ms step_avg:60.94ms
step:1311/2245 train_time:79893ms step_avg:60.94ms
step:1312/2245 train_time:79952ms step_avg:60.94ms
step:1313/2245 train_time:80014ms step_avg:60.94ms
step:1314/2245 train_time:80074ms step_avg:60.94ms
step:1315/2245 train_time:80137ms step_avg:60.94ms
step:1316/2245 train_time:80198ms step_avg:60.94ms
step:1317/2245 train_time:80261ms step_avg:60.94ms
step:1318/2245 train_time:80321ms step_avg:60.94ms
step:1319/2245 train_time:80383ms step_avg:60.94ms
step:1320/2245 train_time:80443ms step_avg:60.94ms
step:1321/2245 train_time:80505ms step_avg:60.94ms
step:1322/2245 train_time:80565ms step_avg:60.94ms
step:1323/2245 train_time:80627ms step_avg:60.94ms
step:1324/2245 train_time:80687ms step_avg:60.94ms
step:1325/2245 train_time:80750ms step_avg:60.94ms
step:1326/2245 train_time:80809ms step_avg:60.94ms
step:1327/2245 train_time:80871ms step_avg:60.94ms
step:1328/2245 train_time:80931ms step_avg:60.94ms
step:1329/2245 train_time:80994ms step_avg:60.94ms
step:1330/2245 train_time:81055ms step_avg:60.94ms
step:1331/2245 train_time:81118ms step_avg:60.95ms
step:1332/2245 train_time:81179ms step_avg:60.95ms
step:1333/2245 train_time:81242ms step_avg:60.95ms
step:1334/2245 train_time:81302ms step_avg:60.95ms
step:1335/2245 train_time:81365ms step_avg:60.95ms
step:1336/2245 train_time:81425ms step_avg:60.95ms
step:1337/2245 train_time:81486ms step_avg:60.95ms
step:1338/2245 train_time:81546ms step_avg:60.95ms
step:1339/2245 train_time:81608ms step_avg:60.95ms
step:1340/2245 train_time:81667ms step_avg:60.95ms
step:1341/2245 train_time:81729ms step_avg:60.95ms
step:1342/2245 train_time:81789ms step_avg:60.95ms
step:1343/2245 train_time:81851ms step_avg:60.95ms
step:1344/2245 train_time:81911ms step_avg:60.95ms
step:1345/2245 train_time:81973ms step_avg:60.95ms
step:1346/2245 train_time:82033ms step_avg:60.95ms
step:1347/2245 train_time:82096ms step_avg:60.95ms
step:1348/2245 train_time:82156ms step_avg:60.95ms
step:1349/2245 train_time:82219ms step_avg:60.95ms
step:1350/2245 train_time:82281ms step_avg:60.95ms
step:1351/2245 train_time:82343ms step_avg:60.95ms
step:1352/2245 train_time:82404ms step_avg:60.95ms
step:1353/2245 train_time:82465ms step_avg:60.95ms
step:1354/2245 train_time:82525ms step_avg:60.95ms
step:1355/2245 train_time:82587ms step_avg:60.95ms
step:1356/2245 train_time:82647ms step_avg:60.95ms
step:1357/2245 train_time:82709ms step_avg:60.95ms
step:1358/2245 train_time:82768ms step_avg:60.95ms
step:1359/2245 train_time:82830ms step_avg:60.95ms
step:1360/2245 train_time:82890ms step_avg:60.95ms
step:1361/2245 train_time:82952ms step_avg:60.95ms
step:1362/2245 train_time:83012ms step_avg:60.95ms
step:1363/2245 train_time:83075ms step_avg:60.95ms
step:1364/2245 train_time:83135ms step_avg:60.95ms
step:1365/2245 train_time:83199ms step_avg:60.95ms
step:1366/2245 train_time:83260ms step_avg:60.95ms
step:1367/2245 train_time:83323ms step_avg:60.95ms
step:1368/2245 train_time:83384ms step_avg:60.95ms
step:1369/2245 train_time:83446ms step_avg:60.95ms
step:1370/2245 train_time:83506ms step_avg:60.95ms
step:1371/2245 train_time:83567ms step_avg:60.95ms
step:1372/2245 train_time:83626ms step_avg:60.95ms
step:1373/2245 train_time:83689ms step_avg:60.95ms
step:1374/2245 train_time:83748ms step_avg:60.95ms
step:1375/2245 train_time:83811ms step_avg:60.95ms
step:1376/2245 train_time:83870ms step_avg:60.95ms
step:1377/2245 train_time:83933ms step_avg:60.95ms
step:1378/2245 train_time:83993ms step_avg:60.95ms
step:1379/2245 train_time:84055ms step_avg:60.95ms
step:1380/2245 train_time:84115ms step_avg:60.95ms
step:1381/2245 train_time:84178ms step_avg:60.95ms
step:1382/2245 train_time:84239ms step_avg:60.95ms
step:1383/2245 train_time:84302ms step_avg:60.96ms
step:1384/2245 train_time:84362ms step_avg:60.96ms
step:1385/2245 train_time:84425ms step_avg:60.96ms
step:1386/2245 train_time:84485ms step_avg:60.96ms
step:1387/2245 train_time:84548ms step_avg:60.96ms
step:1388/2245 train_time:84607ms step_avg:60.96ms
step:1389/2245 train_time:84669ms step_avg:60.96ms
step:1390/2245 train_time:84729ms step_avg:60.96ms
step:1391/2245 train_time:84791ms step_avg:60.96ms
step:1392/2245 train_time:84851ms step_avg:60.96ms
step:1393/2245 train_time:84913ms step_avg:60.96ms
step:1394/2245 train_time:84973ms step_avg:60.96ms
step:1395/2245 train_time:85036ms step_avg:60.96ms
step:1396/2245 train_time:85096ms step_avg:60.96ms
step:1397/2245 train_time:85158ms step_avg:60.96ms
step:1398/2245 train_time:85219ms step_avg:60.96ms
step:1399/2245 train_time:85282ms step_avg:60.96ms
step:1400/2245 train_time:85342ms step_avg:60.96ms
step:1401/2245 train_time:85404ms step_avg:60.96ms
step:1402/2245 train_time:85464ms step_avg:60.96ms
step:1403/2245 train_time:85526ms step_avg:60.96ms
step:1404/2245 train_time:85586ms step_avg:60.96ms
step:1405/2245 train_time:85648ms step_avg:60.96ms
step:1406/2245 train_time:85707ms step_avg:60.96ms
step:1407/2245 train_time:85769ms step_avg:60.96ms
step:1408/2245 train_time:85829ms step_avg:60.96ms
step:1409/2245 train_time:85892ms step_avg:60.96ms
step:1410/2245 train_time:85952ms step_avg:60.96ms
step:1411/2245 train_time:86015ms step_avg:60.96ms
step:1412/2245 train_time:86075ms step_avg:60.96ms
step:1413/2245 train_time:86138ms step_avg:60.96ms
step:1414/2245 train_time:86198ms step_avg:60.96ms
step:1415/2245 train_time:86261ms step_avg:60.96ms
step:1416/2245 train_time:86321ms step_avg:60.96ms
step:1417/2245 train_time:86383ms step_avg:60.96ms
step:1418/2245 train_time:86443ms step_avg:60.96ms
step:1419/2245 train_time:86506ms step_avg:60.96ms
step:1420/2245 train_time:86565ms step_avg:60.96ms
step:1421/2245 train_time:86627ms step_avg:60.96ms
step:1422/2245 train_time:86688ms step_avg:60.96ms
step:1423/2245 train_time:86749ms step_avg:60.96ms
step:1424/2245 train_time:86809ms step_avg:60.96ms
step:1425/2245 train_time:86872ms step_avg:60.96ms
step:1426/2245 train_time:86932ms step_avg:60.96ms
step:1427/2245 train_time:86995ms step_avg:60.96ms
step:1428/2245 train_time:87054ms step_avg:60.96ms
step:1429/2245 train_time:87117ms step_avg:60.96ms
step:1430/2245 train_time:87177ms step_avg:60.96ms
step:1431/2245 train_time:87241ms step_avg:60.96ms
step:1432/2245 train_time:87301ms step_avg:60.96ms
step:1433/2245 train_time:87364ms step_avg:60.97ms
step:1434/2245 train_time:87423ms step_avg:60.96ms
step:1435/2245 train_time:87485ms step_avg:60.97ms
step:1436/2245 train_time:87545ms step_avg:60.96ms
step:1437/2245 train_time:87607ms step_avg:60.96ms
step:1438/2245 train_time:87666ms step_avg:60.96ms
step:1439/2245 train_time:87728ms step_avg:60.96ms
step:1440/2245 train_time:87788ms step_avg:60.96ms
step:1441/2245 train_time:87850ms step_avg:60.96ms
step:1442/2245 train_time:87910ms step_avg:60.96ms
step:1443/2245 train_time:87972ms step_avg:60.96ms
step:1444/2245 train_time:88032ms step_avg:60.96ms
step:1445/2245 train_time:88095ms step_avg:60.97ms
step:1446/2245 train_time:88156ms step_avg:60.97ms
step:1447/2245 train_time:88219ms step_avg:60.97ms
step:1448/2245 train_time:88279ms step_avg:60.97ms
step:1449/2245 train_time:88342ms step_avg:60.97ms
step:1450/2245 train_time:88402ms step_avg:60.97ms
step:1451/2245 train_time:88464ms step_avg:60.97ms
step:1452/2245 train_time:88524ms step_avg:60.97ms
step:1453/2245 train_time:88587ms step_avg:60.97ms
step:1454/2245 train_time:88646ms step_avg:60.97ms
step:1455/2245 train_time:88708ms step_avg:60.97ms
step:1456/2245 train_time:88768ms step_avg:60.97ms
step:1457/2245 train_time:88830ms step_avg:60.97ms
step:1458/2245 train_time:88890ms step_avg:60.97ms
step:1459/2245 train_time:88953ms step_avg:60.97ms
step:1460/2245 train_time:89012ms step_avg:60.97ms
step:1461/2245 train_time:89075ms step_avg:60.97ms
step:1462/2245 train_time:89136ms step_avg:60.97ms
step:1463/2245 train_time:89200ms step_avg:60.97ms
step:1464/2245 train_time:89259ms step_avg:60.97ms
step:1465/2245 train_time:89322ms step_avg:60.97ms
step:1466/2245 train_time:89381ms step_avg:60.97ms
step:1467/2245 train_time:89444ms step_avg:60.97ms
step:1468/2245 train_time:89503ms step_avg:60.97ms
step:1469/2245 train_time:89566ms step_avg:60.97ms
step:1470/2245 train_time:89625ms step_avg:60.97ms
step:1471/2245 train_time:89687ms step_avg:60.97ms
step:1472/2245 train_time:89747ms step_avg:60.97ms
step:1473/2245 train_time:89810ms step_avg:60.97ms
step:1474/2245 train_time:89870ms step_avg:60.97ms
step:1475/2245 train_time:89933ms step_avg:60.97ms
step:1476/2245 train_time:89993ms step_avg:60.97ms
step:1477/2245 train_time:90056ms step_avg:60.97ms
step:1478/2245 train_time:90116ms step_avg:60.97ms
step:1479/2245 train_time:90180ms step_avg:60.97ms
step:1480/2245 train_time:90240ms step_avg:60.97ms
step:1481/2245 train_time:90303ms step_avg:60.97ms
step:1482/2245 train_time:90363ms step_avg:60.97ms
step:1483/2245 train_time:90426ms step_avg:60.98ms
step:1484/2245 train_time:90487ms step_avg:60.98ms
step:1485/2245 train_time:90549ms step_avg:60.98ms
step:1486/2245 train_time:90609ms step_avg:60.98ms
step:1487/2245 train_time:90672ms step_avg:60.98ms
step:1488/2245 train_time:90733ms step_avg:60.98ms
step:1489/2245 train_time:90795ms step_avg:60.98ms
step:1490/2245 train_time:90855ms step_avg:60.98ms
step:1491/2245 train_time:90918ms step_avg:60.98ms
step:1492/2245 train_time:90978ms step_avg:60.98ms
step:1493/2245 train_time:91040ms step_avg:60.98ms
step:1494/2245 train_time:91101ms step_avg:60.98ms
step:1495/2245 train_time:91164ms step_avg:60.98ms
step:1496/2245 train_time:91224ms step_avg:60.98ms
step:1497/2245 train_time:91287ms step_avg:60.98ms
step:1498/2245 train_time:91348ms step_avg:60.98ms
step:1499/2245 train_time:91411ms step_avg:60.98ms
step:1500/2245 train_time:91473ms step_avg:60.98ms
step:1500/2245 val_loss:3.4421 train_time:91537ms step_avg:61.02ms
step:1501/2245 train_time:91558ms step_avg:61.00ms
step:1502/2245 train_time:91600ms step_avg:60.99ms
step:1503/2245 train_time:91662ms step_avg:60.99ms
step:1504/2245 train_time:91724ms step_avg:60.99ms
step:1505/2245 train_time:91787ms step_avg:60.99ms
step:1506/2245 train_time:91848ms step_avg:60.99ms
step:1507/2245 train_time:91910ms step_avg:60.99ms
step:1508/2245 train_time:91969ms step_avg:60.99ms
step:1509/2245 train_time:92030ms step_avg:60.99ms
step:1510/2245 train_time:92090ms step_avg:60.99ms
step:1511/2245 train_time:92151ms step_avg:60.99ms
step:1512/2245 train_time:92211ms step_avg:60.99ms
step:1513/2245 train_time:92273ms step_avg:60.99ms
step:1514/2245 train_time:92333ms step_avg:60.99ms
step:1515/2245 train_time:92395ms step_avg:60.99ms
step:1516/2245 train_time:92458ms step_avg:60.99ms
step:1517/2245 train_time:92523ms step_avg:60.99ms
step:1518/2245 train_time:92585ms step_avg:60.99ms
step:1519/2245 train_time:92648ms step_avg:60.99ms
step:1520/2245 train_time:92709ms step_avg:60.99ms
step:1521/2245 train_time:92772ms step_avg:60.99ms
step:1522/2245 train_time:92832ms step_avg:60.99ms
step:1523/2245 train_time:92895ms step_avg:60.99ms
step:1524/2245 train_time:92954ms step_avg:60.99ms
step:1525/2245 train_time:93016ms step_avg:60.99ms
step:1526/2245 train_time:93075ms step_avg:60.99ms
step:1527/2245 train_time:93137ms step_avg:60.99ms
step:1528/2245 train_time:93197ms step_avg:60.99ms
step:1529/2245 train_time:93259ms step_avg:60.99ms
step:1530/2245 train_time:93320ms step_avg:60.99ms
step:1531/2245 train_time:93384ms step_avg:61.00ms
step:1532/2245 train_time:93445ms step_avg:61.00ms
step:1533/2245 train_time:93510ms step_avg:61.00ms
step:1534/2245 train_time:93570ms step_avg:61.00ms
step:1535/2245 train_time:93633ms step_avg:61.00ms
step:1536/2245 train_time:93693ms step_avg:61.00ms
step:1537/2245 train_time:93757ms step_avg:61.00ms
step:1538/2245 train_time:93817ms step_avg:61.00ms
step:1539/2245 train_time:93881ms step_avg:61.00ms
step:1540/2245 train_time:93942ms step_avg:61.00ms
step:1541/2245 train_time:94004ms step_avg:61.00ms
step:1542/2245 train_time:94064ms step_avg:61.00ms
step:1543/2245 train_time:94126ms step_avg:61.00ms
step:1544/2245 train_time:94185ms step_avg:61.00ms
step:1545/2245 train_time:94248ms step_avg:61.00ms
step:1546/2245 train_time:94308ms step_avg:61.00ms
step:1547/2245 train_time:94370ms step_avg:61.00ms
step:1548/2245 train_time:94431ms step_avg:61.00ms
step:1549/2245 train_time:94494ms step_avg:61.00ms
step:1550/2245 train_time:94555ms step_avg:61.00ms
step:1551/2245 train_time:94618ms step_avg:61.00ms
step:1552/2245 train_time:94679ms step_avg:61.00ms
step:1553/2245 train_time:94742ms step_avg:61.01ms
step:1554/2245 train_time:94804ms step_avg:61.01ms
step:1555/2245 train_time:94867ms step_avg:61.01ms
step:1556/2245 train_time:94927ms step_avg:61.01ms
step:1557/2245 train_time:94989ms step_avg:61.01ms
step:1558/2245 train_time:95049ms step_avg:61.01ms
step:1559/2245 train_time:95111ms step_avg:61.01ms
step:1560/2245 train_time:95171ms step_avg:61.01ms
step:1561/2245 train_time:95233ms step_avg:61.01ms
step:1562/2245 train_time:95293ms step_avg:61.01ms
step:1563/2245 train_time:95356ms step_avg:61.01ms
step:1564/2245 train_time:95416ms step_avg:61.01ms
step:1565/2245 train_time:95479ms step_avg:61.01ms
step:1566/2245 train_time:95540ms step_avg:61.01ms
step:1567/2245 train_time:95604ms step_avg:61.01ms
step:1568/2245 train_time:95665ms step_avg:61.01ms
step:1569/2245 train_time:95727ms step_avg:61.01ms
step:1570/2245 train_time:95788ms step_avg:61.01ms
step:1571/2245 train_time:95851ms step_avg:61.01ms
step:1572/2245 train_time:95911ms step_avg:61.01ms
step:1573/2245 train_time:95974ms step_avg:61.01ms
step:1574/2245 train_time:96034ms step_avg:61.01ms
step:1575/2245 train_time:96096ms step_avg:61.01ms
step:1576/2245 train_time:96157ms step_avg:61.01ms
step:1577/2245 train_time:96220ms step_avg:61.01ms
step:1578/2245 train_time:96281ms step_avg:61.01ms
step:1579/2245 train_time:96344ms step_avg:61.02ms
step:1580/2245 train_time:96405ms step_avg:61.02ms
step:1581/2245 train_time:96468ms step_avg:61.02ms
step:1582/2245 train_time:96528ms step_avg:61.02ms
step:1583/2245 train_time:96590ms step_avg:61.02ms
step:1584/2245 train_time:96651ms step_avg:61.02ms
step:1585/2245 train_time:96714ms step_avg:61.02ms
step:1586/2245 train_time:96774ms step_avg:61.02ms
step:1587/2245 train_time:96836ms step_avg:61.02ms
step:1588/2245 train_time:96896ms step_avg:61.02ms
step:1589/2245 train_time:96959ms step_avg:61.02ms
step:1590/2245 train_time:97019ms step_avg:61.02ms
step:1591/2245 train_time:97082ms step_avg:61.02ms
step:1592/2245 train_time:97142ms step_avg:61.02ms
step:1593/2245 train_time:97205ms step_avg:61.02ms
step:1594/2245 train_time:97266ms step_avg:61.02ms
step:1595/2245 train_time:97329ms step_avg:61.02ms
step:1596/2245 train_time:97389ms step_avg:61.02ms
step:1597/2245 train_time:97452ms step_avg:61.02ms
step:1598/2245 train_time:97512ms step_avg:61.02ms
step:1599/2245 train_time:97574ms step_avg:61.02ms
step:1600/2245 train_time:97635ms step_avg:61.02ms
step:1601/2245 train_time:97697ms step_avg:61.02ms
step:1602/2245 train_time:97758ms step_avg:61.02ms
step:1603/2245 train_time:97821ms step_avg:61.02ms
step:1604/2245 train_time:97883ms step_avg:61.02ms
step:1605/2245 train_time:97946ms step_avg:61.03ms
step:1606/2245 train_time:98007ms step_avg:61.03ms
step:1607/2245 train_time:98069ms step_avg:61.03ms
step:1608/2245 train_time:98128ms step_avg:61.03ms
step:1609/2245 train_time:98191ms step_avg:61.03ms
step:1610/2245 train_time:98252ms step_avg:61.03ms
step:1611/2245 train_time:98315ms step_avg:61.03ms
step:1612/2245 train_time:98375ms step_avg:61.03ms
step:1613/2245 train_time:98438ms step_avg:61.03ms
step:1614/2245 train_time:98498ms step_avg:61.03ms
step:1615/2245 train_time:98561ms step_avg:61.03ms
step:1616/2245 train_time:98621ms step_avg:61.03ms
step:1617/2245 train_time:98684ms step_avg:61.03ms
step:1618/2245 train_time:98745ms step_avg:61.03ms
step:1619/2245 train_time:98808ms step_avg:61.03ms
step:1620/2245 train_time:98867ms step_avg:61.03ms
step:1621/2245 train_time:98930ms step_avg:61.03ms
step:1622/2245 train_time:98990ms step_avg:61.03ms
step:1623/2245 train_time:99053ms step_avg:61.03ms
step:1624/2245 train_time:99113ms step_avg:61.03ms
step:1625/2245 train_time:99175ms step_avg:61.03ms
step:1626/2245 train_time:99235ms step_avg:61.03ms
step:1627/2245 train_time:99299ms step_avg:61.03ms
step:1628/2245 train_time:99359ms step_avg:61.03ms
step:1629/2245 train_time:99422ms step_avg:61.03ms
step:1630/2245 train_time:99483ms step_avg:61.03ms
step:1631/2245 train_time:99545ms step_avg:61.03ms
step:1632/2245 train_time:99605ms step_avg:61.03ms
step:1633/2245 train_time:99669ms step_avg:61.03ms
step:1634/2245 train_time:99729ms step_avg:61.03ms
step:1635/2245 train_time:99790ms step_avg:61.03ms
step:1636/2245 train_time:99851ms step_avg:61.03ms
step:1637/2245 train_time:99913ms step_avg:61.03ms
step:1638/2245 train_time:99973ms step_avg:61.03ms
step:1639/2245 train_time:100036ms step_avg:61.03ms
step:1640/2245 train_time:100097ms step_avg:61.03ms
step:1641/2245 train_time:100159ms step_avg:61.04ms
step:1642/2245 train_time:100220ms step_avg:61.04ms
step:1643/2245 train_time:100283ms step_avg:61.04ms
step:1644/2245 train_time:100343ms step_avg:61.04ms
step:1645/2245 train_time:100407ms step_avg:61.04ms
step:1646/2245 train_time:100467ms step_avg:61.04ms
step:1647/2245 train_time:100529ms step_avg:61.04ms
step:1648/2245 train_time:100589ms step_avg:61.04ms
step:1649/2245 train_time:100652ms step_avg:61.04ms
step:1650/2245 train_time:100712ms step_avg:61.04ms
step:1651/2245 train_time:100775ms step_avg:61.04ms
step:1652/2245 train_time:100835ms step_avg:61.04ms
step:1653/2245 train_time:100897ms step_avg:61.04ms
step:1654/2245 train_time:100957ms step_avg:61.04ms
step:1655/2245 train_time:101020ms step_avg:61.04ms
step:1656/2245 train_time:101080ms step_avg:61.04ms
step:1657/2245 train_time:101143ms step_avg:61.04ms
step:1658/2245 train_time:101204ms step_avg:61.04ms
step:1659/2245 train_time:101268ms step_avg:61.04ms
step:1660/2245 train_time:101327ms step_avg:61.04ms
step:1661/2245 train_time:101391ms step_avg:61.04ms
step:1662/2245 train_time:101451ms step_avg:61.04ms
step:1663/2245 train_time:101514ms step_avg:61.04ms
step:1664/2245 train_time:101574ms step_avg:61.04ms
step:1665/2245 train_time:101637ms step_avg:61.04ms
step:1666/2245 train_time:101697ms step_avg:61.04ms
step:1667/2245 train_time:101760ms step_avg:61.04ms
step:1668/2245 train_time:101821ms step_avg:61.04ms
step:1669/2245 train_time:101883ms step_avg:61.04ms
step:1670/2245 train_time:101945ms step_avg:61.04ms
step:1671/2245 train_time:102008ms step_avg:61.05ms
step:1672/2245 train_time:102068ms step_avg:61.05ms
step:1673/2245 train_time:102130ms step_avg:61.05ms
step:1674/2245 train_time:102190ms step_avg:61.05ms
step:1675/2245 train_time:102253ms step_avg:61.05ms
step:1676/2245 train_time:102313ms step_avg:61.05ms
step:1677/2245 train_time:102376ms step_avg:61.05ms
step:1678/2245 train_time:102437ms step_avg:61.05ms
step:1679/2245 train_time:102499ms step_avg:61.05ms
step:1680/2245 train_time:102559ms step_avg:61.05ms
step:1681/2245 train_time:102622ms step_avg:61.05ms
step:1682/2245 train_time:102682ms step_avg:61.05ms
step:1683/2245 train_time:102745ms step_avg:61.05ms
step:1684/2245 train_time:102807ms step_avg:61.05ms
step:1685/2245 train_time:102869ms step_avg:61.05ms
step:1686/2245 train_time:102929ms step_avg:61.05ms
step:1687/2245 train_time:102991ms step_avg:61.05ms
step:1688/2245 train_time:103051ms step_avg:61.05ms
step:1689/2245 train_time:103114ms step_avg:61.05ms
step:1690/2245 train_time:103174ms step_avg:61.05ms
step:1691/2245 train_time:103237ms step_avg:61.05ms
step:1692/2245 train_time:103297ms step_avg:61.05ms
step:1693/2245 train_time:103360ms step_avg:61.05ms
step:1694/2245 train_time:103421ms step_avg:61.05ms
step:1695/2245 train_time:103484ms step_avg:61.05ms
step:1696/2245 train_time:103544ms step_avg:61.05ms
step:1697/2245 train_time:103607ms step_avg:61.05ms
step:1698/2245 train_time:103667ms step_avg:61.05ms
step:1699/2245 train_time:103730ms step_avg:61.05ms
step:1700/2245 train_time:103790ms step_avg:61.05ms
step:1701/2245 train_time:103853ms step_avg:61.05ms
step:1702/2245 train_time:103914ms step_avg:61.05ms
step:1703/2245 train_time:103976ms step_avg:61.05ms
step:1704/2245 train_time:104037ms step_avg:61.05ms
step:1705/2245 train_time:104100ms step_avg:61.06ms
step:1706/2245 train_time:104160ms step_avg:61.06ms
step:1707/2245 train_time:104224ms step_avg:61.06ms
step:1708/2245 train_time:104284ms step_avg:61.06ms
step:1709/2245 train_time:104347ms step_avg:61.06ms
step:1710/2245 train_time:104407ms step_avg:61.06ms
step:1711/2245 train_time:104470ms step_avg:61.06ms
step:1712/2245 train_time:104529ms step_avg:61.06ms
step:1713/2245 train_time:104592ms step_avg:61.06ms
step:1714/2245 train_time:104653ms step_avg:61.06ms
step:1715/2245 train_time:104715ms step_avg:61.06ms
step:1716/2245 train_time:104775ms step_avg:61.06ms
step:1717/2245 train_time:104838ms step_avg:61.06ms
step:1718/2245 train_time:104897ms step_avg:61.06ms
step:1719/2245 train_time:104961ms step_avg:61.06ms
step:1720/2245 train_time:105021ms step_avg:61.06ms
step:1721/2245 train_time:105084ms step_avg:61.06ms
step:1722/2245 train_time:105145ms step_avg:61.06ms
step:1723/2245 train_time:105208ms step_avg:61.06ms
step:1724/2245 train_time:105268ms step_avg:61.06ms
step:1725/2245 train_time:105330ms step_avg:61.06ms
step:1726/2245 train_time:105390ms step_avg:61.06ms
step:1727/2245 train_time:105453ms step_avg:61.06ms
step:1728/2245 train_time:105513ms step_avg:61.06ms
step:1729/2245 train_time:105576ms step_avg:61.06ms
step:1730/2245 train_time:105637ms step_avg:61.06ms
step:1731/2245 train_time:105700ms step_avg:61.06ms
step:1732/2245 train_time:105762ms step_avg:61.06ms
step:1733/2245 train_time:105824ms step_avg:61.06ms
step:1734/2245 train_time:105885ms step_avg:61.06ms
step:1735/2245 train_time:105948ms step_avg:61.07ms
step:1736/2245 train_time:106008ms step_avg:61.06ms
step:1737/2245 train_time:106071ms step_avg:61.07ms
step:1738/2245 train_time:106131ms step_avg:61.06ms
step:1739/2245 train_time:106193ms step_avg:61.07ms
step:1740/2245 train_time:106254ms step_avg:61.07ms
step:1741/2245 train_time:106316ms step_avg:61.07ms
step:1742/2245 train_time:106376ms step_avg:61.07ms
step:1743/2245 train_time:106439ms step_avg:61.07ms
step:1744/2245 train_time:106500ms step_avg:61.07ms
step:1745/2245 train_time:106564ms step_avg:61.07ms
step:1746/2245 train_time:106625ms step_avg:61.07ms
step:1747/2245 train_time:106688ms step_avg:61.07ms
step:1748/2245 train_time:106749ms step_avg:61.07ms
step:1749/2245 train_time:106812ms step_avg:61.07ms
step:1750/2245 train_time:106872ms step_avg:61.07ms
step:1750/2245 val_loss:3.3778 train_time:106935ms step_avg:61.11ms
step:1751/2245 train_time:106954ms step_avg:61.08ms
step:1752/2245 train_time:106997ms step_avg:61.07ms
step:1753/2245 train_time:107062ms step_avg:61.07ms
step:1754/2245 train_time:107124ms step_avg:61.07ms
step:1755/2245 train_time:107187ms step_avg:61.08ms
step:1756/2245 train_time:107248ms step_avg:61.08ms
step:1757/2245 train_time:107310ms step_avg:61.08ms
step:1758/2245 train_time:107370ms step_avg:61.08ms
step:1759/2245 train_time:107433ms step_avg:61.08ms
step:1760/2245 train_time:107492ms step_avg:61.08ms
step:1761/2245 train_time:107555ms step_avg:61.08ms
step:1762/2245 train_time:107614ms step_avg:61.07ms
step:1763/2245 train_time:107676ms step_avg:61.08ms
step:1764/2245 train_time:107737ms step_avg:61.08ms
step:1765/2245 train_time:107799ms step_avg:61.08ms
step:1766/2245 train_time:107859ms step_avg:61.08ms
step:1767/2245 train_time:107922ms step_avg:61.08ms
step:1768/2245 train_time:107983ms step_avg:61.08ms
step:1769/2245 train_time:108047ms step_avg:61.08ms
step:1770/2245 train_time:108109ms step_avg:61.08ms
step:1771/2245 train_time:108172ms step_avg:61.08ms
step:1772/2245 train_time:108233ms step_avg:61.08ms
step:1773/2245 train_time:108295ms step_avg:61.08ms
step:1774/2245 train_time:108355ms step_avg:61.08ms
step:1775/2245 train_time:108417ms step_avg:61.08ms
step:1776/2245 train_time:108477ms step_avg:61.08ms
step:1777/2245 train_time:108539ms step_avg:61.08ms
step:1778/2245 train_time:108599ms step_avg:61.08ms
step:1779/2245 train_time:108662ms step_avg:61.08ms
step:1780/2245 train_time:108722ms step_avg:61.08ms
step:1781/2245 train_time:108784ms step_avg:61.08ms
step:1782/2245 train_time:108844ms step_avg:61.08ms
step:1783/2245 train_time:108908ms step_avg:61.08ms
step:1784/2245 train_time:108970ms step_avg:61.08ms
step:1785/2245 train_time:109033ms step_avg:61.08ms
step:1786/2245 train_time:109094ms step_avg:61.08ms
step:1787/2245 train_time:109156ms step_avg:61.08ms
step:1788/2245 train_time:109217ms step_avg:61.08ms
step:1789/2245 train_time:109279ms step_avg:61.08ms
step:1790/2245 train_time:109339ms step_avg:61.08ms
step:1791/2245 train_time:109402ms step_avg:61.08ms
step:1792/2245 train_time:109462ms step_avg:61.08ms
step:1793/2245 train_time:109524ms step_avg:61.08ms
step:1794/2245 train_time:109584ms step_avg:61.08ms
step:1795/2245 train_time:109647ms step_avg:61.08ms
step:1796/2245 train_time:109707ms step_avg:61.08ms
step:1797/2245 train_time:109770ms step_avg:61.09ms
step:1798/2245 train_time:109831ms step_avg:61.08ms
step:1799/2245 train_time:109893ms step_avg:61.09ms
step:1800/2245 train_time:109954ms step_avg:61.09ms
step:1801/2245 train_time:110017ms step_avg:61.09ms
step:1802/2245 train_time:110079ms step_avg:61.09ms
step:1803/2245 train_time:110142ms step_avg:61.09ms
step:1804/2245 train_time:110202ms step_avg:61.09ms
step:1805/2245 train_time:110265ms step_avg:61.09ms
step:1806/2245 train_time:110326ms step_avg:61.09ms
step:1807/2245 train_time:110388ms step_avg:61.09ms
step:1808/2245 train_time:110449ms step_avg:61.09ms
step:1809/2245 train_time:110511ms step_avg:61.09ms
step:1810/2245 train_time:110572ms step_avg:61.09ms
step:1811/2245 train_time:110635ms step_avg:61.09ms
step:1812/2245 train_time:110694ms step_avg:61.09ms
step:1813/2245 train_time:110756ms step_avg:61.09ms
step:1814/2245 train_time:110816ms step_avg:61.09ms
step:1815/2245 train_time:110879ms step_avg:61.09ms
step:1816/2245 train_time:110939ms step_avg:61.09ms
step:1817/2245 train_time:111002ms step_avg:61.09ms
step:1818/2245 train_time:111063ms step_avg:61.09ms
step:1819/2245 train_time:111127ms step_avg:61.09ms
step:1820/2245 train_time:111187ms step_avg:61.09ms
step:1821/2245 train_time:111250ms step_avg:61.09ms
step:1822/2245 train_time:111311ms step_avg:61.09ms
step:1823/2245 train_time:111375ms step_avg:61.09ms
step:1824/2245 train_time:111435ms step_avg:61.09ms
step:1825/2245 train_time:111497ms step_avg:61.09ms
step:1826/2245 train_time:111557ms step_avg:61.09ms
step:1827/2245 train_time:111619ms step_avg:61.09ms
step:1828/2245 train_time:111680ms step_avg:61.09ms
step:1829/2245 train_time:111742ms step_avg:61.09ms
step:1830/2245 train_time:111803ms step_avg:61.09ms
step:1831/2245 train_time:111865ms step_avg:61.09ms
step:1832/2245 train_time:111925ms step_avg:61.09ms
step:1833/2245 train_time:111989ms step_avg:61.10ms
step:1834/2245 train_time:112050ms step_avg:61.10ms
step:1835/2245 train_time:112113ms step_avg:61.10ms
step:1836/2245 train_time:112174ms step_avg:61.10ms
step:1837/2245 train_time:112237ms step_avg:61.10ms
step:1838/2245 train_time:112296ms step_avg:61.10ms
step:1839/2245 train_time:112358ms step_avg:61.10ms
step:1840/2245 train_time:112419ms step_avg:61.10ms
step:1841/2245 train_time:112482ms step_avg:61.10ms
step:1842/2245 train_time:112542ms step_avg:61.10ms
step:1843/2245 train_time:112604ms step_avg:61.10ms
step:1844/2245 train_time:112665ms step_avg:61.10ms
step:1845/2245 train_time:112728ms step_avg:61.10ms
step:1846/2245 train_time:112788ms step_avg:61.10ms
step:1847/2245 train_time:112850ms step_avg:61.10ms
step:1848/2245 train_time:112911ms step_avg:61.10ms
step:1849/2245 train_time:112974ms step_avg:61.10ms
step:1850/2245 train_time:113035ms step_avg:61.10ms
step:1851/2245 train_time:113097ms step_avg:61.10ms
step:1852/2245 train_time:113158ms step_avg:61.10ms
step:1853/2245 train_time:113221ms step_avg:61.10ms
step:1854/2245 train_time:113281ms step_avg:61.10ms
step:1855/2245 train_time:113343ms step_avg:61.10ms
step:1856/2245 train_time:113403ms step_avg:61.10ms
step:1857/2245 train_time:113466ms step_avg:61.10ms
step:1858/2245 train_time:113526ms step_avg:61.10ms
step:1859/2245 train_time:113589ms step_avg:61.10ms
step:1860/2245 train_time:113650ms step_avg:61.10ms
step:1861/2245 train_time:113712ms step_avg:61.10ms
step:1862/2245 train_time:113773ms step_avg:61.10ms
step:1863/2245 train_time:113835ms step_avg:61.10ms
step:1864/2245 train_time:113895ms step_avg:61.10ms
step:1865/2245 train_time:113958ms step_avg:61.10ms
step:1866/2245 train_time:114018ms step_avg:61.10ms
step:1867/2245 train_time:114081ms step_avg:61.10ms
step:1868/2245 train_time:114141ms step_avg:61.10ms
step:1869/2245 train_time:114204ms step_avg:61.10ms
step:1870/2245 train_time:114265ms step_avg:61.10ms
step:1871/2245 train_time:114328ms step_avg:61.11ms
step:1872/2245 train_time:114389ms step_avg:61.10ms
step:1873/2245 train_time:114452ms step_avg:61.11ms
step:1874/2245 train_time:114512ms step_avg:61.11ms
step:1875/2245 train_time:114575ms step_avg:61.11ms
step:1876/2245 train_time:114635ms step_avg:61.11ms
step:1877/2245 train_time:114697ms step_avg:61.11ms
step:1878/2245 train_time:114758ms step_avg:61.11ms
step:1879/2245 train_time:114820ms step_avg:61.11ms
step:1880/2245 train_time:114880ms step_avg:61.11ms
step:1881/2245 train_time:114943ms step_avg:61.11ms
step:1882/2245 train_time:115003ms step_avg:61.11ms
step:1883/2245 train_time:115066ms step_avg:61.11ms
step:1884/2245 train_time:115126ms step_avg:61.11ms
step:1885/2245 train_time:115189ms step_avg:61.11ms
step:1886/2245 train_time:115249ms step_avg:61.11ms
step:1887/2245 train_time:115313ms step_avg:61.11ms
step:1888/2245 train_time:115373ms step_avg:61.11ms
step:1889/2245 train_time:115436ms step_avg:61.11ms
step:1890/2245 train_time:115496ms step_avg:61.11ms
step:1891/2245 train_time:115559ms step_avg:61.11ms
step:1892/2245 train_time:115619ms step_avg:61.11ms
step:1893/2245 train_time:115681ms step_avg:61.11ms
step:1894/2245 train_time:115742ms step_avg:61.11ms
step:1895/2245 train_time:115804ms step_avg:61.11ms
step:1896/2245 train_time:115865ms step_avg:61.11ms
step:1897/2245 train_time:115928ms step_avg:61.11ms
step:1898/2245 train_time:115989ms step_avg:61.11ms
step:1899/2245 train_time:116052ms step_avg:61.11ms
step:1900/2245 train_time:116112ms step_avg:61.11ms
step:1901/2245 train_time:116175ms step_avg:61.11ms
step:1902/2245 train_time:116236ms step_avg:61.11ms
step:1903/2245 train_time:116298ms step_avg:61.11ms
step:1904/2245 train_time:116358ms step_avg:61.11ms
step:1905/2245 train_time:116420ms step_avg:61.11ms
step:1906/2245 train_time:116481ms step_avg:61.11ms
step:1907/2245 train_time:116544ms step_avg:61.11ms
step:1908/2245 train_time:116603ms step_avg:61.11ms
step:1909/2245 train_time:116666ms step_avg:61.11ms
step:1910/2245 train_time:116727ms step_avg:61.11ms
step:1911/2245 train_time:116791ms step_avg:61.11ms
step:1912/2245 train_time:116851ms step_avg:61.11ms
step:1913/2245 train_time:116915ms step_avg:61.12ms
step:1914/2245 train_time:116975ms step_avg:61.12ms
step:1915/2245 train_time:117037ms step_avg:61.12ms
step:1916/2245 train_time:117097ms step_avg:61.12ms
step:1917/2245 train_time:117160ms step_avg:61.12ms
step:1918/2245 train_time:117220ms step_avg:61.12ms
step:1919/2245 train_time:117282ms step_avg:61.12ms
step:1920/2245 train_time:117343ms step_avg:61.12ms
step:1921/2245 train_time:117406ms step_avg:61.12ms
step:1922/2245 train_time:117467ms step_avg:61.12ms
step:1923/2245 train_time:117529ms step_avg:61.12ms
step:1924/2245 train_time:117590ms step_avg:61.12ms
step:1925/2245 train_time:117653ms step_avg:61.12ms
step:1926/2245 train_time:117713ms step_avg:61.12ms
step:1927/2245 train_time:117776ms step_avg:61.12ms
step:1928/2245 train_time:117836ms step_avg:61.12ms
step:1929/2245 train_time:117898ms step_avg:61.12ms
step:1930/2245 train_time:117958ms step_avg:61.12ms
step:1931/2245 train_time:118020ms step_avg:61.12ms
step:1932/2245 train_time:118080ms step_avg:61.12ms
step:1933/2245 train_time:118144ms step_avg:61.12ms
step:1934/2245 train_time:118204ms step_avg:61.12ms
step:1935/2245 train_time:118267ms step_avg:61.12ms
step:1936/2245 train_time:118329ms step_avg:61.12ms
step:1937/2245 train_time:118391ms step_avg:61.12ms
step:1938/2245 train_time:118452ms step_avg:61.12ms
step:1939/2245 train_time:118515ms step_avg:61.12ms
step:1940/2245 train_time:118574ms step_avg:61.12ms
step:1941/2245 train_time:118637ms step_avg:61.12ms
step:1942/2245 train_time:118697ms step_avg:61.12ms
step:1943/2245 train_time:118760ms step_avg:61.12ms
step:1944/2245 train_time:118820ms step_avg:61.12ms
step:1945/2245 train_time:118883ms step_avg:61.12ms
step:1946/2245 train_time:118943ms step_avg:61.12ms
step:1947/2245 train_time:119006ms step_avg:61.12ms
step:1948/2245 train_time:119067ms step_avg:61.12ms
step:1949/2245 train_time:119130ms step_avg:61.12ms
step:1950/2245 train_time:119190ms step_avg:61.12ms
step:1951/2245 train_time:119253ms step_avg:61.12ms
step:1952/2245 train_time:119313ms step_avg:61.12ms
step:1953/2245 train_time:119376ms step_avg:61.12ms
step:1954/2245 train_time:119437ms step_avg:61.12ms
step:1955/2245 train_time:119499ms step_avg:61.12ms
step:1956/2245 train_time:119559ms step_avg:61.12ms
step:1957/2245 train_time:119621ms step_avg:61.12ms
step:1958/2245 train_time:119682ms step_avg:61.12ms
step:1959/2245 train_time:119745ms step_avg:61.13ms
step:1960/2245 train_time:119805ms step_avg:61.12ms
step:1961/2245 train_time:119868ms step_avg:61.13ms
step:1962/2245 train_time:119929ms step_avg:61.13ms
step:1963/2245 train_time:119992ms step_avg:61.13ms
step:1964/2245 train_time:120053ms step_avg:61.13ms
step:1965/2245 train_time:120116ms step_avg:61.13ms
step:1966/2245 train_time:120176ms step_avg:61.13ms
step:1967/2245 train_time:120238ms step_avg:61.13ms
step:1968/2245 train_time:120299ms step_avg:61.13ms
step:1969/2245 train_time:120361ms step_avg:61.13ms
step:1970/2245 train_time:120421ms step_avg:61.13ms
step:1971/2245 train_time:120484ms step_avg:61.13ms
step:1972/2245 train_time:120545ms step_avg:61.13ms
step:1973/2245 train_time:120608ms step_avg:61.13ms
step:1974/2245 train_time:120669ms step_avg:61.13ms
step:1975/2245 train_time:120731ms step_avg:61.13ms
step:1976/2245 train_time:120791ms step_avg:61.13ms
step:1977/2245 train_time:120854ms step_avg:61.13ms
step:1978/2245 train_time:120914ms step_avg:61.13ms
step:1979/2245 train_time:120977ms step_avg:61.13ms
step:1980/2245 train_time:121037ms step_avg:61.13ms
step:1981/2245 train_time:121100ms step_avg:61.13ms
step:1982/2245 train_time:121160ms step_avg:61.13ms
step:1983/2245 train_time:121223ms step_avg:61.13ms
step:1984/2245 train_time:121283ms step_avg:61.13ms
step:1985/2245 train_time:121346ms step_avg:61.13ms
step:1986/2245 train_time:121406ms step_avg:61.13ms
step:1987/2245 train_time:121470ms step_avg:61.13ms
step:1988/2245 train_time:121530ms step_avg:61.13ms
step:1989/2245 train_time:121593ms step_avg:61.13ms
step:1990/2245 train_time:121653ms step_avg:61.13ms
step:1991/2245 train_time:121716ms step_avg:61.13ms
step:1992/2245 train_time:121776ms step_avg:61.13ms
step:1993/2245 train_time:121839ms step_avg:61.13ms
step:1994/2245 train_time:121899ms step_avg:61.13ms
step:1995/2245 train_time:121962ms step_avg:61.13ms
step:1996/2245 train_time:122023ms step_avg:61.13ms
step:1997/2245 train_time:122086ms step_avg:61.13ms
step:1998/2245 train_time:122147ms step_avg:61.13ms
step:1999/2245 train_time:122210ms step_avg:61.14ms
step:2000/2245 train_time:122270ms step_avg:61.14ms
step:2000/2245 val_loss:3.3231 train_time:122333ms step_avg:61.17ms
step:2001/2245 train_time:122353ms step_avg:61.15ms
step:2002/2245 train_time:122397ms step_avg:61.14ms
step:2003/2245 train_time:122464ms step_avg:61.14ms
step:2004/2245 train_time:122524ms step_avg:61.14ms
step:2005/2245 train_time:122587ms step_avg:61.14ms
step:2006/2245 train_time:122648ms step_avg:61.14ms
step:2007/2245 train_time:122710ms step_avg:61.14ms
step:2008/2245 train_time:122770ms step_avg:61.14ms
step:2009/2245 train_time:122831ms step_avg:61.14ms
step:2010/2245 train_time:122891ms step_avg:61.14ms
step:2011/2245 train_time:122954ms step_avg:61.14ms
step:2012/2245 train_time:123014ms step_avg:61.14ms
step:2013/2245 train_time:123077ms step_avg:61.14ms
step:2014/2245 train_time:123138ms step_avg:61.14ms
step:2015/2245 train_time:123200ms step_avg:61.14ms
step:2016/2245 train_time:123261ms step_avg:61.14ms
step:2017/2245 train_time:123324ms step_avg:61.14ms
step:2018/2245 train_time:123386ms step_avg:61.14ms
step:2019/2245 train_time:123450ms step_avg:61.14ms
step:2020/2245 train_time:123512ms step_avg:61.14ms
step:2021/2245 train_time:123575ms step_avg:61.15ms
step:2022/2245 train_time:123635ms step_avg:61.15ms
step:2023/2245 train_time:123698ms step_avg:61.15ms
step:2024/2245 train_time:123758ms step_avg:61.15ms
step:2025/2245 train_time:123820ms step_avg:61.15ms
step:2026/2245 train_time:123880ms step_avg:61.15ms
step:2027/2245 train_time:123942ms step_avg:61.15ms
step:2028/2245 train_time:124002ms step_avg:61.15ms
step:2029/2245 train_time:124064ms step_avg:61.15ms
step:2030/2245 train_time:124124ms step_avg:61.14ms
step:2031/2245 train_time:124187ms step_avg:61.15ms
step:2032/2245 train_time:124248ms step_avg:61.15ms
step:2033/2245 train_time:124312ms step_avg:61.15ms
step:2034/2245 train_time:124373ms step_avg:61.15ms
step:2035/2245 train_time:124438ms step_avg:61.15ms
step:2036/2245 train_time:124499ms step_avg:61.15ms
step:2037/2245 train_time:124561ms step_avg:61.15ms
step:2038/2245 train_time:124621ms step_avg:61.15ms
step:2039/2245 train_time:124684ms step_avg:61.15ms
step:2040/2245 train_time:124745ms step_avg:61.15ms
step:2041/2245 train_time:124807ms step_avg:61.15ms
step:2042/2245 train_time:124867ms step_avg:61.15ms
step:2043/2245 train_time:124930ms step_avg:61.15ms
step:2044/2245 train_time:124990ms step_avg:61.15ms
step:2045/2245 train_time:125054ms step_avg:61.15ms
step:2046/2245 train_time:125115ms step_avg:61.15ms
step:2047/2245 train_time:125178ms step_avg:61.15ms
step:2048/2245 train_time:125238ms step_avg:61.15ms
step:2049/2245 train_time:125301ms step_avg:61.15ms
step:2050/2245 train_time:125361ms step_avg:61.15ms
step:2051/2245 train_time:125425ms step_avg:61.15ms
step:2052/2245 train_time:125486ms step_avg:61.15ms
step:2053/2245 train_time:125548ms step_avg:61.15ms
step:2054/2245 train_time:125608ms step_avg:61.15ms
step:2055/2245 train_time:125671ms step_avg:61.15ms
step:2056/2245 train_time:125732ms step_avg:61.15ms
step:2057/2245 train_time:125795ms step_avg:61.15ms
step:2058/2245 train_time:125855ms step_avg:61.15ms
step:2059/2245 train_time:125918ms step_avg:61.15ms
step:2060/2245 train_time:125977ms step_avg:61.15ms
step:2061/2245 train_time:126040ms step_avg:61.15ms
step:2062/2245 train_time:126100ms step_avg:61.15ms
step:2063/2245 train_time:126162ms step_avg:61.15ms
step:2064/2245 train_time:126222ms step_avg:61.15ms
step:2065/2245 train_time:126285ms step_avg:61.15ms
step:2066/2245 train_time:126346ms step_avg:61.15ms
step:2067/2245 train_time:126409ms step_avg:61.16ms
step:2068/2245 train_time:126470ms step_avg:61.16ms
step:2069/2245 train_time:126533ms step_avg:61.16ms
step:2070/2245 train_time:126593ms step_avg:61.16ms
step:2071/2245 train_time:126656ms step_avg:61.16ms
step:2072/2245 train_time:126717ms step_avg:61.16ms
step:2073/2245 train_time:126780ms step_avg:61.16ms
step:2074/2245 train_time:126841ms step_avg:61.16ms
step:2075/2245 train_time:126903ms step_avg:61.16ms
step:2076/2245 train_time:126965ms step_avg:61.16ms
step:2077/2245 train_time:127026ms step_avg:61.16ms
step:2078/2245 train_time:127086ms step_avg:61.16ms
step:2079/2245 train_time:127149ms step_avg:61.16ms
step:2080/2245 train_time:127210ms step_avg:61.16ms
step:2081/2245 train_time:127273ms step_avg:61.16ms
step:2082/2245 train_time:127334ms step_avg:61.16ms
step:2083/2245 train_time:127398ms step_avg:61.16ms
step:2084/2245 train_time:127458ms step_avg:61.16ms
step:2085/2245 train_time:127521ms step_avg:61.16ms
step:2086/2245 train_time:127581ms step_avg:61.16ms
step:2087/2245 train_time:127644ms step_avg:61.16ms
step:2088/2245 train_time:127704ms step_avg:61.16ms
step:2089/2245 train_time:127767ms step_avg:61.16ms
step:2090/2245 train_time:127827ms step_avg:61.16ms
step:2091/2245 train_time:127891ms step_avg:61.16ms
step:2092/2245 train_time:127952ms step_avg:61.16ms
step:2093/2245 train_time:128015ms step_avg:61.16ms
step:2094/2245 train_time:128075ms step_avg:61.16ms
step:2095/2245 train_time:128138ms step_avg:61.16ms
step:2096/2245 train_time:128198ms step_avg:61.16ms
step:2097/2245 train_time:128261ms step_avg:61.16ms
step:2098/2245 train_time:128320ms step_avg:61.16ms
step:2099/2245 train_time:128384ms step_avg:61.16ms
step:2100/2245 train_time:128444ms step_avg:61.16ms
step:2101/2245 train_time:128508ms step_avg:61.16ms
step:2102/2245 train_time:128568ms step_avg:61.16ms
step:2103/2245 train_time:128631ms step_avg:61.17ms
step:2104/2245 train_time:128692ms step_avg:61.17ms
step:2105/2245 train_time:128755ms step_avg:61.17ms
step:2106/2245 train_time:128816ms step_avg:61.17ms
step:2107/2245 train_time:128879ms step_avg:61.17ms
step:2108/2245 train_time:128941ms step_avg:61.17ms
step:2109/2245 train_time:129002ms step_avg:61.17ms
step:2110/2245 train_time:129062ms step_avg:61.17ms
step:2111/2245 train_time:129125ms step_avg:61.17ms
step:2112/2245 train_time:129185ms step_avg:61.17ms
step:2113/2245 train_time:129248ms step_avg:61.17ms
step:2114/2245 train_time:129308ms step_avg:61.17ms
step:2115/2245 train_time:129372ms step_avg:61.17ms
step:2116/2245 train_time:129433ms step_avg:61.17ms
step:2117/2245 train_time:129497ms step_avg:61.17ms
step:2118/2245 train_time:129557ms step_avg:61.17ms
step:2119/2245 train_time:129620ms step_avg:61.17ms
step:2120/2245 train_time:129680ms step_avg:61.17ms
step:2121/2245 train_time:129742ms step_avg:61.17ms
step:2122/2245 train_time:129802ms step_avg:61.17ms
step:2123/2245 train_time:129865ms step_avg:61.17ms
step:2124/2245 train_time:129925ms step_avg:61.17ms
step:2125/2245 train_time:129988ms step_avg:61.17ms
step:2126/2245 train_time:130047ms step_avg:61.17ms
step:2127/2245 train_time:130111ms step_avg:61.17ms
step:2128/2245 train_time:130171ms step_avg:61.17ms
step:2129/2245 train_time:130235ms step_avg:61.17ms
step:2130/2245 train_time:130295ms step_avg:61.17ms
step:2131/2245 train_time:130358ms step_avg:61.17ms
step:2132/2245 train_time:130418ms step_avg:61.17ms
step:2133/2245 train_time:130481ms step_avg:61.17ms
step:2134/2245 train_time:130542ms step_avg:61.17ms
step:2135/2245 train_time:130604ms step_avg:61.17ms
step:2136/2245 train_time:130666ms step_avg:61.17ms
step:2137/2245 train_time:130728ms step_avg:61.17ms
step:2138/2245 train_time:130788ms step_avg:61.17ms
step:2139/2245 train_time:130852ms step_avg:61.17ms
step:2140/2245 train_time:130914ms step_avg:61.17ms
step:2141/2245 train_time:130977ms step_avg:61.18ms
step:2142/2245 train_time:131038ms step_avg:61.18ms
step:2143/2245 train_time:131100ms step_avg:61.18ms
step:2144/2245 train_time:131160ms step_avg:61.18ms
step:2145/2245 train_time:131222ms step_avg:61.18ms
step:2146/2245 train_time:131283ms step_avg:61.18ms
step:2147/2245 train_time:131346ms step_avg:61.18ms
step:2148/2245 train_time:131406ms step_avg:61.18ms
step:2149/2245 train_time:131469ms step_avg:61.18ms
step:2150/2245 train_time:131530ms step_avg:61.18ms
step:2151/2245 train_time:131593ms step_avg:61.18ms
step:2152/2245 train_time:131655ms step_avg:61.18ms
step:2153/2245 train_time:131718ms step_avg:61.18ms
step:2154/2245 train_time:131778ms step_avg:61.18ms
step:2155/2245 train_time:131841ms step_avg:61.18ms
step:2156/2245 train_time:131901ms step_avg:61.18ms
step:2157/2245 train_time:131963ms step_avg:61.18ms
step:2158/2245 train_time:132024ms step_avg:61.18ms
step:2159/2245 train_time:132087ms step_avg:61.18ms
step:2160/2245 train_time:132147ms step_avg:61.18ms
step:2161/2245 train_time:132210ms step_avg:61.18ms
step:2162/2245 train_time:132271ms step_avg:61.18ms
step:2163/2245 train_time:132334ms step_avg:61.18ms
step:2164/2245 train_time:132395ms step_avg:61.18ms
step:2165/2245 train_time:132458ms step_avg:61.18ms
step:2166/2245 train_time:132518ms step_avg:61.18ms
step:2167/2245 train_time:132581ms step_avg:61.18ms
step:2168/2245 train_time:132641ms step_avg:61.18ms
step:2169/2245 train_time:132704ms step_avg:61.18ms
step:2170/2245 train_time:132764ms step_avg:61.18ms
step:2171/2245 train_time:132827ms step_avg:61.18ms
step:2172/2245 train_time:132888ms step_avg:61.18ms
step:2173/2245 train_time:132951ms step_avg:61.18ms
step:2174/2245 train_time:133012ms step_avg:61.18ms
step:2175/2245 train_time:133075ms step_avg:61.18ms
step:2176/2245 train_time:133135ms step_avg:61.18ms
step:2177/2245 train_time:133198ms step_avg:61.18ms
step:2178/2245 train_time:133258ms step_avg:61.18ms
step:2179/2245 train_time:133320ms step_avg:61.18ms
step:2180/2245 train_time:133381ms step_avg:61.18ms
step:2181/2245 train_time:133443ms step_avg:61.18ms
step:2182/2245 train_time:133504ms step_avg:61.18ms
step:2183/2245 train_time:133567ms step_avg:61.18ms
step:2184/2245 train_time:133627ms step_avg:61.18ms
step:2185/2245 train_time:133691ms step_avg:61.19ms
step:2186/2245 train_time:133751ms step_avg:61.19ms
step:2187/2245 train_time:133815ms step_avg:61.19ms
step:2188/2245 train_time:133876ms step_avg:61.19ms
step:2189/2245 train_time:133939ms step_avg:61.19ms
step:2190/2245 train_time:133999ms step_avg:61.19ms
step:2191/2245 train_time:134062ms step_avg:61.19ms
step:2192/2245 train_time:134122ms step_avg:61.19ms
step:2193/2245 train_time:134184ms step_avg:61.19ms
step:2194/2245 train_time:134245ms step_avg:61.19ms
step:2195/2245 train_time:134307ms step_avg:61.19ms
step:2196/2245 train_time:134368ms step_avg:61.19ms
step:2197/2245 train_time:134431ms step_avg:61.19ms
step:2198/2245 train_time:134491ms step_avg:61.19ms
step:2199/2245 train_time:134555ms step_avg:61.19ms
step:2200/2245 train_time:134616ms step_avg:61.19ms
step:2201/2245 train_time:134680ms step_avg:61.19ms
step:2202/2245 train_time:134739ms step_avg:61.19ms
step:2203/2245 train_time:134802ms step_avg:61.19ms
step:2204/2245 train_time:134863ms step_avg:61.19ms
step:2205/2245 train_time:134925ms step_avg:61.19ms
step:2206/2245 train_time:134986ms step_avg:61.19ms
step:2207/2245 train_time:135048ms step_avg:61.19ms
step:2208/2245 train_time:135109ms step_avg:61.19ms
step:2209/2245 train_time:135173ms step_avg:61.19ms
step:2210/2245 train_time:135234ms step_avg:61.19ms
step:2211/2245 train_time:135297ms step_avg:61.19ms
step:2212/2245 train_time:135357ms step_avg:61.19ms
step:2213/2245 train_time:135420ms step_avg:61.19ms
step:2214/2245 train_time:135480ms step_avg:61.19ms
step:2215/2245 train_time:135543ms step_avg:61.19ms
step:2216/2245 train_time:135603ms step_avg:61.19ms
step:2217/2245 train_time:135667ms step_avg:61.19ms
step:2218/2245 train_time:135727ms step_avg:61.19ms
step:2219/2245 train_time:135790ms step_avg:61.19ms
step:2220/2245 train_time:135851ms step_avg:61.19ms
step:2221/2245 train_time:135915ms step_avg:61.20ms
step:2222/2245 train_time:135975ms step_avg:61.19ms
step:2223/2245 train_time:136037ms step_avg:61.20ms
step:2224/2245 train_time:136097ms step_avg:61.19ms
step:2225/2245 train_time:136160ms step_avg:61.20ms
step:2226/2245 train_time:136220ms step_avg:61.19ms
step:2227/2245 train_time:136283ms step_avg:61.20ms
step:2228/2245 train_time:136343ms step_avg:61.20ms
step:2229/2245 train_time:136406ms step_avg:61.20ms
step:2230/2245 train_time:136467ms step_avg:61.20ms
step:2231/2245 train_time:136530ms step_avg:61.20ms
step:2232/2245 train_time:136591ms step_avg:61.20ms
step:2233/2245 train_time:136654ms step_avg:61.20ms
step:2234/2245 train_time:136714ms step_avg:61.20ms
step:2235/2245 train_time:136777ms step_avg:61.20ms
step:2236/2245 train_time:136838ms step_avg:61.20ms
step:2237/2245 train_time:136900ms step_avg:61.20ms
step:2238/2245 train_time:136961ms step_avg:61.20ms
step:2239/2245 train_time:137023ms step_avg:61.20ms
step:2240/2245 train_time:137084ms step_avg:61.20ms
step:2241/2245 train_time:137147ms step_avg:61.20ms
step:2242/2245 train_time:137208ms step_avg:61.20ms
step:2243/2245 train_time:137271ms step_avg:61.20ms
step:2244/2245 train_time:137332ms step_avg:61.20ms
step:2245/2245 train_time:137396ms step_avg:61.20ms
step:2245/2245 val_loss:3.2785 train_time:137456ms step_avg:61.23ms
peak memory allocated: 29249 MiB reserved: 50528 MiB

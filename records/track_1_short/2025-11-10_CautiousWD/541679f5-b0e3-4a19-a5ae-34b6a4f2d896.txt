import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Nov 10 22:09:48 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   41C    P0            131W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   33C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   40C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   34C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   40C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   33C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2245 train_time:126ms step_avg:125.63ms
step:2/2245 train_time:147ms step_avg:73.70ms
step:3/2245 train_time:185ms step_avg:61.74ms
step:4/2245 train_time:241ms step_avg:60.36ms
step:5/2245 train_time:301ms step_avg:60.18ms
step:6/2245 train_time:359ms step_avg:59.90ms
step:7/2245 train_time:420ms step_avg:60.00ms
step:8/2245 train_time:478ms step_avg:59.80ms
step:9/2245 train_time:540ms step_avg:59.97ms
step:10/2245 train_time:598ms step_avg:59.84ms
step:11/2245 train_time:660ms step_avg:59.97ms
step:12/2245 train_time:718ms step_avg:59.84ms
step:13/2245 train_time:779ms step_avg:59.95ms
step:14/2245 train_time:838ms step_avg:59.85ms
step:15/2245 train_time:899ms step_avg:59.94ms
step:16/2245 train_time:958ms step_avg:59.89ms
step:17/2245 train_time:1024ms step_avg:60.22ms
step:18/2245 train_time:1087ms step_avg:60.37ms
step:19/2245 train_time:1152ms step_avg:60.61ms
step:20/2245 train_time:1211ms step_avg:60.57ms
step:21/2245 train_time:1274ms step_avg:60.66ms
step:22/2245 train_time:1334ms step_avg:60.63ms
step:23/2245 train_time:1396ms step_avg:60.69ms
step:24/2245 train_time:1456ms step_avg:60.65ms
step:25/2245 train_time:1518ms step_avg:60.71ms
step:26/2245 train_time:1577ms step_avg:60.63ms
step:27/2245 train_time:1638ms step_avg:60.67ms
step:28/2245 train_time:1697ms step_avg:60.60ms
step:29/2245 train_time:1758ms step_avg:60.62ms
step:30/2245 train_time:1816ms step_avg:60.55ms
step:31/2245 train_time:1877ms step_avg:60.56ms
step:32/2245 train_time:1936ms step_avg:60.51ms
step:33/2245 train_time:1999ms step_avg:60.58ms
step:34/2245 train_time:2058ms step_avg:60.54ms
step:35/2245 train_time:2121ms step_avg:60.60ms
step:36/2245 train_time:2180ms step_avg:60.56ms
step:37/2245 train_time:2244ms step_avg:60.64ms
step:38/2245 train_time:2303ms step_avg:60.62ms
step:39/2245 train_time:2366ms step_avg:60.66ms
step:40/2245 train_time:2425ms step_avg:60.63ms
step:41/2245 train_time:2487ms step_avg:60.66ms
step:42/2245 train_time:2546ms step_avg:60.63ms
step:43/2245 train_time:2608ms step_avg:60.66ms
step:44/2245 train_time:2668ms step_avg:60.63ms
step:45/2245 train_time:2730ms step_avg:60.66ms
step:46/2245 train_time:2789ms step_avg:60.63ms
step:47/2245 train_time:2850ms step_avg:60.65ms
step:48/2245 train_time:2910ms step_avg:60.63ms
step:49/2245 train_time:2972ms step_avg:60.66ms
step:50/2245 train_time:3032ms step_avg:60.64ms
step:51/2245 train_time:3095ms step_avg:60.68ms
step:52/2245 train_time:3155ms step_avg:60.67ms
step:53/2245 train_time:3217ms step_avg:60.71ms
step:54/2245 train_time:3277ms step_avg:60.68ms
step:55/2245 train_time:3339ms step_avg:60.70ms
step:56/2245 train_time:3397ms step_avg:60.67ms
step:57/2245 train_time:3459ms step_avg:60.69ms
step:58/2245 train_time:3518ms step_avg:60.66ms
step:59/2245 train_time:3579ms step_avg:60.66ms
step:60/2245 train_time:3638ms step_avg:60.64ms
step:61/2245 train_time:3700ms step_avg:60.66ms
step:62/2245 train_time:3760ms step_avg:60.64ms
step:63/2245 train_time:3821ms step_avg:60.65ms
step:64/2245 train_time:3880ms step_avg:60.63ms
step:65/2245 train_time:3942ms step_avg:60.65ms
step:66/2245 train_time:4002ms step_avg:60.63ms
step:67/2245 train_time:4064ms step_avg:60.66ms
step:68/2245 train_time:4123ms step_avg:60.64ms
step:69/2245 train_time:4186ms step_avg:60.66ms
step:70/2245 train_time:4245ms step_avg:60.65ms
step:71/2245 train_time:4307ms step_avg:60.66ms
step:72/2245 train_time:4367ms step_avg:60.65ms
step:73/2245 train_time:4428ms step_avg:60.66ms
step:74/2245 train_time:4488ms step_avg:60.64ms
step:75/2245 train_time:4550ms step_avg:60.66ms
step:76/2245 train_time:4609ms step_avg:60.65ms
step:77/2245 train_time:4670ms step_avg:60.66ms
step:78/2245 train_time:4729ms step_avg:60.63ms
step:79/2245 train_time:4791ms step_avg:60.65ms
step:80/2245 train_time:4852ms step_avg:60.65ms
step:81/2245 train_time:4914ms step_avg:60.67ms
step:82/2245 train_time:4974ms step_avg:60.65ms
step:83/2245 train_time:5036ms step_avg:60.67ms
step:84/2245 train_time:5095ms step_avg:60.66ms
step:85/2245 train_time:5157ms step_avg:60.67ms
step:86/2245 train_time:5216ms step_avg:60.65ms
step:87/2245 train_time:5278ms step_avg:60.66ms
step:88/2245 train_time:5337ms step_avg:60.64ms
step:89/2245 train_time:5398ms step_avg:60.65ms
step:90/2245 train_time:5457ms step_avg:60.63ms
step:91/2245 train_time:5518ms step_avg:60.63ms
step:92/2245 train_time:5576ms step_avg:60.61ms
step:93/2245 train_time:5638ms step_avg:60.62ms
step:94/2245 train_time:5697ms step_avg:60.60ms
step:95/2245 train_time:5758ms step_avg:60.61ms
step:96/2245 train_time:5816ms step_avg:60.59ms
step:97/2245 train_time:5878ms step_avg:60.60ms
step:98/2245 train_time:5938ms step_avg:60.59ms
step:99/2245 train_time:6000ms step_avg:60.61ms
step:100/2245 train_time:6059ms step_avg:60.59ms
step:101/2245 train_time:6121ms step_avg:60.60ms
step:102/2245 train_time:6180ms step_avg:60.58ms
step:103/2245 train_time:6241ms step_avg:60.59ms
step:104/2245 train_time:6300ms step_avg:60.58ms
step:105/2245 train_time:6361ms step_avg:60.59ms
step:106/2245 train_time:6420ms step_avg:60.57ms
step:107/2245 train_time:6482ms step_avg:60.58ms
step:108/2245 train_time:6541ms step_avg:60.56ms
step:109/2245 train_time:6602ms step_avg:60.57ms
step:110/2245 train_time:6661ms step_avg:60.55ms
step:111/2245 train_time:6722ms step_avg:60.56ms
step:112/2245 train_time:6781ms step_avg:60.55ms
step:113/2245 train_time:6843ms step_avg:60.56ms
step:114/2245 train_time:6902ms step_avg:60.55ms
step:115/2245 train_time:6965ms step_avg:60.57ms
step:116/2245 train_time:7024ms step_avg:60.55ms
step:117/2245 train_time:7085ms step_avg:60.56ms
step:118/2245 train_time:7145ms step_avg:60.55ms
step:119/2245 train_time:7206ms step_avg:60.56ms
step:120/2245 train_time:7265ms step_avg:60.55ms
step:121/2245 train_time:7326ms step_avg:60.55ms
step:122/2245 train_time:7385ms step_avg:60.53ms
step:123/2245 train_time:7447ms step_avg:60.54ms
step:124/2245 train_time:7505ms step_avg:60.53ms
step:125/2245 train_time:7568ms step_avg:60.54ms
step:126/2245 train_time:7627ms step_avg:60.53ms
step:127/2245 train_time:7688ms step_avg:60.54ms
step:128/2245 train_time:7748ms step_avg:60.53ms
step:129/2245 train_time:7811ms step_avg:60.55ms
step:130/2245 train_time:7870ms step_avg:60.54ms
step:131/2245 train_time:7932ms step_avg:60.55ms
step:132/2245 train_time:7991ms step_avg:60.54ms
step:133/2245 train_time:8053ms step_avg:60.55ms
step:134/2245 train_time:8113ms step_avg:60.55ms
step:135/2245 train_time:8175ms step_avg:60.56ms
step:136/2245 train_time:8234ms step_avg:60.54ms
step:137/2245 train_time:8296ms step_avg:60.55ms
step:138/2245 train_time:8355ms step_avg:60.54ms
step:139/2245 train_time:8416ms step_avg:60.55ms
step:140/2245 train_time:8475ms step_avg:60.54ms
step:141/2245 train_time:8537ms step_avg:60.55ms
step:142/2245 train_time:8596ms step_avg:60.53ms
step:143/2245 train_time:8657ms step_avg:60.54ms
step:144/2245 train_time:8716ms step_avg:60.53ms
step:145/2245 train_time:8777ms step_avg:60.53ms
step:146/2245 train_time:8836ms step_avg:60.52ms
step:147/2245 train_time:8898ms step_avg:60.53ms
step:148/2245 train_time:8957ms step_avg:60.52ms
step:149/2245 train_time:9019ms step_avg:60.53ms
step:150/2245 train_time:9077ms step_avg:60.51ms
step:151/2245 train_time:9139ms step_avg:60.52ms
step:152/2245 train_time:9198ms step_avg:60.51ms
step:153/2245 train_time:9260ms step_avg:60.52ms
step:154/2245 train_time:9318ms step_avg:60.51ms
step:155/2245 train_time:9379ms step_avg:60.51ms
step:156/2245 train_time:9438ms step_avg:60.50ms
step:157/2245 train_time:9499ms step_avg:60.50ms
step:158/2245 train_time:9558ms step_avg:60.49ms
step:159/2245 train_time:9618ms step_avg:60.49ms
step:160/2245 train_time:9677ms step_avg:60.48ms
step:161/2245 train_time:9739ms step_avg:60.49ms
step:162/2245 train_time:9798ms step_avg:60.48ms
step:163/2245 train_time:9858ms step_avg:60.48ms
step:164/2245 train_time:9917ms step_avg:60.47ms
step:165/2245 train_time:9978ms step_avg:60.47ms
step:166/2245 train_time:10037ms step_avg:60.46ms
step:167/2245 train_time:10098ms step_avg:60.47ms
step:168/2245 train_time:10157ms step_avg:60.46ms
step:169/2245 train_time:10218ms step_avg:60.46ms
step:170/2245 train_time:10277ms step_avg:60.45ms
step:171/2245 train_time:10339ms step_avg:60.46ms
step:172/2245 train_time:10398ms step_avg:60.45ms
step:173/2245 train_time:10459ms step_avg:60.46ms
step:174/2245 train_time:10517ms step_avg:60.44ms
step:175/2245 train_time:10579ms step_avg:60.45ms
step:176/2245 train_time:10637ms step_avg:60.44ms
step:177/2245 train_time:10699ms step_avg:60.44ms
step:178/2245 train_time:10757ms step_avg:60.43ms
step:179/2245 train_time:10818ms step_avg:60.44ms
step:180/2245 train_time:10877ms step_avg:60.43ms
step:181/2245 train_time:10938ms step_avg:60.43ms
step:182/2245 train_time:10997ms step_avg:60.42ms
step:183/2245 train_time:11058ms step_avg:60.43ms
step:184/2245 train_time:11117ms step_avg:60.42ms
step:185/2245 train_time:11178ms step_avg:60.42ms
step:186/2245 train_time:11237ms step_avg:60.42ms
step:187/2245 train_time:11299ms step_avg:60.42ms
step:188/2245 train_time:11358ms step_avg:60.41ms
step:189/2245 train_time:11419ms step_avg:60.42ms
step:190/2245 train_time:11478ms step_avg:60.41ms
step:191/2245 train_time:11539ms step_avg:60.41ms
step:192/2245 train_time:11598ms step_avg:60.40ms
step:193/2245 train_time:11659ms step_avg:60.41ms
step:194/2245 train_time:11717ms step_avg:60.40ms
step:195/2245 train_time:11778ms step_avg:60.40ms
step:196/2245 train_time:11837ms step_avg:60.39ms
step:197/2245 train_time:11898ms step_avg:60.40ms
step:198/2245 train_time:11957ms step_avg:60.39ms
step:199/2245 train_time:12018ms step_avg:60.39ms
step:200/2245 train_time:12076ms step_avg:60.38ms
step:201/2245 train_time:12138ms step_avg:60.39ms
step:202/2245 train_time:12197ms step_avg:60.38ms
step:203/2245 train_time:12258ms step_avg:60.39ms
step:204/2245 train_time:12317ms step_avg:60.38ms
step:205/2245 train_time:12377ms step_avg:60.38ms
step:206/2245 train_time:12436ms step_avg:60.37ms
step:207/2245 train_time:12497ms step_avg:60.37ms
step:208/2245 train_time:12556ms step_avg:60.37ms
step:209/2245 train_time:12617ms step_avg:60.37ms
step:210/2245 train_time:12676ms step_avg:60.36ms
step:211/2245 train_time:12737ms step_avg:60.37ms
step:212/2245 train_time:12796ms step_avg:60.36ms
step:213/2245 train_time:12857ms step_avg:60.36ms
step:214/2245 train_time:12916ms step_avg:60.36ms
step:215/2245 train_time:12977ms step_avg:60.36ms
step:216/2245 train_time:13036ms step_avg:60.35ms
step:217/2245 train_time:13097ms step_avg:60.36ms
step:218/2245 train_time:13156ms step_avg:60.35ms
step:219/2245 train_time:13217ms step_avg:60.35ms
step:220/2245 train_time:13276ms step_avg:60.35ms
step:221/2245 train_time:13337ms step_avg:60.35ms
step:222/2245 train_time:13396ms step_avg:60.34ms
step:223/2245 train_time:13458ms step_avg:60.35ms
step:224/2245 train_time:13517ms step_avg:60.34ms
step:225/2245 train_time:13577ms step_avg:60.34ms
step:226/2245 train_time:13636ms step_avg:60.34ms
step:227/2245 train_time:13697ms step_avg:60.34ms
step:228/2245 train_time:13756ms step_avg:60.33ms
step:229/2245 train_time:13818ms step_avg:60.34ms
step:230/2245 train_time:13876ms step_avg:60.33ms
step:231/2245 train_time:13938ms step_avg:60.34ms
step:232/2245 train_time:13996ms step_avg:60.33ms
step:233/2245 train_time:14058ms step_avg:60.33ms
step:234/2245 train_time:14116ms step_avg:60.33ms
step:235/2245 train_time:14177ms step_avg:60.33ms
step:236/2245 train_time:14236ms step_avg:60.32ms
step:237/2245 train_time:14297ms step_avg:60.33ms
step:238/2245 train_time:14356ms step_avg:60.32ms
step:239/2245 train_time:14418ms step_avg:60.33ms
step:240/2245 train_time:14476ms step_avg:60.32ms
step:241/2245 train_time:14537ms step_avg:60.32ms
step:242/2245 train_time:14595ms step_avg:60.31ms
step:243/2245 train_time:14657ms step_avg:60.32ms
step:244/2245 train_time:14716ms step_avg:60.31ms
step:245/2245 train_time:14777ms step_avg:60.32ms
step:246/2245 train_time:14836ms step_avg:60.31ms
step:247/2245 train_time:14897ms step_avg:60.31ms
step:248/2245 train_time:14957ms step_avg:60.31ms
step:249/2245 train_time:15017ms step_avg:60.31ms
step:250/2245 train_time:15076ms step_avg:60.30ms
step:250/2245 val_loss:4.0860 train_time:15138ms step_avg:60.55ms
step:251/2245 train_time:15158ms step_avg:60.39ms
step:252/2245 train_time:15200ms step_avg:60.32ms
step:253/2245 train_time:15266ms step_avg:60.34ms
step:254/2245 train_time:15330ms step_avg:60.36ms
step:255/2245 train_time:15395ms step_avg:60.37ms
step:256/2245 train_time:15453ms step_avg:60.36ms
step:257/2245 train_time:15515ms step_avg:60.37ms
step:258/2245 train_time:15573ms step_avg:60.36ms
step:259/2245 train_time:15634ms step_avg:60.36ms
step:260/2245 train_time:15692ms step_avg:60.36ms
step:261/2245 train_time:15753ms step_avg:60.36ms
step:262/2245 train_time:15811ms step_avg:60.35ms
step:263/2245 train_time:15872ms step_avg:60.35ms
step:264/2245 train_time:15930ms step_avg:60.34ms
step:265/2245 train_time:15990ms step_avg:60.34ms
step:266/2245 train_time:16048ms step_avg:60.33ms
step:267/2245 train_time:16110ms step_avg:60.34ms
step:268/2245 train_time:16169ms step_avg:60.33ms
step:269/2245 train_time:16232ms step_avg:60.34ms
step:270/2245 train_time:16293ms step_avg:60.34ms
step:271/2245 train_time:16356ms step_avg:60.35ms
step:272/2245 train_time:16416ms step_avg:60.35ms
step:273/2245 train_time:16477ms step_avg:60.36ms
step:274/2245 train_time:16536ms step_avg:60.35ms
step:275/2245 train_time:16597ms step_avg:60.35ms
step:276/2245 train_time:16656ms step_avg:60.35ms
step:277/2245 train_time:16717ms step_avg:60.35ms
step:278/2245 train_time:16775ms step_avg:60.34ms
step:279/2245 train_time:16837ms step_avg:60.35ms
step:280/2245 train_time:16895ms step_avg:60.34ms
step:281/2245 train_time:16957ms step_avg:60.34ms
step:282/2245 train_time:17015ms step_avg:60.34ms
step:283/2245 train_time:17077ms step_avg:60.34ms
step:284/2245 train_time:17137ms step_avg:60.34ms
step:285/2245 train_time:17200ms step_avg:60.35ms
step:286/2245 train_time:17260ms step_avg:60.35ms
step:287/2245 train_time:17323ms step_avg:60.36ms
step:288/2245 train_time:17382ms step_avg:60.35ms
step:289/2245 train_time:17443ms step_avg:60.36ms
step:290/2245 train_time:17502ms step_avg:60.35ms
step:291/2245 train_time:17564ms step_avg:60.36ms
step:292/2245 train_time:17622ms step_avg:60.35ms
step:293/2245 train_time:17683ms step_avg:60.35ms
step:294/2245 train_time:17742ms step_avg:60.35ms
step:295/2245 train_time:17803ms step_avg:60.35ms
step:296/2245 train_time:17861ms step_avg:60.34ms
step:297/2245 train_time:17922ms step_avg:60.34ms
step:298/2245 train_time:17981ms step_avg:60.34ms
step:299/2245 train_time:18042ms step_avg:60.34ms
step:300/2245 train_time:18101ms step_avg:60.34ms
step:301/2245 train_time:18163ms step_avg:60.34ms
step:302/2245 train_time:18223ms step_avg:60.34ms
step:303/2245 train_time:18284ms step_avg:60.34ms
step:304/2245 train_time:18343ms step_avg:60.34ms
step:305/2245 train_time:18405ms step_avg:60.34ms
step:306/2245 train_time:18463ms step_avg:60.34ms
step:307/2245 train_time:18525ms step_avg:60.34ms
step:308/2245 train_time:18584ms step_avg:60.34ms
step:309/2245 train_time:18646ms step_avg:60.34ms
step:310/2245 train_time:18704ms step_avg:60.34ms
step:311/2245 train_time:18764ms step_avg:60.33ms
step:312/2245 train_time:18823ms step_avg:60.33ms
step:313/2245 train_time:18884ms step_avg:60.33ms
step:314/2245 train_time:18942ms step_avg:60.33ms
step:315/2245 train_time:19004ms step_avg:60.33ms
step:316/2245 train_time:19062ms step_avg:60.32ms
step:317/2245 train_time:19124ms step_avg:60.33ms
step:318/2245 train_time:19183ms step_avg:60.32ms
step:319/2245 train_time:19246ms step_avg:60.33ms
step:320/2245 train_time:19305ms step_avg:60.33ms
step:321/2245 train_time:19366ms step_avg:60.33ms
step:322/2245 train_time:19424ms step_avg:60.32ms
step:323/2245 train_time:19485ms step_avg:60.33ms
step:324/2245 train_time:19544ms step_avg:60.32ms
step:325/2245 train_time:19605ms step_avg:60.32ms
step:326/2245 train_time:19664ms step_avg:60.32ms
step:327/2245 train_time:19725ms step_avg:60.32ms
step:328/2245 train_time:19784ms step_avg:60.32ms
step:329/2245 train_time:19845ms step_avg:60.32ms
step:330/2245 train_time:19903ms step_avg:60.31ms
step:331/2245 train_time:19964ms step_avg:60.32ms
step:332/2245 train_time:20023ms step_avg:60.31ms
step:333/2245 train_time:20084ms step_avg:60.31ms
step:334/2245 train_time:20143ms step_avg:60.31ms
step:335/2245 train_time:20205ms step_avg:60.31ms
step:336/2245 train_time:20263ms step_avg:60.31ms
step:337/2245 train_time:20325ms step_avg:60.31ms
step:338/2245 train_time:20384ms step_avg:60.31ms
step:339/2245 train_time:20445ms step_avg:60.31ms
step:340/2245 train_time:20504ms step_avg:60.31ms
step:341/2245 train_time:20565ms step_avg:60.31ms
step:342/2245 train_time:20624ms step_avg:60.30ms
step:343/2245 train_time:20685ms step_avg:60.31ms
step:344/2245 train_time:20743ms step_avg:60.30ms
step:345/2245 train_time:20804ms step_avg:60.30ms
step:346/2245 train_time:20864ms step_avg:60.30ms
step:347/2245 train_time:20926ms step_avg:60.30ms
step:348/2245 train_time:20984ms step_avg:60.30ms
step:349/2245 train_time:21045ms step_avg:60.30ms
step:350/2245 train_time:21104ms step_avg:60.30ms
step:351/2245 train_time:21165ms step_avg:60.30ms
step:352/2245 train_time:21224ms step_avg:60.29ms
step:353/2245 train_time:21285ms step_avg:60.30ms
step:354/2245 train_time:21344ms step_avg:60.30ms
step:355/2245 train_time:21406ms step_avg:60.30ms
step:356/2245 train_time:21465ms step_avg:60.29ms
step:357/2245 train_time:21526ms step_avg:60.30ms
step:358/2245 train_time:21584ms step_avg:60.29ms
step:359/2245 train_time:21645ms step_avg:60.29ms
step:360/2245 train_time:21704ms step_avg:60.29ms
step:361/2245 train_time:21765ms step_avg:60.29ms
step:362/2245 train_time:21823ms step_avg:60.29ms
step:363/2245 train_time:21885ms step_avg:60.29ms
step:364/2245 train_time:21943ms step_avg:60.28ms
step:365/2245 train_time:22004ms step_avg:60.29ms
step:366/2245 train_time:22062ms step_avg:60.28ms
step:367/2245 train_time:22124ms step_avg:60.28ms
step:368/2245 train_time:22183ms step_avg:60.28ms
step:369/2245 train_time:22245ms step_avg:60.28ms
step:370/2245 train_time:22304ms step_avg:60.28ms
step:371/2245 train_time:22365ms step_avg:60.28ms
step:372/2245 train_time:22424ms step_avg:60.28ms
step:373/2245 train_time:22486ms step_avg:60.28ms
step:374/2245 train_time:22544ms step_avg:60.28ms
step:375/2245 train_time:22606ms step_avg:60.28ms
step:376/2245 train_time:22664ms step_avg:60.28ms
step:377/2245 train_time:22725ms step_avg:60.28ms
step:378/2245 train_time:22784ms step_avg:60.28ms
step:379/2245 train_time:22845ms step_avg:60.28ms
step:380/2245 train_time:22904ms step_avg:60.27ms
step:381/2245 train_time:22965ms step_avg:60.28ms
step:382/2245 train_time:23023ms step_avg:60.27ms
step:383/2245 train_time:23084ms step_avg:60.27ms
step:384/2245 train_time:23143ms step_avg:60.27ms
step:385/2245 train_time:23205ms step_avg:60.27ms
step:386/2245 train_time:23263ms step_avg:60.27ms
step:387/2245 train_time:23325ms step_avg:60.27ms
step:388/2245 train_time:23384ms step_avg:60.27ms
step:389/2245 train_time:23445ms step_avg:60.27ms
step:390/2245 train_time:23504ms step_avg:60.27ms
step:391/2245 train_time:23565ms step_avg:60.27ms
step:392/2245 train_time:23624ms step_avg:60.26ms
step:393/2245 train_time:23685ms step_avg:60.27ms
step:394/2245 train_time:23743ms step_avg:60.26ms
step:395/2245 train_time:23806ms step_avg:60.27ms
step:396/2245 train_time:23864ms step_avg:60.26ms
step:397/2245 train_time:23925ms step_avg:60.26ms
step:398/2245 train_time:23984ms step_avg:60.26ms
step:399/2245 train_time:24046ms step_avg:60.27ms
step:400/2245 train_time:24105ms step_avg:60.26ms
step:401/2245 train_time:24165ms step_avg:60.26ms
step:402/2245 train_time:24224ms step_avg:60.26ms
step:403/2245 train_time:24285ms step_avg:60.26ms
step:404/2245 train_time:24344ms step_avg:60.26ms
step:405/2245 train_time:24406ms step_avg:60.26ms
step:406/2245 train_time:24464ms step_avg:60.26ms
step:407/2245 train_time:24526ms step_avg:60.26ms
step:408/2245 train_time:24585ms step_avg:60.26ms
step:409/2245 train_time:24646ms step_avg:60.26ms
step:410/2245 train_time:24705ms step_avg:60.26ms
step:411/2245 train_time:24766ms step_avg:60.26ms
step:412/2245 train_time:24825ms step_avg:60.25ms
step:413/2245 train_time:24886ms step_avg:60.26ms
step:414/2245 train_time:24945ms step_avg:60.25ms
step:415/2245 train_time:25006ms step_avg:60.26ms
step:416/2245 train_time:25064ms step_avg:60.25ms
step:417/2245 train_time:25125ms step_avg:60.25ms
step:418/2245 train_time:25184ms step_avg:60.25ms
step:419/2245 train_time:25245ms step_avg:60.25ms
step:420/2245 train_time:25304ms step_avg:60.25ms
step:421/2245 train_time:25366ms step_avg:60.25ms
step:422/2245 train_time:25424ms step_avg:60.25ms
step:423/2245 train_time:25485ms step_avg:60.25ms
step:424/2245 train_time:25544ms step_avg:60.24ms
step:425/2245 train_time:25605ms step_avg:60.25ms
step:426/2245 train_time:25664ms step_avg:60.24ms
step:427/2245 train_time:25725ms step_avg:60.25ms
step:428/2245 train_time:25784ms step_avg:60.24ms
step:429/2245 train_time:25845ms step_avg:60.25ms
step:430/2245 train_time:25904ms step_avg:60.24ms
step:431/2245 train_time:25965ms step_avg:60.24ms
step:432/2245 train_time:26023ms step_avg:60.24ms
step:433/2245 train_time:26084ms step_avg:60.24ms
step:434/2245 train_time:26143ms step_avg:60.24ms
step:435/2245 train_time:26205ms step_avg:60.24ms
step:436/2245 train_time:26264ms step_avg:60.24ms
step:437/2245 train_time:26325ms step_avg:60.24ms
step:438/2245 train_time:26384ms step_avg:60.24ms
step:439/2245 train_time:26446ms step_avg:60.24ms
step:440/2245 train_time:26504ms step_avg:60.24ms
step:441/2245 train_time:26565ms step_avg:60.24ms
step:442/2245 train_time:26623ms step_avg:60.23ms
step:443/2245 train_time:26685ms step_avg:60.24ms
step:444/2245 train_time:26743ms step_avg:60.23ms
step:445/2245 train_time:26805ms step_avg:60.23ms
step:446/2245 train_time:26863ms step_avg:60.23ms
step:447/2245 train_time:26925ms step_avg:60.23ms
step:448/2245 train_time:26983ms step_avg:60.23ms
step:449/2245 train_time:27045ms step_avg:60.23ms
step:450/2245 train_time:27103ms step_avg:60.23ms
step:451/2245 train_time:27165ms step_avg:60.23ms
step:452/2245 train_time:27223ms step_avg:60.23ms
step:453/2245 train_time:27284ms step_avg:60.23ms
step:454/2245 train_time:27343ms step_avg:60.23ms
step:455/2245 train_time:27404ms step_avg:60.23ms
step:456/2245 train_time:27463ms step_avg:60.23ms
step:457/2245 train_time:27524ms step_avg:60.23ms
step:458/2245 train_time:27583ms step_avg:60.23ms
step:459/2245 train_time:27645ms step_avg:60.23ms
step:460/2245 train_time:27704ms step_avg:60.23ms
step:461/2245 train_time:27765ms step_avg:60.23ms
step:462/2245 train_time:27823ms step_avg:60.22ms
step:463/2245 train_time:27885ms step_avg:60.23ms
step:464/2245 train_time:27944ms step_avg:60.22ms
step:465/2245 train_time:28006ms step_avg:60.23ms
step:466/2245 train_time:28065ms step_avg:60.22ms
step:467/2245 train_time:28126ms step_avg:60.23ms
step:468/2245 train_time:28184ms step_avg:60.22ms
step:469/2245 train_time:28246ms step_avg:60.23ms
step:470/2245 train_time:28305ms step_avg:60.22ms
step:471/2245 train_time:28366ms step_avg:60.22ms
step:472/2245 train_time:28424ms step_avg:60.22ms
step:473/2245 train_time:28486ms step_avg:60.22ms
step:474/2245 train_time:28544ms step_avg:60.22ms
step:475/2245 train_time:28606ms step_avg:60.22ms
step:476/2245 train_time:28664ms step_avg:60.22ms
step:477/2245 train_time:28725ms step_avg:60.22ms
step:478/2245 train_time:28784ms step_avg:60.22ms
step:479/2245 train_time:28845ms step_avg:60.22ms
step:480/2245 train_time:28904ms step_avg:60.22ms
step:481/2245 train_time:28966ms step_avg:60.22ms
step:482/2245 train_time:29024ms step_avg:60.22ms
step:483/2245 train_time:29085ms step_avg:60.22ms
step:484/2245 train_time:29144ms step_avg:60.21ms
step:485/2245 train_time:29205ms step_avg:60.22ms
step:486/2245 train_time:29263ms step_avg:60.21ms
step:487/2245 train_time:29324ms step_avg:60.21ms
step:488/2245 train_time:29383ms step_avg:60.21ms
step:489/2245 train_time:29444ms step_avg:60.21ms
step:490/2245 train_time:29503ms step_avg:60.21ms
step:491/2245 train_time:29564ms step_avg:60.21ms
step:492/2245 train_time:29623ms step_avg:60.21ms
step:493/2245 train_time:29684ms step_avg:60.21ms
step:494/2245 train_time:29743ms step_avg:60.21ms
step:495/2245 train_time:29804ms step_avg:60.21ms
step:496/2245 train_time:29863ms step_avg:60.21ms
step:497/2245 train_time:29924ms step_avg:60.21ms
step:498/2245 train_time:29983ms step_avg:60.21ms
step:499/2245 train_time:30045ms step_avg:60.21ms
step:500/2245 train_time:30104ms step_avg:60.21ms
step:500/2245 val_loss:3.8269 train_time:30166ms step_avg:60.33ms
step:501/2245 train_time:30184ms step_avg:60.25ms
step:502/2245 train_time:30227ms step_avg:60.21ms
step:503/2245 train_time:30294ms step_avg:60.23ms
step:504/2245 train_time:30355ms step_avg:60.23ms
step:505/2245 train_time:30416ms step_avg:60.23ms
step:506/2245 train_time:30476ms step_avg:60.23ms
step:507/2245 train_time:30536ms step_avg:60.23ms
step:508/2245 train_time:30594ms step_avg:60.22ms
step:509/2245 train_time:30656ms step_avg:60.23ms
step:510/2245 train_time:30714ms step_avg:60.22ms
step:511/2245 train_time:30775ms step_avg:60.22ms
step:512/2245 train_time:30833ms step_avg:60.22ms
step:513/2245 train_time:30894ms step_avg:60.22ms
step:514/2245 train_time:30952ms step_avg:60.22ms
step:515/2245 train_time:31012ms step_avg:60.22ms
step:516/2245 train_time:31071ms step_avg:60.22ms
step:517/2245 train_time:31134ms step_avg:60.22ms
step:518/2245 train_time:31194ms step_avg:60.22ms
step:519/2245 train_time:31258ms step_avg:60.23ms
step:520/2245 train_time:31318ms step_avg:60.23ms
step:521/2245 train_time:31380ms step_avg:60.23ms
step:522/2245 train_time:31440ms step_avg:60.23ms
step:523/2245 train_time:31501ms step_avg:60.23ms
step:524/2245 train_time:31560ms step_avg:60.23ms
step:525/2245 train_time:31621ms step_avg:60.23ms
step:526/2245 train_time:31680ms step_avg:60.23ms
step:527/2245 train_time:31741ms step_avg:60.23ms
step:528/2245 train_time:31800ms step_avg:60.23ms
step:529/2245 train_time:31862ms step_avg:60.23ms
step:530/2245 train_time:31921ms step_avg:60.23ms
step:531/2245 train_time:31982ms step_avg:60.23ms
step:532/2245 train_time:32041ms step_avg:60.23ms
step:533/2245 train_time:32103ms step_avg:60.23ms
step:534/2245 train_time:32163ms step_avg:60.23ms
step:535/2245 train_time:32225ms step_avg:60.23ms
step:536/2245 train_time:32285ms step_avg:60.23ms
step:537/2245 train_time:32348ms step_avg:60.24ms
step:538/2245 train_time:32408ms step_avg:60.24ms
step:539/2245 train_time:32470ms step_avg:60.24ms
step:540/2245 train_time:32529ms step_avg:60.24ms
step:541/2245 train_time:32591ms step_avg:60.24ms
step:542/2245 train_time:32650ms step_avg:60.24ms
step:543/2245 train_time:32710ms step_avg:60.24ms
step:544/2245 train_time:32769ms step_avg:60.24ms
step:545/2245 train_time:32830ms step_avg:60.24ms
step:546/2245 train_time:32889ms step_avg:60.24ms
step:547/2245 train_time:32950ms step_avg:60.24ms
step:548/2245 train_time:33009ms step_avg:60.23ms
step:549/2245 train_time:33070ms step_avg:60.24ms
step:550/2245 train_time:33129ms step_avg:60.23ms
step:551/2245 train_time:33191ms step_avg:60.24ms
step:552/2245 train_time:33250ms step_avg:60.24ms
step:553/2245 train_time:33313ms step_avg:60.24ms
step:554/2245 train_time:33371ms step_avg:60.24ms
step:555/2245 train_time:33432ms step_avg:60.24ms
step:556/2245 train_time:33491ms step_avg:60.24ms
step:557/2245 train_time:33553ms step_avg:60.24ms
step:558/2245 train_time:33611ms step_avg:60.23ms
step:559/2245 train_time:33672ms step_avg:60.24ms
step:560/2245 train_time:33731ms step_avg:60.23ms
step:561/2245 train_time:33792ms step_avg:60.24ms
step:562/2245 train_time:33850ms step_avg:60.23ms
step:563/2245 train_time:33911ms step_avg:60.23ms
step:564/2245 train_time:33970ms step_avg:60.23ms
step:565/2245 train_time:34031ms step_avg:60.23ms
step:566/2245 train_time:34089ms step_avg:60.23ms
step:567/2245 train_time:34151ms step_avg:60.23ms
step:568/2245 train_time:34210ms step_avg:60.23ms
step:569/2245 train_time:34271ms step_avg:60.23ms
step:570/2245 train_time:34330ms step_avg:60.23ms
step:571/2245 train_time:34392ms step_avg:60.23ms
step:572/2245 train_time:34451ms step_avg:60.23ms
step:573/2245 train_time:34512ms step_avg:60.23ms
step:574/2245 train_time:34571ms step_avg:60.23ms
step:575/2245 train_time:34632ms step_avg:60.23ms
step:576/2245 train_time:34691ms step_avg:60.23ms
step:577/2245 train_time:34752ms step_avg:60.23ms
step:578/2245 train_time:34810ms step_avg:60.23ms
step:579/2245 train_time:34872ms step_avg:60.23ms
step:580/2245 train_time:34930ms step_avg:60.22ms
step:581/2245 train_time:34991ms step_avg:60.23ms
step:582/2245 train_time:35050ms step_avg:60.22ms
step:583/2245 train_time:35112ms step_avg:60.23ms
step:584/2245 train_time:35170ms step_avg:60.22ms
step:585/2245 train_time:35231ms step_avg:60.22ms
step:586/2245 train_time:35290ms step_avg:60.22ms
step:587/2245 train_time:35351ms step_avg:60.22ms
step:588/2245 train_time:35410ms step_avg:60.22ms
step:589/2245 train_time:35472ms step_avg:60.22ms
step:590/2245 train_time:35530ms step_avg:60.22ms
step:591/2245 train_time:35591ms step_avg:60.22ms
step:592/2245 train_time:35650ms step_avg:60.22ms
step:593/2245 train_time:35712ms step_avg:60.22ms
step:594/2245 train_time:35771ms step_avg:60.22ms
step:595/2245 train_time:35832ms step_avg:60.22ms
step:596/2245 train_time:35890ms step_avg:60.22ms
step:597/2245 train_time:35951ms step_avg:60.22ms
step:598/2245 train_time:36010ms step_avg:60.22ms
step:599/2245 train_time:36071ms step_avg:60.22ms
step:600/2245 train_time:36130ms step_avg:60.22ms
step:601/2245 train_time:36191ms step_avg:60.22ms
step:602/2245 train_time:36250ms step_avg:60.22ms
step:603/2245 train_time:36312ms step_avg:60.22ms
step:604/2245 train_time:36371ms step_avg:60.22ms
step:605/2245 train_time:36432ms step_avg:60.22ms
step:606/2245 train_time:36491ms step_avg:60.22ms
step:607/2245 train_time:36553ms step_avg:60.22ms
step:608/2245 train_time:36612ms step_avg:60.22ms
step:609/2245 train_time:36673ms step_avg:60.22ms
step:610/2245 train_time:36732ms step_avg:60.22ms
step:611/2245 train_time:36793ms step_avg:60.22ms
step:612/2245 train_time:36852ms step_avg:60.22ms
step:613/2245 train_time:36913ms step_avg:60.22ms
step:614/2245 train_time:36972ms step_avg:60.21ms
step:615/2245 train_time:37033ms step_avg:60.22ms
step:616/2245 train_time:37092ms step_avg:60.21ms
step:617/2245 train_time:37153ms step_avg:60.22ms
step:618/2245 train_time:37212ms step_avg:60.21ms
step:619/2245 train_time:37274ms step_avg:60.22ms
step:620/2245 train_time:37333ms step_avg:60.21ms
step:621/2245 train_time:37394ms step_avg:60.22ms
step:622/2245 train_time:37453ms step_avg:60.21ms
step:623/2245 train_time:37514ms step_avg:60.22ms
step:624/2245 train_time:37573ms step_avg:60.21ms
step:625/2245 train_time:37634ms step_avg:60.22ms
step:626/2245 train_time:37693ms step_avg:60.21ms
step:627/2245 train_time:37755ms step_avg:60.22ms
step:628/2245 train_time:37814ms step_avg:60.21ms
step:629/2245 train_time:37875ms step_avg:60.22ms
step:630/2245 train_time:37934ms step_avg:60.21ms
step:631/2245 train_time:37996ms step_avg:60.21ms
step:632/2245 train_time:38054ms step_avg:60.21ms
step:633/2245 train_time:38116ms step_avg:60.21ms
step:634/2245 train_time:38175ms step_avg:60.21ms
step:635/2245 train_time:38237ms step_avg:60.22ms
step:636/2245 train_time:38296ms step_avg:60.21ms
step:637/2245 train_time:38358ms step_avg:60.22ms
step:638/2245 train_time:38416ms step_avg:60.21ms
step:639/2245 train_time:38478ms step_avg:60.22ms
step:640/2245 train_time:38537ms step_avg:60.21ms
step:641/2245 train_time:38598ms step_avg:60.22ms
step:642/2245 train_time:38657ms step_avg:60.21ms
step:643/2245 train_time:38719ms step_avg:60.22ms
step:644/2245 train_time:38778ms step_avg:60.21ms
step:645/2245 train_time:38839ms step_avg:60.22ms
step:646/2245 train_time:38898ms step_avg:60.21ms
step:647/2245 train_time:38960ms step_avg:60.22ms
step:648/2245 train_time:39018ms step_avg:60.21ms
step:649/2245 train_time:39080ms step_avg:60.22ms
step:650/2245 train_time:39139ms step_avg:60.21ms
step:651/2245 train_time:39201ms step_avg:60.22ms
step:652/2245 train_time:39260ms step_avg:60.21ms
step:653/2245 train_time:39322ms step_avg:60.22ms
step:654/2245 train_time:39381ms step_avg:60.22ms
step:655/2245 train_time:39443ms step_avg:60.22ms
step:656/2245 train_time:39503ms step_avg:60.22ms
step:657/2245 train_time:39565ms step_avg:60.22ms
step:658/2245 train_time:39624ms step_avg:60.22ms
step:659/2245 train_time:39686ms step_avg:60.22ms
step:660/2245 train_time:39746ms step_avg:60.22ms
step:661/2245 train_time:39808ms step_avg:60.22ms
step:662/2245 train_time:39867ms step_avg:60.22ms
step:663/2245 train_time:39928ms step_avg:60.22ms
step:664/2245 train_time:39988ms step_avg:60.22ms
step:665/2245 train_time:40049ms step_avg:60.22ms
step:666/2245 train_time:40108ms step_avg:60.22ms
step:667/2245 train_time:40170ms step_avg:60.22ms
step:668/2245 train_time:40229ms step_avg:60.22ms
step:669/2245 train_time:40290ms step_avg:60.22ms
step:670/2245 train_time:40349ms step_avg:60.22ms
step:671/2245 train_time:40411ms step_avg:60.23ms
step:672/2245 train_time:40470ms step_avg:60.22ms
step:673/2245 train_time:40531ms step_avg:60.22ms
step:674/2245 train_time:40590ms step_avg:60.22ms
step:675/2245 train_time:40651ms step_avg:60.22ms
step:676/2245 train_time:40710ms step_avg:60.22ms
step:677/2245 train_time:40771ms step_avg:60.22ms
step:678/2245 train_time:40830ms step_avg:60.22ms
step:679/2245 train_time:40891ms step_avg:60.22ms
step:680/2245 train_time:40950ms step_avg:60.22ms
step:681/2245 train_time:41011ms step_avg:60.22ms
step:682/2245 train_time:41070ms step_avg:60.22ms
step:683/2245 train_time:41131ms step_avg:60.22ms
step:684/2245 train_time:41190ms step_avg:60.22ms
step:685/2245 train_time:41252ms step_avg:60.22ms
step:686/2245 train_time:41311ms step_avg:60.22ms
step:687/2245 train_time:41372ms step_avg:60.22ms
step:688/2245 train_time:41431ms step_avg:60.22ms
step:689/2245 train_time:41492ms step_avg:60.22ms
step:690/2245 train_time:41550ms step_avg:60.22ms
step:691/2245 train_time:41612ms step_avg:60.22ms
step:692/2245 train_time:41670ms step_avg:60.22ms
step:693/2245 train_time:41731ms step_avg:60.22ms
step:694/2245 train_time:41790ms step_avg:60.22ms
step:695/2245 train_time:41852ms step_avg:60.22ms
step:696/2245 train_time:41911ms step_avg:60.22ms
step:697/2245 train_time:41972ms step_avg:60.22ms
step:698/2245 train_time:42031ms step_avg:60.22ms
step:699/2245 train_time:42092ms step_avg:60.22ms
step:700/2245 train_time:42151ms step_avg:60.22ms
step:701/2245 train_time:42213ms step_avg:60.22ms
step:702/2245 train_time:42271ms step_avg:60.22ms
step:703/2245 train_time:42333ms step_avg:60.22ms
step:704/2245 train_time:42392ms step_avg:60.22ms
step:705/2245 train_time:42453ms step_avg:60.22ms
step:706/2245 train_time:42512ms step_avg:60.21ms
step:707/2245 train_time:42573ms step_avg:60.22ms
step:708/2245 train_time:42631ms step_avg:60.21ms
step:709/2245 train_time:42693ms step_avg:60.22ms
step:710/2245 train_time:42752ms step_avg:60.21ms
step:711/2245 train_time:42812ms step_avg:60.21ms
step:712/2245 train_time:42871ms step_avg:60.21ms
step:713/2245 train_time:42933ms step_avg:60.21ms
step:714/2245 train_time:42991ms step_avg:60.21ms
step:715/2245 train_time:43053ms step_avg:60.21ms
step:716/2245 train_time:43112ms step_avg:60.21ms
step:717/2245 train_time:43173ms step_avg:60.21ms
step:718/2245 train_time:43655ms step_avg:60.80ms
step:719/2245 train_time:43715ms step_avg:60.80ms
step:720/2245 train_time:43773ms step_avg:60.80ms
step:721/2245 train_time:43833ms step_avg:60.79ms
step:722/2245 train_time:43891ms step_avg:60.79ms
step:723/2245 train_time:43952ms step_avg:60.79ms
step:724/2245 train_time:44010ms step_avg:60.79ms
step:725/2245 train_time:44070ms step_avg:60.79ms
step:726/2245 train_time:44128ms step_avg:60.78ms
step:727/2245 train_time:44189ms step_avg:60.78ms
step:728/2245 train_time:44247ms step_avg:60.78ms
step:729/2245 train_time:44308ms step_avg:60.78ms
step:730/2245 train_time:44366ms step_avg:60.78ms
step:731/2245 train_time:44427ms step_avg:60.78ms
step:732/2245 train_time:44485ms step_avg:60.77ms
step:733/2245 train_time:44551ms step_avg:60.78ms
step:734/2245 train_time:44613ms step_avg:60.78ms
step:735/2245 train_time:44676ms step_avg:60.78ms
step:736/2245 train_time:44735ms step_avg:60.78ms
step:737/2245 train_time:44797ms step_avg:60.78ms
step:738/2245 train_time:44858ms step_avg:60.78ms
step:739/2245 train_time:44921ms step_avg:60.79ms
step:740/2245 train_time:44980ms step_avg:60.78ms
step:741/2245 train_time:45042ms step_avg:60.79ms
step:742/2245 train_time:45102ms step_avg:60.78ms
step:743/2245 train_time:45163ms step_avg:60.79ms
step:744/2245 train_time:45222ms step_avg:60.78ms
step:745/2245 train_time:45284ms step_avg:60.78ms
step:746/2245 train_time:45343ms step_avg:60.78ms
step:747/2245 train_time:45405ms step_avg:60.78ms
step:748/2245 train_time:45465ms step_avg:60.78ms
step:749/2245 train_time:45529ms step_avg:60.79ms
step:750/2245 train_time:45591ms step_avg:60.79ms
step:750/2245 val_loss:3.6718 train_time:45656ms step_avg:60.87ms
step:751/2245 train_time:45676ms step_avg:60.82ms
step:752/2245 train_time:45716ms step_avg:60.79ms
step:753/2245 train_time:45777ms step_avg:60.79ms
step:754/2245 train_time:45836ms step_avg:60.79ms
step:755/2245 train_time:45898ms step_avg:60.79ms
step:756/2245 train_time:45958ms step_avg:60.79ms
step:757/2245 train_time:46019ms step_avg:60.79ms
step:758/2245 train_time:46078ms step_avg:60.79ms
step:759/2245 train_time:46140ms step_avg:60.79ms
step:760/2245 train_time:46199ms step_avg:60.79ms
step:761/2245 train_time:46260ms step_avg:60.79ms
step:762/2245 train_time:46319ms step_avg:60.79ms
step:763/2245 train_time:46381ms step_avg:60.79ms
step:764/2245 train_time:46440ms step_avg:60.79ms
step:765/2245 train_time:46502ms step_avg:60.79ms
step:766/2245 train_time:46566ms step_avg:60.79ms
step:767/2245 train_time:46633ms step_avg:60.80ms
step:768/2245 train_time:46695ms step_avg:60.80ms
step:769/2245 train_time:46758ms step_avg:60.80ms
step:770/2245 train_time:46817ms step_avg:60.80ms
step:771/2245 train_time:46878ms step_avg:60.80ms
step:772/2245 train_time:46938ms step_avg:60.80ms
step:773/2245 train_time:47000ms step_avg:60.80ms
step:774/2245 train_time:47059ms step_avg:60.80ms
step:775/2245 train_time:47121ms step_avg:60.80ms
step:776/2245 train_time:47180ms step_avg:60.80ms
step:777/2245 train_time:47241ms step_avg:60.80ms
step:778/2245 train_time:47301ms step_avg:60.80ms
step:779/2245 train_time:47362ms step_avg:60.80ms
step:780/2245 train_time:47421ms step_avg:60.80ms
step:781/2245 train_time:47484ms step_avg:60.80ms
step:782/2245 train_time:47546ms step_avg:60.80ms
step:783/2245 train_time:47610ms step_avg:60.80ms
step:784/2245 train_time:47671ms step_avg:60.81ms
step:785/2245 train_time:47735ms step_avg:60.81ms
step:786/2245 train_time:47795ms step_avg:60.81ms
step:787/2245 train_time:47857ms step_avg:60.81ms
step:788/2245 train_time:47916ms step_avg:60.81ms
step:789/2245 train_time:47977ms step_avg:60.81ms
step:790/2245 train_time:48037ms step_avg:60.81ms
step:791/2245 train_time:48098ms step_avg:60.81ms
step:792/2245 train_time:48157ms step_avg:60.80ms
step:793/2245 train_time:48219ms step_avg:60.81ms
step:794/2245 train_time:48278ms step_avg:60.80ms
step:795/2245 train_time:48340ms step_avg:60.80ms
step:796/2245 train_time:48399ms step_avg:60.80ms
step:797/2245 train_time:48462ms step_avg:60.81ms
step:798/2245 train_time:48523ms step_avg:60.81ms
step:799/2245 train_time:48587ms step_avg:60.81ms
step:800/2245 train_time:48648ms step_avg:60.81ms
step:801/2245 train_time:48712ms step_avg:60.81ms
step:802/2245 train_time:48772ms step_avg:60.81ms
step:803/2245 train_time:48833ms step_avg:60.81ms
step:804/2245 train_time:48893ms step_avg:60.81ms
step:805/2245 train_time:48955ms step_avg:60.81ms
step:806/2245 train_time:49014ms step_avg:60.81ms
step:807/2245 train_time:49075ms step_avg:60.81ms
step:808/2245 train_time:49134ms step_avg:60.81ms
step:809/2245 train_time:49196ms step_avg:60.81ms
step:810/2245 train_time:49255ms step_avg:60.81ms
step:811/2245 train_time:49317ms step_avg:60.81ms
step:812/2245 train_time:49376ms step_avg:60.81ms
step:813/2245 train_time:49438ms step_avg:60.81ms
step:814/2245 train_time:49499ms step_avg:60.81ms
step:815/2245 train_time:49562ms step_avg:60.81ms
step:816/2245 train_time:49623ms step_avg:60.81ms
step:817/2245 train_time:49686ms step_avg:60.82ms
step:818/2245 train_time:49746ms step_avg:60.81ms
step:819/2245 train_time:49808ms step_avg:60.82ms
step:820/2245 train_time:49869ms step_avg:60.82ms
step:821/2245 train_time:49932ms step_avg:60.82ms
step:822/2245 train_time:49992ms step_avg:60.82ms
step:823/2245 train_time:50054ms step_avg:60.82ms
step:824/2245 train_time:50114ms step_avg:60.82ms
step:825/2245 train_time:50176ms step_avg:60.82ms
step:826/2245 train_time:50235ms step_avg:60.82ms
step:827/2245 train_time:50297ms step_avg:60.82ms
step:828/2245 train_time:50357ms step_avg:60.82ms
step:829/2245 train_time:50419ms step_avg:60.82ms
step:830/2245 train_time:50478ms step_avg:60.82ms
step:831/2245 train_time:50541ms step_avg:60.82ms
step:832/2245 train_time:50602ms step_avg:60.82ms
step:833/2245 train_time:50666ms step_avg:60.82ms
step:834/2245 train_time:50725ms step_avg:60.82ms
step:835/2245 train_time:50788ms step_avg:60.82ms
step:836/2245 train_time:50849ms step_avg:60.82ms
step:837/2245 train_time:50911ms step_avg:60.83ms
step:838/2245 train_time:50971ms step_avg:60.82ms
step:839/2245 train_time:51034ms step_avg:60.83ms
step:840/2245 train_time:51093ms step_avg:60.83ms
step:841/2245 train_time:51155ms step_avg:60.83ms
step:842/2245 train_time:51215ms step_avg:60.83ms
step:843/2245 train_time:51276ms step_avg:60.83ms
step:844/2245 train_time:51337ms step_avg:60.83ms
step:845/2245 train_time:51399ms step_avg:60.83ms
step:846/2245 train_time:51458ms step_avg:60.83ms
step:847/2245 train_time:51520ms step_avg:60.83ms
step:848/2245 train_time:51580ms step_avg:60.83ms
step:849/2245 train_time:51643ms step_avg:60.83ms
step:850/2245 train_time:51703ms step_avg:60.83ms
step:851/2245 train_time:51766ms step_avg:60.83ms
step:852/2245 train_time:51826ms step_avg:60.83ms
step:853/2245 train_time:51889ms step_avg:60.83ms
step:854/2245 train_time:51950ms step_avg:60.83ms
step:855/2245 train_time:52013ms step_avg:60.83ms
step:856/2245 train_time:52073ms step_avg:60.83ms
step:857/2245 train_time:52135ms step_avg:60.83ms
step:858/2245 train_time:52194ms step_avg:60.83ms
step:859/2245 train_time:52256ms step_avg:60.83ms
step:860/2245 train_time:52316ms step_avg:60.83ms
step:861/2245 train_time:52378ms step_avg:60.83ms
step:862/2245 train_time:52438ms step_avg:60.83ms
step:863/2245 train_time:52500ms step_avg:60.83ms
step:864/2245 train_time:52559ms step_avg:60.83ms
step:865/2245 train_time:52622ms step_avg:60.83ms
step:866/2245 train_time:52682ms step_avg:60.83ms
step:867/2245 train_time:52745ms step_avg:60.84ms
step:868/2245 train_time:52805ms step_avg:60.84ms
step:869/2245 train_time:52868ms step_avg:60.84ms
step:870/2245 train_time:52928ms step_avg:60.84ms
step:871/2245 train_time:52991ms step_avg:60.84ms
step:872/2245 train_time:53051ms step_avg:60.84ms
step:873/2245 train_time:53113ms step_avg:60.84ms
step:874/2245 train_time:53173ms step_avg:60.84ms
step:875/2245 train_time:53235ms step_avg:60.84ms
step:876/2245 train_time:53294ms step_avg:60.84ms
step:877/2245 train_time:53356ms step_avg:60.84ms
step:878/2245 train_time:53417ms step_avg:60.84ms
step:879/2245 train_time:53478ms step_avg:60.84ms
step:880/2245 train_time:53538ms step_avg:60.84ms
step:881/2245 train_time:53599ms step_avg:60.84ms
step:882/2245 train_time:53659ms step_avg:60.84ms
step:883/2245 train_time:53721ms step_avg:60.84ms
step:884/2245 train_time:53781ms step_avg:60.84ms
step:885/2245 train_time:53845ms step_avg:60.84ms
step:886/2245 train_time:53905ms step_avg:60.84ms
step:887/2245 train_time:53968ms step_avg:60.84ms
step:888/2245 train_time:54028ms step_avg:60.84ms
step:889/2245 train_time:54090ms step_avg:60.84ms
step:890/2245 train_time:54151ms step_avg:60.84ms
step:891/2245 train_time:54213ms step_avg:60.85ms
step:892/2245 train_time:54273ms step_avg:60.84ms
step:893/2245 train_time:54334ms step_avg:60.84ms
step:894/2245 train_time:54394ms step_avg:60.84ms
step:895/2245 train_time:54456ms step_avg:60.85ms
step:896/2245 train_time:54516ms step_avg:60.84ms
step:897/2245 train_time:54577ms step_avg:60.84ms
step:898/2245 train_time:54637ms step_avg:60.84ms
step:899/2245 train_time:54699ms step_avg:60.84ms
step:900/2245 train_time:54758ms step_avg:60.84ms
step:901/2245 train_time:54822ms step_avg:60.85ms
step:902/2245 train_time:54882ms step_avg:60.84ms
step:903/2245 train_time:54945ms step_avg:60.85ms
step:904/2245 train_time:55005ms step_avg:60.85ms
step:905/2245 train_time:55068ms step_avg:60.85ms
step:906/2245 train_time:55128ms step_avg:60.85ms
step:907/2245 train_time:55191ms step_avg:60.85ms
step:908/2245 train_time:55252ms step_avg:60.85ms
step:909/2245 train_time:55314ms step_avg:60.85ms
step:910/2245 train_time:55374ms step_avg:60.85ms
step:911/2245 train_time:55436ms step_avg:60.85ms
step:912/2245 train_time:55496ms step_avg:60.85ms
step:913/2245 train_time:55559ms step_avg:60.85ms
step:914/2245 train_time:55618ms step_avg:60.85ms
step:915/2245 train_time:55681ms step_avg:60.85ms
step:916/2245 train_time:55740ms step_avg:60.85ms
step:917/2245 train_time:55802ms step_avg:60.85ms
step:918/2245 train_time:55863ms step_avg:60.85ms
step:919/2245 train_time:55925ms step_avg:60.85ms
step:920/2245 train_time:55985ms step_avg:60.85ms
step:921/2245 train_time:56048ms step_avg:60.86ms
step:922/2245 train_time:56107ms step_avg:60.85ms
step:923/2245 train_time:56170ms step_avg:60.86ms
step:924/2245 train_time:56230ms step_avg:60.86ms
step:925/2245 train_time:56293ms step_avg:60.86ms
step:926/2245 train_time:56353ms step_avg:60.86ms
step:927/2245 train_time:56415ms step_avg:60.86ms
step:928/2245 train_time:56475ms step_avg:60.86ms
step:929/2245 train_time:56537ms step_avg:60.86ms
step:930/2245 train_time:56596ms step_avg:60.86ms
step:931/2245 train_time:56658ms step_avg:60.86ms
step:932/2245 train_time:56717ms step_avg:60.86ms
step:933/2245 train_time:56779ms step_avg:60.86ms
step:934/2245 train_time:56839ms step_avg:60.86ms
step:935/2245 train_time:56901ms step_avg:60.86ms
step:936/2245 train_time:56961ms step_avg:60.86ms
step:937/2245 train_time:57024ms step_avg:60.86ms
step:938/2245 train_time:57085ms step_avg:60.86ms
step:939/2245 train_time:57147ms step_avg:60.86ms
step:940/2245 train_time:57208ms step_avg:60.86ms
step:941/2245 train_time:57270ms step_avg:60.86ms
step:942/2245 train_time:57329ms step_avg:60.86ms
step:943/2245 train_time:57392ms step_avg:60.86ms
step:944/2245 train_time:57453ms step_avg:60.86ms
step:945/2245 train_time:57515ms step_avg:60.86ms
step:946/2245 train_time:57575ms step_avg:60.86ms
step:947/2245 train_time:57637ms step_avg:60.86ms
step:948/2245 train_time:57697ms step_avg:60.86ms
step:949/2245 train_time:57759ms step_avg:60.86ms
step:950/2245 train_time:57818ms step_avg:60.86ms
step:951/2245 train_time:57880ms step_avg:60.86ms
step:952/2245 train_time:57941ms step_avg:60.86ms
step:953/2245 train_time:58004ms step_avg:60.86ms
step:954/2245 train_time:58065ms step_avg:60.86ms
step:955/2245 train_time:58128ms step_avg:60.87ms
step:956/2245 train_time:58188ms step_avg:60.87ms
step:957/2245 train_time:58251ms step_avg:60.87ms
step:958/2245 train_time:58311ms step_avg:60.87ms
step:959/2245 train_time:58374ms step_avg:60.87ms
step:960/2245 train_time:58433ms step_avg:60.87ms
step:961/2245 train_time:58496ms step_avg:60.87ms
step:962/2245 train_time:58556ms step_avg:60.87ms
step:963/2245 train_time:58619ms step_avg:60.87ms
step:964/2245 train_time:58678ms step_avg:60.87ms
step:965/2245 train_time:58740ms step_avg:60.87ms
step:966/2245 train_time:58800ms step_avg:60.87ms
step:967/2245 train_time:58862ms step_avg:60.87ms
step:968/2245 train_time:58922ms step_avg:60.87ms
step:969/2245 train_time:58984ms step_avg:60.87ms
step:970/2245 train_time:59044ms step_avg:60.87ms
step:971/2245 train_time:59106ms step_avg:60.87ms
step:972/2245 train_time:59167ms step_avg:60.87ms
step:973/2245 train_time:59229ms step_avg:60.87ms
step:974/2245 train_time:59290ms step_avg:60.87ms
step:975/2245 train_time:59353ms step_avg:60.87ms
step:976/2245 train_time:59413ms step_avg:60.87ms
step:977/2245 train_time:59475ms step_avg:60.88ms
step:978/2245 train_time:59536ms step_avg:60.87ms
step:979/2245 train_time:59598ms step_avg:60.88ms
step:980/2245 train_time:59659ms step_avg:60.88ms
step:981/2245 train_time:59720ms step_avg:60.88ms
step:982/2245 train_time:59781ms step_avg:60.88ms
step:983/2245 train_time:59842ms step_avg:60.88ms
step:984/2245 train_time:59902ms step_avg:60.88ms
step:985/2245 train_time:59964ms step_avg:60.88ms
step:986/2245 train_time:60024ms step_avg:60.88ms
step:987/2245 train_time:60087ms step_avg:60.88ms
step:988/2245 train_time:60147ms step_avg:60.88ms
step:989/2245 train_time:60210ms step_avg:60.88ms
step:990/2245 train_time:60271ms step_avg:60.88ms
step:991/2245 train_time:60333ms step_avg:60.88ms
step:992/2245 train_time:60392ms step_avg:60.88ms
step:993/2245 train_time:60455ms step_avg:60.88ms
step:994/2245 train_time:60515ms step_avg:60.88ms
step:995/2245 train_time:60577ms step_avg:60.88ms
step:996/2245 train_time:60637ms step_avg:60.88ms
step:997/2245 train_time:60699ms step_avg:60.88ms
step:998/2245 train_time:60760ms step_avg:60.88ms
step:999/2245 train_time:60821ms step_avg:60.88ms
step:1000/2245 train_time:60881ms step_avg:60.88ms
step:1000/2245 val_loss:3.5943 train_time:60944ms step_avg:60.94ms
step:1001/2245 train_time:60962ms step_avg:60.90ms
step:1002/2245 train_time:61008ms step_avg:60.89ms
step:1003/2245 train_time:61072ms step_avg:60.89ms
step:1004/2245 train_time:61132ms step_avg:60.89ms
step:1005/2245 train_time:61195ms step_avg:60.89ms
step:1006/2245 train_time:61255ms step_avg:60.89ms
step:1007/2245 train_time:61317ms step_avg:60.89ms
step:1008/2245 train_time:61377ms step_avg:60.89ms
step:1009/2245 train_time:61439ms step_avg:60.89ms
step:1010/2245 train_time:61498ms step_avg:60.89ms
step:1011/2245 train_time:61560ms step_avg:60.89ms
step:1012/2245 train_time:61620ms step_avg:60.89ms
step:1013/2245 train_time:61681ms step_avg:60.89ms
step:1014/2245 train_time:61740ms step_avg:60.89ms
step:1015/2245 train_time:61801ms step_avg:60.89ms
step:1016/2245 train_time:61862ms step_avg:60.89ms
step:1017/2245 train_time:61925ms step_avg:60.89ms
step:1018/2245 train_time:61987ms step_avg:60.89ms
step:1019/2245 train_time:62049ms step_avg:60.89ms
step:1020/2245 train_time:62109ms step_avg:60.89ms
step:1021/2245 train_time:62172ms step_avg:60.89ms
step:1022/2245 train_time:62231ms step_avg:60.89ms
step:1023/2245 train_time:62294ms step_avg:60.89ms
step:1024/2245 train_time:62354ms step_avg:60.89ms
step:1025/2245 train_time:62417ms step_avg:60.89ms
step:1026/2245 train_time:62477ms step_avg:60.89ms
step:1027/2245 train_time:62539ms step_avg:60.90ms
step:1028/2245 train_time:62599ms step_avg:60.89ms
step:1029/2245 train_time:62661ms step_avg:60.89ms
step:1030/2245 train_time:62720ms step_avg:60.89ms
step:1031/2245 train_time:62782ms step_avg:60.89ms
step:1032/2245 train_time:62842ms step_avg:60.89ms
step:1033/2245 train_time:62905ms step_avg:60.90ms
step:1034/2245 train_time:62965ms step_avg:60.89ms
step:1035/2245 train_time:63027ms step_avg:60.90ms
step:1036/2245 train_time:63087ms step_avg:60.89ms
step:1037/2245 train_time:63149ms step_avg:60.90ms
step:1038/2245 train_time:63209ms step_avg:60.90ms
step:1039/2245 train_time:63272ms step_avg:60.90ms
step:1040/2245 train_time:63332ms step_avg:60.90ms
step:1041/2245 train_time:63395ms step_avg:60.90ms
step:1042/2245 train_time:63455ms step_avg:60.90ms
step:1043/2245 train_time:63518ms step_avg:60.90ms
step:1044/2245 train_time:63579ms step_avg:60.90ms
step:1045/2245 train_time:63641ms step_avg:60.90ms
step:1046/2245 train_time:63700ms step_avg:60.90ms
step:1047/2245 train_time:63762ms step_avg:60.90ms
step:1048/2245 train_time:63822ms step_avg:60.90ms
step:1049/2245 train_time:63884ms step_avg:60.90ms
step:1050/2245 train_time:63944ms step_avg:60.90ms
step:1051/2245 train_time:64007ms step_avg:60.90ms
step:1052/2245 train_time:64067ms step_avg:60.90ms
step:1053/2245 train_time:64130ms step_avg:60.90ms
step:1054/2245 train_time:64190ms step_avg:60.90ms
step:1055/2245 train_time:64252ms step_avg:60.90ms
step:1056/2245 train_time:64312ms step_avg:60.90ms
step:1057/2245 train_time:64375ms step_avg:60.90ms
step:1058/2245 train_time:64434ms step_avg:60.90ms
step:1059/2245 train_time:64497ms step_avg:60.90ms
step:1060/2245 train_time:64556ms step_avg:60.90ms
step:1061/2245 train_time:64618ms step_avg:60.90ms
step:1062/2245 train_time:64678ms step_avg:60.90ms
step:1063/2245 train_time:64741ms step_avg:60.90ms
step:1064/2245 train_time:64801ms step_avg:60.90ms
step:1065/2245 train_time:64863ms step_avg:60.90ms
step:1066/2245 train_time:64924ms step_avg:60.90ms
step:1067/2245 train_time:64987ms step_avg:60.91ms
step:1068/2245 train_time:65046ms step_avg:60.90ms
step:1069/2245 train_time:65108ms step_avg:60.91ms
step:1070/2245 train_time:65168ms step_avg:60.90ms
step:1071/2245 train_time:65230ms step_avg:60.91ms
step:1072/2245 train_time:65290ms step_avg:60.91ms
step:1073/2245 train_time:65353ms step_avg:60.91ms
step:1074/2245 train_time:65414ms step_avg:60.91ms
step:1075/2245 train_time:65476ms step_avg:60.91ms
step:1076/2245 train_time:65536ms step_avg:60.91ms
step:1077/2245 train_time:65598ms step_avg:60.91ms
step:1078/2245 train_time:65658ms step_avg:60.91ms
step:1079/2245 train_time:65721ms step_avg:60.91ms
step:1080/2245 train_time:65781ms step_avg:60.91ms
step:1081/2245 train_time:65843ms step_avg:60.91ms
step:1082/2245 train_time:65903ms step_avg:60.91ms
step:1083/2245 train_time:65966ms step_avg:60.91ms
step:1084/2245 train_time:66026ms step_avg:60.91ms
step:1085/2245 train_time:66088ms step_avg:60.91ms
step:1086/2245 train_time:66148ms step_avg:60.91ms
step:1087/2245 train_time:66210ms step_avg:60.91ms
step:1088/2245 train_time:66270ms step_avg:60.91ms
step:1089/2245 train_time:66333ms step_avg:60.91ms
step:1090/2245 train_time:66393ms step_avg:60.91ms
step:1091/2245 train_time:66455ms step_avg:60.91ms
step:1092/2245 train_time:66515ms step_avg:60.91ms
step:1093/2245 train_time:66578ms step_avg:60.91ms
step:1094/2245 train_time:66638ms step_avg:60.91ms
step:1095/2245 train_time:66701ms step_avg:60.91ms
step:1096/2245 train_time:66761ms step_avg:60.91ms
step:1097/2245 train_time:66824ms step_avg:60.92ms
step:1098/2245 train_time:66885ms step_avg:60.91ms
step:1099/2245 train_time:66947ms step_avg:60.92ms
step:1100/2245 train_time:67007ms step_avg:60.92ms
step:1101/2245 train_time:67069ms step_avg:60.92ms
step:1102/2245 train_time:67128ms step_avg:60.91ms
step:1103/2245 train_time:67190ms step_avg:60.92ms
step:1104/2245 train_time:67250ms step_avg:60.91ms
step:1105/2245 train_time:67312ms step_avg:60.92ms
step:1106/2245 train_time:67372ms step_avg:60.92ms
step:1107/2245 train_time:67435ms step_avg:60.92ms
step:1108/2245 train_time:67495ms step_avg:60.92ms
step:1109/2245 train_time:67557ms step_avg:60.92ms
step:1110/2245 train_time:67618ms step_avg:60.92ms
step:1111/2245 train_time:67681ms step_avg:60.92ms
step:1112/2245 train_time:67741ms step_avg:60.92ms
step:1113/2245 train_time:67803ms step_avg:60.92ms
step:1114/2245 train_time:67863ms step_avg:60.92ms
step:1115/2245 train_time:67926ms step_avg:60.92ms
step:1116/2245 train_time:67987ms step_avg:60.92ms
step:1117/2245 train_time:68048ms step_avg:60.92ms
step:1118/2245 train_time:68108ms step_avg:60.92ms
step:1119/2245 train_time:68170ms step_avg:60.92ms
step:1120/2245 train_time:68230ms step_avg:60.92ms
step:1121/2245 train_time:68292ms step_avg:60.92ms
step:1122/2245 train_time:68352ms step_avg:60.92ms
step:1123/2245 train_time:68415ms step_avg:60.92ms
step:1124/2245 train_time:68475ms step_avg:60.92ms
step:1125/2245 train_time:68537ms step_avg:60.92ms
step:1126/2245 train_time:68598ms step_avg:60.92ms
step:1127/2245 train_time:68662ms step_avg:60.92ms
step:1128/2245 train_time:68721ms step_avg:60.92ms
step:1129/2245 train_time:68784ms step_avg:60.92ms
step:1130/2245 train_time:68844ms step_avg:60.92ms
step:1131/2245 train_time:68906ms step_avg:60.92ms
step:1132/2245 train_time:68965ms step_avg:60.92ms
step:1133/2245 train_time:69028ms step_avg:60.93ms
step:1134/2245 train_time:69088ms step_avg:60.92ms
step:1135/2245 train_time:69151ms step_avg:60.93ms
step:1136/2245 train_time:69210ms step_avg:60.92ms
step:1137/2245 train_time:69272ms step_avg:60.93ms
step:1138/2245 train_time:69332ms step_avg:60.92ms
step:1139/2245 train_time:69395ms step_avg:60.93ms
step:1140/2245 train_time:69455ms step_avg:60.93ms
step:1141/2245 train_time:69518ms step_avg:60.93ms
step:1142/2245 train_time:69579ms step_avg:60.93ms
step:1143/2245 train_time:69641ms step_avg:60.93ms
step:1144/2245 train_time:69701ms step_avg:60.93ms
step:1145/2245 train_time:69763ms step_avg:60.93ms
step:1146/2245 train_time:69822ms step_avg:60.93ms
step:1147/2245 train_time:69884ms step_avg:60.93ms
step:1148/2245 train_time:69944ms step_avg:60.93ms
step:1149/2245 train_time:70007ms step_avg:60.93ms
step:1150/2245 train_time:70066ms step_avg:60.93ms
step:1151/2245 train_time:70128ms step_avg:60.93ms
step:1152/2245 train_time:70188ms step_avg:60.93ms
step:1153/2245 train_time:70250ms step_avg:60.93ms
step:1154/2245 train_time:70310ms step_avg:60.93ms
step:1155/2245 train_time:70373ms step_avg:60.93ms
step:1156/2245 train_time:70433ms step_avg:60.93ms
step:1157/2245 train_time:70496ms step_avg:60.93ms
step:1158/2245 train_time:70556ms step_avg:60.93ms
step:1159/2245 train_time:70618ms step_avg:60.93ms
step:1160/2245 train_time:70679ms step_avg:60.93ms
step:1161/2245 train_time:70742ms step_avg:60.93ms
step:1162/2245 train_time:70802ms step_avg:60.93ms
step:1163/2245 train_time:70864ms step_avg:60.93ms
step:1164/2245 train_time:70924ms step_avg:60.93ms
step:1165/2245 train_time:70986ms step_avg:60.93ms
step:1166/2245 train_time:71045ms step_avg:60.93ms
step:1167/2245 train_time:71107ms step_avg:60.93ms
step:1168/2245 train_time:71167ms step_avg:60.93ms
step:1169/2245 train_time:71229ms step_avg:60.93ms
step:1170/2245 train_time:71289ms step_avg:60.93ms
step:1171/2245 train_time:71351ms step_avg:60.93ms
step:1172/2245 train_time:71411ms step_avg:60.93ms
step:1173/2245 train_time:71474ms step_avg:60.93ms
step:1174/2245 train_time:71534ms step_avg:60.93ms
step:1175/2245 train_time:71597ms step_avg:60.93ms
step:1176/2245 train_time:71658ms step_avg:60.93ms
step:1177/2245 train_time:71721ms step_avg:60.94ms
step:1178/2245 train_time:71781ms step_avg:60.93ms
step:1179/2245 train_time:71842ms step_avg:60.94ms
step:1180/2245 train_time:71902ms step_avg:60.93ms
step:1181/2245 train_time:71964ms step_avg:60.93ms
step:1182/2245 train_time:72024ms step_avg:60.93ms
step:1183/2245 train_time:72086ms step_avg:60.94ms
step:1184/2245 train_time:72146ms step_avg:60.93ms
step:1185/2245 train_time:72208ms step_avg:60.94ms
step:1186/2245 train_time:72268ms step_avg:60.93ms
step:1187/2245 train_time:72331ms step_avg:60.94ms
step:1188/2245 train_time:72391ms step_avg:60.94ms
step:1189/2245 train_time:72454ms step_avg:60.94ms
step:1190/2245 train_time:72514ms step_avg:60.94ms
step:1191/2245 train_time:72576ms step_avg:60.94ms
step:1192/2245 train_time:72636ms step_avg:60.94ms
step:1193/2245 train_time:72700ms step_avg:60.94ms
step:1194/2245 train_time:72760ms step_avg:60.94ms
step:1195/2245 train_time:72823ms step_avg:60.94ms
step:1196/2245 train_time:72883ms step_avg:60.94ms
step:1197/2245 train_time:72945ms step_avg:60.94ms
step:1198/2245 train_time:73004ms step_avg:60.94ms
step:1199/2245 train_time:73066ms step_avg:60.94ms
step:1200/2245 train_time:73126ms step_avg:60.94ms
step:1201/2245 train_time:73188ms step_avg:60.94ms
step:1202/2245 train_time:73247ms step_avg:60.94ms
step:1203/2245 train_time:73310ms step_avg:60.94ms
step:1204/2245 train_time:73370ms step_avg:60.94ms
step:1205/2245 train_time:73433ms step_avg:60.94ms
step:1206/2245 train_time:73493ms step_avg:60.94ms
step:1207/2245 train_time:73556ms step_avg:60.94ms
step:1208/2245 train_time:73616ms step_avg:60.94ms
step:1209/2245 train_time:73679ms step_avg:60.94ms
step:1210/2245 train_time:73740ms step_avg:60.94ms
step:1211/2245 train_time:73803ms step_avg:60.94ms
step:1212/2245 train_time:73862ms step_avg:60.94ms
step:1213/2245 train_time:73925ms step_avg:60.94ms
step:1214/2245 train_time:73985ms step_avg:60.94ms
step:1215/2245 train_time:74047ms step_avg:60.94ms
step:1216/2245 train_time:74107ms step_avg:60.94ms
step:1217/2245 train_time:74169ms step_avg:60.94ms
step:1218/2245 train_time:74229ms step_avg:60.94ms
step:1219/2245 train_time:74290ms step_avg:60.94ms
step:1220/2245 train_time:74350ms step_avg:60.94ms
step:1221/2245 train_time:74413ms step_avg:60.94ms
step:1222/2245 train_time:74473ms step_avg:60.94ms
step:1223/2245 train_time:74535ms step_avg:60.94ms
step:1224/2245 train_time:74595ms step_avg:60.94ms
step:1225/2245 train_time:74657ms step_avg:60.94ms
step:1226/2245 train_time:74717ms step_avg:60.94ms
step:1227/2245 train_time:74781ms step_avg:60.95ms
step:1228/2245 train_time:74841ms step_avg:60.95ms
step:1229/2245 train_time:74903ms step_avg:60.95ms
step:1230/2245 train_time:74963ms step_avg:60.95ms
step:1231/2245 train_time:75026ms step_avg:60.95ms
step:1232/2245 train_time:75085ms step_avg:60.95ms
step:1233/2245 train_time:75147ms step_avg:60.95ms
step:1234/2245 train_time:75206ms step_avg:60.95ms
step:1235/2245 train_time:75268ms step_avg:60.95ms
step:1236/2245 train_time:75328ms step_avg:60.94ms
step:1237/2245 train_time:75390ms step_avg:60.95ms
step:1238/2245 train_time:75450ms step_avg:60.94ms
step:1239/2245 train_time:75513ms step_avg:60.95ms
step:1240/2245 train_time:75573ms step_avg:60.95ms
step:1241/2245 train_time:75635ms step_avg:60.95ms
step:1242/2245 train_time:75695ms step_avg:60.95ms
step:1243/2245 train_time:75758ms step_avg:60.95ms
step:1244/2245 train_time:75818ms step_avg:60.95ms
step:1245/2245 train_time:75881ms step_avg:60.95ms
step:1246/2245 train_time:75941ms step_avg:60.95ms
step:1247/2245 train_time:76004ms step_avg:60.95ms
step:1248/2245 train_time:76064ms step_avg:60.95ms
step:1249/2245 train_time:76126ms step_avg:60.95ms
step:1250/2245 train_time:76186ms step_avg:60.95ms
step:1250/2245 val_loss:3.5237 train_time:76250ms step_avg:61.00ms
step:1251/2245 train_time:76268ms step_avg:60.97ms
step:1252/2245 train_time:76312ms step_avg:60.95ms
step:1253/2245 train_time:76381ms step_avg:60.96ms
step:1254/2245 train_time:76441ms step_avg:60.96ms
step:1255/2245 train_time:76503ms step_avg:60.96ms
step:1256/2245 train_time:76563ms step_avg:60.96ms
step:1257/2245 train_time:76625ms step_avg:60.96ms
step:1258/2245 train_time:76684ms step_avg:60.96ms
step:1259/2245 train_time:76745ms step_avg:60.96ms
step:1260/2245 train_time:76804ms step_avg:60.96ms
step:1261/2245 train_time:76865ms step_avg:60.96ms
step:1262/2245 train_time:76925ms step_avg:60.95ms
step:1263/2245 train_time:76987ms step_avg:60.96ms
step:1264/2245 train_time:77046ms step_avg:60.95ms
step:1265/2245 train_time:77108ms step_avg:60.95ms
step:1266/2245 train_time:77167ms step_avg:60.95ms
step:1267/2245 train_time:77231ms step_avg:60.96ms
step:1268/2245 train_time:77295ms step_avg:60.96ms
step:1269/2245 train_time:77359ms step_avg:60.96ms
step:1270/2245 train_time:77420ms step_avg:60.96ms
step:1271/2245 train_time:77483ms step_avg:60.96ms
step:1272/2245 train_time:77543ms step_avg:60.96ms
step:1273/2245 train_time:77604ms step_avg:60.96ms
step:1274/2245 train_time:77664ms step_avg:60.96ms
step:1275/2245 train_time:77726ms step_avg:60.96ms
step:1276/2245 train_time:77785ms step_avg:60.96ms
step:1277/2245 train_time:77846ms step_avg:60.96ms
step:1278/2245 train_time:77906ms step_avg:60.96ms
step:1279/2245 train_time:77968ms step_avg:60.96ms
step:1280/2245 train_time:78027ms step_avg:60.96ms
step:1281/2245 train_time:78089ms step_avg:60.96ms
step:1282/2245 train_time:78150ms step_avg:60.96ms
step:1283/2245 train_time:78213ms step_avg:60.96ms
step:1284/2245 train_time:78275ms step_avg:60.96ms
step:1285/2245 train_time:78338ms step_avg:60.96ms
step:1286/2245 train_time:78399ms step_avg:60.96ms
step:1287/2245 train_time:78462ms step_avg:60.97ms
step:1288/2245 train_time:78522ms step_avg:60.96ms
step:1289/2245 train_time:78584ms step_avg:60.96ms
step:1290/2245 train_time:78643ms step_avg:60.96ms
step:1291/2245 train_time:78705ms step_avg:60.96ms
step:1292/2245 train_time:78764ms step_avg:60.96ms
step:1293/2245 train_time:78826ms step_avg:60.96ms
step:1294/2245 train_time:78885ms step_avg:60.96ms
step:1295/2245 train_time:78947ms step_avg:60.96ms
step:1296/2245 train_time:79007ms step_avg:60.96ms
step:1297/2245 train_time:79070ms step_avg:60.96ms
step:1298/2245 train_time:79130ms step_avg:60.96ms
step:1299/2245 train_time:79193ms step_avg:60.96ms
step:1300/2245 train_time:79254ms step_avg:60.96ms
step:1301/2245 train_time:79318ms step_avg:60.97ms
step:1302/2245 train_time:79379ms step_avg:60.97ms
step:1303/2245 train_time:79442ms step_avg:60.97ms
step:1304/2245 train_time:79502ms step_avg:60.97ms
step:1305/2245 train_time:79564ms step_avg:60.97ms
step:1306/2245 train_time:79625ms step_avg:60.97ms
step:1307/2245 train_time:79688ms step_avg:60.97ms
step:1308/2245 train_time:79747ms step_avg:60.97ms
step:1309/2245 train_time:79809ms step_avg:60.97ms
step:1310/2245 train_time:79869ms step_avg:60.97ms
step:1311/2245 train_time:79931ms step_avg:60.97ms
step:1312/2245 train_time:79991ms step_avg:60.97ms
step:1313/2245 train_time:80052ms step_avg:60.97ms
step:1314/2245 train_time:80112ms step_avg:60.97ms
step:1315/2245 train_time:80175ms step_avg:60.97ms
step:1316/2245 train_time:80236ms step_avg:60.97ms
step:1317/2245 train_time:80299ms step_avg:60.97ms
step:1318/2245 train_time:80359ms step_avg:60.97ms
step:1319/2245 train_time:80422ms step_avg:60.97ms
step:1320/2245 train_time:80482ms step_avg:60.97ms
step:1321/2245 train_time:80545ms step_avg:60.97ms
step:1322/2245 train_time:80604ms step_avg:60.97ms
step:1323/2245 train_time:80667ms step_avg:60.97ms
step:1324/2245 train_time:80726ms step_avg:60.97ms
step:1325/2245 train_time:80788ms step_avg:60.97ms
step:1326/2245 train_time:80848ms step_avg:60.97ms
step:1327/2245 train_time:80910ms step_avg:60.97ms
step:1328/2245 train_time:80970ms step_avg:60.97ms
step:1329/2245 train_time:81032ms step_avg:60.97ms
step:1330/2245 train_time:81092ms step_avg:60.97ms
step:1331/2245 train_time:81155ms step_avg:60.97ms
step:1332/2245 train_time:81215ms step_avg:60.97ms
step:1333/2245 train_time:81278ms step_avg:60.97ms
step:1334/2245 train_time:81339ms step_avg:60.97ms
step:1335/2245 train_time:81402ms step_avg:60.97ms
step:1336/2245 train_time:81462ms step_avg:60.97ms
step:1337/2245 train_time:81524ms step_avg:60.98ms
step:1338/2245 train_time:81584ms step_avg:60.97ms
step:1339/2245 train_time:81645ms step_avg:60.97ms
step:1340/2245 train_time:81705ms step_avg:60.97ms
step:1341/2245 train_time:81767ms step_avg:60.97ms
step:1342/2245 train_time:81827ms step_avg:60.97ms
step:1343/2245 train_time:81889ms step_avg:60.97ms
step:1344/2245 train_time:81948ms step_avg:60.97ms
step:1345/2245 train_time:82011ms step_avg:60.97ms
step:1346/2245 train_time:82070ms step_avg:60.97ms
step:1347/2245 train_time:82133ms step_avg:60.97ms
step:1348/2245 train_time:82193ms step_avg:60.97ms
step:1349/2245 train_time:82256ms step_avg:60.98ms
step:1350/2245 train_time:82317ms step_avg:60.98ms
step:1351/2245 train_time:82379ms step_avg:60.98ms
step:1352/2245 train_time:82439ms step_avg:60.98ms
step:1353/2245 train_time:82502ms step_avg:60.98ms
step:1354/2245 train_time:82561ms step_avg:60.98ms
step:1355/2245 train_time:82624ms step_avg:60.98ms
step:1356/2245 train_time:82684ms step_avg:60.98ms
step:1357/2245 train_time:82746ms step_avg:60.98ms
step:1358/2245 train_time:82805ms step_avg:60.98ms
step:1359/2245 train_time:82868ms step_avg:60.98ms
step:1360/2245 train_time:82927ms step_avg:60.98ms
step:1361/2245 train_time:82990ms step_avg:60.98ms
step:1362/2245 train_time:83050ms step_avg:60.98ms
step:1363/2245 train_time:83112ms step_avg:60.98ms
step:1364/2245 train_time:83172ms step_avg:60.98ms
step:1365/2245 train_time:83235ms step_avg:60.98ms
step:1366/2245 train_time:83295ms step_avg:60.98ms
step:1367/2245 train_time:83358ms step_avg:60.98ms
step:1368/2245 train_time:83419ms step_avg:60.98ms
step:1369/2245 train_time:83481ms step_avg:60.98ms
step:1370/2245 train_time:83541ms step_avg:60.98ms
step:1371/2245 train_time:83603ms step_avg:60.98ms
step:1372/2245 train_time:83662ms step_avg:60.98ms
step:1373/2245 train_time:83724ms step_avg:60.98ms
step:1374/2245 train_time:83783ms step_avg:60.98ms
step:1375/2245 train_time:83846ms step_avg:60.98ms
step:1376/2245 train_time:83905ms step_avg:60.98ms
step:1377/2245 train_time:83968ms step_avg:60.98ms
step:1378/2245 train_time:84028ms step_avg:60.98ms
step:1379/2245 train_time:84091ms step_avg:60.98ms
step:1380/2245 train_time:84151ms step_avg:60.98ms
step:1381/2245 train_time:84214ms step_avg:60.98ms
step:1382/2245 train_time:84275ms step_avg:60.98ms
step:1383/2245 train_time:84339ms step_avg:60.98ms
step:1384/2245 train_time:84399ms step_avg:60.98ms
step:1385/2245 train_time:84462ms step_avg:60.98ms
step:1386/2245 train_time:84522ms step_avg:60.98ms
step:1387/2245 train_time:84584ms step_avg:60.98ms
step:1388/2245 train_time:84643ms step_avg:60.98ms
step:1389/2245 train_time:84706ms step_avg:60.98ms
step:1390/2245 train_time:84765ms step_avg:60.98ms
step:1391/2245 train_time:84827ms step_avg:60.98ms
step:1392/2245 train_time:84887ms step_avg:60.98ms
step:1393/2245 train_time:84950ms step_avg:60.98ms
step:1394/2245 train_time:85009ms step_avg:60.98ms
step:1395/2245 train_time:85072ms step_avg:60.98ms
step:1396/2245 train_time:85132ms step_avg:60.98ms
step:1397/2245 train_time:85195ms step_avg:60.98ms
step:1398/2245 train_time:85255ms step_avg:60.98ms
step:1399/2245 train_time:85318ms step_avg:60.99ms
step:1400/2245 train_time:85378ms step_avg:60.98ms
step:1401/2245 train_time:85441ms step_avg:60.99ms
step:1402/2245 train_time:85501ms step_avg:60.99ms
step:1403/2245 train_time:85563ms step_avg:60.99ms
step:1404/2245 train_time:85623ms step_avg:60.99ms
step:1405/2245 train_time:85685ms step_avg:60.99ms
step:1406/2245 train_time:85744ms step_avg:60.98ms
step:1407/2245 train_time:85807ms step_avg:60.99ms
step:1408/2245 train_time:85866ms step_avg:60.98ms
step:1409/2245 train_time:85929ms step_avg:60.99ms
step:1410/2245 train_time:85988ms step_avg:60.98ms
step:1411/2245 train_time:86050ms step_avg:60.99ms
step:1412/2245 train_time:86111ms step_avg:60.98ms
step:1413/2245 train_time:86173ms step_avg:60.99ms
step:1414/2245 train_time:86235ms step_avg:60.99ms
step:1415/2245 train_time:86298ms step_avg:60.99ms
step:1416/2245 train_time:86358ms step_avg:60.99ms
step:1417/2245 train_time:86421ms step_avg:60.99ms
step:1418/2245 train_time:86481ms step_avg:60.99ms
step:1419/2245 train_time:86543ms step_avg:60.99ms
step:1420/2245 train_time:86603ms step_avg:60.99ms
step:1421/2245 train_time:86665ms step_avg:60.99ms
step:1422/2245 train_time:86724ms step_avg:60.99ms
step:1423/2245 train_time:86786ms step_avg:60.99ms
step:1424/2245 train_time:86846ms step_avg:60.99ms
step:1425/2245 train_time:86908ms step_avg:60.99ms
step:1426/2245 train_time:86969ms step_avg:60.99ms
step:1427/2245 train_time:87031ms step_avg:60.99ms
step:1428/2245 train_time:87091ms step_avg:60.99ms
step:1429/2245 train_time:87154ms step_avg:60.99ms
step:1430/2245 train_time:87215ms step_avg:60.99ms
step:1431/2245 train_time:87278ms step_avg:60.99ms
step:1432/2245 train_time:87338ms step_avg:60.99ms
step:1433/2245 train_time:87400ms step_avg:60.99ms
step:1434/2245 train_time:87460ms step_avg:60.99ms
step:1435/2245 train_time:87523ms step_avg:60.99ms
step:1436/2245 train_time:87583ms step_avg:60.99ms
step:1437/2245 train_time:87645ms step_avg:60.99ms
step:1438/2245 train_time:87704ms step_avg:60.99ms
step:1439/2245 train_time:87766ms step_avg:60.99ms
step:1440/2245 train_time:87826ms step_avg:60.99ms
step:1441/2245 train_time:87888ms step_avg:60.99ms
step:1442/2245 train_time:87948ms step_avg:60.99ms
step:1443/2245 train_time:88010ms step_avg:60.99ms
step:1444/2245 train_time:88070ms step_avg:60.99ms
step:1445/2245 train_time:88133ms step_avg:60.99ms
step:1446/2245 train_time:88193ms step_avg:60.99ms
step:1447/2245 train_time:88257ms step_avg:60.99ms
step:1448/2245 train_time:88317ms step_avg:60.99ms
step:1449/2245 train_time:88379ms step_avg:60.99ms
step:1450/2245 train_time:88440ms step_avg:60.99ms
step:1451/2245 train_time:88501ms step_avg:60.99ms
step:1452/2245 train_time:88562ms step_avg:60.99ms
step:1453/2245 train_time:88624ms step_avg:60.99ms
step:1454/2245 train_time:88684ms step_avg:60.99ms
step:1455/2245 train_time:88746ms step_avg:60.99ms
step:1456/2245 train_time:88806ms step_avg:60.99ms
step:1457/2245 train_time:88867ms step_avg:60.99ms
step:1458/2245 train_time:88928ms step_avg:60.99ms
step:1459/2245 train_time:88990ms step_avg:60.99ms
step:1460/2245 train_time:89050ms step_avg:60.99ms
step:1461/2245 train_time:89112ms step_avg:60.99ms
step:1462/2245 train_time:89173ms step_avg:60.99ms
step:1463/2245 train_time:89237ms step_avg:61.00ms
step:1464/2245 train_time:89297ms step_avg:60.99ms
step:1465/2245 train_time:89360ms step_avg:61.00ms
step:1466/2245 train_time:89421ms step_avg:61.00ms
step:1467/2245 train_time:89483ms step_avg:61.00ms
step:1468/2245 train_time:89542ms step_avg:61.00ms
step:1469/2245 train_time:89604ms step_avg:61.00ms
step:1470/2245 train_time:89664ms step_avg:61.00ms
step:1471/2245 train_time:89726ms step_avg:61.00ms
step:1472/2245 train_time:89786ms step_avg:61.00ms
step:1473/2245 train_time:89849ms step_avg:61.00ms
step:1474/2245 train_time:89909ms step_avg:61.00ms
step:1475/2245 train_time:89972ms step_avg:61.00ms
step:1476/2245 train_time:90033ms step_avg:61.00ms
step:1477/2245 train_time:90095ms step_avg:61.00ms
step:1478/2245 train_time:90155ms step_avg:61.00ms
step:1479/2245 train_time:90218ms step_avg:61.00ms
step:1480/2245 train_time:90278ms step_avg:61.00ms
step:1481/2245 train_time:90341ms step_avg:61.00ms
step:1482/2245 train_time:90402ms step_avg:61.00ms
step:1483/2245 train_time:90465ms step_avg:61.00ms
step:1484/2245 train_time:90526ms step_avg:61.00ms
step:1485/2245 train_time:90589ms step_avg:61.00ms
step:1486/2245 train_time:90650ms step_avg:61.00ms
step:1487/2245 train_time:90712ms step_avg:61.00ms
step:1488/2245 train_time:90772ms step_avg:61.00ms
step:1489/2245 train_time:90835ms step_avg:61.00ms
step:1490/2245 train_time:90896ms step_avg:61.00ms
step:1491/2245 train_time:90958ms step_avg:61.00ms
step:1492/2245 train_time:91018ms step_avg:61.00ms
step:1493/2245 train_time:91080ms step_avg:61.00ms
step:1494/2245 train_time:91140ms step_avg:61.00ms
step:1495/2245 train_time:91203ms step_avg:61.01ms
step:1496/2245 train_time:91264ms step_avg:61.01ms
step:1497/2245 train_time:91327ms step_avg:61.01ms
step:1498/2245 train_time:91387ms step_avg:61.01ms
step:1499/2245 train_time:91451ms step_avg:61.01ms
step:1500/2245 train_time:91512ms step_avg:61.01ms
step:1500/2245 val_loss:3.4445 train_time:91577ms step_avg:61.05ms
step:1501/2245 train_time:91595ms step_avg:61.02ms
step:1502/2245 train_time:91638ms step_avg:61.01ms
step:1503/2245 train_time:91701ms step_avg:61.01ms
step:1504/2245 train_time:91762ms step_avg:61.01ms
step:1505/2245 train_time:91827ms step_avg:61.01ms
step:1506/2245 train_time:91886ms step_avg:61.01ms
step:1507/2245 train_time:91948ms step_avg:61.01ms
step:1508/2245 train_time:92007ms step_avg:61.01ms
step:1509/2245 train_time:92069ms step_avg:61.01ms
step:1510/2245 train_time:92128ms step_avg:61.01ms
step:1511/2245 train_time:92190ms step_avg:61.01ms
step:1512/2245 train_time:92250ms step_avg:61.01ms
step:1513/2245 train_time:92313ms step_avg:61.01ms
step:1514/2245 train_time:92374ms step_avg:61.01ms
step:1515/2245 train_time:92437ms step_avg:61.01ms
step:1516/2245 train_time:92499ms step_avg:61.02ms
step:1517/2245 train_time:92565ms step_avg:61.02ms
step:1518/2245 train_time:92627ms step_avg:61.02ms
step:1519/2245 train_time:92690ms step_avg:61.02ms
step:1520/2245 train_time:92751ms step_avg:61.02ms
step:1521/2245 train_time:92815ms step_avg:61.02ms
step:1522/2245 train_time:92875ms step_avg:61.02ms
step:1523/2245 train_time:92938ms step_avg:61.02ms
step:1524/2245 train_time:92999ms step_avg:61.02ms
step:1525/2245 train_time:93062ms step_avg:61.02ms
step:1526/2245 train_time:93122ms step_avg:61.02ms
step:1527/2245 train_time:93184ms step_avg:61.02ms
step:1528/2245 train_time:93244ms step_avg:61.02ms
step:1529/2245 train_time:93306ms step_avg:61.02ms
step:1530/2245 train_time:93366ms step_avg:61.02ms
step:1531/2245 train_time:93429ms step_avg:61.02ms
step:1532/2245 train_time:93490ms step_avg:61.02ms
step:1533/2245 train_time:93554ms step_avg:61.03ms
step:1534/2245 train_time:93615ms step_avg:61.03ms
step:1535/2245 train_time:93679ms step_avg:61.03ms
step:1536/2245 train_time:93740ms step_avg:61.03ms
step:1537/2245 train_time:93803ms step_avg:61.03ms
step:1538/2245 train_time:93863ms step_avg:61.03ms
step:1539/2245 train_time:93925ms step_avg:61.03ms
step:1540/2245 train_time:93986ms step_avg:61.03ms
step:1541/2245 train_time:94048ms step_avg:61.03ms
step:1542/2245 train_time:94108ms step_avg:61.03ms
step:1543/2245 train_time:94170ms step_avg:61.03ms
step:1544/2245 train_time:94230ms step_avg:61.03ms
step:1545/2245 train_time:94293ms step_avg:61.03ms
step:1546/2245 train_time:94353ms step_avg:61.03ms
step:1547/2245 train_time:94415ms step_avg:61.03ms
step:1548/2245 train_time:94476ms step_avg:61.03ms
step:1549/2245 train_time:94540ms step_avg:61.03ms
step:1550/2245 train_time:94601ms step_avg:61.03ms
step:1551/2245 train_time:94664ms step_avg:61.03ms
step:1552/2245 train_time:94724ms step_avg:61.03ms
step:1553/2245 train_time:94788ms step_avg:61.04ms
step:1554/2245 train_time:94848ms step_avg:61.03ms
step:1555/2245 train_time:94911ms step_avg:61.04ms
step:1556/2245 train_time:94971ms step_avg:61.04ms
step:1557/2245 train_time:95034ms step_avg:61.04ms
step:1558/2245 train_time:95094ms step_avg:61.04ms
step:1559/2245 train_time:95158ms step_avg:61.04ms
step:1560/2245 train_time:95217ms step_avg:61.04ms
step:1561/2245 train_time:95280ms step_avg:61.04ms
step:1562/2245 train_time:95340ms step_avg:61.04ms
step:1563/2245 train_time:95403ms step_avg:61.04ms
step:1564/2245 train_time:95463ms step_avg:61.04ms
step:1565/2245 train_time:95526ms step_avg:61.04ms
step:1566/2245 train_time:95586ms step_avg:61.04ms
step:1567/2245 train_time:95649ms step_avg:61.04ms
step:1568/2245 train_time:95709ms step_avg:61.04ms
step:1569/2245 train_time:95772ms step_avg:61.04ms
step:1570/2245 train_time:95833ms step_avg:61.04ms
step:1571/2245 train_time:95896ms step_avg:61.04ms
step:1572/2245 train_time:95956ms step_avg:61.04ms
step:1573/2245 train_time:96019ms step_avg:61.04ms
step:1574/2245 train_time:96080ms step_avg:61.04ms
step:1575/2245 train_time:96143ms step_avg:61.04ms
step:1576/2245 train_time:96203ms step_avg:61.04ms
step:1577/2245 train_time:96265ms step_avg:61.04ms
step:1578/2245 train_time:96325ms step_avg:61.04ms
step:1579/2245 train_time:96388ms step_avg:61.04ms
step:1580/2245 train_time:96449ms step_avg:61.04ms
step:1581/2245 train_time:96511ms step_avg:61.04ms
step:1582/2245 train_time:96572ms step_avg:61.04ms
step:1583/2245 train_time:96636ms step_avg:61.05ms
step:1584/2245 train_time:96698ms step_avg:61.05ms
step:1585/2245 train_time:96761ms step_avg:61.05ms
step:1586/2245 train_time:96821ms step_avg:61.05ms
step:1587/2245 train_time:96884ms step_avg:61.05ms
step:1588/2245 train_time:96943ms step_avg:61.05ms
step:1589/2245 train_time:97006ms step_avg:61.05ms
step:1590/2245 train_time:97066ms step_avg:61.05ms
step:1591/2245 train_time:97128ms step_avg:61.05ms
step:1592/2245 train_time:97188ms step_avg:61.05ms
step:1593/2245 train_time:97251ms step_avg:61.05ms
step:1594/2245 train_time:97311ms step_avg:61.05ms
step:1595/2245 train_time:97374ms step_avg:61.05ms
step:1596/2245 train_time:97434ms step_avg:61.05ms
step:1597/2245 train_time:97497ms step_avg:61.05ms
step:1598/2245 train_time:97557ms step_avg:61.05ms
step:1599/2245 train_time:97620ms step_avg:61.05ms
step:1600/2245 train_time:97681ms step_avg:61.05ms
step:1601/2245 train_time:97744ms step_avg:61.05ms
step:1602/2245 train_time:97804ms step_avg:61.05ms
step:1603/2245 train_time:97867ms step_avg:61.05ms
step:1604/2245 train_time:97928ms step_avg:61.05ms
step:1605/2245 train_time:97990ms step_avg:61.05ms
step:1606/2245 train_time:98050ms step_avg:61.05ms
step:1607/2245 train_time:98113ms step_avg:61.05ms
step:1608/2245 train_time:98173ms step_avg:61.05ms
step:1609/2245 train_time:98235ms step_avg:61.05ms
step:1610/2245 train_time:98296ms step_avg:61.05ms
step:1611/2245 train_time:98358ms step_avg:61.05ms
step:1612/2245 train_time:98419ms step_avg:61.05ms
step:1613/2245 train_time:98481ms step_avg:61.05ms
step:1614/2245 train_time:98542ms step_avg:61.05ms
step:1615/2245 train_time:98605ms step_avg:61.06ms
step:1616/2245 train_time:98665ms step_avg:61.06ms
step:1617/2245 train_time:98728ms step_avg:61.06ms
step:1618/2245 train_time:98788ms step_avg:61.06ms
step:1619/2245 train_time:98851ms step_avg:61.06ms
step:1620/2245 train_time:98911ms step_avg:61.06ms
step:1621/2245 train_time:98974ms step_avg:61.06ms
step:1622/2245 train_time:99034ms step_avg:61.06ms
step:1623/2245 train_time:99097ms step_avg:61.06ms
step:1624/2245 train_time:99158ms step_avg:61.06ms
step:1625/2245 train_time:99221ms step_avg:61.06ms
step:1626/2245 train_time:99281ms step_avg:61.06ms
step:1627/2245 train_time:99344ms step_avg:61.06ms
step:1628/2245 train_time:99404ms step_avg:61.06ms
step:1629/2245 train_time:99467ms step_avg:61.06ms
step:1630/2245 train_time:99526ms step_avg:61.06ms
step:1631/2245 train_time:99589ms step_avg:61.06ms
step:1632/2245 train_time:99650ms step_avg:61.06ms
step:1633/2245 train_time:99713ms step_avg:61.06ms
step:1634/2245 train_time:99773ms step_avg:61.06ms
step:1635/2245 train_time:99836ms step_avg:61.06ms
step:1636/2245 train_time:99897ms step_avg:61.06ms
step:1637/2245 train_time:99960ms step_avg:61.06ms
step:1638/2245 train_time:100020ms step_avg:61.06ms
step:1639/2245 train_time:100082ms step_avg:61.06ms
step:1640/2245 train_time:100143ms step_avg:61.06ms
step:1641/2245 train_time:100205ms step_avg:61.06ms
step:1642/2245 train_time:100265ms step_avg:61.06ms
step:1643/2245 train_time:100328ms step_avg:61.06ms
step:1644/2245 train_time:100388ms step_avg:61.06ms
step:1645/2245 train_time:100450ms step_avg:61.06ms
step:1646/2245 train_time:100510ms step_avg:61.06ms
step:1647/2245 train_time:100574ms step_avg:61.06ms
step:1648/2245 train_time:100634ms step_avg:61.06ms
step:1649/2245 train_time:100697ms step_avg:61.07ms
step:1650/2245 train_time:100758ms step_avg:61.07ms
step:1651/2245 train_time:100820ms step_avg:61.07ms
step:1652/2245 train_time:100881ms step_avg:61.07ms
step:1653/2245 train_time:100944ms step_avg:61.07ms
step:1654/2245 train_time:101003ms step_avg:61.07ms
step:1655/2245 train_time:101067ms step_avg:61.07ms
step:1656/2245 train_time:101127ms step_avg:61.07ms
step:1657/2245 train_time:101190ms step_avg:61.07ms
step:1658/2245 train_time:101250ms step_avg:61.07ms
step:1659/2245 train_time:101313ms step_avg:61.07ms
step:1660/2245 train_time:101373ms step_avg:61.07ms
step:1661/2245 train_time:101436ms step_avg:61.07ms
step:1662/2245 train_time:101496ms step_avg:61.07ms
step:1663/2245 train_time:101559ms step_avg:61.07ms
step:1664/2245 train_time:101620ms step_avg:61.07ms
step:1665/2245 train_time:101683ms step_avg:61.07ms
step:1666/2245 train_time:101743ms step_avg:61.07ms
step:1667/2245 train_time:101806ms step_avg:61.07ms
step:1668/2245 train_time:101866ms step_avg:61.07ms
step:1669/2245 train_time:101928ms step_avg:61.07ms
step:1670/2245 train_time:101988ms step_avg:61.07ms
step:1671/2245 train_time:102052ms step_avg:61.07ms
step:1672/2245 train_time:102113ms step_avg:61.07ms
step:1673/2245 train_time:102175ms step_avg:61.07ms
step:1674/2245 train_time:102236ms step_avg:61.07ms
step:1675/2245 train_time:102300ms step_avg:61.07ms
step:1676/2245 train_time:102360ms step_avg:61.07ms
step:1677/2245 train_time:102422ms step_avg:61.07ms
step:1678/2245 train_time:102482ms step_avg:61.07ms
step:1679/2245 train_time:102545ms step_avg:61.07ms
step:1680/2245 train_time:102604ms step_avg:61.07ms
step:1681/2245 train_time:102667ms step_avg:61.07ms
step:1682/2245 train_time:102727ms step_avg:61.07ms
step:1683/2245 train_time:102790ms step_avg:61.08ms
step:1684/2245 train_time:102850ms step_avg:61.07ms
step:1685/2245 train_time:102913ms step_avg:61.08ms
step:1686/2245 train_time:102973ms step_avg:61.08ms
step:1687/2245 train_time:103037ms step_avg:61.08ms
step:1688/2245 train_time:103098ms step_avg:61.08ms
step:1689/2245 train_time:103161ms step_avg:61.08ms
step:1690/2245 train_time:103222ms step_avg:61.08ms
step:1691/2245 train_time:103284ms step_avg:61.08ms
step:1692/2245 train_time:103345ms step_avg:61.08ms
step:1693/2245 train_time:103407ms step_avg:61.08ms
step:1694/2245 train_time:103468ms step_avg:61.08ms
step:1695/2245 train_time:103531ms step_avg:61.08ms
step:1696/2245 train_time:103591ms step_avg:61.08ms
step:1697/2245 train_time:103654ms step_avg:61.08ms
step:1698/2245 train_time:103714ms step_avg:61.08ms
step:1699/2245 train_time:103777ms step_avg:61.08ms
step:1700/2245 train_time:103838ms step_avg:61.08ms
step:1701/2245 train_time:103901ms step_avg:61.08ms
step:1702/2245 train_time:103961ms step_avg:61.08ms
step:1703/2245 train_time:104024ms step_avg:61.08ms
step:1704/2245 train_time:104083ms step_avg:61.08ms
step:1705/2245 train_time:104146ms step_avg:61.08ms
step:1706/2245 train_time:104206ms step_avg:61.08ms
step:1707/2245 train_time:104269ms step_avg:61.08ms
step:1708/2245 train_time:104329ms step_avg:61.08ms
step:1709/2245 train_time:104391ms step_avg:61.08ms
step:1710/2245 train_time:104452ms step_avg:61.08ms
step:1711/2245 train_time:104515ms step_avg:61.08ms
step:1712/2245 train_time:104575ms step_avg:61.08ms
step:1713/2245 train_time:104638ms step_avg:61.08ms
step:1714/2245 train_time:104698ms step_avg:61.08ms
step:1715/2245 train_time:104761ms step_avg:61.09ms
step:1716/2245 train_time:104822ms step_avg:61.09ms
step:1717/2245 train_time:104885ms step_avg:61.09ms
step:1718/2245 train_time:104945ms step_avg:61.09ms
step:1719/2245 train_time:105007ms step_avg:61.09ms
step:1720/2245 train_time:105067ms step_avg:61.09ms
step:1721/2245 train_time:105129ms step_avg:61.09ms
step:1722/2245 train_time:105190ms step_avg:61.09ms
step:1723/2245 train_time:105253ms step_avg:61.09ms
step:1724/2245 train_time:105313ms step_avg:61.09ms
step:1725/2245 train_time:105375ms step_avg:61.09ms
step:1726/2245 train_time:105436ms step_avg:61.09ms
step:1727/2245 train_time:105500ms step_avg:61.09ms
step:1728/2245 train_time:105560ms step_avg:61.09ms
step:1729/2245 train_time:105623ms step_avg:61.09ms
step:1730/2245 train_time:105682ms step_avg:61.09ms
step:1731/2245 train_time:105745ms step_avg:61.09ms
step:1732/2245 train_time:105805ms step_avg:61.09ms
step:1733/2245 train_time:105867ms step_avg:61.09ms
step:1734/2245 train_time:105928ms step_avg:61.09ms
step:1735/2245 train_time:105990ms step_avg:61.09ms
step:1736/2245 train_time:106050ms step_avg:61.09ms
step:1737/2245 train_time:106113ms step_avg:61.09ms
step:1738/2245 train_time:106173ms step_avg:61.09ms
step:1739/2245 train_time:106237ms step_avg:61.09ms
step:1740/2245 train_time:106298ms step_avg:61.09ms
step:1741/2245 train_time:106361ms step_avg:61.09ms
step:1742/2245 train_time:106421ms step_avg:61.09ms
step:1743/2245 train_time:106483ms step_avg:61.09ms
step:1744/2245 train_time:106543ms step_avg:61.09ms
step:1745/2245 train_time:106606ms step_avg:61.09ms
step:1746/2245 train_time:106666ms step_avg:61.09ms
step:1747/2245 train_time:106729ms step_avg:61.09ms
step:1748/2245 train_time:106789ms step_avg:61.09ms
step:1749/2245 train_time:106852ms step_avg:61.09ms
step:1750/2245 train_time:106912ms step_avg:61.09ms
step:1750/2245 val_loss:3.3798 train_time:106976ms step_avg:61.13ms
step:1751/2245 train_time:106995ms step_avg:61.11ms
step:1752/2245 train_time:107040ms step_avg:61.10ms
step:1753/2245 train_time:107107ms step_avg:61.10ms
step:1754/2245 train_time:107169ms step_avg:61.10ms
step:1755/2245 train_time:107231ms step_avg:61.10ms
step:1756/2245 train_time:107291ms step_avg:61.10ms
step:1757/2245 train_time:107353ms step_avg:61.10ms
step:1758/2245 train_time:107412ms step_avg:61.10ms
step:1759/2245 train_time:107474ms step_avg:61.10ms
step:1760/2245 train_time:107534ms step_avg:61.10ms
step:1761/2245 train_time:107596ms step_avg:61.10ms
step:1762/2245 train_time:107656ms step_avg:61.10ms
step:1763/2245 train_time:107718ms step_avg:61.10ms
step:1764/2245 train_time:107778ms step_avg:61.10ms
step:1765/2245 train_time:107840ms step_avg:61.10ms
step:1766/2245 train_time:107901ms step_avg:61.10ms
step:1767/2245 train_time:107966ms step_avg:61.10ms
step:1768/2245 train_time:108028ms step_avg:61.10ms
step:1769/2245 train_time:108091ms step_avg:61.10ms
step:1770/2245 train_time:108153ms step_avg:61.10ms
step:1771/2245 train_time:108216ms step_avg:61.10ms
step:1772/2245 train_time:108276ms step_avg:61.10ms
step:1773/2245 train_time:108339ms step_avg:61.10ms
step:1774/2245 train_time:108399ms step_avg:61.10ms
step:1775/2245 train_time:108462ms step_avg:61.11ms
step:1776/2245 train_time:108522ms step_avg:61.10ms
step:1777/2245 train_time:108584ms step_avg:61.11ms
step:1778/2245 train_time:108644ms step_avg:61.10ms
step:1779/2245 train_time:108707ms step_avg:61.11ms
step:1780/2245 train_time:108766ms step_avg:61.10ms
step:1781/2245 train_time:108829ms step_avg:61.11ms
step:1782/2245 train_time:108888ms step_avg:61.10ms
step:1783/2245 train_time:108951ms step_avg:61.11ms
step:1784/2245 train_time:109012ms step_avg:61.11ms
step:1785/2245 train_time:109076ms step_avg:61.11ms
step:1786/2245 train_time:109136ms step_avg:61.11ms
step:1787/2245 train_time:109200ms step_avg:61.11ms
step:1788/2245 train_time:109263ms step_avg:61.11ms
step:1789/2245 train_time:109326ms step_avg:61.11ms
step:1790/2245 train_time:109385ms step_avg:61.11ms
step:1791/2245 train_time:109448ms step_avg:61.11ms
step:1792/2245 train_time:109507ms step_avg:61.11ms
step:1793/2245 train_time:109570ms step_avg:61.11ms
step:1794/2245 train_time:109630ms step_avg:61.11ms
step:1795/2245 train_time:109692ms step_avg:61.11ms
step:1796/2245 train_time:109751ms step_avg:61.11ms
step:1797/2245 train_time:109814ms step_avg:61.11ms
step:1798/2245 train_time:109874ms step_avg:61.11ms
step:1799/2245 train_time:109938ms step_avg:61.11ms
step:1800/2245 train_time:109999ms step_avg:61.11ms
step:1801/2245 train_time:110063ms step_avg:61.11ms
step:1802/2245 train_time:110125ms step_avg:61.11ms
step:1803/2245 train_time:110188ms step_avg:61.11ms
step:1804/2245 train_time:110248ms step_avg:61.11ms
step:1805/2245 train_time:110310ms step_avg:61.11ms
step:1806/2245 train_time:110371ms step_avg:61.11ms
step:1807/2245 train_time:110433ms step_avg:61.11ms
step:1808/2245 train_time:110493ms step_avg:61.11ms
step:1809/2245 train_time:110555ms step_avg:61.11ms
step:1810/2245 train_time:110615ms step_avg:61.11ms
step:1811/2245 train_time:110678ms step_avg:61.11ms
step:1812/2245 train_time:110738ms step_avg:61.11ms
step:1813/2245 train_time:110801ms step_avg:61.11ms
step:1814/2245 train_time:110862ms step_avg:61.11ms
step:1815/2245 train_time:110926ms step_avg:61.12ms
step:1816/2245 train_time:110986ms step_avg:61.12ms
step:1817/2245 train_time:111049ms step_avg:61.12ms
step:1818/2245 train_time:111109ms step_avg:61.12ms
step:1819/2245 train_time:111173ms step_avg:61.12ms
step:1820/2245 train_time:111233ms step_avg:61.12ms
step:1821/2245 train_time:111296ms step_avg:61.12ms
step:1822/2245 train_time:111357ms step_avg:61.12ms
step:1823/2245 train_time:111420ms step_avg:61.12ms
step:1824/2245 train_time:111480ms step_avg:61.12ms
step:1825/2245 train_time:111543ms step_avg:61.12ms
step:1826/2245 train_time:111604ms step_avg:61.12ms
step:1827/2245 train_time:111666ms step_avg:61.12ms
step:1828/2245 train_time:111726ms step_avg:61.12ms
step:1829/2245 train_time:111788ms step_avg:61.12ms
step:1830/2245 train_time:111848ms step_avg:61.12ms
step:1831/2245 train_time:111911ms step_avg:61.12ms
step:1832/2245 train_time:111972ms step_avg:61.12ms
step:1833/2245 train_time:112034ms step_avg:61.12ms
step:1834/2245 train_time:112094ms step_avg:61.12ms
step:1835/2245 train_time:112158ms step_avg:61.12ms
step:1836/2245 train_time:112218ms step_avg:61.12ms
step:1837/2245 train_time:112281ms step_avg:61.12ms
step:1838/2245 train_time:112342ms step_avg:61.12ms
step:1839/2245 train_time:112405ms step_avg:61.12ms
step:1840/2245 train_time:112466ms step_avg:61.12ms
step:1841/2245 train_time:112529ms step_avg:61.12ms
step:1842/2245 train_time:112589ms step_avg:61.12ms
step:1843/2245 train_time:112651ms step_avg:61.12ms
step:1844/2245 train_time:112712ms step_avg:61.12ms
step:1845/2245 train_time:112774ms step_avg:61.12ms
step:1846/2245 train_time:112834ms step_avg:61.12ms
step:1847/2245 train_time:112897ms step_avg:61.12ms
step:1848/2245 train_time:112958ms step_avg:61.12ms
step:1849/2245 train_time:113021ms step_avg:61.13ms
step:1850/2245 train_time:113082ms step_avg:61.13ms
step:1851/2245 train_time:113145ms step_avg:61.13ms
step:1852/2245 train_time:113204ms step_avg:61.13ms
step:1853/2245 train_time:113267ms step_avg:61.13ms
step:1854/2245 train_time:113327ms step_avg:61.13ms
step:1855/2245 train_time:113389ms step_avg:61.13ms
step:1856/2245 train_time:113450ms step_avg:61.13ms
step:1857/2245 train_time:113512ms step_avg:61.13ms
step:1858/2245 train_time:113572ms step_avg:61.13ms
step:1859/2245 train_time:113635ms step_avg:61.13ms
step:1860/2245 train_time:113695ms step_avg:61.13ms
step:1861/2245 train_time:113758ms step_avg:61.13ms
step:1862/2245 train_time:113819ms step_avg:61.13ms
step:1863/2245 train_time:113882ms step_avg:61.13ms
step:1864/2245 train_time:113942ms step_avg:61.13ms
step:1865/2245 train_time:114005ms step_avg:61.13ms
step:1866/2245 train_time:114066ms step_avg:61.13ms
step:1867/2245 train_time:114128ms step_avg:61.13ms
step:1868/2245 train_time:114188ms step_avg:61.13ms
step:1869/2245 train_time:114251ms step_avg:61.13ms
step:1870/2245 train_time:114311ms step_avg:61.13ms
step:1871/2245 train_time:114374ms step_avg:61.13ms
step:1872/2245 train_time:114434ms step_avg:61.13ms
step:1873/2245 train_time:114497ms step_avg:61.13ms
step:1874/2245 train_time:114558ms step_avg:61.13ms
step:1875/2245 train_time:114621ms step_avg:61.13ms
step:1876/2245 train_time:114682ms step_avg:61.13ms
step:1877/2245 train_time:114745ms step_avg:61.13ms
step:1878/2245 train_time:114805ms step_avg:61.13ms
step:1879/2245 train_time:114868ms step_avg:61.13ms
step:1880/2245 train_time:114928ms step_avg:61.13ms
step:1881/2245 train_time:114990ms step_avg:61.13ms
step:1882/2245 train_time:115050ms step_avg:61.13ms
step:1883/2245 train_time:115113ms step_avg:61.13ms
step:1884/2245 train_time:115174ms step_avg:61.13ms
step:1885/2245 train_time:115237ms step_avg:61.13ms
step:1886/2245 train_time:115297ms step_avg:61.13ms
step:1887/2245 train_time:115361ms step_avg:61.13ms
step:1888/2245 train_time:115422ms step_avg:61.13ms
step:1889/2245 train_time:115485ms step_avg:61.14ms
step:1890/2245 train_time:115545ms step_avg:61.14ms
step:1891/2245 train_time:115608ms step_avg:61.14ms
step:1892/2245 train_time:115669ms step_avg:61.14ms
step:1893/2245 train_time:115731ms step_avg:61.14ms
step:1894/2245 train_time:115791ms step_avg:61.14ms
step:1895/2245 train_time:115853ms step_avg:61.14ms
step:1896/2245 train_time:115913ms step_avg:61.14ms
step:1897/2245 train_time:115976ms step_avg:61.14ms
step:1898/2245 train_time:116037ms step_avg:61.14ms
step:1899/2245 train_time:116100ms step_avg:61.14ms
step:1900/2245 train_time:116161ms step_avg:61.14ms
step:1901/2245 train_time:116224ms step_avg:61.14ms
step:1902/2245 train_time:116285ms step_avg:61.14ms
step:1903/2245 train_time:116348ms step_avg:61.14ms
step:1904/2245 train_time:116408ms step_avg:61.14ms
step:1905/2245 train_time:116470ms step_avg:61.14ms
step:1906/2245 train_time:116530ms step_avg:61.14ms
step:1907/2245 train_time:116593ms step_avg:61.14ms
step:1908/2245 train_time:116653ms step_avg:61.14ms
step:1909/2245 train_time:116715ms step_avg:61.14ms
step:1910/2245 train_time:116776ms step_avg:61.14ms
step:1911/2245 train_time:116839ms step_avg:61.14ms
step:1912/2245 train_time:116900ms step_avg:61.14ms
step:1913/2245 train_time:116964ms step_avg:61.14ms
step:1914/2245 train_time:117025ms step_avg:61.14ms
step:1915/2245 train_time:117087ms step_avg:61.14ms
step:1916/2245 train_time:117147ms step_avg:61.14ms
step:1917/2245 train_time:117209ms step_avg:61.14ms
step:1918/2245 train_time:117270ms step_avg:61.14ms
step:1919/2245 train_time:117333ms step_avg:61.14ms
step:1920/2245 train_time:117393ms step_avg:61.14ms
step:1921/2245 train_time:117456ms step_avg:61.14ms
step:1922/2245 train_time:117516ms step_avg:61.14ms
step:1923/2245 train_time:117579ms step_avg:61.14ms
step:1924/2245 train_time:117639ms step_avg:61.14ms
step:1925/2245 train_time:117702ms step_avg:61.14ms
step:1926/2245 train_time:117762ms step_avg:61.14ms
step:1927/2245 train_time:117825ms step_avg:61.14ms
step:1928/2245 train_time:117886ms step_avg:61.14ms
step:1929/2245 train_time:117949ms step_avg:61.15ms
step:1930/2245 train_time:118010ms step_avg:61.14ms
step:1931/2245 train_time:118072ms step_avg:61.15ms
step:1932/2245 train_time:118133ms step_avg:61.15ms
step:1933/2245 train_time:118196ms step_avg:61.15ms
step:1934/2245 train_time:118256ms step_avg:61.15ms
step:1935/2245 train_time:118319ms step_avg:61.15ms
step:1936/2245 train_time:118380ms step_avg:61.15ms
step:1937/2245 train_time:118442ms step_avg:61.15ms
step:1938/2245 train_time:118503ms step_avg:61.15ms
step:1939/2245 train_time:118566ms step_avg:61.15ms
step:1940/2245 train_time:118626ms step_avg:61.15ms
step:1941/2245 train_time:118688ms step_avg:61.15ms
step:1942/2245 train_time:118748ms step_avg:61.15ms
step:1943/2245 train_time:118810ms step_avg:61.15ms
step:1944/2245 train_time:118871ms step_avg:61.15ms
step:1945/2245 train_time:118933ms step_avg:61.15ms
step:1946/2245 train_time:118994ms step_avg:61.15ms
step:1947/2245 train_time:119056ms step_avg:61.15ms
step:1948/2245 train_time:119117ms step_avg:61.15ms
step:1949/2245 train_time:119180ms step_avg:61.15ms
step:1950/2245 train_time:119241ms step_avg:61.15ms
step:1951/2245 train_time:119304ms step_avg:61.15ms
step:1952/2245 train_time:119364ms step_avg:61.15ms
step:1953/2245 train_time:119427ms step_avg:61.15ms
step:1954/2245 train_time:119486ms step_avg:61.15ms
step:1955/2245 train_time:119549ms step_avg:61.15ms
step:1956/2245 train_time:119609ms step_avg:61.15ms
step:1957/2245 train_time:119672ms step_avg:61.15ms
step:1958/2245 train_time:119732ms step_avg:61.15ms
step:1959/2245 train_time:119795ms step_avg:61.15ms
step:1960/2245 train_time:119855ms step_avg:61.15ms
step:1961/2245 train_time:119918ms step_avg:61.15ms
step:1962/2245 train_time:119978ms step_avg:61.15ms
step:1963/2245 train_time:120041ms step_avg:61.15ms
step:1964/2245 train_time:120101ms step_avg:61.15ms
step:1965/2245 train_time:120164ms step_avg:61.15ms
step:1966/2245 train_time:120225ms step_avg:61.15ms
step:1967/2245 train_time:120288ms step_avg:61.15ms
step:1968/2245 train_time:120347ms step_avg:61.15ms
step:1969/2245 train_time:120411ms step_avg:61.15ms
step:1970/2245 train_time:120471ms step_avg:61.15ms
step:1971/2245 train_time:120533ms step_avg:61.15ms
step:1972/2245 train_time:120593ms step_avg:61.15ms
step:1973/2245 train_time:120655ms step_avg:61.15ms
step:1974/2245 train_time:120716ms step_avg:61.15ms
step:1975/2245 train_time:120779ms step_avg:61.15ms
step:1976/2245 train_time:120840ms step_avg:61.15ms
step:1977/2245 train_time:120903ms step_avg:61.15ms
step:1978/2245 train_time:120964ms step_avg:61.15ms
step:1979/2245 train_time:121028ms step_avg:61.16ms
step:1980/2245 train_time:121088ms step_avg:61.16ms
step:1981/2245 train_time:121150ms step_avg:61.16ms
step:1982/2245 train_time:121211ms step_avg:61.16ms
step:1983/2245 train_time:121274ms step_avg:61.16ms
step:1984/2245 train_time:121334ms step_avg:61.16ms
step:1985/2245 train_time:121397ms step_avg:61.16ms
step:1986/2245 train_time:121458ms step_avg:61.16ms
step:1987/2245 train_time:121521ms step_avg:61.16ms
step:1988/2245 train_time:121582ms step_avg:61.16ms
step:1989/2245 train_time:121645ms step_avg:61.16ms
step:1990/2245 train_time:121704ms step_avg:61.16ms
step:1991/2245 train_time:121767ms step_avg:61.16ms
step:1992/2245 train_time:121827ms step_avg:61.16ms
step:1993/2245 train_time:121890ms step_avg:61.16ms
step:1994/2245 train_time:121951ms step_avg:61.16ms
step:1995/2245 train_time:122013ms step_avg:61.16ms
step:1996/2245 train_time:122074ms step_avg:61.16ms
step:1997/2245 train_time:122137ms step_avg:61.16ms
step:1998/2245 train_time:122198ms step_avg:61.16ms
step:1999/2245 train_time:122260ms step_avg:61.16ms
step:2000/2245 train_time:122321ms step_avg:61.16ms
step:2000/2245 val_loss:3.3249 train_time:122385ms step_avg:61.19ms
step:2001/2245 train_time:122403ms step_avg:61.17ms
step:2002/2245 train_time:122447ms step_avg:61.16ms
step:2003/2245 train_time:122514ms step_avg:61.17ms
step:2004/2245 train_time:122576ms step_avg:61.17ms
step:2005/2245 train_time:122639ms step_avg:61.17ms
step:2006/2245 train_time:122699ms step_avg:61.17ms
step:2007/2245 train_time:122762ms step_avg:61.17ms
step:2008/2245 train_time:122821ms step_avg:61.17ms
step:2009/2245 train_time:122884ms step_avg:61.17ms
step:2010/2245 train_time:122943ms step_avg:61.17ms
step:2011/2245 train_time:123006ms step_avg:61.17ms
step:2012/2245 train_time:123066ms step_avg:61.17ms
step:2013/2245 train_time:123128ms step_avg:61.17ms
step:2014/2245 train_time:123189ms step_avg:61.17ms
step:2015/2245 train_time:123251ms step_avg:61.17ms
step:2016/2245 train_time:123311ms step_avg:61.17ms
step:2017/2245 train_time:123375ms step_avg:61.17ms
step:2018/2245 train_time:123437ms step_avg:61.17ms
step:2019/2245 train_time:123501ms step_avg:61.17ms
step:2020/2245 train_time:123562ms step_avg:61.17ms
step:2021/2245 train_time:123626ms step_avg:61.17ms
step:2022/2245 train_time:123687ms step_avg:61.17ms
step:2023/2245 train_time:123750ms step_avg:61.17ms
step:2024/2245 train_time:123811ms step_avg:61.17ms
step:2025/2245 train_time:123873ms step_avg:61.17ms
step:2026/2245 train_time:123933ms step_avg:61.17ms
step:2027/2245 train_time:123996ms step_avg:61.17ms
step:2028/2245 train_time:124055ms step_avg:61.17ms
step:2029/2245 train_time:124117ms step_avg:61.17ms
step:2030/2245 train_time:124176ms step_avg:61.17ms
step:2031/2245 train_time:124239ms step_avg:61.17ms
step:2032/2245 train_time:124299ms step_avg:61.17ms
step:2033/2245 train_time:124362ms step_avg:61.17ms
step:2034/2245 train_time:124424ms step_avg:61.17ms
step:2035/2245 train_time:124487ms step_avg:61.17ms
step:2036/2245 train_time:124549ms step_avg:61.17ms
step:2037/2245 train_time:124612ms step_avg:61.17ms
step:2038/2245 train_time:124673ms step_avg:61.17ms
step:2039/2245 train_time:124735ms step_avg:61.17ms
step:2040/2245 train_time:124796ms step_avg:61.17ms
step:2041/2245 train_time:124858ms step_avg:61.17ms
step:2042/2245 train_time:124918ms step_avg:61.17ms
step:2043/2245 train_time:124981ms step_avg:61.18ms
step:2044/2245 train_time:125041ms step_avg:61.17ms
step:2045/2245 train_time:125103ms step_avg:61.18ms
step:2046/2245 train_time:125163ms step_avg:61.17ms
step:2047/2245 train_time:125227ms step_avg:61.18ms
step:2048/2245 train_time:125287ms step_avg:61.18ms
step:2049/2245 train_time:125351ms step_avg:61.18ms
step:2050/2245 train_time:125412ms step_avg:61.18ms
step:2051/2245 train_time:125476ms step_avg:61.18ms
step:2052/2245 train_time:125536ms step_avg:61.18ms
step:2053/2245 train_time:125599ms step_avg:61.18ms
step:2054/2245 train_time:125659ms step_avg:61.18ms
step:2055/2245 train_time:125723ms step_avg:61.18ms
step:2056/2245 train_time:125783ms step_avg:61.18ms
step:2057/2245 train_time:125846ms step_avg:61.18ms
step:2058/2245 train_time:125906ms step_avg:61.18ms
step:2059/2245 train_time:125970ms step_avg:61.18ms
step:2060/2245 train_time:126030ms step_avg:61.18ms
step:2061/2245 train_time:126093ms step_avg:61.18ms
step:2062/2245 train_time:126153ms step_avg:61.18ms
step:2063/2245 train_time:126216ms step_avg:61.18ms
step:2064/2245 train_time:126276ms step_avg:61.18ms
step:2065/2245 train_time:126338ms step_avg:61.18ms
step:2066/2245 train_time:126399ms step_avg:61.18ms
step:2067/2245 train_time:126463ms step_avg:61.18ms
step:2068/2245 train_time:126523ms step_avg:61.18ms
step:2069/2245 train_time:126586ms step_avg:61.18ms
step:2070/2245 train_time:126646ms step_avg:61.18ms
step:2071/2245 train_time:126710ms step_avg:61.18ms
step:2072/2245 train_time:126770ms step_avg:61.18ms
step:2073/2245 train_time:126833ms step_avg:61.18ms
step:2074/2245 train_time:126894ms step_avg:61.18ms
step:2075/2245 train_time:126957ms step_avg:61.18ms
step:2076/2245 train_time:127018ms step_avg:61.18ms
step:2077/2245 train_time:127080ms step_avg:61.18ms
step:2078/2245 train_time:127140ms step_avg:61.18ms
step:2079/2245 train_time:127204ms step_avg:61.19ms
step:2080/2245 train_time:127264ms step_avg:61.18ms
step:2081/2245 train_time:127327ms step_avg:61.19ms
step:2082/2245 train_time:127388ms step_avg:61.19ms
step:2083/2245 train_time:127451ms step_avg:61.19ms
step:2084/2245 train_time:127512ms step_avg:61.19ms
step:2085/2245 train_time:127574ms step_avg:61.19ms
step:2086/2245 train_time:127634ms step_avg:61.19ms
step:2087/2245 train_time:127697ms step_avg:61.19ms
step:2088/2245 train_time:127757ms step_avg:61.19ms
step:2089/2245 train_time:127820ms step_avg:61.19ms
step:2090/2245 train_time:127880ms step_avg:61.19ms
step:2091/2245 train_time:127943ms step_avg:61.19ms
step:2092/2245 train_time:128004ms step_avg:61.19ms
step:2093/2245 train_time:128068ms step_avg:61.19ms
step:2094/2245 train_time:128129ms step_avg:61.19ms
step:2095/2245 train_time:128192ms step_avg:61.19ms
step:2096/2245 train_time:128253ms step_avg:61.19ms
step:2097/2245 train_time:128315ms step_avg:61.19ms
step:2098/2245 train_time:128376ms step_avg:61.19ms
step:2099/2245 train_time:128438ms step_avg:61.19ms
step:2100/2245 train_time:128499ms step_avg:61.19ms
step:2101/2245 train_time:128561ms step_avg:61.19ms
step:2102/2245 train_time:128621ms step_avg:61.19ms
step:2103/2245 train_time:128684ms step_avg:61.19ms
step:2104/2245 train_time:128745ms step_avg:61.19ms
step:2105/2245 train_time:128809ms step_avg:61.19ms
step:2106/2245 train_time:128869ms step_avg:61.19ms
step:2107/2245 train_time:128933ms step_avg:61.19ms
step:2108/2245 train_time:128994ms step_avg:61.19ms
step:2109/2245 train_time:129057ms step_avg:61.19ms
step:2110/2245 train_time:129117ms step_avg:61.19ms
step:2111/2245 train_time:129179ms step_avg:61.19ms
step:2112/2245 train_time:129240ms step_avg:61.19ms
step:2113/2245 train_time:129302ms step_avg:61.19ms
step:2114/2245 train_time:129363ms step_avg:61.19ms
step:2115/2245 train_time:129426ms step_avg:61.19ms
step:2116/2245 train_time:129486ms step_avg:61.19ms
step:2117/2245 train_time:129548ms step_avg:61.19ms
step:2118/2245 train_time:129609ms step_avg:61.19ms
step:2119/2245 train_time:129672ms step_avg:61.19ms
step:2120/2245 train_time:129732ms step_avg:61.19ms
step:2121/2245 train_time:129795ms step_avg:61.20ms
step:2122/2245 train_time:129855ms step_avg:61.19ms
step:2123/2245 train_time:129918ms step_avg:61.20ms
step:2124/2245 train_time:129979ms step_avg:61.20ms
step:2125/2245 train_time:130041ms step_avg:61.20ms
step:2126/2245 train_time:130102ms step_avg:61.20ms
step:2127/2245 train_time:130164ms step_avg:61.20ms
step:2128/2245 train_time:130224ms step_avg:61.20ms
step:2129/2245 train_time:130287ms step_avg:61.20ms
step:2130/2245 train_time:130347ms step_avg:61.20ms
step:2131/2245 train_time:130411ms step_avg:61.20ms
step:2132/2245 train_time:130472ms step_avg:61.20ms
step:2133/2245 train_time:130536ms step_avg:61.20ms
step:2134/2245 train_time:130596ms step_avg:61.20ms
step:2135/2245 train_time:130658ms step_avg:61.20ms
step:2136/2245 train_time:130718ms step_avg:61.20ms
step:2137/2245 train_time:130781ms step_avg:61.20ms
step:2138/2245 train_time:130841ms step_avg:61.20ms
step:2139/2245 train_time:130904ms step_avg:61.20ms
step:2140/2245 train_time:130964ms step_avg:61.20ms
step:2141/2245 train_time:131027ms step_avg:61.20ms
step:2142/2245 train_time:131088ms step_avg:61.20ms
step:2143/2245 train_time:131151ms step_avg:61.20ms
step:2144/2245 train_time:131212ms step_avg:61.20ms
step:2145/2245 train_time:131276ms step_avg:61.20ms
step:2146/2245 train_time:131335ms step_avg:61.20ms
step:2147/2245 train_time:131398ms step_avg:61.20ms
step:2148/2245 train_time:131458ms step_avg:61.20ms
step:2149/2245 train_time:131521ms step_avg:61.20ms
step:2150/2245 train_time:131581ms step_avg:61.20ms
step:2151/2245 train_time:131644ms step_avg:61.20ms
step:2152/2245 train_time:131704ms step_avg:61.20ms
step:2153/2245 train_time:131768ms step_avg:61.20ms
step:2154/2245 train_time:131829ms step_avg:61.20ms
step:2155/2245 train_time:131891ms step_avg:61.20ms
step:2156/2245 train_time:131951ms step_avg:61.20ms
step:2157/2245 train_time:132014ms step_avg:61.20ms
step:2158/2245 train_time:132075ms step_avg:61.20ms
step:2159/2245 train_time:132138ms step_avg:61.20ms
step:2160/2245 train_time:132198ms step_avg:61.20ms
step:2161/2245 train_time:132260ms step_avg:61.20ms
step:2162/2245 train_time:132321ms step_avg:61.20ms
step:2163/2245 train_time:132383ms step_avg:61.20ms
step:2164/2245 train_time:132444ms step_avg:61.20ms
step:2165/2245 train_time:132507ms step_avg:61.20ms
step:2166/2245 train_time:132568ms step_avg:61.20ms
step:2167/2245 train_time:132630ms step_avg:61.20ms
step:2168/2245 train_time:132691ms step_avg:61.20ms
step:2169/2245 train_time:132754ms step_avg:61.21ms
step:2170/2245 train_time:132814ms step_avg:61.20ms
step:2171/2245 train_time:132878ms step_avg:61.21ms
step:2172/2245 train_time:132938ms step_avg:61.21ms
step:2173/2245 train_time:133000ms step_avg:61.21ms
step:2174/2245 train_time:133060ms step_avg:61.21ms
step:2175/2245 train_time:133123ms step_avg:61.21ms
step:2176/2245 train_time:133183ms step_avg:61.21ms
step:2177/2245 train_time:133246ms step_avg:61.21ms
step:2178/2245 train_time:133307ms step_avg:61.21ms
step:2179/2245 train_time:133370ms step_avg:61.21ms
step:2180/2245 train_time:133430ms step_avg:61.21ms
step:2181/2245 train_time:133494ms step_avg:61.21ms
step:2182/2245 train_time:133554ms step_avg:61.21ms
step:2183/2245 train_time:133616ms step_avg:61.21ms
step:2184/2245 train_time:133676ms step_avg:61.21ms
step:2185/2245 train_time:133740ms step_avg:61.21ms
step:2186/2245 train_time:133801ms step_avg:61.21ms
step:2187/2245 train_time:133863ms step_avg:61.21ms
step:2188/2245 train_time:133923ms step_avg:61.21ms
step:2189/2245 train_time:133986ms step_avg:61.21ms
step:2190/2245 train_time:134047ms step_avg:61.21ms
step:2191/2245 train_time:134110ms step_avg:61.21ms
step:2192/2245 train_time:134170ms step_avg:61.21ms
step:2193/2245 train_time:134234ms step_avg:61.21ms
step:2194/2245 train_time:134295ms step_avg:61.21ms
step:2195/2245 train_time:134357ms step_avg:61.21ms
step:2196/2245 train_time:134417ms step_avg:61.21ms
step:2197/2245 train_time:134480ms step_avg:61.21ms
step:2198/2245 train_time:134541ms step_avg:61.21ms
step:2199/2245 train_time:134603ms step_avg:61.21ms
step:2200/2245 train_time:134664ms step_avg:61.21ms
step:2201/2245 train_time:134727ms step_avg:61.21ms
step:2202/2245 train_time:134787ms step_avg:61.21ms
step:2203/2245 train_time:134850ms step_avg:61.21ms
step:2204/2245 train_time:134911ms step_avg:61.21ms
step:2205/2245 train_time:134974ms step_avg:61.21ms
step:2206/2245 train_time:135035ms step_avg:61.21ms
step:2207/2245 train_time:135097ms step_avg:61.21ms
step:2208/2245 train_time:135158ms step_avg:61.21ms
step:2209/2245 train_time:135221ms step_avg:61.21ms
step:2210/2245 train_time:135281ms step_avg:61.21ms
step:2211/2245 train_time:135344ms step_avg:61.21ms
step:2212/2245 train_time:135405ms step_avg:61.21ms
step:2213/2245 train_time:135468ms step_avg:61.21ms
step:2214/2245 train_time:135529ms step_avg:61.21ms
step:2215/2245 train_time:135592ms step_avg:61.22ms
step:2216/2245 train_time:135652ms step_avg:61.21ms
step:2217/2245 train_time:135716ms step_avg:61.22ms
step:2218/2245 train_time:135776ms step_avg:61.22ms
step:2219/2245 train_time:135838ms step_avg:61.22ms
step:2220/2245 train_time:135899ms step_avg:61.22ms
step:2221/2245 train_time:135961ms step_avg:61.22ms
step:2222/2245 train_time:136022ms step_avg:61.22ms
step:2223/2245 train_time:136085ms step_avg:61.22ms
step:2224/2245 train_time:136146ms step_avg:61.22ms
step:2225/2245 train_time:136209ms step_avg:61.22ms
step:2226/2245 train_time:136269ms step_avg:61.22ms
step:2227/2245 train_time:136333ms step_avg:61.22ms
step:2228/2245 train_time:136394ms step_avg:61.22ms
step:2229/2245 train_time:136456ms step_avg:61.22ms
step:2230/2245 train_time:136516ms step_avg:61.22ms
step:2231/2245 train_time:136580ms step_avg:61.22ms
step:2232/2245 train_time:136640ms step_avg:61.22ms
step:2233/2245 train_time:136704ms step_avg:61.22ms
step:2234/2245 train_time:136764ms step_avg:61.22ms
step:2235/2245 train_time:136827ms step_avg:61.22ms
step:2236/2245 train_time:136888ms step_avg:61.22ms
step:2237/2245 train_time:136951ms step_avg:61.22ms
step:2238/2245 train_time:137013ms step_avg:61.22ms
step:2239/2245 train_time:137075ms step_avg:61.22ms
step:2240/2245 train_time:137136ms step_avg:61.22ms
step:2241/2245 train_time:137199ms step_avg:61.22ms
step:2242/2245 train_time:137259ms step_avg:61.22ms
step:2243/2245 train_time:137322ms step_avg:61.22ms
step:2244/2245 train_time:137383ms step_avg:61.22ms
step:2245/2245 train_time:137446ms step_avg:61.22ms
step:2245/2245 val_loss:3.2790 train_time:137507ms step_avg:61.25ms
peak memory allocated: 29249 MiB reserved: 50528 MiB

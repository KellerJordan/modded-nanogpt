import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Nov 10 21:57:43 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   41C    P0            130W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   33C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   41C    P0            133W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   34C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   40C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   34C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.08ms
step:1/2245 train_time:118ms step_avg:118.18ms
step:2/2245 train_time:140ms step_avg:69.81ms
step:3/2245 train_time:178ms step_avg:59.20ms
step:4/2245 train_time:234ms step_avg:58.50ms
step:5/2245 train_time:294ms step_avg:58.73ms
step:6/2245 train_time:352ms step_avg:58.69ms
step:7/2245 train_time:413ms step_avg:59.01ms
step:8/2245 train_time:472ms step_avg:58.97ms
step:9/2245 train_time:533ms step_avg:59.24ms
step:10/2245 train_time:592ms step_avg:59.21ms
step:11/2245 train_time:653ms step_avg:59.37ms
step:12/2245 train_time:712ms step_avg:59.30ms
step:13/2245 train_time:772ms step_avg:59.42ms
step:14/2245 train_time:831ms step_avg:59.39ms
step:15/2245 train_time:893ms step_avg:59.52ms
step:16/2245 train_time:952ms step_avg:59.48ms
step:17/2245 train_time:1016ms step_avg:59.75ms
step:18/2245 train_time:1078ms step_avg:59.88ms
step:19/2245 train_time:1142ms step_avg:60.10ms
step:20/2245 train_time:1202ms step_avg:60.12ms
step:21/2245 train_time:1265ms step_avg:60.23ms
step:22/2245 train_time:1325ms step_avg:60.22ms
step:23/2245 train_time:1387ms step_avg:60.32ms
step:24/2245 train_time:1447ms step_avg:60.28ms
step:25/2245 train_time:1509ms step_avg:60.35ms
step:26/2245 train_time:1568ms step_avg:60.32ms
step:27/2245 train_time:1630ms step_avg:60.35ms
step:28/2245 train_time:1688ms step_avg:60.29ms
step:29/2245 train_time:1750ms step_avg:60.33ms
step:30/2245 train_time:1808ms step_avg:60.28ms
step:31/2245 train_time:1870ms step_avg:60.32ms
step:32/2245 train_time:1929ms step_avg:60.29ms
step:33/2245 train_time:1992ms step_avg:60.36ms
step:34/2245 train_time:2052ms step_avg:60.36ms
step:35/2245 train_time:2115ms step_avg:60.44ms
step:36/2245 train_time:2175ms step_avg:60.42ms
step:37/2245 train_time:2237ms step_avg:60.45ms
step:38/2245 train_time:2297ms step_avg:60.44ms
step:39/2245 train_time:2359ms step_avg:60.49ms
step:40/2245 train_time:2419ms step_avg:60.47ms
step:41/2245 train_time:2480ms step_avg:60.50ms
step:42/2245 train_time:2539ms step_avg:60.46ms
step:43/2245 train_time:2602ms step_avg:60.52ms
step:44/2245 train_time:2662ms step_avg:60.50ms
step:45/2245 train_time:2724ms step_avg:60.53ms
step:46/2245 train_time:2783ms step_avg:60.49ms
step:47/2245 train_time:2844ms step_avg:60.52ms
step:48/2245 train_time:2903ms step_avg:60.49ms
step:49/2245 train_time:2966ms step_avg:60.52ms
step:50/2245 train_time:3027ms step_avg:60.54ms
step:51/2245 train_time:3090ms step_avg:60.59ms
step:52/2245 train_time:3150ms step_avg:60.58ms
step:53/2245 train_time:3214ms step_avg:60.63ms
step:54/2245 train_time:3274ms step_avg:60.63ms
step:55/2245 train_time:3335ms step_avg:60.64ms
step:56/2245 train_time:3394ms step_avg:60.62ms
step:57/2245 train_time:3456ms step_avg:60.63ms
step:58/2245 train_time:3515ms step_avg:60.60ms
step:59/2245 train_time:3576ms step_avg:60.61ms
step:60/2245 train_time:3635ms step_avg:60.59ms
step:61/2245 train_time:3697ms step_avg:60.61ms
step:62/2245 train_time:3757ms step_avg:60.59ms
step:63/2245 train_time:3818ms step_avg:60.60ms
step:64/2245 train_time:3878ms step_avg:60.59ms
step:65/2245 train_time:3941ms step_avg:60.63ms
step:66/2245 train_time:4000ms step_avg:60.61ms
step:67/2245 train_time:4063ms step_avg:60.64ms
step:68/2245 train_time:4122ms step_avg:60.62ms
step:69/2245 train_time:4185ms step_avg:60.65ms
step:70/2245 train_time:4244ms step_avg:60.63ms
step:71/2245 train_time:4306ms step_avg:60.65ms
step:72/2245 train_time:4367ms step_avg:60.65ms
step:73/2245 train_time:4429ms step_avg:60.67ms
step:74/2245 train_time:4489ms step_avg:60.66ms
step:75/2245 train_time:4551ms step_avg:60.68ms
step:76/2245 train_time:4610ms step_avg:60.66ms
step:77/2245 train_time:4672ms step_avg:60.67ms
step:78/2245 train_time:4731ms step_avg:60.66ms
step:79/2245 train_time:4793ms step_avg:60.67ms
step:80/2245 train_time:4852ms step_avg:60.65ms
step:81/2245 train_time:4915ms step_avg:60.67ms
step:82/2245 train_time:4973ms step_avg:60.65ms
step:83/2245 train_time:5034ms step_avg:60.65ms
step:84/2245 train_time:5094ms step_avg:60.64ms
step:85/2245 train_time:5156ms step_avg:60.66ms
step:86/2245 train_time:5215ms step_avg:60.64ms
step:87/2245 train_time:5277ms step_avg:60.65ms
step:88/2245 train_time:5336ms step_avg:60.63ms
step:89/2245 train_time:5398ms step_avg:60.65ms
step:90/2245 train_time:5458ms step_avg:60.64ms
step:91/2245 train_time:5520ms step_avg:60.65ms
step:92/2245 train_time:5579ms step_avg:60.64ms
step:93/2245 train_time:5641ms step_avg:60.65ms
step:94/2245 train_time:5700ms step_avg:60.63ms
step:95/2245 train_time:5762ms step_avg:60.65ms
step:96/2245 train_time:5821ms step_avg:60.63ms
step:97/2245 train_time:5883ms step_avg:60.65ms
step:98/2245 train_time:5941ms step_avg:60.63ms
step:99/2245 train_time:6003ms step_avg:60.63ms
step:100/2245 train_time:6062ms step_avg:60.62ms
step:101/2245 train_time:6124ms step_avg:60.63ms
step:102/2245 train_time:6183ms step_avg:60.62ms
step:103/2245 train_time:6245ms step_avg:60.63ms
step:104/2245 train_time:6304ms step_avg:60.62ms
step:105/2245 train_time:6367ms step_avg:60.64ms
step:106/2245 train_time:6427ms step_avg:60.64ms
step:107/2245 train_time:6490ms step_avg:60.65ms
step:108/2245 train_time:6549ms step_avg:60.64ms
step:109/2245 train_time:6611ms step_avg:60.65ms
step:110/2245 train_time:6671ms step_avg:60.64ms
step:111/2245 train_time:6732ms step_avg:60.65ms
step:112/2245 train_time:6791ms step_avg:60.64ms
step:113/2245 train_time:6852ms step_avg:60.64ms
step:114/2245 train_time:6911ms step_avg:60.63ms
step:115/2245 train_time:6973ms step_avg:60.64ms
step:116/2245 train_time:7032ms step_avg:60.62ms
step:117/2245 train_time:7094ms step_avg:60.63ms
step:118/2245 train_time:7153ms step_avg:60.62ms
step:119/2245 train_time:7215ms step_avg:60.63ms
step:120/2245 train_time:7274ms step_avg:60.62ms
step:121/2245 train_time:7336ms step_avg:60.63ms
step:122/2245 train_time:7395ms step_avg:60.62ms
step:123/2245 train_time:7457ms step_avg:60.62ms
step:124/2245 train_time:7516ms step_avg:60.61ms
step:125/2245 train_time:7577ms step_avg:60.62ms
step:126/2245 train_time:7637ms step_avg:60.61ms
step:127/2245 train_time:7699ms step_avg:60.62ms
step:128/2245 train_time:7757ms step_avg:60.61ms
step:129/2245 train_time:7820ms step_avg:60.62ms
step:130/2245 train_time:7879ms step_avg:60.60ms
step:131/2245 train_time:7940ms step_avg:60.61ms
step:132/2245 train_time:7999ms step_avg:60.60ms
step:133/2245 train_time:8061ms step_avg:60.61ms
step:134/2245 train_time:8120ms step_avg:60.60ms
step:135/2245 train_time:8182ms step_avg:60.61ms
step:136/2245 train_time:8241ms step_avg:60.59ms
step:137/2245 train_time:8302ms step_avg:60.60ms
step:138/2245 train_time:8361ms step_avg:60.59ms
step:139/2245 train_time:8423ms step_avg:60.60ms
step:140/2245 train_time:8482ms step_avg:60.59ms
step:141/2245 train_time:8544ms step_avg:60.59ms
step:142/2245 train_time:8603ms step_avg:60.58ms
step:143/2245 train_time:8665ms step_avg:60.59ms
step:144/2245 train_time:8724ms step_avg:60.59ms
step:145/2245 train_time:8786ms step_avg:60.59ms
step:146/2245 train_time:8846ms step_avg:60.59ms
step:147/2245 train_time:8907ms step_avg:60.59ms
step:148/2245 train_time:8967ms step_avg:60.59ms
step:149/2245 train_time:9029ms step_avg:60.59ms
step:150/2245 train_time:9088ms step_avg:60.59ms
step:151/2245 train_time:9150ms step_avg:60.59ms
step:152/2245 train_time:9210ms step_avg:60.59ms
step:153/2245 train_time:9272ms step_avg:60.60ms
step:154/2245 train_time:9331ms step_avg:60.59ms
step:155/2245 train_time:9392ms step_avg:60.59ms
step:156/2245 train_time:9451ms step_avg:60.58ms
step:157/2245 train_time:9513ms step_avg:60.59ms
step:158/2245 train_time:9572ms step_avg:60.58ms
step:159/2245 train_time:9633ms step_avg:60.58ms
step:160/2245 train_time:9692ms step_avg:60.57ms
step:161/2245 train_time:9753ms step_avg:60.58ms
step:162/2245 train_time:9812ms step_avg:60.57ms
step:163/2245 train_time:9873ms step_avg:60.57ms
step:164/2245 train_time:9932ms step_avg:60.56ms
step:165/2245 train_time:9993ms step_avg:60.56ms
step:166/2245 train_time:10052ms step_avg:60.55ms
step:167/2245 train_time:10114ms step_avg:60.56ms
step:168/2245 train_time:10173ms step_avg:60.55ms
step:169/2245 train_time:10235ms step_avg:60.56ms
step:170/2245 train_time:10293ms step_avg:60.55ms
step:171/2245 train_time:10354ms step_avg:60.55ms
step:172/2245 train_time:10413ms step_avg:60.54ms
step:173/2245 train_time:10475ms step_avg:60.55ms
step:174/2245 train_time:10533ms step_avg:60.54ms
step:175/2245 train_time:10594ms step_avg:60.54ms
step:176/2245 train_time:10653ms step_avg:60.53ms
step:177/2245 train_time:10714ms step_avg:60.53ms
step:178/2245 train_time:10773ms step_avg:60.52ms
step:179/2245 train_time:10834ms step_avg:60.53ms
step:180/2245 train_time:10893ms step_avg:60.52ms
step:181/2245 train_time:10955ms step_avg:60.52ms
step:182/2245 train_time:11013ms step_avg:60.51ms
step:183/2245 train_time:11074ms step_avg:60.52ms
step:184/2245 train_time:11133ms step_avg:60.51ms
step:185/2245 train_time:11194ms step_avg:60.51ms
step:186/2245 train_time:11253ms step_avg:60.50ms
step:187/2245 train_time:11314ms step_avg:60.50ms
step:188/2245 train_time:11373ms step_avg:60.50ms
step:189/2245 train_time:11434ms step_avg:60.50ms
step:190/2245 train_time:11493ms step_avg:60.49ms
step:191/2245 train_time:11554ms step_avg:60.49ms
step:192/2245 train_time:11613ms step_avg:60.48ms
step:193/2245 train_time:11674ms step_avg:60.49ms
step:194/2245 train_time:11733ms step_avg:60.48ms
step:195/2245 train_time:11795ms step_avg:60.49ms
step:196/2245 train_time:11853ms step_avg:60.48ms
step:197/2245 train_time:11915ms step_avg:60.48ms
step:198/2245 train_time:11974ms step_avg:60.47ms
step:199/2245 train_time:12035ms step_avg:60.48ms
step:200/2245 train_time:12093ms step_avg:60.47ms
step:201/2245 train_time:12155ms step_avg:60.47ms
step:202/2245 train_time:12213ms step_avg:60.46ms
step:203/2245 train_time:12275ms step_avg:60.47ms
step:204/2245 train_time:12333ms step_avg:60.46ms
step:205/2245 train_time:12395ms step_avg:60.46ms
step:206/2245 train_time:12454ms step_avg:60.46ms
step:207/2245 train_time:12515ms step_avg:60.46ms
step:208/2245 train_time:12574ms step_avg:60.45ms
step:209/2245 train_time:12635ms step_avg:60.46ms
step:210/2245 train_time:12694ms step_avg:60.45ms
step:211/2245 train_time:12756ms step_avg:60.46ms
step:212/2245 train_time:12815ms step_avg:60.45ms
step:213/2245 train_time:12877ms step_avg:60.46ms
step:214/2245 train_time:12936ms step_avg:60.45ms
step:215/2245 train_time:12997ms step_avg:60.45ms
step:216/2245 train_time:13057ms step_avg:60.45ms
step:217/2245 train_time:13118ms step_avg:60.45ms
step:218/2245 train_time:13176ms step_avg:60.44ms
step:219/2245 train_time:13238ms step_avg:60.45ms
step:220/2245 train_time:13297ms step_avg:60.44ms
step:221/2245 train_time:13358ms step_avg:60.44ms
step:222/2245 train_time:13418ms step_avg:60.44ms
step:223/2245 train_time:13479ms step_avg:60.45ms
step:224/2245 train_time:13538ms step_avg:60.44ms
step:225/2245 train_time:13599ms step_avg:60.44ms
step:226/2245 train_time:13658ms step_avg:60.43ms
step:227/2245 train_time:13720ms step_avg:60.44ms
step:228/2245 train_time:13779ms step_avg:60.43ms
step:229/2245 train_time:13841ms step_avg:60.44ms
step:230/2245 train_time:13900ms step_avg:60.43ms
step:231/2245 train_time:13962ms step_avg:60.44ms
step:232/2245 train_time:14021ms step_avg:60.43ms
step:233/2245 train_time:14083ms step_avg:60.44ms
step:234/2245 train_time:14141ms step_avg:60.43ms
step:235/2245 train_time:14203ms step_avg:60.44ms
step:236/2245 train_time:14262ms step_avg:60.43ms
step:237/2245 train_time:14324ms step_avg:60.44ms
step:238/2245 train_time:14383ms step_avg:60.43ms
step:239/2245 train_time:14445ms step_avg:60.44ms
step:240/2245 train_time:14504ms step_avg:60.43ms
step:241/2245 train_time:14566ms step_avg:60.44ms
step:242/2245 train_time:14626ms step_avg:60.44ms
step:243/2245 train_time:14688ms step_avg:60.44ms
step:244/2245 train_time:14747ms step_avg:60.44ms
step:245/2245 train_time:14809ms step_avg:60.44ms
step:246/2245 train_time:14869ms step_avg:60.44ms
step:247/2245 train_time:14931ms step_avg:60.45ms
step:248/2245 train_time:14990ms step_avg:60.44ms
step:249/2245 train_time:15052ms step_avg:60.45ms
step:250/2245 train_time:15112ms step_avg:60.45ms
step:250/2245 val_loss:4.0877 train_time:15173ms step_avg:60.69ms
step:251/2245 train_time:15192ms step_avg:60.53ms
step:252/2245 train_time:15233ms step_avg:60.45ms
step:253/2245 train_time:15298ms step_avg:60.47ms
step:254/2245 train_time:15361ms step_avg:60.47ms
step:255/2245 train_time:15423ms step_avg:60.48ms
step:256/2245 train_time:15482ms step_avg:60.48ms
step:257/2245 train_time:15542ms step_avg:60.48ms
step:258/2245 train_time:15601ms step_avg:60.47ms
step:259/2245 train_time:15663ms step_avg:60.47ms
step:260/2245 train_time:15721ms step_avg:60.47ms
step:261/2245 train_time:15781ms step_avg:60.47ms
step:262/2245 train_time:15839ms step_avg:60.46ms
step:263/2245 train_time:15900ms step_avg:60.46ms
step:264/2245 train_time:15958ms step_avg:60.45ms
step:265/2245 train_time:16018ms step_avg:60.45ms
step:266/2245 train_time:16077ms step_avg:60.44ms
step:267/2245 train_time:16138ms step_avg:60.44ms
step:268/2245 train_time:16198ms step_avg:60.44ms
step:269/2245 train_time:16262ms step_avg:60.45ms
step:270/2245 train_time:16322ms step_avg:60.45ms
step:271/2245 train_time:16385ms step_avg:60.46ms
step:272/2245 train_time:16445ms step_avg:60.46ms
step:273/2245 train_time:16506ms step_avg:60.46ms
step:274/2245 train_time:16565ms step_avg:60.46ms
step:275/2245 train_time:16627ms step_avg:60.46ms
step:276/2245 train_time:16686ms step_avg:60.46ms
step:277/2245 train_time:16747ms step_avg:60.46ms
step:278/2245 train_time:16806ms step_avg:60.45ms
step:279/2245 train_time:16867ms step_avg:60.46ms
step:280/2245 train_time:16926ms step_avg:60.45ms
step:281/2245 train_time:16987ms step_avg:60.45ms
step:282/2245 train_time:17047ms step_avg:60.45ms
step:283/2245 train_time:17109ms step_avg:60.46ms
step:284/2245 train_time:17170ms step_avg:60.46ms
step:285/2245 train_time:17232ms step_avg:60.46ms
step:286/2245 train_time:17292ms step_avg:60.46ms
step:287/2245 train_time:17353ms step_avg:60.46ms
step:288/2245 train_time:17413ms step_avg:60.46ms
step:289/2245 train_time:17474ms step_avg:60.46ms
step:290/2245 train_time:17532ms step_avg:60.46ms
step:291/2245 train_time:17594ms step_avg:60.46ms
step:292/2245 train_time:17652ms step_avg:60.45ms
step:293/2245 train_time:17713ms step_avg:60.45ms
step:294/2245 train_time:17772ms step_avg:60.45ms
step:295/2245 train_time:17833ms step_avg:60.45ms
step:296/2245 train_time:17891ms step_avg:60.44ms
step:297/2245 train_time:17952ms step_avg:60.45ms
step:298/2245 train_time:18011ms step_avg:60.44ms
step:299/2245 train_time:18073ms step_avg:60.44ms
step:300/2245 train_time:18132ms step_avg:60.44ms
step:301/2245 train_time:18194ms step_avg:60.44ms
step:302/2245 train_time:18253ms step_avg:60.44ms
step:303/2245 train_time:18314ms step_avg:60.44ms
step:304/2245 train_time:18373ms step_avg:60.44ms
step:305/2245 train_time:18434ms step_avg:60.44ms
step:306/2245 train_time:18493ms step_avg:60.43ms
step:307/2245 train_time:18554ms step_avg:60.44ms
step:308/2245 train_time:18613ms step_avg:60.43ms
step:309/2245 train_time:18674ms step_avg:60.43ms
step:310/2245 train_time:18732ms step_avg:60.43ms
step:311/2245 train_time:18793ms step_avg:60.43ms
step:312/2245 train_time:18852ms step_avg:60.42ms
step:313/2245 train_time:18913ms step_avg:60.43ms
step:314/2245 train_time:18972ms step_avg:60.42ms
step:315/2245 train_time:19033ms step_avg:60.42ms
step:316/2245 train_time:19092ms step_avg:60.42ms
step:317/2245 train_time:19154ms step_avg:60.42ms
step:318/2245 train_time:19213ms step_avg:60.42ms
step:319/2245 train_time:19275ms step_avg:60.42ms
step:320/2245 train_time:19333ms step_avg:60.42ms
step:321/2245 train_time:19394ms step_avg:60.42ms
step:322/2245 train_time:19453ms step_avg:60.41ms
step:323/2245 train_time:19514ms step_avg:60.42ms
step:324/2245 train_time:19573ms step_avg:60.41ms
step:325/2245 train_time:19634ms step_avg:60.41ms
step:326/2245 train_time:19693ms step_avg:60.41ms
step:327/2245 train_time:19754ms step_avg:60.41ms
step:328/2245 train_time:19813ms step_avg:60.40ms
step:329/2245 train_time:19873ms step_avg:60.41ms
step:330/2245 train_time:19932ms step_avg:60.40ms
step:331/2245 train_time:19993ms step_avg:60.40ms
step:332/2245 train_time:20051ms step_avg:60.40ms
step:333/2245 train_time:20113ms step_avg:60.40ms
step:334/2245 train_time:20172ms step_avg:60.40ms
step:335/2245 train_time:20234ms step_avg:60.40ms
step:336/2245 train_time:20293ms step_avg:60.39ms
step:337/2245 train_time:20354ms step_avg:60.40ms
step:338/2245 train_time:20413ms step_avg:60.39ms
step:339/2245 train_time:20475ms step_avg:60.40ms
step:340/2245 train_time:20533ms step_avg:60.39ms
step:341/2245 train_time:20594ms step_avg:60.39ms
step:342/2245 train_time:20653ms step_avg:60.39ms
step:343/2245 train_time:20714ms step_avg:60.39ms
step:344/2245 train_time:20772ms step_avg:60.38ms
step:345/2245 train_time:20833ms step_avg:60.39ms
step:346/2245 train_time:20892ms step_avg:60.38ms
step:347/2245 train_time:20953ms step_avg:60.38ms
step:348/2245 train_time:21012ms step_avg:60.38ms
step:349/2245 train_time:21073ms step_avg:60.38ms
step:350/2245 train_time:21131ms step_avg:60.38ms
step:351/2245 train_time:21193ms step_avg:60.38ms
step:352/2245 train_time:21252ms step_avg:60.38ms
step:353/2245 train_time:21314ms step_avg:60.38ms
step:354/2245 train_time:21373ms step_avg:60.38ms
step:355/2245 train_time:21435ms step_avg:60.38ms
step:356/2245 train_time:21493ms step_avg:60.37ms
step:357/2245 train_time:21554ms step_avg:60.38ms
step:358/2245 train_time:21613ms step_avg:60.37ms
step:359/2245 train_time:21675ms step_avg:60.38ms
step:360/2245 train_time:21733ms step_avg:60.37ms
step:361/2245 train_time:21794ms step_avg:60.37ms
step:362/2245 train_time:21853ms step_avg:60.37ms
step:363/2245 train_time:21914ms step_avg:60.37ms
step:364/2245 train_time:21972ms step_avg:60.36ms
step:365/2245 train_time:22033ms step_avg:60.37ms
step:366/2245 train_time:22092ms step_avg:60.36ms
step:367/2245 train_time:22153ms step_avg:60.36ms
step:368/2245 train_time:22212ms step_avg:60.36ms
step:369/2245 train_time:22274ms step_avg:60.36ms
step:370/2245 train_time:22333ms step_avg:60.36ms
step:371/2245 train_time:22395ms step_avg:60.36ms
step:372/2245 train_time:22453ms step_avg:60.36ms
step:373/2245 train_time:22515ms step_avg:60.36ms
step:374/2245 train_time:22573ms step_avg:60.36ms
step:375/2245 train_time:22635ms step_avg:60.36ms
step:376/2245 train_time:22693ms step_avg:60.35ms
step:377/2245 train_time:22754ms step_avg:60.36ms
step:378/2245 train_time:22813ms step_avg:60.35ms
step:379/2245 train_time:22874ms step_avg:60.35ms
step:380/2245 train_time:22932ms step_avg:60.35ms
step:381/2245 train_time:22993ms step_avg:60.35ms
step:382/2245 train_time:23052ms step_avg:60.35ms
step:383/2245 train_time:23113ms step_avg:60.35ms
step:384/2245 train_time:23172ms step_avg:60.34ms
step:385/2245 train_time:23233ms step_avg:60.35ms
step:386/2245 train_time:23292ms step_avg:60.34ms
step:387/2245 train_time:23354ms step_avg:60.35ms
step:388/2245 train_time:23413ms step_avg:60.34ms
step:389/2245 train_time:23474ms step_avg:60.35ms
step:390/2245 train_time:23533ms step_avg:60.34ms
step:391/2245 train_time:23594ms step_avg:60.34ms
step:392/2245 train_time:23653ms step_avg:60.34ms
step:393/2245 train_time:23714ms step_avg:60.34ms
step:394/2245 train_time:23772ms step_avg:60.34ms
step:395/2245 train_time:23833ms step_avg:60.34ms
step:396/2245 train_time:23892ms step_avg:60.33ms
step:397/2245 train_time:23953ms step_avg:60.34ms
step:398/2245 train_time:24012ms step_avg:60.33ms
step:399/2245 train_time:24073ms step_avg:60.33ms
step:400/2245 train_time:24131ms step_avg:60.33ms
step:401/2245 train_time:24193ms step_avg:60.33ms
step:402/2245 train_time:24252ms step_avg:60.33ms
step:403/2245 train_time:24314ms step_avg:60.33ms
step:404/2245 train_time:24372ms step_avg:60.33ms
step:405/2245 train_time:24434ms step_avg:60.33ms
step:406/2245 train_time:24493ms step_avg:60.33ms
step:407/2245 train_time:24554ms step_avg:60.33ms
step:408/2245 train_time:24613ms step_avg:60.33ms
step:409/2245 train_time:24675ms step_avg:60.33ms
step:410/2245 train_time:24733ms step_avg:60.32ms
step:411/2245 train_time:24794ms step_avg:60.33ms
step:412/2245 train_time:24853ms step_avg:60.32ms
step:413/2245 train_time:24914ms step_avg:60.32ms
step:414/2245 train_time:24972ms step_avg:60.32ms
step:415/2245 train_time:25034ms step_avg:60.32ms
step:416/2245 train_time:25092ms step_avg:60.32ms
step:417/2245 train_time:25154ms step_avg:60.32ms
step:418/2245 train_time:25213ms step_avg:60.32ms
step:419/2245 train_time:25274ms step_avg:60.32ms
step:420/2245 train_time:25332ms step_avg:60.32ms
step:421/2245 train_time:25394ms step_avg:60.32ms
step:422/2245 train_time:25452ms step_avg:60.31ms
step:423/2245 train_time:25515ms step_avg:60.32ms
step:424/2245 train_time:25573ms step_avg:60.31ms
step:425/2245 train_time:25634ms step_avg:60.32ms
step:426/2245 train_time:25693ms step_avg:60.31ms
step:427/2245 train_time:25754ms step_avg:60.31ms
step:428/2245 train_time:25812ms step_avg:60.31ms
step:429/2245 train_time:25873ms step_avg:60.31ms
step:430/2245 train_time:25932ms step_avg:60.31ms
step:431/2245 train_time:25993ms step_avg:60.31ms
step:432/2245 train_time:26052ms step_avg:60.31ms
step:433/2245 train_time:26113ms step_avg:60.31ms
step:434/2245 train_time:26172ms step_avg:60.30ms
step:435/2245 train_time:26233ms step_avg:60.31ms
step:436/2245 train_time:26292ms step_avg:60.30ms
step:437/2245 train_time:26354ms step_avg:60.31ms
step:438/2245 train_time:26413ms step_avg:60.30ms
step:439/2245 train_time:26474ms step_avg:60.31ms
step:440/2245 train_time:26533ms step_avg:60.30ms
step:441/2245 train_time:26594ms step_avg:60.30ms
step:442/2245 train_time:26652ms step_avg:60.30ms
step:443/2245 train_time:26714ms step_avg:60.30ms
step:444/2245 train_time:26772ms step_avg:60.30ms
step:445/2245 train_time:26833ms step_avg:60.30ms
step:446/2245 train_time:26892ms step_avg:60.30ms
step:447/2245 train_time:26953ms step_avg:60.30ms
step:448/2245 train_time:27012ms step_avg:60.29ms
step:449/2245 train_time:27073ms step_avg:60.30ms
step:450/2245 train_time:27131ms step_avg:60.29ms
step:451/2245 train_time:27193ms step_avg:60.30ms
step:452/2245 train_time:27252ms step_avg:60.29ms
step:453/2245 train_time:27313ms step_avg:60.29ms
step:454/2245 train_time:27372ms step_avg:60.29ms
step:455/2245 train_time:27434ms step_avg:60.29ms
step:456/2245 train_time:27493ms step_avg:60.29ms
step:457/2245 train_time:27554ms step_avg:60.29ms
step:458/2245 train_time:27613ms step_avg:60.29ms
step:459/2245 train_time:27674ms step_avg:60.29ms
step:460/2245 train_time:27733ms step_avg:60.29ms
step:461/2245 train_time:27794ms step_avg:60.29ms
step:462/2245 train_time:27852ms step_avg:60.29ms
step:463/2245 train_time:27914ms step_avg:60.29ms
step:464/2245 train_time:27973ms step_avg:60.29ms
step:465/2245 train_time:28034ms step_avg:60.29ms
step:466/2245 train_time:28093ms step_avg:60.28ms
step:467/2245 train_time:28154ms step_avg:60.29ms
step:468/2245 train_time:28213ms step_avg:60.28ms
step:469/2245 train_time:28274ms step_avg:60.29ms
step:470/2245 train_time:28333ms step_avg:60.28ms
step:471/2245 train_time:28394ms step_avg:60.28ms
step:472/2245 train_time:28453ms step_avg:60.28ms
step:473/2245 train_time:28514ms step_avg:60.28ms
step:474/2245 train_time:28573ms step_avg:60.28ms
step:475/2245 train_time:28634ms step_avg:60.28ms
step:476/2245 train_time:28693ms step_avg:60.28ms
step:477/2245 train_time:28754ms step_avg:60.28ms
step:478/2245 train_time:28813ms step_avg:60.28ms
step:479/2245 train_time:28874ms step_avg:60.28ms
step:480/2245 train_time:28932ms step_avg:60.28ms
step:481/2245 train_time:28994ms step_avg:60.28ms
step:482/2245 train_time:29052ms step_avg:60.27ms
step:483/2245 train_time:29114ms step_avg:60.28ms
step:484/2245 train_time:29173ms step_avg:60.27ms
step:485/2245 train_time:29234ms step_avg:60.28ms
step:486/2245 train_time:29292ms step_avg:60.27ms
step:487/2245 train_time:29354ms step_avg:60.27ms
step:488/2245 train_time:29413ms step_avg:60.27ms
step:489/2245 train_time:29474ms step_avg:60.27ms
step:490/2245 train_time:29533ms step_avg:60.27ms
step:491/2245 train_time:29594ms step_avg:60.27ms
step:492/2245 train_time:29653ms step_avg:60.27ms
step:493/2245 train_time:29715ms step_avg:60.27ms
step:494/2245 train_time:29774ms step_avg:60.27ms
step:495/2245 train_time:29835ms step_avg:60.27ms
step:496/2245 train_time:29893ms step_avg:60.27ms
step:497/2245 train_time:29954ms step_avg:60.27ms
step:498/2245 train_time:30013ms step_avg:60.27ms
step:499/2245 train_time:30074ms step_avg:60.27ms
step:500/2245 train_time:30133ms step_avg:60.27ms
step:500/2245 val_loss:3.8285 train_time:30195ms step_avg:60.39ms
step:501/2245 train_time:30213ms step_avg:60.31ms
step:502/2245 train_time:30256ms step_avg:60.27ms
step:503/2245 train_time:30319ms step_avg:60.28ms
step:504/2245 train_time:30379ms step_avg:60.27ms
step:505/2245 train_time:30441ms step_avg:60.28ms
step:506/2245 train_time:30500ms step_avg:60.28ms
step:507/2245 train_time:30561ms step_avg:60.28ms
step:508/2245 train_time:30620ms step_avg:60.28ms
step:509/2245 train_time:30681ms step_avg:60.28ms
step:510/2245 train_time:30740ms step_avg:60.27ms
step:511/2245 train_time:30802ms step_avg:60.28ms
step:512/2245 train_time:30861ms step_avg:60.28ms
step:513/2245 train_time:30923ms step_avg:60.28ms
step:514/2245 train_time:30982ms step_avg:60.28ms
step:515/2245 train_time:31042ms step_avg:60.28ms
step:516/2245 train_time:31101ms step_avg:60.27ms
step:517/2245 train_time:31165ms step_avg:60.28ms
step:518/2245 train_time:31225ms step_avg:60.28ms
step:519/2245 train_time:31286ms step_avg:60.28ms
step:520/2245 train_time:31345ms step_avg:60.28ms
step:521/2245 train_time:31407ms step_avg:60.28ms
step:522/2245 train_time:31466ms step_avg:60.28ms
step:523/2245 train_time:31527ms step_avg:60.28ms
step:524/2245 train_time:31586ms step_avg:60.28ms
step:525/2245 train_time:31647ms step_avg:60.28ms
step:526/2245 train_time:31706ms step_avg:60.28ms
step:527/2245 train_time:31767ms step_avg:60.28ms
step:528/2245 train_time:31826ms step_avg:60.28ms
step:529/2245 train_time:31887ms step_avg:60.28ms
step:530/2245 train_time:31946ms step_avg:60.28ms
step:531/2245 train_time:32007ms step_avg:60.28ms
step:532/2245 train_time:32066ms step_avg:60.27ms
step:533/2245 train_time:32128ms step_avg:60.28ms
step:534/2245 train_time:32187ms step_avg:60.28ms
step:535/2245 train_time:32249ms step_avg:60.28ms
step:536/2245 train_time:32308ms step_avg:60.28ms
step:537/2245 train_time:32369ms step_avg:60.28ms
step:538/2245 train_time:32428ms step_avg:60.28ms
step:539/2245 train_time:32490ms step_avg:60.28ms
step:540/2245 train_time:32549ms step_avg:60.28ms
step:541/2245 train_time:32611ms step_avg:60.28ms
step:542/2245 train_time:32669ms step_avg:60.28ms
step:543/2245 train_time:32730ms step_avg:60.28ms
step:544/2245 train_time:32789ms step_avg:60.27ms
step:545/2245 train_time:32850ms step_avg:60.28ms
step:546/2245 train_time:32909ms step_avg:60.27ms
step:547/2245 train_time:32971ms step_avg:60.28ms
step:548/2245 train_time:33029ms step_avg:60.27ms
step:549/2245 train_time:33091ms step_avg:60.28ms
step:550/2245 train_time:33150ms step_avg:60.27ms
step:551/2245 train_time:33212ms step_avg:60.28ms
step:552/2245 train_time:33271ms step_avg:60.27ms
step:553/2245 train_time:33332ms step_avg:60.28ms
step:554/2245 train_time:33391ms step_avg:60.27ms
step:555/2245 train_time:33452ms step_avg:60.27ms
step:556/2245 train_time:33511ms step_avg:60.27ms
step:557/2245 train_time:33573ms step_avg:60.28ms
step:558/2245 train_time:33632ms step_avg:60.27ms
step:559/2245 train_time:33694ms step_avg:60.28ms
step:560/2245 train_time:33754ms step_avg:60.27ms
step:561/2245 train_time:33816ms step_avg:60.28ms
step:562/2245 train_time:33875ms step_avg:60.28ms
step:563/2245 train_time:33937ms step_avg:60.28ms
step:564/2245 train_time:33996ms step_avg:60.28ms
step:565/2245 train_time:34058ms step_avg:60.28ms
step:566/2245 train_time:34118ms step_avg:60.28ms
step:567/2245 train_time:34180ms step_avg:60.28ms
step:568/2245 train_time:34239ms step_avg:60.28ms
step:569/2245 train_time:34301ms step_avg:60.28ms
step:570/2245 train_time:34360ms step_avg:60.28ms
step:571/2245 train_time:34423ms step_avg:60.28ms
step:572/2245 train_time:34482ms step_avg:60.28ms
step:573/2245 train_time:34543ms step_avg:60.28ms
step:574/2245 train_time:34602ms step_avg:60.28ms
step:575/2245 train_time:34664ms step_avg:60.28ms
step:576/2245 train_time:34723ms step_avg:60.28ms
step:577/2245 train_time:34784ms step_avg:60.28ms
step:578/2245 train_time:34843ms step_avg:60.28ms
step:579/2245 train_time:34904ms step_avg:60.28ms
step:580/2245 train_time:34963ms step_avg:60.28ms
step:581/2245 train_time:35025ms step_avg:60.28ms
step:582/2245 train_time:35083ms step_avg:60.28ms
step:583/2245 train_time:35145ms step_avg:60.28ms
step:584/2245 train_time:35204ms step_avg:60.28ms
step:585/2245 train_time:35265ms step_avg:60.28ms
step:586/2245 train_time:35324ms step_avg:60.28ms
step:587/2245 train_time:35385ms step_avg:60.28ms
step:588/2245 train_time:35444ms step_avg:60.28ms
step:589/2245 train_time:35506ms step_avg:60.28ms
step:590/2245 train_time:35566ms step_avg:60.28ms
step:591/2245 train_time:35627ms step_avg:60.28ms
step:592/2245 train_time:35686ms step_avg:60.28ms
step:593/2245 train_time:35747ms step_avg:60.28ms
step:594/2245 train_time:35805ms step_avg:60.28ms
step:595/2245 train_time:35866ms step_avg:60.28ms
step:596/2245 train_time:35925ms step_avg:60.28ms
step:597/2245 train_time:35986ms step_avg:60.28ms
step:598/2245 train_time:36045ms step_avg:60.28ms
step:599/2245 train_time:36106ms step_avg:60.28ms
step:600/2245 train_time:36166ms step_avg:60.28ms
step:601/2245 train_time:36227ms step_avg:60.28ms
step:602/2245 train_time:36285ms step_avg:60.27ms
step:603/2245 train_time:36347ms step_avg:60.28ms
step:604/2245 train_time:36405ms step_avg:60.27ms
step:605/2245 train_time:36467ms step_avg:60.28ms
step:606/2245 train_time:36526ms step_avg:60.27ms
step:607/2245 train_time:36587ms step_avg:60.28ms
step:608/2245 train_time:36646ms step_avg:60.27ms
step:609/2245 train_time:36708ms step_avg:60.28ms
step:610/2245 train_time:36766ms step_avg:60.27ms
step:611/2245 train_time:36828ms step_avg:60.27ms
step:612/2245 train_time:36887ms step_avg:60.27ms
step:613/2245 train_time:36948ms step_avg:60.27ms
step:614/2245 train_time:37007ms step_avg:60.27ms
step:615/2245 train_time:37068ms step_avg:60.27ms
step:616/2245 train_time:37127ms step_avg:60.27ms
step:617/2245 train_time:37189ms step_avg:60.27ms
step:618/2245 train_time:37248ms step_avg:60.27ms
step:619/2245 train_time:37309ms step_avg:60.27ms
step:620/2245 train_time:37368ms step_avg:60.27ms
step:621/2245 train_time:37430ms step_avg:60.27ms
step:622/2245 train_time:37489ms step_avg:60.27ms
step:623/2245 train_time:37550ms step_avg:60.27ms
step:624/2245 train_time:37610ms step_avg:60.27ms
step:625/2245 train_time:37671ms step_avg:60.27ms
step:626/2245 train_time:37730ms step_avg:60.27ms
step:627/2245 train_time:37791ms step_avg:60.27ms
step:628/2245 train_time:37850ms step_avg:60.27ms
step:629/2245 train_time:37912ms step_avg:60.27ms
step:630/2245 train_time:37971ms step_avg:60.27ms
step:631/2245 train_time:38032ms step_avg:60.27ms
step:632/2245 train_time:38091ms step_avg:60.27ms
step:633/2245 train_time:38152ms step_avg:60.27ms
step:634/2245 train_time:38211ms step_avg:60.27ms
step:635/2245 train_time:38273ms step_avg:60.27ms
step:636/2245 train_time:38332ms step_avg:60.27ms
step:637/2245 train_time:38393ms step_avg:60.27ms
step:638/2245 train_time:38452ms step_avg:60.27ms
step:639/2245 train_time:38514ms step_avg:60.27ms
step:640/2245 train_time:38573ms step_avg:60.27ms
step:641/2245 train_time:38634ms step_avg:60.27ms
step:642/2245 train_time:38693ms step_avg:60.27ms
step:643/2245 train_time:38755ms step_avg:60.27ms
step:644/2245 train_time:38815ms step_avg:60.27ms
step:645/2245 train_time:38876ms step_avg:60.27ms
step:646/2245 train_time:38936ms step_avg:60.27ms
step:647/2245 train_time:38999ms step_avg:60.28ms
step:648/2245 train_time:39059ms step_avg:60.28ms
step:649/2245 train_time:39120ms step_avg:60.28ms
step:650/2245 train_time:39180ms step_avg:60.28ms
step:651/2245 train_time:39242ms step_avg:60.28ms
step:652/2245 train_time:39301ms step_avg:60.28ms
step:653/2245 train_time:39363ms step_avg:60.28ms
step:654/2245 train_time:39423ms step_avg:60.28ms
step:655/2245 train_time:39484ms step_avg:60.28ms
step:656/2245 train_time:39542ms step_avg:60.28ms
step:657/2245 train_time:39604ms step_avg:60.28ms
step:658/2245 train_time:39663ms step_avg:60.28ms
step:659/2245 train_time:39724ms step_avg:60.28ms
step:660/2245 train_time:39783ms step_avg:60.28ms
step:661/2245 train_time:39844ms step_avg:60.28ms
step:662/2245 train_time:39903ms step_avg:60.28ms
step:663/2245 train_time:39965ms step_avg:60.28ms
step:664/2245 train_time:40023ms step_avg:60.28ms
step:665/2245 train_time:40085ms step_avg:60.28ms
step:666/2245 train_time:40144ms step_avg:60.28ms
step:667/2245 train_time:40205ms step_avg:60.28ms
step:668/2245 train_time:40264ms step_avg:60.28ms
step:669/2245 train_time:40326ms step_avg:60.28ms
step:670/2245 train_time:40385ms step_avg:60.28ms
step:671/2245 train_time:40446ms step_avg:60.28ms
step:672/2245 train_time:40505ms step_avg:60.27ms
step:673/2245 train_time:40566ms step_avg:60.28ms
step:674/2245 train_time:40624ms step_avg:60.27ms
step:675/2245 train_time:40686ms step_avg:60.27ms
step:676/2245 train_time:40744ms step_avg:60.27ms
step:677/2245 train_time:40806ms step_avg:60.27ms
step:678/2245 train_time:40864ms step_avg:60.27ms
step:679/2245 train_time:40926ms step_avg:60.27ms
step:680/2245 train_time:40984ms step_avg:60.27ms
step:681/2245 train_time:41045ms step_avg:60.27ms
step:682/2245 train_time:41104ms step_avg:60.27ms
step:683/2245 train_time:41166ms step_avg:60.27ms
step:684/2245 train_time:41225ms step_avg:60.27ms
step:685/2245 train_time:41286ms step_avg:60.27ms
step:686/2245 train_time:41345ms step_avg:60.27ms
step:687/2245 train_time:41406ms step_avg:60.27ms
step:688/2245 train_time:41465ms step_avg:60.27ms
step:689/2245 train_time:41526ms step_avg:60.27ms
step:690/2245 train_time:41584ms step_avg:60.27ms
step:691/2245 train_time:41645ms step_avg:60.27ms
step:692/2245 train_time:41704ms step_avg:60.27ms
step:693/2245 train_time:41765ms step_avg:60.27ms
step:694/2245 train_time:41824ms step_avg:60.27ms
step:695/2245 train_time:41885ms step_avg:60.27ms
step:696/2245 train_time:41944ms step_avg:60.27ms
step:697/2245 train_time:42006ms step_avg:60.27ms
step:698/2245 train_time:42064ms step_avg:60.26ms
step:699/2245 train_time:42126ms step_avg:60.27ms
step:700/2245 train_time:42185ms step_avg:60.26ms
step:701/2245 train_time:42247ms step_avg:60.27ms
step:702/2245 train_time:42306ms step_avg:60.26ms
step:703/2245 train_time:42367ms step_avg:60.27ms
step:704/2245 train_time:42426ms step_avg:60.26ms
step:705/2245 train_time:42487ms step_avg:60.27ms
step:706/2245 train_time:42546ms step_avg:60.26ms
step:707/2245 train_time:42608ms step_avg:60.27ms
step:708/2245 train_time:42667ms step_avg:60.26ms
step:709/2245 train_time:42728ms step_avg:60.27ms
step:710/2245 train_time:42787ms step_avg:60.26ms
step:711/2245 train_time:42849ms step_avg:60.27ms
step:712/2245 train_time:42908ms step_avg:60.26ms
step:713/2245 train_time:42969ms step_avg:60.27ms
step:714/2245 train_time:43028ms step_avg:60.26ms
step:715/2245 train_time:43090ms step_avg:60.27ms
step:716/2245 train_time:43149ms step_avg:60.26ms
step:717/2245 train_time:43211ms step_avg:60.27ms
step:718/2245 train_time:43739ms step_avg:60.92ms
step:719/2245 train_time:43799ms step_avg:60.92ms
step:720/2245 train_time:43857ms step_avg:60.91ms
step:721/2245 train_time:43917ms step_avg:60.91ms
step:722/2245 train_time:43976ms step_avg:60.91ms
step:723/2245 train_time:44036ms step_avg:60.91ms
step:724/2245 train_time:44094ms step_avg:60.90ms
step:725/2245 train_time:44155ms step_avg:60.90ms
step:726/2245 train_time:44213ms step_avg:60.90ms
step:727/2245 train_time:44273ms step_avg:60.90ms
step:728/2245 train_time:44331ms step_avg:60.89ms
step:729/2245 train_time:44392ms step_avg:60.89ms
step:730/2245 train_time:44451ms step_avg:60.89ms
step:731/2245 train_time:44511ms step_avg:60.89ms
step:732/2245 train_time:44571ms step_avg:60.89ms
step:733/2245 train_time:44639ms step_avg:60.90ms
step:734/2245 train_time:44703ms step_avg:60.90ms
step:735/2245 train_time:44767ms step_avg:60.91ms
step:736/2245 train_time:44826ms step_avg:60.91ms
step:737/2245 train_time:44889ms step_avg:60.91ms
step:738/2245 train_time:44948ms step_avg:60.91ms
step:739/2245 train_time:45009ms step_avg:60.91ms
step:740/2245 train_time:45069ms step_avg:60.90ms
step:741/2245 train_time:45130ms step_avg:60.90ms
step:742/2245 train_time:45190ms step_avg:60.90ms
step:743/2245 train_time:45251ms step_avg:60.90ms
step:744/2245 train_time:45310ms step_avg:60.90ms
step:745/2245 train_time:45371ms step_avg:60.90ms
step:746/2245 train_time:45430ms step_avg:60.90ms
step:747/2245 train_time:45493ms step_avg:60.90ms
step:748/2245 train_time:45553ms step_avg:60.90ms
step:749/2245 train_time:45618ms step_avg:60.91ms
step:750/2245 train_time:45680ms step_avg:60.91ms
step:750/2245 val_loss:3.6681 train_time:45745ms step_avg:60.99ms
step:751/2245 train_time:45763ms step_avg:60.94ms
step:752/2245 train_time:45805ms step_avg:60.91ms
step:753/2245 train_time:45868ms step_avg:60.91ms
step:754/2245 train_time:45929ms step_avg:60.91ms
step:755/2245 train_time:45993ms step_avg:60.92ms
step:756/2245 train_time:46053ms step_avg:60.92ms
step:757/2245 train_time:46114ms step_avg:60.92ms
step:758/2245 train_time:46173ms step_avg:60.91ms
step:759/2245 train_time:46235ms step_avg:60.92ms
step:760/2245 train_time:46294ms step_avg:60.91ms
step:761/2245 train_time:46355ms step_avg:60.91ms
step:762/2245 train_time:46414ms step_avg:60.91ms
step:763/2245 train_time:46476ms step_avg:60.91ms
step:764/2245 train_time:46535ms step_avg:60.91ms
step:765/2245 train_time:46597ms step_avg:60.91ms
step:766/2245 train_time:46661ms step_avg:60.92ms
step:767/2245 train_time:46728ms step_avg:60.92ms
step:768/2245 train_time:46789ms step_avg:60.92ms
step:769/2245 train_time:46853ms step_avg:60.93ms
step:770/2245 train_time:46914ms step_avg:60.93ms
step:771/2245 train_time:46976ms step_avg:60.93ms
step:772/2245 train_time:47036ms step_avg:60.93ms
step:773/2245 train_time:47098ms step_avg:60.93ms
step:774/2245 train_time:47157ms step_avg:60.93ms
step:775/2245 train_time:47219ms step_avg:60.93ms
step:776/2245 train_time:47278ms step_avg:60.92ms
step:777/2245 train_time:47339ms step_avg:60.93ms
step:778/2245 train_time:47398ms step_avg:60.92ms
step:779/2245 train_time:47460ms step_avg:60.92ms
step:780/2245 train_time:47518ms step_avg:60.92ms
step:781/2245 train_time:47581ms step_avg:60.92ms
step:782/2245 train_time:47642ms step_avg:60.92ms
step:783/2245 train_time:47705ms step_avg:60.93ms
step:784/2245 train_time:47766ms step_avg:60.93ms
step:785/2245 train_time:47829ms step_avg:60.93ms
step:786/2245 train_time:47890ms step_avg:60.93ms
step:787/2245 train_time:47953ms step_avg:60.93ms
step:788/2245 train_time:48013ms step_avg:60.93ms
step:789/2245 train_time:48075ms step_avg:60.93ms
step:790/2245 train_time:48135ms step_avg:60.93ms
step:791/2245 train_time:48197ms step_avg:60.93ms
step:792/2245 train_time:48256ms step_avg:60.93ms
step:793/2245 train_time:48318ms step_avg:60.93ms
step:794/2245 train_time:48377ms step_avg:60.93ms
step:795/2245 train_time:48439ms step_avg:60.93ms
step:796/2245 train_time:48498ms step_avg:60.93ms
step:797/2245 train_time:48560ms step_avg:60.93ms
step:798/2245 train_time:48620ms step_avg:60.93ms
step:799/2245 train_time:48683ms step_avg:60.93ms
step:800/2245 train_time:48743ms step_avg:60.93ms
step:801/2245 train_time:48806ms step_avg:60.93ms
step:802/2245 train_time:48866ms step_avg:60.93ms
step:803/2245 train_time:48928ms step_avg:60.93ms
step:804/2245 train_time:48989ms step_avg:60.93ms
step:805/2245 train_time:49051ms step_avg:60.93ms
step:806/2245 train_time:49112ms step_avg:60.93ms
step:807/2245 train_time:49174ms step_avg:60.93ms
step:808/2245 train_time:49233ms step_avg:60.93ms
step:809/2245 train_time:49295ms step_avg:60.93ms
step:810/2245 train_time:49354ms step_avg:60.93ms
step:811/2245 train_time:49416ms step_avg:60.93ms
step:812/2245 train_time:49476ms step_avg:60.93ms
step:813/2245 train_time:49539ms step_avg:60.93ms
step:814/2245 train_time:49599ms step_avg:60.93ms
step:815/2245 train_time:49661ms step_avg:60.93ms
step:816/2245 train_time:49722ms step_avg:60.93ms
step:817/2245 train_time:49784ms step_avg:60.94ms
step:818/2245 train_time:49844ms step_avg:60.93ms
step:819/2245 train_time:49906ms step_avg:60.93ms
step:820/2245 train_time:49966ms step_avg:60.93ms
step:821/2245 train_time:50028ms step_avg:60.94ms
step:822/2245 train_time:50088ms step_avg:60.93ms
step:823/2245 train_time:50151ms step_avg:60.94ms
step:824/2245 train_time:50210ms step_avg:60.94ms
step:825/2245 train_time:50273ms step_avg:60.94ms
step:826/2245 train_time:50333ms step_avg:60.94ms
step:827/2245 train_time:50395ms step_avg:60.94ms
step:828/2245 train_time:50455ms step_avg:60.94ms
step:829/2245 train_time:50518ms step_avg:60.94ms
step:830/2245 train_time:50578ms step_avg:60.94ms
step:831/2245 train_time:50640ms step_avg:60.94ms
step:832/2245 train_time:50700ms step_avg:60.94ms
step:833/2245 train_time:50763ms step_avg:60.94ms
step:834/2245 train_time:50822ms step_avg:60.94ms
step:835/2245 train_time:50884ms step_avg:60.94ms
step:836/2245 train_time:50944ms step_avg:60.94ms
step:837/2245 train_time:51006ms step_avg:60.94ms
step:838/2245 train_time:51066ms step_avg:60.94ms
step:839/2245 train_time:51128ms step_avg:60.94ms
step:840/2245 train_time:51188ms step_avg:60.94ms
step:841/2245 train_time:51251ms step_avg:60.94ms
step:842/2245 train_time:51310ms step_avg:60.94ms
step:843/2245 train_time:51373ms step_avg:60.94ms
step:844/2245 train_time:51433ms step_avg:60.94ms
step:845/2245 train_time:51495ms step_avg:60.94ms
step:846/2245 train_time:51556ms step_avg:60.94ms
step:847/2245 train_time:51620ms step_avg:60.94ms
step:848/2245 train_time:51680ms step_avg:60.94ms
step:849/2245 train_time:51742ms step_avg:60.95ms
step:850/2245 train_time:51802ms step_avg:60.94ms
step:851/2245 train_time:51864ms step_avg:60.94ms
step:852/2245 train_time:51924ms step_avg:60.94ms
step:853/2245 train_time:51985ms step_avg:60.94ms
step:854/2245 train_time:52045ms step_avg:60.94ms
step:855/2245 train_time:52107ms step_avg:60.94ms
step:856/2245 train_time:52167ms step_avg:60.94ms
step:857/2245 train_time:52230ms step_avg:60.94ms
step:858/2245 train_time:52290ms step_avg:60.94ms
step:859/2245 train_time:52353ms step_avg:60.95ms
step:860/2245 train_time:52412ms step_avg:60.94ms
step:861/2245 train_time:52475ms step_avg:60.95ms
step:862/2245 train_time:52536ms step_avg:60.95ms
step:863/2245 train_time:52599ms step_avg:60.95ms
step:864/2245 train_time:52660ms step_avg:60.95ms
step:865/2245 train_time:52722ms step_avg:60.95ms
step:866/2245 train_time:52782ms step_avg:60.95ms
step:867/2245 train_time:52845ms step_avg:60.95ms
step:868/2245 train_time:52904ms step_avg:60.95ms
step:869/2245 train_time:52966ms step_avg:60.95ms
step:870/2245 train_time:53025ms step_avg:60.95ms
step:871/2245 train_time:53087ms step_avg:60.95ms
step:872/2245 train_time:53147ms step_avg:60.95ms
step:873/2245 train_time:53209ms step_avg:60.95ms
step:874/2245 train_time:53269ms step_avg:60.95ms
step:875/2245 train_time:53332ms step_avg:60.95ms
step:876/2245 train_time:53392ms step_avg:60.95ms
step:877/2245 train_time:53454ms step_avg:60.95ms
step:878/2245 train_time:53514ms step_avg:60.95ms
step:879/2245 train_time:53577ms step_avg:60.95ms
step:880/2245 train_time:53638ms step_avg:60.95ms
step:881/2245 train_time:53701ms step_avg:60.95ms
step:882/2245 train_time:53761ms step_avg:60.95ms
step:883/2245 train_time:53824ms step_avg:60.96ms
step:884/2245 train_time:53884ms step_avg:60.95ms
step:885/2245 train_time:53947ms step_avg:60.96ms
step:886/2245 train_time:54006ms step_avg:60.96ms
step:887/2245 train_time:54068ms step_avg:60.96ms
step:888/2245 train_time:54127ms step_avg:60.95ms
step:889/2245 train_time:54189ms step_avg:60.96ms
step:890/2245 train_time:54249ms step_avg:60.95ms
step:891/2245 train_time:54312ms step_avg:60.96ms
step:892/2245 train_time:54372ms step_avg:60.96ms
step:893/2245 train_time:54434ms step_avg:60.96ms
step:894/2245 train_time:54495ms step_avg:60.96ms
step:895/2245 train_time:54558ms step_avg:60.96ms
step:896/2245 train_time:54618ms step_avg:60.96ms
step:897/2245 train_time:54681ms step_avg:60.96ms
step:898/2245 train_time:54742ms step_avg:60.96ms
step:899/2245 train_time:54803ms step_avg:60.96ms
step:900/2245 train_time:54863ms step_avg:60.96ms
step:901/2245 train_time:54925ms step_avg:60.96ms
step:902/2245 train_time:54985ms step_avg:60.96ms
step:903/2245 train_time:55048ms step_avg:60.96ms
step:904/2245 train_time:55107ms step_avg:60.96ms
step:905/2245 train_time:55169ms step_avg:60.96ms
step:906/2245 train_time:55228ms step_avg:60.96ms
step:907/2245 train_time:55291ms step_avg:60.96ms
step:908/2245 train_time:55351ms step_avg:60.96ms
step:909/2245 train_time:55414ms step_avg:60.96ms
step:910/2245 train_time:55474ms step_avg:60.96ms
step:911/2245 train_time:55538ms step_avg:60.96ms
step:912/2245 train_time:55598ms step_avg:60.96ms
step:913/2245 train_time:55661ms step_avg:60.96ms
step:914/2245 train_time:55721ms step_avg:60.96ms
step:915/2245 train_time:55784ms step_avg:60.97ms
step:916/2245 train_time:55844ms step_avg:60.96ms
step:917/2245 train_time:55906ms step_avg:60.97ms
step:918/2245 train_time:55965ms step_avg:60.96ms
step:919/2245 train_time:56027ms step_avg:60.97ms
step:920/2245 train_time:56086ms step_avg:60.96ms
step:921/2245 train_time:56148ms step_avg:60.96ms
step:922/2245 train_time:56208ms step_avg:60.96ms
step:923/2245 train_time:56270ms step_avg:60.96ms
step:924/2245 train_time:56329ms step_avg:60.96ms
step:925/2245 train_time:56392ms step_avg:60.96ms
step:926/2245 train_time:56452ms step_avg:60.96ms
step:927/2245 train_time:56514ms step_avg:60.96ms
step:928/2245 train_time:56574ms step_avg:60.96ms
step:929/2245 train_time:56637ms step_avg:60.97ms
step:930/2245 train_time:56697ms step_avg:60.96ms
step:931/2245 train_time:56760ms step_avg:60.97ms
step:932/2245 train_time:56820ms step_avg:60.97ms
step:933/2245 train_time:56882ms step_avg:60.97ms
step:934/2245 train_time:56942ms step_avg:60.97ms
step:935/2245 train_time:57003ms step_avg:60.97ms
step:936/2245 train_time:57063ms step_avg:60.97ms
step:937/2245 train_time:57125ms step_avg:60.97ms
step:938/2245 train_time:57184ms step_avg:60.96ms
step:939/2245 train_time:57246ms step_avg:60.97ms
step:940/2245 train_time:57306ms step_avg:60.96ms
step:941/2245 train_time:57368ms step_avg:60.97ms
step:942/2245 train_time:57428ms step_avg:60.96ms
step:943/2245 train_time:57491ms step_avg:60.97ms
step:944/2245 train_time:57551ms step_avg:60.96ms
step:945/2245 train_time:57613ms step_avg:60.97ms
step:946/2245 train_time:57674ms step_avg:60.97ms
step:947/2245 train_time:57737ms step_avg:60.97ms
step:948/2245 train_time:57797ms step_avg:60.97ms
step:949/2245 train_time:57860ms step_avg:60.97ms
step:950/2245 train_time:57920ms step_avg:60.97ms
step:951/2245 train_time:57982ms step_avg:60.97ms
step:952/2245 train_time:58043ms step_avg:60.97ms
step:953/2245 train_time:58105ms step_avg:60.97ms
step:954/2245 train_time:58164ms step_avg:60.97ms
step:955/2245 train_time:58226ms step_avg:60.97ms
step:956/2245 train_time:58286ms step_avg:60.97ms
step:957/2245 train_time:58348ms step_avg:60.97ms
step:958/2245 train_time:58408ms step_avg:60.97ms
step:959/2245 train_time:58471ms step_avg:60.97ms
step:960/2245 train_time:58531ms step_avg:60.97ms
step:961/2245 train_time:58594ms step_avg:60.97ms
step:962/2245 train_time:58654ms step_avg:60.97ms
step:963/2245 train_time:58717ms step_avg:60.97ms
step:964/2245 train_time:58778ms step_avg:60.97ms
step:965/2245 train_time:58841ms step_avg:60.98ms
step:966/2245 train_time:58901ms step_avg:60.97ms
step:967/2245 train_time:58964ms step_avg:60.98ms
step:968/2245 train_time:59023ms step_avg:60.97ms
step:969/2245 train_time:59086ms step_avg:60.98ms
step:970/2245 train_time:59145ms step_avg:60.97ms
step:971/2245 train_time:59207ms step_avg:60.98ms
step:972/2245 train_time:59267ms step_avg:60.97ms
step:973/2245 train_time:59329ms step_avg:60.97ms
step:974/2245 train_time:59388ms step_avg:60.97ms
step:975/2245 train_time:59451ms step_avg:60.97ms
step:976/2245 train_time:59510ms step_avg:60.97ms
step:977/2245 train_time:59573ms step_avg:60.98ms
step:978/2245 train_time:59633ms step_avg:60.97ms
step:979/2245 train_time:59695ms step_avg:60.98ms
step:980/2245 train_time:59757ms step_avg:60.98ms
step:981/2245 train_time:59820ms step_avg:60.98ms
step:982/2245 train_time:59880ms step_avg:60.98ms
step:983/2245 train_time:59942ms step_avg:60.98ms
step:984/2245 train_time:60001ms step_avg:60.98ms
step:985/2245 train_time:60063ms step_avg:60.98ms
step:986/2245 train_time:60122ms step_avg:60.98ms
step:987/2245 train_time:60185ms step_avg:60.98ms
step:988/2245 train_time:60245ms step_avg:60.98ms
step:989/2245 train_time:60306ms step_avg:60.98ms
step:990/2245 train_time:60366ms step_avg:60.98ms
step:991/2245 train_time:60428ms step_avg:60.98ms
step:992/2245 train_time:60488ms step_avg:60.98ms
step:993/2245 train_time:60551ms step_avg:60.98ms
step:994/2245 train_time:60611ms step_avg:60.98ms
step:995/2245 train_time:60674ms step_avg:60.98ms
step:996/2245 train_time:60734ms step_avg:60.98ms
step:997/2245 train_time:60797ms step_avg:60.98ms
step:998/2245 train_time:60858ms step_avg:60.98ms
step:999/2245 train_time:60920ms step_avg:60.98ms
step:1000/2245 train_time:60980ms step_avg:60.98ms
step:1000/2245 val_loss:3.5907 train_time:61043ms step_avg:61.04ms
step:1001/2245 train_time:61061ms step_avg:61.00ms
step:1002/2245 train_time:61106ms step_avg:60.98ms
step:1003/2245 train_time:61171ms step_avg:60.99ms
step:1004/2245 train_time:61233ms step_avg:60.99ms
step:1005/2245 train_time:61297ms step_avg:60.99ms
step:1006/2245 train_time:61358ms step_avg:60.99ms
step:1007/2245 train_time:61419ms step_avg:60.99ms
step:1008/2245 train_time:61479ms step_avg:60.99ms
step:1009/2245 train_time:61539ms step_avg:60.99ms
step:1010/2245 train_time:61599ms step_avg:60.99ms
step:1011/2245 train_time:61660ms step_avg:60.99ms
step:1012/2245 train_time:61719ms step_avg:60.99ms
step:1013/2245 train_time:61780ms step_avg:60.99ms
step:1014/2245 train_time:61839ms step_avg:60.98ms
step:1015/2245 train_time:61900ms step_avg:60.99ms
step:1016/2245 train_time:61960ms step_avg:60.98ms
step:1017/2245 train_time:62023ms step_avg:60.99ms
step:1018/2245 train_time:62085ms step_avg:60.99ms
step:1019/2245 train_time:62149ms step_avg:60.99ms
step:1020/2245 train_time:62210ms step_avg:60.99ms
step:1021/2245 train_time:62274ms step_avg:60.99ms
step:1022/2245 train_time:62335ms step_avg:60.99ms
step:1023/2245 train_time:62397ms step_avg:60.99ms
step:1024/2245 train_time:62456ms step_avg:60.99ms
step:1025/2245 train_time:62518ms step_avg:60.99ms
step:1026/2245 train_time:62578ms step_avg:60.99ms
step:1027/2245 train_time:62639ms step_avg:60.99ms
step:1028/2245 train_time:62698ms step_avg:60.99ms
step:1029/2245 train_time:62759ms step_avg:60.99ms
step:1030/2245 train_time:62818ms step_avg:60.99ms
step:1031/2245 train_time:62880ms step_avg:60.99ms
step:1032/2245 train_time:62939ms step_avg:60.99ms
step:1033/2245 train_time:63002ms step_avg:60.99ms
step:1034/2245 train_time:63063ms step_avg:60.99ms
step:1035/2245 train_time:63126ms step_avg:60.99ms
step:1036/2245 train_time:63187ms step_avg:60.99ms
step:1037/2245 train_time:63250ms step_avg:60.99ms
step:1038/2245 train_time:63311ms step_avg:60.99ms
step:1039/2245 train_time:63374ms step_avg:61.00ms
step:1040/2245 train_time:63435ms step_avg:60.99ms
step:1041/2245 train_time:63498ms step_avg:61.00ms
step:1042/2245 train_time:63557ms step_avg:61.00ms
step:1043/2245 train_time:63619ms step_avg:61.00ms
step:1044/2245 train_time:63678ms step_avg:60.99ms
step:1045/2245 train_time:63740ms step_avg:60.99ms
step:1046/2245 train_time:63799ms step_avg:60.99ms
step:1047/2245 train_time:63861ms step_avg:60.99ms
step:1048/2245 train_time:63921ms step_avg:60.99ms
step:1049/2245 train_time:63983ms step_avg:60.99ms
step:1050/2245 train_time:64043ms step_avg:60.99ms
step:1051/2245 train_time:64105ms step_avg:60.99ms
step:1052/2245 train_time:64165ms step_avg:60.99ms
step:1053/2245 train_time:64229ms step_avg:61.00ms
step:1054/2245 train_time:64290ms step_avg:61.00ms
step:1055/2245 train_time:64352ms step_avg:61.00ms
step:1056/2245 train_time:64413ms step_avg:61.00ms
step:1057/2245 train_time:64476ms step_avg:61.00ms
step:1058/2245 train_time:64536ms step_avg:61.00ms
step:1059/2245 train_time:64598ms step_avg:61.00ms
step:1060/2245 train_time:64657ms step_avg:61.00ms
step:1061/2245 train_time:64719ms step_avg:61.00ms
step:1062/2245 train_time:64778ms step_avg:61.00ms
step:1063/2245 train_time:64840ms step_avg:61.00ms
step:1064/2245 train_time:64900ms step_avg:61.00ms
step:1065/2245 train_time:64962ms step_avg:61.00ms
step:1066/2245 train_time:65022ms step_avg:61.00ms
step:1067/2245 train_time:65085ms step_avg:61.00ms
step:1068/2245 train_time:65145ms step_avg:61.00ms
step:1069/2245 train_time:65207ms step_avg:61.00ms
step:1070/2245 train_time:65267ms step_avg:61.00ms
step:1071/2245 train_time:65331ms step_avg:61.00ms
step:1072/2245 train_time:65391ms step_avg:61.00ms
step:1073/2245 train_time:65454ms step_avg:61.00ms
step:1074/2245 train_time:65514ms step_avg:61.00ms
step:1075/2245 train_time:65577ms step_avg:61.00ms
step:1076/2245 train_time:65636ms step_avg:61.00ms
step:1077/2245 train_time:65698ms step_avg:61.00ms
step:1078/2245 train_time:65757ms step_avg:61.00ms
step:1079/2245 train_time:65819ms step_avg:61.00ms
step:1080/2245 train_time:65879ms step_avg:61.00ms
step:1081/2245 train_time:65940ms step_avg:61.00ms
step:1082/2245 train_time:66000ms step_avg:61.00ms
step:1083/2245 train_time:66062ms step_avg:61.00ms
step:1084/2245 train_time:66122ms step_avg:61.00ms
step:1085/2245 train_time:66184ms step_avg:61.00ms
step:1086/2245 train_time:66244ms step_avg:61.00ms
step:1087/2245 train_time:66307ms step_avg:61.00ms
step:1088/2245 train_time:66367ms step_avg:61.00ms
step:1089/2245 train_time:66430ms step_avg:61.00ms
step:1090/2245 train_time:66491ms step_avg:61.00ms
step:1091/2245 train_time:66555ms step_avg:61.00ms
step:1092/2245 train_time:66614ms step_avg:61.00ms
step:1093/2245 train_time:66677ms step_avg:61.00ms
step:1094/2245 train_time:66736ms step_avg:61.00ms
step:1095/2245 train_time:66798ms step_avg:61.00ms
step:1096/2245 train_time:66857ms step_avg:61.00ms
step:1097/2245 train_time:66920ms step_avg:61.00ms
step:1098/2245 train_time:66979ms step_avg:61.00ms
step:1099/2245 train_time:67041ms step_avg:61.00ms
step:1100/2245 train_time:67101ms step_avg:61.00ms
step:1101/2245 train_time:67163ms step_avg:61.00ms
step:1102/2245 train_time:67223ms step_avg:61.00ms
step:1103/2245 train_time:67285ms step_avg:61.00ms
step:1104/2245 train_time:67345ms step_avg:61.00ms
step:1105/2245 train_time:67409ms step_avg:61.00ms
step:1106/2245 train_time:67469ms step_avg:61.00ms
step:1107/2245 train_time:67532ms step_avg:61.00ms
step:1108/2245 train_time:67593ms step_avg:61.00ms
step:1109/2245 train_time:67655ms step_avg:61.01ms
step:1110/2245 train_time:67716ms step_avg:61.01ms
step:1111/2245 train_time:67777ms step_avg:61.01ms
step:1112/2245 train_time:67837ms step_avg:61.00ms
step:1113/2245 train_time:67899ms step_avg:61.01ms
step:1114/2245 train_time:67959ms step_avg:61.00ms
step:1115/2245 train_time:68021ms step_avg:61.01ms
step:1116/2245 train_time:68081ms step_avg:61.00ms
step:1117/2245 train_time:68143ms step_avg:61.01ms
step:1118/2245 train_time:68203ms step_avg:61.00ms
step:1119/2245 train_time:68266ms step_avg:61.01ms
step:1120/2245 train_time:68325ms step_avg:61.00ms
step:1121/2245 train_time:68389ms step_avg:61.01ms
step:1122/2245 train_time:68449ms step_avg:61.01ms
step:1123/2245 train_time:68511ms step_avg:61.01ms
step:1124/2245 train_time:68572ms step_avg:61.01ms
step:1125/2245 train_time:68634ms step_avg:61.01ms
step:1126/2245 train_time:68694ms step_avg:61.01ms
step:1127/2245 train_time:68757ms step_avg:61.01ms
step:1128/2245 train_time:68817ms step_avg:61.01ms
step:1129/2245 train_time:68879ms step_avg:61.01ms
step:1130/2245 train_time:68938ms step_avg:61.01ms
step:1131/2245 train_time:69001ms step_avg:61.01ms
step:1132/2245 train_time:69060ms step_avg:61.01ms
step:1133/2245 train_time:69123ms step_avg:61.01ms
step:1134/2245 train_time:69184ms step_avg:61.01ms
step:1135/2245 train_time:69246ms step_avg:61.01ms
step:1136/2245 train_time:69306ms step_avg:61.01ms
step:1137/2245 train_time:69368ms step_avg:61.01ms
step:1138/2245 train_time:69428ms step_avg:61.01ms
step:1139/2245 train_time:69490ms step_avg:61.01ms
step:1140/2245 train_time:69551ms step_avg:61.01ms
step:1141/2245 train_time:69613ms step_avg:61.01ms
step:1142/2245 train_time:69674ms step_avg:61.01ms
step:1143/2245 train_time:69737ms step_avg:61.01ms
step:1144/2245 train_time:69797ms step_avg:61.01ms
step:1145/2245 train_time:69859ms step_avg:61.01ms
step:1146/2245 train_time:69918ms step_avg:61.01ms
step:1147/2245 train_time:69980ms step_avg:61.01ms
step:1148/2245 train_time:70040ms step_avg:61.01ms
step:1149/2245 train_time:70103ms step_avg:61.01ms
step:1150/2245 train_time:70162ms step_avg:61.01ms
step:1151/2245 train_time:70224ms step_avg:61.01ms
step:1152/2245 train_time:70284ms step_avg:61.01ms
step:1153/2245 train_time:70346ms step_avg:61.01ms
step:1154/2245 train_time:70406ms step_avg:61.01ms
step:1155/2245 train_time:70469ms step_avg:61.01ms
step:1156/2245 train_time:70529ms step_avg:61.01ms
step:1157/2245 train_time:70591ms step_avg:61.01ms
step:1158/2245 train_time:70651ms step_avg:61.01ms
step:1159/2245 train_time:70714ms step_avg:61.01ms
step:1160/2245 train_time:70775ms step_avg:61.01ms
step:1161/2245 train_time:70838ms step_avg:61.01ms
step:1162/2245 train_time:70898ms step_avg:61.01ms
step:1163/2245 train_time:70960ms step_avg:61.01ms
step:1164/2245 train_time:71020ms step_avg:61.01ms
step:1165/2245 train_time:71082ms step_avg:61.01ms
step:1166/2245 train_time:71142ms step_avg:61.01ms
step:1167/2245 train_time:71204ms step_avg:61.01ms
step:1168/2245 train_time:71264ms step_avg:61.01ms
step:1169/2245 train_time:71326ms step_avg:61.01ms
step:1170/2245 train_time:71387ms step_avg:61.01ms
step:1171/2245 train_time:71449ms step_avg:61.02ms
step:1172/2245 train_time:71509ms step_avg:61.01ms
step:1173/2245 train_time:71572ms step_avg:61.02ms
step:1174/2245 train_time:71632ms step_avg:61.02ms
step:1175/2245 train_time:71695ms step_avg:61.02ms
step:1176/2245 train_time:71755ms step_avg:61.02ms
step:1177/2245 train_time:71817ms step_avg:61.02ms
step:1178/2245 train_time:71878ms step_avg:61.02ms
step:1179/2245 train_time:71940ms step_avg:61.02ms
step:1180/2245 train_time:71999ms step_avg:61.02ms
step:1181/2245 train_time:72062ms step_avg:61.02ms
step:1182/2245 train_time:72122ms step_avg:61.02ms
step:1183/2245 train_time:72184ms step_avg:61.02ms
step:1184/2245 train_time:72244ms step_avg:61.02ms
step:1185/2245 train_time:72306ms step_avg:61.02ms
step:1186/2245 train_time:72365ms step_avg:61.02ms
step:1187/2245 train_time:72428ms step_avg:61.02ms
step:1188/2245 train_time:72489ms step_avg:61.02ms
step:1189/2245 train_time:72551ms step_avg:61.02ms
step:1190/2245 train_time:72612ms step_avg:61.02ms
step:1191/2245 train_time:72675ms step_avg:61.02ms
step:1192/2245 train_time:72736ms step_avg:61.02ms
step:1193/2245 train_time:72798ms step_avg:61.02ms
step:1194/2245 train_time:72857ms step_avg:61.02ms
step:1195/2245 train_time:72919ms step_avg:61.02ms
step:1196/2245 train_time:72979ms step_avg:61.02ms
step:1197/2245 train_time:73042ms step_avg:61.02ms
step:1198/2245 train_time:73101ms step_avg:61.02ms
step:1199/2245 train_time:73163ms step_avg:61.02ms
step:1200/2245 train_time:73223ms step_avg:61.02ms
step:1201/2245 train_time:73285ms step_avg:61.02ms
step:1202/2245 train_time:73344ms step_avg:61.02ms
step:1203/2245 train_time:73406ms step_avg:61.02ms
step:1204/2245 train_time:73466ms step_avg:61.02ms
step:1205/2245 train_time:73529ms step_avg:61.02ms
step:1206/2245 train_time:73589ms step_avg:61.02ms
step:1207/2245 train_time:73652ms step_avg:61.02ms
step:1208/2245 train_time:73712ms step_avg:61.02ms
step:1209/2245 train_time:73776ms step_avg:61.02ms
step:1210/2245 train_time:73836ms step_avg:61.02ms
step:1211/2245 train_time:73898ms step_avg:61.02ms
step:1212/2245 train_time:73957ms step_avg:61.02ms
step:1213/2245 train_time:74020ms step_avg:61.02ms
step:1214/2245 train_time:74081ms step_avg:61.02ms
step:1215/2245 train_time:74143ms step_avg:61.02ms
step:1216/2245 train_time:74202ms step_avg:61.02ms
step:1217/2245 train_time:74265ms step_avg:61.02ms
step:1218/2245 train_time:74324ms step_avg:61.02ms
step:1219/2245 train_time:74386ms step_avg:61.02ms
step:1220/2245 train_time:74446ms step_avg:61.02ms
step:1221/2245 train_time:74508ms step_avg:61.02ms
step:1222/2245 train_time:74568ms step_avg:61.02ms
step:1223/2245 train_time:74631ms step_avg:61.02ms
step:1224/2245 train_time:74692ms step_avg:61.02ms
step:1225/2245 train_time:74755ms step_avg:61.02ms
step:1226/2245 train_time:74815ms step_avg:61.02ms
step:1227/2245 train_time:74877ms step_avg:61.02ms
step:1228/2245 train_time:74937ms step_avg:61.02ms
step:1229/2245 train_time:75000ms step_avg:61.02ms
step:1230/2245 train_time:75060ms step_avg:61.02ms
step:1231/2245 train_time:75123ms step_avg:61.03ms
step:1232/2245 train_time:75183ms step_avg:61.03ms
step:1233/2245 train_time:75245ms step_avg:61.03ms
step:1234/2245 train_time:75305ms step_avg:61.02ms
step:1235/2245 train_time:75366ms step_avg:61.03ms
step:1236/2245 train_time:75426ms step_avg:61.02ms
step:1237/2245 train_time:75488ms step_avg:61.03ms
step:1238/2245 train_time:75548ms step_avg:61.02ms
step:1239/2245 train_time:75610ms step_avg:61.03ms
step:1240/2245 train_time:75671ms step_avg:61.02ms
step:1241/2245 train_time:75733ms step_avg:61.03ms
step:1242/2245 train_time:75794ms step_avg:61.03ms
step:1243/2245 train_time:75857ms step_avg:61.03ms
step:1244/2245 train_time:75917ms step_avg:61.03ms
step:1245/2245 train_time:75980ms step_avg:61.03ms
step:1246/2245 train_time:76040ms step_avg:61.03ms
step:1247/2245 train_time:76103ms step_avg:61.03ms
step:1248/2245 train_time:76163ms step_avg:61.03ms
step:1249/2245 train_time:76225ms step_avg:61.03ms
step:1250/2245 train_time:76285ms step_avg:61.03ms
step:1250/2245 val_loss:3.5205 train_time:76348ms step_avg:61.08ms
step:1251/2245 train_time:76366ms step_avg:61.04ms
step:1252/2245 train_time:76412ms step_avg:61.03ms
step:1253/2245 train_time:76478ms step_avg:61.04ms
step:1254/2245 train_time:76538ms step_avg:61.04ms
step:1255/2245 train_time:76601ms step_avg:61.04ms
step:1256/2245 train_time:76661ms step_avg:61.04ms
step:1257/2245 train_time:76722ms step_avg:61.04ms
step:1258/2245 train_time:76782ms step_avg:61.03ms
step:1259/2245 train_time:76843ms step_avg:61.04ms
step:1260/2245 train_time:76903ms step_avg:61.03ms
step:1261/2245 train_time:76965ms step_avg:61.04ms
step:1262/2245 train_time:77024ms step_avg:61.03ms
step:1263/2245 train_time:77087ms step_avg:61.03ms
step:1264/2245 train_time:77147ms step_avg:61.03ms
step:1265/2245 train_time:77209ms step_avg:61.04ms
step:1266/2245 train_time:77269ms step_avg:61.03ms
step:1267/2245 train_time:77333ms step_avg:61.04ms
step:1268/2245 train_time:77395ms step_avg:61.04ms
step:1269/2245 train_time:77458ms step_avg:61.04ms
step:1270/2245 train_time:77519ms step_avg:61.04ms
step:1271/2245 train_time:77581ms step_avg:61.04ms
step:1272/2245 train_time:77641ms step_avg:61.04ms
step:1273/2245 train_time:77703ms step_avg:61.04ms
step:1274/2245 train_time:77762ms step_avg:61.04ms
step:1275/2245 train_time:77824ms step_avg:61.04ms
step:1276/2245 train_time:77884ms step_avg:61.04ms
step:1277/2245 train_time:77946ms step_avg:61.04ms
step:1278/2245 train_time:78006ms step_avg:61.04ms
step:1279/2245 train_time:78067ms step_avg:61.04ms
step:1280/2245 train_time:78127ms step_avg:61.04ms
step:1281/2245 train_time:78189ms step_avg:61.04ms
step:1282/2245 train_time:78251ms step_avg:61.04ms
step:1283/2245 train_time:78315ms step_avg:61.04ms
step:1284/2245 train_time:78375ms step_avg:61.04ms
step:1285/2245 train_time:78438ms step_avg:61.04ms
step:1286/2245 train_time:78497ms step_avg:61.04ms
step:1287/2245 train_time:78559ms step_avg:61.04ms
step:1288/2245 train_time:78619ms step_avg:61.04ms
step:1289/2245 train_time:78680ms step_avg:61.04ms
step:1290/2245 train_time:78740ms step_avg:61.04ms
step:1291/2245 train_time:78802ms step_avg:61.04ms
step:1292/2245 train_time:78861ms step_avg:61.04ms
step:1293/2245 train_time:78924ms step_avg:61.04ms
step:1294/2245 train_time:78983ms step_avg:61.04ms
step:1295/2245 train_time:79045ms step_avg:61.04ms
step:1296/2245 train_time:79105ms step_avg:61.04ms
step:1297/2245 train_time:79167ms step_avg:61.04ms
step:1298/2245 train_time:79228ms step_avg:61.04ms
step:1299/2245 train_time:79291ms step_avg:61.04ms
step:1300/2245 train_time:79353ms step_avg:61.04ms
step:1301/2245 train_time:79416ms step_avg:61.04ms
step:1302/2245 train_time:79476ms step_avg:61.04ms
step:1303/2245 train_time:79538ms step_avg:61.04ms
step:1304/2245 train_time:79597ms step_avg:61.04ms
step:1305/2245 train_time:79660ms step_avg:61.04ms
step:1306/2245 train_time:79720ms step_avg:61.04ms
step:1307/2245 train_time:79782ms step_avg:61.04ms
step:1308/2245 train_time:79841ms step_avg:61.04ms
step:1309/2245 train_time:79904ms step_avg:61.04ms
step:1310/2245 train_time:79963ms step_avg:61.04ms
step:1311/2245 train_time:80025ms step_avg:61.04ms
step:1312/2245 train_time:80084ms step_avg:61.04ms
step:1313/2245 train_time:80147ms step_avg:61.04ms
step:1314/2245 train_time:80207ms step_avg:61.04ms
step:1315/2245 train_time:80271ms step_avg:61.04ms
step:1316/2245 train_time:80331ms step_avg:61.04ms
step:1317/2245 train_time:80395ms step_avg:61.04ms
step:1318/2245 train_time:80455ms step_avg:61.04ms
step:1319/2245 train_time:80517ms step_avg:61.04ms
step:1320/2245 train_time:80577ms step_avg:61.04ms
step:1321/2245 train_time:80639ms step_avg:61.04ms
step:1322/2245 train_time:80699ms step_avg:61.04ms
step:1323/2245 train_time:80761ms step_avg:61.04ms
step:1324/2245 train_time:80820ms step_avg:61.04ms
step:1325/2245 train_time:80882ms step_avg:61.04ms
step:1326/2245 train_time:80942ms step_avg:61.04ms
step:1327/2245 train_time:81004ms step_avg:61.04ms
step:1328/2245 train_time:81064ms step_avg:61.04ms
step:1329/2245 train_time:81128ms step_avg:61.04ms
step:1330/2245 train_time:81188ms step_avg:61.04ms
step:1331/2245 train_time:81251ms step_avg:61.05ms
step:1332/2245 train_time:81312ms step_avg:61.04ms
step:1333/2245 train_time:81374ms step_avg:61.05ms
step:1334/2245 train_time:81434ms step_avg:61.05ms
step:1335/2245 train_time:81497ms step_avg:61.05ms
step:1336/2245 train_time:81557ms step_avg:61.05ms
step:1337/2245 train_time:81619ms step_avg:61.05ms
step:1338/2245 train_time:81678ms step_avg:61.05ms
step:1339/2245 train_time:81741ms step_avg:61.05ms
step:1340/2245 train_time:81801ms step_avg:61.05ms
step:1341/2245 train_time:81863ms step_avg:61.05ms
step:1342/2245 train_time:81922ms step_avg:61.05ms
step:1343/2245 train_time:81984ms step_avg:61.05ms
step:1344/2245 train_time:82044ms step_avg:61.04ms
step:1345/2245 train_time:82107ms step_avg:61.05ms
step:1346/2245 train_time:82167ms step_avg:61.05ms
step:1347/2245 train_time:82230ms step_avg:61.05ms
step:1348/2245 train_time:82290ms step_avg:61.05ms
step:1349/2245 train_time:82353ms step_avg:61.05ms
step:1350/2245 train_time:82414ms step_avg:61.05ms
step:1351/2245 train_time:82476ms step_avg:61.05ms
step:1352/2245 train_time:82536ms step_avg:61.05ms
step:1353/2245 train_time:82598ms step_avg:61.05ms
step:1354/2245 train_time:82658ms step_avg:61.05ms
step:1355/2245 train_time:82721ms step_avg:61.05ms
step:1356/2245 train_time:82781ms step_avg:61.05ms
step:1357/2245 train_time:82842ms step_avg:61.05ms
step:1358/2245 train_time:82902ms step_avg:61.05ms
step:1359/2245 train_time:82964ms step_avg:61.05ms
step:1360/2245 train_time:83024ms step_avg:61.05ms
step:1361/2245 train_time:83087ms step_avg:61.05ms
step:1362/2245 train_time:83146ms step_avg:61.05ms
step:1363/2245 train_time:83209ms step_avg:61.05ms
step:1364/2245 train_time:83269ms step_avg:61.05ms
step:1365/2245 train_time:83331ms step_avg:61.05ms
step:1366/2245 train_time:83391ms step_avg:61.05ms
step:1367/2245 train_time:83455ms step_avg:61.05ms
step:1368/2245 train_time:83515ms step_avg:61.05ms
step:1369/2245 train_time:83577ms step_avg:61.05ms
step:1370/2245 train_time:83638ms step_avg:61.05ms
step:1371/2245 train_time:83700ms step_avg:61.05ms
step:1372/2245 train_time:83760ms step_avg:61.05ms
step:1373/2245 train_time:83822ms step_avg:61.05ms
step:1374/2245 train_time:83882ms step_avg:61.05ms
step:1375/2245 train_time:83943ms step_avg:61.05ms
step:1376/2245 train_time:84003ms step_avg:61.05ms
step:1377/2245 train_time:84065ms step_avg:61.05ms
step:1378/2245 train_time:84125ms step_avg:61.05ms
step:1379/2245 train_time:84188ms step_avg:61.05ms
step:1380/2245 train_time:84248ms step_avg:61.05ms
step:1381/2245 train_time:84311ms step_avg:61.05ms
step:1382/2245 train_time:84372ms step_avg:61.05ms
step:1383/2245 train_time:84435ms step_avg:61.05ms
step:1384/2245 train_time:84495ms step_avg:61.05ms
step:1385/2245 train_time:84557ms step_avg:61.05ms
step:1386/2245 train_time:84618ms step_avg:61.05ms
step:1387/2245 train_time:84679ms step_avg:61.05ms
step:1388/2245 train_time:84740ms step_avg:61.05ms
step:1389/2245 train_time:84801ms step_avg:61.05ms
step:1390/2245 train_time:84861ms step_avg:61.05ms
step:1391/2245 train_time:84924ms step_avg:61.05ms
step:1392/2245 train_time:84983ms step_avg:61.05ms
step:1393/2245 train_time:85045ms step_avg:61.05ms
step:1394/2245 train_time:85105ms step_avg:61.05ms
step:1395/2245 train_time:85168ms step_avg:61.05ms
step:1396/2245 train_time:85228ms step_avg:61.05ms
step:1397/2245 train_time:85291ms step_avg:61.05ms
step:1398/2245 train_time:85351ms step_avg:61.05ms
step:1399/2245 train_time:85415ms step_avg:61.05ms
step:1400/2245 train_time:85475ms step_avg:61.05ms
step:1401/2245 train_time:85537ms step_avg:61.05ms
step:1402/2245 train_time:85597ms step_avg:61.05ms
step:1403/2245 train_time:85660ms step_avg:61.05ms
step:1404/2245 train_time:85720ms step_avg:61.05ms
step:1405/2245 train_time:85782ms step_avg:61.05ms
step:1406/2245 train_time:85842ms step_avg:61.05ms
step:1407/2245 train_time:85904ms step_avg:61.05ms
step:1408/2245 train_time:85963ms step_avg:61.05ms
step:1409/2245 train_time:86025ms step_avg:61.05ms
step:1410/2245 train_time:86085ms step_avg:61.05ms
step:1411/2245 train_time:86147ms step_avg:61.05ms
step:1412/2245 train_time:86208ms step_avg:61.05ms
step:1413/2245 train_time:86271ms step_avg:61.06ms
step:1414/2245 train_time:86331ms step_avg:61.05ms
step:1415/2245 train_time:86393ms step_avg:61.06ms
step:1416/2245 train_time:86454ms step_avg:61.05ms
step:1417/2245 train_time:86516ms step_avg:61.06ms
step:1418/2245 train_time:86576ms step_avg:61.06ms
step:1419/2245 train_time:86639ms step_avg:61.06ms
step:1420/2245 train_time:86699ms step_avg:61.06ms
step:1421/2245 train_time:86761ms step_avg:61.06ms
step:1422/2245 train_time:86821ms step_avg:61.06ms
step:1423/2245 train_time:86883ms step_avg:61.06ms
step:1424/2245 train_time:86943ms step_avg:61.06ms
step:1425/2245 train_time:87005ms step_avg:61.06ms
step:1426/2245 train_time:87064ms step_avg:61.05ms
step:1427/2245 train_time:87127ms step_avg:61.06ms
step:1428/2245 train_time:87187ms step_avg:61.06ms
step:1429/2245 train_time:87250ms step_avg:61.06ms
step:1430/2245 train_time:87311ms step_avg:61.06ms
step:1431/2245 train_time:87373ms step_avg:61.06ms
step:1432/2245 train_time:87433ms step_avg:61.06ms
step:1433/2245 train_time:87496ms step_avg:61.06ms
step:1434/2245 train_time:87556ms step_avg:61.06ms
step:1435/2245 train_time:87619ms step_avg:61.06ms
step:1436/2245 train_time:87679ms step_avg:61.06ms
step:1437/2245 train_time:87741ms step_avg:61.06ms
step:1438/2245 train_time:87801ms step_avg:61.06ms
step:1439/2245 train_time:87863ms step_avg:61.06ms
step:1440/2245 train_time:87923ms step_avg:61.06ms
step:1441/2245 train_time:87985ms step_avg:61.06ms
step:1442/2245 train_time:88045ms step_avg:61.06ms
step:1443/2245 train_time:88107ms step_avg:61.06ms
step:1444/2245 train_time:88167ms step_avg:61.06ms
step:1445/2245 train_time:88229ms step_avg:61.06ms
step:1446/2245 train_time:88289ms step_avg:61.06ms
step:1447/2245 train_time:88353ms step_avg:61.06ms
step:1448/2245 train_time:88413ms step_avg:61.06ms
step:1449/2245 train_time:88476ms step_avg:61.06ms
step:1450/2245 train_time:88536ms step_avg:61.06ms
step:1451/2245 train_time:88598ms step_avg:61.06ms
step:1452/2245 train_time:88658ms step_avg:61.06ms
step:1453/2245 train_time:88720ms step_avg:61.06ms
step:1454/2245 train_time:88780ms step_avg:61.06ms
step:1455/2245 train_time:88842ms step_avg:61.06ms
step:1456/2245 train_time:88901ms step_avg:61.06ms
step:1457/2245 train_time:88963ms step_avg:61.06ms
step:1458/2245 train_time:89023ms step_avg:61.06ms
step:1459/2245 train_time:89085ms step_avg:61.06ms
step:1460/2245 train_time:89145ms step_avg:61.06ms
step:1461/2245 train_time:89208ms step_avg:61.06ms
step:1462/2245 train_time:89268ms step_avg:61.06ms
step:1463/2245 train_time:89331ms step_avg:61.06ms
step:1464/2245 train_time:89391ms step_avg:61.06ms
step:1465/2245 train_time:89454ms step_avg:61.06ms
step:1466/2245 train_time:89515ms step_avg:61.06ms
step:1467/2245 train_time:89577ms step_avg:61.06ms
step:1468/2245 train_time:89637ms step_avg:61.06ms
step:1469/2245 train_time:89700ms step_avg:61.06ms
step:1470/2245 train_time:89760ms step_avg:61.06ms
step:1471/2245 train_time:89822ms step_avg:61.06ms
step:1472/2245 train_time:89882ms step_avg:61.06ms
step:1473/2245 train_time:89945ms step_avg:61.06ms
step:1474/2245 train_time:90005ms step_avg:61.06ms
step:1475/2245 train_time:90068ms step_avg:61.06ms
step:1476/2245 train_time:90128ms step_avg:61.06ms
step:1477/2245 train_time:90191ms step_avg:61.06ms
step:1478/2245 train_time:90251ms step_avg:61.06ms
step:1479/2245 train_time:90314ms step_avg:61.06ms
step:1480/2245 train_time:90375ms step_avg:61.06ms
step:1481/2245 train_time:90437ms step_avg:61.06ms
step:1482/2245 train_time:90498ms step_avg:61.06ms
step:1483/2245 train_time:90561ms step_avg:61.07ms
step:1484/2245 train_time:90622ms step_avg:61.07ms
step:1485/2245 train_time:90685ms step_avg:61.07ms
step:1486/2245 train_time:90745ms step_avg:61.07ms
step:1487/2245 train_time:90807ms step_avg:61.07ms
step:1488/2245 train_time:90868ms step_avg:61.07ms
step:1489/2245 train_time:90932ms step_avg:61.07ms
step:1490/2245 train_time:90992ms step_avg:61.07ms
step:1491/2245 train_time:91055ms step_avg:61.07ms
step:1492/2245 train_time:91115ms step_avg:61.07ms
step:1493/2245 train_time:91177ms step_avg:61.07ms
step:1494/2245 train_time:91237ms step_avg:61.07ms
step:1495/2245 train_time:91300ms step_avg:61.07ms
step:1496/2245 train_time:91360ms step_avg:61.07ms
step:1497/2245 train_time:91424ms step_avg:61.07ms
step:1498/2245 train_time:91484ms step_avg:61.07ms
step:1499/2245 train_time:91547ms step_avg:61.07ms
step:1500/2245 train_time:91608ms step_avg:61.07ms
step:1500/2245 val_loss:3.4411 train_time:91673ms step_avg:61.12ms
step:1501/2245 train_time:91692ms step_avg:61.09ms
step:1502/2245 train_time:91736ms step_avg:61.08ms
step:1503/2245 train_time:91799ms step_avg:61.08ms
step:1504/2245 train_time:91860ms step_avg:61.08ms
step:1505/2245 train_time:91924ms step_avg:61.08ms
step:1506/2245 train_time:91984ms step_avg:61.08ms
step:1507/2245 train_time:92047ms step_avg:61.08ms
step:1508/2245 train_time:92106ms step_avg:61.08ms
step:1509/2245 train_time:92167ms step_avg:61.08ms
step:1510/2245 train_time:92227ms step_avg:61.08ms
step:1511/2245 train_time:92288ms step_avg:61.08ms
step:1512/2245 train_time:92348ms step_avg:61.08ms
step:1513/2245 train_time:92410ms step_avg:61.08ms
step:1514/2245 train_time:92471ms step_avg:61.08ms
step:1515/2245 train_time:92533ms step_avg:61.08ms
step:1516/2245 train_time:92596ms step_avg:61.08ms
step:1517/2245 train_time:92660ms step_avg:61.08ms
step:1518/2245 train_time:92721ms step_avg:61.08ms
step:1519/2245 train_time:92786ms step_avg:61.08ms
step:1520/2245 train_time:92847ms step_avg:61.08ms
step:1521/2245 train_time:92910ms step_avg:61.08ms
step:1522/2245 train_time:92969ms step_avg:61.08ms
step:1523/2245 train_time:93032ms step_avg:61.08ms
step:1524/2245 train_time:93092ms step_avg:61.08ms
step:1525/2245 train_time:93155ms step_avg:61.09ms
step:1526/2245 train_time:93214ms step_avg:61.08ms
step:1527/2245 train_time:93277ms step_avg:61.09ms
step:1528/2245 train_time:93338ms step_avg:61.08ms
step:1529/2245 train_time:93401ms step_avg:61.09ms
step:1530/2245 train_time:93462ms step_avg:61.09ms
step:1531/2245 train_time:93525ms step_avg:61.09ms
step:1532/2245 train_time:93586ms step_avg:61.09ms
step:1533/2245 train_time:93649ms step_avg:61.09ms
step:1534/2245 train_time:93710ms step_avg:61.09ms
step:1535/2245 train_time:93773ms step_avg:61.09ms
step:1536/2245 train_time:93833ms step_avg:61.09ms
step:1537/2245 train_time:93897ms step_avg:61.09ms
step:1538/2245 train_time:93957ms step_avg:61.09ms
step:1539/2245 train_time:94020ms step_avg:61.09ms
step:1540/2245 train_time:94081ms step_avg:61.09ms
step:1541/2245 train_time:94144ms step_avg:61.09ms
step:1542/2245 train_time:94204ms step_avg:61.09ms
step:1543/2245 train_time:94266ms step_avg:61.09ms
step:1544/2245 train_time:94326ms step_avg:61.09ms
step:1545/2245 train_time:94388ms step_avg:61.09ms
step:1546/2245 train_time:94448ms step_avg:61.09ms
step:1547/2245 train_time:94510ms step_avg:61.09ms
step:1548/2245 train_time:94571ms step_avg:61.09ms
step:1549/2245 train_time:94634ms step_avg:61.09ms
step:1550/2245 train_time:94694ms step_avg:61.09ms
step:1551/2245 train_time:94758ms step_avg:61.09ms
step:1552/2245 train_time:94819ms step_avg:61.09ms
step:1553/2245 train_time:94883ms step_avg:61.10ms
step:1554/2245 train_time:94944ms step_avg:61.10ms
step:1555/2245 train_time:95007ms step_avg:61.10ms
step:1556/2245 train_time:95067ms step_avg:61.10ms
step:1557/2245 train_time:95129ms step_avg:61.10ms
step:1558/2245 train_time:95189ms step_avg:61.10ms
step:1559/2245 train_time:95252ms step_avg:61.10ms
step:1560/2245 train_time:95312ms step_avg:61.10ms
step:1561/2245 train_time:95375ms step_avg:61.10ms
step:1562/2245 train_time:95435ms step_avg:61.10ms
step:1563/2245 train_time:95498ms step_avg:61.10ms
step:1564/2245 train_time:95558ms step_avg:61.10ms
step:1565/2245 train_time:95621ms step_avg:61.10ms
step:1566/2245 train_time:95683ms step_avg:61.10ms
step:1567/2245 train_time:95747ms step_avg:61.10ms
step:1568/2245 train_time:95807ms step_avg:61.10ms
step:1569/2245 train_time:95870ms step_avg:61.10ms
step:1570/2245 train_time:95930ms step_avg:61.10ms
step:1571/2245 train_time:95992ms step_avg:61.10ms
step:1572/2245 train_time:96053ms step_avg:61.10ms
step:1573/2245 train_time:96115ms step_avg:61.10ms
step:1574/2245 train_time:96175ms step_avg:61.10ms
step:1575/2245 train_time:96238ms step_avg:61.10ms
step:1576/2245 train_time:96299ms step_avg:61.10ms
step:1577/2245 train_time:96362ms step_avg:61.10ms
step:1578/2245 train_time:96422ms step_avg:61.10ms
step:1579/2245 train_time:96485ms step_avg:61.11ms
step:1580/2245 train_time:96546ms step_avg:61.10ms
step:1581/2245 train_time:96608ms step_avg:61.11ms
step:1582/2245 train_time:96668ms step_avg:61.10ms
step:1583/2245 train_time:96730ms step_avg:61.11ms
step:1584/2245 train_time:96792ms step_avg:61.11ms
step:1585/2245 train_time:96855ms step_avg:61.11ms
step:1586/2245 train_time:96915ms step_avg:61.11ms
step:1587/2245 train_time:96978ms step_avg:61.11ms
step:1588/2245 train_time:97038ms step_avg:61.11ms
step:1589/2245 train_time:97101ms step_avg:61.11ms
step:1590/2245 train_time:97162ms step_avg:61.11ms
step:1591/2245 train_time:97224ms step_avg:61.11ms
step:1592/2245 train_time:97285ms step_avg:61.11ms
step:1593/2245 train_time:97348ms step_avg:61.11ms
step:1594/2245 train_time:97408ms step_avg:61.11ms
step:1595/2245 train_time:97470ms step_avg:61.11ms
step:1596/2245 train_time:97530ms step_avg:61.11ms
step:1597/2245 train_time:97592ms step_avg:61.11ms
step:1598/2245 train_time:97652ms step_avg:61.11ms
step:1599/2245 train_time:97716ms step_avg:61.11ms
step:1600/2245 train_time:97776ms step_avg:61.11ms
step:1601/2245 train_time:97840ms step_avg:61.11ms
step:1602/2245 train_time:97900ms step_avg:61.11ms
step:1603/2245 train_time:97964ms step_avg:61.11ms
step:1604/2245 train_time:98024ms step_avg:61.11ms
step:1605/2245 train_time:98087ms step_avg:61.11ms
step:1606/2245 train_time:98148ms step_avg:61.11ms
step:1607/2245 train_time:98210ms step_avg:61.11ms
step:1608/2245 train_time:98270ms step_avg:61.11ms
step:1609/2245 train_time:98332ms step_avg:61.11ms
step:1610/2245 train_time:98392ms step_avg:61.11ms
step:1611/2245 train_time:98455ms step_avg:61.11ms
step:1612/2245 train_time:98515ms step_avg:61.11ms
step:1613/2245 train_time:98578ms step_avg:61.11ms
step:1614/2245 train_time:98639ms step_avg:61.11ms
step:1615/2245 train_time:98703ms step_avg:61.12ms
step:1616/2245 train_time:98763ms step_avg:61.12ms
step:1617/2245 train_time:98827ms step_avg:61.12ms
step:1618/2245 train_time:98886ms step_avg:61.12ms
step:1619/2245 train_time:98949ms step_avg:61.12ms
step:1620/2245 train_time:99009ms step_avg:61.12ms
step:1621/2245 train_time:99072ms step_avg:61.12ms
step:1622/2245 train_time:99132ms step_avg:61.12ms
step:1623/2245 train_time:99195ms step_avg:61.12ms
step:1624/2245 train_time:99256ms step_avg:61.12ms
step:1625/2245 train_time:99319ms step_avg:61.12ms
step:1626/2245 train_time:99380ms step_avg:61.12ms
step:1627/2245 train_time:99443ms step_avg:61.12ms
step:1628/2245 train_time:99503ms step_avg:61.12ms
step:1629/2245 train_time:99567ms step_avg:61.12ms
step:1630/2245 train_time:99627ms step_avg:61.12ms
step:1631/2245 train_time:99689ms step_avg:61.12ms
step:1632/2245 train_time:99750ms step_avg:61.12ms
step:1633/2245 train_time:99812ms step_avg:61.12ms
step:1634/2245 train_time:99872ms step_avg:61.12ms
step:1635/2245 train_time:99935ms step_avg:61.12ms
step:1636/2245 train_time:99995ms step_avg:61.12ms
step:1637/2245 train_time:100058ms step_avg:61.12ms
step:1638/2245 train_time:100118ms step_avg:61.12ms
step:1639/2245 train_time:100181ms step_avg:61.12ms
step:1640/2245 train_time:100242ms step_avg:61.12ms
step:1641/2245 train_time:100305ms step_avg:61.12ms
step:1642/2245 train_time:100365ms step_avg:61.12ms
step:1643/2245 train_time:100427ms step_avg:61.12ms
step:1644/2245 train_time:100487ms step_avg:61.12ms
step:1645/2245 train_time:100550ms step_avg:61.12ms
step:1646/2245 train_time:100610ms step_avg:61.12ms
step:1647/2245 train_time:100674ms step_avg:61.13ms
step:1648/2245 train_time:100734ms step_avg:61.12ms
step:1649/2245 train_time:100797ms step_avg:61.13ms
step:1650/2245 train_time:100858ms step_avg:61.13ms
step:1651/2245 train_time:100921ms step_avg:61.13ms
step:1652/2245 train_time:100981ms step_avg:61.13ms
step:1653/2245 train_time:101044ms step_avg:61.13ms
step:1654/2245 train_time:101104ms step_avg:61.13ms
step:1655/2245 train_time:101167ms step_avg:61.13ms
step:1656/2245 train_time:101227ms step_avg:61.13ms
step:1657/2245 train_time:101289ms step_avg:61.13ms
step:1658/2245 train_time:101350ms step_avg:61.13ms
step:1659/2245 train_time:101412ms step_avg:61.13ms
step:1660/2245 train_time:101472ms step_avg:61.13ms
step:1661/2245 train_time:101535ms step_avg:61.13ms
step:1662/2245 train_time:101595ms step_avg:61.13ms
step:1663/2245 train_time:101658ms step_avg:61.13ms
step:1664/2245 train_time:101718ms step_avg:61.13ms
step:1665/2245 train_time:101782ms step_avg:61.13ms
step:1666/2245 train_time:101843ms step_avg:61.13ms
step:1667/2245 train_time:101907ms step_avg:61.13ms
step:1668/2245 train_time:101967ms step_avg:61.13ms
step:1669/2245 train_time:102029ms step_avg:61.13ms
step:1670/2245 train_time:102089ms step_avg:61.13ms
step:1671/2245 train_time:102152ms step_avg:61.13ms
step:1672/2245 train_time:102212ms step_avg:61.13ms
step:1673/2245 train_time:102274ms step_avg:61.13ms
step:1674/2245 train_time:102334ms step_avg:61.13ms
step:1675/2245 train_time:102398ms step_avg:61.13ms
step:1676/2245 train_time:102458ms step_avg:61.13ms
step:1677/2245 train_time:102520ms step_avg:61.13ms
step:1678/2245 train_time:102581ms step_avg:61.13ms
step:1679/2245 train_time:102644ms step_avg:61.13ms
step:1680/2245 train_time:102705ms step_avg:61.13ms
step:1681/2245 train_time:102768ms step_avg:61.14ms
step:1682/2245 train_time:102828ms step_avg:61.13ms
step:1683/2245 train_time:102891ms step_avg:61.14ms
step:1684/2245 train_time:102952ms step_avg:61.14ms
step:1685/2245 train_time:103014ms step_avg:61.14ms
step:1686/2245 train_time:103074ms step_avg:61.14ms
step:1687/2245 train_time:103137ms step_avg:61.14ms
step:1688/2245 train_time:103198ms step_avg:61.14ms
step:1689/2245 train_time:103261ms step_avg:61.14ms
step:1690/2245 train_time:103322ms step_avg:61.14ms
step:1691/2245 train_time:103385ms step_avg:61.14ms
step:1692/2245 train_time:103445ms step_avg:61.14ms
step:1693/2245 train_time:103508ms step_avg:61.14ms
step:1694/2245 train_time:103568ms step_avg:61.14ms
step:1695/2245 train_time:103631ms step_avg:61.14ms
step:1696/2245 train_time:103691ms step_avg:61.14ms
step:1697/2245 train_time:103754ms step_avg:61.14ms
step:1698/2245 train_time:103814ms step_avg:61.14ms
step:1699/2245 train_time:103878ms step_avg:61.14ms
step:1700/2245 train_time:103938ms step_avg:61.14ms
step:1701/2245 train_time:104002ms step_avg:61.14ms
step:1702/2245 train_time:104062ms step_avg:61.14ms
step:1703/2245 train_time:104125ms step_avg:61.14ms
step:1704/2245 train_time:104185ms step_avg:61.14ms
step:1705/2245 train_time:104248ms step_avg:61.14ms
step:1706/2245 train_time:104307ms step_avg:61.14ms
step:1707/2245 train_time:104370ms step_avg:61.14ms
step:1708/2245 train_time:104431ms step_avg:61.14ms
step:1709/2245 train_time:104493ms step_avg:61.14ms
step:1710/2245 train_time:104555ms step_avg:61.14ms
step:1711/2245 train_time:104617ms step_avg:61.14ms
step:1712/2245 train_time:104677ms step_avg:61.14ms
step:1713/2245 train_time:104740ms step_avg:61.14ms
step:1714/2245 train_time:104801ms step_avg:61.14ms
step:1715/2245 train_time:104864ms step_avg:61.15ms
step:1716/2245 train_time:104925ms step_avg:61.15ms
step:1717/2245 train_time:104988ms step_avg:61.15ms
step:1718/2245 train_time:105048ms step_avg:61.15ms
step:1719/2245 train_time:105110ms step_avg:61.15ms
step:1720/2245 train_time:105170ms step_avg:61.15ms
step:1721/2245 train_time:105233ms step_avg:61.15ms
step:1722/2245 train_time:105293ms step_avg:61.15ms
step:1723/2245 train_time:105356ms step_avg:61.15ms
step:1724/2245 train_time:105416ms step_avg:61.15ms
step:1725/2245 train_time:105479ms step_avg:61.15ms
step:1726/2245 train_time:105541ms step_avg:61.15ms
step:1727/2245 train_time:105604ms step_avg:61.15ms
step:1728/2245 train_time:105664ms step_avg:61.15ms
step:1729/2245 train_time:105727ms step_avg:61.15ms
step:1730/2245 train_time:105786ms step_avg:61.15ms
step:1731/2245 train_time:105849ms step_avg:61.15ms
step:1732/2245 train_time:105909ms step_avg:61.15ms
step:1733/2245 train_time:105972ms step_avg:61.15ms
step:1734/2245 train_time:106032ms step_avg:61.15ms
step:1735/2245 train_time:106095ms step_avg:61.15ms
step:1736/2245 train_time:106155ms step_avg:61.15ms
step:1737/2245 train_time:106218ms step_avg:61.15ms
step:1738/2245 train_time:106279ms step_avg:61.15ms
step:1739/2245 train_time:106341ms step_avg:61.15ms
step:1740/2245 train_time:106402ms step_avg:61.15ms
step:1741/2245 train_time:106464ms step_avg:61.15ms
step:1742/2245 train_time:106525ms step_avg:61.15ms
step:1743/2245 train_time:106587ms step_avg:61.15ms
step:1744/2245 train_time:106647ms step_avg:61.15ms
step:1745/2245 train_time:106709ms step_avg:61.15ms
step:1746/2245 train_time:106769ms step_avg:61.15ms
step:1747/2245 train_time:106832ms step_avg:61.15ms
step:1748/2245 train_time:106893ms step_avg:61.15ms
step:1749/2245 train_time:106955ms step_avg:61.15ms
step:1750/2245 train_time:107015ms step_avg:61.15ms
step:1750/2245 val_loss:3.3772 train_time:107080ms step_avg:61.19ms
step:1751/2245 train_time:107098ms step_avg:61.16ms
step:1752/2245 train_time:107144ms step_avg:61.16ms
step:1753/2245 train_time:107209ms step_avg:61.16ms
step:1754/2245 train_time:107270ms step_avg:61.16ms
step:1755/2245 train_time:107333ms step_avg:61.16ms
step:1756/2245 train_time:107395ms step_avg:61.16ms
step:1757/2245 train_time:107457ms step_avg:61.16ms
step:1758/2245 train_time:107517ms step_avg:61.16ms
step:1759/2245 train_time:107580ms step_avg:61.16ms
step:1760/2245 train_time:107639ms step_avg:61.16ms
step:1761/2245 train_time:107702ms step_avg:61.16ms
step:1762/2245 train_time:107762ms step_avg:61.16ms
step:1763/2245 train_time:107824ms step_avg:61.16ms
step:1764/2245 train_time:107884ms step_avg:61.16ms
step:1765/2245 train_time:107946ms step_avg:61.16ms
step:1766/2245 train_time:108007ms step_avg:61.16ms
step:1767/2245 train_time:108072ms step_avg:61.16ms
step:1768/2245 train_time:108133ms step_avg:61.16ms
step:1769/2245 train_time:108198ms step_avg:61.16ms
step:1770/2245 train_time:108259ms step_avg:61.16ms
step:1771/2245 train_time:108322ms step_avg:61.16ms
step:1772/2245 train_time:108382ms step_avg:61.16ms
step:1773/2245 train_time:108445ms step_avg:61.16ms
step:1774/2245 train_time:108505ms step_avg:61.16ms
step:1775/2245 train_time:108567ms step_avg:61.16ms
step:1776/2245 train_time:108627ms step_avg:61.16ms
step:1777/2245 train_time:108690ms step_avg:61.16ms
step:1778/2245 train_time:108750ms step_avg:61.16ms
step:1779/2245 train_time:108813ms step_avg:61.17ms
step:1780/2245 train_time:108874ms step_avg:61.17ms
step:1781/2245 train_time:108937ms step_avg:61.17ms
step:1782/2245 train_time:108997ms step_avg:61.17ms
step:1783/2245 train_time:109060ms step_avg:61.17ms
step:1784/2245 train_time:109120ms step_avg:61.17ms
step:1785/2245 train_time:109183ms step_avg:61.17ms
step:1786/2245 train_time:109243ms step_avg:61.17ms
step:1787/2245 train_time:109306ms step_avg:61.17ms
step:1788/2245 train_time:109367ms step_avg:61.17ms
step:1789/2245 train_time:109429ms step_avg:61.17ms
step:1790/2245 train_time:109489ms step_avg:61.17ms
step:1791/2245 train_time:109552ms step_avg:61.17ms
step:1792/2245 train_time:109613ms step_avg:61.17ms
step:1793/2245 train_time:109677ms step_avg:61.17ms
step:1794/2245 train_time:109737ms step_avg:61.17ms
step:1795/2245 train_time:109799ms step_avg:61.17ms
step:1796/2245 train_time:109859ms step_avg:61.17ms
step:1797/2245 train_time:109922ms step_avg:61.17ms
step:1798/2245 train_time:109982ms step_avg:61.17ms
step:1799/2245 train_time:110045ms step_avg:61.17ms
step:1800/2245 train_time:110106ms step_avg:61.17ms
step:1801/2245 train_time:110169ms step_avg:61.17ms
step:1802/2245 train_time:110230ms step_avg:61.17ms
step:1803/2245 train_time:110293ms step_avg:61.17ms
step:1804/2245 train_time:110353ms step_avg:61.17ms
step:1805/2245 train_time:110416ms step_avg:61.17ms
step:1806/2245 train_time:110477ms step_avg:61.17ms
step:1807/2245 train_time:110540ms step_avg:61.17ms
step:1808/2245 train_time:110600ms step_avg:61.17ms
step:1809/2245 train_time:110662ms step_avg:61.17ms
step:1810/2245 train_time:110722ms step_avg:61.17ms
step:1811/2245 train_time:110784ms step_avg:61.17ms
step:1812/2245 train_time:110844ms step_avg:61.17ms
step:1813/2245 train_time:110907ms step_avg:61.17ms
step:1814/2245 train_time:110968ms step_avg:61.17ms
step:1815/2245 train_time:111031ms step_avg:61.17ms
step:1816/2245 train_time:111092ms step_avg:61.17ms
step:1817/2245 train_time:111155ms step_avg:61.18ms
step:1818/2245 train_time:111217ms step_avg:61.18ms
step:1819/2245 train_time:111280ms step_avg:61.18ms
step:1820/2245 train_time:111340ms step_avg:61.18ms
step:1821/2245 train_time:111403ms step_avg:61.18ms
step:1822/2245 train_time:111464ms step_avg:61.18ms
step:1823/2245 train_time:111526ms step_avg:61.18ms
step:1824/2245 train_time:111587ms step_avg:61.18ms
step:1825/2245 train_time:111650ms step_avg:61.18ms
step:1826/2245 train_time:111711ms step_avg:61.18ms
step:1827/2245 train_time:111774ms step_avg:61.18ms
step:1828/2245 train_time:111835ms step_avg:61.18ms
step:1829/2245 train_time:111898ms step_avg:61.18ms
step:1830/2245 train_time:111958ms step_avg:61.18ms
step:1831/2245 train_time:112020ms step_avg:61.18ms
step:1832/2245 train_time:112081ms step_avg:61.18ms
step:1833/2245 train_time:112144ms step_avg:61.18ms
step:1834/2245 train_time:112204ms step_avg:61.18ms
step:1835/2245 train_time:112267ms step_avg:61.18ms
step:1836/2245 train_time:112328ms step_avg:61.18ms
step:1837/2245 train_time:112391ms step_avg:61.18ms
step:1838/2245 train_time:112451ms step_avg:61.18ms
step:1839/2245 train_time:112514ms step_avg:61.18ms
step:1840/2245 train_time:112576ms step_avg:61.18ms
step:1841/2245 train_time:112639ms step_avg:61.18ms
step:1842/2245 train_time:112699ms step_avg:61.18ms
step:1843/2245 train_time:112762ms step_avg:61.18ms
step:1844/2245 train_time:112822ms step_avg:61.18ms
step:1845/2245 train_time:112884ms step_avg:61.18ms
step:1846/2245 train_time:112944ms step_avg:61.18ms
step:1847/2245 train_time:113007ms step_avg:61.18ms
step:1848/2245 train_time:113067ms step_avg:61.18ms
step:1849/2245 train_time:113130ms step_avg:61.18ms
step:1850/2245 train_time:113191ms step_avg:61.18ms
step:1851/2245 train_time:113254ms step_avg:61.19ms
step:1852/2245 train_time:113315ms step_avg:61.19ms
step:1853/2245 train_time:113378ms step_avg:61.19ms
step:1854/2245 train_time:113438ms step_avg:61.19ms
step:1855/2245 train_time:113501ms step_avg:61.19ms
step:1856/2245 train_time:113561ms step_avg:61.19ms
step:1857/2245 train_time:113624ms step_avg:61.19ms
step:1858/2245 train_time:113685ms step_avg:61.19ms
step:1859/2245 train_time:113747ms step_avg:61.19ms
step:1860/2245 train_time:113808ms step_avg:61.19ms
step:1861/2245 train_time:113872ms step_avg:61.19ms
step:1862/2245 train_time:113933ms step_avg:61.19ms
step:1863/2245 train_time:113996ms step_avg:61.19ms
step:1864/2245 train_time:114057ms step_avg:61.19ms
step:1865/2245 train_time:114119ms step_avg:61.19ms
step:1866/2245 train_time:114180ms step_avg:61.19ms
step:1867/2245 train_time:114242ms step_avg:61.19ms
step:1868/2245 train_time:114302ms step_avg:61.19ms
step:1869/2245 train_time:114365ms step_avg:61.19ms
step:1870/2245 train_time:114425ms step_avg:61.19ms
step:1871/2245 train_time:114489ms step_avg:61.19ms
step:1872/2245 train_time:114549ms step_avg:61.19ms
step:1873/2245 train_time:114612ms step_avg:61.19ms
step:1874/2245 train_time:114673ms step_avg:61.19ms
step:1875/2245 train_time:114737ms step_avg:61.19ms
step:1876/2245 train_time:114797ms step_avg:61.19ms
step:1877/2245 train_time:114860ms step_avg:61.19ms
step:1878/2245 train_time:114920ms step_avg:61.19ms
step:1879/2245 train_time:114983ms step_avg:61.19ms
step:1880/2245 train_time:115043ms step_avg:61.19ms
step:1881/2245 train_time:115106ms step_avg:61.19ms
step:1882/2245 train_time:115166ms step_avg:61.19ms
step:1883/2245 train_time:115228ms step_avg:61.19ms
step:1884/2245 train_time:115289ms step_avg:61.19ms
step:1885/2245 train_time:115352ms step_avg:61.19ms
step:1886/2245 train_time:115413ms step_avg:61.19ms
step:1887/2245 train_time:115476ms step_avg:61.20ms
step:1888/2245 train_time:115536ms step_avg:61.20ms
step:1889/2245 train_time:115599ms step_avg:61.20ms
step:1890/2245 train_time:115659ms step_avg:61.20ms
step:1891/2245 train_time:115721ms step_avg:61.20ms
step:1892/2245 train_time:115783ms step_avg:61.20ms
step:1893/2245 train_time:115845ms step_avg:61.20ms
step:1894/2245 train_time:115905ms step_avg:61.20ms
step:1895/2245 train_time:115967ms step_avg:61.20ms
step:1896/2245 train_time:116027ms step_avg:61.20ms
step:1897/2245 train_time:116090ms step_avg:61.20ms
step:1898/2245 train_time:116150ms step_avg:61.20ms
step:1899/2245 train_time:116213ms step_avg:61.20ms
step:1900/2245 train_time:116273ms step_avg:61.20ms
step:1901/2245 train_time:116338ms step_avg:61.20ms
step:1902/2245 train_time:116399ms step_avg:61.20ms
step:1903/2245 train_time:116461ms step_avg:61.20ms
step:1904/2245 train_time:116521ms step_avg:61.20ms
step:1905/2245 train_time:116583ms step_avg:61.20ms
step:1906/2245 train_time:116643ms step_avg:61.20ms
step:1907/2245 train_time:116706ms step_avg:61.20ms
step:1908/2245 train_time:116766ms step_avg:61.20ms
step:1909/2245 train_time:116829ms step_avg:61.20ms
step:1910/2245 train_time:116890ms step_avg:61.20ms
step:1911/2245 train_time:116953ms step_avg:61.20ms
step:1912/2245 train_time:117013ms step_avg:61.20ms
step:1913/2245 train_time:117077ms step_avg:61.20ms
step:1914/2245 train_time:117137ms step_avg:61.20ms
step:1915/2245 train_time:117200ms step_avg:61.20ms
step:1916/2245 train_time:117261ms step_avg:61.20ms
step:1917/2245 train_time:117323ms step_avg:61.20ms
step:1918/2245 train_time:117384ms step_avg:61.20ms
step:1919/2245 train_time:117446ms step_avg:61.20ms
step:1920/2245 train_time:117507ms step_avg:61.20ms
step:1921/2245 train_time:117570ms step_avg:61.20ms
step:1922/2245 train_time:117630ms step_avg:61.20ms
step:1923/2245 train_time:117693ms step_avg:61.20ms
step:1924/2245 train_time:117754ms step_avg:61.20ms
step:1925/2245 train_time:117817ms step_avg:61.20ms
step:1926/2245 train_time:117878ms step_avg:61.20ms
step:1927/2245 train_time:117941ms step_avg:61.20ms
step:1928/2245 train_time:118001ms step_avg:61.20ms
step:1929/2245 train_time:118063ms step_avg:61.20ms
step:1930/2245 train_time:118123ms step_avg:61.20ms
step:1931/2245 train_time:118186ms step_avg:61.20ms
step:1932/2245 train_time:118245ms step_avg:61.20ms
step:1933/2245 train_time:118309ms step_avg:61.20ms
step:1934/2245 train_time:118369ms step_avg:61.20ms
step:1935/2245 train_time:118432ms step_avg:61.20ms
step:1936/2245 train_time:118493ms step_avg:61.21ms
step:1937/2245 train_time:118556ms step_avg:61.21ms
step:1938/2245 train_time:118616ms step_avg:61.21ms
step:1939/2245 train_time:118679ms step_avg:61.21ms
step:1940/2245 train_time:118739ms step_avg:61.21ms
step:1941/2245 train_time:118803ms step_avg:61.21ms
step:1942/2245 train_time:118863ms step_avg:61.21ms
step:1943/2245 train_time:118926ms step_avg:61.21ms
step:1944/2245 train_time:118986ms step_avg:61.21ms
step:1945/2245 train_time:119048ms step_avg:61.21ms
step:1946/2245 train_time:119109ms step_avg:61.21ms
step:1947/2245 train_time:119172ms step_avg:61.21ms
step:1948/2245 train_time:119233ms step_avg:61.21ms
step:1949/2245 train_time:119296ms step_avg:61.21ms
step:1950/2245 train_time:119356ms step_avg:61.21ms
step:1951/2245 train_time:119419ms step_avg:61.21ms
step:1952/2245 train_time:119480ms step_avg:61.21ms
step:1953/2245 train_time:119541ms step_avg:61.21ms
step:1954/2245 train_time:119602ms step_avg:61.21ms
step:1955/2245 train_time:119664ms step_avg:61.21ms
step:1956/2245 train_time:119725ms step_avg:61.21ms
step:1957/2245 train_time:119789ms step_avg:61.21ms
step:1958/2245 train_time:119849ms step_avg:61.21ms
step:1959/2245 train_time:119912ms step_avg:61.21ms
step:1960/2245 train_time:119974ms step_avg:61.21ms
step:1961/2245 train_time:120037ms step_avg:61.21ms
step:1962/2245 train_time:120097ms step_avg:61.21ms
step:1963/2245 train_time:120160ms step_avg:61.21ms
step:1964/2245 train_time:120220ms step_avg:61.21ms
step:1965/2245 train_time:120282ms step_avg:61.21ms
step:1966/2245 train_time:120342ms step_avg:61.21ms
step:1967/2245 train_time:120405ms step_avg:61.21ms
step:1968/2245 train_time:120465ms step_avg:61.21ms
step:1969/2245 train_time:120527ms step_avg:61.21ms
step:1970/2245 train_time:120588ms step_avg:61.21ms
step:1971/2245 train_time:120651ms step_avg:61.21ms
step:1972/2245 train_time:120712ms step_avg:61.21ms
step:1973/2245 train_time:120775ms step_avg:61.21ms
step:1974/2245 train_time:120836ms step_avg:61.21ms
step:1975/2245 train_time:120898ms step_avg:61.21ms
step:1976/2245 train_time:120959ms step_avg:61.21ms
step:1977/2245 train_time:121022ms step_avg:61.21ms
step:1978/2245 train_time:121082ms step_avg:61.21ms
step:1979/2245 train_time:121145ms step_avg:61.22ms
step:1980/2245 train_time:121205ms step_avg:61.21ms
step:1981/2245 train_time:121268ms step_avg:61.22ms
step:1982/2245 train_time:121328ms step_avg:61.22ms
step:1983/2245 train_time:121391ms step_avg:61.22ms
step:1984/2245 train_time:121452ms step_avg:61.22ms
step:1985/2245 train_time:121514ms step_avg:61.22ms
step:1986/2245 train_time:121575ms step_avg:61.22ms
step:1987/2245 train_time:121638ms step_avg:61.22ms
step:1988/2245 train_time:121698ms step_avg:61.22ms
step:1989/2245 train_time:121761ms step_avg:61.22ms
step:1990/2245 train_time:121821ms step_avg:61.22ms
step:1991/2245 train_time:121884ms step_avg:61.22ms
step:1992/2245 train_time:121944ms step_avg:61.22ms
step:1993/2245 train_time:122007ms step_avg:61.22ms
step:1994/2245 train_time:122067ms step_avg:61.22ms
step:1995/2245 train_time:122130ms step_avg:61.22ms
step:1996/2245 train_time:122191ms step_avg:61.22ms
step:1997/2245 train_time:122254ms step_avg:61.22ms
step:1998/2245 train_time:122315ms step_avg:61.22ms
step:1999/2245 train_time:122378ms step_avg:61.22ms
step:2000/2245 train_time:122438ms step_avg:61.22ms
step:2000/2245 val_loss:3.3226 train_time:122501ms step_avg:61.25ms
step:2001/2245 train_time:122519ms step_avg:61.23ms
step:2002/2245 train_time:122564ms step_avg:61.22ms
step:2003/2245 train_time:122630ms step_avg:61.22ms
step:2004/2245 train_time:122690ms step_avg:61.22ms
step:2005/2245 train_time:122753ms step_avg:61.22ms
step:2006/2245 train_time:122814ms step_avg:61.22ms
step:2007/2245 train_time:122876ms step_avg:61.22ms
step:2008/2245 train_time:122935ms step_avg:61.22ms
step:2009/2245 train_time:122997ms step_avg:61.22ms
step:2010/2245 train_time:123057ms step_avg:61.22ms
step:2011/2245 train_time:123119ms step_avg:61.22ms
step:2012/2245 train_time:123179ms step_avg:61.22ms
step:2013/2245 train_time:123242ms step_avg:61.22ms
step:2014/2245 train_time:123304ms step_avg:61.22ms
step:2015/2245 train_time:123367ms step_avg:61.22ms
step:2016/2245 train_time:123426ms step_avg:61.22ms
step:2017/2245 train_time:123491ms step_avg:61.22ms
step:2018/2245 train_time:123552ms step_avg:61.23ms
step:2019/2245 train_time:123616ms step_avg:61.23ms
step:2020/2245 train_time:123677ms step_avg:61.23ms
step:2021/2245 train_time:123742ms step_avg:61.23ms
step:2022/2245 train_time:123803ms step_avg:61.23ms
step:2023/2245 train_time:123865ms step_avg:61.23ms
step:2024/2245 train_time:123925ms step_avg:61.23ms
step:2025/2245 train_time:123987ms step_avg:61.23ms
step:2026/2245 train_time:124047ms step_avg:61.23ms
step:2027/2245 train_time:124109ms step_avg:61.23ms
step:2028/2245 train_time:124168ms step_avg:61.23ms
step:2029/2245 train_time:124231ms step_avg:61.23ms
step:2030/2245 train_time:124291ms step_avg:61.23ms
step:2031/2245 train_time:124354ms step_avg:61.23ms
step:2032/2245 train_time:124415ms step_avg:61.23ms
step:2033/2245 train_time:124479ms step_avg:61.23ms
step:2034/2245 train_time:124541ms step_avg:61.23ms
step:2035/2245 train_time:124605ms step_avg:61.23ms
step:2036/2245 train_time:124665ms step_avg:61.23ms
step:2037/2245 train_time:124729ms step_avg:61.23ms
step:2038/2245 train_time:124789ms step_avg:61.23ms
step:2039/2245 train_time:124852ms step_avg:61.23ms
step:2040/2245 train_time:124913ms step_avg:61.23ms
step:2041/2245 train_time:124975ms step_avg:61.23ms
step:2042/2245 train_time:125035ms step_avg:61.23ms
step:2043/2245 train_time:125097ms step_avg:61.23ms
step:2044/2245 train_time:125158ms step_avg:61.23ms
step:2045/2245 train_time:125221ms step_avg:61.23ms
step:2046/2245 train_time:125282ms step_avg:61.23ms
step:2047/2245 train_time:125345ms step_avg:61.23ms
step:2048/2245 train_time:125406ms step_avg:61.23ms
step:2049/2245 train_time:125469ms step_avg:61.23ms
step:2050/2245 train_time:125530ms step_avg:61.23ms
step:2051/2245 train_time:125593ms step_avg:61.23ms
step:2052/2245 train_time:125653ms step_avg:61.23ms
step:2053/2245 train_time:125716ms step_avg:61.24ms
step:2054/2245 train_time:125776ms step_avg:61.23ms
step:2055/2245 train_time:125839ms step_avg:61.24ms
step:2056/2245 train_time:125900ms step_avg:61.24ms
step:2057/2245 train_time:125963ms step_avg:61.24ms
step:2058/2245 train_time:126024ms step_avg:61.24ms
step:2059/2245 train_time:126087ms step_avg:61.24ms
step:2060/2245 train_time:126146ms step_avg:61.24ms
step:2061/2245 train_time:126209ms step_avg:61.24ms
step:2062/2245 train_time:126269ms step_avg:61.24ms
step:2063/2245 train_time:126332ms step_avg:61.24ms
step:2064/2245 train_time:126392ms step_avg:61.24ms
step:2065/2245 train_time:126455ms step_avg:61.24ms
step:2066/2245 train_time:126516ms step_avg:61.24ms
step:2067/2245 train_time:126579ms step_avg:61.24ms
step:2068/2245 train_time:126641ms step_avg:61.24ms
step:2069/2245 train_time:126705ms step_avg:61.24ms
step:2070/2245 train_time:126765ms step_avg:61.24ms
step:2071/2245 train_time:126828ms step_avg:61.24ms
step:2072/2245 train_time:126888ms step_avg:61.24ms
step:2073/2245 train_time:126951ms step_avg:61.24ms
step:2074/2245 train_time:127012ms step_avg:61.24ms
step:2075/2245 train_time:127074ms step_avg:61.24ms
step:2076/2245 train_time:127134ms step_avg:61.24ms
step:2077/2245 train_time:127197ms step_avg:61.24ms
step:2078/2245 train_time:127257ms step_avg:61.24ms
step:2079/2245 train_time:127320ms step_avg:61.24ms
step:2080/2245 train_time:127381ms step_avg:61.24ms
step:2081/2245 train_time:127444ms step_avg:61.24ms
step:2082/2245 train_time:127505ms step_avg:61.24ms
step:2083/2245 train_time:127568ms step_avg:61.24ms
step:2084/2245 train_time:127629ms step_avg:61.24ms
step:2085/2245 train_time:127693ms step_avg:61.24ms
step:2086/2245 train_time:127753ms step_avg:61.24ms
step:2087/2245 train_time:127816ms step_avg:61.24ms
step:2088/2245 train_time:127876ms step_avg:61.24ms
step:2089/2245 train_time:127940ms step_avg:61.24ms
step:2090/2245 train_time:128001ms step_avg:61.24ms
step:2091/2245 train_time:128064ms step_avg:61.25ms
step:2092/2245 train_time:128124ms step_avg:61.24ms
step:2093/2245 train_time:128187ms step_avg:61.25ms
step:2094/2245 train_time:128247ms step_avg:61.25ms
step:2095/2245 train_time:128310ms step_avg:61.25ms
step:2096/2245 train_time:128370ms step_avg:61.25ms
step:2097/2245 train_time:128433ms step_avg:61.25ms
step:2098/2245 train_time:128493ms step_avg:61.25ms
step:2099/2245 train_time:128556ms step_avg:61.25ms
step:2100/2245 train_time:128617ms step_avg:61.25ms
step:2101/2245 train_time:128680ms step_avg:61.25ms
step:2102/2245 train_time:128742ms step_avg:61.25ms
step:2103/2245 train_time:128805ms step_avg:61.25ms
step:2104/2245 train_time:128866ms step_avg:61.25ms
step:2105/2245 train_time:128929ms step_avg:61.25ms
step:2106/2245 train_time:128989ms step_avg:61.25ms
step:2107/2245 train_time:129052ms step_avg:61.25ms
step:2108/2245 train_time:129112ms step_avg:61.25ms
step:2109/2245 train_time:129175ms step_avg:61.25ms
step:2110/2245 train_time:129235ms step_avg:61.25ms
step:2111/2245 train_time:129298ms step_avg:61.25ms
step:2112/2245 train_time:129358ms step_avg:61.25ms
step:2113/2245 train_time:129421ms step_avg:61.25ms
step:2114/2245 train_time:129482ms step_avg:61.25ms
step:2115/2245 train_time:129545ms step_avg:61.25ms
step:2116/2245 train_time:129606ms step_avg:61.25ms
step:2117/2245 train_time:129668ms step_avg:61.25ms
step:2118/2245 train_time:129729ms step_avg:61.25ms
step:2119/2245 train_time:129791ms step_avg:61.25ms
step:2120/2245 train_time:129852ms step_avg:61.25ms
step:2121/2245 train_time:129915ms step_avg:61.25ms
step:2122/2245 train_time:129975ms step_avg:61.25ms
step:2123/2245 train_time:130039ms step_avg:61.25ms
step:2124/2245 train_time:130101ms step_avg:61.25ms
step:2125/2245 train_time:130164ms step_avg:61.25ms
step:2126/2245 train_time:130224ms step_avg:61.25ms
step:2127/2245 train_time:130287ms step_avg:61.25ms
step:2128/2245 train_time:130347ms step_avg:61.25ms
step:2129/2245 train_time:130409ms step_avg:61.25ms
step:2130/2245 train_time:130470ms step_avg:61.25ms
step:2131/2245 train_time:130532ms step_avg:61.25ms
step:2132/2245 train_time:130593ms step_avg:61.25ms
step:2133/2245 train_time:130656ms step_avg:61.25ms
step:2134/2245 train_time:130716ms step_avg:61.25ms
step:2135/2245 train_time:130779ms step_avg:61.25ms
step:2136/2245 train_time:130840ms step_avg:61.25ms
step:2137/2245 train_time:130903ms step_avg:61.26ms
step:2138/2245 train_time:130964ms step_avg:61.26ms
step:2139/2245 train_time:131027ms step_avg:61.26ms
step:2140/2245 train_time:131087ms step_avg:61.26ms
step:2141/2245 train_time:131150ms step_avg:61.26ms
step:2142/2245 train_time:131211ms step_avg:61.26ms
step:2143/2245 train_time:131273ms step_avg:61.26ms
step:2144/2245 train_time:131333ms step_avg:61.26ms
step:2145/2245 train_time:131396ms step_avg:61.26ms
step:2146/2245 train_time:131456ms step_avg:61.26ms
step:2147/2245 train_time:131519ms step_avg:61.26ms
step:2148/2245 train_time:131580ms step_avg:61.26ms
step:2149/2245 train_time:131644ms step_avg:61.26ms
step:2150/2245 train_time:131705ms step_avg:61.26ms
step:2151/2245 train_time:131767ms step_avg:61.26ms
step:2152/2245 train_time:131828ms step_avg:61.26ms
step:2153/2245 train_time:131890ms step_avg:61.26ms
step:2154/2245 train_time:131951ms step_avg:61.26ms
step:2155/2245 train_time:132014ms step_avg:61.26ms
step:2156/2245 train_time:132074ms step_avg:61.26ms
step:2157/2245 train_time:132137ms step_avg:61.26ms
step:2158/2245 train_time:132198ms step_avg:61.26ms
step:2159/2245 train_time:132262ms step_avg:61.26ms
step:2160/2245 train_time:132322ms step_avg:61.26ms
step:2161/2245 train_time:132385ms step_avg:61.26ms
step:2162/2245 train_time:132445ms step_avg:61.26ms
step:2163/2245 train_time:132508ms step_avg:61.26ms
step:2164/2245 train_time:132568ms step_avg:61.26ms
step:2165/2245 train_time:132631ms step_avg:61.26ms
step:2166/2245 train_time:132691ms step_avg:61.26ms
step:2167/2245 train_time:132754ms step_avg:61.26ms
step:2168/2245 train_time:132814ms step_avg:61.26ms
step:2169/2245 train_time:132877ms step_avg:61.26ms
step:2170/2245 train_time:132938ms step_avg:61.26ms
step:2171/2245 train_time:133002ms step_avg:61.26ms
step:2172/2245 train_time:133063ms step_avg:61.26ms
step:2173/2245 train_time:133126ms step_avg:61.26ms
step:2174/2245 train_time:133186ms step_avg:61.26ms
step:2175/2245 train_time:133248ms step_avg:61.26ms
step:2176/2245 train_time:133309ms step_avg:61.26ms
step:2177/2245 train_time:133371ms step_avg:61.26ms
step:2178/2245 train_time:133431ms step_avg:61.26ms
step:2179/2245 train_time:133493ms step_avg:61.26ms
step:2180/2245 train_time:133554ms step_avg:61.26ms
step:2181/2245 train_time:133616ms step_avg:61.26ms
step:2182/2245 train_time:133677ms step_avg:61.26ms
step:2183/2245 train_time:133740ms step_avg:61.26ms
step:2184/2245 train_time:133802ms step_avg:61.26ms
step:2185/2245 train_time:133865ms step_avg:61.27ms
step:2186/2245 train_time:133925ms step_avg:61.27ms
step:2187/2245 train_time:133988ms step_avg:61.27ms
step:2188/2245 train_time:134048ms step_avg:61.27ms
step:2189/2245 train_time:134111ms step_avg:61.27ms
step:2190/2245 train_time:134172ms step_avg:61.27ms
step:2191/2245 train_time:134235ms step_avg:61.27ms
step:2192/2245 train_time:134295ms step_avg:61.27ms
step:2193/2245 train_time:134357ms step_avg:61.27ms
step:2194/2245 train_time:134418ms step_avg:61.27ms
step:2195/2245 train_time:134482ms step_avg:61.27ms
step:2196/2245 train_time:134542ms step_avg:61.27ms
step:2197/2245 train_time:134606ms step_avg:61.27ms
step:2198/2245 train_time:134666ms step_avg:61.27ms
step:2199/2245 train_time:134728ms step_avg:61.27ms
step:2200/2245 train_time:134789ms step_avg:61.27ms
step:2201/2245 train_time:134851ms step_avg:61.27ms
step:2202/2245 train_time:134912ms step_avg:61.27ms
step:2203/2245 train_time:134975ms step_avg:61.27ms
step:2204/2245 train_time:135035ms step_avg:61.27ms
step:2205/2245 train_time:135099ms step_avg:61.27ms
step:2206/2245 train_time:135160ms step_avg:61.27ms
step:2207/2245 train_time:135222ms step_avg:61.27ms
step:2208/2245 train_time:135283ms step_avg:61.27ms
step:2209/2245 train_time:135347ms step_avg:61.27ms
step:2210/2245 train_time:135408ms step_avg:61.27ms
step:2211/2245 train_time:135470ms step_avg:61.27ms
step:2212/2245 train_time:135531ms step_avg:61.27ms
step:2213/2245 train_time:135594ms step_avg:61.27ms
step:2214/2245 train_time:135655ms step_avg:61.27ms
step:2215/2245 train_time:135718ms step_avg:61.27ms
step:2216/2245 train_time:135779ms step_avg:61.27ms
step:2217/2245 train_time:135842ms step_avg:61.27ms
step:2218/2245 train_time:135904ms step_avg:61.27ms
step:2219/2245 train_time:135967ms step_avg:61.27ms
step:2220/2245 train_time:136027ms step_avg:61.27ms
step:2221/2245 train_time:136091ms step_avg:61.27ms
step:2222/2245 train_time:136151ms step_avg:61.27ms
step:2223/2245 train_time:136214ms step_avg:61.27ms
step:2224/2245 train_time:136274ms step_avg:61.27ms
step:2225/2245 train_time:136337ms step_avg:61.28ms
step:2226/2245 train_time:136398ms step_avg:61.27ms
step:2227/2245 train_time:136461ms step_avg:61.28ms
step:2228/2245 train_time:136523ms step_avg:61.28ms
step:2229/2245 train_time:136586ms step_avg:61.28ms
step:2230/2245 train_time:136647ms step_avg:61.28ms
step:2231/2245 train_time:136710ms step_avg:61.28ms
step:2232/2245 train_time:136770ms step_avg:61.28ms
step:2233/2245 train_time:136833ms step_avg:61.28ms
step:2234/2245 train_time:136894ms step_avg:61.28ms
step:2235/2245 train_time:136957ms step_avg:61.28ms
step:2236/2245 train_time:137017ms step_avg:61.28ms
step:2237/2245 train_time:137080ms step_avg:61.28ms
step:2238/2245 train_time:137142ms step_avg:61.28ms
step:2239/2245 train_time:137205ms step_avg:61.28ms
step:2240/2245 train_time:137265ms step_avg:61.28ms
step:2241/2245 train_time:137328ms step_avg:61.28ms
step:2242/2245 train_time:137388ms step_avg:61.28ms
step:2243/2245 train_time:137451ms step_avg:61.28ms
step:2244/2245 train_time:137512ms step_avg:61.28ms
step:2245/2245 train_time:137575ms step_avg:61.28ms
step:2245/2245 val_loss:3.2777 train_time:137636ms step_avg:61.31ms
peak memory allocated: 29249 MiB reserved: 50528 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Nov 10 21:36:01 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   41C    P0            130W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   33C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   41C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   34C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   40C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2245 train_time:119ms step_avg:119.44ms
step:2/2245 train_time:141ms step_avg:70.39ms
step:3/2245 train_time:179ms step_avg:59.71ms
step:4/2245 train_time:236ms step_avg:58.91ms
step:5/2245 train_time:295ms step_avg:58.99ms
step:6/2245 train_time:353ms step_avg:58.89ms
step:7/2245 train_time:415ms step_avg:59.23ms
step:8/2245 train_time:473ms step_avg:59.16ms
step:9/2245 train_time:534ms step_avg:59.36ms
step:10/2245 train_time:593ms step_avg:59.32ms
step:11/2245 train_time:654ms step_avg:59.49ms
step:12/2245 train_time:713ms step_avg:59.43ms
step:13/2245 train_time:775ms step_avg:59.58ms
step:14/2245 train_time:834ms step_avg:59.55ms
step:15/2245 train_time:895ms step_avg:59.66ms
step:16/2245 train_time:954ms step_avg:59.63ms
step:17/2245 train_time:1019ms step_avg:59.93ms
step:18/2245 train_time:1082ms step_avg:60.13ms
step:19/2245 train_time:1148ms step_avg:60.42ms
step:20/2245 train_time:1209ms step_avg:60.46ms
step:21/2245 train_time:1271ms step_avg:60.54ms
step:22/2245 train_time:1331ms step_avg:60.49ms
step:23/2245 train_time:1392ms step_avg:60.52ms
step:24/2245 train_time:1451ms step_avg:60.46ms
step:25/2245 train_time:1512ms step_avg:60.49ms
step:26/2245 train_time:1572ms step_avg:60.45ms
step:27/2245 train_time:1633ms step_avg:60.48ms
step:28/2245 train_time:1692ms step_avg:60.44ms
step:29/2245 train_time:1753ms step_avg:60.44ms
step:30/2245 train_time:1812ms step_avg:60.41ms
step:31/2245 train_time:1874ms step_avg:60.44ms
step:32/2245 train_time:1934ms step_avg:60.44ms
step:33/2245 train_time:1998ms step_avg:60.53ms
step:34/2245 train_time:2059ms step_avg:60.55ms
step:35/2245 train_time:2123ms step_avg:60.65ms
step:36/2245 train_time:2183ms step_avg:60.63ms
step:37/2245 train_time:2245ms step_avg:60.69ms
step:38/2245 train_time:2305ms step_avg:60.65ms
step:39/2245 train_time:2367ms step_avg:60.68ms
step:40/2245 train_time:2426ms step_avg:60.64ms
step:41/2245 train_time:2487ms step_avg:60.67ms
step:42/2245 train_time:2547ms step_avg:60.64ms
step:43/2245 train_time:2609ms step_avg:60.67ms
step:44/2245 train_time:2668ms step_avg:60.64ms
step:45/2245 train_time:2729ms step_avg:60.65ms
step:46/2245 train_time:2789ms step_avg:60.62ms
step:47/2245 train_time:2850ms step_avg:60.64ms
step:48/2245 train_time:2910ms step_avg:60.63ms
step:49/2245 train_time:2974ms step_avg:60.69ms
step:50/2245 train_time:3035ms step_avg:60.70ms
step:51/2245 train_time:3097ms step_avg:60.73ms
step:52/2245 train_time:3158ms step_avg:60.73ms
step:53/2245 train_time:3220ms step_avg:60.75ms
step:54/2245 train_time:3279ms step_avg:60.73ms
step:55/2245 train_time:3341ms step_avg:60.74ms
step:56/2245 train_time:3400ms step_avg:60.71ms
step:57/2245 train_time:3461ms step_avg:60.73ms
step:58/2245 train_time:3521ms step_avg:60.70ms
step:59/2245 train_time:3582ms step_avg:60.72ms
step:60/2245 train_time:3642ms step_avg:60.69ms
step:61/2245 train_time:3704ms step_avg:60.71ms
step:62/2245 train_time:3763ms step_avg:60.70ms
step:63/2245 train_time:3826ms step_avg:60.73ms
step:64/2245 train_time:3886ms step_avg:60.71ms
step:65/2245 train_time:3948ms step_avg:60.74ms
step:66/2245 train_time:4008ms step_avg:60.72ms
step:67/2245 train_time:4070ms step_avg:60.75ms
step:68/2245 train_time:4131ms step_avg:60.75ms
step:69/2245 train_time:4194ms step_avg:60.78ms
step:70/2245 train_time:4257ms step_avg:60.81ms
step:71/2245 train_time:4316ms step_avg:60.79ms
step:72/2245 train_time:4376ms step_avg:60.77ms
step:73/2245 train_time:4438ms step_avg:60.80ms
step:74/2245 train_time:4498ms step_avg:60.78ms
step:75/2245 train_time:4560ms step_avg:60.80ms
step:76/2245 train_time:4620ms step_avg:60.78ms
step:77/2245 train_time:4681ms step_avg:60.79ms
step:78/2245 train_time:4739ms step_avg:60.76ms
step:79/2245 train_time:4802ms step_avg:60.78ms
step:80/2245 train_time:4861ms step_avg:60.76ms
step:81/2245 train_time:4922ms step_avg:60.77ms
step:82/2245 train_time:4982ms step_avg:60.75ms
step:83/2245 train_time:5045ms step_avg:60.78ms
step:84/2245 train_time:5104ms step_avg:60.76ms
step:85/2245 train_time:5166ms step_avg:60.77ms
step:86/2245 train_time:5226ms step_avg:60.76ms
step:87/2245 train_time:5288ms step_avg:60.78ms
step:88/2245 train_time:5347ms step_avg:60.77ms
step:89/2245 train_time:5409ms step_avg:60.78ms
step:90/2245 train_time:5470ms step_avg:60.77ms
step:91/2245 train_time:5532ms step_avg:60.79ms
step:92/2245 train_time:5592ms step_avg:60.78ms
step:93/2245 train_time:5654ms step_avg:60.80ms
step:94/2245 train_time:5713ms step_avg:60.78ms
step:95/2245 train_time:5776ms step_avg:60.80ms
step:96/2245 train_time:5835ms step_avg:60.79ms
step:97/2245 train_time:5897ms step_avg:60.80ms
step:98/2245 train_time:5956ms step_avg:60.78ms
step:99/2245 train_time:6018ms step_avg:60.79ms
step:100/2245 train_time:6077ms step_avg:60.77ms
step:101/2245 train_time:6139ms step_avg:60.78ms
step:102/2245 train_time:6198ms step_avg:60.77ms
step:103/2245 train_time:6261ms step_avg:60.78ms
step:104/2245 train_time:6320ms step_avg:60.77ms
step:105/2245 train_time:6381ms step_avg:60.77ms
step:106/2245 train_time:6441ms step_avg:60.76ms
step:107/2245 train_time:6502ms step_avg:60.77ms
step:108/2245 train_time:6562ms step_avg:60.76ms
step:109/2245 train_time:6624ms step_avg:60.77ms
step:110/2245 train_time:6683ms step_avg:60.76ms
step:111/2245 train_time:6745ms step_avg:60.77ms
step:112/2245 train_time:6805ms step_avg:60.76ms
step:113/2245 train_time:6867ms step_avg:60.77ms
step:114/2245 train_time:6926ms step_avg:60.76ms
step:115/2245 train_time:6988ms step_avg:60.77ms
step:116/2245 train_time:7048ms step_avg:60.76ms
step:117/2245 train_time:7109ms step_avg:60.76ms
step:118/2245 train_time:7169ms step_avg:60.76ms
step:119/2245 train_time:7232ms step_avg:60.77ms
step:120/2245 train_time:7292ms step_avg:60.76ms
step:121/2245 train_time:7354ms step_avg:60.77ms
step:122/2245 train_time:7413ms step_avg:60.76ms
step:123/2245 train_time:7475ms step_avg:60.77ms
step:124/2245 train_time:7535ms step_avg:60.76ms
step:125/2245 train_time:7596ms step_avg:60.77ms
step:126/2245 train_time:7655ms step_avg:60.76ms
step:127/2245 train_time:7717ms step_avg:60.76ms
step:128/2245 train_time:7776ms step_avg:60.75ms
step:129/2245 train_time:7838ms step_avg:60.76ms
step:130/2245 train_time:7897ms step_avg:60.75ms
step:131/2245 train_time:7959ms step_avg:60.76ms
step:132/2245 train_time:8017ms step_avg:60.74ms
step:133/2245 train_time:8079ms step_avg:60.74ms
step:134/2245 train_time:8139ms step_avg:60.74ms
step:135/2245 train_time:8201ms step_avg:60.75ms
step:136/2245 train_time:8260ms step_avg:60.73ms
step:137/2245 train_time:8321ms step_avg:60.74ms
step:138/2245 train_time:8381ms step_avg:60.73ms
step:139/2245 train_time:8442ms step_avg:60.74ms
step:140/2245 train_time:8501ms step_avg:60.72ms
step:141/2245 train_time:8563ms step_avg:60.73ms
step:142/2245 train_time:8622ms step_avg:60.72ms
step:143/2245 train_time:8684ms step_avg:60.72ms
step:144/2245 train_time:8743ms step_avg:60.72ms
step:145/2245 train_time:8805ms step_avg:60.72ms
step:146/2245 train_time:8864ms step_avg:60.71ms
step:147/2245 train_time:8925ms step_avg:60.71ms
step:148/2245 train_time:8984ms step_avg:60.70ms
step:149/2245 train_time:9045ms step_avg:60.71ms
step:150/2245 train_time:9104ms step_avg:60.69ms
step:151/2245 train_time:9166ms step_avg:60.70ms
step:152/2245 train_time:9225ms step_avg:60.69ms
step:153/2245 train_time:9286ms step_avg:60.69ms
step:154/2245 train_time:9346ms step_avg:60.69ms
step:155/2245 train_time:9408ms step_avg:60.69ms
step:156/2245 train_time:9467ms step_avg:60.69ms
step:157/2245 train_time:9529ms step_avg:60.69ms
step:158/2245 train_time:9589ms step_avg:60.69ms
step:159/2245 train_time:9651ms step_avg:60.70ms
step:160/2245 train_time:9710ms step_avg:60.69ms
step:161/2245 train_time:9772ms step_avg:60.70ms
step:162/2245 train_time:9832ms step_avg:60.69ms
step:163/2245 train_time:9894ms step_avg:60.70ms
step:164/2245 train_time:9954ms step_avg:60.69ms
step:165/2245 train_time:10015ms step_avg:60.70ms
step:166/2245 train_time:10074ms step_avg:60.69ms
step:167/2245 train_time:10136ms step_avg:60.69ms
step:168/2245 train_time:10195ms step_avg:60.68ms
step:169/2245 train_time:10256ms step_avg:60.69ms
step:170/2245 train_time:10315ms step_avg:60.67ms
step:171/2245 train_time:10376ms step_avg:60.68ms
step:172/2245 train_time:10436ms step_avg:60.67ms
step:173/2245 train_time:10497ms step_avg:60.68ms
step:174/2245 train_time:10558ms step_avg:60.68ms
step:175/2245 train_time:10619ms step_avg:60.68ms
step:176/2245 train_time:10678ms step_avg:60.67ms
step:177/2245 train_time:10739ms step_avg:60.67ms
step:178/2245 train_time:10798ms step_avg:60.66ms
step:179/2245 train_time:10859ms step_avg:60.66ms
step:180/2245 train_time:10918ms step_avg:60.66ms
step:181/2245 train_time:10980ms step_avg:60.66ms
step:182/2245 train_time:11039ms step_avg:60.65ms
step:183/2245 train_time:11100ms step_avg:60.66ms
step:184/2245 train_time:11159ms step_avg:60.65ms
step:185/2245 train_time:11221ms step_avg:60.65ms
step:186/2245 train_time:11280ms step_avg:60.64ms
step:187/2245 train_time:11341ms step_avg:60.65ms
step:188/2245 train_time:11400ms step_avg:60.64ms
step:189/2245 train_time:11461ms step_avg:60.64ms
step:190/2245 train_time:11520ms step_avg:60.63ms
step:191/2245 train_time:11582ms step_avg:60.64ms
step:192/2245 train_time:11641ms step_avg:60.63ms
step:193/2245 train_time:11703ms step_avg:60.64ms
step:194/2245 train_time:11761ms step_avg:60.62ms
step:195/2245 train_time:11822ms step_avg:60.63ms
step:196/2245 train_time:11881ms step_avg:60.62ms
step:197/2245 train_time:11943ms step_avg:60.62ms
step:198/2245 train_time:12001ms step_avg:60.61ms
step:199/2245 train_time:12063ms step_avg:60.62ms
step:200/2245 train_time:12122ms step_avg:60.61ms
step:201/2245 train_time:12184ms step_avg:60.61ms
step:202/2245 train_time:12243ms step_avg:60.61ms
step:203/2245 train_time:12304ms step_avg:60.61ms
step:204/2245 train_time:12363ms step_avg:60.60ms
step:205/2245 train_time:12424ms step_avg:60.61ms
step:206/2245 train_time:12483ms step_avg:60.60ms
step:207/2245 train_time:12545ms step_avg:60.60ms
step:208/2245 train_time:12604ms step_avg:60.59ms
step:209/2245 train_time:12665ms step_avg:60.60ms
step:210/2245 train_time:12724ms step_avg:60.59ms
step:211/2245 train_time:12785ms step_avg:60.59ms
step:212/2245 train_time:12844ms step_avg:60.58ms
step:213/2245 train_time:12905ms step_avg:60.59ms
step:214/2245 train_time:12964ms step_avg:60.58ms
step:215/2245 train_time:13025ms step_avg:60.58ms
step:216/2245 train_time:13085ms step_avg:60.58ms
step:217/2245 train_time:13146ms step_avg:60.58ms
step:218/2245 train_time:13205ms step_avg:60.57ms
step:219/2245 train_time:13267ms step_avg:60.58ms
step:220/2245 train_time:13326ms step_avg:60.57ms
step:221/2245 train_time:13388ms step_avg:60.58ms
step:222/2245 train_time:13448ms step_avg:60.57ms
step:223/2245 train_time:13509ms step_avg:60.58ms
step:224/2245 train_time:13568ms step_avg:60.57ms
step:225/2245 train_time:13630ms step_avg:60.58ms
step:226/2245 train_time:13690ms step_avg:60.57ms
step:227/2245 train_time:13751ms step_avg:60.58ms
step:228/2245 train_time:13811ms step_avg:60.58ms
step:229/2245 train_time:13873ms step_avg:60.58ms
step:230/2245 train_time:13933ms step_avg:60.58ms
step:231/2245 train_time:13995ms step_avg:60.58ms
step:232/2245 train_time:14054ms step_avg:60.58ms
step:233/2245 train_time:14116ms step_avg:60.58ms
step:234/2245 train_time:14175ms step_avg:60.58ms
step:235/2245 train_time:14237ms step_avg:60.58ms
step:236/2245 train_time:14296ms step_avg:60.58ms
step:237/2245 train_time:14357ms step_avg:60.58ms
step:238/2245 train_time:14416ms step_avg:60.57ms
step:239/2245 train_time:14477ms step_avg:60.57ms
step:240/2245 train_time:14536ms step_avg:60.57ms
step:241/2245 train_time:14599ms step_avg:60.58ms
step:242/2245 train_time:14657ms step_avg:60.57ms
step:243/2245 train_time:14719ms step_avg:60.57ms
step:244/2245 train_time:14778ms step_avg:60.56ms
step:245/2245 train_time:14839ms step_avg:60.57ms
step:246/2245 train_time:14898ms step_avg:60.56ms
step:247/2245 train_time:14959ms step_avg:60.56ms
step:248/2245 train_time:15018ms step_avg:60.56ms
step:249/2245 train_time:15080ms step_avg:60.56ms
step:250/2245 train_time:15139ms step_avg:60.56ms
step:250/2245 val_loss:4.0758 train_time:15201ms step_avg:60.80ms
step:251/2245 train_time:15220ms step_avg:60.64ms
step:252/2245 train_time:15262ms step_avg:60.56ms
step:253/2245 train_time:15329ms step_avg:60.59ms
step:254/2245 train_time:15393ms step_avg:60.60ms
step:255/2245 train_time:15454ms step_avg:60.61ms
step:256/2245 train_time:15514ms step_avg:60.60ms
step:257/2245 train_time:15575ms step_avg:60.60ms
step:258/2245 train_time:15633ms step_avg:60.59ms
step:259/2245 train_time:15695ms step_avg:60.60ms
step:260/2245 train_time:15753ms step_avg:60.59ms
step:261/2245 train_time:15813ms step_avg:60.59ms
step:262/2245 train_time:15871ms step_avg:60.58ms
step:263/2245 train_time:15932ms step_avg:60.58ms
step:264/2245 train_time:15990ms step_avg:60.57ms
step:265/2245 train_time:16050ms step_avg:60.57ms
step:266/2245 train_time:16108ms step_avg:60.56ms
step:267/2245 train_time:16169ms step_avg:60.56ms
step:268/2245 train_time:16228ms step_avg:60.55ms
step:269/2245 train_time:16291ms step_avg:60.56ms
step:270/2245 train_time:16351ms step_avg:60.56ms
step:271/2245 train_time:16414ms step_avg:60.57ms
step:272/2245 train_time:16473ms step_avg:60.56ms
step:273/2245 train_time:16535ms step_avg:60.57ms
step:274/2245 train_time:16593ms step_avg:60.56ms
step:275/2245 train_time:16655ms step_avg:60.56ms
step:276/2245 train_time:16713ms step_avg:60.56ms
step:277/2245 train_time:16774ms step_avg:60.56ms
step:278/2245 train_time:16833ms step_avg:60.55ms
step:279/2245 train_time:16893ms step_avg:60.55ms
step:280/2245 train_time:16952ms step_avg:60.54ms
step:281/2245 train_time:17012ms step_avg:60.54ms
step:282/2245 train_time:17071ms step_avg:60.54ms
step:283/2245 train_time:17133ms step_avg:60.54ms
step:284/2245 train_time:17192ms step_avg:60.54ms
step:285/2245 train_time:17255ms step_avg:60.54ms
step:286/2245 train_time:17314ms step_avg:60.54ms
step:287/2245 train_time:17377ms step_avg:60.55ms
step:288/2245 train_time:17436ms step_avg:60.54ms
step:289/2245 train_time:17498ms step_avg:60.55ms
step:290/2245 train_time:17556ms step_avg:60.54ms
step:291/2245 train_time:17617ms step_avg:60.54ms
step:292/2245 train_time:17676ms step_avg:60.53ms
step:293/2245 train_time:17737ms step_avg:60.54ms
step:294/2245 train_time:17796ms step_avg:60.53ms
step:295/2245 train_time:17856ms step_avg:60.53ms
step:296/2245 train_time:17915ms step_avg:60.52ms
step:297/2245 train_time:17976ms step_avg:60.53ms
step:298/2245 train_time:18035ms step_avg:60.52ms
step:299/2245 train_time:18096ms step_avg:60.52ms
step:300/2245 train_time:18155ms step_avg:60.52ms
step:301/2245 train_time:18216ms step_avg:60.52ms
step:302/2245 train_time:18276ms step_avg:60.52ms
step:303/2245 train_time:18338ms step_avg:60.52ms
step:304/2245 train_time:18397ms step_avg:60.52ms
step:305/2245 train_time:18458ms step_avg:60.52ms
step:306/2245 train_time:18516ms step_avg:60.51ms
step:307/2245 train_time:18578ms step_avg:60.51ms
step:308/2245 train_time:18637ms step_avg:60.51ms
step:309/2245 train_time:18698ms step_avg:60.51ms
step:310/2245 train_time:18757ms step_avg:60.51ms
step:311/2245 train_time:18818ms step_avg:60.51ms
step:312/2245 train_time:18877ms step_avg:60.50ms
step:313/2245 train_time:18938ms step_avg:60.51ms
step:314/2245 train_time:18998ms step_avg:60.50ms
step:315/2245 train_time:19059ms step_avg:60.50ms
step:316/2245 train_time:19118ms step_avg:60.50ms
step:317/2245 train_time:19179ms step_avg:60.50ms
step:318/2245 train_time:19239ms step_avg:60.50ms
step:319/2245 train_time:19300ms step_avg:60.50ms
step:320/2245 train_time:19360ms step_avg:60.50ms
step:321/2245 train_time:19422ms step_avg:60.50ms
step:322/2245 train_time:19481ms step_avg:60.50ms
step:323/2245 train_time:19542ms step_avg:60.50ms
step:324/2245 train_time:19601ms step_avg:60.50ms
step:325/2245 train_time:19664ms step_avg:60.50ms
step:326/2245 train_time:19723ms step_avg:60.50ms
step:327/2245 train_time:19784ms step_avg:60.50ms
step:328/2245 train_time:19844ms step_avg:60.50ms
step:329/2245 train_time:19905ms step_avg:60.50ms
step:330/2245 train_time:19964ms step_avg:60.50ms
step:331/2245 train_time:20026ms step_avg:60.50ms
step:332/2245 train_time:20086ms step_avg:60.50ms
step:333/2245 train_time:20147ms step_avg:60.50ms
step:334/2245 train_time:20207ms step_avg:60.50ms
step:335/2245 train_time:20267ms step_avg:60.50ms
step:336/2245 train_time:20327ms step_avg:60.50ms
step:337/2245 train_time:20388ms step_avg:60.50ms
step:338/2245 train_time:20447ms step_avg:60.49ms
step:339/2245 train_time:20508ms step_avg:60.50ms
step:340/2245 train_time:20568ms step_avg:60.49ms
step:341/2245 train_time:20629ms step_avg:60.50ms
step:342/2245 train_time:20688ms step_avg:60.49ms
step:343/2245 train_time:20750ms step_avg:60.49ms
step:344/2245 train_time:20808ms step_avg:60.49ms
step:345/2245 train_time:20869ms step_avg:60.49ms
step:346/2245 train_time:20928ms step_avg:60.49ms
step:347/2245 train_time:20990ms step_avg:60.49ms
step:348/2245 train_time:21049ms step_avg:60.49ms
step:349/2245 train_time:21110ms step_avg:60.49ms
step:350/2245 train_time:21169ms step_avg:60.48ms
step:351/2245 train_time:21231ms step_avg:60.49ms
step:352/2245 train_time:21290ms step_avg:60.48ms
step:353/2245 train_time:21351ms step_avg:60.49ms
step:354/2245 train_time:21410ms step_avg:60.48ms
step:355/2245 train_time:21471ms step_avg:60.48ms
step:356/2245 train_time:21529ms step_avg:60.48ms
step:357/2245 train_time:21591ms step_avg:60.48ms
step:358/2245 train_time:21650ms step_avg:60.47ms
step:359/2245 train_time:21711ms step_avg:60.48ms
step:360/2245 train_time:21770ms step_avg:60.47ms
step:361/2245 train_time:21831ms step_avg:60.47ms
step:362/2245 train_time:21890ms step_avg:60.47ms
step:363/2245 train_time:21952ms step_avg:60.47ms
step:364/2245 train_time:22011ms step_avg:60.47ms
step:365/2245 train_time:22073ms step_avg:60.47ms
step:366/2245 train_time:22132ms step_avg:60.47ms
step:367/2245 train_time:22193ms step_avg:60.47ms
step:368/2245 train_time:22252ms step_avg:60.47ms
step:369/2245 train_time:22313ms step_avg:60.47ms
step:370/2245 train_time:22372ms step_avg:60.46ms
step:371/2245 train_time:22433ms step_avg:60.47ms
step:372/2245 train_time:22492ms step_avg:60.46ms
step:373/2245 train_time:22554ms step_avg:60.47ms
step:374/2245 train_time:22613ms step_avg:60.46ms
step:375/2245 train_time:22674ms step_avg:60.46ms
step:376/2245 train_time:22733ms step_avg:60.46ms
step:377/2245 train_time:22794ms step_avg:60.46ms
step:378/2245 train_time:22853ms step_avg:60.46ms
step:379/2245 train_time:22915ms step_avg:60.46ms
step:380/2245 train_time:22974ms step_avg:60.46ms
step:381/2245 train_time:23035ms step_avg:60.46ms
step:382/2245 train_time:23094ms step_avg:60.46ms
step:383/2245 train_time:23155ms step_avg:60.46ms
step:384/2245 train_time:23214ms step_avg:60.45ms
step:385/2245 train_time:23275ms step_avg:60.46ms
step:386/2245 train_time:23334ms step_avg:60.45ms
step:387/2245 train_time:23396ms step_avg:60.45ms
step:388/2245 train_time:23454ms step_avg:60.45ms
step:389/2245 train_time:23516ms step_avg:60.45ms
step:390/2245 train_time:23574ms step_avg:60.45ms
step:391/2245 train_time:23636ms step_avg:60.45ms
step:392/2245 train_time:23695ms step_avg:60.45ms
step:393/2245 train_time:23756ms step_avg:60.45ms
step:394/2245 train_time:23816ms step_avg:60.45ms
step:395/2245 train_time:23877ms step_avg:60.45ms
step:396/2245 train_time:23936ms step_avg:60.44ms
step:397/2245 train_time:23998ms step_avg:60.45ms
step:398/2245 train_time:24057ms step_avg:60.44ms
step:399/2245 train_time:24118ms step_avg:60.45ms
step:400/2245 train_time:24177ms step_avg:60.44ms
step:401/2245 train_time:24238ms step_avg:60.44ms
step:402/2245 train_time:24297ms step_avg:60.44ms
step:403/2245 train_time:24359ms step_avg:60.44ms
step:404/2245 train_time:24418ms step_avg:60.44ms
step:405/2245 train_time:24480ms step_avg:60.44ms
step:406/2245 train_time:24539ms step_avg:60.44ms
step:407/2245 train_time:24600ms step_avg:60.44ms
step:408/2245 train_time:24659ms step_avg:60.44ms
step:409/2245 train_time:24721ms step_avg:60.44ms
step:410/2245 train_time:24780ms step_avg:60.44ms
step:411/2245 train_time:24841ms step_avg:60.44ms
step:412/2245 train_time:24900ms step_avg:60.44ms
step:413/2245 train_time:24961ms step_avg:60.44ms
step:414/2245 train_time:25021ms step_avg:60.44ms
step:415/2245 train_time:25082ms step_avg:60.44ms
step:416/2245 train_time:25141ms step_avg:60.44ms
step:417/2245 train_time:25202ms step_avg:60.44ms
step:418/2245 train_time:25261ms step_avg:60.43ms
step:419/2245 train_time:25323ms step_avg:60.44ms
step:420/2245 train_time:25383ms step_avg:60.44ms
step:421/2245 train_time:25445ms step_avg:60.44ms
step:422/2245 train_time:25504ms step_avg:60.44ms
step:423/2245 train_time:25565ms step_avg:60.44ms
step:424/2245 train_time:25625ms step_avg:60.44ms
step:425/2245 train_time:25686ms step_avg:60.44ms
step:426/2245 train_time:25745ms step_avg:60.44ms
step:427/2245 train_time:25807ms step_avg:60.44ms
step:428/2245 train_time:25865ms step_avg:60.43ms
step:429/2245 train_time:25927ms step_avg:60.44ms
step:430/2245 train_time:25987ms step_avg:60.43ms
step:431/2245 train_time:26048ms step_avg:60.44ms
step:432/2245 train_time:26107ms step_avg:60.43ms
step:433/2245 train_time:26168ms step_avg:60.43ms
step:434/2245 train_time:26227ms step_avg:60.43ms
step:435/2245 train_time:26288ms step_avg:60.43ms
step:436/2245 train_time:26347ms step_avg:60.43ms
step:437/2245 train_time:26409ms step_avg:60.43ms
step:438/2245 train_time:26468ms step_avg:60.43ms
step:439/2245 train_time:26531ms step_avg:60.44ms
step:440/2245 train_time:26590ms step_avg:60.43ms
step:441/2245 train_time:26651ms step_avg:60.43ms
step:442/2245 train_time:26710ms step_avg:60.43ms
step:443/2245 train_time:26771ms step_avg:60.43ms
step:444/2245 train_time:26829ms step_avg:60.43ms
step:445/2245 train_time:26891ms step_avg:60.43ms
step:446/2245 train_time:26950ms step_avg:60.43ms
step:447/2245 train_time:27012ms step_avg:60.43ms
step:448/2245 train_time:27070ms step_avg:60.43ms
step:449/2245 train_time:27133ms step_avg:60.43ms
step:450/2245 train_time:27191ms step_avg:60.42ms
step:451/2245 train_time:27253ms step_avg:60.43ms
step:452/2245 train_time:27311ms step_avg:60.42ms
step:453/2245 train_time:27373ms step_avg:60.43ms
step:454/2245 train_time:27432ms step_avg:60.42ms
step:455/2245 train_time:27494ms step_avg:60.43ms
step:456/2245 train_time:27553ms step_avg:60.42ms
step:457/2245 train_time:27614ms step_avg:60.42ms
step:458/2245 train_time:27673ms step_avg:60.42ms
step:459/2245 train_time:27734ms step_avg:60.42ms
step:460/2245 train_time:27793ms step_avg:60.42ms
step:461/2245 train_time:27854ms step_avg:60.42ms
step:462/2245 train_time:27913ms step_avg:60.42ms
step:463/2245 train_time:27974ms step_avg:60.42ms
step:464/2245 train_time:28033ms step_avg:60.42ms
step:465/2245 train_time:28094ms step_avg:60.42ms
step:466/2245 train_time:28153ms step_avg:60.41ms
step:467/2245 train_time:28215ms step_avg:60.42ms
step:468/2245 train_time:28274ms step_avg:60.41ms
step:469/2245 train_time:28336ms step_avg:60.42ms
step:470/2245 train_time:28395ms step_avg:60.41ms
step:471/2245 train_time:28456ms step_avg:60.42ms
step:472/2245 train_time:28516ms step_avg:60.41ms
step:473/2245 train_time:28577ms step_avg:60.42ms
step:474/2245 train_time:28636ms step_avg:60.41ms
step:475/2245 train_time:28699ms step_avg:60.42ms
step:476/2245 train_time:28758ms step_avg:60.42ms
step:477/2245 train_time:28819ms step_avg:60.42ms
step:478/2245 train_time:28878ms step_avg:60.41ms
step:479/2245 train_time:28940ms step_avg:60.42ms
step:480/2245 train_time:28999ms step_avg:60.41ms
step:481/2245 train_time:29060ms step_avg:60.42ms
step:482/2245 train_time:29119ms step_avg:60.41ms
step:483/2245 train_time:29181ms step_avg:60.42ms
step:484/2245 train_time:29241ms step_avg:60.41ms
step:485/2245 train_time:29302ms step_avg:60.42ms
step:486/2245 train_time:29361ms step_avg:60.41ms
step:487/2245 train_time:29423ms step_avg:60.42ms
step:488/2245 train_time:29483ms step_avg:60.42ms
step:489/2245 train_time:29545ms step_avg:60.42ms
step:490/2245 train_time:29605ms step_avg:60.42ms
step:491/2245 train_time:29667ms step_avg:60.42ms
step:492/2245 train_time:29727ms step_avg:60.42ms
step:493/2245 train_time:29788ms step_avg:60.42ms
step:494/2245 train_time:29847ms step_avg:60.42ms
step:495/2245 train_time:29908ms step_avg:60.42ms
step:496/2245 train_time:29968ms step_avg:60.42ms
step:497/2245 train_time:30029ms step_avg:60.42ms
step:498/2245 train_time:30088ms step_avg:60.42ms
step:499/2245 train_time:30150ms step_avg:60.42ms
step:500/2245 train_time:30209ms step_avg:60.42ms
step:500/2245 val_loss:3.8189 train_time:30272ms step_avg:60.54ms
step:501/2245 train_time:30292ms step_avg:60.46ms
step:502/2245 train_time:30333ms step_avg:60.42ms
step:503/2245 train_time:30398ms step_avg:60.43ms
step:504/2245 train_time:30458ms step_avg:60.43ms
step:505/2245 train_time:30520ms step_avg:60.44ms
step:506/2245 train_time:30580ms step_avg:60.43ms
step:507/2245 train_time:30640ms step_avg:60.43ms
step:508/2245 train_time:30699ms step_avg:60.43ms
step:509/2245 train_time:30761ms step_avg:60.43ms
step:510/2245 train_time:30819ms step_avg:60.43ms
step:511/2245 train_time:30880ms step_avg:60.43ms
step:512/2245 train_time:30939ms step_avg:60.43ms
step:513/2245 train_time:30999ms step_avg:60.43ms
step:514/2245 train_time:31058ms step_avg:60.42ms
step:515/2245 train_time:31118ms step_avg:60.42ms
step:516/2245 train_time:31177ms step_avg:60.42ms
step:517/2245 train_time:31241ms step_avg:60.43ms
step:518/2245 train_time:31302ms step_avg:60.43ms
step:519/2245 train_time:31365ms step_avg:60.43ms
step:520/2245 train_time:31425ms step_avg:60.43ms
step:521/2245 train_time:31487ms step_avg:60.44ms
step:522/2245 train_time:31548ms step_avg:60.44ms
step:523/2245 train_time:31609ms step_avg:60.44ms
step:524/2245 train_time:31668ms step_avg:60.44ms
step:525/2245 train_time:31729ms step_avg:60.44ms
step:526/2245 train_time:31788ms step_avg:60.43ms
step:527/2245 train_time:31850ms step_avg:60.44ms
step:528/2245 train_time:31908ms step_avg:60.43ms
step:529/2245 train_time:31969ms step_avg:60.43ms
step:530/2245 train_time:32028ms step_avg:60.43ms
step:531/2245 train_time:32089ms step_avg:60.43ms
step:532/2245 train_time:32148ms step_avg:60.43ms
step:533/2245 train_time:32211ms step_avg:60.43ms
step:534/2245 train_time:32270ms step_avg:60.43ms
step:535/2245 train_time:32333ms step_avg:60.44ms
step:536/2245 train_time:32392ms step_avg:60.43ms
step:537/2245 train_time:32453ms step_avg:60.43ms
step:538/2245 train_time:32512ms step_avg:60.43ms
step:539/2245 train_time:32573ms step_avg:60.43ms
step:540/2245 train_time:32633ms step_avg:60.43ms
step:541/2245 train_time:32693ms step_avg:60.43ms
step:542/2245 train_time:32752ms step_avg:60.43ms
step:543/2245 train_time:32813ms step_avg:60.43ms
step:544/2245 train_time:32871ms step_avg:60.43ms
step:545/2245 train_time:32933ms step_avg:60.43ms
step:546/2245 train_time:32992ms step_avg:60.42ms
step:547/2245 train_time:33053ms step_avg:60.43ms
step:548/2245 train_time:33112ms step_avg:60.42ms
step:549/2245 train_time:33174ms step_avg:60.43ms
step:550/2245 train_time:33233ms step_avg:60.42ms
step:551/2245 train_time:33295ms step_avg:60.43ms
step:552/2245 train_time:33354ms step_avg:60.42ms
step:553/2245 train_time:33415ms step_avg:60.42ms
step:554/2245 train_time:33474ms step_avg:60.42ms
step:555/2245 train_time:33535ms step_avg:60.42ms
step:556/2245 train_time:33594ms step_avg:60.42ms
step:557/2245 train_time:33656ms step_avg:60.42ms
step:558/2245 train_time:33715ms step_avg:60.42ms
step:559/2245 train_time:33776ms step_avg:60.42ms
step:560/2245 train_time:33835ms step_avg:60.42ms
step:561/2245 train_time:33896ms step_avg:60.42ms
step:562/2245 train_time:33957ms step_avg:60.42ms
step:563/2245 train_time:34017ms step_avg:60.42ms
step:564/2245 train_time:34076ms step_avg:60.42ms
step:565/2245 train_time:34138ms step_avg:60.42ms
step:566/2245 train_time:34197ms step_avg:60.42ms
step:567/2245 train_time:34258ms step_avg:60.42ms
step:568/2245 train_time:34317ms step_avg:60.42ms
step:569/2245 train_time:34379ms step_avg:60.42ms
step:570/2245 train_time:34438ms step_avg:60.42ms
step:571/2245 train_time:34499ms step_avg:60.42ms
step:572/2245 train_time:34558ms step_avg:60.42ms
step:573/2245 train_time:34619ms step_avg:60.42ms
step:574/2245 train_time:34679ms step_avg:60.42ms
step:575/2245 train_time:34740ms step_avg:60.42ms
step:576/2245 train_time:34799ms step_avg:60.42ms
step:577/2245 train_time:34860ms step_avg:60.42ms
step:578/2245 train_time:34919ms step_avg:60.41ms
step:579/2245 train_time:34981ms step_avg:60.42ms
step:580/2245 train_time:35040ms step_avg:60.41ms
step:581/2245 train_time:35102ms step_avg:60.42ms
step:582/2245 train_time:35162ms step_avg:60.42ms
step:583/2245 train_time:35224ms step_avg:60.42ms
step:584/2245 train_time:35283ms step_avg:60.42ms
step:585/2245 train_time:35345ms step_avg:60.42ms
step:586/2245 train_time:35404ms step_avg:60.42ms
step:587/2245 train_time:35467ms step_avg:60.42ms
step:588/2245 train_time:35527ms step_avg:60.42ms
step:589/2245 train_time:35589ms step_avg:60.42ms
step:590/2245 train_time:35648ms step_avg:60.42ms
step:591/2245 train_time:35709ms step_avg:60.42ms
step:592/2245 train_time:35769ms step_avg:60.42ms
step:593/2245 train_time:35830ms step_avg:60.42ms
step:594/2245 train_time:35889ms step_avg:60.42ms
step:595/2245 train_time:35950ms step_avg:60.42ms
step:596/2245 train_time:36010ms step_avg:60.42ms
step:597/2245 train_time:36071ms step_avg:60.42ms
step:598/2245 train_time:36130ms step_avg:60.42ms
step:599/2245 train_time:36192ms step_avg:60.42ms
step:600/2245 train_time:36250ms step_avg:60.42ms
step:601/2245 train_time:36312ms step_avg:60.42ms
step:602/2245 train_time:36371ms step_avg:60.42ms
step:603/2245 train_time:36432ms step_avg:60.42ms
step:604/2245 train_time:36491ms step_avg:60.42ms
step:605/2245 train_time:36552ms step_avg:60.42ms
step:606/2245 train_time:36611ms step_avg:60.41ms
step:607/2245 train_time:36673ms step_avg:60.42ms
step:608/2245 train_time:36731ms step_avg:60.41ms
step:609/2245 train_time:36793ms step_avg:60.42ms
step:610/2245 train_time:36852ms step_avg:60.41ms
step:611/2245 train_time:36914ms step_avg:60.42ms
step:612/2245 train_time:36973ms step_avg:60.41ms
step:613/2245 train_time:37034ms step_avg:60.42ms
step:614/2245 train_time:37094ms step_avg:60.41ms
step:615/2245 train_time:37156ms step_avg:60.42ms
step:616/2245 train_time:37215ms step_avg:60.41ms
step:617/2245 train_time:37277ms step_avg:60.42ms
step:618/2245 train_time:37336ms step_avg:60.41ms
step:619/2245 train_time:37398ms step_avg:60.42ms
step:620/2245 train_time:37457ms step_avg:60.41ms
step:621/2245 train_time:37518ms step_avg:60.42ms
step:622/2245 train_time:37578ms step_avg:60.41ms
step:623/2245 train_time:37639ms step_avg:60.42ms
step:624/2245 train_time:37698ms step_avg:60.41ms
step:625/2245 train_time:37760ms step_avg:60.42ms
step:626/2245 train_time:37820ms step_avg:60.41ms
step:627/2245 train_time:37881ms step_avg:60.42ms
step:628/2245 train_time:37941ms step_avg:60.42ms
step:629/2245 train_time:38002ms step_avg:60.42ms
step:630/2245 train_time:38061ms step_avg:60.41ms
step:631/2245 train_time:38123ms step_avg:60.42ms
step:632/2245 train_time:38183ms step_avg:60.42ms
step:633/2245 train_time:38244ms step_avg:60.42ms
step:634/2245 train_time:38304ms step_avg:60.42ms
step:635/2245 train_time:38366ms step_avg:60.42ms
step:636/2245 train_time:38426ms step_avg:60.42ms
step:637/2245 train_time:38487ms step_avg:60.42ms
step:638/2245 train_time:38546ms step_avg:60.42ms
step:639/2245 train_time:38607ms step_avg:60.42ms
step:640/2245 train_time:38668ms step_avg:60.42ms
step:641/2245 train_time:38729ms step_avg:60.42ms
step:642/2245 train_time:38788ms step_avg:60.42ms
step:643/2245 train_time:38850ms step_avg:60.42ms
step:644/2245 train_time:38909ms step_avg:60.42ms
step:645/2245 train_time:38970ms step_avg:60.42ms
step:646/2245 train_time:39030ms step_avg:60.42ms
step:647/2245 train_time:39093ms step_avg:60.42ms
step:648/2245 train_time:39152ms step_avg:60.42ms
step:649/2245 train_time:39213ms step_avg:60.42ms
step:650/2245 train_time:39271ms step_avg:60.42ms
step:651/2245 train_time:39333ms step_avg:60.42ms
step:652/2245 train_time:39392ms step_avg:60.42ms
step:653/2245 train_time:39453ms step_avg:60.42ms
step:654/2245 train_time:39511ms step_avg:60.41ms
step:655/2245 train_time:39573ms step_avg:60.42ms
step:656/2245 train_time:39631ms step_avg:60.41ms
step:657/2245 train_time:39693ms step_avg:60.41ms
step:658/2245 train_time:39751ms step_avg:60.41ms
step:659/2245 train_time:39813ms step_avg:60.41ms
step:660/2245 train_time:39871ms step_avg:60.41ms
step:661/2245 train_time:39933ms step_avg:60.41ms
step:662/2245 train_time:39992ms step_avg:60.41ms
step:663/2245 train_time:40054ms step_avg:60.41ms
step:664/2245 train_time:40112ms step_avg:60.41ms
step:665/2245 train_time:40175ms step_avg:60.41ms
step:666/2245 train_time:40233ms step_avg:60.41ms
step:667/2245 train_time:40295ms step_avg:60.41ms
step:668/2245 train_time:40354ms step_avg:60.41ms
step:669/2245 train_time:40415ms step_avg:60.41ms
step:670/2245 train_time:40474ms step_avg:60.41ms
step:671/2245 train_time:40535ms step_avg:60.41ms
step:672/2245 train_time:40594ms step_avg:60.41ms
step:673/2245 train_time:40656ms step_avg:60.41ms
step:674/2245 train_time:40715ms step_avg:60.41ms
step:675/2245 train_time:40776ms step_avg:60.41ms
step:676/2245 train_time:40836ms step_avg:60.41ms
step:677/2245 train_time:40898ms step_avg:60.41ms
step:678/2245 train_time:40957ms step_avg:60.41ms
step:679/2245 train_time:41019ms step_avg:60.41ms
step:680/2245 train_time:41078ms step_avg:60.41ms
step:681/2245 train_time:41140ms step_avg:60.41ms
step:682/2245 train_time:41199ms step_avg:60.41ms
step:683/2245 train_time:41260ms step_avg:60.41ms
step:684/2245 train_time:41320ms step_avg:60.41ms
step:685/2245 train_time:41381ms step_avg:60.41ms
step:686/2245 train_time:41441ms step_avg:60.41ms
step:687/2245 train_time:41502ms step_avg:60.41ms
step:688/2245 train_time:41561ms step_avg:60.41ms
step:689/2245 train_time:41623ms step_avg:60.41ms
step:690/2245 train_time:41683ms step_avg:60.41ms
step:691/2245 train_time:41744ms step_avg:60.41ms
step:692/2245 train_time:41804ms step_avg:60.41ms
step:693/2245 train_time:41866ms step_avg:60.41ms
step:694/2245 train_time:41926ms step_avg:60.41ms
step:695/2245 train_time:41987ms step_avg:60.41ms
step:696/2245 train_time:42047ms step_avg:60.41ms
step:697/2245 train_time:42108ms step_avg:60.41ms
step:698/2245 train_time:42168ms step_avg:60.41ms
step:699/2245 train_time:42230ms step_avg:60.42ms
step:700/2245 train_time:42289ms step_avg:60.41ms
step:701/2245 train_time:42350ms step_avg:60.41ms
step:702/2245 train_time:42410ms step_avg:60.41ms
step:703/2245 train_time:42471ms step_avg:60.41ms
step:704/2245 train_time:42531ms step_avg:60.41ms
step:705/2245 train_time:42593ms step_avg:60.42ms
step:706/2245 train_time:42652ms step_avg:60.41ms
step:707/2245 train_time:42713ms step_avg:60.42ms
step:708/2245 train_time:42772ms step_avg:60.41ms
step:709/2245 train_time:42834ms step_avg:60.41ms
step:710/2245 train_time:42892ms step_avg:60.41ms
step:711/2245 train_time:42954ms step_avg:60.41ms
step:712/2245 train_time:43012ms step_avg:60.41ms
step:713/2245 train_time:43073ms step_avg:60.41ms
step:714/2245 train_time:43133ms step_avg:60.41ms
step:715/2245 train_time:43194ms step_avg:60.41ms
step:716/2245 train_time:43253ms step_avg:60.41ms
step:717/2245 train_time:43315ms step_avg:60.41ms
step:718/2245 train_time:43373ms step_avg:60.41ms
step:719/2245 train_time:43435ms step_avg:60.41ms
step:720/2245 train_time:43494ms step_avg:60.41ms
step:721/2245 train_time:43558ms step_avg:60.41ms
step:722/2245 train_time:44001ms step_avg:60.94ms
step:723/2245 train_time:44060ms step_avg:60.94ms
step:724/2245 train_time:44118ms step_avg:60.94ms
step:725/2245 train_time:44179ms step_avg:60.94ms
step:726/2245 train_time:44237ms step_avg:60.93ms
step:727/2245 train_time:44298ms step_avg:60.93ms
step:728/2245 train_time:44356ms step_avg:60.93ms
step:729/2245 train_time:44417ms step_avg:60.93ms
step:730/2245 train_time:44475ms step_avg:60.92ms
step:731/2245 train_time:44535ms step_avg:60.92ms
step:732/2245 train_time:44594ms step_avg:60.92ms
step:733/2245 train_time:44654ms step_avg:60.92ms
step:734/2245 train_time:44713ms step_avg:60.92ms
step:735/2245 train_time:44773ms step_avg:60.92ms
step:736/2245 train_time:44834ms step_avg:60.92ms
step:737/2245 train_time:44904ms step_avg:60.93ms
step:738/2245 train_time:44966ms step_avg:60.93ms
step:739/2245 train_time:45029ms step_avg:60.93ms
step:740/2245 train_time:45089ms step_avg:60.93ms
step:741/2245 train_time:45152ms step_avg:60.93ms
step:742/2245 train_time:45211ms step_avg:60.93ms
step:743/2245 train_time:45274ms step_avg:60.93ms
step:744/2245 train_time:45333ms step_avg:60.93ms
step:745/2245 train_time:45394ms step_avg:60.93ms
step:746/2245 train_time:45453ms step_avg:60.93ms
step:747/2245 train_time:45515ms step_avg:60.93ms
step:748/2245 train_time:45574ms step_avg:60.93ms
step:749/2245 train_time:45635ms step_avg:60.93ms
step:750/2245 train_time:45694ms step_avg:60.93ms
step:750/2245 val_loss:3.6675 train_time:45756ms step_avg:61.01ms
step:751/2245 train_time:45778ms step_avg:60.96ms
step:752/2245 train_time:45819ms step_avg:60.93ms
step:753/2245 train_time:45880ms step_avg:60.93ms
step:754/2245 train_time:45940ms step_avg:60.93ms
step:755/2245 train_time:46004ms step_avg:60.93ms
step:756/2245 train_time:46064ms step_avg:60.93ms
step:757/2245 train_time:46126ms step_avg:60.93ms
step:758/2245 train_time:46185ms step_avg:60.93ms
step:759/2245 train_time:46246ms step_avg:60.93ms
step:760/2245 train_time:46305ms step_avg:60.93ms
step:761/2245 train_time:46366ms step_avg:60.93ms
step:762/2245 train_time:46425ms step_avg:60.93ms
step:763/2245 train_time:46487ms step_avg:60.93ms
step:764/2245 train_time:46546ms step_avg:60.92ms
step:765/2245 train_time:46608ms step_avg:60.93ms
step:766/2245 train_time:46672ms step_avg:60.93ms
step:767/2245 train_time:46739ms step_avg:60.94ms
step:768/2245 train_time:46801ms step_avg:60.94ms
step:769/2245 train_time:46863ms step_avg:60.94ms
step:770/2245 train_time:46923ms step_avg:60.94ms
step:771/2245 train_time:46985ms step_avg:60.94ms
step:772/2245 train_time:47045ms step_avg:60.94ms
step:773/2245 train_time:47107ms step_avg:60.94ms
step:774/2245 train_time:47166ms step_avg:60.94ms
step:775/2245 train_time:47228ms step_avg:60.94ms
step:776/2245 train_time:47288ms step_avg:60.94ms
step:777/2245 train_time:47349ms step_avg:60.94ms
step:778/2245 train_time:47409ms step_avg:60.94ms
step:779/2245 train_time:47471ms step_avg:60.94ms
step:780/2245 train_time:47533ms step_avg:60.94ms
step:781/2245 train_time:47594ms step_avg:60.94ms
step:782/2245 train_time:47656ms step_avg:60.94ms
step:783/2245 train_time:47720ms step_avg:60.94ms
step:784/2245 train_time:47781ms step_avg:60.94ms
step:785/2245 train_time:47843ms step_avg:60.95ms
step:786/2245 train_time:47904ms step_avg:60.95ms
step:787/2245 train_time:47966ms step_avg:60.95ms
step:788/2245 train_time:48025ms step_avg:60.95ms
step:789/2245 train_time:48087ms step_avg:60.95ms
step:790/2245 train_time:48146ms step_avg:60.94ms
step:791/2245 train_time:48208ms step_avg:60.95ms
step:792/2245 train_time:48268ms step_avg:60.94ms
step:793/2245 train_time:48330ms step_avg:60.95ms
step:794/2245 train_time:48389ms step_avg:60.94ms
step:795/2245 train_time:48451ms step_avg:60.94ms
step:796/2245 train_time:48511ms step_avg:60.94ms
step:797/2245 train_time:48574ms step_avg:60.95ms
step:798/2245 train_time:48634ms step_avg:60.94ms
step:799/2245 train_time:48698ms step_avg:60.95ms
step:800/2245 train_time:48759ms step_avg:60.95ms
step:801/2245 train_time:48822ms step_avg:60.95ms
step:802/2245 train_time:48882ms step_avg:60.95ms
step:803/2245 train_time:48944ms step_avg:60.95ms
step:804/2245 train_time:49003ms step_avg:60.95ms
step:805/2245 train_time:49065ms step_avg:60.95ms
step:806/2245 train_time:49125ms step_avg:60.95ms
step:807/2245 train_time:49187ms step_avg:60.95ms
step:808/2245 train_time:49246ms step_avg:60.95ms
step:809/2245 train_time:49309ms step_avg:60.95ms
step:810/2245 train_time:49368ms step_avg:60.95ms
step:811/2245 train_time:49430ms step_avg:60.95ms
step:812/2245 train_time:49491ms step_avg:60.95ms
step:813/2245 train_time:49553ms step_avg:60.95ms
step:814/2245 train_time:49613ms step_avg:60.95ms
step:815/2245 train_time:49676ms step_avg:60.95ms
step:816/2245 train_time:49737ms step_avg:60.95ms
step:817/2245 train_time:49800ms step_avg:60.96ms
step:818/2245 train_time:49860ms step_avg:60.95ms
step:819/2245 train_time:49922ms step_avg:60.96ms
step:820/2245 train_time:49982ms step_avg:60.95ms
step:821/2245 train_time:50044ms step_avg:60.96ms
step:822/2245 train_time:50104ms step_avg:60.95ms
step:823/2245 train_time:50166ms step_avg:60.96ms
step:824/2245 train_time:50225ms step_avg:60.95ms
step:825/2245 train_time:50287ms step_avg:60.95ms
step:826/2245 train_time:50347ms step_avg:60.95ms
step:827/2245 train_time:50410ms step_avg:60.96ms
step:828/2245 train_time:50470ms step_avg:60.95ms
step:829/2245 train_time:50532ms step_avg:60.96ms
step:830/2245 train_time:50593ms step_avg:60.96ms
step:831/2245 train_time:50657ms step_avg:60.96ms
step:832/2245 train_time:50717ms step_avg:60.96ms
step:833/2245 train_time:50779ms step_avg:60.96ms
step:834/2245 train_time:50839ms step_avg:60.96ms
step:835/2245 train_time:50902ms step_avg:60.96ms
step:836/2245 train_time:50961ms step_avg:60.96ms
step:837/2245 train_time:51023ms step_avg:60.96ms
step:838/2245 train_time:51083ms step_avg:60.96ms
step:839/2245 train_time:51146ms step_avg:60.96ms
step:840/2245 train_time:51205ms step_avg:60.96ms
step:841/2245 train_time:51267ms step_avg:60.96ms
step:842/2245 train_time:51326ms step_avg:60.96ms
step:843/2245 train_time:51388ms step_avg:60.96ms
step:844/2245 train_time:51448ms step_avg:60.96ms
step:845/2245 train_time:51510ms step_avg:60.96ms
step:846/2245 train_time:51570ms step_avg:60.96ms
step:847/2245 train_time:51634ms step_avg:60.96ms
step:848/2245 train_time:51695ms step_avg:60.96ms
step:849/2245 train_time:51758ms step_avg:60.96ms
step:850/2245 train_time:51818ms step_avg:60.96ms
step:851/2245 train_time:51880ms step_avg:60.96ms
step:852/2245 train_time:51940ms step_avg:60.96ms
step:853/2245 train_time:52002ms step_avg:60.96ms
step:854/2245 train_time:52062ms step_avg:60.96ms
step:855/2245 train_time:52124ms step_avg:60.96ms
step:856/2245 train_time:52183ms step_avg:60.96ms
step:857/2245 train_time:52245ms step_avg:60.96ms
step:858/2245 train_time:52304ms step_avg:60.96ms
step:859/2245 train_time:52367ms step_avg:60.96ms
step:860/2245 train_time:52426ms step_avg:60.96ms
step:861/2245 train_time:52489ms step_avg:60.96ms
step:862/2245 train_time:52549ms step_avg:60.96ms
step:863/2245 train_time:52612ms step_avg:60.96ms
step:864/2245 train_time:52672ms step_avg:60.96ms
step:865/2245 train_time:52735ms step_avg:60.97ms
step:866/2245 train_time:52796ms step_avg:60.96ms
step:867/2245 train_time:52857ms step_avg:60.97ms
step:868/2245 train_time:52917ms step_avg:60.96ms
step:869/2245 train_time:52979ms step_avg:60.97ms
step:870/2245 train_time:53040ms step_avg:60.97ms
step:871/2245 train_time:53102ms step_avg:60.97ms
step:872/2245 train_time:53162ms step_avg:60.97ms
step:873/2245 train_time:53224ms step_avg:60.97ms
step:874/2245 train_time:53283ms step_avg:60.96ms
step:875/2245 train_time:53345ms step_avg:60.97ms
step:876/2245 train_time:53405ms step_avg:60.96ms
step:877/2245 train_time:53468ms step_avg:60.97ms
step:878/2245 train_time:53530ms step_avg:60.97ms
step:879/2245 train_time:53591ms step_avg:60.97ms
step:880/2245 train_time:53652ms step_avg:60.97ms
step:881/2245 train_time:53714ms step_avg:60.97ms
step:882/2245 train_time:53775ms step_avg:60.97ms
step:883/2245 train_time:53838ms step_avg:60.97ms
step:884/2245 train_time:53898ms step_avg:60.97ms
step:885/2245 train_time:53960ms step_avg:60.97ms
step:886/2245 train_time:54020ms step_avg:60.97ms
step:887/2245 train_time:54082ms step_avg:60.97ms
step:888/2245 train_time:54142ms step_avg:60.97ms
step:889/2245 train_time:54204ms step_avg:60.97ms
step:890/2245 train_time:54263ms step_avg:60.97ms
step:891/2245 train_time:54328ms step_avg:60.97ms
step:892/2245 train_time:54385ms step_avg:60.97ms
step:893/2245 train_time:54448ms step_avg:60.97ms
step:894/2245 train_time:54508ms step_avg:60.97ms
step:895/2245 train_time:54571ms step_avg:60.97ms
step:896/2245 train_time:54632ms step_avg:60.97ms
step:897/2245 train_time:54694ms step_avg:60.97ms
step:898/2245 train_time:54755ms step_avg:60.97ms
step:899/2245 train_time:54818ms step_avg:60.98ms
step:900/2245 train_time:54878ms step_avg:60.98ms
step:901/2245 train_time:54940ms step_avg:60.98ms
step:902/2245 train_time:55000ms step_avg:60.98ms
step:903/2245 train_time:55062ms step_avg:60.98ms
step:904/2245 train_time:55122ms step_avg:60.98ms
step:905/2245 train_time:55184ms step_avg:60.98ms
step:906/2245 train_time:55243ms step_avg:60.98ms
step:907/2245 train_time:55305ms step_avg:60.98ms
step:908/2245 train_time:55365ms step_avg:60.97ms
step:909/2245 train_time:55428ms step_avg:60.98ms
step:910/2245 train_time:55488ms step_avg:60.98ms
step:911/2245 train_time:55550ms step_avg:60.98ms
step:912/2245 train_time:55611ms step_avg:60.98ms
step:913/2245 train_time:55673ms step_avg:60.98ms
step:914/2245 train_time:55734ms step_avg:60.98ms
step:915/2245 train_time:55797ms step_avg:60.98ms
step:916/2245 train_time:55857ms step_avg:60.98ms
step:917/2245 train_time:55919ms step_avg:60.98ms
step:918/2245 train_time:55979ms step_avg:60.98ms
step:919/2245 train_time:56041ms step_avg:60.98ms
step:920/2245 train_time:56101ms step_avg:60.98ms
step:921/2245 train_time:56162ms step_avg:60.98ms
step:922/2245 train_time:56222ms step_avg:60.98ms
step:923/2245 train_time:56284ms step_avg:60.98ms
step:924/2245 train_time:56344ms step_avg:60.98ms
step:925/2245 train_time:56406ms step_avg:60.98ms
step:926/2245 train_time:56467ms step_avg:60.98ms
step:927/2245 train_time:56530ms step_avg:60.98ms
step:928/2245 train_time:56589ms step_avg:60.98ms
step:929/2245 train_time:56652ms step_avg:60.98ms
step:930/2245 train_time:56712ms step_avg:60.98ms
step:931/2245 train_time:56775ms step_avg:60.98ms
step:932/2245 train_time:56837ms step_avg:60.98ms
step:933/2245 train_time:56898ms step_avg:60.98ms
step:934/2245 train_time:56958ms step_avg:60.98ms
step:935/2245 train_time:57021ms step_avg:60.99ms
step:936/2245 train_time:57081ms step_avg:60.98ms
step:937/2245 train_time:57144ms step_avg:60.99ms
step:938/2245 train_time:57204ms step_avg:60.98ms
step:939/2245 train_time:57265ms step_avg:60.99ms
step:940/2245 train_time:57325ms step_avg:60.98ms
step:941/2245 train_time:57386ms step_avg:60.98ms
step:942/2245 train_time:57446ms step_avg:60.98ms
step:943/2245 train_time:57508ms step_avg:60.98ms
step:944/2245 train_time:57568ms step_avg:60.98ms
step:945/2245 train_time:57631ms step_avg:60.99ms
step:946/2245 train_time:57692ms step_avg:60.98ms
step:947/2245 train_time:57755ms step_avg:60.99ms
step:948/2245 train_time:57816ms step_avg:60.99ms
step:949/2245 train_time:57878ms step_avg:60.99ms
step:950/2245 train_time:57939ms step_avg:60.99ms
step:951/2245 train_time:58002ms step_avg:60.99ms
step:952/2245 train_time:58062ms step_avg:60.99ms
step:953/2245 train_time:58124ms step_avg:60.99ms
step:954/2245 train_time:58184ms step_avg:60.99ms
step:955/2245 train_time:58246ms step_avg:60.99ms
step:956/2245 train_time:58306ms step_avg:60.99ms
step:957/2245 train_time:58368ms step_avg:60.99ms
step:958/2245 train_time:58428ms step_avg:60.99ms
step:959/2245 train_time:58490ms step_avg:60.99ms
step:960/2245 train_time:58550ms step_avg:60.99ms
step:961/2245 train_time:58612ms step_avg:60.99ms
step:962/2245 train_time:58672ms step_avg:60.99ms
step:963/2245 train_time:58735ms step_avg:60.99ms
step:964/2245 train_time:58796ms step_avg:60.99ms
step:965/2245 train_time:58859ms step_avg:60.99ms
step:966/2245 train_time:58919ms step_avg:60.99ms
step:967/2245 train_time:58981ms step_avg:60.99ms
step:968/2245 train_time:59041ms step_avg:60.99ms
step:969/2245 train_time:59102ms step_avg:60.99ms
step:970/2245 train_time:59162ms step_avg:60.99ms
step:971/2245 train_time:59225ms step_avg:60.99ms
step:972/2245 train_time:59285ms step_avg:60.99ms
step:973/2245 train_time:59346ms step_avg:60.99ms
step:974/2245 train_time:59406ms step_avg:60.99ms
step:975/2245 train_time:59468ms step_avg:60.99ms
step:976/2245 train_time:59528ms step_avg:60.99ms
step:977/2245 train_time:59590ms step_avg:60.99ms
step:978/2245 train_time:59650ms step_avg:60.99ms
step:979/2245 train_time:59713ms step_avg:60.99ms
step:980/2245 train_time:59774ms step_avg:60.99ms
step:981/2245 train_time:59837ms step_avg:61.00ms
step:982/2245 train_time:59897ms step_avg:61.00ms
step:983/2245 train_time:59960ms step_avg:61.00ms
step:984/2245 train_time:60021ms step_avg:61.00ms
step:985/2245 train_time:60082ms step_avg:61.00ms
step:986/2245 train_time:60142ms step_avg:61.00ms
step:987/2245 train_time:60204ms step_avg:61.00ms
step:988/2245 train_time:60264ms step_avg:61.00ms
step:989/2245 train_time:60326ms step_avg:61.00ms
step:990/2245 train_time:60385ms step_avg:61.00ms
step:991/2245 train_time:60447ms step_avg:61.00ms
step:992/2245 train_time:60507ms step_avg:61.00ms
step:993/2245 train_time:60570ms step_avg:61.00ms
step:994/2245 train_time:60630ms step_avg:61.00ms
step:995/2245 train_time:60693ms step_avg:61.00ms
step:996/2245 train_time:60753ms step_avg:61.00ms
step:997/2245 train_time:60816ms step_avg:61.00ms
step:998/2245 train_time:60877ms step_avg:61.00ms
step:999/2245 train_time:60939ms step_avg:61.00ms
step:1000/2245 train_time:61000ms step_avg:61.00ms
step:1000/2245 val_loss:3.5894 train_time:61064ms step_avg:61.06ms
step:1001/2245 train_time:61085ms step_avg:61.02ms
step:1002/2245 train_time:61128ms step_avg:61.01ms
step:1003/2245 train_time:61194ms step_avg:61.01ms
step:1004/2245 train_time:61254ms step_avg:61.01ms
step:1005/2245 train_time:61315ms step_avg:61.01ms
step:1006/2245 train_time:61375ms step_avg:61.01ms
step:1007/2245 train_time:61436ms step_avg:61.01ms
step:1008/2245 train_time:61495ms step_avg:61.01ms
step:1009/2245 train_time:61557ms step_avg:61.01ms
step:1010/2245 train_time:61616ms step_avg:61.01ms
step:1011/2245 train_time:61678ms step_avg:61.01ms
step:1012/2245 train_time:61737ms step_avg:61.00ms
step:1013/2245 train_time:61799ms step_avg:61.01ms
step:1014/2245 train_time:61859ms step_avg:61.01ms
step:1015/2245 train_time:61922ms step_avg:61.01ms
step:1016/2245 train_time:61982ms step_avg:61.01ms
step:1017/2245 train_time:62047ms step_avg:61.01ms
step:1018/2245 train_time:62109ms step_avg:61.01ms
step:1019/2245 train_time:62173ms step_avg:61.01ms
step:1020/2245 train_time:62234ms step_avg:61.01ms
step:1021/2245 train_time:62296ms step_avg:61.02ms
step:1022/2245 train_time:62356ms step_avg:61.01ms
step:1023/2245 train_time:62418ms step_avg:61.01ms
step:1024/2245 train_time:62478ms step_avg:61.01ms
step:1025/2245 train_time:62540ms step_avg:61.01ms
step:1026/2245 train_time:62600ms step_avg:61.01ms
step:1027/2245 train_time:62662ms step_avg:61.01ms
step:1028/2245 train_time:62722ms step_avg:61.01ms
step:1029/2245 train_time:62784ms step_avg:61.01ms
step:1030/2245 train_time:62843ms step_avg:61.01ms
step:1031/2245 train_time:62906ms step_avg:61.01ms
step:1032/2245 train_time:62968ms step_avg:61.02ms
step:1033/2245 train_time:63030ms step_avg:61.02ms
step:1034/2245 train_time:63090ms step_avg:61.02ms
step:1035/2245 train_time:63154ms step_avg:61.02ms
step:1036/2245 train_time:63214ms step_avg:61.02ms
step:1037/2245 train_time:63277ms step_avg:61.02ms
step:1038/2245 train_time:63338ms step_avg:61.02ms
step:1039/2245 train_time:63400ms step_avg:61.02ms
step:1040/2245 train_time:63460ms step_avg:61.02ms
step:1041/2245 train_time:63522ms step_avg:61.02ms
step:1042/2245 train_time:63582ms step_avg:61.02ms
step:1043/2245 train_time:63644ms step_avg:61.02ms
step:1044/2245 train_time:63703ms step_avg:61.02ms
step:1045/2245 train_time:63765ms step_avg:61.02ms
step:1046/2245 train_time:63825ms step_avg:61.02ms
step:1047/2245 train_time:63887ms step_avg:61.02ms
step:1048/2245 train_time:63948ms step_avg:61.02ms
step:1049/2245 train_time:64011ms step_avg:61.02ms
step:1050/2245 train_time:64071ms step_avg:61.02ms
step:1051/2245 train_time:64135ms step_avg:61.02ms
step:1052/2245 train_time:64195ms step_avg:61.02ms
step:1053/2245 train_time:64258ms step_avg:61.02ms
step:1054/2245 train_time:64318ms step_avg:61.02ms
step:1055/2245 train_time:64380ms step_avg:61.02ms
step:1056/2245 train_time:64439ms step_avg:61.02ms
step:1057/2245 train_time:64501ms step_avg:61.02ms
step:1058/2245 train_time:64561ms step_avg:61.02ms
step:1059/2245 train_time:64623ms step_avg:61.02ms
step:1060/2245 train_time:64684ms step_avg:61.02ms
step:1061/2245 train_time:64745ms step_avg:61.02ms
step:1062/2245 train_time:64805ms step_avg:61.02ms
step:1063/2245 train_time:64868ms step_avg:61.02ms
step:1064/2245 train_time:64928ms step_avg:61.02ms
step:1065/2245 train_time:64990ms step_avg:61.02ms
step:1066/2245 train_time:65050ms step_avg:61.02ms
step:1067/2245 train_time:65113ms step_avg:61.02ms
step:1068/2245 train_time:65173ms step_avg:61.02ms
step:1069/2245 train_time:65235ms step_avg:61.02ms
step:1070/2245 train_time:65294ms step_avg:61.02ms
step:1071/2245 train_time:65360ms step_avg:61.03ms
step:1072/2245 train_time:65416ms step_avg:61.02ms
step:1073/2245 train_time:65479ms step_avg:61.02ms
step:1074/2245 train_time:65538ms step_avg:61.02ms
step:1075/2245 train_time:65601ms step_avg:61.02ms
step:1076/2245 train_time:65661ms step_avg:61.02ms
step:1077/2245 train_time:65723ms step_avg:61.02ms
step:1078/2245 train_time:65784ms step_avg:61.02ms
step:1079/2245 train_time:65846ms step_avg:61.03ms
step:1080/2245 train_time:65907ms step_avg:61.02ms
step:1081/2245 train_time:65970ms step_avg:61.03ms
step:1082/2245 train_time:66031ms step_avg:61.03ms
step:1083/2245 train_time:66093ms step_avg:61.03ms
step:1084/2245 train_time:66153ms step_avg:61.03ms
step:1085/2245 train_time:66215ms step_avg:61.03ms
step:1086/2245 train_time:66275ms step_avg:61.03ms
step:1087/2245 train_time:66337ms step_avg:61.03ms
step:1088/2245 train_time:66397ms step_avg:61.03ms
step:1089/2245 train_time:66460ms step_avg:61.03ms
step:1090/2245 train_time:66519ms step_avg:61.03ms
step:1091/2245 train_time:66582ms step_avg:61.03ms
step:1092/2245 train_time:66642ms step_avg:61.03ms
step:1093/2245 train_time:66704ms step_avg:61.03ms
step:1094/2245 train_time:66764ms step_avg:61.03ms
step:1095/2245 train_time:66826ms step_avg:61.03ms
step:1096/2245 train_time:66887ms step_avg:61.03ms
step:1097/2245 train_time:66950ms step_avg:61.03ms
step:1098/2245 train_time:67010ms step_avg:61.03ms
step:1099/2245 train_time:67072ms step_avg:61.03ms
step:1100/2245 train_time:67133ms step_avg:61.03ms
step:1101/2245 train_time:67195ms step_avg:61.03ms
step:1102/2245 train_time:67255ms step_avg:61.03ms
step:1103/2245 train_time:67317ms step_avg:61.03ms
step:1104/2245 train_time:67377ms step_avg:61.03ms
step:1105/2245 train_time:67440ms step_avg:61.03ms
step:1106/2245 train_time:67499ms step_avg:61.03ms
step:1107/2245 train_time:67562ms step_avg:61.03ms
step:1108/2245 train_time:67622ms step_avg:61.03ms
step:1109/2245 train_time:67685ms step_avg:61.03ms
step:1110/2245 train_time:67745ms step_avg:61.03ms
step:1111/2245 train_time:67807ms step_avg:61.03ms
step:1112/2245 train_time:67867ms step_avg:61.03ms
step:1113/2245 train_time:67930ms step_avg:61.03ms
step:1114/2245 train_time:67990ms step_avg:61.03ms
step:1115/2245 train_time:68052ms step_avg:61.03ms
step:1116/2245 train_time:68112ms step_avg:61.03ms
step:1117/2245 train_time:68174ms step_avg:61.03ms
step:1118/2245 train_time:68234ms step_avg:61.03ms
step:1119/2245 train_time:68296ms step_avg:61.03ms
step:1120/2245 train_time:68356ms step_avg:61.03ms
step:1121/2245 train_time:68418ms step_avg:61.03ms
step:1122/2245 train_time:68478ms step_avg:61.03ms
step:1123/2245 train_time:68540ms step_avg:61.03ms
step:1124/2245 train_time:68600ms step_avg:61.03ms
step:1125/2245 train_time:68662ms step_avg:61.03ms
step:1126/2245 train_time:68723ms step_avg:61.03ms
step:1127/2245 train_time:68786ms step_avg:61.03ms
step:1128/2245 train_time:68846ms step_avg:61.03ms
step:1129/2245 train_time:68910ms step_avg:61.04ms
step:1130/2245 train_time:68969ms step_avg:61.03ms
step:1131/2245 train_time:69032ms step_avg:61.04ms
step:1132/2245 train_time:69091ms step_avg:61.03ms
step:1133/2245 train_time:69154ms step_avg:61.04ms
step:1134/2245 train_time:69214ms step_avg:61.04ms
step:1135/2245 train_time:69276ms step_avg:61.04ms
step:1136/2245 train_time:69336ms step_avg:61.04ms
step:1137/2245 train_time:69398ms step_avg:61.04ms
step:1138/2245 train_time:69457ms step_avg:61.03ms
step:1139/2245 train_time:69519ms step_avg:61.04ms
step:1140/2245 train_time:69579ms step_avg:61.03ms
step:1141/2245 train_time:69642ms step_avg:61.04ms
step:1142/2245 train_time:69702ms step_avg:61.04ms
step:1143/2245 train_time:69765ms step_avg:61.04ms
step:1144/2245 train_time:69826ms step_avg:61.04ms
step:1145/2245 train_time:69890ms step_avg:61.04ms
step:1146/2245 train_time:69949ms step_avg:61.04ms
step:1147/2245 train_time:70012ms step_avg:61.04ms
step:1148/2245 train_time:70071ms step_avg:61.04ms
step:1149/2245 train_time:70134ms step_avg:61.04ms
step:1150/2245 train_time:70194ms step_avg:61.04ms
step:1151/2245 train_time:70256ms step_avg:61.04ms
step:1152/2245 train_time:70316ms step_avg:61.04ms
step:1153/2245 train_time:70378ms step_avg:61.04ms
step:1154/2245 train_time:70437ms step_avg:61.04ms
step:1155/2245 train_time:70500ms step_avg:61.04ms
step:1156/2245 train_time:70560ms step_avg:61.04ms
step:1157/2245 train_time:70622ms step_avg:61.04ms
step:1158/2245 train_time:70682ms step_avg:61.04ms
step:1159/2245 train_time:70744ms step_avg:61.04ms
step:1160/2245 train_time:70804ms step_avg:61.04ms
step:1161/2245 train_time:70868ms step_avg:61.04ms
step:1162/2245 train_time:70928ms step_avg:61.04ms
step:1163/2245 train_time:70990ms step_avg:61.04ms
step:1164/2245 train_time:71050ms step_avg:61.04ms
step:1165/2245 train_time:71112ms step_avg:61.04ms
step:1166/2245 train_time:71172ms step_avg:61.04ms
step:1167/2245 train_time:71235ms step_avg:61.04ms
step:1168/2245 train_time:71295ms step_avg:61.04ms
step:1169/2245 train_time:71357ms step_avg:61.04ms
step:1170/2245 train_time:71417ms step_avg:61.04ms
step:1171/2245 train_time:71480ms step_avg:61.04ms
step:1172/2245 train_time:71540ms step_avg:61.04ms
step:1173/2245 train_time:71602ms step_avg:61.04ms
step:1174/2245 train_time:71662ms step_avg:61.04ms
step:1175/2245 train_time:71725ms step_avg:61.04ms
step:1176/2245 train_time:71786ms step_avg:61.04ms
step:1177/2245 train_time:71850ms step_avg:61.04ms
step:1178/2245 train_time:71910ms step_avg:61.04ms
step:1179/2245 train_time:71972ms step_avg:61.04ms
step:1180/2245 train_time:72032ms step_avg:61.04ms
step:1181/2245 train_time:72095ms step_avg:61.05ms
step:1182/2245 train_time:72155ms step_avg:61.04ms
step:1183/2245 train_time:72216ms step_avg:61.05ms
step:1184/2245 train_time:72276ms step_avg:61.04ms
step:1185/2245 train_time:72339ms step_avg:61.05ms
step:1186/2245 train_time:72399ms step_avg:61.04ms
step:1187/2245 train_time:72461ms step_avg:61.05ms
step:1188/2245 train_time:72521ms step_avg:61.04ms
step:1189/2245 train_time:72583ms step_avg:61.05ms
step:1190/2245 train_time:72642ms step_avg:61.04ms
step:1191/2245 train_time:72705ms step_avg:61.05ms
step:1192/2245 train_time:72764ms step_avg:61.04ms
step:1193/2245 train_time:72828ms step_avg:61.05ms
step:1194/2245 train_time:72887ms step_avg:61.04ms
step:1195/2245 train_time:72950ms step_avg:61.05ms
step:1196/2245 train_time:73009ms step_avg:61.04ms
step:1197/2245 train_time:73073ms step_avg:61.05ms
step:1198/2245 train_time:73133ms step_avg:61.05ms
step:1199/2245 train_time:73195ms step_avg:61.05ms
step:1200/2245 train_time:73254ms step_avg:61.05ms
step:1201/2245 train_time:73316ms step_avg:61.05ms
step:1202/2245 train_time:73377ms step_avg:61.05ms
step:1203/2245 train_time:73438ms step_avg:61.05ms
step:1204/2245 train_time:73499ms step_avg:61.05ms
step:1205/2245 train_time:73561ms step_avg:61.05ms
step:1206/2245 train_time:73620ms step_avg:61.05ms
step:1207/2245 train_time:73683ms step_avg:61.05ms
step:1208/2245 train_time:73743ms step_avg:61.05ms
step:1209/2245 train_time:73806ms step_avg:61.05ms
step:1210/2245 train_time:73867ms step_avg:61.05ms
step:1211/2245 train_time:73930ms step_avg:61.05ms
step:1212/2245 train_time:73989ms step_avg:61.05ms
step:1213/2245 train_time:74052ms step_avg:61.05ms
step:1214/2245 train_time:74112ms step_avg:61.05ms
step:1215/2245 train_time:74175ms step_avg:61.05ms
step:1216/2245 train_time:74235ms step_avg:61.05ms
step:1217/2245 train_time:74297ms step_avg:61.05ms
step:1218/2245 train_time:74357ms step_avg:61.05ms
step:1219/2245 train_time:74419ms step_avg:61.05ms
step:1220/2245 train_time:74479ms step_avg:61.05ms
step:1221/2245 train_time:74542ms step_avg:61.05ms
step:1222/2245 train_time:74601ms step_avg:61.05ms
step:1223/2245 train_time:74664ms step_avg:61.05ms
step:1224/2245 train_time:74725ms step_avg:61.05ms
step:1225/2245 train_time:74788ms step_avg:61.05ms
step:1226/2245 train_time:74849ms step_avg:61.05ms
step:1227/2245 train_time:74911ms step_avg:61.05ms
step:1228/2245 train_time:74971ms step_avg:61.05ms
step:1229/2245 train_time:75035ms step_avg:61.05ms
step:1230/2245 train_time:75094ms step_avg:61.05ms
step:1231/2245 train_time:75156ms step_avg:61.05ms
step:1232/2245 train_time:75216ms step_avg:61.05ms
step:1233/2245 train_time:75278ms step_avg:61.05ms
step:1234/2245 train_time:75338ms step_avg:61.05ms
step:1235/2245 train_time:75400ms step_avg:61.05ms
step:1236/2245 train_time:75460ms step_avg:61.05ms
step:1237/2245 train_time:75522ms step_avg:61.05ms
step:1238/2245 train_time:75582ms step_avg:61.05ms
step:1239/2245 train_time:75645ms step_avg:61.05ms
step:1240/2245 train_time:75704ms step_avg:61.05ms
step:1241/2245 train_time:75767ms step_avg:61.05ms
step:1242/2245 train_time:75828ms step_avg:61.05ms
step:1243/2245 train_time:75891ms step_avg:61.05ms
step:1244/2245 train_time:75950ms step_avg:61.05ms
step:1245/2245 train_time:76012ms step_avg:61.05ms
step:1246/2245 train_time:76072ms step_avg:61.05ms
step:1247/2245 train_time:76135ms step_avg:61.05ms
step:1248/2245 train_time:76195ms step_avg:61.05ms
step:1249/2245 train_time:76259ms step_avg:61.06ms
step:1250/2245 train_time:76317ms step_avg:61.05ms
step:1250/2245 val_loss:3.5199 train_time:76380ms step_avg:61.10ms
step:1251/2245 train_time:76399ms step_avg:61.07ms
step:1252/2245 train_time:76442ms step_avg:61.06ms
step:1253/2245 train_time:76506ms step_avg:61.06ms
step:1254/2245 train_time:76567ms step_avg:61.06ms
step:1255/2245 train_time:76630ms step_avg:61.06ms
step:1256/2245 train_time:76689ms step_avg:61.06ms
step:1257/2245 train_time:76751ms step_avg:61.06ms
step:1258/2245 train_time:76810ms step_avg:61.06ms
step:1259/2245 train_time:76873ms step_avg:61.06ms
step:1260/2245 train_time:76932ms step_avg:61.06ms
step:1261/2245 train_time:76994ms step_avg:61.06ms
step:1262/2245 train_time:77054ms step_avg:61.06ms
step:1263/2245 train_time:77117ms step_avg:61.06ms
step:1264/2245 train_time:77178ms step_avg:61.06ms
step:1265/2245 train_time:77239ms step_avg:61.06ms
step:1266/2245 train_time:77299ms step_avg:61.06ms
step:1267/2245 train_time:77363ms step_avg:61.06ms
step:1268/2245 train_time:77424ms step_avg:61.06ms
step:1269/2245 train_time:77487ms step_avg:61.06ms
step:1270/2245 train_time:77547ms step_avg:61.06ms
step:1271/2245 train_time:77610ms step_avg:61.06ms
step:1272/2245 train_time:77670ms step_avg:61.06ms
step:1273/2245 train_time:77732ms step_avg:61.06ms
step:1274/2245 train_time:77791ms step_avg:61.06ms
step:1275/2245 train_time:77853ms step_avg:61.06ms
step:1276/2245 train_time:77913ms step_avg:61.06ms
step:1277/2245 train_time:77975ms step_avg:61.06ms
step:1278/2245 train_time:78034ms step_avg:61.06ms
step:1279/2245 train_time:78097ms step_avg:61.06ms
step:1280/2245 train_time:78157ms step_avg:61.06ms
step:1281/2245 train_time:78220ms step_avg:61.06ms
step:1282/2245 train_time:78280ms step_avg:61.06ms
step:1283/2245 train_time:78343ms step_avg:61.06ms
step:1284/2245 train_time:78404ms step_avg:61.06ms
step:1285/2245 train_time:78466ms step_avg:61.06ms
step:1286/2245 train_time:78526ms step_avg:61.06ms
step:1287/2245 train_time:78589ms step_avg:61.06ms
step:1288/2245 train_time:78649ms step_avg:61.06ms
step:1289/2245 train_time:78711ms step_avg:61.06ms
step:1290/2245 train_time:78771ms step_avg:61.06ms
step:1291/2245 train_time:78833ms step_avg:61.06ms
step:1292/2245 train_time:78893ms step_avg:61.06ms
step:1293/2245 train_time:78955ms step_avg:61.06ms
step:1294/2245 train_time:79015ms step_avg:61.06ms
step:1295/2245 train_time:79078ms step_avg:61.06ms
step:1296/2245 train_time:79138ms step_avg:61.06ms
step:1297/2245 train_time:79200ms step_avg:61.06ms
step:1298/2245 train_time:79260ms step_avg:61.06ms
step:1299/2245 train_time:79323ms step_avg:61.06ms
step:1300/2245 train_time:79383ms step_avg:61.06ms
step:1301/2245 train_time:79445ms step_avg:61.06ms
step:1302/2245 train_time:79505ms step_avg:61.06ms
step:1303/2245 train_time:79568ms step_avg:61.07ms
step:1304/2245 train_time:79627ms step_avg:61.06ms
step:1305/2245 train_time:79690ms step_avg:61.06ms
step:1306/2245 train_time:79749ms step_avg:61.06ms
step:1307/2245 train_time:79812ms step_avg:61.06ms
step:1308/2245 train_time:79872ms step_avg:61.06ms
step:1309/2245 train_time:79934ms step_avg:61.06ms
step:1310/2245 train_time:79994ms step_avg:61.06ms
step:1311/2245 train_time:80056ms step_avg:61.06ms
step:1312/2245 train_time:80117ms step_avg:61.06ms
step:1313/2245 train_time:80180ms step_avg:61.07ms
step:1314/2245 train_time:80240ms step_avg:61.07ms
step:1315/2245 train_time:80302ms step_avg:61.07ms
step:1316/2245 train_time:80363ms step_avg:61.07ms
step:1317/2245 train_time:80425ms step_avg:61.07ms
step:1318/2245 train_time:80485ms step_avg:61.07ms
step:1319/2245 train_time:80547ms step_avg:61.07ms
step:1320/2245 train_time:80607ms step_avg:61.07ms
step:1321/2245 train_time:80670ms step_avg:61.07ms
step:1322/2245 train_time:80729ms step_avg:61.07ms
step:1323/2245 train_time:80792ms step_avg:61.07ms
step:1324/2245 train_time:80852ms step_avg:61.07ms
step:1325/2245 train_time:80914ms step_avg:61.07ms
step:1326/2245 train_time:80974ms step_avg:61.07ms
step:1327/2245 train_time:81036ms step_avg:61.07ms
step:1328/2245 train_time:81096ms step_avg:61.07ms
step:1329/2245 train_time:81158ms step_avg:61.07ms
step:1330/2245 train_time:81218ms step_avg:61.07ms
step:1331/2245 train_time:81281ms step_avg:61.07ms
step:1332/2245 train_time:81342ms step_avg:61.07ms
step:1333/2245 train_time:81405ms step_avg:61.07ms
step:1334/2245 train_time:81465ms step_avg:61.07ms
step:1335/2245 train_time:81527ms step_avg:61.07ms
step:1336/2245 train_time:81586ms step_avg:61.07ms
step:1337/2245 train_time:81648ms step_avg:61.07ms
step:1338/2245 train_time:81708ms step_avg:61.07ms
step:1339/2245 train_time:81770ms step_avg:61.07ms
step:1340/2245 train_time:81831ms step_avg:61.07ms
step:1341/2245 train_time:81893ms step_avg:61.07ms
step:1342/2245 train_time:81953ms step_avg:61.07ms
step:1343/2245 train_time:82015ms step_avg:61.07ms
step:1344/2245 train_time:82076ms step_avg:61.07ms
step:1345/2245 train_time:82138ms step_avg:61.07ms
step:1346/2245 train_time:82199ms step_avg:61.07ms
step:1347/2245 train_time:82262ms step_avg:61.07ms
step:1348/2245 train_time:82321ms step_avg:61.07ms
step:1349/2245 train_time:82384ms step_avg:61.07ms
step:1350/2245 train_time:82444ms step_avg:61.07ms
step:1351/2245 train_time:82506ms step_avg:61.07ms
step:1352/2245 train_time:82566ms step_avg:61.07ms
step:1353/2245 train_time:82628ms step_avg:61.07ms
step:1354/2245 train_time:82688ms step_avg:61.07ms
step:1355/2245 train_time:82750ms step_avg:61.07ms
step:1356/2245 train_time:82810ms step_avg:61.07ms
step:1357/2245 train_time:82874ms step_avg:61.07ms
step:1358/2245 train_time:82933ms step_avg:61.07ms
step:1359/2245 train_time:82995ms step_avg:61.07ms
step:1360/2245 train_time:83056ms step_avg:61.07ms
step:1361/2245 train_time:83119ms step_avg:61.07ms
step:1362/2245 train_time:83179ms step_avg:61.07ms
step:1363/2245 train_time:83241ms step_avg:61.07ms
step:1364/2245 train_time:83301ms step_avg:61.07ms
step:1365/2245 train_time:83364ms step_avg:61.07ms
step:1366/2245 train_time:83424ms step_avg:61.07ms
step:1367/2245 train_time:83486ms step_avg:61.07ms
step:1368/2245 train_time:83546ms step_avg:61.07ms
step:1369/2245 train_time:83608ms step_avg:61.07ms
step:1370/2245 train_time:83668ms step_avg:61.07ms
step:1371/2245 train_time:83730ms step_avg:61.07ms
step:1372/2245 train_time:83790ms step_avg:61.07ms
step:1373/2245 train_time:83852ms step_avg:61.07ms
step:1374/2245 train_time:83912ms step_avg:61.07ms
step:1375/2245 train_time:83974ms step_avg:61.07ms
step:1376/2245 train_time:84035ms step_avg:61.07ms
step:1377/2245 train_time:84099ms step_avg:61.07ms
step:1378/2245 train_time:84161ms step_avg:61.07ms
step:1379/2245 train_time:84223ms step_avg:61.08ms
step:1380/2245 train_time:84284ms step_avg:61.08ms
step:1381/2245 train_time:84346ms step_avg:61.08ms
step:1382/2245 train_time:84407ms step_avg:61.08ms
step:1383/2245 train_time:84469ms step_avg:61.08ms
step:1384/2245 train_time:84529ms step_avg:61.08ms
step:1385/2245 train_time:84591ms step_avg:61.08ms
step:1386/2245 train_time:84651ms step_avg:61.08ms
step:1387/2245 train_time:84714ms step_avg:61.08ms
step:1388/2245 train_time:84774ms step_avg:61.08ms
step:1389/2245 train_time:84836ms step_avg:61.08ms
step:1390/2245 train_time:84896ms step_avg:61.08ms
step:1391/2245 train_time:84959ms step_avg:61.08ms
step:1392/2245 train_time:85019ms step_avg:61.08ms
step:1393/2245 train_time:85082ms step_avg:61.08ms
step:1394/2245 train_time:85141ms step_avg:61.08ms
step:1395/2245 train_time:85204ms step_avg:61.08ms
step:1396/2245 train_time:85263ms step_avg:61.08ms
step:1397/2245 train_time:85325ms step_avg:61.08ms
step:1398/2245 train_time:85385ms step_avg:61.08ms
step:1399/2245 train_time:85447ms step_avg:61.08ms
step:1400/2245 train_time:85507ms step_avg:61.08ms
step:1401/2245 train_time:85569ms step_avg:61.08ms
step:1402/2245 train_time:85629ms step_avg:61.08ms
step:1403/2245 train_time:85692ms step_avg:61.08ms
step:1404/2245 train_time:85752ms step_avg:61.08ms
step:1405/2245 train_time:85814ms step_avg:61.08ms
step:1406/2245 train_time:85875ms step_avg:61.08ms
step:1407/2245 train_time:85937ms step_avg:61.08ms
step:1408/2245 train_time:85998ms step_avg:61.08ms
step:1409/2245 train_time:86060ms step_avg:61.08ms
step:1410/2245 train_time:86120ms step_avg:61.08ms
step:1411/2245 train_time:86183ms step_avg:61.08ms
step:1412/2245 train_time:86243ms step_avg:61.08ms
step:1413/2245 train_time:86305ms step_avg:61.08ms
step:1414/2245 train_time:86365ms step_avg:61.08ms
step:1415/2245 train_time:86427ms step_avg:61.08ms
step:1416/2245 train_time:86487ms step_avg:61.08ms
step:1417/2245 train_time:86549ms step_avg:61.08ms
step:1418/2245 train_time:86608ms step_avg:61.08ms
step:1419/2245 train_time:86671ms step_avg:61.08ms
step:1420/2245 train_time:86731ms step_avg:61.08ms
step:1421/2245 train_time:86794ms step_avg:61.08ms
step:1422/2245 train_time:86854ms step_avg:61.08ms
step:1423/2245 train_time:86916ms step_avg:61.08ms
step:1424/2245 train_time:86976ms step_avg:61.08ms
step:1425/2245 train_time:87039ms step_avg:61.08ms
step:1426/2245 train_time:87099ms step_avg:61.08ms
step:1427/2245 train_time:87161ms step_avg:61.08ms
step:1428/2245 train_time:87221ms step_avg:61.08ms
step:1429/2245 train_time:87284ms step_avg:61.08ms
step:1430/2245 train_time:87344ms step_avg:61.08ms
step:1431/2245 train_time:87406ms step_avg:61.08ms
step:1432/2245 train_time:87465ms step_avg:61.08ms
step:1433/2245 train_time:87527ms step_avg:61.08ms
step:1434/2245 train_time:87587ms step_avg:61.08ms
step:1435/2245 train_time:87650ms step_avg:61.08ms
step:1436/2245 train_time:87709ms step_avg:61.08ms
step:1437/2245 train_time:87772ms step_avg:61.08ms
step:1438/2245 train_time:87832ms step_avg:61.08ms
step:1439/2245 train_time:87895ms step_avg:61.08ms
step:1440/2245 train_time:87955ms step_avg:61.08ms
step:1441/2245 train_time:88018ms step_avg:61.08ms
step:1442/2245 train_time:88079ms step_avg:61.08ms
step:1443/2245 train_time:88141ms step_avg:61.08ms
step:1444/2245 train_time:88201ms step_avg:61.08ms
step:1445/2245 train_time:88264ms step_avg:61.08ms
step:1446/2245 train_time:88324ms step_avg:61.08ms
step:1447/2245 train_time:88387ms step_avg:61.08ms
step:1448/2245 train_time:88446ms step_avg:61.08ms
step:1449/2245 train_time:88507ms step_avg:61.08ms
step:1450/2245 train_time:88568ms step_avg:61.08ms
step:1451/2245 train_time:88630ms step_avg:61.08ms
step:1452/2245 train_time:88690ms step_avg:61.08ms
step:1453/2245 train_time:88753ms step_avg:61.08ms
step:1454/2245 train_time:88813ms step_avg:61.08ms
step:1455/2245 train_time:88875ms step_avg:61.08ms
step:1456/2245 train_time:88935ms step_avg:61.08ms
step:1457/2245 train_time:88998ms step_avg:61.08ms
step:1458/2245 train_time:89059ms step_avg:61.08ms
step:1459/2245 train_time:89122ms step_avg:61.08ms
step:1460/2245 train_time:89182ms step_avg:61.08ms
step:1461/2245 train_time:89244ms step_avg:61.08ms
step:1462/2245 train_time:89304ms step_avg:61.08ms
step:1463/2245 train_time:89366ms step_avg:61.08ms
step:1464/2245 train_time:89426ms step_avg:61.08ms
step:1465/2245 train_time:89488ms step_avg:61.08ms
step:1466/2245 train_time:89549ms step_avg:61.08ms
step:1467/2245 train_time:89611ms step_avg:61.08ms
step:1468/2245 train_time:89671ms step_avg:61.08ms
step:1469/2245 train_time:89733ms step_avg:61.08ms
step:1470/2245 train_time:89793ms step_avg:61.08ms
step:1471/2245 train_time:89855ms step_avg:61.08ms
step:1472/2245 train_time:89916ms step_avg:61.08ms
step:1473/2245 train_time:89980ms step_avg:61.09ms
step:1474/2245 train_time:90041ms step_avg:61.09ms
step:1475/2245 train_time:90103ms step_avg:61.09ms
step:1476/2245 train_time:90165ms step_avg:61.09ms
step:1477/2245 train_time:90227ms step_avg:61.09ms
step:1478/2245 train_time:90291ms step_avg:61.09ms
step:1479/2245 train_time:90349ms step_avg:61.09ms
step:1480/2245 train_time:90410ms step_avg:61.09ms
step:1481/2245 train_time:90473ms step_avg:61.09ms
step:1482/2245 train_time:90533ms step_avg:61.09ms
step:1483/2245 train_time:90596ms step_avg:61.09ms
step:1484/2245 train_time:90657ms step_avg:61.09ms
step:1485/2245 train_time:90720ms step_avg:61.09ms
step:1486/2245 train_time:90779ms step_avg:61.09ms
step:1487/2245 train_time:90842ms step_avg:61.09ms
step:1488/2245 train_time:90902ms step_avg:61.09ms
step:1489/2245 train_time:90966ms step_avg:61.09ms
step:1490/2245 train_time:91026ms step_avg:61.09ms
step:1491/2245 train_time:91089ms step_avg:61.09ms
step:1492/2245 train_time:91150ms step_avg:61.09ms
step:1493/2245 train_time:91214ms step_avg:61.09ms
step:1494/2245 train_time:91274ms step_avg:61.09ms
step:1495/2245 train_time:91338ms step_avg:61.10ms
step:1496/2245 train_time:91400ms step_avg:61.10ms
step:1497/2245 train_time:91460ms step_avg:61.10ms
step:1498/2245 train_time:91521ms step_avg:61.10ms
step:1499/2245 train_time:91583ms step_avg:61.10ms
step:1500/2245 train_time:91643ms step_avg:61.10ms
step:1500/2245 val_loss:3.4412 train_time:91707ms step_avg:61.14ms
step:1501/2245 train_time:91726ms step_avg:61.11ms
step:1502/2245 train_time:91768ms step_avg:61.10ms
step:1503/2245 train_time:91830ms step_avg:61.10ms
step:1504/2245 train_time:91892ms step_avg:61.10ms
step:1505/2245 train_time:91955ms step_avg:61.10ms
step:1506/2245 train_time:92015ms step_avg:61.10ms
step:1507/2245 train_time:92077ms step_avg:61.10ms
step:1508/2245 train_time:92138ms step_avg:61.10ms
step:1509/2245 train_time:92200ms step_avg:61.10ms
step:1510/2245 train_time:92259ms step_avg:61.10ms
step:1511/2245 train_time:92322ms step_avg:61.10ms
step:1512/2245 train_time:92382ms step_avg:61.10ms
step:1513/2245 train_time:92447ms step_avg:61.10ms
step:1514/2245 train_time:92508ms step_avg:61.10ms
step:1515/2245 train_time:92570ms step_avg:61.10ms
step:1516/2245 train_time:92631ms step_avg:61.10ms
step:1517/2245 train_time:92695ms step_avg:61.10ms
step:1518/2245 train_time:92756ms step_avg:61.10ms
step:1519/2245 train_time:92821ms step_avg:61.11ms
step:1520/2245 train_time:92883ms step_avg:61.11ms
step:1521/2245 train_time:92946ms step_avg:61.11ms
step:1522/2245 train_time:93006ms step_avg:61.11ms
step:1523/2245 train_time:93069ms step_avg:61.11ms
step:1524/2245 train_time:93129ms step_avg:61.11ms
step:1525/2245 train_time:93191ms step_avg:61.11ms
step:1526/2245 train_time:93250ms step_avg:61.11ms
step:1527/2245 train_time:93312ms step_avg:61.11ms
step:1528/2245 train_time:93373ms step_avg:61.11ms
step:1529/2245 train_time:93436ms step_avg:61.11ms
step:1530/2245 train_time:93496ms step_avg:61.11ms
step:1531/2245 train_time:93559ms step_avg:61.11ms
step:1532/2245 train_time:93621ms step_avg:61.11ms
step:1533/2245 train_time:93685ms step_avg:61.11ms
step:1534/2245 train_time:93747ms step_avg:61.11ms
step:1535/2245 train_time:93810ms step_avg:61.11ms
step:1536/2245 train_time:93870ms step_avg:61.11ms
step:1537/2245 train_time:93933ms step_avg:61.11ms
step:1538/2245 train_time:93993ms step_avg:61.11ms
step:1539/2245 train_time:94056ms step_avg:61.12ms
step:1540/2245 train_time:94117ms step_avg:61.11ms
step:1541/2245 train_time:94179ms step_avg:61.12ms
step:1542/2245 train_time:94239ms step_avg:61.11ms
step:1543/2245 train_time:94301ms step_avg:61.12ms
step:1544/2245 train_time:94361ms step_avg:61.11ms
step:1545/2245 train_time:94424ms step_avg:61.12ms
step:1546/2245 train_time:94486ms step_avg:61.12ms
step:1547/2245 train_time:94548ms step_avg:61.12ms
step:1548/2245 train_time:94610ms step_avg:61.12ms
step:1549/2245 train_time:94673ms step_avg:61.12ms
step:1550/2245 train_time:94733ms step_avg:61.12ms
step:1551/2245 train_time:94797ms step_avg:61.12ms
step:1552/2245 train_time:94858ms step_avg:61.12ms
step:1553/2245 train_time:94921ms step_avg:61.12ms
step:1554/2245 train_time:94983ms step_avg:61.12ms
step:1555/2245 train_time:95046ms step_avg:61.12ms
step:1556/2245 train_time:95106ms step_avg:61.12ms
step:1557/2245 train_time:95169ms step_avg:61.12ms
step:1558/2245 train_time:95229ms step_avg:61.12ms
step:1559/2245 train_time:95291ms step_avg:61.12ms
step:1560/2245 train_time:95350ms step_avg:61.12ms
step:1561/2245 train_time:95413ms step_avg:61.12ms
step:1562/2245 train_time:95474ms step_avg:61.12ms
step:1563/2245 train_time:95537ms step_avg:61.12ms
step:1564/2245 train_time:95598ms step_avg:61.12ms
step:1565/2245 train_time:95661ms step_avg:61.13ms
step:1566/2245 train_time:95722ms step_avg:61.13ms
step:1567/2245 train_time:95786ms step_avg:61.13ms
step:1568/2245 train_time:95846ms step_avg:61.13ms
step:1569/2245 train_time:95909ms step_avg:61.13ms
step:1570/2245 train_time:95969ms step_avg:61.13ms
step:1571/2245 train_time:96032ms step_avg:61.13ms
step:1572/2245 train_time:96093ms step_avg:61.13ms
step:1573/2245 train_time:96155ms step_avg:61.13ms
step:1574/2245 train_time:96215ms step_avg:61.13ms
step:1575/2245 train_time:96278ms step_avg:61.13ms
step:1576/2245 train_time:96338ms step_avg:61.13ms
step:1577/2245 train_time:96400ms step_avg:61.13ms
step:1578/2245 train_time:96461ms step_avg:61.13ms
step:1579/2245 train_time:96525ms step_avg:61.13ms
step:1580/2245 train_time:96586ms step_avg:61.13ms
step:1581/2245 train_time:96649ms step_avg:61.13ms
step:1582/2245 train_time:96709ms step_avg:61.13ms
step:1583/2245 train_time:96772ms step_avg:61.13ms
step:1584/2245 train_time:96833ms step_avg:61.13ms
step:1585/2245 train_time:96895ms step_avg:61.13ms
step:1586/2245 train_time:96955ms step_avg:61.13ms
step:1587/2245 train_time:97018ms step_avg:61.13ms
step:1588/2245 train_time:97078ms step_avg:61.13ms
step:1589/2245 train_time:97140ms step_avg:61.13ms
step:1590/2245 train_time:97201ms step_avg:61.13ms
step:1591/2245 train_time:97265ms step_avg:61.13ms
step:1592/2245 train_time:97325ms step_avg:61.13ms
step:1593/2245 train_time:97388ms step_avg:61.14ms
step:1594/2245 train_time:97449ms step_avg:61.13ms
step:1595/2245 train_time:97511ms step_avg:61.14ms
step:1596/2245 train_time:97572ms step_avg:61.14ms
step:1597/2245 train_time:97634ms step_avg:61.14ms
step:1598/2245 train_time:97695ms step_avg:61.14ms
step:1599/2245 train_time:97758ms step_avg:61.14ms
step:1600/2245 train_time:97818ms step_avg:61.14ms
step:1601/2245 train_time:97882ms step_avg:61.14ms
step:1602/2245 train_time:97943ms step_avg:61.14ms
step:1603/2245 train_time:98006ms step_avg:61.14ms
step:1604/2245 train_time:98067ms step_avg:61.14ms
step:1605/2245 train_time:98129ms step_avg:61.14ms
step:1606/2245 train_time:98189ms step_avg:61.14ms
step:1607/2245 train_time:98251ms step_avg:61.14ms
step:1608/2245 train_time:98311ms step_avg:61.14ms
step:1609/2245 train_time:98373ms step_avg:61.14ms
step:1610/2245 train_time:98434ms step_avg:61.14ms
step:1611/2245 train_time:98496ms step_avg:61.14ms
step:1612/2245 train_time:98556ms step_avg:61.14ms
step:1613/2245 train_time:98620ms step_avg:61.14ms
step:1614/2245 train_time:98681ms step_avg:61.14ms
step:1615/2245 train_time:98744ms step_avg:61.14ms
step:1616/2245 train_time:98805ms step_avg:61.14ms
step:1617/2245 train_time:98867ms step_avg:61.14ms
step:1618/2245 train_time:98929ms step_avg:61.14ms
step:1619/2245 train_time:98990ms step_avg:61.14ms
step:1620/2245 train_time:99051ms step_avg:61.14ms
step:1621/2245 train_time:99114ms step_avg:61.14ms
step:1622/2245 train_time:99175ms step_avg:61.14ms
step:1623/2245 train_time:99238ms step_avg:61.14ms
step:1624/2245 train_time:99298ms step_avg:61.14ms
step:1625/2245 train_time:99361ms step_avg:61.15ms
step:1626/2245 train_time:99421ms step_avg:61.14ms
step:1627/2245 train_time:99484ms step_avg:61.15ms
step:1628/2245 train_time:99545ms step_avg:61.15ms
step:1629/2245 train_time:99608ms step_avg:61.15ms
step:1630/2245 train_time:99668ms step_avg:61.15ms
step:1631/2245 train_time:99731ms step_avg:61.15ms
step:1632/2245 train_time:99791ms step_avg:61.15ms
step:1633/2245 train_time:99854ms step_avg:61.15ms
step:1634/2245 train_time:99914ms step_avg:61.15ms
step:1635/2245 train_time:99976ms step_avg:61.15ms
step:1636/2245 train_time:100037ms step_avg:61.15ms
step:1637/2245 train_time:100100ms step_avg:61.15ms
step:1638/2245 train_time:100162ms step_avg:61.15ms
step:1639/2245 train_time:100225ms step_avg:61.15ms
step:1640/2245 train_time:100286ms step_avg:61.15ms
step:1641/2245 train_time:100349ms step_avg:61.15ms
step:1642/2245 train_time:100409ms step_avg:61.15ms
step:1643/2245 train_time:100472ms step_avg:61.15ms
step:1644/2245 train_time:100532ms step_avg:61.15ms
step:1645/2245 train_time:100595ms step_avg:61.15ms
step:1646/2245 train_time:100655ms step_avg:61.15ms
step:1647/2245 train_time:100718ms step_avg:61.15ms
step:1648/2245 train_time:100778ms step_avg:61.15ms
step:1649/2245 train_time:100842ms step_avg:61.15ms
step:1650/2245 train_time:100903ms step_avg:61.15ms
step:1651/2245 train_time:100966ms step_avg:61.15ms
step:1652/2245 train_time:101026ms step_avg:61.15ms
step:1653/2245 train_time:101089ms step_avg:61.16ms
step:1654/2245 train_time:101150ms step_avg:61.15ms
step:1655/2245 train_time:101212ms step_avg:61.16ms
step:1656/2245 train_time:101272ms step_avg:61.15ms
step:1657/2245 train_time:101335ms step_avg:61.16ms
step:1658/2245 train_time:101396ms step_avg:61.16ms
step:1659/2245 train_time:101459ms step_avg:61.16ms
step:1660/2245 train_time:101520ms step_avg:61.16ms
step:1661/2245 train_time:101583ms step_avg:61.16ms
step:1662/2245 train_time:101644ms step_avg:61.16ms
step:1663/2245 train_time:101706ms step_avg:61.16ms
step:1664/2245 train_time:101766ms step_avg:61.16ms
step:1665/2245 train_time:101829ms step_avg:61.16ms
step:1666/2245 train_time:101889ms step_avg:61.16ms
step:1667/2245 train_time:101952ms step_avg:61.16ms
step:1668/2245 train_time:102012ms step_avg:61.16ms
step:1669/2245 train_time:102075ms step_avg:61.16ms
step:1670/2245 train_time:102135ms step_avg:61.16ms
step:1671/2245 train_time:102198ms step_avg:61.16ms
step:1672/2245 train_time:102259ms step_avg:61.16ms
step:1673/2245 train_time:102322ms step_avg:61.16ms
step:1674/2245 train_time:102383ms step_avg:61.16ms
step:1675/2245 train_time:102445ms step_avg:61.16ms
step:1676/2245 train_time:102506ms step_avg:61.16ms
step:1677/2245 train_time:102569ms step_avg:61.16ms
step:1678/2245 train_time:102628ms step_avg:61.16ms
step:1679/2245 train_time:102691ms step_avg:61.16ms
step:1680/2245 train_time:102751ms step_avg:61.16ms
step:1681/2245 train_time:102814ms step_avg:61.16ms
step:1682/2245 train_time:102874ms step_avg:61.16ms
step:1683/2245 train_time:102937ms step_avg:61.16ms
step:1684/2245 train_time:102997ms step_avg:61.16ms
step:1685/2245 train_time:103060ms step_avg:61.16ms
step:1686/2245 train_time:103121ms step_avg:61.16ms
step:1687/2245 train_time:103185ms step_avg:61.16ms
step:1688/2245 train_time:103245ms step_avg:61.16ms
step:1689/2245 train_time:103308ms step_avg:61.17ms
step:1690/2245 train_time:103369ms step_avg:61.17ms
step:1691/2245 train_time:103432ms step_avg:61.17ms
step:1692/2245 train_time:103493ms step_avg:61.17ms
step:1693/2245 train_time:103556ms step_avg:61.17ms
step:1694/2245 train_time:103616ms step_avg:61.17ms
step:1695/2245 train_time:103679ms step_avg:61.17ms
step:1696/2245 train_time:103740ms step_avg:61.17ms
step:1697/2245 train_time:103803ms step_avg:61.17ms
step:1698/2245 train_time:103864ms step_avg:61.17ms
step:1699/2245 train_time:103928ms step_avg:61.17ms
step:1700/2245 train_time:103988ms step_avg:61.17ms
step:1701/2245 train_time:104050ms step_avg:61.17ms
step:1702/2245 train_time:104111ms step_avg:61.17ms
step:1703/2245 train_time:104174ms step_avg:61.17ms
step:1704/2245 train_time:104235ms step_avg:61.17ms
step:1705/2245 train_time:104298ms step_avg:61.17ms
step:1706/2245 train_time:104358ms step_avg:61.17ms
step:1707/2245 train_time:104422ms step_avg:61.17ms
step:1708/2245 train_time:104483ms step_avg:61.17ms
step:1709/2245 train_time:104547ms step_avg:61.17ms
step:1710/2245 train_time:104607ms step_avg:61.17ms
step:1711/2245 train_time:104670ms step_avg:61.17ms
step:1712/2245 train_time:104730ms step_avg:61.17ms
step:1713/2245 train_time:104792ms step_avg:61.17ms
step:1714/2245 train_time:104853ms step_avg:61.17ms
step:1715/2245 train_time:104916ms step_avg:61.18ms
step:1716/2245 train_time:104977ms step_avg:61.18ms
step:1717/2245 train_time:105040ms step_avg:61.18ms
step:1718/2245 train_time:105100ms step_avg:61.18ms
step:1719/2245 train_time:105163ms step_avg:61.18ms
step:1720/2245 train_time:105224ms step_avg:61.18ms
step:1721/2245 train_time:105287ms step_avg:61.18ms
step:1722/2245 train_time:105348ms step_avg:61.18ms
step:1723/2245 train_time:105411ms step_avg:61.18ms
step:1724/2245 train_time:105471ms step_avg:61.18ms
step:1725/2245 train_time:105534ms step_avg:61.18ms
step:1726/2245 train_time:105595ms step_avg:61.18ms
step:1727/2245 train_time:105658ms step_avg:61.18ms
step:1728/2245 train_time:105720ms step_avg:61.18ms
step:1729/2245 train_time:105783ms step_avg:61.18ms
step:1730/2245 train_time:105843ms step_avg:61.18ms
step:1731/2245 train_time:105906ms step_avg:61.18ms
step:1732/2245 train_time:105967ms step_avg:61.18ms
step:1733/2245 train_time:106029ms step_avg:61.18ms
step:1734/2245 train_time:106090ms step_avg:61.18ms
step:1735/2245 train_time:106152ms step_avg:61.18ms
step:1736/2245 train_time:106213ms step_avg:61.18ms
step:1737/2245 train_time:106275ms step_avg:61.18ms
step:1738/2245 train_time:106336ms step_avg:61.18ms
step:1739/2245 train_time:106399ms step_avg:61.18ms
step:1740/2245 train_time:106460ms step_avg:61.18ms
step:1741/2245 train_time:106523ms step_avg:61.18ms
step:1742/2245 train_time:106584ms step_avg:61.19ms
step:1743/2245 train_time:106648ms step_avg:61.19ms
step:1744/2245 train_time:106708ms step_avg:61.19ms
step:1745/2245 train_time:106771ms step_avg:61.19ms
step:1746/2245 train_time:106831ms step_avg:61.19ms
step:1747/2245 train_time:106894ms step_avg:61.19ms
step:1748/2245 train_time:106955ms step_avg:61.19ms
step:1749/2245 train_time:107018ms step_avg:61.19ms
step:1750/2245 train_time:107080ms step_avg:61.19ms
step:1750/2245 val_loss:3.3771 train_time:107143ms step_avg:61.22ms
step:1751/2245 train_time:107167ms step_avg:61.20ms
step:1752/2245 train_time:107207ms step_avg:61.19ms
step:1753/2245 train_time:107272ms step_avg:61.19ms
step:1754/2245 train_time:107333ms step_avg:61.19ms
step:1755/2245 train_time:107396ms step_avg:61.19ms
step:1756/2245 train_time:107456ms step_avg:61.19ms
step:1757/2245 train_time:107518ms step_avg:61.19ms
step:1758/2245 train_time:107577ms step_avg:61.19ms
step:1759/2245 train_time:107639ms step_avg:61.19ms
step:1760/2245 train_time:107699ms step_avg:61.19ms
step:1761/2245 train_time:107761ms step_avg:61.19ms
step:1762/2245 train_time:107821ms step_avg:61.19ms
step:1763/2245 train_time:107883ms step_avg:61.19ms
step:1764/2245 train_time:107944ms step_avg:61.19ms
step:1765/2245 train_time:108007ms step_avg:61.19ms
step:1766/2245 train_time:108068ms step_avg:61.19ms
step:1767/2245 train_time:108133ms step_avg:61.20ms
step:1768/2245 train_time:108195ms step_avg:61.20ms
step:1769/2245 train_time:108260ms step_avg:61.20ms
step:1770/2245 train_time:108322ms step_avg:61.20ms
step:1771/2245 train_time:108386ms step_avg:61.20ms
step:1772/2245 train_time:108445ms step_avg:61.20ms
step:1773/2245 train_time:108508ms step_avg:61.20ms
step:1774/2245 train_time:108569ms step_avg:61.20ms
step:1775/2245 train_time:108631ms step_avg:61.20ms
step:1776/2245 train_time:108691ms step_avg:61.20ms
step:1777/2245 train_time:108754ms step_avg:61.20ms
step:1778/2245 train_time:108814ms step_avg:61.20ms
step:1779/2245 train_time:108876ms step_avg:61.20ms
step:1780/2245 train_time:108937ms step_avg:61.20ms
step:1781/2245 train_time:109000ms step_avg:61.20ms
step:1782/2245 train_time:109061ms step_avg:61.20ms
step:1783/2245 train_time:109125ms step_avg:61.20ms
step:1784/2245 train_time:109187ms step_avg:61.20ms
step:1785/2245 train_time:109250ms step_avg:61.20ms
step:1786/2245 train_time:109311ms step_avg:61.20ms
step:1787/2245 train_time:109374ms step_avg:61.21ms
step:1788/2245 train_time:109435ms step_avg:61.21ms
step:1789/2245 train_time:109498ms step_avg:61.21ms
step:1790/2245 train_time:109558ms step_avg:61.21ms
step:1791/2245 train_time:109621ms step_avg:61.21ms
step:1792/2245 train_time:109681ms step_avg:61.21ms
step:1793/2245 train_time:109744ms step_avg:61.21ms
step:1794/2245 train_time:109803ms step_avg:61.21ms
step:1795/2245 train_time:109866ms step_avg:61.21ms
step:1796/2245 train_time:109926ms step_avg:61.21ms
step:1797/2245 train_time:109989ms step_avg:61.21ms
step:1798/2245 train_time:110050ms step_avg:61.21ms
step:1799/2245 train_time:110113ms step_avg:61.21ms
step:1800/2245 train_time:110174ms step_avg:61.21ms
step:1801/2245 train_time:110241ms step_avg:61.21ms
step:1802/2245 train_time:110300ms step_avg:61.21ms
step:1803/2245 train_time:110363ms step_avg:61.21ms
step:1804/2245 train_time:110424ms step_avg:61.21ms
step:1805/2245 train_time:110486ms step_avg:61.21ms
step:1806/2245 train_time:110547ms step_avg:61.21ms
step:1807/2245 train_time:110609ms step_avg:61.21ms
step:1808/2245 train_time:110669ms step_avg:61.21ms
step:1809/2245 train_time:110732ms step_avg:61.21ms
step:1810/2245 train_time:110792ms step_avg:61.21ms
step:1811/2245 train_time:110855ms step_avg:61.21ms
step:1812/2245 train_time:110915ms step_avg:61.21ms
step:1813/2245 train_time:110978ms step_avg:61.21ms
step:1814/2245 train_time:111039ms step_avg:61.21ms
step:1815/2245 train_time:111102ms step_avg:61.21ms
step:1816/2245 train_time:111162ms step_avg:61.21ms
step:1817/2245 train_time:111225ms step_avg:61.21ms
step:1818/2245 train_time:111286ms step_avg:61.21ms
step:1819/2245 train_time:111349ms step_avg:61.21ms
step:1820/2245 train_time:111409ms step_avg:61.21ms
step:1821/2245 train_time:111472ms step_avg:61.21ms
step:1822/2245 train_time:111533ms step_avg:61.21ms
step:1823/2245 train_time:111595ms step_avg:61.22ms
step:1824/2245 train_time:111655ms step_avg:61.21ms
step:1825/2245 train_time:111718ms step_avg:61.22ms
step:1826/2245 train_time:111778ms step_avg:61.21ms
step:1827/2245 train_time:111840ms step_avg:61.22ms
step:1828/2245 train_time:111901ms step_avg:61.21ms
step:1829/2245 train_time:111963ms step_avg:61.22ms
step:1830/2245 train_time:112023ms step_avg:61.22ms
step:1831/2245 train_time:112086ms step_avg:61.22ms
step:1832/2245 train_time:112146ms step_avg:61.22ms
step:1833/2245 train_time:112210ms step_avg:61.22ms
step:1834/2245 train_time:112270ms step_avg:61.22ms
step:1835/2245 train_time:112333ms step_avg:61.22ms
step:1836/2245 train_time:112393ms step_avg:61.22ms
step:1837/2245 train_time:112457ms step_avg:61.22ms
step:1838/2245 train_time:112518ms step_avg:61.22ms
step:1839/2245 train_time:112580ms step_avg:61.22ms
step:1840/2245 train_time:112641ms step_avg:61.22ms
step:1841/2245 train_time:112705ms step_avg:61.22ms
step:1842/2245 train_time:112766ms step_avg:61.22ms
step:1843/2245 train_time:112828ms step_avg:61.22ms
step:1844/2245 train_time:112888ms step_avg:61.22ms
step:1845/2245 train_time:112951ms step_avg:61.22ms
step:1846/2245 train_time:113012ms step_avg:61.22ms
step:1847/2245 train_time:113075ms step_avg:61.22ms
step:1848/2245 train_time:113135ms step_avg:61.22ms
step:1849/2245 train_time:113199ms step_avg:61.22ms
step:1850/2245 train_time:113261ms step_avg:61.22ms
step:1851/2245 train_time:113324ms step_avg:61.22ms
step:1852/2245 train_time:113384ms step_avg:61.22ms
step:1853/2245 train_time:113447ms step_avg:61.22ms
step:1854/2245 train_time:113507ms step_avg:61.22ms
step:1855/2245 train_time:113570ms step_avg:61.22ms
step:1856/2245 train_time:113630ms step_avg:61.22ms
step:1857/2245 train_time:113693ms step_avg:61.22ms
step:1858/2245 train_time:113754ms step_avg:61.22ms
step:1859/2245 train_time:113817ms step_avg:61.22ms
step:1860/2245 train_time:113877ms step_avg:61.22ms
step:1861/2245 train_time:113942ms step_avg:61.23ms
step:1862/2245 train_time:114001ms step_avg:61.23ms
step:1863/2245 train_time:114064ms step_avg:61.23ms
step:1864/2245 train_time:114125ms step_avg:61.23ms
step:1865/2245 train_time:114187ms step_avg:61.23ms
step:1866/2245 train_time:114248ms step_avg:61.23ms
step:1867/2245 train_time:114310ms step_avg:61.23ms
step:1868/2245 train_time:114370ms step_avg:61.23ms
step:1869/2245 train_time:114433ms step_avg:61.23ms
step:1870/2245 train_time:114493ms step_avg:61.23ms
step:1871/2245 train_time:114556ms step_avg:61.23ms
step:1872/2245 train_time:114617ms step_avg:61.23ms
step:1873/2245 train_time:114681ms step_avg:61.23ms
step:1874/2245 train_time:114742ms step_avg:61.23ms
step:1875/2245 train_time:114805ms step_avg:61.23ms
step:1876/2245 train_time:114865ms step_avg:61.23ms
step:1877/2245 train_time:114928ms step_avg:61.23ms
step:1878/2245 train_time:114988ms step_avg:61.23ms
step:1879/2245 train_time:115050ms step_avg:61.23ms
step:1880/2245 train_time:115111ms step_avg:61.23ms
step:1881/2245 train_time:115174ms step_avg:61.23ms
step:1882/2245 train_time:115234ms step_avg:61.23ms
step:1883/2245 train_time:115297ms step_avg:61.23ms
step:1884/2245 train_time:115358ms step_avg:61.23ms
step:1885/2245 train_time:115422ms step_avg:61.23ms
step:1886/2245 train_time:115482ms step_avg:61.23ms
step:1887/2245 train_time:115545ms step_avg:61.23ms
step:1888/2245 train_time:115605ms step_avg:61.23ms
step:1889/2245 train_time:115668ms step_avg:61.23ms
step:1890/2245 train_time:115728ms step_avg:61.23ms
step:1891/2245 train_time:115791ms step_avg:61.23ms
step:1892/2245 train_time:115851ms step_avg:61.23ms
step:1893/2245 train_time:115914ms step_avg:61.23ms
step:1894/2245 train_time:115974ms step_avg:61.23ms
step:1895/2245 train_time:116037ms step_avg:61.23ms
step:1896/2245 train_time:116099ms step_avg:61.23ms
step:1897/2245 train_time:116161ms step_avg:61.23ms
step:1898/2245 train_time:116222ms step_avg:61.23ms
step:1899/2245 train_time:116285ms step_avg:61.23ms
step:1900/2245 train_time:116344ms step_avg:61.23ms
step:1901/2245 train_time:116407ms step_avg:61.23ms
step:1902/2245 train_time:116468ms step_avg:61.23ms
step:1903/2245 train_time:116530ms step_avg:61.23ms
step:1904/2245 train_time:116591ms step_avg:61.23ms
step:1905/2245 train_time:116653ms step_avg:61.24ms
step:1906/2245 train_time:116713ms step_avg:61.23ms
step:1907/2245 train_time:116777ms step_avg:61.24ms
step:1908/2245 train_time:116839ms step_avg:61.24ms
step:1909/2245 train_time:116901ms step_avg:61.24ms
step:1910/2245 train_time:116962ms step_avg:61.24ms
step:1911/2245 train_time:117025ms step_avg:61.24ms
step:1912/2245 train_time:117085ms step_avg:61.24ms
step:1913/2245 train_time:117147ms step_avg:61.24ms
step:1914/2245 train_time:117208ms step_avg:61.24ms
step:1915/2245 train_time:117271ms step_avg:61.24ms
step:1916/2245 train_time:117331ms step_avg:61.24ms
step:1917/2245 train_time:117394ms step_avg:61.24ms
step:1918/2245 train_time:117455ms step_avg:61.24ms
step:1919/2245 train_time:117518ms step_avg:61.24ms
step:1920/2245 train_time:117580ms step_avg:61.24ms
step:1921/2245 train_time:117643ms step_avg:61.24ms
step:1922/2245 train_time:117704ms step_avg:61.24ms
step:1923/2245 train_time:117767ms step_avg:61.24ms
step:1924/2245 train_time:117827ms step_avg:61.24ms
step:1925/2245 train_time:117890ms step_avg:61.24ms
step:1926/2245 train_time:117949ms step_avg:61.24ms
step:1927/2245 train_time:118013ms step_avg:61.24ms
step:1928/2245 train_time:118073ms step_avg:61.24ms
step:1929/2245 train_time:118136ms step_avg:61.24ms
step:1930/2245 train_time:118197ms step_avg:61.24ms
step:1931/2245 train_time:118260ms step_avg:61.24ms
step:1932/2245 train_time:118321ms step_avg:61.24ms
step:1933/2245 train_time:118383ms step_avg:61.24ms
step:1934/2245 train_time:118444ms step_avg:61.24ms
step:1935/2245 train_time:118506ms step_avg:61.24ms
step:1936/2245 train_time:118567ms step_avg:61.24ms
step:1937/2245 train_time:118630ms step_avg:61.24ms
step:1938/2245 train_time:118690ms step_avg:61.24ms
step:1939/2245 train_time:118753ms step_avg:61.24ms
step:1940/2245 train_time:118815ms step_avg:61.24ms
step:1941/2245 train_time:118878ms step_avg:61.25ms
step:1942/2245 train_time:118939ms step_avg:61.25ms
step:1943/2245 train_time:119002ms step_avg:61.25ms
step:1944/2245 train_time:119062ms step_avg:61.25ms
step:1945/2245 train_time:119125ms step_avg:61.25ms
step:1946/2245 train_time:119185ms step_avg:61.25ms
step:1947/2245 train_time:119248ms step_avg:61.25ms
step:1948/2245 train_time:119308ms step_avg:61.25ms
step:1949/2245 train_time:119371ms step_avg:61.25ms
step:1950/2245 train_time:119431ms step_avg:61.25ms
step:1951/2245 train_time:119494ms step_avg:61.25ms
step:1952/2245 train_time:119554ms step_avg:61.25ms
step:1953/2245 train_time:119617ms step_avg:61.25ms
step:1954/2245 train_time:119677ms step_avg:61.25ms
step:1955/2245 train_time:119740ms step_avg:61.25ms
step:1956/2245 train_time:119801ms step_avg:61.25ms
step:1957/2245 train_time:119864ms step_avg:61.25ms
step:1958/2245 train_time:119924ms step_avg:61.25ms
step:1959/2245 train_time:119987ms step_avg:61.25ms
step:1960/2245 train_time:120047ms step_avg:61.25ms
step:1961/2245 train_time:120110ms step_avg:61.25ms
step:1962/2245 train_time:120170ms step_avg:61.25ms
step:1963/2245 train_time:120233ms step_avg:61.25ms
step:1964/2245 train_time:120293ms step_avg:61.25ms
step:1965/2245 train_time:120356ms step_avg:61.25ms
step:1966/2245 train_time:120416ms step_avg:61.25ms
step:1967/2245 train_time:120479ms step_avg:61.25ms
step:1968/2245 train_time:120540ms step_avg:61.25ms
step:1969/2245 train_time:120604ms step_avg:61.25ms
step:1970/2245 train_time:120665ms step_avg:61.25ms
step:1971/2245 train_time:120727ms step_avg:61.25ms
step:1972/2245 train_time:120786ms step_avg:61.25ms
step:1973/2245 train_time:120849ms step_avg:61.25ms
step:1974/2245 train_time:120910ms step_avg:61.25ms
step:1975/2245 train_time:120972ms step_avg:61.25ms
step:1976/2245 train_time:121033ms step_avg:61.25ms
step:1977/2245 train_time:121095ms step_avg:61.25ms
step:1978/2245 train_time:121156ms step_avg:61.25ms
step:1979/2245 train_time:121219ms step_avg:61.25ms
step:1980/2245 train_time:121280ms step_avg:61.25ms
step:1981/2245 train_time:121343ms step_avg:61.25ms
step:1982/2245 train_time:121404ms step_avg:61.25ms
step:1983/2245 train_time:121468ms step_avg:61.25ms
step:1984/2245 train_time:121527ms step_avg:61.25ms
step:1985/2245 train_time:121590ms step_avg:61.25ms
step:1986/2245 train_time:121651ms step_avg:61.25ms
step:1987/2245 train_time:121713ms step_avg:61.25ms
step:1988/2245 train_time:121774ms step_avg:61.25ms
step:1989/2245 train_time:121837ms step_avg:61.26ms
step:1990/2245 train_time:121897ms step_avg:61.26ms
step:1991/2245 train_time:121960ms step_avg:61.26ms
step:1992/2245 train_time:122021ms step_avg:61.26ms
step:1993/2245 train_time:122084ms step_avg:61.26ms
step:1994/2245 train_time:122144ms step_avg:61.26ms
step:1995/2245 train_time:122207ms step_avg:61.26ms
step:1996/2245 train_time:122268ms step_avg:61.26ms
step:1997/2245 train_time:122331ms step_avg:61.26ms
step:1998/2245 train_time:122392ms step_avg:61.26ms
step:1999/2245 train_time:122454ms step_avg:61.26ms
step:2000/2245 train_time:122515ms step_avg:61.26ms
step:2000/2245 val_loss:3.3226 train_time:122579ms step_avg:61.29ms
step:2001/2245 train_time:122598ms step_avg:61.27ms
step:2002/2245 train_time:122643ms step_avg:61.26ms
step:2003/2245 train_time:122708ms step_avg:61.26ms
step:2004/2245 train_time:122769ms step_avg:61.26ms
step:2005/2245 train_time:122831ms step_avg:61.26ms
step:2006/2245 train_time:122891ms step_avg:61.26ms
step:2007/2245 train_time:122953ms step_avg:61.26ms
step:2008/2245 train_time:123013ms step_avg:61.26ms
step:2009/2245 train_time:123075ms step_avg:61.26ms
step:2010/2245 train_time:123135ms step_avg:61.26ms
step:2011/2245 train_time:123197ms step_avg:61.26ms
step:2012/2245 train_time:123257ms step_avg:61.26ms
step:2013/2245 train_time:123319ms step_avg:61.26ms
step:2014/2245 train_time:123379ms step_avg:61.26ms
step:2015/2245 train_time:123441ms step_avg:61.26ms
step:2016/2245 train_time:123502ms step_avg:61.26ms
step:2017/2245 train_time:123567ms step_avg:61.26ms
step:2018/2245 train_time:123630ms step_avg:61.26ms
step:2019/2245 train_time:123693ms step_avg:61.26ms
step:2020/2245 train_time:123753ms step_avg:61.26ms
step:2021/2245 train_time:123816ms step_avg:61.26ms
step:2022/2245 train_time:123877ms step_avg:61.26ms
step:2023/2245 train_time:123939ms step_avg:61.27ms
step:2024/2245 train_time:123999ms step_avg:61.26ms
step:2025/2245 train_time:124062ms step_avg:61.27ms
step:2026/2245 train_time:124122ms step_avg:61.26ms
step:2027/2245 train_time:124185ms step_avg:61.27ms
step:2028/2245 train_time:124246ms step_avg:61.27ms
step:2029/2245 train_time:124309ms step_avg:61.27ms
step:2030/2245 train_time:124369ms step_avg:61.27ms
step:2031/2245 train_time:124431ms step_avg:61.27ms
step:2032/2245 train_time:124492ms step_avg:61.27ms
step:2033/2245 train_time:124556ms step_avg:61.27ms
step:2034/2245 train_time:124617ms step_avg:61.27ms
step:2035/2245 train_time:124680ms step_avg:61.27ms
step:2036/2245 train_time:124741ms step_avg:61.27ms
step:2037/2245 train_time:124805ms step_avg:61.27ms
step:2038/2245 train_time:124865ms step_avg:61.27ms
step:2039/2245 train_time:124929ms step_avg:61.27ms
step:2040/2245 train_time:124989ms step_avg:61.27ms
step:2041/2245 train_time:125052ms step_avg:61.27ms
step:2042/2245 train_time:125112ms step_avg:61.27ms
step:2043/2245 train_time:125174ms step_avg:61.27ms
step:2044/2245 train_time:125235ms step_avg:61.27ms
step:2045/2245 train_time:125297ms step_avg:61.27ms
step:2046/2245 train_time:125358ms step_avg:61.27ms
step:2047/2245 train_time:125422ms step_avg:61.27ms
step:2048/2245 train_time:125483ms step_avg:61.27ms
step:2049/2245 train_time:125546ms step_avg:61.27ms
step:2050/2245 train_time:125608ms step_avg:61.27ms
step:2051/2245 train_time:125672ms step_avg:61.27ms
step:2052/2245 train_time:125733ms step_avg:61.27ms
step:2053/2245 train_time:125796ms step_avg:61.27ms
step:2054/2245 train_time:125856ms step_avg:61.27ms
step:2055/2245 train_time:125918ms step_avg:61.27ms
step:2056/2245 train_time:125979ms step_avg:61.27ms
step:2057/2245 train_time:126042ms step_avg:61.27ms
step:2058/2245 train_time:126102ms step_avg:61.27ms
step:2059/2245 train_time:126166ms step_avg:61.28ms
step:2060/2245 train_time:126226ms step_avg:61.27ms
step:2061/2245 train_time:126290ms step_avg:61.28ms
step:2062/2245 train_time:126350ms step_avg:61.28ms
step:2063/2245 train_time:126413ms step_avg:61.28ms
step:2064/2245 train_time:126477ms step_avg:61.28ms
step:2065/2245 train_time:126537ms step_avg:61.28ms
step:2066/2245 train_time:126597ms step_avg:61.28ms
step:2067/2245 train_time:126660ms step_avg:61.28ms
step:2068/2245 train_time:126721ms step_avg:61.28ms
step:2069/2245 train_time:126785ms step_avg:61.28ms
step:2070/2245 train_time:126846ms step_avg:61.28ms
step:2071/2245 train_time:126909ms step_avg:61.28ms
step:2072/2245 train_time:126969ms step_avg:61.28ms
step:2073/2245 train_time:127032ms step_avg:61.28ms
step:2074/2245 train_time:127093ms step_avg:61.28ms
step:2075/2245 train_time:127156ms step_avg:61.28ms
step:2076/2245 train_time:127216ms step_avg:61.28ms
step:2077/2245 train_time:127278ms step_avg:61.28ms
step:2078/2245 train_time:127340ms step_avg:61.28ms
step:2079/2245 train_time:127403ms step_avg:61.28ms
step:2080/2245 train_time:127464ms step_avg:61.28ms
step:2081/2245 train_time:127527ms step_avg:61.28ms
step:2082/2245 train_time:127588ms step_avg:61.28ms
step:2083/2245 train_time:127651ms step_avg:61.28ms
step:2084/2245 train_time:127711ms step_avg:61.28ms
step:2085/2245 train_time:127774ms step_avg:61.28ms
step:2086/2245 train_time:127835ms step_avg:61.28ms
step:2087/2245 train_time:127899ms step_avg:61.28ms
step:2088/2245 train_time:127960ms step_avg:61.28ms
step:2089/2245 train_time:128023ms step_avg:61.28ms
step:2090/2245 train_time:128083ms step_avg:61.28ms
step:2091/2245 train_time:128146ms step_avg:61.28ms
step:2092/2245 train_time:128206ms step_avg:61.28ms
step:2093/2245 train_time:128269ms step_avg:61.28ms
step:2094/2245 train_time:128330ms step_avg:61.28ms
step:2095/2245 train_time:128392ms step_avg:61.28ms
step:2096/2245 train_time:128452ms step_avg:61.28ms
step:2097/2245 train_time:128514ms step_avg:61.28ms
step:2098/2245 train_time:128575ms step_avg:61.28ms
step:2099/2245 train_time:128637ms step_avg:61.29ms
step:2100/2245 train_time:128698ms step_avg:61.28ms
step:2101/2245 train_time:128763ms step_avg:61.29ms
step:2102/2245 train_time:128824ms step_avg:61.29ms
step:2103/2245 train_time:128887ms step_avg:61.29ms
step:2104/2245 train_time:128948ms step_avg:61.29ms
step:2105/2245 train_time:129010ms step_avg:61.29ms
step:2106/2245 train_time:129071ms step_avg:61.29ms
step:2107/2245 train_time:129133ms step_avg:61.29ms
step:2108/2245 train_time:129193ms step_avg:61.29ms
step:2109/2245 train_time:129256ms step_avg:61.29ms
step:2110/2245 train_time:129316ms step_avg:61.29ms
step:2111/2245 train_time:129379ms step_avg:61.29ms
step:2112/2245 train_time:129440ms step_avg:61.29ms
step:2113/2245 train_time:129503ms step_avg:61.29ms
step:2114/2245 train_time:129565ms step_avg:61.29ms
step:2115/2245 train_time:129629ms step_avg:61.29ms
step:2116/2245 train_time:129689ms step_avg:61.29ms
step:2117/2245 train_time:129752ms step_avg:61.29ms
step:2118/2245 train_time:129812ms step_avg:61.29ms
step:2119/2245 train_time:129875ms step_avg:61.29ms
step:2120/2245 train_time:129936ms step_avg:61.29ms
step:2121/2245 train_time:129998ms step_avg:61.29ms
step:2122/2245 train_time:130059ms step_avg:61.29ms
step:2123/2245 train_time:130122ms step_avg:61.29ms
step:2124/2245 train_time:130182ms step_avg:61.29ms
step:2125/2245 train_time:130245ms step_avg:61.29ms
step:2126/2245 train_time:130305ms step_avg:61.29ms
step:2127/2245 train_time:130368ms step_avg:61.29ms
step:2128/2245 train_time:130429ms step_avg:61.29ms
step:2129/2245 train_time:130491ms step_avg:61.29ms
step:2130/2245 train_time:130552ms step_avg:61.29ms
step:2131/2245 train_time:130615ms step_avg:61.29ms
step:2132/2245 train_time:130676ms step_avg:61.29ms
step:2133/2245 train_time:130738ms step_avg:61.29ms
step:2134/2245 train_time:130798ms step_avg:61.29ms
step:2135/2245 train_time:130861ms step_avg:61.29ms
step:2136/2245 train_time:130923ms step_avg:61.29ms
step:2137/2245 train_time:130986ms step_avg:61.29ms
step:2138/2245 train_time:131047ms step_avg:61.29ms
step:2139/2245 train_time:131109ms step_avg:61.29ms
step:2140/2245 train_time:131170ms step_avg:61.29ms
step:2141/2245 train_time:131232ms step_avg:61.29ms
step:2142/2245 train_time:131292ms step_avg:61.29ms
step:2143/2245 train_time:131355ms step_avg:61.29ms
step:2144/2245 train_time:131415ms step_avg:61.29ms
step:2145/2245 train_time:131478ms step_avg:61.30ms
step:2146/2245 train_time:131538ms step_avg:61.29ms
step:2147/2245 train_time:131602ms step_avg:61.30ms
step:2148/2245 train_time:131663ms step_avg:61.30ms
step:2149/2245 train_time:131726ms step_avg:61.30ms
step:2150/2245 train_time:131786ms step_avg:61.30ms
step:2151/2245 train_time:131850ms step_avg:61.30ms
step:2152/2245 train_time:131910ms step_avg:61.30ms
step:2153/2245 train_time:131973ms step_avg:61.30ms
step:2154/2245 train_time:132033ms step_avg:61.30ms
step:2155/2245 train_time:132095ms step_avg:61.30ms
step:2156/2245 train_time:132156ms step_avg:61.30ms
step:2157/2245 train_time:132218ms step_avg:61.30ms
step:2158/2245 train_time:132279ms step_avg:61.30ms
step:2159/2245 train_time:132342ms step_avg:61.30ms
step:2160/2245 train_time:132403ms step_avg:61.30ms
step:2161/2245 train_time:132466ms step_avg:61.30ms
step:2162/2245 train_time:132527ms step_avg:61.30ms
step:2163/2245 train_time:132590ms step_avg:61.30ms
step:2164/2245 train_time:132651ms step_avg:61.30ms
step:2165/2245 train_time:132714ms step_avg:61.30ms
step:2166/2245 train_time:132775ms step_avg:61.30ms
step:2167/2245 train_time:132837ms step_avg:61.30ms
step:2168/2245 train_time:132897ms step_avg:61.30ms
step:2169/2245 train_time:132960ms step_avg:61.30ms
step:2170/2245 train_time:133021ms step_avg:61.30ms
step:2171/2245 train_time:133084ms step_avg:61.30ms
step:2172/2245 train_time:133145ms step_avg:61.30ms
step:2173/2245 train_time:133208ms step_avg:61.30ms
step:2174/2245 train_time:133268ms step_avg:61.30ms
step:2175/2245 train_time:133331ms step_avg:61.30ms
step:2176/2245 train_time:133392ms step_avg:61.30ms
step:2177/2245 train_time:133454ms step_avg:61.30ms
step:2178/2245 train_time:133514ms step_avg:61.30ms
step:2179/2245 train_time:133577ms step_avg:61.30ms
step:2180/2245 train_time:133638ms step_avg:61.30ms
step:2181/2245 train_time:133702ms step_avg:61.30ms
step:2182/2245 train_time:133763ms step_avg:61.30ms
step:2183/2245 train_time:133826ms step_avg:61.30ms
step:2184/2245 train_time:133887ms step_avg:61.30ms
step:2185/2245 train_time:133951ms step_avg:61.30ms
step:2186/2245 train_time:134011ms step_avg:61.30ms
step:2187/2245 train_time:134074ms step_avg:61.30ms
step:2188/2245 train_time:134134ms step_avg:61.30ms
step:2189/2245 train_time:134196ms step_avg:61.30ms
step:2190/2245 train_time:134257ms step_avg:61.30ms
step:2191/2245 train_time:134320ms step_avg:61.31ms
step:2192/2245 train_time:134381ms step_avg:61.31ms
step:2193/2245 train_time:134443ms step_avg:61.31ms
step:2194/2245 train_time:134504ms step_avg:61.31ms
step:2195/2245 train_time:134567ms step_avg:61.31ms
step:2196/2245 train_time:134628ms step_avg:61.31ms
step:2197/2245 train_time:134690ms step_avg:61.31ms
step:2198/2245 train_time:134751ms step_avg:61.31ms
step:2199/2245 train_time:134813ms step_avg:61.31ms
step:2200/2245 train_time:134877ms step_avg:61.31ms
step:2201/2245 train_time:134937ms step_avg:61.31ms
step:2202/2245 train_time:134997ms step_avg:61.31ms
step:2203/2245 train_time:135060ms step_avg:61.31ms
step:2204/2245 train_time:135122ms step_avg:61.31ms
step:2205/2245 train_time:135186ms step_avg:61.31ms
step:2206/2245 train_time:135246ms step_avg:61.31ms
step:2207/2245 train_time:135309ms step_avg:61.31ms
step:2208/2245 train_time:135369ms step_avg:61.31ms
step:2209/2245 train_time:135431ms step_avg:61.31ms
step:2210/2245 train_time:135491ms step_avg:61.31ms
step:2211/2245 train_time:135555ms step_avg:61.31ms
step:2212/2245 train_time:135615ms step_avg:61.31ms
step:2213/2245 train_time:135678ms step_avg:61.31ms
step:2214/2245 train_time:135740ms step_avg:61.31ms
step:2215/2245 train_time:135804ms step_avg:61.31ms
step:2216/2245 train_time:135865ms step_avg:61.31ms
step:2217/2245 train_time:135928ms step_avg:61.31ms
step:2218/2245 train_time:135989ms step_avg:61.31ms
step:2219/2245 train_time:136051ms step_avg:61.31ms
step:2220/2245 train_time:136111ms step_avg:61.31ms
step:2221/2245 train_time:136175ms step_avg:61.31ms
step:2222/2245 train_time:136236ms step_avg:61.31ms
step:2223/2245 train_time:136298ms step_avg:61.31ms
step:2224/2245 train_time:136359ms step_avg:61.31ms
step:2225/2245 train_time:136422ms step_avg:61.31ms
step:2226/2245 train_time:136482ms step_avg:61.31ms
step:2227/2245 train_time:136545ms step_avg:61.31ms
step:2228/2245 train_time:136607ms step_avg:61.31ms
step:2229/2245 train_time:136670ms step_avg:61.31ms
step:2230/2245 train_time:136731ms step_avg:61.31ms
step:2231/2245 train_time:136795ms step_avg:61.32ms
step:2232/2245 train_time:136855ms step_avg:61.31ms
step:2233/2245 train_time:136918ms step_avg:61.32ms
step:2234/2245 train_time:136978ms step_avg:61.32ms
step:2235/2245 train_time:137041ms step_avg:61.32ms
step:2236/2245 train_time:137102ms step_avg:61.32ms
step:2237/2245 train_time:137166ms step_avg:61.32ms
step:2238/2245 train_time:137227ms step_avg:61.32ms
step:2239/2245 train_time:137290ms step_avg:61.32ms
step:2240/2245 train_time:137350ms step_avg:61.32ms
step:2241/2245 train_time:137413ms step_avg:61.32ms
step:2242/2245 train_time:137474ms step_avg:61.32ms
step:2243/2245 train_time:137537ms step_avg:61.32ms
step:2244/2245 train_time:137598ms step_avg:61.32ms
step:2245/2245 train_time:137661ms step_avg:61.32ms
step:2245/2245 val_loss:3.2772 train_time:137722ms step_avg:61.35ms
peak memory allocated: 29249 MiB reserved: 50528 MiB

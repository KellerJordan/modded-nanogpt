import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Tue Dec 16 00:08:52 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   39C    P0            127W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   43C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   43C    P0            126W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   37C    P0            126W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   43C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   43C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   38C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              88      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              88      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:94ms step_avg:93.63ms
step:2/2110 train_time:125ms step_avg:62.52ms
step:3/2110 train_time:150ms step_avg:50.08ms
step:4/2110 train_time:181ms step_avg:45.25ms
step:5/2110 train_time:216ms step_avg:43.11ms
step:6/2110 train_time:416ms step_avg:69.39ms
step:7/2110 train_time:691ms step_avg:98.66ms
step:8/2110 train_time:723ms step_avg:90.40ms
step:9/2110 train_time:756ms step_avg:83.98ms
step:10/2110 train_time:789ms step_avg:78.86ms
step:11/2110 train_time:822ms step_avg:74.75ms
step:12/2110 train_time:855ms step_avg:71.22ms
step:13/2110 train_time:888ms step_avg:68.29ms
step:14/2110 train_time:921ms step_avg:65.80ms
step:15/2110 train_time:954ms step_avg:63.60ms
step:16/2110 train_time:987ms step_avg:61.68ms
step:17/2110 train_time:1020ms step_avg:60.01ms
step:18/2110 train_time:1053ms step_avg:58.49ms
step:19/2110 train_time:1086ms step_avg:57.16ms
step:20/2110 train_time:1119ms step_avg:55.93ms
step:21/2110 train_time:1152ms step_avg:54.85ms
step:22/2110 train_time:1184ms step_avg:53.84ms
step:23/2110 train_time:1218ms step_avg:52.95ms
step:24/2110 train_time:1252ms step_avg:52.15ms
step:25/2110 train_time:1284ms step_avg:51.36ms
step:26/2110 train_time:1316ms step_avg:50.63ms
step:27/2110 train_time:1350ms step_avg:49.98ms
step:28/2110 train_time:1383ms step_avg:49.38ms
step:29/2110 train_time:1416ms step_avg:48.81ms
step:30/2110 train_time:1448ms step_avg:48.28ms
step:31/2110 train_time:1483ms step_avg:47.82ms
step:32/2110 train_time:1516ms step_avg:47.38ms
step:33/2110 train_time:1548ms step_avg:46.92ms
step:34/2110 train_time:1581ms step_avg:46.51ms
step:35/2110 train_time:1616ms step_avg:46.17ms
step:36/2110 train_time:1651ms step_avg:45.85ms
step:37/2110 train_time:1684ms step_avg:45.51ms
step:38/2110 train_time:1718ms step_avg:45.21ms
step:39/2110 train_time:1751ms step_avg:44.90ms
step:40/2110 train_time:1784ms step_avg:44.60ms
step:41/2110 train_time:1817ms step_avg:44.33ms
step:42/2110 train_time:1850ms step_avg:44.06ms
step:43/2110 train_time:1884ms step_avg:43.81ms
step:44/2110 train_time:1917ms step_avg:43.57ms
step:45/2110 train_time:1950ms step_avg:43.34ms
step:46/2110 train_time:1983ms step_avg:43.11ms
step:47/2110 train_time:2016ms step_avg:42.90ms
step:48/2110 train_time:2049ms step_avg:42.69ms
step:49/2110 train_time:2082ms step_avg:42.50ms
step:50/2110 train_time:2115ms step_avg:42.30ms
step:51/2110 train_time:2149ms step_avg:42.14ms
step:52/2110 train_time:2182ms step_avg:41.96ms
step:53/2110 train_time:2215ms step_avg:41.79ms
step:54/2110 train_time:2248ms step_avg:41.63ms
step:55/2110 train_time:2281ms step_avg:41.47ms
step:56/2110 train_time:2314ms step_avg:41.32ms
step:57/2110 train_time:2347ms step_avg:41.17ms
step:58/2110 train_time:2381ms step_avg:41.06ms
step:59/2110 train_time:2413ms step_avg:40.90ms
step:60/2110 train_time:2446ms step_avg:40.76ms
step:61/2110 train_time:2479ms step_avg:40.64ms
step:62/2110 train_time:2512ms step_avg:40.52ms
step:63/2110 train_time:2545ms step_avg:40.40ms
step:64/2110 train_time:2578ms step_avg:40.29ms
step:65/2110 train_time:2612ms step_avg:40.18ms
step:66/2110 train_time:2645ms step_avg:40.08ms
step:67/2110 train_time:2679ms step_avg:39.98ms
step:68/2110 train_time:2714ms step_avg:39.91ms
step:69/2110 train_time:2746ms step_avg:39.80ms
step:70/2110 train_time:2779ms step_avg:39.71ms
step:71/2110 train_time:2813ms step_avg:39.62ms
step:72/2110 train_time:2846ms step_avg:39.53ms
step:73/2110 train_time:2880ms step_avg:39.45ms
step:74/2110 train_time:2913ms step_avg:39.37ms
step:75/2110 train_time:2946ms step_avg:39.29ms
step:76/2110 train_time:2980ms step_avg:39.21ms
step:77/2110 train_time:3013ms step_avg:39.13ms
step:78/2110 train_time:3046ms step_avg:39.05ms
step:79/2110 train_time:3079ms step_avg:38.98ms
step:80/2110 train_time:3112ms step_avg:38.90ms
step:81/2110 train_time:3145ms step_avg:38.83ms
step:82/2110 train_time:3178ms step_avg:38.75ms
step:83/2110 train_time:3211ms step_avg:38.69ms
step:84/2110 train_time:3244ms step_avg:38.62ms
step:85/2110 train_time:3277ms step_avg:38.55ms
step:86/2110 train_time:3309ms step_avg:38.48ms
step:87/2110 train_time:3343ms step_avg:38.42ms
step:88/2110 train_time:3375ms step_avg:38.36ms
step:89/2110 train_time:3409ms step_avg:38.30ms
step:90/2110 train_time:3442ms step_avg:38.25ms
step:91/2110 train_time:3475ms step_avg:38.18ms
step:92/2110 train_time:3507ms step_avg:38.12ms
step:93/2110 train_time:3541ms step_avg:38.07ms
step:94/2110 train_time:3574ms step_avg:38.02ms
step:95/2110 train_time:3607ms step_avg:37.97ms
step:96/2110 train_time:3641ms step_avg:37.92ms
step:97/2110 train_time:3674ms step_avg:37.88ms
step:98/2110 train_time:3707ms step_avg:37.83ms
step:99/2110 train_time:3741ms step_avg:37.79ms
step:100/2110 train_time:3774ms step_avg:37.74ms
step:101/2110 train_time:3807ms step_avg:37.70ms
step:102/2110 train_time:3840ms step_avg:37.65ms
step:103/2110 train_time:3874ms step_avg:37.61ms
step:104/2110 train_time:3907ms step_avg:37.56ms
step:105/2110 train_time:3939ms step_avg:37.52ms
step:106/2110 train_time:3972ms step_avg:37.47ms
step:107/2110 train_time:4005ms step_avg:37.43ms
step:108/2110 train_time:4038ms step_avg:37.39ms
step:109/2110 train_time:4072ms step_avg:37.35ms
step:110/2110 train_time:4105ms step_avg:37.32ms
step:111/2110 train_time:4138ms step_avg:37.27ms
step:112/2110 train_time:4172ms step_avg:37.25ms
step:113/2110 train_time:4204ms step_avg:37.20ms
step:114/2110 train_time:4237ms step_avg:37.17ms
step:115/2110 train_time:4270ms step_avg:37.13ms
step:116/2110 train_time:4303ms step_avg:37.09ms
step:117/2110 train_time:4336ms step_avg:37.06ms
step:118/2110 train_time:4369ms step_avg:37.03ms
step:119/2110 train_time:4402ms step_avg:36.99ms
step:120/2110 train_time:4434ms step_avg:36.95ms
step:121/2110 train_time:4468ms step_avg:36.92ms
step:122/2110 train_time:4500ms step_avg:36.89ms
step:123/2110 train_time:4534ms step_avg:36.86ms
step:124/2110 train_time:4567ms step_avg:36.83ms
step:125/2110 train_time:4600ms step_avg:36.80ms
step:126/2110 train_time:4633ms step_avg:36.77ms
step:127/2110 train_time:4666ms step_avg:36.74ms
step:128/2110 train_time:4700ms step_avg:36.72ms
step:129/2110 train_time:4732ms step_avg:36.68ms
step:130/2110 train_time:4765ms step_avg:36.66ms
step:131/2110 train_time:4803ms step_avg:36.67ms
step:132/2110 train_time:4834ms step_avg:36.62ms
step:133/2110 train_time:4864ms step_avg:36.57ms
step:134/2110 train_time:4896ms step_avg:36.54ms
step:135/2110 train_time:4930ms step_avg:36.52ms
step:136/2110 train_time:4963ms step_avg:36.49ms
step:137/2110 train_time:4996ms step_avg:36.47ms
step:138/2110 train_time:5030ms step_avg:36.45ms
step:139/2110 train_time:5062ms step_avg:36.42ms
step:140/2110 train_time:5096ms step_avg:36.40ms
step:141/2110 train_time:5128ms step_avg:36.37ms
step:142/2110 train_time:5161ms step_avg:36.35ms
step:143/2110 train_time:5195ms step_avg:36.33ms
step:144/2110 train_time:5228ms step_avg:36.31ms
step:145/2110 train_time:5261ms step_avg:36.28ms
step:146/2110 train_time:5296ms step_avg:36.27ms
step:147/2110 train_time:5327ms step_avg:36.23ms
step:148/2110 train_time:5361ms step_avg:36.22ms
step:149/2110 train_time:5393ms step_avg:36.19ms
step:150/2110 train_time:5426ms step_avg:36.17ms
step:151/2110 train_time:5458ms step_avg:36.14ms
step:152/2110 train_time:5491ms step_avg:36.13ms
step:153/2110 train_time:5524ms step_avg:36.10ms
step:154/2110 train_time:5557ms step_avg:36.08ms
step:155/2110 train_time:5590ms step_avg:36.06ms
step:156/2110 train_time:5622ms step_avg:36.04ms
step:157/2110 train_time:5656ms step_avg:36.02ms
step:158/2110 train_time:5689ms step_avg:36.01ms
step:159/2110 train_time:5721ms step_avg:35.98ms
step:160/2110 train_time:5756ms step_avg:35.98ms
step:161/2110 train_time:5788ms step_avg:35.95ms
step:162/2110 train_time:5821ms step_avg:35.93ms
step:163/2110 train_time:5854ms step_avg:35.92ms
step:164/2110 train_time:5888ms step_avg:35.90ms
step:165/2110 train_time:5921ms step_avg:35.88ms
step:166/2110 train_time:5954ms step_avg:35.87ms
step:167/2110 train_time:5988ms step_avg:35.86ms
step:168/2110 train_time:6023ms step_avg:35.85ms
step:169/2110 train_time:6053ms step_avg:35.82ms
step:170/2110 train_time:6087ms step_avg:35.81ms
step:171/2110 train_time:6119ms step_avg:35.78ms
step:172/2110 train_time:6152ms step_avg:35.76ms
step:173/2110 train_time:6185ms step_avg:35.75ms
step:174/2110 train_time:6218ms step_avg:35.73ms
step:175/2110 train_time:6251ms step_avg:35.72ms
step:176/2110 train_time:6284ms step_avg:35.70ms
step:177/2110 train_time:6317ms step_avg:35.69ms
step:178/2110 train_time:6350ms step_avg:35.67ms
step:179/2110 train_time:6383ms step_avg:35.66ms
step:180/2110 train_time:6416ms step_avg:35.64ms
step:181/2110 train_time:6449ms step_avg:35.63ms
step:182/2110 train_time:6482ms step_avg:35.61ms
step:183/2110 train_time:6515ms step_avg:35.60ms
step:184/2110 train_time:6547ms step_avg:35.58ms
step:185/2110 train_time:6581ms step_avg:35.57ms
step:186/2110 train_time:6614ms step_avg:35.56ms
step:187/2110 train_time:6647ms step_avg:35.55ms
step:188/2110 train_time:6680ms step_avg:35.53ms
step:189/2110 train_time:6714ms step_avg:35.52ms
step:190/2110 train_time:6747ms step_avg:35.51ms
step:191/2110 train_time:6779ms step_avg:35.49ms
step:192/2110 train_time:6812ms step_avg:35.48ms
step:193/2110 train_time:6845ms step_avg:35.47ms
step:194/2110 train_time:6878ms step_avg:35.45ms
step:195/2110 train_time:6912ms step_avg:35.44ms
step:196/2110 train_time:6944ms step_avg:35.43ms
step:197/2110 train_time:6977ms step_avg:35.42ms
step:198/2110 train_time:7011ms step_avg:35.41ms
step:199/2110 train_time:7043ms step_avg:35.39ms
step:200/2110 train_time:7077ms step_avg:35.38ms
step:201/2110 train_time:7109ms step_avg:35.37ms
step:202/2110 train_time:7143ms step_avg:35.36ms
step:203/2110 train_time:7175ms step_avg:35.35ms
step:204/2110 train_time:7209ms step_avg:35.34ms
step:205/2110 train_time:7242ms step_avg:35.32ms
step:206/2110 train_time:7275ms step_avg:35.31ms
step:207/2110 train_time:7307ms step_avg:35.30ms
step:208/2110 train_time:7340ms step_avg:35.29ms
step:209/2110 train_time:7373ms step_avg:35.28ms
step:210/2110 train_time:7406ms step_avg:35.27ms
step:211/2110 train_time:7440ms step_avg:35.26ms
step:212/2110 train_time:7472ms step_avg:35.25ms
step:213/2110 train_time:7505ms step_avg:35.24ms
step:214/2110 train_time:7538ms step_avg:35.23ms
step:215/2110 train_time:7571ms step_avg:35.22ms
step:216/2110 train_time:7604ms step_avg:35.20ms
step:217/2110 train_time:7637ms step_avg:35.20ms
step:218/2110 train_time:7671ms step_avg:35.19ms
step:219/2110 train_time:7703ms step_avg:35.17ms
step:220/2110 train_time:7737ms step_avg:35.17ms
step:221/2110 train_time:7769ms step_avg:35.16ms
step:222/2110 train_time:7802ms step_avg:35.15ms
step:223/2110 train_time:7835ms step_avg:35.14ms
step:224/2110 train_time:7869ms step_avg:35.13ms
step:225/2110 train_time:7901ms step_avg:35.12ms
step:226/2110 train_time:7934ms step_avg:35.11ms
step:227/2110 train_time:7968ms step_avg:35.10ms
step:228/2110 train_time:8001ms step_avg:35.09ms
step:229/2110 train_time:8034ms step_avg:35.08ms
step:230/2110 train_time:8067ms step_avg:35.07ms
step:231/2110 train_time:8100ms step_avg:35.06ms
step:232/2110 train_time:8133ms step_avg:35.05ms
step:233/2110 train_time:8166ms step_avg:35.05ms
step:234/2110 train_time:8199ms step_avg:35.04ms
step:235/2110 train_time:8232ms step_avg:35.03ms
step:236/2110 train_time:8265ms step_avg:35.02ms
step:237/2110 train_time:8298ms step_avg:35.01ms
step:238/2110 train_time:8332ms step_avg:35.01ms
step:239/2110 train_time:8364ms step_avg:34.99ms
step:240/2110 train_time:8397ms step_avg:34.99ms
step:241/2110 train_time:8430ms step_avg:34.98ms
step:242/2110 train_time:8463ms step_avg:34.97ms
step:243/2110 train_time:8496ms step_avg:34.96ms
step:244/2110 train_time:8529ms step_avg:34.95ms
step:245/2110 train_time:8562ms step_avg:34.95ms
step:246/2110 train_time:8596ms step_avg:34.94ms
step:247/2110 train_time:8628ms step_avg:34.93ms
step:248/2110 train_time:8661ms step_avg:34.92ms
step:249/2110 train_time:8694ms step_avg:34.91ms
step:250/2110 train_time:8727ms step_avg:34.91ms
step:250/2110 val_loss:4.3073 train_time:8762ms step_avg:35.05ms
step:251/2110 train_time:8795ms step_avg:35.04ms
step:252/2110 train_time:8825ms step_avg:35.02ms
step:253/2110 train_time:8853ms step_avg:34.99ms
step:254/2110 train_time:8884ms step_avg:34.98ms
step:255/2110 train_time:8915ms step_avg:34.96ms
step:256/2110 train_time:8954ms step_avg:34.98ms
step:257/2110 train_time:8990ms step_avg:34.98ms
step:258/2110 train_time:9027ms step_avg:34.99ms
step:259/2110 train_time:9062ms step_avg:34.99ms
step:260/2110 train_time:9095ms step_avg:34.98ms
step:261/2110 train_time:9129ms step_avg:34.98ms
step:262/2110 train_time:9162ms step_avg:34.97ms
step:263/2110 train_time:9196ms step_avg:34.97ms
step:264/2110 train_time:9233ms step_avg:34.97ms
step:265/2110 train_time:9266ms step_avg:34.97ms
step:266/2110 train_time:9307ms step_avg:34.99ms
step:267/2110 train_time:9341ms step_avg:34.99ms
step:268/2110 train_time:9378ms step_avg:34.99ms
step:269/2110 train_time:9413ms step_avg:34.99ms
step:270/2110 train_time:9452ms step_avg:35.01ms
step:271/2110 train_time:9486ms step_avg:35.00ms
step:272/2110 train_time:9528ms step_avg:35.03ms
step:273/2110 train_time:9563ms step_avg:35.03ms
step:274/2110 train_time:9604ms step_avg:35.05ms
step:275/2110 train_time:9631ms step_avg:35.02ms
step:276/2110 train_time:9664ms step_avg:35.02ms
step:277/2110 train_time:9701ms step_avg:35.02ms
step:278/2110 train_time:9733ms step_avg:35.01ms
step:279/2110 train_time:9768ms step_avg:35.01ms
step:280/2110 train_time:9803ms step_avg:35.01ms
step:281/2110 train_time:9830ms step_avg:34.98ms
step:282/2110 train_time:9861ms step_avg:34.97ms
step:283/2110 train_time:9893ms step_avg:34.96ms
step:284/2110 train_time:9926ms step_avg:34.95ms
step:285/2110 train_time:9959ms step_avg:34.94ms
step:286/2110 train_time:9992ms step_avg:34.94ms
step:287/2110 train_time:10024ms step_avg:34.93ms
step:288/2110 train_time:10057ms step_avg:34.92ms
step:289/2110 train_time:10089ms step_avg:34.91ms
step:290/2110 train_time:10122ms step_avg:34.90ms
step:291/2110 train_time:10154ms step_avg:34.89ms
step:292/2110 train_time:10187ms step_avg:34.89ms
step:293/2110 train_time:10220ms step_avg:34.88ms
step:294/2110 train_time:10253ms step_avg:34.87ms
step:295/2110 train_time:10285ms step_avg:34.87ms
step:296/2110 train_time:10319ms step_avg:34.86ms
step:297/2110 train_time:10352ms step_avg:34.85ms
step:298/2110 train_time:10384ms step_avg:34.84ms
step:299/2110 train_time:10417ms step_avg:34.84ms
step:300/2110 train_time:10449ms step_avg:34.83ms
step:301/2110 train_time:10483ms step_avg:34.83ms
step:302/2110 train_time:10516ms step_avg:34.82ms
step:303/2110 train_time:10550ms step_avg:34.82ms
step:304/2110 train_time:10585ms step_avg:34.82ms
step:305/2110 train_time:10617ms step_avg:34.81ms
step:306/2110 train_time:10652ms step_avg:34.81ms
step:307/2110 train_time:10684ms step_avg:34.80ms
step:308/2110 train_time:10718ms step_avg:34.80ms
step:309/2110 train_time:10750ms step_avg:34.79ms
step:310/2110 train_time:10785ms step_avg:34.79ms
step:311/2110 train_time:10819ms step_avg:34.79ms
step:312/2110 train_time:10859ms step_avg:34.80ms
step:313/2110 train_time:10897ms step_avg:34.81ms
step:314/2110 train_time:10931ms step_avg:34.81ms
step:315/2110 train_time:10972ms step_avg:34.83ms
step:316/2110 train_time:11012ms step_avg:34.85ms
step:317/2110 train_time:11044ms step_avg:34.84ms
step:318/2110 train_time:11078ms step_avg:34.84ms
step:319/2110 train_time:11110ms step_avg:34.83ms
step:320/2110 train_time:11144ms step_avg:34.83ms
step:321/2110 train_time:11179ms step_avg:34.83ms
step:322/2110 train_time:11215ms step_avg:34.83ms
step:323/2110 train_time:11241ms step_avg:34.80ms
step:324/2110 train_time:11270ms step_avg:34.78ms
step:325/2110 train_time:11299ms step_avg:34.77ms
step:326/2110 train_time:11328ms step_avg:34.75ms
step:327/2110 train_time:11355ms step_avg:34.72ms
step:328/2110 train_time:11387ms step_avg:34.72ms
step:329/2110 train_time:11416ms step_avg:34.70ms
step:330/2110 train_time:11447ms step_avg:34.69ms
step:331/2110 train_time:11478ms step_avg:34.68ms
step:332/2110 train_time:11511ms step_avg:34.67ms
step:333/2110 train_time:11544ms step_avg:34.67ms
step:334/2110 train_time:11577ms step_avg:34.66ms
step:335/2110 train_time:11610ms step_avg:34.66ms
step:336/2110 train_time:11644ms step_avg:34.65ms
step:337/2110 train_time:11677ms step_avg:34.65ms
step:338/2110 train_time:11709ms step_avg:34.64ms
step:339/2110 train_time:11743ms step_avg:34.64ms
step:340/2110 train_time:11776ms step_avg:34.63ms
step:341/2110 train_time:11809ms step_avg:34.63ms
step:342/2110 train_time:11842ms step_avg:34.63ms
step:343/2110 train_time:11875ms step_avg:34.62ms
step:344/2110 train_time:11908ms step_avg:34.62ms
step:345/2110 train_time:11941ms step_avg:34.61ms
step:346/2110 train_time:11975ms step_avg:34.61ms
step:347/2110 train_time:12008ms step_avg:34.61ms
step:348/2110 train_time:12040ms step_avg:34.60ms
step:349/2110 train_time:12074ms step_avg:34.60ms
step:350/2110 train_time:12106ms step_avg:34.59ms
step:351/2110 train_time:12139ms step_avg:34.58ms
step:352/2110 train_time:12172ms step_avg:34.58ms
step:353/2110 train_time:12206ms step_avg:34.58ms
step:354/2110 train_time:12238ms step_avg:34.57ms
step:355/2110 train_time:12271ms step_avg:34.57ms
step:356/2110 train_time:12304ms step_avg:34.56ms
step:357/2110 train_time:12337ms step_avg:34.56ms
step:358/2110 train_time:12370ms step_avg:34.55ms
step:359/2110 train_time:12402ms step_avg:34.55ms
step:360/2110 train_time:12436ms step_avg:34.55ms
step:361/2110 train_time:12468ms step_avg:34.54ms
step:362/2110 train_time:12502ms step_avg:34.53ms
step:363/2110 train_time:12534ms step_avg:34.53ms
step:364/2110 train_time:12568ms step_avg:34.53ms
step:365/2110 train_time:12600ms step_avg:34.52ms
step:366/2110 train_time:12634ms step_avg:34.52ms
step:367/2110 train_time:12666ms step_avg:34.51ms
step:368/2110 train_time:12704ms step_avg:34.52ms
step:369/2110 train_time:12737ms step_avg:34.52ms
step:370/2110 train_time:12771ms step_avg:34.52ms
step:371/2110 train_time:12807ms step_avg:34.52ms
step:372/2110 train_time:12847ms step_avg:34.53ms
step:373/2110 train_time:12880ms step_avg:34.53ms
step:374/2110 train_time:12917ms step_avg:34.54ms
step:375/2110 train_time:12952ms step_avg:34.54ms
step:376/2110 train_time:12988ms step_avg:34.54ms
step:377/2110 train_time:13015ms step_avg:34.52ms
step:378/2110 train_time:13044ms step_avg:34.51ms
step:379/2110 train_time:13074ms step_avg:34.50ms
step:380/2110 train_time:13104ms step_avg:34.48ms
step:381/2110 train_time:13130ms step_avg:34.46ms
step:382/2110 train_time:13162ms step_avg:34.46ms
step:383/2110 train_time:13197ms step_avg:34.46ms
step:384/2110 train_time:13227ms step_avg:34.44ms
step:385/2110 train_time:13260ms step_avg:34.44ms
step:386/2110 train_time:13293ms step_avg:34.44ms
step:387/2110 train_time:13327ms step_avg:34.44ms
step:388/2110 train_time:13359ms step_avg:34.43ms
step:389/2110 train_time:13392ms step_avg:34.43ms
step:390/2110 train_time:13425ms step_avg:34.42ms
step:391/2110 train_time:13458ms step_avg:34.42ms
step:392/2110 train_time:13492ms step_avg:34.42ms
step:393/2110 train_time:13524ms step_avg:34.41ms
step:394/2110 train_time:13557ms step_avg:34.41ms
step:395/2110 train_time:13590ms step_avg:34.40ms
step:396/2110 train_time:13623ms step_avg:34.40ms
step:397/2110 train_time:13656ms step_avg:34.40ms
step:398/2110 train_time:13690ms step_avg:34.40ms
step:399/2110 train_time:13722ms step_avg:34.39ms
step:400/2110 train_time:13756ms step_avg:34.39ms
step:401/2110 train_time:13788ms step_avg:34.39ms
step:402/2110 train_time:13822ms step_avg:34.38ms
step:403/2110 train_time:13855ms step_avg:34.38ms
step:404/2110 train_time:13888ms step_avg:34.38ms
step:405/2110 train_time:13921ms step_avg:34.37ms
step:406/2110 train_time:13953ms step_avg:34.37ms
step:407/2110 train_time:13986ms step_avg:34.36ms
step:408/2110 train_time:14019ms step_avg:34.36ms
step:409/2110 train_time:14052ms step_avg:34.36ms
step:410/2110 train_time:14085ms step_avg:34.35ms
step:411/2110 train_time:14118ms step_avg:34.35ms
step:412/2110 train_time:14151ms step_avg:34.35ms
step:413/2110 train_time:14184ms step_avg:34.34ms
step:414/2110 train_time:14217ms step_avg:34.34ms
step:415/2110 train_time:14250ms step_avg:34.34ms
step:416/2110 train_time:14283ms step_avg:34.33ms
step:417/2110 train_time:14316ms step_avg:34.33ms
step:418/2110 train_time:14349ms step_avg:34.33ms
step:419/2110 train_time:14382ms step_avg:34.32ms
step:420/2110 train_time:14415ms step_avg:34.32ms
step:421/2110 train_time:14448ms step_avg:34.32ms
step:422/2110 train_time:14481ms step_avg:34.31ms
step:423/2110 train_time:14514ms step_avg:34.31ms
step:424/2110 train_time:14547ms step_avg:34.31ms
step:425/2110 train_time:14580ms step_avg:34.31ms
step:426/2110 train_time:14613ms step_avg:34.30ms
step:427/2110 train_time:14647ms step_avg:34.30ms
step:428/2110 train_time:14680ms step_avg:34.30ms
step:429/2110 train_time:14713ms step_avg:34.30ms
step:430/2110 train_time:14746ms step_avg:34.29ms
step:431/2110 train_time:14779ms step_avg:34.29ms
step:432/2110 train_time:14812ms step_avg:34.29ms
step:433/2110 train_time:14845ms step_avg:34.28ms
step:434/2110 train_time:14878ms step_avg:34.28ms
step:435/2110 train_time:14911ms step_avg:34.28ms
step:436/2110 train_time:14943ms step_avg:34.27ms
step:437/2110 train_time:14976ms step_avg:34.27ms
step:438/2110 train_time:15010ms step_avg:34.27ms
step:439/2110 train_time:15042ms step_avg:34.27ms
step:440/2110 train_time:15076ms step_avg:34.26ms
step:441/2110 train_time:15108ms step_avg:34.26ms
step:442/2110 train_time:15142ms step_avg:34.26ms
step:443/2110 train_time:15175ms step_avg:34.25ms
step:444/2110 train_time:15208ms step_avg:34.25ms
step:445/2110 train_time:15240ms step_avg:34.25ms
step:446/2110 train_time:15273ms step_avg:34.24ms
step:447/2110 train_time:15307ms step_avg:34.24ms
step:448/2110 train_time:15340ms step_avg:34.24ms
step:449/2110 train_time:15372ms step_avg:34.24ms
step:450/2110 train_time:15405ms step_avg:34.23ms
step:451/2110 train_time:15437ms step_avg:34.23ms
step:452/2110 train_time:15472ms step_avg:34.23ms
step:453/2110 train_time:15505ms step_avg:34.23ms
step:454/2110 train_time:15538ms step_avg:34.23ms
step:455/2110 train_time:15570ms step_avg:34.22ms
step:456/2110 train_time:15604ms step_avg:34.22ms
step:457/2110 train_time:15636ms step_avg:34.22ms
step:458/2110 train_time:15670ms step_avg:34.21ms
step:459/2110 train_time:15703ms step_avg:34.21ms
step:460/2110 train_time:15739ms step_avg:34.21ms
step:461/2110 train_time:15769ms step_avg:34.21ms
step:462/2110 train_time:15802ms step_avg:34.20ms
step:463/2110 train_time:15834ms step_avg:34.20ms
step:464/2110 train_time:15867ms step_avg:34.20ms
step:465/2110 train_time:15900ms step_avg:34.19ms
step:466/2110 train_time:15934ms step_avg:34.19ms
step:467/2110 train_time:15966ms step_avg:34.19ms
step:468/2110 train_time:16000ms step_avg:34.19ms
step:469/2110 train_time:16037ms step_avg:34.19ms
step:470/2110 train_time:16070ms step_avg:34.19ms
step:471/2110 train_time:16098ms step_avg:34.18ms
step:472/2110 train_time:16132ms step_avg:34.18ms
step:473/2110 train_time:16165ms step_avg:34.17ms
step:474/2110 train_time:16197ms step_avg:34.17ms
step:475/2110 train_time:16231ms step_avg:34.17ms
step:476/2110 train_time:16263ms step_avg:34.17ms
step:477/2110 train_time:16296ms step_avg:34.16ms
step:478/2110 train_time:16330ms step_avg:34.16ms
step:479/2110 train_time:16362ms step_avg:34.16ms
step:480/2110 train_time:16396ms step_avg:34.16ms
step:481/2110 train_time:16428ms step_avg:34.15ms
step:482/2110 train_time:16461ms step_avg:34.15ms
step:483/2110 train_time:16494ms step_avg:34.15ms
step:484/2110 train_time:16529ms step_avg:34.15ms
step:485/2110 train_time:16561ms step_avg:34.15ms
step:486/2110 train_time:16593ms step_avg:34.14ms
step:487/2110 train_time:16627ms step_avg:34.14ms
step:488/2110 train_time:16659ms step_avg:34.14ms
step:489/2110 train_time:16692ms step_avg:34.14ms
step:490/2110 train_time:16725ms step_avg:34.13ms
step:491/2110 train_time:16759ms step_avg:34.13ms
step:492/2110 train_time:16793ms step_avg:34.13ms
step:493/2110 train_time:16824ms step_avg:34.13ms
step:494/2110 train_time:16858ms step_avg:34.12ms
step:495/2110 train_time:16890ms step_avg:34.12ms
step:496/2110 train_time:16925ms step_avg:34.12ms
step:497/2110 train_time:16956ms step_avg:34.12ms
step:498/2110 train_time:16989ms step_avg:34.11ms
step:499/2110 train_time:17023ms step_avg:34.11ms
step:500/2110 train_time:17058ms step_avg:34.12ms
step:500/2110 val_loss:4.0290 train_time:17091ms step_avg:34.18ms
step:501/2110 train_time:17119ms step_avg:34.17ms
step:502/2110 train_time:17150ms step_avg:34.16ms
step:503/2110 train_time:17177ms step_avg:34.15ms
step:504/2110 train_time:17207ms step_avg:34.14ms
step:505/2110 train_time:17235ms step_avg:34.13ms
step:506/2110 train_time:17266ms step_avg:34.12ms
step:507/2110 train_time:17302ms step_avg:34.13ms
step:508/2110 train_time:17335ms step_avg:34.12ms
step:509/2110 train_time:17368ms step_avg:34.12ms
step:510/2110 train_time:17401ms step_avg:34.12ms
step:511/2110 train_time:17434ms step_avg:34.12ms
step:512/2110 train_time:17474ms step_avg:34.13ms
step:513/2110 train_time:17505ms step_avg:34.12ms
step:514/2110 train_time:17542ms step_avg:34.13ms
step:515/2110 train_time:17580ms step_avg:34.14ms
step:516/2110 train_time:17614ms step_avg:34.14ms
step:517/2110 train_time:17646ms step_avg:34.13ms
step:518/2110 train_time:17679ms step_avg:34.13ms
step:519/2110 train_time:17714ms step_avg:34.13ms
step:520/2110 train_time:17750ms step_avg:34.13ms
step:521/2110 train_time:17781ms step_avg:34.13ms
step:522/2110 train_time:17813ms step_avg:34.12ms
step:523/2110 train_time:17852ms step_avg:34.13ms
step:524/2110 train_time:17886ms step_avg:34.13ms
step:525/2110 train_time:17917ms step_avg:34.13ms
step:526/2110 train_time:17951ms step_avg:34.13ms
step:527/2110 train_time:17985ms step_avg:34.13ms
step:528/2110 train_time:18019ms step_avg:34.13ms
step:529/2110 train_time:18055ms step_avg:34.13ms
step:530/2110 train_time:18089ms step_avg:34.13ms
step:531/2110 train_time:18123ms step_avg:34.13ms
step:532/2110 train_time:18157ms step_avg:34.13ms
step:533/2110 train_time:18189ms step_avg:34.13ms
step:534/2110 train_time:18224ms step_avg:34.13ms
step:535/2110 train_time:18258ms step_avg:34.13ms
step:536/2110 train_time:18292ms step_avg:34.13ms
step:537/2110 train_time:18325ms step_avg:34.13ms
step:538/2110 train_time:18363ms step_avg:34.13ms
step:539/2110 train_time:18397ms step_avg:34.13ms
step:540/2110 train_time:18431ms step_avg:34.13ms
step:541/2110 train_time:18462ms step_avg:34.13ms
step:542/2110 train_time:18496ms step_avg:34.12ms
step:543/2110 train_time:18528ms step_avg:34.12ms
step:544/2110 train_time:18562ms step_avg:34.12ms
step:545/2110 train_time:18594ms step_avg:34.12ms
step:546/2110 train_time:18628ms step_avg:34.12ms
step:547/2110 train_time:18660ms step_avg:34.11ms
step:548/2110 train_time:18693ms step_avg:34.11ms
step:549/2110 train_time:18726ms step_avg:34.11ms
step:550/2110 train_time:18760ms step_avg:34.11ms
step:551/2110 train_time:18793ms step_avg:34.11ms
step:552/2110 train_time:18826ms step_avg:34.11ms
step:553/2110 train_time:18859ms step_avg:34.10ms
step:554/2110 train_time:18892ms step_avg:34.10ms
step:555/2110 train_time:18925ms step_avg:34.10ms
step:556/2110 train_time:18958ms step_avg:34.10ms
step:557/2110 train_time:18991ms step_avg:34.09ms
step:558/2110 train_time:19024ms step_avg:34.09ms
step:559/2110 train_time:19057ms step_avg:34.09ms
step:560/2110 train_time:19091ms step_avg:34.09ms
step:561/2110 train_time:19123ms step_avg:34.09ms
step:562/2110 train_time:19156ms step_avg:34.08ms
step:563/2110 train_time:19189ms step_avg:34.08ms
step:564/2110 train_time:19224ms step_avg:34.09ms
step:565/2110 train_time:19255ms step_avg:34.08ms
step:566/2110 train_time:19296ms step_avg:34.09ms
step:567/2110 train_time:19331ms step_avg:34.09ms
step:568/2110 train_time:19369ms step_avg:34.10ms
step:569/2110 train_time:19404ms step_avg:34.10ms
step:570/2110 train_time:19433ms step_avg:34.09ms
step:571/2110 train_time:19460ms step_avg:34.08ms
step:572/2110 train_time:19490ms step_avg:34.07ms
step:573/2110 train_time:19520ms step_avg:34.07ms
step:574/2110 train_time:19553ms step_avg:34.06ms
step:575/2110 train_time:19586ms step_avg:34.06ms
step:576/2110 train_time:19619ms step_avg:34.06ms
step:577/2110 train_time:19651ms step_avg:34.06ms
step:578/2110 train_time:19686ms step_avg:34.06ms
step:579/2110 train_time:19717ms step_avg:34.05ms
step:580/2110 train_time:19750ms step_avg:34.05ms
step:581/2110 train_time:19783ms step_avg:34.05ms
step:582/2110 train_time:19817ms step_avg:34.05ms
step:583/2110 train_time:19849ms step_avg:34.05ms
step:584/2110 train_time:19882ms step_avg:34.04ms
step:585/2110 train_time:19915ms step_avg:34.04ms
step:586/2110 train_time:19948ms step_avg:34.04ms
step:587/2110 train_time:19981ms step_avg:34.04ms
step:588/2110 train_time:20015ms step_avg:34.04ms
step:589/2110 train_time:20047ms step_avg:34.04ms
step:590/2110 train_time:20081ms step_avg:34.04ms
step:591/2110 train_time:20115ms step_avg:34.03ms
step:592/2110 train_time:20148ms step_avg:34.03ms
step:593/2110 train_time:20180ms step_avg:34.03ms
step:594/2110 train_time:20213ms step_avg:34.03ms
step:595/2110 train_time:20246ms step_avg:34.03ms
step:596/2110 train_time:20279ms step_avg:34.03ms
step:597/2110 train_time:20312ms step_avg:34.02ms
step:598/2110 train_time:20346ms step_avg:34.02ms
step:599/2110 train_time:20378ms step_avg:34.02ms
step:600/2110 train_time:20413ms step_avg:34.02ms
step:601/2110 train_time:20444ms step_avg:34.02ms
step:602/2110 train_time:20477ms step_avg:34.02ms
step:603/2110 train_time:20510ms step_avg:34.01ms
step:604/2110 train_time:20543ms step_avg:34.01ms
step:605/2110 train_time:20576ms step_avg:34.01ms
step:606/2110 train_time:20610ms step_avg:34.01ms
step:607/2110 train_time:20642ms step_avg:34.01ms
step:608/2110 train_time:20676ms step_avg:34.01ms
step:609/2110 train_time:20708ms step_avg:34.00ms
step:610/2110 train_time:20741ms step_avg:34.00ms
step:611/2110 train_time:20774ms step_avg:34.00ms
step:612/2110 train_time:20807ms step_avg:34.00ms
step:613/2110 train_time:20840ms step_avg:34.00ms
step:614/2110 train_time:20873ms step_avg:34.00ms
step:615/2110 train_time:20906ms step_avg:33.99ms
step:616/2110 train_time:20939ms step_avg:33.99ms
step:617/2110 train_time:20973ms step_avg:33.99ms
step:618/2110 train_time:21006ms step_avg:33.99ms
step:619/2110 train_time:21039ms step_avg:33.99ms
step:620/2110 train_time:21072ms step_avg:33.99ms
step:621/2110 train_time:21106ms step_avg:33.99ms
step:622/2110 train_time:21139ms step_avg:33.98ms
step:623/2110 train_time:21171ms step_avg:33.98ms
step:624/2110 train_time:21205ms step_avg:33.98ms
step:625/2110 train_time:21237ms step_avg:33.98ms
step:626/2110 train_time:21270ms step_avg:33.98ms
step:627/2110 train_time:21303ms step_avg:33.98ms
step:628/2110 train_time:21337ms step_avg:33.98ms
step:629/2110 train_time:21370ms step_avg:33.97ms
step:630/2110 train_time:21402ms step_avg:33.97ms
step:631/2110 train_time:21435ms step_avg:33.97ms
step:632/2110 train_time:21468ms step_avg:33.97ms
step:633/2110 train_time:21501ms step_avg:33.97ms
step:634/2110 train_time:21541ms step_avg:33.98ms
step:635/2110 train_time:21587ms step_avg:33.99ms
step:636/2110 train_time:21621ms step_avg:34.00ms
step:637/2110 train_time:21656ms step_avg:34.00ms
step:638/2110 train_time:21688ms step_avg:33.99ms
step:639/2110 train_time:21732ms step_avg:34.01ms
step:640/2110 train_time:21771ms step_avg:34.02ms
step:641/2110 train_time:21810ms step_avg:34.02ms
step:642/2110 train_time:21844ms step_avg:34.02ms
step:643/2110 train_time:21877ms step_avg:34.02ms
step:644/2110 train_time:21910ms step_avg:34.02ms
step:645/2110 train_time:21944ms step_avg:34.02ms
step:646/2110 train_time:21976ms step_avg:34.02ms
step:647/2110 train_time:22015ms step_avg:34.03ms
step:648/2110 train_time:22052ms step_avg:34.03ms
step:649/2110 train_time:22085ms step_avg:34.03ms
step:650/2110 train_time:22118ms step_avg:34.03ms
step:651/2110 train_time:22155ms step_avg:34.03ms
step:652/2110 train_time:22189ms step_avg:34.03ms
step:653/2110 train_time:22227ms step_avg:34.04ms
step:654/2110 train_time:22259ms step_avg:34.04ms
step:655/2110 train_time:22297ms step_avg:34.04ms
step:656/2110 train_time:22331ms step_avg:34.04ms
step:657/2110 train_time:22365ms step_avg:34.04ms
step:658/2110 train_time:22400ms step_avg:34.04ms
step:659/2110 train_time:22435ms step_avg:34.04ms
step:660/2110 train_time:22468ms step_avg:34.04ms
step:661/2110 train_time:22505ms step_avg:34.05ms
step:662/2110 train_time:22538ms step_avg:34.05ms
step:663/2110 train_time:22577ms step_avg:34.05ms
step:664/2110 train_time:22610ms step_avg:34.05ms
step:665/2110 train_time:22649ms step_avg:34.06ms
step:666/2110 train_time:22682ms step_avg:34.06ms
step:667/2110 train_time:22717ms step_avg:34.06ms
step:668/2110 train_time:22751ms step_avg:34.06ms
step:669/2110 train_time:22786ms step_avg:34.06ms
step:670/2110 train_time:22820ms step_avg:34.06ms
step:671/2110 train_time:22858ms step_avg:34.07ms
step:672/2110 train_time:22892ms step_avg:34.07ms
step:673/2110 train_time:22929ms step_avg:34.07ms
step:674/2110 train_time:22964ms step_avg:34.07ms
step:675/2110 train_time:23002ms step_avg:34.08ms
step:676/2110 train_time:23037ms step_avg:34.08ms
step:677/2110 train_time:23073ms step_avg:34.08ms
step:678/2110 train_time:23109ms step_avg:34.08ms
step:679/2110 train_time:23142ms step_avg:34.08ms
step:680/2110 train_time:23175ms step_avg:34.08ms
step:681/2110 train_time:23213ms step_avg:34.09ms
step:682/2110 train_time:23247ms step_avg:34.09ms
step:683/2110 train_time:23285ms step_avg:34.09ms
step:684/2110 train_time:23319ms step_avg:34.09ms
step:685/2110 train_time:23355ms step_avg:34.10ms
step:686/2110 train_time:23391ms step_avg:34.10ms
step:687/2110 train_time:23429ms step_avg:34.10ms
step:688/2110 train_time:23463ms step_avg:34.10ms
step:689/2110 train_time:23498ms step_avg:34.11ms
step:690/2110 train_time:23533ms step_avg:34.11ms
step:691/2110 train_time:23568ms step_avg:34.11ms
step:692/2110 train_time:23626ms step_avg:34.14ms
step:693/2110 train_time:23684ms step_avg:34.18ms
step:694/2110 train_time:23742ms step_avg:34.21ms
step:695/2110 train_time:23800ms step_avg:34.25ms
step:696/2110 train_time:23859ms step_avg:34.28ms
step:697/2110 train_time:23917ms step_avg:34.31ms
step:698/2110 train_time:23976ms step_avg:34.35ms
step:699/2110 train_time:24036ms step_avg:34.39ms
step:700/2110 train_time:24095ms step_avg:34.42ms
step:701/2110 train_time:24156ms step_avg:34.46ms
step:702/2110 train_time:24214ms step_avg:34.49ms
step:703/2110 train_time:24275ms step_avg:34.53ms
step:704/2110 train_time:24334ms step_avg:34.57ms
step:705/2110 train_time:24395ms step_avg:34.60ms
step:706/2110 train_time:24454ms step_avg:34.64ms
step:707/2110 train_time:24514ms step_avg:34.67ms
step:708/2110 train_time:24573ms step_avg:34.71ms
step:709/2110 train_time:24633ms step_avg:34.74ms
step:710/2110 train_time:24690ms step_avg:34.78ms
step:711/2110 train_time:24750ms step_avg:34.81ms
step:712/2110 train_time:24809ms step_avg:34.84ms
step:713/2110 train_time:24867ms step_avg:34.88ms
step:714/2110 train_time:24926ms step_avg:34.91ms
step:715/2110 train_time:24985ms step_avg:34.94ms
step:716/2110 train_time:25045ms step_avg:34.98ms
step:717/2110 train_time:25105ms step_avg:35.01ms
step:718/2110 train_time:25165ms step_avg:35.05ms
step:719/2110 train_time:25224ms step_avg:35.08ms
step:720/2110 train_time:25284ms step_avg:35.12ms
step:721/2110 train_time:25344ms step_avg:35.15ms
step:722/2110 train_time:25405ms step_avg:35.19ms
step:723/2110 train_time:25464ms step_avg:35.22ms
step:724/2110 train_time:25524ms step_avg:35.25ms
step:725/2110 train_time:25584ms step_avg:35.29ms
step:726/2110 train_time:25642ms step_avg:35.32ms
step:727/2110 train_time:25702ms step_avg:35.35ms
step:728/2110 train_time:25760ms step_avg:35.39ms
step:729/2110 train_time:25819ms step_avg:35.42ms
step:730/2110 train_time:25878ms step_avg:35.45ms
step:731/2110 train_time:25936ms step_avg:35.48ms
step:732/2110 train_time:25995ms step_avg:35.51ms
step:733/2110 train_time:26055ms step_avg:35.55ms
step:734/2110 train_time:26114ms step_avg:35.58ms
step:735/2110 train_time:26174ms step_avg:35.61ms
step:736/2110 train_time:26232ms step_avg:35.64ms
step:737/2110 train_time:26293ms step_avg:35.68ms
step:738/2110 train_time:26352ms step_avg:35.71ms
step:739/2110 train_time:26412ms step_avg:35.74ms
step:740/2110 train_time:26471ms step_avg:35.77ms
step:741/2110 train_time:26532ms step_avg:35.81ms
step:742/2110 train_time:26590ms step_avg:35.84ms
step:743/2110 train_time:26650ms step_avg:35.87ms
step:744/2110 train_time:26708ms step_avg:35.90ms
step:745/2110 train_time:26768ms step_avg:35.93ms
step:746/2110 train_time:26826ms step_avg:35.96ms
step:747/2110 train_time:26886ms step_avg:35.99ms
step:748/2110 train_time:26945ms step_avg:36.02ms
step:749/2110 train_time:27005ms step_avg:36.05ms
step:750/2110 train_time:27064ms step_avg:36.08ms
step:750/2110 val_loss:3.9062 train_time:27125ms step_avg:36.17ms
step:751/2110 train_time:27159ms step_avg:36.16ms
step:752/2110 train_time:27194ms step_avg:36.16ms
step:753/2110 train_time:27248ms step_avg:36.19ms
step:754/2110 train_time:27310ms step_avg:36.22ms
step:755/2110 train_time:27371ms step_avg:36.25ms
step:756/2110 train_time:27429ms step_avg:36.28ms
step:757/2110 train_time:27489ms step_avg:36.31ms
step:758/2110 train_time:27548ms step_avg:36.34ms
step:759/2110 train_time:27607ms step_avg:36.37ms
step:760/2110 train_time:27665ms step_avg:36.40ms
step:761/2110 train_time:27723ms step_avg:36.43ms
step:762/2110 train_time:27781ms step_avg:36.46ms
step:763/2110 train_time:27839ms step_avg:36.49ms
step:764/2110 train_time:27897ms step_avg:36.51ms
step:765/2110 train_time:27956ms step_avg:36.54ms
step:766/2110 train_time:28014ms step_avg:36.57ms
step:767/2110 train_time:28074ms step_avg:36.60ms
step:768/2110 train_time:28133ms step_avg:36.63ms
step:769/2110 train_time:28196ms step_avg:36.67ms
step:770/2110 train_time:28257ms step_avg:36.70ms
step:771/2110 train_time:28318ms step_avg:36.73ms
step:772/2110 train_time:28377ms step_avg:36.76ms
step:773/2110 train_time:28437ms step_avg:36.79ms
step:774/2110 train_time:28496ms step_avg:36.82ms
step:775/2110 train_time:28557ms step_avg:36.85ms
step:776/2110 train_time:28616ms step_avg:36.88ms
step:777/2110 train_time:28675ms step_avg:36.90ms
step:778/2110 train_time:28734ms step_avg:36.93ms
step:779/2110 train_time:28794ms step_avg:36.96ms
step:780/2110 train_time:28852ms step_avg:36.99ms
step:781/2110 train_time:28911ms step_avg:37.02ms
step:782/2110 train_time:28970ms step_avg:37.05ms
step:783/2110 train_time:29029ms step_avg:37.07ms
step:784/2110 train_time:29088ms step_avg:37.10ms
step:785/2110 train_time:29148ms step_avg:37.13ms
step:786/2110 train_time:29208ms step_avg:37.16ms
step:787/2110 train_time:29269ms step_avg:37.19ms
step:788/2110 train_time:29327ms step_avg:37.22ms
step:789/2110 train_time:29387ms step_avg:37.25ms
step:790/2110 train_time:29446ms step_avg:37.27ms
step:791/2110 train_time:29506ms step_avg:37.30ms
step:792/2110 train_time:29563ms step_avg:37.33ms
step:793/2110 train_time:29623ms step_avg:37.36ms
step:794/2110 train_time:29682ms step_avg:37.38ms
step:795/2110 train_time:29741ms step_avg:37.41ms
step:796/2110 train_time:29799ms step_avg:37.44ms
step:797/2110 train_time:29859ms step_avg:37.46ms
step:798/2110 train_time:29917ms step_avg:37.49ms
step:799/2110 train_time:29977ms step_avg:37.52ms
step:800/2110 train_time:30035ms step_avg:37.54ms
step:801/2110 train_time:30096ms step_avg:37.57ms
step:802/2110 train_time:30154ms step_avg:37.60ms
step:803/2110 train_time:30215ms step_avg:37.63ms
step:804/2110 train_time:30275ms step_avg:37.66ms
step:805/2110 train_time:30336ms step_avg:37.68ms
step:806/2110 train_time:30396ms step_avg:37.71ms
step:807/2110 train_time:30457ms step_avg:37.74ms
step:808/2110 train_time:30517ms step_avg:37.77ms
step:809/2110 train_time:30577ms step_avg:37.80ms
step:810/2110 train_time:30635ms step_avg:37.82ms
step:811/2110 train_time:30695ms step_avg:37.85ms
step:812/2110 train_time:30754ms step_avg:37.87ms
step:813/2110 train_time:30813ms step_avg:37.90ms
step:814/2110 train_time:30871ms step_avg:37.93ms
step:815/2110 train_time:30930ms step_avg:37.95ms
step:816/2110 train_time:30989ms step_avg:37.98ms
step:817/2110 train_time:31049ms step_avg:38.00ms
step:818/2110 train_time:31107ms step_avg:38.03ms
step:819/2110 train_time:31166ms step_avg:38.05ms
step:820/2110 train_time:31225ms step_avg:38.08ms
step:821/2110 train_time:31285ms step_avg:38.11ms
step:822/2110 train_time:31343ms step_avg:38.13ms
step:823/2110 train_time:31404ms step_avg:38.16ms
step:824/2110 train_time:31462ms step_avg:38.18ms
step:825/2110 train_time:31523ms step_avg:38.21ms
step:826/2110 train_time:31581ms step_avg:38.23ms
step:827/2110 train_time:31642ms step_avg:38.26ms
step:828/2110 train_time:31700ms step_avg:38.28ms
step:829/2110 train_time:31760ms step_avg:38.31ms
step:830/2110 train_time:31818ms step_avg:38.33ms
step:831/2110 train_time:31879ms step_avg:38.36ms
step:832/2110 train_time:31937ms step_avg:38.39ms
step:833/2110 train_time:31998ms step_avg:38.41ms
step:834/2110 train_time:32056ms step_avg:38.44ms
step:835/2110 train_time:32116ms step_avg:38.46ms
step:836/2110 train_time:32174ms step_avg:38.49ms
step:837/2110 train_time:32234ms step_avg:38.51ms
step:838/2110 train_time:32293ms step_avg:38.54ms
step:839/2110 train_time:32353ms step_avg:38.56ms
step:840/2110 train_time:32412ms step_avg:38.59ms
step:841/2110 train_time:32472ms step_avg:38.61ms
step:842/2110 train_time:32531ms step_avg:38.64ms
step:843/2110 train_time:32592ms step_avg:38.66ms
step:844/2110 train_time:32651ms step_avg:38.69ms
step:845/2110 train_time:32710ms step_avg:38.71ms
step:846/2110 train_time:32770ms step_avg:38.73ms
step:847/2110 train_time:32829ms step_avg:38.76ms
step:848/2110 train_time:32887ms step_avg:38.78ms
step:849/2110 train_time:32947ms step_avg:38.81ms
step:850/2110 train_time:33005ms step_avg:38.83ms
step:851/2110 train_time:33064ms step_avg:38.85ms
step:852/2110 train_time:33122ms step_avg:38.88ms
step:853/2110 train_time:33182ms step_avg:38.90ms
step:854/2110 train_time:33241ms step_avg:38.92ms
step:855/2110 train_time:33301ms step_avg:38.95ms
step:856/2110 train_time:33359ms step_avg:38.97ms
step:857/2110 train_time:33419ms step_avg:39.00ms
step:858/2110 train_time:33478ms step_avg:39.02ms
step:859/2110 train_time:33539ms step_avg:39.04ms
step:860/2110 train_time:33597ms step_avg:39.07ms
step:861/2110 train_time:33658ms step_avg:39.09ms
step:862/2110 train_time:33716ms step_avg:39.11ms
step:863/2110 train_time:33775ms step_avg:39.14ms
step:864/2110 train_time:33834ms step_avg:39.16ms
step:865/2110 train_time:33895ms step_avg:39.18ms
step:866/2110 train_time:33954ms step_avg:39.21ms
step:867/2110 train_time:34013ms step_avg:39.23ms
step:868/2110 train_time:34072ms step_avg:39.25ms
step:869/2110 train_time:34131ms step_avg:39.28ms
step:870/2110 train_time:34190ms step_avg:39.30ms
step:871/2110 train_time:34250ms step_avg:39.32ms
step:872/2110 train_time:34310ms step_avg:39.35ms
step:873/2110 train_time:34370ms step_avg:39.37ms
step:874/2110 train_time:34429ms step_avg:39.39ms
step:875/2110 train_time:34488ms step_avg:39.42ms
step:876/2110 train_time:34547ms step_avg:39.44ms
step:877/2110 train_time:34607ms step_avg:39.46ms
step:878/2110 train_time:34666ms step_avg:39.48ms
step:879/2110 train_time:34725ms step_avg:39.51ms
step:880/2110 train_time:34783ms step_avg:39.53ms
step:881/2110 train_time:34843ms step_avg:39.55ms
step:882/2110 train_time:34902ms step_avg:39.57ms
step:883/2110 train_time:34962ms step_avg:39.59ms
step:884/2110 train_time:35020ms step_avg:39.62ms
step:885/2110 train_time:35080ms step_avg:39.64ms
step:886/2110 train_time:35139ms step_avg:39.66ms
step:887/2110 train_time:35198ms step_avg:39.68ms
step:888/2110 train_time:35257ms step_avg:39.70ms
step:889/2110 train_time:35317ms step_avg:39.73ms
step:890/2110 train_time:35376ms step_avg:39.75ms
step:891/2110 train_time:35437ms step_avg:39.77ms
step:892/2110 train_time:35496ms step_avg:39.79ms
step:893/2110 train_time:35556ms step_avg:39.82ms
step:894/2110 train_time:35616ms step_avg:39.84ms
step:895/2110 train_time:35674ms step_avg:39.86ms
step:896/2110 train_time:35733ms step_avg:39.88ms
step:897/2110 train_time:35793ms step_avg:39.90ms
step:898/2110 train_time:35852ms step_avg:39.92ms
step:899/2110 train_time:35911ms step_avg:39.95ms
step:900/2110 train_time:35969ms step_avg:39.97ms
step:901/2110 train_time:36030ms step_avg:39.99ms
step:902/2110 train_time:36089ms step_avg:40.01ms
step:903/2110 train_time:36148ms step_avg:40.03ms
step:904/2110 train_time:36206ms step_avg:40.05ms
step:905/2110 train_time:36265ms step_avg:40.07ms
step:906/2110 train_time:36323ms step_avg:40.09ms
step:907/2110 train_time:36383ms step_avg:40.11ms
step:908/2110 train_time:36441ms step_avg:40.13ms
step:909/2110 train_time:36502ms step_avg:40.16ms
step:910/2110 train_time:36561ms step_avg:40.18ms
step:911/2110 train_time:36621ms step_avg:40.20ms
step:912/2110 train_time:36679ms step_avg:40.22ms
step:913/2110 train_time:36740ms step_avg:40.24ms
step:914/2110 train_time:36798ms step_avg:40.26ms
step:915/2110 train_time:36858ms step_avg:40.28ms
step:916/2110 train_time:36917ms step_avg:40.30ms
step:917/2110 train_time:36978ms step_avg:40.32ms
step:918/2110 train_time:37037ms step_avg:40.35ms
step:919/2110 train_time:37097ms step_avg:40.37ms
step:920/2110 train_time:37155ms step_avg:40.39ms
step:921/2110 train_time:37215ms step_avg:40.41ms
step:922/2110 train_time:37275ms step_avg:40.43ms
step:923/2110 train_time:37333ms step_avg:40.45ms
step:924/2110 train_time:37392ms step_avg:40.47ms
step:925/2110 train_time:37452ms step_avg:40.49ms
step:926/2110 train_time:37513ms step_avg:40.51ms
step:927/2110 train_time:37572ms step_avg:40.53ms
step:928/2110 train_time:37631ms step_avg:40.55ms
step:929/2110 train_time:37691ms step_avg:40.57ms
step:930/2110 train_time:37759ms step_avg:40.60ms
step:931/2110 train_time:37809ms step_avg:40.61ms
step:932/2110 train_time:37868ms step_avg:40.63ms
step:933/2110 train_time:37927ms step_avg:40.65ms
step:934/2110 train_time:37985ms step_avg:40.67ms
step:935/2110 train_time:38046ms step_avg:40.69ms
step:936/2110 train_time:38104ms step_avg:40.71ms
step:937/2110 train_time:38163ms step_avg:40.73ms
step:938/2110 train_time:38221ms step_avg:40.75ms
step:939/2110 train_time:38282ms step_avg:40.77ms
step:940/2110 train_time:38340ms step_avg:40.79ms
step:941/2110 train_time:38401ms step_avg:40.81ms
step:942/2110 train_time:38459ms step_avg:40.83ms
step:943/2110 train_time:38520ms step_avg:40.85ms
step:944/2110 train_time:38579ms step_avg:40.87ms
step:945/2110 train_time:38639ms step_avg:40.89ms
step:946/2110 train_time:38698ms step_avg:40.91ms
step:947/2110 train_time:38758ms step_avg:40.93ms
step:948/2110 train_time:38816ms step_avg:40.95ms
step:949/2110 train_time:38876ms step_avg:40.97ms
step:950/2110 train_time:38935ms step_avg:40.98ms
step:951/2110 train_time:38994ms step_avg:41.00ms
step:952/2110 train_time:39053ms step_avg:41.02ms
step:953/2110 train_time:39113ms step_avg:41.04ms
step:954/2110 train_time:39172ms step_avg:41.06ms
step:955/2110 train_time:39232ms step_avg:41.08ms
step:956/2110 train_time:39291ms step_avg:41.10ms
step:957/2110 train_time:39351ms step_avg:41.12ms
step:958/2110 train_time:39411ms step_avg:41.14ms
step:959/2110 train_time:39471ms step_avg:41.16ms
step:960/2110 train_time:39530ms step_avg:41.18ms
step:961/2110 train_time:39590ms step_avg:41.20ms
step:962/2110 train_time:39648ms step_avg:41.21ms
step:963/2110 train_time:39708ms step_avg:41.23ms
step:964/2110 train_time:39766ms step_avg:41.25ms
step:965/2110 train_time:39825ms step_avg:41.27ms
step:966/2110 train_time:39883ms step_avg:41.29ms
step:967/2110 train_time:39943ms step_avg:41.31ms
step:968/2110 train_time:40002ms step_avg:41.32ms
step:969/2110 train_time:40062ms step_avg:41.34ms
step:970/2110 train_time:40121ms step_avg:41.36ms
step:971/2110 train_time:40181ms step_avg:41.38ms
step:972/2110 train_time:40240ms step_avg:41.40ms
step:973/2110 train_time:40301ms step_avg:41.42ms
step:974/2110 train_time:40359ms step_avg:41.44ms
step:975/2110 train_time:40419ms step_avg:41.46ms
step:976/2110 train_time:40478ms step_avg:41.47ms
step:977/2110 train_time:40539ms step_avg:41.49ms
step:978/2110 train_time:40598ms step_avg:41.51ms
step:979/2110 train_time:40658ms step_avg:41.53ms
step:980/2110 train_time:40716ms step_avg:41.55ms
step:981/2110 train_time:40776ms step_avg:41.57ms
step:982/2110 train_time:40835ms step_avg:41.58ms
step:983/2110 train_time:40895ms step_avg:41.60ms
step:984/2110 train_time:40954ms step_avg:41.62ms
step:985/2110 train_time:41013ms step_avg:41.64ms
step:986/2110 train_time:41072ms step_avg:41.66ms
step:987/2110 train_time:41132ms step_avg:41.67ms
step:988/2110 train_time:41192ms step_avg:41.69ms
step:989/2110 train_time:41252ms step_avg:41.71ms
step:990/2110 train_time:41311ms step_avg:41.73ms
step:991/2110 train_time:41370ms step_avg:41.75ms
step:992/2110 train_time:41429ms step_avg:41.76ms
step:993/2110 train_time:41488ms step_avg:41.78ms
step:994/2110 train_time:41547ms step_avg:41.80ms
step:995/2110 train_time:41606ms step_avg:41.81ms
step:996/2110 train_time:41665ms step_avg:41.83ms
step:997/2110 train_time:41724ms step_avg:41.85ms
step:998/2110 train_time:41782ms step_avg:41.87ms
step:999/2110 train_time:41842ms step_avg:41.88ms
step:1000/2110 train_time:41902ms step_avg:41.90ms
step:1000/2110 val_loss:3.7592 train_time:41962ms step_avg:41.96ms
step:1001/2110 train_time:41993ms step_avg:41.95ms
step:1002/2110 train_time:42024ms step_avg:41.94ms
step:1003/2110 train_time:42088ms step_avg:41.96ms
step:1004/2110 train_time:42152ms step_avg:41.98ms
step:1005/2110 train_time:42212ms step_avg:42.00ms
step:1006/2110 train_time:42271ms step_avg:42.02ms
step:1007/2110 train_time:42330ms step_avg:42.04ms
step:1008/2110 train_time:42388ms step_avg:42.05ms
step:1009/2110 train_time:42448ms step_avg:42.07ms
step:1010/2110 train_time:42507ms step_avg:42.09ms
step:1011/2110 train_time:42565ms step_avg:42.10ms
step:1012/2110 train_time:42624ms step_avg:42.12ms
step:1013/2110 train_time:42682ms step_avg:42.13ms
step:1014/2110 train_time:42741ms step_avg:42.15ms
step:1015/2110 train_time:42799ms step_avg:42.17ms
step:1016/2110 train_time:42857ms step_avg:42.18ms
step:1017/2110 train_time:42916ms step_avg:42.20ms
step:1018/2110 train_time:42976ms step_avg:42.22ms
step:1019/2110 train_time:43037ms step_avg:42.23ms
step:1020/2110 train_time:43098ms step_avg:42.25ms
step:1021/2110 train_time:43159ms step_avg:42.27ms
step:1022/2110 train_time:43217ms step_avg:42.29ms
step:1023/2110 train_time:43278ms step_avg:42.30ms
step:1024/2110 train_time:43337ms step_avg:42.32ms
step:1025/2110 train_time:43397ms step_avg:42.34ms
step:1026/2110 train_time:43456ms step_avg:42.35ms
step:1027/2110 train_time:43514ms step_avg:42.37ms
step:1028/2110 train_time:43572ms step_avg:42.39ms
step:1029/2110 train_time:43632ms step_avg:42.40ms
step:1030/2110 train_time:43690ms step_avg:42.42ms
step:1031/2110 train_time:43749ms step_avg:42.43ms
step:1032/2110 train_time:43807ms step_avg:42.45ms
step:1033/2110 train_time:43867ms step_avg:42.47ms
step:1034/2110 train_time:43928ms step_avg:42.48ms
step:1035/2110 train_time:43988ms step_avg:42.50ms
step:1036/2110 train_time:44049ms step_avg:42.52ms
step:1037/2110 train_time:44109ms step_avg:42.54ms
step:1038/2110 train_time:44175ms step_avg:42.56ms
step:1039/2110 train_time:44229ms step_avg:42.57ms
step:1040/2110 train_time:44288ms step_avg:42.58ms
step:1041/2110 train_time:44348ms step_avg:42.60ms
step:1042/2110 train_time:44408ms step_avg:42.62ms
step:1043/2110 train_time:44467ms step_avg:42.63ms
step:1044/2110 train_time:44526ms step_avg:42.65ms
step:1045/2110 train_time:44585ms step_avg:42.67ms
step:1046/2110 train_time:44644ms step_avg:42.68ms
step:1047/2110 train_time:44702ms step_avg:42.70ms
step:1048/2110 train_time:44761ms step_avg:42.71ms
step:1049/2110 train_time:44819ms step_avg:42.73ms
step:1050/2110 train_time:44878ms step_avg:42.74ms
step:1051/2110 train_time:44937ms step_avg:42.76ms
step:1052/2110 train_time:44996ms step_avg:42.77ms
step:1053/2110 train_time:45057ms step_avg:42.79ms
step:1054/2110 train_time:45115ms step_avg:42.80ms
step:1055/2110 train_time:45176ms step_avg:42.82ms
step:1056/2110 train_time:45234ms step_avg:42.84ms
step:1057/2110 train_time:45295ms step_avg:42.85ms
step:1058/2110 train_time:45354ms step_avg:42.87ms
step:1059/2110 train_time:45414ms step_avg:42.88ms
step:1060/2110 train_time:45473ms step_avg:42.90ms
step:1061/2110 train_time:45533ms step_avg:42.92ms
step:1062/2110 train_time:45592ms step_avg:42.93ms
step:1063/2110 train_time:45651ms step_avg:42.95ms
step:1064/2110 train_time:45709ms step_avg:42.96ms
step:1065/2110 train_time:45769ms step_avg:42.98ms
step:1066/2110 train_time:45828ms step_avg:42.99ms
step:1067/2110 train_time:45887ms step_avg:43.01ms
step:1068/2110 train_time:45947ms step_avg:43.02ms
step:1069/2110 train_time:46008ms step_avg:43.04ms
step:1070/2110 train_time:46067ms step_avg:43.05ms
step:1071/2110 train_time:46128ms step_avg:43.07ms
step:1072/2110 train_time:46187ms step_avg:43.09ms
step:1073/2110 train_time:46248ms step_avg:43.10ms
step:1074/2110 train_time:46308ms step_avg:43.12ms
step:1075/2110 train_time:46368ms step_avg:43.13ms
step:1076/2110 train_time:46426ms step_avg:43.15ms
step:1077/2110 train_time:46486ms step_avg:43.16ms
step:1078/2110 train_time:46545ms step_avg:43.18ms
step:1079/2110 train_time:46605ms step_avg:43.19ms
step:1080/2110 train_time:46663ms step_avg:43.21ms
step:1081/2110 train_time:46722ms step_avg:43.22ms
step:1082/2110 train_time:46781ms step_avg:43.24ms
step:1083/2110 train_time:46840ms step_avg:43.25ms
step:1084/2110 train_time:46899ms step_avg:43.27ms
step:1085/2110 train_time:46958ms step_avg:43.28ms
step:1086/2110 train_time:47017ms step_avg:43.29ms
step:1087/2110 train_time:47076ms step_avg:43.31ms
step:1088/2110 train_time:47135ms step_avg:43.32ms
step:1089/2110 train_time:47196ms step_avg:43.34ms
step:1090/2110 train_time:47255ms step_avg:43.35ms
step:1091/2110 train_time:47316ms step_avg:43.37ms
step:1092/2110 train_time:47374ms step_avg:43.38ms
step:1093/2110 train_time:47435ms step_avg:43.40ms
step:1094/2110 train_time:47493ms step_avg:43.41ms
step:1095/2110 train_time:47554ms step_avg:43.43ms
step:1096/2110 train_time:47612ms step_avg:43.44ms
step:1097/2110 train_time:47672ms step_avg:43.46ms
step:1098/2110 train_time:47730ms step_avg:43.47ms
step:1099/2110 train_time:47791ms step_avg:43.49ms
step:1100/2110 train_time:47849ms step_avg:43.50ms
step:1101/2110 train_time:47909ms step_avg:43.51ms
step:1102/2110 train_time:47968ms step_avg:43.53ms
step:1103/2110 train_time:48027ms step_avg:43.54ms
step:1104/2110 train_time:48087ms step_avg:43.56ms
step:1105/2110 train_time:48148ms step_avg:43.57ms
step:1106/2110 train_time:48207ms step_avg:43.59ms
step:1107/2110 train_time:48268ms step_avg:43.60ms
step:1108/2110 train_time:48327ms step_avg:43.62ms
step:1109/2110 train_time:48387ms step_avg:43.63ms
step:1110/2110 train_time:48446ms step_avg:43.65ms
step:1111/2110 train_time:48506ms step_avg:43.66ms
step:1112/2110 train_time:48565ms step_avg:43.67ms
step:1113/2110 train_time:48623ms step_avg:43.69ms
step:1114/2110 train_time:48682ms step_avg:43.70ms
step:1115/2110 train_time:48741ms step_avg:43.71ms
step:1116/2110 train_time:48799ms step_avg:43.73ms
step:1117/2110 train_time:48859ms step_avg:43.74ms
step:1118/2110 train_time:48917ms step_avg:43.75ms
step:1119/2110 train_time:48977ms step_avg:43.77ms
step:1120/2110 train_time:49035ms step_avg:43.78ms
step:1121/2110 train_time:49095ms step_avg:43.80ms
step:1122/2110 train_time:49154ms step_avg:43.81ms
step:1123/2110 train_time:49215ms step_avg:43.82ms
step:1124/2110 train_time:49274ms step_avg:43.84ms
step:1125/2110 train_time:49335ms step_avg:43.85ms
step:1126/2110 train_time:49394ms step_avg:43.87ms
step:1127/2110 train_time:49454ms step_avg:43.88ms
step:1128/2110 train_time:49512ms step_avg:43.89ms
step:1129/2110 train_time:49573ms step_avg:43.91ms
step:1130/2110 train_time:49632ms step_avg:43.92ms
step:1131/2110 train_time:49692ms step_avg:43.94ms
step:1132/2110 train_time:49750ms step_avg:43.95ms
step:1133/2110 train_time:49810ms step_avg:43.96ms
step:1134/2110 train_time:49869ms step_avg:43.98ms
step:1135/2110 train_time:49929ms step_avg:43.99ms
step:1136/2110 train_time:49988ms step_avg:44.00ms
step:1137/2110 train_time:50049ms step_avg:44.02ms
step:1138/2110 train_time:50108ms step_avg:44.03ms
step:1139/2110 train_time:50169ms step_avg:44.05ms
step:1140/2110 train_time:50229ms step_avg:44.06ms
step:1141/2110 train_time:50291ms step_avg:44.08ms
step:1142/2110 train_time:50351ms step_avg:44.09ms
step:1143/2110 train_time:50411ms step_avg:44.10ms
step:1144/2110 train_time:50470ms step_avg:44.12ms
step:1145/2110 train_time:50530ms step_avg:44.13ms
step:1146/2110 train_time:50589ms step_avg:44.14ms
step:1147/2110 train_time:50651ms step_avg:44.16ms
step:1148/2110 train_time:50710ms step_avg:44.17ms
step:1149/2110 train_time:50770ms step_avg:44.19ms
step:1150/2110 train_time:50829ms step_avg:44.20ms
step:1151/2110 train_time:50889ms step_avg:44.21ms
step:1152/2110 train_time:50949ms step_avg:44.23ms
step:1153/2110 train_time:51011ms step_avg:44.24ms
step:1154/2110 train_time:51070ms step_avg:44.25ms
step:1155/2110 train_time:51131ms step_avg:44.27ms
step:1156/2110 train_time:51191ms step_avg:44.28ms
step:1157/2110 train_time:51252ms step_avg:44.30ms
step:1158/2110 train_time:51310ms step_avg:44.31ms
step:1159/2110 train_time:51371ms step_avg:44.32ms
step:1160/2110 train_time:51430ms step_avg:44.34ms
step:1161/2110 train_time:51491ms step_avg:44.35ms
step:1162/2110 train_time:51550ms step_avg:44.36ms
step:1163/2110 train_time:51610ms step_avg:44.38ms
step:1164/2110 train_time:51669ms step_avg:44.39ms
step:1165/2110 train_time:51730ms step_avg:44.40ms
step:1166/2110 train_time:51789ms step_avg:44.42ms
step:1167/2110 train_time:51850ms step_avg:44.43ms
step:1168/2110 train_time:51910ms step_avg:44.44ms
step:1169/2110 train_time:51970ms step_avg:44.46ms
step:1170/2110 train_time:52029ms step_avg:44.47ms
step:1171/2110 train_time:52090ms step_avg:44.48ms
step:1172/2110 train_time:52150ms step_avg:44.50ms
step:1173/2110 train_time:52211ms step_avg:44.51ms
step:1174/2110 train_time:52270ms step_avg:44.52ms
step:1175/2110 train_time:52331ms step_avg:44.54ms
step:1176/2110 train_time:52390ms step_avg:44.55ms
step:1177/2110 train_time:52451ms step_avg:44.56ms
step:1178/2110 train_time:52510ms step_avg:44.58ms
step:1179/2110 train_time:52571ms step_avg:44.59ms
step:1180/2110 train_time:52630ms step_avg:44.60ms
step:1181/2110 train_time:52691ms step_avg:44.62ms
step:1182/2110 train_time:52750ms step_avg:44.63ms
step:1183/2110 train_time:52811ms step_avg:44.64ms
step:1184/2110 train_time:52870ms step_avg:44.65ms
step:1185/2110 train_time:52930ms step_avg:44.67ms
step:1186/2110 train_time:52990ms step_avg:44.68ms
step:1187/2110 train_time:53052ms step_avg:44.69ms
step:1188/2110 train_time:53110ms step_avg:44.71ms
step:1189/2110 train_time:53171ms step_avg:44.72ms
step:1190/2110 train_time:53230ms step_avg:44.73ms
step:1191/2110 train_time:53291ms step_avg:44.74ms
step:1192/2110 train_time:53350ms step_avg:44.76ms
step:1193/2110 train_time:53411ms step_avg:44.77ms
step:1194/2110 train_time:53470ms step_avg:44.78ms
step:1195/2110 train_time:53530ms step_avg:44.80ms
step:1196/2110 train_time:53590ms step_avg:44.81ms
step:1197/2110 train_time:53650ms step_avg:44.82ms
step:1198/2110 train_time:53710ms step_avg:44.83ms
step:1199/2110 train_time:53770ms step_avg:44.85ms
step:1200/2110 train_time:53830ms step_avg:44.86ms
step:1201/2110 train_time:53890ms step_avg:44.87ms
step:1202/2110 train_time:53950ms step_avg:44.88ms
step:1203/2110 train_time:54011ms step_avg:44.90ms
step:1204/2110 train_time:54070ms step_avg:44.91ms
step:1205/2110 train_time:54131ms step_avg:44.92ms
step:1206/2110 train_time:54190ms step_avg:44.93ms
step:1207/2110 train_time:54250ms step_avg:44.95ms
step:1208/2110 train_time:54309ms step_avg:44.96ms
step:1209/2110 train_time:54370ms step_avg:44.97ms
step:1210/2110 train_time:54429ms step_avg:44.98ms
step:1211/2110 train_time:54489ms step_avg:45.00ms
step:1212/2110 train_time:54549ms step_avg:45.01ms
step:1213/2110 train_time:54610ms step_avg:45.02ms
step:1214/2110 train_time:54670ms step_avg:45.03ms
step:1215/2110 train_time:54730ms step_avg:45.05ms
step:1216/2110 train_time:54789ms step_avg:45.06ms
step:1217/2110 train_time:54850ms step_avg:45.07ms
step:1218/2110 train_time:54910ms step_avg:45.08ms
step:1219/2110 train_time:54970ms step_avg:45.09ms
step:1220/2110 train_time:55030ms step_avg:45.11ms
step:1221/2110 train_time:55090ms step_avg:45.12ms
step:1222/2110 train_time:55151ms step_avg:45.13ms
step:1223/2110 train_time:55211ms step_avg:45.14ms
step:1224/2110 train_time:55270ms step_avg:45.16ms
step:1225/2110 train_time:55331ms step_avg:45.17ms
step:1226/2110 train_time:55390ms step_avg:45.18ms
step:1227/2110 train_time:55451ms step_avg:45.19ms
step:1228/2110 train_time:55510ms step_avg:45.20ms
step:1229/2110 train_time:55571ms step_avg:45.22ms
step:1230/2110 train_time:55630ms step_avg:45.23ms
step:1231/2110 train_time:55690ms step_avg:45.24ms
step:1232/2110 train_time:55750ms step_avg:45.25ms
step:1233/2110 train_time:55809ms step_avg:45.26ms
step:1234/2110 train_time:55868ms step_avg:45.27ms
step:1235/2110 train_time:55929ms step_avg:45.29ms
step:1236/2110 train_time:55988ms step_avg:45.30ms
step:1237/2110 train_time:56050ms step_avg:45.31ms
step:1238/2110 train_time:56109ms step_avg:45.32ms
step:1239/2110 train_time:56169ms step_avg:45.33ms
step:1240/2110 train_time:56230ms step_avg:45.35ms
step:1241/2110 train_time:56290ms step_avg:45.36ms
step:1242/2110 train_time:56350ms step_avg:45.37ms
step:1243/2110 train_time:56410ms step_avg:45.38ms
step:1244/2110 train_time:56469ms step_avg:45.39ms
step:1245/2110 train_time:56531ms step_avg:45.41ms
step:1246/2110 train_time:56589ms step_avg:45.42ms
step:1247/2110 train_time:56650ms step_avg:45.43ms
step:1248/2110 train_time:56709ms step_avg:45.44ms
step:1249/2110 train_time:56770ms step_avg:45.45ms
step:1250/2110 train_time:56829ms step_avg:45.46ms
step:1250/2110 val_loss:3.5942 train_time:56891ms step_avg:45.51ms
step:1251/2110 train_time:56919ms step_avg:45.50ms
step:1252/2110 train_time:56953ms step_avg:45.49ms
step:1253/2110 train_time:57018ms step_avg:45.51ms
step:1254/2110 train_time:57080ms step_avg:45.52ms
step:1255/2110 train_time:57140ms step_avg:45.53ms
step:1256/2110 train_time:57200ms step_avg:45.54ms
step:1257/2110 train_time:57260ms step_avg:45.55ms
step:1258/2110 train_time:57319ms step_avg:45.56ms
step:1259/2110 train_time:57377ms step_avg:45.57ms
step:1260/2110 train_time:57435ms step_avg:45.58ms
step:1261/2110 train_time:57494ms step_avg:45.59ms
step:1262/2110 train_time:57553ms step_avg:45.60ms
step:1263/2110 train_time:57612ms step_avg:45.61ms
step:1264/2110 train_time:57669ms step_avg:45.62ms
step:1265/2110 train_time:57729ms step_avg:45.64ms
step:1266/2110 train_time:57788ms step_avg:45.65ms
step:1267/2110 train_time:57848ms step_avg:45.66ms
step:1268/2110 train_time:57908ms step_avg:45.67ms
step:1269/2110 train_time:57973ms step_avg:45.68ms
step:1270/2110 train_time:58034ms step_avg:45.70ms
step:1271/2110 train_time:58096ms step_avg:45.71ms
step:1272/2110 train_time:58156ms step_avg:45.72ms
step:1273/2110 train_time:58217ms step_avg:45.73ms
step:1274/2110 train_time:58276ms step_avg:45.74ms
step:1275/2110 train_time:58336ms step_avg:45.75ms
step:1276/2110 train_time:58395ms step_avg:45.76ms
step:1277/2110 train_time:58454ms step_avg:45.77ms
step:1278/2110 train_time:58512ms step_avg:45.78ms
step:1279/2110 train_time:58572ms step_avg:45.80ms
step:1280/2110 train_time:58630ms step_avg:45.80ms
step:1281/2110 train_time:58690ms step_avg:45.82ms
step:1282/2110 train_time:58749ms step_avg:45.83ms
step:1283/2110 train_time:58809ms step_avg:45.84ms
step:1284/2110 train_time:58868ms step_avg:45.85ms
step:1285/2110 train_time:58930ms step_avg:45.86ms
step:1286/2110 train_time:58991ms step_avg:45.87ms
step:1287/2110 train_time:59053ms step_avg:45.88ms
step:1288/2110 train_time:59113ms step_avg:45.90ms
step:1289/2110 train_time:59175ms step_avg:45.91ms
step:1290/2110 train_time:59234ms step_avg:45.92ms
step:1291/2110 train_time:59295ms step_avg:45.93ms
step:1292/2110 train_time:59353ms step_avg:45.94ms
step:1293/2110 train_time:59413ms step_avg:45.95ms
step:1294/2110 train_time:59472ms step_avg:45.96ms
step:1295/2110 train_time:59532ms step_avg:45.97ms
step:1296/2110 train_time:59589ms step_avg:45.98ms
step:1297/2110 train_time:59649ms step_avg:45.99ms
step:1298/2110 train_time:59712ms step_avg:46.00ms
step:1299/2110 train_time:59768ms step_avg:46.01ms
step:1300/2110 train_time:59826ms step_avg:46.02ms
step:1301/2110 train_time:59887ms step_avg:46.03ms
step:1302/2110 train_time:59951ms step_avg:46.05ms
step:1303/2110 train_time:60010ms step_avg:46.05ms
step:1304/2110 train_time:60069ms step_avg:46.07ms
step:1305/2110 train_time:60131ms step_avg:46.08ms
step:1306/2110 train_time:60191ms step_avg:46.09ms
step:1307/2110 train_time:60253ms step_avg:46.10ms
step:1308/2110 train_time:60311ms step_avg:46.11ms
step:1309/2110 train_time:60370ms step_avg:46.12ms
step:1310/2110 train_time:60429ms step_avg:46.13ms
step:1311/2110 train_time:60492ms step_avg:46.14ms
step:1312/2110 train_time:60552ms step_avg:46.15ms
step:1313/2110 train_time:60610ms step_avg:46.16ms
step:1314/2110 train_time:60670ms step_avg:46.17ms
step:1315/2110 train_time:60728ms step_avg:46.18ms
step:1316/2110 train_time:60790ms step_avg:46.19ms
step:1317/2110 train_time:60848ms step_avg:46.20ms
step:1318/2110 train_time:60909ms step_avg:46.21ms
step:1319/2110 train_time:60968ms step_avg:46.22ms
step:1320/2110 train_time:61030ms step_avg:46.23ms
step:1321/2110 train_time:61089ms step_avg:46.24ms
step:1322/2110 train_time:61150ms step_avg:46.26ms
step:1323/2110 train_time:61210ms step_avg:46.27ms
step:1324/2110 train_time:61271ms step_avg:46.28ms
step:1325/2110 train_time:61330ms step_avg:46.29ms
step:1326/2110 train_time:61391ms step_avg:46.30ms
step:1327/2110 train_time:61449ms step_avg:46.31ms
step:1328/2110 train_time:61510ms step_avg:46.32ms
step:1329/2110 train_time:61569ms step_avg:46.33ms
step:1330/2110 train_time:61630ms step_avg:46.34ms
step:1331/2110 train_time:61688ms step_avg:46.35ms
step:1332/2110 train_time:61748ms step_avg:46.36ms
step:1333/2110 train_time:61806ms step_avg:46.37ms
step:1334/2110 train_time:61867ms step_avg:46.38ms
step:1335/2110 train_time:61925ms step_avg:46.39ms
step:1336/2110 train_time:61987ms step_avg:46.40ms
step:1337/2110 train_time:62046ms step_avg:46.41ms
step:1338/2110 train_time:62107ms step_avg:46.42ms
step:1339/2110 train_time:62166ms step_avg:46.43ms
step:1340/2110 train_time:62228ms step_avg:46.44ms
step:1341/2110 train_time:62287ms step_avg:46.45ms
step:1342/2110 train_time:62349ms step_avg:46.46ms
step:1343/2110 train_time:62407ms step_avg:46.47ms
step:1344/2110 train_time:62469ms step_avg:46.48ms
step:1345/2110 train_time:62527ms step_avg:46.49ms
step:1346/2110 train_time:62588ms step_avg:46.50ms
step:1347/2110 train_time:62645ms step_avg:46.51ms
step:1348/2110 train_time:62707ms step_avg:46.52ms
step:1349/2110 train_time:62764ms step_avg:46.53ms
step:1350/2110 train_time:62824ms step_avg:46.54ms
step:1351/2110 train_time:62883ms step_avg:46.55ms
step:1352/2110 train_time:62944ms step_avg:46.56ms
step:1353/2110 train_time:63003ms step_avg:46.57ms
step:1354/2110 train_time:63063ms step_avg:46.58ms
step:1355/2110 train_time:63123ms step_avg:46.58ms
step:1356/2110 train_time:63184ms step_avg:46.60ms
step:1357/2110 train_time:63243ms step_avg:46.60ms
step:1358/2110 train_time:63304ms step_avg:46.62ms
step:1359/2110 train_time:63362ms step_avg:46.62ms
step:1360/2110 train_time:63422ms step_avg:46.63ms
step:1361/2110 train_time:63481ms step_avg:46.64ms
step:1362/2110 train_time:63541ms step_avg:46.65ms
step:1363/2110 train_time:63600ms step_avg:46.66ms
step:1364/2110 train_time:63660ms step_avg:46.67ms
step:1365/2110 train_time:63717ms step_avg:46.68ms
step:1366/2110 train_time:63775ms step_avg:46.69ms
step:1367/2110 train_time:63835ms step_avg:46.70ms
step:1368/2110 train_time:63894ms step_avg:46.71ms
step:1369/2110 train_time:63955ms step_avg:46.72ms
step:1370/2110 train_time:64014ms step_avg:46.73ms
step:1371/2110 train_time:64076ms step_avg:46.74ms
step:1372/2110 train_time:64134ms step_avg:46.75ms
step:1373/2110 train_time:64195ms step_avg:46.76ms
step:1374/2110 train_time:64254ms step_avg:46.76ms
step:1375/2110 train_time:64315ms step_avg:46.77ms
step:1376/2110 train_time:64374ms step_avg:46.78ms
step:1377/2110 train_time:64435ms step_avg:46.79ms
step:1378/2110 train_time:64494ms step_avg:46.80ms
step:1379/2110 train_time:64555ms step_avg:46.81ms
step:1380/2110 train_time:64614ms step_avg:46.82ms
step:1381/2110 train_time:64675ms step_avg:46.83ms
step:1382/2110 train_time:64760ms step_avg:46.86ms
step:1383/2110 train_time:64847ms step_avg:46.89ms
step:1384/2110 train_time:64934ms step_avg:46.92ms
step:1385/2110 train_time:65021ms step_avg:46.95ms
step:1386/2110 train_time:65107ms step_avg:46.97ms
step:1387/2110 train_time:65195ms step_avg:47.00ms
step:1388/2110 train_time:65280ms step_avg:47.03ms
step:1389/2110 train_time:65369ms step_avg:47.06ms
step:1390/2110 train_time:65454ms step_avg:47.09ms
step:1391/2110 train_time:65542ms step_avg:47.12ms
step:1392/2110 train_time:65627ms step_avg:47.15ms
step:1393/2110 train_time:65714ms step_avg:47.17ms
step:1394/2110 train_time:65800ms step_avg:47.20ms
step:1395/2110 train_time:65888ms step_avg:47.23ms
step:1396/2110 train_time:65973ms step_avg:47.26ms
step:1397/2110 train_time:66060ms step_avg:47.29ms
step:1398/2110 train_time:66146ms step_avg:47.31ms
step:1399/2110 train_time:66233ms step_avg:47.34ms
step:1400/2110 train_time:66321ms step_avg:47.37ms
step:1401/2110 train_time:66408ms step_avg:47.40ms
step:1402/2110 train_time:66495ms step_avg:47.43ms
step:1403/2110 train_time:66582ms step_avg:47.46ms
step:1404/2110 train_time:66668ms step_avg:47.48ms
step:1405/2110 train_time:66756ms step_avg:47.51ms
step:1406/2110 train_time:66841ms step_avg:47.54ms
step:1407/2110 train_time:66928ms step_avg:47.57ms
step:1408/2110 train_time:67015ms step_avg:47.60ms
step:1409/2110 train_time:67101ms step_avg:47.62ms
step:1410/2110 train_time:67187ms step_avg:47.65ms
step:1411/2110 train_time:67274ms step_avg:47.68ms
step:1412/2110 train_time:67360ms step_avg:47.71ms
step:1413/2110 train_time:67449ms step_avg:47.73ms
step:1414/2110 train_time:67535ms step_avg:47.76ms
step:1415/2110 train_time:67622ms step_avg:47.79ms
step:1416/2110 train_time:67708ms step_avg:47.82ms
step:1417/2110 train_time:67795ms step_avg:47.84ms
step:1418/2110 train_time:67881ms step_avg:47.87ms
step:1419/2110 train_time:67969ms step_avg:47.90ms
step:1420/2110 train_time:68055ms step_avg:47.93ms
step:1421/2110 train_time:68142ms step_avg:47.95ms
step:1422/2110 train_time:68228ms step_avg:47.98ms
step:1423/2110 train_time:68315ms step_avg:48.01ms
step:1424/2110 train_time:68401ms step_avg:48.03ms
step:1425/2110 train_time:68489ms step_avg:48.06ms
step:1426/2110 train_time:68574ms step_avg:48.09ms
step:1427/2110 train_time:68662ms step_avg:48.12ms
step:1428/2110 train_time:68748ms step_avg:48.14ms
step:1429/2110 train_time:68835ms step_avg:48.17ms
step:1430/2110 train_time:68921ms step_avg:48.20ms
step:1431/2110 train_time:69008ms step_avg:48.22ms
step:1432/2110 train_time:69095ms step_avg:48.25ms
step:1433/2110 train_time:69182ms step_avg:48.28ms
step:1434/2110 train_time:69267ms step_avg:48.30ms
step:1435/2110 train_time:69355ms step_avg:48.33ms
step:1436/2110 train_time:69442ms step_avg:48.36ms
step:1437/2110 train_time:69528ms step_avg:48.38ms
step:1438/2110 train_time:69615ms step_avg:48.41ms
step:1439/2110 train_time:69703ms step_avg:48.44ms
step:1440/2110 train_time:69788ms step_avg:48.46ms
step:1441/2110 train_time:69875ms step_avg:48.49ms
step:1442/2110 train_time:69961ms step_avg:48.52ms
step:1443/2110 train_time:70049ms step_avg:48.54ms
step:1444/2110 train_time:70135ms step_avg:48.57ms
step:1445/2110 train_time:70222ms step_avg:48.60ms
step:1446/2110 train_time:70307ms step_avg:48.62ms
step:1447/2110 train_time:70395ms step_avg:48.65ms
step:1448/2110 train_time:70481ms step_avg:48.67ms
step:1449/2110 train_time:70568ms step_avg:48.70ms
step:1450/2110 train_time:70655ms step_avg:48.73ms
step:1451/2110 train_time:70742ms step_avg:48.75ms
step:1452/2110 train_time:70828ms step_avg:48.78ms
step:1453/2110 train_time:70914ms step_avg:48.81ms
step:1454/2110 train_time:71000ms step_avg:48.83ms
step:1455/2110 train_time:71088ms step_avg:48.86ms
step:1456/2110 train_time:71174ms step_avg:48.88ms
step:1457/2110 train_time:71261ms step_avg:48.91ms
step:1458/2110 train_time:71346ms step_avg:48.93ms
step:1459/2110 train_time:71434ms step_avg:48.96ms
step:1460/2110 train_time:71520ms step_avg:48.99ms
step:1461/2110 train_time:71608ms step_avg:49.01ms
step:1462/2110 train_time:71694ms step_avg:49.04ms
step:1463/2110 train_time:71780ms step_avg:49.06ms
step:1464/2110 train_time:71866ms step_avg:49.09ms
step:1465/2110 train_time:71954ms step_avg:49.12ms
step:1466/2110 train_time:72041ms step_avg:49.14ms
step:1467/2110 train_time:72128ms step_avg:49.17ms
step:1468/2110 train_time:72214ms step_avg:49.19ms
step:1469/2110 train_time:72300ms step_avg:49.22ms
step:1470/2110 train_time:72386ms step_avg:49.24ms
step:1471/2110 train_time:72474ms step_avg:49.27ms
step:1472/2110 train_time:72559ms step_avg:49.29ms
step:1473/2110 train_time:72646ms step_avg:49.32ms
step:1474/2110 train_time:72733ms step_avg:49.34ms
step:1475/2110 train_time:72820ms step_avg:49.37ms
step:1476/2110 train_time:72906ms step_avg:49.39ms
step:1477/2110 train_time:72993ms step_avg:49.42ms
step:1478/2110 train_time:73079ms step_avg:49.44ms
step:1479/2110 train_time:73165ms step_avg:49.47ms
step:1480/2110 train_time:73252ms step_avg:49.49ms
step:1481/2110 train_time:73339ms step_avg:49.52ms
step:1482/2110 train_time:73426ms step_avg:49.54ms
step:1483/2110 train_time:73513ms step_avg:49.57ms
step:1484/2110 train_time:73599ms step_avg:49.60ms
step:1485/2110 train_time:73686ms step_avg:49.62ms
step:1486/2110 train_time:73771ms step_avg:49.64ms
step:1487/2110 train_time:73860ms step_avg:49.67ms
step:1488/2110 train_time:73945ms step_avg:49.69ms
step:1489/2110 train_time:74033ms step_avg:49.72ms
step:1490/2110 train_time:74120ms step_avg:49.75ms
step:1491/2110 train_time:74207ms step_avg:49.77ms
step:1492/2110 train_time:74293ms step_avg:49.79ms
step:1493/2110 train_time:74381ms step_avg:49.82ms
step:1494/2110 train_time:74467ms step_avg:49.84ms
step:1495/2110 train_time:74554ms step_avg:49.87ms
step:1496/2110 train_time:74640ms step_avg:49.89ms
step:1497/2110 train_time:74728ms step_avg:49.92ms
step:1498/2110 train_time:74814ms step_avg:49.94ms
step:1499/2110 train_time:74901ms step_avg:49.97ms
step:1500/2110 train_time:74986ms step_avg:49.99ms
step:1500/2110 val_loss:3.4969 train_time:75075ms step_avg:50.05ms
step:1501/2110 train_time:75101ms step_avg:50.03ms
step:1502/2110 train_time:75164ms step_avg:50.04ms
step:1503/2110 train_time:75258ms step_avg:50.07ms
step:1504/2110 train_time:75347ms step_avg:50.10ms
step:1505/2110 train_time:75433ms step_avg:50.12ms
step:1506/2110 train_time:75518ms step_avg:50.14ms
step:1507/2110 train_time:75605ms step_avg:50.17ms
step:1508/2110 train_time:75690ms step_avg:50.19ms
step:1509/2110 train_time:75776ms step_avg:50.22ms
step:1510/2110 train_time:75863ms step_avg:50.24ms
step:1511/2110 train_time:75948ms step_avg:50.26ms
step:1512/2110 train_time:76034ms step_avg:50.29ms
step:1513/2110 train_time:76123ms step_avg:50.31ms
step:1514/2110 train_time:76212ms step_avg:50.34ms
step:1515/2110 train_time:76302ms step_avg:50.36ms
step:1516/2110 train_time:76390ms step_avg:50.39ms
step:1517/2110 train_time:76476ms step_avg:50.41ms
step:1518/2110 train_time:76563ms step_avg:50.44ms
step:1519/2110 train_time:76650ms step_avg:50.46ms
step:1520/2110 train_time:76735ms step_avg:50.48ms
step:1521/2110 train_time:76822ms step_avg:50.51ms
step:1522/2110 train_time:76907ms step_avg:50.53ms
step:1523/2110 train_time:76994ms step_avg:50.55ms
step:1524/2110 train_time:77081ms step_avg:50.58ms
step:1525/2110 train_time:77169ms step_avg:50.60ms
step:1526/2110 train_time:77256ms step_avg:50.63ms
step:1527/2110 train_time:77345ms step_avg:50.65ms
step:1528/2110 train_time:77433ms step_avg:50.68ms
step:1529/2110 train_time:77519ms step_avg:50.70ms
step:1530/2110 train_time:77606ms step_avg:50.72ms
step:1531/2110 train_time:77692ms step_avg:50.75ms
step:1532/2110 train_time:77778ms step_avg:50.77ms
step:1533/2110 train_time:77864ms step_avg:50.79ms
step:1534/2110 train_time:77950ms step_avg:50.82ms
step:1535/2110 train_time:78038ms step_avg:50.84ms
step:1536/2110 train_time:78124ms step_avg:50.86ms
step:1537/2110 train_time:78213ms step_avg:50.89ms
step:1538/2110 train_time:78300ms step_avg:50.91ms
step:1539/2110 train_time:78387ms step_avg:50.93ms
step:1540/2110 train_time:78474ms step_avg:50.96ms
step:1541/2110 train_time:78561ms step_avg:50.98ms
step:1542/2110 train_time:78647ms step_avg:51.00ms
step:1543/2110 train_time:78733ms step_avg:51.03ms
step:1544/2110 train_time:78819ms step_avg:51.05ms
step:1545/2110 train_time:78907ms step_avg:51.07ms
step:1546/2110 train_time:78994ms step_avg:51.10ms
step:1547/2110 train_time:79080ms step_avg:51.12ms
step:1548/2110 train_time:79168ms step_avg:51.14ms
step:1549/2110 train_time:79256ms step_avg:51.17ms
step:1550/2110 train_time:79344ms step_avg:51.19ms
step:1551/2110 train_time:79431ms step_avg:51.21ms
step:1552/2110 train_time:79517ms step_avg:51.24ms
step:1553/2110 train_time:79604ms step_avg:51.26ms
step:1554/2110 train_time:79690ms step_avg:51.28ms
step:1555/2110 train_time:79777ms step_avg:51.30ms
step:1556/2110 train_time:79863ms step_avg:51.33ms
step:1557/2110 train_time:79950ms step_avg:51.35ms
step:1558/2110 train_time:80037ms step_avg:51.37ms
step:1559/2110 train_time:80124ms step_avg:51.39ms
step:1560/2110 train_time:80213ms step_avg:51.42ms
step:1561/2110 train_time:80300ms step_avg:51.44ms
step:1562/2110 train_time:80388ms step_avg:51.46ms
step:1563/2110 train_time:80475ms step_avg:51.49ms
step:1564/2110 train_time:80561ms step_avg:51.51ms
step:1565/2110 train_time:80648ms step_avg:51.53ms
step:1566/2110 train_time:80733ms step_avg:51.55ms
step:1567/2110 train_time:80820ms step_avg:51.58ms
step:1568/2110 train_time:80908ms step_avg:51.60ms
step:1569/2110 train_time:80995ms step_avg:51.62ms
step:1570/2110 train_time:81082ms step_avg:51.64ms
step:1571/2110 train_time:81170ms step_avg:51.67ms
step:1572/2110 train_time:81256ms step_avg:51.69ms
step:1573/2110 train_time:81344ms step_avg:51.71ms
step:1574/2110 train_time:81431ms step_avg:51.74ms
step:1575/2110 train_time:81518ms step_avg:51.76ms
step:1576/2110 train_time:81605ms step_avg:51.78ms
step:1577/2110 train_time:81693ms step_avg:51.80ms
step:1578/2110 train_time:81779ms step_avg:51.82ms
step:1579/2110 train_time:81866ms step_avg:51.85ms
step:1580/2110 train_time:81952ms step_avg:51.87ms
step:1581/2110 train_time:82039ms step_avg:51.89ms
step:1582/2110 train_time:82126ms step_avg:51.91ms
step:1583/2110 train_time:82214ms step_avg:51.94ms
step:1584/2110 train_time:82300ms step_avg:51.96ms
step:1585/2110 train_time:82387ms step_avg:51.98ms
step:1586/2110 train_time:82473ms step_avg:52.00ms
step:1587/2110 train_time:82561ms step_avg:52.02ms
step:1588/2110 train_time:82647ms step_avg:52.04ms
step:1589/2110 train_time:82733ms step_avg:52.07ms
step:1590/2110 train_time:82820ms step_avg:52.09ms
step:1591/2110 train_time:82907ms step_avg:52.11ms
step:1592/2110 train_time:82993ms step_avg:52.13ms
step:1593/2110 train_time:83081ms step_avg:52.15ms
step:1594/2110 train_time:83168ms step_avg:52.18ms
step:1595/2110 train_time:83255ms step_avg:52.20ms
step:1596/2110 train_time:83341ms step_avg:52.22ms
step:1597/2110 train_time:83428ms step_avg:52.24ms
step:1598/2110 train_time:83515ms step_avg:52.26ms
step:1599/2110 train_time:83603ms step_avg:52.28ms
step:1600/2110 train_time:83689ms step_avg:52.31ms
step:1601/2110 train_time:83776ms step_avg:52.33ms
step:1602/2110 train_time:83863ms step_avg:52.35ms
step:1603/2110 train_time:83951ms step_avg:52.37ms
step:1604/2110 train_time:84037ms step_avg:52.39ms
step:1605/2110 train_time:84124ms step_avg:52.41ms
step:1606/2110 train_time:84210ms step_avg:52.43ms
step:1607/2110 train_time:84299ms step_avg:52.46ms
step:1608/2110 train_time:84385ms step_avg:52.48ms
step:1609/2110 train_time:84472ms step_avg:52.50ms
step:1610/2110 train_time:84558ms step_avg:52.52ms
step:1611/2110 train_time:84645ms step_avg:52.54ms
step:1612/2110 train_time:84732ms step_avg:52.56ms
step:1613/2110 train_time:84819ms step_avg:52.58ms
step:1614/2110 train_time:84906ms step_avg:52.61ms
step:1615/2110 train_time:84994ms step_avg:52.63ms
step:1616/2110 train_time:85080ms step_avg:52.65ms
step:1617/2110 train_time:85168ms step_avg:52.67ms
step:1618/2110 train_time:85255ms step_avg:52.69ms
step:1619/2110 train_time:85342ms step_avg:52.71ms
step:1620/2110 train_time:85429ms step_avg:52.73ms
step:1621/2110 train_time:85516ms step_avg:52.76ms
step:1622/2110 train_time:85603ms step_avg:52.78ms
step:1623/2110 train_time:85690ms step_avg:52.80ms
step:1624/2110 train_time:85776ms step_avg:52.82ms
step:1625/2110 train_time:85862ms step_avg:52.84ms
step:1626/2110 train_time:85950ms step_avg:52.86ms
step:1627/2110 train_time:86036ms step_avg:52.88ms
step:1628/2110 train_time:86123ms step_avg:52.90ms
step:1629/2110 train_time:86210ms step_avg:52.92ms
step:1630/2110 train_time:86296ms step_avg:52.94ms
step:1631/2110 train_time:86384ms step_avg:52.96ms
step:1632/2110 train_time:86472ms step_avg:52.99ms
step:1633/2110 train_time:86557ms step_avg:53.01ms
step:1634/2110 train_time:86645ms step_avg:53.03ms
step:1635/2110 train_time:86731ms step_avg:53.05ms
step:1636/2110 train_time:86817ms step_avg:53.07ms
step:1637/2110 train_time:86904ms step_avg:53.09ms
step:1638/2110 train_time:86993ms step_avg:53.11ms
step:1639/2110 train_time:87078ms step_avg:53.13ms
step:1640/2110 train_time:87165ms step_avg:53.15ms
step:1641/2110 train_time:87254ms step_avg:53.17ms
step:1642/2110 train_time:87337ms step_avg:53.19ms
step:1643/2110 train_time:87425ms step_avg:53.21ms
step:1644/2110 train_time:87512ms step_avg:53.23ms
step:1645/2110 train_time:87599ms step_avg:53.25ms
step:1646/2110 train_time:87685ms step_avg:53.27ms
step:1647/2110 train_time:87772ms step_avg:53.29ms
step:1648/2110 train_time:87857ms step_avg:53.31ms
step:1649/2110 train_time:87945ms step_avg:53.33ms
step:1650/2110 train_time:88031ms step_avg:53.35ms
step:1651/2110 train_time:88118ms step_avg:53.37ms
step:1652/2110 train_time:88205ms step_avg:53.39ms
step:1653/2110 train_time:88292ms step_avg:53.41ms
step:1654/2110 train_time:88379ms step_avg:53.43ms
step:1655/2110 train_time:88465ms step_avg:53.45ms
step:1656/2110 train_time:88551ms step_avg:53.47ms
step:1657/2110 train_time:88640ms step_avg:53.49ms
step:1658/2110 train_time:88727ms step_avg:53.51ms
step:1659/2110 train_time:88815ms step_avg:53.54ms
step:1660/2110 train_time:88903ms step_avg:53.56ms
step:1661/2110 train_time:88992ms step_avg:53.58ms
step:1662/2110 train_time:89079ms step_avg:53.60ms
step:1663/2110 train_time:89167ms step_avg:53.62ms
step:1664/2110 train_time:89254ms step_avg:53.64ms
step:1665/2110 train_time:89343ms step_avg:53.66ms
step:1666/2110 train_time:89432ms step_avg:53.68ms
step:1667/2110 train_time:89520ms step_avg:53.70ms
step:1668/2110 train_time:89607ms step_avg:53.72ms
step:1669/2110 train_time:89695ms step_avg:53.74ms
step:1670/2110 train_time:89783ms step_avg:53.76ms
step:1671/2110 train_time:89871ms step_avg:53.78ms
step:1672/2110 train_time:89960ms step_avg:53.80ms
step:1673/2110 train_time:90048ms step_avg:53.82ms
step:1674/2110 train_time:90136ms step_avg:53.84ms
step:1675/2110 train_time:90223ms step_avg:53.86ms
step:1676/2110 train_time:90312ms step_avg:53.89ms
step:1677/2110 train_time:90400ms step_avg:53.91ms
step:1678/2110 train_time:90488ms step_avg:53.93ms
step:1679/2110 train_time:90575ms step_avg:53.95ms
step:1680/2110 train_time:90664ms step_avg:53.97ms
step:1681/2110 train_time:90752ms step_avg:53.99ms
step:1682/2110 train_time:90839ms step_avg:54.01ms
step:1683/2110 train_time:90927ms step_avg:54.03ms
step:1684/2110 train_time:91015ms step_avg:54.05ms
step:1685/2110 train_time:91103ms step_avg:54.07ms
step:1686/2110 train_time:91191ms step_avg:54.09ms
step:1687/2110 train_time:91280ms step_avg:54.11ms
step:1688/2110 train_time:91369ms step_avg:54.13ms
step:1689/2110 train_time:91456ms step_avg:54.15ms
step:1690/2110 train_time:91544ms step_avg:54.17ms
step:1691/2110 train_time:91632ms step_avg:54.19ms
step:1692/2110 train_time:91720ms step_avg:54.21ms
step:1693/2110 train_time:91809ms step_avg:54.23ms
step:1694/2110 train_time:91896ms step_avg:54.25ms
step:1695/2110 train_time:91984ms step_avg:54.27ms
step:1696/2110 train_time:92073ms step_avg:54.29ms
step:1697/2110 train_time:92161ms step_avg:54.31ms
step:1698/2110 train_time:92250ms step_avg:54.33ms
step:1699/2110 train_time:92337ms step_avg:54.35ms
step:1700/2110 train_time:92425ms step_avg:54.37ms
step:1701/2110 train_time:92514ms step_avg:54.39ms
step:1702/2110 train_time:92603ms step_avg:54.41ms
step:1703/2110 train_time:92692ms step_avg:54.43ms
step:1704/2110 train_time:92778ms step_avg:54.45ms
step:1705/2110 train_time:92867ms step_avg:54.47ms
step:1706/2110 train_time:92955ms step_avg:54.49ms
step:1707/2110 train_time:93044ms step_avg:54.51ms
step:1708/2110 train_time:93131ms step_avg:54.53ms
step:1709/2110 train_time:93220ms step_avg:54.55ms
step:1710/2110 train_time:93308ms step_avg:54.57ms
step:1711/2110 train_time:93397ms step_avg:54.59ms
step:1712/2110 train_time:93485ms step_avg:54.61ms
step:1713/2110 train_time:93573ms step_avg:54.63ms
step:1714/2110 train_time:93661ms step_avg:54.64ms
step:1715/2110 train_time:93749ms step_avg:54.66ms
step:1716/2110 train_time:93837ms step_avg:54.68ms
step:1717/2110 train_time:93925ms step_avg:54.70ms
step:1718/2110 train_time:94013ms step_avg:54.72ms
step:1719/2110 train_time:94101ms step_avg:54.74ms
step:1720/2110 train_time:94189ms step_avg:54.76ms
step:1721/2110 train_time:94277ms step_avg:54.78ms
step:1722/2110 train_time:94365ms step_avg:54.80ms
step:1723/2110 train_time:94454ms step_avg:54.82ms
step:1724/2110 train_time:94542ms step_avg:54.84ms
step:1725/2110 train_time:94630ms step_avg:54.86ms
step:1726/2110 train_time:94717ms step_avg:54.88ms
step:1727/2110 train_time:94806ms step_avg:54.90ms
step:1728/2110 train_time:94894ms step_avg:54.92ms
step:1729/2110 train_time:94983ms step_avg:54.94ms
step:1730/2110 train_time:95073ms step_avg:54.96ms
step:1731/2110 train_time:95160ms step_avg:54.97ms
step:1732/2110 train_time:95249ms step_avg:54.99ms
step:1733/2110 train_time:95336ms step_avg:55.01ms
step:1734/2110 train_time:95424ms step_avg:55.03ms
step:1735/2110 train_time:95513ms step_avg:55.05ms
step:1736/2110 train_time:95600ms step_avg:55.07ms
step:1737/2110 train_time:95688ms step_avg:55.09ms
step:1738/2110 train_time:95776ms step_avg:55.11ms
step:1739/2110 train_time:95866ms step_avg:55.13ms
step:1740/2110 train_time:95953ms step_avg:55.15ms
step:1741/2110 train_time:96043ms step_avg:55.17ms
step:1742/2110 train_time:96133ms step_avg:55.19ms
step:1743/2110 train_time:96220ms step_avg:55.20ms
step:1744/2110 train_time:96307ms step_avg:55.22ms
step:1745/2110 train_time:96396ms step_avg:55.24ms
step:1746/2110 train_time:96483ms step_avg:55.26ms
step:1747/2110 train_time:96572ms step_avg:55.28ms
step:1748/2110 train_time:96660ms step_avg:55.30ms
step:1749/2110 train_time:96748ms step_avg:55.32ms
step:1750/2110 train_time:96835ms step_avg:55.33ms
step:1750/2110 val_loss:3.3809 train_time:96927ms step_avg:55.39ms
step:1751/2110 train_time:96966ms step_avg:55.38ms
step:1752/2110 train_time:97020ms step_avg:55.38ms
step:1753/2110 train_time:97115ms step_avg:55.40ms
step:1754/2110 train_time:97203ms step_avg:55.42ms
step:1755/2110 train_time:97292ms step_avg:55.44ms
step:1756/2110 train_time:97378ms step_avg:55.45ms
step:1757/2110 train_time:97465ms step_avg:55.47ms
step:1758/2110 train_time:97552ms step_avg:55.49ms
step:1759/2110 train_time:97638ms step_avg:55.51ms
step:1760/2110 train_time:97724ms step_avg:55.53ms
step:1761/2110 train_time:97812ms step_avg:55.54ms
step:1762/2110 train_time:97901ms step_avg:55.56ms
step:1763/2110 train_time:97994ms step_avg:55.58ms
step:1764/2110 train_time:98086ms step_avg:55.60ms
step:1765/2110 train_time:98175ms step_avg:55.62ms
step:1766/2110 train_time:98262ms step_avg:55.64ms
step:1767/2110 train_time:98350ms step_avg:55.66ms
step:1768/2110 train_time:98437ms step_avg:55.68ms
step:1769/2110 train_time:98524ms step_avg:55.69ms
step:1770/2110 train_time:98611ms step_avg:55.71ms
step:1771/2110 train_time:98698ms step_avg:55.73ms
step:1772/2110 train_time:98784ms step_avg:55.75ms
step:1773/2110 train_time:98873ms step_avg:55.77ms
step:1774/2110 train_time:98962ms step_avg:55.78ms
step:1775/2110 train_time:99052ms step_avg:55.80ms
step:1776/2110 train_time:99142ms step_avg:55.82ms
step:1777/2110 train_time:99230ms step_avg:55.84ms
step:1778/2110 train_time:99317ms step_avg:55.86ms
step:1779/2110 train_time:99405ms step_avg:55.88ms
step:1780/2110 train_time:99492ms step_avg:55.89ms
step:1781/2110 train_time:99579ms step_avg:55.91ms
step:1782/2110 train_time:99667ms step_avg:55.93ms
step:1783/2110 train_time:99754ms step_avg:55.95ms
step:1784/2110 train_time:99841ms step_avg:55.96ms
step:1785/2110 train_time:99930ms step_avg:55.98ms
step:1786/2110 train_time:100018ms step_avg:56.00ms
step:1787/2110 train_time:100108ms step_avg:56.02ms
step:1788/2110 train_time:100196ms step_avg:56.04ms
step:1789/2110 train_time:100284ms step_avg:56.06ms
step:1790/2110 train_time:100371ms step_avg:56.07ms
step:1791/2110 train_time:100459ms step_avg:56.09ms
step:1792/2110 train_time:100545ms step_avg:56.11ms
step:1793/2110 train_time:100633ms step_avg:56.13ms
step:1794/2110 train_time:100721ms step_avg:56.14ms
step:1795/2110 train_time:100808ms step_avg:56.16ms
step:1796/2110 train_time:100895ms step_avg:56.18ms
step:1797/2110 train_time:100985ms step_avg:56.20ms
step:1798/2110 train_time:101072ms step_avg:56.21ms
step:1799/2110 train_time:101161ms step_avg:56.23ms
step:1800/2110 train_time:101249ms step_avg:56.25ms
step:1801/2110 train_time:101337ms step_avg:56.27ms
step:1802/2110 train_time:101424ms step_avg:56.28ms
step:1803/2110 train_time:101512ms step_avg:56.30ms
step:1804/2110 train_time:101600ms step_avg:56.32ms
step:1805/2110 train_time:101686ms step_avg:56.34ms
step:1806/2110 train_time:101774ms step_avg:56.35ms
step:1807/2110 train_time:101862ms step_avg:56.37ms
step:1808/2110 train_time:101949ms step_avg:56.39ms
step:1809/2110 train_time:102039ms step_avg:56.41ms
step:1810/2110 train_time:102126ms step_avg:56.42ms
step:1811/2110 train_time:102216ms step_avg:56.44ms
step:1812/2110 train_time:102305ms step_avg:56.46ms
step:1813/2110 train_time:102393ms step_avg:56.48ms
step:1814/2110 train_time:102480ms step_avg:56.49ms
step:1815/2110 train_time:102568ms step_avg:56.51ms
step:1816/2110 train_time:102655ms step_avg:56.53ms
step:1817/2110 train_time:102743ms step_avg:56.55ms
step:1818/2110 train_time:102829ms step_avg:56.56ms
step:1819/2110 train_time:102918ms step_avg:56.58ms
step:1820/2110 train_time:103005ms step_avg:56.60ms
step:1821/2110 train_time:103095ms step_avg:56.61ms
step:1822/2110 train_time:103182ms step_avg:56.63ms
step:1823/2110 train_time:103270ms step_avg:56.65ms
step:1824/2110 train_time:103358ms step_avg:56.67ms
step:1825/2110 train_time:103446ms step_avg:56.68ms
step:1826/2110 train_time:103532ms step_avg:56.70ms
step:1827/2110 train_time:103621ms step_avg:56.72ms
step:1828/2110 train_time:103708ms step_avg:56.73ms
step:1829/2110 train_time:103798ms step_avg:56.75ms
step:1830/2110 train_time:103886ms step_avg:56.77ms
step:1831/2110 train_time:103974ms step_avg:56.79ms
step:1832/2110 train_time:104061ms step_avg:56.80ms
step:1833/2110 train_time:104150ms step_avg:56.82ms
step:1834/2110 train_time:104238ms step_avg:56.84ms
step:1835/2110 train_time:104326ms step_avg:56.85ms
step:1836/2110 train_time:104413ms step_avg:56.87ms
step:1837/2110 train_time:104502ms step_avg:56.89ms
step:1838/2110 train_time:104588ms step_avg:56.90ms
step:1839/2110 train_time:104678ms step_avg:56.92ms
step:1840/2110 train_time:104766ms step_avg:56.94ms
step:1841/2110 train_time:104854ms step_avg:56.95ms
step:1842/2110 train_time:104942ms step_avg:56.97ms
step:1843/2110 train_time:105029ms step_avg:56.99ms
step:1844/2110 train_time:105118ms step_avg:57.01ms
step:1845/2110 train_time:105205ms step_avg:57.02ms
step:1846/2110 train_time:105292ms step_avg:57.04ms
step:1847/2110 train_time:105381ms step_avg:57.06ms
step:1848/2110 train_time:105468ms step_avg:57.07ms
step:1849/2110 train_time:105557ms step_avg:57.09ms
step:1850/2110 train_time:105646ms step_avg:57.11ms
step:1851/2110 train_time:105733ms step_avg:57.12ms
step:1852/2110 train_time:105819ms step_avg:57.14ms
step:1853/2110 train_time:105909ms step_avg:57.16ms
step:1854/2110 train_time:105997ms step_avg:57.17ms
step:1855/2110 train_time:106084ms step_avg:57.19ms
step:1856/2110 train_time:106172ms step_avg:57.20ms
step:1857/2110 train_time:106260ms step_avg:57.22ms
step:1858/2110 train_time:106346ms step_avg:57.24ms
step:1859/2110 train_time:106436ms step_avg:57.25ms
step:1860/2110 train_time:106524ms step_avg:57.27ms
step:1861/2110 train_time:106612ms step_avg:57.29ms
step:1862/2110 train_time:106700ms step_avg:57.30ms
step:1863/2110 train_time:106787ms step_avg:57.32ms
step:1864/2110 train_time:106875ms step_avg:57.34ms
step:1865/2110 train_time:106962ms step_avg:57.35ms
step:1866/2110 train_time:107048ms step_avg:57.37ms
step:1867/2110 train_time:107137ms step_avg:57.38ms
step:1868/2110 train_time:107225ms step_avg:57.40ms
step:1869/2110 train_time:107312ms step_avg:57.42ms
step:1870/2110 train_time:107399ms step_avg:57.43ms
step:1871/2110 train_time:107489ms step_avg:57.45ms
step:1872/2110 train_time:107577ms step_avg:57.47ms
step:1873/2110 train_time:107665ms step_avg:57.48ms
step:1874/2110 train_time:107752ms step_avg:57.50ms
step:1875/2110 train_time:107842ms step_avg:57.52ms
step:1876/2110 train_time:107929ms step_avg:57.53ms
step:1877/2110 train_time:108017ms step_avg:57.55ms
step:1878/2110 train_time:108105ms step_avg:57.56ms
step:1879/2110 train_time:108194ms step_avg:57.58ms
step:1880/2110 train_time:108280ms step_avg:57.60ms
step:1881/2110 train_time:108369ms step_avg:57.61ms
step:1882/2110 train_time:108456ms step_avg:57.63ms
step:1883/2110 train_time:108546ms step_avg:57.65ms
step:1884/2110 train_time:108633ms step_avg:57.66ms
step:1885/2110 train_time:108721ms step_avg:57.68ms
step:1886/2110 train_time:108808ms step_avg:57.69ms
step:1887/2110 train_time:108898ms step_avg:57.71ms
step:1888/2110 train_time:108985ms step_avg:57.73ms
step:1889/2110 train_time:109073ms step_avg:57.74ms
step:1890/2110 train_time:109161ms step_avg:57.76ms
step:1891/2110 train_time:109248ms step_avg:57.77ms
step:1892/2110 train_time:109336ms step_avg:57.79ms
step:1893/2110 train_time:109425ms step_avg:57.80ms
step:1894/2110 train_time:109511ms step_avg:57.82ms
step:1895/2110 train_time:109600ms step_avg:57.84ms
step:1896/2110 train_time:109687ms step_avg:57.85ms
step:1897/2110 train_time:109776ms step_avg:57.87ms
step:1898/2110 train_time:109862ms step_avg:57.88ms
step:1899/2110 train_time:109952ms step_avg:57.90ms
step:1900/2110 train_time:110039ms step_avg:57.92ms
step:1901/2110 train_time:110129ms step_avg:57.93ms
step:1902/2110 train_time:110217ms step_avg:57.95ms
step:1903/2110 train_time:110305ms step_avg:57.96ms
step:1904/2110 train_time:110392ms step_avg:57.98ms
step:1905/2110 train_time:110480ms step_avg:57.99ms
step:1906/2110 train_time:110567ms step_avg:58.01ms
step:1907/2110 train_time:110656ms step_avg:58.03ms
step:1908/2110 train_time:110743ms step_avg:58.04ms
step:1909/2110 train_time:110832ms step_avg:58.06ms
step:1910/2110 train_time:110919ms step_avg:58.07ms
step:1911/2110 train_time:111008ms step_avg:58.09ms
step:1912/2110 train_time:111096ms step_avg:58.10ms
step:1913/2110 train_time:111184ms step_avg:58.12ms
step:1914/2110 train_time:111271ms step_avg:58.14ms
step:1915/2110 train_time:111359ms step_avg:58.15ms
step:1916/2110 train_time:111447ms step_avg:58.17ms
step:1917/2110 train_time:111535ms step_avg:58.18ms
step:1918/2110 train_time:111623ms step_avg:58.20ms
step:1919/2110 train_time:111712ms step_avg:58.21ms
step:1920/2110 train_time:111798ms step_avg:58.23ms
step:1921/2110 train_time:111887ms step_avg:58.24ms
step:1922/2110 train_time:111974ms step_avg:58.26ms
step:1923/2110 train_time:112063ms step_avg:58.27ms
step:1924/2110 train_time:112149ms step_avg:58.29ms
step:1925/2110 train_time:112238ms step_avg:58.31ms
step:1926/2110 train_time:112326ms step_avg:58.32ms
step:1927/2110 train_time:112414ms step_avg:58.34ms
step:1928/2110 train_time:112501ms step_avg:58.35ms
step:1929/2110 train_time:112591ms step_avg:58.37ms
step:1930/2110 train_time:112678ms step_avg:58.38ms
step:1931/2110 train_time:112766ms step_avg:58.40ms
step:1932/2110 train_time:112854ms step_avg:58.41ms
step:1933/2110 train_time:112942ms step_avg:58.43ms
step:1934/2110 train_time:113029ms step_avg:58.44ms
step:1935/2110 train_time:113118ms step_avg:58.46ms
step:1936/2110 train_time:113204ms step_avg:58.47ms
step:1937/2110 train_time:113293ms step_avg:58.49ms
step:1938/2110 train_time:113381ms step_avg:58.50ms
step:1939/2110 train_time:113469ms step_avg:58.52ms
step:1940/2110 train_time:113557ms step_avg:58.53ms
step:1941/2110 train_time:113646ms step_avg:58.55ms
step:1942/2110 train_time:113734ms step_avg:58.57ms
step:1943/2110 train_time:113822ms step_avg:58.58ms
step:1944/2110 train_time:113911ms step_avg:58.60ms
step:1945/2110 train_time:113998ms step_avg:58.61ms
step:1946/2110 train_time:114086ms step_avg:58.63ms
step:1947/2110 train_time:114174ms step_avg:58.64ms
step:1948/2110 train_time:114261ms step_avg:58.66ms
step:1949/2110 train_time:114350ms step_avg:58.67ms
step:1950/2110 train_time:114437ms step_avg:58.69ms
step:1951/2110 train_time:114524ms step_avg:58.70ms
step:1952/2110 train_time:114614ms step_avg:58.72ms
step:1953/2110 train_time:114700ms step_avg:58.73ms
step:1954/2110 train_time:114788ms step_avg:58.74ms
step:1955/2110 train_time:114876ms step_avg:58.76ms
step:1956/2110 train_time:114965ms step_avg:58.78ms
step:1957/2110 train_time:115051ms step_avg:58.79ms
step:1958/2110 train_time:115139ms step_avg:58.80ms
step:1959/2110 train_time:115226ms step_avg:58.82ms
step:1960/2110 train_time:115314ms step_avg:58.83ms
step:1961/2110 train_time:115402ms step_avg:58.85ms
step:1962/2110 train_time:115489ms step_avg:58.86ms
step:1963/2110 train_time:115578ms step_avg:58.88ms
step:1964/2110 train_time:115667ms step_avg:58.89ms
step:1965/2110 train_time:115754ms step_avg:58.91ms
step:1966/2110 train_time:115842ms step_avg:58.92ms
step:1967/2110 train_time:115929ms step_avg:58.94ms
step:1968/2110 train_time:116017ms step_avg:58.95ms
step:1969/2110 train_time:116106ms step_avg:58.97ms
step:1970/2110 train_time:116194ms step_avg:58.98ms
step:1971/2110 train_time:116281ms step_avg:59.00ms
step:1972/2110 train_time:116369ms step_avg:59.01ms
step:1973/2110 train_time:116459ms step_avg:59.03ms
step:1974/2110 train_time:116547ms step_avg:59.04ms
step:1975/2110 train_time:116634ms step_avg:59.06ms
step:1976/2110 train_time:116721ms step_avg:59.07ms
step:1977/2110 train_time:116810ms step_avg:59.08ms
step:1978/2110 train_time:116897ms step_avg:59.10ms
step:1979/2110 train_time:116986ms step_avg:59.11ms
step:1980/2110 train_time:117073ms step_avg:59.13ms
step:1981/2110 train_time:117162ms step_avg:59.14ms
step:1982/2110 train_time:117249ms step_avg:59.16ms
step:1983/2110 train_time:117337ms step_avg:59.17ms
step:1984/2110 train_time:117426ms step_avg:59.19ms
step:1985/2110 train_time:117514ms step_avg:59.20ms
step:1986/2110 train_time:117602ms step_avg:59.22ms
step:1987/2110 train_time:117689ms step_avg:59.23ms
step:1988/2110 train_time:117779ms step_avg:59.24ms
step:1989/2110 train_time:117864ms step_avg:59.26ms
step:1990/2110 train_time:117951ms step_avg:59.27ms
step:1991/2110 train_time:118040ms step_avg:59.29ms
step:1992/2110 train_time:118128ms step_avg:59.30ms
step:1993/2110 train_time:118216ms step_avg:59.32ms
step:1994/2110 train_time:118304ms step_avg:59.33ms
step:1995/2110 train_time:118391ms step_avg:59.34ms
step:1996/2110 train_time:118479ms step_avg:59.36ms
step:1997/2110 train_time:118566ms step_avg:59.37ms
step:1998/2110 train_time:118654ms step_avg:59.39ms
step:1999/2110 train_time:118741ms step_avg:59.40ms
step:2000/2110 train_time:118829ms step_avg:59.41ms
step:2000/2110 val_loss:3.3055 train_time:118919ms step_avg:59.46ms
step:2001/2110 train_time:118956ms step_avg:59.45ms
step:2002/2110 train_time:119011ms step_avg:59.45ms
step:2003/2110 train_time:119102ms step_avg:59.46ms
step:2004/2110 train_time:119189ms step_avg:59.48ms
step:2005/2110 train_time:119277ms step_avg:59.49ms
step:2006/2110 train_time:119364ms step_avg:59.50ms
step:2007/2110 train_time:119451ms step_avg:59.52ms
step:2008/2110 train_time:119537ms step_avg:59.53ms
step:2009/2110 train_time:119624ms step_avg:59.54ms
step:2010/2110 train_time:119710ms step_avg:59.56ms
step:2011/2110 train_time:119797ms step_avg:59.57ms
step:2012/2110 train_time:119888ms step_avg:59.59ms
step:2013/2110 train_time:119979ms step_avg:59.60ms
step:2014/2110 train_time:120069ms step_avg:59.62ms
step:2015/2110 train_time:120159ms step_avg:59.63ms
step:2016/2110 train_time:120247ms step_avg:59.65ms
step:2017/2110 train_time:120334ms step_avg:59.66ms
step:2018/2110 train_time:120421ms step_avg:59.67ms
step:2019/2110 train_time:120507ms step_avg:59.69ms
step:2020/2110 train_time:120595ms step_avg:59.70ms
step:2021/2110 train_time:120681ms step_avg:59.71ms
step:2022/2110 train_time:120768ms step_avg:59.73ms
step:2023/2110 train_time:120857ms step_avg:59.74ms
step:2024/2110 train_time:120947ms step_avg:59.76ms
step:2025/2110 train_time:121038ms step_avg:59.77ms
step:2026/2110 train_time:121127ms step_avg:59.79ms
step:2027/2110 train_time:121214ms step_avg:59.80ms
step:2028/2110 train_time:121303ms step_avg:59.81ms
step:2029/2110 train_time:121389ms step_avg:59.83ms
step:2030/2110 train_time:121475ms step_avg:59.84ms
step:2031/2110 train_time:121563ms step_avg:59.85ms
step:2032/2110 train_time:121649ms step_avg:59.87ms
step:2033/2110 train_time:121738ms step_avg:59.88ms
step:2034/2110 train_time:121825ms step_avg:59.89ms
step:2035/2110 train_time:121916ms step_avg:59.91ms
step:2036/2110 train_time:122004ms step_avg:59.92ms
step:2037/2110 train_time:122095ms step_avg:59.94ms
step:2038/2110 train_time:122183ms step_avg:59.95ms
step:2039/2110 train_time:122271ms step_avg:59.97ms
step:2040/2110 train_time:122359ms step_avg:59.98ms
step:2041/2110 train_time:122447ms step_avg:59.99ms
step:2042/2110 train_time:122534ms step_avg:60.01ms
step:2043/2110 train_time:122621ms step_avg:60.02ms
step:2044/2110 train_time:122708ms step_avg:60.03ms
step:2045/2110 train_time:122796ms step_avg:60.05ms
step:2046/2110 train_time:122884ms step_avg:60.06ms
step:2047/2110 train_time:122973ms step_avg:60.07ms
step:2048/2110 train_time:123061ms step_avg:60.09ms
step:2049/2110 train_time:123150ms step_avg:60.10ms
step:2050/2110 train_time:123238ms step_avg:60.12ms
step:2051/2110 train_time:123325ms step_avg:60.13ms
step:2052/2110 train_time:123412ms step_avg:60.14ms
step:2053/2110 train_time:123499ms step_avg:60.16ms
step:2054/2110 train_time:123587ms step_avg:60.17ms
step:2055/2110 train_time:123674ms step_avg:60.18ms
step:2056/2110 train_time:123762ms step_avg:60.20ms
step:2057/2110 train_time:123850ms step_avg:60.21ms
step:2058/2110 train_time:123938ms step_avg:60.22ms
step:2059/2110 train_time:124027ms step_avg:60.24ms
step:2060/2110 train_time:124117ms step_avg:60.25ms
step:2061/2110 train_time:124204ms step_avg:60.26ms
step:2062/2110 train_time:124292ms step_avg:60.28ms
step:2063/2110 train_time:124380ms step_avg:60.29ms
step:2064/2110 train_time:124467ms step_avg:60.30ms
step:2065/2110 train_time:124555ms step_avg:60.32ms
step:2066/2110 train_time:124643ms step_avg:60.33ms
step:2067/2110 train_time:124730ms step_avg:60.34ms
step:2068/2110 train_time:124818ms step_avg:60.36ms
step:2069/2110 train_time:124907ms step_avg:60.37ms
step:2070/2110 train_time:124995ms step_avg:60.38ms
step:2071/2110 train_time:125084ms step_avg:60.40ms
step:2072/2110 train_time:125171ms step_avg:60.41ms
step:2073/2110 train_time:125260ms step_avg:60.42ms
step:2074/2110 train_time:125349ms step_avg:60.44ms
step:2075/2110 train_time:125436ms step_avg:60.45ms
step:2076/2110 train_time:125524ms step_avg:60.46ms
step:2077/2110 train_time:125612ms step_avg:60.48ms
step:2078/2110 train_time:125700ms step_avg:60.49ms
step:2079/2110 train_time:125787ms step_avg:60.50ms
step:2080/2110 train_time:125875ms step_avg:60.52ms
step:2081/2110 train_time:125963ms step_avg:60.53ms
step:2082/2110 train_time:126051ms step_avg:60.54ms
step:2083/2110 train_time:126141ms step_avg:60.56ms
step:2084/2110 train_time:126228ms step_avg:60.57ms
step:2085/2110 train_time:126316ms step_avg:60.58ms
step:2086/2110 train_time:126405ms step_avg:60.60ms
step:2087/2110 train_time:126493ms step_avg:60.61ms
step:2088/2110 train_time:126580ms step_avg:60.62ms
step:2089/2110 train_time:126669ms step_avg:60.64ms
step:2090/2110 train_time:126757ms step_avg:60.65ms
step:2091/2110 train_time:126845ms step_avg:60.66ms
step:2092/2110 train_time:126934ms step_avg:60.68ms
step:2093/2110 train_time:127022ms step_avg:60.69ms
step:2094/2110 train_time:127109ms step_avg:60.70ms
step:2095/2110 train_time:127199ms step_avg:60.72ms
step:2096/2110 train_time:127286ms step_avg:60.73ms
step:2097/2110 train_time:127375ms step_avg:60.74ms
step:2098/2110 train_time:127463ms step_avg:60.75ms
step:2099/2110 train_time:127551ms step_avg:60.77ms
step:2100/2110 train_time:127638ms step_avg:60.78ms
step:2101/2110 train_time:127728ms step_avg:60.79ms
step:2102/2110 train_time:127814ms step_avg:60.81ms
step:2103/2110 train_time:127903ms step_avg:60.82ms
step:2104/2110 train_time:127991ms step_avg:60.83ms
step:2105/2110 train_time:128080ms step_avg:60.85ms
step:2106/2110 train_time:128168ms step_avg:60.86ms
step:2107/2110 train_time:128258ms step_avg:60.87ms
step:2108/2110 train_time:128345ms step_avg:60.88ms
step:2109/2110 train_time:128433ms step_avg:60.90ms
step:2110/2110 train_time:128522ms step_avg:60.91ms
step:2110/2110 val_loss:3.2808 train_time:128611ms step_avg:60.95ms
peak memory allocated: 29244 MiB reserved: 44256 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Tue Dec 16 01:34:15 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   38C    P0            126W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   42C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            120W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   37C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   37C    P0            120W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              80      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              81      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2110 train_time:104ms step_avg:103.62ms
step:2/2110 train_time:140ms step_avg:69.86ms
step:3/2110 train_time:171ms step_avg:57.15ms
step:4/2110 train_time:203ms step_avg:50.72ms
step:5/2110 train_time:230ms step_avg:46.07ms
step:6/2110 train_time:432ms step_avg:72.03ms
step:7/2110 train_time:682ms step_avg:97.42ms
step:8/2110 train_time:715ms step_avg:89.39ms
step:9/2110 train_time:747ms step_avg:82.99ms
step:10/2110 train_time:779ms step_avg:77.93ms
step:11/2110 train_time:812ms step_avg:73.84ms
step:12/2110 train_time:845ms step_avg:70.39ms
step:13/2110 train_time:878ms step_avg:67.55ms
step:14/2110 train_time:911ms step_avg:65.05ms
step:15/2110 train_time:944ms step_avg:62.94ms
step:16/2110 train_time:977ms step_avg:61.05ms
step:17/2110 train_time:1010ms step_avg:59.40ms
step:18/2110 train_time:1042ms step_avg:57.91ms
step:19/2110 train_time:1075ms step_avg:56.60ms
step:20/2110 train_time:1108ms step_avg:55.41ms
step:21/2110 train_time:1141ms step_avg:54.35ms
step:22/2110 train_time:1174ms step_avg:53.36ms
step:23/2110 train_time:1207ms step_avg:52.49ms
step:24/2110 train_time:1240ms step_avg:51.67ms
step:25/2110 train_time:1273ms step_avg:50.93ms
step:26/2110 train_time:1306ms step_avg:50.23ms
step:27/2110 train_time:1339ms step_avg:49.60ms
step:28/2110 train_time:1374ms step_avg:49.07ms
step:29/2110 train_time:1405ms step_avg:48.46ms
step:30/2110 train_time:1438ms step_avg:47.93ms
step:31/2110 train_time:1471ms step_avg:47.46ms
step:32/2110 train_time:1505ms step_avg:47.02ms
step:33/2110 train_time:1538ms step_avg:46.60ms
step:34/2110 train_time:1571ms step_avg:46.20ms
step:35/2110 train_time:1605ms step_avg:45.87ms
step:36/2110 train_time:1639ms step_avg:45.52ms
step:37/2110 train_time:1673ms step_avg:45.23ms
step:38/2110 train_time:1706ms step_avg:44.91ms
step:39/2110 train_time:1740ms step_avg:44.62ms
step:40/2110 train_time:1773ms step_avg:44.32ms
step:41/2110 train_time:1806ms step_avg:44.06ms
step:42/2110 train_time:1839ms step_avg:43.79ms
step:43/2110 train_time:1873ms step_avg:43.55ms
step:44/2110 train_time:1906ms step_avg:43.31ms
step:45/2110 train_time:1939ms step_avg:43.09ms
step:46/2110 train_time:1972ms step_avg:42.87ms
step:47/2110 train_time:2006ms step_avg:42.68ms
step:48/2110 train_time:2038ms step_avg:42.47ms
step:49/2110 train_time:2072ms step_avg:42.28ms
step:50/2110 train_time:2105ms step_avg:42.10ms
step:51/2110 train_time:2138ms step_avg:41.92ms
step:52/2110 train_time:2170ms step_avg:41.74ms
step:53/2110 train_time:2204ms step_avg:41.59ms
step:54/2110 train_time:2237ms step_avg:41.42ms
step:55/2110 train_time:2270ms step_avg:41.27ms
step:56/2110 train_time:2303ms step_avg:41.12ms
step:57/2110 train_time:2336ms step_avg:40.98ms
step:58/2110 train_time:2369ms step_avg:40.84ms
step:59/2110 train_time:2402ms step_avg:40.71ms
step:60/2110 train_time:2434ms step_avg:40.57ms
step:61/2110 train_time:2468ms step_avg:40.45ms
step:62/2110 train_time:2500ms step_avg:40.32ms
step:63/2110 train_time:2534ms step_avg:40.22ms
step:64/2110 train_time:2567ms step_avg:40.10ms
step:65/2110 train_time:2600ms step_avg:40.01ms
step:66/2110 train_time:2633ms step_avg:39.90ms
step:67/2110 train_time:2667ms step_avg:39.81ms
step:68/2110 train_time:2701ms step_avg:39.72ms
step:69/2110 train_time:2734ms step_avg:39.62ms
step:70/2110 train_time:2767ms step_avg:39.52ms
step:71/2110 train_time:2800ms step_avg:39.44ms
step:72/2110 train_time:2834ms step_avg:39.35ms
step:73/2110 train_time:2867ms step_avg:39.28ms
step:74/2110 train_time:2900ms step_avg:39.19ms
step:75/2110 train_time:2934ms step_avg:39.11ms
step:76/2110 train_time:2966ms step_avg:39.03ms
step:77/2110 train_time:3000ms step_avg:38.96ms
step:78/2110 train_time:3032ms step_avg:38.88ms
step:79/2110 train_time:3066ms step_avg:38.81ms
step:80/2110 train_time:3098ms step_avg:38.73ms
step:81/2110 train_time:3132ms step_avg:38.66ms
step:82/2110 train_time:3164ms step_avg:38.59ms
step:83/2110 train_time:3197ms step_avg:38.52ms
step:84/2110 train_time:3230ms step_avg:38.45ms
step:85/2110 train_time:3263ms step_avg:38.39ms
step:86/2110 train_time:3296ms step_avg:38.32ms
step:87/2110 train_time:3329ms step_avg:38.27ms
step:88/2110 train_time:3362ms step_avg:38.20ms
step:89/2110 train_time:3395ms step_avg:38.15ms
step:90/2110 train_time:3429ms step_avg:38.10ms
step:91/2110 train_time:3461ms step_avg:38.04ms
step:92/2110 train_time:3494ms step_avg:37.98ms
step:93/2110 train_time:3527ms step_avg:37.93ms
step:94/2110 train_time:3560ms step_avg:37.87ms
step:95/2110 train_time:3594ms step_avg:37.83ms
step:96/2110 train_time:3627ms step_avg:37.78ms
step:97/2110 train_time:3661ms step_avg:37.74ms
step:98/2110 train_time:3694ms step_avg:37.69ms
step:99/2110 train_time:3727ms step_avg:37.64ms
step:100/2110 train_time:3759ms step_avg:37.59ms
step:101/2110 train_time:3793ms step_avg:37.55ms
step:102/2110 train_time:3826ms step_avg:37.51ms
step:103/2110 train_time:3859ms step_avg:37.47ms
step:104/2110 train_time:3893ms step_avg:37.43ms
step:105/2110 train_time:3925ms step_avg:37.38ms
step:106/2110 train_time:3958ms step_avg:37.34ms
step:107/2110 train_time:3991ms step_avg:37.30ms
step:108/2110 train_time:4025ms step_avg:37.27ms
step:109/2110 train_time:4057ms step_avg:37.22ms
step:110/2110 train_time:4090ms step_avg:37.19ms
step:111/2110 train_time:4123ms step_avg:37.15ms
step:112/2110 train_time:4156ms step_avg:37.10ms
step:113/2110 train_time:4189ms step_avg:37.07ms
step:114/2110 train_time:4222ms step_avg:37.04ms
step:115/2110 train_time:4255ms step_avg:37.00ms
step:116/2110 train_time:4288ms step_avg:36.97ms
step:117/2110 train_time:4321ms step_avg:36.93ms
step:118/2110 train_time:4355ms step_avg:36.91ms
step:119/2110 train_time:4387ms step_avg:36.86ms
step:120/2110 train_time:4421ms step_avg:36.84ms
step:121/2110 train_time:4453ms step_avg:36.80ms
step:122/2110 train_time:4486ms step_avg:36.77ms
step:123/2110 train_time:4519ms step_avg:36.74ms
step:124/2110 train_time:4552ms step_avg:36.71ms
step:125/2110 train_time:4585ms step_avg:36.68ms
step:126/2110 train_time:4619ms step_avg:36.66ms
step:127/2110 train_time:4652ms step_avg:36.63ms
step:128/2110 train_time:4685ms step_avg:36.60ms
step:129/2110 train_time:4718ms step_avg:36.57ms
step:130/2110 train_time:4751ms step_avg:36.54ms
step:131/2110 train_time:4784ms step_avg:36.52ms
step:132/2110 train_time:4816ms step_avg:36.49ms
step:133/2110 train_time:4850ms step_avg:36.46ms
step:134/2110 train_time:4882ms step_avg:36.43ms
step:135/2110 train_time:4915ms step_avg:36.41ms
step:136/2110 train_time:4948ms step_avg:36.39ms
step:137/2110 train_time:4981ms step_avg:36.36ms
step:138/2110 train_time:5014ms step_avg:36.33ms
step:139/2110 train_time:5048ms step_avg:36.31ms
step:140/2110 train_time:5080ms step_avg:36.29ms
step:141/2110 train_time:5113ms step_avg:36.26ms
step:142/2110 train_time:5147ms step_avg:36.24ms
step:143/2110 train_time:5179ms step_avg:36.22ms
step:144/2110 train_time:5212ms step_avg:36.20ms
step:145/2110 train_time:5245ms step_avg:36.17ms
step:146/2110 train_time:5278ms step_avg:36.15ms
step:147/2110 train_time:5311ms step_avg:36.13ms
step:148/2110 train_time:5343ms step_avg:36.10ms
step:149/2110 train_time:5377ms step_avg:36.09ms
step:150/2110 train_time:5410ms step_avg:36.06ms
step:151/2110 train_time:5443ms step_avg:36.05ms
step:152/2110 train_time:5475ms step_avg:36.02ms
step:153/2110 train_time:5508ms step_avg:36.00ms
step:154/2110 train_time:5541ms step_avg:35.98ms
step:155/2110 train_time:5575ms step_avg:35.96ms
step:156/2110 train_time:5608ms step_avg:35.95ms
step:157/2110 train_time:5640ms step_avg:35.93ms
step:158/2110 train_time:5673ms step_avg:35.91ms
step:159/2110 train_time:5707ms step_avg:35.89ms
step:160/2110 train_time:5739ms step_avg:35.87ms
step:161/2110 train_time:5772ms step_avg:35.85ms
step:162/2110 train_time:5805ms step_avg:35.83ms
step:163/2110 train_time:5839ms step_avg:35.82ms
step:164/2110 train_time:5871ms step_avg:35.80ms
step:165/2110 train_time:5905ms step_avg:35.79ms
step:166/2110 train_time:5938ms step_avg:35.77ms
step:167/2110 train_time:5971ms step_avg:35.76ms
step:168/2110 train_time:6004ms step_avg:35.74ms
step:169/2110 train_time:6037ms step_avg:35.72ms
step:170/2110 train_time:6070ms step_avg:35.71ms
step:171/2110 train_time:6103ms step_avg:35.69ms
step:172/2110 train_time:6137ms step_avg:35.68ms
step:173/2110 train_time:6169ms step_avg:35.66ms
step:174/2110 train_time:6202ms step_avg:35.64ms
step:175/2110 train_time:6235ms step_avg:35.63ms
step:176/2110 train_time:6268ms step_avg:35.61ms
step:177/2110 train_time:6301ms step_avg:35.60ms
step:178/2110 train_time:6333ms step_avg:35.58ms
step:179/2110 train_time:6367ms step_avg:35.57ms
step:180/2110 train_time:6399ms step_avg:35.55ms
step:181/2110 train_time:6432ms step_avg:35.54ms
step:182/2110 train_time:6465ms step_avg:35.52ms
step:183/2110 train_time:6498ms step_avg:35.51ms
step:184/2110 train_time:6531ms step_avg:35.50ms
step:185/2110 train_time:6565ms step_avg:35.48ms
step:186/2110 train_time:6599ms step_avg:35.48ms
step:187/2110 train_time:6631ms step_avg:35.46ms
step:188/2110 train_time:6663ms step_avg:35.44ms
step:189/2110 train_time:6697ms step_avg:35.43ms
step:190/2110 train_time:6730ms step_avg:35.42ms
step:191/2110 train_time:6763ms step_avg:35.41ms
step:192/2110 train_time:6795ms step_avg:35.39ms
step:193/2110 train_time:6829ms step_avg:35.38ms
step:194/2110 train_time:6861ms step_avg:35.37ms
step:195/2110 train_time:6895ms step_avg:35.36ms
step:196/2110 train_time:6927ms step_avg:35.34ms
step:197/2110 train_time:6960ms step_avg:35.33ms
step:198/2110 train_time:6993ms step_avg:35.32ms
step:199/2110 train_time:7026ms step_avg:35.31ms
step:200/2110 train_time:7062ms step_avg:35.31ms
step:201/2110 train_time:7093ms step_avg:35.29ms
step:202/2110 train_time:7125ms step_avg:35.27ms
step:203/2110 train_time:7160ms step_avg:35.27ms
step:204/2110 train_time:7191ms step_avg:35.25ms
step:205/2110 train_time:7224ms step_avg:35.24ms
step:206/2110 train_time:7257ms step_avg:35.23ms
step:207/2110 train_time:7290ms step_avg:35.22ms
step:208/2110 train_time:7323ms step_avg:35.21ms
step:209/2110 train_time:7357ms step_avg:35.20ms
step:210/2110 train_time:7389ms step_avg:35.18ms
step:211/2110 train_time:7422ms step_avg:35.18ms
step:212/2110 train_time:7456ms step_avg:35.17ms
step:213/2110 train_time:7489ms step_avg:35.16ms
step:214/2110 train_time:7521ms step_avg:35.14ms
step:215/2110 train_time:7554ms step_avg:35.14ms
step:216/2110 train_time:7587ms step_avg:35.13ms
step:217/2110 train_time:7620ms step_avg:35.12ms
step:218/2110 train_time:7653ms step_avg:35.11ms
step:219/2110 train_time:7686ms step_avg:35.10ms
step:220/2110 train_time:7720ms step_avg:35.09ms
step:221/2110 train_time:7752ms step_avg:35.08ms
step:222/2110 train_time:7785ms step_avg:35.07ms
step:223/2110 train_time:7818ms step_avg:35.06ms
step:224/2110 train_time:7851ms step_avg:35.05ms
step:225/2110 train_time:7884ms step_avg:35.04ms
step:226/2110 train_time:7917ms step_avg:35.03ms
step:227/2110 train_time:7951ms step_avg:35.03ms
step:228/2110 train_time:7983ms step_avg:35.01ms
step:229/2110 train_time:8017ms step_avg:35.01ms
step:230/2110 train_time:8049ms step_avg:35.00ms
step:231/2110 train_time:8083ms step_avg:34.99ms
step:232/2110 train_time:8115ms step_avg:34.98ms
step:233/2110 train_time:8148ms step_avg:34.97ms
step:234/2110 train_time:8181ms step_avg:34.96ms
step:235/2110 train_time:8214ms step_avg:34.95ms
step:236/2110 train_time:8247ms step_avg:34.94ms
step:237/2110 train_time:8280ms step_avg:34.94ms
step:238/2110 train_time:8313ms step_avg:34.93ms
step:239/2110 train_time:8346ms step_avg:34.92ms
step:240/2110 train_time:8379ms step_avg:34.91ms
step:241/2110 train_time:8412ms step_avg:34.90ms
step:242/2110 train_time:8445ms step_avg:34.90ms
step:243/2110 train_time:8478ms step_avg:34.89ms
step:244/2110 train_time:8510ms step_avg:34.88ms
step:245/2110 train_time:8544ms step_avg:34.87ms
step:246/2110 train_time:8576ms step_avg:34.86ms
step:247/2110 train_time:8610ms step_avg:34.86ms
step:248/2110 train_time:8642ms step_avg:34.85ms
step:249/2110 train_time:8676ms step_avg:34.84ms
step:250/2110 train_time:8708ms step_avg:34.83ms
step:250/2110 val_loss:4.2906 train_time:8744ms step_avg:34.98ms
step:251/2110 train_time:8775ms step_avg:34.96ms
step:252/2110 train_time:8801ms step_avg:34.93ms
step:253/2110 train_time:8828ms step_avg:34.89ms
step:254/2110 train_time:8856ms step_avg:34.87ms
step:255/2110 train_time:8883ms step_avg:34.83ms
step:256/2110 train_time:8916ms step_avg:34.83ms
step:257/2110 train_time:8951ms step_avg:34.83ms
step:258/2110 train_time:8984ms step_avg:34.82ms
step:259/2110 train_time:9018ms step_avg:34.82ms
step:260/2110 train_time:9051ms step_avg:34.81ms
step:261/2110 train_time:9084ms step_avg:34.80ms
step:262/2110 train_time:9117ms step_avg:34.80ms
step:263/2110 train_time:9149ms step_avg:34.79ms
step:264/2110 train_time:9183ms step_avg:34.78ms
step:265/2110 train_time:9215ms step_avg:34.77ms
step:266/2110 train_time:9248ms step_avg:34.77ms
step:267/2110 train_time:9281ms step_avg:34.76ms
step:268/2110 train_time:9313ms step_avg:34.75ms
step:269/2110 train_time:9346ms step_avg:34.74ms
step:270/2110 train_time:9380ms step_avg:34.74ms
step:271/2110 train_time:9412ms step_avg:34.73ms
step:272/2110 train_time:9444ms step_avg:34.72ms
step:273/2110 train_time:9477ms step_avg:34.71ms
step:274/2110 train_time:9510ms step_avg:34.71ms
step:275/2110 train_time:9542ms step_avg:34.70ms
step:276/2110 train_time:9575ms step_avg:34.69ms
step:277/2110 train_time:9608ms step_avg:34.69ms
step:278/2110 train_time:9640ms step_avg:34.68ms
step:279/2110 train_time:9673ms step_avg:34.67ms
step:280/2110 train_time:9706ms step_avg:34.66ms
step:281/2110 train_time:9739ms step_avg:34.66ms
step:282/2110 train_time:9772ms step_avg:34.65ms
step:283/2110 train_time:9805ms step_avg:34.65ms
step:284/2110 train_time:9838ms step_avg:34.64ms
step:285/2110 train_time:9872ms step_avg:34.64ms
step:286/2110 train_time:9905ms step_avg:34.63ms
step:287/2110 train_time:9939ms step_avg:34.63ms
step:288/2110 train_time:9972ms step_avg:34.63ms
step:289/2110 train_time:10005ms step_avg:34.62ms
step:290/2110 train_time:10038ms step_avg:34.61ms
step:291/2110 train_time:10071ms step_avg:34.61ms
step:292/2110 train_time:10104ms step_avg:34.60ms
step:293/2110 train_time:10137ms step_avg:34.60ms
step:294/2110 train_time:10171ms step_avg:34.60ms
step:295/2110 train_time:10203ms step_avg:34.59ms
step:296/2110 train_time:10236ms step_avg:34.58ms
step:297/2110 train_time:10269ms step_avg:34.58ms
step:298/2110 train_time:10302ms step_avg:34.57ms
step:299/2110 train_time:10335ms step_avg:34.56ms
step:300/2110 train_time:10367ms step_avg:34.56ms
step:301/2110 train_time:10400ms step_avg:34.55ms
step:302/2110 train_time:10433ms step_avg:34.55ms
step:303/2110 train_time:10466ms step_avg:34.54ms
step:304/2110 train_time:10499ms step_avg:34.54ms
step:305/2110 train_time:10532ms step_avg:34.53ms
step:306/2110 train_time:10564ms step_avg:34.52ms
step:307/2110 train_time:10597ms step_avg:34.52ms
step:308/2110 train_time:10630ms step_avg:34.51ms
step:309/2110 train_time:10663ms step_avg:34.51ms
step:310/2110 train_time:10697ms step_avg:34.51ms
step:311/2110 train_time:10729ms step_avg:34.50ms
step:312/2110 train_time:10762ms step_avg:34.49ms
step:313/2110 train_time:10795ms step_avg:34.49ms
step:314/2110 train_time:10828ms step_avg:34.49ms
step:315/2110 train_time:10862ms step_avg:34.48ms
step:316/2110 train_time:10895ms step_avg:34.48ms
step:317/2110 train_time:10928ms step_avg:34.47ms
step:318/2110 train_time:10960ms step_avg:34.47ms
step:319/2110 train_time:10993ms step_avg:34.46ms
step:320/2110 train_time:11026ms step_avg:34.46ms
step:321/2110 train_time:11060ms step_avg:34.45ms
step:322/2110 train_time:11093ms step_avg:34.45ms
step:323/2110 train_time:11126ms step_avg:34.45ms
step:324/2110 train_time:11159ms step_avg:34.44ms
step:325/2110 train_time:11192ms step_avg:34.44ms
step:326/2110 train_time:11225ms step_avg:34.43ms
step:327/2110 train_time:11258ms step_avg:34.43ms
step:328/2110 train_time:11291ms step_avg:34.42ms
step:329/2110 train_time:11324ms step_avg:34.42ms
step:330/2110 train_time:11357ms step_avg:34.41ms
step:331/2110 train_time:11390ms step_avg:34.41ms
step:332/2110 train_time:11423ms step_avg:34.41ms
step:333/2110 train_time:11455ms step_avg:34.40ms
step:334/2110 train_time:11488ms step_avg:34.40ms
step:335/2110 train_time:11522ms step_avg:34.39ms
step:336/2110 train_time:11554ms step_avg:34.39ms
step:337/2110 train_time:11587ms step_avg:34.38ms
step:338/2110 train_time:11620ms step_avg:34.38ms
step:339/2110 train_time:11653ms step_avg:34.38ms
step:340/2110 train_time:11686ms step_avg:34.37ms
step:341/2110 train_time:11719ms step_avg:34.37ms
step:342/2110 train_time:11752ms step_avg:34.36ms
step:343/2110 train_time:11785ms step_avg:34.36ms
step:344/2110 train_time:11818ms step_avg:34.35ms
step:345/2110 train_time:11851ms step_avg:34.35ms
step:346/2110 train_time:11884ms step_avg:34.35ms
step:347/2110 train_time:11917ms step_avg:34.34ms
step:348/2110 train_time:11950ms step_avg:34.34ms
step:349/2110 train_time:11983ms step_avg:34.34ms
step:350/2110 train_time:12016ms step_avg:34.33ms
step:351/2110 train_time:12050ms step_avg:34.33ms
step:352/2110 train_time:12083ms step_avg:34.33ms
step:353/2110 train_time:12116ms step_avg:34.32ms
step:354/2110 train_time:12148ms step_avg:34.32ms
step:355/2110 train_time:12182ms step_avg:34.31ms
step:356/2110 train_time:12214ms step_avg:34.31ms
step:357/2110 train_time:12248ms step_avg:34.31ms
step:358/2110 train_time:12281ms step_avg:34.30ms
step:359/2110 train_time:12314ms step_avg:34.30ms
step:360/2110 train_time:12347ms step_avg:34.30ms
step:361/2110 train_time:12380ms step_avg:34.29ms
step:362/2110 train_time:12413ms step_avg:34.29ms
step:363/2110 train_time:12446ms step_avg:34.29ms
step:364/2110 train_time:12478ms step_avg:34.28ms
step:365/2110 train_time:12512ms step_avg:34.28ms
step:366/2110 train_time:12544ms step_avg:34.27ms
step:367/2110 train_time:12577ms step_avg:34.27ms
step:368/2110 train_time:12610ms step_avg:34.27ms
step:369/2110 train_time:12643ms step_avg:34.26ms
step:370/2110 train_time:12675ms step_avg:34.26ms
step:371/2110 train_time:12709ms step_avg:34.26ms
step:372/2110 train_time:12742ms step_avg:34.25ms
step:373/2110 train_time:12775ms step_avg:34.25ms
step:374/2110 train_time:12808ms step_avg:34.24ms
step:375/2110 train_time:12841ms step_avg:34.24ms
step:376/2110 train_time:12874ms step_avg:34.24ms
step:377/2110 train_time:12907ms step_avg:34.24ms
step:378/2110 train_time:12939ms step_avg:34.23ms
step:379/2110 train_time:12973ms step_avg:34.23ms
step:380/2110 train_time:13005ms step_avg:34.22ms
step:381/2110 train_time:13039ms step_avg:34.22ms
step:382/2110 train_time:13071ms step_avg:34.22ms
step:383/2110 train_time:13105ms step_avg:34.22ms
step:384/2110 train_time:13138ms step_avg:34.21ms
step:385/2110 train_time:13171ms step_avg:34.21ms
step:386/2110 train_time:13204ms step_avg:34.21ms
step:387/2110 train_time:13237ms step_avg:34.20ms
step:388/2110 train_time:13270ms step_avg:34.20ms
step:389/2110 train_time:13303ms step_avg:34.20ms
step:390/2110 train_time:13336ms step_avg:34.19ms
step:391/2110 train_time:13369ms step_avg:34.19ms
step:392/2110 train_time:13402ms step_avg:34.19ms
step:393/2110 train_time:13435ms step_avg:34.19ms
step:394/2110 train_time:13468ms step_avg:34.18ms
step:395/2110 train_time:13501ms step_avg:34.18ms
step:396/2110 train_time:13533ms step_avg:34.18ms
step:397/2110 train_time:13567ms step_avg:34.17ms
step:398/2110 train_time:13600ms step_avg:34.17ms
step:399/2110 train_time:13633ms step_avg:34.17ms
step:400/2110 train_time:13665ms step_avg:34.16ms
step:401/2110 train_time:13699ms step_avg:34.16ms
step:402/2110 train_time:13731ms step_avg:34.16ms
step:403/2110 train_time:13765ms step_avg:34.16ms
step:404/2110 train_time:13797ms step_avg:34.15ms
step:405/2110 train_time:13830ms step_avg:34.15ms
step:406/2110 train_time:13863ms step_avg:34.15ms
step:407/2110 train_time:13896ms step_avg:34.14ms
step:408/2110 train_time:13929ms step_avg:34.14ms
step:409/2110 train_time:13962ms step_avg:34.14ms
step:410/2110 train_time:13994ms step_avg:34.13ms
step:411/2110 train_time:14028ms step_avg:34.13ms
step:412/2110 train_time:14061ms step_avg:34.13ms
step:413/2110 train_time:14094ms step_avg:34.13ms
step:414/2110 train_time:14127ms step_avg:34.12ms
step:415/2110 train_time:14160ms step_avg:34.12ms
step:416/2110 train_time:14192ms step_avg:34.12ms
step:417/2110 train_time:14226ms step_avg:34.11ms
step:418/2110 train_time:14259ms step_avg:34.11ms
step:419/2110 train_time:14292ms step_avg:34.11ms
step:420/2110 train_time:14325ms step_avg:34.11ms
step:421/2110 train_time:14358ms step_avg:34.11ms
step:422/2110 train_time:14391ms step_avg:34.10ms
step:423/2110 train_time:14424ms step_avg:34.10ms
step:424/2110 train_time:14456ms step_avg:34.09ms
step:425/2110 train_time:14490ms step_avg:34.09ms
step:426/2110 train_time:14522ms step_avg:34.09ms
step:427/2110 train_time:14556ms step_avg:34.09ms
step:428/2110 train_time:14588ms step_avg:34.08ms
step:429/2110 train_time:14621ms step_avg:34.08ms
step:430/2110 train_time:14654ms step_avg:34.08ms
step:431/2110 train_time:14687ms step_avg:34.08ms
step:432/2110 train_time:14720ms step_avg:34.07ms
step:433/2110 train_time:14753ms step_avg:34.07ms
step:434/2110 train_time:14786ms step_avg:34.07ms
step:435/2110 train_time:14819ms step_avg:34.07ms
step:436/2110 train_time:14852ms step_avg:34.06ms
step:437/2110 train_time:14885ms step_avg:34.06ms
step:438/2110 train_time:14918ms step_avg:34.06ms
step:439/2110 train_time:14951ms step_avg:34.06ms
step:440/2110 train_time:14984ms step_avg:34.05ms
step:441/2110 train_time:15017ms step_avg:34.05ms
step:442/2110 train_time:15049ms step_avg:34.05ms
step:443/2110 train_time:15083ms step_avg:34.05ms
step:444/2110 train_time:15116ms step_avg:34.04ms
step:445/2110 train_time:15149ms step_avg:34.04ms
step:446/2110 train_time:15183ms step_avg:34.04ms
step:447/2110 train_time:15215ms step_avg:34.04ms
step:448/2110 train_time:15247ms step_avg:34.03ms
step:449/2110 train_time:15280ms step_avg:34.03ms
step:450/2110 train_time:15313ms step_avg:34.03ms
step:451/2110 train_time:15346ms step_avg:34.03ms
step:452/2110 train_time:15378ms step_avg:34.02ms
step:453/2110 train_time:15412ms step_avg:34.02ms
step:454/2110 train_time:15444ms step_avg:34.02ms
step:455/2110 train_time:15478ms step_avg:34.02ms
step:456/2110 train_time:15511ms step_avg:34.02ms
step:457/2110 train_time:15544ms step_avg:34.01ms
step:458/2110 train_time:15577ms step_avg:34.01ms
step:459/2110 train_time:15610ms step_avg:34.01ms
step:460/2110 train_time:15642ms step_avg:34.00ms
step:461/2110 train_time:15676ms step_avg:34.00ms
step:462/2110 train_time:15708ms step_avg:34.00ms
step:463/2110 train_time:15741ms step_avg:34.00ms
step:464/2110 train_time:15774ms step_avg:34.00ms
step:465/2110 train_time:15807ms step_avg:33.99ms
step:466/2110 train_time:15841ms step_avg:33.99ms
step:467/2110 train_time:15873ms step_avg:33.99ms
step:468/2110 train_time:15906ms step_avg:33.99ms
step:469/2110 train_time:15939ms step_avg:33.99ms
step:470/2110 train_time:15972ms step_avg:33.98ms
step:471/2110 train_time:16005ms step_avg:33.98ms
step:472/2110 train_time:16037ms step_avg:33.98ms
step:473/2110 train_time:16071ms step_avg:33.98ms
step:474/2110 train_time:16103ms step_avg:33.97ms
step:475/2110 train_time:16137ms step_avg:33.97ms
step:476/2110 train_time:16169ms step_avg:33.97ms
step:477/2110 train_time:16203ms step_avg:33.97ms
step:478/2110 train_time:16235ms step_avg:33.97ms
step:479/2110 train_time:16269ms step_avg:33.96ms
step:480/2110 train_time:16301ms step_avg:33.96ms
step:481/2110 train_time:16334ms step_avg:33.96ms
step:482/2110 train_time:16367ms step_avg:33.96ms
step:483/2110 train_time:16400ms step_avg:33.96ms
step:484/2110 train_time:16433ms step_avg:33.95ms
step:485/2110 train_time:16466ms step_avg:33.95ms
step:486/2110 train_time:16499ms step_avg:33.95ms
step:487/2110 train_time:16533ms step_avg:33.95ms
step:488/2110 train_time:16566ms step_avg:33.95ms
step:489/2110 train_time:16599ms step_avg:33.94ms
step:490/2110 train_time:16631ms step_avg:33.94ms
step:491/2110 train_time:16665ms step_avg:33.94ms
step:492/2110 train_time:16697ms step_avg:33.94ms
step:493/2110 train_time:16731ms step_avg:33.94ms
step:494/2110 train_time:16765ms step_avg:33.94ms
step:495/2110 train_time:16797ms step_avg:33.93ms
step:496/2110 train_time:16830ms step_avg:33.93ms
step:497/2110 train_time:16863ms step_avg:33.93ms
step:498/2110 train_time:16896ms step_avg:33.93ms
step:499/2110 train_time:16929ms step_avg:33.93ms
step:500/2110 train_time:16962ms step_avg:33.92ms
step:500/2110 val_loss:4.0275 train_time:16998ms step_avg:34.00ms
step:501/2110 train_time:17026ms step_avg:33.98ms
step:502/2110 train_time:17052ms step_avg:33.97ms
step:503/2110 train_time:17079ms step_avg:33.95ms
step:504/2110 train_time:17107ms step_avg:33.94ms
step:505/2110 train_time:17137ms step_avg:33.94ms
step:506/2110 train_time:17172ms step_avg:33.94ms
step:507/2110 train_time:17204ms step_avg:33.93ms
step:508/2110 train_time:17238ms step_avg:33.93ms
step:509/2110 train_time:17270ms step_avg:33.93ms
step:510/2110 train_time:17303ms step_avg:33.93ms
step:511/2110 train_time:17336ms step_avg:33.93ms
step:512/2110 train_time:17370ms step_avg:33.93ms
step:513/2110 train_time:17402ms step_avg:33.92ms
step:514/2110 train_time:17434ms step_avg:33.92ms
step:515/2110 train_time:17467ms step_avg:33.92ms
step:516/2110 train_time:17500ms step_avg:33.91ms
step:517/2110 train_time:17533ms step_avg:33.91ms
step:518/2110 train_time:17565ms step_avg:33.91ms
step:519/2110 train_time:17598ms step_avg:33.91ms
step:520/2110 train_time:17631ms step_avg:33.91ms
step:521/2110 train_time:17664ms step_avg:33.90ms
step:522/2110 train_time:17696ms step_avg:33.90ms
step:523/2110 train_time:17729ms step_avg:33.90ms
step:524/2110 train_time:17761ms step_avg:33.90ms
step:525/2110 train_time:17794ms step_avg:33.89ms
step:526/2110 train_time:17828ms step_avg:33.89ms
step:527/2110 train_time:17860ms step_avg:33.89ms
step:528/2110 train_time:17892ms step_avg:33.89ms
step:529/2110 train_time:17926ms step_avg:33.89ms
step:530/2110 train_time:17959ms step_avg:33.89ms
step:531/2110 train_time:17993ms step_avg:33.89ms
step:532/2110 train_time:18026ms step_avg:33.88ms
step:533/2110 train_time:18060ms step_avg:33.88ms
step:534/2110 train_time:18093ms step_avg:33.88ms
step:535/2110 train_time:18127ms step_avg:33.88ms
step:536/2110 train_time:18160ms step_avg:33.88ms
step:537/2110 train_time:18193ms step_avg:33.88ms
step:538/2110 train_time:18226ms step_avg:33.88ms
step:539/2110 train_time:18259ms step_avg:33.88ms
step:540/2110 train_time:18292ms step_avg:33.87ms
step:541/2110 train_time:18325ms step_avg:33.87ms
step:542/2110 train_time:18359ms step_avg:33.87ms
step:543/2110 train_time:18392ms step_avg:33.87ms
step:544/2110 train_time:18425ms step_avg:33.87ms
step:545/2110 train_time:18458ms step_avg:33.87ms
step:546/2110 train_time:18490ms step_avg:33.87ms
step:547/2110 train_time:18523ms step_avg:33.86ms
step:548/2110 train_time:18556ms step_avg:33.86ms
step:549/2110 train_time:18589ms step_avg:33.86ms
step:550/2110 train_time:18621ms step_avg:33.86ms
step:551/2110 train_time:18654ms step_avg:33.86ms
step:552/2110 train_time:18687ms step_avg:33.85ms
step:553/2110 train_time:18720ms step_avg:33.85ms
step:554/2110 train_time:18753ms step_avg:33.85ms
step:555/2110 train_time:18786ms step_avg:33.85ms
step:556/2110 train_time:18818ms step_avg:33.85ms
step:557/2110 train_time:18852ms step_avg:33.85ms
step:558/2110 train_time:18884ms step_avg:33.84ms
step:559/2110 train_time:18918ms step_avg:33.84ms
step:560/2110 train_time:18950ms step_avg:33.84ms
step:561/2110 train_time:18984ms step_avg:33.84ms
step:562/2110 train_time:19017ms step_avg:33.84ms
step:563/2110 train_time:19050ms step_avg:33.84ms
step:564/2110 train_time:19082ms step_avg:33.83ms
step:565/2110 train_time:19116ms step_avg:33.83ms
step:566/2110 train_time:19149ms step_avg:33.83ms
step:567/2110 train_time:19182ms step_avg:33.83ms
step:568/2110 train_time:19215ms step_avg:33.83ms
step:569/2110 train_time:19249ms step_avg:33.83ms
step:570/2110 train_time:19281ms step_avg:33.83ms
step:571/2110 train_time:19315ms step_avg:33.83ms
step:572/2110 train_time:19348ms step_avg:33.82ms
step:573/2110 train_time:19381ms step_avg:33.82ms
step:574/2110 train_time:19414ms step_avg:33.82ms
step:575/2110 train_time:19447ms step_avg:33.82ms
step:576/2110 train_time:19480ms step_avg:33.82ms
step:577/2110 train_time:19513ms step_avg:33.82ms
step:578/2110 train_time:19545ms step_avg:33.82ms
step:579/2110 train_time:19579ms step_avg:33.81ms
step:580/2110 train_time:19612ms step_avg:33.81ms
step:581/2110 train_time:19645ms step_avg:33.81ms
step:582/2110 train_time:19677ms step_avg:33.81ms
step:583/2110 train_time:19711ms step_avg:33.81ms
step:584/2110 train_time:19744ms step_avg:33.81ms
step:585/2110 train_time:19777ms step_avg:33.81ms
step:586/2110 train_time:19809ms step_avg:33.80ms
step:587/2110 train_time:19843ms step_avg:33.80ms
step:588/2110 train_time:19875ms step_avg:33.80ms
step:589/2110 train_time:19908ms step_avg:33.80ms
step:590/2110 train_time:19942ms step_avg:33.80ms
step:591/2110 train_time:19975ms step_avg:33.80ms
step:592/2110 train_time:20007ms step_avg:33.80ms
step:593/2110 train_time:20041ms step_avg:33.80ms
step:594/2110 train_time:20074ms step_avg:33.79ms
step:595/2110 train_time:20107ms step_avg:33.79ms
step:596/2110 train_time:20139ms step_avg:33.79ms
step:597/2110 train_time:20173ms step_avg:33.79ms
step:598/2110 train_time:20206ms step_avg:33.79ms
step:599/2110 train_time:20239ms step_avg:33.79ms
step:600/2110 train_time:20273ms step_avg:33.79ms
step:601/2110 train_time:20306ms step_avg:33.79ms
step:602/2110 train_time:20340ms step_avg:33.79ms
step:603/2110 train_time:20372ms step_avg:33.78ms
step:604/2110 train_time:20405ms step_avg:33.78ms
step:605/2110 train_time:20438ms step_avg:33.78ms
step:606/2110 train_time:20471ms step_avg:33.78ms
step:607/2110 train_time:20505ms step_avg:33.78ms
step:608/2110 train_time:20537ms step_avg:33.78ms
step:609/2110 train_time:20570ms step_avg:33.78ms
step:610/2110 train_time:20603ms step_avg:33.77ms
step:611/2110 train_time:20636ms step_avg:33.77ms
step:612/2110 train_time:20669ms step_avg:33.77ms
step:613/2110 train_time:20702ms step_avg:33.77ms
step:614/2110 train_time:20735ms step_avg:33.77ms
step:615/2110 train_time:20768ms step_avg:33.77ms
step:616/2110 train_time:20801ms step_avg:33.77ms
step:617/2110 train_time:20834ms step_avg:33.77ms
step:618/2110 train_time:20867ms step_avg:33.76ms
step:619/2110 train_time:20900ms step_avg:33.76ms
step:620/2110 train_time:20933ms step_avg:33.76ms
step:621/2110 train_time:20966ms step_avg:33.76ms
step:622/2110 train_time:20999ms step_avg:33.76ms
step:623/2110 train_time:21033ms step_avg:33.76ms
step:624/2110 train_time:21066ms step_avg:33.76ms
step:625/2110 train_time:21106ms step_avg:33.77ms
step:626/2110 train_time:21150ms step_avg:33.79ms
step:627/2110 train_time:21196ms step_avg:33.80ms
step:628/2110 train_time:21238ms step_avg:33.82ms
step:629/2110 train_time:21281ms step_avg:33.83ms
step:630/2110 train_time:21321ms step_avg:33.84ms
step:631/2110 train_time:21363ms step_avg:33.86ms
step:632/2110 train_time:21402ms step_avg:33.86ms
step:633/2110 train_time:21443ms step_avg:33.88ms
step:634/2110 train_time:21476ms step_avg:33.87ms
step:635/2110 train_time:21508ms step_avg:33.87ms
step:636/2110 train_time:21541ms step_avg:33.87ms
step:637/2110 train_time:21574ms step_avg:33.87ms
step:638/2110 train_time:21606ms step_avg:33.86ms
step:639/2110 train_time:21639ms step_avg:33.86ms
step:640/2110 train_time:21671ms step_avg:33.86ms
step:641/2110 train_time:21705ms step_avg:33.86ms
step:642/2110 train_time:21737ms step_avg:33.86ms
step:643/2110 train_time:21770ms step_avg:33.86ms
step:644/2110 train_time:21802ms step_avg:33.85ms
step:645/2110 train_time:21835ms step_avg:33.85ms
step:646/2110 train_time:21867ms step_avg:33.85ms
step:647/2110 train_time:21901ms step_avg:33.85ms
step:648/2110 train_time:21934ms step_avg:33.85ms
step:649/2110 train_time:21966ms step_avg:33.85ms
step:650/2110 train_time:21999ms step_avg:33.84ms
step:651/2110 train_time:22032ms step_avg:33.84ms
step:652/2110 train_time:22064ms step_avg:33.84ms
step:653/2110 train_time:22097ms step_avg:33.84ms
step:654/2110 train_time:22130ms step_avg:33.84ms
step:655/2110 train_time:22164ms step_avg:33.84ms
step:656/2110 train_time:22196ms step_avg:33.84ms
step:657/2110 train_time:22230ms step_avg:33.84ms
step:658/2110 train_time:22263ms step_avg:33.83ms
step:659/2110 train_time:22296ms step_avg:33.83ms
step:660/2110 train_time:22329ms step_avg:33.83ms
step:661/2110 train_time:22363ms step_avg:33.83ms
step:662/2110 train_time:22396ms step_avg:33.83ms
step:663/2110 train_time:22430ms step_avg:33.83ms
step:664/2110 train_time:22463ms step_avg:33.83ms
step:665/2110 train_time:22496ms step_avg:33.83ms
step:666/2110 train_time:22529ms step_avg:33.83ms
step:667/2110 train_time:22562ms step_avg:33.83ms
step:668/2110 train_time:22595ms step_avg:33.82ms
step:669/2110 train_time:22628ms step_avg:33.82ms
step:670/2110 train_time:22661ms step_avg:33.82ms
step:671/2110 train_time:22693ms step_avg:33.82ms
step:672/2110 train_time:22726ms step_avg:33.82ms
step:673/2110 train_time:22760ms step_avg:33.82ms
step:674/2110 train_time:22792ms step_avg:33.82ms
step:675/2110 train_time:22825ms step_avg:33.81ms
step:676/2110 train_time:22857ms step_avg:33.81ms
step:677/2110 train_time:22890ms step_avg:33.81ms
step:678/2110 train_time:22922ms step_avg:33.81ms
step:679/2110 train_time:22956ms step_avg:33.81ms
step:680/2110 train_time:22988ms step_avg:33.81ms
step:681/2110 train_time:23021ms step_avg:33.81ms
step:682/2110 train_time:23054ms step_avg:33.80ms
step:683/2110 train_time:23087ms step_avg:33.80ms
step:684/2110 train_time:23119ms step_avg:33.80ms
step:685/2110 train_time:23153ms step_avg:33.80ms
step:686/2110 train_time:23185ms step_avg:33.80ms
step:687/2110 train_time:23219ms step_avg:33.80ms
step:688/2110 train_time:23251ms step_avg:33.80ms
step:689/2110 train_time:23285ms step_avg:33.80ms
step:690/2110 train_time:23318ms step_avg:33.79ms
step:691/2110 train_time:23353ms step_avg:33.80ms
step:692/2110 train_time:23410ms step_avg:33.83ms
step:693/2110 train_time:23470ms step_avg:33.87ms
step:694/2110 train_time:23528ms step_avg:33.90ms
step:695/2110 train_time:23588ms step_avg:33.94ms
step:696/2110 train_time:23646ms step_avg:33.97ms
step:697/2110 train_time:23707ms step_avg:34.01ms
step:698/2110 train_time:23764ms step_avg:34.05ms
step:699/2110 train_time:23824ms step_avg:34.08ms
step:700/2110 train_time:23882ms step_avg:34.12ms
step:701/2110 train_time:23943ms step_avg:34.15ms
step:702/2110 train_time:24002ms step_avg:34.19ms
step:703/2110 train_time:24060ms step_avg:34.22ms
step:704/2110 train_time:24119ms step_avg:34.26ms
step:705/2110 train_time:24179ms step_avg:34.30ms
step:706/2110 train_time:24238ms step_avg:34.33ms
step:707/2110 train_time:24299ms step_avg:34.37ms
step:708/2110 train_time:24358ms step_avg:34.40ms
step:709/2110 train_time:24418ms step_avg:34.44ms
step:710/2110 train_time:24478ms step_avg:34.48ms
step:711/2110 train_time:24539ms step_avg:34.51ms
step:712/2110 train_time:24597ms step_avg:34.55ms
step:713/2110 train_time:24657ms step_avg:34.58ms
step:714/2110 train_time:24715ms step_avg:34.62ms
step:715/2110 train_time:24775ms step_avg:34.65ms
step:716/2110 train_time:24833ms step_avg:34.68ms
step:717/2110 train_time:24893ms step_avg:34.72ms
step:718/2110 train_time:24951ms step_avg:34.75ms
step:719/2110 train_time:25010ms step_avg:34.78ms
step:720/2110 train_time:25069ms step_avg:34.82ms
step:721/2110 train_time:25130ms step_avg:34.85ms
step:722/2110 train_time:25188ms step_avg:34.89ms
step:723/2110 train_time:25248ms step_avg:34.92ms
step:724/2110 train_time:25306ms step_avg:34.95ms
step:725/2110 train_time:25367ms step_avg:34.99ms
step:726/2110 train_time:25425ms step_avg:35.02ms
step:727/2110 train_time:25485ms step_avg:35.06ms
step:728/2110 train_time:25544ms step_avg:35.09ms
step:729/2110 train_time:25604ms step_avg:35.12ms
step:730/2110 train_time:25662ms step_avg:35.15ms
step:731/2110 train_time:25723ms step_avg:35.19ms
step:732/2110 train_time:25781ms step_avg:35.22ms
step:733/2110 train_time:25841ms step_avg:35.25ms
step:734/2110 train_time:25900ms step_avg:35.29ms
step:735/2110 train_time:25960ms step_avg:35.32ms
step:736/2110 train_time:26019ms step_avg:35.35ms
step:737/2110 train_time:26081ms step_avg:35.39ms
step:738/2110 train_time:26140ms step_avg:35.42ms
step:739/2110 train_time:26200ms step_avg:35.45ms
step:740/2110 train_time:26258ms step_avg:35.48ms
step:741/2110 train_time:26318ms step_avg:35.52ms
step:742/2110 train_time:26377ms step_avg:35.55ms
step:743/2110 train_time:26437ms step_avg:35.58ms
step:744/2110 train_time:26496ms step_avg:35.61ms
step:745/2110 train_time:26556ms step_avg:35.65ms
step:746/2110 train_time:26615ms step_avg:35.68ms
step:747/2110 train_time:26675ms step_avg:35.71ms
step:748/2110 train_time:26733ms step_avg:35.74ms
step:749/2110 train_time:26793ms step_avg:35.77ms
step:750/2110 train_time:26852ms step_avg:35.80ms
step:750/2110 val_loss:3.9049 train_time:26913ms step_avg:35.88ms
step:751/2110 train_time:26942ms step_avg:35.87ms
step:752/2110 train_time:26973ms step_avg:35.87ms
step:753/2110 train_time:27036ms step_avg:35.91ms
step:754/2110 train_time:27097ms step_avg:35.94ms
step:755/2110 train_time:27157ms step_avg:35.97ms
step:756/2110 train_time:27215ms step_avg:36.00ms
step:757/2110 train_time:27275ms step_avg:36.03ms
step:758/2110 train_time:27333ms step_avg:36.06ms
step:759/2110 train_time:27392ms step_avg:36.09ms
step:760/2110 train_time:27450ms step_avg:36.12ms
step:761/2110 train_time:27509ms step_avg:36.15ms
step:762/2110 train_time:27567ms step_avg:36.18ms
step:763/2110 train_time:27626ms step_avg:36.21ms
step:764/2110 train_time:27685ms step_avg:36.24ms
step:765/2110 train_time:27746ms step_avg:36.27ms
step:766/2110 train_time:27804ms step_avg:36.30ms
step:767/2110 train_time:27865ms step_avg:36.33ms
step:768/2110 train_time:27925ms step_avg:36.36ms
step:769/2110 train_time:27987ms step_avg:36.39ms
step:770/2110 train_time:28047ms step_avg:36.43ms
step:771/2110 train_time:28108ms step_avg:36.46ms
step:772/2110 train_time:28167ms step_avg:36.49ms
step:773/2110 train_time:28227ms step_avg:36.52ms
step:774/2110 train_time:28286ms step_avg:36.55ms
step:775/2110 train_time:28345ms step_avg:36.57ms
step:776/2110 train_time:28404ms step_avg:36.60ms
step:777/2110 train_time:28463ms step_avg:36.63ms
step:778/2110 train_time:28520ms step_avg:36.66ms
step:779/2110 train_time:28579ms step_avg:36.69ms
step:780/2110 train_time:28637ms step_avg:36.71ms
step:781/2110 train_time:28696ms step_avg:36.74ms
step:782/2110 train_time:28754ms step_avg:36.77ms
step:783/2110 train_time:28814ms step_avg:36.80ms
step:784/2110 train_time:28873ms step_avg:36.83ms
step:785/2110 train_time:28934ms step_avg:36.86ms
step:786/2110 train_time:28994ms step_avg:36.89ms
step:787/2110 train_time:29054ms step_avg:36.92ms
step:788/2110 train_time:29113ms step_avg:36.95ms
step:789/2110 train_time:29174ms step_avg:36.98ms
step:790/2110 train_time:29232ms step_avg:37.00ms
step:791/2110 train_time:29292ms step_avg:37.03ms
step:792/2110 train_time:29351ms step_avg:37.06ms
step:793/2110 train_time:29411ms step_avg:37.09ms
step:794/2110 train_time:29470ms step_avg:37.12ms
step:795/2110 train_time:29529ms step_avg:37.14ms
step:796/2110 train_time:29588ms step_avg:37.17ms
step:797/2110 train_time:29647ms step_avg:37.20ms
step:798/2110 train_time:29705ms step_avg:37.22ms
step:799/2110 train_time:29765ms step_avg:37.25ms
step:800/2110 train_time:29823ms step_avg:37.28ms
step:801/2110 train_time:29883ms step_avg:37.31ms
step:802/2110 train_time:29942ms step_avg:37.33ms
step:803/2110 train_time:30003ms step_avg:37.36ms
step:804/2110 train_time:30061ms step_avg:37.39ms
step:805/2110 train_time:30121ms step_avg:37.42ms
step:806/2110 train_time:30180ms step_avg:37.44ms
step:807/2110 train_time:30240ms step_avg:37.47ms
step:808/2110 train_time:30299ms step_avg:37.50ms
step:809/2110 train_time:30359ms step_avg:37.53ms
step:810/2110 train_time:30417ms step_avg:37.55ms
step:811/2110 train_time:30476ms step_avg:37.58ms
step:812/2110 train_time:30534ms step_avg:37.60ms
step:813/2110 train_time:30594ms step_avg:37.63ms
step:814/2110 train_time:30651ms step_avg:37.66ms
step:815/2110 train_time:30710ms step_avg:37.68ms
step:816/2110 train_time:30768ms step_avg:37.71ms
step:817/2110 train_time:30829ms step_avg:37.73ms
step:818/2110 train_time:30888ms step_avg:37.76ms
step:819/2110 train_time:30949ms step_avg:37.79ms
step:820/2110 train_time:31008ms step_avg:37.81ms
step:821/2110 train_time:31068ms step_avg:37.84ms
step:822/2110 train_time:31127ms step_avg:37.87ms
step:823/2110 train_time:31188ms step_avg:37.90ms
step:824/2110 train_time:31247ms step_avg:37.92ms
step:825/2110 train_time:31307ms step_avg:37.95ms
step:826/2110 train_time:31366ms step_avg:37.97ms
step:827/2110 train_time:31426ms step_avg:38.00ms
step:828/2110 train_time:31485ms step_avg:38.03ms
step:829/2110 train_time:31544ms step_avg:38.05ms
step:830/2110 train_time:31603ms step_avg:38.08ms
step:831/2110 train_time:31662ms step_avg:38.10ms
step:832/2110 train_time:31720ms step_avg:38.12ms
step:833/2110 train_time:31780ms step_avg:38.15ms
step:834/2110 train_time:31838ms step_avg:38.17ms
step:835/2110 train_time:31898ms step_avg:38.20ms
step:836/2110 train_time:31957ms step_avg:38.23ms
step:837/2110 train_time:32017ms step_avg:38.25ms
step:838/2110 train_time:32075ms step_avg:38.28ms
step:839/2110 train_time:32136ms step_avg:38.30ms
step:840/2110 train_time:32194ms step_avg:38.33ms
step:841/2110 train_time:32255ms step_avg:38.35ms
step:842/2110 train_time:32313ms step_avg:38.38ms
step:843/2110 train_time:32373ms step_avg:38.40ms
step:844/2110 train_time:32432ms step_avg:38.43ms
step:845/2110 train_time:32492ms step_avg:38.45ms
step:846/2110 train_time:32550ms step_avg:38.48ms
step:847/2110 train_time:32610ms step_avg:38.50ms
step:848/2110 train_time:32668ms step_avg:38.52ms
step:849/2110 train_time:32729ms step_avg:38.55ms
step:850/2110 train_time:32789ms step_avg:38.57ms
step:851/2110 train_time:32849ms step_avg:38.60ms
step:852/2110 train_time:32908ms step_avg:38.62ms
step:853/2110 train_time:32968ms step_avg:38.65ms
step:854/2110 train_time:33027ms step_avg:38.67ms
step:855/2110 train_time:33087ms step_avg:38.70ms
step:856/2110 train_time:33146ms step_avg:38.72ms
step:857/2110 train_time:33206ms step_avg:38.75ms
step:858/2110 train_time:33265ms step_avg:38.77ms
step:859/2110 train_time:33325ms step_avg:38.79ms
step:860/2110 train_time:33383ms step_avg:38.82ms
step:861/2110 train_time:33443ms step_avg:38.84ms
step:862/2110 train_time:33503ms step_avg:38.87ms
step:863/2110 train_time:33563ms step_avg:38.89ms
step:864/2110 train_time:33621ms step_avg:38.91ms
step:865/2110 train_time:33681ms step_avg:38.94ms
step:866/2110 train_time:33739ms step_avg:38.96ms
step:867/2110 train_time:33800ms step_avg:38.98ms
step:868/2110 train_time:33858ms step_avg:39.01ms
step:869/2110 train_time:33918ms step_avg:39.03ms
step:870/2110 train_time:33975ms step_avg:39.05ms
step:871/2110 train_time:34036ms step_avg:39.08ms
step:872/2110 train_time:34095ms step_avg:39.10ms
step:873/2110 train_time:34155ms step_avg:39.12ms
step:874/2110 train_time:34213ms step_avg:39.15ms
step:875/2110 train_time:34273ms step_avg:39.17ms
step:876/2110 train_time:34331ms step_avg:39.19ms
step:877/2110 train_time:34392ms step_avg:39.22ms
step:878/2110 train_time:34451ms step_avg:39.24ms
step:879/2110 train_time:34511ms step_avg:39.26ms
step:880/2110 train_time:34570ms step_avg:39.28ms
step:881/2110 train_time:34630ms step_avg:39.31ms
step:882/2110 train_time:34690ms step_avg:39.33ms
step:883/2110 train_time:34751ms step_avg:39.36ms
step:884/2110 train_time:34810ms step_avg:39.38ms
step:885/2110 train_time:34870ms step_avg:39.40ms
step:886/2110 train_time:34929ms step_avg:39.42ms
step:887/2110 train_time:34989ms step_avg:39.45ms
step:888/2110 train_time:35048ms step_avg:39.47ms
step:889/2110 train_time:35107ms step_avg:39.49ms
step:890/2110 train_time:35166ms step_avg:39.51ms
step:891/2110 train_time:35226ms step_avg:39.53ms
step:892/2110 train_time:35284ms step_avg:39.56ms
step:893/2110 train_time:35344ms step_avg:39.58ms
step:894/2110 train_time:35403ms step_avg:39.60ms
step:895/2110 train_time:35463ms step_avg:39.62ms
step:896/2110 train_time:35522ms step_avg:39.64ms
step:897/2110 train_time:35582ms step_avg:39.67ms
step:898/2110 train_time:35641ms step_avg:39.69ms
step:899/2110 train_time:35700ms step_avg:39.71ms
step:900/2110 train_time:35758ms step_avg:39.73ms
step:901/2110 train_time:35818ms step_avg:39.75ms
step:902/2110 train_time:35877ms step_avg:39.77ms
step:903/2110 train_time:35937ms step_avg:39.80ms
step:904/2110 train_time:35996ms step_avg:39.82ms
step:905/2110 train_time:36056ms step_avg:39.84ms
step:906/2110 train_time:36114ms step_avg:39.86ms
step:907/2110 train_time:36174ms step_avg:39.88ms
step:908/2110 train_time:36232ms step_avg:39.90ms
step:909/2110 train_time:36292ms step_avg:39.93ms
step:910/2110 train_time:36350ms step_avg:39.95ms
step:911/2110 train_time:36411ms step_avg:39.97ms
step:912/2110 train_time:36470ms step_avg:39.99ms
step:913/2110 train_time:36531ms step_avg:40.01ms
step:914/2110 train_time:36590ms step_avg:40.03ms
step:915/2110 train_time:36651ms step_avg:40.06ms
step:916/2110 train_time:36709ms step_avg:40.08ms
step:917/2110 train_time:36770ms step_avg:40.10ms
step:918/2110 train_time:36830ms step_avg:40.12ms
step:919/2110 train_time:36890ms step_avg:40.14ms
step:920/2110 train_time:36949ms step_avg:40.16ms
step:921/2110 train_time:37009ms step_avg:40.18ms
step:922/2110 train_time:37067ms step_avg:40.20ms
step:923/2110 train_time:37128ms step_avg:40.22ms
step:924/2110 train_time:37186ms step_avg:40.24ms
step:925/2110 train_time:37246ms step_avg:40.27ms
step:926/2110 train_time:37304ms step_avg:40.29ms
step:927/2110 train_time:37364ms step_avg:40.31ms
step:928/2110 train_time:37423ms step_avg:40.33ms
step:929/2110 train_time:37482ms step_avg:40.35ms
step:930/2110 train_time:37541ms step_avg:40.37ms
step:931/2110 train_time:37601ms step_avg:40.39ms
step:932/2110 train_time:37660ms step_avg:40.41ms
step:933/2110 train_time:37721ms step_avg:40.43ms
step:934/2110 train_time:37779ms step_avg:40.45ms
step:935/2110 train_time:37839ms step_avg:40.47ms
step:936/2110 train_time:37897ms step_avg:40.49ms
step:937/2110 train_time:37957ms step_avg:40.51ms
step:938/2110 train_time:38015ms step_avg:40.53ms
step:939/2110 train_time:38075ms step_avg:40.55ms
step:940/2110 train_time:38132ms step_avg:40.57ms
step:941/2110 train_time:38192ms step_avg:40.59ms
step:942/2110 train_time:38251ms step_avg:40.61ms
step:943/2110 train_time:38311ms step_avg:40.63ms
step:944/2110 train_time:38370ms step_avg:40.65ms
step:945/2110 train_time:38430ms step_avg:40.67ms
step:946/2110 train_time:38489ms step_avg:40.69ms
step:947/2110 train_time:38549ms step_avg:40.71ms
step:948/2110 train_time:38609ms step_avg:40.73ms
step:949/2110 train_time:38670ms step_avg:40.75ms
step:950/2110 train_time:38729ms step_avg:40.77ms
step:951/2110 train_time:38790ms step_avg:40.79ms
step:952/2110 train_time:38849ms step_avg:40.81ms
step:953/2110 train_time:38908ms step_avg:40.83ms
step:954/2110 train_time:38966ms step_avg:40.85ms
step:955/2110 train_time:39026ms step_avg:40.86ms
step:956/2110 train_time:39084ms step_avg:40.88ms
step:957/2110 train_time:39144ms step_avg:40.90ms
step:958/2110 train_time:39202ms step_avg:40.92ms
step:959/2110 train_time:39261ms step_avg:40.94ms
step:960/2110 train_time:39319ms step_avg:40.96ms
step:961/2110 train_time:39380ms step_avg:40.98ms
step:962/2110 train_time:39438ms step_avg:41.00ms
step:963/2110 train_time:39499ms step_avg:41.02ms
step:964/2110 train_time:39556ms step_avg:41.03ms
step:965/2110 train_time:39616ms step_avg:41.05ms
step:966/2110 train_time:39675ms step_avg:41.07ms
step:967/2110 train_time:39736ms step_avg:41.09ms
step:968/2110 train_time:39795ms step_avg:41.11ms
step:969/2110 train_time:39854ms step_avg:41.13ms
step:970/2110 train_time:39912ms step_avg:41.15ms
step:971/2110 train_time:39972ms step_avg:41.17ms
step:972/2110 train_time:40031ms step_avg:41.18ms
step:973/2110 train_time:40091ms step_avg:41.20ms
step:974/2110 train_time:40149ms step_avg:41.22ms
step:975/2110 train_time:40209ms step_avg:41.24ms
step:976/2110 train_time:40268ms step_avg:41.26ms
step:977/2110 train_time:40328ms step_avg:41.28ms
step:978/2110 train_time:40388ms step_avg:41.30ms
step:979/2110 train_time:40449ms step_avg:41.32ms
step:980/2110 train_time:40508ms step_avg:41.33ms
step:981/2110 train_time:40568ms step_avg:41.35ms
step:982/2110 train_time:40626ms step_avg:41.37ms
step:983/2110 train_time:40687ms step_avg:41.39ms
step:984/2110 train_time:40746ms step_avg:41.41ms
step:985/2110 train_time:40806ms step_avg:41.43ms
step:986/2110 train_time:40864ms step_avg:41.44ms
step:987/2110 train_time:40924ms step_avg:41.46ms
step:988/2110 train_time:40982ms step_avg:41.48ms
step:989/2110 train_time:41042ms step_avg:41.50ms
step:990/2110 train_time:41100ms step_avg:41.52ms
step:991/2110 train_time:41160ms step_avg:41.53ms
step:992/2110 train_time:41218ms step_avg:41.55ms
step:993/2110 train_time:41278ms step_avg:41.57ms
step:994/2110 train_time:41341ms step_avg:41.59ms
step:995/2110 train_time:41397ms step_avg:41.60ms
step:996/2110 train_time:41455ms step_avg:41.62ms
step:997/2110 train_time:41515ms step_avg:41.64ms
step:998/2110 train_time:41573ms step_avg:41.66ms
step:999/2110 train_time:41634ms step_avg:41.68ms
step:1000/2110 train_time:41693ms step_avg:41.69ms
step:1000/2110 val_loss:3.7557 train_time:41754ms step_avg:41.75ms
step:1001/2110 train_time:41784ms step_avg:41.74ms
step:1002/2110 train_time:41815ms step_avg:41.73ms
step:1003/2110 train_time:41876ms step_avg:41.75ms
step:1004/2110 train_time:41938ms step_avg:41.77ms
step:1005/2110 train_time:41999ms step_avg:41.79ms
step:1006/2110 train_time:42057ms step_avg:41.81ms
step:1007/2110 train_time:42117ms step_avg:41.82ms
step:1008/2110 train_time:42174ms step_avg:41.84ms
step:1009/2110 train_time:42234ms step_avg:41.86ms
step:1010/2110 train_time:42291ms step_avg:41.87ms
step:1011/2110 train_time:42351ms step_avg:41.89ms
step:1012/2110 train_time:42409ms step_avg:41.91ms
step:1013/2110 train_time:42468ms step_avg:41.92ms
step:1014/2110 train_time:42526ms step_avg:41.94ms
step:1015/2110 train_time:42586ms step_avg:41.96ms
step:1016/2110 train_time:42645ms step_avg:41.97ms
step:1017/2110 train_time:42705ms step_avg:41.99ms
step:1018/2110 train_time:42765ms step_avg:42.01ms
step:1019/2110 train_time:42827ms step_avg:42.03ms
step:1020/2110 train_time:42887ms step_avg:42.05ms
step:1021/2110 train_time:42949ms step_avg:42.07ms
step:1022/2110 train_time:43009ms step_avg:42.08ms
step:1023/2110 train_time:43071ms step_avg:42.10ms
step:1024/2110 train_time:43129ms step_avg:42.12ms
step:1025/2110 train_time:43190ms step_avg:42.14ms
step:1026/2110 train_time:43249ms step_avg:42.15ms
step:1027/2110 train_time:43308ms step_avg:42.17ms
step:1028/2110 train_time:43365ms step_avg:42.18ms
step:1029/2110 train_time:43425ms step_avg:42.20ms
step:1030/2110 train_time:43482ms step_avg:42.22ms
step:1031/2110 train_time:43541ms step_avg:42.23ms
step:1032/2110 train_time:43599ms step_avg:42.25ms
step:1033/2110 train_time:43659ms step_avg:42.26ms
step:1034/2110 train_time:43716ms step_avg:42.28ms
step:1035/2110 train_time:43777ms step_avg:42.30ms
step:1036/2110 train_time:43836ms step_avg:42.31ms
step:1037/2110 train_time:43897ms step_avg:42.33ms
step:1038/2110 train_time:43956ms step_avg:42.35ms
step:1039/2110 train_time:44017ms step_avg:42.36ms
step:1040/2110 train_time:44075ms step_avg:42.38ms
step:1041/2110 train_time:44136ms step_avg:42.40ms
step:1042/2110 train_time:44194ms step_avg:42.41ms
step:1043/2110 train_time:44254ms step_avg:42.43ms
step:1044/2110 train_time:44312ms step_avg:42.44ms
step:1045/2110 train_time:44371ms step_avg:42.46ms
step:1046/2110 train_time:44429ms step_avg:42.47ms
step:1047/2110 train_time:44488ms step_avg:42.49ms
step:1048/2110 train_time:44547ms step_avg:42.51ms
step:1049/2110 train_time:44606ms step_avg:42.52ms
step:1050/2110 train_time:44665ms step_avg:42.54ms
step:1051/2110 train_time:44726ms step_avg:42.56ms
step:1052/2110 train_time:44785ms step_avg:42.57ms
step:1053/2110 train_time:44846ms step_avg:42.59ms
step:1054/2110 train_time:44906ms step_avg:42.60ms
step:1055/2110 train_time:44966ms step_avg:42.62ms
step:1056/2110 train_time:45025ms step_avg:42.64ms
step:1057/2110 train_time:45087ms step_avg:42.66ms
step:1058/2110 train_time:45147ms step_avg:42.67ms
step:1059/2110 train_time:45206ms step_avg:42.69ms
step:1060/2110 train_time:45264ms step_avg:42.70ms
step:1061/2110 train_time:45324ms step_avg:42.72ms
step:1062/2110 train_time:45381ms step_avg:42.73ms
step:1063/2110 train_time:45441ms step_avg:42.75ms
step:1064/2110 train_time:45499ms step_avg:42.76ms
step:1065/2110 train_time:45558ms step_avg:42.78ms
step:1066/2110 train_time:45616ms step_avg:42.79ms
step:1067/2110 train_time:45676ms step_avg:42.81ms
step:1068/2110 train_time:45734ms step_avg:42.82ms
step:1069/2110 train_time:45795ms step_avg:42.84ms
step:1070/2110 train_time:45854ms step_avg:42.85ms
step:1071/2110 train_time:45913ms step_avg:42.87ms
step:1072/2110 train_time:45972ms step_avg:42.88ms
step:1073/2110 train_time:46033ms step_avg:42.90ms
step:1074/2110 train_time:46092ms step_avg:42.92ms
step:1075/2110 train_time:46152ms step_avg:42.93ms
step:1076/2110 train_time:46210ms step_avg:42.95ms
step:1077/2110 train_time:46270ms step_avg:42.96ms
step:1078/2110 train_time:46328ms step_avg:42.98ms
step:1079/2110 train_time:46388ms step_avg:42.99ms
step:1080/2110 train_time:46448ms step_avg:43.01ms
step:1081/2110 train_time:46508ms step_avg:43.02ms
step:1082/2110 train_time:46567ms step_avg:43.04ms
step:1083/2110 train_time:46626ms step_avg:43.05ms
step:1084/2110 train_time:46685ms step_avg:43.07ms
step:1085/2110 train_time:46746ms step_avg:43.08ms
step:1086/2110 train_time:46805ms step_avg:43.10ms
step:1087/2110 train_time:46865ms step_avg:43.11ms
step:1088/2110 train_time:46924ms step_avg:43.13ms
step:1089/2110 train_time:46984ms step_avg:43.14ms
step:1090/2110 train_time:47043ms step_avg:43.16ms
step:1091/2110 train_time:47103ms step_avg:43.17ms
step:1092/2110 train_time:47161ms step_avg:43.19ms
step:1093/2110 train_time:47221ms step_avg:43.20ms
step:1094/2110 train_time:47280ms step_avg:43.22ms
step:1095/2110 train_time:47340ms step_avg:43.23ms
step:1096/2110 train_time:47398ms step_avg:43.25ms
step:1097/2110 train_time:47458ms step_avg:43.26ms
step:1098/2110 train_time:47516ms step_avg:43.28ms
step:1099/2110 train_time:47577ms step_avg:43.29ms
step:1100/2110 train_time:47635ms step_avg:43.30ms
step:1101/2110 train_time:47695ms step_avg:43.32ms
step:1102/2110 train_time:47753ms step_avg:43.33ms
step:1103/2110 train_time:47814ms step_avg:43.35ms
step:1104/2110 train_time:47872ms step_avg:43.36ms
step:1105/2110 train_time:47932ms step_avg:43.38ms
step:1106/2110 train_time:47991ms step_avg:43.39ms
step:1107/2110 train_time:48052ms step_avg:43.41ms
step:1108/2110 train_time:48111ms step_avg:43.42ms
step:1109/2110 train_time:48171ms step_avg:43.44ms
step:1110/2110 train_time:48229ms step_avg:43.45ms
step:1111/2110 train_time:48289ms step_avg:43.46ms
step:1112/2110 train_time:48349ms step_avg:43.48ms
step:1113/2110 train_time:48409ms step_avg:43.49ms
step:1114/2110 train_time:48468ms step_avg:43.51ms
step:1115/2110 train_time:48529ms step_avg:43.52ms
step:1116/2110 train_time:48588ms step_avg:43.54ms
step:1117/2110 train_time:48649ms step_avg:43.55ms
step:1118/2110 train_time:48707ms step_avg:43.57ms
step:1119/2110 train_time:48767ms step_avg:43.58ms
step:1120/2110 train_time:48826ms step_avg:43.59ms
step:1121/2110 train_time:48886ms step_avg:43.61ms
step:1122/2110 train_time:48945ms step_avg:43.62ms
step:1123/2110 train_time:49006ms step_avg:43.64ms
step:1124/2110 train_time:49065ms step_avg:43.65ms
step:1125/2110 train_time:49124ms step_avg:43.67ms
step:1126/2110 train_time:49183ms step_avg:43.68ms
step:1127/2110 train_time:49244ms step_avg:43.69ms
step:1128/2110 train_time:49304ms step_avg:43.71ms
step:1129/2110 train_time:49364ms step_avg:43.72ms
step:1130/2110 train_time:49422ms step_avg:43.74ms
step:1131/2110 train_time:49483ms step_avg:43.75ms
step:1132/2110 train_time:49542ms step_avg:43.76ms
step:1133/2110 train_time:49602ms step_avg:43.78ms
step:1134/2110 train_time:49661ms step_avg:43.79ms
step:1135/2110 train_time:49721ms step_avg:43.81ms
step:1136/2110 train_time:49780ms step_avg:43.82ms
step:1137/2110 train_time:49840ms step_avg:43.83ms
step:1138/2110 train_time:49898ms step_avg:43.85ms
step:1139/2110 train_time:49958ms step_avg:43.86ms
step:1140/2110 train_time:50017ms step_avg:43.87ms
step:1141/2110 train_time:50077ms step_avg:43.89ms
step:1142/2110 train_time:50135ms step_avg:43.90ms
step:1143/2110 train_time:50196ms step_avg:43.92ms
step:1144/2110 train_time:50255ms step_avg:43.93ms
step:1145/2110 train_time:50315ms step_avg:43.94ms
step:1146/2110 train_time:50374ms step_avg:43.96ms
step:1147/2110 train_time:50435ms step_avg:43.97ms
step:1148/2110 train_time:50494ms step_avg:43.98ms
step:1149/2110 train_time:50554ms step_avg:44.00ms
step:1150/2110 train_time:50613ms step_avg:44.01ms
step:1151/2110 train_time:50673ms step_avg:44.03ms
step:1152/2110 train_time:50733ms step_avg:44.04ms
step:1153/2110 train_time:50794ms step_avg:44.05ms
step:1154/2110 train_time:50853ms step_avg:44.07ms
step:1155/2110 train_time:50913ms step_avg:44.08ms
step:1156/2110 train_time:50972ms step_avg:44.09ms
step:1157/2110 train_time:51032ms step_avg:44.11ms
step:1158/2110 train_time:51091ms step_avg:44.12ms
step:1159/2110 train_time:51151ms step_avg:44.13ms
step:1160/2110 train_time:51210ms step_avg:44.15ms
step:1161/2110 train_time:51271ms step_avg:44.16ms
step:1162/2110 train_time:51330ms step_avg:44.17ms
step:1163/2110 train_time:51392ms step_avg:44.19ms
step:1164/2110 train_time:51450ms step_avg:44.20ms
step:1165/2110 train_time:51511ms step_avg:44.22ms
step:1166/2110 train_time:51569ms step_avg:44.23ms
step:1167/2110 train_time:51630ms step_avg:44.24ms
step:1168/2110 train_time:51690ms step_avg:44.25ms
step:1169/2110 train_time:51751ms step_avg:44.27ms
step:1170/2110 train_time:51810ms step_avg:44.28ms
step:1171/2110 train_time:51870ms step_avg:44.30ms
step:1172/2110 train_time:51930ms step_avg:44.31ms
step:1173/2110 train_time:51990ms step_avg:44.32ms
step:1174/2110 train_time:52050ms step_avg:44.34ms
step:1175/2110 train_time:52111ms step_avg:44.35ms
step:1176/2110 train_time:52170ms step_avg:44.36ms
step:1177/2110 train_time:52231ms step_avg:44.38ms
step:1178/2110 train_time:52290ms step_avg:44.39ms
step:1179/2110 train_time:52351ms step_avg:44.40ms
step:1180/2110 train_time:52410ms step_avg:44.42ms
step:1181/2110 train_time:52471ms step_avg:44.43ms
step:1182/2110 train_time:52530ms step_avg:44.44ms
step:1183/2110 train_time:52591ms step_avg:44.46ms
step:1184/2110 train_time:52650ms step_avg:44.47ms
step:1185/2110 train_time:52711ms step_avg:44.48ms
step:1186/2110 train_time:52770ms step_avg:44.49ms
step:1187/2110 train_time:52831ms step_avg:44.51ms
step:1188/2110 train_time:52890ms step_avg:44.52ms
step:1189/2110 train_time:52951ms step_avg:44.53ms
step:1190/2110 train_time:53012ms step_avg:44.55ms
step:1191/2110 train_time:53070ms step_avg:44.56ms
step:1192/2110 train_time:53131ms step_avg:44.57ms
step:1193/2110 train_time:53190ms step_avg:44.58ms
step:1194/2110 train_time:53250ms step_avg:44.60ms
step:1195/2110 train_time:53310ms step_avg:44.61ms
step:1196/2110 train_time:53370ms step_avg:44.62ms
step:1197/2110 train_time:53430ms step_avg:44.64ms
step:1198/2110 train_time:53491ms step_avg:44.65ms
step:1199/2110 train_time:53551ms step_avg:44.66ms
step:1200/2110 train_time:53610ms step_avg:44.68ms
step:1201/2110 train_time:53671ms step_avg:44.69ms
step:1202/2110 train_time:53730ms step_avg:44.70ms
step:1203/2110 train_time:53791ms step_avg:44.71ms
step:1204/2110 train_time:53851ms step_avg:44.73ms
step:1205/2110 train_time:53911ms step_avg:44.74ms
step:1206/2110 train_time:53971ms step_avg:44.75ms
step:1207/2110 train_time:54030ms step_avg:44.76ms
step:1208/2110 train_time:54090ms step_avg:44.78ms
step:1209/2110 train_time:54151ms step_avg:44.79ms
step:1210/2110 train_time:54211ms step_avg:44.80ms
step:1211/2110 train_time:54270ms step_avg:44.81ms
step:1212/2110 train_time:54330ms step_avg:44.83ms
step:1213/2110 train_time:54390ms step_avg:44.84ms
step:1214/2110 train_time:54451ms step_avg:44.85ms
step:1215/2110 train_time:54511ms step_avg:44.86ms
step:1216/2110 train_time:54571ms step_avg:44.88ms
step:1217/2110 train_time:54631ms step_avg:44.89ms
step:1218/2110 train_time:54691ms step_avg:44.90ms
step:1219/2110 train_time:54751ms step_avg:44.91ms
step:1220/2110 train_time:54811ms step_avg:44.93ms
step:1221/2110 train_time:54871ms step_avg:44.94ms
step:1222/2110 train_time:54930ms step_avg:44.95ms
step:1223/2110 train_time:54990ms step_avg:44.96ms
step:1224/2110 train_time:55051ms step_avg:44.98ms
step:1225/2110 train_time:55110ms step_avg:44.99ms
step:1226/2110 train_time:55171ms step_avg:45.00ms
step:1227/2110 train_time:55230ms step_avg:45.01ms
step:1228/2110 train_time:55290ms step_avg:45.02ms
step:1229/2110 train_time:55350ms step_avg:45.04ms
step:1230/2110 train_time:55410ms step_avg:45.05ms
step:1231/2110 train_time:55470ms step_avg:45.06ms
step:1232/2110 train_time:55531ms step_avg:45.07ms
step:1233/2110 train_time:55590ms step_avg:45.08ms
step:1234/2110 train_time:55650ms step_avg:45.10ms
step:1235/2110 train_time:55711ms step_avg:45.11ms
step:1236/2110 train_time:55770ms step_avg:45.12ms
step:1237/2110 train_time:55831ms step_avg:45.13ms
step:1238/2110 train_time:55891ms step_avg:45.15ms
step:1239/2110 train_time:55951ms step_avg:45.16ms
step:1240/2110 train_time:56011ms step_avg:45.17ms
step:1241/2110 train_time:56070ms step_avg:45.18ms
step:1242/2110 train_time:56130ms step_avg:45.19ms
step:1243/2110 train_time:56190ms step_avg:45.21ms
step:1244/2110 train_time:56251ms step_avg:45.22ms
step:1245/2110 train_time:56309ms step_avg:45.23ms
step:1246/2110 train_time:56370ms step_avg:45.24ms
step:1247/2110 train_time:56430ms step_avg:45.25ms
step:1248/2110 train_time:56490ms step_avg:45.26ms
step:1249/2110 train_time:56551ms step_avg:45.28ms
step:1250/2110 train_time:56611ms step_avg:45.29ms
step:1250/2110 val_loss:3.5921 train_time:56672ms step_avg:45.34ms
step:1251/2110 train_time:56701ms step_avg:45.32ms
step:1252/2110 train_time:56733ms step_avg:45.31ms
step:1253/2110 train_time:56797ms step_avg:45.33ms
step:1254/2110 train_time:56858ms step_avg:45.34ms
step:1255/2110 train_time:56920ms step_avg:45.35ms
step:1256/2110 train_time:56979ms step_avg:45.37ms
step:1257/2110 train_time:57039ms step_avg:45.38ms
step:1258/2110 train_time:57099ms step_avg:45.39ms
step:1259/2110 train_time:57158ms step_avg:45.40ms
step:1260/2110 train_time:57217ms step_avg:45.41ms
step:1261/2110 train_time:57277ms step_avg:45.42ms
step:1262/2110 train_time:57336ms step_avg:45.43ms
step:1263/2110 train_time:57395ms step_avg:45.44ms
step:1264/2110 train_time:57456ms step_avg:45.46ms
step:1265/2110 train_time:57515ms step_avg:45.47ms
step:1266/2110 train_time:57574ms step_avg:45.48ms
step:1267/2110 train_time:57635ms step_avg:45.49ms
step:1268/2110 train_time:57696ms step_avg:45.50ms
step:1269/2110 train_time:57759ms step_avg:45.52ms
step:1270/2110 train_time:57820ms step_avg:45.53ms
step:1271/2110 train_time:57880ms step_avg:45.54ms
step:1272/2110 train_time:57940ms step_avg:45.55ms
step:1273/2110 train_time:58000ms step_avg:45.56ms
step:1274/2110 train_time:58059ms step_avg:45.57ms
step:1275/2110 train_time:58119ms step_avg:45.58ms
step:1276/2110 train_time:58178ms step_avg:45.59ms
step:1277/2110 train_time:58238ms step_avg:45.61ms
step:1278/2110 train_time:58298ms step_avg:45.62ms
step:1279/2110 train_time:58358ms step_avg:45.63ms
step:1280/2110 train_time:58419ms step_avg:45.64ms
step:1281/2110 train_time:58477ms step_avg:45.65ms
step:1282/2110 train_time:58537ms step_avg:45.66ms
step:1283/2110 train_time:58598ms step_avg:45.67ms
step:1284/2110 train_time:58658ms step_avg:45.68ms
step:1285/2110 train_time:58720ms step_avg:45.70ms
step:1286/2110 train_time:58780ms step_avg:45.71ms
step:1287/2110 train_time:58841ms step_avg:45.72ms
step:1288/2110 train_time:58901ms step_avg:45.73ms
step:1289/2110 train_time:58960ms step_avg:45.74ms
step:1290/2110 train_time:59020ms step_avg:45.75ms
step:1291/2110 train_time:59080ms step_avg:45.76ms
step:1292/2110 train_time:59140ms step_avg:45.77ms
step:1293/2110 train_time:59199ms step_avg:45.78ms
step:1294/2110 train_time:59259ms step_avg:45.80ms
step:1295/2110 train_time:59319ms step_avg:45.81ms
step:1296/2110 train_time:59378ms step_avg:45.82ms
step:1297/2110 train_time:59437ms step_avg:45.83ms
step:1298/2110 train_time:59497ms step_avg:45.84ms
step:1299/2110 train_time:59556ms step_avg:45.85ms
step:1300/2110 train_time:59616ms step_avg:45.86ms
step:1301/2110 train_time:59677ms step_avg:45.87ms
step:1302/2110 train_time:59739ms step_avg:45.88ms
step:1303/2110 train_time:59800ms step_avg:45.89ms
step:1304/2110 train_time:59860ms step_avg:45.90ms
step:1305/2110 train_time:59920ms step_avg:45.92ms
step:1306/2110 train_time:59980ms step_avg:45.93ms
step:1307/2110 train_time:60040ms step_avg:45.94ms
step:1308/2110 train_time:60100ms step_avg:45.95ms
step:1309/2110 train_time:60159ms step_avg:45.96ms
step:1310/2110 train_time:60219ms step_avg:45.97ms
step:1311/2110 train_time:60278ms step_avg:45.98ms
step:1312/2110 train_time:60339ms step_avg:45.99ms
step:1313/2110 train_time:60397ms step_avg:46.00ms
step:1314/2110 train_time:60457ms step_avg:46.01ms
step:1315/2110 train_time:60517ms step_avg:46.02ms
step:1316/2110 train_time:60577ms step_avg:46.03ms
step:1317/2110 train_time:60636ms step_avg:46.04ms
step:1318/2110 train_time:60698ms step_avg:46.05ms
step:1319/2110 train_time:60758ms step_avg:46.06ms
step:1320/2110 train_time:60818ms step_avg:46.07ms
step:1321/2110 train_time:60879ms step_avg:46.09ms
step:1322/2110 train_time:60939ms step_avg:46.10ms
step:1323/2110 train_time:60999ms step_avg:46.11ms
step:1324/2110 train_time:61059ms step_avg:46.12ms
step:1325/2110 train_time:61119ms step_avg:46.13ms
step:1326/2110 train_time:61178ms step_avg:46.14ms
step:1327/2110 train_time:61238ms step_avg:46.15ms
step:1328/2110 train_time:61298ms step_avg:46.16ms
step:1329/2110 train_time:61358ms step_avg:46.17ms
step:1330/2110 train_time:61418ms step_avg:46.18ms
step:1331/2110 train_time:61477ms step_avg:46.19ms
step:1332/2110 train_time:61537ms step_avg:46.20ms
step:1333/2110 train_time:61597ms step_avg:46.21ms
step:1334/2110 train_time:61658ms step_avg:46.22ms
step:1335/2110 train_time:61719ms step_avg:46.23ms
step:1336/2110 train_time:61779ms step_avg:46.24ms
step:1337/2110 train_time:61839ms step_avg:46.25ms
step:1338/2110 train_time:61900ms step_avg:46.26ms
step:1339/2110 train_time:61959ms step_avg:46.27ms
step:1340/2110 train_time:62022ms step_avg:46.28ms
step:1341/2110 train_time:62079ms step_avg:46.29ms
step:1342/2110 train_time:62139ms step_avg:46.30ms
step:1343/2110 train_time:62198ms step_avg:46.31ms
step:1344/2110 train_time:62259ms step_avg:46.32ms
step:1345/2110 train_time:62317ms step_avg:46.33ms
step:1346/2110 train_time:62377ms step_avg:46.34ms
step:1347/2110 train_time:62438ms step_avg:46.35ms
step:1348/2110 train_time:62497ms step_avg:46.36ms
step:1349/2110 train_time:62557ms step_avg:46.37ms
step:1350/2110 train_time:62617ms step_avg:46.38ms
step:1351/2110 train_time:62677ms step_avg:46.39ms
step:1352/2110 train_time:62737ms step_avg:46.40ms
step:1353/2110 train_time:62798ms step_avg:46.41ms
step:1354/2110 train_time:62858ms step_avg:46.42ms
step:1355/2110 train_time:62918ms step_avg:46.43ms
step:1356/2110 train_time:62978ms step_avg:46.44ms
step:1357/2110 train_time:63038ms step_avg:46.45ms
step:1358/2110 train_time:63099ms step_avg:46.46ms
step:1359/2110 train_time:63159ms step_avg:46.47ms
step:1360/2110 train_time:63218ms step_avg:46.48ms
step:1361/2110 train_time:63277ms step_avg:46.49ms
step:1362/2110 train_time:63337ms step_avg:46.50ms
step:1363/2110 train_time:63398ms step_avg:46.51ms
step:1364/2110 train_time:63458ms step_avg:46.52ms
step:1365/2110 train_time:63517ms step_avg:46.53ms
step:1366/2110 train_time:63577ms step_avg:46.54ms
step:1367/2110 train_time:63637ms step_avg:46.55ms
step:1368/2110 train_time:63699ms step_avg:46.56ms
step:1369/2110 train_time:63758ms step_avg:46.57ms
step:1370/2110 train_time:63819ms step_avg:46.58ms
step:1371/2110 train_time:63879ms step_avg:46.59ms
step:1372/2110 train_time:63939ms step_avg:46.60ms
step:1373/2110 train_time:63999ms step_avg:46.61ms
step:1374/2110 train_time:64060ms step_avg:46.62ms
step:1375/2110 train_time:64119ms step_avg:46.63ms
step:1376/2110 train_time:64179ms step_avg:46.64ms
step:1377/2110 train_time:64239ms step_avg:46.65ms
step:1378/2110 train_time:64300ms step_avg:46.66ms
step:1379/2110 train_time:64359ms step_avg:46.67ms
step:1380/2110 train_time:64420ms step_avg:46.68ms
step:1381/2110 train_time:64479ms step_avg:46.69ms
step:1382/2110 train_time:64566ms step_avg:46.72ms
step:1383/2110 train_time:64653ms step_avg:46.75ms
step:1384/2110 train_time:64741ms step_avg:46.78ms
step:1385/2110 train_time:64827ms step_avg:46.81ms
step:1386/2110 train_time:64915ms step_avg:46.84ms
step:1387/2110 train_time:65002ms step_avg:46.86ms
step:1388/2110 train_time:65089ms step_avg:46.89ms
step:1389/2110 train_time:65175ms step_avg:46.92ms
step:1390/2110 train_time:65262ms step_avg:46.95ms
step:1391/2110 train_time:65348ms step_avg:46.98ms
step:1392/2110 train_time:65435ms step_avg:47.01ms
step:1393/2110 train_time:65520ms step_avg:47.04ms
step:1394/2110 train_time:65607ms step_avg:47.06ms
step:1395/2110 train_time:65694ms step_avg:47.09ms
step:1396/2110 train_time:65781ms step_avg:47.12ms
step:1397/2110 train_time:65868ms step_avg:47.15ms
step:1398/2110 train_time:65955ms step_avg:47.18ms
step:1399/2110 train_time:66042ms step_avg:47.21ms
step:1400/2110 train_time:66130ms step_avg:47.24ms
step:1401/2110 train_time:66215ms step_avg:47.26ms
step:1402/2110 train_time:66302ms step_avg:47.29ms
step:1403/2110 train_time:66388ms step_avg:47.32ms
step:1404/2110 train_time:66474ms step_avg:47.35ms
step:1405/2110 train_time:66561ms step_avg:47.37ms
step:1406/2110 train_time:66648ms step_avg:47.40ms
step:1407/2110 train_time:66735ms step_avg:47.43ms
step:1408/2110 train_time:66823ms step_avg:47.46ms
step:1409/2110 train_time:66910ms step_avg:47.49ms
step:1410/2110 train_time:66996ms step_avg:47.52ms
step:1411/2110 train_time:67083ms step_avg:47.54ms
step:1412/2110 train_time:67171ms step_avg:47.57ms
step:1413/2110 train_time:67256ms step_avg:47.60ms
step:1414/2110 train_time:67344ms step_avg:47.63ms
step:1415/2110 train_time:67432ms step_avg:47.65ms
step:1416/2110 train_time:67519ms step_avg:47.68ms
step:1417/2110 train_time:67604ms step_avg:47.71ms
step:1418/2110 train_time:67692ms step_avg:47.74ms
step:1419/2110 train_time:67778ms step_avg:47.76ms
step:1420/2110 train_time:67866ms step_avg:47.79ms
step:1421/2110 train_time:67953ms step_avg:47.82ms
step:1422/2110 train_time:68040ms step_avg:47.85ms
step:1423/2110 train_time:68127ms step_avg:47.88ms
step:1424/2110 train_time:68214ms step_avg:47.90ms
step:1425/2110 train_time:68299ms step_avg:47.93ms
step:1426/2110 train_time:68387ms step_avg:47.96ms
step:1427/2110 train_time:68475ms step_avg:47.99ms
step:1428/2110 train_time:68561ms step_avg:48.01ms
step:1429/2110 train_time:68647ms step_avg:48.04ms
step:1430/2110 train_time:68734ms step_avg:48.07ms
step:1431/2110 train_time:68820ms step_avg:48.09ms
step:1432/2110 train_time:68907ms step_avg:48.12ms
step:1433/2110 train_time:68994ms step_avg:48.15ms
step:1434/2110 train_time:69081ms step_avg:48.17ms
step:1435/2110 train_time:69168ms step_avg:48.20ms
step:1436/2110 train_time:69254ms step_avg:48.23ms
step:1437/2110 train_time:69341ms step_avg:48.25ms
step:1438/2110 train_time:69429ms step_avg:48.28ms
step:1439/2110 train_time:69515ms step_avg:48.31ms
step:1440/2110 train_time:69603ms step_avg:48.34ms
step:1441/2110 train_time:69689ms step_avg:48.36ms
step:1442/2110 train_time:69775ms step_avg:48.39ms
step:1443/2110 train_time:69862ms step_avg:48.41ms
step:1444/2110 train_time:69950ms step_avg:48.44ms
step:1445/2110 train_time:70036ms step_avg:48.47ms
step:1446/2110 train_time:70122ms step_avg:48.49ms
step:1447/2110 train_time:70210ms step_avg:48.52ms
step:1448/2110 train_time:70296ms step_avg:48.55ms
step:1449/2110 train_time:70383ms step_avg:48.57ms
step:1450/2110 train_time:70470ms step_avg:48.60ms
step:1451/2110 train_time:70556ms step_avg:48.63ms
step:1452/2110 train_time:70644ms step_avg:48.65ms
step:1453/2110 train_time:70731ms step_avg:48.68ms
step:1454/2110 train_time:70818ms step_avg:48.71ms
step:1455/2110 train_time:70904ms step_avg:48.73ms
step:1456/2110 train_time:70991ms step_avg:48.76ms
step:1457/2110 train_time:71077ms step_avg:48.78ms
step:1458/2110 train_time:71164ms step_avg:48.81ms
step:1459/2110 train_time:71251ms step_avg:48.84ms
step:1460/2110 train_time:71337ms step_avg:48.86ms
step:1461/2110 train_time:71424ms step_avg:48.89ms
step:1462/2110 train_time:71510ms step_avg:48.91ms
step:1463/2110 train_time:71596ms step_avg:48.94ms
step:1464/2110 train_time:71684ms step_avg:48.96ms
step:1465/2110 train_time:71772ms step_avg:48.99ms
step:1466/2110 train_time:71859ms step_avg:49.02ms
step:1467/2110 train_time:71946ms step_avg:49.04ms
step:1468/2110 train_time:72033ms step_avg:49.07ms
step:1469/2110 train_time:72119ms step_avg:49.09ms
step:1470/2110 train_time:72206ms step_avg:49.12ms
step:1471/2110 train_time:72293ms step_avg:49.15ms
step:1472/2110 train_time:72379ms step_avg:49.17ms
step:1473/2110 train_time:72467ms step_avg:49.20ms
step:1474/2110 train_time:72554ms step_avg:49.22ms
step:1475/2110 train_time:72639ms step_avg:49.25ms
step:1476/2110 train_time:72727ms step_avg:49.27ms
step:1477/2110 train_time:72813ms step_avg:49.30ms
step:1478/2110 train_time:72903ms step_avg:49.33ms
step:1479/2110 train_time:72988ms step_avg:49.35ms
step:1480/2110 train_time:73075ms step_avg:49.37ms
step:1481/2110 train_time:73161ms step_avg:49.40ms
step:1482/2110 train_time:73248ms step_avg:49.43ms
step:1483/2110 train_time:73334ms step_avg:49.45ms
step:1484/2110 train_time:73421ms step_avg:49.48ms
step:1485/2110 train_time:73507ms step_avg:49.50ms
step:1486/2110 train_time:73594ms step_avg:49.53ms
step:1487/2110 train_time:73681ms step_avg:49.55ms
step:1488/2110 train_time:73768ms step_avg:49.58ms
step:1489/2110 train_time:73855ms step_avg:49.60ms
step:1490/2110 train_time:73943ms step_avg:49.63ms
step:1491/2110 train_time:74028ms step_avg:49.65ms
step:1492/2110 train_time:74116ms step_avg:49.68ms
step:1493/2110 train_time:74202ms step_avg:49.70ms
step:1494/2110 train_time:74289ms step_avg:49.73ms
step:1495/2110 train_time:74375ms step_avg:49.75ms
step:1496/2110 train_time:74463ms step_avg:49.77ms
step:1497/2110 train_time:74549ms step_avg:49.80ms
step:1498/2110 train_time:74636ms step_avg:49.82ms
step:1499/2110 train_time:74722ms step_avg:49.85ms
step:1500/2110 train_time:74810ms step_avg:49.87ms
step:1500/2110 val_loss:3.4916 train_time:74897ms step_avg:49.93ms
step:1501/2110 train_time:74930ms step_avg:49.92ms
step:1502/2110 train_time:74987ms step_avg:49.92ms
step:1503/2110 train_time:75080ms step_avg:49.95ms
step:1504/2110 train_time:75170ms step_avg:49.98ms
step:1505/2110 train_time:75257ms step_avg:50.00ms
step:1506/2110 train_time:75343ms step_avg:50.03ms
step:1507/2110 train_time:75428ms step_avg:50.05ms
step:1508/2110 train_time:75515ms step_avg:50.08ms
step:1509/2110 train_time:75600ms step_avg:50.10ms
step:1510/2110 train_time:75686ms step_avg:50.12ms
step:1511/2110 train_time:75771ms step_avg:50.15ms
step:1512/2110 train_time:75857ms step_avg:50.17ms
step:1513/2110 train_time:75947ms step_avg:50.20ms
step:1514/2110 train_time:76036ms step_avg:50.22ms
step:1515/2110 train_time:76125ms step_avg:50.25ms
step:1516/2110 train_time:76214ms step_avg:50.27ms
step:1517/2110 train_time:76300ms step_avg:50.30ms
step:1518/2110 train_time:76387ms step_avg:50.32ms
step:1519/2110 train_time:76473ms step_avg:50.34ms
step:1520/2110 train_time:76559ms step_avg:50.37ms
step:1521/2110 train_time:76644ms step_avg:50.39ms
step:1522/2110 train_time:76730ms step_avg:50.41ms
step:1523/2110 train_time:76816ms step_avg:50.44ms
step:1524/2110 train_time:76903ms step_avg:50.46ms
step:1525/2110 train_time:76991ms step_avg:50.49ms
step:1526/2110 train_time:77079ms step_avg:50.51ms
step:1527/2110 train_time:77168ms step_avg:50.54ms
step:1528/2110 train_time:77255ms step_avg:50.56ms
step:1529/2110 train_time:77341ms step_avg:50.58ms
step:1530/2110 train_time:77427ms step_avg:50.61ms
step:1531/2110 train_time:77513ms step_avg:50.63ms
step:1532/2110 train_time:77600ms step_avg:50.65ms
step:1533/2110 train_time:77685ms step_avg:50.68ms
step:1534/2110 train_time:77772ms step_avg:50.70ms
step:1535/2110 train_time:77857ms step_avg:50.72ms
step:1536/2110 train_time:77945ms step_avg:50.75ms
step:1537/2110 train_time:78032ms step_avg:50.77ms
step:1538/2110 train_time:78119ms step_avg:50.79ms
step:1539/2110 train_time:78208ms step_avg:50.82ms
step:1540/2110 train_time:78295ms step_avg:50.84ms
step:1541/2110 train_time:78381ms step_avg:50.86ms
step:1542/2110 train_time:78468ms step_avg:50.89ms
step:1543/2110 train_time:78554ms step_avg:50.91ms
step:1544/2110 train_time:78640ms step_avg:50.93ms
step:1545/2110 train_time:78726ms step_avg:50.96ms
step:1546/2110 train_time:78813ms step_avg:50.98ms
step:1547/2110 train_time:78899ms step_avg:51.00ms
step:1548/2110 train_time:78987ms step_avg:51.03ms
step:1549/2110 train_time:79076ms step_avg:51.05ms
step:1550/2110 train_time:79162ms step_avg:51.07ms
step:1551/2110 train_time:79250ms step_avg:51.10ms
step:1552/2110 train_time:79336ms step_avg:51.12ms
step:1553/2110 train_time:79422ms step_avg:51.14ms
step:1554/2110 train_time:79509ms step_avg:51.16ms
step:1555/2110 train_time:79594ms step_avg:51.19ms
step:1556/2110 train_time:79682ms step_avg:51.21ms
step:1557/2110 train_time:79767ms step_avg:51.23ms
step:1558/2110 train_time:79857ms step_avg:51.26ms
step:1559/2110 train_time:79941ms step_avg:51.28ms
step:1560/2110 train_time:80029ms step_avg:51.30ms
step:1561/2110 train_time:80115ms step_avg:51.32ms
step:1562/2110 train_time:80202ms step_avg:51.35ms
step:1563/2110 train_time:80289ms step_avg:51.37ms
step:1564/2110 train_time:80377ms step_avg:51.39ms
step:1565/2110 train_time:80463ms step_avg:51.41ms
step:1566/2110 train_time:80549ms step_avg:51.44ms
step:1567/2110 train_time:80635ms step_avg:51.46ms
step:1568/2110 train_time:80722ms step_avg:51.48ms
step:1569/2110 train_time:80809ms step_avg:51.50ms
step:1570/2110 train_time:80896ms step_avg:51.53ms
step:1571/2110 train_time:80983ms step_avg:51.55ms
step:1572/2110 train_time:81070ms step_avg:51.57ms
step:1573/2110 train_time:81158ms step_avg:51.59ms
step:1574/2110 train_time:81246ms step_avg:51.62ms
step:1575/2110 train_time:81333ms step_avg:51.64ms
step:1576/2110 train_time:81419ms step_avg:51.66ms
step:1577/2110 train_time:81506ms step_avg:51.68ms
step:1578/2110 train_time:81593ms step_avg:51.71ms
step:1579/2110 train_time:81678ms step_avg:51.73ms
step:1580/2110 train_time:81765ms step_avg:51.75ms
step:1581/2110 train_time:81851ms step_avg:51.77ms
step:1582/2110 train_time:81938ms step_avg:51.79ms
step:1583/2110 train_time:82025ms step_avg:51.82ms
step:1584/2110 train_time:82113ms step_avg:51.84ms
step:1585/2110 train_time:82199ms step_avg:51.86ms
step:1586/2110 train_time:82287ms step_avg:51.88ms
step:1587/2110 train_time:82374ms step_avg:51.91ms
step:1588/2110 train_time:82461ms step_avg:51.93ms
step:1589/2110 train_time:82548ms step_avg:51.95ms
step:1590/2110 train_time:82635ms step_avg:51.97ms
step:1591/2110 train_time:82720ms step_avg:51.99ms
step:1592/2110 train_time:82807ms step_avg:52.01ms
step:1593/2110 train_time:82893ms step_avg:52.04ms
step:1594/2110 train_time:82980ms step_avg:52.06ms
step:1595/2110 train_time:83067ms step_avg:52.08ms
step:1596/2110 train_time:83153ms step_avg:52.10ms
step:1597/2110 train_time:83240ms step_avg:52.12ms
step:1598/2110 train_time:83326ms step_avg:52.14ms
step:1599/2110 train_time:83414ms step_avg:52.17ms
step:1600/2110 train_time:83501ms step_avg:52.19ms
step:1601/2110 train_time:83588ms step_avg:52.21ms
step:1602/2110 train_time:83675ms step_avg:52.23ms
step:1603/2110 train_time:83761ms step_avg:52.25ms
step:1604/2110 train_time:83848ms step_avg:52.27ms
step:1605/2110 train_time:83933ms step_avg:52.30ms
step:1606/2110 train_time:84020ms step_avg:52.32ms
step:1607/2110 train_time:84107ms step_avg:52.34ms
step:1608/2110 train_time:84195ms step_avg:52.36ms
step:1609/2110 train_time:84281ms step_avg:52.38ms
step:1610/2110 train_time:84369ms step_avg:52.40ms
step:1611/2110 train_time:84456ms step_avg:52.42ms
step:1612/2110 train_time:84544ms step_avg:52.45ms
step:1613/2110 train_time:84631ms step_avg:52.47ms
step:1614/2110 train_time:84717ms step_avg:52.49ms
step:1615/2110 train_time:84804ms step_avg:52.51ms
step:1616/2110 train_time:84891ms step_avg:52.53ms
step:1617/2110 train_time:84977ms step_avg:52.55ms
step:1618/2110 train_time:85064ms step_avg:52.57ms
step:1619/2110 train_time:85151ms step_avg:52.59ms
step:1620/2110 train_time:85238ms step_avg:52.62ms
step:1621/2110 train_time:85325ms step_avg:52.64ms
step:1622/2110 train_time:85413ms step_avg:52.66ms
step:1623/2110 train_time:85499ms step_avg:52.68ms
step:1624/2110 train_time:85587ms step_avg:52.70ms
step:1625/2110 train_time:85674ms step_avg:52.72ms
step:1626/2110 train_time:85761ms step_avg:52.74ms
step:1627/2110 train_time:85848ms step_avg:52.76ms
step:1628/2110 train_time:85934ms step_avg:52.79ms
step:1629/2110 train_time:86021ms step_avg:52.81ms
step:1630/2110 train_time:86108ms step_avg:52.83ms
step:1631/2110 train_time:86194ms step_avg:52.85ms
step:1632/2110 train_time:86281ms step_avg:52.87ms
step:1633/2110 train_time:86368ms step_avg:52.89ms
step:1634/2110 train_time:86456ms step_avg:52.91ms
step:1635/2110 train_time:86541ms step_avg:52.93ms
step:1636/2110 train_time:86629ms step_avg:52.95ms
step:1637/2110 train_time:86717ms step_avg:52.97ms
step:1638/2110 train_time:86805ms step_avg:52.99ms
step:1639/2110 train_time:86891ms step_avg:53.01ms
step:1640/2110 train_time:86977ms step_avg:53.03ms
step:1641/2110 train_time:87065ms step_avg:53.06ms
step:1642/2110 train_time:87151ms step_avg:53.08ms
step:1643/2110 train_time:87238ms step_avg:53.10ms
step:1644/2110 train_time:87326ms step_avg:53.12ms
step:1645/2110 train_time:87412ms step_avg:53.14ms
step:1646/2110 train_time:87499ms step_avg:53.16ms
step:1647/2110 train_time:87586ms step_avg:53.18ms
step:1648/2110 train_time:87673ms step_avg:53.20ms
step:1649/2110 train_time:87759ms step_avg:53.22ms
step:1650/2110 train_time:87846ms step_avg:53.24ms
step:1651/2110 train_time:87933ms step_avg:53.26ms
step:1652/2110 train_time:88019ms step_avg:53.28ms
step:1653/2110 train_time:88107ms step_avg:53.30ms
step:1654/2110 train_time:88195ms step_avg:53.32ms
step:1655/2110 train_time:88281ms step_avg:53.34ms
step:1656/2110 train_time:88369ms step_avg:53.36ms
step:1657/2110 train_time:88456ms step_avg:53.38ms
step:1658/2110 train_time:88544ms step_avg:53.40ms
step:1659/2110 train_time:88632ms step_avg:53.43ms
step:1660/2110 train_time:88720ms step_avg:53.45ms
step:1661/2110 train_time:88808ms step_avg:53.47ms
step:1662/2110 train_time:88896ms step_avg:53.49ms
step:1663/2110 train_time:88984ms step_avg:53.51ms
step:1664/2110 train_time:89072ms step_avg:53.53ms
step:1665/2110 train_time:89160ms step_avg:53.55ms
step:1666/2110 train_time:89249ms step_avg:53.57ms
step:1667/2110 train_time:89337ms step_avg:53.59ms
step:1668/2110 train_time:89425ms step_avg:53.61ms
step:1669/2110 train_time:89513ms step_avg:53.63ms
step:1670/2110 train_time:89600ms step_avg:53.65ms
step:1671/2110 train_time:89688ms step_avg:53.67ms
step:1672/2110 train_time:89778ms step_avg:53.69ms
step:1673/2110 train_time:89865ms step_avg:53.71ms
step:1674/2110 train_time:89953ms step_avg:53.74ms
step:1675/2110 train_time:90040ms step_avg:53.76ms
step:1676/2110 train_time:90129ms step_avg:53.78ms
step:1677/2110 train_time:90216ms step_avg:53.80ms
step:1678/2110 train_time:90303ms step_avg:53.82ms
step:1679/2110 train_time:90392ms step_avg:53.84ms
step:1680/2110 train_time:90480ms step_avg:53.86ms
step:1681/2110 train_time:90568ms step_avg:53.88ms
step:1682/2110 train_time:90656ms step_avg:53.90ms
step:1683/2110 train_time:90744ms step_avg:53.92ms
step:1684/2110 train_time:90831ms step_avg:53.94ms
step:1685/2110 train_time:90920ms step_avg:53.96ms
step:1686/2110 train_time:91008ms step_avg:53.98ms
step:1687/2110 train_time:91096ms step_avg:54.00ms
step:1688/2110 train_time:91185ms step_avg:54.02ms
step:1689/2110 train_time:91272ms step_avg:54.04ms
step:1690/2110 train_time:91360ms step_avg:54.06ms
step:1691/2110 train_time:91449ms step_avg:54.08ms
step:1692/2110 train_time:91537ms step_avg:54.10ms
step:1693/2110 train_time:91626ms step_avg:54.12ms
step:1694/2110 train_time:91714ms step_avg:54.14ms
step:1695/2110 train_time:91801ms step_avg:54.16ms
step:1696/2110 train_time:91890ms step_avg:54.18ms
step:1697/2110 train_time:91978ms step_avg:54.20ms
step:1698/2110 train_time:92067ms step_avg:54.22ms
step:1699/2110 train_time:92154ms step_avg:54.24ms
step:1700/2110 train_time:92242ms step_avg:54.26ms
step:1701/2110 train_time:92331ms step_avg:54.28ms
step:1702/2110 train_time:92418ms step_avg:54.30ms
step:1703/2110 train_time:92508ms step_avg:54.32ms
step:1704/2110 train_time:92596ms step_avg:54.34ms
step:1705/2110 train_time:92684ms step_avg:54.36ms
step:1706/2110 train_time:92774ms step_avg:54.38ms
step:1707/2110 train_time:92860ms step_avg:54.40ms
step:1708/2110 train_time:92950ms step_avg:54.42ms
step:1709/2110 train_time:93037ms step_avg:54.44ms
step:1710/2110 train_time:93126ms step_avg:54.46ms
step:1711/2110 train_time:93214ms step_avg:54.48ms
step:1712/2110 train_time:93302ms step_avg:54.50ms
step:1713/2110 train_time:93389ms step_avg:54.52ms
step:1714/2110 train_time:93477ms step_avg:54.54ms
step:1715/2110 train_time:93565ms step_avg:54.56ms
step:1716/2110 train_time:93654ms step_avg:54.58ms
step:1717/2110 train_time:93742ms step_avg:54.60ms
step:1718/2110 train_time:93829ms step_avg:54.62ms
step:1719/2110 train_time:93917ms step_avg:54.63ms
step:1720/2110 train_time:94006ms step_avg:54.65ms
step:1721/2110 train_time:94094ms step_avg:54.67ms
step:1722/2110 train_time:94182ms step_avg:54.69ms
step:1723/2110 train_time:94271ms step_avg:54.71ms
step:1724/2110 train_time:94360ms step_avg:54.73ms
step:1725/2110 train_time:94446ms step_avg:54.75ms
step:1726/2110 train_time:94535ms step_avg:54.77ms
step:1727/2110 train_time:94623ms step_avg:54.79ms
step:1728/2110 train_time:94711ms step_avg:54.81ms
step:1729/2110 train_time:94799ms step_avg:54.83ms
step:1730/2110 train_time:94887ms step_avg:54.85ms
step:1731/2110 train_time:94975ms step_avg:54.87ms
step:1732/2110 train_time:95064ms step_avg:54.89ms
step:1733/2110 train_time:95152ms step_avg:54.91ms
step:1734/2110 train_time:95239ms step_avg:54.92ms
step:1735/2110 train_time:95327ms step_avg:54.94ms
step:1736/2110 train_time:95417ms step_avg:54.96ms
step:1737/2110 train_time:95503ms step_avg:54.98ms
step:1738/2110 train_time:95592ms step_avg:55.00ms
step:1739/2110 train_time:95680ms step_avg:55.02ms
step:1740/2110 train_time:95769ms step_avg:55.04ms
step:1741/2110 train_time:95857ms step_avg:55.06ms
step:1742/2110 train_time:95945ms step_avg:55.08ms
step:1743/2110 train_time:96034ms step_avg:55.10ms
step:1744/2110 train_time:96121ms step_avg:55.12ms
step:1745/2110 train_time:96210ms step_avg:55.13ms
step:1746/2110 train_time:96299ms step_avg:55.15ms
step:1747/2110 train_time:96387ms step_avg:55.17ms
step:1748/2110 train_time:96476ms step_avg:55.19ms
step:1749/2110 train_time:96563ms step_avg:55.21ms
step:1750/2110 train_time:96652ms step_avg:55.23ms
step:1750/2110 val_loss:3.3777 train_time:96739ms step_avg:55.28ms
step:1751/2110 train_time:96774ms step_avg:55.27ms
step:1752/2110 train_time:96831ms step_avg:55.27ms
step:1753/2110 train_time:96924ms step_avg:55.29ms
step:1754/2110 train_time:97011ms step_avg:55.31ms
step:1755/2110 train_time:97101ms step_avg:55.33ms
step:1756/2110 train_time:97187ms step_avg:55.35ms
step:1757/2110 train_time:97273ms step_avg:55.36ms
step:1758/2110 train_time:97361ms step_avg:55.38ms
step:1759/2110 train_time:97446ms step_avg:55.40ms
step:1760/2110 train_time:97534ms step_avg:55.42ms
step:1761/2110 train_time:97621ms step_avg:55.43ms
step:1762/2110 train_time:97710ms step_avg:55.45ms
step:1763/2110 train_time:97800ms step_avg:55.47ms
step:1764/2110 train_time:97890ms step_avg:55.49ms
step:1765/2110 train_time:97980ms step_avg:55.51ms
step:1766/2110 train_time:98068ms step_avg:55.53ms
step:1767/2110 train_time:98155ms step_avg:55.55ms
step:1768/2110 train_time:98242ms step_avg:55.57ms
step:1769/2110 train_time:98328ms step_avg:55.58ms
step:1770/2110 train_time:98416ms step_avg:55.60ms
step:1771/2110 train_time:98503ms step_avg:55.62ms
step:1772/2110 train_time:98590ms step_avg:55.64ms
step:1773/2110 train_time:98678ms step_avg:55.66ms
step:1774/2110 train_time:98768ms step_avg:55.68ms
step:1775/2110 train_time:98858ms step_avg:55.69ms
step:1776/2110 train_time:98947ms step_avg:55.71ms
step:1777/2110 train_time:99035ms step_avg:55.73ms
step:1778/2110 train_time:99124ms step_avg:55.75ms
step:1779/2110 train_time:99210ms step_avg:55.77ms
step:1780/2110 train_time:99299ms step_avg:55.79ms
step:1781/2110 train_time:99386ms step_avg:55.80ms
step:1782/2110 train_time:99473ms step_avg:55.82ms
step:1783/2110 train_time:99560ms step_avg:55.84ms
step:1784/2110 train_time:99648ms step_avg:55.86ms
step:1785/2110 train_time:99737ms step_avg:55.88ms
step:1786/2110 train_time:99828ms step_avg:55.89ms
step:1787/2110 train_time:99917ms step_avg:55.91ms
step:1788/2110 train_time:100006ms step_avg:55.93ms
step:1789/2110 train_time:100093ms step_avg:55.95ms
step:1790/2110 train_time:100180ms step_avg:55.97ms
step:1791/2110 train_time:100268ms step_avg:55.98ms
step:1792/2110 train_time:100355ms step_avg:56.00ms
step:1793/2110 train_time:100444ms step_avg:56.02ms
step:1794/2110 train_time:100530ms step_avg:56.04ms
step:1795/2110 train_time:100618ms step_avg:56.05ms
step:1796/2110 train_time:100705ms step_avg:56.07ms
step:1797/2110 train_time:100795ms step_avg:56.09ms
step:1798/2110 train_time:100883ms step_avg:56.11ms
step:1799/2110 train_time:100972ms step_avg:56.13ms
step:1800/2110 train_time:101059ms step_avg:56.14ms
step:1801/2110 train_time:101148ms step_avg:56.16ms
step:1802/2110 train_time:101234ms step_avg:56.18ms
step:1803/2110 train_time:101323ms step_avg:56.20ms
step:1804/2110 train_time:101410ms step_avg:56.21ms
step:1805/2110 train_time:101497ms step_avg:56.23ms
step:1806/2110 train_time:101585ms step_avg:56.25ms
step:1807/2110 train_time:101673ms step_avg:56.27ms
step:1808/2110 train_time:101762ms step_avg:56.28ms
step:1809/2110 train_time:101850ms step_avg:56.30ms
step:1810/2110 train_time:101938ms step_avg:56.32ms
step:1811/2110 train_time:102027ms step_avg:56.34ms
step:1812/2110 train_time:102115ms step_avg:56.35ms
step:1813/2110 train_time:102204ms step_avg:56.37ms
step:1814/2110 train_time:102291ms step_avg:56.39ms
step:1815/2110 train_time:102380ms step_avg:56.41ms
step:1816/2110 train_time:102466ms step_avg:56.42ms
step:1817/2110 train_time:102554ms step_avg:56.44ms
step:1818/2110 train_time:102642ms step_avg:56.46ms
step:1819/2110 train_time:102730ms step_avg:56.48ms
step:1820/2110 train_time:102819ms step_avg:56.49ms
step:1821/2110 train_time:102908ms step_avg:56.51ms
step:1822/2110 train_time:102997ms step_avg:56.53ms
step:1823/2110 train_time:103086ms step_avg:56.55ms
step:1824/2110 train_time:103173ms step_avg:56.56ms
step:1825/2110 train_time:103262ms step_avg:56.58ms
step:1826/2110 train_time:103349ms step_avg:56.60ms
step:1827/2110 train_time:103437ms step_avg:56.62ms
step:1828/2110 train_time:103524ms step_avg:56.63ms
step:1829/2110 train_time:103613ms step_avg:56.65ms
step:1830/2110 train_time:103701ms step_avg:56.67ms
step:1831/2110 train_time:103790ms step_avg:56.68ms
step:1832/2110 train_time:103878ms step_avg:56.70ms
step:1833/2110 train_time:103967ms step_avg:56.72ms
step:1834/2110 train_time:104055ms step_avg:56.74ms
step:1835/2110 train_time:104144ms step_avg:56.75ms
step:1836/2110 train_time:104231ms step_avg:56.77ms
step:1837/2110 train_time:104320ms step_avg:56.79ms
step:1838/2110 train_time:104407ms step_avg:56.80ms
step:1839/2110 train_time:104495ms step_avg:56.82ms
step:1840/2110 train_time:104583ms step_avg:56.84ms
step:1841/2110 train_time:104672ms step_avg:56.86ms
step:1842/2110 train_time:104759ms step_avg:56.87ms
step:1843/2110 train_time:104848ms step_avg:56.89ms
step:1844/2110 train_time:104937ms step_avg:56.91ms
step:1845/2110 train_time:105026ms step_avg:56.92ms
step:1846/2110 train_time:105113ms step_avg:56.94ms
step:1847/2110 train_time:105202ms step_avg:56.96ms
step:1848/2110 train_time:105289ms step_avg:56.97ms
step:1849/2110 train_time:105377ms step_avg:56.99ms
step:1850/2110 train_time:105465ms step_avg:57.01ms
step:1851/2110 train_time:105553ms step_avg:57.02ms
step:1852/2110 train_time:105641ms step_avg:57.04ms
step:1853/2110 train_time:105729ms step_avg:57.06ms
step:1854/2110 train_time:105816ms step_avg:57.07ms
step:1855/2110 train_time:105906ms step_avg:57.09ms
step:1856/2110 train_time:105994ms step_avg:57.11ms
step:1857/2110 train_time:106083ms step_avg:57.13ms
step:1858/2110 train_time:106169ms step_avg:57.14ms
step:1859/2110 train_time:106257ms step_avg:57.16ms
step:1860/2110 train_time:106345ms step_avg:57.17ms
step:1861/2110 train_time:106433ms step_avg:57.19ms
step:1862/2110 train_time:106521ms step_avg:57.21ms
step:1863/2110 train_time:106610ms step_avg:57.22ms
step:1864/2110 train_time:106699ms step_avg:57.24ms
step:1865/2110 train_time:106787ms step_avg:57.26ms
step:1866/2110 train_time:106874ms step_avg:57.27ms
step:1867/2110 train_time:106963ms step_avg:57.29ms
step:1868/2110 train_time:107050ms step_avg:57.31ms
step:1869/2110 train_time:107139ms step_avg:57.32ms
step:1870/2110 train_time:107226ms step_avg:57.34ms
step:1871/2110 train_time:107315ms step_avg:57.36ms
step:1872/2110 train_time:107403ms step_avg:57.37ms
step:1873/2110 train_time:107492ms step_avg:57.39ms
step:1874/2110 train_time:107579ms step_avg:57.41ms
step:1875/2110 train_time:107668ms step_avg:57.42ms
step:1876/2110 train_time:107755ms step_avg:57.44ms
step:1877/2110 train_time:107844ms step_avg:57.46ms
step:1878/2110 train_time:107931ms step_avg:57.47ms
step:1879/2110 train_time:108020ms step_avg:57.49ms
step:1880/2110 train_time:108107ms step_avg:57.50ms
step:1881/2110 train_time:108196ms step_avg:57.52ms
step:1882/2110 train_time:108284ms step_avg:57.54ms
step:1883/2110 train_time:108372ms step_avg:57.55ms
step:1884/2110 train_time:108459ms step_avg:57.57ms
step:1885/2110 train_time:108547ms step_avg:57.58ms
step:1886/2110 train_time:108634ms step_avg:57.60ms
step:1887/2110 train_time:108723ms step_avg:57.62ms
step:1888/2110 train_time:108810ms step_avg:57.63ms
step:1889/2110 train_time:108899ms step_avg:57.65ms
step:1890/2110 train_time:108986ms step_avg:57.66ms
step:1891/2110 train_time:109075ms step_avg:57.68ms
step:1892/2110 train_time:109163ms step_avg:57.70ms
step:1893/2110 train_time:109251ms step_avg:57.71ms
step:1894/2110 train_time:109339ms step_avg:57.73ms
step:1895/2110 train_time:109428ms step_avg:57.75ms
step:1896/2110 train_time:109516ms step_avg:57.76ms
step:1897/2110 train_time:109605ms step_avg:57.78ms
step:1898/2110 train_time:109693ms step_avg:57.79ms
step:1899/2110 train_time:109780ms step_avg:57.81ms
step:1900/2110 train_time:109868ms step_avg:57.83ms
step:1901/2110 train_time:109957ms step_avg:57.84ms
step:1902/2110 train_time:110046ms step_avg:57.86ms
step:1903/2110 train_time:110134ms step_avg:57.87ms
step:1904/2110 train_time:110222ms step_avg:57.89ms
step:1905/2110 train_time:110310ms step_avg:57.91ms
step:1906/2110 train_time:110397ms step_avg:57.92ms
step:1907/2110 train_time:110486ms step_avg:57.94ms
step:1908/2110 train_time:110575ms step_avg:57.95ms
step:1909/2110 train_time:110663ms step_avg:57.97ms
step:1910/2110 train_time:110750ms step_avg:57.98ms
step:1911/2110 train_time:110838ms step_avg:58.00ms
step:1912/2110 train_time:110926ms step_avg:58.02ms
step:1913/2110 train_time:111014ms step_avg:58.03ms
step:1914/2110 train_time:111102ms step_avg:58.05ms
step:1915/2110 train_time:111190ms step_avg:58.06ms
step:1916/2110 train_time:111277ms step_avg:58.08ms
step:1917/2110 train_time:111366ms step_avg:58.09ms
step:1918/2110 train_time:111454ms step_avg:58.11ms
step:1919/2110 train_time:111543ms step_avg:58.13ms
step:1920/2110 train_time:111629ms step_avg:58.14ms
step:1921/2110 train_time:111718ms step_avg:58.16ms
step:1922/2110 train_time:111806ms step_avg:58.17ms
step:1923/2110 train_time:111894ms step_avg:58.19ms
step:1924/2110 train_time:111981ms step_avg:58.20ms
step:1925/2110 train_time:112069ms step_avg:58.22ms
step:1926/2110 train_time:112157ms step_avg:58.23ms
step:1927/2110 train_time:112247ms step_avg:58.25ms
step:1928/2110 train_time:112335ms step_avg:58.27ms
step:1929/2110 train_time:112422ms step_avg:58.28ms
step:1930/2110 train_time:112511ms step_avg:58.30ms
step:1931/2110 train_time:112597ms step_avg:58.31ms
step:1932/2110 train_time:112687ms step_avg:58.33ms
step:1933/2110 train_time:112774ms step_avg:58.34ms
step:1934/2110 train_time:112863ms step_avg:58.36ms
step:1935/2110 train_time:112951ms step_avg:58.37ms
step:1936/2110 train_time:113038ms step_avg:58.39ms
step:1937/2110 train_time:113126ms step_avg:58.40ms
step:1938/2110 train_time:113215ms step_avg:58.42ms
step:1939/2110 train_time:113303ms step_avg:58.43ms
step:1940/2110 train_time:113391ms step_avg:58.45ms
step:1941/2110 train_time:113479ms step_avg:58.46ms
step:1942/2110 train_time:113567ms step_avg:58.48ms
step:1943/2110 train_time:113653ms step_avg:58.49ms
step:1944/2110 train_time:113742ms step_avg:58.51ms
step:1945/2110 train_time:113829ms step_avg:58.52ms
step:1946/2110 train_time:113918ms step_avg:58.54ms
step:1947/2110 train_time:114006ms step_avg:58.55ms
step:1948/2110 train_time:114094ms step_avg:58.57ms
step:1949/2110 train_time:114182ms step_avg:58.59ms
step:1950/2110 train_time:114270ms step_avg:58.60ms
step:1951/2110 train_time:114358ms step_avg:58.62ms
step:1952/2110 train_time:114447ms step_avg:58.63ms
step:1953/2110 train_time:114533ms step_avg:58.64ms
step:1954/2110 train_time:114622ms step_avg:58.66ms
step:1955/2110 train_time:114710ms step_avg:58.68ms
step:1956/2110 train_time:114799ms step_avg:58.69ms
step:1957/2110 train_time:114886ms step_avg:58.71ms
step:1958/2110 train_time:114975ms step_avg:58.72ms
step:1959/2110 train_time:115063ms step_avg:58.74ms
step:1960/2110 train_time:115150ms step_avg:58.75ms
step:1961/2110 train_time:115238ms step_avg:58.77ms
step:1962/2110 train_time:115327ms step_avg:58.78ms
step:1963/2110 train_time:115415ms step_avg:58.79ms
step:1964/2110 train_time:115504ms step_avg:58.81ms
step:1965/2110 train_time:115592ms step_avg:58.83ms
step:1966/2110 train_time:115681ms step_avg:58.84ms
step:1967/2110 train_time:115768ms step_avg:58.86ms
step:1968/2110 train_time:115855ms step_avg:58.87ms
step:1969/2110 train_time:115944ms step_avg:58.88ms
step:1970/2110 train_time:116033ms step_avg:58.90ms
step:1971/2110 train_time:116121ms step_avg:58.91ms
step:1972/2110 train_time:116210ms step_avg:58.93ms
step:1973/2110 train_time:116297ms step_avg:58.94ms
step:1974/2110 train_time:116384ms step_avg:58.96ms
step:1975/2110 train_time:116473ms step_avg:58.97ms
step:1976/2110 train_time:116561ms step_avg:58.99ms
step:1977/2110 train_time:116648ms step_avg:59.00ms
step:1978/2110 train_time:116736ms step_avg:59.02ms
step:1979/2110 train_time:116826ms step_avg:59.03ms
step:1980/2110 train_time:116914ms step_avg:59.05ms
step:1981/2110 train_time:117002ms step_avg:59.06ms
step:1982/2110 train_time:117090ms step_avg:59.08ms
step:1983/2110 train_time:117178ms step_avg:59.09ms
step:1984/2110 train_time:117266ms step_avg:59.11ms
step:1985/2110 train_time:117354ms step_avg:59.12ms
step:1986/2110 train_time:117443ms step_avg:59.14ms
step:1987/2110 train_time:117530ms step_avg:59.15ms
step:1988/2110 train_time:117618ms step_avg:59.16ms
step:1989/2110 train_time:117706ms step_avg:59.18ms
step:1990/2110 train_time:117795ms step_avg:59.19ms
step:1991/2110 train_time:117883ms step_avg:59.21ms
step:1992/2110 train_time:117971ms step_avg:59.22ms
step:1993/2110 train_time:118059ms step_avg:59.24ms
step:1994/2110 train_time:118147ms step_avg:59.25ms
step:1995/2110 train_time:118236ms step_avg:59.27ms
step:1996/2110 train_time:118325ms step_avg:59.28ms
step:1997/2110 train_time:118412ms step_avg:59.30ms
step:1998/2110 train_time:118500ms step_avg:59.31ms
step:1999/2110 train_time:118588ms step_avg:59.32ms
step:2000/2110 train_time:118676ms step_avg:59.34ms
step:2000/2110 val_loss:3.3028 train_time:118764ms step_avg:59.38ms
step:2001/2110 train_time:118799ms step_avg:59.37ms
step:2002/2110 train_time:118858ms step_avg:59.37ms
step:2003/2110 train_time:118949ms step_avg:59.39ms
step:2004/2110 train_time:119038ms step_avg:59.40ms
step:2005/2110 train_time:119127ms step_avg:59.41ms
step:2006/2110 train_time:119214ms step_avg:59.43ms
step:2007/2110 train_time:119301ms step_avg:59.44ms
step:2008/2110 train_time:119388ms step_avg:59.46ms
step:2009/2110 train_time:119475ms step_avg:59.47ms
step:2010/2110 train_time:119562ms step_avg:59.48ms
step:2011/2110 train_time:119650ms step_avg:59.50ms
step:2012/2110 train_time:119738ms step_avg:59.51ms
step:2013/2110 train_time:119830ms step_avg:59.53ms
step:2014/2110 train_time:119919ms step_avg:59.54ms
step:2015/2110 train_time:120009ms step_avg:59.56ms
step:2016/2110 train_time:120097ms step_avg:59.57ms
step:2017/2110 train_time:120184ms step_avg:59.59ms
step:2018/2110 train_time:120272ms step_avg:59.60ms
step:2019/2110 train_time:120358ms step_avg:59.61ms
step:2020/2110 train_time:120446ms step_avg:59.63ms
step:2021/2110 train_time:120534ms step_avg:59.64ms
step:2022/2110 train_time:120621ms step_avg:59.65ms
step:2023/2110 train_time:120711ms step_avg:59.67ms
step:2024/2110 train_time:120799ms step_avg:59.68ms
step:2025/2110 train_time:120889ms step_avg:59.70ms
step:2026/2110 train_time:120977ms step_avg:59.71ms
step:2027/2110 train_time:121066ms step_avg:59.73ms
step:2028/2110 train_time:121154ms step_avg:59.74ms
step:2029/2110 train_time:121243ms step_avg:59.75ms
step:2030/2110 train_time:121332ms step_avg:59.77ms
step:2031/2110 train_time:121419ms step_avg:59.78ms
step:2032/2110 train_time:121506ms step_avg:59.80ms
step:2033/2110 train_time:121594ms step_avg:59.81ms
step:2034/2110 train_time:121682ms step_avg:59.82ms
step:2035/2110 train_time:121774ms step_avg:59.84ms
step:2036/2110 train_time:121863ms step_avg:59.85ms
step:2037/2110 train_time:121951ms step_avg:59.87ms
step:2038/2110 train_time:122040ms step_avg:59.88ms
step:2039/2110 train_time:122127ms step_avg:59.90ms
step:2040/2110 train_time:122215ms step_avg:59.91ms
step:2041/2110 train_time:122303ms step_avg:59.92ms
step:2042/2110 train_time:122389ms step_avg:59.94ms
step:2043/2110 train_time:122477ms step_avg:59.95ms
step:2044/2110 train_time:122566ms step_avg:59.96ms
step:2045/2110 train_time:122654ms step_avg:59.98ms
step:2046/2110 train_time:122742ms step_avg:59.99ms
step:2047/2110 train_time:122831ms step_avg:60.01ms
step:2048/2110 train_time:122919ms step_avg:60.02ms
step:2049/2110 train_time:123008ms step_avg:60.03ms
step:2050/2110 train_time:123097ms step_avg:60.05ms
step:2051/2110 train_time:123185ms step_avg:60.06ms
step:2052/2110 train_time:123273ms step_avg:60.07ms
step:2053/2110 train_time:123361ms step_avg:60.09ms
step:2054/2110 train_time:123449ms step_avg:60.10ms
step:2055/2110 train_time:123536ms step_avg:60.11ms
step:2056/2110 train_time:123624ms step_avg:60.13ms
step:2057/2110 train_time:123713ms step_avg:60.14ms
step:2058/2110 train_time:123801ms step_avg:60.16ms
step:2059/2110 train_time:123889ms step_avg:60.17ms
step:2060/2110 train_time:123977ms step_avg:60.18ms
step:2061/2110 train_time:124066ms step_avg:60.20ms
step:2062/2110 train_time:124154ms step_avg:60.21ms
step:2063/2110 train_time:124243ms step_avg:60.22ms
step:2064/2110 train_time:124331ms step_avg:60.24ms
step:2065/2110 train_time:124419ms step_avg:60.25ms
step:2066/2110 train_time:124507ms step_avg:60.26ms
step:2067/2110 train_time:124595ms step_avg:60.28ms
step:2068/2110 train_time:124684ms step_avg:60.29ms
step:2069/2110 train_time:124772ms step_avg:60.31ms
step:2070/2110 train_time:124860ms step_avg:60.32ms
step:2071/2110 train_time:124950ms step_avg:60.33ms
step:2072/2110 train_time:125038ms step_avg:60.35ms
step:2073/2110 train_time:125127ms step_avg:60.36ms
step:2074/2110 train_time:125215ms step_avg:60.37ms
step:2075/2110 train_time:125303ms step_avg:60.39ms
step:2076/2110 train_time:125390ms step_avg:60.40ms
step:2077/2110 train_time:125477ms step_avg:60.41ms
step:2078/2110 train_time:125567ms step_avg:60.43ms
step:2079/2110 train_time:125654ms step_avg:60.44ms
step:2080/2110 train_time:125742ms step_avg:60.45ms
step:2081/2110 train_time:125832ms step_avg:60.47ms
step:2082/2110 train_time:125920ms step_avg:60.48ms
step:2083/2110 train_time:126008ms step_avg:60.49ms
step:2084/2110 train_time:126095ms step_avg:60.51ms
step:2085/2110 train_time:126186ms step_avg:60.52ms
step:2086/2110 train_time:126274ms step_avg:60.53ms
step:2087/2110 train_time:126363ms step_avg:60.55ms
step:2088/2110 train_time:126450ms step_avg:60.56ms
step:2089/2110 train_time:126539ms step_avg:60.57ms
step:2090/2110 train_time:126626ms step_avg:60.59ms
step:2091/2110 train_time:126715ms step_avg:60.60ms
step:2092/2110 train_time:126804ms step_avg:60.61ms
step:2093/2110 train_time:126894ms step_avg:60.63ms
step:2094/2110 train_time:126982ms step_avg:60.64ms
step:2095/2110 train_time:127072ms step_avg:60.65ms
step:2096/2110 train_time:127159ms step_avg:60.67ms
step:2097/2110 train_time:127249ms step_avg:60.68ms
step:2098/2110 train_time:127337ms step_avg:60.69ms
step:2099/2110 train_time:127426ms step_avg:60.71ms
step:2100/2110 train_time:127513ms step_avg:60.72ms
step:2101/2110 train_time:127602ms step_avg:60.73ms
step:2102/2110 train_time:127690ms step_avg:60.75ms
step:2103/2110 train_time:127779ms step_avg:60.76ms
step:2104/2110 train_time:127866ms step_avg:60.77ms
step:2105/2110 train_time:127956ms step_avg:60.79ms
step:2106/2110 train_time:128045ms step_avg:60.80ms
step:2107/2110 train_time:128135ms step_avg:60.81ms
step:2108/2110 train_time:128222ms step_avg:60.83ms
step:2109/2110 train_time:128312ms step_avg:60.84ms
step:2110/2110 train_time:128400ms step_avg:60.85ms
step:2110/2110 val_loss:3.2788 train_time:128491ms step_avg:60.90ms
peak memory allocated: 29892 MiB reserved: 44556 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 15 21:36:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   38C    P0            126W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   43C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   42C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   42C    P0            126W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              88      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              88      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2110 train_time:88ms step_avg:88.12ms
step:2/2110 train_time:118ms step_avg:59.04ms
step:3/2110 train_time:162ms step_avg:53.99ms
step:4/2110 train_time:197ms step_avg:49.36ms
step:5/2110 train_time:235ms step_avg:46.93ms
step:6/2110 train_time:668ms step_avg:111.33ms
step:7/2110 train_time:691ms step_avg:98.75ms
step:8/2110 train_time:713ms step_avg:89.17ms
step:9/2110 train_time:741ms step_avg:82.32ms
step:10/2110 train_time:773ms step_avg:77.34ms
step:11/2110 train_time:806ms step_avg:73.32ms
step:12/2110 train_time:839ms step_avg:69.90ms
step:13/2110 train_time:872ms step_avg:67.10ms
step:14/2110 train_time:905ms step_avg:64.63ms
step:15/2110 train_time:938ms step_avg:62.53ms
step:16/2110 train_time:971ms step_avg:60.66ms
step:17/2110 train_time:1004ms step_avg:59.05ms
step:18/2110 train_time:1037ms step_avg:57.59ms
step:19/2110 train_time:1070ms step_avg:56.30ms
step:20/2110 train_time:1102ms step_avg:55.12ms
step:21/2110 train_time:1135ms step_avg:54.07ms
step:22/2110 train_time:1168ms step_avg:53.08ms
step:23/2110 train_time:1201ms step_avg:52.23ms
step:24/2110 train_time:1234ms step_avg:51.42ms
step:25/2110 train_time:1267ms step_avg:50.69ms
step:26/2110 train_time:1300ms step_avg:49.99ms
step:27/2110 train_time:1333ms step_avg:49.37ms
step:28/2110 train_time:1366ms step_avg:48.78ms
step:29/2110 train_time:1399ms step_avg:48.23ms
step:30/2110 train_time:1431ms step_avg:47.71ms
step:31/2110 train_time:1465ms step_avg:47.25ms
step:32/2110 train_time:1497ms step_avg:46.79ms
step:33/2110 train_time:1531ms step_avg:46.41ms
step:34/2110 train_time:1565ms step_avg:46.04ms
step:35/2110 train_time:1600ms step_avg:45.70ms
step:36/2110 train_time:1633ms step_avg:45.36ms
step:37/2110 train_time:1668ms step_avg:45.07ms
step:38/2110 train_time:1701ms step_avg:44.75ms
step:39/2110 train_time:1734ms step_avg:44.47ms
step:40/2110 train_time:1767ms step_avg:44.17ms
step:41/2110 train_time:1801ms step_avg:43.92ms
step:42/2110 train_time:1833ms step_avg:43.65ms
step:43/2110 train_time:1867ms step_avg:43.42ms
step:44/2110 train_time:1900ms step_avg:43.17ms
step:45/2110 train_time:1933ms step_avg:42.95ms
step:46/2110 train_time:1966ms step_avg:42.73ms
step:47/2110 train_time:1999ms step_avg:42.53ms
step:48/2110 train_time:2032ms step_avg:42.34ms
step:49/2110 train_time:2065ms step_avg:42.15ms
step:50/2110 train_time:2098ms step_avg:41.96ms
step:51/2110 train_time:2131ms step_avg:41.79ms
step:52/2110 train_time:2164ms step_avg:41.61ms
step:53/2110 train_time:2197ms step_avg:41.45ms
step:54/2110 train_time:2230ms step_avg:41.30ms
step:55/2110 train_time:2263ms step_avg:41.15ms
step:56/2110 train_time:2296ms step_avg:40.99ms
step:57/2110 train_time:2329ms step_avg:40.86ms
step:58/2110 train_time:2361ms step_avg:40.72ms
step:59/2110 train_time:2395ms step_avg:40.59ms
step:60/2110 train_time:2427ms step_avg:40.45ms
step:61/2110 train_time:2460ms step_avg:40.33ms
step:62/2110 train_time:2493ms step_avg:40.21ms
step:63/2110 train_time:2527ms step_avg:40.11ms
step:64/2110 train_time:2559ms step_avg:39.99ms
step:65/2110 train_time:2593ms step_avg:39.89ms
step:66/2110 train_time:2626ms step_avg:39.79ms
step:67/2110 train_time:2660ms step_avg:39.70ms
step:68/2110 train_time:2693ms step_avg:39.60ms
step:69/2110 train_time:2727ms step_avg:39.52ms
step:70/2110 train_time:2759ms step_avg:39.42ms
step:71/2110 train_time:2793ms step_avg:39.34ms
step:72/2110 train_time:2826ms step_avg:39.25ms
step:73/2110 train_time:2859ms step_avg:39.17ms
step:74/2110 train_time:2892ms step_avg:39.08ms
step:75/2110 train_time:2926ms step_avg:39.01ms
step:76/2110 train_time:2959ms step_avg:38.93ms
step:77/2110 train_time:2992ms step_avg:38.86ms
step:78/2110 train_time:3024ms step_avg:38.78ms
step:79/2110 train_time:3058ms step_avg:38.70ms
step:80/2110 train_time:3090ms step_avg:38.63ms
step:81/2110 train_time:3123ms step_avg:38.56ms
step:82/2110 train_time:3156ms step_avg:38.49ms
step:83/2110 train_time:3189ms step_avg:38.42ms
step:84/2110 train_time:3222ms step_avg:38.36ms
step:85/2110 train_time:3255ms step_avg:38.30ms
step:86/2110 train_time:3288ms step_avg:38.23ms
step:87/2110 train_time:3321ms step_avg:38.17ms
step:88/2110 train_time:3353ms step_avg:38.11ms
step:89/2110 train_time:3387ms step_avg:38.05ms
step:90/2110 train_time:3419ms step_avg:37.99ms
step:91/2110 train_time:3453ms step_avg:37.94ms
step:92/2110 train_time:3485ms step_avg:37.88ms
step:93/2110 train_time:3519ms step_avg:37.83ms
step:94/2110 train_time:3552ms step_avg:37.78ms
step:95/2110 train_time:3585ms step_avg:37.73ms
step:96/2110 train_time:3617ms step_avg:37.68ms
step:97/2110 train_time:3651ms step_avg:37.64ms
step:98/2110 train_time:3683ms step_avg:37.59ms
step:99/2110 train_time:3717ms step_avg:37.54ms
step:100/2110 train_time:3750ms step_avg:37.50ms
step:101/2110 train_time:3784ms step_avg:37.46ms
step:102/2110 train_time:3816ms step_avg:37.41ms
step:103/2110 train_time:3850ms step_avg:37.38ms
step:104/2110 train_time:3883ms step_avg:37.33ms
step:105/2110 train_time:3916ms step_avg:37.30ms
step:106/2110 train_time:3949ms step_avg:37.25ms
step:107/2110 train_time:3982ms step_avg:37.22ms
step:108/2110 train_time:4015ms step_avg:37.17ms
step:109/2110 train_time:4048ms step_avg:37.14ms
step:110/2110 train_time:4081ms step_avg:37.10ms
step:111/2110 train_time:4114ms step_avg:37.07ms
step:112/2110 train_time:4147ms step_avg:37.03ms
step:113/2110 train_time:4180ms step_avg:36.99ms
step:114/2110 train_time:4213ms step_avg:36.96ms
step:115/2110 train_time:4246ms step_avg:36.92ms
step:116/2110 train_time:4278ms step_avg:36.88ms
step:117/2110 train_time:4312ms step_avg:36.85ms
step:118/2110 train_time:4344ms step_avg:36.82ms
step:119/2110 train_time:4377ms step_avg:36.79ms
step:120/2110 train_time:4410ms step_avg:36.75ms
step:121/2110 train_time:4443ms step_avg:36.72ms
step:122/2110 train_time:4476ms step_avg:36.69ms
step:123/2110 train_time:4509ms step_avg:36.66ms
step:124/2110 train_time:4542ms step_avg:36.63ms
step:125/2110 train_time:4575ms step_avg:36.60ms
step:126/2110 train_time:4608ms step_avg:36.57ms
step:127/2110 train_time:4641ms step_avg:36.54ms
step:128/2110 train_time:4673ms step_avg:36.51ms
step:129/2110 train_time:4707ms step_avg:36.49ms
step:130/2110 train_time:4740ms step_avg:36.46ms
step:131/2110 train_time:4773ms step_avg:36.44ms
step:132/2110 train_time:4806ms step_avg:36.41ms
step:133/2110 train_time:4839ms step_avg:36.38ms
step:134/2110 train_time:4871ms step_avg:36.35ms
step:135/2110 train_time:4905ms step_avg:36.33ms
step:136/2110 train_time:4937ms step_avg:36.30ms
step:137/2110 train_time:4970ms step_avg:36.28ms
step:138/2110 train_time:5003ms step_avg:36.26ms
step:139/2110 train_time:5037ms step_avg:36.23ms
step:140/2110 train_time:5070ms step_avg:36.21ms
step:141/2110 train_time:5103ms step_avg:36.19ms
step:142/2110 train_time:5136ms step_avg:36.17ms
step:143/2110 train_time:5169ms step_avg:36.15ms
step:144/2110 train_time:5201ms step_avg:36.12ms
step:145/2110 train_time:5235ms step_avg:36.10ms
step:146/2110 train_time:5268ms step_avg:36.08ms
step:147/2110 train_time:5301ms step_avg:36.06ms
step:148/2110 train_time:5334ms step_avg:36.04ms
step:149/2110 train_time:5367ms step_avg:36.02ms
step:150/2110 train_time:5399ms step_avg:35.99ms
step:151/2110 train_time:5432ms step_avg:35.98ms
step:152/2110 train_time:5465ms step_avg:35.95ms
step:153/2110 train_time:5498ms step_avg:35.94ms
step:154/2110 train_time:5531ms step_avg:35.92ms
step:155/2110 train_time:5565ms step_avg:35.90ms
step:156/2110 train_time:5598ms step_avg:35.88ms
step:157/2110 train_time:5631ms step_avg:35.86ms
step:158/2110 train_time:5663ms step_avg:35.84ms
step:159/2110 train_time:5696ms step_avg:35.83ms
step:160/2110 train_time:5729ms step_avg:35.81ms
step:161/2110 train_time:5763ms step_avg:35.79ms
step:162/2110 train_time:5795ms step_avg:35.77ms
step:163/2110 train_time:5829ms step_avg:35.76ms
step:164/2110 train_time:5861ms step_avg:35.74ms
step:165/2110 train_time:5895ms step_avg:35.73ms
step:166/2110 train_time:5927ms step_avg:35.71ms
step:167/2110 train_time:5961ms step_avg:35.69ms
step:168/2110 train_time:5993ms step_avg:35.67ms
step:169/2110 train_time:6027ms step_avg:35.66ms
step:170/2110 train_time:6059ms step_avg:35.64ms
step:171/2110 train_time:6092ms step_avg:35.63ms
step:172/2110 train_time:6125ms step_avg:35.61ms
step:173/2110 train_time:6158ms step_avg:35.60ms
step:174/2110 train_time:6190ms step_avg:35.58ms
step:175/2110 train_time:6224ms step_avg:35.57ms
step:176/2110 train_time:6256ms step_avg:35.55ms
step:177/2110 train_time:6290ms step_avg:35.54ms
step:178/2110 train_time:6322ms step_avg:35.52ms
step:179/2110 train_time:6355ms step_avg:35.51ms
step:180/2110 train_time:6388ms step_avg:35.49ms
step:181/2110 train_time:6421ms step_avg:35.48ms
step:182/2110 train_time:6454ms step_avg:35.46ms
step:183/2110 train_time:6487ms step_avg:35.45ms
step:184/2110 train_time:6520ms step_avg:35.43ms
step:185/2110 train_time:6553ms step_avg:35.42ms
step:186/2110 train_time:6586ms step_avg:35.41ms
step:187/2110 train_time:6619ms step_avg:35.40ms
step:188/2110 train_time:6652ms step_avg:35.38ms
step:189/2110 train_time:6685ms step_avg:35.37ms
step:190/2110 train_time:6717ms step_avg:35.35ms
step:191/2110 train_time:6751ms step_avg:35.35ms
step:192/2110 train_time:6784ms step_avg:35.33ms
step:193/2110 train_time:6817ms step_avg:35.32ms
step:194/2110 train_time:6849ms step_avg:35.31ms
step:195/2110 train_time:6883ms step_avg:35.30ms
step:196/2110 train_time:6916ms step_avg:35.28ms
step:197/2110 train_time:6949ms step_avg:35.27ms
step:198/2110 train_time:6982ms step_avg:35.26ms
step:199/2110 train_time:7015ms step_avg:35.25ms
step:200/2110 train_time:7047ms step_avg:35.24ms
step:201/2110 train_time:7080ms step_avg:35.23ms
step:202/2110 train_time:7113ms step_avg:35.21ms
step:203/2110 train_time:7146ms step_avg:35.20ms
step:204/2110 train_time:7179ms step_avg:35.19ms
step:205/2110 train_time:7212ms step_avg:35.18ms
step:206/2110 train_time:7244ms step_avg:35.17ms
step:207/2110 train_time:7278ms step_avg:35.16ms
step:208/2110 train_time:7311ms step_avg:35.15ms
step:209/2110 train_time:7344ms step_avg:35.14ms
step:210/2110 train_time:7376ms step_avg:35.12ms
step:211/2110 train_time:7409ms step_avg:35.12ms
step:212/2110 train_time:7442ms step_avg:35.10ms
step:213/2110 train_time:7475ms step_avg:35.09ms
step:214/2110 train_time:7508ms step_avg:35.08ms
step:215/2110 train_time:7541ms step_avg:35.08ms
step:216/2110 train_time:7574ms step_avg:35.06ms
step:217/2110 train_time:7607ms step_avg:35.06ms
step:218/2110 train_time:7640ms step_avg:35.04ms
step:219/2110 train_time:7673ms step_avg:35.04ms
step:220/2110 train_time:7706ms step_avg:35.03ms
step:221/2110 train_time:7739ms step_avg:35.02ms
step:222/2110 train_time:7771ms step_avg:35.01ms
step:223/2110 train_time:7805ms step_avg:35.00ms
step:224/2110 train_time:7837ms step_avg:34.99ms
step:225/2110 train_time:7871ms step_avg:34.98ms
step:226/2110 train_time:7903ms step_avg:34.97ms
step:227/2110 train_time:7937ms step_avg:34.96ms
step:228/2110 train_time:7969ms step_avg:34.95ms
step:229/2110 train_time:8003ms step_avg:34.95ms
step:230/2110 train_time:8035ms step_avg:34.94ms
step:231/2110 train_time:8069ms step_avg:34.93ms
step:232/2110 train_time:8101ms step_avg:34.92ms
step:233/2110 train_time:8134ms step_avg:34.91ms
step:234/2110 train_time:8167ms step_avg:34.90ms
step:235/2110 train_time:8201ms step_avg:34.90ms
step:236/2110 train_time:8233ms step_avg:34.89ms
step:237/2110 train_time:8266ms step_avg:34.88ms
step:238/2110 train_time:8299ms step_avg:34.87ms
step:239/2110 train_time:8332ms step_avg:34.86ms
step:240/2110 train_time:8364ms step_avg:34.85ms
step:241/2110 train_time:8398ms step_avg:34.85ms
step:242/2110 train_time:8430ms step_avg:34.83ms
step:243/2110 train_time:8463ms step_avg:34.83ms
step:244/2110 train_time:8496ms step_avg:34.82ms
step:245/2110 train_time:8529ms step_avg:34.81ms
step:246/2110 train_time:8562ms step_avg:34.80ms
step:247/2110 train_time:8595ms step_avg:34.80ms
step:248/2110 train_time:8628ms step_avg:34.79ms
step:249/2110 train_time:8661ms step_avg:34.78ms
step:250/2110 train_time:8694ms step_avg:34.77ms
step:250/2110 val_loss:4.2991 train_time:8729ms step_avg:34.92ms
step:251/2110 train_time:8758ms step_avg:34.89ms
step:252/2110 train_time:8790ms step_avg:34.88ms
step:253/2110 train_time:8816ms step_avg:34.85ms
step:254/2110 train_time:8844ms step_avg:34.82ms
step:255/2110 train_time:8872ms step_avg:34.79ms
step:256/2110 train_time:8900ms step_avg:34.77ms
step:257/2110 train_time:8933ms step_avg:34.76ms
step:258/2110 train_time:8966ms step_avg:34.75ms
step:259/2110 train_time:9000ms step_avg:34.75ms
step:260/2110 train_time:9033ms step_avg:34.74ms
step:261/2110 train_time:9067ms step_avg:34.74ms
step:262/2110 train_time:9099ms step_avg:34.73ms
step:263/2110 train_time:9132ms step_avg:34.72ms
step:264/2110 train_time:9165ms step_avg:34.72ms
step:265/2110 train_time:9198ms step_avg:34.71ms
step:266/2110 train_time:9230ms step_avg:34.70ms
step:267/2110 train_time:9263ms step_avg:34.69ms
step:268/2110 train_time:9296ms step_avg:34.69ms
step:269/2110 train_time:9329ms step_avg:34.68ms
step:270/2110 train_time:9361ms step_avg:34.67ms
step:271/2110 train_time:9394ms step_avg:34.66ms
step:272/2110 train_time:9426ms step_avg:34.65ms
step:273/2110 train_time:9459ms step_avg:34.65ms
step:274/2110 train_time:9492ms step_avg:34.64ms
step:275/2110 train_time:9525ms step_avg:34.64ms
step:276/2110 train_time:9557ms step_avg:34.63ms
step:277/2110 train_time:9590ms step_avg:34.62ms
step:278/2110 train_time:9623ms step_avg:34.61ms
step:279/2110 train_time:9656ms step_avg:34.61ms
step:280/2110 train_time:9689ms step_avg:34.60ms
step:281/2110 train_time:9723ms step_avg:34.60ms
step:282/2110 train_time:9756ms step_avg:34.59ms
step:283/2110 train_time:9789ms step_avg:34.59ms
step:284/2110 train_time:9823ms step_avg:34.59ms
step:285/2110 train_time:9856ms step_avg:34.58ms
step:286/2110 train_time:9889ms step_avg:34.58ms
step:287/2110 train_time:9923ms step_avg:34.57ms
step:288/2110 train_time:9956ms step_avg:34.57ms
step:289/2110 train_time:9989ms step_avg:34.56ms
step:290/2110 train_time:10022ms step_avg:34.56ms
step:291/2110 train_time:10055ms step_avg:34.55ms
step:292/2110 train_time:10088ms step_avg:34.55ms
step:293/2110 train_time:10121ms step_avg:34.54ms
step:294/2110 train_time:10153ms step_avg:34.53ms
step:295/2110 train_time:10187ms step_avg:34.53ms
step:296/2110 train_time:10219ms step_avg:34.52ms
step:297/2110 train_time:10252ms step_avg:34.52ms
step:298/2110 train_time:10285ms step_avg:34.51ms
step:299/2110 train_time:10318ms step_avg:34.51ms
step:300/2110 train_time:10350ms step_avg:34.50ms
step:301/2110 train_time:10384ms step_avg:34.50ms
step:302/2110 train_time:10416ms step_avg:34.49ms
step:303/2110 train_time:10449ms step_avg:34.49ms
step:304/2110 train_time:10482ms step_avg:34.48ms
step:305/2110 train_time:10515ms step_avg:34.47ms
step:306/2110 train_time:10547ms step_avg:34.47ms
step:307/2110 train_time:10580ms step_avg:34.46ms
step:308/2110 train_time:10613ms step_avg:34.46ms
step:309/2110 train_time:10646ms step_avg:34.45ms
step:310/2110 train_time:10681ms step_avg:34.45ms
step:311/2110 train_time:10719ms step_avg:34.47ms
step:312/2110 train_time:10754ms step_avg:34.47ms
step:313/2110 train_time:10788ms step_avg:34.47ms
step:314/2110 train_time:10820ms step_avg:34.46ms
step:315/2110 train_time:10853ms step_avg:34.45ms
step:316/2110 train_time:10885ms step_avg:34.45ms
step:317/2110 train_time:10919ms step_avg:34.44ms
step:318/2110 train_time:10951ms step_avg:34.44ms
step:319/2110 train_time:10985ms step_avg:34.43ms
step:320/2110 train_time:11017ms step_avg:34.43ms
step:321/2110 train_time:11051ms step_avg:34.43ms
step:322/2110 train_time:11084ms step_avg:34.42ms
step:323/2110 train_time:11117ms step_avg:34.42ms
step:324/2110 train_time:11149ms step_avg:34.41ms
step:325/2110 train_time:11183ms step_avg:34.41ms
step:326/2110 train_time:11215ms step_avg:34.40ms
step:327/2110 train_time:11248ms step_avg:34.40ms
step:328/2110 train_time:11281ms step_avg:34.39ms
step:329/2110 train_time:11314ms step_avg:34.39ms
step:330/2110 train_time:11346ms step_avg:34.38ms
step:331/2110 train_time:11379ms step_avg:34.38ms
step:332/2110 train_time:11412ms step_avg:34.37ms
step:333/2110 train_time:11445ms step_avg:34.37ms
step:334/2110 train_time:11478ms step_avg:34.36ms
step:335/2110 train_time:11511ms step_avg:34.36ms
step:336/2110 train_time:11543ms step_avg:34.36ms
step:337/2110 train_time:11576ms step_avg:34.35ms
step:338/2110 train_time:11609ms step_avg:34.35ms
step:339/2110 train_time:11643ms step_avg:34.34ms
step:340/2110 train_time:11675ms step_avg:34.34ms
step:341/2110 train_time:11709ms step_avg:34.34ms
step:342/2110 train_time:11742ms step_avg:34.33ms
step:343/2110 train_time:11775ms step_avg:34.33ms
step:344/2110 train_time:11807ms step_avg:34.32ms
step:345/2110 train_time:11841ms step_avg:34.32ms
step:346/2110 train_time:11873ms step_avg:34.32ms
step:347/2110 train_time:11907ms step_avg:34.31ms
step:348/2110 train_time:11939ms step_avg:34.31ms
step:349/2110 train_time:11972ms step_avg:34.30ms
step:350/2110 train_time:12006ms step_avg:34.30ms
step:351/2110 train_time:12039ms step_avg:34.30ms
step:352/2110 train_time:12071ms step_avg:34.29ms
step:353/2110 train_time:12105ms step_avg:34.29ms
step:354/2110 train_time:12137ms step_avg:34.29ms
step:355/2110 train_time:12170ms step_avg:34.28ms
step:356/2110 train_time:12204ms step_avg:34.28ms
step:357/2110 train_time:12236ms step_avg:34.27ms
step:358/2110 train_time:12269ms step_avg:34.27ms
step:359/2110 train_time:12302ms step_avg:34.27ms
step:360/2110 train_time:12334ms step_avg:34.26ms
step:361/2110 train_time:12367ms step_avg:34.26ms
step:362/2110 train_time:12400ms step_avg:34.25ms
step:363/2110 train_time:12433ms step_avg:34.25ms
step:364/2110 train_time:12466ms step_avg:34.25ms
step:365/2110 train_time:12499ms step_avg:34.24ms
step:366/2110 train_time:12531ms step_avg:34.24ms
step:367/2110 train_time:12565ms step_avg:34.24ms
step:368/2110 train_time:12597ms step_avg:34.23ms
step:369/2110 train_time:12630ms step_avg:34.23ms
step:370/2110 train_time:12663ms step_avg:34.22ms
step:371/2110 train_time:12696ms step_avg:34.22ms
step:372/2110 train_time:12728ms step_avg:34.22ms
step:373/2110 train_time:12762ms step_avg:34.22ms
step:374/2110 train_time:12795ms step_avg:34.21ms
step:375/2110 train_time:12828ms step_avg:34.21ms
step:376/2110 train_time:12861ms step_avg:34.20ms
step:377/2110 train_time:12894ms step_avg:34.20ms
step:378/2110 train_time:12927ms step_avg:34.20ms
step:379/2110 train_time:12960ms step_avg:34.20ms
step:380/2110 train_time:12993ms step_avg:34.19ms
step:381/2110 train_time:13026ms step_avg:34.19ms
step:382/2110 train_time:13059ms step_avg:34.18ms
step:383/2110 train_time:13092ms step_avg:34.18ms
step:384/2110 train_time:13124ms step_avg:34.18ms
step:385/2110 train_time:13158ms step_avg:34.18ms
step:386/2110 train_time:13190ms step_avg:34.17ms
step:387/2110 train_time:13224ms step_avg:34.17ms
step:388/2110 train_time:13256ms step_avg:34.17ms
step:389/2110 train_time:13290ms step_avg:34.16ms
step:390/2110 train_time:13322ms step_avg:34.16ms
step:391/2110 train_time:13355ms step_avg:34.16ms
step:392/2110 train_time:13388ms step_avg:34.15ms
step:393/2110 train_time:13421ms step_avg:34.15ms
step:394/2110 train_time:13454ms step_avg:34.15ms
step:395/2110 train_time:13487ms step_avg:34.14ms
step:396/2110 train_time:13519ms step_avg:34.14ms
step:397/2110 train_time:13553ms step_avg:34.14ms
step:398/2110 train_time:13585ms step_avg:34.13ms
step:399/2110 train_time:13618ms step_avg:34.13ms
step:400/2110 train_time:13651ms step_avg:34.13ms
step:401/2110 train_time:13684ms step_avg:34.12ms
step:402/2110 train_time:13717ms step_avg:34.12ms
step:403/2110 train_time:13750ms step_avg:34.12ms
step:404/2110 train_time:13782ms step_avg:34.11ms
step:405/2110 train_time:13815ms step_avg:34.11ms
step:406/2110 train_time:13848ms step_avg:34.11ms
step:407/2110 train_time:13881ms step_avg:34.11ms
step:408/2110 train_time:13914ms step_avg:34.10ms
step:409/2110 train_time:13947ms step_avg:34.10ms
step:410/2110 train_time:13980ms step_avg:34.10ms
step:411/2110 train_time:14013ms step_avg:34.10ms
step:412/2110 train_time:14046ms step_avg:34.09ms
step:413/2110 train_time:14079ms step_avg:34.09ms
step:414/2110 train_time:14112ms step_avg:34.09ms
step:415/2110 train_time:14146ms step_avg:34.09ms
step:416/2110 train_time:14178ms step_avg:34.08ms
step:417/2110 train_time:14211ms step_avg:34.08ms
step:418/2110 train_time:14244ms step_avg:34.08ms
step:419/2110 train_time:14277ms step_avg:34.07ms
step:420/2110 train_time:14310ms step_avg:34.07ms
step:421/2110 train_time:14343ms step_avg:34.07ms
step:422/2110 train_time:14376ms step_avg:34.07ms
step:423/2110 train_time:14409ms step_avg:34.06ms
step:424/2110 train_time:14441ms step_avg:34.06ms
step:425/2110 train_time:14475ms step_avg:34.06ms
step:426/2110 train_time:14507ms step_avg:34.05ms
step:427/2110 train_time:14540ms step_avg:34.05ms
step:428/2110 train_time:14573ms step_avg:34.05ms
step:429/2110 train_time:14606ms step_avg:34.05ms
step:430/2110 train_time:14639ms step_avg:34.04ms
step:431/2110 train_time:14672ms step_avg:34.04ms
step:432/2110 train_time:14705ms step_avg:34.04ms
step:433/2110 train_time:14738ms step_avg:34.04ms
step:434/2110 train_time:14771ms step_avg:34.03ms
step:435/2110 train_time:14804ms step_avg:34.03ms
step:436/2110 train_time:14836ms step_avg:34.03ms
step:437/2110 train_time:14869ms step_avg:34.03ms
step:438/2110 train_time:14902ms step_avg:34.02ms
step:439/2110 train_time:14936ms step_avg:34.02ms
step:440/2110 train_time:14968ms step_avg:34.02ms
step:441/2110 train_time:15002ms step_avg:34.02ms
step:442/2110 train_time:15035ms step_avg:34.02ms
step:443/2110 train_time:15068ms step_avg:34.01ms
step:444/2110 train_time:15100ms step_avg:34.01ms
step:445/2110 train_time:15133ms step_avg:34.01ms
step:446/2110 train_time:15166ms step_avg:34.00ms
step:447/2110 train_time:15199ms step_avg:34.00ms
step:448/2110 train_time:15232ms step_avg:34.00ms
step:449/2110 train_time:15265ms step_avg:34.00ms
step:450/2110 train_time:15298ms step_avg:34.00ms
step:451/2110 train_time:15331ms step_avg:33.99ms
step:452/2110 train_time:15365ms step_avg:33.99ms
step:453/2110 train_time:15398ms step_avg:33.99ms
step:454/2110 train_time:15431ms step_avg:33.99ms
step:455/2110 train_time:15464ms step_avg:33.99ms
step:456/2110 train_time:15497ms step_avg:33.98ms
step:457/2110 train_time:15530ms step_avg:33.98ms
step:458/2110 train_time:15562ms step_avg:33.98ms
step:459/2110 train_time:15595ms step_avg:33.98ms
step:460/2110 train_time:15627ms step_avg:33.97ms
step:461/2110 train_time:15661ms step_avg:33.97ms
step:462/2110 train_time:15694ms step_avg:33.97ms
step:463/2110 train_time:15727ms step_avg:33.97ms
step:464/2110 train_time:15760ms step_avg:33.96ms
step:465/2110 train_time:15793ms step_avg:33.96ms
step:466/2110 train_time:15825ms step_avg:33.96ms
step:467/2110 train_time:15859ms step_avg:33.96ms
step:468/2110 train_time:15892ms step_avg:33.96ms
step:469/2110 train_time:15925ms step_avg:33.96ms
step:470/2110 train_time:15958ms step_avg:33.95ms
step:471/2110 train_time:15991ms step_avg:33.95ms
step:472/2110 train_time:16024ms step_avg:33.95ms
step:473/2110 train_time:16057ms step_avg:33.95ms
step:474/2110 train_time:16090ms step_avg:33.94ms
step:475/2110 train_time:16123ms step_avg:33.94ms
step:476/2110 train_time:16155ms step_avg:33.94ms
step:477/2110 train_time:16188ms step_avg:33.94ms
step:478/2110 train_time:16221ms step_avg:33.94ms
step:479/2110 train_time:16254ms step_avg:33.93ms
step:480/2110 train_time:16287ms step_avg:33.93ms
step:481/2110 train_time:16321ms step_avg:33.93ms
step:482/2110 train_time:16353ms step_avg:33.93ms
step:483/2110 train_time:16387ms step_avg:33.93ms
step:484/2110 train_time:16419ms step_avg:33.92ms
step:485/2110 train_time:16453ms step_avg:33.92ms
step:486/2110 train_time:16485ms step_avg:33.92ms
step:487/2110 train_time:16519ms step_avg:33.92ms
step:488/2110 train_time:16551ms step_avg:33.92ms
step:489/2110 train_time:16585ms step_avg:33.92ms
step:490/2110 train_time:16617ms step_avg:33.91ms
step:491/2110 train_time:16651ms step_avg:33.91ms
step:492/2110 train_time:16683ms step_avg:33.91ms
step:493/2110 train_time:16717ms step_avg:33.91ms
step:494/2110 train_time:16749ms step_avg:33.91ms
step:495/2110 train_time:16783ms step_avg:33.90ms
step:496/2110 train_time:16815ms step_avg:33.90ms
step:497/2110 train_time:16848ms step_avg:33.90ms
step:498/2110 train_time:16881ms step_avg:33.90ms
step:499/2110 train_time:16914ms step_avg:33.90ms
step:500/2110 train_time:16947ms step_avg:33.89ms
step:500/2110 val_loss:4.0342 train_time:16982ms step_avg:33.96ms
step:501/2110 train_time:17010ms step_avg:33.95ms
step:502/2110 train_time:17045ms step_avg:33.95ms
step:503/2110 train_time:17070ms step_avg:33.94ms
step:504/2110 train_time:17094ms step_avg:33.92ms
step:505/2110 train_time:17119ms step_avg:33.90ms
step:506/2110 train_time:17153ms step_avg:33.90ms
step:507/2110 train_time:17187ms step_avg:33.90ms
step:508/2110 train_time:17219ms step_avg:33.90ms
step:509/2110 train_time:17253ms step_avg:33.90ms
step:510/2110 train_time:17286ms step_avg:33.89ms
step:511/2110 train_time:17319ms step_avg:33.89ms
step:512/2110 train_time:17351ms step_avg:33.89ms
step:513/2110 train_time:17384ms step_avg:33.89ms
step:514/2110 train_time:17416ms step_avg:33.88ms
step:515/2110 train_time:17449ms step_avg:33.88ms
step:516/2110 train_time:17481ms step_avg:33.88ms
step:517/2110 train_time:17514ms step_avg:33.88ms
step:518/2110 train_time:17547ms step_avg:33.87ms
step:519/2110 train_time:17580ms step_avg:33.87ms
step:520/2110 train_time:17612ms step_avg:33.87ms
step:521/2110 train_time:17645ms step_avg:33.87ms
step:522/2110 train_time:17678ms step_avg:33.87ms
step:523/2110 train_time:17711ms step_avg:33.86ms
step:524/2110 train_time:17743ms step_avg:33.86ms
step:525/2110 train_time:17776ms step_avg:33.86ms
step:526/2110 train_time:17808ms step_avg:33.86ms
step:527/2110 train_time:17842ms step_avg:33.85ms
step:528/2110 train_time:17874ms step_avg:33.85ms
step:529/2110 train_time:17907ms step_avg:33.85ms
step:530/2110 train_time:17940ms step_avg:33.85ms
step:531/2110 train_time:17974ms step_avg:33.85ms
step:532/2110 train_time:18007ms step_avg:33.85ms
step:533/2110 train_time:18041ms step_avg:33.85ms
step:534/2110 train_time:18074ms step_avg:33.85ms
step:535/2110 train_time:18108ms step_avg:33.85ms
step:536/2110 train_time:18141ms step_avg:33.84ms
step:537/2110 train_time:18174ms step_avg:33.84ms
step:538/2110 train_time:18207ms step_avg:33.84ms
step:539/2110 train_time:18241ms step_avg:33.84ms
step:540/2110 train_time:18274ms step_avg:33.84ms
step:541/2110 train_time:18307ms step_avg:33.84ms
step:542/2110 train_time:18340ms step_avg:33.84ms
step:543/2110 train_time:18373ms step_avg:33.84ms
step:544/2110 train_time:18406ms step_avg:33.83ms
step:545/2110 train_time:18439ms step_avg:33.83ms
step:546/2110 train_time:18471ms step_avg:33.83ms
step:547/2110 train_time:18504ms step_avg:33.83ms
step:548/2110 train_time:18537ms step_avg:33.83ms
step:549/2110 train_time:18570ms step_avg:33.82ms
step:550/2110 train_time:18602ms step_avg:33.82ms
step:551/2110 train_time:18636ms step_avg:33.82ms
step:552/2110 train_time:18668ms step_avg:33.82ms
step:553/2110 train_time:18701ms step_avg:33.82ms
step:554/2110 train_time:18734ms step_avg:33.82ms
step:555/2110 train_time:18767ms step_avg:33.81ms
step:556/2110 train_time:18799ms step_avg:33.81ms
step:557/2110 train_time:18832ms step_avg:33.81ms
step:558/2110 train_time:18865ms step_avg:33.81ms
step:559/2110 train_time:18898ms step_avg:33.81ms
step:560/2110 train_time:18932ms step_avg:33.81ms
step:561/2110 train_time:18964ms step_avg:33.80ms
step:562/2110 train_time:18997ms step_avg:33.80ms
step:563/2110 train_time:19030ms step_avg:33.80ms
step:564/2110 train_time:19063ms step_avg:33.80ms
step:565/2110 train_time:19097ms step_avg:33.80ms
step:566/2110 train_time:19129ms step_avg:33.80ms
step:567/2110 train_time:19163ms step_avg:33.80ms
step:568/2110 train_time:19196ms step_avg:33.80ms
step:569/2110 train_time:19229ms step_avg:33.79ms
step:570/2110 train_time:19262ms step_avg:33.79ms
step:571/2110 train_time:19295ms step_avg:33.79ms
step:572/2110 train_time:19328ms step_avg:33.79ms
step:573/2110 train_time:19361ms step_avg:33.79ms
step:574/2110 train_time:19394ms step_avg:33.79ms
step:575/2110 train_time:19427ms step_avg:33.79ms
step:576/2110 train_time:19459ms step_avg:33.78ms
step:577/2110 train_time:19493ms step_avg:33.78ms
step:578/2110 train_time:19526ms step_avg:33.78ms
step:579/2110 train_time:19559ms step_avg:33.78ms
step:580/2110 train_time:19592ms step_avg:33.78ms
step:581/2110 train_time:19625ms step_avg:33.78ms
step:582/2110 train_time:19658ms step_avg:33.78ms
step:583/2110 train_time:19691ms step_avg:33.77ms
step:584/2110 train_time:19723ms step_avg:33.77ms
step:585/2110 train_time:19756ms step_avg:33.77ms
step:586/2110 train_time:19789ms step_avg:33.77ms
step:587/2110 train_time:19822ms step_avg:33.77ms
step:588/2110 train_time:19854ms step_avg:33.77ms
step:589/2110 train_time:19888ms step_avg:33.77ms
step:590/2110 train_time:19920ms step_avg:33.76ms
step:591/2110 train_time:19954ms step_avg:33.76ms
step:592/2110 train_time:19986ms step_avg:33.76ms
step:593/2110 train_time:20020ms step_avg:33.76ms
step:594/2110 train_time:20052ms step_avg:33.76ms
step:595/2110 train_time:20085ms step_avg:33.76ms
step:596/2110 train_time:20118ms step_avg:33.76ms
step:597/2110 train_time:20152ms step_avg:33.75ms
step:598/2110 train_time:20184ms step_avg:33.75ms
step:599/2110 train_time:20218ms step_avg:33.75ms
step:600/2110 train_time:20250ms step_avg:33.75ms
step:601/2110 train_time:20284ms step_avg:33.75ms
step:602/2110 train_time:20316ms step_avg:33.75ms
step:603/2110 train_time:20349ms step_avg:33.75ms
step:604/2110 train_time:20382ms step_avg:33.74ms
step:605/2110 train_time:20415ms step_avg:33.74ms
step:606/2110 train_time:20448ms step_avg:33.74ms
step:607/2110 train_time:20481ms step_avg:33.74ms
step:608/2110 train_time:20514ms step_avg:33.74ms
step:609/2110 train_time:20547ms step_avg:33.74ms
step:610/2110 train_time:20580ms step_avg:33.74ms
step:611/2110 train_time:20613ms step_avg:33.74ms
step:612/2110 train_time:20645ms step_avg:33.73ms
step:613/2110 train_time:20678ms step_avg:33.73ms
step:614/2110 train_time:20711ms step_avg:33.73ms
step:615/2110 train_time:20744ms step_avg:33.73ms
step:616/2110 train_time:20776ms step_avg:33.73ms
step:617/2110 train_time:20810ms step_avg:33.73ms
step:618/2110 train_time:20842ms step_avg:33.73ms
step:619/2110 train_time:20876ms step_avg:33.72ms
step:620/2110 train_time:20908ms step_avg:33.72ms
step:621/2110 train_time:20941ms step_avg:33.72ms
step:622/2110 train_time:20973ms step_avg:33.72ms
step:623/2110 train_time:21007ms step_avg:33.72ms
step:624/2110 train_time:21040ms step_avg:33.72ms
step:625/2110 train_time:21073ms step_avg:33.72ms
step:626/2110 train_time:21106ms step_avg:33.72ms
step:627/2110 train_time:21139ms step_avg:33.71ms
step:628/2110 train_time:21172ms step_avg:33.71ms
step:629/2110 train_time:21205ms step_avg:33.71ms
step:630/2110 train_time:21238ms step_avg:33.71ms
step:631/2110 train_time:21271ms step_avg:33.71ms
step:632/2110 train_time:21304ms step_avg:33.71ms
step:633/2110 train_time:21337ms step_avg:33.71ms
step:634/2110 train_time:21370ms step_avg:33.71ms
step:635/2110 train_time:21403ms step_avg:33.71ms
step:636/2110 train_time:21436ms step_avg:33.70ms
step:637/2110 train_time:21469ms step_avg:33.70ms
step:638/2110 train_time:21501ms step_avg:33.70ms
step:639/2110 train_time:21535ms step_avg:33.70ms
step:640/2110 train_time:21567ms step_avg:33.70ms
step:641/2110 train_time:21600ms step_avg:33.70ms
step:642/2110 train_time:21633ms step_avg:33.70ms
step:643/2110 train_time:21667ms step_avg:33.70ms
step:644/2110 train_time:21699ms step_avg:33.69ms
step:645/2110 train_time:21732ms step_avg:33.69ms
step:646/2110 train_time:21765ms step_avg:33.69ms
step:647/2110 train_time:21798ms step_avg:33.69ms
step:648/2110 train_time:21831ms step_avg:33.69ms
step:649/2110 train_time:21864ms step_avg:33.69ms
step:650/2110 train_time:21897ms step_avg:33.69ms
step:651/2110 train_time:21930ms step_avg:33.69ms
step:652/2110 train_time:21963ms step_avg:33.69ms
step:653/2110 train_time:21996ms step_avg:33.68ms
step:654/2110 train_time:22028ms step_avg:33.68ms
step:655/2110 train_time:22062ms step_avg:33.68ms
step:656/2110 train_time:22094ms step_avg:33.68ms
step:657/2110 train_time:22128ms step_avg:33.68ms
step:658/2110 train_time:22160ms step_avg:33.68ms
step:659/2110 train_time:22194ms step_avg:33.68ms
step:660/2110 train_time:22226ms step_avg:33.68ms
step:661/2110 train_time:22260ms step_avg:33.68ms
step:662/2110 train_time:22293ms step_avg:33.67ms
step:663/2110 train_time:22326ms step_avg:33.67ms
step:664/2110 train_time:22359ms step_avg:33.67ms
step:665/2110 train_time:22392ms step_avg:33.67ms
step:666/2110 train_time:22425ms step_avg:33.67ms
step:667/2110 train_time:22458ms step_avg:33.67ms
step:668/2110 train_time:22491ms step_avg:33.67ms
step:669/2110 train_time:22524ms step_avg:33.67ms
step:670/2110 train_time:22556ms step_avg:33.67ms
step:671/2110 train_time:22590ms step_avg:33.67ms
step:672/2110 train_time:22623ms step_avg:33.66ms
step:673/2110 train_time:22656ms step_avg:33.66ms
step:674/2110 train_time:22688ms step_avg:33.66ms
step:675/2110 train_time:22721ms step_avg:33.66ms
step:676/2110 train_time:22754ms step_avg:33.66ms
step:677/2110 train_time:22787ms step_avg:33.66ms
step:678/2110 train_time:22820ms step_avg:33.66ms
step:679/2110 train_time:22853ms step_avg:33.66ms
step:680/2110 train_time:22885ms step_avg:33.65ms
step:681/2110 train_time:22919ms step_avg:33.65ms
step:682/2110 train_time:22951ms step_avg:33.65ms
step:683/2110 train_time:22985ms step_avg:33.65ms
step:684/2110 train_time:23018ms step_avg:33.65ms
step:685/2110 train_time:23051ms step_avg:33.65ms
step:686/2110 train_time:23084ms step_avg:33.65ms
step:687/2110 train_time:23117ms step_avg:33.65ms
step:688/2110 train_time:23150ms step_avg:33.65ms
step:689/2110 train_time:23183ms step_avg:33.65ms
step:690/2110 train_time:23216ms step_avg:33.65ms
step:691/2110 train_time:23250ms step_avg:33.65ms
step:692/2110 train_time:23307ms step_avg:33.68ms
step:693/2110 train_time:23367ms step_avg:33.72ms
step:694/2110 train_time:23426ms step_avg:33.75ms
step:695/2110 train_time:23486ms step_avg:33.79ms
step:696/2110 train_time:23545ms step_avg:33.83ms
step:697/2110 train_time:23605ms step_avg:33.87ms
step:698/2110 train_time:23664ms step_avg:33.90ms
step:699/2110 train_time:23724ms step_avg:33.94ms
step:700/2110 train_time:23782ms step_avg:33.97ms
step:701/2110 train_time:23842ms step_avg:34.01ms
step:702/2110 train_time:23901ms step_avg:34.05ms
step:703/2110 train_time:23961ms step_avg:34.08ms
step:704/2110 train_time:24020ms step_avg:34.12ms
step:705/2110 train_time:24081ms step_avg:34.16ms
step:706/2110 train_time:24139ms step_avg:34.19ms
step:707/2110 train_time:24199ms step_avg:34.23ms
step:708/2110 train_time:24258ms step_avg:34.26ms
step:709/2110 train_time:24318ms step_avg:34.30ms
step:710/2110 train_time:24377ms step_avg:34.33ms
step:711/2110 train_time:24437ms step_avg:34.37ms
step:712/2110 train_time:24496ms step_avg:34.40ms
step:713/2110 train_time:24556ms step_avg:34.44ms
step:714/2110 train_time:24614ms step_avg:34.47ms
step:715/2110 train_time:24673ms step_avg:34.51ms
step:716/2110 train_time:24731ms step_avg:34.54ms
step:717/2110 train_time:24791ms step_avg:34.58ms
step:718/2110 train_time:24849ms step_avg:34.61ms
step:719/2110 train_time:24909ms step_avg:34.64ms
step:720/2110 train_time:24967ms step_avg:34.68ms
step:721/2110 train_time:25028ms step_avg:34.71ms
step:722/2110 train_time:25087ms step_avg:34.75ms
step:723/2110 train_time:25147ms step_avg:34.78ms
step:724/2110 train_time:25205ms step_avg:34.81ms
step:725/2110 train_time:25266ms step_avg:34.85ms
step:726/2110 train_time:25324ms step_avg:34.88ms
step:727/2110 train_time:25386ms step_avg:34.92ms
step:728/2110 train_time:25444ms step_avg:34.95ms
step:729/2110 train_time:25504ms step_avg:34.98ms
step:730/2110 train_time:25564ms step_avg:35.02ms
step:731/2110 train_time:25624ms step_avg:35.05ms
step:732/2110 train_time:25682ms step_avg:35.09ms
step:733/2110 train_time:25742ms step_avg:35.12ms
step:734/2110 train_time:25800ms step_avg:35.15ms
step:735/2110 train_time:25861ms step_avg:35.18ms
step:736/2110 train_time:25919ms step_avg:35.22ms
step:737/2110 train_time:25980ms step_avg:35.25ms
step:738/2110 train_time:26038ms step_avg:35.28ms
step:739/2110 train_time:26098ms step_avg:35.32ms
step:740/2110 train_time:26156ms step_avg:35.35ms
step:741/2110 train_time:26217ms step_avg:35.38ms
step:742/2110 train_time:26275ms step_avg:35.41ms
step:743/2110 train_time:26335ms step_avg:35.44ms
step:744/2110 train_time:26393ms step_avg:35.47ms
step:745/2110 train_time:26453ms step_avg:35.51ms
step:746/2110 train_time:26511ms step_avg:35.54ms
step:747/2110 train_time:26571ms step_avg:35.57ms
step:748/2110 train_time:26628ms step_avg:35.60ms
step:749/2110 train_time:26688ms step_avg:35.63ms
step:750/2110 train_time:26746ms step_avg:35.66ms
step:750/2110 val_loss:3.9109 train_time:26808ms step_avg:35.74ms
step:751/2110 train_time:26839ms step_avg:35.74ms
step:752/2110 train_time:26868ms step_avg:35.73ms
step:753/2110 train_time:26930ms step_avg:35.76ms
step:754/2110 train_time:26995ms step_avg:35.80ms
step:755/2110 train_time:27057ms step_avg:35.84ms
step:756/2110 train_time:27116ms step_avg:35.87ms
step:757/2110 train_time:27176ms step_avg:35.90ms
step:758/2110 train_time:27233ms step_avg:35.93ms
step:759/2110 train_time:27293ms step_avg:35.96ms
step:760/2110 train_time:27351ms step_avg:35.99ms
step:761/2110 train_time:27409ms step_avg:36.02ms
step:762/2110 train_time:27468ms step_avg:36.05ms
step:763/2110 train_time:27527ms step_avg:36.08ms
step:764/2110 train_time:27584ms step_avg:36.10ms
step:765/2110 train_time:27643ms step_avg:36.13ms
step:766/2110 train_time:27700ms step_avg:36.16ms
step:767/2110 train_time:27760ms step_avg:36.19ms
step:768/2110 train_time:27818ms step_avg:36.22ms
step:769/2110 train_time:27880ms step_avg:36.25ms
step:770/2110 train_time:27940ms step_avg:36.29ms
step:771/2110 train_time:28001ms step_avg:36.32ms
step:772/2110 train_time:28060ms step_avg:36.35ms
step:773/2110 train_time:28120ms step_avg:36.38ms
step:774/2110 train_time:28178ms step_avg:36.41ms
step:775/2110 train_time:28238ms step_avg:36.44ms
step:776/2110 train_time:28296ms step_avg:36.46ms
step:777/2110 train_time:28355ms step_avg:36.49ms
step:778/2110 train_time:28414ms step_avg:36.52ms
step:779/2110 train_time:28474ms step_avg:36.55ms
step:780/2110 train_time:28532ms step_avg:36.58ms
step:781/2110 train_time:28592ms step_avg:36.61ms
step:782/2110 train_time:28650ms step_avg:36.64ms
step:783/2110 train_time:28709ms step_avg:36.67ms
step:784/2110 train_time:28768ms step_avg:36.69ms
step:785/2110 train_time:28828ms step_avg:36.72ms
step:786/2110 train_time:28887ms step_avg:36.75ms
step:787/2110 train_time:28947ms step_avg:36.78ms
step:788/2110 train_time:29006ms step_avg:36.81ms
step:789/2110 train_time:29067ms step_avg:36.84ms
step:790/2110 train_time:29125ms step_avg:36.87ms
step:791/2110 train_time:29185ms step_avg:36.90ms
step:792/2110 train_time:29243ms step_avg:36.92ms
step:793/2110 train_time:29302ms step_avg:36.95ms
step:794/2110 train_time:29360ms step_avg:36.98ms
step:795/2110 train_time:29420ms step_avg:37.01ms
step:796/2110 train_time:29478ms step_avg:37.03ms
step:797/2110 train_time:29538ms step_avg:37.06ms
step:798/2110 train_time:29596ms step_avg:37.09ms
step:799/2110 train_time:29656ms step_avg:37.12ms
step:800/2110 train_time:29714ms step_avg:37.14ms
step:801/2110 train_time:29774ms step_avg:37.17ms
step:802/2110 train_time:29833ms step_avg:37.20ms
step:803/2110 train_time:29894ms step_avg:37.23ms
step:804/2110 train_time:29953ms step_avg:37.25ms
step:805/2110 train_time:30014ms step_avg:37.28ms
step:806/2110 train_time:30073ms step_avg:37.31ms
step:807/2110 train_time:30134ms step_avg:37.34ms
step:808/2110 train_time:30192ms step_avg:37.37ms
step:809/2110 train_time:30252ms step_avg:37.39ms
step:810/2110 train_time:30310ms step_avg:37.42ms
step:811/2110 train_time:30371ms step_avg:37.45ms
step:812/2110 train_time:30429ms step_avg:37.47ms
step:813/2110 train_time:30489ms step_avg:37.50ms
step:814/2110 train_time:30547ms step_avg:37.53ms
step:815/2110 train_time:30607ms step_avg:37.55ms
step:816/2110 train_time:30665ms step_avg:37.58ms
step:817/2110 train_time:30724ms step_avg:37.61ms
step:818/2110 train_time:30783ms step_avg:37.63ms
step:819/2110 train_time:30843ms step_avg:37.66ms
step:820/2110 train_time:30901ms step_avg:37.68ms
step:821/2110 train_time:30961ms step_avg:37.71ms
step:822/2110 train_time:31019ms step_avg:37.74ms
step:823/2110 train_time:31081ms step_avg:37.77ms
step:824/2110 train_time:31139ms step_avg:37.79ms
step:825/2110 train_time:31199ms step_avg:37.82ms
step:826/2110 train_time:31257ms step_avg:37.84ms
step:827/2110 train_time:31317ms step_avg:37.87ms
step:828/2110 train_time:31375ms step_avg:37.89ms
step:829/2110 train_time:31436ms step_avg:37.92ms
step:830/2110 train_time:31494ms step_avg:37.94ms
step:831/2110 train_time:31554ms step_avg:37.97ms
step:832/2110 train_time:31613ms step_avg:38.00ms
step:833/2110 train_time:31672ms step_avg:38.02ms
step:834/2110 train_time:31731ms step_avg:38.05ms
step:835/2110 train_time:31791ms step_avg:38.07ms
step:836/2110 train_time:31850ms step_avg:38.10ms
step:837/2110 train_time:31910ms step_avg:38.12ms
step:838/2110 train_time:31969ms step_avg:38.15ms
step:839/2110 train_time:32030ms step_avg:38.18ms
step:840/2110 train_time:32089ms step_avg:38.20ms
step:841/2110 train_time:32148ms step_avg:38.23ms
step:842/2110 train_time:32206ms step_avg:38.25ms
step:843/2110 train_time:32267ms step_avg:38.28ms
step:844/2110 train_time:32325ms step_avg:38.30ms
step:845/2110 train_time:32384ms step_avg:38.32ms
step:846/2110 train_time:32442ms step_avg:38.35ms
step:847/2110 train_time:32502ms step_avg:38.37ms
step:848/2110 train_time:32560ms step_avg:38.40ms
step:849/2110 train_time:32620ms step_avg:38.42ms
step:850/2110 train_time:32678ms step_avg:38.44ms
step:851/2110 train_time:32738ms step_avg:38.47ms
step:852/2110 train_time:32796ms step_avg:38.49ms
step:853/2110 train_time:32857ms step_avg:38.52ms
step:854/2110 train_time:32917ms step_avg:38.54ms
step:855/2110 train_time:32977ms step_avg:38.57ms
step:856/2110 train_time:33036ms step_avg:38.59ms
step:857/2110 train_time:33096ms step_avg:38.62ms
step:858/2110 train_time:33155ms step_avg:38.64ms
step:859/2110 train_time:33216ms step_avg:38.67ms
step:860/2110 train_time:33275ms step_avg:38.69ms
step:861/2110 train_time:33335ms step_avg:38.72ms
step:862/2110 train_time:33394ms step_avg:38.74ms
step:863/2110 train_time:33453ms step_avg:38.76ms
step:864/2110 train_time:33512ms step_avg:38.79ms
step:865/2110 train_time:33572ms step_avg:38.81ms
step:866/2110 train_time:33631ms step_avg:38.83ms
step:867/2110 train_time:33690ms step_avg:38.86ms
step:868/2110 train_time:33748ms step_avg:38.88ms
step:869/2110 train_time:33808ms step_avg:38.90ms
step:870/2110 train_time:33867ms step_avg:38.93ms
step:871/2110 train_time:33927ms step_avg:38.95ms
step:872/2110 train_time:33985ms step_avg:38.97ms
step:873/2110 train_time:34044ms step_avg:39.00ms
step:874/2110 train_time:34101ms step_avg:39.02ms
step:875/2110 train_time:34161ms step_avg:39.04ms
step:876/2110 train_time:34220ms step_avg:39.06ms
step:877/2110 train_time:34280ms step_avg:39.09ms
step:878/2110 train_time:34338ms step_avg:39.11ms
step:879/2110 train_time:34397ms step_avg:39.13ms
step:880/2110 train_time:34456ms step_avg:39.15ms
step:881/2110 train_time:34516ms step_avg:39.18ms
step:882/2110 train_time:34574ms step_avg:39.20ms
step:883/2110 train_time:34633ms step_avg:39.22ms
step:884/2110 train_time:34692ms step_avg:39.24ms
step:885/2110 train_time:34752ms step_avg:39.27ms
step:886/2110 train_time:34812ms step_avg:39.29ms
step:887/2110 train_time:34872ms step_avg:39.31ms
step:888/2110 train_time:34931ms step_avg:39.34ms
step:889/2110 train_time:34991ms step_avg:39.36ms
step:890/2110 train_time:35049ms step_avg:39.38ms
step:891/2110 train_time:35109ms step_avg:39.40ms
step:892/2110 train_time:35168ms step_avg:39.43ms
step:893/2110 train_time:35228ms step_avg:39.45ms
step:894/2110 train_time:35287ms step_avg:39.47ms
step:895/2110 train_time:35347ms step_avg:39.49ms
step:896/2110 train_time:35406ms step_avg:39.52ms
step:897/2110 train_time:35465ms step_avg:39.54ms
step:898/2110 train_time:35523ms step_avg:39.56ms
step:899/2110 train_time:35582ms step_avg:39.58ms
step:900/2110 train_time:35640ms step_avg:39.60ms
step:901/2110 train_time:35700ms step_avg:39.62ms
step:902/2110 train_time:35758ms step_avg:39.64ms
step:903/2110 train_time:35818ms step_avg:39.67ms
step:904/2110 train_time:35877ms step_avg:39.69ms
step:905/2110 train_time:35938ms step_avg:39.71ms
step:906/2110 train_time:35997ms step_avg:39.73ms
step:907/2110 train_time:36057ms step_avg:39.75ms
step:908/2110 train_time:36116ms step_avg:39.78ms
step:909/2110 train_time:36176ms step_avg:39.80ms
step:910/2110 train_time:36235ms step_avg:39.82ms
step:911/2110 train_time:36295ms step_avg:39.84ms
step:912/2110 train_time:36354ms step_avg:39.86ms
step:913/2110 train_time:36414ms step_avg:39.88ms
step:914/2110 train_time:36473ms step_avg:39.91ms
step:915/2110 train_time:36533ms step_avg:39.93ms
step:916/2110 train_time:36591ms step_avg:39.95ms
step:917/2110 train_time:36651ms step_avg:39.97ms
step:918/2110 train_time:36711ms step_avg:39.99ms
step:919/2110 train_time:36771ms step_avg:40.01ms
step:920/2110 train_time:36829ms step_avg:40.03ms
step:921/2110 train_time:36889ms step_avg:40.05ms
step:922/2110 train_time:36947ms step_avg:40.07ms
step:923/2110 train_time:37007ms step_avg:40.09ms
step:924/2110 train_time:37065ms step_avg:40.11ms
step:925/2110 train_time:37125ms step_avg:40.14ms
step:926/2110 train_time:37183ms step_avg:40.15ms
step:927/2110 train_time:37243ms step_avg:40.18ms
step:928/2110 train_time:37301ms step_avg:40.20ms
step:929/2110 train_time:37361ms step_avg:40.22ms
step:930/2110 train_time:37419ms step_avg:40.24ms
step:931/2110 train_time:37479ms step_avg:40.26ms
step:932/2110 train_time:37538ms step_avg:40.28ms
step:933/2110 train_time:37598ms step_avg:40.30ms
step:934/2110 train_time:37657ms step_avg:40.32ms
step:935/2110 train_time:37717ms step_avg:40.34ms
step:936/2110 train_time:37775ms step_avg:40.36ms
step:937/2110 train_time:37835ms step_avg:40.38ms
step:938/2110 train_time:37894ms step_avg:40.40ms
step:939/2110 train_time:37953ms step_avg:40.42ms
step:940/2110 train_time:38013ms step_avg:40.44ms
step:941/2110 train_time:38073ms step_avg:40.46ms
step:942/2110 train_time:38132ms step_avg:40.48ms
step:943/2110 train_time:38193ms step_avg:40.50ms
step:944/2110 train_time:38252ms step_avg:40.52ms
step:945/2110 train_time:38312ms step_avg:40.54ms
step:946/2110 train_time:38370ms step_avg:40.56ms
step:947/2110 train_time:38431ms step_avg:40.58ms
step:948/2110 train_time:38489ms step_avg:40.60ms
step:949/2110 train_time:38549ms step_avg:40.62ms
step:950/2110 train_time:38608ms step_avg:40.64ms
step:951/2110 train_time:38668ms step_avg:40.66ms
step:952/2110 train_time:38726ms step_avg:40.68ms
step:953/2110 train_time:38785ms step_avg:40.70ms
step:954/2110 train_time:38843ms step_avg:40.72ms
step:955/2110 train_time:38902ms step_avg:40.74ms
step:956/2110 train_time:38960ms step_avg:40.75ms
step:957/2110 train_time:39020ms step_avg:40.77ms
step:958/2110 train_time:39078ms step_avg:40.79ms
step:959/2110 train_time:39139ms step_avg:40.81ms
step:960/2110 train_time:39197ms step_avg:40.83ms
step:961/2110 train_time:39257ms step_avg:40.85ms
step:962/2110 train_time:39315ms step_avg:40.87ms
step:963/2110 train_time:39375ms step_avg:40.89ms
step:964/2110 train_time:39434ms step_avg:40.91ms
step:965/2110 train_time:39495ms step_avg:40.93ms
step:966/2110 train_time:39554ms step_avg:40.95ms
step:967/2110 train_time:39615ms step_avg:40.97ms
step:968/2110 train_time:39673ms step_avg:40.98ms
step:969/2110 train_time:39733ms step_avg:41.00ms
step:970/2110 train_time:39791ms step_avg:41.02ms
step:971/2110 train_time:39852ms step_avg:41.04ms
step:972/2110 train_time:39911ms step_avg:41.06ms
step:973/2110 train_time:39971ms step_avg:41.08ms
step:974/2110 train_time:40030ms step_avg:41.10ms
step:975/2110 train_time:40090ms step_avg:41.12ms
step:976/2110 train_time:40149ms step_avg:41.14ms
step:977/2110 train_time:40209ms step_avg:41.16ms
step:978/2110 train_time:40268ms step_avg:41.17ms
step:979/2110 train_time:40327ms step_avg:41.19ms
step:980/2110 train_time:40384ms step_avg:41.21ms
step:981/2110 train_time:40443ms step_avg:41.23ms
step:982/2110 train_time:40501ms step_avg:41.24ms
step:983/2110 train_time:40561ms step_avg:41.26ms
step:984/2110 train_time:40620ms step_avg:41.28ms
step:985/2110 train_time:40680ms step_avg:41.30ms
step:986/2110 train_time:40738ms step_avg:41.32ms
step:987/2110 train_time:40799ms step_avg:41.34ms
step:988/2110 train_time:40857ms step_avg:41.35ms
step:989/2110 train_time:40917ms step_avg:41.37ms
step:990/2110 train_time:40975ms step_avg:41.39ms
step:991/2110 train_time:41036ms step_avg:41.41ms
step:992/2110 train_time:41095ms step_avg:41.43ms
step:993/2110 train_time:41155ms step_avg:41.45ms
step:994/2110 train_time:41214ms step_avg:41.46ms
step:995/2110 train_time:41274ms step_avg:41.48ms
step:996/2110 train_time:41332ms step_avg:41.50ms
step:997/2110 train_time:41392ms step_avg:41.52ms
step:998/2110 train_time:41452ms step_avg:41.53ms
step:999/2110 train_time:41512ms step_avg:41.55ms
step:1000/2110 train_time:41571ms step_avg:41.57ms
step:1000/2110 val_loss:3.7605 train_time:41633ms step_avg:41.63ms
step:1001/2110 train_time:41673ms step_avg:41.63ms
step:1002/2110 train_time:41712ms step_avg:41.63ms
step:1003/2110 train_time:41755ms step_avg:41.63ms
step:1004/2110 train_time:41816ms step_avg:41.65ms
step:1005/2110 train_time:41877ms step_avg:41.67ms
step:1006/2110 train_time:41935ms step_avg:41.69ms
step:1007/2110 train_time:41995ms step_avg:41.70ms
step:1008/2110 train_time:42052ms step_avg:41.72ms
step:1009/2110 train_time:42112ms step_avg:41.74ms
step:1010/2110 train_time:42169ms step_avg:41.75ms
step:1011/2110 train_time:42229ms step_avg:41.77ms
step:1012/2110 train_time:42287ms step_avg:41.79ms
step:1013/2110 train_time:42346ms step_avg:41.80ms
step:1014/2110 train_time:42405ms step_avg:41.82ms
step:1015/2110 train_time:42465ms step_avg:41.84ms
step:1016/2110 train_time:42522ms step_avg:41.85ms
step:1017/2110 train_time:42582ms step_avg:41.87ms
step:1018/2110 train_time:42641ms step_avg:41.89ms
step:1019/2110 train_time:42702ms step_avg:41.91ms
step:1020/2110 train_time:42761ms step_avg:41.92ms
step:1021/2110 train_time:42823ms step_avg:41.94ms
step:1022/2110 train_time:42882ms step_avg:41.96ms
step:1023/2110 train_time:42943ms step_avg:41.98ms
step:1024/2110 train_time:43002ms step_avg:41.99ms
step:1025/2110 train_time:43062ms step_avg:42.01ms
step:1026/2110 train_time:43121ms step_avg:42.03ms
step:1027/2110 train_time:43180ms step_avg:42.05ms
step:1028/2110 train_time:43238ms step_avg:42.06ms
step:1029/2110 train_time:43298ms step_avg:42.08ms
step:1030/2110 train_time:43355ms step_avg:42.09ms
step:1031/2110 train_time:43415ms step_avg:42.11ms
step:1032/2110 train_time:43472ms step_avg:42.12ms
step:1033/2110 train_time:43532ms step_avg:42.14ms
step:1034/2110 train_time:43590ms step_avg:42.16ms
step:1035/2110 train_time:43650ms step_avg:42.17ms
step:1036/2110 train_time:43710ms step_avg:42.19ms
step:1037/2110 train_time:43771ms step_avg:42.21ms
step:1038/2110 train_time:43830ms step_avg:42.23ms
step:1039/2110 train_time:43891ms step_avg:42.24ms
step:1040/2110 train_time:43950ms step_avg:42.26ms
step:1041/2110 train_time:44010ms step_avg:42.28ms
step:1042/2110 train_time:44069ms step_avg:42.29ms
step:1043/2110 train_time:44130ms step_avg:42.31ms
step:1044/2110 train_time:44188ms step_avg:42.33ms
step:1045/2110 train_time:44248ms step_avg:42.34ms
step:1046/2110 train_time:44307ms step_avg:42.36ms
step:1047/2110 train_time:44367ms step_avg:42.38ms
step:1048/2110 train_time:44426ms step_avg:42.39ms
step:1049/2110 train_time:44485ms step_avg:42.41ms
step:1050/2110 train_time:44543ms step_avg:42.42ms
step:1051/2110 train_time:44604ms step_avg:42.44ms
step:1052/2110 train_time:44663ms step_avg:42.45ms
step:1053/2110 train_time:44723ms step_avg:42.47ms
step:1054/2110 train_time:44782ms step_avg:42.49ms
step:1055/2110 train_time:44842ms step_avg:42.50ms
step:1056/2110 train_time:44901ms step_avg:42.52ms
step:1057/2110 train_time:44963ms step_avg:42.54ms
step:1058/2110 train_time:45022ms step_avg:42.55ms
step:1059/2110 train_time:45082ms step_avg:42.57ms
step:1060/2110 train_time:45140ms step_avg:42.58ms
step:1061/2110 train_time:45199ms step_avg:42.60ms
step:1062/2110 train_time:45257ms step_avg:42.62ms
step:1063/2110 train_time:45317ms step_avg:42.63ms
step:1064/2110 train_time:45375ms step_avg:42.65ms
step:1065/2110 train_time:45435ms step_avg:42.66ms
step:1066/2110 train_time:45492ms step_avg:42.68ms
step:1067/2110 train_time:45552ms step_avg:42.69ms
step:1068/2110 train_time:45610ms step_avg:42.71ms
step:1069/2110 train_time:45670ms step_avg:42.72ms
step:1070/2110 train_time:45728ms step_avg:42.74ms
step:1071/2110 train_time:45789ms step_avg:42.75ms
step:1072/2110 train_time:45848ms step_avg:42.77ms
step:1073/2110 train_time:45909ms step_avg:42.79ms
step:1074/2110 train_time:45969ms step_avg:42.80ms
step:1075/2110 train_time:46029ms step_avg:42.82ms
step:1076/2110 train_time:46088ms step_avg:42.83ms
step:1077/2110 train_time:46149ms step_avg:42.85ms
step:1078/2110 train_time:46208ms step_avg:42.86ms
step:1079/2110 train_time:46268ms step_avg:42.88ms
step:1080/2110 train_time:46328ms step_avg:42.90ms
step:1081/2110 train_time:46388ms step_avg:42.91ms
step:1082/2110 train_time:46446ms step_avg:42.93ms
step:1083/2110 train_time:46506ms step_avg:42.94ms
step:1084/2110 train_time:46565ms step_avg:42.96ms
step:1085/2110 train_time:46624ms step_avg:42.97ms
step:1086/2110 train_time:46682ms step_avg:42.99ms
step:1087/2110 train_time:46742ms step_avg:43.00ms
step:1088/2110 train_time:46801ms step_avg:43.02ms
step:1089/2110 train_time:46862ms step_avg:43.03ms
step:1090/2110 train_time:46921ms step_avg:43.05ms
step:1091/2110 train_time:46982ms step_avg:43.06ms
step:1092/2110 train_time:47040ms step_avg:43.08ms
step:1093/2110 train_time:47101ms step_avg:43.09ms
step:1094/2110 train_time:47159ms step_avg:43.11ms
step:1095/2110 train_time:47219ms step_avg:43.12ms
step:1096/2110 train_time:47278ms step_avg:43.14ms
step:1097/2110 train_time:47337ms step_avg:43.15ms
step:1098/2110 train_time:47396ms step_avg:43.17ms
step:1099/2110 train_time:47456ms step_avg:43.18ms
step:1100/2110 train_time:47514ms step_avg:43.19ms
step:1101/2110 train_time:47574ms step_avg:43.21ms
step:1102/2110 train_time:47632ms step_avg:43.22ms
step:1103/2110 train_time:47692ms step_avg:43.24ms
step:1104/2110 train_time:47750ms step_avg:43.25ms
step:1105/2110 train_time:47810ms step_avg:43.27ms
step:1106/2110 train_time:47869ms step_avg:43.28ms
step:1107/2110 train_time:47929ms step_avg:43.30ms
step:1108/2110 train_time:47988ms step_avg:43.31ms
step:1109/2110 train_time:48049ms step_avg:43.33ms
step:1110/2110 train_time:48108ms step_avg:43.34ms
step:1111/2110 train_time:48168ms step_avg:43.36ms
step:1112/2110 train_time:48227ms step_avg:43.37ms
step:1113/2110 train_time:48287ms step_avg:43.38ms
step:1114/2110 train_time:48346ms step_avg:43.40ms
step:1115/2110 train_time:48406ms step_avg:43.41ms
step:1116/2110 train_time:48465ms step_avg:43.43ms
step:1117/2110 train_time:48525ms step_avg:43.44ms
step:1118/2110 train_time:48583ms step_avg:43.46ms
step:1119/2110 train_time:48643ms step_avg:43.47ms
step:1120/2110 train_time:48702ms step_avg:43.48ms
step:1121/2110 train_time:48762ms step_avg:43.50ms
step:1122/2110 train_time:48821ms step_avg:43.51ms
step:1123/2110 train_time:48882ms step_avg:43.53ms
step:1124/2110 train_time:48940ms step_avg:43.54ms
step:1125/2110 train_time:48999ms step_avg:43.55ms
step:1126/2110 train_time:49058ms step_avg:43.57ms
step:1127/2110 train_time:49118ms step_avg:43.58ms
step:1128/2110 train_time:49177ms step_avg:43.60ms
step:1129/2110 train_time:49236ms step_avg:43.61ms
step:1130/2110 train_time:49295ms step_avg:43.62ms
step:1131/2110 train_time:49354ms step_avg:43.64ms
step:1132/2110 train_time:49413ms step_avg:43.65ms
step:1133/2110 train_time:49472ms step_avg:43.66ms
step:1134/2110 train_time:49530ms step_avg:43.68ms
step:1135/2110 train_time:49591ms step_avg:43.69ms
step:1136/2110 train_time:49648ms step_avg:43.70ms
step:1137/2110 train_time:49708ms step_avg:43.72ms
step:1138/2110 train_time:49768ms step_avg:43.73ms
step:1139/2110 train_time:49827ms step_avg:43.75ms
step:1140/2110 train_time:49886ms step_avg:43.76ms
step:1141/2110 train_time:49948ms step_avg:43.78ms
step:1142/2110 train_time:50008ms step_avg:43.79ms
step:1143/2110 train_time:50069ms step_avg:43.80ms
step:1144/2110 train_time:50128ms step_avg:43.82ms
step:1145/2110 train_time:50190ms step_avg:43.83ms
step:1146/2110 train_time:50249ms step_avg:43.85ms
step:1147/2110 train_time:50309ms step_avg:43.86ms
step:1148/2110 train_time:50369ms step_avg:43.88ms
step:1149/2110 train_time:50429ms step_avg:43.89ms
step:1150/2110 train_time:50488ms step_avg:43.90ms
step:1151/2110 train_time:50549ms step_avg:43.92ms
step:1152/2110 train_time:50608ms step_avg:43.93ms
step:1153/2110 train_time:50668ms step_avg:43.94ms
step:1154/2110 train_time:50727ms step_avg:43.96ms
step:1155/2110 train_time:50787ms step_avg:43.97ms
step:1156/2110 train_time:50846ms step_avg:43.98ms
step:1157/2110 train_time:50907ms step_avg:44.00ms
step:1158/2110 train_time:50966ms step_avg:44.01ms
step:1159/2110 train_time:51027ms step_avg:44.03ms
step:1160/2110 train_time:51087ms step_avg:44.04ms
step:1161/2110 train_time:51148ms step_avg:44.06ms
step:1162/2110 train_time:51208ms step_avg:44.07ms
step:1163/2110 train_time:51270ms step_avg:44.08ms
step:1164/2110 train_time:51329ms step_avg:44.10ms
step:1165/2110 train_time:51390ms step_avg:44.11ms
step:1166/2110 train_time:51449ms step_avg:44.12ms
step:1167/2110 train_time:51510ms step_avg:44.14ms
step:1168/2110 train_time:51570ms step_avg:44.15ms
step:1169/2110 train_time:51630ms step_avg:44.17ms
step:1170/2110 train_time:51689ms step_avg:44.18ms
step:1171/2110 train_time:51750ms step_avg:44.19ms
step:1172/2110 train_time:51809ms step_avg:44.21ms
step:1173/2110 train_time:51869ms step_avg:44.22ms
step:1174/2110 train_time:51928ms step_avg:44.23ms
step:1175/2110 train_time:51989ms step_avg:44.25ms
step:1176/2110 train_time:52048ms step_avg:44.26ms
step:1177/2110 train_time:52109ms step_avg:44.27ms
step:1178/2110 train_time:52169ms step_avg:44.29ms
step:1179/2110 train_time:52230ms step_avg:44.30ms
step:1180/2110 train_time:52289ms step_avg:44.31ms
step:1181/2110 train_time:52350ms step_avg:44.33ms
step:1182/2110 train_time:52409ms step_avg:44.34ms
step:1183/2110 train_time:52470ms step_avg:44.35ms
step:1184/2110 train_time:52529ms step_avg:44.37ms
step:1185/2110 train_time:52589ms step_avg:44.38ms
step:1186/2110 train_time:52649ms step_avg:44.39ms
step:1187/2110 train_time:52709ms step_avg:44.41ms
step:1188/2110 train_time:52768ms step_avg:44.42ms
step:1189/2110 train_time:52829ms step_avg:44.43ms
step:1190/2110 train_time:52888ms step_avg:44.44ms
step:1191/2110 train_time:52948ms step_avg:44.46ms
step:1192/2110 train_time:53007ms step_avg:44.47ms
step:1193/2110 train_time:53068ms step_avg:44.48ms
step:1194/2110 train_time:53128ms step_avg:44.50ms
step:1195/2110 train_time:53189ms step_avg:44.51ms
step:1196/2110 train_time:53249ms step_avg:44.52ms
step:1197/2110 train_time:53310ms step_avg:44.54ms
step:1198/2110 train_time:53370ms step_avg:44.55ms
step:1199/2110 train_time:53430ms step_avg:44.56ms
step:1200/2110 train_time:53489ms step_avg:44.57ms
step:1201/2110 train_time:53550ms step_avg:44.59ms
step:1202/2110 train_time:53609ms step_avg:44.60ms
step:1203/2110 train_time:53670ms step_avg:44.61ms
step:1204/2110 train_time:53729ms step_avg:44.63ms
step:1205/2110 train_time:53790ms step_avg:44.64ms
step:1206/2110 train_time:53848ms step_avg:44.65ms
step:1207/2110 train_time:53909ms step_avg:44.66ms
step:1208/2110 train_time:53969ms step_avg:44.68ms
step:1209/2110 train_time:54029ms step_avg:44.69ms
step:1210/2110 train_time:54089ms step_avg:44.70ms
step:1211/2110 train_time:54149ms step_avg:44.71ms
step:1212/2110 train_time:54209ms step_avg:44.73ms
step:1213/2110 train_time:54271ms step_avg:44.74ms
step:1214/2110 train_time:54330ms step_avg:44.75ms
step:1215/2110 train_time:54390ms step_avg:44.77ms
step:1216/2110 train_time:54449ms step_avg:44.78ms
step:1217/2110 train_time:54510ms step_avg:44.79ms
step:1218/2110 train_time:54570ms step_avg:44.80ms
step:1219/2110 train_time:54631ms step_avg:44.82ms
step:1220/2110 train_time:54690ms step_avg:44.83ms
step:1221/2110 train_time:54751ms step_avg:44.84ms
step:1222/2110 train_time:54810ms step_avg:44.85ms
step:1223/2110 train_time:54870ms step_avg:44.87ms
step:1224/2110 train_time:54930ms step_avg:44.88ms
step:1225/2110 train_time:54991ms step_avg:44.89ms
step:1226/2110 train_time:55050ms step_avg:44.90ms
step:1227/2110 train_time:55111ms step_avg:44.91ms
step:1228/2110 train_time:55171ms step_avg:44.93ms
step:1229/2110 train_time:55231ms step_avg:44.94ms
step:1230/2110 train_time:55291ms step_avg:44.95ms
step:1231/2110 train_time:55351ms step_avg:44.96ms
step:1232/2110 train_time:55410ms step_avg:44.98ms
step:1233/2110 train_time:55472ms step_avg:44.99ms
step:1234/2110 train_time:55530ms step_avg:45.00ms
step:1235/2110 train_time:55591ms step_avg:45.01ms
step:1236/2110 train_time:55649ms step_avg:45.02ms
step:1237/2110 train_time:55710ms step_avg:45.04ms
step:1238/2110 train_time:55770ms step_avg:45.05ms
step:1239/2110 train_time:55829ms step_avg:45.06ms
step:1240/2110 train_time:55889ms step_avg:45.07ms
step:1241/2110 train_time:55950ms step_avg:45.08ms
step:1242/2110 train_time:56009ms step_avg:45.10ms
step:1243/2110 train_time:56070ms step_avg:45.11ms
step:1244/2110 train_time:56130ms step_avg:45.12ms
step:1245/2110 train_time:56191ms step_avg:45.13ms
step:1246/2110 train_time:56250ms step_avg:45.14ms
step:1247/2110 train_time:56310ms step_avg:45.16ms
step:1248/2110 train_time:56370ms step_avg:45.17ms
step:1249/2110 train_time:56431ms step_avg:45.18ms
step:1250/2110 train_time:56490ms step_avg:45.19ms
step:1250/2110 val_loss:3.5952 train_time:56552ms step_avg:45.24ms
step:1251/2110 train_time:56576ms step_avg:45.22ms
step:1252/2110 train_time:56613ms step_avg:45.22ms
step:1253/2110 train_time:56676ms step_avg:45.23ms
step:1254/2110 train_time:56736ms step_avg:45.24ms
step:1255/2110 train_time:56797ms step_avg:45.26ms
step:1256/2110 train_time:56856ms step_avg:45.27ms
step:1257/2110 train_time:56916ms step_avg:45.28ms
step:1258/2110 train_time:56974ms step_avg:45.29ms
step:1259/2110 train_time:57033ms step_avg:45.30ms
step:1260/2110 train_time:57091ms step_avg:45.31ms
step:1261/2110 train_time:57150ms step_avg:45.32ms
step:1262/2110 train_time:57209ms step_avg:45.33ms
step:1263/2110 train_time:57268ms step_avg:45.34ms
step:1264/2110 train_time:57327ms step_avg:45.35ms
step:1265/2110 train_time:57386ms step_avg:45.36ms
step:1266/2110 train_time:57445ms step_avg:45.38ms
step:1267/2110 train_time:57508ms step_avg:45.39ms
step:1268/2110 train_time:57569ms step_avg:45.40ms
step:1269/2110 train_time:57632ms step_avg:45.42ms
step:1270/2110 train_time:57692ms step_avg:45.43ms
step:1271/2110 train_time:57753ms step_avg:45.44ms
step:1272/2110 train_time:57812ms step_avg:45.45ms
step:1273/2110 train_time:57873ms step_avg:45.46ms
step:1274/2110 train_time:57931ms step_avg:45.47ms
step:1275/2110 train_time:57991ms step_avg:45.48ms
step:1276/2110 train_time:58049ms step_avg:45.49ms
step:1277/2110 train_time:58109ms step_avg:45.50ms
step:1278/2110 train_time:58168ms step_avg:45.51ms
step:1279/2110 train_time:58227ms step_avg:45.53ms
step:1280/2110 train_time:58287ms step_avg:45.54ms
step:1281/2110 train_time:58346ms step_avg:45.55ms
step:1282/2110 train_time:58405ms step_avg:45.56ms
step:1283/2110 train_time:58465ms step_avg:45.57ms
step:1284/2110 train_time:58525ms step_avg:45.58ms
step:1285/2110 train_time:58586ms step_avg:45.59ms
step:1286/2110 train_time:58647ms step_avg:45.60ms
step:1287/2110 train_time:58709ms step_avg:45.62ms
step:1288/2110 train_time:58769ms step_avg:45.63ms
step:1289/2110 train_time:58830ms step_avg:45.64ms
step:1290/2110 train_time:58889ms step_avg:45.65ms
step:1291/2110 train_time:58951ms step_avg:45.66ms
step:1292/2110 train_time:59010ms step_avg:45.67ms
step:1293/2110 train_time:59069ms step_avg:45.68ms
step:1294/2110 train_time:59128ms step_avg:45.69ms
step:1295/2110 train_time:59187ms step_avg:45.70ms
step:1296/2110 train_time:59246ms step_avg:45.71ms
step:1297/2110 train_time:59305ms step_avg:45.72ms
step:1298/2110 train_time:59364ms step_avg:45.73ms
step:1299/2110 train_time:59425ms step_avg:45.75ms
step:1300/2110 train_time:59484ms step_avg:45.76ms
step:1301/2110 train_time:59545ms step_avg:45.77ms
step:1302/2110 train_time:59606ms step_avg:45.78ms
step:1303/2110 train_time:59668ms step_avg:45.79ms
step:1304/2110 train_time:59728ms step_avg:45.80ms
step:1305/2110 train_time:59790ms step_avg:45.82ms
step:1306/2110 train_time:59849ms step_avg:45.83ms
step:1307/2110 train_time:59910ms step_avg:45.84ms
step:1308/2110 train_time:59969ms step_avg:45.85ms
step:1309/2110 train_time:60030ms step_avg:45.86ms
step:1310/2110 train_time:60088ms step_avg:45.87ms
step:1311/2110 train_time:60148ms step_avg:45.88ms
step:1312/2110 train_time:60207ms step_avg:45.89ms
step:1313/2110 train_time:60266ms step_avg:45.90ms
step:1314/2110 train_time:60325ms step_avg:45.91ms
step:1315/2110 train_time:60385ms step_avg:45.92ms
step:1316/2110 train_time:60443ms step_avg:45.93ms
step:1317/2110 train_time:60505ms step_avg:45.94ms
step:1318/2110 train_time:60565ms step_avg:45.95ms
step:1319/2110 train_time:60627ms step_avg:45.96ms
step:1320/2110 train_time:60686ms step_avg:45.97ms
step:1321/2110 train_time:60748ms step_avg:45.99ms
step:1322/2110 train_time:60807ms step_avg:46.00ms
step:1323/2110 train_time:60869ms step_avg:46.01ms
step:1324/2110 train_time:60928ms step_avg:46.02ms
step:1325/2110 train_time:60989ms step_avg:46.03ms
step:1326/2110 train_time:61047ms step_avg:46.04ms
step:1327/2110 train_time:61108ms step_avg:46.05ms
step:1328/2110 train_time:61167ms step_avg:46.06ms
step:1329/2110 train_time:61227ms step_avg:46.07ms
step:1330/2110 train_time:61286ms step_avg:46.08ms
step:1331/2110 train_time:61346ms step_avg:46.09ms
step:1332/2110 train_time:61405ms step_avg:46.10ms
step:1333/2110 train_time:61465ms step_avg:46.11ms
step:1334/2110 train_time:61525ms step_avg:46.12ms
step:1335/2110 train_time:61586ms step_avg:46.13ms
step:1336/2110 train_time:61647ms step_avg:46.14ms
step:1337/2110 train_time:61708ms step_avg:46.15ms
step:1338/2110 train_time:61768ms step_avg:46.16ms
step:1339/2110 train_time:61829ms step_avg:46.18ms
step:1340/2110 train_time:61889ms step_avg:46.19ms
step:1341/2110 train_time:61949ms step_avg:46.20ms
step:1342/2110 train_time:62009ms step_avg:46.21ms
step:1343/2110 train_time:62069ms step_avg:46.22ms
step:1344/2110 train_time:62128ms step_avg:46.23ms
step:1345/2110 train_time:62189ms step_avg:46.24ms
step:1346/2110 train_time:62247ms step_avg:46.25ms
step:1347/2110 train_time:62308ms step_avg:46.26ms
step:1348/2110 train_time:62367ms step_avg:46.27ms
step:1349/2110 train_time:62428ms step_avg:46.28ms
step:1350/2110 train_time:62487ms step_avg:46.29ms
step:1351/2110 train_time:62548ms step_avg:46.30ms
step:1352/2110 train_time:62608ms step_avg:46.31ms
step:1353/2110 train_time:62668ms step_avg:46.32ms
step:1354/2110 train_time:62728ms step_avg:46.33ms
step:1355/2110 train_time:62789ms step_avg:46.34ms
step:1356/2110 train_time:62848ms step_avg:46.35ms
step:1357/2110 train_time:62909ms step_avg:46.36ms
step:1358/2110 train_time:62968ms step_avg:46.37ms
step:1359/2110 train_time:63029ms step_avg:46.38ms
step:1360/2110 train_time:63088ms step_avg:46.39ms
step:1361/2110 train_time:63149ms step_avg:46.40ms
step:1362/2110 train_time:63208ms step_avg:46.41ms
step:1363/2110 train_time:63268ms step_avg:46.42ms
step:1364/2110 train_time:63327ms step_avg:46.43ms
step:1365/2110 train_time:63388ms step_avg:46.44ms
step:1366/2110 train_time:63446ms step_avg:46.45ms
step:1367/2110 train_time:63507ms step_avg:46.46ms
step:1368/2110 train_time:63568ms step_avg:46.47ms
step:1369/2110 train_time:63629ms step_avg:46.48ms
step:1370/2110 train_time:63689ms step_avg:46.49ms
step:1371/2110 train_time:63750ms step_avg:46.50ms
step:1372/2110 train_time:63810ms step_avg:46.51ms
step:1373/2110 train_time:63870ms step_avg:46.52ms
step:1374/2110 train_time:63931ms step_avg:46.53ms
step:1375/2110 train_time:63991ms step_avg:46.54ms
step:1376/2110 train_time:64050ms step_avg:46.55ms
step:1377/2110 train_time:64111ms step_avg:46.56ms
step:1378/2110 train_time:64170ms step_avg:46.57ms
step:1379/2110 train_time:64229ms step_avg:46.58ms
step:1380/2110 train_time:64289ms step_avg:46.59ms
step:1381/2110 train_time:64350ms step_avg:46.60ms
step:1382/2110 train_time:64437ms step_avg:46.63ms
step:1383/2110 train_time:64523ms step_avg:46.65ms
step:1384/2110 train_time:64610ms step_avg:46.68ms
step:1385/2110 train_time:64697ms step_avg:46.71ms
step:1386/2110 train_time:64785ms step_avg:46.74ms
step:1387/2110 train_time:64871ms step_avg:46.77ms
step:1388/2110 train_time:64959ms step_avg:46.80ms
step:1389/2110 train_time:65045ms step_avg:46.83ms
step:1390/2110 train_time:65132ms step_avg:46.86ms
step:1391/2110 train_time:65219ms step_avg:46.89ms
step:1392/2110 train_time:65305ms step_avg:46.91ms
step:1393/2110 train_time:65390ms step_avg:46.94ms
step:1394/2110 train_time:65477ms step_avg:46.97ms
step:1395/2110 train_time:65564ms step_avg:47.00ms
step:1396/2110 train_time:65651ms step_avg:47.03ms
step:1397/2110 train_time:65738ms step_avg:47.06ms
step:1398/2110 train_time:65824ms step_avg:47.08ms
step:1399/2110 train_time:65911ms step_avg:47.11ms
step:1400/2110 train_time:66000ms step_avg:47.14ms
step:1401/2110 train_time:66085ms step_avg:47.17ms
step:1402/2110 train_time:66172ms step_avg:47.20ms
step:1403/2110 train_time:66258ms step_avg:47.23ms
step:1404/2110 train_time:66345ms step_avg:47.25ms
step:1405/2110 train_time:66431ms step_avg:47.28ms
step:1406/2110 train_time:66518ms step_avg:47.31ms
step:1407/2110 train_time:66605ms step_avg:47.34ms
step:1408/2110 train_time:66690ms step_avg:47.37ms
step:1409/2110 train_time:66778ms step_avg:47.39ms
step:1410/2110 train_time:66864ms step_avg:47.42ms
step:1411/2110 train_time:66952ms step_avg:47.45ms
step:1412/2110 train_time:67038ms step_avg:47.48ms
step:1413/2110 train_time:67126ms step_avg:47.51ms
step:1414/2110 train_time:67211ms step_avg:47.53ms
step:1415/2110 train_time:67299ms step_avg:47.56ms
step:1416/2110 train_time:67384ms step_avg:47.59ms
step:1417/2110 train_time:67471ms step_avg:47.62ms
step:1418/2110 train_time:67558ms step_avg:47.64ms
step:1419/2110 train_time:67644ms step_avg:47.67ms
step:1420/2110 train_time:67732ms step_avg:47.70ms
step:1421/2110 train_time:67819ms step_avg:47.73ms
step:1422/2110 train_time:67905ms step_avg:47.75ms
step:1423/2110 train_time:67992ms step_avg:47.78ms
step:1424/2110 train_time:68079ms step_avg:47.81ms
step:1425/2110 train_time:68166ms step_avg:47.84ms
step:1426/2110 train_time:68253ms step_avg:47.86ms
step:1427/2110 train_time:68340ms step_avg:47.89ms
step:1428/2110 train_time:68425ms step_avg:47.92ms
step:1429/2110 train_time:68513ms step_avg:47.94ms
step:1430/2110 train_time:68599ms step_avg:47.97ms
step:1431/2110 train_time:68686ms step_avg:48.00ms
step:1432/2110 train_time:68772ms step_avg:48.02ms
step:1433/2110 train_time:68860ms step_avg:48.05ms
step:1434/2110 train_time:68946ms step_avg:48.08ms
step:1435/2110 train_time:69034ms step_avg:48.11ms
step:1436/2110 train_time:69120ms step_avg:48.13ms
step:1437/2110 train_time:69207ms step_avg:48.16ms
step:1438/2110 train_time:69293ms step_avg:48.19ms
step:1439/2110 train_time:69380ms step_avg:48.21ms
step:1440/2110 train_time:69466ms step_avg:48.24ms
step:1441/2110 train_time:69553ms step_avg:48.27ms
step:1442/2110 train_time:69639ms step_avg:48.29ms
step:1443/2110 train_time:69726ms step_avg:48.32ms
step:1444/2110 train_time:69812ms step_avg:48.35ms
step:1445/2110 train_time:69900ms step_avg:48.37ms
step:1446/2110 train_time:69986ms step_avg:48.40ms
step:1447/2110 train_time:70075ms step_avg:48.43ms
step:1448/2110 train_time:70161ms step_avg:48.45ms
step:1449/2110 train_time:70247ms step_avg:48.48ms
step:1450/2110 train_time:70334ms step_avg:48.51ms
step:1451/2110 train_time:70422ms step_avg:48.53ms
step:1452/2110 train_time:70507ms step_avg:48.56ms
step:1453/2110 train_time:70594ms step_avg:48.58ms
step:1454/2110 train_time:70680ms step_avg:48.61ms
step:1455/2110 train_time:70767ms step_avg:48.64ms
step:1456/2110 train_time:70854ms step_avg:48.66ms
step:1457/2110 train_time:70942ms step_avg:48.69ms
step:1458/2110 train_time:71027ms step_avg:48.72ms
step:1459/2110 train_time:71115ms step_avg:48.74ms
step:1460/2110 train_time:71201ms step_avg:48.77ms
step:1461/2110 train_time:71288ms step_avg:48.79ms
step:1462/2110 train_time:71375ms step_avg:48.82ms
step:1463/2110 train_time:71462ms step_avg:48.85ms
step:1464/2110 train_time:71548ms step_avg:48.87ms
step:1465/2110 train_time:71635ms step_avg:48.90ms
step:1466/2110 train_time:71723ms step_avg:48.92ms
step:1467/2110 train_time:71808ms step_avg:48.95ms
step:1468/2110 train_time:71895ms step_avg:48.98ms
step:1469/2110 train_time:71982ms step_avg:49.00ms
step:1470/2110 train_time:72069ms step_avg:49.03ms
step:1471/2110 train_time:72156ms step_avg:49.05ms
step:1472/2110 train_time:72243ms step_avg:49.08ms
step:1473/2110 train_time:72329ms step_avg:49.10ms
step:1474/2110 train_time:72417ms step_avg:49.13ms
step:1475/2110 train_time:72502ms step_avg:49.15ms
step:1476/2110 train_time:72589ms step_avg:49.18ms
step:1477/2110 train_time:72676ms step_avg:49.20ms
step:1478/2110 train_time:72763ms step_avg:49.23ms
step:1479/2110 train_time:72850ms step_avg:49.26ms
step:1480/2110 train_time:72938ms step_avg:49.28ms
step:1481/2110 train_time:73024ms step_avg:49.31ms
step:1482/2110 train_time:73111ms step_avg:49.33ms
step:1483/2110 train_time:73198ms step_avg:49.36ms
step:1484/2110 train_time:73284ms step_avg:49.38ms
step:1485/2110 train_time:73371ms step_avg:49.41ms
step:1486/2110 train_time:73458ms step_avg:49.43ms
step:1487/2110 train_time:73544ms step_avg:49.46ms
step:1488/2110 train_time:73631ms step_avg:49.48ms
step:1489/2110 train_time:73718ms step_avg:49.51ms
step:1490/2110 train_time:73805ms step_avg:49.53ms
step:1491/2110 train_time:73890ms step_avg:49.56ms
step:1492/2110 train_time:73978ms step_avg:49.58ms
step:1493/2110 train_time:74065ms step_avg:49.61ms
step:1494/2110 train_time:74152ms step_avg:49.63ms
step:1495/2110 train_time:74239ms step_avg:49.66ms
step:1496/2110 train_time:74325ms step_avg:49.68ms
step:1497/2110 train_time:74412ms step_avg:49.71ms
step:1498/2110 train_time:74499ms step_avg:49.73ms
step:1499/2110 train_time:74585ms step_avg:49.76ms
step:1500/2110 train_time:74672ms step_avg:49.78ms
step:1500/2110 val_loss:3.4953 train_time:74760ms step_avg:49.84ms
step:1501/2110 train_time:74792ms step_avg:49.83ms
step:1502/2110 train_time:74849ms step_avg:49.83ms
step:1503/2110 train_time:74944ms step_avg:49.86ms
step:1504/2110 train_time:75031ms step_avg:49.89ms
step:1505/2110 train_time:75117ms step_avg:49.91ms
step:1506/2110 train_time:75202ms step_avg:49.94ms
step:1507/2110 train_time:75288ms step_avg:49.96ms
step:1508/2110 train_time:75373ms step_avg:49.98ms
step:1509/2110 train_time:75459ms step_avg:50.01ms
step:1510/2110 train_time:75545ms step_avg:50.03ms
step:1511/2110 train_time:75631ms step_avg:50.05ms
step:1512/2110 train_time:75717ms step_avg:50.08ms
step:1513/2110 train_time:75808ms step_avg:50.10ms
step:1514/2110 train_time:75897ms step_avg:50.13ms
step:1515/2110 train_time:75986ms step_avg:50.16ms
step:1516/2110 train_time:76072ms step_avg:50.18ms
step:1517/2110 train_time:76159ms step_avg:50.20ms
step:1518/2110 train_time:76246ms step_avg:50.23ms
step:1519/2110 train_time:76331ms step_avg:50.25ms
step:1520/2110 train_time:76417ms step_avg:50.27ms
step:1521/2110 train_time:76503ms step_avg:50.30ms
step:1522/2110 train_time:76590ms step_avg:50.32ms
step:1523/2110 train_time:76675ms step_avg:50.35ms
step:1524/2110 train_time:76763ms step_avg:50.37ms
step:1525/2110 train_time:76852ms step_avg:50.39ms
step:1526/2110 train_time:76939ms step_avg:50.42ms
step:1527/2110 train_time:77027ms step_avg:50.44ms
step:1528/2110 train_time:77113ms step_avg:50.47ms
step:1529/2110 train_time:77199ms step_avg:50.49ms
step:1530/2110 train_time:77286ms step_avg:50.51ms
step:1531/2110 train_time:77371ms step_avg:50.54ms
step:1532/2110 train_time:77457ms step_avg:50.56ms
step:1533/2110 train_time:77544ms step_avg:50.58ms
step:1534/2110 train_time:77630ms step_avg:50.61ms
step:1535/2110 train_time:77716ms step_avg:50.63ms
step:1536/2110 train_time:77803ms step_avg:50.65ms
step:1537/2110 train_time:77892ms step_avg:50.68ms
step:1538/2110 train_time:77979ms step_avg:50.70ms
step:1539/2110 train_time:78066ms step_avg:50.73ms
step:1540/2110 train_time:78152ms step_avg:50.75ms
step:1541/2110 train_time:78240ms step_avg:50.77ms
step:1542/2110 train_time:78327ms step_avg:50.80ms
step:1543/2110 train_time:78412ms step_avg:50.82ms
step:1544/2110 train_time:78498ms step_avg:50.84ms
step:1545/2110 train_time:78584ms step_avg:50.86ms
step:1546/2110 train_time:78670ms step_avg:50.89ms
step:1547/2110 train_time:78758ms step_avg:50.91ms
step:1548/2110 train_time:78845ms step_avg:50.93ms
step:1549/2110 train_time:78934ms step_avg:50.96ms
step:1550/2110 train_time:79020ms step_avg:50.98ms
step:1551/2110 train_time:79108ms step_avg:51.00ms
step:1552/2110 train_time:79194ms step_avg:51.03ms
step:1553/2110 train_time:79281ms step_avg:51.05ms
step:1554/2110 train_time:79368ms step_avg:51.07ms
step:1555/2110 train_time:79453ms step_avg:51.10ms
step:1556/2110 train_time:79540ms step_avg:51.12ms
step:1557/2110 train_time:79626ms step_avg:51.14ms
step:1558/2110 train_time:79713ms step_avg:51.16ms
step:1559/2110 train_time:79799ms step_avg:51.19ms
step:1560/2110 train_time:79886ms step_avg:51.21ms
step:1561/2110 train_time:79973ms step_avg:51.23ms
step:1562/2110 train_time:80060ms step_avg:51.25ms
step:1563/2110 train_time:80147ms step_avg:51.28ms
step:1564/2110 train_time:80233ms step_avg:51.30ms
step:1565/2110 train_time:80319ms step_avg:51.32ms
step:1566/2110 train_time:80406ms step_avg:51.34ms
step:1567/2110 train_time:80492ms step_avg:51.37ms
step:1568/2110 train_time:80578ms step_avg:51.39ms
step:1569/2110 train_time:80664ms step_avg:51.41ms
step:1570/2110 train_time:80751ms step_avg:51.43ms
step:1571/2110 train_time:80838ms step_avg:51.46ms
step:1572/2110 train_time:80926ms step_avg:51.48ms
step:1573/2110 train_time:81012ms step_avg:51.50ms
step:1574/2110 train_time:81099ms step_avg:51.52ms
step:1575/2110 train_time:81186ms step_avg:51.55ms
step:1576/2110 train_time:81271ms step_avg:51.57ms
step:1577/2110 train_time:81358ms step_avg:51.59ms
step:1578/2110 train_time:81445ms step_avg:51.61ms
step:1579/2110 train_time:81531ms step_avg:51.63ms
step:1580/2110 train_time:81618ms step_avg:51.66ms
step:1581/2110 train_time:81704ms step_avg:51.68ms
step:1582/2110 train_time:81791ms step_avg:51.70ms
step:1583/2110 train_time:81878ms step_avg:51.72ms
step:1584/2110 train_time:81964ms step_avg:51.75ms
step:1585/2110 train_time:82052ms step_avg:51.77ms
step:1586/2110 train_time:82137ms step_avg:51.79ms
step:1587/2110 train_time:82224ms step_avg:51.81ms
step:1588/2110 train_time:82311ms step_avg:51.83ms
step:1589/2110 train_time:82397ms step_avg:51.85ms
step:1590/2110 train_time:82484ms step_avg:51.88ms
step:1591/2110 train_time:82570ms step_avg:51.90ms
step:1592/2110 train_time:82657ms step_avg:51.92ms
step:1593/2110 train_time:82743ms step_avg:51.94ms
step:1594/2110 train_time:82830ms step_avg:51.96ms
step:1595/2110 train_time:82917ms step_avg:51.99ms
step:1596/2110 train_time:83003ms step_avg:52.01ms
step:1597/2110 train_time:83090ms step_avg:52.03ms
step:1598/2110 train_time:83177ms step_avg:52.05ms
step:1599/2110 train_time:83264ms step_avg:52.07ms
step:1600/2110 train_time:83349ms step_avg:52.09ms
step:1601/2110 train_time:83436ms step_avg:52.12ms
step:1602/2110 train_time:83523ms step_avg:52.14ms
step:1603/2110 train_time:83610ms step_avg:52.16ms
step:1604/2110 train_time:83697ms step_avg:52.18ms
step:1605/2110 train_time:83784ms step_avg:52.20ms
step:1606/2110 train_time:83871ms step_avg:52.22ms
step:1607/2110 train_time:83957ms step_avg:52.24ms
step:1608/2110 train_time:84044ms step_avg:52.27ms
step:1609/2110 train_time:84130ms step_avg:52.29ms
step:1610/2110 train_time:84216ms step_avg:52.31ms
step:1611/2110 train_time:84305ms step_avg:52.33ms
step:1612/2110 train_time:84391ms step_avg:52.35ms
step:1613/2110 train_time:84478ms step_avg:52.37ms
step:1614/2110 train_time:84565ms step_avg:52.39ms
step:1615/2110 train_time:84652ms step_avg:52.42ms
step:1616/2110 train_time:84738ms step_avg:52.44ms
step:1617/2110 train_time:84825ms step_avg:52.46ms
step:1618/2110 train_time:84912ms step_avg:52.48ms
step:1619/2110 train_time:84998ms step_avg:52.50ms
step:1620/2110 train_time:85086ms step_avg:52.52ms
step:1621/2110 train_time:85172ms step_avg:52.54ms
step:1622/2110 train_time:85258ms step_avg:52.56ms
step:1623/2110 train_time:85345ms step_avg:52.58ms
step:1624/2110 train_time:85430ms step_avg:52.60ms
step:1625/2110 train_time:85517ms step_avg:52.63ms
step:1626/2110 train_time:85604ms step_avg:52.65ms
step:1627/2110 train_time:85690ms step_avg:52.67ms
step:1628/2110 train_time:85777ms step_avg:52.69ms
step:1629/2110 train_time:85864ms step_avg:52.71ms
step:1630/2110 train_time:85950ms step_avg:52.73ms
step:1631/2110 train_time:86036ms step_avg:52.75ms
step:1632/2110 train_time:86124ms step_avg:52.77ms
step:1633/2110 train_time:86211ms step_avg:52.79ms
step:1634/2110 train_time:86298ms step_avg:52.81ms
step:1635/2110 train_time:86385ms step_avg:52.83ms
step:1636/2110 train_time:86470ms step_avg:52.85ms
step:1637/2110 train_time:86557ms step_avg:52.88ms
step:1638/2110 train_time:86644ms step_avg:52.90ms
step:1639/2110 train_time:86731ms step_avg:52.92ms
step:1640/2110 train_time:86818ms step_avg:52.94ms
step:1641/2110 train_time:86905ms step_avg:52.96ms
step:1642/2110 train_time:86992ms step_avg:52.98ms
step:1643/2110 train_time:87078ms step_avg:53.00ms
step:1644/2110 train_time:87165ms step_avg:53.02ms
step:1645/2110 train_time:87251ms step_avg:53.04ms
step:1646/2110 train_time:87338ms step_avg:53.06ms
step:1647/2110 train_time:87425ms step_avg:53.08ms
step:1648/2110 train_time:87511ms step_avg:53.10ms
step:1649/2110 train_time:87597ms step_avg:53.12ms
step:1650/2110 train_time:87683ms step_avg:53.14ms
step:1651/2110 train_time:87771ms step_avg:53.16ms
step:1652/2110 train_time:87858ms step_avg:53.18ms
step:1653/2110 train_time:87944ms step_avg:53.20ms
step:1654/2110 train_time:88031ms step_avg:53.22ms
step:1655/2110 train_time:88119ms step_avg:53.24ms
step:1656/2110 train_time:88205ms step_avg:53.26ms
step:1657/2110 train_time:88292ms step_avg:53.28ms
step:1658/2110 train_time:88380ms step_avg:53.30ms
step:1659/2110 train_time:88467ms step_avg:53.33ms
step:1660/2110 train_time:88555ms step_avg:53.35ms
step:1661/2110 train_time:88644ms step_avg:53.37ms
step:1662/2110 train_time:88731ms step_avg:53.39ms
step:1663/2110 train_time:88819ms step_avg:53.41ms
step:1664/2110 train_time:88907ms step_avg:53.43ms
step:1665/2110 train_time:88995ms step_avg:53.45ms
step:1666/2110 train_time:89084ms step_avg:53.47ms
step:1667/2110 train_time:89173ms step_avg:53.49ms
step:1668/2110 train_time:89261ms step_avg:53.51ms
step:1669/2110 train_time:89350ms step_avg:53.53ms
step:1670/2110 train_time:89438ms step_avg:53.56ms
step:1671/2110 train_time:89526ms step_avg:53.58ms
step:1672/2110 train_time:89614ms step_avg:53.60ms
step:1673/2110 train_time:89701ms step_avg:53.62ms
step:1674/2110 train_time:89788ms step_avg:53.64ms
step:1675/2110 train_time:89876ms step_avg:53.66ms
step:1676/2110 train_time:89964ms step_avg:53.68ms
step:1677/2110 train_time:90053ms step_avg:53.70ms
step:1678/2110 train_time:90140ms step_avg:53.72ms
step:1679/2110 train_time:90229ms step_avg:53.74ms
step:1680/2110 train_time:90317ms step_avg:53.76ms
step:1681/2110 train_time:90405ms step_avg:53.78ms
step:1682/2110 train_time:90493ms step_avg:53.80ms
step:1683/2110 train_time:90581ms step_avg:53.82ms
step:1684/2110 train_time:90669ms step_avg:53.84ms
step:1685/2110 train_time:90755ms step_avg:53.86ms
step:1686/2110 train_time:90843ms step_avg:53.88ms
step:1687/2110 train_time:90931ms step_avg:53.90ms
step:1688/2110 train_time:91019ms step_avg:53.92ms
step:1689/2110 train_time:91107ms step_avg:53.94ms
step:1690/2110 train_time:91194ms step_avg:53.96ms
step:1691/2110 train_time:91282ms step_avg:53.98ms
step:1692/2110 train_time:91371ms step_avg:54.00ms
step:1693/2110 train_time:91459ms step_avg:54.02ms
step:1694/2110 train_time:91546ms step_avg:54.04ms
step:1695/2110 train_time:91634ms step_avg:54.06ms
step:1696/2110 train_time:91722ms step_avg:54.08ms
step:1697/2110 train_time:91811ms step_avg:54.10ms
step:1698/2110 train_time:91898ms step_avg:54.12ms
step:1699/2110 train_time:91986ms step_avg:54.14ms
step:1700/2110 train_time:92073ms step_avg:54.16ms
step:1701/2110 train_time:92161ms step_avg:54.18ms
step:1702/2110 train_time:92249ms step_avg:54.20ms
step:1703/2110 train_time:92337ms step_avg:54.22ms
step:1704/2110 train_time:92425ms step_avg:54.24ms
step:1705/2110 train_time:92514ms step_avg:54.26ms
step:1706/2110 train_time:92603ms step_avg:54.28ms
step:1707/2110 train_time:92690ms step_avg:54.30ms
step:1708/2110 train_time:92778ms step_avg:54.32ms
step:1709/2110 train_time:92866ms step_avg:54.34ms
step:1710/2110 train_time:92953ms step_avg:54.36ms
step:1711/2110 train_time:93042ms step_avg:54.38ms
step:1712/2110 train_time:93129ms step_avg:54.40ms
step:1713/2110 train_time:93216ms step_avg:54.42ms
step:1714/2110 train_time:93305ms step_avg:54.44ms
step:1715/2110 train_time:93393ms step_avg:54.46ms
step:1716/2110 train_time:93481ms step_avg:54.48ms
step:1717/2110 train_time:93570ms step_avg:54.50ms
step:1718/2110 train_time:93658ms step_avg:54.52ms
step:1719/2110 train_time:93746ms step_avg:54.54ms
step:1720/2110 train_time:93833ms step_avg:54.55ms
step:1721/2110 train_time:93921ms step_avg:54.57ms
step:1722/2110 train_time:94009ms step_avg:54.59ms
step:1723/2110 train_time:94097ms step_avg:54.61ms
step:1724/2110 train_time:94185ms step_avg:54.63ms
step:1725/2110 train_time:94273ms step_avg:54.65ms
step:1726/2110 train_time:94361ms step_avg:54.67ms
step:1727/2110 train_time:94449ms step_avg:54.69ms
step:1728/2110 train_time:94537ms step_avg:54.71ms
step:1729/2110 train_time:94626ms step_avg:54.73ms
step:1730/2110 train_time:94713ms step_avg:54.75ms
step:1731/2110 train_time:94801ms step_avg:54.77ms
step:1732/2110 train_time:94890ms step_avg:54.79ms
step:1733/2110 train_time:94977ms step_avg:54.81ms
step:1734/2110 train_time:95065ms step_avg:54.82ms
step:1735/2110 train_time:95154ms step_avg:54.84ms
step:1736/2110 train_time:95242ms step_avg:54.86ms
step:1737/2110 train_time:95329ms step_avg:54.88ms
step:1738/2110 train_time:95417ms step_avg:54.90ms
step:1739/2110 train_time:95506ms step_avg:54.92ms
step:1740/2110 train_time:95594ms step_avg:54.94ms
step:1741/2110 train_time:95682ms step_avg:54.96ms
step:1742/2110 train_time:95770ms step_avg:54.98ms
step:1743/2110 train_time:95857ms step_avg:55.00ms
step:1744/2110 train_time:95945ms step_avg:55.01ms
step:1745/2110 train_time:96033ms step_avg:55.03ms
step:1746/2110 train_time:96120ms step_avg:55.05ms
step:1747/2110 train_time:96209ms step_avg:55.07ms
step:1748/2110 train_time:96296ms step_avg:55.09ms
step:1749/2110 train_time:96385ms step_avg:55.11ms
step:1750/2110 train_time:96472ms step_avg:55.13ms
step:1750/2110 val_loss:3.3803 train_time:96561ms step_avg:55.18ms
step:1751/2110 train_time:96592ms step_avg:55.16ms
step:1752/2110 train_time:96653ms step_avg:55.17ms
step:1753/2110 train_time:96747ms step_avg:55.19ms
step:1754/2110 train_time:96835ms step_avg:55.21ms
step:1755/2110 train_time:96923ms step_avg:55.23ms
step:1756/2110 train_time:97009ms step_avg:55.24ms
step:1757/2110 train_time:97097ms step_avg:55.26ms
step:1758/2110 train_time:97183ms step_avg:55.28ms
step:1759/2110 train_time:97269ms step_avg:55.30ms
step:1760/2110 train_time:97358ms step_avg:55.32ms
step:1761/2110 train_time:97443ms step_avg:55.33ms
step:1762/2110 train_time:97533ms step_avg:55.35ms
step:1763/2110 train_time:97624ms step_avg:55.37ms
step:1764/2110 train_time:97717ms step_avg:55.39ms
step:1765/2110 train_time:97805ms step_avg:55.41ms
step:1766/2110 train_time:97893ms step_avg:55.43ms
step:1767/2110 train_time:97981ms step_avg:55.45ms
step:1768/2110 train_time:98067ms step_avg:55.47ms
step:1769/2110 train_time:98155ms step_avg:55.49ms
step:1770/2110 train_time:98241ms step_avg:55.50ms
step:1771/2110 train_time:98328ms step_avg:55.52ms
step:1772/2110 train_time:98416ms step_avg:55.54ms
step:1773/2110 train_time:98504ms step_avg:55.56ms
step:1774/2110 train_time:98594ms step_avg:55.58ms
step:1775/2110 train_time:98684ms step_avg:55.60ms
step:1776/2110 train_time:98773ms step_avg:55.62ms
step:1777/2110 train_time:98863ms step_avg:55.63ms
step:1778/2110 train_time:98952ms step_avg:55.65ms
step:1779/2110 train_time:99039ms step_avg:55.67ms
step:1780/2110 train_time:99126ms step_avg:55.69ms
step:1781/2110 train_time:99213ms step_avg:55.71ms
step:1782/2110 train_time:99299ms step_avg:55.72ms
step:1783/2110 train_time:99386ms step_avg:55.74ms
step:1784/2110 train_time:99473ms step_avg:55.76ms
step:1785/2110 train_time:99563ms step_avg:55.78ms
step:1786/2110 train_time:99651ms step_avg:55.80ms
step:1787/2110 train_time:99740ms step_avg:55.81ms
step:1788/2110 train_time:99829ms step_avg:55.83ms
step:1789/2110 train_time:99918ms step_avg:55.85ms
step:1790/2110 train_time:100005ms step_avg:55.87ms
step:1791/2110 train_time:100093ms step_avg:55.89ms
step:1792/2110 train_time:100180ms step_avg:55.90ms
step:1793/2110 train_time:100267ms step_avg:55.92ms
step:1794/2110 train_time:100354ms step_avg:55.94ms
step:1795/2110 train_time:100442ms step_avg:55.96ms
step:1796/2110 train_time:100530ms step_avg:55.97ms
step:1797/2110 train_time:100619ms step_avg:55.99ms
step:1798/2110 train_time:100709ms step_avg:56.01ms
step:1799/2110 train_time:100798ms step_avg:56.03ms
step:1800/2110 train_time:100885ms step_avg:56.05ms
step:1801/2110 train_time:100974ms step_avg:56.07ms
step:1802/2110 train_time:101061ms step_avg:56.08ms
step:1803/2110 train_time:101148ms step_avg:56.10ms
step:1804/2110 train_time:101235ms step_avg:56.12ms
step:1805/2110 train_time:101323ms step_avg:56.13ms
step:1806/2110 train_time:101411ms step_avg:56.15ms
step:1807/2110 train_time:101498ms step_avg:56.17ms
step:1808/2110 train_time:101585ms step_avg:56.19ms
step:1809/2110 train_time:101674ms step_avg:56.20ms
step:1810/2110 train_time:101762ms step_avg:56.22ms
step:1811/2110 train_time:101850ms step_avg:56.24ms
step:1812/2110 train_time:101938ms step_avg:56.26ms
step:1813/2110 train_time:102026ms step_avg:56.27ms
step:1814/2110 train_time:102114ms step_avg:56.29ms
step:1815/2110 train_time:102201ms step_avg:56.31ms
step:1816/2110 train_time:102288ms step_avg:56.33ms
step:1817/2110 train_time:102376ms step_avg:56.34ms
step:1818/2110 train_time:102462ms step_avg:56.36ms
step:1819/2110 train_time:102551ms step_avg:56.38ms
step:1820/2110 train_time:102638ms step_avg:56.39ms
step:1821/2110 train_time:102728ms step_avg:56.41ms
step:1822/2110 train_time:102816ms step_avg:56.43ms
step:1823/2110 train_time:102906ms step_avg:56.45ms
step:1824/2110 train_time:102994ms step_avg:56.47ms
step:1825/2110 train_time:103081ms step_avg:56.48ms
step:1826/2110 train_time:103168ms step_avg:56.50ms
step:1827/2110 train_time:103257ms step_avg:56.52ms
step:1828/2110 train_time:103344ms step_avg:56.53ms
step:1829/2110 train_time:103432ms step_avg:56.55ms
step:1830/2110 train_time:103520ms step_avg:56.57ms
step:1831/2110 train_time:103608ms step_avg:56.59ms
step:1832/2110 train_time:103697ms step_avg:56.60ms
step:1833/2110 train_time:103785ms step_avg:56.62ms
step:1834/2110 train_time:103872ms step_avg:56.64ms
step:1835/2110 train_time:103961ms step_avg:56.65ms
step:1836/2110 train_time:104049ms step_avg:56.67ms
step:1837/2110 train_time:104138ms step_avg:56.69ms
step:1838/2110 train_time:104225ms step_avg:56.71ms
step:1839/2110 train_time:104312ms step_avg:56.72ms
step:1840/2110 train_time:104399ms step_avg:56.74ms
step:1841/2110 train_time:104487ms step_avg:56.76ms
step:1842/2110 train_time:104575ms step_avg:56.77ms
step:1843/2110 train_time:104664ms step_avg:56.79ms
step:1844/2110 train_time:104752ms step_avg:56.81ms
step:1845/2110 train_time:104842ms step_avg:56.82ms
step:1846/2110 train_time:104929ms step_avg:56.84ms
step:1847/2110 train_time:105017ms step_avg:56.86ms
step:1848/2110 train_time:105104ms step_avg:56.87ms
step:1849/2110 train_time:105192ms step_avg:56.89ms
step:1850/2110 train_time:105280ms step_avg:56.91ms
step:1851/2110 train_time:105368ms step_avg:56.92ms
step:1852/2110 train_time:105456ms step_avg:56.94ms
step:1853/2110 train_time:105544ms step_avg:56.96ms
step:1854/2110 train_time:105632ms step_avg:56.98ms
step:1855/2110 train_time:105720ms step_avg:56.99ms
step:1856/2110 train_time:105808ms step_avg:57.01ms
step:1857/2110 train_time:105896ms step_avg:57.03ms
step:1858/2110 train_time:105983ms step_avg:57.04ms
step:1859/2110 train_time:106071ms step_avg:57.06ms
step:1860/2110 train_time:106158ms step_avg:57.07ms
step:1861/2110 train_time:106246ms step_avg:57.09ms
step:1862/2110 train_time:106333ms step_avg:57.11ms
step:1863/2110 train_time:106422ms step_avg:57.12ms
step:1864/2110 train_time:106511ms step_avg:57.14ms
step:1865/2110 train_time:106599ms step_avg:57.16ms
step:1866/2110 train_time:106687ms step_avg:57.17ms
step:1867/2110 train_time:106775ms step_avg:57.19ms
step:1868/2110 train_time:106862ms step_avg:57.21ms
step:1869/2110 train_time:106951ms step_avg:57.22ms
step:1870/2110 train_time:107039ms step_avg:57.24ms
step:1871/2110 train_time:107126ms step_avg:57.26ms
step:1872/2110 train_time:107214ms step_avg:57.27ms
step:1873/2110 train_time:107303ms step_avg:57.29ms
step:1874/2110 train_time:107390ms step_avg:57.31ms
step:1875/2110 train_time:107479ms step_avg:57.32ms
step:1876/2110 train_time:107566ms step_avg:57.34ms
step:1877/2110 train_time:107655ms step_avg:57.35ms
step:1878/2110 train_time:107742ms step_avg:57.37ms
step:1879/2110 train_time:107830ms step_avg:57.39ms
step:1880/2110 train_time:107918ms step_avg:57.40ms
step:1881/2110 train_time:108006ms step_avg:57.42ms
step:1882/2110 train_time:108093ms step_avg:57.44ms
step:1883/2110 train_time:108182ms step_avg:57.45ms
step:1884/2110 train_time:108269ms step_avg:57.47ms
step:1885/2110 train_time:108358ms step_avg:57.48ms
step:1886/2110 train_time:108444ms step_avg:57.50ms
step:1887/2110 train_time:108533ms step_avg:57.52ms
step:1888/2110 train_time:108622ms step_avg:57.53ms
step:1889/2110 train_time:108709ms step_avg:57.55ms
step:1890/2110 train_time:108797ms step_avg:57.56ms
step:1891/2110 train_time:108885ms step_avg:57.58ms
step:1892/2110 train_time:108973ms step_avg:57.60ms
step:1893/2110 train_time:109060ms step_avg:57.61ms
step:1894/2110 train_time:109149ms step_avg:57.63ms
step:1895/2110 train_time:109236ms step_avg:57.64ms
step:1896/2110 train_time:109323ms step_avg:57.66ms
step:1897/2110 train_time:109411ms step_avg:57.68ms
step:1898/2110 train_time:109499ms step_avg:57.69ms
step:1899/2110 train_time:109587ms step_avg:57.71ms
step:1900/2110 train_time:109675ms step_avg:57.72ms
step:1901/2110 train_time:109763ms step_avg:57.74ms
step:1902/2110 train_time:109852ms step_avg:57.76ms
step:1903/2110 train_time:109940ms step_avg:57.77ms
step:1904/2110 train_time:110028ms step_avg:57.79ms
step:1905/2110 train_time:110116ms step_avg:57.80ms
step:1906/2110 train_time:110203ms step_avg:57.82ms
step:1907/2110 train_time:110292ms step_avg:57.84ms
step:1908/2110 train_time:110380ms step_avg:57.85ms
step:1909/2110 train_time:110467ms step_avg:57.87ms
step:1910/2110 train_time:110556ms step_avg:57.88ms
step:1911/2110 train_time:110644ms step_avg:57.90ms
step:1912/2110 train_time:110731ms step_avg:57.91ms
step:1913/2110 train_time:110821ms step_avg:57.93ms
step:1914/2110 train_time:110909ms step_avg:57.95ms
step:1915/2110 train_time:110996ms step_avg:57.96ms
step:1916/2110 train_time:111084ms step_avg:57.98ms
step:1917/2110 train_time:111172ms step_avg:57.99ms
step:1918/2110 train_time:111259ms step_avg:58.01ms
step:1919/2110 train_time:111347ms step_avg:58.02ms
step:1920/2110 train_time:111435ms step_avg:58.04ms
step:1921/2110 train_time:111523ms step_avg:58.05ms
step:1922/2110 train_time:111611ms step_avg:58.07ms
step:1923/2110 train_time:111699ms step_avg:58.09ms
step:1924/2110 train_time:111788ms step_avg:58.10ms
step:1925/2110 train_time:111875ms step_avg:58.12ms
step:1926/2110 train_time:111963ms step_avg:58.13ms
step:1927/2110 train_time:112051ms step_avg:58.15ms
step:1928/2110 train_time:112139ms step_avg:58.16ms
step:1929/2110 train_time:112226ms step_avg:58.18ms
step:1930/2110 train_time:112315ms step_avg:58.19ms
step:1931/2110 train_time:112402ms step_avg:58.21ms
step:1932/2110 train_time:112490ms step_avg:58.22ms
step:1933/2110 train_time:112577ms step_avg:58.24ms
step:1934/2110 train_time:112665ms step_avg:58.25ms
step:1935/2110 train_time:112752ms step_avg:58.27ms
step:1936/2110 train_time:112841ms step_avg:58.29ms
step:1937/2110 train_time:112928ms step_avg:58.30ms
step:1938/2110 train_time:113017ms step_avg:58.32ms
step:1939/2110 train_time:113104ms step_avg:58.33ms
step:1940/2110 train_time:113191ms step_avg:58.35ms
step:1941/2110 train_time:113281ms step_avg:58.36ms
step:1942/2110 train_time:113367ms step_avg:58.38ms
step:1943/2110 train_time:113456ms step_avg:58.39ms
step:1944/2110 train_time:113543ms step_avg:58.41ms
step:1945/2110 train_time:113631ms step_avg:58.42ms
step:1946/2110 train_time:113720ms step_avg:58.44ms
step:1947/2110 train_time:113807ms step_avg:58.45ms
step:1948/2110 train_time:113894ms step_avg:58.47ms
step:1949/2110 train_time:113983ms step_avg:58.48ms
step:1950/2110 train_time:114071ms step_avg:58.50ms
step:1951/2110 train_time:114160ms step_avg:58.51ms
step:1952/2110 train_time:114247ms step_avg:58.53ms
step:1953/2110 train_time:114336ms step_avg:58.54ms
step:1954/2110 train_time:114423ms step_avg:58.56ms
step:1955/2110 train_time:114513ms step_avg:58.57ms
step:1956/2110 train_time:114601ms step_avg:58.59ms
step:1957/2110 train_time:114689ms step_avg:58.60ms
step:1958/2110 train_time:114777ms step_avg:58.62ms
step:1959/2110 train_time:114865ms step_avg:58.63ms
step:1960/2110 train_time:114952ms step_avg:58.65ms
step:1961/2110 train_time:115042ms step_avg:58.66ms
step:1962/2110 train_time:115130ms step_avg:58.68ms
step:1963/2110 train_time:115218ms step_avg:58.69ms
step:1964/2110 train_time:115306ms step_avg:58.71ms
step:1965/2110 train_time:115394ms step_avg:58.72ms
step:1966/2110 train_time:115481ms step_avg:58.74ms
step:1967/2110 train_time:115570ms step_avg:58.75ms
step:1968/2110 train_time:115657ms step_avg:58.77ms
step:1969/2110 train_time:115746ms step_avg:58.78ms
step:1970/2110 train_time:115833ms step_avg:58.80ms
step:1971/2110 train_time:115921ms step_avg:58.81ms
step:1972/2110 train_time:116010ms step_avg:58.83ms
step:1973/2110 train_time:116098ms step_avg:58.84ms
step:1974/2110 train_time:116185ms step_avg:58.86ms
step:1975/2110 train_time:116274ms step_avg:58.87ms
step:1976/2110 train_time:116362ms step_avg:58.89ms
step:1977/2110 train_time:116450ms step_avg:58.90ms
step:1978/2110 train_time:116538ms step_avg:58.92ms
step:1979/2110 train_time:116626ms step_avg:58.93ms
step:1980/2110 train_time:116714ms step_avg:58.95ms
step:1981/2110 train_time:116802ms step_avg:58.96ms
step:1982/2110 train_time:116891ms step_avg:58.98ms
step:1983/2110 train_time:116980ms step_avg:58.99ms
step:1984/2110 train_time:117067ms step_avg:59.01ms
step:1985/2110 train_time:117155ms step_avg:59.02ms
step:1986/2110 train_time:117242ms step_avg:59.03ms
step:1987/2110 train_time:117332ms step_avg:59.05ms
step:1988/2110 train_time:117420ms step_avg:59.06ms
step:1989/2110 train_time:117508ms step_avg:59.08ms
step:1990/2110 train_time:117595ms step_avg:59.09ms
step:1991/2110 train_time:117683ms step_avg:59.11ms
step:1992/2110 train_time:117771ms step_avg:59.12ms
step:1993/2110 train_time:117860ms step_avg:59.14ms
step:1994/2110 train_time:117949ms step_avg:59.15ms
step:1995/2110 train_time:118036ms step_avg:59.17ms
step:1996/2110 train_time:118124ms step_avg:59.18ms
step:1997/2110 train_time:118211ms step_avg:59.19ms
step:1998/2110 train_time:118299ms step_avg:59.21ms
step:1999/2110 train_time:118387ms step_avg:59.22ms
step:2000/2110 train_time:118475ms step_avg:59.24ms
step:2000/2110 val_loss:3.3051 train_time:118564ms step_avg:59.28ms
step:2001/2110 train_time:118596ms step_avg:59.27ms
step:2002/2110 train_time:118655ms step_avg:59.27ms
step:2003/2110 train_time:118748ms step_avg:59.29ms
step:2004/2110 train_time:118836ms step_avg:59.30ms
step:2005/2110 train_time:118924ms step_avg:59.31ms
step:2006/2110 train_time:119009ms step_avg:59.33ms
step:2007/2110 train_time:119096ms step_avg:59.34ms
step:2008/2110 train_time:119184ms step_avg:59.35ms
step:2009/2110 train_time:119270ms step_avg:59.37ms
step:2010/2110 train_time:119359ms step_avg:59.38ms
step:2011/2110 train_time:119445ms step_avg:59.40ms
step:2012/2110 train_time:119533ms step_avg:59.41ms
step:2013/2110 train_time:119627ms step_avg:59.43ms
step:2014/2110 train_time:119716ms step_avg:59.44ms
step:2015/2110 train_time:119806ms step_avg:59.46ms
step:2016/2110 train_time:119894ms step_avg:59.47ms
step:2017/2110 train_time:119981ms step_avg:59.49ms
step:2018/2110 train_time:120068ms step_avg:59.50ms
step:2019/2110 train_time:120155ms step_avg:59.51ms
step:2020/2110 train_time:120243ms step_avg:59.53ms
step:2021/2110 train_time:120330ms step_avg:59.54ms
step:2022/2110 train_time:120418ms step_avg:59.55ms
step:2023/2110 train_time:120506ms step_avg:59.57ms
step:2024/2110 train_time:120594ms step_avg:59.58ms
step:2025/2110 train_time:120686ms step_avg:59.60ms
step:2026/2110 train_time:120773ms step_avg:59.61ms
step:2027/2110 train_time:120863ms step_avg:59.63ms
step:2028/2110 train_time:120950ms step_avg:59.64ms
step:2029/2110 train_time:121037ms step_avg:59.65ms
step:2030/2110 train_time:121125ms step_avg:59.67ms
step:2031/2110 train_time:121211ms step_avg:59.68ms
step:2032/2110 train_time:121298ms step_avg:59.69ms
step:2033/2110 train_time:121387ms step_avg:59.71ms
step:2034/2110 train_time:121474ms step_avg:59.72ms
step:2035/2110 train_time:121564ms step_avg:59.74ms
step:2036/2110 train_time:121652ms step_avg:59.75ms
step:2037/2110 train_time:121742ms step_avg:59.77ms
step:2038/2110 train_time:121829ms step_avg:59.78ms
step:2039/2110 train_time:121918ms step_avg:59.79ms
step:2040/2110 train_time:122005ms step_avg:59.81ms
step:2041/2110 train_time:122092ms step_avg:59.82ms
step:2042/2110 train_time:122180ms step_avg:59.83ms
step:2043/2110 train_time:122267ms step_avg:59.85ms
step:2044/2110 train_time:122354ms step_avg:59.86ms
step:2045/2110 train_time:122442ms step_avg:59.87ms
step:2046/2110 train_time:122529ms step_avg:59.89ms
step:2047/2110 train_time:122619ms step_avg:59.90ms
step:2048/2110 train_time:122707ms step_avg:59.92ms
step:2049/2110 train_time:122796ms step_avg:59.93ms
step:2050/2110 train_time:122885ms step_avg:59.94ms
step:2051/2110 train_time:122972ms step_avg:59.96ms
step:2052/2110 train_time:123060ms step_avg:59.97ms
step:2053/2110 train_time:123148ms step_avg:59.98ms
step:2054/2110 train_time:123236ms step_avg:60.00ms
step:2055/2110 train_time:123323ms step_avg:60.01ms
step:2056/2110 train_time:123409ms step_avg:60.02ms
step:2057/2110 train_time:123498ms step_avg:60.04ms
step:2058/2110 train_time:123587ms step_avg:60.05ms
step:2059/2110 train_time:123675ms step_avg:60.07ms
step:2060/2110 train_time:123763ms step_avg:60.08ms
step:2061/2110 train_time:123851ms step_avg:60.09ms
step:2062/2110 train_time:123939ms step_avg:60.11ms
step:2063/2110 train_time:124027ms step_avg:60.12ms
step:2064/2110 train_time:124114ms step_avg:60.13ms
step:2065/2110 train_time:124201ms step_avg:60.15ms
step:2066/2110 train_time:124290ms step_avg:60.16ms
step:2067/2110 train_time:124377ms step_avg:60.17ms
step:2068/2110 train_time:124464ms step_avg:60.19ms
step:2069/2110 train_time:124553ms step_avg:60.20ms
step:2070/2110 train_time:124641ms step_avg:60.21ms
step:2071/2110 train_time:124730ms step_avg:60.23ms
step:2072/2110 train_time:124818ms step_avg:60.24ms
step:2073/2110 train_time:124908ms step_avg:60.25ms
step:2074/2110 train_time:124994ms step_avg:60.27ms
step:2075/2110 train_time:125084ms step_avg:60.28ms
step:2076/2110 train_time:125171ms step_avg:60.29ms
step:2077/2110 train_time:125259ms step_avg:60.31ms
step:2078/2110 train_time:125348ms step_avg:60.32ms
step:2079/2110 train_time:125436ms step_avg:60.33ms
step:2080/2110 train_time:125524ms step_avg:60.35ms
step:2081/2110 train_time:125612ms step_avg:60.36ms
step:2082/2110 train_time:125700ms step_avg:60.37ms
step:2083/2110 train_time:125789ms step_avg:60.39ms
step:2084/2110 train_time:125877ms step_avg:60.40ms
step:2085/2110 train_time:125965ms step_avg:60.42ms
step:2086/2110 train_time:126053ms step_avg:60.43ms
step:2087/2110 train_time:126141ms step_avg:60.44ms
step:2088/2110 train_time:126227ms step_avg:60.45ms
step:2089/2110 train_time:126316ms step_avg:60.47ms
step:2090/2110 train_time:126405ms step_avg:60.48ms
step:2091/2110 train_time:126493ms step_avg:60.49ms
step:2092/2110 train_time:126581ms step_avg:60.51ms
step:2093/2110 train_time:126669ms step_avg:60.52ms
step:2094/2110 train_time:126757ms step_avg:60.53ms
step:2095/2110 train_time:126846ms step_avg:60.55ms
step:2096/2110 train_time:126934ms step_avg:60.56ms
step:2097/2110 train_time:127024ms step_avg:60.57ms
step:2098/2110 train_time:127111ms step_avg:60.59ms
step:2099/2110 train_time:127200ms step_avg:60.60ms
step:2100/2110 train_time:127288ms step_avg:60.61ms
step:2101/2110 train_time:127375ms step_avg:60.63ms
step:2102/2110 train_time:127463ms step_avg:60.64ms
step:2103/2110 train_time:127551ms step_avg:60.65ms
step:2104/2110 train_time:127639ms step_avg:60.66ms
step:2105/2110 train_time:127728ms step_avg:60.68ms
step:2106/2110 train_time:127817ms step_avg:60.69ms
step:2107/2110 train_time:127905ms step_avg:60.70ms
step:2108/2110 train_time:127992ms step_avg:60.72ms
step:2109/2110 train_time:128083ms step_avg:60.73ms
step:2110/2110 train_time:128171ms step_avg:60.74ms
step:2110/2110 val_loss:3.2808 train_time:128261ms step_avg:60.79ms
peak memory allocated: 29892 MiB reserved: 44736 MiB

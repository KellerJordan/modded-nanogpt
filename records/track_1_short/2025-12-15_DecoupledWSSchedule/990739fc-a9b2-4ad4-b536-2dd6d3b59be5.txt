import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Tue Dec 16 01:13:01 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   38C    P0            129W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   42C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   43C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            120W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   37C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              88      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              88      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:88ms step_avg:87.87ms
step:2/2110 train_time:112ms step_avg:56.24ms
step:3/2110 train_time:149ms step_avg:49.58ms
step:4/2110 train_time:183ms step_avg:45.81ms
step:5/2110 train_time:226ms step_avg:45.23ms
step:6/2110 train_time:400ms step_avg:66.60ms
step:7/2110 train_time:611ms step_avg:87.33ms
step:8/2110 train_time:644ms step_avg:80.46ms
step:9/2110 train_time:680ms step_avg:75.51ms
step:10/2110 train_time:712ms step_avg:71.19ms
step:11/2110 train_time:747ms step_avg:67.88ms
step:12/2110 train_time:780ms step_avg:64.97ms
step:13/2110 train_time:814ms step_avg:62.65ms
step:14/2110 train_time:847ms step_avg:60.50ms
step:15/2110 train_time:881ms step_avg:58.70ms
step:16/2110 train_time:914ms step_avg:57.11ms
step:17/2110 train_time:948ms step_avg:55.77ms
step:18/2110 train_time:981ms step_avg:54.48ms
step:19/2110 train_time:1015ms step_avg:53.42ms
step:20/2110 train_time:1048ms step_avg:52.40ms
step:21/2110 train_time:1084ms step_avg:51.61ms
step:22/2110 train_time:1119ms step_avg:50.86ms
step:23/2110 train_time:1155ms step_avg:50.22ms
step:24/2110 train_time:1188ms step_avg:49.48ms
step:25/2110 train_time:1223ms step_avg:48.91ms
step:26/2110 train_time:1257ms step_avg:48.33ms
step:27/2110 train_time:1291ms step_avg:47.82ms
step:28/2110 train_time:1324ms step_avg:47.28ms
step:29/2110 train_time:1357ms step_avg:46.80ms
step:30/2110 train_time:1390ms step_avg:46.33ms
step:31/2110 train_time:1423ms step_avg:45.91ms
step:32/2110 train_time:1456ms step_avg:45.49ms
step:33/2110 train_time:1489ms step_avg:45.13ms
step:34/2110 train_time:1523ms step_avg:44.80ms
step:35/2110 train_time:1557ms step_avg:44.48ms
step:36/2110 train_time:1590ms step_avg:44.18ms
step:37/2110 train_time:1624ms step_avg:43.89ms
step:38/2110 train_time:1657ms step_avg:43.60ms
step:39/2110 train_time:1693ms step_avg:43.41ms
step:40/2110 train_time:1726ms step_avg:43.14ms
step:41/2110 train_time:1760ms step_avg:42.94ms
step:42/2110 train_time:1793ms step_avg:42.70ms
step:43/2110 train_time:1829ms step_avg:42.55ms
step:44/2110 train_time:1862ms step_avg:42.33ms
step:45/2110 train_time:1897ms step_avg:42.16ms
step:46/2110 train_time:1930ms step_avg:41.97ms
step:47/2110 train_time:1967ms step_avg:41.84ms
step:48/2110 train_time:2000ms step_avg:41.67ms
step:49/2110 train_time:2037ms step_avg:41.57ms
step:50/2110 train_time:2070ms step_avg:41.40ms
step:51/2110 train_time:2107ms step_avg:41.31ms
step:52/2110 train_time:2140ms step_avg:41.15ms
step:53/2110 train_time:2175ms step_avg:41.05ms
step:54/2110 train_time:2209ms step_avg:40.91ms
step:55/2110 train_time:2245ms step_avg:40.81ms
step:56/2110 train_time:2277ms step_avg:40.66ms
step:57/2110 train_time:2313ms step_avg:40.57ms
step:58/2110 train_time:2345ms step_avg:40.43ms
step:59/2110 train_time:2380ms step_avg:40.35ms
step:60/2110 train_time:2414ms step_avg:40.24ms
step:61/2110 train_time:2451ms step_avg:40.18ms
step:62/2110 train_time:2485ms step_avg:40.08ms
step:63/2110 train_time:2521ms step_avg:40.01ms
step:64/2110 train_time:2553ms step_avg:39.89ms
step:65/2110 train_time:2589ms step_avg:39.84ms
step:66/2110 train_time:2622ms step_avg:39.73ms
step:67/2110 train_time:2659ms step_avg:39.68ms
step:68/2110 train_time:2691ms step_avg:39.58ms
step:69/2110 train_time:2727ms step_avg:39.52ms
step:70/2110 train_time:2760ms step_avg:39.43ms
step:71/2110 train_time:2795ms step_avg:39.37ms
step:72/2110 train_time:2828ms step_avg:39.27ms
step:73/2110 train_time:2863ms step_avg:39.22ms
step:74/2110 train_time:2896ms step_avg:39.13ms
step:75/2110 train_time:2931ms step_avg:39.07ms
step:76/2110 train_time:2963ms step_avg:38.99ms
step:77/2110 train_time:2997ms step_avg:38.93ms
step:78/2110 train_time:3030ms step_avg:38.85ms
step:79/2110 train_time:3066ms step_avg:38.81ms
step:80/2110 train_time:3098ms step_avg:38.73ms
step:81/2110 train_time:3132ms step_avg:38.67ms
step:82/2110 train_time:3165ms step_avg:38.59ms
step:83/2110 train_time:3199ms step_avg:38.54ms
step:84/2110 train_time:3232ms step_avg:38.47ms
step:85/2110 train_time:3268ms step_avg:38.44ms
step:86/2110 train_time:3301ms step_avg:38.39ms
step:87/2110 train_time:3336ms step_avg:38.34ms
step:88/2110 train_time:3368ms step_avg:38.28ms
step:89/2110 train_time:3402ms step_avg:38.22ms
step:90/2110 train_time:3435ms step_avg:38.16ms
step:91/2110 train_time:3471ms step_avg:38.14ms
step:92/2110 train_time:3504ms step_avg:38.09ms
step:93/2110 train_time:3541ms step_avg:38.07ms
step:94/2110 train_time:3574ms step_avg:38.02ms
step:95/2110 train_time:3607ms step_avg:37.97ms
step:96/2110 train_time:3640ms step_avg:37.92ms
step:97/2110 train_time:3676ms step_avg:37.90ms
step:98/2110 train_time:3708ms step_avg:37.84ms
step:99/2110 train_time:3744ms step_avg:37.82ms
step:100/2110 train_time:3778ms step_avg:37.78ms
step:101/2110 train_time:3814ms step_avg:37.77ms
step:102/2110 train_time:3847ms step_avg:37.72ms
step:103/2110 train_time:3883ms step_avg:37.70ms
step:104/2110 train_time:3917ms step_avg:37.66ms
step:105/2110 train_time:3953ms step_avg:37.65ms
step:106/2110 train_time:3986ms step_avg:37.60ms
step:107/2110 train_time:4021ms step_avg:37.58ms
step:108/2110 train_time:4054ms step_avg:37.54ms
step:109/2110 train_time:4090ms step_avg:37.52ms
step:110/2110 train_time:4123ms step_avg:37.48ms
step:111/2110 train_time:4159ms step_avg:37.46ms
step:112/2110 train_time:4192ms step_avg:37.42ms
step:113/2110 train_time:4228ms step_avg:37.42ms
step:114/2110 train_time:4261ms step_avg:37.38ms
step:115/2110 train_time:4298ms step_avg:37.37ms
step:116/2110 train_time:4331ms step_avg:37.34ms
step:117/2110 train_time:4368ms step_avg:37.34ms
step:118/2110 train_time:4401ms step_avg:37.30ms
step:119/2110 train_time:4437ms step_avg:37.28ms
step:120/2110 train_time:4471ms step_avg:37.26ms
step:121/2110 train_time:4507ms step_avg:37.25ms
step:122/2110 train_time:4540ms step_avg:37.21ms
step:123/2110 train_time:4576ms step_avg:37.20ms
step:124/2110 train_time:4608ms step_avg:37.16ms
step:125/2110 train_time:4644ms step_avg:37.15ms
step:126/2110 train_time:4679ms step_avg:37.14ms
step:127/2110 train_time:4714ms step_avg:37.12ms
step:128/2110 train_time:4747ms step_avg:37.09ms
step:129/2110 train_time:4783ms step_avg:37.07ms
step:130/2110 train_time:4815ms step_avg:37.04ms
step:131/2110 train_time:4848ms step_avg:37.01ms
step:132/2110 train_time:4881ms step_avg:36.97ms
step:133/2110 train_time:4914ms step_avg:36.95ms
step:134/2110 train_time:4946ms step_avg:36.91ms
step:135/2110 train_time:4980ms step_avg:36.89ms
step:136/2110 train_time:5012ms step_avg:36.86ms
step:137/2110 train_time:5046ms step_avg:36.83ms
step:138/2110 train_time:5078ms step_avg:36.80ms
step:139/2110 train_time:5112ms step_avg:36.77ms
step:140/2110 train_time:5144ms step_avg:36.74ms
step:141/2110 train_time:5178ms step_avg:36.72ms
step:142/2110 train_time:5210ms step_avg:36.69ms
step:143/2110 train_time:5244ms step_avg:36.67ms
step:144/2110 train_time:5277ms step_avg:36.65ms
step:145/2110 train_time:5311ms step_avg:36.63ms
step:146/2110 train_time:5343ms step_avg:36.60ms
step:147/2110 train_time:5377ms step_avg:36.58ms
step:148/2110 train_time:5410ms step_avg:36.55ms
step:149/2110 train_time:5443ms step_avg:36.53ms
step:150/2110 train_time:5477ms step_avg:36.51ms
step:151/2110 train_time:5510ms step_avg:36.49ms
step:152/2110 train_time:5543ms step_avg:36.47ms
step:153/2110 train_time:5577ms step_avg:36.45ms
step:154/2110 train_time:5610ms step_avg:36.43ms
step:155/2110 train_time:5643ms step_avg:36.41ms
step:156/2110 train_time:5676ms step_avg:36.38ms
step:157/2110 train_time:5712ms step_avg:36.38ms
step:158/2110 train_time:5745ms step_avg:36.36ms
step:159/2110 train_time:5781ms step_avg:36.36ms
step:160/2110 train_time:5814ms step_avg:36.34ms
step:161/2110 train_time:5849ms step_avg:36.33ms
step:162/2110 train_time:5883ms step_avg:36.32ms
step:163/2110 train_time:5920ms step_avg:36.32ms
step:164/2110 train_time:5953ms step_avg:36.30ms
step:165/2110 train_time:5990ms step_avg:36.31ms
step:166/2110 train_time:6023ms step_avg:36.28ms
step:167/2110 train_time:6059ms step_avg:36.28ms
step:168/2110 train_time:6093ms step_avg:36.27ms
step:169/2110 train_time:6130ms step_avg:36.27ms
step:170/2110 train_time:6164ms step_avg:36.26ms
step:171/2110 train_time:6200ms step_avg:36.26ms
step:172/2110 train_time:6232ms step_avg:36.23ms
step:173/2110 train_time:6265ms step_avg:36.22ms
step:174/2110 train_time:6298ms step_avg:36.19ms
step:175/2110 train_time:6332ms step_avg:36.19ms
step:176/2110 train_time:6365ms step_avg:36.17ms
step:177/2110 train_time:6401ms step_avg:36.17ms
step:178/2110 train_time:6435ms step_avg:36.15ms
step:179/2110 train_time:6469ms step_avg:36.14ms
step:180/2110 train_time:6503ms step_avg:36.13ms
step:181/2110 train_time:6539ms step_avg:36.13ms
step:182/2110 train_time:6574ms step_avg:36.12ms
step:183/2110 train_time:6609ms step_avg:36.11ms
step:184/2110 train_time:6641ms step_avg:36.09ms
step:185/2110 train_time:6676ms step_avg:36.09ms
step:186/2110 train_time:6709ms step_avg:36.07ms
step:187/2110 train_time:6744ms step_avg:36.06ms
step:188/2110 train_time:6778ms step_avg:36.05ms
step:189/2110 train_time:6813ms step_avg:36.05ms
step:190/2110 train_time:6845ms step_avg:36.03ms
step:191/2110 train_time:6880ms step_avg:36.02ms
step:192/2110 train_time:6913ms step_avg:36.00ms
step:193/2110 train_time:6946ms step_avg:35.99ms
step:194/2110 train_time:6980ms step_avg:35.98ms
step:195/2110 train_time:7016ms step_avg:35.98ms
step:196/2110 train_time:7048ms step_avg:35.96ms
step:197/2110 train_time:7083ms step_avg:35.95ms
step:198/2110 train_time:7116ms step_avg:35.94ms
step:199/2110 train_time:7150ms step_avg:35.93ms
step:200/2110 train_time:7184ms step_avg:35.92ms
step:201/2110 train_time:7222ms step_avg:35.93ms
step:202/2110 train_time:7255ms step_avg:35.92ms
step:203/2110 train_time:7292ms step_avg:35.92ms
step:204/2110 train_time:7325ms step_avg:35.91ms
step:205/2110 train_time:7360ms step_avg:35.90ms
step:206/2110 train_time:7392ms step_avg:35.88ms
step:207/2110 train_time:7425ms step_avg:35.87ms
step:208/2110 train_time:7458ms step_avg:35.85ms
step:209/2110 train_time:7492ms step_avg:35.85ms
step:210/2110 train_time:7525ms step_avg:35.83ms
step:211/2110 train_time:7559ms step_avg:35.82ms
step:212/2110 train_time:7593ms step_avg:35.82ms
step:213/2110 train_time:7627ms step_avg:35.81ms
step:214/2110 train_time:7660ms step_avg:35.79ms
step:215/2110 train_time:7694ms step_avg:35.79ms
step:216/2110 train_time:7727ms step_avg:35.77ms
step:217/2110 train_time:7761ms step_avg:35.77ms
step:218/2110 train_time:7794ms step_avg:35.75ms
step:219/2110 train_time:7831ms step_avg:35.76ms
step:220/2110 train_time:7864ms step_avg:35.74ms
step:221/2110 train_time:7897ms step_avg:35.73ms
step:222/2110 train_time:7930ms step_avg:35.72ms
step:223/2110 train_time:7963ms step_avg:35.71ms
step:224/2110 train_time:7995ms step_avg:35.69ms
step:225/2110 train_time:8029ms step_avg:35.68ms
step:226/2110 train_time:8062ms step_avg:35.67ms
step:227/2110 train_time:8095ms step_avg:35.66ms
step:228/2110 train_time:8128ms step_avg:35.65ms
step:229/2110 train_time:8162ms step_avg:35.64ms
step:230/2110 train_time:8194ms step_avg:35.63ms
step:231/2110 train_time:8228ms step_avg:35.62ms
step:232/2110 train_time:8260ms step_avg:35.61ms
step:233/2110 train_time:8294ms step_avg:35.60ms
step:234/2110 train_time:8327ms step_avg:35.58ms
step:235/2110 train_time:8363ms step_avg:35.59ms
step:236/2110 train_time:8396ms step_avg:35.58ms
step:237/2110 train_time:8430ms step_avg:35.57ms
step:238/2110 train_time:8463ms step_avg:35.56ms
step:239/2110 train_time:8499ms step_avg:35.56ms
step:240/2110 train_time:8531ms step_avg:35.55ms
step:241/2110 train_time:8567ms step_avg:35.55ms
step:242/2110 train_time:8598ms step_avg:35.53ms
step:243/2110 train_time:8632ms step_avg:35.52ms
step:244/2110 train_time:8665ms step_avg:35.51ms
step:245/2110 train_time:8698ms step_avg:35.50ms
step:246/2110 train_time:8731ms step_avg:35.49ms
step:247/2110 train_time:8764ms step_avg:35.48ms
step:248/2110 train_time:8797ms step_avg:35.47ms
step:249/2110 train_time:8830ms step_avg:35.46ms
step:250/2110 train_time:8862ms step_avg:35.45ms
step:250/2110 val_loss:4.2978 train_time:8898ms step_avg:35.59ms
step:251/2110 train_time:8927ms step_avg:35.57ms
step:252/2110 train_time:8954ms step_avg:35.53ms
step:253/2110 train_time:8982ms step_avg:35.50ms
step:254/2110 train_time:9007ms step_avg:35.46ms
step:255/2110 train_time:9039ms step_avg:35.45ms
step:256/2110 train_time:9073ms step_avg:35.44ms
step:257/2110 train_time:9107ms step_avg:35.43ms
step:258/2110 train_time:9139ms step_avg:35.42ms
step:259/2110 train_time:9173ms step_avg:35.42ms
step:260/2110 train_time:9206ms step_avg:35.41ms
step:261/2110 train_time:9240ms step_avg:35.40ms
step:262/2110 train_time:9272ms step_avg:35.39ms
step:263/2110 train_time:9306ms step_avg:35.38ms
step:264/2110 train_time:9339ms step_avg:35.38ms
step:265/2110 train_time:9371ms step_avg:35.36ms
step:266/2110 train_time:9404ms step_avg:35.35ms
step:267/2110 train_time:9437ms step_avg:35.35ms
step:268/2110 train_time:9470ms step_avg:35.33ms
step:269/2110 train_time:9503ms step_avg:35.33ms
step:270/2110 train_time:9535ms step_avg:35.32ms
step:271/2110 train_time:9568ms step_avg:35.31ms
step:272/2110 train_time:9601ms step_avg:35.30ms
step:273/2110 train_time:9634ms step_avg:35.29ms
step:274/2110 train_time:9667ms step_avg:35.28ms
step:275/2110 train_time:9700ms step_avg:35.27ms
step:276/2110 train_time:9732ms step_avg:35.26ms
step:277/2110 train_time:9766ms step_avg:35.26ms
step:278/2110 train_time:9798ms step_avg:35.25ms
step:279/2110 train_time:9832ms step_avg:35.24ms
step:280/2110 train_time:9864ms step_avg:35.23ms
step:281/2110 train_time:9898ms step_avg:35.22ms
step:282/2110 train_time:9932ms step_avg:35.22ms
step:283/2110 train_time:9965ms step_avg:35.21ms
step:284/2110 train_time:9998ms step_avg:35.20ms
step:285/2110 train_time:10032ms step_avg:35.20ms
step:286/2110 train_time:10065ms step_avg:35.19ms
step:287/2110 train_time:10098ms step_avg:35.19ms
step:288/2110 train_time:10132ms step_avg:35.18ms
step:289/2110 train_time:10166ms step_avg:35.18ms
step:290/2110 train_time:10198ms step_avg:35.17ms
step:291/2110 train_time:10232ms step_avg:35.16ms
step:292/2110 train_time:10265ms step_avg:35.15ms
step:293/2110 train_time:10298ms step_avg:35.15ms
step:294/2110 train_time:10331ms step_avg:35.14ms
step:295/2110 train_time:10364ms step_avg:35.13ms
step:296/2110 train_time:10396ms step_avg:35.12ms
step:297/2110 train_time:10430ms step_avg:35.12ms
step:298/2110 train_time:10462ms step_avg:35.11ms
step:299/2110 train_time:10495ms step_avg:35.10ms
step:300/2110 train_time:10528ms step_avg:35.09ms
step:301/2110 train_time:10561ms step_avg:35.09ms
step:302/2110 train_time:10593ms step_avg:35.08ms
step:303/2110 train_time:10627ms step_avg:35.07ms
step:304/2110 train_time:10659ms step_avg:35.06ms
step:305/2110 train_time:10692ms step_avg:35.06ms
step:306/2110 train_time:10725ms step_avg:35.05ms
step:307/2110 train_time:10758ms step_avg:35.04ms
step:308/2110 train_time:10790ms step_avg:35.03ms
step:309/2110 train_time:10824ms step_avg:35.03ms
step:310/2110 train_time:10856ms step_avg:35.02ms
step:311/2110 train_time:10890ms step_avg:35.02ms
step:312/2110 train_time:10923ms step_avg:35.01ms
step:313/2110 train_time:10957ms step_avg:35.01ms
step:314/2110 train_time:10990ms step_avg:35.00ms
step:315/2110 train_time:11023ms step_avg:34.99ms
step:316/2110 train_time:11056ms step_avg:34.99ms
step:317/2110 train_time:11090ms step_avg:34.98ms
step:318/2110 train_time:11123ms step_avg:34.98ms
step:319/2110 train_time:11156ms step_avg:34.97ms
step:320/2110 train_time:11189ms step_avg:34.96ms
step:321/2110 train_time:11222ms step_avg:34.96ms
step:322/2110 train_time:11255ms step_avg:34.95ms
step:323/2110 train_time:11289ms step_avg:34.95ms
step:324/2110 train_time:11322ms step_avg:34.94ms
step:325/2110 train_time:11355ms step_avg:34.94ms
step:326/2110 train_time:11388ms step_avg:34.93ms
step:327/2110 train_time:11422ms step_avg:34.93ms
step:328/2110 train_time:11454ms step_avg:34.92ms
step:329/2110 train_time:11488ms step_avg:34.92ms
step:330/2110 train_time:11520ms step_avg:34.91ms
step:331/2110 train_time:11553ms step_avg:34.90ms
step:332/2110 train_time:11586ms step_avg:34.90ms
step:333/2110 train_time:11620ms step_avg:34.89ms
step:334/2110 train_time:11652ms step_avg:34.89ms
step:335/2110 train_time:11685ms step_avg:34.88ms
step:336/2110 train_time:11718ms step_avg:34.87ms
step:337/2110 train_time:11751ms step_avg:34.87ms
step:338/2110 train_time:11783ms step_avg:34.86ms
step:339/2110 train_time:11816ms step_avg:34.86ms
step:340/2110 train_time:11849ms step_avg:34.85ms
step:341/2110 train_time:11882ms step_avg:34.85ms
step:342/2110 train_time:11915ms step_avg:34.84ms
step:343/2110 train_time:11949ms step_avg:34.84ms
step:344/2110 train_time:11982ms step_avg:34.83ms
step:345/2110 train_time:12015ms step_avg:34.83ms
step:346/2110 train_time:12048ms step_avg:34.82ms
step:347/2110 train_time:12082ms step_avg:34.82ms
step:348/2110 train_time:12115ms step_avg:34.81ms
step:349/2110 train_time:12148ms step_avg:34.81ms
step:350/2110 train_time:12181ms step_avg:34.80ms
step:351/2110 train_time:12215ms step_avg:34.80ms
step:352/2110 train_time:12248ms step_avg:34.79ms
step:353/2110 train_time:12281ms step_avg:34.79ms
step:354/2110 train_time:12314ms step_avg:34.79ms
step:355/2110 train_time:12348ms step_avg:34.78ms
step:356/2110 train_time:12380ms step_avg:34.78ms
step:357/2110 train_time:12414ms step_avg:34.77ms
step:358/2110 train_time:12446ms step_avg:34.77ms
step:359/2110 train_time:12480ms step_avg:34.76ms
step:360/2110 train_time:12512ms step_avg:34.76ms
step:361/2110 train_time:12546ms step_avg:34.75ms
step:362/2110 train_time:12578ms step_avg:34.75ms
step:363/2110 train_time:12612ms step_avg:34.74ms
step:364/2110 train_time:12644ms step_avg:34.74ms
step:365/2110 train_time:12678ms step_avg:34.73ms
step:366/2110 train_time:12710ms step_avg:34.73ms
step:367/2110 train_time:12743ms step_avg:34.72ms
step:368/2110 train_time:12776ms step_avg:34.72ms
step:369/2110 train_time:12810ms step_avg:34.71ms
step:370/2110 train_time:12842ms step_avg:34.71ms
step:371/2110 train_time:12876ms step_avg:34.71ms
step:372/2110 train_time:12908ms step_avg:34.70ms
step:373/2110 train_time:12942ms step_avg:34.70ms
step:374/2110 train_time:12974ms step_avg:34.69ms
step:375/2110 train_time:13007ms step_avg:34.69ms
step:376/2110 train_time:13040ms step_avg:34.68ms
step:377/2110 train_time:13073ms step_avg:34.68ms
step:378/2110 train_time:13106ms step_avg:34.67ms
step:379/2110 train_time:13140ms step_avg:34.67ms
step:380/2110 train_time:13173ms step_avg:34.67ms
step:381/2110 train_time:13206ms step_avg:34.66ms
step:382/2110 train_time:13239ms step_avg:34.66ms
step:383/2110 train_time:13273ms step_avg:34.65ms
step:384/2110 train_time:13306ms step_avg:34.65ms
step:385/2110 train_time:13339ms step_avg:34.65ms
step:386/2110 train_time:13372ms step_avg:34.64ms
step:387/2110 train_time:13405ms step_avg:34.64ms
step:388/2110 train_time:13437ms step_avg:34.63ms
step:389/2110 train_time:13471ms step_avg:34.63ms
step:390/2110 train_time:13503ms step_avg:34.62ms
step:391/2110 train_time:13537ms step_avg:34.62ms
step:392/2110 train_time:13569ms step_avg:34.62ms
step:393/2110 train_time:13603ms step_avg:34.61ms
step:394/2110 train_time:13636ms step_avg:34.61ms
step:395/2110 train_time:13669ms step_avg:34.61ms
step:396/2110 train_time:13702ms step_avg:34.60ms
step:397/2110 train_time:13735ms step_avg:34.60ms
step:398/2110 train_time:13767ms step_avg:34.59ms
step:399/2110 train_time:13801ms step_avg:34.59ms
step:400/2110 train_time:13834ms step_avg:34.58ms
step:401/2110 train_time:13867ms step_avg:34.58ms
step:402/2110 train_time:13900ms step_avg:34.58ms
step:403/2110 train_time:13933ms step_avg:34.57ms
step:404/2110 train_time:13966ms step_avg:34.57ms
step:405/2110 train_time:13999ms step_avg:34.57ms
step:406/2110 train_time:14032ms step_avg:34.56ms
step:407/2110 train_time:14066ms step_avg:34.56ms
step:408/2110 train_time:14098ms step_avg:34.55ms
step:409/2110 train_time:14132ms step_avg:34.55ms
step:410/2110 train_time:14164ms step_avg:34.55ms
step:411/2110 train_time:14198ms step_avg:34.54ms
step:412/2110 train_time:14231ms step_avg:34.54ms
step:413/2110 train_time:14264ms step_avg:34.54ms
step:414/2110 train_time:14297ms step_avg:34.53ms
step:415/2110 train_time:14330ms step_avg:34.53ms
step:416/2110 train_time:14363ms step_avg:34.53ms
step:417/2110 train_time:14396ms step_avg:34.52ms
step:418/2110 train_time:14429ms step_avg:34.52ms
step:419/2110 train_time:14463ms step_avg:34.52ms
step:420/2110 train_time:14495ms step_avg:34.51ms
step:421/2110 train_time:14529ms step_avg:34.51ms
step:422/2110 train_time:14561ms step_avg:34.51ms
step:423/2110 train_time:14595ms step_avg:34.50ms
step:424/2110 train_time:14627ms step_avg:34.50ms
step:425/2110 train_time:14661ms step_avg:34.50ms
step:426/2110 train_time:14693ms step_avg:34.49ms
step:427/2110 train_time:14727ms step_avg:34.49ms
step:428/2110 train_time:14759ms step_avg:34.48ms
step:429/2110 train_time:14792ms step_avg:34.48ms
step:430/2110 train_time:14825ms step_avg:34.48ms
step:431/2110 train_time:14859ms step_avg:34.48ms
step:432/2110 train_time:14892ms step_avg:34.47ms
step:433/2110 train_time:14925ms step_avg:34.47ms
step:434/2110 train_time:14957ms step_avg:34.46ms
step:435/2110 train_time:14991ms step_avg:34.46ms
step:436/2110 train_time:15023ms step_avg:34.46ms
step:437/2110 train_time:15057ms step_avg:34.45ms
step:438/2110 train_time:15089ms step_avg:34.45ms
step:439/2110 train_time:15123ms step_avg:34.45ms
step:440/2110 train_time:15156ms step_avg:34.44ms
step:441/2110 train_time:15189ms step_avg:34.44ms
step:442/2110 train_time:15222ms step_avg:34.44ms
step:443/2110 train_time:15255ms step_avg:34.44ms
step:444/2110 train_time:15288ms step_avg:34.43ms
step:445/2110 train_time:15321ms step_avg:34.43ms
step:446/2110 train_time:15353ms step_avg:34.42ms
step:447/2110 train_time:15387ms step_avg:34.42ms
step:448/2110 train_time:15420ms step_avg:34.42ms
step:449/2110 train_time:15453ms step_avg:34.42ms
step:450/2110 train_time:15485ms step_avg:34.41ms
step:451/2110 train_time:15519ms step_avg:34.41ms
step:452/2110 train_time:15552ms step_avg:34.41ms
step:453/2110 train_time:15586ms step_avg:34.41ms
step:454/2110 train_time:15618ms step_avg:34.40ms
step:455/2110 train_time:15651ms step_avg:34.40ms
step:456/2110 train_time:15684ms step_avg:34.39ms
step:457/2110 train_time:15718ms step_avg:34.39ms
step:458/2110 train_time:15750ms step_avg:34.39ms
step:459/2110 train_time:15785ms step_avg:34.39ms
step:460/2110 train_time:15817ms step_avg:34.39ms
step:461/2110 train_time:15851ms step_avg:34.38ms
step:462/2110 train_time:15884ms step_avg:34.38ms
step:463/2110 train_time:15917ms step_avg:34.38ms
step:464/2110 train_time:15950ms step_avg:34.37ms
step:465/2110 train_time:15983ms step_avg:34.37ms
step:466/2110 train_time:16015ms step_avg:34.37ms
step:467/2110 train_time:16049ms step_avg:34.37ms
step:468/2110 train_time:16082ms step_avg:34.36ms
step:469/2110 train_time:16115ms step_avg:34.36ms
step:470/2110 train_time:16148ms step_avg:34.36ms
step:471/2110 train_time:16181ms step_avg:34.35ms
step:472/2110 train_time:16214ms step_avg:34.35ms
step:473/2110 train_time:16247ms step_avg:34.35ms
step:474/2110 train_time:16279ms step_avg:34.34ms
step:475/2110 train_time:16313ms step_avg:34.34ms
step:476/2110 train_time:16346ms step_avg:34.34ms
step:477/2110 train_time:16379ms step_avg:34.34ms
step:478/2110 train_time:16412ms step_avg:34.33ms
step:479/2110 train_time:16445ms step_avg:34.33ms
step:480/2110 train_time:16478ms step_avg:34.33ms
step:481/2110 train_time:16512ms step_avg:34.33ms
step:482/2110 train_time:16544ms step_avg:34.32ms
step:483/2110 train_time:16577ms step_avg:34.32ms
step:484/2110 train_time:16610ms step_avg:34.32ms
step:485/2110 train_time:16643ms step_avg:34.32ms
step:486/2110 train_time:16676ms step_avg:34.31ms
step:487/2110 train_time:16709ms step_avg:34.31ms
step:488/2110 train_time:16742ms step_avg:34.31ms
step:489/2110 train_time:16776ms step_avg:34.31ms
step:490/2110 train_time:16809ms step_avg:34.30ms
step:491/2110 train_time:16842ms step_avg:34.30ms
step:492/2110 train_time:16875ms step_avg:34.30ms
step:493/2110 train_time:16909ms step_avg:34.30ms
step:494/2110 train_time:16941ms step_avg:34.29ms
step:495/2110 train_time:16975ms step_avg:34.29ms
step:496/2110 train_time:17007ms step_avg:34.29ms
step:497/2110 train_time:17041ms step_avg:34.29ms
step:498/2110 train_time:17073ms step_avg:34.28ms
step:499/2110 train_time:17107ms step_avg:34.28ms
step:500/2110 train_time:17139ms step_avg:34.28ms
step:500/2110 val_loss:4.0319 train_time:17175ms step_avg:34.35ms
step:501/2110 train_time:17201ms step_avg:34.33ms
step:502/2110 train_time:17221ms step_avg:34.30ms
step:503/2110 train_time:17251ms step_avg:34.30ms
step:504/2110 train_time:17284ms step_avg:34.29ms
step:505/2110 train_time:17319ms step_avg:34.29ms
step:506/2110 train_time:17351ms step_avg:34.29ms
step:507/2110 train_time:17385ms step_avg:34.29ms
step:508/2110 train_time:17418ms step_avg:34.29ms
step:509/2110 train_time:17451ms step_avg:34.28ms
step:510/2110 train_time:17484ms step_avg:34.28ms
step:511/2110 train_time:17517ms step_avg:34.28ms
step:512/2110 train_time:17549ms step_avg:34.28ms
step:513/2110 train_time:17582ms step_avg:34.27ms
step:514/2110 train_time:17615ms step_avg:34.27ms
step:515/2110 train_time:17648ms step_avg:34.27ms
step:516/2110 train_time:17680ms step_avg:34.26ms
step:517/2110 train_time:17713ms step_avg:34.26ms
step:518/2110 train_time:17745ms step_avg:34.26ms
step:519/2110 train_time:17778ms step_avg:34.26ms
step:520/2110 train_time:17811ms step_avg:34.25ms
step:521/2110 train_time:17844ms step_avg:34.25ms
step:522/2110 train_time:17876ms step_avg:34.25ms
step:523/2110 train_time:17909ms step_avg:34.24ms
step:524/2110 train_time:17941ms step_avg:34.24ms
step:525/2110 train_time:17974ms step_avg:34.24ms
step:526/2110 train_time:18007ms step_avg:34.23ms
step:527/2110 train_time:18040ms step_avg:34.23ms
step:528/2110 train_time:18072ms step_avg:34.23ms
step:529/2110 train_time:18106ms step_avg:34.23ms
step:530/2110 train_time:18139ms step_avg:34.22ms
step:531/2110 train_time:18173ms step_avg:34.22ms
step:532/2110 train_time:18206ms step_avg:34.22ms
step:533/2110 train_time:18240ms step_avg:34.22ms
step:534/2110 train_time:18274ms step_avg:34.22ms
step:535/2110 train_time:18307ms step_avg:34.22ms
step:536/2110 train_time:18340ms step_avg:34.22ms
step:537/2110 train_time:18374ms step_avg:34.22ms
step:538/2110 train_time:18407ms step_avg:34.21ms
step:539/2110 train_time:18440ms step_avg:34.21ms
step:540/2110 train_time:18473ms step_avg:34.21ms
step:541/2110 train_time:18506ms step_avg:34.21ms
step:542/2110 train_time:18539ms step_avg:34.21ms
step:543/2110 train_time:18573ms step_avg:34.20ms
step:544/2110 train_time:18605ms step_avg:34.20ms
step:545/2110 train_time:18639ms step_avg:34.20ms
step:546/2110 train_time:18671ms step_avg:34.20ms
step:547/2110 train_time:18705ms step_avg:34.19ms
step:548/2110 train_time:18737ms step_avg:34.19ms
step:549/2110 train_time:18770ms step_avg:34.19ms
step:550/2110 train_time:18803ms step_avg:34.19ms
step:551/2110 train_time:18836ms step_avg:34.18ms
step:552/2110 train_time:18869ms step_avg:34.18ms
step:553/2110 train_time:18901ms step_avg:34.18ms
step:554/2110 train_time:18934ms step_avg:34.18ms
step:555/2110 train_time:18967ms step_avg:34.17ms
step:556/2110 train_time:18999ms step_avg:34.17ms
step:557/2110 train_time:19033ms step_avg:34.17ms
step:558/2110 train_time:19065ms step_avg:34.17ms
step:559/2110 train_time:19098ms step_avg:34.17ms
step:560/2110 train_time:19131ms step_avg:34.16ms
step:561/2110 train_time:19164ms step_avg:34.16ms
step:562/2110 train_time:19197ms step_avg:34.16ms
step:563/2110 train_time:19231ms step_avg:34.16ms
step:564/2110 train_time:19263ms step_avg:34.15ms
step:565/2110 train_time:19297ms step_avg:34.15ms
step:566/2110 train_time:19330ms step_avg:34.15ms
step:567/2110 train_time:19364ms step_avg:34.15ms
step:568/2110 train_time:19396ms step_avg:34.15ms
step:569/2110 train_time:19430ms step_avg:34.15ms
step:570/2110 train_time:19462ms step_avg:34.14ms
step:571/2110 train_time:19496ms step_avg:34.14ms
step:572/2110 train_time:19528ms step_avg:34.14ms
step:573/2110 train_time:19562ms step_avg:34.14ms
step:574/2110 train_time:19595ms step_avg:34.14ms
step:575/2110 train_time:19628ms step_avg:34.14ms
step:576/2110 train_time:19660ms step_avg:34.13ms
step:577/2110 train_time:19694ms step_avg:34.13ms
step:578/2110 train_time:19726ms step_avg:34.13ms
step:579/2110 train_time:19760ms step_avg:34.13ms
step:580/2110 train_time:19793ms step_avg:34.13ms
step:581/2110 train_time:19826ms step_avg:34.12ms
step:582/2110 train_time:19859ms step_avg:34.12ms
step:583/2110 train_time:19892ms step_avg:34.12ms
step:584/2110 train_time:19925ms step_avg:34.12ms
step:585/2110 train_time:19958ms step_avg:34.12ms
step:586/2110 train_time:19990ms step_avg:34.11ms
step:587/2110 train_time:20024ms step_avg:34.11ms
step:588/2110 train_time:20056ms step_avg:34.11ms
step:589/2110 train_time:20090ms step_avg:34.11ms
step:590/2110 train_time:20122ms step_avg:34.11ms
step:591/2110 train_time:20156ms step_avg:34.10ms
step:592/2110 train_time:20188ms step_avg:34.10ms
step:593/2110 train_time:20222ms step_avg:34.10ms
step:594/2110 train_time:20255ms step_avg:34.10ms
step:595/2110 train_time:20288ms step_avg:34.10ms
step:596/2110 train_time:20321ms step_avg:34.10ms
step:597/2110 train_time:20354ms step_avg:34.09ms
step:598/2110 train_time:20387ms step_avg:34.09ms
step:599/2110 train_time:20421ms step_avg:34.09ms
step:600/2110 train_time:20453ms step_avg:34.09ms
step:601/2110 train_time:20487ms step_avg:34.09ms
step:602/2110 train_time:20520ms step_avg:34.09ms
step:603/2110 train_time:20553ms step_avg:34.08ms
step:604/2110 train_time:20586ms step_avg:34.08ms
step:605/2110 train_time:20619ms step_avg:34.08ms
step:606/2110 train_time:20652ms step_avg:34.08ms
step:607/2110 train_time:20685ms step_avg:34.08ms
step:608/2110 train_time:20718ms step_avg:34.08ms
step:609/2110 train_time:20752ms step_avg:34.07ms
step:610/2110 train_time:20784ms step_avg:34.07ms
step:611/2110 train_time:20818ms step_avg:34.07ms
step:612/2110 train_time:20850ms step_avg:34.07ms
step:613/2110 train_time:20884ms step_avg:34.07ms
step:614/2110 train_time:20916ms step_avg:34.07ms
step:615/2110 train_time:20950ms step_avg:34.06ms
step:616/2110 train_time:20982ms step_avg:34.06ms
step:617/2110 train_time:21016ms step_avg:34.06ms
step:618/2110 train_time:21048ms step_avg:34.06ms
step:619/2110 train_time:21081ms step_avg:34.06ms
step:620/2110 train_time:21114ms step_avg:34.05ms
step:621/2110 train_time:21147ms step_avg:34.05ms
step:622/2110 train_time:21180ms step_avg:34.05ms
step:623/2110 train_time:21214ms step_avg:34.05ms
step:624/2110 train_time:21246ms step_avg:34.05ms
step:625/2110 train_time:21280ms step_avg:34.05ms
step:626/2110 train_time:21317ms step_avg:34.05ms
step:627/2110 train_time:21357ms step_avg:34.06ms
step:628/2110 train_time:21395ms step_avg:34.07ms
step:629/2110 train_time:21434ms step_avg:34.08ms
step:630/2110 train_time:21470ms step_avg:34.08ms
step:631/2110 train_time:21508ms step_avg:34.09ms
step:632/2110 train_time:21543ms step_avg:34.09ms
step:633/2110 train_time:21582ms step_avg:34.10ms
step:634/2110 train_time:21618ms step_avg:34.10ms
step:635/2110 train_time:21656ms step_avg:34.10ms
step:636/2110 train_time:21692ms step_avg:34.11ms
step:637/2110 train_time:21729ms step_avg:34.11ms
step:638/2110 train_time:21764ms step_avg:34.11ms
step:639/2110 train_time:21798ms step_avg:34.11ms
step:640/2110 train_time:21830ms step_avg:34.11ms
step:641/2110 train_time:21863ms step_avg:34.11ms
step:642/2110 train_time:21895ms step_avg:34.10ms
step:643/2110 train_time:21928ms step_avg:34.10ms
step:644/2110 train_time:21961ms step_avg:34.10ms
step:645/2110 train_time:21994ms step_avg:34.10ms
step:646/2110 train_time:22026ms step_avg:34.10ms
step:647/2110 train_time:22059ms step_avg:34.09ms
step:648/2110 train_time:22092ms step_avg:34.09ms
step:649/2110 train_time:22125ms step_avg:34.09ms
step:650/2110 train_time:22157ms step_avg:34.09ms
step:651/2110 train_time:22190ms step_avg:34.09ms
step:652/2110 train_time:22222ms step_avg:34.08ms
step:653/2110 train_time:22255ms step_avg:34.08ms
step:654/2110 train_time:22288ms step_avg:34.08ms
step:655/2110 train_time:22322ms step_avg:34.08ms
step:656/2110 train_time:22354ms step_avg:34.08ms
step:657/2110 train_time:22388ms step_avg:34.08ms
step:658/2110 train_time:22420ms step_avg:34.07ms
step:659/2110 train_time:22454ms step_avg:34.07ms
step:660/2110 train_time:22486ms step_avg:34.07ms
step:661/2110 train_time:22520ms step_avg:34.07ms
step:662/2110 train_time:22553ms step_avg:34.07ms
step:663/2110 train_time:22587ms step_avg:34.07ms
step:664/2110 train_time:22620ms step_avg:34.07ms
step:665/2110 train_time:22654ms step_avg:34.07ms
step:666/2110 train_time:22688ms step_avg:34.07ms
step:667/2110 train_time:22721ms step_avg:34.06ms
step:668/2110 train_time:22754ms step_avg:34.06ms
step:669/2110 train_time:22787ms step_avg:34.06ms
step:670/2110 train_time:22820ms step_avg:34.06ms
step:671/2110 train_time:22853ms step_avg:34.06ms
step:672/2110 train_time:22886ms step_avg:34.06ms
step:673/2110 train_time:22920ms step_avg:34.06ms
step:674/2110 train_time:22952ms step_avg:34.05ms
step:675/2110 train_time:22986ms step_avg:34.05ms
step:676/2110 train_time:23018ms step_avg:34.05ms
step:677/2110 train_time:23051ms step_avg:34.05ms
step:678/2110 train_time:23084ms step_avg:34.05ms
step:679/2110 train_time:23117ms step_avg:34.05ms
step:680/2110 train_time:23150ms step_avg:34.04ms
step:681/2110 train_time:23183ms step_avg:34.04ms
step:682/2110 train_time:23216ms step_avg:34.04ms
step:683/2110 train_time:23249ms step_avg:34.04ms
step:684/2110 train_time:23282ms step_avg:34.04ms
step:685/2110 train_time:23315ms step_avg:34.04ms
step:686/2110 train_time:23347ms step_avg:34.03ms
step:687/2110 train_time:23381ms step_avg:34.03ms
step:688/2110 train_time:23414ms step_avg:34.03ms
step:689/2110 train_time:23448ms step_avg:34.03ms
step:690/2110 train_time:23482ms step_avg:34.03ms
step:691/2110 train_time:23515ms step_avg:34.03ms
step:692/2110 train_time:23572ms step_avg:34.06ms
step:693/2110 train_time:23632ms step_avg:34.10ms
step:694/2110 train_time:23691ms step_avg:34.14ms
step:695/2110 train_time:23751ms step_avg:34.17ms
step:696/2110 train_time:23811ms step_avg:34.21ms
step:697/2110 train_time:23869ms step_avg:34.25ms
step:698/2110 train_time:23928ms step_avg:34.28ms
step:699/2110 train_time:23987ms step_avg:34.32ms
step:700/2110 train_time:24046ms step_avg:34.35ms
step:701/2110 train_time:24105ms step_avg:34.39ms
step:702/2110 train_time:24164ms step_avg:34.42ms
step:703/2110 train_time:24224ms step_avg:34.46ms
step:704/2110 train_time:24282ms step_avg:34.49ms
step:705/2110 train_time:24341ms step_avg:34.53ms
step:706/2110 train_time:24400ms step_avg:34.56ms
step:707/2110 train_time:24460ms step_avg:34.60ms
step:708/2110 train_time:24519ms step_avg:34.63ms
step:709/2110 train_time:24578ms step_avg:34.67ms
step:710/2110 train_time:24638ms step_avg:34.70ms
step:711/2110 train_time:24697ms step_avg:34.74ms
step:712/2110 train_time:24756ms step_avg:34.77ms
step:713/2110 train_time:24816ms step_avg:34.80ms
step:714/2110 train_time:24875ms step_avg:34.84ms
step:715/2110 train_time:24934ms step_avg:34.87ms
step:716/2110 train_time:24993ms step_avg:34.91ms
step:717/2110 train_time:25054ms step_avg:34.94ms
step:718/2110 train_time:25112ms step_avg:34.97ms
step:719/2110 train_time:25171ms step_avg:35.01ms
step:720/2110 train_time:25229ms step_avg:35.04ms
step:721/2110 train_time:25289ms step_avg:35.07ms
step:722/2110 train_time:25348ms step_avg:35.11ms
step:723/2110 train_time:25407ms step_avg:35.14ms
step:724/2110 train_time:25467ms step_avg:35.18ms
step:725/2110 train_time:25526ms step_avg:35.21ms
step:726/2110 train_time:25585ms step_avg:35.24ms
step:727/2110 train_time:25646ms step_avg:35.28ms
step:728/2110 train_time:25706ms step_avg:35.31ms
step:729/2110 train_time:25766ms step_avg:35.34ms
step:730/2110 train_time:25826ms step_avg:35.38ms
step:731/2110 train_time:25886ms step_avg:35.41ms
step:732/2110 train_time:25946ms step_avg:35.45ms
step:733/2110 train_time:26005ms step_avg:35.48ms
step:734/2110 train_time:26065ms step_avg:35.51ms
step:735/2110 train_time:26124ms step_avg:35.54ms
step:736/2110 train_time:26182ms step_avg:35.57ms
step:737/2110 train_time:26242ms step_avg:35.61ms
step:738/2110 train_time:26301ms step_avg:35.64ms
step:739/2110 train_time:26360ms step_avg:35.67ms
step:740/2110 train_time:26419ms step_avg:35.70ms
step:741/2110 train_time:26478ms step_avg:35.73ms
step:742/2110 train_time:26537ms step_avg:35.76ms
step:743/2110 train_time:26596ms step_avg:35.80ms
step:744/2110 train_time:26655ms step_avg:35.83ms
step:745/2110 train_time:26715ms step_avg:35.86ms
step:746/2110 train_time:26773ms step_avg:35.89ms
step:747/2110 train_time:26833ms step_avg:35.92ms
step:748/2110 train_time:26891ms step_avg:35.95ms
step:749/2110 train_time:26951ms step_avg:35.98ms
step:750/2110 train_time:27010ms step_avg:36.01ms
step:750/2110 val_loss:3.9084 train_time:27071ms step_avg:36.09ms
step:751/2110 train_time:27106ms step_avg:36.09ms
step:752/2110 train_time:27137ms step_avg:36.09ms
step:753/2110 train_time:27196ms step_avg:36.12ms
step:754/2110 train_time:27260ms step_avg:36.15ms
step:755/2110 train_time:27320ms step_avg:36.18ms
step:756/2110 train_time:27378ms step_avg:36.21ms
step:757/2110 train_time:27437ms step_avg:36.24ms
step:758/2110 train_time:27496ms step_avg:36.27ms
step:759/2110 train_time:27556ms step_avg:36.31ms
step:760/2110 train_time:27615ms step_avg:36.33ms
step:761/2110 train_time:27673ms step_avg:36.36ms
step:762/2110 train_time:27732ms step_avg:36.39ms
step:763/2110 train_time:27791ms step_avg:36.42ms
step:764/2110 train_time:27848ms step_avg:36.45ms
step:765/2110 train_time:27907ms step_avg:36.48ms
step:766/2110 train_time:27965ms step_avg:36.51ms
step:767/2110 train_time:28026ms step_avg:36.54ms
step:768/2110 train_time:28085ms step_avg:36.57ms
step:769/2110 train_time:28147ms step_avg:36.60ms
step:770/2110 train_time:28207ms step_avg:36.63ms
step:771/2110 train_time:28268ms step_avg:36.66ms
step:772/2110 train_time:28327ms step_avg:36.69ms
step:773/2110 train_time:28388ms step_avg:36.72ms
step:774/2110 train_time:28446ms step_avg:36.75ms
step:775/2110 train_time:28506ms step_avg:36.78ms
step:776/2110 train_time:28564ms step_avg:36.81ms
step:777/2110 train_time:28624ms step_avg:36.84ms
step:778/2110 train_time:28681ms step_avg:36.87ms
step:779/2110 train_time:28741ms step_avg:36.89ms
step:780/2110 train_time:28799ms step_avg:36.92ms
step:781/2110 train_time:28857ms step_avg:36.95ms
step:782/2110 train_time:28916ms step_avg:36.98ms
step:783/2110 train_time:28975ms step_avg:37.01ms
step:784/2110 train_time:29034ms step_avg:37.03ms
step:785/2110 train_time:29096ms step_avg:37.06ms
step:786/2110 train_time:29156ms step_avg:37.09ms
step:787/2110 train_time:29217ms step_avg:37.12ms
step:788/2110 train_time:29277ms step_avg:37.15ms
step:789/2110 train_time:29338ms step_avg:37.18ms
step:790/2110 train_time:29396ms step_avg:37.21ms
step:791/2110 train_time:29457ms step_avg:37.24ms
step:792/2110 train_time:29516ms step_avg:37.27ms
step:793/2110 train_time:29576ms step_avg:37.30ms
step:794/2110 train_time:29634ms step_avg:37.32ms
step:795/2110 train_time:29694ms step_avg:37.35ms
step:796/2110 train_time:29752ms step_avg:37.38ms
step:797/2110 train_time:29812ms step_avg:37.41ms
step:798/2110 train_time:29870ms step_avg:37.43ms
step:799/2110 train_time:29929ms step_avg:37.46ms
step:800/2110 train_time:29987ms step_avg:37.48ms
step:801/2110 train_time:30047ms step_avg:37.51ms
step:802/2110 train_time:30106ms step_avg:37.54ms
step:803/2110 train_time:30166ms step_avg:37.57ms
step:804/2110 train_time:30225ms step_avg:37.59ms
step:805/2110 train_time:30286ms step_avg:37.62ms
step:806/2110 train_time:30344ms step_avg:37.65ms
step:807/2110 train_time:30405ms step_avg:37.68ms
step:808/2110 train_time:30463ms step_avg:37.70ms
step:809/2110 train_time:30523ms step_avg:37.73ms
step:810/2110 train_time:30583ms step_avg:37.76ms
step:811/2110 train_time:30643ms step_avg:37.78ms
step:812/2110 train_time:30701ms step_avg:37.81ms
step:813/2110 train_time:30760ms step_avg:37.84ms
step:814/2110 train_time:30818ms step_avg:37.86ms
step:815/2110 train_time:30878ms step_avg:37.89ms
step:816/2110 train_time:30936ms step_avg:37.91ms
step:817/2110 train_time:30996ms step_avg:37.94ms
step:818/2110 train_time:31055ms step_avg:37.96ms
step:819/2110 train_time:31115ms step_avg:37.99ms
step:820/2110 train_time:31175ms step_avg:38.02ms
step:821/2110 train_time:31235ms step_avg:38.05ms
step:822/2110 train_time:31295ms step_avg:38.07ms
step:823/2110 train_time:31355ms step_avg:38.10ms
step:824/2110 train_time:31414ms step_avg:38.12ms
step:825/2110 train_time:31475ms step_avg:38.15ms
step:826/2110 train_time:31535ms step_avg:38.18ms
step:827/2110 train_time:31595ms step_avg:38.20ms
step:828/2110 train_time:31655ms step_avg:38.23ms
step:829/2110 train_time:31714ms step_avg:38.26ms
step:830/2110 train_time:31772ms step_avg:38.28ms
step:831/2110 train_time:31832ms step_avg:38.31ms
step:832/2110 train_time:31890ms step_avg:38.33ms
step:833/2110 train_time:31950ms step_avg:38.36ms
step:834/2110 train_time:32008ms step_avg:38.38ms
step:835/2110 train_time:32067ms step_avg:38.40ms
step:836/2110 train_time:32126ms step_avg:38.43ms
step:837/2110 train_time:32186ms step_avg:38.45ms
step:838/2110 train_time:32244ms step_avg:38.48ms
step:839/2110 train_time:32304ms step_avg:38.50ms
step:840/2110 train_time:32363ms step_avg:38.53ms
step:841/2110 train_time:32423ms step_avg:38.55ms
step:842/2110 train_time:32481ms step_avg:38.58ms
step:843/2110 train_time:32542ms step_avg:38.60ms
step:844/2110 train_time:32601ms step_avg:38.63ms
step:845/2110 train_time:32661ms step_avg:38.65ms
step:846/2110 train_time:32720ms step_avg:38.68ms
step:847/2110 train_time:32779ms step_avg:38.70ms
step:848/2110 train_time:32837ms step_avg:38.72ms
step:849/2110 train_time:32897ms step_avg:38.75ms
step:850/2110 train_time:32956ms step_avg:38.77ms
step:851/2110 train_time:33016ms step_avg:38.80ms
step:852/2110 train_time:33075ms step_avg:38.82ms
step:853/2110 train_time:33135ms step_avg:38.85ms
step:854/2110 train_time:33195ms step_avg:38.87ms
step:855/2110 train_time:33255ms step_avg:38.90ms
step:856/2110 train_time:33315ms step_avg:38.92ms
step:857/2110 train_time:33376ms step_avg:38.94ms
step:858/2110 train_time:33435ms step_avg:38.97ms
step:859/2110 train_time:33496ms step_avg:38.99ms
step:860/2110 train_time:33556ms step_avg:39.02ms
step:861/2110 train_time:33616ms step_avg:39.04ms
step:862/2110 train_time:33675ms step_avg:39.07ms
step:863/2110 train_time:33735ms step_avg:39.09ms
step:864/2110 train_time:33793ms step_avg:39.11ms
step:865/2110 train_time:33854ms step_avg:39.14ms
step:866/2110 train_time:33912ms step_avg:39.16ms
step:867/2110 train_time:33972ms step_avg:39.18ms
step:868/2110 train_time:34030ms step_avg:39.20ms
step:869/2110 train_time:34091ms step_avg:39.23ms
step:870/2110 train_time:34149ms step_avg:39.25ms
step:871/2110 train_time:34209ms step_avg:39.28ms
step:872/2110 train_time:34268ms step_avg:39.30ms
step:873/2110 train_time:34329ms step_avg:39.32ms
step:874/2110 train_time:34389ms step_avg:39.35ms
step:875/2110 train_time:34449ms step_avg:39.37ms
step:876/2110 train_time:34508ms step_avg:39.39ms
step:877/2110 train_time:34568ms step_avg:39.42ms
step:878/2110 train_time:34626ms step_avg:39.44ms
step:879/2110 train_time:34687ms step_avg:39.46ms
step:880/2110 train_time:34745ms step_avg:39.48ms
step:881/2110 train_time:34805ms step_avg:39.51ms
step:882/2110 train_time:34863ms step_avg:39.53ms
step:883/2110 train_time:34923ms step_avg:39.55ms
step:884/2110 train_time:34981ms step_avg:39.57ms
step:885/2110 train_time:35041ms step_avg:39.59ms
step:886/2110 train_time:35099ms step_avg:39.62ms
step:887/2110 train_time:35159ms step_avg:39.64ms
step:888/2110 train_time:35218ms step_avg:39.66ms
step:889/2110 train_time:35278ms step_avg:39.68ms
step:890/2110 train_time:35337ms step_avg:39.70ms
step:891/2110 train_time:35397ms step_avg:39.73ms
step:892/2110 train_time:35456ms step_avg:39.75ms
step:893/2110 train_time:35517ms step_avg:39.77ms
step:894/2110 train_time:35576ms step_avg:39.79ms
step:895/2110 train_time:35636ms step_avg:39.82ms
step:896/2110 train_time:35695ms step_avg:39.84ms
step:897/2110 train_time:35755ms step_avg:39.86ms
step:898/2110 train_time:35813ms step_avg:39.88ms
step:899/2110 train_time:35873ms step_avg:39.90ms
step:900/2110 train_time:35932ms step_avg:39.92ms
step:901/2110 train_time:35992ms step_avg:39.95ms
step:902/2110 train_time:36051ms step_avg:39.97ms
step:903/2110 train_time:36110ms step_avg:39.99ms
step:904/2110 train_time:36169ms step_avg:40.01ms
step:905/2110 train_time:36229ms step_avg:40.03ms
step:906/2110 train_time:36288ms step_avg:40.05ms
step:907/2110 train_time:36348ms step_avg:40.08ms
step:908/2110 train_time:36407ms step_avg:40.10ms
step:909/2110 train_time:36468ms step_avg:40.12ms
step:910/2110 train_time:36527ms step_avg:40.14ms
step:911/2110 train_time:36587ms step_avg:40.16ms
step:912/2110 train_time:36646ms step_avg:40.18ms
step:913/2110 train_time:36705ms step_avg:40.20ms
step:914/2110 train_time:36764ms step_avg:40.22ms
step:915/2110 train_time:36823ms step_avg:40.24ms
step:916/2110 train_time:36881ms step_avg:40.26ms
step:917/2110 train_time:36941ms step_avg:40.28ms
step:918/2110 train_time:36999ms step_avg:40.30ms
step:919/2110 train_time:37059ms step_avg:40.33ms
step:920/2110 train_time:37117ms step_avg:40.34ms
step:921/2110 train_time:37177ms step_avg:40.37ms
step:922/2110 train_time:37236ms step_avg:40.39ms
step:923/2110 train_time:37296ms step_avg:40.41ms
step:924/2110 train_time:37355ms step_avg:40.43ms
step:925/2110 train_time:37416ms step_avg:40.45ms
step:926/2110 train_time:37474ms step_avg:40.47ms
step:927/2110 train_time:37534ms step_avg:40.49ms
step:928/2110 train_time:37593ms step_avg:40.51ms
step:929/2110 train_time:37654ms step_avg:40.53ms
step:930/2110 train_time:37713ms step_avg:40.55ms
step:931/2110 train_time:37774ms step_avg:40.57ms
step:932/2110 train_time:37833ms step_avg:40.59ms
step:933/2110 train_time:37893ms step_avg:40.61ms
step:934/2110 train_time:37953ms step_avg:40.64ms
step:935/2110 train_time:38012ms step_avg:40.65ms
step:936/2110 train_time:38071ms step_avg:40.67ms
step:937/2110 train_time:38130ms step_avg:40.69ms
step:938/2110 train_time:38189ms step_avg:40.71ms
step:939/2110 train_time:38250ms step_avg:40.73ms
step:940/2110 train_time:38308ms step_avg:40.75ms
step:941/2110 train_time:38368ms step_avg:40.77ms
step:942/2110 train_time:38426ms step_avg:40.79ms
step:943/2110 train_time:38486ms step_avg:40.81ms
step:944/2110 train_time:38545ms step_avg:40.83ms
step:945/2110 train_time:38606ms step_avg:40.85ms
step:946/2110 train_time:38665ms step_avg:40.87ms
step:947/2110 train_time:38724ms step_avg:40.89ms
step:948/2110 train_time:38783ms step_avg:40.91ms
step:949/2110 train_time:38842ms step_avg:40.93ms
step:950/2110 train_time:38901ms step_avg:40.95ms
step:951/2110 train_time:38961ms step_avg:40.97ms
step:952/2110 train_time:39019ms step_avg:40.99ms
step:953/2110 train_time:39078ms step_avg:41.01ms
step:954/2110 train_time:39136ms step_avg:41.02ms
step:955/2110 train_time:39197ms step_avg:41.04ms
step:956/2110 train_time:39256ms step_avg:41.06ms
step:957/2110 train_time:39316ms step_avg:41.08ms
step:958/2110 train_time:39374ms step_avg:41.10ms
step:959/2110 train_time:39434ms step_avg:41.12ms
step:960/2110 train_time:39493ms step_avg:41.14ms
step:961/2110 train_time:39554ms step_avg:41.16ms
step:962/2110 train_time:39613ms step_avg:41.18ms
step:963/2110 train_time:39673ms step_avg:41.20ms
step:964/2110 train_time:39733ms step_avg:41.22ms
step:965/2110 train_time:39794ms step_avg:41.24ms
step:966/2110 train_time:39853ms step_avg:41.26ms
step:967/2110 train_time:39914ms step_avg:41.28ms
step:968/2110 train_time:39972ms step_avg:41.29ms
step:969/2110 train_time:40032ms step_avg:41.31ms
step:970/2110 train_time:40090ms step_avg:41.33ms
step:971/2110 train_time:40151ms step_avg:41.35ms
step:972/2110 train_time:40210ms step_avg:41.37ms
step:973/2110 train_time:40270ms step_avg:41.39ms
step:974/2110 train_time:40328ms step_avg:41.40ms
step:975/2110 train_time:40388ms step_avg:41.42ms
step:976/2110 train_time:40446ms step_avg:41.44ms
step:977/2110 train_time:40506ms step_avg:41.46ms
step:978/2110 train_time:40565ms step_avg:41.48ms
step:979/2110 train_time:40625ms step_avg:41.50ms
step:980/2110 train_time:40684ms step_avg:41.51ms
step:981/2110 train_time:40745ms step_avg:41.53ms
step:982/2110 train_time:40803ms step_avg:41.55ms
step:983/2110 train_time:40863ms step_avg:41.57ms
step:984/2110 train_time:40921ms step_avg:41.59ms
step:985/2110 train_time:40982ms step_avg:41.61ms
step:986/2110 train_time:41040ms step_avg:41.62ms
step:987/2110 train_time:41100ms step_avg:41.64ms
step:988/2110 train_time:41159ms step_avg:41.66ms
step:989/2110 train_time:41218ms step_avg:41.68ms
step:990/2110 train_time:41276ms step_avg:41.69ms
step:991/2110 train_time:41336ms step_avg:41.71ms
step:992/2110 train_time:41395ms step_avg:41.73ms
step:993/2110 train_time:41456ms step_avg:41.75ms
step:994/2110 train_time:41514ms step_avg:41.76ms
step:995/2110 train_time:41575ms step_avg:41.78ms
step:996/2110 train_time:41635ms step_avg:41.80ms
step:997/2110 train_time:41695ms step_avg:41.82ms
step:998/2110 train_time:41754ms step_avg:41.84ms
step:999/2110 train_time:41815ms step_avg:41.86ms
step:1000/2110 train_time:41874ms step_avg:41.87ms
step:1000/2110 val_loss:3.7571 train_time:41936ms step_avg:41.94ms
step:1001/2110 train_time:41958ms step_avg:41.92ms
step:1002/2110 train_time:41996ms step_avg:41.91ms
step:1003/2110 train_time:42059ms step_avg:41.93ms
step:1004/2110 train_time:42122ms step_avg:41.95ms
step:1005/2110 train_time:42182ms step_avg:41.97ms
step:1006/2110 train_time:42241ms step_avg:41.99ms
step:1007/2110 train_time:42301ms step_avg:42.01ms
step:1008/2110 train_time:42359ms step_avg:42.02ms
step:1009/2110 train_time:42419ms step_avg:42.04ms
step:1010/2110 train_time:42477ms step_avg:42.06ms
step:1011/2110 train_time:42536ms step_avg:42.07ms
step:1012/2110 train_time:42593ms step_avg:42.09ms
step:1013/2110 train_time:42653ms step_avg:42.11ms
step:1014/2110 train_time:42710ms step_avg:42.12ms
step:1015/2110 train_time:42769ms step_avg:42.14ms
step:1016/2110 train_time:42827ms step_avg:42.15ms
step:1017/2110 train_time:42886ms step_avg:42.17ms
step:1018/2110 train_time:42946ms step_avg:42.19ms
step:1019/2110 train_time:43007ms step_avg:42.21ms
step:1020/2110 train_time:43067ms step_avg:42.22ms
step:1021/2110 train_time:43128ms step_avg:42.24ms
step:1022/2110 train_time:43187ms step_avg:42.26ms
step:1023/2110 train_time:43248ms step_avg:42.28ms
step:1024/2110 train_time:43308ms step_avg:42.29ms
step:1025/2110 train_time:43367ms step_avg:42.31ms
step:1026/2110 train_time:43426ms step_avg:42.33ms
step:1027/2110 train_time:43485ms step_avg:42.34ms
step:1028/2110 train_time:43543ms step_avg:42.36ms
step:1029/2110 train_time:43603ms step_avg:42.37ms
step:1030/2110 train_time:43661ms step_avg:42.39ms
step:1031/2110 train_time:43720ms step_avg:42.41ms
step:1032/2110 train_time:43778ms step_avg:42.42ms
step:1033/2110 train_time:43837ms step_avg:42.44ms
step:1034/2110 train_time:43896ms step_avg:42.45ms
step:1035/2110 train_time:43956ms step_avg:42.47ms
step:1036/2110 train_time:44015ms step_avg:42.49ms
step:1037/2110 train_time:44076ms step_avg:42.50ms
step:1038/2110 train_time:44136ms step_avg:42.52ms
step:1039/2110 train_time:44198ms step_avg:42.54ms
step:1040/2110 train_time:44257ms step_avg:42.55ms
step:1041/2110 train_time:44317ms step_avg:42.57ms
step:1042/2110 train_time:44375ms step_avg:42.59ms
step:1043/2110 train_time:44435ms step_avg:42.60ms
step:1044/2110 train_time:44493ms step_avg:42.62ms
step:1045/2110 train_time:44552ms step_avg:42.63ms
step:1046/2110 train_time:44610ms step_avg:42.65ms
step:1047/2110 train_time:44670ms step_avg:42.66ms
step:1048/2110 train_time:44727ms step_avg:42.68ms
step:1049/2110 train_time:44787ms step_avg:42.70ms
step:1050/2110 train_time:44846ms step_avg:42.71ms
step:1051/2110 train_time:44906ms step_avg:42.73ms
step:1052/2110 train_time:44965ms step_avg:42.74ms
step:1053/2110 train_time:45026ms step_avg:42.76ms
step:1054/2110 train_time:45086ms step_avg:42.78ms
step:1055/2110 train_time:45148ms step_avg:42.79ms
step:1056/2110 train_time:45207ms step_avg:42.81ms
step:1057/2110 train_time:45268ms step_avg:42.83ms
step:1058/2110 train_time:45327ms step_avg:42.84ms
step:1059/2110 train_time:45387ms step_avg:42.86ms
step:1060/2110 train_time:45446ms step_avg:42.87ms
step:1061/2110 train_time:45506ms step_avg:42.89ms
step:1062/2110 train_time:45564ms step_avg:42.90ms
step:1063/2110 train_time:45623ms step_avg:42.92ms
step:1064/2110 train_time:45681ms step_avg:42.93ms
step:1065/2110 train_time:45740ms step_avg:42.95ms
step:1066/2110 train_time:45798ms step_avg:42.96ms
step:1067/2110 train_time:45858ms step_avg:42.98ms
step:1068/2110 train_time:45916ms step_avg:42.99ms
step:1069/2110 train_time:45976ms step_avg:43.01ms
step:1070/2110 train_time:46035ms step_avg:43.02ms
step:1071/2110 train_time:46096ms step_avg:43.04ms
step:1072/2110 train_time:46154ms step_avg:43.05ms
step:1073/2110 train_time:46215ms step_avg:43.07ms
step:1074/2110 train_time:46273ms step_avg:43.08ms
step:1075/2110 train_time:46334ms step_avg:43.10ms
step:1076/2110 train_time:46392ms step_avg:43.12ms
step:1077/2110 train_time:46452ms step_avg:43.13ms
step:1078/2110 train_time:46511ms step_avg:43.15ms
step:1079/2110 train_time:46570ms step_avg:43.16ms
step:1080/2110 train_time:46628ms step_avg:43.17ms
step:1081/2110 train_time:46687ms step_avg:43.19ms
step:1082/2110 train_time:46745ms step_avg:43.20ms
step:1083/2110 train_time:46805ms step_avg:43.22ms
step:1084/2110 train_time:46863ms step_avg:43.23ms
step:1085/2110 train_time:46924ms step_avg:43.25ms
step:1086/2110 train_time:46983ms step_avg:43.26ms
step:1087/2110 train_time:47044ms step_avg:43.28ms
step:1088/2110 train_time:47104ms step_avg:43.29ms
step:1089/2110 train_time:47164ms step_avg:43.31ms
step:1090/2110 train_time:47224ms step_avg:43.32ms
step:1091/2110 train_time:47285ms step_avg:43.34ms
step:1092/2110 train_time:47344ms step_avg:43.36ms
step:1093/2110 train_time:47404ms step_avg:43.37ms
step:1094/2110 train_time:47463ms step_avg:43.38ms
step:1095/2110 train_time:47522ms step_avg:43.40ms
step:1096/2110 train_time:47580ms step_avg:43.41ms
step:1097/2110 train_time:47640ms step_avg:43.43ms
step:1098/2110 train_time:47698ms step_avg:43.44ms
step:1099/2110 train_time:47759ms step_avg:43.46ms
step:1100/2110 train_time:47816ms step_avg:43.47ms
step:1101/2110 train_time:47876ms step_avg:43.48ms
step:1102/2110 train_time:47935ms step_avg:43.50ms
step:1103/2110 train_time:47995ms step_avg:43.51ms
step:1104/2110 train_time:48053ms step_avg:43.53ms
step:1105/2110 train_time:48114ms step_avg:43.54ms
step:1106/2110 train_time:48173ms step_avg:43.56ms
step:1107/2110 train_time:48234ms step_avg:43.57ms
step:1108/2110 train_time:48292ms step_avg:43.59ms
step:1109/2110 train_time:48353ms step_avg:43.60ms
step:1110/2110 train_time:48411ms step_avg:43.61ms
step:1111/2110 train_time:48471ms step_avg:43.63ms
step:1112/2110 train_time:48529ms step_avg:43.64ms
step:1113/2110 train_time:48590ms step_avg:43.66ms
step:1114/2110 train_time:48648ms step_avg:43.67ms
step:1115/2110 train_time:48708ms step_avg:43.68ms
step:1116/2110 train_time:48766ms step_avg:43.70ms
step:1117/2110 train_time:48826ms step_avg:43.71ms
step:1118/2110 train_time:48884ms step_avg:43.72ms
step:1119/2110 train_time:48944ms step_avg:43.74ms
step:1120/2110 train_time:49003ms step_avg:43.75ms
step:1121/2110 train_time:49063ms step_avg:43.77ms
step:1122/2110 train_time:49123ms step_avg:43.78ms
step:1123/2110 train_time:49183ms step_avg:43.80ms
step:1124/2110 train_time:49242ms step_avg:43.81ms
step:1125/2110 train_time:49303ms step_avg:43.82ms
step:1126/2110 train_time:49361ms step_avg:43.84ms
step:1127/2110 train_time:49421ms step_avg:43.85ms
step:1128/2110 train_time:49479ms step_avg:43.86ms
step:1129/2110 train_time:49539ms step_avg:43.88ms
step:1130/2110 train_time:49597ms step_avg:43.89ms
step:1131/2110 train_time:49657ms step_avg:43.91ms
step:1132/2110 train_time:49716ms step_avg:43.92ms
step:1133/2110 train_time:49775ms step_avg:43.93ms
step:1134/2110 train_time:49834ms step_avg:43.94ms
step:1135/2110 train_time:49893ms step_avg:43.96ms
step:1136/2110 train_time:49952ms step_avg:43.97ms
step:1137/2110 train_time:50012ms step_avg:43.99ms
step:1138/2110 train_time:50071ms step_avg:44.00ms
step:1139/2110 train_time:50132ms step_avg:44.01ms
step:1140/2110 train_time:50191ms step_avg:44.03ms
step:1141/2110 train_time:50252ms step_avg:44.04ms
step:1142/2110 train_time:50311ms step_avg:44.05ms
step:1143/2110 train_time:50372ms step_avg:44.07ms
step:1144/2110 train_time:50431ms step_avg:44.08ms
step:1145/2110 train_time:50492ms step_avg:44.10ms
step:1146/2110 train_time:50550ms step_avg:44.11ms
step:1147/2110 train_time:50611ms step_avg:44.12ms
step:1148/2110 train_time:50669ms step_avg:44.14ms
step:1149/2110 train_time:50730ms step_avg:44.15ms
step:1150/2110 train_time:50789ms step_avg:44.16ms
step:1151/2110 train_time:50849ms step_avg:44.18ms
step:1152/2110 train_time:50908ms step_avg:44.19ms
step:1153/2110 train_time:50969ms step_avg:44.21ms
step:1154/2110 train_time:51027ms step_avg:44.22ms
step:1155/2110 train_time:51089ms step_avg:44.23ms
step:1156/2110 train_time:51148ms step_avg:44.25ms
step:1157/2110 train_time:51209ms step_avg:44.26ms
step:1158/2110 train_time:51268ms step_avg:44.27ms
step:1159/2110 train_time:51329ms step_avg:44.29ms
step:1160/2110 train_time:51388ms step_avg:44.30ms
step:1161/2110 train_time:51450ms step_avg:44.32ms
step:1162/2110 train_time:51508ms step_avg:44.33ms
step:1163/2110 train_time:51568ms step_avg:44.34ms
step:1164/2110 train_time:51627ms step_avg:44.35ms
step:1165/2110 train_time:51688ms step_avg:44.37ms
step:1166/2110 train_time:51747ms step_avg:44.38ms
step:1167/2110 train_time:51808ms step_avg:44.39ms
step:1168/2110 train_time:51867ms step_avg:44.41ms
step:1169/2110 train_time:51928ms step_avg:44.42ms
step:1170/2110 train_time:51988ms step_avg:44.43ms
step:1171/2110 train_time:52049ms step_avg:44.45ms
step:1172/2110 train_time:52109ms step_avg:44.46ms
step:1173/2110 train_time:52169ms step_avg:44.47ms
step:1174/2110 train_time:52228ms step_avg:44.49ms
step:1175/2110 train_time:52289ms step_avg:44.50ms
step:1176/2110 train_time:52348ms step_avg:44.51ms
step:1177/2110 train_time:52409ms step_avg:44.53ms
step:1178/2110 train_time:52467ms step_avg:44.54ms
step:1179/2110 train_time:52528ms step_avg:44.55ms
step:1180/2110 train_time:52587ms step_avg:44.56ms
step:1181/2110 train_time:52648ms step_avg:44.58ms
step:1182/2110 train_time:52708ms step_avg:44.59ms
step:1183/2110 train_time:52768ms step_avg:44.61ms
step:1184/2110 train_time:52827ms step_avg:44.62ms
step:1185/2110 train_time:52887ms step_avg:44.63ms
step:1186/2110 train_time:52946ms step_avg:44.64ms
step:1187/2110 train_time:53007ms step_avg:44.66ms
step:1188/2110 train_time:53066ms step_avg:44.67ms
step:1189/2110 train_time:53127ms step_avg:44.68ms
step:1190/2110 train_time:53186ms step_avg:44.69ms
step:1191/2110 train_time:53247ms step_avg:44.71ms
step:1192/2110 train_time:53307ms step_avg:44.72ms
step:1193/2110 train_time:53367ms step_avg:44.73ms
step:1194/2110 train_time:53426ms step_avg:44.75ms
step:1195/2110 train_time:53487ms step_avg:44.76ms
step:1196/2110 train_time:53547ms step_avg:44.77ms
step:1197/2110 train_time:53608ms step_avg:44.79ms
step:1198/2110 train_time:53668ms step_avg:44.80ms
step:1199/2110 train_time:53729ms step_avg:44.81ms
step:1200/2110 train_time:53788ms step_avg:44.82ms
step:1201/2110 train_time:53848ms step_avg:44.84ms
step:1202/2110 train_time:53908ms step_avg:44.85ms
step:1203/2110 train_time:53968ms step_avg:44.86ms
step:1204/2110 train_time:54027ms step_avg:44.87ms
step:1205/2110 train_time:54088ms step_avg:44.89ms
step:1206/2110 train_time:54147ms step_avg:44.90ms
step:1207/2110 train_time:54208ms step_avg:44.91ms
step:1208/2110 train_time:54267ms step_avg:44.92ms
step:1209/2110 train_time:54328ms step_avg:44.94ms
step:1210/2110 train_time:54387ms step_avg:44.95ms
step:1211/2110 train_time:54447ms step_avg:44.96ms
step:1212/2110 train_time:54507ms step_avg:44.97ms
step:1213/2110 train_time:54568ms step_avg:44.99ms
step:1214/2110 train_time:54627ms step_avg:45.00ms
step:1215/2110 train_time:54688ms step_avg:45.01ms
step:1216/2110 train_time:54747ms step_avg:45.02ms
step:1217/2110 train_time:54808ms step_avg:45.03ms
step:1218/2110 train_time:54867ms step_avg:45.05ms
step:1219/2110 train_time:54927ms step_avg:45.06ms
step:1220/2110 train_time:54986ms step_avg:45.07ms
step:1221/2110 train_time:55046ms step_avg:45.08ms
step:1222/2110 train_time:55105ms step_avg:45.09ms
step:1223/2110 train_time:55165ms step_avg:45.11ms
step:1224/2110 train_time:55224ms step_avg:45.12ms
step:1225/2110 train_time:55284ms step_avg:45.13ms
step:1226/2110 train_time:55344ms step_avg:45.14ms
step:1227/2110 train_time:55405ms step_avg:45.15ms
step:1228/2110 train_time:55464ms step_avg:45.17ms
step:1229/2110 train_time:55525ms step_avg:45.18ms
step:1230/2110 train_time:55584ms step_avg:45.19ms
step:1231/2110 train_time:55645ms step_avg:45.20ms
step:1232/2110 train_time:55704ms step_avg:45.21ms
step:1233/2110 train_time:55764ms step_avg:45.23ms
step:1234/2110 train_time:55823ms step_avg:45.24ms
step:1235/2110 train_time:55885ms step_avg:45.25ms
step:1236/2110 train_time:55944ms step_avg:45.26ms
step:1237/2110 train_time:56005ms step_avg:45.28ms
step:1238/2110 train_time:56064ms step_avg:45.29ms
step:1239/2110 train_time:56124ms step_avg:45.30ms
step:1240/2110 train_time:56184ms step_avg:45.31ms
step:1241/2110 train_time:56244ms step_avg:45.32ms
step:1242/2110 train_time:56304ms step_avg:45.33ms
step:1243/2110 train_time:56364ms step_avg:45.35ms
step:1244/2110 train_time:56422ms step_avg:45.36ms
step:1245/2110 train_time:56483ms step_avg:45.37ms
step:1246/2110 train_time:56542ms step_avg:45.38ms
step:1247/2110 train_time:56603ms step_avg:45.39ms
step:1248/2110 train_time:56661ms step_avg:45.40ms
step:1249/2110 train_time:56722ms step_avg:45.41ms
step:1250/2110 train_time:56780ms step_avg:45.42ms
step:1250/2110 val_loss:3.5929 train_time:56843ms step_avg:45.47ms
step:1251/2110 train_time:56865ms step_avg:45.46ms
step:1252/2110 train_time:56902ms step_avg:45.45ms
step:1253/2110 train_time:56968ms step_avg:45.47ms
step:1254/2110 train_time:57031ms step_avg:45.48ms
step:1255/2110 train_time:57092ms step_avg:45.49ms
step:1256/2110 train_time:57151ms step_avg:45.50ms
step:1257/2110 train_time:57210ms step_avg:45.51ms
step:1258/2110 train_time:57269ms step_avg:45.52ms
step:1259/2110 train_time:57329ms step_avg:45.54ms
step:1260/2110 train_time:57387ms step_avg:45.55ms
step:1261/2110 train_time:57447ms step_avg:45.56ms
step:1262/2110 train_time:57506ms step_avg:45.57ms
step:1263/2110 train_time:57565ms step_avg:45.58ms
step:1264/2110 train_time:57624ms step_avg:45.59ms
step:1265/2110 train_time:57684ms step_avg:45.60ms
step:1266/2110 train_time:57742ms step_avg:45.61ms
step:1267/2110 train_time:57805ms step_avg:45.62ms
step:1268/2110 train_time:57864ms step_avg:45.63ms
step:1269/2110 train_time:57927ms step_avg:45.65ms
step:1270/2110 train_time:57988ms step_avg:45.66ms
step:1271/2110 train_time:58050ms step_avg:45.67ms
step:1272/2110 train_time:58109ms step_avg:45.68ms
step:1273/2110 train_time:58170ms step_avg:45.70ms
step:1274/2110 train_time:58229ms step_avg:45.71ms
step:1275/2110 train_time:58289ms step_avg:45.72ms
step:1276/2110 train_time:58348ms step_avg:45.73ms
step:1277/2110 train_time:58408ms step_avg:45.74ms
step:1278/2110 train_time:58466ms step_avg:45.75ms
step:1279/2110 train_time:58526ms step_avg:45.76ms
step:1280/2110 train_time:58584ms step_avg:45.77ms
step:1281/2110 train_time:58644ms step_avg:45.78ms
step:1282/2110 train_time:58703ms step_avg:45.79ms
step:1283/2110 train_time:58764ms step_avg:45.80ms
step:1284/2110 train_time:58823ms step_avg:45.81ms
step:1285/2110 train_time:58885ms step_avg:45.83ms
step:1286/2110 train_time:58946ms step_avg:45.84ms
step:1287/2110 train_time:59009ms step_avg:45.85ms
step:1288/2110 train_time:59069ms step_avg:45.86ms
step:1289/2110 train_time:59130ms step_avg:45.87ms
step:1290/2110 train_time:59189ms step_avg:45.88ms
step:1291/2110 train_time:59250ms step_avg:45.89ms
step:1292/2110 train_time:59309ms step_avg:45.91ms
step:1293/2110 train_time:59369ms step_avg:45.92ms
step:1294/2110 train_time:59428ms step_avg:45.93ms
step:1295/2110 train_time:59488ms step_avg:45.94ms
step:1296/2110 train_time:59547ms step_avg:45.95ms
step:1297/2110 train_time:59606ms step_avg:45.96ms
step:1298/2110 train_time:59665ms step_avg:45.97ms
step:1299/2110 train_time:59726ms step_avg:45.98ms
step:1300/2110 train_time:59786ms step_avg:45.99ms
step:1301/2110 train_time:59847ms step_avg:46.00ms
step:1302/2110 train_time:59907ms step_avg:46.01ms
step:1303/2110 train_time:59969ms step_avg:46.02ms
step:1304/2110 train_time:60029ms step_avg:46.03ms
step:1305/2110 train_time:60090ms step_avg:46.05ms
step:1306/2110 train_time:60150ms step_avg:46.06ms
step:1307/2110 train_time:60210ms step_avg:46.07ms
step:1308/2110 train_time:60269ms step_avg:46.08ms
step:1309/2110 train_time:60330ms step_avg:46.09ms
step:1310/2110 train_time:60389ms step_avg:46.10ms
step:1311/2110 train_time:60450ms step_avg:46.11ms
step:1312/2110 train_time:60509ms step_avg:46.12ms
step:1313/2110 train_time:60569ms step_avg:46.13ms
step:1314/2110 train_time:60628ms step_avg:46.14ms
step:1315/2110 train_time:60688ms step_avg:46.15ms
step:1316/2110 train_time:60747ms step_avg:46.16ms
step:1317/2110 train_time:60808ms step_avg:46.17ms
step:1318/2110 train_time:60868ms step_avg:46.18ms
step:1319/2110 train_time:60929ms step_avg:46.19ms
step:1320/2110 train_time:60990ms step_avg:46.20ms
step:1321/2110 train_time:61051ms step_avg:46.22ms
step:1322/2110 train_time:61110ms step_avg:46.23ms
step:1323/2110 train_time:61171ms step_avg:46.24ms
step:1324/2110 train_time:61229ms step_avg:46.25ms
step:1325/2110 train_time:61290ms step_avg:46.26ms
step:1326/2110 train_time:61351ms step_avg:46.27ms
step:1327/2110 train_time:61410ms step_avg:46.28ms
step:1328/2110 train_time:61470ms step_avg:46.29ms
step:1329/2110 train_time:61528ms step_avg:46.30ms
step:1330/2110 train_time:61588ms step_avg:46.31ms
step:1331/2110 train_time:61648ms step_avg:46.32ms
step:1332/2110 train_time:61709ms step_avg:46.33ms
step:1333/2110 train_time:61769ms step_avg:46.34ms
step:1334/2110 train_time:61829ms step_avg:46.35ms
step:1335/2110 train_time:61890ms step_avg:46.36ms
step:1336/2110 train_time:61951ms step_avg:46.37ms
step:1337/2110 train_time:62011ms step_avg:46.38ms
step:1338/2110 train_time:62071ms step_avg:46.39ms
step:1339/2110 train_time:62132ms step_avg:46.40ms
step:1340/2110 train_time:62192ms step_avg:46.41ms
step:1341/2110 train_time:62252ms step_avg:46.42ms
step:1342/2110 train_time:62311ms step_avg:46.43ms
step:1343/2110 train_time:62370ms step_avg:46.44ms
step:1344/2110 train_time:62430ms step_avg:46.45ms
step:1345/2110 train_time:62489ms step_avg:46.46ms
step:1346/2110 train_time:62549ms step_avg:46.47ms
step:1347/2110 train_time:62609ms step_avg:46.48ms
step:1348/2110 train_time:62668ms step_avg:46.49ms
step:1349/2110 train_time:62728ms step_avg:46.50ms
step:1350/2110 train_time:62790ms step_avg:46.51ms
step:1351/2110 train_time:62850ms step_avg:46.52ms
step:1352/2110 train_time:62910ms step_avg:46.53ms
step:1353/2110 train_time:62969ms step_avg:46.54ms
step:1354/2110 train_time:63029ms step_avg:46.55ms
step:1355/2110 train_time:63090ms step_avg:46.56ms
step:1356/2110 train_time:63150ms step_avg:46.57ms
step:1357/2110 train_time:63211ms step_avg:46.58ms
step:1358/2110 train_time:63270ms step_avg:46.59ms
step:1359/2110 train_time:63330ms step_avg:46.60ms
step:1360/2110 train_time:63389ms step_avg:46.61ms
step:1361/2110 train_time:63450ms step_avg:46.62ms
step:1362/2110 train_time:63509ms step_avg:46.63ms
step:1363/2110 train_time:63569ms step_avg:46.64ms
step:1364/2110 train_time:63629ms step_avg:46.65ms
step:1365/2110 train_time:63689ms step_avg:46.66ms
step:1366/2110 train_time:63750ms step_avg:46.67ms
step:1367/2110 train_time:63810ms step_avg:46.68ms
step:1368/2110 train_time:63870ms step_avg:46.69ms
step:1369/2110 train_time:63929ms step_avg:46.70ms
step:1370/2110 train_time:63989ms step_avg:46.71ms
step:1371/2110 train_time:64050ms step_avg:46.72ms
step:1372/2110 train_time:64111ms step_avg:46.73ms
step:1373/2110 train_time:64171ms step_avg:46.74ms
step:1374/2110 train_time:64231ms step_avg:46.75ms
step:1375/2110 train_time:64291ms step_avg:46.76ms
step:1376/2110 train_time:64351ms step_avg:46.77ms
step:1377/2110 train_time:64411ms step_avg:46.78ms
step:1378/2110 train_time:64470ms step_avg:46.78ms
step:1379/2110 train_time:64529ms step_avg:46.79ms
step:1380/2110 train_time:64589ms step_avg:46.80ms
step:1381/2110 train_time:64650ms step_avg:46.81ms
step:1382/2110 train_time:64737ms step_avg:46.84ms
step:1383/2110 train_time:64824ms step_avg:46.87ms
step:1384/2110 train_time:64911ms step_avg:46.90ms
step:1385/2110 train_time:64999ms step_avg:46.93ms
step:1386/2110 train_time:65086ms step_avg:46.96ms
step:1387/2110 train_time:65173ms step_avg:46.99ms
step:1388/2110 train_time:65260ms step_avg:47.02ms
step:1389/2110 train_time:65346ms step_avg:47.05ms
step:1390/2110 train_time:65432ms step_avg:47.07ms
step:1391/2110 train_time:65520ms step_avg:47.10ms
step:1392/2110 train_time:65606ms step_avg:47.13ms
step:1393/2110 train_time:65694ms step_avg:47.16ms
step:1394/2110 train_time:65781ms step_avg:47.19ms
step:1395/2110 train_time:65868ms step_avg:47.22ms
step:1396/2110 train_time:65956ms step_avg:47.25ms
step:1397/2110 train_time:66043ms step_avg:47.27ms
step:1398/2110 train_time:66130ms step_avg:47.30ms
step:1399/2110 train_time:66216ms step_avg:47.33ms
step:1400/2110 train_time:66303ms step_avg:47.36ms
step:1401/2110 train_time:66389ms step_avg:47.39ms
step:1402/2110 train_time:66476ms step_avg:47.42ms
step:1403/2110 train_time:66563ms step_avg:47.44ms
step:1404/2110 train_time:66651ms step_avg:47.47ms
step:1405/2110 train_time:66737ms step_avg:47.50ms
step:1406/2110 train_time:66824ms step_avg:47.53ms
step:1407/2110 train_time:66911ms step_avg:47.56ms
step:1408/2110 train_time:66999ms step_avg:47.58ms
step:1409/2110 train_time:67085ms step_avg:47.61ms
step:1410/2110 train_time:67172ms step_avg:47.64ms
step:1411/2110 train_time:67258ms step_avg:47.67ms
step:1412/2110 train_time:67345ms step_avg:47.69ms
step:1413/2110 train_time:67433ms step_avg:47.72ms
step:1414/2110 train_time:67518ms step_avg:47.75ms
step:1415/2110 train_time:67604ms step_avg:47.78ms
step:1416/2110 train_time:67691ms step_avg:47.80ms
step:1417/2110 train_time:67778ms step_avg:47.83ms
step:1418/2110 train_time:67865ms step_avg:47.86ms
step:1419/2110 train_time:67952ms step_avg:47.89ms
step:1420/2110 train_time:68040ms step_avg:47.92ms
step:1421/2110 train_time:68126ms step_avg:47.94ms
step:1422/2110 train_time:68213ms step_avg:47.97ms
step:1423/2110 train_time:68299ms step_avg:48.00ms
step:1424/2110 train_time:68386ms step_avg:48.02ms
step:1425/2110 train_time:68473ms step_avg:48.05ms
step:1426/2110 train_time:68559ms step_avg:48.08ms
step:1427/2110 train_time:68646ms step_avg:48.11ms
step:1428/2110 train_time:68734ms step_avg:48.13ms
step:1429/2110 train_time:68821ms step_avg:48.16ms
step:1430/2110 train_time:68908ms step_avg:48.19ms
step:1431/2110 train_time:68997ms step_avg:48.22ms
step:1432/2110 train_time:69084ms step_avg:48.24ms
step:1433/2110 train_time:69170ms step_avg:48.27ms
step:1434/2110 train_time:69257ms step_avg:48.30ms
step:1435/2110 train_time:69344ms step_avg:48.32ms
step:1436/2110 train_time:69430ms step_avg:48.35ms
step:1437/2110 train_time:69517ms step_avg:48.38ms
step:1438/2110 train_time:69604ms step_avg:48.40ms
step:1439/2110 train_time:69690ms step_avg:48.43ms
step:1440/2110 train_time:69778ms step_avg:48.46ms
step:1441/2110 train_time:69864ms step_avg:48.48ms
step:1442/2110 train_time:69952ms step_avg:48.51ms
step:1443/2110 train_time:70040ms step_avg:48.54ms
step:1444/2110 train_time:70126ms step_avg:48.56ms
step:1445/2110 train_time:70213ms step_avg:48.59ms
step:1446/2110 train_time:70301ms step_avg:48.62ms
step:1447/2110 train_time:70387ms step_avg:48.64ms
step:1448/2110 train_time:70475ms step_avg:48.67ms
step:1449/2110 train_time:70562ms step_avg:48.70ms
step:1450/2110 train_time:70648ms step_avg:48.72ms
step:1451/2110 train_time:70734ms step_avg:48.75ms
step:1452/2110 train_time:70822ms step_avg:48.78ms
step:1453/2110 train_time:70909ms step_avg:48.80ms
step:1454/2110 train_time:70997ms step_avg:48.83ms
step:1455/2110 train_time:71083ms step_avg:48.85ms
step:1456/2110 train_time:71170ms step_avg:48.88ms
step:1457/2110 train_time:71257ms step_avg:48.91ms
step:1458/2110 train_time:71344ms step_avg:48.93ms
step:1459/2110 train_time:71430ms step_avg:48.96ms
step:1460/2110 train_time:71517ms step_avg:48.98ms
step:1461/2110 train_time:71603ms step_avg:49.01ms
step:1462/2110 train_time:71689ms step_avg:49.03ms
step:1463/2110 train_time:71777ms step_avg:49.06ms
step:1464/2110 train_time:71864ms step_avg:49.09ms
step:1465/2110 train_time:71951ms step_avg:49.11ms
step:1466/2110 train_time:72039ms step_avg:49.14ms
step:1467/2110 train_time:72125ms step_avg:49.16ms
step:1468/2110 train_time:72212ms step_avg:49.19ms
step:1469/2110 train_time:72299ms step_avg:49.22ms
step:1470/2110 train_time:72386ms step_avg:49.24ms
step:1471/2110 train_time:72472ms step_avg:49.27ms
step:1472/2110 train_time:72559ms step_avg:49.29ms
step:1473/2110 train_time:72645ms step_avg:49.32ms
step:1474/2110 train_time:72733ms step_avg:49.34ms
step:1475/2110 train_time:72820ms step_avg:49.37ms
step:1476/2110 train_time:72907ms step_avg:49.39ms
step:1477/2110 train_time:72996ms step_avg:49.42ms
step:1478/2110 train_time:73081ms step_avg:49.45ms
step:1479/2110 train_time:73167ms step_avg:49.47ms
step:1480/2110 train_time:73254ms step_avg:49.50ms
step:1481/2110 train_time:73341ms step_avg:49.52ms
step:1482/2110 train_time:73428ms step_avg:49.55ms
step:1483/2110 train_time:73515ms step_avg:49.57ms
step:1484/2110 train_time:73602ms step_avg:49.60ms
step:1485/2110 train_time:73688ms step_avg:49.62ms
step:1486/2110 train_time:73774ms step_avg:49.65ms
step:1487/2110 train_time:73863ms step_avg:49.67ms
step:1488/2110 train_time:73950ms step_avg:49.70ms
step:1489/2110 train_time:74037ms step_avg:49.72ms
step:1490/2110 train_time:74124ms step_avg:49.75ms
step:1491/2110 train_time:74211ms step_avg:49.77ms
step:1492/2110 train_time:74298ms step_avg:49.80ms
step:1493/2110 train_time:74385ms step_avg:49.82ms
step:1494/2110 train_time:74471ms step_avg:49.85ms
step:1495/2110 train_time:74558ms step_avg:49.87ms
step:1496/2110 train_time:74644ms step_avg:49.90ms
step:1497/2110 train_time:74731ms step_avg:49.92ms
step:1498/2110 train_time:74818ms step_avg:49.95ms
step:1499/2110 train_time:74905ms step_avg:49.97ms
step:1500/2110 train_time:74993ms step_avg:50.00ms
step:1500/2110 val_loss:3.4943 train_time:75080ms step_avg:50.05ms
step:1501/2110 train_time:75112ms step_avg:50.04ms
step:1502/2110 train_time:75169ms step_avg:50.05ms
step:1503/2110 train_time:75264ms step_avg:50.08ms
step:1504/2110 train_time:75351ms step_avg:50.10ms
step:1505/2110 train_time:75439ms step_avg:50.13ms
step:1506/2110 train_time:75524ms step_avg:50.15ms
step:1507/2110 train_time:75610ms step_avg:50.17ms
step:1508/2110 train_time:75696ms step_avg:50.20ms
step:1509/2110 train_time:75781ms step_avg:50.22ms
step:1510/2110 train_time:75867ms step_avg:50.24ms
step:1511/2110 train_time:75952ms step_avg:50.27ms
step:1512/2110 train_time:76039ms step_avg:50.29ms
step:1513/2110 train_time:76129ms step_avg:50.32ms
step:1514/2110 train_time:76219ms step_avg:50.34ms
step:1515/2110 train_time:76307ms step_avg:50.37ms
step:1516/2110 train_time:76395ms step_avg:50.39ms
step:1517/2110 train_time:76481ms step_avg:50.42ms
step:1518/2110 train_time:76568ms step_avg:50.44ms
step:1519/2110 train_time:76654ms step_avg:50.46ms
step:1520/2110 train_time:76740ms step_avg:50.49ms
step:1521/2110 train_time:76825ms step_avg:50.51ms
step:1522/2110 train_time:76911ms step_avg:50.53ms
step:1523/2110 train_time:76997ms step_avg:50.56ms
step:1524/2110 train_time:77086ms step_avg:50.58ms
step:1525/2110 train_time:77175ms step_avg:50.61ms
step:1526/2110 train_time:77262ms step_avg:50.63ms
step:1527/2110 train_time:77351ms step_avg:50.66ms
step:1528/2110 train_time:77439ms step_avg:50.68ms
step:1529/2110 train_time:77525ms step_avg:50.70ms
step:1530/2110 train_time:77612ms step_avg:50.73ms
step:1531/2110 train_time:77698ms step_avg:50.75ms
step:1532/2110 train_time:77784ms step_avg:50.77ms
step:1533/2110 train_time:77870ms step_avg:50.80ms
step:1534/2110 train_time:77956ms step_avg:50.82ms
step:1535/2110 train_time:78043ms step_avg:50.84ms
step:1536/2110 train_time:78131ms step_avg:50.87ms
step:1537/2110 train_time:78219ms step_avg:50.89ms
step:1538/2110 train_time:78308ms step_avg:50.92ms
step:1539/2110 train_time:78396ms step_avg:50.94ms
step:1540/2110 train_time:78482ms step_avg:50.96ms
step:1541/2110 train_time:78568ms step_avg:50.99ms
step:1542/2110 train_time:78656ms step_avg:51.01ms
step:1543/2110 train_time:78743ms step_avg:51.03ms
step:1544/2110 train_time:78830ms step_avg:51.06ms
step:1545/2110 train_time:78915ms step_avg:51.08ms
step:1546/2110 train_time:79001ms step_avg:51.10ms
step:1547/2110 train_time:79089ms step_avg:51.12ms
step:1548/2110 train_time:79178ms step_avg:51.15ms
step:1549/2110 train_time:79265ms step_avg:51.17ms
step:1550/2110 train_time:79353ms step_avg:51.20ms
step:1551/2110 train_time:79440ms step_avg:51.22ms
step:1552/2110 train_time:79527ms step_avg:51.24ms
step:1553/2110 train_time:79613ms step_avg:51.26ms
step:1554/2110 train_time:79700ms step_avg:51.29ms
step:1555/2110 train_time:79786ms step_avg:51.31ms
step:1556/2110 train_time:79874ms step_avg:51.33ms
step:1557/2110 train_time:79959ms step_avg:51.35ms
step:1558/2110 train_time:80047ms step_avg:51.38ms
step:1559/2110 train_time:80134ms step_avg:51.40ms
step:1560/2110 train_time:80221ms step_avg:51.42ms
step:1561/2110 train_time:80308ms step_avg:51.45ms
step:1562/2110 train_time:80395ms step_avg:51.47ms
step:1563/2110 train_time:80483ms step_avg:51.49ms
step:1564/2110 train_time:80570ms step_avg:51.52ms
step:1565/2110 train_time:80657ms step_avg:51.54ms
step:1566/2110 train_time:80744ms step_avg:51.56ms
step:1567/2110 train_time:80830ms step_avg:51.58ms
step:1568/2110 train_time:80917ms step_avg:51.61ms
step:1569/2110 train_time:81004ms step_avg:51.63ms
step:1570/2110 train_time:81091ms step_avg:51.65ms
step:1571/2110 train_time:81178ms step_avg:51.67ms
step:1572/2110 train_time:81265ms step_avg:51.70ms
step:1573/2110 train_time:81352ms step_avg:51.72ms
step:1574/2110 train_time:81441ms step_avg:51.74ms
step:1575/2110 train_time:81527ms step_avg:51.76ms
step:1576/2110 train_time:81614ms step_avg:51.79ms
step:1577/2110 train_time:81700ms step_avg:51.81ms
step:1578/2110 train_time:81788ms step_avg:51.83ms
step:1579/2110 train_time:81875ms step_avg:51.85ms
step:1580/2110 train_time:81960ms step_avg:51.87ms
step:1581/2110 train_time:82048ms step_avg:51.90ms
step:1582/2110 train_time:82136ms step_avg:51.92ms
step:1583/2110 train_time:82222ms step_avg:51.94ms
step:1584/2110 train_time:82310ms step_avg:51.96ms
step:1585/2110 train_time:82397ms step_avg:51.99ms
step:1586/2110 train_time:82484ms step_avg:52.01ms
step:1587/2110 train_time:82572ms step_avg:52.03ms
step:1588/2110 train_time:82659ms step_avg:52.05ms
step:1589/2110 train_time:82746ms step_avg:52.07ms
step:1590/2110 train_time:82834ms step_avg:52.10ms
step:1591/2110 train_time:82920ms step_avg:52.12ms
step:1592/2110 train_time:83007ms step_avg:52.14ms
step:1593/2110 train_time:83093ms step_avg:52.16ms
step:1594/2110 train_time:83182ms step_avg:52.18ms
step:1595/2110 train_time:83267ms step_avg:52.20ms
step:1596/2110 train_time:83355ms step_avg:52.23ms
step:1597/2110 train_time:83441ms step_avg:52.25ms
step:1598/2110 train_time:83529ms step_avg:52.27ms
step:1599/2110 train_time:83616ms step_avg:52.29ms
step:1600/2110 train_time:83703ms step_avg:52.31ms
step:1601/2110 train_time:83791ms step_avg:52.34ms
step:1602/2110 train_time:83877ms step_avg:52.36ms
step:1603/2110 train_time:83964ms step_avg:52.38ms
step:1604/2110 train_time:84051ms step_avg:52.40ms
step:1605/2110 train_time:84138ms step_avg:52.42ms
step:1606/2110 train_time:84225ms step_avg:52.44ms
step:1607/2110 train_time:84311ms step_avg:52.47ms
step:1608/2110 train_time:84398ms step_avg:52.49ms
step:1609/2110 train_time:84486ms step_avg:52.51ms
step:1610/2110 train_time:84574ms step_avg:52.53ms
step:1611/2110 train_time:84660ms step_avg:52.55ms
step:1612/2110 train_time:84748ms step_avg:52.57ms
step:1613/2110 train_time:84834ms step_avg:52.59ms
step:1614/2110 train_time:84921ms step_avg:52.62ms
step:1615/2110 train_time:85008ms step_avg:52.64ms
step:1616/2110 train_time:85095ms step_avg:52.66ms
step:1617/2110 train_time:85182ms step_avg:52.68ms
step:1618/2110 train_time:85269ms step_avg:52.70ms
step:1619/2110 train_time:85356ms step_avg:52.72ms
step:1620/2110 train_time:85443ms step_avg:52.74ms
step:1621/2110 train_time:85530ms step_avg:52.76ms
step:1622/2110 train_time:85618ms step_avg:52.79ms
step:1623/2110 train_time:85705ms step_avg:52.81ms
step:1624/2110 train_time:85791ms step_avg:52.83ms
step:1625/2110 train_time:85878ms step_avg:52.85ms
step:1626/2110 train_time:85965ms step_avg:52.87ms
step:1627/2110 train_time:86051ms step_avg:52.89ms
step:1628/2110 train_time:86138ms step_avg:52.91ms
step:1629/2110 train_time:86225ms step_avg:52.93ms
step:1630/2110 train_time:86314ms step_avg:52.95ms
step:1631/2110 train_time:86400ms step_avg:52.97ms
step:1632/2110 train_time:86487ms step_avg:52.99ms
step:1633/2110 train_time:86575ms step_avg:53.02ms
step:1634/2110 train_time:86662ms step_avg:53.04ms
step:1635/2110 train_time:86749ms step_avg:53.06ms
step:1636/2110 train_time:86836ms step_avg:53.08ms
step:1637/2110 train_time:86922ms step_avg:53.10ms
step:1638/2110 train_time:87010ms step_avg:53.12ms
step:1639/2110 train_time:87096ms step_avg:53.14ms
step:1640/2110 train_time:87183ms step_avg:53.16ms
step:1641/2110 train_time:87271ms step_avg:53.18ms
step:1642/2110 train_time:87359ms step_avg:53.20ms
step:1643/2110 train_time:87445ms step_avg:53.22ms
step:1644/2110 train_time:87533ms step_avg:53.24ms
step:1645/2110 train_time:87619ms step_avg:53.26ms
step:1646/2110 train_time:87707ms step_avg:53.28ms
step:1647/2110 train_time:87794ms step_avg:53.31ms
step:1648/2110 train_time:87880ms step_avg:53.33ms
step:1649/2110 train_time:87967ms step_avg:53.35ms
step:1650/2110 train_time:88055ms step_avg:53.37ms
step:1651/2110 train_time:88141ms step_avg:53.39ms
step:1652/2110 train_time:88229ms step_avg:53.41ms
step:1653/2110 train_time:88317ms step_avg:53.43ms
step:1654/2110 train_time:88405ms step_avg:53.45ms
step:1655/2110 train_time:88492ms step_avg:53.47ms
step:1656/2110 train_time:88579ms step_avg:53.49ms
step:1657/2110 train_time:88666ms step_avg:53.51ms
step:1658/2110 train_time:88755ms step_avg:53.53ms
step:1659/2110 train_time:88841ms step_avg:53.55ms
step:1660/2110 train_time:88929ms step_avg:53.57ms
step:1661/2110 train_time:89018ms step_avg:53.59ms
step:1662/2110 train_time:89107ms step_avg:53.61ms
step:1663/2110 train_time:89195ms step_avg:53.64ms
step:1664/2110 train_time:89284ms step_avg:53.66ms
step:1665/2110 train_time:89371ms step_avg:53.68ms
step:1666/2110 train_time:89459ms step_avg:53.70ms
step:1667/2110 train_time:89548ms step_avg:53.72ms
step:1668/2110 train_time:89636ms step_avg:53.74ms
step:1669/2110 train_time:89723ms step_avg:53.76ms
step:1670/2110 train_time:89811ms step_avg:53.78ms
step:1671/2110 train_time:89899ms step_avg:53.80ms
step:1672/2110 train_time:89988ms step_avg:53.82ms
step:1673/2110 train_time:90077ms step_avg:53.84ms
step:1674/2110 train_time:90165ms step_avg:53.86ms
step:1675/2110 train_time:90255ms step_avg:53.88ms
step:1676/2110 train_time:90342ms step_avg:53.90ms
step:1677/2110 train_time:90430ms step_avg:53.92ms
step:1678/2110 train_time:90519ms step_avg:53.94ms
step:1679/2110 train_time:90607ms step_avg:53.96ms
step:1680/2110 train_time:90695ms step_avg:53.98ms
step:1681/2110 train_time:90782ms step_avg:54.00ms
step:1682/2110 train_time:90871ms step_avg:54.03ms
step:1683/2110 train_time:90959ms step_avg:54.05ms
step:1684/2110 train_time:91047ms step_avg:54.07ms
step:1685/2110 train_time:91136ms step_avg:54.09ms
step:1686/2110 train_time:91224ms step_avg:54.11ms
step:1687/2110 train_time:91311ms step_avg:54.13ms
step:1688/2110 train_time:91399ms step_avg:54.15ms
step:1689/2110 train_time:91488ms step_avg:54.17ms
step:1690/2110 train_time:91577ms step_avg:54.19ms
step:1691/2110 train_time:91664ms step_avg:54.21ms
step:1692/2110 train_time:91753ms step_avg:54.23ms
step:1693/2110 train_time:91840ms step_avg:54.25ms
step:1694/2110 train_time:91928ms step_avg:54.27ms
step:1695/2110 train_time:92017ms step_avg:54.29ms
step:1696/2110 train_time:92104ms step_avg:54.31ms
step:1697/2110 train_time:92193ms step_avg:54.33ms
step:1698/2110 train_time:92281ms step_avg:54.35ms
step:1699/2110 train_time:92369ms step_avg:54.37ms
step:1700/2110 train_time:92458ms step_avg:54.39ms
step:1701/2110 train_time:92546ms step_avg:54.41ms
step:1702/2110 train_time:92633ms step_avg:54.43ms
step:1703/2110 train_time:92721ms step_avg:54.45ms
step:1704/2110 train_time:92810ms step_avg:54.47ms
step:1705/2110 train_time:92897ms step_avg:54.49ms
step:1706/2110 train_time:92986ms step_avg:54.51ms
step:1707/2110 train_time:93075ms step_avg:54.53ms
step:1708/2110 train_time:93162ms step_avg:54.54ms
step:1709/2110 train_time:93251ms step_avg:54.56ms
step:1710/2110 train_time:93340ms step_avg:54.58ms
step:1711/2110 train_time:93427ms step_avg:54.60ms
step:1712/2110 train_time:93516ms step_avg:54.62ms
step:1713/2110 train_time:93603ms step_avg:54.64ms
step:1714/2110 train_time:93691ms step_avg:54.66ms
step:1715/2110 train_time:93780ms step_avg:54.68ms
step:1716/2110 train_time:93868ms step_avg:54.70ms
step:1717/2110 train_time:93957ms step_avg:54.72ms
step:1718/2110 train_time:94044ms step_avg:54.74ms
step:1719/2110 train_time:94133ms step_avg:54.76ms
step:1720/2110 train_time:94221ms step_avg:54.78ms
step:1721/2110 train_time:94310ms step_avg:54.80ms
step:1722/2110 train_time:94398ms step_avg:54.82ms
step:1723/2110 train_time:94486ms step_avg:54.84ms
step:1724/2110 train_time:94574ms step_avg:54.86ms
step:1725/2110 train_time:94664ms step_avg:54.88ms
step:1726/2110 train_time:94752ms step_avg:54.90ms
step:1727/2110 train_time:94839ms step_avg:54.92ms
step:1728/2110 train_time:94927ms step_avg:54.93ms
step:1729/2110 train_time:95016ms step_avg:54.95ms
step:1730/2110 train_time:95105ms step_avg:54.97ms
step:1731/2110 train_time:95193ms step_avg:54.99ms
step:1732/2110 train_time:95281ms step_avg:55.01ms
step:1733/2110 train_time:95369ms step_avg:55.03ms
step:1734/2110 train_time:95458ms step_avg:55.05ms
step:1735/2110 train_time:95545ms step_avg:55.07ms
step:1736/2110 train_time:95633ms step_avg:55.09ms
step:1737/2110 train_time:95722ms step_avg:55.11ms
step:1738/2110 train_time:95811ms step_avg:55.13ms
step:1739/2110 train_time:95899ms step_avg:55.15ms
step:1740/2110 train_time:95988ms step_avg:55.17ms
step:1741/2110 train_time:96077ms step_avg:55.18ms
step:1742/2110 train_time:96166ms step_avg:55.20ms
step:1743/2110 train_time:96254ms step_avg:55.22ms
step:1744/2110 train_time:96344ms step_avg:55.24ms
step:1745/2110 train_time:96432ms step_avg:55.26ms
step:1746/2110 train_time:96521ms step_avg:55.28ms
step:1747/2110 train_time:96608ms step_avg:55.30ms
step:1748/2110 train_time:96697ms step_avg:55.32ms
step:1749/2110 train_time:96785ms step_avg:55.34ms
step:1750/2110 train_time:96872ms step_avg:55.36ms
step:1750/2110 val_loss:3.3781 train_time:96962ms step_avg:55.41ms
step:1751/2110 train_time:96995ms step_avg:55.39ms
step:1752/2110 train_time:97055ms step_avg:55.40ms
step:1753/2110 train_time:97147ms step_avg:55.42ms
step:1754/2110 train_time:97236ms step_avg:55.44ms
step:1755/2110 train_time:97324ms step_avg:55.46ms
step:1756/2110 train_time:97411ms step_avg:55.47ms
step:1757/2110 train_time:97498ms step_avg:55.49ms
step:1758/2110 train_time:97585ms step_avg:55.51ms
step:1759/2110 train_time:97671ms step_avg:55.53ms
step:1760/2110 train_time:97758ms step_avg:55.54ms
step:1761/2110 train_time:97845ms step_avg:55.56ms
step:1762/2110 train_time:97934ms step_avg:55.58ms
step:1763/2110 train_time:98025ms step_avg:55.60ms
step:1764/2110 train_time:98118ms step_avg:55.62ms
step:1765/2110 train_time:98207ms step_avg:55.64ms
step:1766/2110 train_time:98297ms step_avg:55.66ms
step:1767/2110 train_time:98384ms step_avg:55.68ms
step:1768/2110 train_time:98472ms step_avg:55.70ms
step:1769/2110 train_time:98559ms step_avg:55.71ms
step:1770/2110 train_time:98646ms step_avg:55.73ms
step:1771/2110 train_time:98733ms step_avg:55.75ms
step:1772/2110 train_time:98820ms step_avg:55.77ms
step:1773/2110 train_time:98909ms step_avg:55.79ms
step:1774/2110 train_time:98999ms step_avg:55.81ms
step:1775/2110 train_time:99088ms step_avg:55.82ms
step:1776/2110 train_time:99178ms step_avg:55.84ms
step:1777/2110 train_time:99266ms step_avg:55.86ms
step:1778/2110 train_time:99355ms step_avg:55.88ms
step:1779/2110 train_time:99444ms step_avg:55.90ms
step:1780/2110 train_time:99532ms step_avg:55.92ms
step:1781/2110 train_time:99619ms step_avg:55.93ms
step:1782/2110 train_time:99705ms step_avg:55.95ms
step:1783/2110 train_time:99792ms step_avg:55.97ms
step:1784/2110 train_time:99881ms step_avg:55.99ms
step:1785/2110 train_time:99968ms step_avg:56.00ms
step:1786/2110 train_time:100057ms step_avg:56.02ms
step:1787/2110 train_time:100147ms step_avg:56.04ms
step:1788/2110 train_time:100236ms step_avg:56.06ms
step:1789/2110 train_time:100324ms step_avg:56.08ms
step:1790/2110 train_time:100413ms step_avg:56.10ms
step:1791/2110 train_time:100501ms step_avg:56.11ms
step:1792/2110 train_time:100588ms step_avg:56.13ms
step:1793/2110 train_time:100676ms step_avg:56.15ms
step:1794/2110 train_time:100764ms step_avg:56.17ms
step:1795/2110 train_time:100850ms step_avg:56.18ms
step:1796/2110 train_time:100940ms step_avg:56.20ms
step:1797/2110 train_time:101027ms step_avg:56.22ms
step:1798/2110 train_time:101116ms step_avg:56.24ms
step:1799/2110 train_time:101205ms step_avg:56.26ms
step:1800/2110 train_time:101294ms step_avg:56.27ms
step:1801/2110 train_time:101382ms step_avg:56.29ms
step:1802/2110 train_time:101470ms step_avg:56.31ms
step:1803/2110 train_time:101557ms step_avg:56.33ms
step:1804/2110 train_time:101645ms step_avg:56.34ms
step:1805/2110 train_time:101732ms step_avg:56.36ms
step:1806/2110 train_time:101821ms step_avg:56.38ms
step:1807/2110 train_time:101907ms step_avg:56.40ms
step:1808/2110 train_time:101996ms step_avg:56.41ms
step:1809/2110 train_time:102084ms step_avg:56.43ms
step:1810/2110 train_time:102173ms step_avg:56.45ms
step:1811/2110 train_time:102262ms step_avg:56.47ms
step:1812/2110 train_time:102351ms step_avg:56.49ms
step:1813/2110 train_time:102438ms step_avg:56.50ms
step:1814/2110 train_time:102526ms step_avg:56.52ms
step:1815/2110 train_time:102614ms step_avg:56.54ms
step:1816/2110 train_time:102702ms step_avg:56.55ms
step:1817/2110 train_time:102789ms step_avg:56.57ms
step:1818/2110 train_time:102877ms step_avg:56.59ms
step:1819/2110 train_time:102965ms step_avg:56.61ms
step:1820/2110 train_time:103053ms step_avg:56.62ms
step:1821/2110 train_time:103142ms step_avg:56.64ms
step:1822/2110 train_time:103231ms step_avg:56.66ms
step:1823/2110 train_time:103319ms step_avg:56.68ms
step:1824/2110 train_time:103408ms step_avg:56.69ms
step:1825/2110 train_time:103494ms step_avg:56.71ms
step:1826/2110 train_time:103583ms step_avg:56.73ms
step:1827/2110 train_time:103671ms step_avg:56.74ms
step:1828/2110 train_time:103760ms step_avg:56.76ms
step:1829/2110 train_time:103846ms step_avg:56.78ms
step:1830/2110 train_time:103934ms step_avg:56.79ms
step:1831/2110 train_time:104023ms step_avg:56.81ms
step:1832/2110 train_time:104112ms step_avg:56.83ms
step:1833/2110 train_time:104200ms step_avg:56.85ms
step:1834/2110 train_time:104288ms step_avg:56.86ms
step:1835/2110 train_time:104377ms step_avg:56.88ms
step:1836/2110 train_time:104465ms step_avg:56.90ms
step:1837/2110 train_time:104554ms step_avg:56.92ms
step:1838/2110 train_time:104643ms step_avg:56.93ms
step:1839/2110 train_time:104729ms step_avg:56.95ms
step:1840/2110 train_time:104818ms step_avg:56.97ms
step:1841/2110 train_time:104905ms step_avg:56.98ms
step:1842/2110 train_time:104993ms step_avg:57.00ms
step:1843/2110 train_time:105081ms step_avg:57.02ms
step:1844/2110 train_time:105168ms step_avg:57.03ms
step:1845/2110 train_time:105258ms step_avg:57.05ms
step:1846/2110 train_time:105346ms step_avg:57.07ms
step:1847/2110 train_time:105433ms step_avg:57.08ms
step:1848/2110 train_time:105522ms step_avg:57.10ms
step:1849/2110 train_time:105610ms step_avg:57.12ms
step:1850/2110 train_time:105698ms step_avg:57.13ms
step:1851/2110 train_time:105786ms step_avg:57.15ms
step:1852/2110 train_time:105874ms step_avg:57.17ms
step:1853/2110 train_time:105961ms step_avg:57.18ms
step:1854/2110 train_time:106049ms step_avg:57.20ms
step:1855/2110 train_time:106138ms step_avg:57.22ms
step:1856/2110 train_time:106227ms step_avg:57.23ms
step:1857/2110 train_time:106315ms step_avg:57.25ms
step:1858/2110 train_time:106406ms step_avg:57.27ms
step:1859/2110 train_time:106491ms step_avg:57.28ms
step:1860/2110 train_time:106581ms step_avg:57.30ms
step:1861/2110 train_time:106668ms step_avg:57.32ms
step:1862/2110 train_time:106756ms step_avg:57.33ms
step:1863/2110 train_time:106845ms step_avg:57.35ms
step:1864/2110 train_time:106933ms step_avg:57.37ms
step:1865/2110 train_time:107021ms step_avg:57.38ms
step:1866/2110 train_time:107110ms step_avg:57.40ms
step:1867/2110 train_time:107197ms step_avg:57.42ms
step:1868/2110 train_time:107287ms step_avg:57.43ms
step:1869/2110 train_time:107374ms step_avg:57.45ms
step:1870/2110 train_time:107463ms step_avg:57.47ms
step:1871/2110 train_time:107551ms step_avg:57.48ms
step:1872/2110 train_time:107639ms step_avg:57.50ms
step:1873/2110 train_time:107728ms step_avg:57.52ms
step:1874/2110 train_time:107817ms step_avg:57.53ms
step:1875/2110 train_time:107904ms step_avg:57.55ms
step:1876/2110 train_time:107992ms step_avg:57.57ms
step:1877/2110 train_time:108080ms step_avg:57.58ms
step:1878/2110 train_time:108168ms step_avg:57.60ms
step:1879/2110 train_time:108255ms step_avg:57.61ms
step:1880/2110 train_time:108344ms step_avg:57.63ms
step:1881/2110 train_time:108432ms step_avg:57.65ms
step:1882/2110 train_time:108520ms step_avg:57.66ms
step:1883/2110 train_time:108607ms step_avg:57.68ms
step:1884/2110 train_time:108695ms step_avg:57.69ms
step:1885/2110 train_time:108783ms step_avg:57.71ms
step:1886/2110 train_time:108871ms step_avg:57.73ms
step:1887/2110 train_time:108960ms step_avg:57.74ms
step:1888/2110 train_time:109048ms step_avg:57.76ms
step:1889/2110 train_time:109137ms step_avg:57.77ms
step:1890/2110 train_time:109225ms step_avg:57.79ms
step:1891/2110 train_time:109313ms step_avg:57.81ms
step:1892/2110 train_time:109403ms step_avg:57.82ms
step:1893/2110 train_time:109489ms step_avg:57.84ms
step:1894/2110 train_time:109579ms step_avg:57.86ms
step:1895/2110 train_time:109666ms step_avg:57.87ms
step:1896/2110 train_time:109755ms step_avg:57.89ms
step:1897/2110 train_time:109843ms step_avg:57.90ms
step:1898/2110 train_time:109932ms step_avg:57.92ms
step:1899/2110 train_time:110019ms step_avg:57.94ms
step:1900/2110 train_time:110107ms step_avg:57.95ms
step:1901/2110 train_time:110195ms step_avg:57.97ms
step:1902/2110 train_time:110284ms step_avg:57.98ms
step:1903/2110 train_time:110372ms step_avg:58.00ms
step:1904/2110 train_time:110460ms step_avg:58.01ms
step:1905/2110 train_time:110547ms step_avg:58.03ms
step:1906/2110 train_time:110635ms step_avg:58.05ms
step:1907/2110 train_time:110723ms step_avg:58.06ms
step:1908/2110 train_time:110812ms step_avg:58.08ms
step:1909/2110 train_time:110899ms step_avg:58.09ms
step:1910/2110 train_time:110987ms step_avg:58.11ms
step:1911/2110 train_time:111076ms step_avg:58.12ms
step:1912/2110 train_time:111164ms step_avg:58.14ms
step:1913/2110 train_time:111253ms step_avg:58.16ms
step:1914/2110 train_time:111342ms step_avg:58.17ms
step:1915/2110 train_time:111429ms step_avg:58.19ms
step:1916/2110 train_time:111518ms step_avg:58.20ms
step:1917/2110 train_time:111606ms step_avg:58.22ms
step:1918/2110 train_time:111693ms step_avg:58.23ms
step:1919/2110 train_time:111782ms step_avg:58.25ms
step:1920/2110 train_time:111870ms step_avg:58.27ms
step:1921/2110 train_time:111957ms step_avg:58.28ms
step:1922/2110 train_time:112046ms step_avg:58.30ms
step:1923/2110 train_time:112133ms step_avg:58.31ms
step:1924/2110 train_time:112222ms step_avg:58.33ms
step:1925/2110 train_time:112310ms step_avg:58.34ms
step:1926/2110 train_time:112398ms step_avg:58.36ms
step:1927/2110 train_time:112485ms step_avg:58.37ms
step:1928/2110 train_time:112574ms step_avg:58.39ms
step:1929/2110 train_time:112663ms step_avg:58.40ms
step:1930/2110 train_time:112751ms step_avg:58.42ms
step:1931/2110 train_time:112839ms step_avg:58.44ms
step:1932/2110 train_time:112926ms step_avg:58.45ms
step:1933/2110 train_time:113015ms step_avg:58.47ms
step:1934/2110 train_time:113104ms step_avg:58.48ms
step:1935/2110 train_time:113192ms step_avg:58.50ms
step:1936/2110 train_time:113281ms step_avg:58.51ms
step:1937/2110 train_time:113368ms step_avg:58.53ms
step:1938/2110 train_time:113458ms step_avg:58.54ms
step:1939/2110 train_time:113546ms step_avg:58.56ms
step:1940/2110 train_time:113635ms step_avg:58.57ms
step:1941/2110 train_time:113723ms step_avg:58.59ms
step:1942/2110 train_time:113811ms step_avg:58.60ms
step:1943/2110 train_time:113898ms step_avg:58.62ms
step:1944/2110 train_time:113986ms step_avg:58.63ms
step:1945/2110 train_time:114074ms step_avg:58.65ms
step:1946/2110 train_time:114163ms step_avg:58.67ms
step:1947/2110 train_time:114251ms step_avg:58.68ms
step:1948/2110 train_time:114339ms step_avg:58.70ms
step:1949/2110 train_time:114427ms step_avg:58.71ms
step:1950/2110 train_time:114515ms step_avg:58.73ms
step:1951/2110 train_time:114603ms step_avg:58.74ms
step:1952/2110 train_time:114691ms step_avg:58.76ms
step:1953/2110 train_time:114779ms step_avg:58.77ms
step:1954/2110 train_time:114867ms step_avg:58.79ms
step:1955/2110 train_time:114955ms step_avg:58.80ms
step:1956/2110 train_time:115047ms step_avg:58.82ms
step:1957/2110 train_time:115133ms step_avg:58.83ms
step:1958/2110 train_time:115222ms step_avg:58.85ms
step:1959/2110 train_time:115309ms step_avg:58.86ms
step:1960/2110 train_time:115397ms step_avg:58.88ms
step:1961/2110 train_time:115486ms step_avg:58.89ms
step:1962/2110 train_time:115573ms step_avg:58.91ms
step:1963/2110 train_time:115661ms step_avg:58.92ms
step:1964/2110 train_time:115749ms step_avg:58.94ms
step:1965/2110 train_time:115837ms step_avg:58.95ms
step:1966/2110 train_time:115925ms step_avg:58.96ms
step:1967/2110 train_time:116014ms step_avg:58.98ms
step:1968/2110 train_time:116102ms step_avg:58.99ms
step:1969/2110 train_time:116191ms step_avg:59.01ms
step:1970/2110 train_time:116279ms step_avg:59.03ms
step:1971/2110 train_time:116367ms step_avg:59.04ms
step:1972/2110 train_time:116456ms step_avg:59.05ms
step:1973/2110 train_time:116545ms step_avg:59.07ms
step:1974/2110 train_time:116635ms step_avg:59.09ms
step:1975/2110 train_time:116723ms step_avg:59.10ms
step:1976/2110 train_time:116811ms step_avg:59.12ms
step:1977/2110 train_time:116899ms step_avg:59.13ms
step:1978/2110 train_time:116986ms step_avg:59.14ms
step:1979/2110 train_time:117076ms step_avg:59.16ms
step:1980/2110 train_time:117164ms step_avg:59.17ms
step:1981/2110 train_time:117251ms step_avg:59.19ms
step:1982/2110 train_time:117341ms step_avg:59.20ms
step:1983/2110 train_time:117428ms step_avg:59.22ms
step:1984/2110 train_time:117517ms step_avg:59.23ms
step:1985/2110 train_time:117605ms step_avg:59.25ms
step:1986/2110 train_time:117694ms step_avg:59.26ms
step:1987/2110 train_time:117782ms step_avg:59.28ms
step:1988/2110 train_time:117870ms step_avg:59.29ms
step:1989/2110 train_time:117959ms step_avg:59.31ms
step:1990/2110 train_time:118046ms step_avg:59.32ms
step:1991/2110 train_time:118134ms step_avg:59.33ms
step:1992/2110 train_time:118223ms step_avg:59.35ms
step:1993/2110 train_time:118311ms step_avg:59.36ms
step:1994/2110 train_time:118398ms step_avg:59.38ms
step:1995/2110 train_time:118486ms step_avg:59.39ms
step:1996/2110 train_time:118575ms step_avg:59.41ms
step:1997/2110 train_time:118662ms step_avg:59.42ms
step:1998/2110 train_time:118751ms step_avg:59.43ms
step:1999/2110 train_time:118839ms step_avg:59.45ms
step:2000/2110 train_time:118927ms step_avg:59.46ms
step:2000/2110 val_loss:3.3030 train_time:119016ms step_avg:59.51ms
step:2001/2110 train_time:119048ms step_avg:59.49ms
step:2002/2110 train_time:119109ms step_avg:59.50ms
step:2003/2110 train_time:119201ms step_avg:59.51ms
step:2004/2110 train_time:119290ms step_avg:59.53ms
step:2005/2110 train_time:119378ms step_avg:59.54ms
step:2006/2110 train_time:119465ms step_avg:59.55ms
step:2007/2110 train_time:119551ms step_avg:59.57ms
step:2008/2110 train_time:119638ms step_avg:59.58ms
step:2009/2110 train_time:119725ms step_avg:59.59ms
step:2010/2110 train_time:119812ms step_avg:59.61ms
step:2011/2110 train_time:119899ms step_avg:59.62ms
step:2012/2110 train_time:119989ms step_avg:59.64ms
step:2013/2110 train_time:120079ms step_avg:59.65ms
step:2014/2110 train_time:120170ms step_avg:59.67ms
step:2015/2110 train_time:120259ms step_avg:59.68ms
step:2016/2110 train_time:120348ms step_avg:59.70ms
step:2017/2110 train_time:120434ms step_avg:59.71ms
step:2018/2110 train_time:120521ms step_avg:59.72ms
step:2019/2110 train_time:120609ms step_avg:59.74ms
step:2020/2110 train_time:120696ms step_avg:59.75ms
step:2021/2110 train_time:120783ms step_avg:59.76ms
step:2022/2110 train_time:120870ms step_avg:59.78ms
step:2023/2110 train_time:120958ms step_avg:59.79ms
step:2024/2110 train_time:121049ms step_avg:59.81ms
step:2025/2110 train_time:121139ms step_avg:59.82ms
step:2026/2110 train_time:121230ms step_avg:59.84ms
step:2027/2110 train_time:121318ms step_avg:59.85ms
step:2028/2110 train_time:121405ms step_avg:59.86ms
step:2029/2110 train_time:121493ms step_avg:59.88ms
step:2030/2110 train_time:121580ms step_avg:59.89ms
step:2031/2110 train_time:121669ms step_avg:59.91ms
step:2032/2110 train_time:121757ms step_avg:59.92ms
step:2033/2110 train_time:121846ms step_avg:59.93ms
step:2034/2110 train_time:121933ms step_avg:59.95ms
step:2035/2110 train_time:122023ms step_avg:59.96ms
step:2036/2110 train_time:122111ms step_avg:59.98ms
step:2037/2110 train_time:122201ms step_avg:59.99ms
step:2038/2110 train_time:122289ms step_avg:60.00ms
step:2039/2110 train_time:122378ms step_avg:60.02ms
step:2040/2110 train_time:122467ms step_avg:60.03ms
step:2041/2110 train_time:122554ms step_avg:60.05ms
step:2042/2110 train_time:122642ms step_avg:60.06ms
step:2043/2110 train_time:122728ms step_avg:60.07ms
step:2044/2110 train_time:122816ms step_avg:60.09ms
step:2045/2110 train_time:122904ms step_avg:60.10ms
step:2046/2110 train_time:122992ms step_avg:60.11ms
step:2047/2110 train_time:123081ms step_avg:60.13ms
step:2048/2110 train_time:123171ms step_avg:60.14ms
step:2049/2110 train_time:123260ms step_avg:60.16ms
step:2050/2110 train_time:123349ms step_avg:60.17ms
step:2051/2110 train_time:123437ms step_avg:60.18ms
step:2052/2110 train_time:123525ms step_avg:60.20ms
step:2053/2110 train_time:123614ms step_avg:60.21ms
step:2054/2110 train_time:123701ms step_avg:60.22ms
step:2055/2110 train_time:123789ms step_avg:60.24ms
step:2056/2110 train_time:123878ms step_avg:60.25ms
step:2057/2110 train_time:123967ms step_avg:60.27ms
step:2058/2110 train_time:124054ms step_avg:60.28ms
step:2059/2110 train_time:124143ms step_avg:60.29ms
step:2060/2110 train_time:124231ms step_avg:60.31ms
step:2061/2110 train_time:124320ms step_avg:60.32ms
step:2062/2110 train_time:124410ms step_avg:60.33ms
step:2063/2110 train_time:124497ms step_avg:60.35ms
step:2064/2110 train_time:124585ms step_avg:60.36ms
step:2065/2110 train_time:124673ms step_avg:60.37ms
step:2066/2110 train_time:124761ms step_avg:60.39ms
step:2067/2110 train_time:124849ms step_avg:60.40ms
step:2068/2110 train_time:124937ms step_avg:60.41ms
step:2069/2110 train_time:125025ms step_avg:60.43ms
step:2070/2110 train_time:125113ms step_avg:60.44ms
step:2071/2110 train_time:125203ms step_avg:60.46ms
step:2072/2110 train_time:125291ms step_avg:60.47ms
step:2073/2110 train_time:125380ms step_avg:60.48ms
step:2074/2110 train_time:125469ms step_avg:60.50ms
step:2075/2110 train_time:125557ms step_avg:60.51ms
step:2076/2110 train_time:125644ms step_avg:60.52ms
step:2077/2110 train_time:125733ms step_avg:60.54ms
step:2078/2110 train_time:125822ms step_avg:60.55ms
step:2079/2110 train_time:125910ms step_avg:60.56ms
step:2080/2110 train_time:125999ms step_avg:60.58ms
step:2081/2110 train_time:126087ms step_avg:60.59ms
step:2082/2110 train_time:126175ms step_avg:60.60ms
step:2083/2110 train_time:126265ms step_avg:60.62ms
step:2084/2110 train_time:126352ms step_avg:60.63ms
step:2085/2110 train_time:126443ms step_avg:60.64ms
step:2086/2110 train_time:126531ms step_avg:60.66ms
step:2087/2110 train_time:126619ms step_avg:60.67ms
step:2088/2110 train_time:126708ms step_avg:60.68ms
step:2089/2110 train_time:126795ms step_avg:60.70ms
step:2090/2110 train_time:126885ms step_avg:60.71ms
step:2091/2110 train_time:126974ms step_avg:60.72ms
step:2092/2110 train_time:127063ms step_avg:60.74ms
step:2093/2110 train_time:127152ms step_avg:60.75ms
step:2094/2110 train_time:127242ms step_avg:60.77ms
step:2095/2110 train_time:127330ms step_avg:60.78ms
step:2096/2110 train_time:127419ms step_avg:60.79ms
step:2097/2110 train_time:127508ms step_avg:60.81ms
step:2098/2110 train_time:127597ms step_avg:60.82ms
step:2099/2110 train_time:127686ms step_avg:60.83ms
step:2100/2110 train_time:127774ms step_avg:60.84ms
step:2101/2110 train_time:127863ms step_avg:60.86ms
step:2102/2110 train_time:127950ms step_avg:60.87ms
step:2103/2110 train_time:128039ms step_avg:60.88ms
step:2104/2110 train_time:128128ms step_avg:60.90ms
step:2105/2110 train_time:128215ms step_avg:60.91ms
step:2106/2110 train_time:128304ms step_avg:60.92ms
step:2107/2110 train_time:128392ms step_avg:60.94ms
step:2108/2110 train_time:128481ms step_avg:60.95ms
step:2109/2110 train_time:128570ms step_avg:60.96ms
step:2110/2110 train_time:128660ms step_avg:60.98ms
step:2110/2110 val_loss:3.2781 train_time:128749ms step_avg:61.02ms
peak memory allocated: 29892 MiB reserved: 43856 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 15 21:26:08 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   39C    P0            126W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   43C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   43C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            122W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   42C    P0            126W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   38C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              88      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              88      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2110 train_time:88ms step_avg:88.29ms
step:2/2110 train_time:119ms step_avg:59.36ms
step:3/2110 train_time:146ms step_avg:48.74ms
step:4/2110 train_time:171ms step_avg:42.79ms
step:5/2110 train_time:203ms step_avg:40.69ms
step:6/2110 train_time:617ms step_avg:102.76ms
step:7/2110 train_time:642ms step_avg:91.73ms
step:8/2110 train_time:667ms step_avg:83.34ms
step:9/2110 train_time:693ms step_avg:77.00ms
step:10/2110 train_time:719ms step_avg:71.94ms
step:11/2110 train_time:752ms step_avg:68.37ms
step:12/2110 train_time:785ms step_avg:65.39ms
step:13/2110 train_time:818ms step_avg:62.92ms
step:14/2110 train_time:851ms step_avg:60.78ms
step:15/2110 train_time:884ms step_avg:58.94ms
step:16/2110 train_time:917ms step_avg:57.29ms
step:17/2110 train_time:950ms step_avg:55.89ms
step:18/2110 train_time:983ms step_avg:54.59ms
step:19/2110 train_time:1016ms step_avg:53.46ms
step:20/2110 train_time:1049ms step_avg:52.44ms
step:21/2110 train_time:1082ms step_avg:51.53ms
step:22/2110 train_time:1115ms step_avg:50.68ms
step:23/2110 train_time:1148ms step_avg:49.92ms
step:24/2110 train_time:1181ms step_avg:49.21ms
step:25/2110 train_time:1214ms step_avg:48.57ms
step:26/2110 train_time:1247ms step_avg:47.97ms
step:27/2110 train_time:1280ms step_avg:47.42ms
step:28/2110 train_time:1314ms step_avg:46.91ms
step:29/2110 train_time:1347ms step_avg:46.43ms
step:30/2110 train_time:1380ms step_avg:45.99ms
step:31/2110 train_time:1413ms step_avg:45.57ms
step:32/2110 train_time:1446ms step_avg:45.18ms
step:33/2110 train_time:1478ms step_avg:44.80ms
step:34/2110 train_time:1512ms step_avg:44.46ms
step:35/2110 train_time:1547ms step_avg:44.20ms
step:36/2110 train_time:1581ms step_avg:43.92ms
step:37/2110 train_time:1615ms step_avg:43.64ms
step:38/2110 train_time:1649ms step_avg:43.38ms
step:39/2110 train_time:1682ms step_avg:43.12ms
step:40/2110 train_time:1714ms step_avg:42.86ms
step:41/2110 train_time:1749ms step_avg:42.65ms
step:42/2110 train_time:1781ms step_avg:42.41ms
step:43/2110 train_time:1814ms step_avg:42.20ms
step:44/2110 train_time:1847ms step_avg:41.98ms
step:45/2110 train_time:1881ms step_avg:41.79ms
step:46/2110 train_time:1913ms step_avg:41.60ms
step:47/2110 train_time:1947ms step_avg:41.43ms
step:48/2110 train_time:1980ms step_avg:41.25ms
step:49/2110 train_time:2013ms step_avg:41.09ms
step:50/2110 train_time:2047ms step_avg:40.93ms
step:51/2110 train_time:2079ms step_avg:40.77ms
step:52/2110 train_time:2112ms step_avg:40.62ms
step:53/2110 train_time:2146ms step_avg:40.49ms
step:54/2110 train_time:2179ms step_avg:40.35ms
step:55/2110 train_time:2212ms step_avg:40.21ms
step:56/2110 train_time:2244ms step_avg:40.08ms
step:57/2110 train_time:2278ms step_avg:39.96ms
step:58/2110 train_time:2310ms step_avg:39.83ms
step:59/2110 train_time:2344ms step_avg:39.73ms
step:60/2110 train_time:2377ms step_avg:39.61ms
step:61/2110 train_time:2409ms step_avg:39.50ms
step:62/2110 train_time:2444ms step_avg:39.42ms
step:63/2110 train_time:2476ms step_avg:39.30ms
step:64/2110 train_time:2509ms step_avg:39.20ms
step:65/2110 train_time:2543ms step_avg:39.12ms
step:66/2110 train_time:2576ms step_avg:39.03ms
step:67/2110 train_time:2610ms step_avg:38.95ms
step:68/2110 train_time:2643ms step_avg:38.87ms
step:69/2110 train_time:2676ms step_avg:38.79ms
step:70/2110 train_time:2709ms step_avg:38.70ms
step:71/2110 train_time:2743ms step_avg:38.64ms
step:72/2110 train_time:2776ms step_avg:38.56ms
step:73/2110 train_time:2810ms step_avg:38.49ms
step:74/2110 train_time:2842ms step_avg:38.41ms
step:75/2110 train_time:2876ms step_avg:38.35ms
step:76/2110 train_time:2908ms step_avg:38.27ms
step:77/2110 train_time:2942ms step_avg:38.21ms
step:78/2110 train_time:2975ms step_avg:38.14ms
step:79/2110 train_time:3009ms step_avg:38.09ms
step:80/2110 train_time:3042ms step_avg:38.02ms
step:81/2110 train_time:3074ms step_avg:37.96ms
step:82/2110 train_time:3108ms step_avg:37.91ms
step:83/2110 train_time:3140ms step_avg:37.83ms
step:84/2110 train_time:3173ms step_avg:37.77ms
step:85/2110 train_time:3206ms step_avg:37.72ms
step:86/2110 train_time:3239ms step_avg:37.66ms
step:87/2110 train_time:3273ms step_avg:37.62ms
step:88/2110 train_time:3306ms step_avg:37.57ms
step:89/2110 train_time:3339ms step_avg:37.51ms
step:90/2110 train_time:3371ms step_avg:37.46ms
step:91/2110 train_time:3405ms step_avg:37.42ms
step:92/2110 train_time:3437ms step_avg:37.36ms
step:93/2110 train_time:3471ms step_avg:37.32ms
step:94/2110 train_time:3503ms step_avg:37.27ms
step:95/2110 train_time:3537ms step_avg:37.23ms
step:96/2110 train_time:3570ms step_avg:37.19ms
step:97/2110 train_time:3603ms step_avg:37.14ms
step:98/2110 train_time:3636ms step_avg:37.10ms
step:99/2110 train_time:3669ms step_avg:37.07ms
step:100/2110 train_time:3703ms step_avg:37.03ms
step:101/2110 train_time:3736ms step_avg:36.99ms
step:102/2110 train_time:3769ms step_avg:36.95ms
step:103/2110 train_time:3803ms step_avg:36.92ms
step:104/2110 train_time:3836ms step_avg:36.88ms
step:105/2110 train_time:3869ms step_avg:36.85ms
step:106/2110 train_time:3902ms step_avg:36.81ms
step:107/2110 train_time:3935ms step_avg:36.78ms
step:108/2110 train_time:3969ms step_avg:36.75ms
step:109/2110 train_time:4001ms step_avg:36.71ms
step:110/2110 train_time:4034ms step_avg:36.67ms
step:111/2110 train_time:4068ms step_avg:36.65ms
step:112/2110 train_time:4101ms step_avg:36.62ms
step:113/2110 train_time:4133ms step_avg:36.58ms
step:114/2110 train_time:4166ms step_avg:36.54ms
step:115/2110 train_time:4199ms step_avg:36.52ms
step:116/2110 train_time:4232ms step_avg:36.48ms
step:117/2110 train_time:4265ms step_avg:36.45ms
step:118/2110 train_time:4298ms step_avg:36.42ms
step:119/2110 train_time:4331ms step_avg:36.40ms
step:120/2110 train_time:4365ms step_avg:36.37ms
step:121/2110 train_time:4398ms step_avg:36.34ms
step:122/2110 train_time:4431ms step_avg:36.32ms
step:123/2110 train_time:4464ms step_avg:36.29ms
step:124/2110 train_time:4497ms step_avg:36.26ms
step:125/2110 train_time:4530ms step_avg:36.24ms
step:126/2110 train_time:4563ms step_avg:36.22ms
step:127/2110 train_time:4597ms step_avg:36.19ms
step:128/2110 train_time:4629ms step_avg:36.17ms
step:129/2110 train_time:4663ms step_avg:36.15ms
step:130/2110 train_time:4696ms step_avg:36.12ms
step:131/2110 train_time:4729ms step_avg:36.10ms
step:132/2110 train_time:4762ms step_avg:36.08ms
step:133/2110 train_time:4795ms step_avg:36.05ms
step:134/2110 train_time:4828ms step_avg:36.03ms
step:135/2110 train_time:4861ms step_avg:36.01ms
step:136/2110 train_time:4893ms step_avg:35.98ms
step:137/2110 train_time:4927ms step_avg:35.96ms
step:138/2110 train_time:4960ms step_avg:35.94ms
step:139/2110 train_time:4993ms step_avg:35.92ms
step:140/2110 train_time:5027ms step_avg:35.90ms
step:141/2110 train_time:5059ms step_avg:35.88ms
step:142/2110 train_time:5092ms step_avg:35.86ms
step:143/2110 train_time:5125ms step_avg:35.84ms
step:144/2110 train_time:5158ms step_avg:35.82ms
step:145/2110 train_time:5191ms step_avg:35.80ms
step:146/2110 train_time:5224ms step_avg:35.78ms
step:147/2110 train_time:5257ms step_avg:35.76ms
step:148/2110 train_time:5290ms step_avg:35.74ms
step:149/2110 train_time:5323ms step_avg:35.72ms
step:150/2110 train_time:5356ms step_avg:35.70ms
step:151/2110 train_time:5388ms step_avg:35.69ms
step:152/2110 train_time:5421ms step_avg:35.66ms
step:153/2110 train_time:5454ms step_avg:35.65ms
step:154/2110 train_time:5487ms step_avg:35.63ms
step:155/2110 train_time:5520ms step_avg:35.62ms
step:156/2110 train_time:5553ms step_avg:35.60ms
step:157/2110 train_time:5586ms step_avg:35.58ms
step:158/2110 train_time:5620ms step_avg:35.57ms
step:159/2110 train_time:5652ms step_avg:35.55ms
step:160/2110 train_time:5685ms step_avg:35.53ms
step:161/2110 train_time:5718ms step_avg:35.52ms
step:162/2110 train_time:5751ms step_avg:35.50ms
step:163/2110 train_time:5785ms step_avg:35.49ms
step:164/2110 train_time:5817ms step_avg:35.47ms
step:165/2110 train_time:5851ms step_avg:35.46ms
step:166/2110 train_time:5884ms step_avg:35.45ms
step:167/2110 train_time:5916ms step_avg:35.43ms
step:168/2110 train_time:5949ms step_avg:35.41ms
step:169/2110 train_time:5983ms step_avg:35.40ms
step:170/2110 train_time:6015ms step_avg:35.38ms
step:171/2110 train_time:6049ms step_avg:35.37ms
step:172/2110 train_time:6082ms step_avg:35.36ms
step:173/2110 train_time:6115ms step_avg:35.34ms
step:174/2110 train_time:6148ms step_avg:35.33ms
step:175/2110 train_time:6181ms step_avg:35.32ms
step:176/2110 train_time:6213ms step_avg:35.30ms
step:177/2110 train_time:6246ms step_avg:35.29ms
step:178/2110 train_time:6279ms step_avg:35.27ms
step:179/2110 train_time:6312ms step_avg:35.26ms
step:180/2110 train_time:6344ms step_avg:35.25ms
step:181/2110 train_time:6378ms step_avg:35.23ms
step:182/2110 train_time:6410ms step_avg:35.22ms
step:183/2110 train_time:6443ms step_avg:35.21ms
step:184/2110 train_time:6476ms step_avg:35.20ms
step:185/2110 train_time:6509ms step_avg:35.18ms
step:186/2110 train_time:6542ms step_avg:35.17ms
step:187/2110 train_time:6575ms step_avg:35.16ms
step:188/2110 train_time:6609ms step_avg:35.15ms
step:189/2110 train_time:6642ms step_avg:35.14ms
step:190/2110 train_time:6675ms step_avg:35.13ms
step:191/2110 train_time:6708ms step_avg:35.12ms
step:192/2110 train_time:6740ms step_avg:35.11ms
step:193/2110 train_time:6773ms step_avg:35.09ms
step:194/2110 train_time:6808ms step_avg:35.09ms
step:195/2110 train_time:6840ms step_avg:35.08ms
step:196/2110 train_time:6873ms step_avg:35.06ms
step:197/2110 train_time:6906ms step_avg:35.06ms
step:198/2110 train_time:6939ms step_avg:35.04ms
step:199/2110 train_time:6972ms step_avg:35.03ms
step:200/2110 train_time:7005ms step_avg:35.02ms
step:201/2110 train_time:7038ms step_avg:35.01ms
step:202/2110 train_time:7071ms step_avg:35.00ms
step:203/2110 train_time:7105ms step_avg:35.00ms
step:204/2110 train_time:7137ms step_avg:34.99ms
step:205/2110 train_time:7170ms step_avg:34.98ms
step:206/2110 train_time:7203ms step_avg:34.97ms
step:207/2110 train_time:7236ms step_avg:34.96ms
step:208/2110 train_time:7269ms step_avg:34.95ms
step:209/2110 train_time:7302ms step_avg:34.94ms
step:210/2110 train_time:7335ms step_avg:34.93ms
step:211/2110 train_time:7368ms step_avg:34.92ms
step:212/2110 train_time:7401ms step_avg:34.91ms
step:213/2110 train_time:7434ms step_avg:34.90ms
step:214/2110 train_time:7467ms step_avg:34.89ms
step:215/2110 train_time:7500ms step_avg:34.88ms
step:216/2110 train_time:7533ms step_avg:34.88ms
step:217/2110 train_time:7566ms step_avg:34.87ms
step:218/2110 train_time:7599ms step_avg:34.86ms
step:219/2110 train_time:7632ms step_avg:34.85ms
step:220/2110 train_time:7666ms step_avg:34.84ms
step:221/2110 train_time:7699ms step_avg:34.84ms
step:222/2110 train_time:7731ms step_avg:34.83ms
step:223/2110 train_time:7765ms step_avg:34.82ms
step:224/2110 train_time:7797ms step_avg:34.81ms
step:225/2110 train_time:7830ms step_avg:34.80ms
step:226/2110 train_time:7864ms step_avg:34.80ms
step:227/2110 train_time:7896ms step_avg:34.79ms
step:228/2110 train_time:7929ms step_avg:34.78ms
step:229/2110 train_time:7962ms step_avg:34.77ms
step:230/2110 train_time:7995ms step_avg:34.76ms
step:231/2110 train_time:8028ms step_avg:34.76ms
step:232/2110 train_time:8062ms step_avg:34.75ms
step:233/2110 train_time:8095ms step_avg:34.74ms
step:234/2110 train_time:8127ms step_avg:34.73ms
step:235/2110 train_time:8161ms step_avg:34.73ms
step:236/2110 train_time:8193ms step_avg:34.72ms
step:237/2110 train_time:8227ms step_avg:34.71ms
step:238/2110 train_time:8260ms step_avg:34.70ms
step:239/2110 train_time:8293ms step_avg:34.70ms
step:240/2110 train_time:8326ms step_avg:34.69ms
step:241/2110 train_time:8359ms step_avg:34.68ms
step:242/2110 train_time:8391ms step_avg:34.67ms
step:243/2110 train_time:8425ms step_avg:34.67ms
step:244/2110 train_time:8458ms step_avg:34.66ms
step:245/2110 train_time:8491ms step_avg:34.66ms
step:246/2110 train_time:8524ms step_avg:34.65ms
step:247/2110 train_time:8557ms step_avg:34.64ms
step:248/2110 train_time:8590ms step_avg:34.64ms
step:249/2110 train_time:8623ms step_avg:34.63ms
step:250/2110 train_time:8656ms step_avg:34.62ms
step:250/2110 val_loss:4.2981 train_time:8691ms step_avg:34.76ms
step:251/2110 train_time:8716ms step_avg:34.73ms
step:252/2110 train_time:8744ms step_avg:34.70ms
step:253/2110 train_time:8769ms step_avg:34.66ms
step:254/2110 train_time:8797ms step_avg:34.63ms
step:255/2110 train_time:8832ms step_avg:34.63ms
step:256/2110 train_time:8865ms step_avg:34.63ms
step:257/2110 train_time:8899ms step_avg:34.63ms
step:258/2110 train_time:8932ms step_avg:34.62ms
step:259/2110 train_time:8965ms step_avg:34.61ms
step:260/2110 train_time:8999ms step_avg:34.61ms
step:261/2110 train_time:9032ms step_avg:34.61ms
step:262/2110 train_time:9065ms step_avg:34.60ms
step:263/2110 train_time:9098ms step_avg:34.59ms
step:264/2110 train_time:9131ms step_avg:34.59ms
step:265/2110 train_time:9164ms step_avg:34.58ms
step:266/2110 train_time:9197ms step_avg:34.57ms
step:267/2110 train_time:9230ms step_avg:34.57ms
step:268/2110 train_time:9263ms step_avg:34.56ms
step:269/2110 train_time:9295ms step_avg:34.55ms
step:270/2110 train_time:9328ms step_avg:34.55ms
step:271/2110 train_time:9361ms step_avg:34.54ms
step:272/2110 train_time:9394ms step_avg:34.54ms
step:273/2110 train_time:9427ms step_avg:34.53ms
step:274/2110 train_time:9459ms step_avg:34.52ms
step:275/2110 train_time:9492ms step_avg:34.52ms
step:276/2110 train_time:9525ms step_avg:34.51ms
step:277/2110 train_time:9557ms step_avg:34.50ms
step:278/2110 train_time:9590ms step_avg:34.50ms
step:279/2110 train_time:9623ms step_avg:34.49ms
step:280/2110 train_time:9656ms step_avg:34.48ms
step:281/2110 train_time:9689ms step_avg:34.48ms
step:282/2110 train_time:9722ms step_avg:34.48ms
step:283/2110 train_time:9756ms step_avg:34.47ms
step:284/2110 train_time:9790ms step_avg:34.47ms
step:285/2110 train_time:9823ms step_avg:34.47ms
step:286/2110 train_time:9856ms step_avg:34.46ms
step:287/2110 train_time:9890ms step_avg:34.46ms
step:288/2110 train_time:9923ms step_avg:34.46ms
step:289/2110 train_time:9957ms step_avg:34.45ms
step:290/2110 train_time:9989ms step_avg:34.45ms
step:291/2110 train_time:10023ms step_avg:34.44ms
step:292/2110 train_time:10056ms step_avg:34.44ms
step:293/2110 train_time:10089ms step_avg:34.43ms
step:294/2110 train_time:10122ms step_avg:34.43ms
step:295/2110 train_time:10155ms step_avg:34.42ms
step:296/2110 train_time:10188ms step_avg:34.42ms
step:297/2110 train_time:10221ms step_avg:34.41ms
step:298/2110 train_time:10253ms step_avg:34.41ms
step:299/2110 train_time:10286ms step_avg:34.40ms
step:300/2110 train_time:10319ms step_avg:34.40ms
step:301/2110 train_time:10352ms step_avg:34.39ms
step:302/2110 train_time:10384ms step_avg:34.38ms
step:303/2110 train_time:10417ms step_avg:34.38ms
step:304/2110 train_time:10450ms step_avg:34.37ms
step:305/2110 train_time:10483ms step_avg:34.37ms
step:306/2110 train_time:10516ms step_avg:34.37ms
step:307/2110 train_time:10549ms step_avg:34.36ms
step:308/2110 train_time:10582ms step_avg:34.36ms
step:309/2110 train_time:10615ms step_avg:34.35ms
step:310/2110 train_time:10647ms step_avg:34.35ms
step:311/2110 train_time:10681ms step_avg:34.34ms
step:312/2110 train_time:10713ms step_avg:34.34ms
step:313/2110 train_time:10747ms step_avg:34.34ms
step:314/2110 train_time:10780ms step_avg:34.33ms
step:315/2110 train_time:10813ms step_avg:34.33ms
step:316/2110 train_time:10846ms step_avg:34.32ms
step:317/2110 train_time:10879ms step_avg:34.32ms
step:318/2110 train_time:10912ms step_avg:34.31ms
step:319/2110 train_time:10945ms step_avg:34.31ms
step:320/2110 train_time:10978ms step_avg:34.31ms
step:321/2110 train_time:11011ms step_avg:34.30ms
step:322/2110 train_time:11045ms step_avg:34.30ms
step:323/2110 train_time:11078ms step_avg:34.30ms
step:324/2110 train_time:11111ms step_avg:34.29ms
step:325/2110 train_time:11144ms step_avg:34.29ms
step:326/2110 train_time:11177ms step_avg:34.28ms
step:327/2110 train_time:11210ms step_avg:34.28ms
step:328/2110 train_time:11243ms step_avg:34.28ms
step:329/2110 train_time:11276ms step_avg:34.27ms
step:330/2110 train_time:11309ms step_avg:34.27ms
step:331/2110 train_time:11342ms step_avg:34.27ms
step:332/2110 train_time:11375ms step_avg:34.26ms
step:333/2110 train_time:11408ms step_avg:34.26ms
step:334/2110 train_time:11440ms step_avg:34.25ms
step:335/2110 train_time:11474ms step_avg:34.25ms
step:336/2110 train_time:11506ms step_avg:34.25ms
step:337/2110 train_time:11540ms step_avg:34.24ms
step:338/2110 train_time:11572ms step_avg:34.24ms
step:339/2110 train_time:11605ms step_avg:34.23ms
step:340/2110 train_time:11638ms step_avg:34.23ms
step:341/2110 train_time:11671ms step_avg:34.23ms
step:342/2110 train_time:11704ms step_avg:34.22ms
step:343/2110 train_time:11737ms step_avg:34.22ms
step:344/2110 train_time:11770ms step_avg:34.21ms
step:345/2110 train_time:11803ms step_avg:34.21ms
step:346/2110 train_time:11836ms step_avg:34.21ms
step:347/2110 train_time:11869ms step_avg:34.21ms
step:348/2110 train_time:11902ms step_avg:34.20ms
step:349/2110 train_time:11936ms step_avg:34.20ms
step:350/2110 train_time:11968ms step_avg:34.20ms
step:351/2110 train_time:12002ms step_avg:34.19ms
step:352/2110 train_time:12035ms step_avg:34.19ms
step:353/2110 train_time:12068ms step_avg:34.19ms
step:354/2110 train_time:12101ms step_avg:34.18ms
step:355/2110 train_time:12134ms step_avg:34.18ms
step:356/2110 train_time:12166ms step_avg:34.18ms
step:357/2110 train_time:12200ms step_avg:34.17ms
step:358/2110 train_time:12233ms step_avg:34.17ms
step:359/2110 train_time:12266ms step_avg:34.17ms
step:360/2110 train_time:12298ms step_avg:34.16ms
step:361/2110 train_time:12332ms step_avg:34.16ms
step:362/2110 train_time:12365ms step_avg:34.16ms
step:363/2110 train_time:12398ms step_avg:34.15ms
step:364/2110 train_time:12430ms step_avg:34.15ms
step:365/2110 train_time:12463ms step_avg:34.15ms
step:366/2110 train_time:12496ms step_avg:34.14ms
step:367/2110 train_time:12529ms step_avg:34.14ms
step:368/2110 train_time:12562ms step_avg:34.14ms
step:369/2110 train_time:12595ms step_avg:34.13ms
step:370/2110 train_time:12628ms step_avg:34.13ms
step:371/2110 train_time:12661ms step_avg:34.13ms
step:372/2110 train_time:12694ms step_avg:34.12ms
step:373/2110 train_time:12727ms step_avg:34.12ms
step:374/2110 train_time:12761ms step_avg:34.12ms
step:375/2110 train_time:12794ms step_avg:34.12ms
step:376/2110 train_time:12827ms step_avg:34.11ms
step:377/2110 train_time:12860ms step_avg:34.11ms
step:378/2110 train_time:12892ms step_avg:34.11ms
step:379/2110 train_time:12926ms step_avg:34.10ms
step:380/2110 train_time:12959ms step_avg:34.10ms
step:381/2110 train_time:12992ms step_avg:34.10ms
step:382/2110 train_time:13025ms step_avg:34.10ms
step:383/2110 train_time:13058ms step_avg:34.09ms
step:384/2110 train_time:13091ms step_avg:34.09ms
step:385/2110 train_time:13124ms step_avg:34.09ms
step:386/2110 train_time:13157ms step_avg:34.09ms
step:387/2110 train_time:13190ms step_avg:34.08ms
step:388/2110 train_time:13223ms step_avg:34.08ms
step:389/2110 train_time:13256ms step_avg:34.08ms
step:390/2110 train_time:13289ms step_avg:34.08ms
step:391/2110 train_time:13322ms step_avg:34.07ms
step:392/2110 train_time:13355ms step_avg:34.07ms
step:393/2110 train_time:13388ms step_avg:34.07ms
step:394/2110 train_time:13421ms step_avg:34.06ms
step:395/2110 train_time:13455ms step_avg:34.06ms
step:396/2110 train_time:13488ms step_avg:34.06ms
step:397/2110 train_time:13521ms step_avg:34.06ms
step:398/2110 train_time:13554ms step_avg:34.06ms
step:399/2110 train_time:13587ms step_avg:34.05ms
step:400/2110 train_time:13619ms step_avg:34.05ms
step:401/2110 train_time:13652ms step_avg:34.05ms
step:402/2110 train_time:13685ms step_avg:34.04ms
step:403/2110 train_time:13719ms step_avg:34.04ms
step:404/2110 train_time:13751ms step_avg:34.04ms
step:405/2110 train_time:13784ms step_avg:34.04ms
step:406/2110 train_time:13817ms step_avg:34.03ms
step:407/2110 train_time:13850ms step_avg:34.03ms
step:408/2110 train_time:13883ms step_avg:34.03ms
step:409/2110 train_time:13916ms step_avg:34.02ms
step:410/2110 train_time:13949ms step_avg:34.02ms
step:411/2110 train_time:13982ms step_avg:34.02ms
step:412/2110 train_time:14015ms step_avg:34.02ms
step:413/2110 train_time:14048ms step_avg:34.02ms
step:414/2110 train_time:14081ms step_avg:34.01ms
step:415/2110 train_time:14114ms step_avg:34.01ms
step:416/2110 train_time:14147ms step_avg:34.01ms
step:417/2110 train_time:14180ms step_avg:34.01ms
step:418/2110 train_time:14213ms step_avg:34.00ms
step:419/2110 train_time:14247ms step_avg:34.00ms
step:420/2110 train_time:14279ms step_avg:34.00ms
step:421/2110 train_time:14312ms step_avg:34.00ms
step:422/2110 train_time:14345ms step_avg:33.99ms
step:423/2110 train_time:14378ms step_avg:33.99ms
step:424/2110 train_time:14411ms step_avg:33.99ms
step:425/2110 train_time:14444ms step_avg:33.99ms
step:426/2110 train_time:14477ms step_avg:33.98ms
step:427/2110 train_time:14510ms step_avg:33.98ms
step:428/2110 train_time:14543ms step_avg:33.98ms
step:429/2110 train_time:14576ms step_avg:33.98ms
step:430/2110 train_time:14610ms step_avg:33.98ms
step:431/2110 train_time:14642ms step_avg:33.97ms
step:432/2110 train_time:14675ms step_avg:33.97ms
step:433/2110 train_time:14708ms step_avg:33.97ms
step:434/2110 train_time:14742ms step_avg:33.97ms
step:435/2110 train_time:14774ms step_avg:33.96ms
step:436/2110 train_time:14807ms step_avg:33.96ms
step:437/2110 train_time:14840ms step_avg:33.96ms
step:438/2110 train_time:14873ms step_avg:33.96ms
step:439/2110 train_time:14906ms step_avg:33.95ms
step:440/2110 train_time:14939ms step_avg:33.95ms
step:441/2110 train_time:14972ms step_avg:33.95ms
step:442/2110 train_time:15005ms step_avg:33.95ms
step:443/2110 train_time:15038ms step_avg:33.95ms
step:444/2110 train_time:15071ms step_avg:33.94ms
step:445/2110 train_time:15104ms step_avg:33.94ms
step:446/2110 train_time:15137ms step_avg:33.94ms
step:447/2110 train_time:15170ms step_avg:33.94ms
step:448/2110 train_time:15203ms step_avg:33.94ms
step:449/2110 train_time:15237ms step_avg:33.93ms
step:450/2110 train_time:15270ms step_avg:33.93ms
step:451/2110 train_time:15303ms step_avg:33.93ms
step:452/2110 train_time:15336ms step_avg:33.93ms
step:453/2110 train_time:15369ms step_avg:33.93ms
step:454/2110 train_time:15401ms step_avg:33.92ms
step:455/2110 train_time:15434ms step_avg:33.92ms
step:456/2110 train_time:15467ms step_avg:33.92ms
step:457/2110 train_time:15500ms step_avg:33.92ms
step:458/2110 train_time:15534ms step_avg:33.92ms
step:459/2110 train_time:15566ms step_avg:33.91ms
step:460/2110 train_time:15600ms step_avg:33.91ms
step:461/2110 train_time:15632ms step_avg:33.91ms
step:462/2110 train_time:15665ms step_avg:33.91ms
step:463/2110 train_time:15699ms step_avg:33.91ms
step:464/2110 train_time:15731ms step_avg:33.90ms
step:465/2110 train_time:15765ms step_avg:33.90ms
step:466/2110 train_time:15798ms step_avg:33.90ms
step:467/2110 train_time:15831ms step_avg:33.90ms
step:468/2110 train_time:15864ms step_avg:33.90ms
step:469/2110 train_time:15897ms step_avg:33.90ms
step:470/2110 train_time:15930ms step_avg:33.89ms
step:471/2110 train_time:15963ms step_avg:33.89ms
step:472/2110 train_time:15995ms step_avg:33.89ms
step:473/2110 train_time:16029ms step_avg:33.89ms
step:474/2110 train_time:16062ms step_avg:33.89ms
step:475/2110 train_time:16095ms step_avg:33.88ms
step:476/2110 train_time:16127ms step_avg:33.88ms
step:477/2110 train_time:16161ms step_avg:33.88ms
step:478/2110 train_time:16194ms step_avg:33.88ms
step:479/2110 train_time:16227ms step_avg:33.88ms
step:480/2110 train_time:16260ms step_avg:33.88ms
step:481/2110 train_time:16293ms step_avg:33.87ms
step:482/2110 train_time:16326ms step_avg:33.87ms
step:483/2110 train_time:16359ms step_avg:33.87ms
step:484/2110 train_time:16392ms step_avg:33.87ms
step:485/2110 train_time:16425ms step_avg:33.87ms
step:486/2110 train_time:16457ms step_avg:33.86ms
step:487/2110 train_time:16491ms step_avg:33.86ms
step:488/2110 train_time:16524ms step_avg:33.86ms
step:489/2110 train_time:16557ms step_avg:33.86ms
step:490/2110 train_time:16589ms step_avg:33.86ms
step:491/2110 train_time:16623ms step_avg:33.85ms
step:492/2110 train_time:16655ms step_avg:33.85ms
step:493/2110 train_time:16689ms step_avg:33.85ms
step:494/2110 train_time:16722ms step_avg:33.85ms
step:495/2110 train_time:16755ms step_avg:33.85ms
step:496/2110 train_time:16789ms step_avg:33.85ms
step:497/2110 train_time:16822ms step_avg:33.85ms
step:498/2110 train_time:16854ms step_avg:33.84ms
step:499/2110 train_time:16888ms step_avg:33.84ms
step:500/2110 train_time:16920ms step_avg:33.84ms
step:500/2110 val_loss:4.0317 train_time:16956ms step_avg:33.91ms
step:501/2110 train_time:16989ms step_avg:33.91ms
step:502/2110 train_time:17019ms step_avg:33.90ms
step:503/2110 train_time:17047ms step_avg:33.89ms
step:504/2110 train_time:17079ms step_avg:33.89ms
step:505/2110 train_time:17105ms step_avg:33.87ms
step:506/2110 train_time:17136ms step_avg:33.87ms
step:507/2110 train_time:17164ms step_avg:33.85ms
step:508/2110 train_time:17197ms step_avg:33.85ms
step:509/2110 train_time:17231ms step_avg:33.85ms
step:510/2110 train_time:17264ms step_avg:33.85ms
step:511/2110 train_time:17297ms step_avg:33.85ms
step:512/2110 train_time:17329ms step_avg:33.85ms
step:513/2110 train_time:17363ms step_avg:33.85ms
step:514/2110 train_time:17395ms step_avg:33.84ms
step:515/2110 train_time:17428ms step_avg:33.84ms
step:516/2110 train_time:17461ms step_avg:33.84ms
step:517/2110 train_time:17493ms step_avg:33.84ms
step:518/2110 train_time:17527ms step_avg:33.84ms
step:519/2110 train_time:17559ms step_avg:33.83ms
step:520/2110 train_time:17592ms step_avg:33.83ms
step:521/2110 train_time:17624ms step_avg:33.83ms
step:522/2110 train_time:17657ms step_avg:33.82ms
step:523/2110 train_time:17690ms step_avg:33.82ms
step:524/2110 train_time:17722ms step_avg:33.82ms
step:525/2110 train_time:17755ms step_avg:33.82ms
step:526/2110 train_time:17788ms step_avg:33.82ms
step:527/2110 train_time:17821ms step_avg:33.82ms
step:528/2110 train_time:17854ms step_avg:33.81ms
step:529/2110 train_time:17887ms step_avg:33.81ms
step:530/2110 train_time:17920ms step_avg:33.81ms
step:531/2110 train_time:17953ms step_avg:33.81ms
step:532/2110 train_time:17987ms step_avg:33.81ms
step:533/2110 train_time:18020ms step_avg:33.81ms
step:534/2110 train_time:18053ms step_avg:33.81ms
step:535/2110 train_time:18087ms step_avg:33.81ms
step:536/2110 train_time:18120ms step_avg:33.81ms
step:537/2110 train_time:18153ms step_avg:33.81ms
step:538/2110 train_time:18186ms step_avg:33.80ms
step:539/2110 train_time:18220ms step_avg:33.80ms
step:540/2110 train_time:18253ms step_avg:33.80ms
step:541/2110 train_time:18286ms step_avg:33.80ms
step:542/2110 train_time:18319ms step_avg:33.80ms
step:543/2110 train_time:18352ms step_avg:33.80ms
step:544/2110 train_time:18385ms step_avg:33.80ms
step:545/2110 train_time:18418ms step_avg:33.79ms
step:546/2110 train_time:18451ms step_avg:33.79ms
step:547/2110 train_time:18484ms step_avg:33.79ms
step:548/2110 train_time:18517ms step_avg:33.79ms
step:549/2110 train_time:18550ms step_avg:33.79ms
step:550/2110 train_time:18583ms step_avg:33.79ms
step:551/2110 train_time:18616ms step_avg:33.78ms
step:552/2110 train_time:18649ms step_avg:33.78ms
step:553/2110 train_time:18681ms step_avg:33.78ms
step:554/2110 train_time:18716ms step_avg:33.78ms
step:555/2110 train_time:18748ms step_avg:33.78ms
step:556/2110 train_time:18780ms step_avg:33.78ms
step:557/2110 train_time:18813ms step_avg:33.78ms
step:558/2110 train_time:18845ms step_avg:33.77ms
step:559/2110 train_time:18879ms step_avg:33.77ms
step:560/2110 train_time:18911ms step_avg:33.77ms
step:561/2110 train_time:18945ms step_avg:33.77ms
step:562/2110 train_time:18977ms step_avg:33.77ms
step:563/2110 train_time:19011ms step_avg:33.77ms
step:564/2110 train_time:19044ms step_avg:33.77ms
step:565/2110 train_time:19077ms step_avg:33.76ms
step:566/2110 train_time:19110ms step_avg:33.76ms
step:567/2110 train_time:19143ms step_avg:33.76ms
step:568/2110 train_time:19177ms step_avg:33.76ms
step:569/2110 train_time:19210ms step_avg:33.76ms
step:570/2110 train_time:19242ms step_avg:33.76ms
step:571/2110 train_time:19277ms step_avg:33.76ms
step:572/2110 train_time:19309ms step_avg:33.76ms
step:573/2110 train_time:19342ms step_avg:33.76ms
step:574/2110 train_time:19375ms step_avg:33.75ms
step:575/2110 train_time:19408ms step_avg:33.75ms
step:576/2110 train_time:19441ms step_avg:33.75ms
step:577/2110 train_time:19474ms step_avg:33.75ms
step:578/2110 train_time:19506ms step_avg:33.75ms
step:579/2110 train_time:19540ms step_avg:33.75ms
step:580/2110 train_time:19573ms step_avg:33.75ms
step:581/2110 train_time:19606ms step_avg:33.75ms
step:582/2110 train_time:19639ms step_avg:33.74ms
step:583/2110 train_time:19672ms step_avg:33.74ms
step:584/2110 train_time:19705ms step_avg:33.74ms
step:585/2110 train_time:19738ms step_avg:33.74ms
step:586/2110 train_time:19771ms step_avg:33.74ms
step:587/2110 train_time:19805ms step_avg:33.74ms
step:588/2110 train_time:19837ms step_avg:33.74ms
step:589/2110 train_time:19870ms step_avg:33.74ms
step:590/2110 train_time:19904ms step_avg:33.74ms
step:591/2110 train_time:19936ms step_avg:33.73ms
step:592/2110 train_time:19969ms step_avg:33.73ms
step:593/2110 train_time:20002ms step_avg:33.73ms
step:594/2110 train_time:20035ms step_avg:33.73ms
step:595/2110 train_time:20068ms step_avg:33.73ms
step:596/2110 train_time:20101ms step_avg:33.73ms
step:597/2110 train_time:20134ms step_avg:33.72ms
step:598/2110 train_time:20167ms step_avg:33.72ms
step:599/2110 train_time:20201ms step_avg:33.72ms
step:600/2110 train_time:20233ms step_avg:33.72ms
step:601/2110 train_time:20267ms step_avg:33.72ms
step:602/2110 train_time:20300ms step_avg:33.72ms
step:603/2110 train_time:20333ms step_avg:33.72ms
step:604/2110 train_time:20366ms step_avg:33.72ms
step:605/2110 train_time:20399ms step_avg:33.72ms
step:606/2110 train_time:20432ms step_avg:33.72ms
step:607/2110 train_time:20465ms step_avg:33.71ms
step:608/2110 train_time:20497ms step_avg:33.71ms
step:609/2110 train_time:20530ms step_avg:33.71ms
step:610/2110 train_time:20563ms step_avg:33.71ms
step:611/2110 train_time:20597ms step_avg:33.71ms
step:612/2110 train_time:20629ms step_avg:33.71ms
step:613/2110 train_time:20663ms step_avg:33.71ms
step:614/2110 train_time:20695ms step_avg:33.71ms
step:615/2110 train_time:20729ms step_avg:33.70ms
step:616/2110 train_time:20762ms step_avg:33.70ms
step:617/2110 train_time:20795ms step_avg:33.70ms
step:618/2110 train_time:20827ms step_avg:33.70ms
step:619/2110 train_time:20860ms step_avg:33.70ms
step:620/2110 train_time:20894ms step_avg:33.70ms
step:621/2110 train_time:20927ms step_avg:33.70ms
step:622/2110 train_time:20960ms step_avg:33.70ms
step:623/2110 train_time:20993ms step_avg:33.70ms
step:624/2110 train_time:21026ms step_avg:33.70ms
step:625/2110 train_time:21059ms step_avg:33.69ms
step:626/2110 train_time:21092ms step_avg:33.69ms
step:627/2110 train_time:21125ms step_avg:33.69ms
step:628/2110 train_time:21157ms step_avg:33.69ms
step:629/2110 train_time:21190ms step_avg:33.69ms
step:630/2110 train_time:21225ms step_avg:33.69ms
step:631/2110 train_time:21257ms step_avg:33.69ms
step:632/2110 train_time:21306ms step_avg:33.71ms
step:633/2110 train_time:21333ms step_avg:33.70ms
step:634/2110 train_time:21361ms step_avg:33.69ms
step:635/2110 train_time:21389ms step_avg:33.68ms
step:636/2110 train_time:21421ms step_avg:33.68ms
step:637/2110 train_time:21455ms step_avg:33.68ms
step:638/2110 train_time:21489ms step_avg:33.68ms
step:639/2110 train_time:21521ms step_avg:33.68ms
step:640/2110 train_time:21554ms step_avg:33.68ms
step:641/2110 train_time:21587ms step_avg:33.68ms
step:642/2110 train_time:21620ms step_avg:33.68ms
step:643/2110 train_time:21653ms step_avg:33.67ms
step:644/2110 train_time:21686ms step_avg:33.67ms
step:645/2110 train_time:21719ms step_avg:33.67ms
step:646/2110 train_time:21751ms step_avg:33.67ms
step:647/2110 train_time:21784ms step_avg:33.67ms
step:648/2110 train_time:21817ms step_avg:33.67ms
step:649/2110 train_time:21850ms step_avg:33.67ms
step:650/2110 train_time:21883ms step_avg:33.67ms
step:651/2110 train_time:21917ms step_avg:33.67ms
step:652/2110 train_time:21950ms step_avg:33.67ms
step:653/2110 train_time:21983ms step_avg:33.66ms
step:654/2110 train_time:22016ms step_avg:33.66ms
step:655/2110 train_time:22049ms step_avg:33.66ms
step:656/2110 train_time:22081ms step_avg:33.66ms
step:657/2110 train_time:22114ms step_avg:33.66ms
step:658/2110 train_time:22147ms step_avg:33.66ms
step:659/2110 train_time:22180ms step_avg:33.66ms
step:660/2110 train_time:22213ms step_avg:33.66ms
step:661/2110 train_time:22246ms step_avg:33.65ms
step:662/2110 train_time:22278ms step_avg:33.65ms
step:663/2110 train_time:22312ms step_avg:33.65ms
step:664/2110 train_time:22345ms step_avg:33.65ms
step:665/2110 train_time:22378ms step_avg:33.65ms
step:666/2110 train_time:22412ms step_avg:33.65ms
step:667/2110 train_time:22444ms step_avg:33.65ms
step:668/2110 train_time:22477ms step_avg:33.65ms
step:669/2110 train_time:22510ms step_avg:33.65ms
step:670/2110 train_time:22543ms step_avg:33.65ms
step:671/2110 train_time:22576ms step_avg:33.64ms
step:672/2110 train_time:22609ms step_avg:33.64ms
step:673/2110 train_time:22642ms step_avg:33.64ms
step:674/2110 train_time:22675ms step_avg:33.64ms
step:675/2110 train_time:22708ms step_avg:33.64ms
step:676/2110 train_time:22741ms step_avg:33.64ms
step:677/2110 train_time:22774ms step_avg:33.64ms
step:678/2110 train_time:22807ms step_avg:33.64ms
step:679/2110 train_time:22840ms step_avg:33.64ms
step:680/2110 train_time:22872ms step_avg:33.64ms
step:681/2110 train_time:22906ms step_avg:33.64ms
step:682/2110 train_time:22938ms step_avg:33.63ms
step:683/2110 train_time:22971ms step_avg:33.63ms
step:684/2110 train_time:23004ms step_avg:33.63ms
step:685/2110 train_time:23038ms step_avg:33.63ms
step:686/2110 train_time:23071ms step_avg:33.63ms
step:687/2110 train_time:23104ms step_avg:33.63ms
step:688/2110 train_time:23137ms step_avg:33.63ms
step:689/2110 train_time:23170ms step_avg:33.63ms
step:690/2110 train_time:23202ms step_avg:33.63ms
step:691/2110 train_time:23236ms step_avg:33.63ms
step:692/2110 train_time:23294ms step_avg:33.66ms
step:693/2110 train_time:23354ms step_avg:33.70ms
step:694/2110 train_time:23413ms step_avg:33.74ms
step:695/2110 train_time:23473ms step_avg:33.77ms
step:696/2110 train_time:23531ms step_avg:33.81ms
step:697/2110 train_time:23591ms step_avg:33.85ms
step:698/2110 train_time:23650ms step_avg:33.88ms
step:699/2110 train_time:23710ms step_avg:33.92ms
step:700/2110 train_time:23770ms step_avg:33.96ms
step:701/2110 train_time:23830ms step_avg:33.99ms
step:702/2110 train_time:23888ms step_avg:34.03ms
step:703/2110 train_time:23948ms step_avg:34.07ms
step:704/2110 train_time:24007ms step_avg:34.10ms
step:705/2110 train_time:24067ms step_avg:34.14ms
step:706/2110 train_time:24126ms step_avg:34.17ms
step:707/2110 train_time:24186ms step_avg:34.21ms
step:708/2110 train_time:24244ms step_avg:34.24ms
step:709/2110 train_time:24304ms step_avg:34.28ms
step:710/2110 train_time:24362ms step_avg:34.31ms
step:711/2110 train_time:24421ms step_avg:34.35ms
step:712/2110 train_time:24480ms step_avg:34.38ms
step:713/2110 train_time:24539ms step_avg:34.42ms
step:714/2110 train_time:24597ms step_avg:34.45ms
step:715/2110 train_time:24657ms step_avg:34.48ms
step:716/2110 train_time:24715ms step_avg:34.52ms
step:717/2110 train_time:24775ms step_avg:34.55ms
step:718/2110 train_time:24834ms step_avg:34.59ms
step:719/2110 train_time:24894ms step_avg:34.62ms
step:720/2110 train_time:24952ms step_avg:34.66ms
step:721/2110 train_time:25012ms step_avg:34.69ms
step:722/2110 train_time:25071ms step_avg:34.72ms
step:723/2110 train_time:25131ms step_avg:34.76ms
step:724/2110 train_time:25189ms step_avg:34.79ms
step:725/2110 train_time:25249ms step_avg:34.83ms
step:726/2110 train_time:25308ms step_avg:34.86ms
step:727/2110 train_time:25368ms step_avg:34.89ms
step:728/2110 train_time:25426ms step_avg:34.93ms
step:729/2110 train_time:25486ms step_avg:34.96ms
step:730/2110 train_time:25545ms step_avg:34.99ms
step:731/2110 train_time:25605ms step_avg:35.03ms
step:732/2110 train_time:25663ms step_avg:35.06ms
step:733/2110 train_time:25722ms step_avg:35.09ms
step:734/2110 train_time:25781ms step_avg:35.12ms
step:735/2110 train_time:25840ms step_avg:35.16ms
step:736/2110 train_time:25901ms step_avg:35.19ms
step:737/2110 train_time:25959ms step_avg:35.22ms
step:738/2110 train_time:26018ms step_avg:35.25ms
step:739/2110 train_time:26078ms step_avg:35.29ms
step:740/2110 train_time:26136ms step_avg:35.32ms
step:741/2110 train_time:26196ms step_avg:35.35ms
step:742/2110 train_time:26255ms step_avg:35.38ms
step:743/2110 train_time:26314ms step_avg:35.42ms
step:744/2110 train_time:26373ms step_avg:35.45ms
step:745/2110 train_time:26433ms step_avg:35.48ms
step:746/2110 train_time:26493ms step_avg:35.51ms
step:747/2110 train_time:26552ms step_avg:35.54ms
step:748/2110 train_time:26612ms step_avg:35.58ms
step:749/2110 train_time:26671ms step_avg:35.61ms
step:750/2110 train_time:26733ms step_avg:35.64ms
step:750/2110 val_loss:3.9100 train_time:26792ms step_avg:35.72ms
step:751/2110 train_time:26814ms step_avg:35.70ms
step:752/2110 train_time:26853ms step_avg:35.71ms
step:753/2110 train_time:26915ms step_avg:35.74ms
step:754/2110 train_time:26974ms step_avg:35.77ms
step:755/2110 train_time:27034ms step_avg:35.81ms
step:756/2110 train_time:27093ms step_avg:35.84ms
step:757/2110 train_time:27152ms step_avg:35.87ms
step:758/2110 train_time:27209ms step_avg:35.90ms
step:759/2110 train_time:27268ms step_avg:35.93ms
step:760/2110 train_time:27326ms step_avg:35.95ms
step:761/2110 train_time:27385ms step_avg:35.98ms
step:762/2110 train_time:27442ms step_avg:36.01ms
step:763/2110 train_time:27501ms step_avg:36.04ms
step:764/2110 train_time:27559ms step_avg:36.07ms
step:765/2110 train_time:27619ms step_avg:36.10ms
step:766/2110 train_time:27677ms step_avg:36.13ms
step:767/2110 train_time:27738ms step_avg:36.16ms
step:768/2110 train_time:27798ms step_avg:36.19ms
step:769/2110 train_time:27860ms step_avg:36.23ms
step:770/2110 train_time:27920ms step_avg:36.26ms
step:771/2110 train_time:27982ms step_avg:36.29ms
step:772/2110 train_time:28041ms step_avg:36.32ms
step:773/2110 train_time:28101ms step_avg:36.35ms
step:774/2110 train_time:28159ms step_avg:36.38ms
step:775/2110 train_time:28218ms step_avg:36.41ms
step:776/2110 train_time:28276ms step_avg:36.44ms
step:777/2110 train_time:28335ms step_avg:36.47ms
step:778/2110 train_time:28392ms step_avg:36.49ms
step:779/2110 train_time:28452ms step_avg:36.52ms
step:780/2110 train_time:28509ms step_avg:36.55ms
step:781/2110 train_time:28569ms step_avg:36.58ms
step:782/2110 train_time:28627ms step_avg:36.61ms
step:783/2110 train_time:28688ms step_avg:36.64ms
step:784/2110 train_time:28747ms step_avg:36.67ms
step:785/2110 train_time:28809ms step_avg:36.70ms
step:786/2110 train_time:28869ms step_avg:36.73ms
step:787/2110 train_time:28931ms step_avg:36.76ms
step:788/2110 train_time:28992ms step_avg:36.79ms
step:789/2110 train_time:29050ms step_avg:36.82ms
step:790/2110 train_time:29108ms step_avg:36.85ms
step:791/2110 train_time:29168ms step_avg:36.87ms
step:792/2110 train_time:29227ms step_avg:36.90ms
step:793/2110 train_time:29286ms step_avg:36.93ms
step:794/2110 train_time:29345ms step_avg:36.96ms
step:795/2110 train_time:29403ms step_avg:36.98ms
step:796/2110 train_time:29462ms step_avg:37.01ms
step:797/2110 train_time:29521ms step_avg:37.04ms
step:798/2110 train_time:29580ms step_avg:37.07ms
step:799/2110 train_time:29637ms step_avg:37.09ms
step:800/2110 train_time:29696ms step_avg:37.12ms
step:801/2110 train_time:29754ms step_avg:37.15ms
step:802/2110 train_time:29813ms step_avg:37.17ms
step:803/2110 train_time:29874ms step_avg:37.20ms
step:804/2110 train_time:29933ms step_avg:37.23ms
step:805/2110 train_time:29993ms step_avg:37.26ms
step:806/2110 train_time:30052ms step_avg:37.28ms
step:807/2110 train_time:30112ms step_avg:37.31ms
step:808/2110 train_time:30171ms step_avg:37.34ms
step:809/2110 train_time:30230ms step_avg:37.37ms
step:810/2110 train_time:30288ms step_avg:37.39ms
step:811/2110 train_time:30347ms step_avg:37.42ms
step:812/2110 train_time:30407ms step_avg:37.45ms
step:813/2110 train_time:30465ms step_avg:37.47ms
step:814/2110 train_time:30524ms step_avg:37.50ms
step:815/2110 train_time:30584ms step_avg:37.53ms
step:816/2110 train_time:30643ms step_avg:37.55ms
step:817/2110 train_time:30703ms step_avg:37.58ms
step:818/2110 train_time:30763ms step_avg:37.61ms
step:819/2110 train_time:30822ms step_avg:37.63ms
step:820/2110 train_time:30881ms step_avg:37.66ms
step:821/2110 train_time:30942ms step_avg:37.69ms
step:822/2110 train_time:31003ms step_avg:37.72ms
step:823/2110 train_time:31061ms step_avg:37.74ms
step:824/2110 train_time:31121ms step_avg:37.77ms
step:825/2110 train_time:31180ms step_avg:37.79ms
step:826/2110 train_time:31238ms step_avg:37.82ms
step:827/2110 train_time:31298ms step_avg:37.85ms
step:828/2110 train_time:31356ms step_avg:37.87ms
step:829/2110 train_time:31416ms step_avg:37.90ms
step:830/2110 train_time:31474ms step_avg:37.92ms
step:831/2110 train_time:31533ms step_avg:37.95ms
step:832/2110 train_time:31591ms step_avg:37.97ms
step:833/2110 train_time:31651ms step_avg:38.00ms
step:834/2110 train_time:31709ms step_avg:38.02ms
step:835/2110 train_time:31768ms step_avg:38.05ms
step:836/2110 train_time:31828ms step_avg:38.07ms
step:837/2110 train_time:31887ms step_avg:38.10ms
step:838/2110 train_time:31950ms step_avg:38.13ms
step:839/2110 train_time:32007ms step_avg:38.15ms
step:840/2110 train_time:32066ms step_avg:38.17ms
step:841/2110 train_time:32125ms step_avg:38.20ms
step:842/2110 train_time:32184ms step_avg:38.22ms
step:843/2110 train_time:32244ms step_avg:38.25ms
step:844/2110 train_time:32302ms step_avg:38.27ms
step:845/2110 train_time:32361ms step_avg:38.30ms
step:846/2110 train_time:32421ms step_avg:38.32ms
step:847/2110 train_time:32480ms step_avg:38.35ms
step:848/2110 train_time:32541ms step_avg:38.37ms
step:849/2110 train_time:32598ms step_avg:38.40ms
step:850/2110 train_time:32656ms step_avg:38.42ms
step:851/2110 train_time:32716ms step_avg:38.44ms
step:852/2110 train_time:32774ms step_avg:38.47ms
step:853/2110 train_time:32833ms step_avg:38.49ms
step:854/2110 train_time:32892ms step_avg:38.52ms
step:855/2110 train_time:32951ms step_avg:38.54ms
step:856/2110 train_time:33010ms step_avg:38.56ms
step:857/2110 train_time:33069ms step_avg:38.59ms
step:858/2110 train_time:33129ms step_avg:38.61ms
step:859/2110 train_time:33187ms step_avg:38.63ms
step:860/2110 train_time:33247ms step_avg:38.66ms
step:861/2110 train_time:33306ms step_avg:38.68ms
step:862/2110 train_time:33365ms step_avg:38.71ms
step:863/2110 train_time:33425ms step_avg:38.73ms
step:864/2110 train_time:33485ms step_avg:38.76ms
step:865/2110 train_time:33543ms step_avg:38.78ms
step:866/2110 train_time:33603ms step_avg:38.80ms
step:867/2110 train_time:33661ms step_avg:38.83ms
step:868/2110 train_time:33720ms step_avg:38.85ms
step:869/2110 train_time:33781ms step_avg:38.87ms
step:870/2110 train_time:33839ms step_avg:38.90ms
step:871/2110 train_time:33900ms step_avg:38.92ms
step:872/2110 train_time:33959ms step_avg:38.94ms
step:873/2110 train_time:34018ms step_avg:38.97ms
step:874/2110 train_time:34077ms step_avg:38.99ms
step:875/2110 train_time:34136ms step_avg:39.01ms
step:876/2110 train_time:34195ms step_avg:39.04ms
step:877/2110 train_time:34254ms step_avg:39.06ms
step:878/2110 train_time:34314ms step_avg:39.08ms
step:879/2110 train_time:34374ms step_avg:39.11ms
step:880/2110 train_time:34431ms step_avg:39.13ms
step:881/2110 train_time:34491ms step_avg:39.15ms
step:882/2110 train_time:34549ms step_avg:39.17ms
step:883/2110 train_time:34609ms step_avg:39.19ms
step:884/2110 train_time:34669ms step_avg:39.22ms
step:885/2110 train_time:34727ms step_avg:39.24ms
step:886/2110 train_time:34786ms step_avg:39.26ms
step:887/2110 train_time:34845ms step_avg:39.28ms
step:888/2110 train_time:34905ms step_avg:39.31ms
step:889/2110 train_time:34964ms step_avg:39.33ms
step:890/2110 train_time:35025ms step_avg:39.35ms
step:891/2110 train_time:35083ms step_avg:39.38ms
step:892/2110 train_time:35142ms step_avg:39.40ms
step:893/2110 train_time:35202ms step_avg:39.42ms
step:894/2110 train_time:35262ms step_avg:39.44ms
step:895/2110 train_time:35321ms step_avg:39.47ms
step:896/2110 train_time:35380ms step_avg:39.49ms
step:897/2110 train_time:35439ms step_avg:39.51ms
step:898/2110 train_time:35498ms step_avg:39.53ms
step:899/2110 train_time:35558ms step_avg:39.55ms
step:900/2110 train_time:35616ms step_avg:39.57ms
step:901/2110 train_time:35675ms step_avg:39.59ms
step:902/2110 train_time:35733ms step_avg:39.62ms
step:903/2110 train_time:35792ms step_avg:39.64ms
step:904/2110 train_time:35853ms step_avg:39.66ms
step:905/2110 train_time:35910ms step_avg:39.68ms
step:906/2110 train_time:35969ms step_avg:39.70ms
step:907/2110 train_time:36027ms step_avg:39.72ms
step:908/2110 train_time:36086ms step_avg:39.74ms
step:909/2110 train_time:36146ms step_avg:39.76ms
step:910/2110 train_time:36206ms step_avg:39.79ms
step:911/2110 train_time:36266ms step_avg:39.81ms
step:912/2110 train_time:36325ms step_avg:39.83ms
step:913/2110 train_time:36384ms step_avg:39.85ms
step:914/2110 train_time:36444ms step_avg:39.87ms
step:915/2110 train_time:36503ms step_avg:39.89ms
step:916/2110 train_time:36562ms step_avg:39.92ms
step:917/2110 train_time:36622ms step_avg:39.94ms
step:918/2110 train_time:36681ms step_avg:39.96ms
step:919/2110 train_time:36740ms step_avg:39.98ms
step:920/2110 train_time:36799ms step_avg:40.00ms
step:921/2110 train_time:36858ms step_avg:40.02ms
step:922/2110 train_time:36917ms step_avg:40.04ms
step:923/2110 train_time:36977ms step_avg:40.06ms
step:924/2110 train_time:37034ms step_avg:40.08ms
step:925/2110 train_time:37095ms step_avg:40.10ms
step:926/2110 train_time:37153ms step_avg:40.12ms
step:927/2110 train_time:37213ms step_avg:40.14ms
step:928/2110 train_time:37272ms step_avg:40.16ms
step:929/2110 train_time:37332ms step_avg:40.19ms
step:930/2110 train_time:37390ms step_avg:40.20ms
step:931/2110 train_time:37449ms step_avg:40.22ms
step:932/2110 train_time:37508ms step_avg:40.25ms
step:933/2110 train_time:37569ms step_avg:40.27ms
step:934/2110 train_time:37628ms step_avg:40.29ms
step:935/2110 train_time:37685ms step_avg:40.31ms
step:936/2110 train_time:37745ms step_avg:40.33ms
step:937/2110 train_time:37804ms step_avg:40.35ms
step:938/2110 train_time:37865ms step_avg:40.37ms
step:939/2110 train_time:37923ms step_avg:40.39ms
step:940/2110 train_time:37983ms step_avg:40.41ms
step:941/2110 train_time:38042ms step_avg:40.43ms
step:942/2110 train_time:38101ms step_avg:40.45ms
step:943/2110 train_time:38162ms step_avg:40.47ms
step:944/2110 train_time:38221ms step_avg:40.49ms
step:945/2110 train_time:38280ms step_avg:40.51ms
step:946/2110 train_time:38340ms step_avg:40.53ms
step:947/2110 train_time:38398ms step_avg:40.55ms
step:948/2110 train_time:38459ms step_avg:40.57ms
step:949/2110 train_time:38517ms step_avg:40.59ms
step:950/2110 train_time:38575ms step_avg:40.61ms
step:951/2110 train_time:38635ms step_avg:40.63ms
step:952/2110 train_time:38693ms step_avg:40.64ms
step:953/2110 train_time:38753ms step_avg:40.66ms
step:954/2110 train_time:38810ms step_avg:40.68ms
step:955/2110 train_time:38872ms step_avg:40.70ms
step:956/2110 train_time:38930ms step_avg:40.72ms
step:957/2110 train_time:38989ms step_avg:40.74ms
step:958/2110 train_time:39050ms step_avg:40.76ms
step:959/2110 train_time:39108ms step_avg:40.78ms
step:960/2110 train_time:39167ms step_avg:40.80ms
step:961/2110 train_time:39226ms step_avg:40.82ms
step:962/2110 train_time:39285ms step_avg:40.84ms
step:963/2110 train_time:39345ms step_avg:40.86ms
step:964/2110 train_time:39406ms step_avg:40.88ms
step:965/2110 train_time:39464ms step_avg:40.90ms
step:966/2110 train_time:39523ms step_avg:40.91ms
step:967/2110 train_time:39583ms step_avg:40.93ms
step:968/2110 train_time:39643ms step_avg:40.95ms
step:969/2110 train_time:39703ms step_avg:40.97ms
step:970/2110 train_time:39761ms step_avg:40.99ms
step:971/2110 train_time:39821ms step_avg:41.01ms
step:972/2110 train_time:39880ms step_avg:41.03ms
step:973/2110 train_time:39940ms step_avg:41.05ms
step:974/2110 train_time:39999ms step_avg:41.07ms
step:975/2110 train_time:40056ms step_avg:41.08ms
step:976/2110 train_time:40114ms step_avg:41.10ms
step:977/2110 train_time:40174ms step_avg:41.12ms
step:978/2110 train_time:40234ms step_avg:41.14ms
step:979/2110 train_time:40292ms step_avg:41.16ms
step:980/2110 train_time:40351ms step_avg:41.17ms
step:981/2110 train_time:40410ms step_avg:41.19ms
step:982/2110 train_time:40469ms step_avg:41.21ms
step:983/2110 train_time:40530ms step_avg:41.23ms
step:984/2110 train_time:40589ms step_avg:41.25ms
step:985/2110 train_time:40647ms step_avg:41.27ms
step:986/2110 train_time:40705ms step_avg:41.28ms
step:987/2110 train_time:40766ms step_avg:41.30ms
step:988/2110 train_time:40827ms step_avg:41.32ms
step:989/2110 train_time:40884ms step_avg:41.34ms
step:990/2110 train_time:40944ms step_avg:41.36ms
step:991/2110 train_time:41003ms step_avg:41.38ms
step:992/2110 train_time:41062ms step_avg:41.39ms
step:993/2110 train_time:41123ms step_avg:41.41ms
step:994/2110 train_time:41181ms step_avg:41.43ms
step:995/2110 train_time:41240ms step_avg:41.45ms
step:996/2110 train_time:41299ms step_avg:41.46ms
step:997/2110 train_time:41357ms step_avg:41.48ms
step:998/2110 train_time:41418ms step_avg:41.50ms
step:999/2110 train_time:41475ms step_avg:41.52ms
step:1000/2110 train_time:41533ms step_avg:41.53ms
step:1000/2110 val_loss:3.7559 train_time:41594ms step_avg:41.59ms
step:1001/2110 train_time:41628ms step_avg:41.59ms
step:1002/2110 train_time:41662ms step_avg:41.58ms
step:1003/2110 train_time:41720ms step_avg:41.60ms
step:1004/2110 train_time:41783ms step_avg:41.62ms
step:1005/2110 train_time:41841ms step_avg:41.63ms
step:1006/2110 train_time:41901ms step_avg:41.65ms
step:1007/2110 train_time:41959ms step_avg:41.67ms
step:1008/2110 train_time:42019ms step_avg:41.69ms
step:1009/2110 train_time:42077ms step_avg:41.70ms
step:1010/2110 train_time:42135ms step_avg:41.72ms
step:1011/2110 train_time:42195ms step_avg:41.74ms
step:1012/2110 train_time:42253ms step_avg:41.75ms
step:1013/2110 train_time:42313ms step_avg:41.77ms
step:1014/2110 train_time:42371ms step_avg:41.79ms
step:1015/2110 train_time:42429ms step_avg:41.80ms
step:1016/2110 train_time:42487ms step_avg:41.82ms
step:1017/2110 train_time:42548ms step_avg:41.84ms
step:1018/2110 train_time:42609ms step_avg:41.86ms
step:1019/2110 train_time:42668ms step_avg:41.87ms
step:1020/2110 train_time:42728ms step_avg:41.89ms
step:1021/2110 train_time:42787ms step_avg:41.91ms
step:1022/2110 train_time:42847ms step_avg:41.92ms
step:1023/2110 train_time:42906ms step_avg:41.94ms
step:1024/2110 train_time:42966ms step_avg:41.96ms
step:1025/2110 train_time:43024ms step_avg:41.98ms
step:1026/2110 train_time:43083ms step_avg:41.99ms
step:1027/2110 train_time:43141ms step_avg:42.01ms
step:1028/2110 train_time:43200ms step_avg:42.02ms
step:1029/2110 train_time:43259ms step_avg:42.04ms
step:1030/2110 train_time:43317ms step_avg:42.06ms
step:1031/2110 train_time:43377ms step_avg:42.07ms
step:1032/2110 train_time:43434ms step_avg:42.09ms
step:1033/2110 train_time:43495ms step_avg:42.11ms
step:1034/2110 train_time:43554ms step_avg:42.12ms
step:1035/2110 train_time:43615ms step_avg:42.14ms
step:1036/2110 train_time:43673ms step_avg:42.16ms
step:1037/2110 train_time:43735ms step_avg:42.17ms
step:1038/2110 train_time:43794ms step_avg:42.19ms
step:1039/2110 train_time:43856ms step_avg:42.21ms
step:1040/2110 train_time:43915ms step_avg:42.23ms
step:1041/2110 train_time:43975ms step_avg:42.24ms
step:1042/2110 train_time:44033ms step_avg:42.26ms
step:1043/2110 train_time:44093ms step_avg:42.28ms
step:1044/2110 train_time:44151ms step_avg:42.29ms
step:1045/2110 train_time:44210ms step_avg:42.31ms
step:1046/2110 train_time:44268ms step_avg:42.32ms
step:1047/2110 train_time:44327ms step_avg:42.34ms
step:1048/2110 train_time:44384ms step_avg:42.35ms
step:1049/2110 train_time:44444ms step_avg:42.37ms
step:1050/2110 train_time:44501ms step_avg:42.38ms
step:1051/2110 train_time:44562ms step_avg:42.40ms
step:1052/2110 train_time:44621ms step_avg:42.42ms
step:1053/2110 train_time:44681ms step_avg:42.43ms
step:1054/2110 train_time:44740ms step_avg:42.45ms
step:1055/2110 train_time:44800ms step_avg:42.46ms
step:1056/2110 train_time:44859ms step_avg:42.48ms
step:1057/2110 train_time:44919ms step_avg:42.50ms
step:1058/2110 train_time:44978ms step_avg:42.51ms
step:1059/2110 train_time:45038ms step_avg:42.53ms
step:1060/2110 train_time:45096ms step_avg:42.54ms
step:1061/2110 train_time:45155ms step_avg:42.56ms
step:1062/2110 train_time:45214ms step_avg:42.57ms
step:1063/2110 train_time:45274ms step_avg:42.59ms
step:1064/2110 train_time:45333ms step_avg:42.61ms
step:1065/2110 train_time:45392ms step_avg:42.62ms
step:1066/2110 train_time:45451ms step_avg:42.64ms
step:1067/2110 train_time:45511ms step_avg:42.65ms
step:1068/2110 train_time:45569ms step_avg:42.67ms
step:1069/2110 train_time:45629ms step_avg:42.68ms
step:1070/2110 train_time:45687ms step_avg:42.70ms
step:1071/2110 train_time:45747ms step_avg:42.71ms
step:1072/2110 train_time:45805ms step_avg:42.73ms
step:1073/2110 train_time:45865ms step_avg:42.74ms
step:1074/2110 train_time:45923ms step_avg:42.76ms
step:1075/2110 train_time:45984ms step_avg:42.78ms
step:1076/2110 train_time:46042ms step_avg:42.79ms
step:1077/2110 train_time:46102ms step_avg:42.81ms
step:1078/2110 train_time:46160ms step_avg:42.82ms
step:1079/2110 train_time:46220ms step_avg:42.84ms
step:1080/2110 train_time:46278ms step_avg:42.85ms
step:1081/2110 train_time:46338ms step_avg:42.87ms
step:1082/2110 train_time:46396ms step_avg:42.88ms
step:1083/2110 train_time:46456ms step_avg:42.90ms
step:1084/2110 train_time:46515ms step_avg:42.91ms
step:1085/2110 train_time:46575ms step_avg:42.93ms
step:1086/2110 train_time:46634ms step_avg:42.94ms
step:1087/2110 train_time:46695ms step_avg:42.96ms
step:1088/2110 train_time:46754ms step_avg:42.97ms
step:1089/2110 train_time:46814ms step_avg:42.99ms
step:1090/2110 train_time:46873ms step_avg:43.00ms
step:1091/2110 train_time:46933ms step_avg:43.02ms
step:1092/2110 train_time:46993ms step_avg:43.03ms
step:1093/2110 train_time:47052ms step_avg:43.05ms
step:1094/2110 train_time:47110ms step_avg:43.06ms
step:1095/2110 train_time:47170ms step_avg:43.08ms
step:1096/2110 train_time:47231ms step_avg:43.09ms
step:1097/2110 train_time:47287ms step_avg:43.11ms
step:1098/2110 train_time:47345ms step_avg:43.12ms
step:1099/2110 train_time:47404ms step_avg:43.13ms
step:1100/2110 train_time:47461ms step_avg:43.15ms
step:1101/2110 train_time:47522ms step_avg:43.16ms
step:1102/2110 train_time:47580ms step_avg:43.18ms
step:1103/2110 train_time:47640ms step_avg:43.19ms
step:1104/2110 train_time:47700ms step_avg:43.21ms
step:1105/2110 train_time:47760ms step_avg:43.22ms
step:1106/2110 train_time:47818ms step_avg:43.24ms
step:1107/2110 train_time:47878ms step_avg:43.25ms
step:1108/2110 train_time:47937ms step_avg:43.26ms
step:1109/2110 train_time:47997ms step_avg:43.28ms
step:1110/2110 train_time:48056ms step_avg:43.29ms
step:1111/2110 train_time:48116ms step_avg:43.31ms
step:1112/2110 train_time:48174ms step_avg:43.32ms
step:1113/2110 train_time:48234ms step_avg:43.34ms
step:1114/2110 train_time:48293ms step_avg:43.35ms
step:1115/2110 train_time:48353ms step_avg:43.37ms
step:1116/2110 train_time:48411ms step_avg:43.38ms
step:1117/2110 train_time:48471ms step_avg:43.39ms
step:1118/2110 train_time:48530ms step_avg:43.41ms
step:1119/2110 train_time:48590ms step_avg:43.42ms
step:1120/2110 train_time:48648ms step_avg:43.44ms
step:1121/2110 train_time:48708ms step_avg:43.45ms
step:1122/2110 train_time:48767ms step_avg:43.46ms
step:1123/2110 train_time:48827ms step_avg:43.48ms
step:1124/2110 train_time:48885ms step_avg:43.49ms
step:1125/2110 train_time:48945ms step_avg:43.51ms
step:1126/2110 train_time:49003ms step_avg:43.52ms
step:1127/2110 train_time:49063ms step_avg:43.53ms
step:1128/2110 train_time:49120ms step_avg:43.55ms
step:1129/2110 train_time:49181ms step_avg:43.56ms
step:1130/2110 train_time:49239ms step_avg:43.57ms
step:1131/2110 train_time:49299ms step_avg:43.59ms
step:1132/2110 train_time:49358ms step_avg:43.60ms
step:1133/2110 train_time:49418ms step_avg:43.62ms
step:1134/2110 train_time:49477ms step_avg:43.63ms
step:1135/2110 train_time:49537ms step_avg:43.64ms
step:1136/2110 train_time:49596ms step_avg:43.66ms
step:1137/2110 train_time:49657ms step_avg:43.67ms
step:1138/2110 train_time:49716ms step_avg:43.69ms
step:1139/2110 train_time:49775ms step_avg:43.70ms
step:1140/2110 train_time:49835ms step_avg:43.72ms
step:1141/2110 train_time:49896ms step_avg:43.73ms
step:1142/2110 train_time:49955ms step_avg:43.74ms
step:1143/2110 train_time:50017ms step_avg:43.76ms
step:1144/2110 train_time:50076ms step_avg:43.77ms
step:1145/2110 train_time:50136ms step_avg:43.79ms
step:1146/2110 train_time:50196ms step_avg:43.80ms
step:1147/2110 train_time:50256ms step_avg:43.82ms
step:1148/2110 train_time:50315ms step_avg:43.83ms
step:1149/2110 train_time:50376ms step_avg:43.84ms
step:1150/2110 train_time:50435ms step_avg:43.86ms
step:1151/2110 train_time:50495ms step_avg:43.87ms
step:1152/2110 train_time:50554ms step_avg:43.88ms
step:1153/2110 train_time:50616ms step_avg:43.90ms
step:1154/2110 train_time:50675ms step_avg:43.91ms
step:1155/2110 train_time:50736ms step_avg:43.93ms
step:1156/2110 train_time:50796ms step_avg:43.94ms
step:1157/2110 train_time:50857ms step_avg:43.96ms
step:1158/2110 train_time:50916ms step_avg:43.97ms
step:1159/2110 train_time:50977ms step_avg:43.98ms
step:1160/2110 train_time:51036ms step_avg:44.00ms
step:1161/2110 train_time:51097ms step_avg:44.01ms
step:1162/2110 train_time:51155ms step_avg:44.02ms
step:1163/2110 train_time:51216ms step_avg:44.04ms
step:1164/2110 train_time:51275ms step_avg:44.05ms
step:1165/2110 train_time:51335ms step_avg:44.06ms
step:1166/2110 train_time:51395ms step_avg:44.08ms
step:1167/2110 train_time:51455ms step_avg:44.09ms
step:1168/2110 train_time:51514ms step_avg:44.10ms
step:1169/2110 train_time:51575ms step_avg:44.12ms
step:1170/2110 train_time:51634ms step_avg:44.13ms
step:1171/2110 train_time:51695ms step_avg:44.15ms
step:1172/2110 train_time:51755ms step_avg:44.16ms
step:1173/2110 train_time:51815ms step_avg:44.17ms
step:1174/2110 train_time:51875ms step_avg:44.19ms
step:1175/2110 train_time:51935ms step_avg:44.20ms
step:1176/2110 train_time:51995ms step_avg:44.21ms
step:1177/2110 train_time:52056ms step_avg:44.23ms
step:1178/2110 train_time:52115ms step_avg:44.24ms
step:1179/2110 train_time:52176ms step_avg:44.25ms
step:1180/2110 train_time:52235ms step_avg:44.27ms
step:1181/2110 train_time:52296ms step_avg:44.28ms
step:1182/2110 train_time:52355ms step_avg:44.29ms
step:1183/2110 train_time:52416ms step_avg:44.31ms
step:1184/2110 train_time:52475ms step_avg:44.32ms
step:1185/2110 train_time:52535ms step_avg:44.33ms
step:1186/2110 train_time:52596ms step_avg:44.35ms
step:1187/2110 train_time:52655ms step_avg:44.36ms
step:1188/2110 train_time:52714ms step_avg:44.37ms
step:1189/2110 train_time:52775ms step_avg:44.39ms
step:1190/2110 train_time:52835ms step_avg:44.40ms
step:1191/2110 train_time:52896ms step_avg:44.41ms
step:1192/2110 train_time:52955ms step_avg:44.43ms
step:1193/2110 train_time:53016ms step_avg:44.44ms
step:1194/2110 train_time:53075ms step_avg:44.45ms
step:1195/2110 train_time:53135ms step_avg:44.46ms
step:1196/2110 train_time:53195ms step_avg:44.48ms
step:1197/2110 train_time:53256ms step_avg:44.49ms
step:1198/2110 train_time:53315ms step_avg:44.50ms
step:1199/2110 train_time:53375ms step_avg:44.52ms
step:1200/2110 train_time:53434ms step_avg:44.53ms
step:1201/2110 train_time:53495ms step_avg:44.54ms
step:1202/2110 train_time:53555ms step_avg:44.55ms
step:1203/2110 train_time:53615ms step_avg:44.57ms
step:1204/2110 train_time:53674ms step_avg:44.58ms
step:1205/2110 train_time:53735ms step_avg:44.59ms
step:1206/2110 train_time:53795ms step_avg:44.61ms
step:1207/2110 train_time:53856ms step_avg:44.62ms
step:1208/2110 train_time:53916ms step_avg:44.63ms
step:1209/2110 train_time:53976ms step_avg:44.65ms
step:1210/2110 train_time:54035ms step_avg:44.66ms
step:1211/2110 train_time:54097ms step_avg:44.67ms
step:1212/2110 train_time:54156ms step_avg:44.68ms
step:1213/2110 train_time:54217ms step_avg:44.70ms
step:1214/2110 train_time:54275ms step_avg:44.71ms
step:1215/2110 train_time:54336ms step_avg:44.72ms
step:1216/2110 train_time:54395ms step_avg:44.73ms
step:1217/2110 train_time:54456ms step_avg:44.75ms
step:1218/2110 train_time:54515ms step_avg:44.76ms
step:1219/2110 train_time:54576ms step_avg:44.77ms
step:1220/2110 train_time:54635ms step_avg:44.78ms
step:1221/2110 train_time:54696ms step_avg:44.80ms
step:1222/2110 train_time:54756ms step_avg:44.81ms
step:1223/2110 train_time:54817ms step_avg:44.82ms
step:1224/2110 train_time:54876ms step_avg:44.83ms
step:1225/2110 train_time:54937ms step_avg:44.85ms
step:1226/2110 train_time:54996ms step_avg:44.86ms
step:1227/2110 train_time:55057ms step_avg:44.87ms
step:1228/2110 train_time:55117ms step_avg:44.88ms
step:1229/2110 train_time:55177ms step_avg:44.90ms
step:1230/2110 train_time:55236ms step_avg:44.91ms
step:1231/2110 train_time:55297ms step_avg:44.92ms
step:1232/2110 train_time:55356ms step_avg:44.93ms
step:1233/2110 train_time:55417ms step_avg:44.94ms
step:1234/2110 train_time:55476ms step_avg:44.96ms
step:1235/2110 train_time:55537ms step_avg:44.97ms
step:1236/2110 train_time:55597ms step_avg:44.98ms
step:1237/2110 train_time:55658ms step_avg:44.99ms
step:1238/2110 train_time:55717ms step_avg:45.01ms
step:1239/2110 train_time:55778ms step_avg:45.02ms
step:1240/2110 train_time:55837ms step_avg:45.03ms
step:1241/2110 train_time:55899ms step_avg:45.04ms
step:1242/2110 train_time:55958ms step_avg:45.05ms
step:1243/2110 train_time:56018ms step_avg:45.07ms
step:1244/2110 train_time:56076ms step_avg:45.08ms
step:1245/2110 train_time:56137ms step_avg:45.09ms
step:1246/2110 train_time:56197ms step_avg:45.10ms
step:1247/2110 train_time:56257ms step_avg:45.11ms
step:1248/2110 train_time:56316ms step_avg:45.13ms
step:1249/2110 train_time:56377ms step_avg:45.14ms
step:1250/2110 train_time:56436ms step_avg:45.15ms
step:1250/2110 val_loss:3.5928 train_time:56499ms step_avg:45.20ms
step:1251/2110 train_time:56527ms step_avg:45.19ms
step:1252/2110 train_time:56560ms step_avg:45.18ms
step:1253/2110 train_time:56626ms step_avg:45.19ms
step:1254/2110 train_time:56686ms step_avg:45.20ms
step:1255/2110 train_time:56747ms step_avg:45.22ms
step:1256/2110 train_time:56806ms step_avg:45.23ms
step:1257/2110 train_time:56866ms step_avg:45.24ms
step:1258/2110 train_time:56925ms step_avg:45.25ms
step:1259/2110 train_time:56984ms step_avg:45.26ms
step:1260/2110 train_time:57043ms step_avg:45.27ms
step:1261/2110 train_time:57102ms step_avg:45.28ms
step:1262/2110 train_time:57161ms step_avg:45.29ms
step:1263/2110 train_time:57220ms step_avg:45.30ms
step:1264/2110 train_time:57278ms step_avg:45.31ms
step:1265/2110 train_time:57338ms step_avg:45.33ms
step:1266/2110 train_time:57396ms step_avg:45.34ms
step:1267/2110 train_time:57459ms step_avg:45.35ms
step:1268/2110 train_time:57518ms step_avg:45.36ms
step:1269/2110 train_time:57582ms step_avg:45.38ms
step:1270/2110 train_time:57642ms step_avg:45.39ms
step:1271/2110 train_time:57702ms step_avg:45.40ms
step:1272/2110 train_time:57761ms step_avg:45.41ms
step:1273/2110 train_time:57820ms step_avg:45.42ms
step:1274/2110 train_time:57879ms step_avg:45.43ms
step:1275/2110 train_time:57939ms step_avg:45.44ms
step:1276/2110 train_time:57997ms step_avg:45.45ms
step:1277/2110 train_time:58058ms step_avg:45.46ms
step:1278/2110 train_time:58116ms step_avg:45.47ms
step:1279/2110 train_time:58176ms step_avg:45.49ms
step:1280/2110 train_time:58235ms step_avg:45.50ms
step:1281/2110 train_time:58294ms step_avg:45.51ms
step:1282/2110 train_time:58353ms step_avg:45.52ms
step:1283/2110 train_time:58413ms step_avg:45.53ms
step:1284/2110 train_time:58474ms step_avg:45.54ms
step:1285/2110 train_time:58536ms step_avg:45.55ms
step:1286/2110 train_time:58596ms step_avg:45.56ms
step:1287/2110 train_time:58658ms step_avg:45.58ms
step:1288/2110 train_time:58718ms step_avg:45.59ms
step:1289/2110 train_time:58778ms step_avg:45.60ms
step:1290/2110 train_time:58837ms step_avg:45.61ms
step:1291/2110 train_time:58897ms step_avg:45.62ms
step:1292/2110 train_time:58956ms step_avg:45.63ms
step:1293/2110 train_time:59016ms step_avg:45.64ms
step:1294/2110 train_time:59075ms step_avg:45.65ms
step:1295/2110 train_time:59135ms step_avg:45.66ms
step:1296/2110 train_time:59194ms step_avg:45.67ms
step:1297/2110 train_time:59253ms step_avg:45.68ms
step:1298/2110 train_time:59312ms step_avg:45.69ms
step:1299/2110 train_time:59372ms step_avg:45.71ms
step:1300/2110 train_time:59432ms step_avg:45.72ms
step:1301/2110 train_time:59493ms step_avg:45.73ms
step:1302/2110 train_time:59553ms step_avg:45.74ms
step:1303/2110 train_time:59614ms step_avg:45.75ms
step:1304/2110 train_time:59675ms step_avg:45.76ms
step:1305/2110 train_time:59735ms step_avg:45.77ms
step:1306/2110 train_time:59794ms step_avg:45.78ms
step:1307/2110 train_time:59855ms step_avg:45.80ms
step:1308/2110 train_time:59914ms step_avg:45.81ms
step:1309/2110 train_time:59975ms step_avg:45.82ms
step:1310/2110 train_time:60034ms step_avg:45.83ms
step:1311/2110 train_time:60094ms step_avg:45.84ms
step:1312/2110 train_time:60153ms step_avg:45.85ms
step:1313/2110 train_time:60213ms step_avg:45.86ms
step:1314/2110 train_time:60272ms step_avg:45.87ms
step:1315/2110 train_time:60334ms step_avg:45.88ms
step:1316/2110 train_time:60393ms step_avg:45.89ms
step:1317/2110 train_time:60453ms step_avg:45.90ms
step:1318/2110 train_time:60513ms step_avg:45.91ms
step:1319/2110 train_time:60574ms step_avg:45.92ms
step:1320/2110 train_time:60634ms step_avg:45.93ms
step:1321/2110 train_time:60694ms step_avg:45.95ms
step:1322/2110 train_time:60754ms step_avg:45.96ms
step:1323/2110 train_time:60815ms step_avg:45.97ms
step:1324/2110 train_time:60874ms step_avg:45.98ms
step:1325/2110 train_time:60935ms step_avg:45.99ms
step:1326/2110 train_time:60994ms step_avg:46.00ms
step:1327/2110 train_time:61054ms step_avg:46.01ms
step:1328/2110 train_time:61113ms step_avg:46.02ms
step:1329/2110 train_time:61173ms step_avg:46.03ms
step:1330/2110 train_time:61233ms step_avg:46.04ms
step:1331/2110 train_time:61293ms step_avg:46.05ms
step:1332/2110 train_time:61353ms step_avg:46.06ms
step:1333/2110 train_time:61413ms step_avg:46.07ms
step:1334/2110 train_time:61473ms step_avg:46.08ms
step:1335/2110 train_time:61534ms step_avg:46.09ms
step:1336/2110 train_time:61594ms step_avg:46.10ms
step:1337/2110 train_time:61654ms step_avg:46.11ms
step:1338/2110 train_time:61714ms step_avg:46.12ms
step:1339/2110 train_time:61774ms step_avg:46.13ms
step:1340/2110 train_time:61834ms step_avg:46.14ms
step:1341/2110 train_time:61894ms step_avg:46.16ms
step:1342/2110 train_time:61954ms step_avg:46.17ms
step:1343/2110 train_time:62014ms step_avg:46.18ms
step:1344/2110 train_time:62073ms step_avg:46.19ms
step:1345/2110 train_time:62133ms step_avg:46.20ms
step:1346/2110 train_time:62193ms step_avg:46.21ms
step:1347/2110 train_time:62253ms step_avg:46.22ms
step:1348/2110 train_time:62312ms step_avg:46.23ms
step:1349/2110 train_time:62373ms step_avg:46.24ms
step:1350/2110 train_time:62432ms step_avg:46.25ms
step:1351/2110 train_time:62493ms step_avg:46.26ms
step:1352/2110 train_time:62553ms step_avg:46.27ms
step:1353/2110 train_time:62614ms step_avg:46.28ms
step:1354/2110 train_time:62674ms step_avg:46.29ms
step:1355/2110 train_time:62735ms step_avg:46.30ms
step:1356/2110 train_time:62794ms step_avg:46.31ms
step:1357/2110 train_time:62856ms step_avg:46.32ms
step:1358/2110 train_time:62915ms step_avg:46.33ms
step:1359/2110 train_time:62975ms step_avg:46.34ms
step:1360/2110 train_time:63035ms step_avg:46.35ms
step:1361/2110 train_time:63095ms step_avg:46.36ms
step:1362/2110 train_time:63154ms step_avg:46.37ms
step:1363/2110 train_time:63214ms step_avg:46.38ms
step:1364/2110 train_time:63274ms step_avg:46.39ms
step:1365/2110 train_time:63335ms step_avg:46.40ms
step:1366/2110 train_time:63394ms step_avg:46.41ms
step:1367/2110 train_time:63455ms step_avg:46.42ms
step:1368/2110 train_time:63514ms step_avg:46.43ms
step:1369/2110 train_time:63575ms step_avg:46.44ms
step:1370/2110 train_time:63635ms step_avg:46.45ms
step:1371/2110 train_time:63696ms step_avg:46.46ms
step:1372/2110 train_time:63755ms step_avg:46.47ms
step:1373/2110 train_time:63816ms step_avg:46.48ms
step:1374/2110 train_time:63876ms step_avg:46.49ms
step:1375/2110 train_time:63937ms step_avg:46.50ms
step:1376/2110 train_time:63996ms step_avg:46.51ms
step:1377/2110 train_time:64056ms step_avg:46.52ms
step:1378/2110 train_time:64115ms step_avg:46.53ms
step:1379/2110 train_time:64176ms step_avg:46.54ms
step:1380/2110 train_time:64236ms step_avg:46.55ms
step:1381/2110 train_time:64296ms step_avg:46.56ms
step:1382/2110 train_time:64383ms step_avg:46.59ms
step:1383/2110 train_time:64470ms step_avg:46.62ms
step:1384/2110 train_time:64557ms step_avg:46.65ms
step:1385/2110 train_time:64645ms step_avg:46.67ms
step:1386/2110 train_time:64730ms step_avg:46.70ms
step:1387/2110 train_time:64818ms step_avg:46.73ms
step:1388/2110 train_time:64905ms step_avg:46.76ms
step:1389/2110 train_time:64992ms step_avg:46.79ms
step:1390/2110 train_time:65078ms step_avg:46.82ms
step:1391/2110 train_time:65166ms step_avg:46.85ms
step:1392/2110 train_time:65251ms step_avg:46.88ms
step:1393/2110 train_time:65338ms step_avg:46.90ms
step:1394/2110 train_time:65424ms step_avg:46.93ms
step:1395/2110 train_time:65511ms step_avg:46.96ms
step:1396/2110 train_time:65597ms step_avg:46.99ms
step:1397/2110 train_time:65685ms step_avg:47.02ms
step:1398/2110 train_time:65771ms step_avg:47.05ms
step:1399/2110 train_time:65859ms step_avg:47.08ms
step:1400/2110 train_time:65946ms step_avg:47.10ms
step:1401/2110 train_time:66032ms step_avg:47.13ms
step:1402/2110 train_time:66119ms step_avg:47.16ms
step:1403/2110 train_time:66206ms step_avg:47.19ms
step:1404/2110 train_time:66292ms step_avg:47.22ms
step:1405/2110 train_time:66380ms step_avg:47.25ms
step:1406/2110 train_time:66465ms step_avg:47.27ms
step:1407/2110 train_time:66551ms step_avg:47.30ms
step:1408/2110 train_time:66638ms step_avg:47.33ms
step:1409/2110 train_time:66726ms step_avg:47.36ms
step:1410/2110 train_time:66811ms step_avg:47.38ms
step:1411/2110 train_time:66898ms step_avg:47.41ms
step:1412/2110 train_time:66985ms step_avg:47.44ms
step:1413/2110 train_time:67072ms step_avg:47.47ms
step:1414/2110 train_time:67159ms step_avg:47.50ms
step:1415/2110 train_time:67246ms step_avg:47.52ms
step:1416/2110 train_time:67332ms step_avg:47.55ms
step:1417/2110 train_time:67418ms step_avg:47.58ms
step:1418/2110 train_time:67505ms step_avg:47.61ms
step:1419/2110 train_time:67591ms step_avg:47.63ms
step:1420/2110 train_time:67678ms step_avg:47.66ms
step:1421/2110 train_time:67767ms step_avg:47.69ms
step:1422/2110 train_time:67852ms step_avg:47.72ms
step:1423/2110 train_time:67939ms step_avg:47.74ms
step:1424/2110 train_time:68026ms step_avg:47.77ms
step:1425/2110 train_time:68112ms step_avg:47.80ms
step:1426/2110 train_time:68200ms step_avg:47.83ms
step:1427/2110 train_time:68288ms step_avg:47.85ms
step:1428/2110 train_time:68374ms step_avg:47.88ms
step:1429/2110 train_time:68461ms step_avg:47.91ms
step:1430/2110 train_time:68547ms step_avg:47.93ms
step:1431/2110 train_time:68633ms step_avg:47.96ms
step:1432/2110 train_time:68721ms step_avg:47.99ms
step:1433/2110 train_time:68809ms step_avg:48.02ms
step:1434/2110 train_time:68895ms step_avg:48.04ms
step:1435/2110 train_time:68983ms step_avg:48.07ms
step:1436/2110 train_time:69069ms step_avg:48.10ms
step:1437/2110 train_time:69155ms step_avg:48.12ms
step:1438/2110 train_time:69242ms step_avg:48.15ms
step:1439/2110 train_time:69330ms step_avg:48.18ms
step:1440/2110 train_time:69416ms step_avg:48.21ms
step:1441/2110 train_time:69503ms step_avg:48.23ms
step:1442/2110 train_time:69588ms step_avg:48.26ms
step:1443/2110 train_time:69675ms step_avg:48.28ms
step:1444/2110 train_time:69762ms step_avg:48.31ms
step:1445/2110 train_time:69849ms step_avg:48.34ms
step:1446/2110 train_time:69936ms step_avg:48.37ms
step:1447/2110 train_time:70023ms step_avg:48.39ms
step:1448/2110 train_time:70109ms step_avg:48.42ms
step:1449/2110 train_time:70195ms step_avg:48.44ms
step:1450/2110 train_time:70282ms step_avg:48.47ms
step:1451/2110 train_time:70370ms step_avg:48.50ms
step:1452/2110 train_time:70456ms step_avg:48.52ms
step:1453/2110 train_time:70544ms step_avg:48.55ms
step:1454/2110 train_time:70629ms step_avg:48.58ms
step:1455/2110 train_time:70716ms step_avg:48.60ms
step:1456/2110 train_time:70803ms step_avg:48.63ms
step:1457/2110 train_time:70890ms step_avg:48.65ms
step:1458/2110 train_time:70977ms step_avg:48.68ms
step:1459/2110 train_time:71065ms step_avg:48.71ms
step:1460/2110 train_time:71151ms step_avg:48.73ms
step:1461/2110 train_time:71238ms step_avg:48.76ms
step:1462/2110 train_time:71325ms step_avg:48.79ms
step:1463/2110 train_time:71412ms step_avg:48.81ms
step:1464/2110 train_time:71498ms step_avg:48.84ms
step:1465/2110 train_time:71586ms step_avg:48.86ms
step:1466/2110 train_time:71672ms step_avg:48.89ms
step:1467/2110 train_time:71759ms step_avg:48.92ms
step:1468/2110 train_time:71845ms step_avg:48.94ms
step:1469/2110 train_time:71932ms step_avg:48.97ms
step:1470/2110 train_time:72019ms step_avg:48.99ms
step:1471/2110 train_time:72107ms step_avg:49.02ms
step:1472/2110 train_time:72192ms step_avg:49.04ms
step:1473/2110 train_time:72279ms step_avg:49.07ms
step:1474/2110 train_time:72366ms step_avg:49.10ms
step:1475/2110 train_time:72453ms step_avg:49.12ms
step:1476/2110 train_time:72539ms step_avg:49.15ms
step:1477/2110 train_time:72626ms step_avg:49.17ms
step:1478/2110 train_time:72713ms step_avg:49.20ms
step:1479/2110 train_time:72800ms step_avg:49.22ms
step:1480/2110 train_time:72887ms step_avg:49.25ms
step:1481/2110 train_time:72974ms step_avg:49.27ms
step:1482/2110 train_time:73061ms step_avg:49.30ms
step:1483/2110 train_time:73147ms step_avg:49.32ms
step:1484/2110 train_time:73234ms step_avg:49.35ms
step:1485/2110 train_time:73321ms step_avg:49.37ms
step:1486/2110 train_time:73407ms step_avg:49.40ms
step:1487/2110 train_time:73493ms step_avg:49.42ms
step:1488/2110 train_time:73580ms step_avg:49.45ms
step:1489/2110 train_time:73668ms step_avg:49.47ms
step:1490/2110 train_time:73755ms step_avg:49.50ms
step:1491/2110 train_time:73841ms step_avg:49.52ms
step:1492/2110 train_time:73928ms step_avg:49.55ms
step:1493/2110 train_time:74015ms step_avg:49.57ms
step:1494/2110 train_time:74102ms step_avg:49.60ms
step:1495/2110 train_time:74190ms step_avg:49.63ms
step:1496/2110 train_time:74276ms step_avg:49.65ms
step:1497/2110 train_time:74364ms step_avg:49.68ms
step:1498/2110 train_time:74449ms step_avg:49.70ms
step:1499/2110 train_time:74537ms step_avg:49.72ms
step:1500/2110 train_time:74624ms step_avg:49.75ms
step:1500/2110 val_loss:3.4945 train_time:74712ms step_avg:49.81ms
step:1501/2110 train_time:74740ms step_avg:49.79ms
step:1502/2110 train_time:74804ms step_avg:49.80ms
step:1503/2110 train_time:74897ms step_avg:49.83ms
step:1504/2110 train_time:74983ms step_avg:49.86ms
step:1505/2110 train_time:75070ms step_avg:49.88ms
step:1506/2110 train_time:75154ms step_avg:49.90ms
step:1507/2110 train_time:75241ms step_avg:49.93ms
step:1508/2110 train_time:75326ms step_avg:49.95ms
step:1509/2110 train_time:75412ms step_avg:49.97ms
step:1510/2110 train_time:75499ms step_avg:50.00ms
step:1511/2110 train_time:75584ms step_avg:50.02ms
step:1512/2110 train_time:75672ms step_avg:50.05ms
step:1513/2110 train_time:75762ms step_avg:50.07ms
step:1514/2110 train_time:75851ms step_avg:50.10ms
step:1515/2110 train_time:75939ms step_avg:50.12ms
step:1516/2110 train_time:76026ms step_avg:50.15ms
step:1517/2110 train_time:76112ms step_avg:50.17ms
step:1518/2110 train_time:76198ms step_avg:50.20ms
step:1519/2110 train_time:76284ms step_avg:50.22ms
step:1520/2110 train_time:76369ms step_avg:50.24ms
step:1521/2110 train_time:76455ms step_avg:50.27ms
step:1522/2110 train_time:76541ms step_avg:50.29ms
step:1523/2110 train_time:76629ms step_avg:50.31ms
step:1524/2110 train_time:76718ms step_avg:50.34ms
step:1525/2110 train_time:76808ms step_avg:50.37ms
step:1526/2110 train_time:76896ms step_avg:50.39ms
step:1527/2110 train_time:76983ms step_avg:50.41ms
step:1528/2110 train_time:77069ms step_avg:50.44ms
step:1529/2110 train_time:77156ms step_avg:50.46ms
step:1530/2110 train_time:77242ms step_avg:50.49ms
step:1531/2110 train_time:77328ms step_avg:50.51ms
step:1532/2110 train_time:77413ms step_avg:50.53ms
step:1533/2110 train_time:77501ms step_avg:50.55ms
step:1534/2110 train_time:77587ms step_avg:50.58ms
step:1535/2110 train_time:77674ms step_avg:50.60ms
step:1536/2110 train_time:77762ms step_avg:50.63ms
step:1537/2110 train_time:77850ms step_avg:50.65ms
step:1538/2110 train_time:77936ms step_avg:50.67ms
step:1539/2110 train_time:78025ms step_avg:50.70ms
step:1540/2110 train_time:78111ms step_avg:50.72ms
step:1541/2110 train_time:78198ms step_avg:50.74ms
step:1542/2110 train_time:78285ms step_avg:50.77ms
step:1543/2110 train_time:78371ms step_avg:50.79ms
step:1544/2110 train_time:78456ms step_avg:50.81ms
step:1545/2110 train_time:78544ms step_avg:50.84ms
step:1546/2110 train_time:78630ms step_avg:50.86ms
step:1547/2110 train_time:78718ms step_avg:50.88ms
step:1548/2110 train_time:78806ms step_avg:50.91ms
step:1549/2110 train_time:78893ms step_avg:50.93ms
step:1550/2110 train_time:78980ms step_avg:50.96ms
step:1551/2110 train_time:79068ms step_avg:50.98ms
step:1552/2110 train_time:79154ms step_avg:51.00ms
step:1553/2110 train_time:79241ms step_avg:51.02ms
step:1554/2110 train_time:79327ms step_avg:51.05ms
step:1555/2110 train_time:79413ms step_avg:51.07ms
step:1556/2110 train_time:79500ms step_avg:51.09ms
step:1557/2110 train_time:79586ms step_avg:51.12ms
step:1558/2110 train_time:79673ms step_avg:51.14ms
step:1559/2110 train_time:79761ms step_avg:51.16ms
step:1560/2110 train_time:79848ms step_avg:51.18ms
step:1561/2110 train_time:79935ms step_avg:51.21ms
step:1562/2110 train_time:80022ms step_avg:51.23ms
step:1563/2110 train_time:80109ms step_avg:51.25ms
step:1564/2110 train_time:80195ms step_avg:51.28ms
step:1565/2110 train_time:80283ms step_avg:51.30ms
step:1566/2110 train_time:80369ms step_avg:51.32ms
step:1567/2110 train_time:80455ms step_avg:51.34ms
step:1568/2110 train_time:80542ms step_avg:51.37ms
step:1569/2110 train_time:80629ms step_avg:51.39ms
step:1570/2110 train_time:80715ms step_avg:51.41ms
step:1571/2110 train_time:80803ms step_avg:51.43ms
step:1572/2110 train_time:80889ms step_avg:51.46ms
step:1573/2110 train_time:80977ms step_avg:51.48ms
step:1574/2110 train_time:81066ms step_avg:51.50ms
step:1575/2110 train_time:81152ms step_avg:51.53ms
step:1576/2110 train_time:81239ms step_avg:51.55ms
step:1577/2110 train_time:81327ms step_avg:51.57ms
step:1578/2110 train_time:81413ms step_avg:51.59ms
step:1579/2110 train_time:81500ms step_avg:51.61ms
step:1580/2110 train_time:81586ms step_avg:51.64ms
step:1581/2110 train_time:81672ms step_avg:51.66ms
step:1582/2110 train_time:81758ms step_avg:51.68ms
step:1583/2110 train_time:81846ms step_avg:51.70ms
step:1584/2110 train_time:81932ms step_avg:51.72ms
step:1585/2110 train_time:82020ms step_avg:51.75ms
step:1586/2110 train_time:82107ms step_avg:51.77ms
step:1587/2110 train_time:82194ms step_avg:51.79ms
step:1588/2110 train_time:82281ms step_avg:51.81ms
step:1589/2110 train_time:82368ms step_avg:51.84ms
step:1590/2110 train_time:82454ms step_avg:51.86ms
step:1591/2110 train_time:82541ms step_avg:51.88ms
step:1592/2110 train_time:82628ms step_avg:51.90ms
step:1593/2110 train_time:82714ms step_avg:51.92ms
step:1594/2110 train_time:82801ms step_avg:51.95ms
step:1595/2110 train_time:82889ms step_avg:51.97ms
step:1596/2110 train_time:82975ms step_avg:51.99ms
step:1597/2110 train_time:83062ms step_avg:52.01ms
step:1598/2110 train_time:83148ms step_avg:52.03ms
step:1599/2110 train_time:83235ms step_avg:52.05ms
step:1600/2110 train_time:83322ms step_avg:52.08ms
step:1601/2110 train_time:83409ms step_avg:52.10ms
step:1602/2110 train_time:83495ms step_avg:52.12ms
step:1603/2110 train_time:83582ms step_avg:52.14ms
step:1604/2110 train_time:83668ms step_avg:52.16ms
step:1605/2110 train_time:83755ms step_avg:52.18ms
step:1606/2110 train_time:83843ms step_avg:52.21ms
step:1607/2110 train_time:83929ms step_avg:52.23ms
step:1608/2110 train_time:84015ms step_avg:52.25ms
step:1609/2110 train_time:84103ms step_avg:52.27ms
step:1610/2110 train_time:84189ms step_avg:52.29ms
step:1611/2110 train_time:84276ms step_avg:52.31ms
step:1612/2110 train_time:84363ms step_avg:52.33ms
step:1613/2110 train_time:84450ms step_avg:52.36ms
step:1614/2110 train_time:84537ms step_avg:52.38ms
step:1615/2110 train_time:84625ms step_avg:52.40ms
step:1616/2110 train_time:84710ms step_avg:52.42ms
step:1617/2110 train_time:84797ms step_avg:52.44ms
step:1618/2110 train_time:84884ms step_avg:52.46ms
step:1619/2110 train_time:84971ms step_avg:52.48ms
step:1620/2110 train_time:85058ms step_avg:52.51ms
step:1621/2110 train_time:85146ms step_avg:52.53ms
step:1622/2110 train_time:85233ms step_avg:52.55ms
step:1623/2110 train_time:85320ms step_avg:52.57ms
step:1624/2110 train_time:85411ms step_avg:52.59ms
step:1625/2110 train_time:85493ms step_avg:52.61ms
step:1626/2110 train_time:85581ms step_avg:52.63ms
step:1627/2110 train_time:85668ms step_avg:52.65ms
step:1628/2110 train_time:85755ms step_avg:52.67ms
step:1629/2110 train_time:85842ms step_avg:52.70ms
step:1630/2110 train_time:85928ms step_avg:52.72ms
step:1631/2110 train_time:86015ms step_avg:52.74ms
step:1632/2110 train_time:86103ms step_avg:52.76ms
step:1633/2110 train_time:86189ms step_avg:52.78ms
step:1634/2110 train_time:86277ms step_avg:52.80ms
step:1635/2110 train_time:86364ms step_avg:52.82ms
step:1636/2110 train_time:86449ms step_avg:52.84ms
step:1637/2110 train_time:86537ms step_avg:52.86ms
step:1638/2110 train_time:86623ms step_avg:52.88ms
step:1639/2110 train_time:86710ms step_avg:52.90ms
step:1640/2110 train_time:86797ms step_avg:52.92ms
step:1641/2110 train_time:86884ms step_avg:52.95ms
step:1642/2110 train_time:86970ms step_avg:52.97ms
step:1643/2110 train_time:87056ms step_avg:52.99ms
step:1644/2110 train_time:87143ms step_avg:53.01ms
step:1645/2110 train_time:87230ms step_avg:53.03ms
step:1646/2110 train_time:87316ms step_avg:53.05ms
step:1647/2110 train_time:87403ms step_avg:53.07ms
step:1648/2110 train_time:87489ms step_avg:53.09ms
step:1649/2110 train_time:87576ms step_avg:53.11ms
step:1650/2110 train_time:87664ms step_avg:53.13ms
step:1651/2110 train_time:87750ms step_avg:53.15ms
step:1652/2110 train_time:87836ms step_avg:53.17ms
step:1653/2110 train_time:87923ms step_avg:53.19ms
step:1654/2110 train_time:88009ms step_avg:53.21ms
step:1655/2110 train_time:88097ms step_avg:53.23ms
step:1656/2110 train_time:88183ms step_avg:53.25ms
step:1657/2110 train_time:88270ms step_avg:53.27ms
step:1658/2110 train_time:88359ms step_avg:53.29ms
step:1659/2110 train_time:88447ms step_avg:53.31ms
step:1660/2110 train_time:88534ms step_avg:53.33ms
step:1661/2110 train_time:88622ms step_avg:53.35ms
step:1662/2110 train_time:88709ms step_avg:53.38ms
step:1663/2110 train_time:88798ms step_avg:53.40ms
step:1664/2110 train_time:88886ms step_avg:53.42ms
step:1665/2110 train_time:88974ms step_avg:53.44ms
step:1666/2110 train_time:89062ms step_avg:53.46ms
step:1667/2110 train_time:89150ms step_avg:53.48ms
step:1668/2110 train_time:89238ms step_avg:53.50ms
step:1669/2110 train_time:89327ms step_avg:53.52ms
step:1670/2110 train_time:89415ms step_avg:53.54ms
step:1671/2110 train_time:89504ms step_avg:53.56ms
step:1672/2110 train_time:89591ms step_avg:53.58ms
step:1673/2110 train_time:89679ms step_avg:53.60ms
step:1674/2110 train_time:89769ms step_avg:53.63ms
step:1675/2110 train_time:89856ms step_avg:53.65ms
step:1676/2110 train_time:89944ms step_avg:53.67ms
step:1677/2110 train_time:90032ms step_avg:53.69ms
step:1678/2110 train_time:90121ms step_avg:53.71ms
step:1679/2110 train_time:90210ms step_avg:53.73ms
step:1680/2110 train_time:90298ms step_avg:53.75ms
step:1681/2110 train_time:90386ms step_avg:53.77ms
step:1682/2110 train_time:90474ms step_avg:53.79ms
step:1683/2110 train_time:90561ms step_avg:53.81ms
step:1684/2110 train_time:90650ms step_avg:53.83ms
step:1685/2110 train_time:90737ms step_avg:53.85ms
step:1686/2110 train_time:90825ms step_avg:53.87ms
step:1687/2110 train_time:90912ms step_avg:53.89ms
step:1688/2110 train_time:91002ms step_avg:53.91ms
step:1689/2110 train_time:91089ms step_avg:53.93ms
step:1690/2110 train_time:91178ms step_avg:53.95ms
step:1691/2110 train_time:91265ms step_avg:53.97ms
step:1692/2110 train_time:91353ms step_avg:53.99ms
step:1693/2110 train_time:91441ms step_avg:54.01ms
step:1694/2110 train_time:91530ms step_avg:54.03ms
step:1695/2110 train_time:91617ms step_avg:54.05ms
step:1696/2110 train_time:91707ms step_avg:54.07ms
step:1697/2110 train_time:91794ms step_avg:54.09ms
step:1698/2110 train_time:91881ms step_avg:54.11ms
step:1699/2110 train_time:91970ms step_avg:54.13ms
step:1700/2110 train_time:92058ms step_avg:54.15ms
step:1701/2110 train_time:92146ms step_avg:54.17ms
step:1702/2110 train_time:92234ms step_avg:54.19ms
step:1703/2110 train_time:92323ms step_avg:54.21ms
step:1704/2110 train_time:92410ms step_avg:54.23ms
step:1705/2110 train_time:92498ms step_avg:54.25ms
step:1706/2110 train_time:92587ms step_avg:54.27ms
step:1707/2110 train_time:92674ms step_avg:54.29ms
step:1708/2110 train_time:92764ms step_avg:54.31ms
step:1709/2110 train_time:92851ms step_avg:54.33ms
step:1710/2110 train_time:92940ms step_avg:54.35ms
step:1711/2110 train_time:93026ms step_avg:54.37ms
step:1712/2110 train_time:93114ms step_avg:54.39ms
step:1713/2110 train_time:93203ms step_avg:54.41ms
step:1714/2110 train_time:93291ms step_avg:54.43ms
step:1715/2110 train_time:93382ms step_avg:54.45ms
step:1716/2110 train_time:93467ms step_avg:54.47ms
step:1717/2110 train_time:93555ms step_avg:54.49ms
step:1718/2110 train_time:93644ms step_avg:54.51ms
step:1719/2110 train_time:93731ms step_avg:54.53ms
step:1720/2110 train_time:93819ms step_avg:54.55ms
step:1721/2110 train_time:93908ms step_avg:54.57ms
step:1722/2110 train_time:93996ms step_avg:54.59ms
step:1723/2110 train_time:94084ms step_avg:54.60ms
step:1724/2110 train_time:94174ms step_avg:54.63ms
step:1725/2110 train_time:94260ms step_avg:54.64ms
step:1726/2110 train_time:94348ms step_avg:54.66ms
step:1727/2110 train_time:94435ms step_avg:54.68ms
step:1728/2110 train_time:94524ms step_avg:54.70ms
step:1729/2110 train_time:94612ms step_avg:54.72ms
step:1730/2110 train_time:94701ms step_avg:54.74ms
step:1731/2110 train_time:94789ms step_avg:54.76ms
step:1732/2110 train_time:94878ms step_avg:54.78ms
step:1733/2110 train_time:94965ms step_avg:54.80ms
step:1734/2110 train_time:95052ms step_avg:54.82ms
step:1735/2110 train_time:95140ms step_avg:54.84ms
step:1736/2110 train_time:95228ms step_avg:54.85ms
step:1737/2110 train_time:95316ms step_avg:54.87ms
step:1738/2110 train_time:95404ms step_avg:54.89ms
step:1739/2110 train_time:95492ms step_avg:54.91ms
step:1740/2110 train_time:95581ms step_avg:54.93ms
step:1741/2110 train_time:95668ms step_avg:54.95ms
step:1742/2110 train_time:95757ms step_avg:54.97ms
step:1743/2110 train_time:95846ms step_avg:54.99ms
step:1744/2110 train_time:95934ms step_avg:55.01ms
step:1745/2110 train_time:96022ms step_avg:55.03ms
step:1746/2110 train_time:96110ms step_avg:55.05ms
step:1747/2110 train_time:96198ms step_avg:55.06ms
step:1748/2110 train_time:96286ms step_avg:55.08ms
step:1749/2110 train_time:96373ms step_avg:55.10ms
step:1750/2110 train_time:96462ms step_avg:55.12ms
step:1750/2110 val_loss:3.3783 train_time:96551ms step_avg:55.17ms
step:1751/2110 train_time:96598ms step_avg:55.17ms
step:1752/2110 train_time:96644ms step_avg:55.16ms
step:1753/2110 train_time:96736ms step_avg:55.18ms
step:1754/2110 train_time:96825ms step_avg:55.20ms
step:1755/2110 train_time:96912ms step_avg:55.22ms
step:1756/2110 train_time:96998ms step_avg:55.24ms
step:1757/2110 train_time:97086ms step_avg:55.26ms
step:1758/2110 train_time:97172ms step_avg:55.27ms
step:1759/2110 train_time:97258ms step_avg:55.29ms
step:1760/2110 train_time:97345ms step_avg:55.31ms
step:1761/2110 train_time:97434ms step_avg:55.33ms
step:1762/2110 train_time:97525ms step_avg:55.35ms
step:1763/2110 train_time:97615ms step_avg:55.37ms
step:1764/2110 train_time:97706ms step_avg:55.39ms
step:1765/2110 train_time:97793ms step_avg:55.41ms
step:1766/2110 train_time:97881ms step_avg:55.43ms
step:1767/2110 train_time:97969ms step_avg:55.44ms
step:1768/2110 train_time:98054ms step_avg:55.46ms
step:1769/2110 train_time:98141ms step_avg:55.48ms
step:1770/2110 train_time:98229ms step_avg:55.50ms
step:1771/2110 train_time:98315ms step_avg:55.51ms
step:1772/2110 train_time:98402ms step_avg:55.53ms
step:1773/2110 train_time:98494ms step_avg:55.55ms
step:1774/2110 train_time:98582ms step_avg:55.57ms
step:1775/2110 train_time:98673ms step_avg:55.59ms
step:1776/2110 train_time:98762ms step_avg:55.61ms
step:1777/2110 train_time:98851ms step_avg:55.63ms
step:1778/2110 train_time:98938ms step_avg:55.65ms
step:1779/2110 train_time:99026ms step_avg:55.66ms
step:1780/2110 train_time:99114ms step_avg:55.68ms
step:1781/2110 train_time:99199ms step_avg:55.70ms
step:1782/2110 train_time:99287ms step_avg:55.72ms
step:1783/2110 train_time:99374ms step_avg:55.73ms
step:1784/2110 train_time:99464ms step_avg:55.75ms
step:1785/2110 train_time:99550ms step_avg:55.77ms
step:1786/2110 train_time:99640ms step_avg:55.79ms
step:1787/2110 train_time:99729ms step_avg:55.81ms
step:1788/2110 train_time:99817ms step_avg:55.83ms
step:1789/2110 train_time:99904ms step_avg:55.84ms
step:1790/2110 train_time:99991ms step_avg:55.86ms
step:1791/2110 train_time:100080ms step_avg:55.88ms
step:1792/2110 train_time:100167ms step_avg:55.90ms
step:1793/2110 train_time:100254ms step_avg:55.91ms
step:1794/2110 train_time:100343ms step_avg:55.93ms
step:1795/2110 train_time:100429ms step_avg:55.95ms
step:1796/2110 train_time:100517ms step_avg:55.97ms
step:1797/2110 train_time:100605ms step_avg:55.99ms
step:1798/2110 train_time:100692ms step_avg:56.00ms
step:1799/2110 train_time:100781ms step_avg:56.02ms
step:1800/2110 train_time:100869ms step_avg:56.04ms
step:1801/2110 train_time:100957ms step_avg:56.06ms
step:1802/2110 train_time:101044ms step_avg:56.07ms
step:1803/2110 train_time:101132ms step_avg:56.09ms
step:1804/2110 train_time:101220ms step_avg:56.11ms
step:1805/2110 train_time:101306ms step_avg:56.13ms
step:1806/2110 train_time:101393ms step_avg:56.14ms
step:1807/2110 train_time:101481ms step_avg:56.16ms
step:1808/2110 train_time:101570ms step_avg:56.18ms
step:1809/2110 train_time:101658ms step_avg:56.20ms
step:1810/2110 train_time:101747ms step_avg:56.21ms
step:1811/2110 train_time:101835ms step_avg:56.23ms
step:1812/2110 train_time:101924ms step_avg:56.25ms
step:1813/2110 train_time:102010ms step_avg:56.27ms
step:1814/2110 train_time:102098ms step_avg:56.28ms
step:1815/2110 train_time:102187ms step_avg:56.30ms
step:1816/2110 train_time:102274ms step_avg:56.32ms
step:1817/2110 train_time:102362ms step_avg:56.34ms
step:1818/2110 train_time:102449ms step_avg:56.35ms
step:1819/2110 train_time:102537ms step_avg:56.37ms
step:1820/2110 train_time:102626ms step_avg:56.39ms
step:1821/2110 train_time:102714ms step_avg:56.41ms
step:1822/2110 train_time:102801ms step_avg:56.42ms
step:1823/2110 train_time:102891ms step_avg:56.44ms
step:1824/2110 train_time:102977ms step_avg:56.46ms
step:1825/2110 train_time:103066ms step_avg:56.47ms
step:1826/2110 train_time:103153ms step_avg:56.49ms
step:1827/2110 train_time:103240ms step_avg:56.51ms
step:1828/2110 train_time:103329ms step_avg:56.53ms
step:1829/2110 train_time:103416ms step_avg:56.54ms
step:1830/2110 train_time:103504ms step_avg:56.56ms
step:1831/2110 train_time:103592ms step_avg:56.58ms
step:1832/2110 train_time:103680ms step_avg:56.59ms
step:1833/2110 train_time:103768ms step_avg:56.61ms
step:1834/2110 train_time:103855ms step_avg:56.63ms
step:1835/2110 train_time:103943ms step_avg:56.64ms
step:1836/2110 train_time:104030ms step_avg:56.66ms
step:1837/2110 train_time:104120ms step_avg:56.68ms
step:1838/2110 train_time:104206ms step_avg:56.70ms
step:1839/2110 train_time:104296ms step_avg:56.71ms
step:1840/2110 train_time:104381ms step_avg:56.73ms
step:1841/2110 train_time:104470ms step_avg:56.75ms
step:1842/2110 train_time:104558ms step_avg:56.76ms
step:1843/2110 train_time:104646ms step_avg:56.78ms
step:1844/2110 train_time:104734ms step_avg:56.80ms
step:1845/2110 train_time:104822ms step_avg:56.81ms
step:1846/2110 train_time:104909ms step_avg:56.83ms
step:1847/2110 train_time:104997ms step_avg:56.85ms
step:1848/2110 train_time:105084ms step_avg:56.86ms
step:1849/2110 train_time:105173ms step_avg:56.88ms
step:1850/2110 train_time:105260ms step_avg:56.90ms
step:1851/2110 train_time:105348ms step_avg:56.91ms
step:1852/2110 train_time:105434ms step_avg:56.93ms
step:1853/2110 train_time:105523ms step_avg:56.95ms
step:1854/2110 train_time:105610ms step_avg:56.96ms
step:1855/2110 train_time:105697ms step_avg:56.98ms
step:1856/2110 train_time:105785ms step_avg:57.00ms
step:1857/2110 train_time:105873ms step_avg:57.01ms
step:1858/2110 train_time:105961ms step_avg:57.03ms
step:1859/2110 train_time:106049ms step_avg:57.05ms
step:1860/2110 train_time:106137ms step_avg:57.06ms
step:1861/2110 train_time:106226ms step_avg:57.08ms
step:1862/2110 train_time:106312ms step_avg:57.10ms
step:1863/2110 train_time:106401ms step_avg:57.11ms
step:1864/2110 train_time:106489ms step_avg:57.13ms
step:1865/2110 train_time:106576ms step_avg:57.15ms
step:1866/2110 train_time:106664ms step_avg:57.16ms
step:1867/2110 train_time:106753ms step_avg:57.18ms
step:1868/2110 train_time:106840ms step_avg:57.19ms
step:1869/2110 train_time:106929ms step_avg:57.21ms
step:1870/2110 train_time:107016ms step_avg:57.23ms
step:1871/2110 train_time:107106ms step_avg:57.25ms
step:1872/2110 train_time:107192ms step_avg:57.26ms
step:1873/2110 train_time:107280ms step_avg:57.28ms
step:1874/2110 train_time:107368ms step_avg:57.29ms
step:1875/2110 train_time:107456ms step_avg:57.31ms
step:1876/2110 train_time:107544ms step_avg:57.33ms
step:1877/2110 train_time:107632ms step_avg:57.34ms
step:1878/2110 train_time:107720ms step_avg:57.36ms
step:1879/2110 train_time:107808ms step_avg:57.38ms
step:1880/2110 train_time:107896ms step_avg:57.39ms
step:1881/2110 train_time:107985ms step_avg:57.41ms
step:1882/2110 train_time:108073ms step_avg:57.42ms
step:1883/2110 train_time:108160ms step_avg:57.44ms
step:1884/2110 train_time:108247ms step_avg:57.46ms
step:1885/2110 train_time:108334ms step_avg:57.47ms
step:1886/2110 train_time:108421ms step_avg:57.49ms
step:1887/2110 train_time:108510ms step_avg:57.50ms
step:1888/2110 train_time:108597ms step_avg:57.52ms
step:1889/2110 train_time:108686ms step_avg:57.54ms
step:1890/2110 train_time:108772ms step_avg:57.55ms
step:1891/2110 train_time:108862ms step_avg:57.57ms
step:1892/2110 train_time:108949ms step_avg:57.58ms
step:1893/2110 train_time:109037ms step_avg:57.60ms
step:1894/2110 train_time:109125ms step_avg:57.62ms
step:1895/2110 train_time:109213ms step_avg:57.63ms
step:1896/2110 train_time:109300ms step_avg:57.65ms
step:1897/2110 train_time:109388ms step_avg:57.66ms
step:1898/2110 train_time:109475ms step_avg:57.68ms
step:1899/2110 train_time:109564ms step_avg:57.70ms
step:1900/2110 train_time:109652ms step_avg:57.71ms
step:1901/2110 train_time:109740ms step_avg:57.73ms
step:1902/2110 train_time:109828ms step_avg:57.74ms
step:1903/2110 train_time:109916ms step_avg:57.76ms
step:1904/2110 train_time:110002ms step_avg:57.77ms
step:1905/2110 train_time:110092ms step_avg:57.79ms
step:1906/2110 train_time:110179ms step_avg:57.81ms
step:1907/2110 train_time:110268ms step_avg:57.82ms
step:1908/2110 train_time:110355ms step_avg:57.84ms
step:1909/2110 train_time:110443ms step_avg:57.85ms
step:1910/2110 train_time:110531ms step_avg:57.87ms
step:1911/2110 train_time:110618ms step_avg:57.89ms
step:1912/2110 train_time:110706ms step_avg:57.90ms
step:1913/2110 train_time:110794ms step_avg:57.92ms
step:1914/2110 train_time:110883ms step_avg:57.93ms
step:1915/2110 train_time:110971ms step_avg:57.95ms
step:1916/2110 train_time:111058ms step_avg:57.96ms
step:1917/2110 train_time:111147ms step_avg:57.98ms
step:1918/2110 train_time:111233ms step_avg:57.99ms
step:1919/2110 train_time:111322ms step_avg:58.01ms
step:1920/2110 train_time:111410ms step_avg:58.03ms
step:1921/2110 train_time:111497ms step_avg:58.04ms
step:1922/2110 train_time:111585ms step_avg:58.06ms
step:1923/2110 train_time:111673ms step_avg:58.07ms
step:1924/2110 train_time:111762ms step_avg:58.09ms
step:1925/2110 train_time:111849ms step_avg:58.10ms
step:1926/2110 train_time:111936ms step_avg:58.12ms
step:1927/2110 train_time:112027ms step_avg:58.14ms
step:1928/2110 train_time:112112ms step_avg:58.15ms
step:1929/2110 train_time:112201ms step_avg:58.17ms
step:1930/2110 train_time:112289ms step_avg:58.18ms
step:1931/2110 train_time:112376ms step_avg:58.20ms
step:1932/2110 train_time:112463ms step_avg:58.21ms
step:1933/2110 train_time:112553ms step_avg:58.23ms
step:1934/2110 train_time:112641ms step_avg:58.24ms
step:1935/2110 train_time:112729ms step_avg:58.26ms
step:1936/2110 train_time:112815ms step_avg:58.27ms
step:1937/2110 train_time:112903ms step_avg:58.29ms
step:1938/2110 train_time:112991ms step_avg:58.30ms
step:1939/2110 train_time:113078ms step_avg:58.32ms
step:1940/2110 train_time:113167ms step_avg:58.33ms
step:1941/2110 train_time:113254ms step_avg:58.35ms
step:1942/2110 train_time:113340ms step_avg:58.36ms
step:1943/2110 train_time:113429ms step_avg:58.38ms
step:1944/2110 train_time:113516ms step_avg:58.39ms
step:1945/2110 train_time:113604ms step_avg:58.41ms
step:1946/2110 train_time:113692ms step_avg:58.42ms
step:1947/2110 train_time:113780ms step_avg:58.44ms
step:1948/2110 train_time:113868ms step_avg:58.45ms
step:1949/2110 train_time:113955ms step_avg:58.47ms
step:1950/2110 train_time:114042ms step_avg:58.48ms
step:1951/2110 train_time:114132ms step_avg:58.50ms
step:1952/2110 train_time:114219ms step_avg:58.51ms
step:1953/2110 train_time:114307ms step_avg:58.53ms
step:1954/2110 train_time:114394ms step_avg:58.54ms
step:1955/2110 train_time:114482ms step_avg:58.56ms
step:1956/2110 train_time:114571ms step_avg:58.57ms
step:1957/2110 train_time:114658ms step_avg:58.59ms
step:1958/2110 train_time:114745ms step_avg:58.60ms
step:1959/2110 train_time:114834ms step_avg:58.62ms
step:1960/2110 train_time:114921ms step_avg:58.63ms
step:1961/2110 train_time:115011ms step_avg:58.65ms
step:1962/2110 train_time:115099ms step_avg:58.66ms
step:1963/2110 train_time:115187ms step_avg:58.68ms
step:1964/2110 train_time:115274ms step_avg:58.69ms
step:1965/2110 train_time:115362ms step_avg:58.71ms
step:1966/2110 train_time:115449ms step_avg:58.72ms
step:1967/2110 train_time:115538ms step_avg:58.74ms
step:1968/2110 train_time:115627ms step_avg:58.75ms
step:1969/2110 train_time:115713ms step_avg:58.77ms
step:1970/2110 train_time:115801ms step_avg:58.78ms
step:1971/2110 train_time:115890ms step_avg:58.80ms
step:1972/2110 train_time:115977ms step_avg:58.81ms
step:1973/2110 train_time:116066ms step_avg:58.83ms
step:1974/2110 train_time:116153ms step_avg:58.84ms
step:1975/2110 train_time:116241ms step_avg:58.86ms
step:1976/2110 train_time:116328ms step_avg:58.87ms
step:1977/2110 train_time:116416ms step_avg:58.89ms
step:1978/2110 train_time:116503ms step_avg:58.90ms
step:1979/2110 train_time:116592ms step_avg:58.91ms
step:1980/2110 train_time:116680ms step_avg:58.93ms
step:1981/2110 train_time:116767ms step_avg:58.94ms
step:1982/2110 train_time:116854ms step_avg:58.96ms
step:1983/2110 train_time:116944ms step_avg:58.97ms
step:1984/2110 train_time:117031ms step_avg:58.99ms
step:1985/2110 train_time:117119ms step_avg:59.00ms
step:1986/2110 train_time:117207ms step_avg:59.02ms
step:1987/2110 train_time:117295ms step_avg:59.03ms
step:1988/2110 train_time:117383ms step_avg:59.05ms
step:1989/2110 train_time:117471ms step_avg:59.06ms
step:1990/2110 train_time:117558ms step_avg:59.07ms
step:1991/2110 train_time:117647ms step_avg:59.09ms
step:1992/2110 train_time:117733ms step_avg:59.10ms
step:1993/2110 train_time:117823ms step_avg:59.12ms
step:1994/2110 train_time:117911ms step_avg:59.13ms
step:1995/2110 train_time:117998ms step_avg:59.15ms
step:1996/2110 train_time:118086ms step_avg:59.16ms
step:1997/2110 train_time:118174ms step_avg:59.18ms
step:1998/2110 train_time:118260ms step_avg:59.19ms
step:1999/2110 train_time:118349ms step_avg:59.20ms
step:2000/2110 train_time:118437ms step_avg:59.22ms
step:2000/2110 val_loss:3.3031 train_time:118527ms step_avg:59.26ms
step:2001/2110 train_time:118556ms step_avg:59.25ms
step:2002/2110 train_time:118619ms step_avg:59.25ms
step:2003/2110 train_time:118715ms step_avg:59.27ms
step:2004/2110 train_time:118803ms step_avg:59.28ms
step:2005/2110 train_time:118892ms step_avg:59.30ms
step:2006/2110 train_time:118978ms step_avg:59.31ms
step:2007/2110 train_time:119066ms step_avg:59.33ms
step:2008/2110 train_time:119152ms step_avg:59.34ms
step:2009/2110 train_time:119239ms step_avg:59.35ms
step:2010/2110 train_time:119327ms step_avg:59.37ms
step:2011/2110 train_time:119413ms step_avg:59.38ms
step:2012/2110 train_time:119501ms step_avg:59.39ms
step:2013/2110 train_time:119594ms step_avg:59.41ms
step:2014/2110 train_time:119685ms step_avg:59.43ms
step:2015/2110 train_time:119775ms step_avg:59.44ms
step:2016/2110 train_time:119862ms step_avg:59.46ms
step:2017/2110 train_time:119952ms step_avg:59.47ms
step:2018/2110 train_time:120038ms step_avg:59.48ms
step:2019/2110 train_time:120125ms step_avg:59.50ms
step:2020/2110 train_time:120213ms step_avg:59.51ms
step:2021/2110 train_time:120299ms step_avg:59.52ms
step:2022/2110 train_time:120386ms step_avg:59.54ms
step:2023/2110 train_time:120476ms step_avg:59.55ms
step:2024/2110 train_time:120564ms step_avg:59.57ms
step:2025/2110 train_time:120655ms step_avg:59.58ms
step:2026/2110 train_time:120744ms step_avg:59.60ms
step:2027/2110 train_time:120834ms step_avg:59.61ms
step:2028/2110 train_time:120920ms step_avg:59.63ms
step:2029/2110 train_time:121009ms step_avg:59.64ms
step:2030/2110 train_time:121096ms step_avg:59.65ms
step:2031/2110 train_time:121183ms step_avg:59.67ms
step:2032/2110 train_time:121270ms step_avg:59.68ms
step:2033/2110 train_time:121358ms step_avg:59.69ms
step:2034/2110 train_time:121446ms step_avg:59.71ms
step:2035/2110 train_time:121536ms step_avg:59.72ms
step:2036/2110 train_time:121624ms step_avg:59.74ms
step:2037/2110 train_time:121714ms step_avg:59.75ms
step:2038/2110 train_time:121801ms step_avg:59.77ms
step:2039/2110 train_time:121891ms step_avg:59.78ms
step:2040/2110 train_time:121979ms step_avg:59.79ms
step:2041/2110 train_time:122067ms step_avg:59.81ms
step:2042/2110 train_time:122155ms step_avg:59.82ms
step:2043/2110 train_time:122242ms step_avg:59.83ms
step:2044/2110 train_time:122329ms step_avg:59.85ms
step:2045/2110 train_time:122418ms step_avg:59.86ms
step:2046/2110 train_time:122505ms step_avg:59.88ms
step:2047/2110 train_time:122594ms step_avg:59.89ms
step:2048/2110 train_time:122682ms step_avg:59.90ms
step:2049/2110 train_time:122773ms step_avg:59.92ms
step:2050/2110 train_time:122860ms step_avg:59.93ms
step:2051/2110 train_time:122950ms step_avg:59.95ms
step:2052/2110 train_time:123037ms step_avg:59.96ms
step:2053/2110 train_time:123125ms step_avg:59.97ms
step:2054/2110 train_time:123213ms step_avg:59.99ms
step:2055/2110 train_time:123300ms step_avg:60.00ms
step:2056/2110 train_time:123387ms step_avg:60.01ms
step:2057/2110 train_time:123476ms step_avg:60.03ms
step:2058/2110 train_time:123564ms step_avg:60.04ms
step:2059/2110 train_time:123654ms step_avg:60.06ms
step:2060/2110 train_time:123742ms step_avg:60.07ms
step:2061/2110 train_time:123832ms step_avg:60.08ms
step:2062/2110 train_time:123918ms step_avg:60.10ms
step:2063/2110 train_time:124008ms step_avg:60.11ms
step:2064/2110 train_time:124097ms step_avg:60.12ms
step:2065/2110 train_time:124184ms step_avg:60.14ms
step:2066/2110 train_time:124271ms step_avg:60.15ms
step:2067/2110 train_time:124360ms step_avg:60.16ms
step:2068/2110 train_time:124448ms step_avg:60.18ms
step:2069/2110 train_time:124536ms step_avg:60.19ms
step:2070/2110 train_time:124624ms step_avg:60.20ms
step:2071/2110 train_time:124714ms step_avg:60.22ms
step:2072/2110 train_time:124801ms step_avg:60.23ms
step:2073/2110 train_time:124890ms step_avg:60.25ms
step:2074/2110 train_time:124978ms step_avg:60.26ms
step:2075/2110 train_time:125067ms step_avg:60.27ms
step:2076/2110 train_time:125154ms step_avg:60.29ms
step:2077/2110 train_time:125242ms step_avg:60.30ms
step:2078/2110 train_time:125331ms step_avg:60.31ms
step:2079/2110 train_time:125419ms step_avg:60.33ms
step:2080/2110 train_time:125507ms step_avg:60.34ms
step:2081/2110 train_time:125596ms step_avg:60.35ms
step:2082/2110 train_time:125683ms step_avg:60.37ms
step:2083/2110 train_time:125773ms step_avg:60.38ms
step:2084/2110 train_time:125860ms step_avg:60.39ms
step:2085/2110 train_time:125950ms step_avg:60.41ms
step:2086/2110 train_time:126037ms step_avg:60.42ms
step:2087/2110 train_time:126125ms step_avg:60.43ms
step:2088/2110 train_time:126213ms step_avg:60.45ms
step:2089/2110 train_time:126301ms step_avg:60.46ms
step:2090/2110 train_time:126389ms step_avg:60.47ms
step:2091/2110 train_time:126478ms step_avg:60.49ms
step:2092/2110 train_time:126566ms step_avg:60.50ms
step:2093/2110 train_time:126656ms step_avg:60.51ms
step:2094/2110 train_time:126744ms step_avg:60.53ms
step:2095/2110 train_time:126834ms step_avg:60.54ms
step:2096/2110 train_time:126922ms step_avg:60.55ms
step:2097/2110 train_time:127012ms step_avg:60.57ms
step:2098/2110 train_time:127099ms step_avg:60.58ms
step:2099/2110 train_time:127187ms step_avg:60.59ms
step:2100/2110 train_time:127276ms step_avg:60.61ms
step:2101/2110 train_time:127364ms step_avg:60.62ms
step:2102/2110 train_time:127452ms step_avg:60.63ms
step:2103/2110 train_time:127540ms step_avg:60.65ms
step:2104/2110 train_time:127629ms step_avg:60.66ms
step:2105/2110 train_time:127718ms step_avg:60.67ms
step:2106/2110 train_time:127806ms step_avg:60.69ms
step:2107/2110 train_time:127896ms step_avg:60.70ms
step:2108/2110 train_time:127985ms step_avg:60.71ms
step:2109/2110 train_time:128075ms step_avg:60.73ms
step:2110/2110 train_time:128162ms step_avg:60.74ms
step:2110/2110 val_loss:3.2783 train_time:128253ms step_avg:60.78ms
peak memory allocated: 29892 MiB reserved: 43956 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 15 23:44:45 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   38C    P0            126W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   42C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   43C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   36C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              88      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              88      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:105ms step_avg:104.78ms
step:2/2110 train_time:139ms step_avg:69.65ms
step:3/2110 train_time:168ms step_avg:56.10ms
step:4/2110 train_time:197ms step_avg:49.17ms
step:5/2110 train_time:225ms step_avg:44.91ms
step:6/2110 train_time:426ms step_avg:70.99ms
step:7/2110 train_time:610ms step_avg:87.12ms
step:8/2110 train_time:643ms step_avg:80.36ms
step:9/2110 train_time:675ms step_avg:75.02ms
step:10/2110 train_time:708ms step_avg:70.78ms
step:11/2110 train_time:741ms step_avg:67.35ms
step:12/2110 train_time:774ms step_avg:64.52ms
step:13/2110 train_time:807ms step_avg:62.06ms
step:14/2110 train_time:840ms step_avg:60.00ms
step:15/2110 train_time:873ms step_avg:58.19ms
step:16/2110 train_time:906ms step_avg:56.61ms
step:17/2110 train_time:939ms step_avg:55.25ms
step:18/2110 train_time:972ms step_avg:54.00ms
step:19/2110 train_time:1005ms step_avg:52.90ms
step:20/2110 train_time:1039ms step_avg:51.95ms
step:21/2110 train_time:1071ms step_avg:51.00ms
step:22/2110 train_time:1104ms step_avg:50.17ms
step:23/2110 train_time:1137ms step_avg:49.43ms
step:24/2110 train_time:1170ms step_avg:48.74ms
step:25/2110 train_time:1203ms step_avg:48.11ms
step:26/2110 train_time:1236ms step_avg:47.53ms
step:27/2110 train_time:1269ms step_avg:47.00ms
step:28/2110 train_time:1302ms step_avg:46.50ms
step:29/2110 train_time:1335ms step_avg:46.03ms
step:30/2110 train_time:1368ms step_avg:45.59ms
step:31/2110 train_time:1401ms step_avg:45.19ms
step:32/2110 train_time:1434ms step_avg:44.80ms
step:33/2110 train_time:1467ms step_avg:44.45ms
step:34/2110 train_time:1500ms step_avg:44.12ms
step:35/2110 train_time:1534ms step_avg:43.84ms
step:36/2110 train_time:1568ms step_avg:43.56ms
step:37/2110 train_time:1602ms step_avg:43.29ms
step:38/2110 train_time:1635ms step_avg:43.03ms
step:39/2110 train_time:1669ms step_avg:42.79ms
step:40/2110 train_time:1702ms step_avg:42.54ms
step:41/2110 train_time:1735ms step_avg:42.32ms
step:42/2110 train_time:1769ms step_avg:42.11ms
step:43/2110 train_time:1802ms step_avg:41.90ms
step:44/2110 train_time:1835ms step_avg:41.70ms
step:45/2110 train_time:1868ms step_avg:41.50ms
step:46/2110 train_time:1901ms step_avg:41.32ms
step:47/2110 train_time:1934ms step_avg:41.14ms
step:48/2110 train_time:1966ms step_avg:40.97ms
step:49/2110 train_time:2000ms step_avg:40.81ms
step:50/2110 train_time:2032ms step_avg:40.65ms
step:51/2110 train_time:2066ms step_avg:40.50ms
step:52/2110 train_time:2098ms step_avg:40.35ms
step:53/2110 train_time:2132ms step_avg:40.22ms
step:54/2110 train_time:2165ms step_avg:40.09ms
step:55/2110 train_time:2198ms step_avg:39.96ms
step:56/2110 train_time:2231ms step_avg:39.84ms
step:57/2110 train_time:2264ms step_avg:39.73ms
step:58/2110 train_time:2298ms step_avg:39.62ms
step:59/2110 train_time:2330ms step_avg:39.49ms
step:60/2110 train_time:2363ms step_avg:39.38ms
step:61/2110 train_time:2396ms step_avg:39.27ms
step:62/2110 train_time:2429ms step_avg:39.17ms
step:63/2110 train_time:2462ms step_avg:39.08ms
step:64/2110 train_time:2496ms step_avg:38.99ms
step:65/2110 train_time:2529ms step_avg:38.90ms
step:66/2110 train_time:2562ms step_avg:38.82ms
step:67/2110 train_time:2595ms step_avg:38.74ms
step:68/2110 train_time:2628ms step_avg:38.65ms
step:69/2110 train_time:2662ms step_avg:38.58ms
step:70/2110 train_time:2695ms step_avg:38.50ms
step:71/2110 train_time:2729ms step_avg:38.43ms
step:72/2110 train_time:2762ms step_avg:38.36ms
step:73/2110 train_time:2796ms step_avg:38.30ms
step:74/2110 train_time:2829ms step_avg:38.22ms
step:75/2110 train_time:2862ms step_avg:38.16ms
step:76/2110 train_time:2895ms step_avg:38.09ms
step:77/2110 train_time:2928ms step_avg:38.03ms
step:78/2110 train_time:2961ms step_avg:37.96ms
step:79/2110 train_time:2994ms step_avg:37.90ms
step:80/2110 train_time:3027ms step_avg:37.84ms
step:81/2110 train_time:3061ms step_avg:37.79ms
step:82/2110 train_time:3094ms step_avg:37.73ms
step:83/2110 train_time:3127ms step_avg:37.67ms
step:84/2110 train_time:3160ms step_avg:37.61ms
step:85/2110 train_time:3193ms step_avg:37.56ms
step:86/2110 train_time:3226ms step_avg:37.51ms
step:87/2110 train_time:3259ms step_avg:37.46ms
step:88/2110 train_time:3292ms step_avg:37.41ms
step:89/2110 train_time:3325ms step_avg:37.36ms
step:90/2110 train_time:3358ms step_avg:37.31ms
step:91/2110 train_time:3391ms step_avg:37.26ms
step:92/2110 train_time:3423ms step_avg:37.21ms
step:93/2110 train_time:3457ms step_avg:37.17ms
step:94/2110 train_time:3489ms step_avg:37.12ms
step:95/2110 train_time:3523ms step_avg:37.08ms
step:96/2110 train_time:3555ms step_avg:37.04ms
step:97/2110 train_time:3589ms step_avg:37.00ms
step:98/2110 train_time:3622ms step_avg:36.96ms
step:99/2110 train_time:3655ms step_avg:36.92ms
step:100/2110 train_time:3688ms step_avg:36.88ms
step:101/2110 train_time:3722ms step_avg:36.85ms
step:102/2110 train_time:3754ms step_avg:36.81ms
step:103/2110 train_time:3788ms step_avg:36.78ms
step:104/2110 train_time:3821ms step_avg:36.74ms
step:105/2110 train_time:3855ms step_avg:36.71ms
step:106/2110 train_time:3888ms step_avg:36.68ms
step:107/2110 train_time:3921ms step_avg:36.64ms
step:108/2110 train_time:3953ms step_avg:36.60ms
step:109/2110 train_time:3987ms step_avg:36.57ms
step:110/2110 train_time:4020ms step_avg:36.54ms
step:111/2110 train_time:4053ms step_avg:36.51ms
step:112/2110 train_time:4086ms step_avg:36.48ms
step:113/2110 train_time:4119ms step_avg:36.45ms
step:114/2110 train_time:4152ms step_avg:36.42ms
step:115/2110 train_time:4185ms step_avg:36.39ms
step:116/2110 train_time:4219ms step_avg:36.37ms
step:117/2110 train_time:4251ms step_avg:36.34ms
step:118/2110 train_time:4285ms step_avg:36.31ms
step:119/2110 train_time:4318ms step_avg:36.29ms
step:120/2110 train_time:4351ms step_avg:36.26ms
step:121/2110 train_time:4384ms step_avg:36.23ms
step:122/2110 train_time:4417ms step_avg:36.21ms
step:123/2110 train_time:4451ms step_avg:36.18ms
step:124/2110 train_time:4484ms step_avg:36.16ms
step:125/2110 train_time:4517ms step_avg:36.13ms
step:126/2110 train_time:4549ms step_avg:36.11ms
step:127/2110 train_time:4583ms step_avg:36.09ms
step:128/2110 train_time:4616ms step_avg:36.06ms
step:129/2110 train_time:4649ms step_avg:36.04ms
step:130/2110 train_time:4682ms step_avg:36.01ms
step:131/2110 train_time:4715ms step_avg:35.99ms
step:132/2110 train_time:4749ms step_avg:35.97ms
step:133/2110 train_time:4782ms step_avg:35.96ms
step:134/2110 train_time:4815ms step_avg:35.93ms
step:135/2110 train_time:4848ms step_avg:35.91ms
step:136/2110 train_time:4881ms step_avg:35.89ms
step:137/2110 train_time:4914ms step_avg:35.87ms
step:138/2110 train_time:4947ms step_avg:35.85ms
step:139/2110 train_time:4980ms step_avg:35.83ms
step:140/2110 train_time:5013ms step_avg:35.81ms
step:141/2110 train_time:5046ms step_avg:35.78ms
step:142/2110 train_time:5078ms step_avg:35.76ms
step:143/2110 train_time:5111ms step_avg:35.74ms
step:144/2110 train_time:5144ms step_avg:35.72ms
step:145/2110 train_time:5177ms step_avg:35.71ms
step:146/2110 train_time:5210ms step_avg:35.69ms
step:147/2110 train_time:5243ms step_avg:35.67ms
step:148/2110 train_time:5276ms step_avg:35.65ms
step:149/2110 train_time:5309ms step_avg:35.63ms
step:150/2110 train_time:5343ms step_avg:35.62ms
step:151/2110 train_time:5376ms step_avg:35.60ms
step:152/2110 train_time:5408ms step_avg:35.58ms
step:153/2110 train_time:5442ms step_avg:35.57ms
step:154/2110 train_time:5474ms step_avg:35.55ms
step:155/2110 train_time:5508ms step_avg:35.53ms
step:156/2110 train_time:5541ms step_avg:35.52ms
step:157/2110 train_time:5574ms step_avg:35.50ms
step:158/2110 train_time:5606ms step_avg:35.48ms
step:159/2110 train_time:5640ms step_avg:35.47ms
step:160/2110 train_time:5673ms step_avg:35.45ms
step:161/2110 train_time:5706ms step_avg:35.44ms
step:162/2110 train_time:5739ms step_avg:35.42ms
step:163/2110 train_time:5772ms step_avg:35.41ms
step:164/2110 train_time:5805ms step_avg:35.40ms
step:165/2110 train_time:5838ms step_avg:35.38ms
step:166/2110 train_time:5871ms step_avg:35.37ms
step:167/2110 train_time:5904ms step_avg:35.35ms
step:168/2110 train_time:5937ms step_avg:35.34ms
step:169/2110 train_time:5970ms step_avg:35.33ms
step:170/2110 train_time:6011ms step_avg:35.36ms
step:171/2110 train_time:6055ms step_avg:35.41ms
step:172/2110 train_time:6095ms step_avg:35.44ms
step:173/2110 train_time:6139ms step_avg:35.49ms
step:174/2110 train_time:6177ms step_avg:35.50ms
step:175/2110 train_time:6220ms step_avg:35.54ms
step:176/2110 train_time:6260ms step_avg:35.57ms
step:177/2110 train_time:6303ms step_avg:35.61ms
step:178/2110 train_time:6341ms step_avg:35.62ms
step:179/2110 train_time:6387ms step_avg:35.68ms
step:180/2110 train_time:6425ms step_avg:35.69ms
step:181/2110 train_time:6467ms step_avg:35.73ms
step:182/2110 train_time:6507ms step_avg:35.75ms
step:183/2110 train_time:6549ms step_avg:35.79ms
step:184/2110 train_time:6590ms step_avg:35.81ms
step:185/2110 train_time:6635ms step_avg:35.86ms
step:186/2110 train_time:6674ms step_avg:35.88ms
step:187/2110 train_time:6717ms step_avg:35.92ms
step:188/2110 train_time:6757ms step_avg:35.94ms
step:189/2110 train_time:6799ms step_avg:35.97ms
step:190/2110 train_time:6838ms step_avg:35.99ms
step:191/2110 train_time:6883ms step_avg:36.04ms
step:192/2110 train_time:6923ms step_avg:36.06ms
step:193/2110 train_time:6969ms step_avg:36.11ms
step:194/2110 train_time:7008ms step_avg:36.13ms
step:195/2110 train_time:7052ms step_avg:36.16ms
step:196/2110 train_time:7089ms step_avg:36.17ms
step:197/2110 train_time:7136ms step_avg:36.22ms
step:198/2110 train_time:7177ms step_avg:36.25ms
step:199/2110 train_time:7223ms step_avg:36.29ms
step:200/2110 train_time:7262ms step_avg:36.31ms
step:201/2110 train_time:7307ms step_avg:36.35ms
step:202/2110 train_time:7346ms step_avg:36.37ms
step:203/2110 train_time:7391ms step_avg:36.41ms
step:204/2110 train_time:7431ms step_avg:36.43ms
step:205/2110 train_time:7476ms step_avg:36.47ms
step:206/2110 train_time:7515ms step_avg:36.48ms
step:207/2110 train_time:7559ms step_avg:36.51ms
step:208/2110 train_time:7597ms step_avg:36.52ms
step:209/2110 train_time:7641ms step_avg:36.56ms
step:210/2110 train_time:7681ms step_avg:36.57ms
step:211/2110 train_time:7724ms step_avg:36.61ms
step:212/2110 train_time:7762ms step_avg:36.61ms
step:213/2110 train_time:7796ms step_avg:36.60ms
step:214/2110 train_time:7829ms step_avg:36.58ms
step:215/2110 train_time:7865ms step_avg:36.58ms
step:216/2110 train_time:7899ms step_avg:36.57ms
step:217/2110 train_time:7933ms step_avg:36.56ms
step:218/2110 train_time:7966ms step_avg:36.54ms
step:219/2110 train_time:8001ms step_avg:36.54ms
step:220/2110 train_time:8034ms step_avg:36.52ms
step:221/2110 train_time:8068ms step_avg:36.51ms
step:222/2110 train_time:8101ms step_avg:36.49ms
step:223/2110 train_time:8133ms step_avg:36.47ms
step:224/2110 train_time:8166ms step_avg:36.45ms
step:225/2110 train_time:8199ms step_avg:36.44ms
step:226/2110 train_time:8231ms step_avg:36.42ms
step:227/2110 train_time:8264ms step_avg:36.40ms
step:228/2110 train_time:8296ms step_avg:36.39ms
step:229/2110 train_time:8329ms step_avg:36.37ms
step:230/2110 train_time:8362ms step_avg:36.36ms
step:231/2110 train_time:8395ms step_avg:36.34ms
step:232/2110 train_time:8427ms step_avg:36.33ms
step:233/2110 train_time:8460ms step_avg:36.31ms
step:234/2110 train_time:8493ms step_avg:36.29ms
step:235/2110 train_time:8526ms step_avg:36.28ms
step:236/2110 train_time:8558ms step_avg:36.26ms
step:237/2110 train_time:8592ms step_avg:36.25ms
step:238/2110 train_time:8625ms step_avg:36.24ms
step:239/2110 train_time:8659ms step_avg:36.23ms
step:240/2110 train_time:8692ms step_avg:36.22ms
step:241/2110 train_time:8726ms step_avg:36.21ms
step:242/2110 train_time:8759ms step_avg:36.19ms
step:243/2110 train_time:8792ms step_avg:36.18ms
step:244/2110 train_time:8825ms step_avg:36.17ms
step:245/2110 train_time:8859ms step_avg:36.16ms
step:246/2110 train_time:8892ms step_avg:36.14ms
step:247/2110 train_time:8926ms step_avg:36.14ms
step:248/2110 train_time:8959ms step_avg:36.12ms
step:249/2110 train_time:8992ms step_avg:36.11ms
step:250/2110 train_time:9025ms step_avg:36.10ms
step:250/2110 val_loss:4.2949 train_time:9060ms step_avg:36.24ms
step:251/2110 train_time:9087ms step_avg:36.20ms
step:252/2110 train_time:9112ms step_avg:36.16ms
step:253/2110 train_time:9140ms step_avg:36.12ms
step:254/2110 train_time:9172ms step_avg:36.11ms
step:255/2110 train_time:9211ms step_avg:36.12ms
step:256/2110 train_time:9248ms step_avg:36.12ms
step:257/2110 train_time:9286ms step_avg:36.13ms
step:258/2110 train_time:9321ms step_avg:36.13ms
step:259/2110 train_time:9358ms step_avg:36.13ms
step:260/2110 train_time:9391ms step_avg:36.12ms
step:261/2110 train_time:9427ms step_avg:36.12ms
step:262/2110 train_time:9461ms step_avg:36.11ms
step:263/2110 train_time:9499ms step_avg:36.12ms
step:264/2110 train_time:9533ms step_avg:36.11ms
step:265/2110 train_time:9571ms step_avg:36.12ms
step:266/2110 train_time:9607ms step_avg:36.11ms
step:267/2110 train_time:9646ms step_avg:36.13ms
step:268/2110 train_time:9680ms step_avg:36.12ms
step:269/2110 train_time:9719ms step_avg:36.13ms
step:270/2110 train_time:9753ms step_avg:36.12ms
step:271/2110 train_time:9789ms step_avg:36.12ms
step:272/2110 train_time:9824ms step_avg:36.12ms
step:273/2110 train_time:9863ms step_avg:36.13ms
step:274/2110 train_time:9897ms step_avg:36.12ms
step:275/2110 train_time:9933ms step_avg:36.12ms
step:276/2110 train_time:9967ms step_avg:36.11ms
step:277/2110 train_time:10005ms step_avg:36.12ms
step:278/2110 train_time:10040ms step_avg:36.12ms
step:279/2110 train_time:10077ms step_avg:36.12ms
step:280/2110 train_time:10109ms step_avg:36.11ms
step:281/2110 train_time:10145ms step_avg:36.10ms
step:282/2110 train_time:10178ms step_avg:36.09ms
step:283/2110 train_time:10211ms step_avg:36.08ms
step:284/2110 train_time:10244ms step_avg:36.07ms
step:285/2110 train_time:10278ms step_avg:36.06ms
step:286/2110 train_time:10310ms step_avg:36.05ms
step:287/2110 train_time:10344ms step_avg:36.04ms
step:288/2110 train_time:10379ms step_avg:36.04ms
step:289/2110 train_time:10412ms step_avg:36.03ms
step:290/2110 train_time:10445ms step_avg:36.02ms
step:291/2110 train_time:10483ms step_avg:36.02ms
step:292/2110 train_time:10516ms step_avg:36.01ms
step:293/2110 train_time:10553ms step_avg:36.02ms
step:294/2110 train_time:10587ms step_avg:36.01ms
step:295/2110 train_time:10624ms step_avg:36.02ms
step:296/2110 train_time:10658ms step_avg:36.01ms
step:297/2110 train_time:10692ms step_avg:36.00ms
step:298/2110 train_time:10726ms step_avg:35.99ms
step:299/2110 train_time:10761ms step_avg:35.99ms
step:300/2110 train_time:10794ms step_avg:35.98ms
step:301/2110 train_time:10826ms step_avg:35.97ms
step:302/2110 train_time:10859ms step_avg:35.96ms
step:303/2110 train_time:10892ms step_avg:35.95ms
step:304/2110 train_time:10925ms step_avg:35.94ms
step:305/2110 train_time:10958ms step_avg:35.93ms
step:306/2110 train_time:10991ms step_avg:35.92ms
step:307/2110 train_time:11024ms step_avg:35.91ms
step:308/2110 train_time:11057ms step_avg:35.90ms
step:309/2110 train_time:11091ms step_avg:35.89ms
step:310/2110 train_time:11124ms step_avg:35.88ms
step:311/2110 train_time:11157ms step_avg:35.87ms
step:312/2110 train_time:11191ms step_avg:35.87ms
step:313/2110 train_time:11223ms step_avg:35.86ms
step:314/2110 train_time:11255ms step_avg:35.85ms
step:315/2110 train_time:11288ms step_avg:35.84ms
step:316/2110 train_time:11321ms step_avg:35.83ms
step:317/2110 train_time:11354ms step_avg:35.82ms
step:318/2110 train_time:11388ms step_avg:35.81ms
step:319/2110 train_time:11420ms step_avg:35.80ms
step:320/2110 train_time:11453ms step_avg:35.79ms
step:321/2110 train_time:11487ms step_avg:35.78ms
step:322/2110 train_time:11520ms step_avg:35.78ms
step:323/2110 train_time:11553ms step_avg:35.77ms
step:324/2110 train_time:11586ms step_avg:35.76ms
step:325/2110 train_time:11619ms step_avg:35.75ms
step:326/2110 train_time:11652ms step_avg:35.74ms
step:327/2110 train_time:11685ms step_avg:35.73ms
step:328/2110 train_time:11718ms step_avg:35.73ms
step:329/2110 train_time:11751ms step_avg:35.72ms
step:330/2110 train_time:11784ms step_avg:35.71ms
step:331/2110 train_time:11817ms step_avg:35.70ms
step:332/2110 train_time:11850ms step_avg:35.69ms
step:333/2110 train_time:11883ms step_avg:35.68ms
step:334/2110 train_time:11916ms step_avg:35.68ms
step:335/2110 train_time:11949ms step_avg:35.67ms
step:336/2110 train_time:11982ms step_avg:35.66ms
step:337/2110 train_time:12015ms step_avg:35.65ms
step:338/2110 train_time:12048ms step_avg:35.64ms
step:339/2110 train_time:12081ms step_avg:35.64ms
step:340/2110 train_time:12114ms step_avg:35.63ms
step:341/2110 train_time:12147ms step_avg:35.62ms
step:342/2110 train_time:12181ms step_avg:35.62ms
step:343/2110 train_time:12216ms step_avg:35.62ms
step:344/2110 train_time:12246ms step_avg:35.60ms
step:345/2110 train_time:12279ms step_avg:35.59ms
step:346/2110 train_time:12312ms step_avg:35.58ms
step:347/2110 train_time:12345ms step_avg:35.58ms
step:348/2110 train_time:12378ms step_avg:35.57ms
step:349/2110 train_time:12411ms step_avg:35.56ms
step:350/2110 train_time:12444ms step_avg:35.55ms
step:351/2110 train_time:12477ms step_avg:35.55ms
step:352/2110 train_time:12510ms step_avg:35.54ms
step:353/2110 train_time:12543ms step_avg:35.53ms
step:354/2110 train_time:12576ms step_avg:35.53ms
step:355/2110 train_time:12609ms step_avg:35.52ms
step:356/2110 train_time:12642ms step_avg:35.51ms
step:357/2110 train_time:12675ms step_avg:35.50ms
step:358/2110 train_time:12708ms step_avg:35.50ms
step:359/2110 train_time:12741ms step_avg:35.49ms
step:360/2110 train_time:12774ms step_avg:35.48ms
step:361/2110 train_time:12807ms step_avg:35.48ms
step:362/2110 train_time:12840ms step_avg:35.47ms
step:363/2110 train_time:12873ms step_avg:35.46ms
step:364/2110 train_time:12906ms step_avg:35.46ms
step:365/2110 train_time:12939ms step_avg:35.45ms
step:366/2110 train_time:12972ms step_avg:35.44ms
step:367/2110 train_time:13005ms step_avg:35.44ms
step:368/2110 train_time:13039ms step_avg:35.43ms
step:369/2110 train_time:13071ms step_avg:35.42ms
step:370/2110 train_time:13104ms step_avg:35.42ms
step:371/2110 train_time:13137ms step_avg:35.41ms
step:372/2110 train_time:13170ms step_avg:35.40ms
step:373/2110 train_time:13204ms step_avg:35.40ms
step:374/2110 train_time:13237ms step_avg:35.39ms
step:375/2110 train_time:13273ms step_avg:35.39ms
step:376/2110 train_time:13305ms step_avg:35.39ms
step:377/2110 train_time:13339ms step_avg:35.38ms
step:378/2110 train_time:13371ms step_avg:35.37ms
step:379/2110 train_time:13404ms step_avg:35.37ms
step:380/2110 train_time:13437ms step_avg:35.36ms
step:381/2110 train_time:13471ms step_avg:35.36ms
step:382/2110 train_time:13503ms step_avg:35.35ms
step:383/2110 train_time:13538ms step_avg:35.35ms
step:384/2110 train_time:13571ms step_avg:35.34ms
step:385/2110 train_time:13605ms step_avg:35.34ms
step:386/2110 train_time:13637ms step_avg:35.33ms
step:387/2110 train_time:13673ms step_avg:35.33ms
step:388/2110 train_time:13706ms step_avg:35.33ms
step:389/2110 train_time:13740ms step_avg:35.32ms
step:390/2110 train_time:13773ms step_avg:35.31ms
step:391/2110 train_time:13807ms step_avg:35.31ms
step:392/2110 train_time:13841ms step_avg:35.31ms
step:393/2110 train_time:13877ms step_avg:35.31ms
step:394/2110 train_time:13909ms step_avg:35.30ms
step:395/2110 train_time:13942ms step_avg:35.30ms
step:396/2110 train_time:13976ms step_avg:35.29ms
step:397/2110 train_time:14011ms step_avg:35.29ms
step:398/2110 train_time:14044ms step_avg:35.29ms
step:399/2110 train_time:14078ms step_avg:35.28ms
step:400/2110 train_time:14110ms step_avg:35.28ms
step:401/2110 train_time:14143ms step_avg:35.27ms
step:402/2110 train_time:14176ms step_avg:35.26ms
step:403/2110 train_time:14213ms step_avg:35.27ms
step:404/2110 train_time:14249ms step_avg:35.27ms
step:405/2110 train_time:14284ms step_avg:35.27ms
step:406/2110 train_time:14317ms step_avg:35.26ms
step:407/2110 train_time:14356ms step_avg:35.27ms
step:408/2110 train_time:14392ms step_avg:35.27ms
step:409/2110 train_time:14430ms step_avg:35.28ms
step:410/2110 train_time:14465ms step_avg:35.28ms
step:411/2110 train_time:14504ms step_avg:35.29ms
step:412/2110 train_time:14541ms step_avg:35.29ms
step:413/2110 train_time:14581ms step_avg:35.31ms
step:414/2110 train_time:14615ms step_avg:35.30ms
step:415/2110 train_time:14655ms step_avg:35.31ms
step:416/2110 train_time:14690ms step_avg:35.31ms
step:417/2110 train_time:14729ms step_avg:35.32ms
step:418/2110 train_time:14764ms step_avg:35.32ms
step:419/2110 train_time:14804ms step_avg:35.33ms
step:420/2110 train_time:14839ms step_avg:35.33ms
step:421/2110 train_time:14878ms step_avg:35.34ms
step:422/2110 train_time:14912ms step_avg:35.34ms
step:423/2110 train_time:14953ms step_avg:35.35ms
step:424/2110 train_time:14987ms step_avg:35.35ms
step:425/2110 train_time:15027ms step_avg:35.36ms
step:426/2110 train_time:15063ms step_avg:35.36ms
step:427/2110 train_time:15102ms step_avg:35.37ms
step:428/2110 train_time:15136ms step_avg:35.37ms
step:429/2110 train_time:15176ms step_avg:35.37ms
step:430/2110 train_time:15209ms step_avg:35.37ms
step:431/2110 train_time:15242ms step_avg:35.36ms
step:432/2110 train_time:15275ms step_avg:35.36ms
step:433/2110 train_time:15307ms step_avg:35.35ms
step:434/2110 train_time:15341ms step_avg:35.35ms
step:435/2110 train_time:15373ms step_avg:35.34ms
step:436/2110 train_time:15405ms step_avg:35.33ms
step:437/2110 train_time:15438ms step_avg:35.33ms
step:438/2110 train_time:15471ms step_avg:35.32ms
step:439/2110 train_time:15503ms step_avg:35.32ms
step:440/2110 train_time:15537ms step_avg:35.31ms
step:441/2110 train_time:15569ms step_avg:35.30ms
step:442/2110 train_time:15602ms step_avg:35.30ms
step:443/2110 train_time:15636ms step_avg:35.30ms
step:444/2110 train_time:15669ms step_avg:35.29ms
step:445/2110 train_time:15701ms step_avg:35.28ms
step:446/2110 train_time:15736ms step_avg:35.28ms
step:447/2110 train_time:15774ms step_avg:35.29ms
step:448/2110 train_time:15808ms step_avg:35.29ms
step:449/2110 train_time:15846ms step_avg:35.29ms
step:450/2110 train_time:15880ms step_avg:35.29ms
step:451/2110 train_time:15918ms step_avg:35.29ms
step:452/2110 train_time:15951ms step_avg:35.29ms
step:453/2110 train_time:15986ms step_avg:35.29ms
step:454/2110 train_time:16019ms step_avg:35.28ms
step:455/2110 train_time:16055ms step_avg:35.29ms
step:456/2110 train_time:16089ms step_avg:35.28ms
step:457/2110 train_time:16125ms step_avg:35.29ms
step:458/2110 train_time:16159ms step_avg:35.28ms
step:459/2110 train_time:16195ms step_avg:35.28ms
step:460/2110 train_time:16228ms step_avg:35.28ms
step:461/2110 train_time:16267ms step_avg:35.29ms
step:462/2110 train_time:16299ms step_avg:35.28ms
step:463/2110 train_time:16335ms step_avg:35.28ms
step:464/2110 train_time:16371ms step_avg:35.28ms
step:465/2110 train_time:16408ms step_avg:35.29ms
step:466/2110 train_time:16443ms step_avg:35.29ms
step:467/2110 train_time:16481ms step_avg:35.29ms
step:468/2110 train_time:16516ms step_avg:35.29ms
step:469/2110 train_time:16555ms step_avg:35.30ms
step:470/2110 train_time:16590ms step_avg:35.30ms
step:471/2110 train_time:16628ms step_avg:35.30ms
step:472/2110 train_time:16663ms step_avg:35.30ms
step:473/2110 train_time:16701ms step_avg:35.31ms
step:474/2110 train_time:16735ms step_avg:35.31ms
step:475/2110 train_time:16772ms step_avg:35.31ms
step:476/2110 train_time:16806ms step_avg:35.31ms
step:477/2110 train_time:16845ms step_avg:35.31ms
step:478/2110 train_time:16878ms step_avg:35.31ms
step:479/2110 train_time:16915ms step_avg:35.31ms
step:480/2110 train_time:16949ms step_avg:35.31ms
step:481/2110 train_time:16987ms step_avg:35.32ms
step:482/2110 train_time:17020ms step_avg:35.31ms
step:483/2110 train_time:17058ms step_avg:35.32ms
step:484/2110 train_time:17091ms step_avg:35.31ms
step:485/2110 train_time:17125ms step_avg:35.31ms
step:486/2110 train_time:17157ms step_avg:35.30ms
step:487/2110 train_time:17191ms step_avg:35.30ms
step:488/2110 train_time:17223ms step_avg:35.29ms
step:489/2110 train_time:17256ms step_avg:35.29ms
step:490/2110 train_time:17289ms step_avg:35.28ms
step:491/2110 train_time:17322ms step_avg:35.28ms
step:492/2110 train_time:17355ms step_avg:35.27ms
step:493/2110 train_time:17387ms step_avg:35.27ms
step:494/2110 train_time:17420ms step_avg:35.26ms
step:495/2110 train_time:17453ms step_avg:35.26ms
step:496/2110 train_time:17486ms step_avg:35.25ms
step:497/2110 train_time:17518ms step_avg:35.25ms
step:498/2110 train_time:17551ms step_avg:35.24ms
step:499/2110 train_time:17584ms step_avg:35.24ms
step:500/2110 train_time:17617ms step_avg:35.23ms
step:500/2110 val_loss:4.0296 train_time:17653ms step_avg:35.31ms
step:501/2110 train_time:17679ms step_avg:35.29ms
step:502/2110 train_time:17706ms step_avg:35.27ms
step:503/2110 train_time:17731ms step_avg:35.25ms
step:504/2110 train_time:17760ms step_avg:35.24ms
step:505/2110 train_time:17795ms step_avg:35.24ms
step:506/2110 train_time:17832ms step_avg:35.24ms
step:507/2110 train_time:17869ms step_avg:35.24ms
step:508/2110 train_time:17907ms step_avg:35.25ms
step:509/2110 train_time:17946ms step_avg:35.26ms
step:510/2110 train_time:17982ms step_avg:35.26ms
step:511/2110 train_time:18022ms step_avg:35.27ms
step:512/2110 train_time:18060ms step_avg:35.27ms
step:513/2110 train_time:18099ms step_avg:35.28ms
step:514/2110 train_time:18136ms step_avg:35.28ms
step:515/2110 train_time:18176ms step_avg:35.29ms
step:516/2110 train_time:18212ms step_avg:35.29ms
step:517/2110 train_time:18251ms step_avg:35.30ms
step:518/2110 train_time:18286ms step_avg:35.30ms
step:519/2110 train_time:18326ms step_avg:35.31ms
step:520/2110 train_time:18362ms step_avg:35.31ms
step:521/2110 train_time:18402ms step_avg:35.32ms
step:522/2110 train_time:18436ms step_avg:35.32ms
step:523/2110 train_time:18471ms step_avg:35.32ms
step:524/2110 train_time:18503ms step_avg:35.31ms
step:525/2110 train_time:18540ms step_avg:35.31ms
step:526/2110 train_time:18577ms step_avg:35.32ms
step:527/2110 train_time:18616ms step_avg:35.32ms
step:528/2110 train_time:18653ms step_avg:35.33ms
step:529/2110 train_time:18691ms step_avg:35.33ms
step:530/2110 train_time:18728ms step_avg:35.34ms
step:531/2110 train_time:18766ms step_avg:35.34ms
step:532/2110 train_time:18804ms step_avg:35.35ms
step:533/2110 train_time:18844ms step_avg:35.35ms
step:534/2110 train_time:18881ms step_avg:35.36ms
step:535/2110 train_time:18920ms step_avg:35.36ms
step:536/2110 train_time:18957ms step_avg:35.37ms
step:537/2110 train_time:18996ms step_avg:35.38ms
step:538/2110 train_time:19035ms step_avg:35.38ms
step:539/2110 train_time:19074ms step_avg:35.39ms
step:540/2110 train_time:19110ms step_avg:35.39ms
step:541/2110 train_time:19151ms step_avg:35.40ms
step:542/2110 train_time:19188ms step_avg:35.40ms
step:543/2110 train_time:19228ms step_avg:35.41ms
step:544/2110 train_time:19265ms step_avg:35.41ms
step:545/2110 train_time:19304ms step_avg:35.42ms
step:546/2110 train_time:19341ms step_avg:35.42ms
step:547/2110 train_time:19378ms step_avg:35.43ms
step:548/2110 train_time:19416ms step_avg:35.43ms
step:549/2110 train_time:19455ms step_avg:35.44ms
step:550/2110 train_time:19492ms step_avg:35.44ms
step:551/2110 train_time:19531ms step_avg:35.45ms
step:552/2110 train_time:19568ms step_avg:35.45ms
step:553/2110 train_time:19607ms step_avg:35.45ms
step:554/2110 train_time:19643ms step_avg:35.46ms
step:555/2110 train_time:19684ms step_avg:35.47ms
step:556/2110 train_time:19720ms step_avg:35.47ms
step:557/2110 train_time:19760ms step_avg:35.48ms
step:558/2110 train_time:19796ms step_avg:35.48ms
step:559/2110 train_time:19833ms step_avg:35.48ms
step:560/2110 train_time:19869ms step_avg:35.48ms
step:561/2110 train_time:19907ms step_avg:35.48ms
step:562/2110 train_time:19943ms step_avg:35.49ms
step:563/2110 train_time:19978ms step_avg:35.49ms
step:564/2110 train_time:20012ms step_avg:35.48ms
step:565/2110 train_time:20047ms step_avg:35.48ms
step:566/2110 train_time:20079ms step_avg:35.48ms
step:567/2110 train_time:20113ms step_avg:35.47ms
step:568/2110 train_time:20146ms step_avg:35.47ms
step:569/2110 train_time:20181ms step_avg:35.47ms
step:570/2110 train_time:20214ms step_avg:35.46ms
step:571/2110 train_time:20247ms step_avg:35.46ms
step:572/2110 train_time:20280ms step_avg:35.45ms
step:573/2110 train_time:20314ms step_avg:35.45ms
step:574/2110 train_time:20346ms step_avg:35.45ms
step:575/2110 train_time:20384ms step_avg:35.45ms
step:576/2110 train_time:20417ms step_avg:35.45ms
step:577/2110 train_time:20450ms step_avg:35.44ms
step:578/2110 train_time:20487ms step_avg:35.44ms
step:579/2110 train_time:20524ms step_avg:35.45ms
step:580/2110 train_time:20557ms step_avg:35.44ms
step:581/2110 train_time:20592ms step_avg:35.44ms
step:582/2110 train_time:20630ms step_avg:35.45ms
step:583/2110 train_time:20668ms step_avg:35.45ms
step:584/2110 train_time:20702ms step_avg:35.45ms
step:585/2110 train_time:20736ms step_avg:35.45ms
step:586/2110 train_time:20771ms step_avg:35.45ms
step:587/2110 train_time:20810ms step_avg:35.45ms
step:588/2110 train_time:20843ms step_avg:35.45ms
step:589/2110 train_time:20876ms step_avg:35.44ms
step:590/2110 train_time:20912ms step_avg:35.44ms
step:591/2110 train_time:20952ms step_avg:35.45ms
step:592/2110 train_time:20987ms step_avg:35.45ms
step:593/2110 train_time:21021ms step_avg:35.45ms
step:594/2110 train_time:21057ms step_avg:35.45ms
step:595/2110 train_time:21097ms step_avg:35.46ms
step:596/2110 train_time:21130ms step_avg:35.45ms
step:597/2110 train_time:21163ms step_avg:35.45ms
step:598/2110 train_time:21199ms step_avg:35.45ms
step:599/2110 train_time:21235ms step_avg:35.45ms
step:600/2110 train_time:21269ms step_avg:35.45ms
step:601/2110 train_time:21308ms step_avg:35.45ms
step:602/2110 train_time:21342ms step_avg:35.45ms
step:603/2110 train_time:21378ms step_avg:35.45ms
step:604/2110 train_time:21414ms step_avg:35.45ms
step:605/2110 train_time:21452ms step_avg:35.46ms
step:606/2110 train_time:21484ms step_avg:35.45ms
step:607/2110 train_time:21523ms step_avg:35.46ms
step:608/2110 train_time:21559ms step_avg:35.46ms
step:609/2110 train_time:21594ms step_avg:35.46ms
step:610/2110 train_time:21628ms step_avg:35.46ms
step:611/2110 train_time:21667ms step_avg:35.46ms
step:612/2110 train_time:21699ms step_avg:35.46ms
step:613/2110 train_time:21734ms step_avg:35.45ms
step:614/2110 train_time:21775ms step_avg:35.46ms
step:615/2110 train_time:21812ms step_avg:35.47ms
step:616/2110 train_time:21845ms step_avg:35.46ms
step:617/2110 train_time:21883ms step_avg:35.47ms
step:618/2110 train_time:21918ms step_avg:35.47ms
step:619/2110 train_time:21954ms step_avg:35.47ms
step:620/2110 train_time:21987ms step_avg:35.46ms
step:621/2110 train_time:22021ms step_avg:35.46ms
step:622/2110 train_time:22054ms step_avg:35.46ms
step:623/2110 train_time:22097ms step_avg:35.47ms
step:624/2110 train_time:22130ms step_avg:35.46ms
step:625/2110 train_time:22159ms step_avg:35.46ms
step:626/2110 train_time:22194ms step_avg:35.45ms
step:627/2110 train_time:22227ms step_avg:35.45ms
step:628/2110 train_time:22260ms step_avg:35.45ms
step:629/2110 train_time:22296ms step_avg:35.45ms
step:630/2110 train_time:22329ms step_avg:35.44ms
step:631/2110 train_time:22362ms step_avg:35.44ms
step:632/2110 train_time:22395ms step_avg:35.44ms
step:633/2110 train_time:22427ms step_avg:35.43ms
step:634/2110 train_time:22460ms step_avg:35.43ms
step:635/2110 train_time:22494ms step_avg:35.42ms
step:636/2110 train_time:22527ms step_avg:35.42ms
step:637/2110 train_time:22561ms step_avg:35.42ms
step:638/2110 train_time:22593ms step_avg:35.41ms
step:639/2110 train_time:22627ms step_avg:35.41ms
step:640/2110 train_time:22660ms step_avg:35.41ms
step:641/2110 train_time:22694ms step_avg:35.40ms
step:642/2110 train_time:22728ms step_avg:35.40ms
step:643/2110 train_time:22763ms step_avg:35.40ms
step:644/2110 train_time:22795ms step_avg:35.40ms
step:645/2110 train_time:22827ms step_avg:35.39ms
step:646/2110 train_time:22861ms step_avg:35.39ms
step:647/2110 train_time:22894ms step_avg:35.38ms
step:648/2110 train_time:22926ms step_avg:35.38ms
step:649/2110 train_time:22959ms step_avg:35.38ms
step:650/2110 train_time:22993ms step_avg:35.37ms
step:651/2110 train_time:23026ms step_avg:35.37ms
step:652/2110 train_time:23059ms step_avg:35.37ms
step:653/2110 train_time:23092ms step_avg:35.36ms
step:654/2110 train_time:23125ms step_avg:35.36ms
step:655/2110 train_time:23159ms step_avg:35.36ms
step:656/2110 train_time:23191ms step_avg:35.35ms
step:657/2110 train_time:23225ms step_avg:35.35ms
step:658/2110 train_time:23261ms step_avg:35.35ms
step:659/2110 train_time:23295ms step_avg:35.35ms
step:660/2110 train_time:23329ms step_avg:35.35ms
step:661/2110 train_time:23361ms step_avg:35.34ms
step:662/2110 train_time:23394ms step_avg:35.34ms
step:663/2110 train_time:23428ms step_avg:35.34ms
step:664/2110 train_time:23460ms step_avg:35.33ms
step:665/2110 train_time:23493ms step_avg:35.33ms
step:666/2110 train_time:23526ms step_avg:35.32ms
step:667/2110 train_time:23559ms step_avg:35.32ms
step:668/2110 train_time:23592ms step_avg:35.32ms
step:669/2110 train_time:23625ms step_avg:35.31ms
step:670/2110 train_time:23658ms step_avg:35.31ms
step:671/2110 train_time:23691ms step_avg:35.31ms
step:672/2110 train_time:23724ms step_avg:35.30ms
step:673/2110 train_time:23757ms step_avg:35.30ms
step:674/2110 train_time:23791ms step_avg:35.30ms
step:675/2110 train_time:23824ms step_avg:35.29ms
step:676/2110 train_time:23857ms step_avg:35.29ms
step:677/2110 train_time:23890ms step_avg:35.29ms
step:678/2110 train_time:23922ms step_avg:35.28ms
step:679/2110 train_time:23956ms step_avg:35.28ms
step:680/2110 train_time:23988ms step_avg:35.28ms
step:681/2110 train_time:24022ms step_avg:35.27ms
step:682/2110 train_time:24056ms step_avg:35.27ms
step:683/2110 train_time:24088ms step_avg:35.27ms
step:684/2110 train_time:24121ms step_avg:35.26ms
step:685/2110 train_time:24154ms step_avg:35.26ms
step:686/2110 train_time:24187ms step_avg:35.26ms
step:687/2110 train_time:24221ms step_avg:35.26ms
step:688/2110 train_time:24253ms step_avg:35.25ms
step:689/2110 train_time:24287ms step_avg:35.25ms
step:690/2110 train_time:24319ms step_avg:35.25ms
step:691/2110 train_time:24354ms step_avg:35.24ms
step:692/2110 train_time:24411ms step_avg:35.28ms
step:693/2110 train_time:24471ms step_avg:35.31ms
step:694/2110 train_time:24530ms step_avg:35.35ms
step:695/2110 train_time:24589ms step_avg:35.38ms
step:696/2110 train_time:24647ms step_avg:35.41ms
step:697/2110 train_time:24707ms step_avg:35.45ms
step:698/2110 train_time:24765ms step_avg:35.48ms
step:699/2110 train_time:24825ms step_avg:35.52ms
step:700/2110 train_time:24884ms step_avg:35.55ms
step:701/2110 train_time:24944ms step_avg:35.58ms
step:702/2110 train_time:25002ms step_avg:35.62ms
step:703/2110 train_time:25063ms step_avg:35.65ms
step:704/2110 train_time:25121ms step_avg:35.68ms
step:705/2110 train_time:25181ms step_avg:35.72ms
step:706/2110 train_time:25240ms step_avg:35.75ms
step:707/2110 train_time:25300ms step_avg:35.79ms
step:708/2110 train_time:25358ms step_avg:35.82ms
step:709/2110 train_time:25418ms step_avg:35.85ms
step:710/2110 train_time:25477ms step_avg:35.88ms
step:711/2110 train_time:25538ms step_avg:35.92ms
step:712/2110 train_time:25599ms step_avg:35.95ms
step:713/2110 train_time:25658ms step_avg:35.99ms
step:714/2110 train_time:25717ms step_avg:36.02ms
step:715/2110 train_time:25777ms step_avg:36.05ms
step:716/2110 train_time:25836ms step_avg:36.08ms
step:717/2110 train_time:25897ms step_avg:36.12ms
step:718/2110 train_time:25956ms step_avg:36.15ms
step:719/2110 train_time:26016ms step_avg:36.18ms
step:720/2110 train_time:26074ms step_avg:36.21ms
step:721/2110 train_time:26134ms step_avg:36.25ms
step:722/2110 train_time:26193ms step_avg:36.28ms
step:723/2110 train_time:26253ms step_avg:36.31ms
step:724/2110 train_time:26312ms step_avg:36.34ms
step:725/2110 train_time:26372ms step_avg:36.37ms
step:726/2110 train_time:26430ms step_avg:36.41ms
step:727/2110 train_time:26490ms step_avg:36.44ms
step:728/2110 train_time:26549ms step_avg:36.47ms
step:729/2110 train_time:26609ms step_avg:36.50ms
step:730/2110 train_time:26668ms step_avg:36.53ms
step:731/2110 train_time:26728ms step_avg:36.56ms
step:732/2110 train_time:26787ms step_avg:36.59ms
step:733/2110 train_time:26846ms step_avg:36.63ms
step:734/2110 train_time:26905ms step_avg:36.66ms
step:735/2110 train_time:26965ms step_avg:36.69ms
step:736/2110 train_time:27023ms step_avg:36.72ms
step:737/2110 train_time:27084ms step_avg:36.75ms
step:738/2110 train_time:27142ms step_avg:36.78ms
step:739/2110 train_time:27202ms step_avg:36.81ms
step:740/2110 train_time:27260ms step_avg:36.84ms
step:741/2110 train_time:27320ms step_avg:36.87ms
step:742/2110 train_time:27379ms step_avg:36.90ms
step:743/2110 train_time:27439ms step_avg:36.93ms
step:744/2110 train_time:27499ms step_avg:36.96ms
step:745/2110 train_time:27558ms step_avg:36.99ms
step:746/2110 train_time:27617ms step_avg:37.02ms
step:747/2110 train_time:27676ms step_avg:37.05ms
step:748/2110 train_time:27735ms step_avg:37.08ms
step:749/2110 train_time:27795ms step_avg:37.11ms
step:750/2110 train_time:27854ms step_avg:37.14ms
step:750/2110 val_loss:3.9173 train_time:27916ms step_avg:37.22ms
step:751/2110 train_time:27944ms step_avg:37.21ms
step:752/2110 train_time:27976ms step_avg:37.20ms
step:753/2110 train_time:28040ms step_avg:37.24ms
step:754/2110 train_time:28103ms step_avg:37.27ms
step:755/2110 train_time:28166ms step_avg:37.31ms
step:756/2110 train_time:28225ms step_avg:37.33ms
step:757/2110 train_time:28284ms step_avg:37.36ms
step:758/2110 train_time:28342ms step_avg:37.39ms
step:759/2110 train_time:28401ms step_avg:37.42ms
step:760/2110 train_time:28459ms step_avg:37.45ms
step:761/2110 train_time:28518ms step_avg:37.47ms
step:762/2110 train_time:28576ms step_avg:37.50ms
step:763/2110 train_time:28634ms step_avg:37.53ms
step:764/2110 train_time:28692ms step_avg:37.55ms
step:765/2110 train_time:28751ms step_avg:37.58ms
step:766/2110 train_time:28809ms step_avg:37.61ms
step:767/2110 train_time:28869ms step_avg:37.64ms
step:768/2110 train_time:28929ms step_avg:37.67ms
step:769/2110 train_time:28990ms step_avg:37.70ms
step:770/2110 train_time:29051ms step_avg:37.73ms
step:771/2110 train_time:29113ms step_avg:37.76ms
step:772/2110 train_time:29173ms step_avg:37.79ms
step:773/2110 train_time:29234ms step_avg:37.82ms
step:774/2110 train_time:29292ms step_avg:37.85ms
step:775/2110 train_time:29352ms step_avg:37.87ms
step:776/2110 train_time:29411ms step_avg:37.90ms
step:777/2110 train_time:29470ms step_avg:37.93ms
step:778/2110 train_time:29528ms step_avg:37.95ms
step:779/2110 train_time:29587ms step_avg:37.98ms
step:780/2110 train_time:29645ms step_avg:38.01ms
step:781/2110 train_time:29704ms step_avg:38.03ms
step:782/2110 train_time:29762ms step_avg:38.06ms
step:783/2110 train_time:29822ms step_avg:38.09ms
step:784/2110 train_time:29881ms step_avg:38.11ms
step:785/2110 train_time:29942ms step_avg:38.14ms
step:786/2110 train_time:30003ms step_avg:38.17ms
step:787/2110 train_time:30064ms step_avg:38.20ms
step:788/2110 train_time:30123ms step_avg:38.23ms
step:789/2110 train_time:30184ms step_avg:38.26ms
step:790/2110 train_time:30244ms step_avg:38.28ms
step:791/2110 train_time:30304ms step_avg:38.31ms
step:792/2110 train_time:30362ms step_avg:38.34ms
step:793/2110 train_time:30422ms step_avg:38.36ms
step:794/2110 train_time:30481ms step_avg:38.39ms
step:795/2110 train_time:30540ms step_avg:38.42ms
step:796/2110 train_time:30598ms step_avg:38.44ms
step:797/2110 train_time:30657ms step_avg:38.47ms
step:798/2110 train_time:30715ms step_avg:38.49ms
step:799/2110 train_time:30774ms step_avg:38.52ms
step:800/2110 train_time:30833ms step_avg:38.54ms
step:801/2110 train_time:30893ms step_avg:38.57ms
step:802/2110 train_time:30952ms step_avg:38.59ms
step:803/2110 train_time:31013ms step_avg:38.62ms
step:804/2110 train_time:31071ms step_avg:38.65ms
step:805/2110 train_time:31132ms step_avg:38.67ms
step:806/2110 train_time:31191ms step_avg:38.70ms
step:807/2110 train_time:31251ms step_avg:38.73ms
step:808/2110 train_time:31310ms step_avg:38.75ms
step:809/2110 train_time:31370ms step_avg:38.78ms
step:810/2110 train_time:31428ms step_avg:38.80ms
step:811/2110 train_time:31489ms step_avg:38.83ms
step:812/2110 train_time:31548ms step_avg:38.85ms
step:813/2110 train_time:31607ms step_avg:38.88ms
step:814/2110 train_time:31666ms step_avg:38.90ms
step:815/2110 train_time:31725ms step_avg:38.93ms
step:816/2110 train_time:31784ms step_avg:38.95ms
step:817/2110 train_time:31843ms step_avg:38.98ms
step:818/2110 train_time:31903ms step_avg:39.00ms
step:819/2110 train_time:31963ms step_avg:39.03ms
step:820/2110 train_time:32022ms step_avg:39.05ms
step:821/2110 train_time:32082ms step_avg:39.08ms
step:822/2110 train_time:32142ms step_avg:39.10ms
step:823/2110 train_time:32203ms step_avg:39.13ms
step:824/2110 train_time:32262ms step_avg:39.15ms
step:825/2110 train_time:32322ms step_avg:39.18ms
step:826/2110 train_time:32381ms step_avg:39.20ms
step:827/2110 train_time:32440ms step_avg:39.23ms
step:828/2110 train_time:32498ms step_avg:39.25ms
step:829/2110 train_time:32558ms step_avg:39.27ms
step:830/2110 train_time:32617ms step_avg:39.30ms
step:831/2110 train_time:32676ms step_avg:39.32ms
step:832/2110 train_time:32735ms step_avg:39.34ms
step:833/2110 train_time:32795ms step_avg:39.37ms
step:834/2110 train_time:32853ms step_avg:39.39ms
step:835/2110 train_time:32913ms step_avg:39.42ms
step:836/2110 train_time:32971ms step_avg:39.44ms
step:837/2110 train_time:33032ms step_avg:39.47ms
step:838/2110 train_time:33090ms step_avg:39.49ms
step:839/2110 train_time:33151ms step_avg:39.51ms
step:840/2110 train_time:33210ms step_avg:39.54ms
step:841/2110 train_time:33269ms step_avg:39.56ms
step:842/2110 train_time:33328ms step_avg:39.58ms
step:843/2110 train_time:33388ms step_avg:39.61ms
step:844/2110 train_time:33447ms step_avg:39.63ms
step:845/2110 train_time:33506ms step_avg:39.65ms
step:846/2110 train_time:33565ms step_avg:39.67ms
step:847/2110 train_time:33624ms step_avg:39.70ms
step:848/2110 train_time:33683ms step_avg:39.72ms
step:849/2110 train_time:33742ms step_avg:39.74ms
step:850/2110 train_time:33801ms step_avg:39.77ms
step:851/2110 train_time:33861ms step_avg:39.79ms
step:852/2110 train_time:33920ms step_avg:39.81ms
step:853/2110 train_time:33979ms step_avg:39.83ms
step:854/2110 train_time:34037ms step_avg:39.86ms
step:855/2110 train_time:34098ms step_avg:39.88ms
step:856/2110 train_time:34157ms step_avg:39.90ms
step:857/2110 train_time:34217ms step_avg:39.93ms
step:858/2110 train_time:34276ms step_avg:39.95ms
step:859/2110 train_time:34336ms step_avg:39.97ms
step:860/2110 train_time:34394ms step_avg:39.99ms
step:861/2110 train_time:34455ms step_avg:40.02ms
step:862/2110 train_time:34514ms step_avg:40.04ms
step:863/2110 train_time:34575ms step_avg:40.06ms
step:864/2110 train_time:34634ms step_avg:40.09ms
step:865/2110 train_time:34694ms step_avg:40.11ms
step:866/2110 train_time:34752ms step_avg:40.13ms
step:867/2110 train_time:34812ms step_avg:40.15ms
step:868/2110 train_time:34870ms step_avg:40.17ms
step:869/2110 train_time:34931ms step_avg:40.20ms
step:870/2110 train_time:34990ms step_avg:40.22ms
step:871/2110 train_time:35049ms step_avg:40.24ms
step:872/2110 train_time:35108ms step_avg:40.26ms
step:873/2110 train_time:35169ms step_avg:40.29ms
step:874/2110 train_time:35228ms step_avg:40.31ms
step:875/2110 train_time:35289ms step_avg:40.33ms
step:876/2110 train_time:35348ms step_avg:40.35ms
step:877/2110 train_time:35407ms step_avg:40.37ms
step:878/2110 train_time:35466ms step_avg:40.39ms
step:879/2110 train_time:35526ms step_avg:40.42ms
step:880/2110 train_time:35585ms step_avg:40.44ms
step:881/2110 train_time:35644ms step_avg:40.46ms
step:882/2110 train_time:35703ms step_avg:40.48ms
step:883/2110 train_time:35763ms step_avg:40.50ms
step:884/2110 train_time:35821ms step_avg:40.52ms
step:885/2110 train_time:35880ms step_avg:40.54ms
step:886/2110 train_time:35938ms step_avg:40.56ms
step:887/2110 train_time:35998ms step_avg:40.58ms
step:888/2110 train_time:36056ms step_avg:40.60ms
step:889/2110 train_time:36116ms step_avg:40.63ms
step:890/2110 train_time:36174ms step_avg:40.65ms
step:891/2110 train_time:36235ms step_avg:40.67ms
step:892/2110 train_time:36294ms step_avg:40.69ms
step:893/2110 train_time:36354ms step_avg:40.71ms
step:894/2110 train_time:36412ms step_avg:40.73ms
step:895/2110 train_time:36473ms step_avg:40.75ms
step:896/2110 train_time:36532ms step_avg:40.77ms
step:897/2110 train_time:36592ms step_avg:40.79ms
step:898/2110 train_time:36651ms step_avg:40.81ms
step:899/2110 train_time:36711ms step_avg:40.84ms
step:900/2110 train_time:36770ms step_avg:40.86ms
step:901/2110 train_time:36830ms step_avg:40.88ms
step:902/2110 train_time:36888ms step_avg:40.90ms
step:903/2110 train_time:36947ms step_avg:40.92ms
step:904/2110 train_time:37006ms step_avg:40.94ms
step:905/2110 train_time:37066ms step_avg:40.96ms
step:906/2110 train_time:37125ms step_avg:40.98ms
step:907/2110 train_time:37184ms step_avg:41.00ms
step:908/2110 train_time:37243ms step_avg:41.02ms
step:909/2110 train_time:37304ms step_avg:41.04ms
step:910/2110 train_time:37362ms step_avg:41.06ms
step:911/2110 train_time:37422ms step_avg:41.08ms
step:912/2110 train_time:37481ms step_avg:41.10ms
step:913/2110 train_time:37540ms step_avg:41.12ms
step:914/2110 train_time:37598ms step_avg:41.14ms
step:915/2110 train_time:37659ms step_avg:41.16ms
step:916/2110 train_time:37717ms step_avg:41.18ms
step:917/2110 train_time:37777ms step_avg:41.20ms
step:918/2110 train_time:37836ms step_avg:41.22ms
step:919/2110 train_time:37895ms step_avg:41.24ms
step:920/2110 train_time:37953ms step_avg:41.25ms
step:921/2110 train_time:38013ms step_avg:41.27ms
step:922/2110 train_time:38072ms step_avg:41.29ms
step:923/2110 train_time:38132ms step_avg:41.31ms
step:924/2110 train_time:38191ms step_avg:41.33ms
step:925/2110 train_time:38251ms step_avg:41.35ms
step:926/2110 train_time:38309ms step_avg:41.37ms
step:927/2110 train_time:38370ms step_avg:41.39ms
step:928/2110 train_time:38428ms step_avg:41.41ms
step:929/2110 train_time:38488ms step_avg:41.43ms
step:930/2110 train_time:38548ms step_avg:41.45ms
step:931/2110 train_time:38608ms step_avg:41.47ms
step:932/2110 train_time:38667ms step_avg:41.49ms
step:933/2110 train_time:38726ms step_avg:41.51ms
step:934/2110 train_time:38785ms step_avg:41.53ms
step:935/2110 train_time:38845ms step_avg:41.54ms
step:936/2110 train_time:38904ms step_avg:41.56ms
step:937/2110 train_time:38963ms step_avg:41.58ms
step:938/2110 train_time:39022ms step_avg:41.60ms
step:939/2110 train_time:39081ms step_avg:41.62ms
step:940/2110 train_time:39140ms step_avg:41.64ms
step:941/2110 train_time:39200ms step_avg:41.66ms
step:942/2110 train_time:39259ms step_avg:41.68ms
step:943/2110 train_time:39319ms step_avg:41.70ms
step:944/2110 train_time:39377ms step_avg:41.71ms
step:945/2110 train_time:39438ms step_avg:41.73ms
step:946/2110 train_time:39496ms step_avg:41.75ms
step:947/2110 train_time:39556ms step_avg:41.77ms
step:948/2110 train_time:39615ms step_avg:41.79ms
step:949/2110 train_time:39675ms step_avg:41.81ms
step:950/2110 train_time:39734ms step_avg:41.82ms
step:951/2110 train_time:39794ms step_avg:41.84ms
step:952/2110 train_time:39852ms step_avg:41.86ms
step:953/2110 train_time:39912ms step_avg:41.88ms
step:954/2110 train_time:39970ms step_avg:41.90ms
step:955/2110 train_time:40031ms step_avg:41.92ms
step:956/2110 train_time:40089ms step_avg:41.93ms
step:957/2110 train_time:40150ms step_avg:41.95ms
step:958/2110 train_time:40209ms step_avg:41.97ms
step:959/2110 train_time:40269ms step_avg:41.99ms
step:960/2110 train_time:40327ms step_avg:42.01ms
step:961/2110 train_time:40387ms step_avg:42.03ms
step:962/2110 train_time:40447ms step_avg:42.04ms
step:963/2110 train_time:40506ms step_avg:42.06ms
step:964/2110 train_time:40566ms step_avg:42.08ms
step:965/2110 train_time:40625ms step_avg:42.10ms
step:966/2110 train_time:40684ms step_avg:42.12ms
step:967/2110 train_time:40744ms step_avg:42.13ms
step:968/2110 train_time:40803ms step_avg:42.15ms
step:969/2110 train_time:40863ms step_avg:42.17ms
step:970/2110 train_time:40921ms step_avg:42.19ms
step:971/2110 train_time:40980ms step_avg:42.20ms
step:972/2110 train_time:41039ms step_avg:42.22ms
step:973/2110 train_time:41098ms step_avg:42.24ms
step:974/2110 train_time:41157ms step_avg:42.26ms
step:975/2110 train_time:41217ms step_avg:42.27ms
step:976/2110 train_time:41275ms step_avg:42.29ms
step:977/2110 train_time:41335ms step_avg:42.31ms
step:978/2110 train_time:41393ms step_avg:42.32ms
step:979/2110 train_time:41453ms step_avg:42.34ms
step:980/2110 train_time:41512ms step_avg:42.36ms
step:981/2110 train_time:41573ms step_avg:42.38ms
step:982/2110 train_time:41631ms step_avg:42.39ms
step:983/2110 train_time:41691ms step_avg:42.41ms
step:984/2110 train_time:41750ms step_avg:42.43ms
step:985/2110 train_time:41810ms step_avg:42.45ms
step:986/2110 train_time:41869ms step_avg:42.46ms
step:987/2110 train_time:41928ms step_avg:42.48ms
step:988/2110 train_time:41987ms step_avg:42.50ms
step:989/2110 train_time:42047ms step_avg:42.51ms
step:990/2110 train_time:42106ms step_avg:42.53ms
step:991/2110 train_time:42166ms step_avg:42.55ms
step:992/2110 train_time:42226ms step_avg:42.57ms
step:993/2110 train_time:42285ms step_avg:42.58ms
step:994/2110 train_time:42344ms step_avg:42.60ms
step:995/2110 train_time:42404ms step_avg:42.62ms
step:996/2110 train_time:42463ms step_avg:42.63ms
step:997/2110 train_time:42523ms step_avg:42.65ms
step:998/2110 train_time:42583ms step_avg:42.67ms
step:999/2110 train_time:42643ms step_avg:42.69ms
step:1000/2110 train_time:42702ms step_avg:42.70ms
step:1000/2110 val_loss:3.7645 train_time:42763ms step_avg:42.76ms
step:1001/2110 train_time:42791ms step_avg:42.75ms
step:1002/2110 train_time:42822ms step_avg:42.74ms
step:1003/2110 train_time:42884ms step_avg:42.76ms
step:1004/2110 train_time:42947ms step_avg:42.78ms
step:1005/2110 train_time:43008ms step_avg:42.79ms
step:1006/2110 train_time:43066ms step_avg:42.81ms
step:1007/2110 train_time:43125ms step_avg:42.83ms
step:1008/2110 train_time:43183ms step_avg:42.84ms
step:1009/2110 train_time:43243ms step_avg:42.86ms
step:1010/2110 train_time:43301ms step_avg:42.87ms
step:1011/2110 train_time:43360ms step_avg:42.89ms
step:1012/2110 train_time:43418ms step_avg:42.90ms
step:1013/2110 train_time:43477ms step_avg:42.92ms
step:1014/2110 train_time:43535ms step_avg:42.93ms
step:1015/2110 train_time:43594ms step_avg:42.95ms
step:1016/2110 train_time:43651ms step_avg:42.96ms
step:1017/2110 train_time:43714ms step_avg:42.98ms
step:1018/2110 train_time:43773ms step_avg:43.00ms
step:1019/2110 train_time:43834ms step_avg:43.02ms
step:1020/2110 train_time:43894ms step_avg:43.03ms
step:1021/2110 train_time:43956ms step_avg:43.05ms
step:1022/2110 train_time:44015ms step_avg:43.07ms
step:1023/2110 train_time:44075ms step_avg:43.08ms
step:1024/2110 train_time:44133ms step_avg:43.10ms
step:1025/2110 train_time:44193ms step_avg:43.11ms
step:1026/2110 train_time:44250ms step_avg:43.13ms
step:1027/2110 train_time:44310ms step_avg:43.14ms
step:1028/2110 train_time:44367ms step_avg:43.16ms
step:1029/2110 train_time:44427ms step_avg:43.17ms
step:1030/2110 train_time:44485ms step_avg:43.19ms
step:1031/2110 train_time:44545ms step_avg:43.21ms
step:1032/2110 train_time:44604ms step_avg:43.22ms
step:1033/2110 train_time:44663ms step_avg:43.24ms
step:1034/2110 train_time:44722ms step_avg:43.25ms
step:1035/2110 train_time:44782ms step_avg:43.27ms
step:1036/2110 train_time:44841ms step_avg:43.28ms
step:1037/2110 train_time:44901ms step_avg:43.30ms
step:1038/2110 train_time:44961ms step_avg:43.32ms
step:1039/2110 train_time:45022ms step_avg:43.33ms
step:1040/2110 train_time:45081ms step_avg:43.35ms
step:1041/2110 train_time:45141ms step_avg:43.36ms
step:1042/2110 train_time:45200ms step_avg:43.38ms
step:1043/2110 train_time:45260ms step_avg:43.39ms
step:1044/2110 train_time:45318ms step_avg:43.41ms
step:1045/2110 train_time:45378ms step_avg:43.42ms
step:1046/2110 train_time:45436ms step_avg:43.44ms
step:1047/2110 train_time:45495ms step_avg:43.45ms
step:1048/2110 train_time:45554ms step_avg:43.47ms
step:1049/2110 train_time:45613ms step_avg:43.48ms
step:1050/2110 train_time:45672ms step_avg:43.50ms
step:1051/2110 train_time:45732ms step_avg:43.51ms
step:1052/2110 train_time:45791ms step_avg:43.53ms
step:1053/2110 train_time:45852ms step_avg:43.54ms
step:1054/2110 train_time:45910ms step_avg:43.56ms
step:1055/2110 train_time:45971ms step_avg:43.57ms
step:1056/2110 train_time:46030ms step_avg:43.59ms
step:1057/2110 train_time:46091ms step_avg:43.61ms
step:1058/2110 train_time:46149ms step_avg:43.62ms
step:1059/2110 train_time:46209ms step_avg:43.63ms
step:1060/2110 train_time:46267ms step_avg:43.65ms
step:1061/2110 train_time:46327ms step_avg:43.66ms
step:1062/2110 train_time:46385ms step_avg:43.68ms
step:1063/2110 train_time:46445ms step_avg:43.69ms
step:1064/2110 train_time:46504ms step_avg:43.71ms
step:1065/2110 train_time:46563ms step_avg:43.72ms
step:1066/2110 train_time:46622ms step_avg:43.74ms
step:1067/2110 train_time:46682ms step_avg:43.75ms
step:1068/2110 train_time:46741ms step_avg:43.76ms
step:1069/2110 train_time:46800ms step_avg:43.78ms
step:1070/2110 train_time:46859ms step_avg:43.79ms
step:1071/2110 train_time:46919ms step_avg:43.81ms
step:1072/2110 train_time:46978ms step_avg:43.82ms
step:1073/2110 train_time:47038ms step_avg:43.84ms
step:1074/2110 train_time:47098ms step_avg:43.85ms
step:1075/2110 train_time:47157ms step_avg:43.87ms
step:1076/2110 train_time:47215ms step_avg:43.88ms
step:1077/2110 train_time:47275ms step_avg:43.89ms
step:1078/2110 train_time:47333ms step_avg:43.91ms
step:1079/2110 train_time:47393ms step_avg:43.92ms
step:1080/2110 train_time:47452ms step_avg:43.94ms
step:1081/2110 train_time:47512ms step_avg:43.95ms
step:1082/2110 train_time:47570ms step_avg:43.96ms
step:1083/2110 train_time:47630ms step_avg:43.98ms
step:1084/2110 train_time:47688ms step_avg:43.99ms
step:1085/2110 train_time:47749ms step_avg:44.01ms
step:1086/2110 train_time:47806ms step_avg:44.02ms
step:1087/2110 train_time:47867ms step_avg:44.04ms
step:1088/2110 train_time:47926ms step_avg:44.05ms
step:1089/2110 train_time:47988ms step_avg:44.07ms
step:1090/2110 train_time:48046ms step_avg:44.08ms
step:1091/2110 train_time:48106ms step_avg:44.09ms
step:1092/2110 train_time:48165ms step_avg:44.11ms
step:1093/2110 train_time:48224ms step_avg:44.12ms
step:1094/2110 train_time:48283ms step_avg:44.13ms
step:1095/2110 train_time:48343ms step_avg:44.15ms
step:1096/2110 train_time:48402ms step_avg:44.16ms
step:1097/2110 train_time:48461ms step_avg:44.18ms
step:1098/2110 train_time:48520ms step_avg:44.19ms
step:1099/2110 train_time:48579ms step_avg:44.20ms
step:1100/2110 train_time:48638ms step_avg:44.22ms
step:1101/2110 train_time:48698ms step_avg:44.23ms
step:1102/2110 train_time:48756ms step_avg:44.24ms
step:1103/2110 train_time:48816ms step_avg:44.26ms
step:1104/2110 train_time:48876ms step_avg:44.27ms
step:1105/2110 train_time:48935ms step_avg:44.29ms
step:1106/2110 train_time:48994ms step_avg:44.30ms
step:1107/2110 train_time:49056ms step_avg:44.31ms
step:1108/2110 train_time:49114ms step_avg:44.33ms
step:1109/2110 train_time:49173ms step_avg:44.34ms
step:1110/2110 train_time:49232ms step_avg:44.35ms
step:1111/2110 train_time:49292ms step_avg:44.37ms
step:1112/2110 train_time:49350ms step_avg:44.38ms
step:1113/2110 train_time:49410ms step_avg:44.39ms
step:1114/2110 train_time:49468ms step_avg:44.41ms
step:1115/2110 train_time:49529ms step_avg:44.42ms
step:1116/2110 train_time:49587ms step_avg:44.43ms
step:1117/2110 train_time:49647ms step_avg:44.45ms
step:1118/2110 train_time:49705ms step_avg:44.46ms
step:1119/2110 train_time:49766ms step_avg:44.47ms
step:1120/2110 train_time:49824ms step_avg:44.49ms
step:1121/2110 train_time:49884ms step_avg:44.50ms
step:1122/2110 train_time:49943ms step_avg:44.51ms
step:1123/2110 train_time:50003ms step_avg:44.53ms
step:1124/2110 train_time:50062ms step_avg:44.54ms
step:1125/2110 train_time:50121ms step_avg:44.55ms
step:1126/2110 train_time:50180ms step_avg:44.56ms
step:1127/2110 train_time:50239ms step_avg:44.58ms
step:1128/2110 train_time:50298ms step_avg:44.59ms
step:1129/2110 train_time:50357ms step_avg:44.60ms
step:1130/2110 train_time:50415ms step_avg:44.62ms
step:1131/2110 train_time:50475ms step_avg:44.63ms
step:1132/2110 train_time:50533ms step_avg:44.64ms
step:1133/2110 train_time:50593ms step_avg:44.65ms
step:1134/2110 train_time:50652ms step_avg:44.67ms
step:1135/2110 train_time:50713ms step_avg:44.68ms
step:1136/2110 train_time:50771ms step_avg:44.69ms
step:1137/2110 train_time:50832ms step_avg:44.71ms
step:1138/2110 train_time:50891ms step_avg:44.72ms
step:1139/2110 train_time:50951ms step_avg:44.73ms
step:1140/2110 train_time:51010ms step_avg:44.75ms
step:1141/2110 train_time:51070ms step_avg:44.76ms
step:1142/2110 train_time:51130ms step_avg:44.77ms
step:1143/2110 train_time:51191ms step_avg:44.79ms
step:1144/2110 train_time:51250ms step_avg:44.80ms
step:1145/2110 train_time:51311ms step_avg:44.81ms
step:1146/2110 train_time:51370ms step_avg:44.83ms
step:1147/2110 train_time:51431ms step_avg:44.84ms
step:1148/2110 train_time:51489ms step_avg:44.85ms
step:1149/2110 train_time:51550ms step_avg:44.87ms
step:1150/2110 train_time:51609ms step_avg:44.88ms
step:1151/2110 train_time:51669ms step_avg:44.89ms
step:1152/2110 train_time:51728ms step_avg:44.90ms
step:1153/2110 train_time:51789ms step_avg:44.92ms
step:1154/2110 train_time:51848ms step_avg:44.93ms
step:1155/2110 train_time:51909ms step_avg:44.94ms
step:1156/2110 train_time:51968ms step_avg:44.96ms
step:1157/2110 train_time:52029ms step_avg:44.97ms
step:1158/2110 train_time:52089ms step_avg:44.98ms
step:1159/2110 train_time:52150ms step_avg:45.00ms
step:1160/2110 train_time:52209ms step_avg:45.01ms
step:1161/2110 train_time:52270ms step_avg:45.02ms
step:1162/2110 train_time:52329ms step_avg:45.03ms
step:1163/2110 train_time:52390ms step_avg:45.05ms
step:1164/2110 train_time:52448ms step_avg:45.06ms
step:1165/2110 train_time:52509ms step_avg:45.07ms
step:1166/2110 train_time:52568ms step_avg:45.08ms
step:1167/2110 train_time:52628ms step_avg:45.10ms
step:1168/2110 train_time:52688ms step_avg:45.11ms
step:1169/2110 train_time:52748ms step_avg:45.12ms
step:1170/2110 train_time:52807ms step_avg:45.13ms
step:1171/2110 train_time:52868ms step_avg:45.15ms
step:1172/2110 train_time:52926ms step_avg:45.16ms
step:1173/2110 train_time:52987ms step_avg:45.17ms
step:1174/2110 train_time:53046ms step_avg:45.18ms
step:1175/2110 train_time:53107ms step_avg:45.20ms
step:1176/2110 train_time:53166ms step_avg:45.21ms
step:1177/2110 train_time:53227ms step_avg:45.22ms
step:1178/2110 train_time:53286ms step_avg:45.23ms
step:1179/2110 train_time:53347ms step_avg:45.25ms
step:1180/2110 train_time:53406ms step_avg:45.26ms
step:1181/2110 train_time:53467ms step_avg:45.27ms
step:1182/2110 train_time:53526ms step_avg:45.28ms
step:1183/2110 train_time:53587ms step_avg:45.30ms
step:1184/2110 train_time:53645ms step_avg:45.31ms
step:1185/2110 train_time:53706ms step_avg:45.32ms
step:1186/2110 train_time:53765ms step_avg:45.33ms
step:1187/2110 train_time:53826ms step_avg:45.35ms
step:1188/2110 train_time:53885ms step_avg:45.36ms
step:1189/2110 train_time:53946ms step_avg:45.37ms
step:1190/2110 train_time:54005ms step_avg:45.38ms
step:1191/2110 train_time:54066ms step_avg:45.40ms
step:1192/2110 train_time:54125ms step_avg:45.41ms
step:1193/2110 train_time:54185ms step_avg:45.42ms
step:1194/2110 train_time:54246ms step_avg:45.43ms
step:1195/2110 train_time:54305ms step_avg:45.44ms
step:1196/2110 train_time:54365ms step_avg:45.46ms
step:1197/2110 train_time:54425ms step_avg:45.47ms
step:1198/2110 train_time:54484ms step_avg:45.48ms
step:1199/2110 train_time:54545ms step_avg:45.49ms
step:1200/2110 train_time:54604ms step_avg:45.50ms
step:1201/2110 train_time:54664ms step_avg:45.52ms
step:1202/2110 train_time:54724ms step_avg:45.53ms
step:1203/2110 train_time:54784ms step_avg:45.54ms
step:1204/2110 train_time:54842ms step_avg:45.55ms
step:1205/2110 train_time:54903ms step_avg:45.56ms
step:1206/2110 train_time:54963ms step_avg:45.57ms
step:1207/2110 train_time:55023ms step_avg:45.59ms
step:1208/2110 train_time:55082ms step_avg:45.60ms
step:1209/2110 train_time:55142ms step_avg:45.61ms
step:1210/2110 train_time:55202ms step_avg:45.62ms
step:1211/2110 train_time:55261ms step_avg:45.63ms
step:1212/2110 train_time:55320ms step_avg:45.64ms
step:1213/2110 train_time:55380ms step_avg:45.66ms
step:1214/2110 train_time:55440ms step_avg:45.67ms
step:1215/2110 train_time:55501ms step_avg:45.68ms
step:1216/2110 train_time:55560ms step_avg:45.69ms
step:1217/2110 train_time:55620ms step_avg:45.70ms
step:1218/2110 train_time:55679ms step_avg:45.71ms
step:1219/2110 train_time:55739ms step_avg:45.73ms
step:1220/2110 train_time:55799ms step_avg:45.74ms
step:1221/2110 train_time:55859ms step_avg:45.75ms
step:1222/2110 train_time:55918ms step_avg:45.76ms
step:1223/2110 train_time:55978ms step_avg:45.77ms
step:1224/2110 train_time:56037ms step_avg:45.78ms
step:1225/2110 train_time:56097ms step_avg:45.79ms
step:1226/2110 train_time:56157ms step_avg:45.80ms
step:1227/2110 train_time:56216ms step_avg:45.82ms
step:1228/2110 train_time:56275ms step_avg:45.83ms
step:1229/2110 train_time:56336ms step_avg:45.84ms
step:1230/2110 train_time:56394ms step_avg:45.85ms
step:1231/2110 train_time:56455ms step_avg:45.86ms
step:1232/2110 train_time:56514ms step_avg:45.87ms
step:1233/2110 train_time:56574ms step_avg:45.88ms
step:1234/2110 train_time:56632ms step_avg:45.89ms
step:1235/2110 train_time:56694ms step_avg:45.91ms
step:1236/2110 train_time:56753ms step_avg:45.92ms
step:1237/2110 train_time:56813ms step_avg:45.93ms
step:1238/2110 train_time:56872ms step_avg:45.94ms
step:1239/2110 train_time:56933ms step_avg:45.95ms
step:1240/2110 train_time:56991ms step_avg:45.96ms
step:1241/2110 train_time:57053ms step_avg:45.97ms
step:1242/2110 train_time:57112ms step_avg:45.98ms
step:1243/2110 train_time:57173ms step_avg:46.00ms
step:1244/2110 train_time:57232ms step_avg:46.01ms
step:1245/2110 train_time:57293ms step_avg:46.02ms
step:1246/2110 train_time:57351ms step_avg:46.03ms
step:1247/2110 train_time:57412ms step_avg:46.04ms
step:1248/2110 train_time:57471ms step_avg:46.05ms
step:1249/2110 train_time:57532ms step_avg:46.06ms
step:1250/2110 train_time:57591ms step_avg:46.07ms
step:1250/2110 val_loss:3.5952 train_time:57654ms step_avg:46.12ms
step:1251/2110 train_time:57680ms step_avg:46.11ms
step:1252/2110 train_time:57715ms step_avg:46.10ms
step:1253/2110 train_time:57780ms step_avg:46.11ms
step:1254/2110 train_time:57842ms step_avg:46.13ms
step:1255/2110 train_time:57902ms step_avg:46.14ms
step:1256/2110 train_time:57962ms step_avg:46.15ms
step:1257/2110 train_time:58022ms step_avg:46.16ms
step:1258/2110 train_time:58080ms step_avg:46.17ms
step:1259/2110 train_time:58141ms step_avg:46.18ms
step:1260/2110 train_time:58199ms step_avg:46.19ms
step:1261/2110 train_time:58259ms step_avg:46.20ms
step:1262/2110 train_time:58318ms step_avg:46.21ms
step:1263/2110 train_time:58378ms step_avg:46.22ms
step:1264/2110 train_time:58436ms step_avg:46.23ms
step:1265/2110 train_time:58496ms step_avg:46.24ms
step:1266/2110 train_time:58554ms step_avg:46.25ms
step:1267/2110 train_time:58615ms step_avg:46.26ms
step:1268/2110 train_time:58676ms step_avg:46.27ms
step:1269/2110 train_time:58738ms step_avg:46.29ms
step:1270/2110 train_time:58800ms step_avg:46.30ms
step:1271/2110 train_time:58861ms step_avg:46.31ms
step:1272/2110 train_time:58920ms step_avg:46.32ms
step:1273/2110 train_time:58981ms step_avg:46.33ms
step:1274/2110 train_time:59040ms step_avg:46.34ms
step:1275/2110 train_time:59101ms step_avg:46.35ms
step:1276/2110 train_time:59160ms step_avg:46.36ms
step:1277/2110 train_time:59220ms step_avg:46.37ms
step:1278/2110 train_time:59278ms step_avg:46.38ms
step:1279/2110 train_time:59339ms step_avg:46.39ms
step:1280/2110 train_time:59397ms step_avg:46.40ms
step:1281/2110 train_time:59456ms step_avg:46.41ms
step:1282/2110 train_time:59515ms step_avg:46.42ms
step:1283/2110 train_time:59576ms step_avg:46.43ms
step:1284/2110 train_time:59635ms step_avg:46.45ms
step:1285/2110 train_time:59697ms step_avg:46.46ms
step:1286/2110 train_time:59757ms step_avg:46.47ms
step:1287/2110 train_time:59818ms step_avg:46.48ms
step:1288/2110 train_time:59879ms step_avg:46.49ms
step:1289/2110 train_time:59941ms step_avg:46.50ms
step:1290/2110 train_time:60000ms step_avg:46.51ms
step:1291/2110 train_time:60060ms step_avg:46.52ms
step:1292/2110 train_time:60120ms step_avg:46.53ms
step:1293/2110 train_time:60180ms step_avg:46.54ms
step:1294/2110 train_time:60239ms step_avg:46.55ms
step:1295/2110 train_time:60298ms step_avg:46.56ms
step:1296/2110 train_time:60357ms step_avg:46.57ms
step:1297/2110 train_time:60417ms step_avg:46.58ms
step:1298/2110 train_time:60476ms step_avg:46.59ms
step:1299/2110 train_time:60536ms step_avg:46.60ms
step:1300/2110 train_time:60595ms step_avg:46.61ms
step:1301/2110 train_time:60656ms step_avg:46.62ms
step:1302/2110 train_time:60716ms step_avg:46.63ms
step:1303/2110 train_time:60778ms step_avg:46.64ms
step:1304/2110 train_time:60839ms step_avg:46.66ms
step:1305/2110 train_time:60900ms step_avg:46.67ms
step:1306/2110 train_time:60960ms step_avg:46.68ms
step:1307/2110 train_time:61020ms step_avg:46.69ms
step:1308/2110 train_time:61081ms step_avg:46.70ms
step:1309/2110 train_time:61140ms step_avg:46.71ms
step:1310/2110 train_time:61200ms step_avg:46.72ms
step:1311/2110 train_time:61259ms step_avg:46.73ms
step:1312/2110 train_time:61318ms step_avg:46.74ms
step:1313/2110 train_time:61379ms step_avg:46.75ms
step:1314/2110 train_time:61439ms step_avg:46.76ms
step:1315/2110 train_time:61499ms step_avg:46.77ms
step:1316/2110 train_time:61559ms step_avg:46.78ms
step:1317/2110 train_time:61620ms step_avg:46.79ms
step:1318/2110 train_time:61679ms step_avg:46.80ms
step:1319/2110 train_time:61742ms step_avg:46.81ms
step:1320/2110 train_time:61801ms step_avg:46.82ms
step:1321/2110 train_time:61862ms step_avg:46.83ms
step:1322/2110 train_time:61921ms step_avg:46.84ms
step:1323/2110 train_time:61982ms step_avg:46.85ms
step:1324/2110 train_time:62041ms step_avg:46.86ms
step:1325/2110 train_time:62101ms step_avg:46.87ms
step:1326/2110 train_time:62161ms step_avg:46.88ms
step:1327/2110 train_time:62221ms step_avg:46.89ms
step:1328/2110 train_time:62280ms step_avg:46.90ms
step:1329/2110 train_time:62340ms step_avg:46.91ms
step:1330/2110 train_time:62399ms step_avg:46.92ms
step:1331/2110 train_time:62460ms step_avg:46.93ms
step:1332/2110 train_time:62519ms step_avg:46.94ms
step:1333/2110 train_time:62580ms step_avg:46.95ms
step:1334/2110 train_time:62640ms step_avg:46.96ms
step:1335/2110 train_time:62701ms step_avg:46.97ms
step:1336/2110 train_time:62760ms step_avg:46.98ms
step:1337/2110 train_time:62821ms step_avg:46.99ms
step:1338/2110 train_time:62881ms step_avg:47.00ms
step:1339/2110 train_time:62942ms step_avg:47.01ms
step:1340/2110 train_time:63000ms step_avg:47.02ms
step:1341/2110 train_time:63060ms step_avg:47.02ms
step:1342/2110 train_time:63119ms step_avg:47.03ms
step:1343/2110 train_time:63180ms step_avg:47.04ms
step:1344/2110 train_time:63239ms step_avg:47.05ms
step:1345/2110 train_time:63300ms step_avg:47.06ms
step:1346/2110 train_time:63359ms step_avg:47.07ms
step:1347/2110 train_time:63419ms step_avg:47.08ms
step:1348/2110 train_time:63479ms step_avg:47.09ms
step:1349/2110 train_time:63540ms step_avg:47.10ms
step:1350/2110 train_time:63599ms step_avg:47.11ms
step:1351/2110 train_time:63660ms step_avg:47.12ms
step:1352/2110 train_time:63719ms step_avg:47.13ms
step:1353/2110 train_time:63780ms step_avg:47.14ms
step:1354/2110 train_time:63840ms step_avg:47.15ms
step:1355/2110 train_time:63901ms step_avg:47.16ms
step:1356/2110 train_time:63960ms step_avg:47.17ms
step:1357/2110 train_time:64020ms step_avg:47.18ms
step:1358/2110 train_time:64079ms step_avg:47.19ms
step:1359/2110 train_time:64141ms step_avg:47.20ms
step:1360/2110 train_time:64200ms step_avg:47.21ms
step:1361/2110 train_time:64260ms step_avg:47.22ms
step:1362/2110 train_time:64318ms step_avg:47.22ms
step:1363/2110 train_time:64380ms step_avg:47.23ms
step:1364/2110 train_time:64439ms step_avg:47.24ms
step:1365/2110 train_time:64500ms step_avg:47.25ms
step:1366/2110 train_time:64559ms step_avg:47.26ms
step:1367/2110 train_time:64620ms step_avg:47.27ms
step:1368/2110 train_time:64679ms step_avg:47.28ms
step:1369/2110 train_time:64740ms step_avg:47.29ms
step:1370/2110 train_time:64799ms step_avg:47.30ms
step:1371/2110 train_time:64861ms step_avg:47.31ms
step:1372/2110 train_time:64920ms step_avg:47.32ms
step:1373/2110 train_time:64980ms step_avg:47.33ms
step:1374/2110 train_time:65040ms step_avg:47.34ms
step:1375/2110 train_time:65100ms step_avg:47.35ms
step:1376/2110 train_time:65161ms step_avg:47.36ms
step:1377/2110 train_time:65220ms step_avg:47.36ms
step:1378/2110 train_time:65279ms step_avg:47.37ms
step:1379/2110 train_time:65340ms step_avg:47.38ms
step:1380/2110 train_time:65400ms step_avg:47.39ms
step:1381/2110 train_time:65461ms step_avg:47.40ms
step:1382/2110 train_time:65548ms step_avg:47.43ms
step:1383/2110 train_time:65635ms step_avg:47.46ms
step:1384/2110 train_time:65721ms step_avg:47.49ms
step:1385/2110 train_time:65808ms step_avg:47.51ms
step:1386/2110 train_time:65895ms step_avg:47.54ms
step:1387/2110 train_time:65982ms step_avg:47.57ms
step:1388/2110 train_time:66069ms step_avg:47.60ms
step:1389/2110 train_time:66156ms step_avg:47.63ms
step:1390/2110 train_time:66242ms step_avg:47.66ms
step:1391/2110 train_time:66329ms step_avg:47.68ms
step:1392/2110 train_time:66415ms step_avg:47.71ms
step:1393/2110 train_time:66502ms step_avg:47.74ms
step:1394/2110 train_time:66590ms step_avg:47.77ms
step:1395/2110 train_time:66677ms step_avg:47.80ms
step:1396/2110 train_time:66763ms step_avg:47.82ms
step:1397/2110 train_time:66851ms step_avg:47.85ms
step:1398/2110 train_time:66937ms step_avg:47.88ms
step:1399/2110 train_time:67025ms step_avg:47.91ms
step:1400/2110 train_time:67112ms step_avg:47.94ms
step:1401/2110 train_time:67199ms step_avg:47.97ms
step:1402/2110 train_time:67285ms step_avg:47.99ms
step:1403/2110 train_time:67372ms step_avg:48.02ms
step:1404/2110 train_time:67458ms step_avg:48.05ms
step:1405/2110 train_time:67546ms step_avg:48.08ms
step:1406/2110 train_time:67633ms step_avg:48.10ms
step:1407/2110 train_time:67719ms step_avg:48.13ms
step:1408/2110 train_time:67807ms step_avg:48.16ms
step:1409/2110 train_time:67894ms step_avg:48.19ms
step:1410/2110 train_time:67980ms step_avg:48.21ms
step:1411/2110 train_time:68068ms step_avg:48.24ms
step:1412/2110 train_time:68154ms step_avg:48.27ms
step:1413/2110 train_time:68241ms step_avg:48.30ms
step:1414/2110 train_time:68328ms step_avg:48.32ms
step:1415/2110 train_time:68414ms step_avg:48.35ms
step:1416/2110 train_time:68502ms step_avg:48.38ms
step:1417/2110 train_time:68589ms step_avg:48.40ms
step:1418/2110 train_time:68675ms step_avg:48.43ms
step:1419/2110 train_time:68762ms step_avg:48.46ms
step:1420/2110 train_time:68848ms step_avg:48.48ms
step:1421/2110 train_time:68935ms step_avg:48.51ms
step:1422/2110 train_time:69022ms step_avg:48.54ms
step:1423/2110 train_time:69109ms step_avg:48.57ms
step:1424/2110 train_time:69195ms step_avg:48.59ms
step:1425/2110 train_time:69283ms step_avg:48.62ms
step:1426/2110 train_time:69370ms step_avg:48.65ms
step:1427/2110 train_time:69456ms step_avg:48.67ms
step:1428/2110 train_time:69542ms step_avg:48.70ms
step:1429/2110 train_time:69629ms step_avg:48.73ms
step:1430/2110 train_time:69715ms step_avg:48.75ms
step:1431/2110 train_time:69803ms step_avg:48.78ms
step:1432/2110 train_time:69890ms step_avg:48.81ms
step:1433/2110 train_time:69977ms step_avg:48.83ms
step:1434/2110 train_time:70064ms step_avg:48.86ms
step:1435/2110 train_time:70151ms step_avg:48.89ms
step:1436/2110 train_time:70238ms step_avg:48.91ms
step:1437/2110 train_time:70324ms step_avg:48.94ms
step:1438/2110 train_time:70412ms step_avg:48.97ms
step:1439/2110 train_time:70499ms step_avg:48.99ms
step:1440/2110 train_time:70585ms step_avg:49.02ms
step:1441/2110 train_time:70672ms step_avg:49.04ms
step:1442/2110 train_time:70758ms step_avg:49.07ms
step:1443/2110 train_time:70846ms step_avg:49.10ms
step:1444/2110 train_time:70933ms step_avg:49.12ms
step:1445/2110 train_time:71019ms step_avg:49.15ms
step:1446/2110 train_time:71106ms step_avg:49.17ms
step:1447/2110 train_time:71194ms step_avg:49.20ms
step:1448/2110 train_time:71280ms step_avg:49.23ms
step:1449/2110 train_time:71368ms step_avg:49.25ms
step:1450/2110 train_time:71454ms step_avg:49.28ms
step:1451/2110 train_time:71541ms step_avg:49.30ms
step:1452/2110 train_time:71628ms step_avg:49.33ms
step:1453/2110 train_time:71715ms step_avg:49.36ms
step:1454/2110 train_time:71802ms step_avg:49.38ms
step:1455/2110 train_time:71889ms step_avg:49.41ms
step:1456/2110 train_time:71974ms step_avg:49.43ms
step:1457/2110 train_time:72062ms step_avg:49.46ms
step:1458/2110 train_time:72148ms step_avg:49.48ms
step:1459/2110 train_time:72235ms step_avg:49.51ms
step:1460/2110 train_time:72323ms step_avg:49.54ms
step:1461/2110 train_time:72410ms step_avg:49.56ms
step:1462/2110 train_time:72495ms step_avg:49.59ms
step:1463/2110 train_time:72584ms step_avg:49.61ms
step:1464/2110 train_time:72671ms step_avg:49.64ms
step:1465/2110 train_time:72757ms step_avg:49.66ms
step:1466/2110 train_time:72844ms step_avg:49.69ms
step:1467/2110 train_time:72930ms step_avg:49.71ms
step:1468/2110 train_time:73016ms step_avg:49.74ms
step:1469/2110 train_time:73103ms step_avg:49.76ms
step:1470/2110 train_time:73190ms step_avg:49.79ms
step:1471/2110 train_time:73277ms step_avg:49.81ms
step:1472/2110 train_time:73364ms step_avg:49.84ms
step:1473/2110 train_time:73451ms step_avg:49.86ms
step:1474/2110 train_time:73537ms step_avg:49.89ms
step:1475/2110 train_time:73624ms step_avg:49.91ms
step:1476/2110 train_time:73712ms step_avg:49.94ms
step:1477/2110 train_time:73799ms step_avg:49.97ms
step:1478/2110 train_time:73885ms step_avg:49.99ms
step:1479/2110 train_time:73972ms step_avg:50.01ms
step:1480/2110 train_time:74058ms step_avg:50.04ms
step:1481/2110 train_time:74145ms step_avg:50.06ms
step:1482/2110 train_time:74232ms step_avg:50.09ms
step:1483/2110 train_time:74320ms step_avg:50.11ms
step:1484/2110 train_time:74407ms step_avg:50.14ms
step:1485/2110 train_time:74494ms step_avg:50.16ms
step:1486/2110 train_time:74581ms step_avg:50.19ms
step:1487/2110 train_time:74668ms step_avg:50.21ms
step:1488/2110 train_time:74754ms step_avg:50.24ms
step:1489/2110 train_time:74841ms step_avg:50.26ms
step:1490/2110 train_time:74928ms step_avg:50.29ms
step:1491/2110 train_time:75015ms step_avg:50.31ms
step:1492/2110 train_time:75101ms step_avg:50.34ms
step:1493/2110 train_time:75189ms step_avg:50.36ms
step:1494/2110 train_time:75275ms step_avg:50.39ms
step:1495/2110 train_time:75362ms step_avg:50.41ms
step:1496/2110 train_time:75449ms step_avg:50.43ms
step:1497/2110 train_time:75535ms step_avg:50.46ms
step:1498/2110 train_time:75621ms step_avg:50.48ms
step:1499/2110 train_time:75709ms step_avg:50.51ms
step:1500/2110 train_time:75795ms step_avg:50.53ms
step:1500/2110 val_loss:3.4951 train_time:75883ms step_avg:50.59ms
step:1501/2110 train_time:75917ms step_avg:50.58ms
step:1502/2110 train_time:75972ms step_avg:50.58ms
step:1503/2110 train_time:76068ms step_avg:50.61ms
step:1504/2110 train_time:76155ms step_avg:50.63ms
step:1505/2110 train_time:76241ms step_avg:50.66ms
step:1506/2110 train_time:76327ms step_avg:50.68ms
step:1507/2110 train_time:76412ms step_avg:50.70ms
step:1508/2110 train_time:76498ms step_avg:50.73ms
step:1509/2110 train_time:76583ms step_avg:50.75ms
step:1510/2110 train_time:76669ms step_avg:50.77ms
step:1511/2110 train_time:76756ms step_avg:50.80ms
step:1512/2110 train_time:76844ms step_avg:50.82ms
step:1513/2110 train_time:76934ms step_avg:50.85ms
step:1514/2110 train_time:77023ms step_avg:50.87ms
step:1515/2110 train_time:77112ms step_avg:50.90ms
step:1516/2110 train_time:77199ms step_avg:50.92ms
step:1517/2110 train_time:77285ms step_avg:50.95ms
step:1518/2110 train_time:77370ms step_avg:50.97ms
step:1519/2110 train_time:77457ms step_avg:50.99ms
step:1520/2110 train_time:77542ms step_avg:51.01ms
step:1521/2110 train_time:77628ms step_avg:51.04ms
step:1522/2110 train_time:77714ms step_avg:51.06ms
step:1523/2110 train_time:77802ms step_avg:51.08ms
step:1524/2110 train_time:77889ms step_avg:51.11ms
step:1525/2110 train_time:77979ms step_avg:51.13ms
step:1526/2110 train_time:78066ms step_avg:51.16ms
step:1527/2110 train_time:78154ms step_avg:51.18ms
step:1528/2110 train_time:78240ms step_avg:51.20ms
step:1529/2110 train_time:78327ms step_avg:51.23ms
step:1530/2110 train_time:78414ms step_avg:51.25ms
step:1531/2110 train_time:78500ms step_avg:51.27ms
step:1532/2110 train_time:78586ms step_avg:51.30ms
step:1533/2110 train_time:78673ms step_avg:51.32ms
step:1534/2110 train_time:78760ms step_avg:51.34ms
step:1535/2110 train_time:78847ms step_avg:51.37ms
step:1536/2110 train_time:78934ms step_avg:51.39ms
step:1537/2110 train_time:79021ms step_avg:51.41ms
step:1538/2110 train_time:79109ms step_avg:51.44ms
step:1539/2110 train_time:79196ms step_avg:51.46ms
step:1540/2110 train_time:79283ms step_avg:51.48ms
step:1541/2110 train_time:79369ms step_avg:51.51ms
step:1542/2110 train_time:79456ms step_avg:51.53ms
step:1543/2110 train_time:79542ms step_avg:51.55ms
step:1544/2110 train_time:79628ms step_avg:51.57ms
step:1545/2110 train_time:79715ms step_avg:51.60ms
step:1546/2110 train_time:79801ms step_avg:51.62ms
step:1547/2110 train_time:79889ms step_avg:51.64ms
step:1548/2110 train_time:79977ms step_avg:51.66ms
step:1549/2110 train_time:80064ms step_avg:51.69ms
step:1550/2110 train_time:80151ms step_avg:51.71ms
step:1551/2110 train_time:80239ms step_avg:51.73ms
step:1552/2110 train_time:80324ms step_avg:51.76ms
step:1553/2110 train_time:80412ms step_avg:51.78ms
step:1554/2110 train_time:80499ms step_avg:51.80ms
step:1555/2110 train_time:80585ms step_avg:51.82ms
step:1556/2110 train_time:80670ms step_avg:51.84ms
step:1557/2110 train_time:80757ms step_avg:51.87ms
step:1558/2110 train_time:80843ms step_avg:51.89ms
step:1559/2110 train_time:80931ms step_avg:51.91ms
step:1560/2110 train_time:81019ms step_avg:51.93ms
step:1561/2110 train_time:81106ms step_avg:51.96ms
step:1562/2110 train_time:81194ms step_avg:51.98ms
step:1563/2110 train_time:81281ms step_avg:52.00ms
step:1564/2110 train_time:81366ms step_avg:52.02ms
step:1565/2110 train_time:81454ms step_avg:52.05ms
step:1566/2110 train_time:81540ms step_avg:52.07ms
step:1567/2110 train_time:81627ms step_avg:52.09ms
step:1568/2110 train_time:81713ms step_avg:52.11ms
step:1569/2110 train_time:81801ms step_avg:52.14ms
step:1570/2110 train_time:81887ms step_avg:52.16ms
step:1571/2110 train_time:81975ms step_avg:52.18ms
step:1572/2110 train_time:82061ms step_avg:52.20ms
step:1573/2110 train_time:82149ms step_avg:52.22ms
step:1574/2110 train_time:82236ms step_avg:52.25ms
step:1575/2110 train_time:82323ms step_avg:52.27ms
step:1576/2110 train_time:82410ms step_avg:52.29ms
step:1577/2110 train_time:82498ms step_avg:52.31ms
step:1578/2110 train_time:82583ms step_avg:52.33ms
step:1579/2110 train_time:82670ms step_avg:52.36ms
step:1580/2110 train_time:82757ms step_avg:52.38ms
step:1581/2110 train_time:82844ms step_avg:52.40ms
step:1582/2110 train_time:82931ms step_avg:52.42ms
step:1583/2110 train_time:83020ms step_avg:52.44ms
step:1584/2110 train_time:83106ms step_avg:52.47ms
step:1585/2110 train_time:83193ms step_avg:52.49ms
step:1586/2110 train_time:83279ms step_avg:52.51ms
step:1587/2110 train_time:83367ms step_avg:52.53ms
step:1588/2110 train_time:83453ms step_avg:52.55ms
step:1589/2110 train_time:83540ms step_avg:52.57ms
step:1590/2110 train_time:83627ms step_avg:52.60ms
step:1591/2110 train_time:83714ms step_avg:52.62ms
step:1592/2110 train_time:83800ms step_avg:52.64ms
step:1593/2110 train_time:83887ms step_avg:52.66ms
step:1594/2110 train_time:83974ms step_avg:52.68ms
step:1595/2110 train_time:84061ms step_avg:52.70ms
step:1596/2110 train_time:84148ms step_avg:52.72ms
step:1597/2110 train_time:84235ms step_avg:52.75ms
step:1598/2110 train_time:84321ms step_avg:52.77ms
step:1599/2110 train_time:84408ms step_avg:52.79ms
step:1600/2110 train_time:84494ms step_avg:52.81ms
step:1601/2110 train_time:84581ms step_avg:52.83ms
step:1602/2110 train_time:84668ms step_avg:52.85ms
step:1603/2110 train_time:84756ms step_avg:52.87ms
step:1604/2110 train_time:84842ms step_avg:52.89ms
step:1605/2110 train_time:84929ms step_avg:52.92ms
step:1606/2110 train_time:85017ms step_avg:52.94ms
step:1607/2110 train_time:85104ms step_avg:52.96ms
step:1608/2110 train_time:85190ms step_avg:52.98ms
step:1609/2110 train_time:85277ms step_avg:53.00ms
step:1610/2110 train_time:85363ms step_avg:53.02ms
step:1611/2110 train_time:85451ms step_avg:53.04ms
step:1612/2110 train_time:85537ms step_avg:53.06ms
step:1613/2110 train_time:85625ms step_avg:53.08ms
step:1614/2110 train_time:85711ms step_avg:53.10ms
step:1615/2110 train_time:85797ms step_avg:53.13ms
step:1616/2110 train_time:85883ms step_avg:53.15ms
step:1617/2110 train_time:85971ms step_avg:53.17ms
step:1618/2110 train_time:86057ms step_avg:53.19ms
step:1619/2110 train_time:86145ms step_avg:53.21ms
step:1620/2110 train_time:86232ms step_avg:53.23ms
step:1621/2110 train_time:86320ms step_avg:53.25ms
step:1622/2110 train_time:86407ms step_avg:53.27ms
step:1623/2110 train_time:86494ms step_avg:53.29ms
step:1624/2110 train_time:86580ms step_avg:53.31ms
step:1625/2110 train_time:86667ms step_avg:53.33ms
step:1626/2110 train_time:86753ms step_avg:53.35ms
step:1627/2110 train_time:86840ms step_avg:53.37ms
step:1628/2110 train_time:86927ms step_avg:53.39ms
step:1629/2110 train_time:87014ms step_avg:53.42ms
step:1630/2110 train_time:87100ms step_avg:53.44ms
step:1631/2110 train_time:87187ms step_avg:53.46ms
step:1632/2110 train_time:87273ms step_avg:53.48ms
step:1633/2110 train_time:87361ms step_avg:53.50ms
step:1634/2110 train_time:87448ms step_avg:53.52ms
step:1635/2110 train_time:87535ms step_avg:53.54ms
step:1636/2110 train_time:87621ms step_avg:53.56ms
step:1637/2110 train_time:87708ms step_avg:53.58ms
step:1638/2110 train_time:87795ms step_avg:53.60ms
step:1639/2110 train_time:87881ms step_avg:53.62ms
step:1640/2110 train_time:87968ms step_avg:53.64ms
step:1641/2110 train_time:88055ms step_avg:53.66ms
step:1642/2110 train_time:88141ms step_avg:53.68ms
step:1643/2110 train_time:88229ms step_avg:53.70ms
step:1644/2110 train_time:88317ms step_avg:53.72ms
step:1645/2110 train_time:88405ms step_avg:53.74ms
step:1646/2110 train_time:88492ms step_avg:53.76ms
step:1647/2110 train_time:88578ms step_avg:53.78ms
step:1648/2110 train_time:88664ms step_avg:53.80ms
step:1649/2110 train_time:88752ms step_avg:53.82ms
step:1650/2110 train_time:88838ms step_avg:53.84ms
step:1651/2110 train_time:88925ms step_avg:53.86ms
step:1652/2110 train_time:89011ms step_avg:53.88ms
step:1653/2110 train_time:89099ms step_avg:53.90ms
step:1654/2110 train_time:89185ms step_avg:53.92ms
step:1655/2110 train_time:89273ms step_avg:53.94ms
step:1656/2110 train_time:89359ms step_avg:53.96ms
step:1657/2110 train_time:89447ms step_avg:53.98ms
step:1658/2110 train_time:89534ms step_avg:54.00ms
step:1659/2110 train_time:89622ms step_avg:54.02ms
step:1660/2110 train_time:89709ms step_avg:54.04ms
step:1661/2110 train_time:89797ms step_avg:54.06ms
step:1662/2110 train_time:89885ms step_avg:54.08ms
step:1663/2110 train_time:89973ms step_avg:54.10ms
step:1664/2110 train_time:90061ms step_avg:54.12ms
step:1665/2110 train_time:90149ms step_avg:54.14ms
step:1666/2110 train_time:90237ms step_avg:54.16ms
step:1667/2110 train_time:90325ms step_avg:54.18ms
step:1668/2110 train_time:90413ms step_avg:54.20ms
step:1669/2110 train_time:90501ms step_avg:54.22ms
step:1670/2110 train_time:90588ms step_avg:54.24ms
step:1671/2110 train_time:90676ms step_avg:54.26ms
step:1672/2110 train_time:90763ms step_avg:54.28ms
step:1673/2110 train_time:90852ms step_avg:54.30ms
step:1674/2110 train_time:90940ms step_avg:54.32ms
step:1675/2110 train_time:91029ms step_avg:54.35ms
step:1676/2110 train_time:91116ms step_avg:54.37ms
step:1677/2110 train_time:91205ms step_avg:54.39ms
step:1678/2110 train_time:91292ms step_avg:54.41ms
step:1679/2110 train_time:91381ms step_avg:54.43ms
step:1680/2110 train_time:91469ms step_avg:54.45ms
step:1681/2110 train_time:91557ms step_avg:54.47ms
step:1682/2110 train_time:91644ms step_avg:54.49ms
step:1683/2110 train_time:91733ms step_avg:54.51ms
step:1684/2110 train_time:91820ms step_avg:54.52ms
step:1685/2110 train_time:91908ms step_avg:54.54ms
step:1686/2110 train_time:91996ms step_avg:54.56ms
step:1687/2110 train_time:92084ms step_avg:54.58ms
step:1688/2110 train_time:92172ms step_avg:54.60ms
step:1689/2110 train_time:92260ms step_avg:54.62ms
step:1690/2110 train_time:92348ms step_avg:54.64ms
step:1691/2110 train_time:92436ms step_avg:54.66ms
step:1692/2110 train_time:92524ms step_avg:54.68ms
step:1693/2110 train_time:92612ms step_avg:54.70ms
step:1694/2110 train_time:92699ms step_avg:54.72ms
step:1695/2110 train_time:92787ms step_avg:54.74ms
step:1696/2110 train_time:92875ms step_avg:54.76ms
step:1697/2110 train_time:92963ms step_avg:54.78ms
step:1698/2110 train_time:93052ms step_avg:54.80ms
step:1699/2110 train_time:93140ms step_avg:54.82ms
step:1700/2110 train_time:93228ms step_avg:54.84ms
step:1701/2110 train_time:93316ms step_avg:54.86ms
step:1702/2110 train_time:93404ms step_avg:54.88ms
step:1703/2110 train_time:93492ms step_avg:54.90ms
step:1704/2110 train_time:93579ms step_avg:54.92ms
step:1705/2110 train_time:93668ms step_avg:54.94ms
step:1706/2110 train_time:93756ms step_avg:54.96ms
step:1707/2110 train_time:93845ms step_avg:54.98ms
step:1708/2110 train_time:93933ms step_avg:55.00ms
step:1709/2110 train_time:94021ms step_avg:55.02ms
step:1710/2110 train_time:94109ms step_avg:55.03ms
step:1711/2110 train_time:94197ms step_avg:55.05ms
step:1712/2110 train_time:94284ms step_avg:55.07ms
step:1713/2110 train_time:94373ms step_avg:55.09ms
step:1714/2110 train_time:94461ms step_avg:55.11ms
step:1715/2110 train_time:94549ms step_avg:55.13ms
step:1716/2110 train_time:94637ms step_avg:55.15ms
step:1717/2110 train_time:94726ms step_avg:55.17ms
step:1718/2110 train_time:94814ms step_avg:55.19ms
step:1719/2110 train_time:94902ms step_avg:55.21ms
step:1720/2110 train_time:94990ms step_avg:55.23ms
step:1721/2110 train_time:95078ms step_avg:55.25ms
step:1722/2110 train_time:95165ms step_avg:55.26ms
step:1723/2110 train_time:95255ms step_avg:55.28ms
step:1724/2110 train_time:95342ms step_avg:55.30ms
step:1725/2110 train_time:95432ms step_avg:55.32ms
step:1726/2110 train_time:95520ms step_avg:55.34ms
step:1727/2110 train_time:95608ms step_avg:55.36ms
step:1728/2110 train_time:95696ms step_avg:55.38ms
step:1729/2110 train_time:95784ms step_avg:55.40ms
step:1730/2110 train_time:95872ms step_avg:55.42ms
step:1731/2110 train_time:95960ms step_avg:55.44ms
step:1732/2110 train_time:96048ms step_avg:55.45ms
step:1733/2110 train_time:96137ms step_avg:55.47ms
step:1734/2110 train_time:96224ms step_avg:55.49ms
step:1735/2110 train_time:96312ms step_avg:55.51ms
step:1736/2110 train_time:96399ms step_avg:55.53ms
step:1737/2110 train_time:96488ms step_avg:55.55ms
step:1738/2110 train_time:96575ms step_avg:55.57ms
step:1739/2110 train_time:96664ms step_avg:55.59ms
step:1740/2110 train_time:96753ms step_avg:55.60ms
step:1741/2110 train_time:96841ms step_avg:55.62ms
step:1742/2110 train_time:96930ms step_avg:55.64ms
step:1743/2110 train_time:97019ms step_avg:55.66ms
step:1744/2110 train_time:97107ms step_avg:55.68ms
step:1745/2110 train_time:97195ms step_avg:55.70ms
step:1746/2110 train_time:97283ms step_avg:55.72ms
step:1747/2110 train_time:97371ms step_avg:55.74ms
step:1748/2110 train_time:97459ms step_avg:55.75ms
step:1749/2110 train_time:97547ms step_avg:55.77ms
step:1750/2110 train_time:97636ms step_avg:55.79ms
step:1750/2110 val_loss:3.3810 train_time:97726ms step_avg:55.84ms
step:1751/2110 train_time:97759ms step_avg:55.83ms
step:1752/2110 train_time:97818ms step_avg:55.83ms
step:1753/2110 train_time:97911ms step_avg:55.85ms
step:1754/2110 train_time:97998ms step_avg:55.87ms
step:1755/2110 train_time:98086ms step_avg:55.89ms
step:1756/2110 train_time:98173ms step_avg:55.91ms
step:1757/2110 train_time:98261ms step_avg:55.93ms
step:1758/2110 train_time:98348ms step_avg:55.94ms
step:1759/2110 train_time:98434ms step_avg:55.96ms
step:1760/2110 train_time:98521ms step_avg:55.98ms
step:1761/2110 train_time:98609ms step_avg:56.00ms
step:1762/2110 train_time:98697ms step_avg:56.01ms
step:1763/2110 train_time:98790ms step_avg:56.04ms
step:1764/2110 train_time:98880ms step_avg:56.05ms
step:1765/2110 train_time:98969ms step_avg:56.07ms
step:1766/2110 train_time:99056ms step_avg:56.09ms
step:1767/2110 train_time:99144ms step_avg:56.11ms
step:1768/2110 train_time:99231ms step_avg:56.13ms
step:1769/2110 train_time:99317ms step_avg:56.14ms
step:1770/2110 train_time:99404ms step_avg:56.16ms
step:1771/2110 train_time:99491ms step_avg:56.18ms
step:1772/2110 train_time:99578ms step_avg:56.20ms
step:1773/2110 train_time:99669ms step_avg:56.22ms
step:1774/2110 train_time:99758ms step_avg:56.23ms
step:1775/2110 train_time:99848ms step_avg:56.25ms
step:1776/2110 train_time:99936ms step_avg:56.27ms
step:1777/2110 train_time:100025ms step_avg:56.29ms
step:1778/2110 train_time:100112ms step_avg:56.31ms
step:1779/2110 train_time:100200ms step_avg:56.32ms
step:1780/2110 train_time:100287ms step_avg:56.34ms
step:1781/2110 train_time:100375ms step_avg:56.36ms
step:1782/2110 train_time:100463ms step_avg:56.38ms
step:1783/2110 train_time:100551ms step_avg:56.39ms
step:1784/2110 train_time:100639ms step_avg:56.41ms
step:1785/2110 train_time:100727ms step_avg:56.43ms
step:1786/2110 train_time:100815ms step_avg:56.45ms
step:1787/2110 train_time:100904ms step_avg:56.47ms
step:1788/2110 train_time:100992ms step_avg:56.48ms
step:1789/2110 train_time:101082ms step_avg:56.50ms
step:1790/2110 train_time:101169ms step_avg:56.52ms
step:1791/2110 train_time:101257ms step_avg:56.54ms
step:1792/2110 train_time:101344ms step_avg:56.55ms
step:1793/2110 train_time:101432ms step_avg:56.57ms
step:1794/2110 train_time:101519ms step_avg:56.59ms
step:1795/2110 train_time:101608ms step_avg:56.61ms
step:1796/2110 train_time:101695ms step_avg:56.62ms
step:1797/2110 train_time:101784ms step_avg:56.64ms
step:1798/2110 train_time:101874ms step_avg:56.66ms
step:1799/2110 train_time:101964ms step_avg:56.68ms
step:1800/2110 train_time:102051ms step_avg:56.70ms
step:1801/2110 train_time:102139ms step_avg:56.71ms
step:1802/2110 train_time:102227ms step_avg:56.73ms
step:1803/2110 train_time:102314ms step_avg:56.75ms
step:1804/2110 train_time:102402ms step_avg:56.76ms
step:1805/2110 train_time:102489ms step_avg:56.78ms
step:1806/2110 train_time:102577ms step_avg:56.80ms
step:1807/2110 train_time:102666ms step_avg:56.82ms
step:1808/2110 train_time:102754ms step_avg:56.83ms
step:1809/2110 train_time:102843ms step_avg:56.85ms
step:1810/2110 train_time:102931ms step_avg:56.87ms
step:1811/2110 train_time:103021ms step_avg:56.89ms
step:1812/2110 train_time:103108ms step_avg:56.90ms
step:1813/2110 train_time:103196ms step_avg:56.92ms
step:1814/2110 train_time:103285ms step_avg:56.94ms
step:1815/2110 train_time:103373ms step_avg:56.95ms
step:1816/2110 train_time:103459ms step_avg:56.97ms
step:1817/2110 train_time:103548ms step_avg:56.99ms
step:1818/2110 train_time:103635ms step_avg:57.00ms
step:1819/2110 train_time:103722ms step_avg:57.02ms
step:1820/2110 train_time:103810ms step_avg:57.04ms
step:1821/2110 train_time:103900ms step_avg:57.06ms
step:1822/2110 train_time:103988ms step_avg:57.07ms
step:1823/2110 train_time:104075ms step_avg:57.09ms
step:1824/2110 train_time:104163ms step_avg:57.11ms
step:1825/2110 train_time:104251ms step_avg:57.12ms
step:1826/2110 train_time:104339ms step_avg:57.14ms
step:1827/2110 train_time:104427ms step_avg:57.16ms
step:1828/2110 train_time:104515ms step_avg:57.17ms
step:1829/2110 train_time:104602ms step_avg:57.19ms
step:1830/2110 train_time:104690ms step_avg:57.21ms
step:1831/2110 train_time:104779ms step_avg:57.23ms
step:1832/2110 train_time:104867ms step_avg:57.24ms
step:1833/2110 train_time:104955ms step_avg:57.26ms
step:1834/2110 train_time:105044ms step_avg:57.28ms
step:1835/2110 train_time:105132ms step_avg:57.29ms
step:1836/2110 train_time:105219ms step_avg:57.31ms
step:1837/2110 train_time:105307ms step_avg:57.33ms
step:1838/2110 train_time:105394ms step_avg:57.34ms
step:1839/2110 train_time:105483ms step_avg:57.36ms
step:1840/2110 train_time:105570ms step_avg:57.37ms
step:1841/2110 train_time:105658ms step_avg:57.39ms
step:1842/2110 train_time:105747ms step_avg:57.41ms
step:1843/2110 train_time:105835ms step_avg:57.43ms
step:1844/2110 train_time:105923ms step_avg:57.44ms
step:1845/2110 train_time:106012ms step_avg:57.46ms
step:1846/2110 train_time:106100ms step_avg:57.48ms
step:1847/2110 train_time:106187ms step_avg:57.49ms
step:1848/2110 train_time:106274ms step_avg:57.51ms
step:1849/2110 train_time:106362ms step_avg:57.52ms
step:1850/2110 train_time:106449ms step_avg:57.54ms
step:1851/2110 train_time:106537ms step_avg:57.56ms
step:1852/2110 train_time:106625ms step_avg:57.57ms
step:1853/2110 train_time:106714ms step_avg:57.59ms
step:1854/2110 train_time:106802ms step_avg:57.61ms
step:1855/2110 train_time:106890ms step_avg:57.62ms
step:1856/2110 train_time:106979ms step_avg:57.64ms
step:1857/2110 train_time:107067ms step_avg:57.66ms
step:1858/2110 train_time:107155ms step_avg:57.67ms
step:1859/2110 train_time:107244ms step_avg:57.69ms
step:1860/2110 train_time:107331ms step_avg:57.71ms
step:1861/2110 train_time:107419ms step_avg:57.72ms
step:1862/2110 train_time:107506ms step_avg:57.74ms
step:1863/2110 train_time:107594ms step_avg:57.75ms
step:1864/2110 train_time:107682ms step_avg:57.77ms
step:1865/2110 train_time:107771ms step_avg:57.79ms
step:1866/2110 train_time:107860ms step_avg:57.80ms
step:1867/2110 train_time:107948ms step_avg:57.82ms
step:1868/2110 train_time:108037ms step_avg:57.84ms
step:1869/2110 train_time:108126ms step_avg:57.85ms
step:1870/2110 train_time:108214ms step_avg:57.87ms
step:1871/2110 train_time:108303ms step_avg:57.88ms
step:1872/2110 train_time:108390ms step_avg:57.90ms
step:1873/2110 train_time:108478ms step_avg:57.92ms
step:1874/2110 train_time:108565ms step_avg:57.93ms
step:1875/2110 train_time:108654ms step_avg:57.95ms
step:1876/2110 train_time:108742ms step_avg:57.96ms
step:1877/2110 train_time:108830ms step_avg:57.98ms
step:1878/2110 train_time:108918ms step_avg:58.00ms
step:1879/2110 train_time:109006ms step_avg:58.01ms
step:1880/2110 train_time:109093ms step_avg:58.03ms
step:1881/2110 train_time:109182ms step_avg:58.04ms
step:1882/2110 train_time:109269ms step_avg:58.06ms
step:1883/2110 train_time:109358ms step_avg:58.08ms
step:1884/2110 train_time:109446ms step_avg:58.09ms
step:1885/2110 train_time:109534ms step_avg:58.11ms
step:1886/2110 train_time:109621ms step_avg:58.12ms
step:1887/2110 train_time:109709ms step_avg:58.14ms
step:1888/2110 train_time:109797ms step_avg:58.16ms
step:1889/2110 train_time:109887ms step_avg:58.17ms
step:1890/2110 train_time:109975ms step_avg:58.19ms
step:1891/2110 train_time:110064ms step_avg:58.20ms
step:1892/2110 train_time:110153ms step_avg:58.22ms
step:1893/2110 train_time:110242ms step_avg:58.24ms
step:1894/2110 train_time:110329ms step_avg:58.25ms
step:1895/2110 train_time:110418ms step_avg:58.27ms
step:1896/2110 train_time:110506ms step_avg:58.28ms
step:1897/2110 train_time:110594ms step_avg:58.30ms
step:1898/2110 train_time:110682ms step_avg:58.31ms
step:1899/2110 train_time:110769ms step_avg:58.33ms
step:1900/2110 train_time:110858ms step_avg:58.35ms
step:1901/2110 train_time:110947ms step_avg:58.36ms
step:1902/2110 train_time:111035ms step_avg:58.38ms
step:1903/2110 train_time:111123ms step_avg:58.39ms
step:1904/2110 train_time:111211ms step_avg:58.41ms
step:1905/2110 train_time:111299ms step_avg:58.42ms
step:1906/2110 train_time:111386ms step_avg:58.44ms
step:1907/2110 train_time:111474ms step_avg:58.46ms
step:1908/2110 train_time:111563ms step_avg:58.47ms
step:1909/2110 train_time:111651ms step_avg:58.49ms
step:1910/2110 train_time:111738ms step_avg:58.50ms
step:1911/2110 train_time:111827ms step_avg:58.52ms
step:1912/2110 train_time:111914ms step_avg:58.53ms
step:1913/2110 train_time:112003ms step_avg:58.55ms
step:1914/2110 train_time:112089ms step_avg:58.56ms
step:1915/2110 train_time:112178ms step_avg:58.58ms
step:1916/2110 train_time:112267ms step_avg:58.59ms
step:1917/2110 train_time:112355ms step_avg:58.61ms
step:1918/2110 train_time:112443ms step_avg:58.63ms
step:1919/2110 train_time:112531ms step_avg:58.64ms
step:1920/2110 train_time:112618ms step_avg:58.66ms
step:1921/2110 train_time:112707ms step_avg:58.67ms
step:1922/2110 train_time:112795ms step_avg:58.69ms
step:1923/2110 train_time:112882ms step_avg:58.70ms
step:1924/2110 train_time:112970ms step_avg:58.72ms
step:1925/2110 train_time:113060ms step_avg:58.73ms
step:1926/2110 train_time:113147ms step_avg:58.75ms
step:1927/2110 train_time:113237ms step_avg:58.76ms
step:1928/2110 train_time:113325ms step_avg:58.78ms
step:1929/2110 train_time:113413ms step_avg:58.79ms
step:1930/2110 train_time:113500ms step_avg:58.81ms
step:1931/2110 train_time:113588ms step_avg:58.82ms
step:1932/2110 train_time:113676ms step_avg:58.84ms
step:1933/2110 train_time:113765ms step_avg:58.85ms
step:1934/2110 train_time:113852ms step_avg:58.87ms
step:1935/2110 train_time:113941ms step_avg:58.88ms
step:1936/2110 train_time:114029ms step_avg:58.90ms
step:1937/2110 train_time:114118ms step_avg:58.91ms
step:1938/2110 train_time:114207ms step_avg:58.93ms
step:1939/2110 train_time:114294ms step_avg:58.94ms
step:1940/2110 train_time:114384ms step_avg:58.96ms
step:1941/2110 train_time:114472ms step_avg:58.98ms
step:1942/2110 train_time:114559ms step_avg:58.99ms
step:1943/2110 train_time:114647ms step_avg:59.01ms
step:1944/2110 train_time:114735ms step_avg:59.02ms
step:1945/2110 train_time:114823ms step_avg:59.03ms
step:1946/2110 train_time:114910ms step_avg:59.05ms
step:1947/2110 train_time:114999ms step_avg:59.06ms
step:1948/2110 train_time:115087ms step_avg:59.08ms
step:1949/2110 train_time:115176ms step_avg:59.09ms
step:1950/2110 train_time:115264ms step_avg:59.11ms
step:1951/2110 train_time:115352ms step_avg:59.12ms
step:1952/2110 train_time:115440ms step_avg:59.14ms
step:1953/2110 train_time:115528ms step_avg:59.15ms
step:1954/2110 train_time:115616ms step_avg:59.17ms
step:1955/2110 train_time:115704ms step_avg:59.18ms
step:1956/2110 train_time:115792ms step_avg:59.20ms
step:1957/2110 train_time:115880ms step_avg:59.21ms
step:1958/2110 train_time:115967ms step_avg:59.23ms
step:1959/2110 train_time:116056ms step_avg:59.24ms
step:1960/2110 train_time:116145ms step_avg:59.26ms
step:1961/2110 train_time:116233ms step_avg:59.27ms
step:1962/2110 train_time:116322ms step_avg:59.29ms
step:1963/2110 train_time:116409ms step_avg:59.30ms
step:1964/2110 train_time:116497ms step_avg:59.32ms
step:1965/2110 train_time:116586ms step_avg:59.33ms
step:1966/2110 train_time:116674ms step_avg:59.35ms
step:1967/2110 train_time:116763ms step_avg:59.36ms
step:1968/2110 train_time:116850ms step_avg:59.37ms
step:1969/2110 train_time:116939ms step_avg:59.39ms
step:1970/2110 train_time:117026ms step_avg:59.40ms
step:1971/2110 train_time:117115ms step_avg:59.42ms
step:1972/2110 train_time:117203ms step_avg:59.43ms
step:1973/2110 train_time:117291ms step_avg:59.45ms
step:1974/2110 train_time:117379ms step_avg:59.46ms
step:1975/2110 train_time:117467ms step_avg:59.48ms
step:1976/2110 train_time:117556ms step_avg:59.49ms
step:1977/2110 train_time:117645ms step_avg:59.51ms
step:1978/2110 train_time:117732ms step_avg:59.52ms
step:1979/2110 train_time:117820ms step_avg:59.53ms
step:1980/2110 train_time:117907ms step_avg:59.55ms
step:1981/2110 train_time:117995ms step_avg:59.56ms
step:1982/2110 train_time:118083ms step_avg:59.58ms
step:1983/2110 train_time:118172ms step_avg:59.59ms
step:1984/2110 train_time:118260ms step_avg:59.61ms
step:1985/2110 train_time:118348ms step_avg:59.62ms
step:1986/2110 train_time:118436ms step_avg:59.64ms
step:1987/2110 train_time:118524ms step_avg:59.65ms
step:1988/2110 train_time:118611ms step_avg:59.66ms
step:1989/2110 train_time:118699ms step_avg:59.68ms
step:1990/2110 train_time:118787ms step_avg:59.69ms
step:1991/2110 train_time:118875ms step_avg:59.71ms
step:1992/2110 train_time:118963ms step_avg:59.72ms
step:1993/2110 train_time:119051ms step_avg:59.73ms
step:1994/2110 train_time:119140ms step_avg:59.75ms
step:1995/2110 train_time:119228ms step_avg:59.76ms
step:1996/2110 train_time:119316ms step_avg:59.78ms
step:1997/2110 train_time:119405ms step_avg:59.79ms
step:1998/2110 train_time:119493ms step_avg:59.81ms
step:1999/2110 train_time:119582ms step_avg:59.82ms
step:2000/2110 train_time:119669ms step_avg:59.83ms
step:2000/2110 val_loss:3.3063 train_time:119760ms step_avg:59.88ms
step:2001/2110 train_time:119789ms step_avg:59.86ms
step:2002/2110 train_time:119855ms step_avg:59.87ms
step:2003/2110 train_time:119947ms step_avg:59.88ms
step:2004/2110 train_time:120035ms step_avg:59.90ms
step:2005/2110 train_time:120123ms step_avg:59.91ms
step:2006/2110 train_time:120211ms step_avg:59.93ms
step:2007/2110 train_time:120299ms step_avg:59.94ms
step:2008/2110 train_time:120385ms step_avg:59.95ms
step:2009/2110 train_time:120473ms step_avg:59.97ms
step:2010/2110 train_time:120560ms step_avg:59.98ms
step:2011/2110 train_time:120647ms step_avg:59.99ms
step:2012/2110 train_time:120737ms step_avg:60.01ms
step:2013/2110 train_time:120830ms step_avg:60.02ms
step:2014/2110 train_time:120920ms step_avg:60.04ms
step:2015/2110 train_time:121009ms step_avg:60.05ms
step:2016/2110 train_time:121097ms step_avg:60.07ms
step:2017/2110 train_time:121185ms step_avg:60.08ms
step:2018/2110 train_time:121272ms step_avg:60.10ms
step:2019/2110 train_time:121360ms step_avg:60.11ms
step:2020/2110 train_time:121447ms step_avg:60.12ms
step:2021/2110 train_time:121534ms step_avg:60.14ms
step:2022/2110 train_time:121621ms step_avg:60.15ms
step:2023/2110 train_time:121710ms step_avg:60.16ms
step:2024/2110 train_time:121799ms step_avg:60.18ms
step:2025/2110 train_time:121890ms step_avg:60.19ms
step:2026/2110 train_time:121979ms step_avg:60.21ms
step:2027/2110 train_time:122067ms step_avg:60.22ms
step:2028/2110 train_time:122154ms step_avg:60.23ms
step:2029/2110 train_time:122242ms step_avg:60.25ms
step:2030/2110 train_time:122329ms step_avg:60.26ms
step:2031/2110 train_time:122418ms step_avg:60.27ms
step:2032/2110 train_time:122505ms step_avg:60.29ms
step:2033/2110 train_time:122593ms step_avg:60.30ms
step:2034/2110 train_time:122681ms step_avg:60.31ms
step:2035/2110 train_time:122771ms step_avg:60.33ms
step:2036/2110 train_time:122859ms step_avg:60.34ms
step:2037/2110 train_time:122949ms step_avg:60.36ms
step:2038/2110 train_time:123038ms step_avg:60.37ms
step:2039/2110 train_time:123126ms step_avg:60.39ms
step:2040/2110 train_time:123214ms step_avg:60.40ms
step:2041/2110 train_time:123302ms step_avg:60.41ms
step:2042/2110 train_time:123389ms step_avg:60.43ms
step:2043/2110 train_time:123477ms step_avg:60.44ms
step:2044/2110 train_time:123563ms step_avg:60.45ms
step:2045/2110 train_time:123653ms step_avg:60.47ms
step:2046/2110 train_time:123740ms step_avg:60.48ms
step:2047/2110 train_time:123830ms step_avg:60.49ms
step:2048/2110 train_time:123919ms step_avg:60.51ms
step:2049/2110 train_time:124008ms step_avg:60.52ms
step:2050/2110 train_time:124096ms step_avg:60.53ms
step:2051/2110 train_time:124184ms step_avg:60.55ms
step:2052/2110 train_time:124272ms step_avg:60.56ms
step:2053/2110 train_time:124360ms step_avg:60.57ms
step:2054/2110 train_time:124448ms step_avg:60.59ms
step:2055/2110 train_time:124536ms step_avg:60.60ms
step:2056/2110 train_time:124623ms step_avg:60.61ms
step:2057/2110 train_time:124712ms step_avg:60.63ms
step:2058/2110 train_time:124800ms step_avg:60.64ms
step:2059/2110 train_time:124889ms step_avg:60.66ms
step:2060/2110 train_time:124978ms step_avg:60.67ms
step:2061/2110 train_time:125067ms step_avg:60.68ms
step:2062/2110 train_time:125155ms step_avg:60.70ms
step:2063/2110 train_time:125243ms step_avg:60.71ms
step:2064/2110 train_time:125331ms step_avg:60.72ms
step:2065/2110 train_time:125419ms step_avg:60.74ms
step:2066/2110 train_time:125507ms step_avg:60.75ms
step:2067/2110 train_time:125595ms step_avg:60.76ms
step:2068/2110 train_time:125683ms step_avg:60.78ms
step:2069/2110 train_time:125772ms step_avg:60.79ms
step:2070/2110 train_time:125861ms step_avg:60.80ms
step:2071/2110 train_time:125949ms step_avg:60.82ms
step:2072/2110 train_time:126037ms step_avg:60.83ms
step:2073/2110 train_time:126126ms step_avg:60.84ms
step:2074/2110 train_time:126214ms step_avg:60.86ms
step:2075/2110 train_time:126303ms step_avg:60.87ms
step:2076/2110 train_time:126391ms step_avg:60.88ms
step:2077/2110 train_time:126481ms step_avg:60.90ms
step:2078/2110 train_time:126570ms step_avg:60.91ms
step:2079/2110 train_time:126659ms step_avg:60.92ms
step:2080/2110 train_time:126747ms step_avg:60.94ms
step:2081/2110 train_time:126835ms step_avg:60.95ms
step:2082/2110 train_time:126923ms step_avg:60.96ms
step:2083/2110 train_time:127013ms step_avg:60.98ms
step:2084/2110 train_time:127101ms step_avg:60.99ms
step:2085/2110 train_time:127190ms step_avg:61.00ms
step:2086/2110 train_time:127279ms step_avg:61.02ms
step:2087/2110 train_time:127367ms step_avg:61.03ms
step:2088/2110 train_time:127454ms step_avg:61.04ms
step:2089/2110 train_time:127543ms step_avg:61.05ms
step:2090/2110 train_time:127632ms step_avg:61.07ms
step:2091/2110 train_time:127722ms step_avg:61.08ms
step:2092/2110 train_time:127809ms step_avg:61.09ms
step:2093/2110 train_time:127898ms step_avg:61.11ms
step:2094/2110 train_time:127985ms step_avg:61.12ms
step:2095/2110 train_time:128075ms step_avg:61.13ms
step:2096/2110 train_time:128162ms step_avg:61.15ms
step:2097/2110 train_time:128253ms step_avg:61.16ms
step:2098/2110 train_time:128340ms step_avg:61.17ms
step:2099/2110 train_time:128428ms step_avg:61.19ms
step:2100/2110 train_time:128516ms step_avg:61.20ms
step:2101/2110 train_time:128605ms step_avg:61.21ms
step:2102/2110 train_time:128693ms step_avg:61.22ms
step:2103/2110 train_time:128782ms step_avg:61.24ms
step:2104/2110 train_time:128871ms step_avg:61.25ms
step:2105/2110 train_time:128961ms step_avg:61.26ms
step:2106/2110 train_time:129049ms step_avg:61.28ms
step:2107/2110 train_time:129137ms step_avg:61.29ms
step:2108/2110 train_time:129226ms step_avg:61.30ms
step:2109/2110 train_time:129315ms step_avg:61.32ms
step:2110/2110 train_time:129403ms step_avg:61.33ms
step:2110/2110 val_loss:3.2821 train_time:129494ms step_avg:61.37ms
peak memory allocated: 30540 MiB reserved: 44136 MiB

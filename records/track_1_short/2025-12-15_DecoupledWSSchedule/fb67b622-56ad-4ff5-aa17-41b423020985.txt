import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Tue Dec 16 00:30:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   38C    P0            128W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   42C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   43C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   37C    P0            126W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   42C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              80      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              81      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:91ms step_avg:90.90ms
step:2/2110 train_time:120ms step_avg:59.96ms
step:3/2110 train_time:146ms step_avg:48.71ms
step:4/2110 train_time:179ms step_avg:44.75ms
step:5/2110 train_time:216ms step_avg:43.25ms
step:6/2110 train_time:406ms step_avg:67.75ms
step:7/2110 train_time:617ms step_avg:88.11ms
step:8/2110 train_time:649ms step_avg:81.15ms
step:9/2110 train_time:682ms step_avg:75.79ms
step:10/2110 train_time:715ms step_avg:71.46ms
step:11/2110 train_time:748ms step_avg:67.97ms
step:12/2110 train_time:781ms step_avg:65.05ms
step:13/2110 train_time:814ms step_avg:62.60ms
step:14/2110 train_time:847ms step_avg:60.51ms
step:15/2110 train_time:880ms step_avg:58.67ms
step:16/2110 train_time:913ms step_avg:57.03ms
step:17/2110 train_time:946ms step_avg:55.63ms
step:18/2110 train_time:978ms step_avg:54.36ms
step:19/2110 train_time:1012ms step_avg:53.26ms
step:20/2110 train_time:1045ms step_avg:52.25ms
step:21/2110 train_time:1078ms step_avg:51.34ms
step:22/2110 train_time:1112ms step_avg:50.53ms
step:23/2110 train_time:1144ms step_avg:49.75ms
step:24/2110 train_time:1179ms step_avg:49.13ms
step:25/2110 train_time:1210ms step_avg:48.41ms
step:26/2110 train_time:1243ms step_avg:47.80ms
step:27/2110 train_time:1276ms step_avg:47.26ms
step:28/2110 train_time:1309ms step_avg:46.75ms
step:29/2110 train_time:1342ms step_avg:46.28ms
step:30/2110 train_time:1375ms step_avg:45.83ms
step:31/2110 train_time:1408ms step_avg:45.42ms
step:32/2110 train_time:1441ms step_avg:45.02ms
step:33/2110 train_time:1474ms step_avg:44.66ms
step:34/2110 train_time:1507ms step_avg:44.33ms
step:35/2110 train_time:1542ms step_avg:44.06ms
step:36/2110 train_time:1577ms step_avg:43.80ms
step:37/2110 train_time:1610ms step_avg:43.51ms
step:38/2110 train_time:1643ms step_avg:43.24ms
step:39/2110 train_time:1677ms step_avg:42.99ms
step:40/2110 train_time:1710ms step_avg:42.74ms
step:41/2110 train_time:1743ms step_avg:42.52ms
step:42/2110 train_time:1776ms step_avg:42.29ms
step:43/2110 train_time:1810ms step_avg:42.09ms
step:44/2110 train_time:1843ms step_avg:41.88ms
step:45/2110 train_time:1876ms step_avg:41.68ms
step:46/2110 train_time:1909ms step_avg:41.49ms
step:47/2110 train_time:1942ms step_avg:41.32ms
step:48/2110 train_time:1975ms step_avg:41.14ms
step:49/2110 train_time:2008ms step_avg:40.98ms
step:50/2110 train_time:2041ms step_avg:40.82ms
step:51/2110 train_time:2074ms step_avg:40.67ms
step:52/2110 train_time:2107ms step_avg:40.53ms
step:53/2110 train_time:2140ms step_avg:40.38ms
step:54/2110 train_time:2174ms step_avg:40.25ms
step:55/2110 train_time:2207ms step_avg:40.13ms
step:56/2110 train_time:2240ms step_avg:40.00ms
step:57/2110 train_time:2273ms step_avg:39.88ms
step:58/2110 train_time:2306ms step_avg:39.76ms
step:59/2110 train_time:2339ms step_avg:39.65ms
step:60/2110 train_time:2372ms step_avg:39.54ms
step:61/2110 train_time:2406ms step_avg:39.44ms
step:62/2110 train_time:2438ms step_avg:39.33ms
step:63/2110 train_time:2472ms step_avg:39.24ms
step:64/2110 train_time:2505ms step_avg:39.15ms
step:65/2110 train_time:2539ms step_avg:39.06ms
step:66/2110 train_time:2573ms step_avg:38.98ms
step:67/2110 train_time:2606ms step_avg:38.89ms
step:68/2110 train_time:2639ms step_avg:38.80ms
step:69/2110 train_time:2673ms step_avg:38.73ms
step:70/2110 train_time:2706ms step_avg:38.66ms
step:71/2110 train_time:2740ms step_avg:38.58ms
step:72/2110 train_time:2773ms step_avg:38.51ms
step:73/2110 train_time:2806ms step_avg:38.43ms
step:74/2110 train_time:2838ms step_avg:38.36ms
step:75/2110 train_time:2872ms step_avg:38.29ms
step:76/2110 train_time:2905ms step_avg:38.22ms
step:77/2110 train_time:2938ms step_avg:38.15ms
step:78/2110 train_time:2971ms step_avg:38.09ms
step:79/2110 train_time:3004ms step_avg:38.03ms
step:80/2110 train_time:3038ms step_avg:37.97ms
step:81/2110 train_time:3070ms step_avg:37.90ms
step:82/2110 train_time:3103ms step_avg:37.84ms
step:83/2110 train_time:3136ms step_avg:37.78ms
step:84/2110 train_time:3169ms step_avg:37.73ms
step:85/2110 train_time:3202ms step_avg:37.68ms
step:86/2110 train_time:3235ms step_avg:37.62ms
step:87/2110 train_time:3269ms step_avg:37.57ms
step:88/2110 train_time:3301ms step_avg:37.51ms
step:89/2110 train_time:3334ms step_avg:37.47ms
step:90/2110 train_time:3367ms step_avg:37.42ms
step:91/2110 train_time:3401ms step_avg:37.37ms
step:92/2110 train_time:3433ms step_avg:37.32ms
step:93/2110 train_time:3466ms step_avg:37.27ms
step:94/2110 train_time:3499ms step_avg:37.22ms
step:95/2110 train_time:3533ms step_avg:37.19ms
step:96/2110 train_time:3567ms step_avg:37.15ms
step:97/2110 train_time:3599ms step_avg:37.10ms
step:98/2110 train_time:3632ms step_avg:37.06ms
step:99/2110 train_time:3666ms step_avg:37.03ms
step:100/2110 train_time:3698ms step_avg:36.98ms
step:101/2110 train_time:3732ms step_avg:36.95ms
step:102/2110 train_time:3765ms step_avg:36.92ms
step:103/2110 train_time:3799ms step_avg:36.88ms
step:104/2110 train_time:3832ms step_avg:36.84ms
step:105/2110 train_time:3865ms step_avg:36.81ms
step:106/2110 train_time:3898ms step_avg:36.77ms
step:107/2110 train_time:3931ms step_avg:36.74ms
step:108/2110 train_time:3964ms step_avg:36.70ms
step:109/2110 train_time:3997ms step_avg:36.67ms
step:110/2110 train_time:4033ms step_avg:36.66ms
step:111/2110 train_time:4066ms step_avg:36.63ms
step:112/2110 train_time:4097ms step_avg:36.58ms
step:113/2110 train_time:4131ms step_avg:36.55ms
step:114/2110 train_time:4162ms step_avg:36.51ms
step:115/2110 train_time:4196ms step_avg:36.48ms
step:116/2110 train_time:4228ms step_avg:36.45ms
step:117/2110 train_time:4263ms step_avg:36.43ms
step:118/2110 train_time:4298ms step_avg:36.43ms
step:119/2110 train_time:4330ms step_avg:36.38ms
step:120/2110 train_time:4361ms step_avg:36.34ms
step:121/2110 train_time:4394ms step_avg:36.31ms
step:122/2110 train_time:4426ms step_avg:36.28ms
step:123/2110 train_time:4460ms step_avg:36.26ms
step:124/2110 train_time:4495ms step_avg:36.25ms
step:125/2110 train_time:4528ms step_avg:36.22ms
step:126/2110 train_time:4565ms step_avg:36.23ms
step:127/2110 train_time:4596ms step_avg:36.19ms
step:128/2110 train_time:4630ms step_avg:36.17ms
step:129/2110 train_time:4659ms step_avg:36.12ms
step:130/2110 train_time:4693ms step_avg:36.10ms
step:131/2110 train_time:4725ms step_avg:36.07ms
step:132/2110 train_time:4759ms step_avg:36.05ms
step:133/2110 train_time:4792ms step_avg:36.03ms
step:134/2110 train_time:4828ms step_avg:36.03ms
step:135/2110 train_time:4867ms step_avg:36.05ms
step:136/2110 train_time:4900ms step_avg:36.03ms
step:137/2110 train_time:4928ms step_avg:35.97ms
step:138/2110 train_time:4959ms step_avg:35.93ms
step:139/2110 train_time:4990ms step_avg:35.90ms
step:140/2110 train_time:5024ms step_avg:35.89ms
step:141/2110 train_time:5056ms step_avg:35.86ms
step:142/2110 train_time:5090ms step_avg:35.84ms
step:143/2110 train_time:5122ms step_avg:35.82ms
step:144/2110 train_time:5157ms step_avg:35.81ms
step:145/2110 train_time:5191ms step_avg:35.80ms
step:146/2110 train_time:5226ms step_avg:35.79ms
step:147/2110 train_time:5255ms step_avg:35.75ms
step:148/2110 train_time:5289ms step_avg:35.73ms
step:149/2110 train_time:5320ms step_avg:35.70ms
step:150/2110 train_time:5354ms step_avg:35.69ms
step:151/2110 train_time:5386ms step_avg:35.67ms
step:152/2110 train_time:5421ms step_avg:35.66ms
step:153/2110 train_time:5454ms step_avg:35.65ms
step:154/2110 train_time:5489ms step_avg:35.64ms
step:155/2110 train_time:5519ms step_avg:35.61ms
step:156/2110 train_time:5554ms step_avg:35.60ms
step:157/2110 train_time:5583ms step_avg:35.56ms
step:158/2110 train_time:5616ms step_avg:35.55ms
step:159/2110 train_time:5649ms step_avg:35.53ms
step:160/2110 train_time:5681ms step_avg:35.51ms
step:161/2110 train_time:5715ms step_avg:35.50ms
step:162/2110 train_time:5750ms step_avg:35.50ms
step:163/2110 train_time:5783ms step_avg:35.48ms
step:164/2110 train_time:5815ms step_avg:35.46ms
step:165/2110 train_time:5848ms step_avg:35.44ms
step:166/2110 train_time:5881ms step_avg:35.43ms
step:167/2110 train_time:5915ms step_avg:35.42ms
step:168/2110 train_time:5950ms step_avg:35.42ms
step:169/2110 train_time:5982ms step_avg:35.39ms
step:170/2110 train_time:6018ms step_avg:35.40ms
step:171/2110 train_time:6047ms step_avg:35.36ms
step:172/2110 train_time:6080ms step_avg:35.35ms
step:173/2110 train_time:6112ms step_avg:35.33ms
step:174/2110 train_time:6147ms step_avg:35.33ms
step:175/2110 train_time:6179ms step_avg:35.31ms
step:176/2110 train_time:6212ms step_avg:35.30ms
step:177/2110 train_time:6245ms step_avg:35.28ms
step:178/2110 train_time:6279ms step_avg:35.28ms
step:179/2110 train_time:6312ms step_avg:35.27ms
step:180/2110 train_time:6348ms step_avg:35.27ms
step:181/2110 train_time:6376ms step_avg:35.23ms
step:182/2110 train_time:6410ms step_avg:35.22ms
step:183/2110 train_time:6443ms step_avg:35.21ms
step:184/2110 train_time:6477ms step_avg:35.20ms
step:185/2110 train_time:6509ms step_avg:35.18ms
step:186/2110 train_time:6545ms step_avg:35.19ms
step:187/2110 train_time:6578ms step_avg:35.18ms
step:188/2110 train_time:6608ms step_avg:35.15ms
step:189/2110 train_time:6640ms step_avg:35.13ms
step:190/2110 train_time:6675ms step_avg:35.13ms
step:191/2110 train_time:6706ms step_avg:35.11ms
step:192/2110 train_time:6740ms step_avg:35.10ms
step:193/2110 train_time:6773ms step_avg:35.09ms
step:194/2110 train_time:6806ms step_avg:35.08ms
step:195/2110 train_time:6839ms step_avg:35.07ms
step:196/2110 train_time:6871ms step_avg:35.06ms
step:197/2110 train_time:6904ms step_avg:35.05ms
step:198/2110 train_time:6937ms step_avg:35.03ms
step:199/2110 train_time:6970ms step_avg:35.02ms
step:200/2110 train_time:7002ms step_avg:35.01ms
step:201/2110 train_time:7036ms step_avg:35.01ms
step:202/2110 train_time:7069ms step_avg:34.99ms
step:203/2110 train_time:7102ms step_avg:34.99ms
step:204/2110 train_time:7135ms step_avg:34.97ms
step:205/2110 train_time:7168ms step_avg:34.97ms
step:206/2110 train_time:7201ms step_avg:34.95ms
step:207/2110 train_time:7234ms step_avg:34.95ms
step:208/2110 train_time:7266ms step_avg:34.93ms
step:209/2110 train_time:7300ms step_avg:34.93ms
step:210/2110 train_time:7332ms step_avg:34.92ms
step:211/2110 train_time:7366ms step_avg:34.91ms
step:212/2110 train_time:7399ms step_avg:34.90ms
step:213/2110 train_time:7432ms step_avg:34.89ms
step:214/2110 train_time:7465ms step_avg:34.88ms
step:215/2110 train_time:7498ms step_avg:34.88ms
step:216/2110 train_time:7531ms step_avg:34.87ms
step:217/2110 train_time:7564ms step_avg:34.86ms
step:218/2110 train_time:7599ms step_avg:34.86ms
step:219/2110 train_time:7631ms step_avg:34.84ms
step:220/2110 train_time:7667ms step_avg:34.85ms
step:221/2110 train_time:7698ms step_avg:34.83ms
step:222/2110 train_time:7733ms step_avg:34.83ms
step:223/2110 train_time:7766ms step_avg:34.82ms
step:224/2110 train_time:7800ms step_avg:34.82ms
step:225/2110 train_time:7833ms step_avg:34.81ms
step:226/2110 train_time:7862ms step_avg:34.79ms
step:227/2110 train_time:7894ms step_avg:34.78ms
step:228/2110 train_time:7927ms step_avg:34.77ms
step:229/2110 train_time:7960ms step_avg:34.76ms
step:230/2110 train_time:7993ms step_avg:34.75ms
step:231/2110 train_time:8026ms step_avg:34.75ms
step:232/2110 train_time:8059ms step_avg:34.74ms
step:233/2110 train_time:8092ms step_avg:34.73ms
step:234/2110 train_time:8125ms step_avg:34.72ms
step:235/2110 train_time:8158ms step_avg:34.72ms
step:236/2110 train_time:8193ms step_avg:34.72ms
step:237/2110 train_time:8225ms step_avg:34.70ms
step:238/2110 train_time:8258ms step_avg:34.70ms
step:239/2110 train_time:8291ms step_avg:34.69ms
step:240/2110 train_time:8323ms step_avg:34.68ms
step:241/2110 train_time:8356ms step_avg:34.67ms
step:242/2110 train_time:8390ms step_avg:34.67ms
step:243/2110 train_time:8423ms step_avg:34.66ms
step:244/2110 train_time:8455ms step_avg:34.65ms
step:245/2110 train_time:8489ms step_avg:34.65ms
step:246/2110 train_time:8521ms step_avg:34.64ms
step:247/2110 train_time:8554ms step_avg:34.63ms
step:248/2110 train_time:8587ms step_avg:34.63ms
step:249/2110 train_time:8620ms step_avg:34.62ms
step:250/2110 train_time:8653ms step_avg:34.61ms
step:250/2110 val_loss:4.3023 train_time:8689ms step_avg:34.75ms
step:251/2110 train_time:8715ms step_avg:34.72ms
step:252/2110 train_time:8740ms step_avg:34.68ms
step:253/2110 train_time:8767ms step_avg:34.65ms
step:254/2110 train_time:8793ms step_avg:34.62ms
step:255/2110 train_time:8828ms step_avg:34.62ms
step:256/2110 train_time:8862ms step_avg:34.62ms
step:257/2110 train_time:8896ms step_avg:34.62ms
step:258/2110 train_time:8929ms step_avg:34.61ms
step:259/2110 train_time:8963ms step_avg:34.61ms
step:260/2110 train_time:8996ms step_avg:34.60ms
step:261/2110 train_time:9029ms step_avg:34.59ms
step:262/2110 train_time:9062ms step_avg:34.59ms
step:263/2110 train_time:9095ms step_avg:34.58ms
step:264/2110 train_time:9128ms step_avg:34.57ms
step:265/2110 train_time:9161ms step_avg:34.57ms
step:266/2110 train_time:9194ms step_avg:34.56ms
step:267/2110 train_time:9226ms step_avg:34.55ms
step:268/2110 train_time:9259ms step_avg:34.55ms
step:269/2110 train_time:9292ms step_avg:34.54ms
step:270/2110 train_time:9324ms step_avg:34.53ms
step:271/2110 train_time:9357ms step_avg:34.53ms
step:272/2110 train_time:9390ms step_avg:34.52ms
step:273/2110 train_time:9423ms step_avg:34.51ms
step:274/2110 train_time:9455ms step_avg:34.51ms
step:275/2110 train_time:9488ms step_avg:34.50ms
step:276/2110 train_time:9520ms step_avg:34.49ms
step:277/2110 train_time:9554ms step_avg:34.49ms
step:278/2110 train_time:9586ms step_avg:34.48ms
step:279/2110 train_time:9620ms step_avg:34.48ms
step:280/2110 train_time:9653ms step_avg:34.47ms
step:281/2110 train_time:9686ms step_avg:34.47ms
step:282/2110 train_time:9719ms step_avg:34.47ms
step:283/2110 train_time:9753ms step_avg:34.46ms
step:284/2110 train_time:9786ms step_avg:34.46ms
step:285/2110 train_time:9819ms step_avg:34.45ms
step:286/2110 train_time:9852ms step_avg:34.45ms
step:287/2110 train_time:9886ms step_avg:34.44ms
step:288/2110 train_time:9918ms step_avg:34.44ms
step:289/2110 train_time:9952ms step_avg:34.44ms
step:290/2110 train_time:9984ms step_avg:34.43ms
step:291/2110 train_time:10018ms step_avg:34.42ms
step:292/2110 train_time:10050ms step_avg:34.42ms
step:293/2110 train_time:10083ms step_avg:34.41ms
step:294/2110 train_time:10116ms step_avg:34.41ms
step:295/2110 train_time:10150ms step_avg:34.41ms
step:296/2110 train_time:10182ms step_avg:34.40ms
step:297/2110 train_time:10215ms step_avg:34.40ms
step:298/2110 train_time:10248ms step_avg:34.39ms
step:299/2110 train_time:10281ms step_avg:34.38ms
step:300/2110 train_time:10314ms step_avg:34.38ms
step:301/2110 train_time:10346ms step_avg:34.37ms
step:302/2110 train_time:10379ms step_avg:34.37ms
step:303/2110 train_time:10412ms step_avg:34.36ms
step:304/2110 train_time:10444ms step_avg:34.36ms
step:305/2110 train_time:10477ms step_avg:34.35ms
step:306/2110 train_time:10510ms step_avg:34.35ms
step:307/2110 train_time:10543ms step_avg:34.34ms
step:308/2110 train_time:10576ms step_avg:34.34ms
step:309/2110 train_time:10609ms step_avg:34.33ms
step:310/2110 train_time:10642ms step_avg:34.33ms
step:311/2110 train_time:10675ms step_avg:34.33ms
step:312/2110 train_time:10708ms step_avg:34.32ms
step:313/2110 train_time:10742ms step_avg:34.32ms
step:314/2110 train_time:10774ms step_avg:34.31ms
step:315/2110 train_time:10808ms step_avg:34.31ms
step:316/2110 train_time:10841ms step_avg:34.31ms
step:317/2110 train_time:10875ms step_avg:34.31ms
step:318/2110 train_time:10908ms step_avg:34.30ms
step:319/2110 train_time:10941ms step_avg:34.30ms
step:320/2110 train_time:10975ms step_avg:34.30ms
step:321/2110 train_time:11008ms step_avg:34.29ms
step:322/2110 train_time:11041ms step_avg:34.29ms
step:323/2110 train_time:11074ms step_avg:34.29ms
step:324/2110 train_time:11107ms step_avg:34.28ms
step:325/2110 train_time:11140ms step_avg:34.28ms
step:326/2110 train_time:11173ms step_avg:34.27ms
step:327/2110 train_time:11206ms step_avg:34.27ms
step:328/2110 train_time:11239ms step_avg:34.27ms
step:329/2110 train_time:11272ms step_avg:34.26ms
step:330/2110 train_time:11304ms step_avg:34.26ms
step:331/2110 train_time:11338ms step_avg:34.25ms
step:332/2110 train_time:11370ms step_avg:34.25ms
step:333/2110 train_time:11403ms step_avg:34.24ms
step:334/2110 train_time:11436ms step_avg:34.24ms
step:335/2110 train_time:11469ms step_avg:34.24ms
step:336/2110 train_time:11502ms step_avg:34.23ms
step:337/2110 train_time:11535ms step_avg:34.23ms
step:338/2110 train_time:11567ms step_avg:34.22ms
step:339/2110 train_time:11602ms step_avg:34.23ms
step:340/2110 train_time:11633ms step_avg:34.21ms
step:341/2110 train_time:11666ms step_avg:34.21ms
step:342/2110 train_time:11699ms step_avg:34.21ms
step:343/2110 train_time:11733ms step_avg:34.21ms
step:344/2110 train_time:11765ms step_avg:34.20ms
step:345/2110 train_time:11799ms step_avg:34.20ms
step:346/2110 train_time:11832ms step_avg:34.20ms
step:347/2110 train_time:11865ms step_avg:34.19ms
step:348/2110 train_time:11898ms step_avg:34.19ms
step:349/2110 train_time:11932ms step_avg:34.19ms
step:350/2110 train_time:11965ms step_avg:34.19ms
step:351/2110 train_time:11998ms step_avg:34.18ms
step:352/2110 train_time:12031ms step_avg:34.18ms
step:353/2110 train_time:12064ms step_avg:34.18ms
step:354/2110 train_time:12097ms step_avg:34.17ms
step:355/2110 train_time:12130ms step_avg:34.17ms
step:356/2110 train_time:12164ms step_avg:34.17ms
step:357/2110 train_time:12196ms step_avg:34.16ms
step:358/2110 train_time:12229ms step_avg:34.16ms
step:359/2110 train_time:12262ms step_avg:34.16ms
step:360/2110 train_time:12295ms step_avg:34.15ms
step:361/2110 train_time:12328ms step_avg:34.15ms
step:362/2110 train_time:12361ms step_avg:34.15ms
step:363/2110 train_time:12394ms step_avg:34.14ms
step:364/2110 train_time:12426ms step_avg:34.14ms
step:365/2110 train_time:12460ms step_avg:34.14ms
step:366/2110 train_time:12492ms step_avg:34.13ms
step:367/2110 train_time:12525ms step_avg:34.13ms
step:368/2110 train_time:12558ms step_avg:34.12ms
step:369/2110 train_time:12591ms step_avg:34.12ms
step:370/2110 train_time:12624ms step_avg:34.12ms
step:371/2110 train_time:12657ms step_avg:34.12ms
step:372/2110 train_time:12690ms step_avg:34.11ms
step:373/2110 train_time:12723ms step_avg:34.11ms
step:374/2110 train_time:12755ms step_avg:34.10ms
step:375/2110 train_time:12789ms step_avg:34.10ms
step:376/2110 train_time:12821ms step_avg:34.10ms
step:377/2110 train_time:12855ms step_avg:34.10ms
step:378/2110 train_time:12888ms step_avg:34.09ms
step:379/2110 train_time:12921ms step_avg:34.09ms
step:380/2110 train_time:12953ms step_avg:34.09ms
step:381/2110 train_time:12987ms step_avg:34.09ms
step:382/2110 train_time:13019ms step_avg:34.08ms
step:383/2110 train_time:13053ms step_avg:34.08ms
step:384/2110 train_time:13086ms step_avg:34.08ms
step:385/2110 train_time:13119ms step_avg:34.08ms
step:386/2110 train_time:13153ms step_avg:34.07ms
step:387/2110 train_time:13185ms step_avg:34.07ms
step:388/2110 train_time:13218ms step_avg:34.07ms
step:389/2110 train_time:13251ms step_avg:34.06ms
step:390/2110 train_time:13284ms step_avg:34.06ms
step:391/2110 train_time:13317ms step_avg:34.06ms
step:392/2110 train_time:13349ms step_avg:34.05ms
step:393/2110 train_time:13383ms step_avg:34.05ms
step:394/2110 train_time:13415ms step_avg:34.05ms
step:395/2110 train_time:13448ms step_avg:34.05ms
step:396/2110 train_time:13481ms step_avg:34.04ms
step:397/2110 train_time:13514ms step_avg:34.04ms
step:398/2110 train_time:13547ms step_avg:34.04ms
step:399/2110 train_time:13580ms step_avg:34.04ms
step:400/2110 train_time:13613ms step_avg:34.03ms
step:401/2110 train_time:13646ms step_avg:34.03ms
step:402/2110 train_time:13678ms step_avg:34.03ms
step:403/2110 train_time:13712ms step_avg:34.02ms
step:404/2110 train_time:13745ms step_avg:34.02ms
step:405/2110 train_time:13778ms step_avg:34.02ms
step:406/2110 train_time:13811ms step_avg:34.02ms
step:407/2110 train_time:13844ms step_avg:34.02ms
step:408/2110 train_time:13877ms step_avg:34.01ms
step:409/2110 train_time:13910ms step_avg:34.01ms
step:410/2110 train_time:13942ms step_avg:34.01ms
step:411/2110 train_time:13976ms step_avg:34.00ms
step:412/2110 train_time:14009ms step_avg:34.00ms
step:413/2110 train_time:14042ms step_avg:34.00ms
step:414/2110 train_time:14075ms step_avg:34.00ms
step:415/2110 train_time:14108ms step_avg:33.99ms
step:416/2110 train_time:14140ms step_avg:33.99ms
step:417/2110 train_time:14174ms step_avg:33.99ms
step:418/2110 train_time:14208ms step_avg:33.99ms
step:419/2110 train_time:14240ms step_avg:33.99ms
step:420/2110 train_time:14273ms step_avg:33.98ms
step:421/2110 train_time:14307ms step_avg:33.98ms
step:422/2110 train_time:14339ms step_avg:33.98ms
step:423/2110 train_time:14373ms step_avg:33.98ms
step:424/2110 train_time:14405ms step_avg:33.98ms
step:425/2110 train_time:14439ms step_avg:33.97ms
step:426/2110 train_time:14471ms step_avg:33.97ms
step:427/2110 train_time:14504ms step_avg:33.97ms
step:428/2110 train_time:14537ms step_avg:33.96ms
step:429/2110 train_time:14570ms step_avg:33.96ms
step:430/2110 train_time:14603ms step_avg:33.96ms
step:431/2110 train_time:14636ms step_avg:33.96ms
step:432/2110 train_time:14669ms step_avg:33.96ms
step:433/2110 train_time:14702ms step_avg:33.95ms
step:434/2110 train_time:14735ms step_avg:33.95ms
step:435/2110 train_time:14768ms step_avg:33.95ms
step:436/2110 train_time:14801ms step_avg:33.95ms
step:437/2110 train_time:14835ms step_avg:33.95ms
step:438/2110 train_time:14868ms step_avg:33.95ms
step:439/2110 train_time:14901ms step_avg:33.94ms
step:440/2110 train_time:14934ms step_avg:33.94ms
step:441/2110 train_time:14967ms step_avg:33.94ms
step:442/2110 train_time:15000ms step_avg:33.94ms
step:443/2110 train_time:15032ms step_avg:33.93ms
step:444/2110 train_time:15066ms step_avg:33.93ms
step:445/2110 train_time:15098ms step_avg:33.93ms
step:446/2110 train_time:15133ms step_avg:33.93ms
step:447/2110 train_time:15164ms step_avg:33.92ms
step:448/2110 train_time:15199ms step_avg:33.93ms
step:449/2110 train_time:15232ms step_avg:33.92ms
step:450/2110 train_time:15264ms step_avg:33.92ms
step:451/2110 train_time:15296ms step_avg:33.92ms
step:452/2110 train_time:15330ms step_avg:33.92ms
step:453/2110 train_time:15362ms step_avg:33.91ms
step:454/2110 train_time:15395ms step_avg:33.91ms
step:455/2110 train_time:15428ms step_avg:33.91ms
step:456/2110 train_time:15461ms step_avg:33.91ms
step:457/2110 train_time:15494ms step_avg:33.90ms
step:458/2110 train_time:15531ms step_avg:33.91ms
step:459/2110 train_time:15571ms step_avg:33.92ms
step:460/2110 train_time:15613ms step_avg:33.94ms
step:461/2110 train_time:15648ms step_avg:33.94ms
step:462/2110 train_time:15687ms step_avg:33.95ms
step:463/2110 train_time:15720ms step_avg:33.95ms
step:464/2110 train_time:15753ms step_avg:33.95ms
step:465/2110 train_time:15785ms step_avg:33.95ms
step:466/2110 train_time:15818ms step_avg:33.94ms
step:467/2110 train_time:15849ms step_avg:33.94ms
step:468/2110 train_time:15885ms step_avg:33.94ms
step:469/2110 train_time:15917ms step_avg:33.94ms
step:470/2110 train_time:15950ms step_avg:33.94ms
step:471/2110 train_time:15982ms step_avg:33.93ms
step:472/2110 train_time:16015ms step_avg:33.93ms
step:473/2110 train_time:16046ms step_avg:33.92ms
step:474/2110 train_time:16080ms step_avg:33.92ms
step:475/2110 train_time:16111ms step_avg:33.92ms
step:476/2110 train_time:16146ms step_avg:33.92ms
step:477/2110 train_time:16176ms step_avg:33.91ms
step:478/2110 train_time:16212ms step_avg:33.92ms
step:479/2110 train_time:16243ms step_avg:33.91ms
step:480/2110 train_time:16278ms step_avg:33.91ms
step:481/2110 train_time:16311ms step_avg:33.91ms
step:482/2110 train_time:16346ms step_avg:33.91ms
step:483/2110 train_time:16377ms step_avg:33.91ms
step:484/2110 train_time:16411ms step_avg:33.91ms
step:485/2110 train_time:16444ms step_avg:33.90ms
step:486/2110 train_time:16477ms step_avg:33.90ms
step:487/2110 train_time:16509ms step_avg:33.90ms
step:488/2110 train_time:16543ms step_avg:33.90ms
step:489/2110 train_time:16575ms step_avg:33.89ms
step:490/2110 train_time:16609ms step_avg:33.90ms
step:491/2110 train_time:16641ms step_avg:33.89ms
step:492/2110 train_time:16676ms step_avg:33.89ms
step:493/2110 train_time:16707ms step_avg:33.89ms
step:494/2110 train_time:16741ms step_avg:33.89ms
step:495/2110 train_time:16773ms step_avg:33.88ms
step:496/2110 train_time:16807ms step_avg:33.89ms
step:497/2110 train_time:16838ms step_avg:33.88ms
step:498/2110 train_time:16873ms step_avg:33.88ms
step:499/2110 train_time:16905ms step_avg:33.88ms
step:500/2110 train_time:16938ms step_avg:33.88ms
step:500/2110 val_loss:4.0342 train_time:16948ms step_avg:33.90ms
step:501/2110 train_time:16973ms step_avg:33.88ms
step:502/2110 train_time:17007ms step_avg:33.88ms
step:503/2110 train_time:17036ms step_avg:33.87ms
step:504/2110 train_time:17069ms step_avg:33.87ms
step:505/2110 train_time:17099ms step_avg:33.86ms
step:506/2110 train_time:17126ms step_avg:33.85ms
step:507/2110 train_time:17156ms step_avg:33.84ms
step:508/2110 train_time:17187ms step_avg:33.83ms
step:509/2110 train_time:17221ms step_avg:33.83ms
step:510/2110 train_time:17254ms step_avg:33.83ms
step:511/2110 train_time:17287ms step_avg:33.83ms
step:512/2110 train_time:17320ms step_avg:33.83ms
step:513/2110 train_time:17353ms step_avg:33.83ms
step:514/2110 train_time:17385ms step_avg:33.82ms
step:515/2110 train_time:17419ms step_avg:33.82ms
step:516/2110 train_time:17451ms step_avg:33.82ms
step:517/2110 train_time:17484ms step_avg:33.82ms
step:518/2110 train_time:17517ms step_avg:33.82ms
step:519/2110 train_time:17550ms step_avg:33.81ms
step:520/2110 train_time:17582ms step_avg:33.81ms
step:521/2110 train_time:17615ms step_avg:33.81ms
step:522/2110 train_time:17648ms step_avg:33.81ms
step:523/2110 train_time:17681ms step_avg:33.81ms
step:524/2110 train_time:17713ms step_avg:33.80ms
step:525/2110 train_time:17746ms step_avg:33.80ms
step:526/2110 train_time:17779ms step_avg:33.80ms
step:527/2110 train_time:17812ms step_avg:33.80ms
step:528/2110 train_time:17844ms step_avg:33.80ms
step:529/2110 train_time:17878ms step_avg:33.80ms
step:530/2110 train_time:17910ms step_avg:33.79ms
step:531/2110 train_time:17944ms step_avg:33.79ms
step:532/2110 train_time:17977ms step_avg:33.79ms
step:533/2110 train_time:18011ms step_avg:33.79ms
step:534/2110 train_time:18044ms step_avg:33.79ms
step:535/2110 train_time:18078ms step_avg:33.79ms
step:536/2110 train_time:18111ms step_avg:33.79ms
step:537/2110 train_time:18145ms step_avg:33.79ms
step:538/2110 train_time:18178ms step_avg:33.79ms
step:539/2110 train_time:18211ms step_avg:33.79ms
step:540/2110 train_time:18244ms step_avg:33.78ms
step:541/2110 train_time:18277ms step_avg:33.78ms
step:542/2110 train_time:18310ms step_avg:33.78ms
step:543/2110 train_time:18343ms step_avg:33.78ms
step:544/2110 train_time:18376ms step_avg:33.78ms
step:545/2110 train_time:18410ms step_avg:33.78ms
step:546/2110 train_time:18443ms step_avg:33.78ms
step:547/2110 train_time:18476ms step_avg:33.78ms
step:548/2110 train_time:18509ms step_avg:33.77ms
step:549/2110 train_time:18541ms step_avg:33.77ms
step:550/2110 train_time:18574ms step_avg:33.77ms
step:551/2110 train_time:18607ms step_avg:33.77ms
step:552/2110 train_time:18640ms step_avg:33.77ms
step:553/2110 train_time:18673ms step_avg:33.77ms
step:554/2110 train_time:18706ms step_avg:33.77ms
step:555/2110 train_time:18739ms step_avg:33.76ms
step:556/2110 train_time:18772ms step_avg:33.76ms
step:557/2110 train_time:18805ms step_avg:33.76ms
step:558/2110 train_time:18837ms step_avg:33.76ms
step:559/2110 train_time:18871ms step_avg:33.76ms
step:560/2110 train_time:18903ms step_avg:33.76ms
step:561/2110 train_time:18937ms step_avg:33.76ms
step:562/2110 train_time:18970ms step_avg:33.75ms
step:563/2110 train_time:19003ms step_avg:33.75ms
step:564/2110 train_time:19036ms step_avg:33.75ms
step:565/2110 train_time:19069ms step_avg:33.75ms
step:566/2110 train_time:19102ms step_avg:33.75ms
step:567/2110 train_time:19135ms step_avg:33.75ms
step:568/2110 train_time:19169ms step_avg:33.75ms
step:569/2110 train_time:19202ms step_avg:33.75ms
step:570/2110 train_time:19235ms step_avg:33.75ms
step:571/2110 train_time:19269ms step_avg:33.75ms
step:572/2110 train_time:19302ms step_avg:33.74ms
step:573/2110 train_time:19335ms step_avg:33.74ms
step:574/2110 train_time:19368ms step_avg:33.74ms
step:575/2110 train_time:19401ms step_avg:33.74ms
step:576/2110 train_time:19434ms step_avg:33.74ms
step:577/2110 train_time:19467ms step_avg:33.74ms
step:578/2110 train_time:19499ms step_avg:33.74ms
step:579/2110 train_time:19532ms step_avg:33.73ms
step:580/2110 train_time:19565ms step_avg:33.73ms
step:581/2110 train_time:19598ms step_avg:33.73ms
step:582/2110 train_time:19631ms step_avg:33.73ms
step:583/2110 train_time:19664ms step_avg:33.73ms
step:584/2110 train_time:19697ms step_avg:33.73ms
step:585/2110 train_time:19731ms step_avg:33.73ms
step:586/2110 train_time:19763ms step_avg:33.73ms
step:587/2110 train_time:19796ms step_avg:33.72ms
step:588/2110 train_time:19829ms step_avg:33.72ms
step:589/2110 train_time:19862ms step_avg:33.72ms
step:590/2110 train_time:19895ms step_avg:33.72ms
step:591/2110 train_time:19928ms step_avg:33.72ms
step:592/2110 train_time:19960ms step_avg:33.72ms
step:593/2110 train_time:19994ms step_avg:33.72ms
step:594/2110 train_time:20027ms step_avg:33.71ms
step:595/2110 train_time:20060ms step_avg:33.71ms
step:596/2110 train_time:20092ms step_avg:33.71ms
step:597/2110 train_time:20126ms step_avg:33.71ms
step:598/2110 train_time:20158ms step_avg:33.71ms
step:599/2110 train_time:20192ms step_avg:33.71ms
step:600/2110 train_time:20225ms step_avg:33.71ms
step:601/2110 train_time:20258ms step_avg:33.71ms
step:602/2110 train_time:20291ms step_avg:33.71ms
step:603/2110 train_time:20324ms step_avg:33.71ms
step:604/2110 train_time:20357ms step_avg:33.70ms
step:605/2110 train_time:20391ms step_avg:33.70ms
step:606/2110 train_time:20424ms step_avg:33.70ms
step:607/2110 train_time:20457ms step_avg:33.70ms
step:608/2110 train_time:20489ms step_avg:33.70ms
step:609/2110 train_time:20523ms step_avg:33.70ms
step:610/2110 train_time:20555ms step_avg:33.70ms
step:611/2110 train_time:20588ms step_avg:33.70ms
step:612/2110 train_time:20621ms step_avg:33.69ms
step:613/2110 train_time:20654ms step_avg:33.69ms
step:614/2110 train_time:20687ms step_avg:33.69ms
step:615/2110 train_time:20720ms step_avg:33.69ms
step:616/2110 train_time:20752ms step_avg:33.69ms
step:617/2110 train_time:20785ms step_avg:33.69ms
step:618/2110 train_time:20818ms step_avg:33.69ms
step:619/2110 train_time:20851ms step_avg:33.69ms
step:620/2110 train_time:20884ms step_avg:33.68ms
step:621/2110 train_time:20917ms step_avg:33.68ms
step:622/2110 train_time:20950ms step_avg:33.68ms
step:623/2110 train_time:20983ms step_avg:33.68ms
step:624/2110 train_time:21016ms step_avg:33.68ms
step:625/2110 train_time:21049ms step_avg:33.68ms
step:626/2110 train_time:21082ms step_avg:33.68ms
step:627/2110 train_time:21116ms step_avg:33.68ms
step:628/2110 train_time:21152ms step_avg:33.68ms
step:629/2110 train_time:21191ms step_avg:33.69ms
step:630/2110 train_time:21226ms step_avg:33.69ms
step:631/2110 train_time:21262ms step_avg:33.70ms
step:632/2110 train_time:21296ms step_avg:33.70ms
step:633/2110 train_time:21335ms step_avg:33.70ms
step:634/2110 train_time:21369ms step_avg:33.71ms
step:635/2110 train_time:21404ms step_avg:33.71ms
step:636/2110 train_time:21439ms step_avg:33.71ms
step:637/2110 train_time:21475ms step_avg:33.71ms
step:638/2110 train_time:21509ms step_avg:33.71ms
step:639/2110 train_time:21544ms step_avg:33.71ms
step:640/2110 train_time:21576ms step_avg:33.71ms
step:641/2110 train_time:21610ms step_avg:33.71ms
step:642/2110 train_time:21642ms step_avg:33.71ms
step:643/2110 train_time:21678ms step_avg:33.71ms
step:644/2110 train_time:21711ms step_avg:33.71ms
step:645/2110 train_time:21743ms step_avg:33.71ms
step:646/2110 train_time:21775ms step_avg:33.71ms
step:647/2110 train_time:21808ms step_avg:33.71ms
step:648/2110 train_time:21841ms step_avg:33.70ms
step:649/2110 train_time:21874ms step_avg:33.70ms
step:650/2110 train_time:21907ms step_avg:33.70ms
step:651/2110 train_time:21940ms step_avg:33.70ms
step:652/2110 train_time:21973ms step_avg:33.70ms
step:653/2110 train_time:22006ms step_avg:33.70ms
step:654/2110 train_time:22038ms step_avg:33.70ms
step:655/2110 train_time:22072ms step_avg:33.70ms
step:656/2110 train_time:22105ms step_avg:33.70ms
step:657/2110 train_time:22138ms step_avg:33.70ms
step:658/2110 train_time:22172ms step_avg:33.70ms
step:659/2110 train_time:22205ms step_avg:33.70ms
step:660/2110 train_time:22238ms step_avg:33.69ms
step:661/2110 train_time:22272ms step_avg:33.69ms
step:662/2110 train_time:22305ms step_avg:33.69ms
step:663/2110 train_time:22339ms step_avg:33.69ms
step:664/2110 train_time:22372ms step_avg:33.69ms
step:665/2110 train_time:22405ms step_avg:33.69ms
step:666/2110 train_time:22438ms step_avg:33.69ms
step:667/2110 train_time:22472ms step_avg:33.69ms
step:668/2110 train_time:22504ms step_avg:33.69ms
step:669/2110 train_time:22537ms step_avg:33.69ms
step:670/2110 train_time:22570ms step_avg:33.69ms
step:671/2110 train_time:22604ms step_avg:33.69ms
step:672/2110 train_time:22636ms step_avg:33.69ms
step:673/2110 train_time:22670ms step_avg:33.68ms
step:674/2110 train_time:22703ms step_avg:33.68ms
step:675/2110 train_time:22736ms step_avg:33.68ms
step:676/2110 train_time:22768ms step_avg:33.68ms
step:677/2110 train_time:22802ms step_avg:33.68ms
step:678/2110 train_time:22834ms step_avg:33.68ms
step:679/2110 train_time:22867ms step_avg:33.68ms
step:680/2110 train_time:22900ms step_avg:33.68ms
step:681/2110 train_time:22933ms step_avg:33.68ms
step:682/2110 train_time:22965ms step_avg:33.67ms
step:683/2110 train_time:22998ms step_avg:33.67ms
step:684/2110 train_time:23031ms step_avg:33.67ms
step:685/2110 train_time:23064ms step_avg:33.67ms
step:686/2110 train_time:23097ms step_avg:33.67ms
step:687/2110 train_time:23130ms step_avg:33.67ms
step:688/2110 train_time:23163ms step_avg:33.67ms
step:689/2110 train_time:23197ms step_avg:33.67ms
step:690/2110 train_time:23229ms step_avg:33.67ms
step:691/2110 train_time:23264ms step_avg:33.67ms
step:692/2110 train_time:23322ms step_avg:33.70ms
step:693/2110 train_time:23382ms step_avg:33.74ms
step:694/2110 train_time:23440ms step_avg:33.78ms
step:695/2110 train_time:23501ms step_avg:33.81ms
step:696/2110 train_time:23559ms step_avg:33.85ms
step:697/2110 train_time:23619ms step_avg:33.89ms
step:698/2110 train_time:23677ms step_avg:33.92ms
step:699/2110 train_time:23738ms step_avg:33.96ms
step:700/2110 train_time:23797ms step_avg:34.00ms
step:701/2110 train_time:23856ms step_avg:34.03ms
step:702/2110 train_time:23915ms step_avg:34.07ms
step:703/2110 train_time:23974ms step_avg:34.10ms
step:704/2110 train_time:24033ms step_avg:34.14ms
step:705/2110 train_time:24093ms step_avg:34.17ms
step:706/2110 train_time:24151ms step_avg:34.21ms
step:707/2110 train_time:24211ms step_avg:34.24ms
step:708/2110 train_time:24269ms step_avg:34.28ms
step:709/2110 train_time:24330ms step_avg:34.32ms
step:710/2110 train_time:24390ms step_avg:34.35ms
step:711/2110 train_time:24451ms step_avg:34.39ms
step:712/2110 train_time:24510ms step_avg:34.42ms
step:713/2110 train_time:24570ms step_avg:34.46ms
step:714/2110 train_time:24628ms step_avg:34.49ms
step:715/2110 train_time:24688ms step_avg:34.53ms
step:716/2110 train_time:24746ms step_avg:34.56ms
step:717/2110 train_time:24806ms step_avg:34.60ms
step:718/2110 train_time:24864ms step_avg:34.63ms
step:719/2110 train_time:24925ms step_avg:34.67ms
step:720/2110 train_time:24983ms step_avg:34.70ms
step:721/2110 train_time:25044ms step_avg:34.73ms
step:722/2110 train_time:25102ms step_avg:34.77ms
step:723/2110 train_time:25162ms step_avg:34.80ms
step:724/2110 train_time:25225ms step_avg:34.84ms
step:725/2110 train_time:25280ms step_avg:34.87ms
step:726/2110 train_time:25339ms step_avg:34.90ms
step:727/2110 train_time:25400ms step_avg:34.94ms
step:728/2110 train_time:25458ms step_avg:34.97ms
step:729/2110 train_time:25519ms step_avg:35.01ms
step:730/2110 train_time:25578ms step_avg:35.04ms
step:731/2110 train_time:25637ms step_avg:35.07ms
step:732/2110 train_time:25697ms step_avg:35.11ms
step:733/2110 train_time:25757ms step_avg:35.14ms
step:734/2110 train_time:25817ms step_avg:35.17ms
step:735/2110 train_time:25876ms step_avg:35.21ms
step:736/2110 train_time:25936ms step_avg:35.24ms
step:737/2110 train_time:25996ms step_avg:35.27ms
step:738/2110 train_time:26054ms step_avg:35.30ms
step:739/2110 train_time:26114ms step_avg:35.34ms
step:740/2110 train_time:26173ms step_avg:35.37ms
step:741/2110 train_time:26232ms step_avg:35.40ms
step:742/2110 train_time:26291ms step_avg:35.43ms
step:743/2110 train_time:26351ms step_avg:35.47ms
step:744/2110 train_time:26409ms step_avg:35.50ms
step:745/2110 train_time:26469ms step_avg:35.53ms
step:746/2110 train_time:26527ms step_avg:35.56ms
step:747/2110 train_time:26587ms step_avg:35.59ms
step:748/2110 train_time:26645ms step_avg:35.62ms
step:749/2110 train_time:26705ms step_avg:35.65ms
step:750/2110 train_time:26765ms step_avg:35.69ms
step:750/2110 val_loss:3.9101 train_time:26825ms step_avg:35.77ms
step:751/2110 train_time:26850ms step_avg:35.75ms
step:752/2110 train_time:26887ms step_avg:35.75ms
step:753/2110 train_time:26948ms step_avg:35.79ms
step:754/2110 train_time:27010ms step_avg:35.82ms
step:755/2110 train_time:27070ms step_avg:35.85ms
step:756/2110 train_time:27128ms step_avg:35.88ms
step:757/2110 train_time:27187ms step_avg:35.91ms
step:758/2110 train_time:27246ms step_avg:35.94ms
step:759/2110 train_time:27304ms step_avg:35.97ms
step:760/2110 train_time:27362ms step_avg:36.00ms
step:761/2110 train_time:27421ms step_avg:36.03ms
step:762/2110 train_time:27479ms step_avg:36.06ms
step:763/2110 train_time:27537ms step_avg:36.09ms
step:764/2110 train_time:27595ms step_avg:36.12ms
step:765/2110 train_time:27654ms step_avg:36.15ms
step:766/2110 train_time:27713ms step_avg:36.18ms
step:767/2110 train_time:27773ms step_avg:36.21ms
step:768/2110 train_time:27833ms step_avg:36.24ms
step:769/2110 train_time:27894ms step_avg:36.27ms
step:770/2110 train_time:27955ms step_avg:36.30ms
step:771/2110 train_time:28015ms step_avg:36.34ms
step:772/2110 train_time:28075ms step_avg:36.37ms
step:773/2110 train_time:28136ms step_avg:36.40ms
step:774/2110 train_time:28194ms step_avg:36.43ms
step:775/2110 train_time:28254ms step_avg:36.46ms
step:776/2110 train_time:28313ms step_avg:36.49ms
step:777/2110 train_time:28372ms step_avg:36.51ms
step:778/2110 train_time:28430ms step_avg:36.54ms
step:779/2110 train_time:28489ms step_avg:36.57ms
step:780/2110 train_time:28547ms step_avg:36.60ms
step:781/2110 train_time:28607ms step_avg:36.63ms
step:782/2110 train_time:28665ms step_avg:36.66ms
step:783/2110 train_time:28724ms step_avg:36.68ms
step:784/2110 train_time:28783ms step_avg:36.71ms
step:785/2110 train_time:28843ms step_avg:36.74ms
step:786/2110 train_time:28902ms step_avg:36.77ms
step:787/2110 train_time:28962ms step_avg:36.80ms
step:788/2110 train_time:29021ms step_avg:36.83ms
step:789/2110 train_time:29081ms step_avg:36.86ms
step:790/2110 train_time:29139ms step_avg:36.89ms
step:791/2110 train_time:29199ms step_avg:36.91ms
step:792/2110 train_time:29258ms step_avg:36.94ms
step:793/2110 train_time:29318ms step_avg:36.97ms
step:794/2110 train_time:29376ms step_avg:37.00ms
step:795/2110 train_time:29436ms step_avg:37.03ms
step:796/2110 train_time:29495ms step_avg:37.05ms
step:797/2110 train_time:29554ms step_avg:37.08ms
step:798/2110 train_time:29613ms step_avg:37.11ms
step:799/2110 train_time:29672ms step_avg:37.14ms
step:800/2110 train_time:29731ms step_avg:37.16ms
step:801/2110 train_time:29791ms step_avg:37.19ms
step:802/2110 train_time:29852ms step_avg:37.22ms
step:803/2110 train_time:29912ms step_avg:37.25ms
step:804/2110 train_time:29972ms step_avg:37.28ms
step:805/2110 train_time:30031ms step_avg:37.31ms
step:806/2110 train_time:30090ms step_avg:37.33ms
step:807/2110 train_time:30151ms step_avg:37.36ms
step:808/2110 train_time:30210ms step_avg:37.39ms
step:809/2110 train_time:30270ms step_avg:37.42ms
step:810/2110 train_time:30328ms step_avg:37.44ms
step:811/2110 train_time:30388ms step_avg:37.47ms
step:812/2110 train_time:30447ms step_avg:37.50ms
step:813/2110 train_time:30506ms step_avg:37.52ms
step:814/2110 train_time:30564ms step_avg:37.55ms
step:815/2110 train_time:30623ms step_avg:37.57ms
step:816/2110 train_time:30681ms step_avg:37.60ms
step:817/2110 train_time:30741ms step_avg:37.63ms
step:818/2110 train_time:30800ms step_avg:37.65ms
step:819/2110 train_time:30860ms step_avg:37.68ms
step:820/2110 train_time:30919ms step_avg:37.71ms
step:821/2110 train_time:30980ms step_avg:37.73ms
step:822/2110 train_time:31038ms step_avg:37.76ms
step:823/2110 train_time:31099ms step_avg:37.79ms
step:824/2110 train_time:31157ms step_avg:37.81ms
step:825/2110 train_time:31218ms step_avg:37.84ms
step:826/2110 train_time:31277ms step_avg:37.87ms
step:827/2110 train_time:31336ms step_avg:37.89ms
step:828/2110 train_time:31395ms step_avg:37.92ms
step:829/2110 train_time:31455ms step_avg:37.94ms
step:830/2110 train_time:31514ms step_avg:37.97ms
step:831/2110 train_time:31574ms step_avg:37.99ms
step:832/2110 train_time:31632ms step_avg:38.02ms
step:833/2110 train_time:31692ms step_avg:38.05ms
step:834/2110 train_time:31753ms step_avg:38.07ms
step:835/2110 train_time:31812ms step_avg:38.10ms
step:836/2110 train_time:31870ms step_avg:38.12ms
step:837/2110 train_time:31930ms step_avg:38.15ms
step:838/2110 train_time:31989ms step_avg:38.17ms
step:839/2110 train_time:32049ms step_avg:38.20ms
step:840/2110 train_time:32109ms step_avg:38.23ms
step:841/2110 train_time:32168ms step_avg:38.25ms
step:842/2110 train_time:32228ms step_avg:38.27ms
step:843/2110 train_time:32286ms step_avg:38.30ms
step:844/2110 train_time:32345ms step_avg:38.32ms
step:845/2110 train_time:32404ms step_avg:38.35ms
step:846/2110 train_time:32463ms step_avg:38.37ms
step:847/2110 train_time:32522ms step_avg:38.40ms
step:848/2110 train_time:32579ms step_avg:38.42ms
step:849/2110 train_time:32640ms step_avg:38.44ms
step:850/2110 train_time:32698ms step_avg:38.47ms
step:851/2110 train_time:32758ms step_avg:38.49ms
step:852/2110 train_time:32817ms step_avg:38.52ms
step:853/2110 train_time:32877ms step_avg:38.54ms
step:854/2110 train_time:32935ms step_avg:38.57ms
step:855/2110 train_time:32996ms step_avg:38.59ms
step:856/2110 train_time:33055ms step_avg:38.62ms
step:857/2110 train_time:33114ms step_avg:38.64ms
step:858/2110 train_time:33173ms step_avg:38.66ms
step:859/2110 train_time:33233ms step_avg:38.69ms
step:860/2110 train_time:33291ms step_avg:38.71ms
step:861/2110 train_time:33353ms step_avg:38.74ms
step:862/2110 train_time:33412ms step_avg:38.76ms
step:863/2110 train_time:33472ms step_avg:38.79ms
step:864/2110 train_time:33531ms step_avg:38.81ms
step:865/2110 train_time:33590ms step_avg:38.83ms
step:866/2110 train_time:33650ms step_avg:38.86ms
step:867/2110 train_time:33710ms step_avg:38.88ms
step:868/2110 train_time:33768ms step_avg:38.90ms
step:869/2110 train_time:33828ms step_avg:38.93ms
step:870/2110 train_time:33887ms step_avg:38.95ms
step:871/2110 train_time:33946ms step_avg:38.97ms
step:872/2110 train_time:34005ms step_avg:39.00ms
step:873/2110 train_time:34065ms step_avg:39.02ms
step:874/2110 train_time:34122ms step_avg:39.04ms
step:875/2110 train_time:34183ms step_avg:39.07ms
step:876/2110 train_time:34241ms step_avg:39.09ms
step:877/2110 train_time:34301ms step_avg:39.11ms
step:878/2110 train_time:34360ms step_avg:39.13ms
step:879/2110 train_time:34420ms step_avg:39.16ms
step:880/2110 train_time:34478ms step_avg:39.18ms
step:881/2110 train_time:34538ms step_avg:39.20ms
step:882/2110 train_time:34596ms step_avg:39.22ms
step:883/2110 train_time:34657ms step_avg:39.25ms
step:884/2110 train_time:34716ms step_avg:39.27ms
step:885/2110 train_time:34775ms step_avg:39.29ms
step:886/2110 train_time:34834ms step_avg:39.32ms
step:887/2110 train_time:34894ms step_avg:39.34ms
step:888/2110 train_time:34954ms step_avg:39.36ms
step:889/2110 train_time:35013ms step_avg:39.38ms
step:890/2110 train_time:35072ms step_avg:39.41ms
step:891/2110 train_time:35133ms step_avg:39.43ms
step:892/2110 train_time:35192ms step_avg:39.45ms
step:893/2110 train_time:35253ms step_avg:39.48ms
step:894/2110 train_time:35311ms step_avg:39.50ms
step:895/2110 train_time:35371ms step_avg:39.52ms
step:896/2110 train_time:35430ms step_avg:39.54ms
step:897/2110 train_time:35489ms step_avg:39.56ms
step:898/2110 train_time:35548ms step_avg:39.59ms
step:899/2110 train_time:35609ms step_avg:39.61ms
step:900/2110 train_time:35667ms step_avg:39.63ms
step:901/2110 train_time:35726ms step_avg:39.65ms
step:902/2110 train_time:35785ms step_avg:39.67ms
step:903/2110 train_time:35845ms step_avg:39.70ms
step:904/2110 train_time:35903ms step_avg:39.72ms
step:905/2110 train_time:35964ms step_avg:39.74ms
step:906/2110 train_time:36023ms step_avg:39.76ms
step:907/2110 train_time:36083ms step_avg:39.78ms
step:908/2110 train_time:36142ms step_avg:39.80ms
step:909/2110 train_time:36202ms step_avg:39.83ms
step:910/2110 train_time:36261ms step_avg:39.85ms
step:911/2110 train_time:36321ms step_avg:39.87ms
step:912/2110 train_time:36379ms step_avg:39.89ms
step:913/2110 train_time:36440ms step_avg:39.91ms
step:914/2110 train_time:36499ms step_avg:39.93ms
step:915/2110 train_time:36559ms step_avg:39.96ms
step:916/2110 train_time:36617ms step_avg:39.98ms
step:917/2110 train_time:36678ms step_avg:40.00ms
step:918/2110 train_time:36736ms step_avg:40.02ms
step:919/2110 train_time:36797ms step_avg:40.04ms
step:920/2110 train_time:36856ms step_avg:40.06ms
step:921/2110 train_time:36916ms step_avg:40.08ms
step:922/2110 train_time:36975ms step_avg:40.10ms
step:923/2110 train_time:37035ms step_avg:40.12ms
step:924/2110 train_time:37094ms step_avg:40.14ms
step:925/2110 train_time:37154ms step_avg:40.17ms
step:926/2110 train_time:37213ms step_avg:40.19ms
step:927/2110 train_time:37273ms step_avg:40.21ms
step:928/2110 train_time:37332ms step_avg:40.23ms
step:929/2110 train_time:37392ms step_avg:40.25ms
step:930/2110 train_time:37451ms step_avg:40.27ms
step:931/2110 train_time:37511ms step_avg:40.29ms
step:932/2110 train_time:37570ms step_avg:40.31ms
step:933/2110 train_time:37629ms step_avg:40.33ms
step:934/2110 train_time:37687ms step_avg:40.35ms
step:935/2110 train_time:37747ms step_avg:40.37ms
step:936/2110 train_time:37807ms step_avg:40.39ms
step:937/2110 train_time:37867ms step_avg:40.41ms
step:938/2110 train_time:37926ms step_avg:40.43ms
step:939/2110 train_time:37985ms step_avg:40.45ms
step:940/2110 train_time:38044ms step_avg:40.47ms
step:941/2110 train_time:38104ms step_avg:40.49ms
step:942/2110 train_time:38164ms step_avg:40.51ms
step:943/2110 train_time:38222ms step_avg:40.53ms
step:944/2110 train_time:38281ms step_avg:40.55ms
step:945/2110 train_time:38342ms step_avg:40.57ms
step:946/2110 train_time:38401ms step_avg:40.59ms
step:947/2110 train_time:38461ms step_avg:40.61ms
step:948/2110 train_time:38519ms step_avg:40.63ms
step:949/2110 train_time:38579ms step_avg:40.65ms
step:950/2110 train_time:38638ms step_avg:40.67ms
step:951/2110 train_time:38698ms step_avg:40.69ms
step:952/2110 train_time:38756ms step_avg:40.71ms
step:953/2110 train_time:38816ms step_avg:40.73ms
step:954/2110 train_time:38874ms step_avg:40.75ms
step:955/2110 train_time:38935ms step_avg:40.77ms
step:956/2110 train_time:38994ms step_avg:40.79ms
step:957/2110 train_time:39055ms step_avg:40.81ms
step:958/2110 train_time:39115ms step_avg:40.83ms
step:959/2110 train_time:39174ms step_avg:40.85ms
step:960/2110 train_time:39234ms step_avg:40.87ms
step:961/2110 train_time:39294ms step_avg:40.89ms
step:962/2110 train_time:39354ms step_avg:40.91ms
step:963/2110 train_time:39414ms step_avg:40.93ms
step:964/2110 train_time:39473ms step_avg:40.95ms
step:965/2110 train_time:39533ms step_avg:40.97ms
step:966/2110 train_time:39591ms step_avg:40.98ms
step:967/2110 train_time:39651ms step_avg:41.00ms
step:968/2110 train_time:39710ms step_avg:41.02ms
step:969/2110 train_time:39769ms step_avg:41.04ms
step:970/2110 train_time:39827ms step_avg:41.06ms
step:971/2110 train_time:39887ms step_avg:41.08ms
step:972/2110 train_time:39946ms step_avg:41.10ms
step:973/2110 train_time:40005ms step_avg:41.12ms
step:974/2110 train_time:40063ms step_avg:41.13ms
step:975/2110 train_time:40123ms step_avg:41.15ms
step:976/2110 train_time:40182ms step_avg:41.17ms
step:977/2110 train_time:40242ms step_avg:41.19ms
step:978/2110 train_time:40300ms step_avg:41.21ms
step:979/2110 train_time:40361ms step_avg:41.23ms
step:980/2110 train_time:40419ms step_avg:41.24ms
step:981/2110 train_time:40479ms step_avg:41.26ms
step:982/2110 train_time:40541ms step_avg:41.28ms
step:983/2110 train_time:40597ms step_avg:41.30ms
step:984/2110 train_time:40655ms step_avg:41.32ms
step:985/2110 train_time:40716ms step_avg:41.34ms
step:986/2110 train_time:40775ms step_avg:41.35ms
step:987/2110 train_time:40834ms step_avg:41.37ms
step:988/2110 train_time:40893ms step_avg:41.39ms
step:989/2110 train_time:40954ms step_avg:41.41ms
step:990/2110 train_time:41013ms step_avg:41.43ms
step:991/2110 train_time:41073ms step_avg:41.45ms
step:992/2110 train_time:41134ms step_avg:41.47ms
step:993/2110 train_time:41193ms step_avg:41.48ms
step:994/2110 train_time:41254ms step_avg:41.50ms
step:995/2110 train_time:41314ms step_avg:41.52ms
step:996/2110 train_time:41374ms step_avg:41.54ms
step:997/2110 train_time:41434ms step_avg:41.56ms
step:998/2110 train_time:41492ms step_avg:41.58ms
step:999/2110 train_time:41552ms step_avg:41.59ms
step:1000/2110 train_time:41612ms step_avg:41.61ms
step:1000/2110 val_loss:3.7586 train_time:41672ms step_avg:41.67ms
step:1001/2110 train_time:41700ms step_avg:41.66ms
step:1002/2110 train_time:41732ms step_avg:41.65ms
step:1003/2110 train_time:41795ms step_avg:41.67ms
step:1004/2110 train_time:41859ms step_avg:41.69ms
step:1005/2110 train_time:41919ms step_avg:41.71ms
step:1006/2110 train_time:41978ms step_avg:41.73ms
step:1007/2110 train_time:42037ms step_avg:41.74ms
step:1008/2110 train_time:42095ms step_avg:41.76ms
step:1009/2110 train_time:42154ms step_avg:41.78ms
step:1010/2110 train_time:42211ms step_avg:41.79ms
step:1011/2110 train_time:42271ms step_avg:41.81ms
step:1012/2110 train_time:42328ms step_avg:41.83ms
step:1013/2110 train_time:42387ms step_avg:41.84ms
step:1014/2110 train_time:42445ms step_avg:41.86ms
step:1015/2110 train_time:42504ms step_avg:41.88ms
step:1016/2110 train_time:42561ms step_avg:41.89ms
step:1017/2110 train_time:42621ms step_avg:41.91ms
step:1018/2110 train_time:42681ms step_avg:41.93ms
step:1019/2110 train_time:42742ms step_avg:41.95ms
step:1020/2110 train_time:42803ms step_avg:41.96ms
step:1021/2110 train_time:42865ms step_avg:41.98ms
step:1022/2110 train_time:42923ms step_avg:42.00ms
step:1023/2110 train_time:42984ms step_avg:42.02ms
step:1024/2110 train_time:43042ms step_avg:42.03ms
step:1025/2110 train_time:43103ms step_avg:42.05ms
step:1026/2110 train_time:43161ms step_avg:42.07ms
step:1027/2110 train_time:43221ms step_avg:42.08ms
step:1028/2110 train_time:43279ms step_avg:42.10ms
step:1029/2110 train_time:43339ms step_avg:42.12ms
step:1030/2110 train_time:43396ms step_avg:42.13ms
step:1031/2110 train_time:43455ms step_avg:42.15ms
step:1032/2110 train_time:43513ms step_avg:42.16ms
step:1033/2110 train_time:43572ms step_avg:42.18ms
step:1034/2110 train_time:43631ms step_avg:42.20ms
step:1035/2110 train_time:43690ms step_avg:42.21ms
step:1036/2110 train_time:43749ms step_avg:42.23ms
step:1037/2110 train_time:43811ms step_avg:42.25ms
step:1038/2110 train_time:43869ms step_avg:42.26ms
step:1039/2110 train_time:43931ms step_avg:42.28ms
step:1040/2110 train_time:43990ms step_avg:42.30ms
step:1041/2110 train_time:44050ms step_avg:42.32ms
step:1042/2110 train_time:44108ms step_avg:42.33ms
step:1043/2110 train_time:44168ms step_avg:42.35ms
step:1044/2110 train_time:44226ms step_avg:42.36ms
step:1045/2110 train_time:44286ms step_avg:42.38ms
step:1046/2110 train_time:44344ms step_avg:42.39ms
step:1047/2110 train_time:44404ms step_avg:42.41ms
step:1048/2110 train_time:44462ms step_avg:42.43ms
step:1049/2110 train_time:44521ms step_avg:42.44ms
step:1050/2110 train_time:44579ms step_avg:42.46ms
step:1051/2110 train_time:44639ms step_avg:42.47ms
step:1052/2110 train_time:44699ms step_avg:42.49ms
step:1053/2110 train_time:44759ms step_avg:42.51ms
step:1054/2110 train_time:44818ms step_avg:42.52ms
step:1055/2110 train_time:44879ms step_avg:42.54ms
step:1056/2110 train_time:44938ms step_avg:42.55ms
step:1057/2110 train_time:44999ms step_avg:42.57ms
step:1058/2110 train_time:45057ms step_avg:42.59ms
step:1059/2110 train_time:45117ms step_avg:42.60ms
step:1060/2110 train_time:45175ms step_avg:42.62ms
step:1061/2110 train_time:45235ms step_avg:42.63ms
step:1062/2110 train_time:45293ms step_avg:42.65ms
step:1063/2110 train_time:45353ms step_avg:42.66ms
step:1064/2110 train_time:45410ms step_avg:42.68ms
step:1065/2110 train_time:45470ms step_avg:42.69ms
step:1066/2110 train_time:45528ms step_avg:42.71ms
step:1067/2110 train_time:45588ms step_avg:42.73ms
step:1068/2110 train_time:45647ms step_avg:42.74ms
step:1069/2110 train_time:45708ms step_avg:42.76ms
step:1070/2110 train_time:45767ms step_avg:42.77ms
step:1071/2110 train_time:45827ms step_avg:42.79ms
step:1072/2110 train_time:45886ms step_avg:42.80ms
step:1073/2110 train_time:45947ms step_avg:42.82ms
step:1074/2110 train_time:46006ms step_avg:42.84ms
step:1075/2110 train_time:46067ms step_avg:42.85ms
step:1076/2110 train_time:46125ms step_avg:42.87ms
step:1077/2110 train_time:46185ms step_avg:42.88ms
step:1078/2110 train_time:46243ms step_avg:42.90ms
step:1079/2110 train_time:46303ms step_avg:42.91ms
step:1080/2110 train_time:46363ms step_avg:42.93ms
step:1081/2110 train_time:46422ms step_avg:42.94ms
step:1082/2110 train_time:46480ms step_avg:42.96ms
step:1083/2110 train_time:46539ms step_avg:42.97ms
step:1084/2110 train_time:46598ms step_avg:42.99ms
step:1085/2110 train_time:46657ms step_avg:43.00ms
step:1086/2110 train_time:46714ms step_avg:43.02ms
step:1087/2110 train_time:46774ms step_avg:43.03ms
step:1088/2110 train_time:46833ms step_avg:43.04ms
step:1089/2110 train_time:46893ms step_avg:43.06ms
step:1090/2110 train_time:46951ms step_avg:43.07ms
step:1091/2110 train_time:47011ms step_avg:43.09ms
step:1092/2110 train_time:47070ms step_avg:43.10ms
step:1093/2110 train_time:47130ms step_avg:43.12ms
step:1094/2110 train_time:47188ms step_avg:43.13ms
step:1095/2110 train_time:47248ms step_avg:43.15ms
step:1096/2110 train_time:47307ms step_avg:43.16ms
step:1097/2110 train_time:47367ms step_avg:43.18ms
step:1098/2110 train_time:47425ms step_avg:43.19ms
step:1099/2110 train_time:47486ms step_avg:43.21ms
step:1100/2110 train_time:47544ms step_avg:43.22ms
step:1101/2110 train_time:47603ms step_avg:43.24ms
step:1102/2110 train_time:47662ms step_avg:43.25ms
step:1103/2110 train_time:47722ms step_avg:43.27ms
step:1104/2110 train_time:47781ms step_avg:43.28ms
step:1105/2110 train_time:47841ms step_avg:43.29ms
step:1106/2110 train_time:47900ms step_avg:43.31ms
step:1107/2110 train_time:47959ms step_avg:43.32ms
step:1108/2110 train_time:48017ms step_avg:43.34ms
step:1109/2110 train_time:48076ms step_avg:43.35ms
step:1110/2110 train_time:48135ms step_avg:43.36ms
step:1111/2110 train_time:48195ms step_avg:43.38ms
step:1112/2110 train_time:48254ms step_avg:43.39ms
step:1113/2110 train_time:48314ms step_avg:43.41ms
step:1114/2110 train_time:48372ms step_avg:43.42ms
step:1115/2110 train_time:48433ms step_avg:43.44ms
step:1116/2110 train_time:48490ms step_avg:43.45ms
step:1117/2110 train_time:48551ms step_avg:43.47ms
step:1118/2110 train_time:48609ms step_avg:43.48ms
step:1119/2110 train_time:48669ms step_avg:43.49ms
step:1120/2110 train_time:48728ms step_avg:43.51ms
step:1121/2110 train_time:48789ms step_avg:43.52ms
step:1122/2110 train_time:48848ms step_avg:43.54ms
step:1123/2110 train_time:48908ms step_avg:43.55ms
step:1124/2110 train_time:48966ms step_avg:43.56ms
step:1125/2110 train_time:49026ms step_avg:43.58ms
step:1126/2110 train_time:49084ms step_avg:43.59ms
step:1127/2110 train_time:49145ms step_avg:43.61ms
step:1128/2110 train_time:49203ms step_avg:43.62ms
step:1129/2110 train_time:49263ms step_avg:43.63ms
step:1130/2110 train_time:49322ms step_avg:43.65ms
step:1131/2110 train_time:49382ms step_avg:43.66ms
step:1132/2110 train_time:49441ms step_avg:43.68ms
step:1133/2110 train_time:49501ms step_avg:43.69ms
step:1134/2110 train_time:49559ms step_avg:43.70ms
step:1135/2110 train_time:49618ms step_avg:43.72ms
step:1136/2110 train_time:49676ms step_avg:43.73ms
step:1137/2110 train_time:49736ms step_avg:43.74ms
step:1138/2110 train_time:49794ms step_avg:43.76ms
step:1139/2110 train_time:49854ms step_avg:43.77ms
step:1140/2110 train_time:49913ms step_avg:43.78ms
step:1141/2110 train_time:49973ms step_avg:43.80ms
step:1142/2110 train_time:50032ms step_avg:43.81ms
step:1143/2110 train_time:50093ms step_avg:43.83ms
step:1144/2110 train_time:50153ms step_avg:43.84ms
step:1145/2110 train_time:50214ms step_avg:43.86ms
step:1146/2110 train_time:50273ms step_avg:43.87ms
step:1147/2110 train_time:50333ms step_avg:43.88ms
step:1148/2110 train_time:50392ms step_avg:43.90ms
step:1149/2110 train_time:50454ms step_avg:43.91ms
step:1150/2110 train_time:50512ms step_avg:43.92ms
step:1151/2110 train_time:50572ms step_avg:43.94ms
step:1152/2110 train_time:50631ms step_avg:43.95ms
step:1153/2110 train_time:50692ms step_avg:43.97ms
step:1154/2110 train_time:50750ms step_avg:43.98ms
step:1155/2110 train_time:50811ms step_avg:43.99ms
step:1156/2110 train_time:50870ms step_avg:44.00ms
step:1157/2110 train_time:50931ms step_avg:44.02ms
step:1158/2110 train_time:50990ms step_avg:44.03ms
step:1159/2110 train_time:51051ms step_avg:44.05ms
step:1160/2110 train_time:51109ms step_avg:44.06ms
step:1161/2110 train_time:51170ms step_avg:44.07ms
step:1162/2110 train_time:51229ms step_avg:44.09ms
step:1163/2110 train_time:51290ms step_avg:44.10ms
step:1164/2110 train_time:51350ms step_avg:44.12ms
step:1165/2110 train_time:51410ms step_avg:44.13ms
step:1166/2110 train_time:51469ms step_avg:44.14ms
step:1167/2110 train_time:51529ms step_avg:44.16ms
step:1168/2110 train_time:51589ms step_avg:44.17ms
step:1169/2110 train_time:51649ms step_avg:44.18ms
step:1170/2110 train_time:51708ms step_avg:44.19ms
step:1171/2110 train_time:51769ms step_avg:44.21ms
step:1172/2110 train_time:51827ms step_avg:44.22ms
step:1173/2110 train_time:51889ms step_avg:44.24ms
step:1174/2110 train_time:51947ms step_avg:44.25ms
step:1175/2110 train_time:52008ms step_avg:44.26ms
step:1176/2110 train_time:52066ms step_avg:44.27ms
step:1177/2110 train_time:52128ms step_avg:44.29ms
step:1178/2110 train_time:52187ms step_avg:44.30ms
step:1179/2110 train_time:52248ms step_avg:44.32ms
step:1180/2110 train_time:52307ms step_avg:44.33ms
step:1181/2110 train_time:52368ms step_avg:44.34ms
step:1182/2110 train_time:52427ms step_avg:44.35ms
step:1183/2110 train_time:52487ms step_avg:44.37ms
step:1184/2110 train_time:52546ms step_avg:44.38ms
step:1185/2110 train_time:52607ms step_avg:44.39ms
step:1186/2110 train_time:52666ms step_avg:44.41ms
step:1187/2110 train_time:52727ms step_avg:44.42ms
step:1188/2110 train_time:52786ms step_avg:44.43ms
step:1189/2110 train_time:52847ms step_avg:44.45ms
step:1190/2110 train_time:52906ms step_avg:44.46ms
step:1191/2110 train_time:52967ms step_avg:44.47ms
step:1192/2110 train_time:53025ms step_avg:44.48ms
step:1193/2110 train_time:53086ms step_avg:44.50ms
step:1194/2110 train_time:53145ms step_avg:44.51ms
step:1195/2110 train_time:53206ms step_avg:44.52ms
step:1196/2110 train_time:53265ms step_avg:44.54ms
step:1197/2110 train_time:53326ms step_avg:44.55ms
step:1198/2110 train_time:53385ms step_avg:44.56ms
step:1199/2110 train_time:53446ms step_avg:44.58ms
step:1200/2110 train_time:53506ms step_avg:44.59ms
step:1201/2110 train_time:53567ms step_avg:44.60ms
step:1202/2110 train_time:53626ms step_avg:44.61ms
step:1203/2110 train_time:53686ms step_avg:44.63ms
step:1204/2110 train_time:53745ms step_avg:44.64ms
step:1205/2110 train_time:53805ms step_avg:44.65ms
step:1206/2110 train_time:53865ms step_avg:44.66ms
step:1207/2110 train_time:53925ms step_avg:44.68ms
step:1208/2110 train_time:53985ms step_avg:44.69ms
step:1209/2110 train_time:54045ms step_avg:44.70ms
step:1210/2110 train_time:54104ms step_avg:44.71ms
step:1211/2110 train_time:54164ms step_avg:44.73ms
step:1212/2110 train_time:54223ms step_avg:44.74ms
step:1213/2110 train_time:54283ms step_avg:44.75ms
step:1214/2110 train_time:54342ms step_avg:44.76ms
step:1215/2110 train_time:54402ms step_avg:44.78ms
step:1216/2110 train_time:54461ms step_avg:44.79ms
step:1217/2110 train_time:54521ms step_avg:44.80ms
step:1218/2110 train_time:54579ms step_avg:44.81ms
step:1219/2110 train_time:54640ms step_avg:44.82ms
step:1220/2110 train_time:54700ms step_avg:44.84ms
step:1221/2110 train_time:54760ms step_avg:44.85ms
step:1222/2110 train_time:54820ms step_avg:44.86ms
step:1223/2110 train_time:54880ms step_avg:44.87ms
step:1224/2110 train_time:54940ms step_avg:44.89ms
step:1225/2110 train_time:55000ms step_avg:44.90ms
step:1226/2110 train_time:55060ms step_avg:44.91ms
step:1227/2110 train_time:55120ms step_avg:44.92ms
step:1228/2110 train_time:55179ms step_avg:44.93ms
step:1229/2110 train_time:55239ms step_avg:44.95ms
step:1230/2110 train_time:55298ms step_avg:44.96ms
step:1231/2110 train_time:55359ms step_avg:44.97ms
step:1232/2110 train_time:55418ms step_avg:44.98ms
step:1233/2110 train_time:55479ms step_avg:44.99ms
step:1234/2110 train_time:55538ms step_avg:45.01ms
step:1235/2110 train_time:55598ms step_avg:45.02ms
step:1236/2110 train_time:55659ms step_avg:45.03ms
step:1237/2110 train_time:55719ms step_avg:45.04ms
step:1238/2110 train_time:55779ms step_avg:45.06ms
step:1239/2110 train_time:55839ms step_avg:45.07ms
step:1240/2110 train_time:55899ms step_avg:45.08ms
step:1241/2110 train_time:55958ms step_avg:45.09ms
step:1242/2110 train_time:56017ms step_avg:45.10ms
step:1243/2110 train_time:56078ms step_avg:45.11ms
step:1244/2110 train_time:56137ms step_avg:45.13ms
step:1245/2110 train_time:56197ms step_avg:45.14ms
step:1246/2110 train_time:56256ms step_avg:45.15ms
step:1247/2110 train_time:56317ms step_avg:45.16ms
step:1248/2110 train_time:56375ms step_avg:45.17ms
step:1249/2110 train_time:56436ms step_avg:45.19ms
step:1250/2110 train_time:56494ms step_avg:45.20ms
step:1250/2110 val_loss:3.5955 train_time:56557ms step_avg:45.25ms
step:1251/2110 train_time:56584ms step_avg:45.23ms
step:1252/2110 train_time:56619ms step_avg:45.22ms
step:1253/2110 train_time:56683ms step_avg:45.24ms
step:1254/2110 train_time:56744ms step_avg:45.25ms
step:1255/2110 train_time:56804ms step_avg:45.26ms
step:1256/2110 train_time:56864ms step_avg:45.27ms
step:1257/2110 train_time:56924ms step_avg:45.29ms
step:1258/2110 train_time:56982ms step_avg:45.30ms
step:1259/2110 train_time:57042ms step_avg:45.31ms
step:1260/2110 train_time:57100ms step_avg:45.32ms
step:1261/2110 train_time:57159ms step_avg:45.33ms
step:1262/2110 train_time:57218ms step_avg:45.34ms
step:1263/2110 train_time:57276ms step_avg:45.35ms
step:1264/2110 train_time:57334ms step_avg:45.36ms
step:1265/2110 train_time:57394ms step_avg:45.37ms
step:1266/2110 train_time:57452ms step_avg:45.38ms
step:1267/2110 train_time:57513ms step_avg:45.39ms
step:1268/2110 train_time:57573ms step_avg:45.40ms
step:1269/2110 train_time:57636ms step_avg:45.42ms
step:1270/2110 train_time:57696ms step_avg:45.43ms
step:1271/2110 train_time:57759ms step_avg:45.44ms
step:1272/2110 train_time:57819ms step_avg:45.45ms
step:1273/2110 train_time:57879ms step_avg:45.47ms
step:1274/2110 train_time:57938ms step_avg:45.48ms
step:1275/2110 train_time:57998ms step_avg:45.49ms
step:1276/2110 train_time:58058ms step_avg:45.50ms
step:1277/2110 train_time:58117ms step_avg:45.51ms
step:1278/2110 train_time:58174ms step_avg:45.52ms
step:1279/2110 train_time:58234ms step_avg:45.53ms
step:1280/2110 train_time:58292ms step_avg:45.54ms
step:1281/2110 train_time:58351ms step_avg:45.55ms
step:1282/2110 train_time:58409ms step_avg:45.56ms
step:1283/2110 train_time:58469ms step_avg:45.57ms
step:1284/2110 train_time:58528ms step_avg:45.58ms
step:1285/2110 train_time:58589ms step_avg:45.59ms
step:1286/2110 train_time:58650ms step_avg:45.61ms
step:1287/2110 train_time:58713ms step_avg:45.62ms
step:1288/2110 train_time:58773ms step_avg:45.63ms
step:1289/2110 train_time:58835ms step_avg:45.64ms
step:1290/2110 train_time:58894ms step_avg:45.65ms
step:1291/2110 train_time:58956ms step_avg:45.67ms
step:1292/2110 train_time:59014ms step_avg:45.68ms
step:1293/2110 train_time:59075ms step_avg:45.69ms
step:1294/2110 train_time:59133ms step_avg:45.70ms
step:1295/2110 train_time:59194ms step_avg:45.71ms
step:1296/2110 train_time:59253ms step_avg:45.72ms
step:1297/2110 train_time:59311ms step_avg:45.73ms
step:1298/2110 train_time:59369ms step_avg:45.74ms
step:1299/2110 train_time:59429ms step_avg:45.75ms
step:1300/2110 train_time:59488ms step_avg:45.76ms
step:1301/2110 train_time:59549ms step_avg:45.77ms
step:1302/2110 train_time:59608ms step_avg:45.78ms
step:1303/2110 train_time:59670ms step_avg:45.79ms
step:1304/2110 train_time:59729ms step_avg:45.80ms
step:1305/2110 train_time:59791ms step_avg:45.82ms
step:1306/2110 train_time:59850ms step_avg:45.83ms
step:1307/2110 train_time:59910ms step_avg:45.84ms
step:1308/2110 train_time:59969ms step_avg:45.85ms
step:1309/2110 train_time:60031ms step_avg:45.86ms
step:1310/2110 train_time:60089ms step_avg:45.87ms
step:1311/2110 train_time:60149ms step_avg:45.88ms
step:1312/2110 train_time:60207ms step_avg:45.89ms
step:1313/2110 train_time:60267ms step_avg:45.90ms
step:1314/2110 train_time:60326ms step_avg:45.91ms
step:1315/2110 train_time:60386ms step_avg:45.92ms
step:1316/2110 train_time:60445ms step_avg:45.93ms
step:1317/2110 train_time:60506ms step_avg:45.94ms
step:1318/2110 train_time:60565ms step_avg:45.95ms
step:1319/2110 train_time:60625ms step_avg:45.96ms
step:1320/2110 train_time:60685ms step_avg:45.97ms
step:1321/2110 train_time:60747ms step_avg:45.99ms
step:1322/2110 train_time:60806ms step_avg:46.00ms
step:1323/2110 train_time:60868ms step_avg:46.01ms
step:1324/2110 train_time:60927ms step_avg:46.02ms
step:1325/2110 train_time:60988ms step_avg:46.03ms
step:1326/2110 train_time:61047ms step_avg:46.04ms
step:1327/2110 train_time:61107ms step_avg:46.05ms
step:1328/2110 train_time:61166ms step_avg:46.06ms
step:1329/2110 train_time:61227ms step_avg:46.07ms
step:1330/2110 train_time:61285ms step_avg:46.08ms
step:1331/2110 train_time:61345ms step_avg:46.09ms
step:1332/2110 train_time:61404ms step_avg:46.10ms
step:1333/2110 train_time:61463ms step_avg:46.11ms
step:1334/2110 train_time:61522ms step_avg:46.12ms
step:1335/2110 train_time:61582ms step_avg:46.13ms
step:1336/2110 train_time:61643ms step_avg:46.14ms
step:1337/2110 train_time:61703ms step_avg:46.15ms
step:1338/2110 train_time:61762ms step_avg:46.16ms
step:1339/2110 train_time:61824ms step_avg:46.17ms
step:1340/2110 train_time:61884ms step_avg:46.18ms
step:1341/2110 train_time:61945ms step_avg:46.19ms
step:1342/2110 train_time:62005ms step_avg:46.20ms
step:1343/2110 train_time:62065ms step_avg:46.21ms
step:1344/2110 train_time:62124ms step_avg:46.22ms
step:1345/2110 train_time:62184ms step_avg:46.23ms
step:1346/2110 train_time:62245ms step_avg:46.24ms
step:1347/2110 train_time:62305ms step_avg:46.25ms
step:1348/2110 train_time:62364ms step_avg:46.26ms
step:1349/2110 train_time:62423ms step_avg:46.27ms
step:1350/2110 train_time:62482ms step_avg:46.28ms
step:1351/2110 train_time:62541ms step_avg:46.29ms
step:1352/2110 train_time:62600ms step_avg:46.30ms
step:1353/2110 train_time:62661ms step_avg:46.31ms
step:1354/2110 train_time:62720ms step_avg:46.32ms
step:1355/2110 train_time:62780ms step_avg:46.33ms
step:1356/2110 train_time:62840ms step_avg:46.34ms
step:1357/2110 train_time:62901ms step_avg:46.35ms
step:1358/2110 train_time:62961ms step_avg:46.36ms
step:1359/2110 train_time:63022ms step_avg:46.37ms
step:1360/2110 train_time:63080ms step_avg:46.38ms
step:1361/2110 train_time:63140ms step_avg:46.39ms
step:1362/2110 train_time:63199ms step_avg:46.40ms
step:1363/2110 train_time:63260ms step_avg:46.41ms
step:1364/2110 train_time:63318ms step_avg:46.42ms
step:1365/2110 train_time:63378ms step_avg:46.43ms
step:1366/2110 train_time:63436ms step_avg:46.44ms
step:1367/2110 train_time:63496ms step_avg:46.45ms
step:1368/2110 train_time:63555ms step_avg:46.46ms
step:1369/2110 train_time:63615ms step_avg:46.47ms
step:1370/2110 train_time:63674ms step_avg:46.48ms
step:1371/2110 train_time:63735ms step_avg:46.49ms
step:1372/2110 train_time:63794ms step_avg:46.50ms
step:1373/2110 train_time:63855ms step_avg:46.51ms
step:1374/2110 train_time:63914ms step_avg:46.52ms
step:1375/2110 train_time:63975ms step_avg:46.53ms
step:1376/2110 train_time:64034ms step_avg:46.54ms
step:1377/2110 train_time:64094ms step_avg:46.55ms
step:1378/2110 train_time:64153ms step_avg:46.55ms
step:1379/2110 train_time:64214ms step_avg:46.57ms
step:1380/2110 train_time:64272ms step_avg:46.57ms
step:1381/2110 train_time:64334ms step_avg:46.58ms
step:1382/2110 train_time:64419ms step_avg:46.61ms
step:1383/2110 train_time:64505ms step_avg:46.64ms
step:1384/2110 train_time:64591ms step_avg:46.67ms
step:1385/2110 train_time:64678ms step_avg:46.70ms
step:1386/2110 train_time:64764ms step_avg:46.73ms
step:1387/2110 train_time:64852ms step_avg:46.76ms
step:1388/2110 train_time:64939ms step_avg:46.79ms
step:1389/2110 train_time:65025ms step_avg:46.81ms
step:1390/2110 train_time:65112ms step_avg:46.84ms
step:1391/2110 train_time:65199ms step_avg:46.87ms
step:1392/2110 train_time:65284ms step_avg:46.90ms
step:1393/2110 train_time:65371ms step_avg:46.93ms
step:1394/2110 train_time:65458ms step_avg:46.96ms
step:1395/2110 train_time:65544ms step_avg:46.99ms
step:1396/2110 train_time:65630ms step_avg:47.01ms
step:1397/2110 train_time:65718ms step_avg:47.04ms
step:1398/2110 train_time:65804ms step_avg:47.07ms
step:1399/2110 train_time:65891ms step_avg:47.10ms
step:1400/2110 train_time:65978ms step_avg:47.13ms
step:1401/2110 train_time:66065ms step_avg:47.16ms
step:1402/2110 train_time:66151ms step_avg:47.18ms
step:1403/2110 train_time:66238ms step_avg:47.21ms
step:1404/2110 train_time:66323ms step_avg:47.24ms
step:1405/2110 train_time:66411ms step_avg:47.27ms
step:1406/2110 train_time:66497ms step_avg:47.30ms
step:1407/2110 train_time:66583ms step_avg:47.32ms
step:1408/2110 train_time:66670ms step_avg:47.35ms
step:1409/2110 train_time:66757ms step_avg:47.38ms
step:1410/2110 train_time:66843ms step_avg:47.41ms
step:1411/2110 train_time:66930ms step_avg:47.43ms
step:1412/2110 train_time:67016ms step_avg:47.46ms
step:1413/2110 train_time:67103ms step_avg:47.49ms
step:1414/2110 train_time:67189ms step_avg:47.52ms
step:1415/2110 train_time:67276ms step_avg:47.54ms
step:1416/2110 train_time:67362ms step_avg:47.57ms
step:1417/2110 train_time:67449ms step_avg:47.60ms
step:1418/2110 train_time:67535ms step_avg:47.63ms
step:1419/2110 train_time:67622ms step_avg:47.65ms
step:1420/2110 train_time:67708ms step_avg:47.68ms
step:1421/2110 train_time:67795ms step_avg:47.71ms
step:1422/2110 train_time:67881ms step_avg:47.74ms
step:1423/2110 train_time:67968ms step_avg:47.76ms
step:1424/2110 train_time:68054ms step_avg:47.79ms
step:1425/2110 train_time:68141ms step_avg:47.82ms
step:1426/2110 train_time:68226ms step_avg:47.84ms
step:1427/2110 train_time:68314ms step_avg:47.87ms
step:1428/2110 train_time:68400ms step_avg:47.90ms
step:1429/2110 train_time:68487ms step_avg:47.93ms
step:1430/2110 train_time:68574ms step_avg:47.95ms
step:1431/2110 train_time:68662ms step_avg:47.98ms
step:1432/2110 train_time:68748ms step_avg:48.01ms
step:1433/2110 train_time:68835ms step_avg:48.04ms
step:1434/2110 train_time:68921ms step_avg:48.06ms
step:1435/2110 train_time:69009ms step_avg:48.09ms
step:1436/2110 train_time:69095ms step_avg:48.12ms
step:1437/2110 train_time:69183ms step_avg:48.14ms
step:1438/2110 train_time:69268ms step_avg:48.17ms
step:1439/2110 train_time:69355ms step_avg:48.20ms
step:1440/2110 train_time:69441ms step_avg:48.22ms
step:1441/2110 train_time:69528ms step_avg:48.25ms
step:1442/2110 train_time:69614ms step_avg:48.28ms
step:1443/2110 train_time:69703ms step_avg:48.30ms
step:1444/2110 train_time:69789ms step_avg:48.33ms
step:1445/2110 train_time:69876ms step_avg:48.36ms
step:1446/2110 train_time:69962ms step_avg:48.38ms
step:1447/2110 train_time:70049ms step_avg:48.41ms
step:1448/2110 train_time:70135ms step_avg:48.44ms
step:1449/2110 train_time:70223ms step_avg:48.46ms
step:1450/2110 train_time:70308ms step_avg:48.49ms
step:1451/2110 train_time:70395ms step_avg:48.52ms
step:1452/2110 train_time:70481ms step_avg:48.54ms
step:1453/2110 train_time:70569ms step_avg:48.57ms
step:1454/2110 train_time:70656ms step_avg:48.59ms
step:1455/2110 train_time:70742ms step_avg:48.62ms
step:1456/2110 train_time:70828ms step_avg:48.65ms
step:1457/2110 train_time:70916ms step_avg:48.67ms
step:1458/2110 train_time:71001ms step_avg:48.70ms
step:1459/2110 train_time:71089ms step_avg:48.72ms
step:1460/2110 train_time:71176ms step_avg:48.75ms
step:1461/2110 train_time:71263ms step_avg:48.78ms
step:1462/2110 train_time:71349ms step_avg:48.80ms
step:1463/2110 train_time:71436ms step_avg:48.83ms
step:1464/2110 train_time:71522ms step_avg:48.85ms
step:1465/2110 train_time:71610ms step_avg:48.88ms
step:1466/2110 train_time:71695ms step_avg:48.91ms
step:1467/2110 train_time:71783ms step_avg:48.93ms
step:1468/2110 train_time:71869ms step_avg:48.96ms
step:1469/2110 train_time:71956ms step_avg:48.98ms
step:1470/2110 train_time:72042ms step_avg:49.01ms
step:1471/2110 train_time:72129ms step_avg:49.03ms
step:1472/2110 train_time:72215ms step_avg:49.06ms
step:1473/2110 train_time:72303ms step_avg:49.09ms
step:1474/2110 train_time:72390ms step_avg:49.11ms
step:1475/2110 train_time:72477ms step_avg:49.14ms
step:1476/2110 train_time:72564ms step_avg:49.16ms
step:1477/2110 train_time:72649ms step_avg:49.19ms
step:1478/2110 train_time:72736ms step_avg:49.21ms
step:1479/2110 train_time:72824ms step_avg:49.24ms
step:1480/2110 train_time:72910ms step_avg:49.26ms
step:1481/2110 train_time:72997ms step_avg:49.29ms
step:1482/2110 train_time:73083ms step_avg:49.31ms
step:1483/2110 train_time:73171ms step_avg:49.34ms
step:1484/2110 train_time:73256ms step_avg:49.36ms
step:1485/2110 train_time:73344ms step_avg:49.39ms
step:1486/2110 train_time:73431ms step_avg:49.42ms
step:1487/2110 train_time:73518ms step_avg:49.44ms
step:1488/2110 train_time:73603ms step_avg:49.46ms
step:1489/2110 train_time:73691ms step_avg:49.49ms
step:1490/2110 train_time:73777ms step_avg:49.51ms
step:1491/2110 train_time:73864ms step_avg:49.54ms
step:1492/2110 train_time:73951ms step_avg:49.57ms
step:1493/2110 train_time:74038ms step_avg:49.59ms
step:1494/2110 train_time:74124ms step_avg:49.61ms
step:1495/2110 train_time:74211ms step_avg:49.64ms
step:1496/2110 train_time:74298ms step_avg:49.66ms
step:1497/2110 train_time:74385ms step_avg:49.69ms
step:1498/2110 train_time:74472ms step_avg:49.71ms
step:1499/2110 train_time:74558ms step_avg:49.74ms
step:1500/2110 train_time:74644ms step_avg:49.76ms
step:1500/2110 val_loss:3.4950 train_time:74733ms step_avg:49.82ms
step:1501/2110 train_time:74764ms step_avg:49.81ms
step:1502/2110 train_time:74825ms step_avg:49.82ms
step:1503/2110 train_time:74917ms step_avg:49.84ms
step:1504/2110 train_time:75006ms step_avg:49.87ms
step:1505/2110 train_time:75093ms step_avg:49.90ms
step:1506/2110 train_time:75178ms step_avg:49.92ms
step:1507/2110 train_time:75264ms step_avg:49.94ms
step:1508/2110 train_time:75350ms step_avg:49.97ms
step:1509/2110 train_time:75436ms step_avg:49.99ms
step:1510/2110 train_time:75520ms step_avg:50.01ms
step:1511/2110 train_time:75606ms step_avg:50.04ms
step:1512/2110 train_time:75694ms step_avg:50.06ms
step:1513/2110 train_time:75783ms step_avg:50.09ms
step:1514/2110 train_time:75872ms step_avg:50.11ms
step:1515/2110 train_time:75962ms step_avg:50.14ms
step:1516/2110 train_time:76050ms step_avg:50.16ms
step:1517/2110 train_time:76137ms step_avg:50.19ms
step:1518/2110 train_time:76223ms step_avg:50.21ms
step:1519/2110 train_time:76309ms step_avg:50.24ms
step:1520/2110 train_time:76394ms step_avg:50.26ms
step:1521/2110 train_time:76480ms step_avg:50.28ms
step:1522/2110 train_time:76564ms step_avg:50.31ms
step:1523/2110 train_time:76652ms step_avg:50.33ms
step:1524/2110 train_time:76739ms step_avg:50.35ms
step:1525/2110 train_time:76827ms step_avg:50.38ms
step:1526/2110 train_time:76916ms step_avg:50.40ms
step:1527/2110 train_time:77004ms step_avg:50.43ms
step:1528/2110 train_time:77091ms step_avg:50.45ms
step:1529/2110 train_time:77178ms step_avg:50.48ms
step:1530/2110 train_time:77264ms step_avg:50.50ms
step:1531/2110 train_time:77351ms step_avg:50.52ms
step:1532/2110 train_time:77436ms step_avg:50.55ms
step:1533/2110 train_time:77521ms step_avg:50.57ms
step:1534/2110 train_time:77607ms step_avg:50.59ms
step:1535/2110 train_time:77694ms step_avg:50.61ms
step:1536/2110 train_time:77780ms step_avg:50.64ms
step:1537/2110 train_time:77869ms step_avg:50.66ms
step:1538/2110 train_time:77956ms step_avg:50.69ms
step:1539/2110 train_time:78044ms step_avg:50.71ms
step:1540/2110 train_time:78130ms step_avg:50.73ms
step:1541/2110 train_time:78217ms step_avg:50.76ms
step:1542/2110 train_time:78304ms step_avg:50.78ms
step:1543/2110 train_time:78391ms step_avg:50.80ms
step:1544/2110 train_time:78477ms step_avg:50.83ms
step:1545/2110 train_time:78564ms step_avg:50.85ms
step:1546/2110 train_time:78650ms step_avg:50.87ms
step:1547/2110 train_time:78737ms step_avg:50.90ms
step:1548/2110 train_time:78823ms step_avg:50.92ms
step:1549/2110 train_time:78912ms step_avg:50.94ms
step:1550/2110 train_time:78999ms step_avg:50.97ms
step:1551/2110 train_time:79086ms step_avg:50.99ms
step:1552/2110 train_time:79173ms step_avg:51.01ms
step:1553/2110 train_time:79261ms step_avg:51.04ms
step:1554/2110 train_time:79347ms step_avg:51.06ms
step:1555/2110 train_time:79433ms step_avg:51.08ms
step:1556/2110 train_time:79518ms step_avg:51.10ms
step:1557/2110 train_time:79605ms step_avg:51.13ms
step:1558/2110 train_time:79691ms step_avg:51.15ms
step:1559/2110 train_time:79779ms step_avg:51.17ms
step:1560/2110 train_time:79866ms step_avg:51.20ms
step:1561/2110 train_time:79954ms step_avg:51.22ms
step:1562/2110 train_time:80040ms step_avg:51.24ms
step:1563/2110 train_time:80128ms step_avg:51.27ms
step:1564/2110 train_time:80215ms step_avg:51.29ms
step:1565/2110 train_time:80301ms step_avg:51.31ms
step:1566/2110 train_time:80388ms step_avg:51.33ms
step:1567/2110 train_time:80474ms step_avg:51.36ms
step:1568/2110 train_time:80560ms step_avg:51.38ms
step:1569/2110 train_time:80647ms step_avg:51.40ms
step:1570/2110 train_time:80735ms step_avg:51.42ms
step:1571/2110 train_time:80823ms step_avg:51.45ms
step:1572/2110 train_time:80910ms step_avg:51.47ms
step:1573/2110 train_time:80998ms step_avg:51.49ms
step:1574/2110 train_time:81085ms step_avg:51.52ms
step:1575/2110 train_time:81172ms step_avg:51.54ms
step:1576/2110 train_time:81259ms step_avg:51.56ms
step:1577/2110 train_time:81346ms step_avg:51.58ms
step:1578/2110 train_time:81433ms step_avg:51.61ms
step:1579/2110 train_time:81520ms step_avg:51.63ms
step:1580/2110 train_time:81606ms step_avg:51.65ms
step:1581/2110 train_time:81693ms step_avg:51.67ms
step:1582/2110 train_time:81780ms step_avg:51.69ms
step:1583/2110 train_time:81867ms step_avg:51.72ms
step:1584/2110 train_time:81955ms step_avg:51.74ms
step:1585/2110 train_time:82043ms step_avg:51.76ms
step:1586/2110 train_time:82130ms step_avg:51.78ms
step:1587/2110 train_time:82218ms step_avg:51.81ms
step:1588/2110 train_time:82303ms step_avg:51.83ms
step:1589/2110 train_time:82391ms step_avg:51.85ms
step:1590/2110 train_time:82477ms step_avg:51.87ms
step:1591/2110 train_time:82564ms step_avg:51.89ms
step:1592/2110 train_time:82651ms step_avg:51.92ms
step:1593/2110 train_time:82738ms step_avg:51.94ms
step:1594/2110 train_time:82825ms step_avg:51.96ms
step:1595/2110 train_time:82912ms step_avg:51.98ms
step:1596/2110 train_time:82999ms step_avg:52.00ms
step:1597/2110 train_time:83086ms step_avg:52.03ms
step:1598/2110 train_time:83173ms step_avg:52.05ms
step:1599/2110 train_time:83261ms step_avg:52.07ms
step:1600/2110 train_time:83348ms step_avg:52.09ms
step:1601/2110 train_time:83435ms step_avg:52.11ms
step:1602/2110 train_time:83520ms step_avg:52.14ms
step:1603/2110 train_time:83607ms step_avg:52.16ms
step:1604/2110 train_time:83693ms step_avg:52.18ms
step:1605/2110 train_time:83780ms step_avg:52.20ms
step:1606/2110 train_time:83867ms step_avg:52.22ms
step:1607/2110 train_time:83954ms step_avg:52.24ms
step:1608/2110 train_time:84040ms step_avg:52.26ms
step:1609/2110 train_time:84129ms step_avg:52.29ms
step:1610/2110 train_time:84216ms step_avg:52.31ms
step:1611/2110 train_time:84303ms step_avg:52.33ms
step:1612/2110 train_time:84391ms step_avg:52.35ms
step:1613/2110 train_time:84478ms step_avg:52.37ms
step:1614/2110 train_time:84564ms step_avg:52.39ms
step:1615/2110 train_time:84652ms step_avg:52.42ms
step:1616/2110 train_time:84738ms step_avg:52.44ms
step:1617/2110 train_time:84825ms step_avg:52.46ms
step:1618/2110 train_time:84912ms step_avg:52.48ms
step:1619/2110 train_time:84999ms step_avg:52.50ms
step:1620/2110 train_time:85087ms step_avg:52.52ms
step:1621/2110 train_time:85173ms step_avg:52.54ms
step:1622/2110 train_time:85260ms step_avg:52.56ms
step:1623/2110 train_time:85348ms step_avg:52.59ms
step:1624/2110 train_time:85434ms step_avg:52.61ms
step:1625/2110 train_time:85522ms step_avg:52.63ms
step:1626/2110 train_time:85609ms step_avg:52.65ms
step:1627/2110 train_time:85696ms step_avg:52.67ms
step:1628/2110 train_time:85781ms step_avg:52.69ms
step:1629/2110 train_time:85869ms step_avg:52.71ms
step:1630/2110 train_time:85955ms step_avg:52.73ms
step:1631/2110 train_time:86042ms step_avg:52.75ms
step:1632/2110 train_time:86130ms step_avg:52.78ms
step:1633/2110 train_time:86218ms step_avg:52.80ms
step:1634/2110 train_time:86304ms step_avg:52.82ms
step:1635/2110 train_time:86390ms step_avg:52.84ms
step:1636/2110 train_time:86477ms step_avg:52.86ms
step:1637/2110 train_time:86565ms step_avg:52.88ms
step:1638/2110 train_time:86651ms step_avg:52.90ms
step:1639/2110 train_time:86739ms step_avg:52.92ms
step:1640/2110 train_time:86824ms step_avg:52.94ms
step:1641/2110 train_time:86912ms step_avg:52.96ms
step:1642/2110 train_time:86998ms step_avg:52.98ms
step:1643/2110 train_time:87086ms step_avg:53.00ms
step:1644/2110 train_time:87174ms step_avg:53.03ms
step:1645/2110 train_time:87261ms step_avg:53.05ms
step:1646/2110 train_time:87348ms step_avg:53.07ms
step:1647/2110 train_time:87435ms step_avg:53.09ms
step:1648/2110 train_time:87521ms step_avg:53.11ms
step:1649/2110 train_time:87608ms step_avg:53.13ms
step:1650/2110 train_time:87696ms step_avg:53.15ms
step:1651/2110 train_time:87781ms step_avg:53.17ms
step:1652/2110 train_time:87867ms step_avg:53.19ms
step:1653/2110 train_time:87954ms step_avg:53.21ms
step:1654/2110 train_time:88040ms step_avg:53.23ms
step:1655/2110 train_time:88128ms step_avg:53.25ms
step:1656/2110 train_time:88215ms step_avg:53.27ms
step:1657/2110 train_time:88302ms step_avg:53.29ms
step:1658/2110 train_time:88391ms step_avg:53.31ms
step:1659/2110 train_time:88479ms step_avg:53.33ms
step:1660/2110 train_time:88567ms step_avg:53.35ms
step:1661/2110 train_time:88654ms step_avg:53.37ms
step:1662/2110 train_time:88741ms step_avg:53.39ms
step:1663/2110 train_time:88831ms step_avg:53.42ms
step:1664/2110 train_time:88919ms step_avg:53.44ms
step:1665/2110 train_time:89009ms step_avg:53.46ms
step:1666/2110 train_time:89097ms step_avg:53.48ms
step:1667/2110 train_time:89185ms step_avg:53.50ms
step:1668/2110 train_time:89274ms step_avg:53.52ms
step:1669/2110 train_time:89361ms step_avg:53.54ms
step:1670/2110 train_time:89450ms step_avg:53.56ms
step:1671/2110 train_time:89538ms step_avg:53.58ms
step:1672/2110 train_time:89625ms step_avg:53.60ms
step:1673/2110 train_time:89713ms step_avg:53.62ms
step:1674/2110 train_time:89801ms step_avg:53.64ms
step:1675/2110 train_time:89890ms step_avg:53.67ms
step:1676/2110 train_time:89978ms step_avg:53.69ms
step:1677/2110 train_time:90067ms step_avg:53.71ms
step:1678/2110 train_time:90153ms step_avg:53.73ms
step:1679/2110 train_time:90244ms step_avg:53.75ms
step:1680/2110 train_time:90333ms step_avg:53.77ms
step:1681/2110 train_time:90421ms step_avg:53.79ms
step:1682/2110 train_time:90509ms step_avg:53.81ms
step:1683/2110 train_time:90597ms step_avg:53.83ms
step:1684/2110 train_time:90684ms step_avg:53.85ms
step:1685/2110 train_time:90773ms step_avg:53.87ms
step:1686/2110 train_time:90861ms step_avg:53.89ms
step:1687/2110 train_time:90950ms step_avg:53.91ms
step:1688/2110 train_time:91038ms step_avg:53.93ms
step:1689/2110 train_time:91127ms step_avg:53.95ms
step:1690/2110 train_time:91215ms step_avg:53.97ms
step:1691/2110 train_time:91303ms step_avg:53.99ms
step:1692/2110 train_time:91391ms step_avg:54.01ms
step:1693/2110 train_time:91480ms step_avg:54.03ms
step:1694/2110 train_time:91567ms step_avg:54.05ms
step:1695/2110 train_time:91655ms step_avg:54.07ms
step:1696/2110 train_time:91742ms step_avg:54.09ms
step:1697/2110 train_time:91831ms step_avg:54.11ms
step:1698/2110 train_time:91919ms step_avg:54.13ms
step:1699/2110 train_time:92008ms step_avg:54.15ms
step:1700/2110 train_time:92096ms step_avg:54.17ms
step:1701/2110 train_time:92184ms step_avg:54.19ms
step:1702/2110 train_time:92272ms step_avg:54.21ms
step:1703/2110 train_time:92361ms step_avg:54.23ms
step:1704/2110 train_time:92451ms step_avg:54.26ms
step:1705/2110 train_time:92540ms step_avg:54.28ms
step:1706/2110 train_time:92629ms step_avg:54.30ms
step:1707/2110 train_time:92716ms step_avg:54.32ms
step:1708/2110 train_time:92802ms step_avg:54.33ms
step:1709/2110 train_time:92891ms step_avg:54.35ms
step:1710/2110 train_time:92979ms step_avg:54.37ms
step:1711/2110 train_time:93067ms step_avg:54.39ms
step:1712/2110 train_time:93155ms step_avg:54.41ms
step:1713/2110 train_time:93243ms step_avg:54.43ms
step:1714/2110 train_time:93331ms step_avg:54.45ms
step:1715/2110 train_time:93419ms step_avg:54.47ms
step:1716/2110 train_time:93507ms step_avg:54.49ms
step:1717/2110 train_time:93596ms step_avg:54.51ms
step:1718/2110 train_time:93683ms step_avg:54.53ms
step:1719/2110 train_time:93772ms step_avg:54.55ms
step:1720/2110 train_time:93860ms step_avg:54.57ms
step:1721/2110 train_time:93948ms step_avg:54.59ms
step:1722/2110 train_time:94036ms step_avg:54.61ms
step:1723/2110 train_time:94125ms step_avg:54.63ms
step:1724/2110 train_time:94213ms step_avg:54.65ms
step:1725/2110 train_time:94302ms step_avg:54.67ms
step:1726/2110 train_time:94390ms step_avg:54.69ms
step:1727/2110 train_time:94478ms step_avg:54.71ms
step:1728/2110 train_time:94565ms step_avg:54.72ms
step:1729/2110 train_time:94654ms step_avg:54.74ms
step:1730/2110 train_time:94741ms step_avg:54.76ms
step:1731/2110 train_time:94829ms step_avg:54.78ms
step:1732/2110 train_time:94917ms step_avg:54.80ms
step:1733/2110 train_time:95006ms step_avg:54.82ms
step:1734/2110 train_time:95093ms step_avg:54.84ms
step:1735/2110 train_time:95181ms step_avg:54.86ms
step:1736/2110 train_time:95267ms step_avg:54.88ms
step:1737/2110 train_time:95357ms step_avg:54.90ms
step:1738/2110 train_time:95444ms step_avg:54.92ms
step:1739/2110 train_time:95533ms step_avg:54.94ms
step:1740/2110 train_time:95621ms step_avg:54.95ms
step:1741/2110 train_time:95709ms step_avg:54.97ms
step:1742/2110 train_time:95798ms step_avg:54.99ms
step:1743/2110 train_time:95887ms step_avg:55.01ms
step:1744/2110 train_time:95974ms step_avg:55.03ms
step:1745/2110 train_time:96062ms step_avg:55.05ms
step:1746/2110 train_time:96150ms step_avg:55.07ms
step:1747/2110 train_time:96238ms step_avg:55.09ms
step:1748/2110 train_time:96325ms step_avg:55.11ms
step:1749/2110 train_time:96415ms step_avg:55.13ms
step:1750/2110 train_time:96503ms step_avg:55.14ms
step:1750/2110 val_loss:3.3797 train_time:96594ms step_avg:55.20ms
step:1751/2110 train_time:96625ms step_avg:55.18ms
step:1752/2110 train_time:96687ms step_avg:55.19ms
step:1753/2110 train_time:96778ms step_avg:55.21ms
step:1754/2110 train_time:96868ms step_avg:55.23ms
step:1755/2110 train_time:96956ms step_avg:55.25ms
step:1756/2110 train_time:97042ms step_avg:55.26ms
step:1757/2110 train_time:97129ms step_avg:55.28ms
step:1758/2110 train_time:97216ms step_avg:55.30ms
step:1759/2110 train_time:97303ms step_avg:55.32ms
step:1760/2110 train_time:97389ms step_avg:55.33ms
step:1761/2110 train_time:97476ms step_avg:55.35ms
step:1762/2110 train_time:97565ms step_avg:55.37ms
step:1763/2110 train_time:97656ms step_avg:55.39ms
step:1764/2110 train_time:97746ms step_avg:55.41ms
step:1765/2110 train_time:97835ms step_avg:55.43ms
step:1766/2110 train_time:97923ms step_avg:55.45ms
step:1767/2110 train_time:98011ms step_avg:55.47ms
step:1768/2110 train_time:98097ms step_avg:55.48ms
step:1769/2110 train_time:98185ms step_avg:55.50ms
step:1770/2110 train_time:98272ms step_avg:55.52ms
step:1771/2110 train_time:98359ms step_avg:55.54ms
step:1772/2110 train_time:98447ms step_avg:55.56ms
step:1773/2110 train_time:98534ms step_avg:55.57ms
step:1774/2110 train_time:98623ms step_avg:55.59ms
step:1775/2110 train_time:98713ms step_avg:55.61ms
step:1776/2110 train_time:98802ms step_avg:55.63ms
step:1777/2110 train_time:98891ms step_avg:55.65ms
step:1778/2110 train_time:98977ms step_avg:55.67ms
step:1779/2110 train_time:99066ms step_avg:55.69ms
step:1780/2110 train_time:99153ms step_avg:55.70ms
step:1781/2110 train_time:99241ms step_avg:55.72ms
step:1782/2110 train_time:99328ms step_avg:55.74ms
step:1783/2110 train_time:99415ms step_avg:55.76ms
step:1784/2110 train_time:99503ms step_avg:55.78ms
step:1785/2110 train_time:99591ms step_avg:55.79ms
step:1786/2110 train_time:99679ms step_avg:55.81ms
step:1787/2110 train_time:99769ms step_avg:55.83ms
step:1788/2110 train_time:99858ms step_avg:55.85ms
step:1789/2110 train_time:99946ms step_avg:55.87ms
step:1790/2110 train_time:100033ms step_avg:55.88ms
step:1791/2110 train_time:100121ms step_avg:55.90ms
step:1792/2110 train_time:100208ms step_avg:55.92ms
step:1793/2110 train_time:100295ms step_avg:55.94ms
step:1794/2110 train_time:100382ms step_avg:55.95ms
step:1795/2110 train_time:100470ms step_avg:55.97ms
step:1796/2110 train_time:100558ms step_avg:55.99ms
step:1797/2110 train_time:100646ms step_avg:56.01ms
step:1798/2110 train_time:100733ms step_avg:56.03ms
step:1799/2110 train_time:100823ms step_avg:56.04ms
step:1800/2110 train_time:100911ms step_avg:56.06ms
step:1801/2110 train_time:100999ms step_avg:56.08ms
step:1802/2110 train_time:101087ms step_avg:56.10ms
step:1803/2110 train_time:101176ms step_avg:56.12ms
step:1804/2110 train_time:101263ms step_avg:56.13ms
step:1805/2110 train_time:101350ms step_avg:56.15ms
step:1806/2110 train_time:101437ms step_avg:56.17ms
step:1807/2110 train_time:101525ms step_avg:56.18ms
step:1808/2110 train_time:101612ms step_avg:56.20ms
step:1809/2110 train_time:101702ms step_avg:56.22ms
step:1810/2110 train_time:101790ms step_avg:56.24ms
step:1811/2110 train_time:101879ms step_avg:56.26ms
step:1812/2110 train_time:101966ms step_avg:56.27ms
step:1813/2110 train_time:102054ms step_avg:56.29ms
step:1814/2110 train_time:102142ms step_avg:56.31ms
step:1815/2110 train_time:102230ms step_avg:56.33ms
step:1816/2110 train_time:102318ms step_avg:56.34ms
step:1817/2110 train_time:102406ms step_avg:56.36ms
step:1818/2110 train_time:102493ms step_avg:56.38ms
step:1819/2110 train_time:102581ms step_avg:56.39ms
step:1820/2110 train_time:102669ms step_avg:56.41ms
step:1821/2110 train_time:102760ms step_avg:56.43ms
step:1822/2110 train_time:102848ms step_avg:56.45ms
step:1823/2110 train_time:102936ms step_avg:56.46ms
step:1824/2110 train_time:103024ms step_avg:56.48ms
step:1825/2110 train_time:103111ms step_avg:56.50ms
step:1826/2110 train_time:103198ms step_avg:56.52ms
step:1827/2110 train_time:103287ms step_avg:56.53ms
step:1828/2110 train_time:103374ms step_avg:56.55ms
step:1829/2110 train_time:103462ms step_avg:56.57ms
step:1830/2110 train_time:103550ms step_avg:56.58ms
step:1831/2110 train_time:103639ms step_avg:56.60ms
step:1832/2110 train_time:103726ms step_avg:56.62ms
step:1833/2110 train_time:103815ms step_avg:56.64ms
step:1834/2110 train_time:103902ms step_avg:56.65ms
step:1835/2110 train_time:103991ms step_avg:56.67ms
step:1836/2110 train_time:104078ms step_avg:56.69ms
step:1837/2110 train_time:104166ms step_avg:56.70ms
step:1838/2110 train_time:104253ms step_avg:56.72ms
step:1839/2110 train_time:104341ms step_avg:56.74ms
step:1840/2110 train_time:104428ms step_avg:56.75ms
step:1841/2110 train_time:104516ms step_avg:56.77ms
step:1842/2110 train_time:104604ms step_avg:56.79ms
step:1843/2110 train_time:104693ms step_avg:56.81ms
step:1844/2110 train_time:104780ms step_avg:56.82ms
step:1845/2110 train_time:104868ms step_avg:56.84ms
step:1846/2110 train_time:104956ms step_avg:56.86ms
step:1847/2110 train_time:105045ms step_avg:56.87ms
step:1848/2110 train_time:105131ms step_avg:56.89ms
step:1849/2110 train_time:105220ms step_avg:56.91ms
step:1850/2110 train_time:105307ms step_avg:56.92ms
step:1851/2110 train_time:105395ms step_avg:56.94ms
step:1852/2110 train_time:105482ms step_avg:56.96ms
step:1853/2110 train_time:105571ms step_avg:56.97ms
step:1854/2110 train_time:105657ms step_avg:56.99ms
step:1855/2110 train_time:105746ms step_avg:57.01ms
step:1856/2110 train_time:105834ms step_avg:57.02ms
step:1857/2110 train_time:105923ms step_avg:57.04ms
step:1858/2110 train_time:106010ms step_avg:57.06ms
step:1859/2110 train_time:106098ms step_avg:57.07ms
step:1860/2110 train_time:106186ms step_avg:57.09ms
step:1861/2110 train_time:106274ms step_avg:57.11ms
step:1862/2110 train_time:106362ms step_avg:57.12ms
step:1863/2110 train_time:106451ms step_avg:57.14ms
step:1864/2110 train_time:106539ms step_avg:57.16ms
step:1865/2110 train_time:106627ms step_avg:57.17ms
step:1866/2110 train_time:106714ms step_avg:57.19ms
step:1867/2110 train_time:106801ms step_avg:57.20ms
step:1868/2110 train_time:106889ms step_avg:57.22ms
step:1869/2110 train_time:106979ms step_avg:57.24ms
step:1870/2110 train_time:107067ms step_avg:57.25ms
step:1871/2110 train_time:107154ms step_avg:57.27ms
step:1872/2110 train_time:107243ms step_avg:57.29ms
step:1873/2110 train_time:107330ms step_avg:57.30ms
step:1874/2110 train_time:107417ms step_avg:57.32ms
step:1875/2110 train_time:107505ms step_avg:57.34ms
step:1876/2110 train_time:107592ms step_avg:57.35ms
step:1877/2110 train_time:107680ms step_avg:57.37ms
step:1878/2110 train_time:107767ms step_avg:57.38ms
step:1879/2110 train_time:107855ms step_avg:57.40ms
step:1880/2110 train_time:107943ms step_avg:57.42ms
step:1881/2110 train_time:108031ms step_avg:57.43ms
step:1882/2110 train_time:108119ms step_avg:57.45ms
step:1883/2110 train_time:108208ms step_avg:57.47ms
step:1884/2110 train_time:108295ms step_avg:57.48ms
step:1885/2110 train_time:108382ms step_avg:57.50ms
step:1886/2110 train_time:108469ms step_avg:57.51ms
step:1887/2110 train_time:108560ms step_avg:57.53ms
step:1888/2110 train_time:108647ms step_avg:57.55ms
step:1889/2110 train_time:108736ms step_avg:57.56ms
step:1890/2110 train_time:108825ms step_avg:57.58ms
step:1891/2110 train_time:108912ms step_avg:57.59ms
step:1892/2110 train_time:108999ms step_avg:57.61ms
step:1893/2110 train_time:109088ms step_avg:57.63ms
step:1894/2110 train_time:109175ms step_avg:57.64ms
step:1895/2110 train_time:109263ms step_avg:57.66ms
step:1896/2110 train_time:109352ms step_avg:57.68ms
step:1897/2110 train_time:109438ms step_avg:57.69ms
step:1898/2110 train_time:109526ms step_avg:57.71ms
step:1899/2110 train_time:109615ms step_avg:57.72ms
step:1900/2110 train_time:109703ms step_avg:57.74ms
step:1901/2110 train_time:109791ms step_avg:57.75ms
step:1902/2110 train_time:109880ms step_avg:57.77ms
step:1903/2110 train_time:109968ms step_avg:57.79ms
step:1904/2110 train_time:110055ms step_avg:57.80ms
step:1905/2110 train_time:110143ms step_avg:57.82ms
step:1906/2110 train_time:110229ms step_avg:57.83ms
step:1907/2110 train_time:110318ms step_avg:57.85ms
step:1908/2110 train_time:110406ms step_avg:57.86ms
step:1909/2110 train_time:110493ms step_avg:57.88ms
step:1910/2110 train_time:110581ms step_avg:57.90ms
step:1911/2110 train_time:110669ms step_avg:57.91ms
step:1912/2110 train_time:110757ms step_avg:57.93ms
step:1913/2110 train_time:110846ms step_avg:57.94ms
step:1914/2110 train_time:110933ms step_avg:57.96ms
step:1915/2110 train_time:111021ms step_avg:57.97ms
step:1916/2110 train_time:111109ms step_avg:57.99ms
step:1917/2110 train_time:111197ms step_avg:58.01ms
step:1918/2110 train_time:111285ms step_avg:58.02ms
step:1919/2110 train_time:111373ms step_avg:58.04ms
step:1920/2110 train_time:111461ms step_avg:58.05ms
step:1921/2110 train_time:111549ms step_avg:58.07ms
step:1922/2110 train_time:111637ms step_avg:58.08ms
step:1923/2110 train_time:111724ms step_avg:58.10ms
step:1924/2110 train_time:111812ms step_avg:58.11ms
step:1925/2110 train_time:111900ms step_avg:58.13ms
step:1926/2110 train_time:111988ms step_avg:58.15ms
step:1927/2110 train_time:112076ms step_avg:58.16ms
step:1928/2110 train_time:112163ms step_avg:58.18ms
step:1929/2110 train_time:112252ms step_avg:58.19ms
step:1930/2110 train_time:112340ms step_avg:58.21ms
step:1931/2110 train_time:112428ms step_avg:58.22ms
step:1932/2110 train_time:112516ms step_avg:58.24ms
step:1933/2110 train_time:112604ms step_avg:58.25ms
step:1934/2110 train_time:112691ms step_avg:58.27ms
step:1935/2110 train_time:112780ms step_avg:58.28ms
step:1936/2110 train_time:112869ms step_avg:58.30ms
step:1937/2110 train_time:112956ms step_avg:58.31ms
step:1938/2110 train_time:113045ms step_avg:58.33ms
step:1939/2110 train_time:113132ms step_avg:58.35ms
step:1940/2110 train_time:113220ms step_avg:58.36ms
step:1941/2110 train_time:113307ms step_avg:58.38ms
step:1942/2110 train_time:113395ms step_avg:58.39ms
step:1943/2110 train_time:113482ms step_avg:58.41ms
step:1944/2110 train_time:113570ms step_avg:58.42ms
step:1945/2110 train_time:113658ms step_avg:58.44ms
step:1946/2110 train_time:113747ms step_avg:58.45ms
step:1947/2110 train_time:113834ms step_avg:58.47ms
step:1948/2110 train_time:113921ms step_avg:58.48ms
step:1949/2110 train_time:114009ms step_avg:58.50ms
step:1950/2110 train_time:114097ms step_avg:58.51ms
step:1951/2110 train_time:114184ms step_avg:58.53ms
step:1952/2110 train_time:114272ms step_avg:58.54ms
step:1953/2110 train_time:114360ms step_avg:58.56ms
step:1954/2110 train_time:114448ms step_avg:58.57ms
step:1955/2110 train_time:114536ms step_avg:58.59ms
step:1956/2110 train_time:114624ms step_avg:58.60ms
step:1957/2110 train_time:114712ms step_avg:58.62ms
step:1958/2110 train_time:114800ms step_avg:58.63ms
step:1959/2110 train_time:114888ms step_avg:58.65ms
step:1960/2110 train_time:114976ms step_avg:58.66ms
step:1961/2110 train_time:115063ms step_avg:58.68ms
step:1962/2110 train_time:115151ms step_avg:58.69ms
step:1963/2110 train_time:115239ms step_avg:58.71ms
step:1964/2110 train_time:115327ms step_avg:58.72ms
step:1965/2110 train_time:115417ms step_avg:58.74ms
step:1966/2110 train_time:115505ms step_avg:58.75ms
step:1967/2110 train_time:115592ms step_avg:58.77ms
step:1968/2110 train_time:115680ms step_avg:58.78ms
step:1969/2110 train_time:115768ms step_avg:58.80ms
step:1970/2110 train_time:115857ms step_avg:58.81ms
step:1971/2110 train_time:115944ms step_avg:58.83ms
step:1972/2110 train_time:116032ms step_avg:58.84ms
step:1973/2110 train_time:116120ms step_avg:58.85ms
step:1974/2110 train_time:116207ms step_avg:58.87ms
step:1975/2110 train_time:116295ms step_avg:58.88ms
step:1976/2110 train_time:116382ms step_avg:58.90ms
step:1977/2110 train_time:116471ms step_avg:58.91ms
step:1978/2110 train_time:116558ms step_avg:58.93ms
step:1979/2110 train_time:116646ms step_avg:58.94ms
step:1980/2110 train_time:116734ms step_avg:58.96ms
step:1981/2110 train_time:116823ms step_avg:58.97ms
step:1982/2110 train_time:116910ms step_avg:58.99ms
step:1983/2110 train_time:117002ms step_avg:59.00ms
step:1984/2110 train_time:117089ms step_avg:59.02ms
step:1985/2110 train_time:117176ms step_avg:59.03ms
step:1986/2110 train_time:117265ms step_avg:59.05ms
step:1987/2110 train_time:117353ms step_avg:59.06ms
step:1988/2110 train_time:117441ms step_avg:59.07ms
step:1989/2110 train_time:117528ms step_avg:59.09ms
step:1990/2110 train_time:117616ms step_avg:59.10ms
step:1991/2110 train_time:117703ms step_avg:59.12ms
step:1992/2110 train_time:117792ms step_avg:59.13ms
step:1993/2110 train_time:117879ms step_avg:59.15ms
step:1994/2110 train_time:117968ms step_avg:59.16ms
step:1995/2110 train_time:118056ms step_avg:59.18ms
step:1996/2110 train_time:118146ms step_avg:59.19ms
step:1997/2110 train_time:118233ms step_avg:59.21ms
step:1998/2110 train_time:118322ms step_avg:59.22ms
step:1999/2110 train_time:118408ms step_avg:59.23ms
step:2000/2110 train_time:118496ms step_avg:59.25ms
step:2000/2110 val_loss:3.3045 train_time:118584ms step_avg:59.29ms
step:2001/2110 train_time:118631ms step_avg:59.29ms
step:2002/2110 train_time:118680ms step_avg:59.28ms
step:2003/2110 train_time:118770ms step_avg:59.30ms
step:2004/2110 train_time:118859ms step_avg:59.31ms
step:2005/2110 train_time:118946ms step_avg:59.32ms
step:2006/2110 train_time:119034ms step_avg:59.34ms
step:2007/2110 train_time:119120ms step_avg:59.35ms
step:2008/2110 train_time:119208ms step_avg:59.37ms
step:2009/2110 train_time:119294ms step_avg:59.38ms
step:2010/2110 train_time:119383ms step_avg:59.39ms
step:2011/2110 train_time:119468ms step_avg:59.41ms
step:2012/2110 train_time:119557ms step_avg:59.42ms
step:2013/2110 train_time:119648ms step_avg:59.44ms
step:2014/2110 train_time:119737ms step_avg:59.45ms
step:2015/2110 train_time:119828ms step_avg:59.47ms
step:2016/2110 train_time:119915ms step_avg:59.48ms
step:2017/2110 train_time:120003ms step_avg:59.50ms
step:2018/2110 train_time:120090ms step_avg:59.51ms
step:2019/2110 train_time:120177ms step_avg:59.52ms
step:2020/2110 train_time:120264ms step_avg:59.54ms
step:2021/2110 train_time:120351ms step_avg:59.55ms
step:2022/2110 train_time:120438ms step_avg:59.56ms
step:2023/2110 train_time:120526ms step_avg:59.58ms
step:2024/2110 train_time:120616ms step_avg:59.59ms
step:2025/2110 train_time:120705ms step_avg:59.61ms
step:2026/2110 train_time:120794ms step_avg:59.62ms
step:2027/2110 train_time:120883ms step_avg:59.64ms
step:2028/2110 train_time:120971ms step_avg:59.65ms
step:2029/2110 train_time:121059ms step_avg:59.66ms
step:2030/2110 train_time:121146ms step_avg:59.68ms
step:2031/2110 train_time:121233ms step_avg:59.69ms
step:2032/2110 train_time:121321ms step_avg:59.71ms
step:2033/2110 train_time:121409ms step_avg:59.72ms
step:2034/2110 train_time:121497ms step_avg:59.73ms
step:2035/2110 train_time:121586ms step_avg:59.75ms
step:2036/2110 train_time:121675ms step_avg:59.76ms
step:2037/2110 train_time:121764ms step_avg:59.78ms
step:2038/2110 train_time:121852ms step_avg:59.79ms
step:2039/2110 train_time:121941ms step_avg:59.80ms
step:2040/2110 train_time:122029ms step_avg:59.82ms
step:2041/2110 train_time:122116ms step_avg:59.83ms
step:2042/2110 train_time:122204ms step_avg:59.85ms
step:2043/2110 train_time:122291ms step_avg:59.86ms
step:2044/2110 train_time:122379ms step_avg:59.87ms
step:2045/2110 train_time:122467ms step_avg:59.89ms
step:2046/2110 train_time:122556ms step_avg:59.90ms
step:2047/2110 train_time:122644ms step_avg:59.91ms
step:2048/2110 train_time:122732ms step_avg:59.93ms
step:2049/2110 train_time:122821ms step_avg:59.94ms
step:2050/2110 train_time:122909ms step_avg:59.96ms
step:2051/2110 train_time:122998ms step_avg:59.97ms
step:2052/2110 train_time:123086ms step_avg:59.98ms
step:2053/2110 train_time:123173ms step_avg:60.00ms
step:2054/2110 train_time:123261ms step_avg:60.01ms
step:2055/2110 train_time:123349ms step_avg:60.02ms
step:2056/2110 train_time:123437ms step_avg:60.04ms
step:2057/2110 train_time:123525ms step_avg:60.05ms
step:2058/2110 train_time:123612ms step_avg:60.06ms
step:2059/2110 train_time:123702ms step_avg:60.08ms
step:2060/2110 train_time:123791ms step_avg:60.09ms
step:2061/2110 train_time:123879ms step_avg:60.11ms
step:2062/2110 train_time:123968ms step_avg:60.12ms
step:2063/2110 train_time:124055ms step_avg:60.13ms
step:2064/2110 train_time:124143ms step_avg:60.15ms
step:2065/2110 train_time:124230ms step_avg:60.16ms
step:2066/2110 train_time:124320ms step_avg:60.17ms
step:2067/2110 train_time:124407ms step_avg:60.19ms
step:2068/2110 train_time:124495ms step_avg:60.20ms
step:2069/2110 train_time:124583ms step_avg:60.21ms
step:2070/2110 train_time:124672ms step_avg:60.23ms
step:2071/2110 train_time:124760ms step_avg:60.24ms
step:2072/2110 train_time:124849ms step_avg:60.26ms
step:2073/2110 train_time:124938ms step_avg:60.27ms
step:2074/2110 train_time:125027ms step_avg:60.28ms
step:2075/2110 train_time:125115ms step_avg:60.30ms
step:2076/2110 train_time:125202ms step_avg:60.31ms
step:2077/2110 train_time:125291ms step_avg:60.32ms
step:2078/2110 train_time:125381ms step_avg:60.34ms
step:2079/2110 train_time:125470ms step_avg:60.35ms
step:2080/2110 train_time:125558ms step_avg:60.36ms
step:2081/2110 train_time:125646ms step_avg:60.38ms
step:2082/2110 train_time:125734ms step_avg:60.39ms
step:2083/2110 train_time:125822ms step_avg:60.40ms
step:2084/2110 train_time:125911ms step_avg:60.42ms
step:2085/2110 train_time:125999ms step_avg:60.43ms
step:2086/2110 train_time:126088ms step_avg:60.44ms
step:2087/2110 train_time:126176ms step_avg:60.46ms
step:2088/2110 train_time:126263ms step_avg:60.47ms
step:2089/2110 train_time:126351ms step_avg:60.48ms
step:2090/2110 train_time:126440ms step_avg:60.50ms
step:2091/2110 train_time:126528ms step_avg:60.51ms
step:2092/2110 train_time:126617ms step_avg:60.52ms
step:2093/2110 train_time:126705ms step_avg:60.54ms
step:2094/2110 train_time:126794ms step_avg:60.55ms
step:2095/2110 train_time:126884ms step_avg:60.56ms
step:2096/2110 train_time:126972ms step_avg:60.58ms
step:2097/2110 train_time:127060ms step_avg:60.59ms
step:2098/2110 train_time:127149ms step_avg:60.60ms
step:2099/2110 train_time:127237ms step_avg:60.62ms
step:2100/2110 train_time:127324ms step_avg:60.63ms
step:2101/2110 train_time:127412ms step_avg:60.64ms
step:2102/2110 train_time:127502ms step_avg:60.66ms
step:2103/2110 train_time:127590ms step_avg:60.67ms
step:2104/2110 train_time:127678ms step_avg:60.68ms
step:2105/2110 train_time:127767ms step_avg:60.70ms
step:2106/2110 train_time:127855ms step_avg:60.71ms
step:2107/2110 train_time:127944ms step_avg:60.72ms
step:2108/2110 train_time:128032ms step_avg:60.74ms
step:2109/2110 train_time:128120ms step_avg:60.75ms
step:2110/2110 train_time:128209ms step_avg:60.76ms
step:2110/2110 val_loss:3.2796 train_time:128299ms step_avg:60.80ms
peak memory allocated: 29892 MiB reserved: 44156 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 15 23:02:08 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   39C    P0            126W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   43C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   43C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   37C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   42C    P0            126W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   38C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              80      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              81      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:100ms step_avg:99.70ms
step:2/2110 train_time:136ms step_avg:68.01ms
step:3/2110 train_time:172ms step_avg:57.36ms
step:4/2110 train_time:206ms step_avg:51.38ms
step:5/2110 train_time:237ms step_avg:47.30ms
step:6/2110 train_time:428ms step_avg:71.41ms
step:7/2110 train_time:681ms step_avg:97.31ms
step:8/2110 train_time:715ms step_avg:89.32ms
step:9/2110 train_time:747ms step_avg:83.01ms
step:10/2110 train_time:780ms step_avg:78.05ms
step:11/2110 train_time:813ms step_avg:73.89ms
step:12/2110 train_time:846ms step_avg:70.54ms
step:13/2110 train_time:878ms step_avg:67.57ms
step:14/2110 train_time:913ms step_avg:65.22ms
step:15/2110 train_time:945ms step_avg:63.01ms
step:16/2110 train_time:979ms step_avg:61.16ms
step:17/2110 train_time:1010ms step_avg:59.42ms
step:18/2110 train_time:1043ms step_avg:57.96ms
step:19/2110 train_time:1076ms step_avg:56.62ms
step:20/2110 train_time:1109ms step_avg:55.43ms
step:21/2110 train_time:1141ms step_avg:54.36ms
step:22/2110 train_time:1175ms step_avg:53.42ms
step:23/2110 train_time:1207ms step_avg:52.49ms
step:24/2110 train_time:1241ms step_avg:51.70ms
step:25/2110 train_time:1273ms step_avg:50.91ms
step:26/2110 train_time:1307ms step_avg:50.25ms
step:27/2110 train_time:1339ms step_avg:49.58ms
step:28/2110 train_time:1372ms step_avg:49.01ms
step:29/2110 train_time:1405ms step_avg:48.44ms
step:30/2110 train_time:1439ms step_avg:47.95ms
step:31/2110 train_time:1471ms step_avg:47.46ms
step:32/2110 train_time:1504ms step_avg:47.00ms
step:33/2110 train_time:1537ms step_avg:46.56ms
step:34/2110 train_time:1571ms step_avg:46.22ms
step:35/2110 train_time:1605ms step_avg:45.86ms
step:36/2110 train_time:1641ms step_avg:45.58ms
step:37/2110 train_time:1672ms step_avg:45.20ms
step:38/2110 train_time:1707ms step_avg:44.93ms
step:39/2110 train_time:1739ms step_avg:44.59ms
step:40/2110 train_time:1773ms step_avg:44.32ms
step:41/2110 train_time:1806ms step_avg:44.04ms
step:42/2110 train_time:1840ms step_avg:43.80ms
step:43/2110 train_time:1872ms step_avg:43.54ms
step:44/2110 train_time:1908ms step_avg:43.36ms
step:45/2110 train_time:1939ms step_avg:43.09ms
step:46/2110 train_time:1973ms step_avg:42.89ms
step:47/2110 train_time:2006ms step_avg:42.68ms
step:48/2110 train_time:2040ms step_avg:42.50ms
step:49/2110 train_time:2071ms step_avg:42.27ms
step:50/2110 train_time:2104ms step_avg:42.09ms
step:51/2110 train_time:2137ms step_avg:41.90ms
step:52/2110 train_time:2170ms step_avg:41.74ms
step:53/2110 train_time:2203ms step_avg:41.56ms
step:54/2110 train_time:2237ms step_avg:41.42ms
step:55/2110 train_time:2269ms step_avg:41.25ms
step:56/2110 train_time:2302ms step_avg:41.12ms
step:57/2110 train_time:2336ms step_avg:40.98ms
step:58/2110 train_time:2370ms step_avg:40.85ms
step:59/2110 train_time:2402ms step_avg:40.70ms
step:60/2110 train_time:2435ms step_avg:40.59ms
step:61/2110 train_time:2467ms step_avg:40.45ms
step:62/2110 train_time:2500ms step_avg:40.32ms
step:63/2110 train_time:2533ms step_avg:40.20ms
step:64/2110 train_time:2567ms step_avg:40.11ms
step:65/2110 train_time:2599ms step_avg:39.99ms
step:66/2110 train_time:2633ms step_avg:39.89ms
step:67/2110 train_time:2666ms step_avg:39.79ms
step:68/2110 train_time:2701ms step_avg:39.71ms
step:69/2110 train_time:2733ms step_avg:39.61ms
step:70/2110 train_time:2768ms step_avg:39.54ms
step:71/2110 train_time:2799ms step_avg:39.42ms
step:72/2110 train_time:2833ms step_avg:39.34ms
step:73/2110 train_time:2865ms step_avg:39.25ms
step:74/2110 train_time:2898ms step_avg:39.17ms
step:75/2110 train_time:2932ms step_avg:39.09ms
step:76/2110 train_time:2965ms step_avg:39.01ms
step:77/2110 train_time:2998ms step_avg:38.93ms
step:78/2110 train_time:3032ms step_avg:38.87ms
step:79/2110 train_time:3064ms step_avg:38.79ms
step:80/2110 train_time:3099ms step_avg:38.73ms
step:81/2110 train_time:3131ms step_avg:38.65ms
step:82/2110 train_time:3166ms step_avg:38.61ms
step:83/2110 train_time:3196ms step_avg:38.51ms
step:84/2110 train_time:3231ms step_avg:38.47ms
step:85/2110 train_time:3262ms step_avg:38.38ms
step:86/2110 train_time:3296ms step_avg:38.32ms
step:87/2110 train_time:3328ms step_avg:38.26ms
step:88/2110 train_time:3362ms step_avg:38.20ms
step:89/2110 train_time:3394ms step_avg:38.14ms
step:90/2110 train_time:3428ms step_avg:38.09ms
step:91/2110 train_time:3460ms step_avg:38.02ms
step:92/2110 train_time:3493ms step_avg:37.97ms
step:93/2110 train_time:3527ms step_avg:37.92ms
step:94/2110 train_time:3560ms step_avg:37.87ms
step:95/2110 train_time:3592ms step_avg:37.82ms
step:96/2110 train_time:3627ms step_avg:37.78ms
step:97/2110 train_time:3658ms step_avg:37.71ms
step:98/2110 train_time:3691ms step_avg:37.67ms
step:99/2110 train_time:3724ms step_avg:37.62ms
step:100/2110 train_time:3758ms step_avg:37.58ms
step:101/2110 train_time:3791ms step_avg:37.53ms
step:102/2110 train_time:3824ms step_avg:37.49ms
step:103/2110 train_time:3857ms step_avg:37.44ms
step:104/2110 train_time:3890ms step_avg:37.40ms
step:105/2110 train_time:3923ms step_avg:37.36ms
step:106/2110 train_time:3957ms step_avg:37.33ms
step:107/2110 train_time:3989ms step_avg:37.28ms
step:108/2110 train_time:4024ms step_avg:37.26ms
step:109/2110 train_time:4056ms step_avg:37.21ms
step:110/2110 train_time:4089ms step_avg:37.17ms
step:111/2110 train_time:4122ms step_avg:37.13ms
step:112/2110 train_time:4155ms step_avg:37.10ms
step:113/2110 train_time:4187ms step_avg:37.06ms
step:114/2110 train_time:4221ms step_avg:37.03ms
step:115/2110 train_time:4255ms step_avg:37.00ms
step:116/2110 train_time:4290ms step_avg:36.98ms
step:117/2110 train_time:4320ms step_avg:36.93ms
step:118/2110 train_time:4355ms step_avg:36.91ms
step:119/2110 train_time:4387ms step_avg:36.86ms
step:120/2110 train_time:4421ms step_avg:36.84ms
step:121/2110 train_time:4452ms step_avg:36.80ms
step:122/2110 train_time:4486ms step_avg:36.77ms
step:123/2110 train_time:4518ms step_avg:36.73ms
step:124/2110 train_time:4552ms step_avg:36.71ms
step:125/2110 train_time:4584ms step_avg:36.67ms
step:126/2110 train_time:4618ms step_avg:36.65ms
step:127/2110 train_time:4650ms step_avg:36.61ms
step:128/2110 train_time:4683ms step_avg:36.58ms
step:129/2110 train_time:4715ms step_avg:36.55ms
step:130/2110 train_time:4749ms step_avg:36.53ms
step:131/2110 train_time:4781ms step_avg:36.50ms
step:132/2110 train_time:4815ms step_avg:36.48ms
step:133/2110 train_time:4847ms step_avg:36.44ms
step:134/2110 train_time:4880ms step_avg:36.42ms
step:135/2110 train_time:4913ms step_avg:36.39ms
step:136/2110 train_time:4947ms step_avg:36.38ms
step:137/2110 train_time:4979ms step_avg:36.34ms
step:138/2110 train_time:5013ms step_avg:36.32ms
step:139/2110 train_time:5045ms step_avg:36.30ms
step:140/2110 train_time:5078ms step_avg:36.27ms
step:141/2110 train_time:5111ms step_avg:36.25ms
step:142/2110 train_time:5145ms step_avg:36.23ms
step:143/2110 train_time:5176ms step_avg:36.20ms
step:144/2110 train_time:5210ms step_avg:36.18ms
step:145/2110 train_time:5244ms step_avg:36.16ms
step:146/2110 train_time:5277ms step_avg:36.14ms
step:147/2110 train_time:5310ms step_avg:36.12ms
step:148/2110 train_time:5343ms step_avg:36.10ms
step:149/2110 train_time:5374ms step_avg:36.07ms
step:150/2110 train_time:5408ms step_avg:36.05ms
step:151/2110 train_time:5440ms step_avg:36.03ms
step:152/2110 train_time:5475ms step_avg:36.02ms
step:153/2110 train_time:5507ms step_avg:35.99ms
step:154/2110 train_time:5541ms step_avg:35.98ms
step:155/2110 train_time:5572ms step_avg:35.95ms
step:156/2110 train_time:5605ms step_avg:35.93ms
step:157/2110 train_time:5637ms step_avg:35.91ms
step:158/2110 train_time:5672ms step_avg:35.90ms
step:159/2110 train_time:5704ms step_avg:35.87ms
step:160/2110 train_time:5736ms step_avg:35.85ms
step:161/2110 train_time:5770ms step_avg:35.84ms
step:162/2110 train_time:5803ms step_avg:35.82ms
step:163/2110 train_time:5836ms step_avg:35.80ms
step:164/2110 train_time:5868ms step_avg:35.78ms
step:165/2110 train_time:5901ms step_avg:35.76ms
step:166/2110 train_time:5935ms step_avg:35.75ms
step:167/2110 train_time:5967ms step_avg:35.73ms
step:168/2110 train_time:6000ms step_avg:35.72ms
step:169/2110 train_time:6033ms step_avg:35.70ms
step:170/2110 train_time:6067ms step_avg:35.69ms
step:171/2110 train_time:6100ms step_avg:35.67ms
step:172/2110 train_time:6134ms step_avg:35.66ms
step:173/2110 train_time:6165ms step_avg:35.64ms
step:174/2110 train_time:6200ms step_avg:35.63ms
step:175/2110 train_time:6232ms step_avg:35.61ms
step:176/2110 train_time:6265ms step_avg:35.60ms
step:177/2110 train_time:6298ms step_avg:35.58ms
step:178/2110 train_time:6332ms step_avg:35.57ms
step:179/2110 train_time:6364ms step_avg:35.56ms
step:180/2110 train_time:6399ms step_avg:35.55ms
step:181/2110 train_time:6432ms step_avg:35.54ms
step:182/2110 train_time:6464ms step_avg:35.52ms
step:183/2110 train_time:6497ms step_avg:35.51ms
step:184/2110 train_time:6531ms step_avg:35.49ms
step:185/2110 train_time:6561ms step_avg:35.47ms
step:186/2110 train_time:6597ms step_avg:35.47ms
step:187/2110 train_time:6628ms step_avg:35.44ms
step:188/2110 train_time:6662ms step_avg:35.44ms
step:189/2110 train_time:6694ms step_avg:35.42ms
step:190/2110 train_time:6727ms step_avg:35.41ms
step:191/2110 train_time:6760ms step_avg:35.40ms
step:192/2110 train_time:6793ms step_avg:35.38ms
step:193/2110 train_time:6825ms step_avg:35.37ms
step:194/2110 train_time:6859ms step_avg:35.36ms
step:195/2110 train_time:6892ms step_avg:35.34ms
step:196/2110 train_time:6926ms step_avg:35.34ms
step:197/2110 train_time:6959ms step_avg:35.33ms
step:198/2110 train_time:6991ms step_avg:35.31ms
step:199/2110 train_time:7024ms step_avg:35.30ms
step:200/2110 train_time:7057ms step_avg:35.29ms
step:201/2110 train_time:7089ms step_avg:35.27ms
step:202/2110 train_time:7123ms step_avg:35.26ms
step:203/2110 train_time:7155ms step_avg:35.25ms
step:204/2110 train_time:7189ms step_avg:35.24ms
step:205/2110 train_time:7221ms step_avg:35.23ms
step:206/2110 train_time:7256ms step_avg:35.22ms
step:207/2110 train_time:7287ms step_avg:35.20ms
step:208/2110 train_time:7321ms step_avg:35.20ms
step:209/2110 train_time:7353ms step_avg:35.18ms
step:210/2110 train_time:7387ms step_avg:35.17ms
step:211/2110 train_time:7420ms step_avg:35.16ms
step:212/2110 train_time:7455ms step_avg:35.16ms
step:213/2110 train_time:7486ms step_avg:35.15ms
step:214/2110 train_time:7520ms step_avg:35.14ms
step:215/2110 train_time:7551ms step_avg:35.12ms
step:216/2110 train_time:7584ms step_avg:35.11ms
step:217/2110 train_time:7617ms step_avg:35.10ms
step:218/2110 train_time:7650ms step_avg:35.09ms
step:219/2110 train_time:7683ms step_avg:35.08ms
step:220/2110 train_time:7716ms step_avg:35.07ms
step:221/2110 train_time:7749ms step_avg:35.06ms
step:222/2110 train_time:7783ms step_avg:35.06ms
step:223/2110 train_time:7815ms step_avg:35.04ms
step:224/2110 train_time:7848ms step_avg:35.04ms
step:225/2110 train_time:7881ms step_avg:35.03ms
step:226/2110 train_time:7914ms step_avg:35.02ms
step:227/2110 train_time:7946ms step_avg:35.01ms
step:228/2110 train_time:7980ms step_avg:35.00ms
step:229/2110 train_time:8011ms step_avg:34.98ms
step:230/2110 train_time:8045ms step_avg:34.98ms
step:231/2110 train_time:8079ms step_avg:34.98ms
step:232/2110 train_time:8112ms step_avg:34.96ms
step:233/2110 train_time:8143ms step_avg:34.95ms
step:234/2110 train_time:8179ms step_avg:34.95ms
step:235/2110 train_time:8210ms step_avg:34.94ms
step:236/2110 train_time:8244ms step_avg:34.93ms
step:237/2110 train_time:8276ms step_avg:34.92ms
step:238/2110 train_time:8309ms step_avg:34.91ms
step:239/2110 train_time:8341ms step_avg:34.90ms
step:240/2110 train_time:8377ms step_avg:34.90ms
step:241/2110 train_time:8408ms step_avg:34.89ms
step:242/2110 train_time:8440ms step_avg:34.88ms
step:243/2110 train_time:8473ms step_avg:34.87ms
step:244/2110 train_time:8506ms step_avg:34.86ms
step:245/2110 train_time:8539ms step_avg:34.85ms
step:246/2110 train_time:8572ms step_avg:34.85ms
step:247/2110 train_time:8605ms step_avg:34.84ms
step:248/2110 train_time:8639ms step_avg:34.83ms
step:249/2110 train_time:8672ms step_avg:34.83ms
step:250/2110 train_time:8705ms step_avg:34.82ms
step:250/2110 val_loss:4.3216 train_time:8737ms step_avg:34.95ms
step:251/2110 train_time:8770ms step_avg:34.94ms
step:252/2110 train_time:8802ms step_avg:34.93ms
step:253/2110 train_time:8833ms step_avg:34.91ms
step:254/2110 train_time:8864ms step_avg:34.90ms
step:255/2110 train_time:8892ms step_avg:34.87ms
step:256/2110 train_time:8926ms step_avg:34.87ms
step:257/2110 train_time:8950ms step_avg:34.82ms
step:258/2110 train_time:8983ms step_avg:34.82ms
step:259/2110 train_time:9017ms step_avg:34.81ms
step:260/2110 train_time:9050ms step_avg:34.81ms
step:261/2110 train_time:9082ms step_avg:34.80ms
step:262/2110 train_time:9115ms step_avg:34.79ms
step:263/2110 train_time:9146ms step_avg:34.78ms
step:264/2110 train_time:9179ms step_avg:34.77ms
step:265/2110 train_time:9212ms step_avg:34.76ms
step:266/2110 train_time:9245ms step_avg:34.76ms
step:267/2110 train_time:9277ms step_avg:34.75ms
step:268/2110 train_time:9311ms step_avg:34.74ms
step:269/2110 train_time:9342ms step_avg:34.73ms
step:270/2110 train_time:9375ms step_avg:34.72ms
step:271/2110 train_time:9408ms step_avg:34.72ms
step:272/2110 train_time:9442ms step_avg:34.71ms
step:273/2110 train_time:9474ms step_avg:34.70ms
step:274/2110 train_time:9508ms step_avg:34.70ms
step:275/2110 train_time:9538ms step_avg:34.68ms
step:276/2110 train_time:9573ms step_avg:34.68ms
step:277/2110 train_time:9604ms step_avg:34.67ms
step:278/2110 train_time:9637ms step_avg:34.67ms
step:279/2110 train_time:9670ms step_avg:34.66ms
step:280/2110 train_time:9703ms step_avg:34.66ms
step:281/2110 train_time:9736ms step_avg:34.65ms
step:282/2110 train_time:9770ms step_avg:34.65ms
step:283/2110 train_time:9803ms step_avg:34.64ms
step:284/2110 train_time:9837ms step_avg:34.64ms
step:285/2110 train_time:9869ms step_avg:34.63ms
step:286/2110 train_time:9904ms step_avg:34.63ms
step:287/2110 train_time:9938ms step_avg:34.63ms
step:288/2110 train_time:9974ms step_avg:34.63ms
step:289/2110 train_time:10005ms step_avg:34.62ms
step:290/2110 train_time:10038ms step_avg:34.61ms
step:291/2110 train_time:10070ms step_avg:34.61ms
step:292/2110 train_time:10104ms step_avg:34.60ms
step:293/2110 train_time:10137ms step_avg:34.60ms
step:294/2110 train_time:10168ms step_avg:34.59ms
step:295/2110 train_time:10201ms step_avg:34.58ms
step:296/2110 train_time:10233ms step_avg:34.57ms
step:297/2110 train_time:10267ms step_avg:34.57ms
step:298/2110 train_time:10300ms step_avg:34.56ms
step:299/2110 train_time:10333ms step_avg:34.56ms
step:300/2110 train_time:10365ms step_avg:34.55ms
step:301/2110 train_time:10398ms step_avg:34.54ms
step:302/2110 train_time:10432ms step_avg:34.54ms
step:303/2110 train_time:10462ms step_avg:34.53ms
step:304/2110 train_time:10497ms step_avg:34.53ms
step:305/2110 train_time:10529ms step_avg:34.52ms
step:306/2110 train_time:10562ms step_avg:34.52ms
step:307/2110 train_time:10593ms step_avg:34.51ms
step:308/2110 train_time:10627ms step_avg:34.50ms
step:309/2110 train_time:10659ms step_avg:34.50ms
step:310/2110 train_time:10694ms step_avg:34.50ms
step:311/2110 train_time:10725ms step_avg:34.49ms
step:312/2110 train_time:10760ms step_avg:34.49ms
step:313/2110 train_time:10792ms step_avg:34.48ms
step:314/2110 train_time:10825ms step_avg:34.47ms
step:315/2110 train_time:10857ms step_avg:34.47ms
step:316/2110 train_time:10891ms step_avg:34.46ms
step:317/2110 train_time:10924ms step_avg:34.46ms
step:318/2110 train_time:10957ms step_avg:34.46ms
step:319/2110 train_time:10990ms step_avg:34.45ms
step:320/2110 train_time:11026ms step_avg:34.46ms
step:321/2110 train_time:11058ms step_avg:34.45ms
step:322/2110 train_time:11090ms step_avg:34.44ms
step:323/2110 train_time:11122ms step_avg:34.43ms
step:324/2110 train_time:11157ms step_avg:34.44ms
step:325/2110 train_time:11189ms step_avg:34.43ms
step:326/2110 train_time:11223ms step_avg:34.43ms
step:327/2110 train_time:11255ms step_avg:34.42ms
step:328/2110 train_time:11288ms step_avg:34.42ms
step:329/2110 train_time:11321ms step_avg:34.41ms
step:330/2110 train_time:11354ms step_avg:34.41ms
step:331/2110 train_time:11386ms step_avg:34.40ms
step:332/2110 train_time:11419ms step_avg:34.40ms
step:333/2110 train_time:11452ms step_avg:34.39ms
step:334/2110 train_time:11484ms step_avg:34.38ms
step:335/2110 train_time:11517ms step_avg:34.38ms
step:336/2110 train_time:11551ms step_avg:34.38ms
step:337/2110 train_time:11582ms step_avg:34.37ms
step:338/2110 train_time:11617ms step_avg:34.37ms
step:339/2110 train_time:11648ms step_avg:34.36ms
step:340/2110 train_time:11682ms step_avg:34.36ms
step:341/2110 train_time:11714ms step_avg:34.35ms
step:342/2110 train_time:11747ms step_avg:34.35ms
step:343/2110 train_time:11780ms step_avg:34.34ms
step:344/2110 train_time:11814ms step_avg:34.34ms
step:345/2110 train_time:11846ms step_avg:34.34ms
step:346/2110 train_time:11879ms step_avg:34.33ms
step:347/2110 train_time:11912ms step_avg:34.33ms
step:348/2110 train_time:11948ms step_avg:34.33ms
step:349/2110 train_time:11979ms step_avg:34.32ms
step:350/2110 train_time:12013ms step_avg:34.32ms
step:351/2110 train_time:12045ms step_avg:34.32ms
step:352/2110 train_time:12078ms step_avg:34.31ms
step:353/2110 train_time:12111ms step_avg:34.31ms
step:354/2110 train_time:12145ms step_avg:34.31ms
step:355/2110 train_time:12177ms step_avg:34.30ms
step:356/2110 train_time:12209ms step_avg:34.30ms
step:357/2110 train_time:12243ms step_avg:34.29ms
step:358/2110 train_time:12275ms step_avg:34.29ms
step:359/2110 train_time:12308ms step_avg:34.29ms
step:360/2110 train_time:12341ms step_avg:34.28ms
step:361/2110 train_time:12374ms step_avg:34.28ms
step:362/2110 train_time:12406ms step_avg:34.27ms
step:363/2110 train_time:12440ms step_avg:34.27ms
step:364/2110 train_time:12472ms step_avg:34.26ms
step:365/2110 train_time:12506ms step_avg:34.26ms
step:366/2110 train_time:12538ms step_avg:34.26ms
step:367/2110 train_time:12571ms step_avg:34.25ms
step:368/2110 train_time:12604ms step_avg:34.25ms
step:369/2110 train_time:12637ms step_avg:34.25ms
step:370/2110 train_time:12669ms step_avg:34.24ms
step:371/2110 train_time:12703ms step_avg:34.24ms
step:372/2110 train_time:12735ms step_avg:34.23ms
step:373/2110 train_time:12769ms step_avg:34.23ms
step:374/2110 train_time:12802ms step_avg:34.23ms
step:375/2110 train_time:12835ms step_avg:34.23ms
step:376/2110 train_time:12868ms step_avg:34.22ms
step:377/2110 train_time:12901ms step_avg:34.22ms
step:378/2110 train_time:12933ms step_avg:34.22ms
step:379/2110 train_time:12967ms step_avg:34.21ms
step:380/2110 train_time:12999ms step_avg:34.21ms
step:381/2110 train_time:13033ms step_avg:34.21ms
step:382/2110 train_time:13065ms step_avg:34.20ms
step:383/2110 train_time:13099ms step_avg:34.20ms
step:384/2110 train_time:13131ms step_avg:34.20ms
step:385/2110 train_time:13165ms step_avg:34.19ms
step:386/2110 train_time:13197ms step_avg:34.19ms
step:387/2110 train_time:13231ms step_avg:34.19ms
step:388/2110 train_time:13263ms step_avg:34.18ms
step:389/2110 train_time:13296ms step_avg:34.18ms
step:390/2110 train_time:13329ms step_avg:34.18ms
step:391/2110 train_time:13363ms step_avg:34.18ms
step:392/2110 train_time:13395ms step_avg:34.17ms
step:393/2110 train_time:13428ms step_avg:34.17ms
step:394/2110 train_time:13461ms step_avg:34.16ms
step:395/2110 train_time:13494ms step_avg:34.16ms
step:396/2110 train_time:13527ms step_avg:34.16ms
step:397/2110 train_time:13560ms step_avg:34.16ms
step:398/2110 train_time:13592ms step_avg:34.15ms
step:399/2110 train_time:13625ms step_avg:34.15ms
step:400/2110 train_time:13658ms step_avg:34.15ms
step:401/2110 train_time:13691ms step_avg:34.14ms
step:402/2110 train_time:13724ms step_avg:34.14ms
step:403/2110 train_time:13757ms step_avg:34.14ms
step:404/2110 train_time:13790ms step_avg:34.13ms
step:405/2110 train_time:13823ms step_avg:34.13ms
step:406/2110 train_time:13855ms step_avg:34.13ms
step:407/2110 train_time:13889ms step_avg:34.12ms
step:408/2110 train_time:13921ms step_avg:34.12ms
step:409/2110 train_time:13955ms step_avg:34.12ms
step:410/2110 train_time:13987ms step_avg:34.12ms
step:411/2110 train_time:14020ms step_avg:34.11ms
step:412/2110 train_time:14053ms step_avg:34.11ms
step:413/2110 train_time:14087ms step_avg:34.11ms
step:414/2110 train_time:14119ms step_avg:34.10ms
step:415/2110 train_time:14153ms step_avg:34.10ms
step:416/2110 train_time:14185ms step_avg:34.10ms
step:417/2110 train_time:14218ms step_avg:34.10ms
step:418/2110 train_time:14251ms step_avg:34.09ms
step:419/2110 train_time:14285ms step_avg:34.09ms
step:420/2110 train_time:14318ms step_avg:34.09ms
step:421/2110 train_time:14351ms step_avg:34.09ms
step:422/2110 train_time:14384ms step_avg:34.08ms
step:423/2110 train_time:14417ms step_avg:34.08ms
step:424/2110 train_time:14449ms step_avg:34.08ms
step:425/2110 train_time:14482ms step_avg:34.08ms
step:426/2110 train_time:14515ms step_avg:34.07ms
step:427/2110 train_time:14549ms step_avg:34.07ms
step:428/2110 train_time:14581ms step_avg:34.07ms
step:429/2110 train_time:14615ms step_avg:34.07ms
step:430/2110 train_time:14647ms step_avg:34.06ms
step:431/2110 train_time:14680ms step_avg:34.06ms
step:432/2110 train_time:14712ms step_avg:34.06ms
step:433/2110 train_time:14746ms step_avg:34.05ms
step:434/2110 train_time:14779ms step_avg:34.05ms
step:435/2110 train_time:14812ms step_avg:34.05ms
step:436/2110 train_time:14844ms step_avg:34.05ms
step:437/2110 train_time:14877ms step_avg:34.04ms
step:438/2110 train_time:14910ms step_avg:34.04ms
step:439/2110 train_time:14943ms step_avg:34.04ms
step:440/2110 train_time:14976ms step_avg:34.04ms
step:441/2110 train_time:15009ms step_avg:34.03ms
step:442/2110 train_time:15042ms step_avg:34.03ms
step:443/2110 train_time:15075ms step_avg:34.03ms
step:444/2110 train_time:15108ms step_avg:34.03ms
step:445/2110 train_time:15141ms step_avg:34.03ms
step:446/2110 train_time:15174ms step_avg:34.02ms
step:447/2110 train_time:15207ms step_avg:34.02ms
step:448/2110 train_time:15240ms step_avg:34.02ms
step:449/2110 train_time:15273ms step_avg:34.02ms
step:450/2110 train_time:15305ms step_avg:34.01ms
step:451/2110 train_time:15339ms step_avg:34.01ms
step:452/2110 train_time:15372ms step_avg:34.01ms
step:453/2110 train_time:15405ms step_avg:34.01ms
step:454/2110 train_time:15439ms step_avg:34.01ms
step:455/2110 train_time:15471ms step_avg:34.00ms
step:456/2110 train_time:15503ms step_avg:34.00ms
step:457/2110 train_time:15537ms step_avg:34.00ms
step:458/2110 train_time:15570ms step_avg:34.00ms
step:459/2110 train_time:15603ms step_avg:33.99ms
step:460/2110 train_time:15635ms step_avg:33.99ms
step:461/2110 train_time:15669ms step_avg:33.99ms
step:462/2110 train_time:15701ms step_avg:33.99ms
step:463/2110 train_time:15735ms step_avg:33.98ms
step:464/2110 train_time:15767ms step_avg:33.98ms
step:465/2110 train_time:15801ms step_avg:33.98ms
step:466/2110 train_time:15833ms step_avg:33.98ms
step:467/2110 train_time:15867ms step_avg:33.98ms
step:468/2110 train_time:15899ms step_avg:33.97ms
step:469/2110 train_time:15933ms step_avg:33.97ms
step:470/2110 train_time:15965ms step_avg:33.97ms
step:471/2110 train_time:15999ms step_avg:33.97ms
step:472/2110 train_time:16032ms step_avg:33.97ms
step:473/2110 train_time:16065ms step_avg:33.96ms
step:474/2110 train_time:16097ms step_avg:33.96ms
step:475/2110 train_time:16131ms step_avg:33.96ms
step:476/2110 train_time:16163ms step_avg:33.96ms
step:477/2110 train_time:16197ms step_avg:33.96ms
step:478/2110 train_time:16229ms step_avg:33.95ms
step:479/2110 train_time:16263ms step_avg:33.95ms
step:480/2110 train_time:16295ms step_avg:33.95ms
step:481/2110 train_time:16328ms step_avg:33.95ms
step:482/2110 train_time:16361ms step_avg:33.94ms
step:483/2110 train_time:16394ms step_avg:33.94ms
step:484/2110 train_time:16427ms step_avg:33.94ms
step:485/2110 train_time:16460ms step_avg:33.94ms
step:486/2110 train_time:16493ms step_avg:33.94ms
step:487/2110 train_time:16527ms step_avg:33.94ms
step:488/2110 train_time:16559ms step_avg:33.93ms
step:489/2110 train_time:16593ms step_avg:33.93ms
step:490/2110 train_time:16625ms step_avg:33.93ms
step:491/2110 train_time:16659ms step_avg:33.93ms
step:492/2110 train_time:16691ms step_avg:33.93ms
step:493/2110 train_time:16724ms step_avg:33.92ms
step:494/2110 train_time:16757ms step_avg:33.92ms
step:495/2110 train_time:16790ms step_avg:33.92ms
step:496/2110 train_time:16823ms step_avg:33.92ms
step:497/2110 train_time:16856ms step_avg:33.92ms
step:498/2110 train_time:16889ms step_avg:33.91ms
step:499/2110 train_time:16922ms step_avg:33.91ms
step:500/2110 train_time:16954ms step_avg:33.91ms
step:500/2110 val_loss:4.0302 train_time:16990ms step_avg:33.98ms
step:501/2110 train_time:17017ms step_avg:33.97ms
step:502/2110 train_time:17043ms step_avg:33.95ms
step:503/2110 train_time:17068ms step_avg:33.93ms
step:504/2110 train_time:17096ms step_avg:33.92ms
step:505/2110 train_time:17128ms step_avg:33.92ms
step:506/2110 train_time:17163ms step_avg:33.92ms
step:507/2110 train_time:17197ms step_avg:33.92ms
step:508/2110 train_time:17229ms step_avg:33.92ms
step:509/2110 train_time:17264ms step_avg:33.92ms
step:510/2110 train_time:17297ms step_avg:33.92ms
step:511/2110 train_time:17330ms step_avg:33.91ms
step:512/2110 train_time:17362ms step_avg:33.91ms
step:513/2110 train_time:17395ms step_avg:33.91ms
step:514/2110 train_time:17428ms step_avg:33.91ms
step:515/2110 train_time:17461ms step_avg:33.91ms
step:516/2110 train_time:17494ms step_avg:33.90ms
step:517/2110 train_time:17527ms step_avg:33.90ms
step:518/2110 train_time:17559ms step_avg:33.90ms
step:519/2110 train_time:17592ms step_avg:33.90ms
step:520/2110 train_time:17624ms step_avg:33.89ms
step:521/2110 train_time:17657ms step_avg:33.89ms
step:522/2110 train_time:17689ms step_avg:33.89ms
step:523/2110 train_time:17722ms step_avg:33.89ms
step:524/2110 train_time:17754ms step_avg:33.88ms
step:525/2110 train_time:17788ms step_avg:33.88ms
step:526/2110 train_time:17820ms step_avg:33.88ms
step:527/2110 train_time:17853ms step_avg:33.88ms
step:528/2110 train_time:17885ms step_avg:33.87ms
step:529/2110 train_time:17918ms step_avg:33.87ms
step:530/2110 train_time:17951ms step_avg:33.87ms
step:531/2110 train_time:17984ms step_avg:33.87ms
step:532/2110 train_time:18017ms step_avg:33.87ms
step:533/2110 train_time:18051ms step_avg:33.87ms
step:534/2110 train_time:18084ms step_avg:33.86ms
step:535/2110 train_time:18117ms step_avg:33.86ms
step:536/2110 train_time:18150ms step_avg:33.86ms
step:537/2110 train_time:18184ms step_avg:33.86ms
step:538/2110 train_time:18216ms step_avg:33.86ms
step:539/2110 train_time:18250ms step_avg:33.86ms
step:540/2110 train_time:18283ms step_avg:33.86ms
step:541/2110 train_time:18316ms step_avg:33.86ms
step:542/2110 train_time:18350ms step_avg:33.86ms
step:543/2110 train_time:18383ms step_avg:33.85ms
step:544/2110 train_time:18416ms step_avg:33.85ms
step:545/2110 train_time:18449ms step_avg:33.85ms
step:546/2110 train_time:18481ms step_avg:33.85ms
step:547/2110 train_time:18514ms step_avg:33.85ms
step:548/2110 train_time:18547ms step_avg:33.84ms
step:549/2110 train_time:18580ms step_avg:33.84ms
step:550/2110 train_time:18612ms step_avg:33.84ms
step:551/2110 train_time:18645ms step_avg:33.84ms
step:552/2110 train_time:18677ms step_avg:33.84ms
step:553/2110 train_time:18710ms step_avg:33.83ms
step:554/2110 train_time:18743ms step_avg:33.83ms
step:555/2110 train_time:18776ms step_avg:33.83ms
step:556/2110 train_time:18809ms step_avg:33.83ms
step:557/2110 train_time:18841ms step_avg:33.83ms
step:558/2110 train_time:18874ms step_avg:33.82ms
step:559/2110 train_time:18907ms step_avg:33.82ms
step:560/2110 train_time:18939ms step_avg:33.82ms
step:561/2110 train_time:18973ms step_avg:33.82ms
step:562/2110 train_time:19005ms step_avg:33.82ms
step:563/2110 train_time:19039ms step_avg:33.82ms
step:564/2110 train_time:19072ms step_avg:33.82ms
step:565/2110 train_time:19105ms step_avg:33.81ms
step:566/2110 train_time:19138ms step_avg:33.81ms
step:567/2110 train_time:19171ms step_avg:33.81ms
step:568/2110 train_time:19204ms step_avg:33.81ms
step:569/2110 train_time:19237ms step_avg:33.81ms
step:570/2110 train_time:19270ms step_avg:33.81ms
step:571/2110 train_time:19303ms step_avg:33.81ms
step:572/2110 train_time:19336ms step_avg:33.80ms
step:573/2110 train_time:19370ms step_avg:33.80ms
step:574/2110 train_time:19402ms step_avg:33.80ms
step:575/2110 train_time:19436ms step_avg:33.80ms
step:576/2110 train_time:19468ms step_avg:33.80ms
step:577/2110 train_time:19502ms step_avg:33.80ms
step:578/2110 train_time:19534ms step_avg:33.80ms
step:579/2110 train_time:19567ms step_avg:33.80ms
step:580/2110 train_time:19600ms step_avg:33.79ms
step:581/2110 train_time:19633ms step_avg:33.79ms
step:582/2110 train_time:19665ms step_avg:33.79ms
step:583/2110 train_time:19699ms step_avg:33.79ms
step:584/2110 train_time:19731ms step_avg:33.79ms
step:585/2110 train_time:19764ms step_avg:33.78ms
step:586/2110 train_time:19796ms step_avg:33.78ms
step:587/2110 train_time:19830ms step_avg:33.78ms
step:588/2110 train_time:19863ms step_avg:33.78ms
step:589/2110 train_time:19896ms step_avg:33.78ms
step:590/2110 train_time:19929ms step_avg:33.78ms
step:591/2110 train_time:19961ms step_avg:33.78ms
step:592/2110 train_time:19994ms step_avg:33.77ms
step:593/2110 train_time:20027ms step_avg:33.77ms
step:594/2110 train_time:20060ms step_avg:33.77ms
step:595/2110 train_time:20093ms step_avg:33.77ms
step:596/2110 train_time:20127ms step_avg:33.77ms
step:597/2110 train_time:20160ms step_avg:33.77ms
step:598/2110 train_time:20192ms step_avg:33.77ms
step:599/2110 train_time:20226ms step_avg:33.77ms
step:600/2110 train_time:20259ms step_avg:33.76ms
step:601/2110 train_time:20292ms step_avg:33.76ms
step:602/2110 train_time:20325ms step_avg:33.76ms
step:603/2110 train_time:20358ms step_avg:33.76ms
step:604/2110 train_time:20391ms step_avg:33.76ms
step:605/2110 train_time:20424ms step_avg:33.76ms
step:606/2110 train_time:20456ms step_avg:33.76ms
step:607/2110 train_time:20490ms step_avg:33.76ms
step:608/2110 train_time:20522ms step_avg:33.75ms
step:609/2110 train_time:20556ms step_avg:33.75ms
step:610/2110 train_time:20589ms step_avg:33.75ms
step:611/2110 train_time:20622ms step_avg:33.75ms
step:612/2110 train_time:20655ms step_avg:33.75ms
step:613/2110 train_time:20688ms step_avg:33.75ms
step:614/2110 train_time:20720ms step_avg:33.75ms
step:615/2110 train_time:20754ms step_avg:33.75ms
step:616/2110 train_time:20787ms step_avg:33.74ms
step:617/2110 train_time:20820ms step_avg:33.74ms
step:618/2110 train_time:20852ms step_avg:33.74ms
step:619/2110 train_time:20886ms step_avg:33.74ms
step:620/2110 train_time:20919ms step_avg:33.74ms
step:621/2110 train_time:20952ms step_avg:33.74ms
step:622/2110 train_time:20984ms step_avg:33.74ms
step:623/2110 train_time:21018ms step_avg:33.74ms
step:624/2110 train_time:21050ms step_avg:33.73ms
step:625/2110 train_time:21083ms step_avg:33.73ms
step:626/2110 train_time:21116ms step_avg:33.73ms
step:627/2110 train_time:21149ms step_avg:33.73ms
step:628/2110 train_time:21182ms step_avg:33.73ms
step:629/2110 train_time:21215ms step_avg:33.73ms
step:630/2110 train_time:21248ms step_avg:33.73ms
step:631/2110 train_time:21282ms step_avg:33.73ms
step:632/2110 train_time:21315ms step_avg:33.73ms
step:633/2110 train_time:21348ms step_avg:33.72ms
step:634/2110 train_time:21382ms step_avg:33.72ms
step:635/2110 train_time:21414ms step_avg:33.72ms
step:636/2110 train_time:21447ms step_avg:33.72ms
step:637/2110 train_time:21480ms step_avg:33.72ms
step:638/2110 train_time:21513ms step_avg:33.72ms
step:639/2110 train_time:21546ms step_avg:33.72ms
step:640/2110 train_time:21579ms step_avg:33.72ms
step:641/2110 train_time:21612ms step_avg:33.72ms
step:642/2110 train_time:21645ms step_avg:33.71ms
step:643/2110 train_time:21678ms step_avg:33.71ms
step:644/2110 train_time:21711ms step_avg:33.71ms
step:645/2110 train_time:21744ms step_avg:33.71ms
step:646/2110 train_time:21776ms step_avg:33.71ms
step:647/2110 train_time:21809ms step_avg:33.71ms
step:648/2110 train_time:21842ms step_avg:33.71ms
step:649/2110 train_time:21875ms step_avg:33.71ms
step:650/2110 train_time:21908ms step_avg:33.70ms
step:651/2110 train_time:21941ms step_avg:33.70ms
step:652/2110 train_time:21973ms step_avg:33.70ms
step:653/2110 train_time:22007ms step_avg:33.70ms
step:654/2110 train_time:22039ms step_avg:33.70ms
step:655/2110 train_time:22073ms step_avg:33.70ms
step:656/2110 train_time:22105ms step_avg:33.70ms
step:657/2110 train_time:22138ms step_avg:33.70ms
step:658/2110 train_time:22171ms step_avg:33.70ms
step:659/2110 train_time:22205ms step_avg:33.69ms
step:660/2110 train_time:22238ms step_avg:33.69ms
step:661/2110 train_time:22271ms step_avg:33.69ms
step:662/2110 train_time:22304ms step_avg:33.69ms
step:663/2110 train_time:22337ms step_avg:33.69ms
step:664/2110 train_time:22370ms step_avg:33.69ms
step:665/2110 train_time:22403ms step_avg:33.69ms
step:666/2110 train_time:22435ms step_avg:33.69ms
step:667/2110 train_time:22469ms step_avg:33.69ms
step:668/2110 train_time:22502ms step_avg:33.68ms
step:669/2110 train_time:22535ms step_avg:33.68ms
step:670/2110 train_time:22568ms step_avg:33.68ms
step:671/2110 train_time:22601ms step_avg:33.68ms
step:672/2110 train_time:22633ms step_avg:33.68ms
step:673/2110 train_time:22667ms step_avg:33.68ms
step:674/2110 train_time:22699ms step_avg:33.68ms
step:675/2110 train_time:22732ms step_avg:33.68ms
step:676/2110 train_time:22765ms step_avg:33.68ms
step:677/2110 train_time:22798ms step_avg:33.68ms
step:678/2110 train_time:22831ms step_avg:33.67ms
step:679/2110 train_time:22864ms step_avg:33.67ms
step:680/2110 train_time:22896ms step_avg:33.67ms
step:681/2110 train_time:22930ms step_avg:33.67ms
step:682/2110 train_time:22962ms step_avg:33.67ms
step:683/2110 train_time:22995ms step_avg:33.67ms
step:684/2110 train_time:23028ms step_avg:33.67ms
step:685/2110 train_time:23061ms step_avg:33.67ms
step:686/2110 train_time:23094ms step_avg:33.66ms
step:687/2110 train_time:23127ms step_avg:33.66ms
step:688/2110 train_time:23159ms step_avg:33.66ms
step:689/2110 train_time:23193ms step_avg:33.66ms
step:690/2110 train_time:23226ms step_avg:33.66ms
step:691/2110 train_time:23260ms step_avg:33.66ms
step:692/2110 train_time:23317ms step_avg:33.70ms
step:693/2110 train_time:23377ms step_avg:33.73ms
step:694/2110 train_time:23435ms step_avg:33.77ms
step:695/2110 train_time:23496ms step_avg:33.81ms
step:696/2110 train_time:23555ms step_avg:33.84ms
step:697/2110 train_time:23615ms step_avg:33.88ms
step:698/2110 train_time:23673ms step_avg:33.92ms
step:699/2110 train_time:23732ms step_avg:33.95ms
step:700/2110 train_time:23791ms step_avg:33.99ms
step:701/2110 train_time:23850ms step_avg:34.02ms
step:702/2110 train_time:23909ms step_avg:34.06ms
step:703/2110 train_time:23969ms step_avg:34.09ms
step:704/2110 train_time:24027ms step_avg:34.13ms
step:705/2110 train_time:24087ms step_avg:34.17ms
step:706/2110 train_time:24146ms step_avg:34.20ms
step:707/2110 train_time:24206ms step_avg:34.24ms
step:708/2110 train_time:24264ms step_avg:34.27ms
step:709/2110 train_time:24324ms step_avg:34.31ms
step:710/2110 train_time:24382ms step_avg:34.34ms
step:711/2110 train_time:24442ms step_avg:34.38ms
step:712/2110 train_time:24500ms step_avg:34.41ms
step:713/2110 train_time:24560ms step_avg:34.45ms
step:714/2110 train_time:24618ms step_avg:34.48ms
step:715/2110 train_time:24679ms step_avg:34.52ms
step:716/2110 train_time:24737ms step_avg:34.55ms
step:717/2110 train_time:24797ms step_avg:34.58ms
step:718/2110 train_time:24855ms step_avg:34.62ms
step:719/2110 train_time:24916ms step_avg:34.65ms
step:720/2110 train_time:24974ms step_avg:34.69ms
step:721/2110 train_time:25034ms step_avg:34.72ms
step:722/2110 train_time:25093ms step_avg:34.76ms
step:723/2110 train_time:25153ms step_avg:34.79ms
step:724/2110 train_time:25212ms step_avg:34.82ms
step:725/2110 train_time:25271ms step_avg:34.86ms
step:726/2110 train_time:25331ms step_avg:34.89ms
step:727/2110 train_time:25391ms step_avg:34.93ms
step:728/2110 train_time:25451ms step_avg:34.96ms
step:729/2110 train_time:25511ms step_avg:34.99ms
step:730/2110 train_time:25570ms step_avg:35.03ms
step:731/2110 train_time:25629ms step_avg:35.06ms
step:732/2110 train_time:25689ms step_avg:35.09ms
step:733/2110 train_time:25749ms step_avg:35.13ms
step:734/2110 train_time:25809ms step_avg:35.16ms
step:735/2110 train_time:25867ms step_avg:35.19ms
step:736/2110 train_time:25925ms step_avg:35.22ms
step:737/2110 train_time:25985ms step_avg:35.26ms
step:738/2110 train_time:26044ms step_avg:35.29ms
step:739/2110 train_time:26105ms step_avg:35.32ms
step:740/2110 train_time:26163ms step_avg:35.36ms
step:741/2110 train_time:26223ms step_avg:35.39ms
step:742/2110 train_time:26282ms step_avg:35.42ms
step:743/2110 train_time:26341ms step_avg:35.45ms
step:744/2110 train_time:26399ms step_avg:35.48ms
step:745/2110 train_time:26459ms step_avg:35.52ms
step:746/2110 train_time:26518ms step_avg:35.55ms
step:747/2110 train_time:26578ms step_avg:35.58ms
step:748/2110 train_time:26636ms step_avg:35.61ms
step:749/2110 train_time:26696ms step_avg:35.64ms
step:750/2110 train_time:26755ms step_avg:35.67ms
step:750/2110 val_loss:3.9141 train_time:26817ms step_avg:35.76ms
step:751/2110 train_time:26845ms step_avg:35.75ms
step:752/2110 train_time:26876ms step_avg:35.74ms
step:753/2110 train_time:26940ms step_avg:35.78ms
step:754/2110 train_time:27002ms step_avg:35.81ms
step:755/2110 train_time:27063ms step_avg:35.85ms
step:756/2110 train_time:27122ms step_avg:35.88ms
step:757/2110 train_time:27181ms step_avg:35.91ms
step:758/2110 train_time:27239ms step_avg:35.94ms
step:759/2110 train_time:27298ms step_avg:35.97ms
step:760/2110 train_time:27355ms step_avg:35.99ms
step:761/2110 train_time:27414ms step_avg:36.02ms
step:762/2110 train_time:27471ms step_avg:36.05ms
step:763/2110 train_time:27530ms step_avg:36.08ms
step:764/2110 train_time:27588ms step_avg:36.11ms
step:765/2110 train_time:27647ms step_avg:36.14ms
step:766/2110 train_time:27704ms step_avg:36.17ms
step:767/2110 train_time:27764ms step_avg:36.20ms
step:768/2110 train_time:27823ms step_avg:36.23ms
step:769/2110 train_time:27885ms step_avg:36.26ms
step:770/2110 train_time:27945ms step_avg:36.29ms
step:771/2110 train_time:28005ms step_avg:36.32ms
step:772/2110 train_time:28064ms step_avg:36.35ms
step:773/2110 train_time:28124ms step_avg:36.38ms
step:774/2110 train_time:28182ms step_avg:36.41ms
step:775/2110 train_time:28242ms step_avg:36.44ms
step:776/2110 train_time:28300ms step_avg:36.47ms
step:777/2110 train_time:28360ms step_avg:36.50ms
step:778/2110 train_time:28419ms step_avg:36.53ms
step:779/2110 train_time:28478ms step_avg:36.56ms
step:780/2110 train_time:28535ms step_avg:36.58ms
step:781/2110 train_time:28594ms step_avg:36.61ms
step:782/2110 train_time:28652ms step_avg:36.64ms
step:783/2110 train_time:28712ms step_avg:36.67ms
step:784/2110 train_time:28770ms step_avg:36.70ms
step:785/2110 train_time:28831ms step_avg:36.73ms
step:786/2110 train_time:28890ms step_avg:36.76ms
step:787/2110 train_time:28951ms step_avg:36.79ms
step:788/2110 train_time:29009ms step_avg:36.81ms
step:789/2110 train_time:29070ms step_avg:36.84ms
step:790/2110 train_time:29129ms step_avg:36.87ms
step:791/2110 train_time:29189ms step_avg:36.90ms
step:792/2110 train_time:29247ms step_avg:36.93ms
step:793/2110 train_time:29308ms step_avg:36.96ms
step:794/2110 train_time:29366ms step_avg:36.98ms
step:795/2110 train_time:29426ms step_avg:37.01ms
step:796/2110 train_time:29485ms step_avg:37.04ms
step:797/2110 train_time:29543ms step_avg:37.07ms
step:798/2110 train_time:29602ms step_avg:37.10ms
step:799/2110 train_time:29661ms step_avg:37.12ms
step:800/2110 train_time:29720ms step_avg:37.15ms
step:801/2110 train_time:29779ms step_avg:37.18ms
step:802/2110 train_time:29839ms step_avg:37.21ms
step:803/2110 train_time:29898ms step_avg:37.23ms
step:804/2110 train_time:29958ms step_avg:37.26ms
step:805/2110 train_time:30017ms step_avg:37.29ms
step:806/2110 train_time:30076ms step_avg:37.32ms
step:807/2110 train_time:30136ms step_avg:37.34ms
step:808/2110 train_time:30195ms step_avg:37.37ms
step:809/2110 train_time:30255ms step_avg:37.40ms
step:810/2110 train_time:30313ms step_avg:37.42ms
step:811/2110 train_time:30372ms step_avg:37.45ms
step:812/2110 train_time:30430ms step_avg:37.48ms
step:813/2110 train_time:30490ms step_avg:37.50ms
step:814/2110 train_time:30548ms step_avg:37.53ms
step:815/2110 train_time:30608ms step_avg:37.56ms
step:816/2110 train_time:30666ms step_avg:37.58ms
step:817/2110 train_time:30725ms step_avg:37.61ms
step:818/2110 train_time:30785ms step_avg:37.63ms
step:819/2110 train_time:30844ms step_avg:37.66ms
step:820/2110 train_time:30903ms step_avg:37.69ms
step:821/2110 train_time:30962ms step_avg:37.71ms
step:822/2110 train_time:31021ms step_avg:37.74ms
step:823/2110 train_time:31081ms step_avg:37.77ms
step:824/2110 train_time:31141ms step_avg:37.79ms
step:825/2110 train_time:31200ms step_avg:37.82ms
step:826/2110 train_time:31259ms step_avg:37.84ms
step:827/2110 train_time:31318ms step_avg:37.87ms
step:828/2110 train_time:31377ms step_avg:37.89ms
step:829/2110 train_time:31436ms step_avg:37.92ms
step:830/2110 train_time:31495ms step_avg:37.95ms
step:831/2110 train_time:31555ms step_avg:37.97ms
step:832/2110 train_time:31613ms step_avg:38.00ms
step:833/2110 train_time:31672ms step_avg:38.02ms
step:834/2110 train_time:31731ms step_avg:38.05ms
step:835/2110 train_time:31791ms step_avg:38.07ms
step:836/2110 train_time:31850ms step_avg:38.10ms
step:837/2110 train_time:31910ms step_avg:38.12ms
step:838/2110 train_time:31969ms step_avg:38.15ms
step:839/2110 train_time:32029ms step_avg:38.18ms
step:840/2110 train_time:32087ms step_avg:38.20ms
step:841/2110 train_time:32147ms step_avg:38.22ms
step:842/2110 train_time:32206ms step_avg:38.25ms
step:843/2110 train_time:32266ms step_avg:38.27ms
step:844/2110 train_time:32325ms step_avg:38.30ms
step:845/2110 train_time:32384ms step_avg:38.32ms
step:846/2110 train_time:32444ms step_avg:38.35ms
step:847/2110 train_time:32503ms step_avg:38.37ms
step:848/2110 train_time:32562ms step_avg:38.40ms
step:849/2110 train_time:32620ms step_avg:38.42ms
step:850/2110 train_time:32679ms step_avg:38.45ms
step:851/2110 train_time:32738ms step_avg:38.47ms
step:852/2110 train_time:32798ms step_avg:38.49ms
step:853/2110 train_time:32857ms step_avg:38.52ms
step:854/2110 train_time:32915ms step_avg:38.54ms
step:855/2110 train_time:32975ms step_avg:38.57ms
step:856/2110 train_time:33034ms step_avg:38.59ms
step:857/2110 train_time:33094ms step_avg:38.62ms
step:858/2110 train_time:33153ms step_avg:38.64ms
step:859/2110 train_time:33213ms step_avg:38.66ms
step:860/2110 train_time:33271ms step_avg:38.69ms
step:861/2110 train_time:33331ms step_avg:38.71ms
step:862/2110 train_time:33390ms step_avg:38.74ms
step:863/2110 train_time:33450ms step_avg:38.76ms
step:864/2110 train_time:33508ms step_avg:38.78ms
step:865/2110 train_time:33568ms step_avg:38.81ms
step:866/2110 train_time:33626ms step_avg:38.83ms
step:867/2110 train_time:33686ms step_avg:38.85ms
step:868/2110 train_time:33746ms step_avg:38.88ms
step:869/2110 train_time:33805ms step_avg:38.90ms
step:870/2110 train_time:33863ms step_avg:38.92ms
step:871/2110 train_time:33922ms step_avg:38.95ms
step:872/2110 train_time:33982ms step_avg:38.97ms
step:873/2110 train_time:34042ms step_avg:38.99ms
step:874/2110 train_time:34101ms step_avg:39.02ms
step:875/2110 train_time:34160ms step_avg:39.04ms
step:876/2110 train_time:34219ms step_avg:39.06ms
step:877/2110 train_time:34279ms step_avg:39.09ms
step:878/2110 train_time:34338ms step_avg:39.11ms
step:879/2110 train_time:34397ms step_avg:39.13ms
step:880/2110 train_time:34456ms step_avg:39.15ms
step:881/2110 train_time:34517ms step_avg:39.18ms
step:882/2110 train_time:34575ms step_avg:39.20ms
step:883/2110 train_time:34636ms step_avg:39.22ms
step:884/2110 train_time:34694ms step_avg:39.25ms
step:885/2110 train_time:34754ms step_avg:39.27ms
step:886/2110 train_time:34812ms step_avg:39.29ms
step:887/2110 train_time:34871ms step_avg:39.31ms
step:888/2110 train_time:34930ms step_avg:39.34ms
step:889/2110 train_time:34990ms step_avg:39.36ms
step:890/2110 train_time:35048ms step_avg:39.38ms
step:891/2110 train_time:35107ms step_avg:39.40ms
step:892/2110 train_time:35166ms step_avg:39.42ms
step:893/2110 train_time:35226ms step_avg:39.45ms
step:894/2110 train_time:35285ms step_avg:39.47ms
step:895/2110 train_time:35345ms step_avg:39.49ms
step:896/2110 train_time:35404ms step_avg:39.51ms
step:897/2110 train_time:35464ms step_avg:39.54ms
step:898/2110 train_time:35523ms step_avg:39.56ms
step:899/2110 train_time:35582ms step_avg:39.58ms
step:900/2110 train_time:35641ms step_avg:39.60ms
step:901/2110 train_time:35700ms step_avg:39.62ms
step:902/2110 train_time:35758ms step_avg:39.64ms
step:903/2110 train_time:35817ms step_avg:39.66ms
step:904/2110 train_time:35875ms step_avg:39.68ms
step:905/2110 train_time:35935ms step_avg:39.71ms
step:906/2110 train_time:35993ms step_avg:39.73ms
step:907/2110 train_time:36053ms step_avg:39.75ms
step:908/2110 train_time:36111ms step_avg:39.77ms
step:909/2110 train_time:36171ms step_avg:39.79ms
step:910/2110 train_time:36229ms step_avg:39.81ms
step:911/2110 train_time:36290ms step_avg:39.84ms
step:912/2110 train_time:36349ms step_avg:39.86ms
step:913/2110 train_time:36409ms step_avg:39.88ms
step:914/2110 train_time:36468ms step_avg:39.90ms
step:915/2110 train_time:36527ms step_avg:39.92ms
step:916/2110 train_time:36586ms step_avg:39.94ms
step:917/2110 train_time:36645ms step_avg:39.96ms
step:918/2110 train_time:36703ms step_avg:39.98ms
step:919/2110 train_time:36763ms step_avg:40.00ms
step:920/2110 train_time:36822ms step_avg:40.02ms
step:921/2110 train_time:36881ms step_avg:40.04ms
step:922/2110 train_time:36940ms step_avg:40.07ms
step:923/2110 train_time:37000ms step_avg:40.09ms
step:924/2110 train_time:37059ms step_avg:40.11ms
step:925/2110 train_time:37118ms step_avg:40.13ms
step:926/2110 train_time:37176ms step_avg:40.15ms
step:927/2110 train_time:37237ms step_avg:40.17ms
step:928/2110 train_time:37295ms step_avg:40.19ms
step:929/2110 train_time:37356ms step_avg:40.21ms
step:930/2110 train_time:37414ms step_avg:40.23ms
step:931/2110 train_time:37475ms step_avg:40.25ms
step:932/2110 train_time:37533ms step_avg:40.27ms
step:933/2110 train_time:37593ms step_avg:40.29ms
step:934/2110 train_time:37651ms step_avg:40.31ms
step:935/2110 train_time:37710ms step_avg:40.33ms
step:936/2110 train_time:37768ms step_avg:40.35ms
step:937/2110 train_time:37828ms step_avg:40.37ms
step:938/2110 train_time:37887ms step_avg:40.39ms
step:939/2110 train_time:37946ms step_avg:40.41ms
step:940/2110 train_time:38005ms step_avg:40.43ms
step:941/2110 train_time:38064ms step_avg:40.45ms
step:942/2110 train_time:38123ms step_avg:40.47ms
step:943/2110 train_time:38182ms step_avg:40.49ms
step:944/2110 train_time:38242ms step_avg:40.51ms
step:945/2110 train_time:38301ms step_avg:40.53ms
step:946/2110 train_time:38360ms step_avg:40.55ms
step:947/2110 train_time:38420ms step_avg:40.57ms
step:948/2110 train_time:38479ms step_avg:40.59ms
step:949/2110 train_time:38539ms step_avg:40.61ms
step:950/2110 train_time:38599ms step_avg:40.63ms
step:951/2110 train_time:38658ms step_avg:40.65ms
step:952/2110 train_time:38716ms step_avg:40.67ms
step:953/2110 train_time:38776ms step_avg:40.69ms
step:954/2110 train_time:38834ms step_avg:40.71ms
step:955/2110 train_time:38894ms step_avg:40.73ms
step:956/2110 train_time:38952ms step_avg:40.74ms
step:957/2110 train_time:39012ms step_avg:40.76ms
step:958/2110 train_time:39071ms step_avg:40.78ms
step:959/2110 train_time:39130ms step_avg:40.80ms
step:960/2110 train_time:39189ms step_avg:40.82ms
step:961/2110 train_time:39250ms step_avg:40.84ms
step:962/2110 train_time:39308ms step_avg:40.86ms
step:963/2110 train_time:39368ms step_avg:40.88ms
step:964/2110 train_time:39427ms step_avg:40.90ms
step:965/2110 train_time:39487ms step_avg:40.92ms
step:966/2110 train_time:39546ms step_avg:40.94ms
step:967/2110 train_time:39605ms step_avg:40.96ms
step:968/2110 train_time:39663ms step_avg:40.97ms
step:969/2110 train_time:39723ms step_avg:40.99ms
step:970/2110 train_time:39782ms step_avg:41.01ms
step:971/2110 train_time:39841ms step_avg:41.03ms
step:972/2110 train_time:39900ms step_avg:41.05ms
step:973/2110 train_time:39960ms step_avg:41.07ms
step:974/2110 train_time:40018ms step_avg:41.09ms
step:975/2110 train_time:40077ms step_avg:41.11ms
step:976/2110 train_time:40136ms step_avg:41.12ms
step:977/2110 train_time:40196ms step_avg:41.14ms
step:978/2110 train_time:40255ms step_avg:41.16ms
step:979/2110 train_time:40315ms step_avg:41.18ms
step:980/2110 train_time:40373ms step_avg:41.20ms
step:981/2110 train_time:40434ms step_avg:41.22ms
step:982/2110 train_time:40493ms step_avg:41.24ms
step:983/2110 train_time:40553ms step_avg:41.25ms
step:984/2110 train_time:40612ms step_avg:41.27ms
step:985/2110 train_time:40673ms step_avg:41.29ms
step:986/2110 train_time:40730ms step_avg:41.31ms
step:987/2110 train_time:40790ms step_avg:41.33ms
step:988/2110 train_time:40848ms step_avg:41.34ms
step:989/2110 train_time:40907ms step_avg:41.36ms
step:990/2110 train_time:40966ms step_avg:41.38ms
step:991/2110 train_time:41025ms step_avg:41.40ms
step:992/2110 train_time:41084ms step_avg:41.42ms
step:993/2110 train_time:41144ms step_avg:41.43ms
step:994/2110 train_time:41203ms step_avg:41.45ms
step:995/2110 train_time:41262ms step_avg:41.47ms
step:996/2110 train_time:41321ms step_avg:41.49ms
step:997/2110 train_time:41381ms step_avg:41.51ms
step:998/2110 train_time:41441ms step_avg:41.52ms
step:999/2110 train_time:41500ms step_avg:41.54ms
step:1000/2110 train_time:41560ms step_avg:41.56ms
step:1000/2110 val_loss:3.7589 train_time:41620ms step_avg:41.62ms
step:1001/2110 train_time:41654ms step_avg:41.61ms
step:1002/2110 train_time:41687ms step_avg:41.60ms
step:1003/2110 train_time:41742ms step_avg:41.62ms
step:1004/2110 train_time:41809ms step_avg:41.64ms
step:1005/2110 train_time:41870ms step_avg:41.66ms
step:1006/2110 train_time:41928ms step_avg:41.68ms
step:1007/2110 train_time:41988ms step_avg:41.70ms
step:1008/2110 train_time:42045ms step_avg:41.71ms
step:1009/2110 train_time:42105ms step_avg:41.73ms
step:1010/2110 train_time:42162ms step_avg:41.74ms
step:1011/2110 train_time:42222ms step_avg:41.76ms
step:1012/2110 train_time:42280ms step_avg:41.78ms
step:1013/2110 train_time:42338ms step_avg:41.80ms
step:1014/2110 train_time:42397ms step_avg:41.81ms
step:1015/2110 train_time:42456ms step_avg:41.83ms
step:1016/2110 train_time:42513ms step_avg:41.84ms
step:1017/2110 train_time:42573ms step_avg:41.86ms
step:1018/2110 train_time:42632ms step_avg:41.88ms
step:1019/2110 train_time:42694ms step_avg:41.90ms
step:1020/2110 train_time:42755ms step_avg:41.92ms
step:1021/2110 train_time:42816ms step_avg:41.94ms
step:1022/2110 train_time:42875ms step_avg:41.95ms
step:1023/2110 train_time:42936ms step_avg:41.97ms
step:1024/2110 train_time:42995ms step_avg:41.99ms
step:1025/2110 train_time:43054ms step_avg:42.00ms
step:1026/2110 train_time:43113ms step_avg:42.02ms
step:1027/2110 train_time:43172ms step_avg:42.04ms
step:1028/2110 train_time:43231ms step_avg:42.05ms
step:1029/2110 train_time:43290ms step_avg:42.07ms
step:1030/2110 train_time:43348ms step_avg:42.09ms
step:1031/2110 train_time:43408ms step_avg:42.10ms
step:1032/2110 train_time:43465ms step_avg:42.12ms
step:1033/2110 train_time:43525ms step_avg:42.13ms
step:1034/2110 train_time:43584ms step_avg:42.15ms
step:1035/2110 train_time:43646ms step_avg:42.17ms
step:1036/2110 train_time:43706ms step_avg:42.19ms
step:1037/2110 train_time:43767ms step_avg:42.21ms
step:1038/2110 train_time:43826ms step_avg:42.22ms
step:1039/2110 train_time:43886ms step_avg:42.24ms
step:1040/2110 train_time:43945ms step_avg:42.26ms
step:1041/2110 train_time:44006ms step_avg:42.27ms
step:1042/2110 train_time:44064ms step_avg:42.29ms
step:1043/2110 train_time:44124ms step_avg:42.30ms
step:1044/2110 train_time:44182ms step_avg:42.32ms
step:1045/2110 train_time:44242ms step_avg:42.34ms
step:1046/2110 train_time:44300ms step_avg:42.35ms
step:1047/2110 train_time:44359ms step_avg:42.37ms
step:1048/2110 train_time:44417ms step_avg:42.38ms
step:1049/2110 train_time:44476ms step_avg:42.40ms
step:1050/2110 train_time:44535ms step_avg:42.41ms
step:1051/2110 train_time:44595ms step_avg:42.43ms
step:1052/2110 train_time:44654ms step_avg:42.45ms
step:1053/2110 train_time:44714ms step_avg:42.46ms
step:1054/2110 train_time:44773ms step_avg:42.48ms
step:1055/2110 train_time:44833ms step_avg:42.50ms
step:1056/2110 train_time:44892ms step_avg:42.51ms
step:1057/2110 train_time:44952ms step_avg:42.53ms
step:1058/2110 train_time:45012ms step_avg:42.54ms
step:1059/2110 train_time:45071ms step_avg:42.56ms
step:1060/2110 train_time:45129ms step_avg:42.57ms
step:1061/2110 train_time:45189ms step_avg:42.59ms
step:1062/2110 train_time:45247ms step_avg:42.61ms
step:1063/2110 train_time:45306ms step_avg:42.62ms
step:1064/2110 train_time:45364ms step_avg:42.64ms
step:1065/2110 train_time:45424ms step_avg:42.65ms
step:1066/2110 train_time:45486ms step_avg:42.67ms
step:1067/2110 train_time:45542ms step_avg:42.68ms
step:1068/2110 train_time:45600ms step_avg:42.70ms
step:1069/2110 train_time:45661ms step_avg:42.71ms
step:1070/2110 train_time:45719ms step_avg:42.73ms
step:1071/2110 train_time:45779ms step_avg:42.74ms
step:1072/2110 train_time:45838ms step_avg:42.76ms
step:1073/2110 train_time:45898ms step_avg:42.78ms
step:1074/2110 train_time:45957ms step_avg:42.79ms
step:1075/2110 train_time:46017ms step_avg:42.81ms
step:1076/2110 train_time:46076ms step_avg:42.82ms
step:1077/2110 train_time:46135ms step_avg:42.84ms
step:1078/2110 train_time:46195ms step_avg:42.85ms
step:1079/2110 train_time:46254ms step_avg:42.87ms
step:1080/2110 train_time:46314ms step_avg:42.88ms
step:1081/2110 train_time:46372ms step_avg:42.90ms
step:1082/2110 train_time:46431ms step_avg:42.91ms
step:1083/2110 train_time:46491ms step_avg:42.93ms
step:1084/2110 train_time:46550ms step_avg:42.94ms
step:1085/2110 train_time:46610ms step_avg:42.96ms
step:1086/2110 train_time:46668ms step_avg:42.97ms
step:1087/2110 train_time:46729ms step_avg:42.99ms
step:1088/2110 train_time:46788ms step_avg:43.00ms
step:1089/2110 train_time:46848ms step_avg:43.02ms
step:1090/2110 train_time:46906ms step_avg:43.03ms
step:1091/2110 train_time:46966ms step_avg:43.05ms
step:1092/2110 train_time:47025ms step_avg:43.06ms
step:1093/2110 train_time:47085ms step_avg:43.08ms
step:1094/2110 train_time:47144ms step_avg:43.09ms
step:1095/2110 train_time:47203ms step_avg:43.11ms
step:1096/2110 train_time:47262ms step_avg:43.12ms
step:1097/2110 train_time:47322ms step_avg:43.14ms
step:1098/2110 train_time:47380ms step_avg:43.15ms
step:1099/2110 train_time:47440ms step_avg:43.17ms
step:1100/2110 train_time:47499ms step_avg:43.18ms
step:1101/2110 train_time:47558ms step_avg:43.20ms
step:1102/2110 train_time:47617ms step_avg:43.21ms
step:1103/2110 train_time:47676ms step_avg:43.22ms
step:1104/2110 train_time:47735ms step_avg:43.24ms
step:1105/2110 train_time:47795ms step_avg:43.25ms
step:1106/2110 train_time:47854ms step_avg:43.27ms
step:1107/2110 train_time:47914ms step_avg:43.28ms
step:1108/2110 train_time:47973ms step_avg:43.30ms
step:1109/2110 train_time:48032ms step_avg:43.31ms
step:1110/2110 train_time:48091ms step_avg:43.33ms
step:1111/2110 train_time:48151ms step_avg:43.34ms
step:1112/2110 train_time:48210ms step_avg:43.35ms
step:1113/2110 train_time:48269ms step_avg:43.37ms
step:1114/2110 train_time:48327ms step_avg:43.38ms
step:1115/2110 train_time:48387ms step_avg:43.40ms
step:1116/2110 train_time:48445ms step_avg:43.41ms
step:1117/2110 train_time:48506ms step_avg:43.43ms
step:1118/2110 train_time:48564ms step_avg:43.44ms
step:1119/2110 train_time:48624ms step_avg:43.45ms
step:1120/2110 train_time:48682ms step_avg:43.47ms
step:1121/2110 train_time:48742ms step_avg:43.48ms
step:1122/2110 train_time:48801ms step_avg:43.49ms
step:1123/2110 train_time:48861ms step_avg:43.51ms
step:1124/2110 train_time:48919ms step_avg:43.52ms
step:1125/2110 train_time:48978ms step_avg:43.54ms
step:1126/2110 train_time:49037ms step_avg:43.55ms
step:1127/2110 train_time:49097ms step_avg:43.56ms
step:1128/2110 train_time:49155ms step_avg:43.58ms
step:1129/2110 train_time:49215ms step_avg:43.59ms
step:1130/2110 train_time:49274ms step_avg:43.61ms
step:1131/2110 train_time:49334ms step_avg:43.62ms
step:1132/2110 train_time:49393ms step_avg:43.63ms
step:1133/2110 train_time:49453ms step_avg:43.65ms
step:1134/2110 train_time:49512ms step_avg:43.66ms
step:1135/2110 train_time:49572ms step_avg:43.68ms
step:1136/2110 train_time:49630ms step_avg:43.69ms
step:1137/2110 train_time:49690ms step_avg:43.70ms
step:1138/2110 train_time:49748ms step_avg:43.72ms
step:1139/2110 train_time:49808ms step_avg:43.73ms
step:1140/2110 train_time:49867ms step_avg:43.74ms
step:1141/2110 train_time:49927ms step_avg:43.76ms
step:1142/2110 train_time:49987ms step_avg:43.77ms
step:1143/2110 train_time:50048ms step_avg:43.79ms
step:1144/2110 train_time:50106ms step_avg:43.80ms
step:1145/2110 train_time:50167ms step_avg:43.81ms
step:1146/2110 train_time:50225ms step_avg:43.83ms
step:1147/2110 train_time:50286ms step_avg:43.84ms
step:1148/2110 train_time:50345ms step_avg:43.85ms
step:1149/2110 train_time:50406ms step_avg:43.87ms
step:1150/2110 train_time:50465ms step_avg:43.88ms
step:1151/2110 train_time:50525ms step_avg:43.90ms
step:1152/2110 train_time:50584ms step_avg:43.91ms
step:1153/2110 train_time:50643ms step_avg:43.92ms
step:1154/2110 train_time:50702ms step_avg:43.94ms
step:1155/2110 train_time:50761ms step_avg:43.95ms
step:1156/2110 train_time:50820ms step_avg:43.96ms
step:1157/2110 train_time:50881ms step_avg:43.98ms
step:1158/2110 train_time:50940ms step_avg:43.99ms
step:1159/2110 train_time:51001ms step_avg:44.00ms
step:1160/2110 train_time:51060ms step_avg:44.02ms
step:1161/2110 train_time:51120ms step_avg:44.03ms
step:1162/2110 train_time:51179ms step_avg:44.04ms
step:1163/2110 train_time:51239ms step_avg:44.06ms
step:1164/2110 train_time:51299ms step_avg:44.07ms
step:1165/2110 train_time:51359ms step_avg:44.09ms
step:1166/2110 train_time:51418ms step_avg:44.10ms
step:1167/2110 train_time:51479ms step_avg:44.11ms
step:1168/2110 train_time:51538ms step_avg:44.13ms
step:1169/2110 train_time:51598ms step_avg:44.14ms
step:1170/2110 train_time:51658ms step_avg:44.15ms
step:1171/2110 train_time:51717ms step_avg:44.16ms
step:1172/2110 train_time:51776ms step_avg:44.18ms
step:1173/2110 train_time:51836ms step_avg:44.19ms
step:1174/2110 train_time:51897ms step_avg:44.21ms
step:1175/2110 train_time:51957ms step_avg:44.22ms
step:1176/2110 train_time:52016ms step_avg:44.23ms
step:1177/2110 train_time:52076ms step_avg:44.24ms
step:1178/2110 train_time:52135ms step_avg:44.26ms
step:1179/2110 train_time:52195ms step_avg:44.27ms
step:1180/2110 train_time:52254ms step_avg:44.28ms
step:1181/2110 train_time:52314ms step_avg:44.30ms
step:1182/2110 train_time:52374ms step_avg:44.31ms
step:1183/2110 train_time:52434ms step_avg:44.32ms
step:1184/2110 train_time:52493ms step_avg:44.34ms
step:1185/2110 train_time:52553ms step_avg:44.35ms
step:1186/2110 train_time:52616ms step_avg:44.36ms
step:1187/2110 train_time:52674ms step_avg:44.38ms
step:1188/2110 train_time:52735ms step_avg:44.39ms
step:1189/2110 train_time:52793ms step_avg:44.40ms
step:1190/2110 train_time:52854ms step_avg:44.42ms
step:1191/2110 train_time:52913ms step_avg:44.43ms
step:1192/2110 train_time:52975ms step_avg:44.44ms
step:1193/2110 train_time:53033ms step_avg:44.45ms
step:1194/2110 train_time:53095ms step_avg:44.47ms
step:1195/2110 train_time:53153ms step_avg:44.48ms
step:1196/2110 train_time:53215ms step_avg:44.49ms
step:1197/2110 train_time:53273ms step_avg:44.51ms
step:1198/2110 train_time:53335ms step_avg:44.52ms
step:1199/2110 train_time:53393ms step_avg:44.53ms
step:1200/2110 train_time:53454ms step_avg:44.55ms
step:1201/2110 train_time:53512ms step_avg:44.56ms
step:1202/2110 train_time:53573ms step_avg:44.57ms
step:1203/2110 train_time:53630ms step_avg:44.58ms
step:1204/2110 train_time:53690ms step_avg:44.59ms
step:1205/2110 train_time:53750ms step_avg:44.61ms
step:1206/2110 train_time:53811ms step_avg:44.62ms
step:1207/2110 train_time:53870ms step_avg:44.63ms
step:1208/2110 train_time:53930ms step_avg:44.64ms
step:1209/2110 train_time:53989ms step_avg:44.66ms
step:1210/2110 train_time:54049ms step_avg:44.67ms
step:1211/2110 train_time:54109ms step_avg:44.68ms
step:1212/2110 train_time:54168ms step_avg:44.69ms
step:1213/2110 train_time:54228ms step_avg:44.71ms
step:1214/2110 train_time:54288ms step_avg:44.72ms
step:1215/2110 train_time:54347ms step_avg:44.73ms
step:1216/2110 train_time:54408ms step_avg:44.74ms
step:1217/2110 train_time:54467ms step_avg:44.76ms
step:1218/2110 train_time:54527ms step_avg:44.77ms
step:1219/2110 train_time:54586ms step_avg:44.78ms
step:1220/2110 train_time:54646ms step_avg:44.79ms
step:1221/2110 train_time:54705ms step_avg:44.80ms
step:1222/2110 train_time:54766ms step_avg:44.82ms
step:1223/2110 train_time:54825ms step_avg:44.83ms
step:1224/2110 train_time:54885ms step_avg:44.84ms
step:1225/2110 train_time:54944ms step_avg:44.85ms
step:1226/2110 train_time:55005ms step_avg:44.87ms
step:1227/2110 train_time:55064ms step_avg:44.88ms
step:1228/2110 train_time:55124ms step_avg:44.89ms
step:1229/2110 train_time:55183ms step_avg:44.90ms
step:1230/2110 train_time:55244ms step_avg:44.91ms
step:1231/2110 train_time:55303ms step_avg:44.93ms
step:1232/2110 train_time:55363ms step_avg:44.94ms
step:1233/2110 train_time:55423ms step_avg:44.95ms
step:1234/2110 train_time:55480ms step_avg:44.96ms
step:1235/2110 train_time:55540ms step_avg:44.97ms
step:1236/2110 train_time:55600ms step_avg:44.98ms
step:1237/2110 train_time:55659ms step_avg:45.00ms
step:1238/2110 train_time:55719ms step_avg:45.01ms
step:1239/2110 train_time:55778ms step_avg:45.02ms
step:1240/2110 train_time:55837ms step_avg:45.03ms
step:1241/2110 train_time:55898ms step_avg:45.04ms
step:1242/2110 train_time:55957ms step_avg:45.05ms
step:1243/2110 train_time:56017ms step_avg:45.07ms
step:1244/2110 train_time:56077ms step_avg:45.08ms
step:1245/2110 train_time:56137ms step_avg:45.09ms
step:1246/2110 train_time:56197ms step_avg:45.10ms
step:1247/2110 train_time:56257ms step_avg:45.11ms
step:1248/2110 train_time:56316ms step_avg:45.13ms
step:1249/2110 train_time:56377ms step_avg:45.14ms
step:1250/2110 train_time:56436ms step_avg:45.15ms
step:1250/2110 val_loss:3.5941 train_time:56499ms step_avg:45.20ms
step:1251/2110 train_time:56543ms step_avg:45.20ms
step:1252/2110 train_time:56579ms step_avg:45.19ms
step:1253/2110 train_time:56625ms step_avg:45.19ms
step:1254/2110 train_time:56686ms step_avg:45.20ms
step:1255/2110 train_time:56748ms step_avg:45.22ms
step:1256/2110 train_time:56807ms step_avg:45.23ms
step:1257/2110 train_time:56866ms step_avg:45.24ms
step:1258/2110 train_time:56924ms step_avg:45.25ms
step:1259/2110 train_time:56984ms step_avg:45.26ms
step:1260/2110 train_time:57042ms step_avg:45.27ms
step:1261/2110 train_time:57100ms step_avg:45.28ms
step:1262/2110 train_time:57159ms step_avg:45.29ms
step:1263/2110 train_time:57218ms step_avg:45.30ms
step:1264/2110 train_time:57276ms step_avg:45.31ms
step:1265/2110 train_time:57336ms step_avg:45.32ms
step:1266/2110 train_time:57395ms step_avg:45.34ms
step:1267/2110 train_time:57456ms step_avg:45.35ms
step:1268/2110 train_time:57517ms step_avg:45.36ms
step:1269/2110 train_time:57579ms step_avg:45.37ms
step:1270/2110 train_time:57641ms step_avg:45.39ms
step:1271/2110 train_time:57702ms step_avg:45.40ms
step:1272/2110 train_time:57762ms step_avg:45.41ms
step:1273/2110 train_time:57821ms step_avg:45.42ms
step:1274/2110 train_time:57881ms step_avg:45.43ms
step:1275/2110 train_time:57940ms step_avg:45.44ms
step:1276/2110 train_time:57999ms step_avg:45.45ms
step:1277/2110 train_time:58058ms step_avg:45.46ms
step:1278/2110 train_time:58117ms step_avg:45.47ms
step:1279/2110 train_time:58176ms step_avg:45.49ms
step:1280/2110 train_time:58234ms step_avg:45.50ms
step:1281/2110 train_time:58293ms step_avg:45.51ms
step:1282/2110 train_time:58352ms step_avg:45.52ms
step:1283/2110 train_time:58412ms step_avg:45.53ms
step:1284/2110 train_time:58471ms step_avg:45.54ms
step:1285/2110 train_time:58533ms step_avg:45.55ms
step:1286/2110 train_time:58594ms step_avg:45.56ms
step:1287/2110 train_time:58656ms step_avg:45.58ms
step:1288/2110 train_time:58716ms step_avg:45.59ms
step:1289/2110 train_time:58777ms step_avg:45.60ms
step:1290/2110 train_time:58837ms step_avg:45.61ms
step:1291/2110 train_time:58897ms step_avg:45.62ms
step:1292/2110 train_time:58957ms step_avg:45.63ms
step:1293/2110 train_time:59015ms step_avg:45.64ms
step:1294/2110 train_time:59074ms step_avg:45.65ms
step:1295/2110 train_time:59133ms step_avg:45.66ms
step:1296/2110 train_time:59192ms step_avg:45.67ms
step:1297/2110 train_time:59251ms step_avg:45.68ms
step:1298/2110 train_time:59310ms step_avg:45.69ms
step:1299/2110 train_time:59369ms step_avg:45.70ms
step:1300/2110 train_time:59428ms step_avg:45.71ms
step:1301/2110 train_time:59489ms step_avg:45.73ms
step:1302/2110 train_time:59548ms step_avg:45.74ms
step:1303/2110 train_time:59609ms step_avg:45.75ms
step:1304/2110 train_time:59668ms step_avg:45.76ms
step:1305/2110 train_time:59730ms step_avg:45.77ms
step:1306/2110 train_time:59790ms step_avg:45.78ms
step:1307/2110 train_time:59851ms step_avg:45.79ms
step:1308/2110 train_time:59910ms step_avg:45.80ms
step:1309/2110 train_time:59970ms step_avg:45.81ms
step:1310/2110 train_time:60029ms step_avg:45.82ms
step:1311/2110 train_time:60089ms step_avg:45.83ms
step:1312/2110 train_time:60147ms step_avg:45.84ms
step:1313/2110 train_time:60206ms step_avg:45.85ms
step:1314/2110 train_time:60265ms step_avg:45.86ms
step:1315/2110 train_time:60324ms step_avg:45.87ms
step:1316/2110 train_time:60383ms step_avg:45.88ms
step:1317/2110 train_time:60443ms step_avg:45.89ms
step:1318/2110 train_time:60502ms step_avg:45.90ms
step:1319/2110 train_time:60563ms step_avg:45.92ms
step:1320/2110 train_time:60622ms step_avg:45.93ms
step:1321/2110 train_time:60684ms step_avg:45.94ms
step:1322/2110 train_time:60743ms step_avg:45.95ms
step:1323/2110 train_time:60804ms step_avg:45.96ms
step:1324/2110 train_time:60863ms step_avg:45.97ms
step:1325/2110 train_time:60924ms step_avg:45.98ms
step:1326/2110 train_time:60983ms step_avg:45.99ms
step:1327/2110 train_time:61044ms step_avg:46.00ms
step:1328/2110 train_time:61103ms step_avg:46.01ms
step:1329/2110 train_time:61162ms step_avg:46.02ms
step:1330/2110 train_time:61221ms step_avg:46.03ms
step:1331/2110 train_time:61280ms step_avg:46.04ms
step:1332/2110 train_time:61340ms step_avg:46.05ms
step:1333/2110 train_time:61400ms step_avg:46.06ms
step:1334/2110 train_time:61459ms step_avg:46.07ms
step:1335/2110 train_time:61521ms step_avg:46.08ms
step:1336/2110 train_time:61580ms step_avg:46.09ms
step:1337/2110 train_time:61641ms step_avg:46.10ms
step:1338/2110 train_time:61701ms step_avg:46.11ms
step:1339/2110 train_time:61760ms step_avg:46.12ms
step:1340/2110 train_time:61820ms step_avg:46.13ms
step:1341/2110 train_time:61880ms step_avg:46.14ms
step:1342/2110 train_time:61940ms step_avg:46.15ms
step:1343/2110 train_time:61999ms step_avg:46.16ms
step:1344/2110 train_time:62058ms step_avg:46.17ms
step:1345/2110 train_time:62118ms step_avg:46.18ms
step:1346/2110 train_time:62177ms step_avg:46.19ms
step:1347/2110 train_time:62236ms step_avg:46.20ms
step:1348/2110 train_time:62296ms step_avg:46.21ms
step:1349/2110 train_time:62355ms step_avg:46.22ms
step:1350/2110 train_time:62415ms step_avg:46.23ms
step:1351/2110 train_time:62474ms step_avg:46.24ms
step:1352/2110 train_time:62533ms step_avg:46.25ms
step:1353/2110 train_time:62593ms step_avg:46.26ms
step:1354/2110 train_time:62652ms step_avg:46.27ms
step:1355/2110 train_time:62712ms step_avg:46.28ms
step:1356/2110 train_time:62771ms step_avg:46.29ms
step:1357/2110 train_time:62832ms step_avg:46.30ms
step:1358/2110 train_time:62892ms step_avg:46.31ms
step:1359/2110 train_time:62951ms step_avg:46.32ms
step:1360/2110 train_time:63009ms step_avg:46.33ms
step:1361/2110 train_time:63070ms step_avg:46.34ms
step:1362/2110 train_time:63128ms step_avg:46.35ms
step:1363/2110 train_time:63189ms step_avg:46.36ms
step:1364/2110 train_time:63248ms step_avg:46.37ms
step:1365/2110 train_time:63310ms step_avg:46.38ms
step:1366/2110 train_time:63368ms step_avg:46.39ms
step:1367/2110 train_time:63429ms step_avg:46.40ms
step:1368/2110 train_time:63488ms step_avg:46.41ms
step:1369/2110 train_time:63549ms step_avg:46.42ms
step:1370/2110 train_time:63607ms step_avg:46.43ms
step:1371/2110 train_time:63668ms step_avg:46.44ms
step:1372/2110 train_time:63727ms step_avg:46.45ms
step:1373/2110 train_time:63787ms step_avg:46.46ms
step:1374/2110 train_time:63846ms step_avg:46.47ms
step:1375/2110 train_time:63906ms step_avg:46.48ms
step:1376/2110 train_time:63966ms step_avg:46.49ms
step:1377/2110 train_time:64026ms step_avg:46.50ms
step:1378/2110 train_time:64085ms step_avg:46.51ms
step:1379/2110 train_time:64146ms step_avg:46.52ms
step:1380/2110 train_time:64204ms step_avg:46.52ms
step:1381/2110 train_time:64266ms step_avg:46.54ms
step:1382/2110 train_time:64352ms step_avg:46.56ms
step:1383/2110 train_time:64439ms step_avg:46.59ms
step:1384/2110 train_time:64524ms step_avg:46.62ms
step:1385/2110 train_time:64612ms step_avg:46.65ms
step:1386/2110 train_time:64698ms step_avg:46.68ms
step:1387/2110 train_time:64786ms step_avg:46.71ms
step:1388/2110 train_time:64872ms step_avg:46.74ms
step:1389/2110 train_time:64959ms step_avg:46.77ms
step:1390/2110 train_time:65046ms step_avg:46.80ms
step:1391/2110 train_time:65133ms step_avg:46.82ms
step:1392/2110 train_time:65217ms step_avg:46.85ms
step:1393/2110 train_time:65305ms step_avg:46.88ms
step:1394/2110 train_time:65391ms step_avg:46.91ms
step:1395/2110 train_time:65478ms step_avg:46.94ms
step:1396/2110 train_time:65565ms step_avg:46.97ms
step:1397/2110 train_time:65651ms step_avg:46.99ms
step:1398/2110 train_time:65737ms step_avg:47.02ms
step:1399/2110 train_time:65823ms step_avg:47.05ms
step:1400/2110 train_time:65909ms step_avg:47.08ms
step:1401/2110 train_time:65997ms step_avg:47.11ms
step:1402/2110 train_time:66084ms step_avg:47.14ms
step:1403/2110 train_time:66170ms step_avg:47.16ms
step:1404/2110 train_time:66256ms step_avg:47.19ms
step:1405/2110 train_time:66343ms step_avg:47.22ms
step:1406/2110 train_time:66428ms step_avg:47.25ms
step:1407/2110 train_time:66516ms step_avg:47.28ms
step:1408/2110 train_time:66602ms step_avg:47.30ms
step:1409/2110 train_time:66688ms step_avg:47.33ms
step:1410/2110 train_time:66774ms step_avg:47.36ms
step:1411/2110 train_time:66862ms step_avg:47.39ms
step:1412/2110 train_time:66947ms step_avg:47.41ms
step:1413/2110 train_time:67035ms step_avg:47.44ms
step:1414/2110 train_time:67120ms step_avg:47.47ms
step:1415/2110 train_time:67208ms step_avg:47.50ms
step:1416/2110 train_time:67294ms step_avg:47.52ms
step:1417/2110 train_time:67382ms step_avg:47.55ms
step:1418/2110 train_time:67468ms step_avg:47.58ms
step:1419/2110 train_time:67555ms step_avg:47.61ms
step:1420/2110 train_time:67641ms step_avg:47.63ms
step:1421/2110 train_time:67728ms step_avg:47.66ms
step:1422/2110 train_time:67814ms step_avg:47.69ms
step:1423/2110 train_time:67902ms step_avg:47.72ms
step:1424/2110 train_time:67987ms step_avg:47.74ms
step:1425/2110 train_time:68075ms step_avg:47.77ms
step:1426/2110 train_time:68161ms step_avg:47.80ms
step:1427/2110 train_time:68248ms step_avg:47.83ms
step:1428/2110 train_time:68333ms step_avg:47.85ms
step:1429/2110 train_time:68421ms step_avg:47.88ms
step:1430/2110 train_time:68507ms step_avg:47.91ms
step:1431/2110 train_time:68594ms step_avg:47.93ms
step:1432/2110 train_time:68680ms step_avg:47.96ms
step:1433/2110 train_time:68767ms step_avg:47.99ms
step:1434/2110 train_time:68852ms step_avg:48.01ms
step:1435/2110 train_time:68939ms step_avg:48.04ms
step:1436/2110 train_time:69025ms step_avg:48.07ms
step:1437/2110 train_time:69112ms step_avg:48.09ms
step:1438/2110 train_time:69198ms step_avg:48.12ms
step:1439/2110 train_time:69286ms step_avg:48.15ms
step:1440/2110 train_time:69372ms step_avg:48.18ms
step:1441/2110 train_time:69459ms step_avg:48.20ms
step:1442/2110 train_time:69544ms step_avg:48.23ms
step:1443/2110 train_time:69631ms step_avg:48.25ms
step:1444/2110 train_time:69716ms step_avg:48.28ms
step:1445/2110 train_time:69804ms step_avg:48.31ms
step:1446/2110 train_time:69891ms step_avg:48.33ms
step:1447/2110 train_time:69977ms step_avg:48.36ms
step:1448/2110 train_time:70062ms step_avg:48.39ms
step:1449/2110 train_time:70149ms step_avg:48.41ms
step:1450/2110 train_time:70235ms step_avg:48.44ms
step:1451/2110 train_time:70322ms step_avg:48.46ms
step:1452/2110 train_time:70408ms step_avg:48.49ms
step:1453/2110 train_time:70495ms step_avg:48.52ms
step:1454/2110 train_time:70580ms step_avg:48.54ms
step:1455/2110 train_time:70668ms step_avg:48.57ms
step:1456/2110 train_time:70753ms step_avg:48.59ms
step:1457/2110 train_time:70841ms step_avg:48.62ms
step:1458/2110 train_time:70926ms step_avg:48.65ms
step:1459/2110 train_time:71013ms step_avg:48.67ms
step:1460/2110 train_time:71099ms step_avg:48.70ms
step:1461/2110 train_time:71187ms step_avg:48.72ms
step:1462/2110 train_time:71272ms step_avg:48.75ms
step:1463/2110 train_time:71360ms step_avg:48.78ms
step:1464/2110 train_time:71446ms step_avg:48.80ms
step:1465/2110 train_time:71533ms step_avg:48.83ms
step:1466/2110 train_time:71619ms step_avg:48.85ms
step:1467/2110 train_time:71706ms step_avg:48.88ms
step:1468/2110 train_time:71792ms step_avg:48.90ms
step:1469/2110 train_time:71878ms step_avg:48.93ms
step:1470/2110 train_time:71965ms step_avg:48.96ms
step:1471/2110 train_time:72051ms step_avg:48.98ms
step:1472/2110 train_time:72137ms step_avg:49.01ms
step:1473/2110 train_time:72224ms step_avg:49.03ms
step:1474/2110 train_time:72310ms step_avg:49.06ms
step:1475/2110 train_time:72397ms step_avg:49.08ms
step:1476/2110 train_time:72483ms step_avg:49.11ms
step:1477/2110 train_time:72570ms step_avg:49.13ms
step:1478/2110 train_time:72656ms step_avg:49.16ms
step:1479/2110 train_time:72744ms step_avg:49.18ms
step:1480/2110 train_time:72829ms step_avg:49.21ms
step:1481/2110 train_time:72917ms step_avg:49.23ms
step:1482/2110 train_time:73003ms step_avg:49.26ms
step:1483/2110 train_time:73090ms step_avg:49.29ms
step:1484/2110 train_time:73175ms step_avg:49.31ms
step:1485/2110 train_time:73264ms step_avg:49.34ms
step:1486/2110 train_time:73349ms step_avg:49.36ms
step:1487/2110 train_time:73437ms step_avg:49.39ms
step:1488/2110 train_time:73523ms step_avg:49.41ms
step:1489/2110 train_time:73610ms step_avg:49.44ms
step:1490/2110 train_time:73695ms step_avg:49.46ms
step:1491/2110 train_time:73783ms step_avg:49.49ms
step:1492/2110 train_time:73869ms step_avg:49.51ms
step:1493/2110 train_time:73956ms step_avg:49.54ms
step:1494/2110 train_time:74042ms step_avg:49.56ms
step:1495/2110 train_time:74129ms step_avg:49.58ms
step:1496/2110 train_time:74215ms step_avg:49.61ms
step:1497/2110 train_time:74302ms step_avg:49.63ms
step:1498/2110 train_time:74388ms step_avg:49.66ms
step:1499/2110 train_time:74476ms step_avg:49.68ms
step:1500/2110 train_time:74562ms step_avg:49.71ms
step:1500/2110 val_loss:3.4946 train_time:74650ms step_avg:49.77ms
step:1501/2110 train_time:74680ms step_avg:49.75ms
step:1502/2110 train_time:74741ms step_avg:49.76ms
step:1503/2110 train_time:74833ms step_avg:49.79ms
step:1504/2110 train_time:74920ms step_avg:49.81ms
step:1505/2110 train_time:75007ms step_avg:49.84ms
step:1506/2110 train_time:75092ms step_avg:49.86ms
step:1507/2110 train_time:75178ms step_avg:49.89ms
step:1508/2110 train_time:75263ms step_avg:49.91ms
step:1509/2110 train_time:75349ms step_avg:49.93ms
step:1510/2110 train_time:75434ms step_avg:49.96ms
step:1511/2110 train_time:75519ms step_avg:49.98ms
step:1512/2110 train_time:75607ms step_avg:50.00ms
step:1513/2110 train_time:75696ms step_avg:50.03ms
step:1514/2110 train_time:75786ms step_avg:50.06ms
step:1515/2110 train_time:75875ms step_avg:50.08ms
step:1516/2110 train_time:75962ms step_avg:50.11ms
step:1517/2110 train_time:76049ms step_avg:50.13ms
step:1518/2110 train_time:76134ms step_avg:50.15ms
step:1519/2110 train_time:76221ms step_avg:50.18ms
step:1520/2110 train_time:76306ms step_avg:50.20ms
step:1521/2110 train_time:76392ms step_avg:50.22ms
step:1522/2110 train_time:76477ms step_avg:50.25ms
step:1523/2110 train_time:76564ms step_avg:50.27ms
step:1524/2110 train_time:76651ms step_avg:50.30ms
step:1525/2110 train_time:76740ms step_avg:50.32ms
step:1526/2110 train_time:76828ms step_avg:50.35ms
step:1527/2110 train_time:76916ms step_avg:50.37ms
step:1528/2110 train_time:77004ms step_avg:50.39ms
step:1529/2110 train_time:77090ms step_avg:50.42ms
step:1530/2110 train_time:77175ms step_avg:50.44ms
step:1531/2110 train_time:77261ms step_avg:50.46ms
step:1532/2110 train_time:77346ms step_avg:50.49ms
step:1533/2110 train_time:77433ms step_avg:50.51ms
step:1534/2110 train_time:77519ms step_avg:50.53ms
step:1535/2110 train_time:77606ms step_avg:50.56ms
step:1536/2110 train_time:77694ms step_avg:50.58ms
step:1537/2110 train_time:77782ms step_avg:50.61ms
step:1538/2110 train_time:77869ms step_avg:50.63ms
step:1539/2110 train_time:77957ms step_avg:50.65ms
step:1540/2110 train_time:78044ms step_avg:50.68ms
step:1541/2110 train_time:78130ms step_avg:50.70ms
step:1542/2110 train_time:78216ms step_avg:50.72ms
step:1543/2110 train_time:78302ms step_avg:50.75ms
step:1544/2110 train_time:78388ms step_avg:50.77ms
step:1545/2110 train_time:78474ms step_avg:50.79ms
step:1546/2110 train_time:78560ms step_avg:50.82ms
step:1547/2110 train_time:78648ms step_avg:50.84ms
step:1548/2110 train_time:78735ms step_avg:50.86ms
step:1549/2110 train_time:78823ms step_avg:50.89ms
step:1550/2110 train_time:78909ms step_avg:50.91ms
step:1551/2110 train_time:78997ms step_avg:50.93ms
step:1552/2110 train_time:79083ms step_avg:50.96ms
step:1553/2110 train_time:79170ms step_avg:50.98ms
step:1554/2110 train_time:79255ms step_avg:51.00ms
step:1555/2110 train_time:79342ms step_avg:51.02ms
step:1556/2110 train_time:79427ms step_avg:51.05ms
step:1557/2110 train_time:79513ms step_avg:51.07ms
step:1558/2110 train_time:79600ms step_avg:51.09ms
step:1559/2110 train_time:79688ms step_avg:51.11ms
step:1560/2110 train_time:79774ms step_avg:51.14ms
step:1561/2110 train_time:79862ms step_avg:51.16ms
step:1562/2110 train_time:79950ms step_avg:51.18ms
step:1563/2110 train_time:80037ms step_avg:51.21ms
step:1564/2110 train_time:80123ms step_avg:51.23ms
step:1565/2110 train_time:80210ms step_avg:51.25ms
step:1566/2110 train_time:80295ms step_avg:51.27ms
step:1567/2110 train_time:80382ms step_avg:51.30ms
step:1568/2110 train_time:80467ms step_avg:51.32ms
step:1569/2110 train_time:80555ms step_avg:51.34ms
step:1570/2110 train_time:80641ms step_avg:51.36ms
step:1571/2110 train_time:80728ms step_avg:51.39ms
step:1572/2110 train_time:80815ms step_avg:51.41ms
step:1573/2110 train_time:80903ms step_avg:51.43ms
step:1574/2110 train_time:80991ms step_avg:51.46ms
step:1575/2110 train_time:81077ms step_avg:51.48ms
step:1576/2110 train_time:81164ms step_avg:51.50ms
step:1577/2110 train_time:81252ms step_avg:51.52ms
step:1578/2110 train_time:81337ms step_avg:51.54ms
step:1579/2110 train_time:81424ms step_avg:51.57ms
step:1580/2110 train_time:81510ms step_avg:51.59ms
step:1581/2110 train_time:81597ms step_avg:51.61ms
step:1582/2110 train_time:81684ms step_avg:51.63ms
step:1583/2110 train_time:81772ms step_avg:51.66ms
step:1584/2110 train_time:81857ms step_avg:51.68ms
step:1585/2110 train_time:81945ms step_avg:51.70ms
step:1586/2110 train_time:82033ms step_avg:51.72ms
step:1587/2110 train_time:82119ms step_avg:51.74ms
step:1588/2110 train_time:82206ms step_avg:51.77ms
step:1589/2110 train_time:82293ms step_avg:51.79ms
step:1590/2110 train_time:82380ms step_avg:51.81ms
step:1591/2110 train_time:82466ms step_avg:51.83ms
step:1592/2110 train_time:82552ms step_avg:51.85ms
step:1593/2110 train_time:82639ms step_avg:51.88ms
step:1594/2110 train_time:82725ms step_avg:51.90ms
step:1595/2110 train_time:82812ms step_avg:51.92ms
step:1596/2110 train_time:82898ms step_avg:51.94ms
step:1597/2110 train_time:82985ms step_avg:51.96ms
step:1598/2110 train_time:83071ms step_avg:51.98ms
step:1599/2110 train_time:83158ms step_avg:52.01ms
step:1600/2110 train_time:83245ms step_avg:52.03ms
step:1601/2110 train_time:83333ms step_avg:52.05ms
step:1602/2110 train_time:83419ms step_avg:52.07ms
step:1603/2110 train_time:83507ms step_avg:52.09ms
step:1604/2110 train_time:83593ms step_avg:52.12ms
step:1605/2110 train_time:83680ms step_avg:52.14ms
step:1606/2110 train_time:83767ms step_avg:52.16ms
step:1607/2110 train_time:83854ms step_avg:52.18ms
step:1608/2110 train_time:83940ms step_avg:52.20ms
step:1609/2110 train_time:84028ms step_avg:52.22ms
step:1610/2110 train_time:84114ms step_avg:52.24ms
step:1611/2110 train_time:84201ms step_avg:52.27ms
step:1612/2110 train_time:84288ms step_avg:52.29ms
step:1613/2110 train_time:84375ms step_avg:52.31ms
step:1614/2110 train_time:84460ms step_avg:52.33ms
step:1615/2110 train_time:84547ms step_avg:52.35ms
step:1616/2110 train_time:84634ms step_avg:52.37ms
step:1617/2110 train_time:84721ms step_avg:52.39ms
step:1618/2110 train_time:84808ms step_avg:52.42ms
step:1619/2110 train_time:84894ms step_avg:52.44ms
step:1620/2110 train_time:84980ms step_avg:52.46ms
step:1621/2110 train_time:85067ms step_avg:52.48ms
step:1622/2110 train_time:85154ms step_avg:52.50ms
step:1623/2110 train_time:85240ms step_avg:52.52ms
step:1624/2110 train_time:85327ms step_avg:52.54ms
step:1625/2110 train_time:85415ms step_avg:52.56ms
step:1626/2110 train_time:85501ms step_avg:52.58ms
step:1627/2110 train_time:85587ms step_avg:52.60ms
step:1628/2110 train_time:85674ms step_avg:52.63ms
step:1629/2110 train_time:85760ms step_avg:52.65ms
step:1630/2110 train_time:85847ms step_avg:52.67ms
step:1631/2110 train_time:85933ms step_avg:52.69ms
step:1632/2110 train_time:86019ms step_avg:52.71ms
step:1633/2110 train_time:86107ms step_avg:52.73ms
step:1634/2110 train_time:86193ms step_avg:52.75ms
step:1635/2110 train_time:86280ms step_avg:52.77ms
step:1636/2110 train_time:86366ms step_avg:52.79ms
step:1637/2110 train_time:86455ms step_avg:52.81ms
step:1638/2110 train_time:86540ms step_avg:52.83ms
step:1639/2110 train_time:86628ms step_avg:52.85ms
step:1640/2110 train_time:86714ms step_avg:52.87ms
step:1641/2110 train_time:86800ms step_avg:52.89ms
step:1642/2110 train_time:86886ms step_avg:52.91ms
step:1643/2110 train_time:86973ms step_avg:52.94ms
step:1644/2110 train_time:87059ms step_avg:52.96ms
step:1645/2110 train_time:87147ms step_avg:52.98ms
step:1646/2110 train_time:87234ms step_avg:53.00ms
step:1647/2110 train_time:87321ms step_avg:53.02ms
step:1648/2110 train_time:87407ms step_avg:53.04ms
step:1649/2110 train_time:87494ms step_avg:53.06ms
step:1650/2110 train_time:87580ms step_avg:53.08ms
step:1651/2110 train_time:87667ms step_avg:53.10ms
step:1652/2110 train_time:87753ms step_avg:53.12ms
step:1653/2110 train_time:87839ms step_avg:53.14ms
step:1654/2110 train_time:87926ms step_avg:53.16ms
step:1655/2110 train_time:88013ms step_avg:53.18ms
step:1656/2110 train_time:88099ms step_avg:53.20ms
step:1657/2110 train_time:88186ms step_avg:53.22ms
step:1658/2110 train_time:88274ms step_avg:53.24ms
step:1659/2110 train_time:88362ms step_avg:53.26ms
step:1660/2110 train_time:88450ms step_avg:53.28ms
step:1661/2110 train_time:88538ms step_avg:53.30ms
step:1662/2110 train_time:88625ms step_avg:53.32ms
step:1663/2110 train_time:88714ms step_avg:53.35ms
step:1664/2110 train_time:88802ms step_avg:53.37ms
step:1665/2110 train_time:88891ms step_avg:53.39ms
step:1666/2110 train_time:88978ms step_avg:53.41ms
step:1667/2110 train_time:89067ms step_avg:53.43ms
step:1668/2110 train_time:89154ms step_avg:53.45ms
step:1669/2110 train_time:89242ms step_avg:53.47ms
step:1670/2110 train_time:89329ms step_avg:53.49ms
step:1671/2110 train_time:89417ms step_avg:53.51ms
step:1672/2110 train_time:89506ms step_avg:53.53ms
step:1673/2110 train_time:89594ms step_avg:53.55ms
step:1674/2110 train_time:89681ms step_avg:53.57ms
step:1675/2110 train_time:89770ms step_avg:53.59ms
step:1676/2110 train_time:89856ms step_avg:53.61ms
step:1677/2110 train_time:89945ms step_avg:53.63ms
step:1678/2110 train_time:90033ms step_avg:53.66ms
step:1679/2110 train_time:90121ms step_avg:53.68ms
step:1680/2110 train_time:90209ms step_avg:53.70ms
step:1681/2110 train_time:90297ms step_avg:53.72ms
step:1682/2110 train_time:90384ms step_avg:53.74ms
step:1683/2110 train_time:90474ms step_avg:53.76ms
step:1684/2110 train_time:90561ms step_avg:53.78ms
step:1685/2110 train_time:90650ms step_avg:53.80ms
step:1686/2110 train_time:90737ms step_avg:53.82ms
step:1687/2110 train_time:90825ms step_avg:53.84ms
step:1688/2110 train_time:90913ms step_avg:53.86ms
step:1689/2110 train_time:91002ms step_avg:53.88ms
step:1690/2110 train_time:91089ms step_avg:53.90ms
step:1691/2110 train_time:91177ms step_avg:53.92ms
step:1692/2110 train_time:91264ms step_avg:53.94ms
step:1693/2110 train_time:91353ms step_avg:53.96ms
step:1694/2110 train_time:91440ms step_avg:53.98ms
step:1695/2110 train_time:91529ms step_avg:54.00ms
step:1696/2110 train_time:91616ms step_avg:54.02ms
step:1697/2110 train_time:91705ms step_avg:54.04ms
step:1698/2110 train_time:91792ms step_avg:54.06ms
step:1699/2110 train_time:91880ms step_avg:54.08ms
step:1700/2110 train_time:91968ms step_avg:54.10ms
step:1701/2110 train_time:92056ms step_avg:54.12ms
step:1702/2110 train_time:92143ms step_avg:54.14ms
step:1703/2110 train_time:92231ms step_avg:54.16ms
step:1704/2110 train_time:92317ms step_avg:54.18ms
step:1705/2110 train_time:92406ms step_avg:54.20ms
step:1706/2110 train_time:92494ms step_avg:54.22ms
step:1707/2110 train_time:92581ms step_avg:54.24ms
step:1708/2110 train_time:92669ms step_avg:54.26ms
step:1709/2110 train_time:92758ms step_avg:54.28ms
step:1710/2110 train_time:92846ms step_avg:54.30ms
step:1711/2110 train_time:92935ms step_avg:54.32ms
step:1712/2110 train_time:93023ms step_avg:54.34ms
step:1713/2110 train_time:93111ms step_avg:54.36ms
step:1714/2110 train_time:93198ms step_avg:54.37ms
step:1715/2110 train_time:93286ms step_avg:54.39ms
step:1716/2110 train_time:93373ms step_avg:54.41ms
step:1717/2110 train_time:93461ms step_avg:54.43ms
step:1718/2110 train_time:93548ms step_avg:54.45ms
step:1719/2110 train_time:93637ms step_avg:54.47ms
step:1720/2110 train_time:93725ms step_avg:54.49ms
step:1721/2110 train_time:93815ms step_avg:54.51ms
step:1722/2110 train_time:93901ms step_avg:54.53ms
step:1723/2110 train_time:93989ms step_avg:54.55ms
step:1724/2110 train_time:94076ms step_avg:54.57ms
step:1725/2110 train_time:94165ms step_avg:54.59ms
step:1726/2110 train_time:94253ms step_avg:54.61ms
step:1727/2110 train_time:94341ms step_avg:54.63ms
step:1728/2110 train_time:94428ms step_avg:54.65ms
step:1729/2110 train_time:94516ms step_avg:54.67ms
step:1730/2110 train_time:94604ms step_avg:54.68ms
step:1731/2110 train_time:94692ms step_avg:54.70ms
step:1732/2110 train_time:94780ms step_avg:54.72ms
step:1733/2110 train_time:94868ms step_avg:54.74ms
step:1734/2110 train_time:94955ms step_avg:54.76ms
step:1735/2110 train_time:95043ms step_avg:54.78ms
step:1736/2110 train_time:95130ms step_avg:54.80ms
step:1737/2110 train_time:95218ms step_avg:54.82ms
step:1738/2110 train_time:95306ms step_avg:54.84ms
step:1739/2110 train_time:95395ms step_avg:54.86ms
step:1740/2110 train_time:95483ms step_avg:54.88ms
step:1741/2110 train_time:95572ms step_avg:54.89ms
step:1742/2110 train_time:95659ms step_avg:54.91ms
step:1743/2110 train_time:95747ms step_avg:54.93ms
step:1744/2110 train_time:95835ms step_avg:54.95ms
step:1745/2110 train_time:95924ms step_avg:54.97ms
step:1746/2110 train_time:96012ms step_avg:54.99ms
step:1747/2110 train_time:96100ms step_avg:55.01ms
step:1748/2110 train_time:96186ms step_avg:55.03ms
step:1749/2110 train_time:96275ms step_avg:55.05ms
step:1750/2110 train_time:96362ms step_avg:55.06ms
step:1750/2110 val_loss:3.3788 train_time:96454ms step_avg:55.12ms
step:1751/2110 train_time:96487ms step_avg:55.10ms
step:1752/2110 train_time:96544ms step_avg:55.11ms
step:1753/2110 train_time:96636ms step_avg:55.13ms
step:1754/2110 train_time:96725ms step_avg:55.15ms
step:1755/2110 train_time:96814ms step_avg:55.16ms
step:1756/2110 train_time:96899ms step_avg:55.18ms
step:1757/2110 train_time:96987ms step_avg:55.20ms
step:1758/2110 train_time:97074ms step_avg:55.22ms
step:1759/2110 train_time:97161ms step_avg:55.24ms
step:1760/2110 train_time:97249ms step_avg:55.26ms
step:1761/2110 train_time:97336ms step_avg:55.27ms
step:1762/2110 train_time:97425ms step_avg:55.29ms
step:1763/2110 train_time:97517ms step_avg:55.31ms
step:1764/2110 train_time:97605ms step_avg:55.33ms
step:1765/2110 train_time:97695ms step_avg:55.35ms
step:1766/2110 train_time:97782ms step_avg:55.37ms
step:1767/2110 train_time:97870ms step_avg:55.39ms
step:1768/2110 train_time:97956ms step_avg:55.40ms
step:1769/2110 train_time:98044ms step_avg:55.42ms
step:1770/2110 train_time:98130ms step_avg:55.44ms
step:1771/2110 train_time:98218ms step_avg:55.46ms
step:1772/2110 train_time:98305ms step_avg:55.48ms
step:1773/2110 train_time:98394ms step_avg:55.50ms
step:1774/2110 train_time:98483ms step_avg:55.51ms
step:1775/2110 train_time:98573ms step_avg:55.53ms
step:1776/2110 train_time:98662ms step_avg:55.55ms
step:1777/2110 train_time:98752ms step_avg:55.57ms
step:1778/2110 train_time:98838ms step_avg:55.59ms
step:1779/2110 train_time:98926ms step_avg:55.61ms
step:1780/2110 train_time:99012ms step_avg:55.62ms
step:1781/2110 train_time:99099ms step_avg:55.64ms
step:1782/2110 train_time:99186ms step_avg:55.66ms
step:1783/2110 train_time:99274ms step_avg:55.68ms
step:1784/2110 train_time:99361ms step_avg:55.70ms
step:1785/2110 train_time:99449ms step_avg:55.71ms
step:1786/2110 train_time:99537ms step_avg:55.73ms
step:1787/2110 train_time:99627ms step_avg:55.75ms
step:1788/2110 train_time:99715ms step_avg:55.77ms
step:1789/2110 train_time:99803ms step_avg:55.79ms
step:1790/2110 train_time:99890ms step_avg:55.80ms
step:1791/2110 train_time:99978ms step_avg:55.82ms
step:1792/2110 train_time:100064ms step_avg:55.84ms
step:1793/2110 train_time:100152ms step_avg:55.86ms
step:1794/2110 train_time:100239ms step_avg:55.87ms
step:1795/2110 train_time:100327ms step_avg:55.89ms
step:1796/2110 train_time:100414ms step_avg:55.91ms
step:1797/2110 train_time:100504ms step_avg:55.93ms
step:1798/2110 train_time:100592ms step_avg:55.95ms
step:1799/2110 train_time:100681ms step_avg:55.97ms
step:1800/2110 train_time:100768ms step_avg:55.98ms
step:1801/2110 train_time:100857ms step_avg:56.00ms
step:1802/2110 train_time:100945ms step_avg:56.02ms
step:1803/2110 train_time:101032ms step_avg:56.04ms
step:1804/2110 train_time:101119ms step_avg:56.05ms
step:1805/2110 train_time:101206ms step_avg:56.07ms
step:1806/2110 train_time:101293ms step_avg:56.09ms
step:1807/2110 train_time:101382ms step_avg:56.11ms
step:1808/2110 train_time:101470ms step_avg:56.12ms
step:1809/2110 train_time:101559ms step_avg:56.14ms
step:1810/2110 train_time:101647ms step_avg:56.16ms
step:1811/2110 train_time:101735ms step_avg:56.18ms
step:1812/2110 train_time:101823ms step_avg:56.19ms
step:1813/2110 train_time:101911ms step_avg:56.21ms
step:1814/2110 train_time:101999ms step_avg:56.23ms
step:1815/2110 train_time:102087ms step_avg:56.25ms
step:1816/2110 train_time:102174ms step_avg:56.26ms
step:1817/2110 train_time:102261ms step_avg:56.28ms
step:1818/2110 train_time:102348ms step_avg:56.30ms
step:1819/2110 train_time:102436ms step_avg:56.31ms
step:1820/2110 train_time:102525ms step_avg:56.33ms
step:1821/2110 train_time:102615ms step_avg:56.35ms
step:1822/2110 train_time:102702ms step_avg:56.37ms
step:1823/2110 train_time:102791ms step_avg:56.39ms
step:1824/2110 train_time:102878ms step_avg:56.40ms
step:1825/2110 train_time:102967ms step_avg:56.42ms
step:1826/2110 train_time:103054ms step_avg:56.44ms
step:1827/2110 train_time:103141ms step_avg:56.45ms
step:1828/2110 train_time:103229ms step_avg:56.47ms
step:1829/2110 train_time:103316ms step_avg:56.49ms
step:1830/2110 train_time:103403ms step_avg:56.50ms
step:1831/2110 train_time:103492ms step_avg:56.52ms
step:1832/2110 train_time:103579ms step_avg:56.54ms
step:1833/2110 train_time:103668ms step_avg:56.56ms
step:1834/2110 train_time:103756ms step_avg:56.57ms
step:1835/2110 train_time:103844ms step_avg:56.59ms
step:1836/2110 train_time:103932ms step_avg:56.61ms
step:1837/2110 train_time:104020ms step_avg:56.62ms
step:1838/2110 train_time:104107ms step_avg:56.64ms
step:1839/2110 train_time:104194ms step_avg:56.66ms
step:1840/2110 train_time:104281ms step_avg:56.67ms
step:1841/2110 train_time:104369ms step_avg:56.69ms
step:1842/2110 train_time:104456ms step_avg:56.71ms
step:1843/2110 train_time:104546ms step_avg:56.73ms
step:1844/2110 train_time:104633ms step_avg:56.74ms
step:1845/2110 train_time:104724ms step_avg:56.76ms
step:1846/2110 train_time:104813ms step_avg:56.78ms
step:1847/2110 train_time:104900ms step_avg:56.79ms
step:1848/2110 train_time:104990ms step_avg:56.81ms
step:1849/2110 train_time:105076ms step_avg:56.83ms
step:1850/2110 train_time:105165ms step_avg:56.85ms
step:1851/2110 train_time:105252ms step_avg:56.86ms
step:1852/2110 train_time:105339ms step_avg:56.88ms
step:1853/2110 train_time:105426ms step_avg:56.89ms
step:1854/2110 train_time:105515ms step_avg:56.91ms
step:1855/2110 train_time:105602ms step_avg:56.93ms
step:1856/2110 train_time:105688ms step_avg:56.94ms
step:1857/2110 train_time:105777ms step_avg:56.96ms
step:1858/2110 train_time:105866ms step_avg:56.98ms
step:1859/2110 train_time:105955ms step_avg:57.00ms
step:1860/2110 train_time:106043ms step_avg:57.01ms
step:1861/2110 train_time:106130ms step_avg:57.03ms
step:1862/2110 train_time:106217ms step_avg:57.04ms
step:1863/2110 train_time:106304ms step_avg:57.06ms
step:1864/2110 train_time:106392ms step_avg:57.08ms
step:1865/2110 train_time:106479ms step_avg:57.09ms
step:1866/2110 train_time:106568ms step_avg:57.11ms
step:1867/2110 train_time:106655ms step_avg:57.13ms
step:1868/2110 train_time:106743ms step_avg:57.14ms
step:1869/2110 train_time:106831ms step_avg:57.16ms
step:1870/2110 train_time:106918ms step_avg:57.18ms
step:1871/2110 train_time:107007ms step_avg:57.19ms
step:1872/2110 train_time:107096ms step_avg:57.21ms
step:1873/2110 train_time:107182ms step_avg:57.22ms
step:1874/2110 train_time:107269ms step_avg:57.24ms
step:1875/2110 train_time:107357ms step_avg:57.26ms
step:1876/2110 train_time:107445ms step_avg:57.27ms
step:1877/2110 train_time:107533ms step_avg:57.29ms
step:1878/2110 train_time:107621ms step_avg:57.31ms
step:1879/2110 train_time:107709ms step_avg:57.32ms
step:1880/2110 train_time:107796ms step_avg:57.34ms
step:1881/2110 train_time:107885ms step_avg:57.36ms
step:1882/2110 train_time:107974ms step_avg:57.37ms
step:1883/2110 train_time:108061ms step_avg:57.39ms
step:1884/2110 train_time:108149ms step_avg:57.40ms
step:1885/2110 train_time:108236ms step_avg:57.42ms
step:1886/2110 train_time:108324ms step_avg:57.44ms
step:1887/2110 train_time:108411ms step_avg:57.45ms
step:1888/2110 train_time:108498ms step_avg:57.47ms
step:1889/2110 train_time:108586ms step_avg:57.48ms
step:1890/2110 train_time:108674ms step_avg:57.50ms
step:1891/2110 train_time:108761ms step_avg:57.52ms
step:1892/2110 train_time:108850ms step_avg:57.53ms
step:1893/2110 train_time:108937ms step_avg:57.55ms
step:1894/2110 train_time:109025ms step_avg:57.56ms
step:1895/2110 train_time:109113ms step_avg:57.58ms
step:1896/2110 train_time:109200ms step_avg:57.59ms
step:1897/2110 train_time:109287ms step_avg:57.61ms
step:1898/2110 train_time:109375ms step_avg:57.63ms
step:1899/2110 train_time:109463ms step_avg:57.64ms
step:1900/2110 train_time:109551ms step_avg:57.66ms
step:1901/2110 train_time:109639ms step_avg:57.67ms
step:1902/2110 train_time:109728ms step_avg:57.69ms
step:1903/2110 train_time:109815ms step_avg:57.71ms
step:1904/2110 train_time:109903ms step_avg:57.72ms
step:1905/2110 train_time:109991ms step_avg:57.74ms
step:1906/2110 train_time:110077ms step_avg:57.75ms
step:1907/2110 train_time:110166ms step_avg:57.77ms
step:1908/2110 train_time:110255ms step_avg:57.79ms
step:1909/2110 train_time:110342ms step_avg:57.80ms
step:1910/2110 train_time:110429ms step_avg:57.82ms
step:1911/2110 train_time:110517ms step_avg:57.83ms
step:1912/2110 train_time:110604ms step_avg:57.85ms
step:1913/2110 train_time:110692ms step_avg:57.86ms
step:1914/2110 train_time:110780ms step_avg:57.88ms
step:1915/2110 train_time:110868ms step_avg:57.89ms
step:1916/2110 train_time:110957ms step_avg:57.91ms
step:1917/2110 train_time:111044ms step_avg:57.93ms
step:1918/2110 train_time:111132ms step_avg:57.94ms
step:1919/2110 train_time:111220ms step_avg:57.96ms
step:1920/2110 train_time:111308ms step_avg:57.97ms
step:1921/2110 train_time:111396ms step_avg:57.99ms
step:1922/2110 train_time:111483ms step_avg:58.00ms
step:1923/2110 train_time:111571ms step_avg:58.02ms
step:1924/2110 train_time:111658ms step_avg:58.03ms
step:1925/2110 train_time:111745ms step_avg:58.05ms
step:1926/2110 train_time:111833ms step_avg:58.06ms
step:1927/2110 train_time:111922ms step_avg:58.08ms
step:1928/2110 train_time:112010ms step_avg:58.10ms
step:1929/2110 train_time:112097ms step_avg:58.11ms
step:1930/2110 train_time:112185ms step_avg:58.13ms
step:1931/2110 train_time:112273ms step_avg:58.14ms
step:1932/2110 train_time:112361ms step_avg:58.16ms
step:1933/2110 train_time:112448ms step_avg:58.17ms
step:1934/2110 train_time:112536ms step_avg:58.19ms
step:1935/2110 train_time:112624ms step_avg:58.20ms
step:1936/2110 train_time:112712ms step_avg:58.22ms
step:1937/2110 train_time:112799ms step_avg:58.23ms
step:1938/2110 train_time:112888ms step_avg:58.25ms
step:1939/2110 train_time:112975ms step_avg:58.26ms
step:1940/2110 train_time:113063ms step_avg:58.28ms
step:1941/2110 train_time:113151ms step_avg:58.30ms
step:1942/2110 train_time:113238ms step_avg:58.31ms
step:1943/2110 train_time:113326ms step_avg:58.33ms
step:1944/2110 train_time:113414ms step_avg:58.34ms
step:1945/2110 train_time:113501ms step_avg:58.36ms
step:1946/2110 train_time:113590ms step_avg:58.37ms
step:1947/2110 train_time:113677ms step_avg:58.39ms
step:1948/2110 train_time:113765ms step_avg:58.40ms
step:1949/2110 train_time:113854ms step_avg:58.42ms
step:1950/2110 train_time:113942ms step_avg:58.43ms
step:1951/2110 train_time:114029ms step_avg:58.45ms
step:1952/2110 train_time:114117ms step_avg:58.46ms
step:1953/2110 train_time:114205ms step_avg:58.48ms
step:1954/2110 train_time:114293ms step_avg:58.49ms
step:1955/2110 train_time:114381ms step_avg:58.51ms
step:1956/2110 train_time:114469ms step_avg:58.52ms
step:1957/2110 train_time:114556ms step_avg:58.54ms
step:1958/2110 train_time:114644ms step_avg:58.55ms
step:1959/2110 train_time:114732ms step_avg:58.57ms
step:1960/2110 train_time:114819ms step_avg:58.58ms
step:1961/2110 train_time:114907ms step_avg:58.60ms
step:1962/2110 train_time:114995ms step_avg:58.61ms
step:1963/2110 train_time:115083ms step_avg:58.63ms
step:1964/2110 train_time:115171ms step_avg:58.64ms
step:1965/2110 train_time:115258ms step_avg:58.66ms
step:1966/2110 train_time:115347ms step_avg:58.67ms
step:1967/2110 train_time:115435ms step_avg:58.69ms
step:1968/2110 train_time:115522ms step_avg:58.70ms
step:1969/2110 train_time:115609ms step_avg:58.71ms
step:1970/2110 train_time:115696ms step_avg:58.73ms
step:1971/2110 train_time:115786ms step_avg:58.74ms
step:1972/2110 train_time:115874ms step_avg:58.76ms
step:1973/2110 train_time:115962ms step_avg:58.77ms
step:1974/2110 train_time:116050ms step_avg:58.79ms
step:1975/2110 train_time:116139ms step_avg:58.80ms
step:1976/2110 train_time:116226ms step_avg:58.82ms
step:1977/2110 train_time:116313ms step_avg:58.83ms
step:1978/2110 train_time:116401ms step_avg:58.85ms
step:1979/2110 train_time:116488ms step_avg:58.86ms
step:1980/2110 train_time:116576ms step_avg:58.88ms
step:1981/2110 train_time:116664ms step_avg:58.89ms
step:1982/2110 train_time:116753ms step_avg:58.91ms
step:1983/2110 train_time:116840ms step_avg:58.92ms
step:1984/2110 train_time:116928ms step_avg:58.94ms
step:1985/2110 train_time:117016ms step_avg:58.95ms
step:1986/2110 train_time:117104ms step_avg:58.96ms
step:1987/2110 train_time:117192ms step_avg:58.98ms
step:1988/2110 train_time:117279ms step_avg:58.99ms
step:1989/2110 train_time:117367ms step_avg:59.01ms
step:1990/2110 train_time:117455ms step_avg:59.02ms
step:1991/2110 train_time:117542ms step_avg:59.04ms
step:1992/2110 train_time:117631ms step_avg:59.05ms
step:1993/2110 train_time:117719ms step_avg:59.07ms
step:1994/2110 train_time:117807ms step_avg:59.08ms
step:1995/2110 train_time:117895ms step_avg:59.10ms
step:1996/2110 train_time:117984ms step_avg:59.11ms
step:1997/2110 train_time:118071ms step_avg:59.12ms
step:1998/2110 train_time:118158ms step_avg:59.14ms
step:1999/2110 train_time:118246ms step_avg:59.15ms
step:2000/2110 train_time:118334ms step_avg:59.17ms
step:2000/2110 val_loss:3.3034 train_time:118422ms step_avg:59.21ms
step:2001/2110 train_time:118462ms step_avg:59.20ms
step:2002/2110 train_time:118514ms step_avg:59.20ms
step:2003/2110 train_time:118606ms step_avg:59.21ms
step:2004/2110 train_time:118695ms step_avg:59.23ms
step:2005/2110 train_time:118781ms step_avg:59.24ms
step:2006/2110 train_time:118867ms step_avg:59.26ms
step:2007/2110 train_time:118955ms step_avg:59.27ms
step:2008/2110 train_time:119042ms step_avg:59.28ms
step:2009/2110 train_time:119128ms step_avg:59.30ms
step:2010/2110 train_time:119215ms step_avg:59.31ms
step:2011/2110 train_time:119303ms step_avg:59.33ms
step:2012/2110 train_time:119391ms step_avg:59.34ms
step:2013/2110 train_time:119482ms step_avg:59.36ms
step:2014/2110 train_time:119572ms step_avg:59.37ms
step:2015/2110 train_time:119661ms step_avg:59.38ms
step:2016/2110 train_time:119748ms step_avg:59.40ms
step:2017/2110 train_time:119835ms step_avg:59.41ms
step:2018/2110 train_time:119922ms step_avg:59.43ms
step:2019/2110 train_time:120010ms step_avg:59.44ms
step:2020/2110 train_time:120097ms step_avg:59.45ms
step:2021/2110 train_time:120184ms step_avg:59.47ms
step:2022/2110 train_time:120271ms step_avg:59.48ms
step:2023/2110 train_time:120360ms step_avg:59.50ms
step:2024/2110 train_time:120450ms step_avg:59.51ms
step:2025/2110 train_time:120540ms step_avg:59.53ms
step:2026/2110 train_time:120628ms step_avg:59.54ms
step:2027/2110 train_time:120716ms step_avg:59.55ms
step:2028/2110 train_time:120804ms step_avg:59.57ms
step:2029/2110 train_time:120891ms step_avg:59.58ms
step:2030/2110 train_time:120979ms step_avg:59.60ms
step:2031/2110 train_time:121066ms step_avg:59.61ms
step:2032/2110 train_time:121154ms step_avg:59.62ms
step:2033/2110 train_time:121241ms step_avg:59.64ms
step:2034/2110 train_time:121329ms step_avg:59.65ms
step:2035/2110 train_time:121419ms step_avg:59.67ms
step:2036/2110 train_time:121507ms step_avg:59.68ms
step:2037/2110 train_time:121596ms step_avg:59.69ms
step:2038/2110 train_time:121685ms step_avg:59.71ms
step:2039/2110 train_time:121773ms step_avg:59.72ms
step:2040/2110 train_time:121861ms step_avg:59.74ms
step:2041/2110 train_time:121947ms step_avg:59.75ms
step:2042/2110 train_time:122034ms step_avg:59.76ms
step:2043/2110 train_time:122121ms step_avg:59.78ms
step:2044/2110 train_time:122208ms step_avg:59.79ms
step:2045/2110 train_time:122296ms step_avg:59.80ms
step:2046/2110 train_time:122384ms step_avg:59.82ms
step:2047/2110 train_time:122473ms step_avg:59.83ms
step:2048/2110 train_time:122561ms step_avg:59.84ms
step:2049/2110 train_time:122650ms step_avg:59.86ms
step:2050/2110 train_time:122738ms step_avg:59.87ms
step:2051/2110 train_time:122826ms step_avg:59.89ms
step:2052/2110 train_time:122914ms step_avg:59.90ms
step:2053/2110 train_time:123001ms step_avg:59.91ms
step:2054/2110 train_time:123087ms step_avg:59.93ms
step:2055/2110 train_time:123175ms step_avg:59.94ms
step:2056/2110 train_time:123264ms step_avg:59.95ms
step:2057/2110 train_time:123351ms step_avg:59.97ms
step:2058/2110 train_time:123439ms step_avg:59.98ms
step:2059/2110 train_time:123528ms step_avg:59.99ms
step:2060/2110 train_time:123616ms step_avg:60.01ms
step:2061/2110 train_time:123705ms step_avg:60.02ms
step:2062/2110 train_time:123792ms step_avg:60.03ms
step:2063/2110 train_time:123879ms step_avg:60.05ms
step:2064/2110 train_time:123968ms step_avg:60.06ms
step:2065/2110 train_time:124055ms step_avg:60.08ms
step:2066/2110 train_time:124143ms step_avg:60.09ms
step:2067/2110 train_time:124229ms step_avg:60.10ms
step:2068/2110 train_time:124318ms step_avg:60.12ms
step:2069/2110 train_time:124405ms step_avg:60.13ms
step:2070/2110 train_time:124494ms step_avg:60.14ms
step:2071/2110 train_time:124582ms step_avg:60.16ms
step:2072/2110 train_time:124669ms step_avg:60.17ms
step:2073/2110 train_time:124758ms step_avg:60.18ms
step:2074/2110 train_time:124846ms step_avg:60.20ms
step:2075/2110 train_time:124935ms step_avg:60.21ms
step:2076/2110 train_time:125023ms step_avg:60.22ms
step:2077/2110 train_time:125111ms step_avg:60.24ms
step:2078/2110 train_time:125200ms step_avg:60.25ms
step:2079/2110 train_time:125287ms step_avg:60.26ms
step:2080/2110 train_time:125376ms step_avg:60.28ms
step:2081/2110 train_time:125463ms step_avg:60.29ms
step:2082/2110 train_time:125551ms step_avg:60.30ms
step:2083/2110 train_time:125640ms step_avg:60.32ms
step:2084/2110 train_time:125727ms step_avg:60.33ms
step:2085/2110 train_time:125816ms step_avg:60.34ms
step:2086/2110 train_time:125906ms step_avg:60.36ms
step:2087/2110 train_time:125993ms step_avg:60.37ms
step:2088/2110 train_time:126080ms step_avg:60.38ms
step:2089/2110 train_time:126170ms step_avg:60.40ms
step:2090/2110 train_time:126259ms step_avg:60.41ms
step:2091/2110 train_time:126347ms step_avg:60.42ms
step:2092/2110 train_time:126435ms step_avg:60.44ms
step:2093/2110 train_time:126524ms step_avg:60.45ms
step:2094/2110 train_time:126611ms step_avg:60.46ms
step:2095/2110 train_time:126701ms step_avg:60.48ms
step:2096/2110 train_time:126788ms step_avg:60.49ms
step:2097/2110 train_time:126877ms step_avg:60.50ms
step:2098/2110 train_time:126965ms step_avg:60.52ms
step:2099/2110 train_time:127053ms step_avg:60.53ms
step:2100/2110 train_time:127141ms step_avg:60.54ms
step:2101/2110 train_time:127229ms step_avg:60.56ms
step:2102/2110 train_time:127316ms step_avg:60.57ms
step:2103/2110 train_time:127405ms step_avg:60.58ms
step:2104/2110 train_time:127493ms step_avg:60.60ms
step:2105/2110 train_time:127581ms step_avg:60.61ms
step:2106/2110 train_time:127669ms step_avg:60.62ms
step:2107/2110 train_time:127756ms step_avg:60.63ms
step:2108/2110 train_time:127845ms step_avg:60.65ms
step:2109/2110 train_time:127933ms step_avg:60.66ms
step:2110/2110 train_time:128021ms step_avg:60.67ms
step:2110/2110 val_loss:3.2795 train_time:128111ms step_avg:60.72ms
peak memory allocated: 29892 MiB reserved: 44256 MiB

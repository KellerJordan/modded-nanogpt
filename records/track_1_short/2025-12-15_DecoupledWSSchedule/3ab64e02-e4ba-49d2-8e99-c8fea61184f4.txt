import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 15 20:44:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   38C    P0            126W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            120W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   42C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   42C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              88      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              88      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:105ms step_avg:104.51ms
step:2/2110 train_time:140ms step_avg:70.13ms
step:3/2110 train_time:171ms step_avg:57.06ms
step:4/2110 train_time:200ms step_avg:50.06ms
step:5/2110 train_time:231ms step_avg:46.25ms
step:6/2110 train_time:430ms step_avg:71.67ms
step:7/2110 train_time:614ms step_avg:87.71ms
step:8/2110 train_time:646ms step_avg:80.79ms
step:9/2110 train_time:679ms step_avg:75.45ms
step:10/2110 train_time:711ms step_avg:71.13ms
step:11/2110 train_time:745ms step_avg:67.73ms
step:12/2110 train_time:778ms step_avg:64.83ms
step:13/2110 train_time:811ms step_avg:62.40ms
step:14/2110 train_time:844ms step_avg:60.28ms
step:15/2110 train_time:877ms step_avg:58.48ms
step:16/2110 train_time:910ms step_avg:56.86ms
step:17/2110 train_time:943ms step_avg:55.48ms
step:18/2110 train_time:978ms step_avg:54.31ms
step:19/2110 train_time:1015ms step_avg:53.40ms
step:20/2110 train_time:1049ms step_avg:52.46ms
step:21/2110 train_time:1086ms step_avg:51.72ms
step:22/2110 train_time:1120ms step_avg:50.91ms
step:23/2110 train_time:1158ms step_avg:50.33ms
step:24/2110 train_time:1193ms step_avg:49.70ms
step:25/2110 train_time:1232ms step_avg:49.30ms
step:26/2110 train_time:1267ms step_avg:48.74ms
step:27/2110 train_time:1305ms step_avg:48.34ms
step:28/2110 train_time:1340ms step_avg:47.87ms
step:29/2110 train_time:1378ms step_avg:47.53ms
step:30/2110 train_time:1414ms step_avg:47.13ms
step:31/2110 train_time:1451ms step_avg:46.82ms
step:32/2110 train_time:1487ms step_avg:46.47ms
step:33/2110 train_time:1524ms step_avg:46.17ms
step:34/2110 train_time:1559ms step_avg:45.85ms
step:35/2110 train_time:1598ms step_avg:45.65ms
step:36/2110 train_time:1632ms step_avg:45.33ms
step:37/2110 train_time:1670ms step_avg:45.12ms
step:38/2110 train_time:1705ms step_avg:44.86ms
step:39/2110 train_time:1742ms step_avg:44.66ms
step:40/2110 train_time:1776ms step_avg:44.40ms
step:41/2110 train_time:1813ms step_avg:44.21ms
step:42/2110 train_time:1847ms step_avg:43.97ms
step:43/2110 train_time:1884ms step_avg:43.81ms
step:44/2110 train_time:1918ms step_avg:43.58ms
step:45/2110 train_time:1954ms step_avg:43.41ms
step:46/2110 train_time:1988ms step_avg:43.23ms
step:47/2110 train_time:2026ms step_avg:43.11ms
step:48/2110 train_time:2060ms step_avg:42.92ms
step:49/2110 train_time:2098ms step_avg:42.81ms
step:50/2110 train_time:2132ms step_avg:42.63ms
step:51/2110 train_time:2170ms step_avg:42.56ms
step:52/2110 train_time:2205ms step_avg:42.40ms
step:53/2110 train_time:2244ms step_avg:42.33ms
step:54/2110 train_time:2279ms step_avg:42.20ms
step:55/2110 train_time:2317ms step_avg:42.12ms
step:56/2110 train_time:2353ms step_avg:42.01ms
step:57/2110 train_time:2390ms step_avg:41.92ms
step:58/2110 train_time:2425ms step_avg:41.81ms
step:59/2110 train_time:2464ms step_avg:41.76ms
step:60/2110 train_time:2501ms step_avg:41.68ms
step:61/2110 train_time:2539ms step_avg:41.63ms
step:62/2110 train_time:2575ms step_avg:41.53ms
step:63/2110 train_time:2611ms step_avg:41.44ms
step:64/2110 train_time:2645ms step_avg:41.33ms
step:65/2110 train_time:2684ms step_avg:41.29ms
step:66/2110 train_time:2719ms step_avg:41.20ms
step:67/2110 train_time:2758ms step_avg:41.16ms
step:68/2110 train_time:2793ms step_avg:41.07ms
step:69/2110 train_time:2833ms step_avg:41.05ms
step:70/2110 train_time:2868ms step_avg:40.96ms
step:71/2110 train_time:2908ms step_avg:40.95ms
step:72/2110 train_time:2944ms step_avg:40.88ms
step:73/2110 train_time:2981ms step_avg:40.83ms
step:74/2110 train_time:3014ms step_avg:40.72ms
step:75/2110 train_time:3050ms step_avg:40.67ms
step:76/2110 train_time:3085ms step_avg:40.60ms
step:77/2110 train_time:3123ms step_avg:40.56ms
step:78/2110 train_time:3159ms step_avg:40.50ms
step:79/2110 train_time:3198ms step_avg:40.48ms
step:80/2110 train_time:3233ms step_avg:40.41ms
step:81/2110 train_time:3272ms step_avg:40.40ms
step:82/2110 train_time:3309ms step_avg:40.35ms
step:83/2110 train_time:3347ms step_avg:40.33ms
step:84/2110 train_time:3381ms step_avg:40.25ms
step:85/2110 train_time:3418ms step_avg:40.21ms
step:86/2110 train_time:3453ms step_avg:40.15ms
step:87/2110 train_time:3491ms step_avg:40.13ms
step:88/2110 train_time:3526ms step_avg:40.07ms
step:89/2110 train_time:3564ms step_avg:40.04ms
step:90/2110 train_time:3599ms step_avg:39.99ms
step:91/2110 train_time:3637ms step_avg:39.97ms
step:92/2110 train_time:3672ms step_avg:39.92ms
step:93/2110 train_time:3709ms step_avg:39.88ms
step:94/2110 train_time:3744ms step_avg:39.83ms
step:95/2110 train_time:3782ms step_avg:39.81ms
step:96/2110 train_time:3817ms step_avg:39.76ms
step:97/2110 train_time:3855ms step_avg:39.74ms
step:98/2110 train_time:3890ms step_avg:39.70ms
step:99/2110 train_time:3929ms step_avg:39.69ms
step:100/2110 train_time:3965ms step_avg:39.65ms
step:101/2110 train_time:4004ms step_avg:39.64ms
step:102/2110 train_time:4038ms step_avg:39.59ms
step:103/2110 train_time:4075ms step_avg:39.57ms
step:104/2110 train_time:4110ms step_avg:39.52ms
step:105/2110 train_time:4148ms step_avg:39.51ms
step:106/2110 train_time:4183ms step_avg:39.46ms
step:107/2110 train_time:4223ms step_avg:39.47ms
step:108/2110 train_time:4259ms step_avg:39.43ms
step:109/2110 train_time:4296ms step_avg:39.41ms
step:110/2110 train_time:4331ms step_avg:39.37ms
step:111/2110 train_time:4368ms step_avg:39.36ms
step:112/2110 train_time:4404ms step_avg:39.32ms
step:113/2110 train_time:4442ms step_avg:39.31ms
step:114/2110 train_time:4476ms step_avg:39.26ms
step:115/2110 train_time:4512ms step_avg:39.24ms
step:116/2110 train_time:4545ms step_avg:39.18ms
step:117/2110 train_time:4583ms step_avg:39.17ms
step:118/2110 train_time:4617ms step_avg:39.13ms
step:119/2110 train_time:4655ms step_avg:39.12ms
step:120/2110 train_time:4691ms step_avg:39.09ms
step:121/2110 train_time:4729ms step_avg:39.08ms
step:122/2110 train_time:4763ms step_avg:39.04ms
step:123/2110 train_time:4803ms step_avg:39.05ms
step:124/2110 train_time:4836ms step_avg:39.00ms
step:125/2110 train_time:4874ms step_avg:38.99ms
step:126/2110 train_time:4909ms step_avg:38.96ms
step:127/2110 train_time:4946ms step_avg:38.94ms
step:128/2110 train_time:4981ms step_avg:38.91ms
step:129/2110 train_time:5020ms step_avg:38.91ms
step:130/2110 train_time:5055ms step_avg:38.89ms
step:131/2110 train_time:5093ms step_avg:38.88ms
step:132/2110 train_time:5127ms step_avg:38.84ms
step:133/2110 train_time:5164ms step_avg:38.83ms
step:134/2110 train_time:5202ms step_avg:38.82ms
step:135/2110 train_time:5238ms step_avg:38.80ms
step:136/2110 train_time:5270ms step_avg:38.75ms
step:137/2110 train_time:5308ms step_avg:38.74ms
step:138/2110 train_time:5343ms step_avg:38.72ms
step:139/2110 train_time:5381ms step_avg:38.71ms
step:140/2110 train_time:5418ms step_avg:38.70ms
step:141/2110 train_time:5454ms step_avg:38.68ms
step:142/2110 train_time:5487ms step_avg:38.64ms
step:143/2110 train_time:5520ms step_avg:38.60ms
step:144/2110 train_time:5552ms step_avg:38.56ms
step:145/2110 train_time:5585ms step_avg:38.52ms
step:146/2110 train_time:5618ms step_avg:38.48ms
step:147/2110 train_time:5659ms step_avg:38.50ms
step:148/2110 train_time:5692ms step_avg:38.46ms
step:149/2110 train_time:5730ms step_avg:38.45ms
step:150/2110 train_time:5765ms step_avg:38.43ms
step:151/2110 train_time:5800ms step_avg:38.41ms
step:152/2110 train_time:5838ms step_avg:38.40ms
step:153/2110 train_time:5873ms step_avg:38.39ms
step:154/2110 train_time:5908ms step_avg:38.37ms
step:155/2110 train_time:5946ms step_avg:38.36ms
step:156/2110 train_time:5980ms step_avg:38.33ms
step:157/2110 train_time:6017ms step_avg:38.33ms
step:158/2110 train_time:6051ms step_avg:38.29ms
step:159/2110 train_time:6087ms step_avg:38.28ms
step:160/2110 train_time:6120ms step_avg:38.25ms
step:161/2110 train_time:6157ms step_avg:38.24ms
step:162/2110 train_time:6191ms step_avg:38.22ms
step:163/2110 train_time:6228ms step_avg:38.21ms
step:164/2110 train_time:6263ms step_avg:38.19ms
step:165/2110 train_time:6299ms step_avg:38.18ms
step:166/2110 train_time:6339ms step_avg:38.19ms
step:167/2110 train_time:6375ms step_avg:38.17ms
step:168/2110 train_time:6410ms step_avg:38.16ms
step:169/2110 train_time:6447ms step_avg:38.15ms
step:170/2110 train_time:6480ms step_avg:38.12ms
step:171/2110 train_time:6518ms step_avg:38.11ms
step:172/2110 train_time:6552ms step_avg:38.10ms
step:173/2110 train_time:6588ms step_avg:38.08ms
step:174/2110 train_time:6624ms step_avg:38.07ms
step:175/2110 train_time:6662ms step_avg:38.07ms
step:176/2110 train_time:6697ms step_avg:38.05ms
step:177/2110 train_time:6733ms step_avg:38.04ms
step:178/2110 train_time:6768ms step_avg:38.02ms
step:179/2110 train_time:6805ms step_avg:38.01ms
step:180/2110 train_time:6838ms step_avg:37.99ms
step:181/2110 train_time:6875ms step_avg:37.98ms
step:182/2110 train_time:6911ms step_avg:37.97ms
step:183/2110 train_time:6947ms step_avg:37.96ms
step:184/2110 train_time:6981ms step_avg:37.94ms
step:185/2110 train_time:7019ms step_avg:37.94ms
step:186/2110 train_time:7053ms step_avg:37.92ms
step:187/2110 train_time:7091ms step_avg:37.92ms
step:188/2110 train_time:7125ms step_avg:37.90ms
step:189/2110 train_time:7161ms step_avg:37.89ms
step:190/2110 train_time:7196ms step_avg:37.88ms
step:191/2110 train_time:7234ms step_avg:37.88ms
step:192/2110 train_time:7269ms step_avg:37.86ms
step:193/2110 train_time:7306ms step_avg:37.85ms
step:194/2110 train_time:7340ms step_avg:37.83ms
step:195/2110 train_time:7377ms step_avg:37.83ms
step:196/2110 train_time:7411ms step_avg:37.81ms
step:197/2110 train_time:7448ms step_avg:37.81ms
step:198/2110 train_time:7482ms step_avg:37.79ms
step:199/2110 train_time:7519ms step_avg:37.78ms
step:200/2110 train_time:7554ms step_avg:37.77ms
step:201/2110 train_time:7590ms step_avg:37.76ms
step:202/2110 train_time:7623ms step_avg:37.74ms
step:203/2110 train_time:7661ms step_avg:37.74ms
step:204/2110 train_time:7695ms step_avg:37.72ms
step:205/2110 train_time:7732ms step_avg:37.72ms
step:206/2110 train_time:7766ms step_avg:37.70ms
step:207/2110 train_time:7803ms step_avg:37.69ms
step:208/2110 train_time:7837ms step_avg:37.68ms
step:209/2110 train_time:7874ms step_avg:37.68ms
step:210/2110 train_time:7908ms step_avg:37.66ms
step:211/2110 train_time:7944ms step_avg:37.65ms
step:212/2110 train_time:7979ms step_avg:37.64ms
step:213/2110 train_time:8015ms step_avg:37.63ms
step:214/2110 train_time:8051ms step_avg:37.62ms
step:215/2110 train_time:8087ms step_avg:37.61ms
step:216/2110 train_time:8121ms step_avg:37.60ms
step:217/2110 train_time:8158ms step_avg:37.59ms
step:218/2110 train_time:8191ms step_avg:37.57ms
step:219/2110 train_time:8228ms step_avg:37.57ms
step:220/2110 train_time:8262ms step_avg:37.55ms
step:221/2110 train_time:8298ms step_avg:37.55ms
step:222/2110 train_time:8335ms step_avg:37.54ms
step:223/2110 train_time:8372ms step_avg:37.54ms
step:224/2110 train_time:8407ms step_avg:37.53ms
step:225/2110 train_time:8443ms step_avg:37.53ms
step:226/2110 train_time:8477ms step_avg:37.51ms
step:227/2110 train_time:8515ms step_avg:37.51ms
step:228/2110 train_time:8549ms step_avg:37.49ms
step:229/2110 train_time:8586ms step_avg:37.49ms
step:230/2110 train_time:8619ms step_avg:37.47ms
step:231/2110 train_time:8657ms step_avg:37.48ms
step:232/2110 train_time:8692ms step_avg:37.46ms
step:233/2110 train_time:8730ms step_avg:37.47ms
step:234/2110 train_time:8765ms step_avg:37.46ms
step:235/2110 train_time:8801ms step_avg:37.45ms
step:236/2110 train_time:8836ms step_avg:37.44ms
step:237/2110 train_time:8873ms step_avg:37.44ms
step:238/2110 train_time:8908ms step_avg:37.43ms
step:239/2110 train_time:8945ms step_avg:37.43ms
step:240/2110 train_time:8980ms step_avg:37.42ms
step:241/2110 train_time:9015ms step_avg:37.41ms
step:242/2110 train_time:9051ms step_avg:37.40ms
step:243/2110 train_time:9088ms step_avg:37.40ms
step:244/2110 train_time:9123ms step_avg:37.39ms
step:245/2110 train_time:9159ms step_avg:37.38ms
step:246/2110 train_time:9194ms step_avg:37.37ms
step:247/2110 train_time:9230ms step_avg:37.37ms
step:248/2110 train_time:9267ms step_avg:37.37ms
step:249/2110 train_time:9303ms step_avg:37.36ms
step:250/2110 train_time:9337ms step_avg:37.35ms
step:250/2110 val_loss:4.3080 train_time:9376ms step_avg:37.51ms
step:251/2110 train_time:9402ms step_avg:37.46ms
step:252/2110 train_time:9426ms step_avg:37.41ms
step:253/2110 train_time:9452ms step_avg:37.36ms
step:254/2110 train_time:9485ms step_avg:37.34ms
step:255/2110 train_time:9520ms step_avg:37.33ms
step:256/2110 train_time:9553ms step_avg:37.32ms
step:257/2110 train_time:9589ms step_avg:37.31ms
step:258/2110 train_time:9627ms step_avg:37.31ms
step:259/2110 train_time:9666ms step_avg:37.32ms
step:260/2110 train_time:9701ms step_avg:37.31ms
step:261/2110 train_time:9739ms step_avg:37.31ms
step:262/2110 train_time:9773ms step_avg:37.30ms
step:263/2110 train_time:9809ms step_avg:37.30ms
step:264/2110 train_time:9844ms step_avg:37.29ms
step:265/2110 train_time:9880ms step_avg:37.28ms
step:266/2110 train_time:9914ms step_avg:37.27ms
step:267/2110 train_time:9952ms step_avg:37.27ms
step:268/2110 train_time:9987ms step_avg:37.27ms
step:269/2110 train_time:10024ms step_avg:37.26ms
step:270/2110 train_time:10060ms step_avg:37.26ms
step:271/2110 train_time:10095ms step_avg:37.25ms
step:272/2110 train_time:10129ms step_avg:37.24ms
step:273/2110 train_time:10166ms step_avg:37.24ms
step:274/2110 train_time:10199ms step_avg:37.22ms
step:275/2110 train_time:10237ms step_avg:37.23ms
step:276/2110 train_time:10273ms step_avg:37.22ms
step:277/2110 train_time:10311ms step_avg:37.23ms
step:278/2110 train_time:10348ms step_avg:37.22ms
step:279/2110 train_time:10384ms step_avg:37.22ms
step:280/2110 train_time:10419ms step_avg:37.21ms
step:281/2110 train_time:10455ms step_avg:37.21ms
step:282/2110 train_time:10491ms step_avg:37.20ms
step:283/2110 train_time:10527ms step_avg:37.20ms
step:284/2110 train_time:10562ms step_avg:37.19ms
step:285/2110 train_time:10604ms step_avg:37.21ms
step:286/2110 train_time:10637ms step_avg:37.19ms
step:287/2110 train_time:10674ms step_avg:37.19ms
step:288/2110 train_time:10710ms step_avg:37.19ms
step:289/2110 train_time:10746ms step_avg:37.18ms
step:290/2110 train_time:10781ms step_avg:37.17ms
step:291/2110 train_time:10818ms step_avg:37.18ms
step:292/2110 train_time:10853ms step_avg:37.17ms
step:293/2110 train_time:10891ms step_avg:37.17ms
step:294/2110 train_time:10925ms step_avg:37.16ms
step:295/2110 train_time:10962ms step_avg:37.16ms
step:296/2110 train_time:10996ms step_avg:37.15ms
step:297/2110 train_time:11033ms step_avg:37.15ms
step:298/2110 train_time:11066ms step_avg:37.13ms
step:299/2110 train_time:11105ms step_avg:37.14ms
step:300/2110 train_time:11139ms step_avg:37.13ms
step:301/2110 train_time:11176ms step_avg:37.13ms
step:302/2110 train_time:11211ms step_avg:37.12ms
step:303/2110 train_time:11248ms step_avg:37.12ms
step:304/2110 train_time:11282ms step_avg:37.11ms
step:305/2110 train_time:11320ms step_avg:37.12ms
step:306/2110 train_time:11355ms step_avg:37.11ms
step:307/2110 train_time:11391ms step_avg:37.10ms
step:308/2110 train_time:11425ms step_avg:37.09ms
step:309/2110 train_time:11463ms step_avg:37.10ms
step:310/2110 train_time:11498ms step_avg:37.09ms
step:311/2110 train_time:11536ms step_avg:37.09ms
step:312/2110 train_time:11569ms step_avg:37.08ms
step:313/2110 train_time:11606ms step_avg:37.08ms
step:314/2110 train_time:11642ms step_avg:37.08ms
step:315/2110 train_time:11678ms step_avg:37.07ms
step:316/2110 train_time:11713ms step_avg:37.07ms
step:317/2110 train_time:11751ms step_avg:37.07ms
step:318/2110 train_time:11784ms step_avg:37.06ms
step:319/2110 train_time:11822ms step_avg:37.06ms
step:320/2110 train_time:11856ms step_avg:37.05ms
step:321/2110 train_time:11893ms step_avg:37.05ms
step:322/2110 train_time:11927ms step_avg:37.04ms
step:323/2110 train_time:11964ms step_avg:37.04ms
step:324/2110 train_time:11999ms step_avg:37.03ms
step:325/2110 train_time:12034ms step_avg:37.03ms
step:326/2110 train_time:12069ms step_avg:37.02ms
step:327/2110 train_time:12105ms step_avg:37.02ms
step:328/2110 train_time:12141ms step_avg:37.01ms
step:329/2110 train_time:12176ms step_avg:37.01ms
step:330/2110 train_time:12211ms step_avg:37.00ms
step:331/2110 train_time:12247ms step_avg:37.00ms
step:332/2110 train_time:12282ms step_avg:36.99ms
step:333/2110 train_time:12319ms step_avg:37.00ms
step:334/2110 train_time:12354ms step_avg:36.99ms
step:335/2110 train_time:12390ms step_avg:36.98ms
step:336/2110 train_time:12424ms step_avg:36.98ms
step:337/2110 train_time:12461ms step_avg:36.98ms
step:338/2110 train_time:12495ms step_avg:36.97ms
step:339/2110 train_time:12532ms step_avg:36.97ms
step:340/2110 train_time:12567ms step_avg:36.96ms
step:341/2110 train_time:12603ms step_avg:36.96ms
step:342/2110 train_time:12638ms step_avg:36.95ms
step:343/2110 train_time:12674ms step_avg:36.95ms
step:344/2110 train_time:12708ms step_avg:36.94ms
step:345/2110 train_time:12744ms step_avg:36.94ms
step:346/2110 train_time:12781ms step_avg:36.94ms
step:347/2110 train_time:12818ms step_avg:36.94ms
step:348/2110 train_time:12850ms step_avg:36.93ms
step:349/2110 train_time:12884ms step_avg:36.92ms
step:350/2110 train_time:12916ms step_avg:36.90ms
step:351/2110 train_time:12948ms step_avg:36.89ms
step:352/2110 train_time:12981ms step_avg:36.88ms
step:353/2110 train_time:13014ms step_avg:36.87ms
step:354/2110 train_time:13047ms step_avg:36.86ms
step:355/2110 train_time:13079ms step_avg:36.84ms
step:356/2110 train_time:13112ms step_avg:36.83ms
step:357/2110 train_time:13144ms step_avg:36.82ms
step:358/2110 train_time:13177ms step_avg:36.81ms
step:359/2110 train_time:13210ms step_avg:36.80ms
step:360/2110 train_time:13242ms step_avg:36.78ms
step:361/2110 train_time:13275ms step_avg:36.77ms
step:362/2110 train_time:13307ms step_avg:36.76ms
step:363/2110 train_time:13340ms step_avg:36.75ms
step:364/2110 train_time:13373ms step_avg:36.74ms
step:365/2110 train_time:13406ms step_avg:36.73ms
step:366/2110 train_time:13439ms step_avg:36.72ms
step:367/2110 train_time:13473ms step_avg:36.71ms
step:368/2110 train_time:13505ms step_avg:36.70ms
step:369/2110 train_time:13539ms step_avg:36.69ms
step:370/2110 train_time:13572ms step_avg:36.68ms
step:371/2110 train_time:13609ms step_avg:36.68ms
step:372/2110 train_time:13642ms step_avg:36.67ms
step:373/2110 train_time:13676ms step_avg:36.66ms
step:374/2110 train_time:13708ms step_avg:36.65ms
step:375/2110 train_time:13743ms step_avg:36.65ms
step:376/2110 train_time:13775ms step_avg:36.64ms
step:377/2110 train_time:13809ms step_avg:36.63ms
step:378/2110 train_time:13841ms step_avg:36.62ms
step:379/2110 train_time:13875ms step_avg:36.61ms
step:380/2110 train_time:13908ms step_avg:36.60ms
step:381/2110 train_time:13945ms step_avg:36.60ms
step:382/2110 train_time:13977ms step_avg:36.59ms
step:383/2110 train_time:14010ms step_avg:36.58ms
step:384/2110 train_time:14044ms step_avg:36.57ms
step:385/2110 train_time:14079ms step_avg:36.57ms
step:386/2110 train_time:14112ms step_avg:36.56ms
step:387/2110 train_time:14148ms step_avg:36.56ms
step:388/2110 train_time:14181ms step_avg:36.55ms
step:389/2110 train_time:14217ms step_avg:36.55ms
step:390/2110 train_time:14248ms step_avg:36.53ms
step:391/2110 train_time:14284ms step_avg:36.53ms
step:392/2110 train_time:14317ms step_avg:36.52ms
step:393/2110 train_time:14353ms step_avg:36.52ms
step:394/2110 train_time:14386ms step_avg:36.51ms
step:395/2110 train_time:14421ms step_avg:36.51ms
step:396/2110 train_time:14456ms step_avg:36.50ms
step:397/2110 train_time:14490ms step_avg:36.50ms
step:398/2110 train_time:14522ms step_avg:36.49ms
step:399/2110 train_time:14557ms step_avg:36.48ms
step:400/2110 train_time:14590ms step_avg:36.48ms
step:401/2110 train_time:14626ms step_avg:36.47ms
step:402/2110 train_time:14659ms step_avg:36.46ms
step:403/2110 train_time:14696ms step_avg:36.47ms
step:404/2110 train_time:14729ms step_avg:36.46ms
step:405/2110 train_time:14762ms step_avg:36.45ms
step:406/2110 train_time:14795ms step_avg:36.44ms
step:407/2110 train_time:14829ms step_avg:36.44ms
step:408/2110 train_time:14862ms step_avg:36.43ms
step:409/2110 train_time:14897ms step_avg:36.42ms
step:410/2110 train_time:14930ms step_avg:36.41ms
step:411/2110 train_time:14965ms step_avg:36.41ms
step:412/2110 train_time:14998ms step_avg:36.40ms
step:413/2110 train_time:15033ms step_avg:36.40ms
step:414/2110 train_time:15065ms step_avg:36.39ms
step:415/2110 train_time:15102ms step_avg:36.39ms
step:416/2110 train_time:15134ms step_avg:36.38ms
step:417/2110 train_time:15167ms step_avg:36.37ms
step:418/2110 train_time:15199ms step_avg:36.36ms
step:419/2110 train_time:15232ms step_avg:36.35ms
step:420/2110 train_time:15266ms step_avg:36.35ms
step:421/2110 train_time:15301ms step_avg:36.34ms
step:422/2110 train_time:15333ms step_avg:36.34ms
step:423/2110 train_time:15368ms step_avg:36.33ms
step:424/2110 train_time:15401ms step_avg:36.32ms
step:425/2110 train_time:15437ms step_avg:36.32ms
step:426/2110 train_time:15470ms step_avg:36.31ms
step:427/2110 train_time:15505ms step_avg:36.31ms
step:428/2110 train_time:15538ms step_avg:36.30ms
step:429/2110 train_time:15574ms step_avg:36.30ms
step:430/2110 train_time:15608ms step_avg:36.30ms
step:431/2110 train_time:15645ms step_avg:36.30ms
step:432/2110 train_time:15677ms step_avg:36.29ms
step:433/2110 train_time:15714ms step_avg:36.29ms
step:434/2110 train_time:15746ms step_avg:36.28ms
step:435/2110 train_time:15782ms step_avg:36.28ms
step:436/2110 train_time:15814ms step_avg:36.27ms
step:437/2110 train_time:15850ms step_avg:36.27ms
step:438/2110 train_time:15884ms step_avg:36.26ms
step:439/2110 train_time:15921ms step_avg:36.27ms
step:440/2110 train_time:15953ms step_avg:36.26ms
step:441/2110 train_time:15986ms step_avg:36.25ms
step:442/2110 train_time:16018ms step_avg:36.24ms
step:443/2110 train_time:16052ms step_avg:36.24ms
step:444/2110 train_time:16086ms step_avg:36.23ms
step:445/2110 train_time:16120ms step_avg:36.23ms
step:446/2110 train_time:16154ms step_avg:36.22ms
step:447/2110 train_time:16189ms step_avg:36.22ms
step:448/2110 train_time:16223ms step_avg:36.21ms
step:449/2110 train_time:16259ms step_avg:36.21ms
step:450/2110 train_time:16294ms step_avg:36.21ms
step:451/2110 train_time:16328ms step_avg:36.20ms
step:452/2110 train_time:16362ms step_avg:36.20ms
step:453/2110 train_time:16397ms step_avg:36.20ms
step:454/2110 train_time:16432ms step_avg:36.19ms
step:455/2110 train_time:16468ms step_avg:36.19ms
step:456/2110 train_time:16501ms step_avg:36.19ms
step:457/2110 train_time:16537ms step_avg:36.19ms
step:458/2110 train_time:16571ms step_avg:36.18ms
step:459/2110 train_time:16607ms step_avg:36.18ms
step:460/2110 train_time:16641ms step_avg:36.18ms
step:461/2110 train_time:16676ms step_avg:36.17ms
step:462/2110 train_time:16709ms step_avg:36.17ms
step:463/2110 train_time:16745ms step_avg:36.17ms
step:464/2110 train_time:16778ms step_avg:36.16ms
step:465/2110 train_time:16812ms step_avg:36.15ms
step:466/2110 train_time:16845ms step_avg:36.15ms
step:467/2110 train_time:16879ms step_avg:36.14ms
step:468/2110 train_time:16913ms step_avg:36.14ms
step:469/2110 train_time:16947ms step_avg:36.13ms
step:470/2110 train_time:16980ms step_avg:36.13ms
step:471/2110 train_time:17015ms step_avg:36.12ms
step:472/2110 train_time:17047ms step_avg:36.12ms
step:473/2110 train_time:17083ms step_avg:36.12ms
step:474/2110 train_time:17117ms step_avg:36.11ms
step:475/2110 train_time:17152ms step_avg:36.11ms
step:476/2110 train_time:17185ms step_avg:36.10ms
step:477/2110 train_time:17221ms step_avg:36.10ms
step:478/2110 train_time:17257ms step_avg:36.10ms
step:479/2110 train_time:17292ms step_avg:36.10ms
step:480/2110 train_time:17324ms step_avg:36.09ms
step:481/2110 train_time:17360ms step_avg:36.09ms
step:482/2110 train_time:17393ms step_avg:36.08ms
step:483/2110 train_time:17430ms step_avg:36.09ms
step:484/2110 train_time:17462ms step_avg:36.08ms
step:485/2110 train_time:17497ms step_avg:36.08ms
step:486/2110 train_time:17531ms step_avg:36.07ms
step:487/2110 train_time:17567ms step_avg:36.07ms
step:488/2110 train_time:17601ms step_avg:36.07ms
step:489/2110 train_time:17635ms step_avg:36.06ms
step:490/2110 train_time:17667ms step_avg:36.05ms
step:491/2110 train_time:17700ms step_avg:36.05ms
step:492/2110 train_time:17734ms step_avg:36.04ms
step:493/2110 train_time:17767ms step_avg:36.04ms
step:494/2110 train_time:17800ms step_avg:36.03ms
step:495/2110 train_time:17835ms step_avg:36.03ms
step:496/2110 train_time:17869ms step_avg:36.03ms
step:497/2110 train_time:17903ms step_avg:36.02ms
step:498/2110 train_time:17936ms step_avg:36.02ms
step:499/2110 train_time:17969ms step_avg:36.01ms
step:500/2110 train_time:18002ms step_avg:36.00ms
step:500/2110 val_loss:4.0305 train_time:18040ms step_avg:36.08ms
step:501/2110 train_time:18065ms step_avg:36.06ms
step:502/2110 train_time:18087ms step_avg:36.03ms
step:503/2110 train_time:18112ms step_avg:36.01ms
step:504/2110 train_time:18145ms step_avg:36.00ms
step:505/2110 train_time:18180ms step_avg:36.00ms
step:506/2110 train_time:18214ms step_avg:36.00ms
step:507/2110 train_time:18249ms step_avg:35.99ms
step:508/2110 train_time:18283ms step_avg:35.99ms
step:509/2110 train_time:18319ms step_avg:35.99ms
step:510/2110 train_time:18351ms step_avg:35.98ms
step:511/2110 train_time:18388ms step_avg:35.98ms
step:512/2110 train_time:18421ms step_avg:35.98ms
step:513/2110 train_time:18455ms step_avg:35.97ms
step:514/2110 train_time:18487ms step_avg:35.97ms
step:515/2110 train_time:18520ms step_avg:35.96ms
step:516/2110 train_time:18553ms step_avg:35.96ms
step:517/2110 train_time:18586ms step_avg:35.95ms
step:518/2110 train_time:18618ms step_avg:35.94ms
step:519/2110 train_time:18651ms step_avg:35.94ms
step:520/2110 train_time:18683ms step_avg:35.93ms
step:521/2110 train_time:18717ms step_avg:35.92ms
step:522/2110 train_time:18749ms step_avg:35.92ms
step:523/2110 train_time:18782ms step_avg:35.91ms
step:524/2110 train_time:18814ms step_avg:35.91ms
step:525/2110 train_time:18847ms step_avg:35.90ms
step:526/2110 train_time:18880ms step_avg:35.89ms
step:527/2110 train_time:18913ms step_avg:35.89ms
step:528/2110 train_time:18945ms step_avg:35.88ms
step:529/2110 train_time:18979ms step_avg:35.88ms
step:530/2110 train_time:19012ms step_avg:35.87ms
step:531/2110 train_time:19045ms step_avg:35.87ms
step:532/2110 train_time:19079ms step_avg:35.86ms
step:533/2110 train_time:19113ms step_avg:35.86ms
step:534/2110 train_time:19145ms step_avg:35.85ms
step:535/2110 train_time:19179ms step_avg:35.85ms
step:536/2110 train_time:19213ms step_avg:35.84ms
step:537/2110 train_time:19246ms step_avg:35.84ms
step:538/2110 train_time:19279ms step_avg:35.83ms
step:539/2110 train_time:19313ms step_avg:35.83ms
step:540/2110 train_time:19346ms step_avg:35.83ms
step:541/2110 train_time:19382ms step_avg:35.83ms
step:542/2110 train_time:19416ms step_avg:35.82ms
step:543/2110 train_time:19452ms step_avg:35.82ms
step:544/2110 train_time:19485ms step_avg:35.82ms
step:545/2110 train_time:19519ms step_avg:35.81ms
step:546/2110 train_time:19551ms step_avg:35.81ms
step:547/2110 train_time:19584ms step_avg:35.80ms
step:548/2110 train_time:19618ms step_avg:35.80ms
step:549/2110 train_time:19652ms step_avg:35.80ms
step:550/2110 train_time:19685ms step_avg:35.79ms
step:551/2110 train_time:19721ms step_avg:35.79ms
step:552/2110 train_time:19753ms step_avg:35.78ms
step:553/2110 train_time:19789ms step_avg:35.79ms
step:554/2110 train_time:19822ms step_avg:35.78ms
step:555/2110 train_time:19857ms step_avg:35.78ms
step:556/2110 train_time:19890ms step_avg:35.77ms
step:557/2110 train_time:19924ms step_avg:35.77ms
step:558/2110 train_time:19957ms step_avg:35.77ms
step:559/2110 train_time:19991ms step_avg:35.76ms
step:560/2110 train_time:20024ms step_avg:35.76ms
step:561/2110 train_time:20058ms step_avg:35.75ms
step:562/2110 train_time:20090ms step_avg:35.75ms
step:563/2110 train_time:20123ms step_avg:35.74ms
step:564/2110 train_time:20158ms step_avg:35.74ms
step:565/2110 train_time:20192ms step_avg:35.74ms
step:566/2110 train_time:20225ms step_avg:35.73ms
step:567/2110 train_time:20259ms step_avg:35.73ms
step:568/2110 train_time:20292ms step_avg:35.72ms
step:569/2110 train_time:20328ms step_avg:35.73ms
step:570/2110 train_time:20360ms step_avg:35.72ms
step:571/2110 train_time:20395ms step_avg:35.72ms
step:572/2110 train_time:20428ms step_avg:35.71ms
step:573/2110 train_time:20463ms step_avg:35.71ms
step:574/2110 train_time:20496ms step_avg:35.71ms
step:575/2110 train_time:20530ms step_avg:35.71ms
step:576/2110 train_time:20565ms step_avg:35.70ms
step:577/2110 train_time:20599ms step_avg:35.70ms
step:578/2110 train_time:20632ms step_avg:35.70ms
step:579/2110 train_time:20665ms step_avg:35.69ms
step:580/2110 train_time:20698ms step_avg:35.69ms
step:581/2110 train_time:20731ms step_avg:35.68ms
step:582/2110 train_time:20764ms step_avg:35.68ms
step:583/2110 train_time:20797ms step_avg:35.67ms
step:584/2110 train_time:20829ms step_avg:35.67ms
step:585/2110 train_time:20862ms step_avg:35.66ms
step:586/2110 train_time:20895ms step_avg:35.66ms
step:587/2110 train_time:20928ms step_avg:35.65ms
step:588/2110 train_time:20960ms step_avg:35.65ms
step:589/2110 train_time:20993ms step_avg:35.64ms
step:590/2110 train_time:21026ms step_avg:35.64ms
step:591/2110 train_time:21059ms step_avg:35.63ms
step:592/2110 train_time:21092ms step_avg:35.63ms
step:593/2110 train_time:21125ms step_avg:35.62ms
step:594/2110 train_time:21158ms step_avg:35.62ms
step:595/2110 train_time:21192ms step_avg:35.62ms
step:596/2110 train_time:21225ms step_avg:35.61ms
step:597/2110 train_time:21258ms step_avg:35.61ms
step:598/2110 train_time:21291ms step_avg:35.60ms
step:599/2110 train_time:21325ms step_avg:35.60ms
step:600/2110 train_time:21357ms step_avg:35.60ms
step:601/2110 train_time:21391ms step_avg:35.59ms
step:602/2110 train_time:21424ms step_avg:35.59ms
step:603/2110 train_time:21457ms step_avg:35.58ms
step:604/2110 train_time:21489ms step_avg:35.58ms
step:605/2110 train_time:21523ms step_avg:35.57ms
step:606/2110 train_time:21556ms step_avg:35.57ms
step:607/2110 train_time:21589ms step_avg:35.57ms
step:608/2110 train_time:21622ms step_avg:35.56ms
step:609/2110 train_time:21655ms step_avg:35.56ms
step:610/2110 train_time:21687ms step_avg:35.55ms
step:611/2110 train_time:21721ms step_avg:35.55ms
step:612/2110 train_time:21754ms step_avg:35.55ms
step:613/2110 train_time:21787ms step_avg:35.54ms
step:614/2110 train_time:21819ms step_avg:35.54ms
step:615/2110 train_time:21853ms step_avg:35.53ms
step:616/2110 train_time:21885ms step_avg:35.53ms
step:617/2110 train_time:21918ms step_avg:35.52ms
step:618/2110 train_time:21951ms step_avg:35.52ms
step:619/2110 train_time:21984ms step_avg:35.52ms
step:620/2110 train_time:22017ms step_avg:35.51ms
step:621/2110 train_time:22050ms step_avg:35.51ms
step:622/2110 train_time:22083ms step_avg:35.50ms
step:623/2110 train_time:22117ms step_avg:35.50ms
step:624/2110 train_time:22149ms step_avg:35.49ms
step:625/2110 train_time:22182ms step_avg:35.49ms
step:626/2110 train_time:22216ms step_avg:35.49ms
step:627/2110 train_time:22257ms step_avg:35.50ms
step:628/2110 train_time:22295ms step_avg:35.50ms
step:629/2110 train_time:22334ms step_avg:35.51ms
step:630/2110 train_time:22372ms step_avg:35.51ms
step:631/2110 train_time:22412ms step_avg:35.52ms
step:632/2110 train_time:22449ms step_avg:35.52ms
step:633/2110 train_time:22487ms step_avg:35.52ms
step:634/2110 train_time:22523ms step_avg:35.53ms
step:635/2110 train_time:22561ms step_avg:35.53ms
step:636/2110 train_time:22597ms step_avg:35.53ms
step:637/2110 train_time:22635ms step_avg:35.53ms
step:638/2110 train_time:22669ms step_avg:35.53ms
step:639/2110 train_time:22703ms step_avg:35.53ms
step:640/2110 train_time:22736ms step_avg:35.52ms
step:641/2110 train_time:22768ms step_avg:35.52ms
step:642/2110 train_time:22801ms step_avg:35.52ms
step:643/2110 train_time:22834ms step_avg:35.51ms
step:644/2110 train_time:22866ms step_avg:35.51ms
step:645/2110 train_time:22899ms step_avg:35.50ms
step:646/2110 train_time:22932ms step_avg:35.50ms
step:647/2110 train_time:22965ms step_avg:35.49ms
step:648/2110 train_time:22997ms step_avg:35.49ms
step:649/2110 train_time:23030ms step_avg:35.49ms
step:650/2110 train_time:23063ms step_avg:35.48ms
step:651/2110 train_time:23096ms step_avg:35.48ms
step:652/2110 train_time:23128ms step_avg:35.47ms
step:653/2110 train_time:23161ms step_avg:35.47ms
step:654/2110 train_time:23193ms step_avg:35.46ms
step:655/2110 train_time:23226ms step_avg:35.46ms
step:656/2110 train_time:23259ms step_avg:35.46ms
step:657/2110 train_time:23292ms step_avg:35.45ms
step:658/2110 train_time:23325ms step_avg:35.45ms
step:659/2110 train_time:23358ms step_avg:35.44ms
step:660/2110 train_time:23391ms step_avg:35.44ms
step:661/2110 train_time:23426ms step_avg:35.44ms
step:662/2110 train_time:23458ms step_avg:35.44ms
step:663/2110 train_time:23492ms step_avg:35.43ms
step:664/2110 train_time:23525ms step_avg:35.43ms
step:665/2110 train_time:23559ms step_avg:35.43ms
step:666/2110 train_time:23592ms step_avg:35.42ms
step:667/2110 train_time:23626ms step_avg:35.42ms
step:668/2110 train_time:23659ms step_avg:35.42ms
step:669/2110 train_time:23692ms step_avg:35.41ms
step:670/2110 train_time:23725ms step_avg:35.41ms
step:671/2110 train_time:23759ms step_avg:35.41ms
step:672/2110 train_time:23791ms step_avg:35.40ms
step:673/2110 train_time:23825ms step_avg:35.40ms
step:674/2110 train_time:23857ms step_avg:35.40ms
step:675/2110 train_time:23892ms step_avg:35.40ms
step:676/2110 train_time:23923ms step_avg:35.39ms
step:677/2110 train_time:23956ms step_avg:35.38ms
step:678/2110 train_time:23988ms step_avg:35.38ms
step:679/2110 train_time:24021ms step_avg:35.38ms
step:680/2110 train_time:24054ms step_avg:35.37ms
step:681/2110 train_time:24087ms step_avg:35.37ms
step:682/2110 train_time:24119ms step_avg:35.36ms
step:683/2110 train_time:24152ms step_avg:35.36ms
step:684/2110 train_time:24185ms step_avg:35.36ms
step:685/2110 train_time:24218ms step_avg:35.35ms
step:686/2110 train_time:24251ms step_avg:35.35ms
step:687/2110 train_time:24284ms step_avg:35.35ms
step:688/2110 train_time:24316ms step_avg:35.34ms
step:689/2110 train_time:24350ms step_avg:35.34ms
step:690/2110 train_time:24382ms step_avg:35.34ms
step:691/2110 train_time:24416ms step_avg:35.33ms
step:692/2110 train_time:24474ms step_avg:35.37ms
step:693/2110 train_time:24534ms step_avg:35.40ms
step:694/2110 train_time:24593ms step_avg:35.44ms
step:695/2110 train_time:24653ms step_avg:35.47ms
step:696/2110 train_time:24711ms step_avg:35.51ms
step:697/2110 train_time:24771ms step_avg:35.54ms
step:698/2110 train_time:24829ms step_avg:35.57ms
step:699/2110 train_time:24889ms step_avg:35.61ms
step:700/2110 train_time:24946ms step_avg:35.64ms
step:701/2110 train_time:25007ms step_avg:35.67ms
step:702/2110 train_time:25065ms step_avg:35.71ms
step:703/2110 train_time:25125ms step_avg:35.74ms
step:704/2110 train_time:25183ms step_avg:35.77ms
step:705/2110 train_time:25244ms step_avg:35.81ms
step:706/2110 train_time:25302ms step_avg:35.84ms
step:707/2110 train_time:25362ms step_avg:35.87ms
step:708/2110 train_time:25420ms step_avg:35.90ms
step:709/2110 train_time:25481ms step_avg:35.94ms
step:710/2110 train_time:25540ms step_avg:35.97ms
step:711/2110 train_time:25601ms step_avg:36.01ms
step:712/2110 train_time:25660ms step_avg:36.04ms
step:713/2110 train_time:25720ms step_avg:36.07ms
step:714/2110 train_time:25778ms step_avg:36.10ms
step:715/2110 train_time:25838ms step_avg:36.14ms
step:716/2110 train_time:25896ms step_avg:36.17ms
step:717/2110 train_time:25956ms step_avg:36.20ms
step:718/2110 train_time:26014ms step_avg:36.23ms
step:719/2110 train_time:26073ms step_avg:36.26ms
step:720/2110 train_time:26131ms step_avg:36.29ms
step:721/2110 train_time:26190ms step_avg:36.33ms
step:722/2110 train_time:26249ms step_avg:36.36ms
step:723/2110 train_time:26309ms step_avg:36.39ms
step:724/2110 train_time:26367ms step_avg:36.42ms
step:725/2110 train_time:26427ms step_avg:36.45ms
step:726/2110 train_time:26485ms step_avg:36.48ms
step:727/2110 train_time:26546ms step_avg:36.51ms
step:728/2110 train_time:26605ms step_avg:36.55ms
step:729/2110 train_time:26665ms step_avg:36.58ms
step:730/2110 train_time:26724ms step_avg:36.61ms
step:731/2110 train_time:26785ms step_avg:36.64ms
step:732/2110 train_time:26843ms step_avg:36.67ms
step:733/2110 train_time:26904ms step_avg:36.70ms
step:734/2110 train_time:26962ms step_avg:36.73ms
step:735/2110 train_time:27022ms step_avg:36.77ms
step:736/2110 train_time:27081ms step_avg:36.79ms
step:737/2110 train_time:27141ms step_avg:36.83ms
step:738/2110 train_time:27200ms step_avg:36.86ms
step:739/2110 train_time:27260ms step_avg:36.89ms
step:740/2110 train_time:27318ms step_avg:36.92ms
step:741/2110 train_time:27377ms step_avg:36.95ms
step:742/2110 train_time:27435ms step_avg:36.98ms
step:743/2110 train_time:27496ms step_avg:37.01ms
step:744/2110 train_time:27554ms step_avg:37.03ms
step:745/2110 train_time:27614ms step_avg:37.07ms
step:746/2110 train_time:27672ms step_avg:37.09ms
step:747/2110 train_time:27733ms step_avg:37.13ms
step:748/2110 train_time:27791ms step_avg:37.15ms
step:749/2110 train_time:27850ms step_avg:37.18ms
step:750/2110 train_time:27909ms step_avg:37.21ms
step:750/2110 val_loss:3.9089 train_time:27971ms step_avg:37.29ms
step:751/2110 train_time:27996ms step_avg:37.28ms
step:752/2110 train_time:28029ms step_avg:37.27ms
step:753/2110 train_time:28092ms step_avg:37.31ms
step:754/2110 train_time:28154ms step_avg:37.34ms
step:755/2110 train_time:28214ms step_avg:37.37ms
step:756/2110 train_time:28273ms step_avg:37.40ms
step:757/2110 train_time:28333ms step_avg:37.43ms
step:758/2110 train_time:28391ms step_avg:37.46ms
step:759/2110 train_time:28450ms step_avg:37.48ms
step:760/2110 train_time:28508ms step_avg:37.51ms
step:761/2110 train_time:28566ms step_avg:37.54ms
step:762/2110 train_time:28624ms step_avg:37.56ms
step:763/2110 train_time:28683ms step_avg:37.59ms
step:764/2110 train_time:28740ms step_avg:37.62ms
step:765/2110 train_time:28800ms step_avg:37.65ms
step:766/2110 train_time:28858ms step_avg:37.67ms
step:767/2110 train_time:28918ms step_avg:37.70ms
step:768/2110 train_time:28977ms step_avg:37.73ms
step:769/2110 train_time:29038ms step_avg:37.76ms
step:770/2110 train_time:29098ms step_avg:37.79ms
step:771/2110 train_time:29160ms step_avg:37.82ms
step:772/2110 train_time:29219ms step_avg:37.85ms
step:773/2110 train_time:29279ms step_avg:37.88ms
step:774/2110 train_time:29338ms step_avg:37.90ms
step:775/2110 train_time:29398ms step_avg:37.93ms
step:776/2110 train_time:29456ms step_avg:37.96ms
step:777/2110 train_time:29516ms step_avg:37.99ms
step:778/2110 train_time:29575ms step_avg:38.01ms
step:779/2110 train_time:29634ms step_avg:38.04ms
step:780/2110 train_time:29691ms step_avg:38.07ms
step:781/2110 train_time:29750ms step_avg:38.09ms
step:782/2110 train_time:29807ms step_avg:38.12ms
step:783/2110 train_time:29866ms step_avg:38.14ms
step:784/2110 train_time:29924ms step_avg:38.17ms
step:785/2110 train_time:29984ms step_avg:38.20ms
step:786/2110 train_time:30042ms step_avg:38.22ms
step:787/2110 train_time:30103ms step_avg:38.25ms
step:788/2110 train_time:30162ms step_avg:38.28ms
step:789/2110 train_time:30223ms step_avg:38.31ms
step:790/2110 train_time:30282ms step_avg:38.33ms
step:791/2110 train_time:30343ms step_avg:38.36ms
step:792/2110 train_time:30400ms step_avg:38.38ms
step:793/2110 train_time:30460ms step_avg:38.41ms
step:794/2110 train_time:30519ms step_avg:38.44ms
step:795/2110 train_time:30579ms step_avg:38.46ms
step:796/2110 train_time:30637ms step_avg:38.49ms
step:797/2110 train_time:30697ms step_avg:38.52ms
step:798/2110 train_time:30755ms step_avg:38.54ms
step:799/2110 train_time:30815ms step_avg:38.57ms
step:800/2110 train_time:30873ms step_avg:38.59ms
step:801/2110 train_time:30933ms step_avg:38.62ms
step:802/2110 train_time:30992ms step_avg:38.64ms
step:803/2110 train_time:31052ms step_avg:38.67ms
step:804/2110 train_time:31110ms step_avg:38.69ms
step:805/2110 train_time:31171ms step_avg:38.72ms
step:806/2110 train_time:31229ms step_avg:38.75ms
step:807/2110 train_time:31290ms step_avg:38.77ms
step:808/2110 train_time:31348ms step_avg:38.80ms
step:809/2110 train_time:31408ms step_avg:38.82ms
step:810/2110 train_time:31466ms step_avg:38.85ms
step:811/2110 train_time:31526ms step_avg:38.87ms
step:812/2110 train_time:31584ms step_avg:38.90ms
step:813/2110 train_time:31644ms step_avg:38.92ms
step:814/2110 train_time:31701ms step_avg:38.95ms
step:815/2110 train_time:31761ms step_avg:38.97ms
step:816/2110 train_time:31820ms step_avg:38.99ms
step:817/2110 train_time:31879ms step_avg:39.02ms
step:818/2110 train_time:31937ms step_avg:39.04ms
step:819/2110 train_time:31997ms step_avg:39.07ms
step:820/2110 train_time:32056ms step_avg:39.09ms
step:821/2110 train_time:32117ms step_avg:39.12ms
step:822/2110 train_time:32176ms step_avg:39.14ms
step:823/2110 train_time:32236ms step_avg:39.17ms
step:824/2110 train_time:32295ms step_avg:39.19ms
step:825/2110 train_time:32356ms step_avg:39.22ms
step:826/2110 train_time:32414ms step_avg:39.24ms
step:827/2110 train_time:32474ms step_avg:39.27ms
step:828/2110 train_time:32532ms step_avg:39.29ms
step:829/2110 train_time:32592ms step_avg:39.31ms
step:830/2110 train_time:32649ms step_avg:39.34ms
step:831/2110 train_time:32708ms step_avg:39.36ms
step:832/2110 train_time:32766ms step_avg:39.38ms
step:833/2110 train_time:32825ms step_avg:39.41ms
step:834/2110 train_time:32883ms step_avg:39.43ms
step:835/2110 train_time:32944ms step_avg:39.45ms
step:836/2110 train_time:33002ms step_avg:39.48ms
step:837/2110 train_time:33062ms step_avg:39.50ms
step:838/2110 train_time:33121ms step_avg:39.52ms
step:839/2110 train_time:33181ms step_avg:39.55ms
step:840/2110 train_time:33240ms step_avg:39.57ms
step:841/2110 train_time:33300ms step_avg:39.60ms
step:842/2110 train_time:33359ms step_avg:39.62ms
step:843/2110 train_time:33420ms step_avg:39.64ms
step:844/2110 train_time:33479ms step_avg:39.67ms
step:845/2110 train_time:33538ms step_avg:39.69ms
step:846/2110 train_time:33597ms step_avg:39.71ms
step:847/2110 train_time:33657ms step_avg:39.74ms
step:848/2110 train_time:33715ms step_avg:39.76ms
step:849/2110 train_time:33775ms step_avg:39.78ms
step:850/2110 train_time:33832ms step_avg:39.80ms
step:851/2110 train_time:33892ms step_avg:39.83ms
step:852/2110 train_time:33950ms step_avg:39.85ms
step:853/2110 train_time:34010ms step_avg:39.87ms
step:854/2110 train_time:34069ms step_avg:39.89ms
step:855/2110 train_time:34129ms step_avg:39.92ms
step:856/2110 train_time:34187ms step_avg:39.94ms
step:857/2110 train_time:34247ms step_avg:39.96ms
step:858/2110 train_time:34305ms step_avg:39.98ms
step:859/2110 train_time:34365ms step_avg:40.01ms
step:860/2110 train_time:34424ms step_avg:40.03ms
step:861/2110 train_time:34484ms step_avg:40.05ms
step:862/2110 train_time:34542ms step_avg:40.07ms
step:863/2110 train_time:34602ms step_avg:40.10ms
step:864/2110 train_time:34661ms step_avg:40.12ms
step:865/2110 train_time:34721ms step_avg:40.14ms
step:866/2110 train_time:34779ms step_avg:40.16ms
step:867/2110 train_time:34838ms step_avg:40.18ms
step:868/2110 train_time:34897ms step_avg:40.20ms
step:869/2110 train_time:34958ms step_avg:40.23ms
step:870/2110 train_time:35017ms step_avg:40.25ms
step:871/2110 train_time:35077ms step_avg:40.27ms
step:872/2110 train_time:35136ms step_avg:40.29ms
step:873/2110 train_time:35196ms step_avg:40.32ms
step:874/2110 train_time:35254ms step_avg:40.34ms
step:875/2110 train_time:35315ms step_avg:40.36ms
step:876/2110 train_time:35373ms step_avg:40.38ms
step:877/2110 train_time:35433ms step_avg:40.40ms
step:878/2110 train_time:35491ms step_avg:40.42ms
step:879/2110 train_time:35551ms step_avg:40.44ms
step:880/2110 train_time:35609ms step_avg:40.46ms
step:881/2110 train_time:35668ms step_avg:40.49ms
step:882/2110 train_time:35726ms step_avg:40.51ms
step:883/2110 train_time:35786ms step_avg:40.53ms
step:884/2110 train_time:35843ms step_avg:40.55ms
step:885/2110 train_time:35903ms step_avg:40.57ms
step:886/2110 train_time:35962ms step_avg:40.59ms
step:887/2110 train_time:36022ms step_avg:40.61ms
step:888/2110 train_time:36080ms step_avg:40.63ms
step:889/2110 train_time:36140ms step_avg:40.65ms
step:890/2110 train_time:36198ms step_avg:40.67ms
step:891/2110 train_time:36259ms step_avg:40.69ms
step:892/2110 train_time:36317ms step_avg:40.71ms
step:893/2110 train_time:36378ms step_avg:40.74ms
step:894/2110 train_time:36436ms step_avg:40.76ms
step:895/2110 train_time:36497ms step_avg:40.78ms
step:896/2110 train_time:36555ms step_avg:40.80ms
step:897/2110 train_time:36616ms step_avg:40.82ms
step:898/2110 train_time:36675ms step_avg:40.84ms
step:899/2110 train_time:36735ms step_avg:40.86ms
step:900/2110 train_time:36793ms step_avg:40.88ms
step:901/2110 train_time:36853ms step_avg:40.90ms
step:902/2110 train_time:36911ms step_avg:40.92ms
step:903/2110 train_time:36970ms step_avg:40.94ms
step:904/2110 train_time:37028ms step_avg:40.96ms
step:905/2110 train_time:37088ms step_avg:40.98ms
step:906/2110 train_time:37146ms step_avg:41.00ms
step:907/2110 train_time:37206ms step_avg:41.02ms
step:908/2110 train_time:37264ms step_avg:41.04ms
step:909/2110 train_time:37324ms step_avg:41.06ms
step:910/2110 train_time:37382ms step_avg:41.08ms
step:911/2110 train_time:37443ms step_avg:41.10ms
step:912/2110 train_time:37501ms step_avg:41.12ms
step:913/2110 train_time:37561ms step_avg:41.14ms
step:914/2110 train_time:37620ms step_avg:41.16ms
step:915/2110 train_time:37680ms step_avg:41.18ms
step:916/2110 train_time:37739ms step_avg:41.20ms
step:917/2110 train_time:37799ms step_avg:41.22ms
step:918/2110 train_time:37858ms step_avg:41.24ms
step:919/2110 train_time:37919ms step_avg:41.26ms
step:920/2110 train_time:37978ms step_avg:41.28ms
step:921/2110 train_time:38037ms step_avg:41.30ms
step:922/2110 train_time:38096ms step_avg:41.32ms
step:923/2110 train_time:38157ms step_avg:41.34ms
step:924/2110 train_time:38216ms step_avg:41.36ms
step:925/2110 train_time:38275ms step_avg:41.38ms
step:926/2110 train_time:38334ms step_avg:41.40ms
step:927/2110 train_time:38394ms step_avg:41.42ms
step:928/2110 train_time:38452ms step_avg:41.44ms
step:929/2110 train_time:38512ms step_avg:41.45ms
step:930/2110 train_time:38570ms step_avg:41.47ms
step:931/2110 train_time:38629ms step_avg:41.49ms
step:932/2110 train_time:38687ms step_avg:41.51ms
step:933/2110 train_time:38746ms step_avg:41.53ms
step:934/2110 train_time:38804ms step_avg:41.55ms
step:935/2110 train_time:38864ms step_avg:41.57ms
step:936/2110 train_time:38921ms step_avg:41.58ms
step:937/2110 train_time:38982ms step_avg:41.60ms
step:938/2110 train_time:39040ms step_avg:41.62ms
step:939/2110 train_time:39100ms step_avg:41.64ms
step:940/2110 train_time:39160ms step_avg:41.66ms
step:941/2110 train_time:39220ms step_avg:41.68ms
step:942/2110 train_time:39278ms step_avg:41.70ms
step:943/2110 train_time:39338ms step_avg:41.72ms
step:944/2110 train_time:39397ms step_avg:41.73ms
step:945/2110 train_time:39458ms step_avg:41.75ms
step:946/2110 train_time:39517ms step_avg:41.77ms
step:947/2110 train_time:39577ms step_avg:41.79ms
step:948/2110 train_time:39635ms step_avg:41.81ms
step:949/2110 train_time:39696ms step_avg:41.83ms
step:950/2110 train_time:39754ms step_avg:41.85ms
step:951/2110 train_time:39814ms step_avg:41.87ms
step:952/2110 train_time:39873ms step_avg:41.88ms
step:953/2110 train_time:39933ms step_avg:41.90ms
step:954/2110 train_time:39991ms step_avg:41.92ms
step:955/2110 train_time:40050ms step_avg:41.94ms
step:956/2110 train_time:40108ms step_avg:41.95ms
step:957/2110 train_time:40168ms step_avg:41.97ms
step:958/2110 train_time:40226ms step_avg:41.99ms
step:959/2110 train_time:40287ms step_avg:42.01ms
step:960/2110 train_time:40345ms step_avg:42.03ms
step:961/2110 train_time:40406ms step_avg:42.05ms
step:962/2110 train_time:40463ms step_avg:42.06ms
step:963/2110 train_time:40524ms step_avg:42.08ms
step:964/2110 train_time:40582ms step_avg:42.10ms
step:965/2110 train_time:40643ms step_avg:42.12ms
step:966/2110 train_time:40701ms step_avg:42.13ms
step:967/2110 train_time:40761ms step_avg:42.15ms
step:968/2110 train_time:40819ms step_avg:42.17ms
step:969/2110 train_time:40879ms step_avg:42.19ms
step:970/2110 train_time:40938ms step_avg:42.20ms
step:971/2110 train_time:40998ms step_avg:42.22ms
step:972/2110 train_time:41058ms step_avg:42.24ms
step:973/2110 train_time:41119ms step_avg:42.26ms
step:974/2110 train_time:41178ms step_avg:42.28ms
step:975/2110 train_time:41237ms step_avg:42.29ms
step:976/2110 train_time:41295ms step_avg:42.31ms
step:977/2110 train_time:41355ms step_avg:42.33ms
step:978/2110 train_time:41414ms step_avg:42.35ms
step:979/2110 train_time:41473ms step_avg:42.36ms
step:980/2110 train_time:41531ms step_avg:42.38ms
step:981/2110 train_time:41590ms step_avg:42.40ms
step:982/2110 train_time:41648ms step_avg:42.41ms
step:983/2110 train_time:41708ms step_avg:42.43ms
step:984/2110 train_time:41766ms step_avg:42.44ms
step:985/2110 train_time:41825ms step_avg:42.46ms
step:986/2110 train_time:41884ms step_avg:42.48ms
step:987/2110 train_time:41945ms step_avg:42.50ms
step:988/2110 train_time:42003ms step_avg:42.51ms
step:989/2110 train_time:42064ms step_avg:42.53ms
step:990/2110 train_time:42123ms step_avg:42.55ms
step:991/2110 train_time:42183ms step_avg:42.57ms
step:992/2110 train_time:42241ms step_avg:42.58ms
step:993/2110 train_time:42301ms step_avg:42.60ms
step:994/2110 train_time:42360ms step_avg:42.62ms
step:995/2110 train_time:42421ms step_avg:42.63ms
step:996/2110 train_time:42479ms step_avg:42.65ms
step:997/2110 train_time:42540ms step_avg:42.67ms
step:998/2110 train_time:42599ms step_avg:42.68ms
step:999/2110 train_time:42660ms step_avg:42.70ms
step:1000/2110 train_time:42718ms step_avg:42.72ms
step:1000/2110 val_loss:3.7563 train_time:42780ms step_avg:42.78ms
step:1001/2110 train_time:42806ms step_avg:42.76ms
step:1002/2110 train_time:42842ms step_avg:42.76ms
step:1003/2110 train_time:42903ms step_avg:42.78ms
step:1004/2110 train_time:42967ms step_avg:42.80ms
step:1005/2110 train_time:43027ms step_avg:42.81ms
step:1006/2110 train_time:43086ms step_avg:42.83ms
step:1007/2110 train_time:43145ms step_avg:42.84ms
step:1008/2110 train_time:43202ms step_avg:42.86ms
step:1009/2110 train_time:43261ms step_avg:42.87ms
step:1010/2110 train_time:43319ms step_avg:42.89ms
step:1011/2110 train_time:43377ms step_avg:42.91ms
step:1012/2110 train_time:43434ms step_avg:42.92ms
step:1013/2110 train_time:43493ms step_avg:42.93ms
step:1014/2110 train_time:43551ms step_avg:42.95ms
step:1015/2110 train_time:43610ms step_avg:42.97ms
step:1016/2110 train_time:43667ms step_avg:42.98ms
step:1017/2110 train_time:43730ms step_avg:43.00ms
step:1018/2110 train_time:43790ms step_avg:43.02ms
step:1019/2110 train_time:43853ms step_avg:43.04ms
step:1020/2110 train_time:43913ms step_avg:43.05ms
step:1021/2110 train_time:43975ms step_avg:43.07ms
step:1022/2110 train_time:44034ms step_avg:43.09ms
step:1023/2110 train_time:44095ms step_avg:43.10ms
step:1024/2110 train_time:44154ms step_avg:43.12ms
step:1025/2110 train_time:44214ms step_avg:43.14ms
step:1026/2110 train_time:44273ms step_avg:43.15ms
step:1027/2110 train_time:44332ms step_avg:43.17ms
step:1028/2110 train_time:44390ms step_avg:43.18ms
step:1029/2110 train_time:44450ms step_avg:43.20ms
step:1030/2110 train_time:44507ms step_avg:43.21ms
step:1031/2110 train_time:44567ms step_avg:43.23ms
step:1032/2110 train_time:44625ms step_avg:43.24ms
step:1033/2110 train_time:44685ms step_avg:43.26ms
step:1034/2110 train_time:44744ms step_avg:43.27ms
step:1035/2110 train_time:44804ms step_avg:43.29ms
step:1036/2110 train_time:44863ms step_avg:43.30ms
step:1037/2110 train_time:44924ms step_avg:43.32ms
step:1038/2110 train_time:44983ms step_avg:43.34ms
step:1039/2110 train_time:45044ms step_avg:43.35ms
step:1040/2110 train_time:45102ms step_avg:43.37ms
step:1041/2110 train_time:45162ms step_avg:43.38ms
step:1042/2110 train_time:45220ms step_avg:43.40ms
step:1043/2110 train_time:45279ms step_avg:43.41ms
step:1044/2110 train_time:45337ms step_avg:43.43ms
step:1045/2110 train_time:45396ms step_avg:43.44ms
step:1046/2110 train_time:45453ms step_avg:43.45ms
step:1047/2110 train_time:45513ms step_avg:43.47ms
step:1048/2110 train_time:45572ms step_avg:43.48ms
step:1049/2110 train_time:45631ms step_avg:43.50ms
step:1050/2110 train_time:45690ms step_avg:43.51ms
step:1051/2110 train_time:45750ms step_avg:43.53ms
step:1052/2110 train_time:45811ms step_avg:43.55ms
step:1053/2110 train_time:45871ms step_avg:43.56ms
step:1054/2110 train_time:45930ms step_avg:43.58ms
step:1055/2110 train_time:45991ms step_avg:43.59ms
step:1056/2110 train_time:46050ms step_avg:43.61ms
step:1057/2110 train_time:46111ms step_avg:43.62ms
step:1058/2110 train_time:46170ms step_avg:43.64ms
step:1059/2110 train_time:46230ms step_avg:43.65ms
step:1060/2110 train_time:46289ms step_avg:43.67ms
step:1061/2110 train_time:46348ms step_avg:43.68ms
step:1062/2110 train_time:46407ms step_avg:43.70ms
step:1063/2110 train_time:46466ms step_avg:43.71ms
step:1064/2110 train_time:46524ms step_avg:43.73ms
step:1065/2110 train_time:46583ms step_avg:43.74ms
step:1066/2110 train_time:46641ms step_avg:43.75ms
step:1067/2110 train_time:46700ms step_avg:43.77ms
step:1068/2110 train_time:46758ms step_avg:43.78ms
step:1069/2110 train_time:46818ms step_avg:43.80ms
step:1070/2110 train_time:46876ms step_avg:43.81ms
step:1071/2110 train_time:46936ms step_avg:43.82ms
step:1072/2110 train_time:46995ms step_avg:43.84ms
step:1073/2110 train_time:47056ms step_avg:43.85ms
step:1074/2110 train_time:47114ms step_avg:43.87ms
step:1075/2110 train_time:47175ms step_avg:43.88ms
step:1076/2110 train_time:47233ms step_avg:43.90ms
step:1077/2110 train_time:47293ms step_avg:43.91ms
step:1078/2110 train_time:47352ms step_avg:43.93ms
step:1079/2110 train_time:47411ms step_avg:43.94ms
step:1080/2110 train_time:47469ms step_avg:43.95ms
step:1081/2110 train_time:47530ms step_avg:43.97ms
step:1082/2110 train_time:47588ms step_avg:43.98ms
step:1083/2110 train_time:47648ms step_avg:44.00ms
step:1084/2110 train_time:47706ms step_avg:44.01ms
step:1085/2110 train_time:47766ms step_avg:44.02ms
step:1086/2110 train_time:47825ms step_avg:44.04ms
step:1087/2110 train_time:47886ms step_avg:44.05ms
step:1088/2110 train_time:47944ms step_avg:44.07ms
step:1089/2110 train_time:48005ms step_avg:44.08ms
step:1090/2110 train_time:48063ms step_avg:44.09ms
step:1091/2110 train_time:48124ms step_avg:44.11ms
step:1092/2110 train_time:48182ms step_avg:44.12ms
step:1093/2110 train_time:48242ms step_avg:44.14ms
step:1094/2110 train_time:48300ms step_avg:44.15ms
step:1095/2110 train_time:48360ms step_avg:44.16ms
step:1096/2110 train_time:48418ms step_avg:44.18ms
step:1097/2110 train_time:48478ms step_avg:44.19ms
step:1098/2110 train_time:48535ms step_avg:44.20ms
step:1099/2110 train_time:48595ms step_avg:44.22ms
step:1100/2110 train_time:48653ms step_avg:44.23ms
step:1101/2110 train_time:48713ms step_avg:44.24ms
step:1102/2110 train_time:48771ms step_avg:44.26ms
step:1103/2110 train_time:48832ms step_avg:44.27ms
step:1104/2110 train_time:48890ms step_avg:44.28ms
step:1105/2110 train_time:48951ms step_avg:44.30ms
step:1106/2110 train_time:49010ms step_avg:44.31ms
step:1107/2110 train_time:49070ms step_avg:44.33ms
step:1108/2110 train_time:49130ms step_avg:44.34ms
step:1109/2110 train_time:49191ms step_avg:44.36ms
step:1110/2110 train_time:49249ms step_avg:44.37ms
step:1111/2110 train_time:49310ms step_avg:44.38ms
step:1112/2110 train_time:49368ms step_avg:44.40ms
step:1113/2110 train_time:49428ms step_avg:44.41ms
step:1114/2110 train_time:49487ms step_avg:44.42ms
step:1115/2110 train_time:49546ms step_avg:44.44ms
step:1116/2110 train_time:49605ms step_avg:44.45ms
step:1117/2110 train_time:49664ms step_avg:44.46ms
step:1118/2110 train_time:49722ms step_avg:44.47ms
step:1119/2110 train_time:49782ms step_avg:44.49ms
step:1120/2110 train_time:49840ms step_avg:44.50ms
step:1121/2110 train_time:49899ms step_avg:44.51ms
step:1122/2110 train_time:49957ms step_avg:44.52ms
step:1123/2110 train_time:50016ms step_avg:44.54ms
step:1124/2110 train_time:50075ms step_avg:44.55ms
step:1125/2110 train_time:50136ms step_avg:44.57ms
step:1126/2110 train_time:50193ms step_avg:44.58ms
step:1127/2110 train_time:50254ms step_avg:44.59ms
step:1128/2110 train_time:50313ms step_avg:44.60ms
step:1129/2110 train_time:50373ms step_avg:44.62ms
step:1130/2110 train_time:50432ms step_avg:44.63ms
step:1131/2110 train_time:50491ms step_avg:44.64ms
step:1132/2110 train_time:50550ms step_avg:44.66ms
step:1133/2110 train_time:50610ms step_avg:44.67ms
step:1134/2110 train_time:50669ms step_avg:44.68ms
step:1135/2110 train_time:50729ms step_avg:44.70ms
step:1136/2110 train_time:50787ms step_avg:44.71ms
step:1137/2110 train_time:50848ms step_avg:44.72ms
step:1138/2110 train_time:50906ms step_avg:44.73ms
step:1139/2110 train_time:50966ms step_avg:44.75ms
step:1140/2110 train_time:51025ms step_avg:44.76ms
step:1141/2110 train_time:51086ms step_avg:44.77ms
step:1142/2110 train_time:51145ms step_avg:44.79ms
step:1143/2110 train_time:51205ms step_avg:44.80ms
step:1144/2110 train_time:51265ms step_avg:44.81ms
step:1145/2110 train_time:51325ms step_avg:44.83ms
step:1146/2110 train_time:51384ms step_avg:44.84ms
step:1147/2110 train_time:51445ms step_avg:44.85ms
step:1148/2110 train_time:51505ms step_avg:44.86ms
step:1149/2110 train_time:51564ms step_avg:44.88ms
step:1150/2110 train_time:51623ms step_avg:44.89ms
step:1151/2110 train_time:51683ms step_avg:44.90ms
step:1152/2110 train_time:51741ms step_avg:44.91ms
step:1153/2110 train_time:51802ms step_avg:44.93ms
step:1154/2110 train_time:51860ms step_avg:44.94ms
step:1155/2110 train_time:51920ms step_avg:44.95ms
step:1156/2110 train_time:51979ms step_avg:44.96ms
step:1157/2110 train_time:52040ms step_avg:44.98ms
step:1158/2110 train_time:52098ms step_avg:44.99ms
step:1159/2110 train_time:52158ms step_avg:45.00ms
step:1160/2110 train_time:52217ms step_avg:45.01ms
step:1161/2110 train_time:52278ms step_avg:45.03ms
step:1162/2110 train_time:52337ms step_avg:45.04ms
step:1163/2110 train_time:52398ms step_avg:45.05ms
step:1164/2110 train_time:52457ms step_avg:45.07ms
step:1165/2110 train_time:52517ms step_avg:45.08ms
step:1166/2110 train_time:52575ms step_avg:45.09ms
step:1167/2110 train_time:52636ms step_avg:45.10ms
step:1168/2110 train_time:52695ms step_avg:45.12ms
step:1169/2110 train_time:52756ms step_avg:45.13ms
step:1170/2110 train_time:52815ms step_avg:45.14ms
step:1171/2110 train_time:52875ms step_avg:45.15ms
step:1172/2110 train_time:52934ms step_avg:45.17ms
step:1173/2110 train_time:52995ms step_avg:45.18ms
step:1174/2110 train_time:53054ms step_avg:45.19ms
step:1175/2110 train_time:53114ms step_avg:45.20ms
step:1176/2110 train_time:53173ms step_avg:45.22ms
step:1177/2110 train_time:53235ms step_avg:45.23ms
step:1178/2110 train_time:53293ms step_avg:45.24ms
step:1179/2110 train_time:53354ms step_avg:45.25ms
step:1180/2110 train_time:53413ms step_avg:45.27ms
step:1181/2110 train_time:53474ms step_avg:45.28ms
step:1182/2110 train_time:53533ms step_avg:45.29ms
step:1183/2110 train_time:53594ms step_avg:45.30ms
step:1184/2110 train_time:53652ms step_avg:45.31ms
step:1185/2110 train_time:53714ms step_avg:45.33ms
step:1186/2110 train_time:53773ms step_avg:45.34ms
step:1187/2110 train_time:53833ms step_avg:45.35ms
step:1188/2110 train_time:53892ms step_avg:45.36ms
step:1189/2110 train_time:53953ms step_avg:45.38ms
step:1190/2110 train_time:54012ms step_avg:45.39ms
step:1191/2110 train_time:54072ms step_avg:45.40ms
step:1192/2110 train_time:54131ms step_avg:45.41ms
step:1193/2110 train_time:54192ms step_avg:45.43ms
step:1194/2110 train_time:54251ms step_avg:45.44ms
step:1195/2110 train_time:54313ms step_avg:45.45ms
step:1196/2110 train_time:54372ms step_avg:45.46ms
step:1197/2110 train_time:54433ms step_avg:45.47ms
step:1198/2110 train_time:54491ms step_avg:45.49ms
step:1199/2110 train_time:54552ms step_avg:45.50ms
step:1200/2110 train_time:54611ms step_avg:45.51ms
step:1201/2110 train_time:54672ms step_avg:45.52ms
step:1202/2110 train_time:54731ms step_avg:45.53ms
step:1203/2110 train_time:54791ms step_avg:45.55ms
step:1204/2110 train_time:54850ms step_avg:45.56ms
step:1205/2110 train_time:54911ms step_avg:45.57ms
step:1206/2110 train_time:54970ms step_avg:45.58ms
step:1207/2110 train_time:55031ms step_avg:45.59ms
step:1208/2110 train_time:55090ms step_avg:45.60ms
step:1209/2110 train_time:55151ms step_avg:45.62ms
step:1210/2110 train_time:55211ms step_avg:45.63ms
step:1211/2110 train_time:55273ms step_avg:45.64ms
step:1212/2110 train_time:55332ms step_avg:45.65ms
step:1213/2110 train_time:55393ms step_avg:45.67ms
step:1214/2110 train_time:55451ms step_avg:45.68ms
step:1215/2110 train_time:55512ms step_avg:45.69ms
step:1216/2110 train_time:55571ms step_avg:45.70ms
step:1217/2110 train_time:55631ms step_avg:45.71ms
step:1218/2110 train_time:55690ms step_avg:45.72ms
step:1219/2110 train_time:55751ms step_avg:45.74ms
step:1220/2110 train_time:55810ms step_avg:45.75ms
step:1221/2110 train_time:55871ms step_avg:45.76ms
step:1222/2110 train_time:55931ms step_avg:45.77ms
step:1223/2110 train_time:55992ms step_avg:45.78ms
step:1224/2110 train_time:56051ms step_avg:45.79ms
step:1225/2110 train_time:56112ms step_avg:45.81ms
step:1226/2110 train_time:56172ms step_avg:45.82ms
step:1227/2110 train_time:56233ms step_avg:45.83ms
step:1228/2110 train_time:56292ms step_avg:45.84ms
step:1229/2110 train_time:56353ms step_avg:45.85ms
step:1230/2110 train_time:56413ms step_avg:45.86ms
step:1231/2110 train_time:56474ms step_avg:45.88ms
step:1232/2110 train_time:56532ms step_avg:45.89ms
step:1233/2110 train_time:56593ms step_avg:45.90ms
step:1234/2110 train_time:56652ms step_avg:45.91ms
step:1235/2110 train_time:56712ms step_avg:45.92ms
step:1236/2110 train_time:56771ms step_avg:45.93ms
step:1237/2110 train_time:56832ms step_avg:45.94ms
step:1238/2110 train_time:56890ms step_avg:45.95ms
step:1239/2110 train_time:56951ms step_avg:45.97ms
step:1240/2110 train_time:57010ms step_avg:45.98ms
step:1241/2110 train_time:57071ms step_avg:45.99ms
step:1242/2110 train_time:57130ms step_avg:46.00ms
step:1243/2110 train_time:57191ms step_avg:46.01ms
step:1244/2110 train_time:57250ms step_avg:46.02ms
step:1245/2110 train_time:57312ms step_avg:46.03ms
step:1246/2110 train_time:57371ms step_avg:46.04ms
step:1247/2110 train_time:57431ms step_avg:46.06ms
step:1248/2110 train_time:57490ms step_avg:46.07ms
step:1249/2110 train_time:57551ms step_avg:46.08ms
step:1250/2110 train_time:57611ms step_avg:46.09ms
step:1250/2110 val_loss:3.5950 train_time:57674ms step_avg:46.14ms
step:1251/2110 train_time:57700ms step_avg:46.12ms
step:1252/2110 train_time:57733ms step_avg:46.11ms
step:1253/2110 train_time:57799ms step_avg:46.13ms
step:1254/2110 train_time:57860ms step_avg:46.14ms
step:1255/2110 train_time:57921ms step_avg:46.15ms
step:1256/2110 train_time:57980ms step_avg:46.16ms
step:1257/2110 train_time:58040ms step_avg:46.17ms
step:1258/2110 train_time:58098ms step_avg:46.18ms
step:1259/2110 train_time:58157ms step_avg:46.19ms
step:1260/2110 train_time:58215ms step_avg:46.20ms
step:1261/2110 train_time:58275ms step_avg:46.21ms
step:1262/2110 train_time:58333ms step_avg:46.22ms
step:1263/2110 train_time:58392ms step_avg:46.23ms
step:1264/2110 train_time:58451ms step_avg:46.24ms
step:1265/2110 train_time:58511ms step_avg:46.25ms
step:1266/2110 train_time:58569ms step_avg:46.26ms
step:1267/2110 train_time:58630ms step_avg:46.27ms
step:1268/2110 train_time:58690ms step_avg:46.29ms
step:1269/2110 train_time:58754ms step_avg:46.30ms
step:1270/2110 train_time:58815ms step_avg:46.31ms
step:1271/2110 train_time:58877ms step_avg:46.32ms
step:1272/2110 train_time:58937ms step_avg:46.33ms
step:1273/2110 train_time:58997ms step_avg:46.35ms
step:1274/2110 train_time:59056ms step_avg:46.35ms
step:1275/2110 train_time:59117ms step_avg:46.37ms
step:1276/2110 train_time:59175ms step_avg:46.38ms
step:1277/2110 train_time:59236ms step_avg:46.39ms
step:1278/2110 train_time:59294ms step_avg:46.40ms
step:1279/2110 train_time:59354ms step_avg:46.41ms
step:1280/2110 train_time:59412ms step_avg:46.42ms
step:1281/2110 train_time:59472ms step_avg:46.43ms
step:1282/2110 train_time:59530ms step_avg:46.43ms
step:1283/2110 train_time:59590ms step_avg:46.45ms
step:1284/2110 train_time:59649ms step_avg:46.46ms
step:1285/2110 train_time:59711ms step_avg:46.47ms
step:1286/2110 train_time:59772ms step_avg:46.48ms
step:1287/2110 train_time:59835ms step_avg:46.49ms
step:1288/2110 train_time:59894ms step_avg:46.50ms
step:1289/2110 train_time:59956ms step_avg:46.51ms
step:1290/2110 train_time:60015ms step_avg:46.52ms
step:1291/2110 train_time:60076ms step_avg:46.53ms
step:1292/2110 train_time:60135ms step_avg:46.54ms
step:1293/2110 train_time:60194ms step_avg:46.55ms
step:1294/2110 train_time:60253ms step_avg:46.56ms
step:1295/2110 train_time:60312ms step_avg:46.57ms
step:1296/2110 train_time:60370ms step_avg:46.58ms
step:1297/2110 train_time:60430ms step_avg:46.59ms
step:1298/2110 train_time:60489ms step_avg:46.60ms
step:1299/2110 train_time:60549ms step_avg:46.61ms
step:1300/2110 train_time:60609ms step_avg:46.62ms
step:1301/2110 train_time:60669ms step_avg:46.63ms
step:1302/2110 train_time:60729ms step_avg:46.64ms
step:1303/2110 train_time:60790ms step_avg:46.65ms
step:1304/2110 train_time:60850ms step_avg:46.66ms
step:1305/2110 train_time:60912ms step_avg:46.68ms
step:1306/2110 train_time:60971ms step_avg:46.69ms
step:1307/2110 train_time:61033ms step_avg:46.70ms
step:1308/2110 train_time:61092ms step_avg:46.71ms
step:1309/2110 train_time:61153ms step_avg:46.72ms
step:1310/2110 train_time:61212ms step_avg:46.73ms
step:1311/2110 train_time:61272ms step_avg:46.74ms
step:1312/2110 train_time:61330ms step_avg:46.75ms
step:1313/2110 train_time:61390ms step_avg:46.76ms
step:1314/2110 train_time:61448ms step_avg:46.76ms
step:1315/2110 train_time:61508ms step_avg:46.77ms
step:1316/2110 train_time:61567ms step_avg:46.78ms
step:1317/2110 train_time:61627ms step_avg:46.79ms
step:1318/2110 train_time:61686ms step_avg:46.80ms
step:1319/2110 train_time:61747ms step_avg:46.81ms
step:1320/2110 train_time:61807ms step_avg:46.82ms
step:1321/2110 train_time:61869ms step_avg:46.83ms
step:1322/2110 train_time:61928ms step_avg:46.84ms
step:1323/2110 train_time:61989ms step_avg:46.85ms
step:1324/2110 train_time:62049ms step_avg:46.86ms
step:1325/2110 train_time:62110ms step_avg:46.88ms
step:1326/2110 train_time:62169ms step_avg:46.88ms
step:1327/2110 train_time:62230ms step_avg:46.89ms
step:1328/2110 train_time:62288ms step_avg:46.90ms
step:1329/2110 train_time:62348ms step_avg:46.91ms
step:1330/2110 train_time:62407ms step_avg:46.92ms
step:1331/2110 train_time:62467ms step_avg:46.93ms
step:1332/2110 train_time:62525ms step_avg:46.94ms
step:1333/2110 train_time:62586ms step_avg:46.95ms
step:1334/2110 train_time:62644ms step_avg:46.96ms
step:1335/2110 train_time:62705ms step_avg:46.97ms
step:1336/2110 train_time:62764ms step_avg:46.98ms
step:1337/2110 train_time:62825ms step_avg:46.99ms
step:1338/2110 train_time:62885ms step_avg:47.00ms
step:1339/2110 train_time:62946ms step_avg:47.01ms
step:1340/2110 train_time:63005ms step_avg:47.02ms
step:1341/2110 train_time:63065ms step_avg:47.03ms
step:1342/2110 train_time:63124ms step_avg:47.04ms
step:1343/2110 train_time:63185ms step_avg:47.05ms
step:1344/2110 train_time:63244ms step_avg:47.06ms
step:1345/2110 train_time:63304ms step_avg:47.07ms
step:1346/2110 train_time:63362ms step_avg:47.07ms
step:1347/2110 train_time:63423ms step_avg:47.08ms
step:1348/2110 train_time:63480ms step_avg:47.09ms
step:1349/2110 train_time:63540ms step_avg:47.10ms
step:1350/2110 train_time:63598ms step_avg:47.11ms
step:1351/2110 train_time:63659ms step_avg:47.12ms
step:1352/2110 train_time:63717ms step_avg:47.13ms
step:1353/2110 train_time:63778ms step_avg:47.14ms
step:1354/2110 train_time:63837ms step_avg:47.15ms
step:1355/2110 train_time:63897ms step_avg:47.16ms
step:1356/2110 train_time:63956ms step_avg:47.17ms
step:1357/2110 train_time:64017ms step_avg:47.18ms
step:1358/2110 train_time:64075ms step_avg:47.18ms
step:1359/2110 train_time:64136ms step_avg:47.19ms
step:1360/2110 train_time:64195ms step_avg:47.20ms
step:1361/2110 train_time:64256ms step_avg:47.21ms
step:1362/2110 train_time:64314ms step_avg:47.22ms
step:1363/2110 train_time:64375ms step_avg:47.23ms
step:1364/2110 train_time:64433ms step_avg:47.24ms
step:1365/2110 train_time:64494ms step_avg:47.25ms
step:1366/2110 train_time:64554ms step_avg:47.26ms
step:1367/2110 train_time:64614ms step_avg:47.27ms
step:1368/2110 train_time:64673ms step_avg:47.28ms
step:1369/2110 train_time:64733ms step_avg:47.28ms
step:1370/2110 train_time:64792ms step_avg:47.29ms
step:1371/2110 train_time:64854ms step_avg:47.30ms
step:1372/2110 train_time:64913ms step_avg:47.31ms
step:1373/2110 train_time:64973ms step_avg:47.32ms
step:1374/2110 train_time:65032ms step_avg:47.33ms
step:1375/2110 train_time:65093ms step_avg:47.34ms
step:1376/2110 train_time:65152ms step_avg:47.35ms
step:1377/2110 train_time:65213ms step_avg:47.36ms
step:1378/2110 train_time:65273ms step_avg:47.37ms
step:1379/2110 train_time:65332ms step_avg:47.38ms
step:1380/2110 train_time:65391ms step_avg:47.38ms
step:1381/2110 train_time:65452ms step_avg:47.39ms
step:1382/2110 train_time:65537ms step_avg:47.42ms
step:1383/2110 train_time:65625ms step_avg:47.45ms
step:1384/2110 train_time:65711ms step_avg:47.48ms
step:1385/2110 train_time:65798ms step_avg:47.51ms
step:1386/2110 train_time:65886ms step_avg:47.54ms
step:1387/2110 train_time:65972ms step_avg:47.56ms
step:1388/2110 train_time:66059ms step_avg:47.59ms
step:1389/2110 train_time:66146ms step_avg:47.62ms
step:1390/2110 train_time:66231ms step_avg:47.65ms
step:1391/2110 train_time:66318ms step_avg:47.68ms
step:1392/2110 train_time:66404ms step_avg:47.70ms
step:1393/2110 train_time:66491ms step_avg:47.73ms
step:1394/2110 train_time:66577ms step_avg:47.76ms
step:1395/2110 train_time:66664ms step_avg:47.79ms
step:1396/2110 train_time:66749ms step_avg:47.81ms
step:1397/2110 train_time:66836ms step_avg:47.84ms
step:1398/2110 train_time:66923ms step_avg:47.87ms
step:1399/2110 train_time:67010ms step_avg:47.90ms
step:1400/2110 train_time:67096ms step_avg:47.93ms
step:1401/2110 train_time:67184ms step_avg:47.95ms
step:1402/2110 train_time:67269ms step_avg:47.98ms
step:1403/2110 train_time:67356ms step_avg:48.01ms
step:1404/2110 train_time:67442ms step_avg:48.04ms
step:1405/2110 train_time:67529ms step_avg:48.06ms
step:1406/2110 train_time:67615ms step_avg:48.09ms
step:1407/2110 train_time:67702ms step_avg:48.12ms
step:1408/2110 train_time:67790ms step_avg:48.15ms
step:1409/2110 train_time:67876ms step_avg:48.17ms
step:1410/2110 train_time:67962ms step_avg:48.20ms
step:1411/2110 train_time:68049ms step_avg:48.23ms
step:1412/2110 train_time:68135ms step_avg:48.25ms
step:1413/2110 train_time:68222ms step_avg:48.28ms
step:1414/2110 train_time:68307ms step_avg:48.31ms
step:1415/2110 train_time:68394ms step_avg:48.33ms
step:1416/2110 train_time:68480ms step_avg:48.36ms
step:1417/2110 train_time:68566ms step_avg:48.39ms
step:1418/2110 train_time:68653ms step_avg:48.42ms
step:1419/2110 train_time:68740ms step_avg:48.44ms
step:1420/2110 train_time:68827ms step_avg:48.47ms
step:1421/2110 train_time:68913ms step_avg:48.50ms
step:1422/2110 train_time:68998ms step_avg:48.52ms
step:1423/2110 train_time:69086ms step_avg:48.55ms
step:1424/2110 train_time:69172ms step_avg:48.58ms
step:1425/2110 train_time:69259ms step_avg:48.60ms
step:1426/2110 train_time:69345ms step_avg:48.63ms
step:1427/2110 train_time:69432ms step_avg:48.66ms
step:1428/2110 train_time:69518ms step_avg:48.68ms
step:1429/2110 train_time:69606ms step_avg:48.71ms
step:1430/2110 train_time:69691ms step_avg:48.73ms
step:1431/2110 train_time:69778ms step_avg:48.76ms
step:1432/2110 train_time:69865ms step_avg:48.79ms
step:1433/2110 train_time:69951ms step_avg:48.81ms
step:1434/2110 train_time:70037ms step_avg:48.84ms
step:1435/2110 train_time:70126ms step_avg:48.87ms
step:1436/2110 train_time:70211ms step_avg:48.89ms
step:1437/2110 train_time:70299ms step_avg:48.92ms
step:1438/2110 train_time:70386ms step_avg:48.95ms
step:1439/2110 train_time:70472ms step_avg:48.97ms
step:1440/2110 train_time:70558ms step_avg:49.00ms
step:1441/2110 train_time:70646ms step_avg:49.03ms
step:1442/2110 train_time:70731ms step_avg:49.05ms
step:1443/2110 train_time:70819ms step_avg:49.08ms
step:1444/2110 train_time:70906ms step_avg:49.10ms
step:1445/2110 train_time:70992ms step_avg:49.13ms
step:1446/2110 train_time:71078ms step_avg:49.16ms
step:1447/2110 train_time:71167ms step_avg:49.18ms
step:1448/2110 train_time:71252ms step_avg:49.21ms
step:1449/2110 train_time:71338ms step_avg:49.23ms
step:1450/2110 train_time:71425ms step_avg:49.26ms
step:1451/2110 train_time:71513ms step_avg:49.29ms
step:1452/2110 train_time:71598ms step_avg:49.31ms
step:1453/2110 train_time:71686ms step_avg:49.34ms
step:1454/2110 train_time:71771ms step_avg:49.36ms
step:1455/2110 train_time:71857ms step_avg:49.39ms
step:1456/2110 train_time:71944ms step_avg:49.41ms
step:1457/2110 train_time:72031ms step_avg:49.44ms
step:1458/2110 train_time:72117ms step_avg:49.46ms
step:1459/2110 train_time:72205ms step_avg:49.49ms
step:1460/2110 train_time:72290ms step_avg:49.51ms
step:1461/2110 train_time:72377ms step_avg:49.54ms
step:1462/2110 train_time:72464ms step_avg:49.57ms
step:1463/2110 train_time:72550ms step_avg:49.59ms
step:1464/2110 train_time:72636ms step_avg:49.62ms
step:1465/2110 train_time:72724ms step_avg:49.64ms
step:1466/2110 train_time:72809ms step_avg:49.67ms
step:1467/2110 train_time:72896ms step_avg:49.69ms
step:1468/2110 train_time:72983ms step_avg:49.72ms
step:1469/2110 train_time:73070ms step_avg:49.74ms
step:1470/2110 train_time:73156ms step_avg:49.77ms
step:1471/2110 train_time:73243ms step_avg:49.79ms
step:1472/2110 train_time:73328ms step_avg:49.82ms
step:1473/2110 train_time:73416ms step_avg:49.84ms
step:1474/2110 train_time:73503ms step_avg:49.87ms
step:1475/2110 train_time:73590ms step_avg:49.89ms
step:1476/2110 train_time:73676ms step_avg:49.92ms
step:1477/2110 train_time:73763ms step_avg:49.94ms
step:1478/2110 train_time:73850ms step_avg:49.97ms
step:1479/2110 train_time:73936ms step_avg:49.99ms
step:1480/2110 train_time:74023ms step_avg:50.02ms
step:1481/2110 train_time:74110ms step_avg:50.04ms
step:1482/2110 train_time:74197ms step_avg:50.07ms
step:1483/2110 train_time:74283ms step_avg:50.09ms
step:1484/2110 train_time:74369ms step_avg:50.11ms
step:1485/2110 train_time:74456ms step_avg:50.14ms
step:1486/2110 train_time:74542ms step_avg:50.16ms
step:1487/2110 train_time:74630ms step_avg:50.19ms
step:1488/2110 train_time:74716ms step_avg:50.21ms
step:1489/2110 train_time:74804ms step_avg:50.24ms
step:1490/2110 train_time:74889ms step_avg:50.26ms
step:1491/2110 train_time:74976ms step_avg:50.29ms
step:1492/2110 train_time:75062ms step_avg:50.31ms
step:1493/2110 train_time:75150ms step_avg:50.33ms
step:1494/2110 train_time:75235ms step_avg:50.36ms
step:1495/2110 train_time:75323ms step_avg:50.38ms
step:1496/2110 train_time:75408ms step_avg:50.41ms
step:1497/2110 train_time:75495ms step_avg:50.43ms
step:1498/2110 train_time:75582ms step_avg:50.46ms
step:1499/2110 train_time:75669ms step_avg:50.48ms
step:1500/2110 train_time:75755ms step_avg:50.50ms
step:1500/2110 val_loss:3.4957 train_time:75844ms step_avg:50.56ms
step:1501/2110 train_time:75876ms step_avg:50.55ms
step:1502/2110 train_time:75932ms step_avg:50.55ms
step:1503/2110 train_time:76024ms step_avg:50.58ms
step:1504/2110 train_time:76112ms step_avg:50.61ms
step:1505/2110 train_time:76199ms step_avg:50.63ms
step:1506/2110 train_time:76285ms step_avg:50.65ms
step:1507/2110 train_time:76371ms step_avg:50.68ms
step:1508/2110 train_time:76455ms step_avg:50.70ms
step:1509/2110 train_time:76541ms step_avg:50.72ms
step:1510/2110 train_time:76627ms step_avg:50.75ms
step:1511/2110 train_time:76713ms step_avg:50.77ms
step:1512/2110 train_time:76800ms step_avg:50.79ms
step:1513/2110 train_time:76890ms step_avg:50.82ms
step:1514/2110 train_time:76977ms step_avg:50.84ms
step:1515/2110 train_time:77067ms step_avg:50.87ms
step:1516/2110 train_time:77153ms step_avg:50.89ms
step:1517/2110 train_time:77239ms step_avg:50.92ms
step:1518/2110 train_time:77326ms step_avg:50.94ms
step:1519/2110 train_time:77412ms step_avg:50.96ms
step:1520/2110 train_time:77496ms step_avg:50.98ms
step:1521/2110 train_time:77582ms step_avg:51.01ms
step:1522/2110 train_time:77667ms step_avg:51.03ms
step:1523/2110 train_time:77755ms step_avg:51.05ms
step:1524/2110 train_time:77842ms step_avg:51.08ms
step:1525/2110 train_time:77931ms step_avg:51.10ms
step:1526/2110 train_time:78018ms step_avg:51.13ms
step:1527/2110 train_time:78105ms step_avg:51.15ms
step:1528/2110 train_time:78192ms step_avg:51.17ms
step:1529/2110 train_time:78278ms step_avg:51.20ms
step:1530/2110 train_time:78364ms step_avg:51.22ms
step:1531/2110 train_time:78451ms step_avg:51.24ms
step:1532/2110 train_time:78535ms step_avg:51.26ms
step:1533/2110 train_time:78622ms step_avg:51.29ms
step:1534/2110 train_time:78707ms step_avg:51.31ms
step:1535/2110 train_time:78795ms step_avg:51.33ms
step:1536/2110 train_time:78881ms step_avg:51.36ms
step:1537/2110 train_time:78970ms step_avg:51.38ms
step:1538/2110 train_time:79056ms step_avg:51.40ms
step:1539/2110 train_time:79143ms step_avg:51.43ms
step:1540/2110 train_time:79230ms step_avg:51.45ms
step:1541/2110 train_time:79316ms step_avg:51.47ms
step:1542/2110 train_time:79403ms step_avg:51.49ms
step:1543/2110 train_time:79490ms step_avg:51.52ms
step:1544/2110 train_time:79575ms step_avg:51.54ms
step:1545/2110 train_time:79661ms step_avg:51.56ms
step:1546/2110 train_time:79748ms step_avg:51.58ms
step:1547/2110 train_time:79835ms step_avg:51.61ms
step:1548/2110 train_time:79921ms step_avg:51.63ms
step:1549/2110 train_time:80009ms step_avg:51.65ms
step:1550/2110 train_time:80095ms step_avg:51.67ms
step:1551/2110 train_time:80183ms step_avg:51.70ms
step:1552/2110 train_time:80269ms step_avg:51.72ms
step:1553/2110 train_time:80356ms step_avg:51.74ms
step:1554/2110 train_time:80442ms step_avg:51.76ms
step:1555/2110 train_time:80529ms step_avg:51.79ms
step:1556/2110 train_time:80614ms step_avg:51.81ms
step:1557/2110 train_time:80701ms step_avg:51.83ms
step:1558/2110 train_time:80787ms step_avg:51.85ms
step:1559/2110 train_time:80874ms step_avg:51.88ms
step:1560/2110 train_time:80960ms step_avg:51.90ms
step:1561/2110 train_time:81049ms step_avg:51.92ms
step:1562/2110 train_time:81134ms step_avg:51.94ms
step:1563/2110 train_time:81222ms step_avg:51.97ms
step:1564/2110 train_time:81307ms step_avg:51.99ms
step:1565/2110 train_time:81395ms step_avg:52.01ms
step:1566/2110 train_time:81480ms step_avg:52.03ms
step:1567/2110 train_time:81567ms step_avg:52.05ms
step:1568/2110 train_time:81653ms step_avg:52.07ms
step:1569/2110 train_time:81740ms step_avg:52.10ms
step:1570/2110 train_time:81827ms step_avg:52.12ms
step:1571/2110 train_time:81915ms step_avg:52.14ms
step:1572/2110 train_time:82000ms step_avg:52.16ms
step:1573/2110 train_time:82088ms step_avg:52.19ms
step:1574/2110 train_time:82174ms step_avg:52.21ms
step:1575/2110 train_time:82262ms step_avg:52.23ms
step:1576/2110 train_time:82348ms step_avg:52.25ms
step:1577/2110 train_time:82436ms step_avg:52.27ms
step:1578/2110 train_time:82521ms step_avg:52.29ms
step:1579/2110 train_time:82609ms step_avg:52.32ms
step:1580/2110 train_time:82694ms step_avg:52.34ms
step:1581/2110 train_time:82781ms step_avg:52.36ms
step:1582/2110 train_time:82868ms step_avg:52.38ms
step:1583/2110 train_time:82955ms step_avg:52.40ms
step:1584/2110 train_time:83041ms step_avg:52.42ms
step:1585/2110 train_time:83129ms step_avg:52.45ms
step:1586/2110 train_time:83214ms step_avg:52.47ms
step:1587/2110 train_time:83301ms step_avg:52.49ms
step:1588/2110 train_time:83387ms step_avg:52.51ms
step:1589/2110 train_time:83474ms step_avg:52.53ms
step:1590/2110 train_time:83560ms step_avg:52.55ms
step:1591/2110 train_time:83647ms step_avg:52.58ms
step:1592/2110 train_time:83733ms step_avg:52.60ms
step:1593/2110 train_time:83820ms step_avg:52.62ms
step:1594/2110 train_time:83907ms step_avg:52.64ms
step:1595/2110 train_time:83994ms step_avg:52.66ms
step:1596/2110 train_time:84079ms step_avg:52.68ms
step:1597/2110 train_time:84167ms step_avg:52.70ms
step:1598/2110 train_time:84252ms step_avg:52.72ms
step:1599/2110 train_time:84340ms step_avg:52.75ms
step:1600/2110 train_time:84427ms step_avg:52.77ms
step:1601/2110 train_time:84513ms step_avg:52.79ms
step:1602/2110 train_time:84599ms step_avg:52.81ms
step:1603/2110 train_time:84686ms step_avg:52.83ms
step:1604/2110 train_time:84772ms step_avg:52.85ms
step:1605/2110 train_time:84859ms step_avg:52.87ms
step:1606/2110 train_time:84944ms step_avg:52.89ms
step:1607/2110 train_time:85032ms step_avg:52.91ms
step:1608/2110 train_time:85118ms step_avg:52.93ms
step:1609/2110 train_time:85206ms step_avg:52.96ms
step:1610/2110 train_time:85292ms step_avg:52.98ms
step:1611/2110 train_time:85378ms step_avg:53.00ms
step:1612/2110 train_time:85465ms step_avg:53.02ms
step:1613/2110 train_time:85552ms step_avg:53.04ms
step:1614/2110 train_time:85637ms step_avg:53.06ms
step:1615/2110 train_time:85725ms step_avg:53.08ms
step:1616/2110 train_time:85810ms step_avg:53.10ms
step:1617/2110 train_time:85898ms step_avg:53.12ms
step:1618/2110 train_time:85984ms step_avg:53.14ms
step:1619/2110 train_time:86072ms step_avg:53.16ms
step:1620/2110 train_time:86158ms step_avg:53.18ms
step:1621/2110 train_time:86246ms step_avg:53.21ms
step:1622/2110 train_time:86331ms step_avg:53.23ms
step:1623/2110 train_time:86418ms step_avg:53.25ms
step:1624/2110 train_time:86504ms step_avg:53.27ms
step:1625/2110 train_time:86591ms step_avg:53.29ms
step:1626/2110 train_time:86676ms step_avg:53.31ms
step:1627/2110 train_time:86764ms step_avg:53.33ms
step:1628/2110 train_time:86849ms step_avg:53.35ms
step:1629/2110 train_time:86937ms step_avg:53.37ms
step:1630/2110 train_time:87023ms step_avg:53.39ms
step:1631/2110 train_time:87111ms step_avg:53.41ms
step:1632/2110 train_time:87197ms step_avg:53.43ms
step:1633/2110 train_time:87285ms step_avg:53.45ms
step:1634/2110 train_time:87371ms step_avg:53.47ms
step:1635/2110 train_time:87458ms step_avg:53.49ms
step:1636/2110 train_time:87544ms step_avg:53.51ms
step:1637/2110 train_time:87630ms step_avg:53.53ms
step:1638/2110 train_time:87715ms step_avg:53.55ms
step:1639/2110 train_time:87803ms step_avg:53.57ms
step:1640/2110 train_time:87889ms step_avg:53.59ms
step:1641/2110 train_time:87976ms step_avg:53.61ms
step:1642/2110 train_time:88062ms step_avg:53.63ms
step:1643/2110 train_time:88150ms step_avg:53.65ms
step:1644/2110 train_time:88235ms step_avg:53.67ms
step:1645/2110 train_time:88323ms step_avg:53.69ms
step:1646/2110 train_time:88410ms step_avg:53.71ms
step:1647/2110 train_time:88497ms step_avg:53.73ms
step:1648/2110 train_time:88583ms step_avg:53.75ms
step:1649/2110 train_time:88670ms step_avg:53.77ms
step:1650/2110 train_time:88755ms step_avg:53.79ms
step:1651/2110 train_time:88843ms step_avg:53.81ms
step:1652/2110 train_time:88929ms step_avg:53.83ms
step:1653/2110 train_time:89017ms step_avg:53.85ms
step:1654/2110 train_time:89103ms step_avg:53.87ms
step:1655/2110 train_time:89191ms step_avg:53.89ms
step:1656/2110 train_time:89276ms step_avg:53.91ms
step:1657/2110 train_time:89363ms step_avg:53.93ms
step:1658/2110 train_time:89451ms step_avg:53.95ms
step:1659/2110 train_time:89539ms step_avg:53.97ms
step:1660/2110 train_time:89626ms step_avg:53.99ms
step:1661/2110 train_time:89715ms step_avg:54.01ms
step:1662/2110 train_time:89802ms step_avg:54.03ms
step:1663/2110 train_time:89890ms step_avg:54.05ms
step:1664/2110 train_time:89976ms step_avg:54.07ms
step:1665/2110 train_time:90066ms step_avg:54.09ms
step:1666/2110 train_time:90153ms step_avg:54.11ms
step:1667/2110 train_time:90242ms step_avg:54.13ms
step:1668/2110 train_time:90329ms step_avg:54.15ms
step:1669/2110 train_time:90417ms step_avg:54.17ms
step:1670/2110 train_time:90504ms step_avg:54.19ms
step:1671/2110 train_time:90593ms step_avg:54.22ms
step:1672/2110 train_time:90681ms step_avg:54.23ms
step:1673/2110 train_time:90769ms step_avg:54.26ms
step:1674/2110 train_time:90855ms step_avg:54.27ms
step:1675/2110 train_time:90944ms step_avg:54.30ms
step:1676/2110 train_time:91032ms step_avg:54.31ms
step:1677/2110 train_time:91120ms step_avg:54.34ms
step:1678/2110 train_time:91208ms step_avg:54.35ms
step:1679/2110 train_time:91296ms step_avg:54.38ms
step:1680/2110 train_time:91383ms step_avg:54.39ms
step:1681/2110 train_time:91471ms step_avg:54.41ms
step:1682/2110 train_time:91558ms step_avg:54.43ms
step:1683/2110 train_time:91646ms step_avg:54.45ms
step:1684/2110 train_time:91734ms step_avg:54.47ms
step:1685/2110 train_time:91822ms step_avg:54.49ms
step:1686/2110 train_time:91910ms step_avg:54.51ms
step:1687/2110 train_time:91999ms step_avg:54.53ms
step:1688/2110 train_time:92086ms step_avg:54.55ms
step:1689/2110 train_time:92175ms step_avg:54.57ms
step:1690/2110 train_time:92263ms step_avg:54.59ms
step:1691/2110 train_time:92351ms step_avg:54.61ms
step:1692/2110 train_time:92438ms step_avg:54.63ms
step:1693/2110 train_time:92526ms step_avg:54.65ms
step:1694/2110 train_time:92613ms step_avg:54.67ms
step:1695/2110 train_time:92701ms step_avg:54.69ms
step:1696/2110 train_time:92788ms step_avg:54.71ms
step:1697/2110 train_time:92877ms step_avg:54.73ms
step:1698/2110 train_time:92965ms step_avg:54.75ms
step:1699/2110 train_time:93053ms step_avg:54.77ms
step:1700/2110 train_time:93141ms step_avg:54.79ms
step:1701/2110 train_time:93229ms step_avg:54.81ms
step:1702/2110 train_time:93316ms step_avg:54.83ms
step:1703/2110 train_time:93405ms step_avg:54.85ms
step:1704/2110 train_time:93493ms step_avg:54.87ms
step:1705/2110 train_time:93581ms step_avg:54.89ms
step:1706/2110 train_time:93668ms step_avg:54.91ms
step:1707/2110 train_time:93756ms step_avg:54.92ms
step:1708/2110 train_time:93844ms step_avg:54.94ms
step:1709/2110 train_time:93933ms step_avg:54.96ms
step:1710/2110 train_time:94021ms step_avg:54.98ms
step:1711/2110 train_time:94108ms step_avg:55.00ms
step:1712/2110 train_time:94195ms step_avg:55.02ms
step:1713/2110 train_time:94283ms step_avg:55.04ms
step:1714/2110 train_time:94371ms step_avg:55.06ms
step:1715/2110 train_time:94459ms step_avg:55.08ms
step:1716/2110 train_time:94546ms step_avg:55.10ms
step:1717/2110 train_time:94635ms step_avg:55.12ms
step:1718/2110 train_time:94722ms step_avg:55.14ms
step:1719/2110 train_time:94812ms step_avg:55.16ms
step:1720/2110 train_time:94899ms step_avg:55.17ms
step:1721/2110 train_time:94988ms step_avg:55.19ms
step:1722/2110 train_time:95075ms step_avg:55.21ms
step:1723/2110 train_time:95162ms step_avg:55.23ms
step:1724/2110 train_time:95251ms step_avg:55.25ms
step:1725/2110 train_time:95339ms step_avg:55.27ms
step:1726/2110 train_time:95427ms step_avg:55.29ms
step:1727/2110 train_time:95515ms step_avg:55.31ms
step:1728/2110 train_time:95601ms step_avg:55.32ms
step:1729/2110 train_time:95691ms step_avg:55.34ms
step:1730/2110 train_time:95777ms step_avg:55.36ms
step:1731/2110 train_time:95867ms step_avg:55.38ms
step:1732/2110 train_time:95954ms step_avg:55.40ms
step:1733/2110 train_time:96042ms step_avg:55.42ms
step:1734/2110 train_time:96129ms step_avg:55.44ms
step:1735/2110 train_time:96218ms step_avg:55.46ms
step:1736/2110 train_time:96305ms step_avg:55.48ms
step:1737/2110 train_time:96394ms step_avg:55.49ms
step:1738/2110 train_time:96481ms step_avg:55.51ms
step:1739/2110 train_time:96571ms step_avg:55.53ms
step:1740/2110 train_time:96659ms step_avg:55.55ms
step:1741/2110 train_time:96747ms step_avg:55.57ms
step:1742/2110 train_time:96834ms step_avg:55.59ms
step:1743/2110 train_time:96923ms step_avg:55.61ms
step:1744/2110 train_time:97010ms step_avg:55.62ms
step:1745/2110 train_time:97098ms step_avg:55.64ms
step:1746/2110 train_time:97186ms step_avg:55.66ms
step:1747/2110 train_time:97275ms step_avg:55.68ms
step:1748/2110 train_time:97362ms step_avg:55.70ms
step:1749/2110 train_time:97451ms step_avg:55.72ms
step:1750/2110 train_time:97538ms step_avg:55.74ms
step:1750/2110 val_loss:3.3784 train_time:97629ms step_avg:55.79ms
step:1751/2110 train_time:97653ms step_avg:55.77ms
step:1752/2110 train_time:97720ms step_avg:55.78ms
step:1753/2110 train_time:97817ms step_avg:55.80ms
step:1754/2110 train_time:97903ms step_avg:55.82ms
step:1755/2110 train_time:97992ms step_avg:55.84ms
step:1756/2110 train_time:98078ms step_avg:55.85ms
step:1757/2110 train_time:98164ms step_avg:55.87ms
step:1758/2110 train_time:98252ms step_avg:55.89ms
step:1759/2110 train_time:98338ms step_avg:55.91ms
step:1760/2110 train_time:98424ms step_avg:55.92ms
step:1761/2110 train_time:98511ms step_avg:55.94ms
step:1762/2110 train_time:98602ms step_avg:55.96ms
step:1763/2110 train_time:98694ms step_avg:55.98ms
step:1764/2110 train_time:98784ms step_avg:56.00ms
step:1765/2110 train_time:98874ms step_avg:56.02ms
step:1766/2110 train_time:98961ms step_avg:56.04ms
step:1767/2110 train_time:99049ms step_avg:56.05ms
step:1768/2110 train_time:99135ms step_avg:56.07ms
step:1769/2110 train_time:99222ms step_avg:56.09ms
step:1770/2110 train_time:99308ms step_avg:56.11ms
step:1771/2110 train_time:99396ms step_avg:56.12ms
step:1772/2110 train_time:99482ms step_avg:56.14ms
step:1773/2110 train_time:99571ms step_avg:56.16ms
step:1774/2110 train_time:99661ms step_avg:56.18ms
step:1775/2110 train_time:99752ms step_avg:56.20ms
step:1776/2110 train_time:99840ms step_avg:56.22ms
step:1777/2110 train_time:99931ms step_avg:56.24ms
step:1778/2110 train_time:100018ms step_avg:56.25ms
step:1779/2110 train_time:100105ms step_avg:56.27ms
step:1780/2110 train_time:100192ms step_avg:56.29ms
step:1781/2110 train_time:100279ms step_avg:56.30ms
step:1782/2110 train_time:100366ms step_avg:56.32ms
step:1783/2110 train_time:100455ms step_avg:56.34ms
step:1784/2110 train_time:100541ms step_avg:56.36ms
step:1785/2110 train_time:100630ms step_avg:56.38ms
step:1786/2110 train_time:100719ms step_avg:56.39ms
step:1787/2110 train_time:100810ms step_avg:56.41ms
step:1788/2110 train_time:100899ms step_avg:56.43ms
step:1789/2110 train_time:100986ms step_avg:56.45ms
step:1790/2110 train_time:101074ms step_avg:56.47ms
step:1791/2110 train_time:101162ms step_avg:56.48ms
step:1792/2110 train_time:101248ms step_avg:56.50ms
step:1793/2110 train_time:101336ms step_avg:56.52ms
step:1794/2110 train_time:101422ms step_avg:56.53ms
step:1795/2110 train_time:101511ms step_avg:56.55ms
step:1796/2110 train_time:101599ms step_avg:56.57ms
step:1797/2110 train_time:101688ms step_avg:56.59ms
step:1798/2110 train_time:101777ms step_avg:56.61ms
step:1799/2110 train_time:101865ms step_avg:56.62ms
step:1800/2110 train_time:101952ms step_avg:56.64ms
step:1801/2110 train_time:102041ms step_avg:56.66ms
step:1802/2110 train_time:102128ms step_avg:56.67ms
step:1803/2110 train_time:102215ms step_avg:56.69ms
step:1804/2110 train_time:102302ms step_avg:56.71ms
step:1805/2110 train_time:102389ms step_avg:56.73ms
step:1806/2110 train_time:102477ms step_avg:56.74ms
step:1807/2110 train_time:102565ms step_avg:56.76ms
step:1808/2110 train_time:102653ms step_avg:56.78ms
step:1809/2110 train_time:102742ms step_avg:56.80ms
step:1810/2110 train_time:102830ms step_avg:56.81ms
step:1811/2110 train_time:102919ms step_avg:56.83ms
step:1812/2110 train_time:103006ms step_avg:56.85ms
step:1813/2110 train_time:103094ms step_avg:56.86ms
step:1814/2110 train_time:103181ms step_avg:56.88ms
step:1815/2110 train_time:103270ms step_avg:56.90ms
step:1816/2110 train_time:103357ms step_avg:56.91ms
step:1817/2110 train_time:103446ms step_avg:56.93ms
step:1818/2110 train_time:103533ms step_avg:56.95ms
step:1819/2110 train_time:103622ms step_avg:56.97ms
step:1820/2110 train_time:103709ms step_avg:56.98ms
step:1821/2110 train_time:103798ms step_avg:57.00ms
step:1822/2110 train_time:103886ms step_avg:57.02ms
step:1823/2110 train_time:103974ms step_avg:57.03ms
step:1824/2110 train_time:104061ms step_avg:57.05ms
step:1825/2110 train_time:104149ms step_avg:57.07ms
step:1826/2110 train_time:104236ms step_avg:57.08ms
step:1827/2110 train_time:104325ms step_avg:57.10ms
step:1828/2110 train_time:104412ms step_avg:57.12ms
step:1829/2110 train_time:104501ms step_avg:57.14ms
step:1830/2110 train_time:104589ms step_avg:57.15ms
step:1831/2110 train_time:104678ms step_avg:57.17ms
step:1832/2110 train_time:104765ms step_avg:57.19ms
step:1833/2110 train_time:104854ms step_avg:57.20ms
step:1834/2110 train_time:104941ms step_avg:57.22ms
step:1835/2110 train_time:105029ms step_avg:57.24ms
step:1836/2110 train_time:105116ms step_avg:57.25ms
step:1837/2110 train_time:105204ms step_avg:57.27ms
step:1838/2110 train_time:105291ms step_avg:57.29ms
step:1839/2110 train_time:105380ms step_avg:57.30ms
step:1840/2110 train_time:105468ms step_avg:57.32ms
step:1841/2110 train_time:105556ms step_avg:57.34ms
step:1842/2110 train_time:105643ms step_avg:57.35ms
step:1843/2110 train_time:105732ms step_avg:57.37ms
step:1844/2110 train_time:105819ms step_avg:57.39ms
step:1845/2110 train_time:105908ms step_avg:57.40ms
step:1846/2110 train_time:105996ms step_avg:57.42ms
step:1847/2110 train_time:106084ms step_avg:57.44ms
step:1848/2110 train_time:106171ms step_avg:57.45ms
step:1849/2110 train_time:106262ms step_avg:57.47ms
step:1850/2110 train_time:106349ms step_avg:57.49ms
step:1851/2110 train_time:106438ms step_avg:57.50ms
step:1852/2110 train_time:106525ms step_avg:57.52ms
step:1853/2110 train_time:106614ms step_avg:57.54ms
step:1854/2110 train_time:106701ms step_avg:57.55ms
step:1855/2110 train_time:106789ms step_avg:57.57ms
step:1856/2110 train_time:106878ms step_avg:57.58ms
step:1857/2110 train_time:106966ms step_avg:57.60ms
step:1858/2110 train_time:107053ms step_avg:57.62ms
step:1859/2110 train_time:107141ms step_avg:57.63ms
step:1860/2110 train_time:107228ms step_avg:57.65ms
step:1861/2110 train_time:107316ms step_avg:57.67ms
step:1862/2110 train_time:107403ms step_avg:57.68ms
step:1863/2110 train_time:107492ms step_avg:57.70ms
step:1864/2110 train_time:107580ms step_avg:57.71ms
step:1865/2110 train_time:107669ms step_avg:57.73ms
step:1866/2110 train_time:107757ms step_avg:57.75ms
step:1867/2110 train_time:107845ms step_avg:57.76ms
step:1868/2110 train_time:107932ms step_avg:57.78ms
step:1869/2110 train_time:108021ms step_avg:57.80ms
step:1870/2110 train_time:108107ms step_avg:57.81ms
step:1871/2110 train_time:108196ms step_avg:57.83ms
step:1872/2110 train_time:108283ms step_avg:57.84ms
step:1873/2110 train_time:108372ms step_avg:57.86ms
step:1874/2110 train_time:108460ms step_avg:57.88ms
step:1875/2110 train_time:108548ms step_avg:57.89ms
step:1876/2110 train_time:108635ms step_avg:57.91ms
step:1877/2110 train_time:108724ms step_avg:57.92ms
step:1878/2110 train_time:108811ms step_avg:57.94ms
step:1879/2110 train_time:108899ms step_avg:57.96ms
step:1880/2110 train_time:108986ms step_avg:57.97ms
step:1881/2110 train_time:109075ms step_avg:57.99ms
step:1882/2110 train_time:109161ms step_avg:58.00ms
step:1883/2110 train_time:109250ms step_avg:58.02ms
step:1884/2110 train_time:109337ms step_avg:58.03ms
step:1885/2110 train_time:109426ms step_avg:58.05ms
step:1886/2110 train_time:109513ms step_avg:58.07ms
step:1887/2110 train_time:109601ms step_avg:58.08ms
step:1888/2110 train_time:109689ms step_avg:58.10ms
step:1889/2110 train_time:109779ms step_avg:58.11ms
step:1890/2110 train_time:109866ms step_avg:58.13ms
step:1891/2110 train_time:109955ms step_avg:58.15ms
step:1892/2110 train_time:110041ms step_avg:58.16ms
step:1893/2110 train_time:110130ms step_avg:58.18ms
step:1894/2110 train_time:110217ms step_avg:58.19ms
step:1895/2110 train_time:110306ms step_avg:58.21ms
step:1896/2110 train_time:110393ms step_avg:58.22ms
step:1897/2110 train_time:110482ms step_avg:58.24ms
step:1898/2110 train_time:110569ms step_avg:58.26ms
step:1899/2110 train_time:110658ms step_avg:58.27ms
step:1900/2110 train_time:110745ms step_avg:58.29ms
step:1901/2110 train_time:110835ms step_avg:58.30ms
step:1902/2110 train_time:110921ms step_avg:58.32ms
step:1903/2110 train_time:111009ms step_avg:58.33ms
step:1904/2110 train_time:111096ms step_avg:58.35ms
step:1905/2110 train_time:111184ms step_avg:58.36ms
step:1906/2110 train_time:111271ms step_avg:58.38ms
step:1907/2110 train_time:111360ms step_avg:58.40ms
step:1908/2110 train_time:111447ms step_avg:58.41ms
step:1909/2110 train_time:111536ms step_avg:58.43ms
step:1910/2110 train_time:111623ms step_avg:58.44ms
step:1911/2110 train_time:111711ms step_avg:58.46ms
step:1912/2110 train_time:111799ms step_avg:58.47ms
step:1913/2110 train_time:111887ms step_avg:58.49ms
step:1914/2110 train_time:111975ms step_avg:58.50ms
step:1915/2110 train_time:112064ms step_avg:58.52ms
step:1916/2110 train_time:112152ms step_avg:58.53ms
step:1917/2110 train_time:112239ms step_avg:58.55ms
step:1918/2110 train_time:112326ms step_avg:58.56ms
step:1919/2110 train_time:112416ms step_avg:58.58ms
step:1920/2110 train_time:112502ms step_avg:58.59ms
step:1921/2110 train_time:112591ms step_avg:58.61ms
step:1922/2110 train_time:112679ms step_avg:58.63ms
step:1923/2110 train_time:112767ms step_avg:58.64ms
step:1924/2110 train_time:112855ms step_avg:58.66ms
step:1925/2110 train_time:112944ms step_avg:58.67ms
step:1926/2110 train_time:113031ms step_avg:58.69ms
step:1927/2110 train_time:113120ms step_avg:58.70ms
step:1928/2110 train_time:113206ms step_avg:58.72ms
step:1929/2110 train_time:113295ms step_avg:58.73ms
step:1930/2110 train_time:113382ms step_avg:58.75ms
step:1931/2110 train_time:113470ms step_avg:58.76ms
step:1932/2110 train_time:113557ms step_avg:58.78ms
step:1933/2110 train_time:113645ms step_avg:58.79ms
step:1934/2110 train_time:113734ms step_avg:58.81ms
step:1935/2110 train_time:113822ms step_avg:58.82ms
step:1936/2110 train_time:113910ms step_avg:58.84ms
step:1937/2110 train_time:113998ms step_avg:58.85ms
step:1938/2110 train_time:114085ms step_avg:58.87ms
step:1939/2110 train_time:114174ms step_avg:58.88ms
step:1940/2110 train_time:114261ms step_avg:58.90ms
step:1941/2110 train_time:114349ms step_avg:58.91ms
step:1942/2110 train_time:114436ms step_avg:58.93ms
step:1943/2110 train_time:114525ms step_avg:58.94ms
step:1944/2110 train_time:114612ms step_avg:58.96ms
step:1945/2110 train_time:114700ms step_avg:58.97ms
step:1946/2110 train_time:114788ms step_avg:58.99ms
step:1947/2110 train_time:114877ms step_avg:59.00ms
step:1948/2110 train_time:114964ms step_avg:59.02ms
step:1949/2110 train_time:115053ms step_avg:59.03ms
step:1950/2110 train_time:115140ms step_avg:59.05ms
step:1951/2110 train_time:115228ms step_avg:59.06ms
step:1952/2110 train_time:115315ms step_avg:59.08ms
step:1953/2110 train_time:115404ms step_avg:59.09ms
step:1954/2110 train_time:115491ms step_avg:59.11ms
step:1955/2110 train_time:115579ms step_avg:59.12ms
step:1956/2110 train_time:115666ms step_avg:59.13ms
step:1957/2110 train_time:115756ms step_avg:59.15ms
step:1958/2110 train_time:115842ms step_avg:59.16ms
step:1959/2110 train_time:115932ms step_avg:59.18ms
step:1960/2110 train_time:116019ms step_avg:59.19ms
step:1961/2110 train_time:116107ms step_avg:59.21ms
step:1962/2110 train_time:116195ms step_avg:59.22ms
step:1963/2110 train_time:116283ms step_avg:59.24ms
step:1964/2110 train_time:116371ms step_avg:59.25ms
step:1965/2110 train_time:116459ms step_avg:59.27ms
step:1966/2110 train_time:116546ms step_avg:59.28ms
step:1967/2110 train_time:116635ms step_avg:59.30ms
step:1968/2110 train_time:116722ms step_avg:59.31ms
step:1969/2110 train_time:116811ms step_avg:59.33ms
step:1970/2110 train_time:116899ms step_avg:59.34ms
step:1971/2110 train_time:116989ms step_avg:59.36ms
step:1972/2110 train_time:117076ms step_avg:59.37ms
step:1973/2110 train_time:117165ms step_avg:59.38ms
step:1974/2110 train_time:117252ms step_avg:59.40ms
step:1975/2110 train_time:117340ms step_avg:59.41ms
step:1976/2110 train_time:117426ms step_avg:59.43ms
step:1977/2110 train_time:117515ms step_avg:59.44ms
step:1978/2110 train_time:117602ms step_avg:59.45ms
step:1979/2110 train_time:117690ms step_avg:59.47ms
step:1980/2110 train_time:117778ms step_avg:59.48ms
step:1981/2110 train_time:117867ms step_avg:59.50ms
step:1982/2110 train_time:117955ms step_avg:59.51ms
step:1983/2110 train_time:118043ms step_avg:59.53ms
step:1984/2110 train_time:118130ms step_avg:59.54ms
step:1985/2110 train_time:118218ms step_avg:59.56ms
step:1986/2110 train_time:118305ms step_avg:59.57ms
step:1987/2110 train_time:118393ms step_avg:59.58ms
step:1988/2110 train_time:118481ms step_avg:59.60ms
step:1989/2110 train_time:118569ms step_avg:59.61ms
step:1990/2110 train_time:118657ms step_avg:59.63ms
step:1991/2110 train_time:118744ms step_avg:59.64ms
step:1992/2110 train_time:118832ms step_avg:59.65ms
step:1993/2110 train_time:118921ms step_avg:59.67ms
step:1994/2110 train_time:119008ms step_avg:59.68ms
step:1995/2110 train_time:119097ms step_avg:59.70ms
step:1996/2110 train_time:119183ms step_avg:59.71ms
step:1997/2110 train_time:119271ms step_avg:59.73ms
step:1998/2110 train_time:119359ms step_avg:59.74ms
step:1999/2110 train_time:119447ms step_avg:59.75ms
step:2000/2110 train_time:119535ms step_avg:59.77ms
step:2000/2110 val_loss:3.3037 train_time:119626ms step_avg:59.81ms
step:2001/2110 train_time:119656ms step_avg:59.80ms
step:2002/2110 train_time:119716ms step_avg:59.80ms
step:2003/2110 train_time:119810ms step_avg:59.82ms
step:2004/2110 train_time:119898ms step_avg:59.83ms
step:2005/2110 train_time:119986ms step_avg:59.84ms
step:2006/2110 train_time:120072ms step_avg:59.86ms
step:2007/2110 train_time:120159ms step_avg:59.87ms
step:2008/2110 train_time:120245ms step_avg:59.88ms
step:2009/2110 train_time:120333ms step_avg:59.90ms
step:2010/2110 train_time:120419ms step_avg:59.91ms
step:2011/2110 train_time:120508ms step_avg:59.92ms
step:2012/2110 train_time:120596ms step_avg:59.94ms
step:2013/2110 train_time:120687ms step_avg:59.95ms
step:2014/2110 train_time:120776ms step_avg:59.97ms
step:2015/2110 train_time:120867ms step_avg:59.98ms
step:2016/2110 train_time:120954ms step_avg:60.00ms
step:2017/2110 train_time:121041ms step_avg:60.01ms
step:2018/2110 train_time:121128ms step_avg:60.02ms
step:2019/2110 train_time:121216ms step_avg:60.04ms
step:2020/2110 train_time:121302ms step_avg:60.05ms
step:2021/2110 train_time:121390ms step_avg:60.06ms
step:2022/2110 train_time:121476ms step_avg:60.08ms
step:2023/2110 train_time:121565ms step_avg:60.09ms
step:2024/2110 train_time:121654ms step_avg:60.11ms
step:2025/2110 train_time:121745ms step_avg:60.12ms
step:2026/2110 train_time:121833ms step_avg:60.13ms
step:2027/2110 train_time:121922ms step_avg:60.15ms
step:2028/2110 train_time:122009ms step_avg:60.16ms
step:2029/2110 train_time:122097ms step_avg:60.18ms
step:2030/2110 train_time:122184ms step_avg:60.19ms
step:2031/2110 train_time:122271ms step_avg:60.20ms
step:2032/2110 train_time:122358ms step_avg:60.22ms
step:2033/2110 train_time:122445ms step_avg:60.23ms
step:2034/2110 train_time:122533ms step_avg:60.24ms
step:2035/2110 train_time:122623ms step_avg:60.26ms
step:2036/2110 train_time:122711ms step_avg:60.27ms
step:2037/2110 train_time:122799ms step_avg:60.28ms
step:2038/2110 train_time:122887ms step_avg:60.30ms
step:2039/2110 train_time:122976ms step_avg:60.31ms
step:2040/2110 train_time:123063ms step_avg:60.32ms
step:2041/2110 train_time:123150ms step_avg:60.34ms
step:2042/2110 train_time:123236ms step_avg:60.35ms
step:2043/2110 train_time:123324ms step_avg:60.36ms
step:2044/2110 train_time:123410ms step_avg:60.38ms
step:2045/2110 train_time:123499ms step_avg:60.39ms
step:2046/2110 train_time:123587ms step_avg:60.40ms
step:2047/2110 train_time:123677ms step_avg:60.42ms
step:2048/2110 train_time:123766ms step_avg:60.43ms
step:2049/2110 train_time:123856ms step_avg:60.45ms
step:2050/2110 train_time:123943ms step_avg:60.46ms
step:2051/2110 train_time:124032ms step_avg:60.47ms
step:2052/2110 train_time:124118ms step_avg:60.49ms
step:2053/2110 train_time:124206ms step_avg:60.50ms
step:2054/2110 train_time:124293ms step_avg:60.51ms
step:2055/2110 train_time:124381ms step_avg:60.53ms
step:2056/2110 train_time:124467ms step_avg:60.54ms
step:2057/2110 train_time:124556ms step_avg:60.55ms
step:2058/2110 train_time:124644ms step_avg:60.57ms
step:2059/2110 train_time:124734ms step_avg:60.58ms
step:2060/2110 train_time:124823ms step_avg:60.59ms
step:2061/2110 train_time:124911ms step_avg:60.61ms
step:2062/2110 train_time:124998ms step_avg:60.62ms
step:2063/2110 train_time:125086ms step_avg:60.63ms
step:2064/2110 train_time:125173ms step_avg:60.65ms
step:2065/2110 train_time:125260ms step_avg:60.66ms
step:2066/2110 train_time:125348ms step_avg:60.67ms
step:2067/2110 train_time:125436ms step_avg:60.68ms
step:2068/2110 train_time:125523ms step_avg:60.70ms
step:2069/2110 train_time:125613ms step_avg:60.71ms
step:2070/2110 train_time:125701ms step_avg:60.72ms
step:2071/2110 train_time:125790ms step_avg:60.74ms
step:2072/2110 train_time:125877ms step_avg:60.75ms
step:2073/2110 train_time:125966ms step_avg:60.77ms
step:2074/2110 train_time:126053ms step_avg:60.78ms
step:2075/2110 train_time:126142ms step_avg:60.79ms
step:2076/2110 train_time:126229ms step_avg:60.80ms
step:2077/2110 train_time:126317ms step_avg:60.82ms
step:2078/2110 train_time:126405ms step_avg:60.83ms
step:2079/2110 train_time:126495ms step_avg:60.84ms
step:2080/2110 train_time:126582ms step_avg:60.86ms
step:2081/2110 train_time:126672ms step_avg:60.87ms
step:2082/2110 train_time:126758ms step_avg:60.88ms
step:2083/2110 train_time:126848ms step_avg:60.90ms
step:2084/2110 train_time:126935ms step_avg:60.91ms
step:2085/2110 train_time:127025ms step_avg:60.92ms
step:2086/2110 train_time:127113ms step_avg:60.94ms
step:2087/2110 train_time:127202ms step_avg:60.95ms
step:2088/2110 train_time:127289ms step_avg:60.96ms
step:2089/2110 train_time:127377ms step_avg:60.98ms
step:2090/2110 train_time:127466ms step_avg:60.99ms
step:2091/2110 train_time:127555ms step_avg:61.00ms
step:2092/2110 train_time:127642ms step_avg:61.01ms
step:2093/2110 train_time:127731ms step_avg:61.03ms
step:2094/2110 train_time:127818ms step_avg:61.04ms
step:2095/2110 train_time:127907ms step_avg:61.05ms
step:2096/2110 train_time:127995ms step_avg:61.07ms
step:2097/2110 train_time:128084ms step_avg:61.08ms
step:2098/2110 train_time:128171ms step_avg:61.09ms
step:2099/2110 train_time:128260ms step_avg:61.11ms
step:2100/2110 train_time:128347ms step_avg:61.12ms
step:2101/2110 train_time:128436ms step_avg:61.13ms
step:2102/2110 train_time:128523ms step_avg:61.14ms
step:2103/2110 train_time:128612ms step_avg:61.16ms
step:2104/2110 train_time:128699ms step_avg:61.17ms
step:2105/2110 train_time:128788ms step_avg:61.18ms
step:2106/2110 train_time:128875ms step_avg:61.19ms
step:2107/2110 train_time:128964ms step_avg:61.21ms
step:2108/2110 train_time:129052ms step_avg:61.22ms
step:2109/2110 train_time:129140ms step_avg:61.23ms
step:2110/2110 train_time:129229ms step_avg:61.25ms
step:2110/2110 val_loss:3.2795 train_time:129319ms step_avg:61.29ms
peak memory allocated: 29892 MiB reserved: 44556 MiB

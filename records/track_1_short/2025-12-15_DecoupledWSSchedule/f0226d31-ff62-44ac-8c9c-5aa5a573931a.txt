import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 15 21:05:16 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   39C    P0            128W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   43C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   43C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   37C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   42C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   42C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              88      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              88      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:102ms step_avg:102.34ms
step:2/2110 train_time:132ms step_avg:65.98ms
step:3/2110 train_time:157ms step_avg:52.33ms
step:4/2110 train_time:189ms step_avg:47.24ms
step:5/2110 train_time:223ms step_avg:44.51ms
step:6/2110 train_time:619ms step_avg:103.16ms
step:7/2110 train_time:698ms step_avg:99.68ms
step:8/2110 train_time:730ms step_avg:91.24ms
step:9/2110 train_time:763ms step_avg:84.74ms
step:10/2110 train_time:795ms step_avg:79.48ms
step:11/2110 train_time:828ms step_avg:75.30ms
step:12/2110 train_time:861ms step_avg:71.74ms
step:13/2110 train_time:894ms step_avg:68.78ms
step:14/2110 train_time:927ms step_avg:66.18ms
step:15/2110 train_time:960ms step_avg:63.99ms
step:16/2110 train_time:993ms step_avg:62.05ms
step:17/2110 train_time:1026ms step_avg:60.34ms
step:18/2110 train_time:1059ms step_avg:58.82ms
step:19/2110 train_time:1092ms step_avg:57.47ms
step:20/2110 train_time:1125ms step_avg:56.23ms
step:21/2110 train_time:1158ms step_avg:55.14ms
step:22/2110 train_time:1191ms step_avg:54.12ms
step:23/2110 train_time:1224ms step_avg:53.21ms
step:24/2110 train_time:1257ms step_avg:52.37ms
step:25/2110 train_time:1290ms step_avg:51.61ms
step:26/2110 train_time:1323ms step_avg:50.88ms
step:27/2110 train_time:1356ms step_avg:50.24ms
step:28/2110 train_time:1389ms step_avg:49.61ms
step:29/2110 train_time:1422ms step_avg:49.04ms
step:30/2110 train_time:1455ms step_avg:48.50ms
step:31/2110 train_time:1488ms step_avg:48.00ms
step:32/2110 train_time:1521ms step_avg:47.52ms
step:33/2110 train_time:1554ms step_avg:47.10ms
step:34/2110 train_time:1587ms step_avg:46.69ms
step:35/2110 train_time:1622ms step_avg:46.35ms
step:36/2110 train_time:1657ms step_avg:46.03ms
step:37/2110 train_time:1691ms step_avg:45.70ms
step:38/2110 train_time:1724ms step_avg:45.36ms
step:39/2110 train_time:1758ms step_avg:45.07ms
step:40/2110 train_time:1791ms step_avg:44.76ms
step:41/2110 train_time:1824ms step_avg:44.49ms
step:42/2110 train_time:1857ms step_avg:44.22ms
step:43/2110 train_time:1891ms step_avg:43.98ms
step:44/2110 train_time:1924ms step_avg:43.73ms
step:45/2110 train_time:1957ms step_avg:43.50ms
step:46/2110 train_time:1990ms step_avg:43.26ms
step:47/2110 train_time:2023ms step_avg:43.05ms
step:48/2110 train_time:2056ms step_avg:42.83ms
step:49/2110 train_time:2089ms step_avg:42.64ms
step:50/2110 train_time:2122ms step_avg:42.44ms
step:51/2110 train_time:2155ms step_avg:42.26ms
step:52/2110 train_time:2188ms step_avg:42.07ms
step:53/2110 train_time:2221ms step_avg:41.91ms
step:54/2110 train_time:2254ms step_avg:41.74ms
step:55/2110 train_time:2287ms step_avg:41.59ms
step:56/2110 train_time:2320ms step_avg:41.43ms
step:57/2110 train_time:2353ms step_avg:41.29ms
step:58/2110 train_time:2386ms step_avg:41.13ms
step:59/2110 train_time:2419ms step_avg:41.00ms
step:60/2110 train_time:2452ms step_avg:40.86ms
step:61/2110 train_time:2485ms step_avg:40.74ms
step:62/2110 train_time:2518ms step_avg:40.61ms
step:63/2110 train_time:2552ms step_avg:40.50ms
step:64/2110 train_time:2584ms step_avg:40.38ms
step:65/2110 train_time:2618ms step_avg:40.28ms
step:66/2110 train_time:2652ms step_avg:40.18ms
step:67/2110 train_time:2685ms step_avg:40.07ms
step:68/2110 train_time:2718ms step_avg:39.97ms
step:69/2110 train_time:2751ms step_avg:39.88ms
step:70/2110 train_time:2784ms step_avg:39.77ms
step:71/2110 train_time:2819ms step_avg:39.71ms
step:72/2110 train_time:2851ms step_avg:39.59ms
step:73/2110 train_time:2884ms step_avg:39.51ms
step:74/2110 train_time:2917ms step_avg:39.42ms
step:75/2110 train_time:2951ms step_avg:39.34ms
step:76/2110 train_time:2984ms step_avg:39.26ms
step:77/2110 train_time:3017ms step_avg:39.18ms
step:78/2110 train_time:3050ms step_avg:39.10ms
step:79/2110 train_time:3083ms step_avg:39.03ms
step:80/2110 train_time:3116ms step_avg:38.95ms
step:81/2110 train_time:3149ms step_avg:38.88ms
step:82/2110 train_time:3182ms step_avg:38.80ms
step:83/2110 train_time:3215ms step_avg:38.73ms
step:84/2110 train_time:3247ms step_avg:38.66ms
step:85/2110 train_time:3281ms step_avg:38.59ms
step:86/2110 train_time:3314ms step_avg:38.53ms
step:87/2110 train_time:3347ms step_avg:38.47ms
step:88/2110 train_time:3379ms step_avg:38.40ms
step:89/2110 train_time:3412ms step_avg:38.34ms
step:90/2110 train_time:3445ms step_avg:38.28ms
step:91/2110 train_time:3478ms step_avg:38.22ms
step:92/2110 train_time:3511ms step_avg:38.16ms
step:93/2110 train_time:3544ms step_avg:38.11ms
step:94/2110 train_time:3577ms step_avg:38.05ms
step:95/2110 train_time:3610ms step_avg:38.00ms
step:96/2110 train_time:3643ms step_avg:37.94ms
step:97/2110 train_time:3676ms step_avg:37.90ms
step:98/2110 train_time:3709ms step_avg:37.85ms
step:99/2110 train_time:3743ms step_avg:37.80ms
step:100/2110 train_time:3776ms step_avg:37.76ms
step:101/2110 train_time:3809ms step_avg:37.71ms
step:102/2110 train_time:3842ms step_avg:37.66ms
step:103/2110 train_time:3875ms step_avg:37.62ms
step:104/2110 train_time:3908ms step_avg:37.57ms
step:105/2110 train_time:3941ms step_avg:37.53ms
step:106/2110 train_time:3973ms step_avg:37.49ms
step:107/2110 train_time:4007ms step_avg:37.45ms
step:108/2110 train_time:4040ms step_avg:37.40ms
step:109/2110 train_time:4073ms step_avg:37.37ms
step:110/2110 train_time:4105ms step_avg:37.32ms
step:111/2110 train_time:4138ms step_avg:37.28ms
step:112/2110 train_time:4171ms step_avg:37.24ms
step:113/2110 train_time:4204ms step_avg:37.21ms
step:114/2110 train_time:4237ms step_avg:37.17ms
step:115/2110 train_time:4271ms step_avg:37.14ms
step:116/2110 train_time:4303ms step_avg:37.10ms
step:117/2110 train_time:4337ms step_avg:37.06ms
step:118/2110 train_time:4369ms step_avg:37.03ms
step:119/2110 train_time:4404ms step_avg:37.01ms
step:120/2110 train_time:4441ms step_avg:37.00ms
step:121/2110 train_time:4474ms step_avg:36.97ms
step:122/2110 train_time:4510ms step_avg:36.97ms
step:123/2110 train_time:4546ms step_avg:36.96ms
step:124/2110 train_time:4585ms step_avg:36.98ms
step:125/2110 train_time:4621ms step_avg:36.97ms
step:126/2110 train_time:4658ms step_avg:36.97ms
step:127/2110 train_time:4693ms step_avg:36.95ms
step:128/2110 train_time:4732ms step_avg:36.97ms
step:129/2110 train_time:4769ms step_avg:36.97ms
step:130/2110 train_time:4805ms step_avg:36.96ms
step:131/2110 train_time:4842ms step_avg:36.96ms
step:132/2110 train_time:4879ms step_avg:36.96ms
step:133/2110 train_time:4913ms step_avg:36.94ms
step:134/2110 train_time:4936ms step_avg:36.83ms
step:135/2110 train_time:4957ms step_avg:36.72ms
step:136/2110 train_time:4979ms step_avg:36.61ms
step:137/2110 train_time:5012ms step_avg:36.58ms
step:138/2110 train_time:5044ms step_avg:36.55ms
step:139/2110 train_time:5077ms step_avg:36.53ms
step:140/2110 train_time:5109ms step_avg:36.50ms
step:141/2110 train_time:5143ms step_avg:36.47ms
step:142/2110 train_time:5175ms step_avg:36.44ms
step:143/2110 train_time:5209ms step_avg:36.42ms
step:144/2110 train_time:5241ms step_avg:36.40ms
step:145/2110 train_time:5275ms step_avg:36.38ms
step:146/2110 train_time:5307ms step_avg:36.35ms
step:147/2110 train_time:5340ms step_avg:36.33ms
step:148/2110 train_time:5372ms step_avg:36.30ms
step:149/2110 train_time:5406ms step_avg:36.28ms
step:150/2110 train_time:5438ms step_avg:36.25ms
step:151/2110 train_time:5471ms step_avg:36.23ms
step:152/2110 train_time:5504ms step_avg:36.21ms
step:153/2110 train_time:5537ms step_avg:36.19ms
step:154/2110 train_time:5570ms step_avg:36.17ms
step:155/2110 train_time:5603ms step_avg:36.15ms
step:156/2110 train_time:5636ms step_avg:36.13ms
step:157/2110 train_time:5669ms step_avg:36.11ms
step:158/2110 train_time:5702ms step_avg:36.09ms
step:159/2110 train_time:5736ms step_avg:36.08ms
step:160/2110 train_time:5769ms step_avg:36.06ms
step:161/2110 train_time:5803ms step_avg:36.04ms
step:162/2110 train_time:5836ms step_avg:36.02ms
step:163/2110 train_time:5869ms step_avg:36.01ms
step:164/2110 train_time:5902ms step_avg:35.99ms
step:165/2110 train_time:5935ms step_avg:35.97ms
step:166/2110 train_time:5968ms step_avg:35.95ms
step:167/2110 train_time:6001ms step_avg:35.94ms
step:168/2110 train_time:6034ms step_avg:35.92ms
step:169/2110 train_time:6067ms step_avg:35.90ms
step:170/2110 train_time:6100ms step_avg:35.88ms
step:171/2110 train_time:6133ms step_avg:35.87ms
step:172/2110 train_time:6166ms step_avg:35.85ms
step:173/2110 train_time:6199ms step_avg:35.83ms
step:174/2110 train_time:6231ms step_avg:35.81ms
step:175/2110 train_time:6265ms step_avg:35.80ms
step:176/2110 train_time:6297ms step_avg:35.78ms
step:177/2110 train_time:6330ms step_avg:35.77ms
step:178/2110 train_time:6363ms step_avg:35.75ms
step:179/2110 train_time:6396ms step_avg:35.73ms
step:180/2110 train_time:6429ms step_avg:35.71ms
step:181/2110 train_time:6462ms step_avg:35.70ms
step:182/2110 train_time:6494ms step_avg:35.68ms
step:183/2110 train_time:6528ms step_avg:35.67ms
step:184/2110 train_time:6561ms step_avg:35.66ms
step:185/2110 train_time:6593ms step_avg:35.64ms
step:186/2110 train_time:6626ms step_avg:35.62ms
step:187/2110 train_time:6659ms step_avg:35.61ms
step:188/2110 train_time:6692ms step_avg:35.59ms
step:189/2110 train_time:6725ms step_avg:35.58ms
step:190/2110 train_time:6758ms step_avg:35.57ms
step:191/2110 train_time:6792ms step_avg:35.56ms
step:192/2110 train_time:6824ms step_avg:35.54ms
step:193/2110 train_time:6858ms step_avg:35.53ms
step:194/2110 train_time:6891ms step_avg:35.52ms
step:195/2110 train_time:6923ms step_avg:35.50ms
step:196/2110 train_time:6957ms step_avg:35.50ms
step:197/2110 train_time:6990ms step_avg:35.48ms
step:198/2110 train_time:7022ms step_avg:35.47ms
step:199/2110 train_time:7056ms step_avg:35.46ms
step:200/2110 train_time:7088ms step_avg:35.44ms
step:201/2110 train_time:7122ms step_avg:35.43ms
step:202/2110 train_time:7155ms step_avg:35.42ms
step:203/2110 train_time:7188ms step_avg:35.41ms
step:204/2110 train_time:7221ms step_avg:35.40ms
step:205/2110 train_time:7254ms step_avg:35.38ms
step:206/2110 train_time:7286ms step_avg:35.37ms
step:207/2110 train_time:7319ms step_avg:35.36ms
step:208/2110 train_time:7352ms step_avg:35.35ms
step:209/2110 train_time:7385ms step_avg:35.34ms
step:210/2110 train_time:7417ms step_avg:35.32ms
step:211/2110 train_time:7451ms step_avg:35.31ms
step:212/2110 train_time:7484ms step_avg:35.30ms
step:213/2110 train_time:7517ms step_avg:35.29ms
step:214/2110 train_time:7549ms step_avg:35.28ms
step:215/2110 train_time:7583ms step_avg:35.27ms
step:216/2110 train_time:7615ms step_avg:35.26ms
step:217/2110 train_time:7649ms step_avg:35.25ms
step:218/2110 train_time:7681ms step_avg:35.23ms
step:219/2110 train_time:7715ms step_avg:35.23ms
step:220/2110 train_time:7748ms step_avg:35.22ms
step:221/2110 train_time:7781ms step_avg:35.21ms
step:222/2110 train_time:7814ms step_avg:35.20ms
step:223/2110 train_time:7846ms step_avg:35.19ms
step:224/2110 train_time:7880ms step_avg:35.18ms
step:225/2110 train_time:7913ms step_avg:35.17ms
step:226/2110 train_time:7945ms step_avg:35.16ms
step:227/2110 train_time:7979ms step_avg:35.15ms
step:228/2110 train_time:8011ms step_avg:35.14ms
step:229/2110 train_time:8044ms step_avg:35.13ms
step:230/2110 train_time:8077ms step_avg:35.12ms
step:231/2110 train_time:8111ms step_avg:35.11ms
step:232/2110 train_time:8144ms step_avg:35.10ms
step:233/2110 train_time:8177ms step_avg:35.09ms
step:234/2110 train_time:8210ms step_avg:35.08ms
step:235/2110 train_time:8243ms step_avg:35.08ms
step:236/2110 train_time:8276ms step_avg:35.07ms
step:237/2110 train_time:8309ms step_avg:35.06ms
step:238/2110 train_time:8342ms step_avg:35.05ms
step:239/2110 train_time:8375ms step_avg:35.04ms
step:240/2110 train_time:8407ms step_avg:35.03ms
step:241/2110 train_time:8440ms step_avg:35.02ms
step:242/2110 train_time:8474ms step_avg:35.02ms
step:243/2110 train_time:8506ms step_avg:35.00ms
step:244/2110 train_time:8545ms step_avg:35.02ms
step:245/2110 train_time:8571ms step_avg:34.99ms
step:246/2110 train_time:8604ms step_avg:34.98ms
step:247/2110 train_time:8637ms step_avg:34.97ms
step:248/2110 train_time:8671ms step_avg:34.96ms
step:249/2110 train_time:8703ms step_avg:34.95ms
step:250/2110 train_time:8736ms step_avg:34.94ms
step:250/2110 val_loss:4.3005 train_time:8772ms step_avg:35.09ms
step:251/2110 train_time:8799ms step_avg:35.06ms
step:252/2110 train_time:8826ms step_avg:35.03ms
step:253/2110 train_time:8852ms step_avg:34.99ms
step:254/2110 train_time:8879ms step_avg:34.96ms
step:255/2110 train_time:8909ms step_avg:34.94ms
step:256/2110 train_time:8943ms step_avg:34.93ms
step:257/2110 train_time:8977ms step_avg:34.93ms
step:258/2110 train_time:9009ms step_avg:34.92ms
step:259/2110 train_time:9044ms step_avg:34.92ms
step:260/2110 train_time:9077ms step_avg:34.91ms
step:261/2110 train_time:9110ms step_avg:34.90ms
step:262/2110 train_time:9142ms step_avg:34.89ms
step:263/2110 train_time:9175ms step_avg:34.89ms
step:264/2110 train_time:9208ms step_avg:34.88ms
step:265/2110 train_time:9241ms step_avg:34.87ms
step:266/2110 train_time:9273ms step_avg:34.86ms
step:267/2110 train_time:9306ms step_avg:34.85ms
step:268/2110 train_time:9338ms step_avg:34.84ms
step:269/2110 train_time:9371ms step_avg:34.84ms
step:270/2110 train_time:9404ms step_avg:34.83ms
step:271/2110 train_time:9437ms step_avg:34.82ms
step:272/2110 train_time:9470ms step_avg:34.81ms
step:273/2110 train_time:9503ms step_avg:34.81ms
step:274/2110 train_time:9535ms step_avg:34.80ms
step:275/2110 train_time:9568ms step_avg:34.79ms
step:276/2110 train_time:9602ms step_avg:34.79ms
step:277/2110 train_time:9634ms step_avg:34.78ms
step:278/2110 train_time:9666ms step_avg:34.77ms
step:279/2110 train_time:9699ms step_avg:34.76ms
step:280/2110 train_time:9732ms step_avg:34.76ms
step:281/2110 train_time:9765ms step_avg:34.75ms
step:282/2110 train_time:9798ms step_avg:34.74ms
step:283/2110 train_time:9832ms step_avg:34.74ms
step:284/2110 train_time:9865ms step_avg:34.73ms
step:285/2110 train_time:9899ms step_avg:34.73ms
step:286/2110 train_time:9932ms step_avg:34.73ms
step:287/2110 train_time:9966ms step_avg:34.72ms
step:288/2110 train_time:9999ms step_avg:34.72ms
step:289/2110 train_time:10032ms step_avg:34.71ms
step:290/2110 train_time:10065ms step_avg:34.71ms
step:291/2110 train_time:10099ms step_avg:34.70ms
step:292/2110 train_time:10131ms step_avg:34.70ms
step:293/2110 train_time:10165ms step_avg:34.69ms
step:294/2110 train_time:10197ms step_avg:34.68ms
step:295/2110 train_time:10230ms step_avg:34.68ms
step:296/2110 train_time:10263ms step_avg:34.67ms
step:297/2110 train_time:10296ms step_avg:34.67ms
step:298/2110 train_time:10329ms step_avg:34.66ms
step:299/2110 train_time:10362ms step_avg:34.66ms
step:300/2110 train_time:10394ms step_avg:34.65ms
step:301/2110 train_time:10427ms step_avg:34.64ms
step:302/2110 train_time:10460ms step_avg:34.64ms
step:303/2110 train_time:10493ms step_avg:34.63ms
step:304/2110 train_time:10525ms step_avg:34.62ms
step:305/2110 train_time:10559ms step_avg:34.62ms
step:306/2110 train_time:10591ms step_avg:34.61ms
step:307/2110 train_time:10624ms step_avg:34.61ms
step:308/2110 train_time:10657ms step_avg:34.60ms
step:309/2110 train_time:10690ms step_avg:34.59ms
step:310/2110 train_time:10722ms step_avg:34.59ms
step:311/2110 train_time:10755ms step_avg:34.58ms
step:312/2110 train_time:10788ms step_avg:34.58ms
step:313/2110 train_time:10821ms step_avg:34.57ms
step:314/2110 train_time:10854ms step_avg:34.57ms
step:315/2110 train_time:10887ms step_avg:34.56ms
step:316/2110 train_time:10920ms step_avg:34.56ms
step:317/2110 train_time:10954ms step_avg:34.55ms
step:318/2110 train_time:10986ms step_avg:34.55ms
step:319/2110 train_time:11020ms step_avg:34.54ms
step:320/2110 train_time:11052ms step_avg:34.54ms
step:321/2110 train_time:11086ms step_avg:34.54ms
step:322/2110 train_time:11119ms step_avg:34.53ms
step:323/2110 train_time:11152ms step_avg:34.53ms
step:324/2110 train_time:11185ms step_avg:34.52ms
step:325/2110 train_time:11218ms step_avg:34.52ms
step:326/2110 train_time:11250ms step_avg:34.51ms
step:327/2110 train_time:11283ms step_avg:34.51ms
step:328/2110 train_time:11316ms step_avg:34.50ms
step:329/2110 train_time:11349ms step_avg:34.50ms
step:330/2110 train_time:11382ms step_avg:34.49ms
step:331/2110 train_time:11415ms step_avg:34.49ms
step:332/2110 train_time:11448ms step_avg:34.48ms
step:333/2110 train_time:11481ms step_avg:34.48ms
step:334/2110 train_time:11513ms step_avg:34.47ms
step:335/2110 train_time:11546ms step_avg:34.47ms
step:336/2110 train_time:11579ms step_avg:34.46ms
step:337/2110 train_time:11612ms step_avg:34.46ms
step:338/2110 train_time:11644ms step_avg:34.45ms
step:339/2110 train_time:11678ms step_avg:34.45ms
step:340/2110 train_time:11710ms step_avg:34.44ms
step:341/2110 train_time:11743ms step_avg:34.44ms
step:342/2110 train_time:11775ms step_avg:34.43ms
step:343/2110 train_time:11809ms step_avg:34.43ms
step:344/2110 train_time:11842ms step_avg:34.43ms
step:345/2110 train_time:11875ms step_avg:34.42ms
step:346/2110 train_time:11907ms step_avg:34.41ms
step:347/2110 train_time:11941ms step_avg:34.41ms
step:348/2110 train_time:11973ms step_avg:34.41ms
step:349/2110 train_time:12007ms step_avg:34.40ms
step:350/2110 train_time:12040ms step_avg:34.40ms
step:351/2110 train_time:12073ms step_avg:34.39ms
step:352/2110 train_time:12106ms step_avg:34.39ms
step:353/2110 train_time:12139ms step_avg:34.39ms
step:354/2110 train_time:12172ms step_avg:34.39ms
step:355/2110 train_time:12206ms step_avg:34.38ms
step:356/2110 train_time:12238ms step_avg:34.38ms
step:357/2110 train_time:12272ms step_avg:34.38ms
step:358/2110 train_time:12304ms step_avg:34.37ms
step:359/2110 train_time:12337ms step_avg:34.37ms
step:360/2110 train_time:12370ms step_avg:34.36ms
step:361/2110 train_time:12403ms step_avg:34.36ms
step:362/2110 train_time:12435ms step_avg:34.35ms
step:363/2110 train_time:12468ms step_avg:34.35ms
step:364/2110 train_time:12501ms step_avg:34.34ms
step:365/2110 train_time:12534ms step_avg:34.34ms
step:366/2110 train_time:12568ms step_avg:34.34ms
step:367/2110 train_time:12600ms step_avg:34.33ms
step:368/2110 train_time:12632ms step_avg:34.33ms
step:369/2110 train_time:12666ms step_avg:34.33ms
step:370/2110 train_time:12699ms step_avg:34.32ms
step:371/2110 train_time:12732ms step_avg:34.32ms
step:372/2110 train_time:12765ms step_avg:34.32ms
step:373/2110 train_time:12798ms step_avg:34.31ms
step:374/2110 train_time:12831ms step_avg:34.31ms
step:375/2110 train_time:12864ms step_avg:34.30ms
step:376/2110 train_time:12897ms step_avg:34.30ms
step:377/2110 train_time:12929ms step_avg:34.30ms
step:378/2110 train_time:12962ms step_avg:34.29ms
step:379/2110 train_time:12996ms step_avg:34.29ms
step:380/2110 train_time:13028ms step_avg:34.28ms
step:381/2110 train_time:13062ms step_avg:34.28ms
step:382/2110 train_time:13095ms step_avg:34.28ms
step:383/2110 train_time:13128ms step_avg:34.28ms
step:384/2110 train_time:13161ms step_avg:34.27ms
step:385/2110 train_time:13193ms step_avg:34.27ms
step:386/2110 train_time:13226ms step_avg:34.26ms
step:387/2110 train_time:13260ms step_avg:34.26ms
step:388/2110 train_time:13293ms step_avg:34.26ms
step:389/2110 train_time:13326ms step_avg:34.26ms
step:390/2110 train_time:13358ms step_avg:34.25ms
step:391/2110 train_time:13391ms step_avg:34.25ms
step:392/2110 train_time:13424ms step_avg:34.24ms
step:393/2110 train_time:13457ms step_avg:34.24ms
step:394/2110 train_time:13490ms step_avg:34.24ms
step:395/2110 train_time:13523ms step_avg:34.24ms
step:396/2110 train_time:13555ms step_avg:34.23ms
step:397/2110 train_time:13589ms step_avg:34.23ms
step:398/2110 train_time:13621ms step_avg:34.22ms
step:399/2110 train_time:13654ms step_avg:34.22ms
step:400/2110 train_time:13687ms step_avg:34.22ms
step:401/2110 train_time:13720ms step_avg:34.22ms
step:402/2110 train_time:13753ms step_avg:34.21ms
step:403/2110 train_time:13786ms step_avg:34.21ms
step:404/2110 train_time:13818ms step_avg:34.20ms
step:405/2110 train_time:13851ms step_avg:34.20ms
step:406/2110 train_time:13884ms step_avg:34.20ms
step:407/2110 train_time:13917ms step_avg:34.20ms
step:408/2110 train_time:13950ms step_avg:34.19ms
step:409/2110 train_time:13983ms step_avg:34.19ms
step:410/2110 train_time:14017ms step_avg:34.19ms
step:411/2110 train_time:14049ms step_avg:34.18ms
step:412/2110 train_time:14082ms step_avg:34.18ms
step:413/2110 train_time:14115ms step_avg:34.18ms
step:414/2110 train_time:14148ms step_avg:34.17ms
step:415/2110 train_time:14181ms step_avg:34.17ms
step:416/2110 train_time:14214ms step_avg:34.17ms
step:417/2110 train_time:14247ms step_avg:34.17ms
step:418/2110 train_time:14281ms step_avg:34.17ms
step:419/2110 train_time:14313ms step_avg:34.16ms
step:420/2110 train_time:14346ms step_avg:34.16ms
step:421/2110 train_time:14379ms step_avg:34.15ms
step:422/2110 train_time:14412ms step_avg:34.15ms
step:423/2110 train_time:14445ms step_avg:34.15ms
step:424/2110 train_time:14477ms step_avg:34.14ms
step:425/2110 train_time:14511ms step_avg:34.14ms
step:426/2110 train_time:14543ms step_avg:34.14ms
step:427/2110 train_time:14576ms step_avg:34.14ms
step:428/2110 train_time:14608ms step_avg:34.13ms
step:429/2110 train_time:14642ms step_avg:34.13ms
step:430/2110 train_time:14674ms step_avg:34.13ms
step:431/2110 train_time:14707ms step_avg:34.12ms
step:432/2110 train_time:14740ms step_avg:34.12ms
step:433/2110 train_time:14773ms step_avg:34.12ms
step:434/2110 train_time:14805ms step_avg:34.11ms
step:435/2110 train_time:14839ms step_avg:34.11ms
step:436/2110 train_time:14871ms step_avg:34.11ms
step:437/2110 train_time:14905ms step_avg:34.11ms
step:438/2110 train_time:14937ms step_avg:34.10ms
step:439/2110 train_time:14970ms step_avg:34.10ms
step:440/2110 train_time:15004ms step_avg:34.10ms
step:441/2110 train_time:15037ms step_avg:34.10ms
step:442/2110 train_time:15070ms step_avg:34.09ms
step:443/2110 train_time:15103ms step_avg:34.09ms
step:444/2110 train_time:15136ms step_avg:34.09ms
step:445/2110 train_time:15169ms step_avg:34.09ms
step:446/2110 train_time:15201ms step_avg:34.08ms
step:447/2110 train_time:15235ms step_avg:34.08ms
step:448/2110 train_time:15267ms step_avg:34.08ms
step:449/2110 train_time:15301ms step_avg:34.08ms
step:450/2110 train_time:15334ms step_avg:34.07ms
step:451/2110 train_time:15366ms step_avg:34.07ms
step:452/2110 train_time:15399ms step_avg:34.07ms
step:453/2110 train_time:15432ms step_avg:34.07ms
step:454/2110 train_time:15465ms step_avg:34.06ms
step:455/2110 train_time:15498ms step_avg:34.06ms
step:456/2110 train_time:15530ms step_avg:34.06ms
step:457/2110 train_time:15564ms step_avg:34.06ms
step:458/2110 train_time:15597ms step_avg:34.05ms
step:459/2110 train_time:15630ms step_avg:34.05ms
step:460/2110 train_time:15663ms step_avg:34.05ms
step:461/2110 train_time:15695ms step_avg:34.05ms
step:462/2110 train_time:15728ms step_avg:34.04ms
step:463/2110 train_time:15761ms step_avg:34.04ms
step:464/2110 train_time:15794ms step_avg:34.04ms
step:465/2110 train_time:15827ms step_avg:34.04ms
step:466/2110 train_time:15859ms step_avg:34.03ms
step:467/2110 train_time:15892ms step_avg:34.03ms
step:468/2110 train_time:15925ms step_avg:34.03ms
step:469/2110 train_time:15958ms step_avg:34.03ms
step:470/2110 train_time:15991ms step_avg:34.02ms
step:471/2110 train_time:16024ms step_avg:34.02ms
step:472/2110 train_time:16057ms step_avg:34.02ms
step:473/2110 train_time:16090ms step_avg:34.02ms
step:474/2110 train_time:16124ms step_avg:34.02ms
step:475/2110 train_time:16156ms step_avg:34.01ms
step:476/2110 train_time:16189ms step_avg:34.01ms
step:477/2110 train_time:16222ms step_avg:34.01ms
step:478/2110 train_time:16254ms step_avg:34.00ms
step:479/2110 train_time:16287ms step_avg:34.00ms
step:480/2110 train_time:16320ms step_avg:34.00ms
step:481/2110 train_time:16353ms step_avg:34.00ms
step:482/2110 train_time:16386ms step_avg:34.00ms
step:483/2110 train_time:16420ms step_avg:34.00ms
step:484/2110 train_time:16452ms step_avg:33.99ms
step:485/2110 train_time:16486ms step_avg:33.99ms
step:486/2110 train_time:16518ms step_avg:33.99ms
step:487/2110 train_time:16552ms step_avg:33.99ms
step:488/2110 train_time:16584ms step_avg:33.98ms
step:489/2110 train_time:16617ms step_avg:33.98ms
step:490/2110 train_time:16651ms step_avg:33.98ms
step:491/2110 train_time:16683ms step_avg:33.98ms
step:492/2110 train_time:16716ms step_avg:33.98ms
step:493/2110 train_time:16749ms step_avg:33.97ms
step:494/2110 train_time:16782ms step_avg:33.97ms
step:495/2110 train_time:16815ms step_avg:33.97ms
step:496/2110 train_time:16847ms step_avg:33.97ms
step:497/2110 train_time:16880ms step_avg:33.96ms
step:498/2110 train_time:16913ms step_avg:33.96ms
step:499/2110 train_time:16946ms step_avg:33.96ms
step:500/2110 train_time:16978ms step_avg:33.96ms
step:500/2110 val_loss:4.0363 train_time:17014ms step_avg:34.03ms
step:501/2110 train_time:17041ms step_avg:34.01ms
step:502/2110 train_time:17070ms step_avg:34.00ms
step:503/2110 train_time:17097ms step_avg:33.99ms
step:504/2110 train_time:17125ms step_avg:33.98ms
step:505/2110 train_time:17152ms step_avg:33.96ms
step:506/2110 train_time:17185ms step_avg:33.96ms
step:507/2110 train_time:17218ms step_avg:33.96ms
step:508/2110 train_time:17251ms step_avg:33.96ms
step:509/2110 train_time:17284ms step_avg:33.96ms
step:510/2110 train_time:17317ms step_avg:33.95ms
step:511/2110 train_time:17350ms step_avg:33.95ms
step:512/2110 train_time:17383ms step_avg:33.95ms
step:513/2110 train_time:17415ms step_avg:33.95ms
step:514/2110 train_time:17448ms step_avg:33.95ms
step:515/2110 train_time:17481ms step_avg:33.94ms
step:516/2110 train_time:17513ms step_avg:33.94ms
step:517/2110 train_time:17546ms step_avg:33.94ms
step:518/2110 train_time:17578ms step_avg:33.93ms
step:519/2110 train_time:17611ms step_avg:33.93ms
step:520/2110 train_time:17643ms step_avg:33.93ms
step:521/2110 train_time:17676ms step_avg:33.93ms
step:522/2110 train_time:17709ms step_avg:33.92ms
step:523/2110 train_time:17741ms step_avg:33.92ms
step:524/2110 train_time:17774ms step_avg:33.92ms
step:525/2110 train_time:17806ms step_avg:33.92ms
step:526/2110 train_time:17840ms step_avg:33.92ms
step:527/2110 train_time:17872ms step_avg:33.91ms
step:528/2110 train_time:17904ms step_avg:33.91ms
step:529/2110 train_time:17937ms step_avg:33.91ms
step:530/2110 train_time:17970ms step_avg:33.91ms
step:531/2110 train_time:18003ms step_avg:33.90ms
step:532/2110 train_time:18037ms step_avg:33.90ms
step:533/2110 train_time:18071ms step_avg:33.90ms
step:534/2110 train_time:18104ms step_avg:33.90ms
step:535/2110 train_time:18137ms step_avg:33.90ms
step:536/2110 train_time:18170ms step_avg:33.90ms
step:537/2110 train_time:18203ms step_avg:33.90ms
step:538/2110 train_time:18237ms step_avg:33.90ms
step:539/2110 train_time:18270ms step_avg:33.90ms
step:540/2110 train_time:18303ms step_avg:33.89ms
step:541/2110 train_time:18336ms step_avg:33.89ms
step:542/2110 train_time:18369ms step_avg:33.89ms
step:543/2110 train_time:18402ms step_avg:33.89ms
step:544/2110 train_time:18435ms step_avg:33.89ms
step:545/2110 train_time:18468ms step_avg:33.89ms
step:546/2110 train_time:18500ms step_avg:33.88ms
step:547/2110 train_time:18533ms step_avg:33.88ms
step:548/2110 train_time:18566ms step_avg:33.88ms
step:549/2110 train_time:18599ms step_avg:33.88ms
step:550/2110 train_time:18631ms step_avg:33.87ms
step:551/2110 train_time:18664ms step_avg:33.87ms
step:552/2110 train_time:18697ms step_avg:33.87ms
step:553/2110 train_time:18730ms step_avg:33.87ms
step:554/2110 train_time:18762ms step_avg:33.87ms
step:555/2110 train_time:18795ms step_avg:33.86ms
step:556/2110 train_time:18830ms step_avg:33.87ms
step:557/2110 train_time:18860ms step_avg:33.86ms
step:558/2110 train_time:18893ms step_avg:33.86ms
step:559/2110 train_time:18926ms step_avg:33.86ms
step:560/2110 train_time:18958ms step_avg:33.85ms
step:561/2110 train_time:18992ms step_avg:33.85ms
step:562/2110 train_time:19024ms step_avg:33.85ms
step:563/2110 train_time:19058ms step_avg:33.85ms
step:564/2110 train_time:19091ms step_avg:33.85ms
step:565/2110 train_time:19124ms step_avg:33.85ms
step:566/2110 train_time:19157ms step_avg:33.85ms
step:567/2110 train_time:19190ms step_avg:33.84ms
step:568/2110 train_time:19223ms step_avg:33.84ms
step:569/2110 train_time:19257ms step_avg:33.84ms
step:570/2110 train_time:19289ms step_avg:33.84ms
step:571/2110 train_time:19323ms step_avg:33.84ms
step:572/2110 train_time:19355ms step_avg:33.84ms
step:573/2110 train_time:19388ms step_avg:33.84ms
step:574/2110 train_time:19421ms step_avg:33.83ms
step:575/2110 train_time:19454ms step_avg:33.83ms
step:576/2110 train_time:19487ms step_avg:33.83ms
step:577/2110 train_time:19520ms step_avg:33.83ms
step:578/2110 train_time:19553ms step_avg:33.83ms
step:579/2110 train_time:19586ms step_avg:33.83ms
step:580/2110 train_time:19619ms step_avg:33.83ms
step:581/2110 train_time:19651ms step_avg:33.82ms
step:582/2110 train_time:19684ms step_avg:33.82ms
step:583/2110 train_time:19717ms step_avg:33.82ms
step:584/2110 train_time:19750ms step_avg:33.82ms
step:585/2110 train_time:19782ms step_avg:33.82ms
step:586/2110 train_time:19815ms step_avg:33.81ms
step:587/2110 train_time:19849ms step_avg:33.81ms
step:588/2110 train_time:19882ms step_avg:33.81ms
step:589/2110 train_time:19915ms step_avg:33.81ms
step:590/2110 train_time:19947ms step_avg:33.81ms
step:591/2110 train_time:19981ms step_avg:33.81ms
step:592/2110 train_time:20013ms step_avg:33.81ms
step:593/2110 train_time:20046ms step_avg:33.81ms
step:594/2110 train_time:20079ms step_avg:33.80ms
step:595/2110 train_time:20113ms step_avg:33.80ms
step:596/2110 train_time:20145ms step_avg:33.80ms
step:597/2110 train_time:20179ms step_avg:33.80ms
step:598/2110 train_time:20211ms step_avg:33.80ms
step:599/2110 train_time:20245ms step_avg:33.80ms
step:600/2110 train_time:20278ms step_avg:33.80ms
step:601/2110 train_time:20311ms step_avg:33.80ms
step:602/2110 train_time:20344ms step_avg:33.79ms
step:603/2110 train_time:20377ms step_avg:33.79ms
step:604/2110 train_time:20410ms step_avg:33.79ms
step:605/2110 train_time:20443ms step_avg:33.79ms
step:606/2110 train_time:20475ms step_avg:33.79ms
step:607/2110 train_time:20509ms step_avg:33.79ms
step:608/2110 train_time:20541ms step_avg:33.78ms
step:609/2110 train_time:20575ms step_avg:33.78ms
step:610/2110 train_time:20607ms step_avg:33.78ms
step:611/2110 train_time:20640ms step_avg:33.78ms
step:612/2110 train_time:20673ms step_avg:33.78ms
step:613/2110 train_time:20706ms step_avg:33.78ms
step:614/2110 train_time:20739ms step_avg:33.78ms
step:615/2110 train_time:20772ms step_avg:33.78ms
step:616/2110 train_time:20804ms step_avg:33.77ms
step:617/2110 train_time:20838ms step_avg:33.77ms
step:618/2110 train_time:20870ms step_avg:33.77ms
step:619/2110 train_time:20903ms step_avg:33.77ms
step:620/2110 train_time:20936ms step_avg:33.77ms
step:621/2110 train_time:20969ms step_avg:33.77ms
step:622/2110 train_time:21002ms step_avg:33.76ms
step:623/2110 train_time:21035ms step_avg:33.76ms
step:624/2110 train_time:21068ms step_avg:33.76ms
step:625/2110 train_time:21101ms step_avg:33.76ms
step:626/2110 train_time:21135ms step_avg:33.76ms
step:627/2110 train_time:21167ms step_avg:33.76ms
step:628/2110 train_time:21200ms step_avg:33.76ms
step:629/2110 train_time:21233ms step_avg:33.76ms
step:630/2110 train_time:21266ms step_avg:33.75ms
step:631/2110 train_time:21299ms step_avg:33.75ms
step:632/2110 train_time:21332ms step_avg:33.75ms
step:633/2110 train_time:21365ms step_avg:33.75ms
step:634/2110 train_time:21398ms step_avg:33.75ms
step:635/2110 train_time:21431ms step_avg:33.75ms
step:636/2110 train_time:21464ms step_avg:33.75ms
step:637/2110 train_time:21497ms step_avg:33.75ms
step:638/2110 train_time:21530ms step_avg:33.75ms
step:639/2110 train_time:21563ms step_avg:33.74ms
step:640/2110 train_time:21596ms step_avg:33.74ms
step:641/2110 train_time:21629ms step_avg:33.74ms
step:642/2110 train_time:21662ms step_avg:33.74ms
step:643/2110 train_time:21695ms step_avg:33.74ms
step:644/2110 train_time:21727ms step_avg:33.74ms
step:645/2110 train_time:21761ms step_avg:33.74ms
step:646/2110 train_time:21794ms step_avg:33.74ms
step:647/2110 train_time:21827ms step_avg:33.74ms
step:648/2110 train_time:21859ms step_avg:33.73ms
step:649/2110 train_time:21893ms step_avg:33.73ms
step:650/2110 train_time:21925ms step_avg:33.73ms
step:651/2110 train_time:21959ms step_avg:33.73ms
step:652/2110 train_time:21992ms step_avg:33.73ms
step:653/2110 train_time:22024ms step_avg:33.73ms
step:654/2110 train_time:22057ms step_avg:33.73ms
step:655/2110 train_time:22090ms step_avg:33.73ms
step:656/2110 train_time:22123ms step_avg:33.72ms
step:657/2110 train_time:22156ms step_avg:33.72ms
step:658/2110 train_time:22189ms step_avg:33.72ms
step:659/2110 train_time:22223ms step_avg:33.72ms
step:660/2110 train_time:22255ms step_avg:33.72ms
step:661/2110 train_time:22289ms step_avg:33.72ms
step:662/2110 train_time:22322ms step_avg:33.72ms
step:663/2110 train_time:22355ms step_avg:33.72ms
step:664/2110 train_time:22387ms step_avg:33.72ms
step:665/2110 train_time:22420ms step_avg:33.71ms
step:666/2110 train_time:22453ms step_avg:33.71ms
step:667/2110 train_time:22486ms step_avg:33.71ms
step:668/2110 train_time:22519ms step_avg:33.71ms
step:669/2110 train_time:22552ms step_avg:33.71ms
step:670/2110 train_time:22585ms step_avg:33.71ms
step:671/2110 train_time:22618ms step_avg:33.71ms
step:672/2110 train_time:22650ms step_avg:33.71ms
step:673/2110 train_time:22683ms step_avg:33.70ms
step:674/2110 train_time:22716ms step_avg:33.70ms
step:675/2110 train_time:22749ms step_avg:33.70ms
step:676/2110 train_time:22782ms step_avg:33.70ms
step:677/2110 train_time:22815ms step_avg:33.70ms
step:678/2110 train_time:22848ms step_avg:33.70ms
step:679/2110 train_time:22881ms step_avg:33.70ms
step:680/2110 train_time:22914ms step_avg:33.70ms
step:681/2110 train_time:22947ms step_avg:33.70ms
step:682/2110 train_time:22980ms step_avg:33.69ms
step:683/2110 train_time:23014ms step_avg:33.69ms
step:684/2110 train_time:23046ms step_avg:33.69ms
step:685/2110 train_time:23080ms step_avg:33.69ms
step:686/2110 train_time:23112ms step_avg:33.69ms
step:687/2110 train_time:23146ms step_avg:33.69ms
step:688/2110 train_time:23178ms step_avg:33.69ms
step:689/2110 train_time:23212ms step_avg:33.69ms
step:690/2110 train_time:23244ms step_avg:33.69ms
step:691/2110 train_time:23278ms step_avg:33.69ms
step:692/2110 train_time:23336ms step_avg:33.72ms
step:693/2110 train_time:23396ms step_avg:33.76ms
step:694/2110 train_time:23454ms step_avg:33.80ms
step:695/2110 train_time:23514ms step_avg:33.83ms
step:696/2110 train_time:23572ms step_avg:33.87ms
step:697/2110 train_time:23631ms step_avg:33.90ms
step:698/2110 train_time:23688ms step_avg:33.94ms
step:699/2110 train_time:23748ms step_avg:33.97ms
step:700/2110 train_time:23806ms step_avg:34.01ms
step:701/2110 train_time:23866ms step_avg:34.05ms
step:702/2110 train_time:23924ms step_avg:34.08ms
step:703/2110 train_time:23985ms step_avg:34.12ms
step:704/2110 train_time:24044ms step_avg:34.15ms
step:705/2110 train_time:24104ms step_avg:34.19ms
step:706/2110 train_time:24162ms step_avg:34.22ms
step:707/2110 train_time:24222ms step_avg:34.26ms
step:708/2110 train_time:24281ms step_avg:34.29ms
step:709/2110 train_time:24340ms step_avg:34.33ms
step:710/2110 train_time:24399ms step_avg:34.36ms
step:711/2110 train_time:24458ms step_avg:34.40ms
step:712/2110 train_time:24517ms step_avg:34.43ms
step:713/2110 train_time:24578ms step_avg:34.47ms
step:714/2110 train_time:24637ms step_avg:34.51ms
step:715/2110 train_time:24697ms step_avg:34.54ms
step:716/2110 train_time:24755ms step_avg:34.57ms
step:717/2110 train_time:24815ms step_avg:34.61ms
step:718/2110 train_time:24873ms step_avg:34.64ms
step:719/2110 train_time:24933ms step_avg:34.68ms
step:720/2110 train_time:24991ms step_avg:34.71ms
step:721/2110 train_time:25050ms step_avg:34.74ms
step:722/2110 train_time:25108ms step_avg:34.78ms
step:723/2110 train_time:25169ms step_avg:34.81ms
step:724/2110 train_time:25227ms step_avg:34.84ms
step:725/2110 train_time:25287ms step_avg:34.88ms
step:726/2110 train_time:25346ms step_avg:34.91ms
step:727/2110 train_time:25406ms step_avg:34.95ms
step:728/2110 train_time:25464ms step_avg:34.98ms
step:729/2110 train_time:25524ms step_avg:35.01ms
step:730/2110 train_time:25584ms step_avg:35.05ms
step:731/2110 train_time:25644ms step_avg:35.08ms
step:732/2110 train_time:25703ms step_avg:35.11ms
step:733/2110 train_time:25764ms step_avg:35.15ms
step:734/2110 train_time:25822ms step_avg:35.18ms
step:735/2110 train_time:25882ms step_avg:35.21ms
step:736/2110 train_time:25941ms step_avg:35.25ms
step:737/2110 train_time:26002ms step_avg:35.28ms
step:738/2110 train_time:26060ms step_avg:35.31ms
step:739/2110 train_time:26121ms step_avg:35.35ms
step:740/2110 train_time:26179ms step_avg:35.38ms
step:741/2110 train_time:26239ms step_avg:35.41ms
step:742/2110 train_time:26298ms step_avg:35.44ms
step:743/2110 train_time:26357ms step_avg:35.47ms
step:744/2110 train_time:26416ms step_avg:35.51ms
step:745/2110 train_time:26475ms step_avg:35.54ms
step:746/2110 train_time:26533ms step_avg:35.57ms
step:747/2110 train_time:26593ms step_avg:35.60ms
step:748/2110 train_time:26651ms step_avg:35.63ms
step:749/2110 train_time:26711ms step_avg:35.66ms
step:750/2110 train_time:26769ms step_avg:35.69ms
step:750/2110 val_loss:3.9102 train_time:26830ms step_avg:35.77ms
step:751/2110 train_time:26869ms step_avg:35.78ms
step:752/2110 train_time:26908ms step_avg:35.78ms
step:753/2110 train_time:26953ms step_avg:35.79ms
step:754/2110 train_time:27016ms step_avg:35.83ms
step:755/2110 train_time:27078ms step_avg:35.87ms
step:756/2110 train_time:27139ms step_avg:35.90ms
step:757/2110 train_time:27198ms step_avg:35.93ms
step:758/2110 train_time:27256ms step_avg:35.96ms
step:759/2110 train_time:27316ms step_avg:35.99ms
step:760/2110 train_time:27374ms step_avg:36.02ms
step:761/2110 train_time:27432ms step_avg:36.05ms
step:762/2110 train_time:27490ms step_avg:36.08ms
step:763/2110 train_time:27549ms step_avg:36.11ms
step:764/2110 train_time:27606ms step_avg:36.13ms
step:765/2110 train_time:27665ms step_avg:36.16ms
step:766/2110 train_time:27723ms step_avg:36.19ms
step:767/2110 train_time:27783ms step_avg:36.22ms
step:768/2110 train_time:27842ms step_avg:36.25ms
step:769/2110 train_time:27904ms step_avg:36.29ms
step:770/2110 train_time:27963ms step_avg:36.32ms
step:771/2110 train_time:28024ms step_avg:36.35ms
step:772/2110 train_time:28082ms step_avg:36.38ms
step:773/2110 train_time:28142ms step_avg:36.41ms
step:774/2110 train_time:28201ms step_avg:36.43ms
step:775/2110 train_time:28260ms step_avg:36.46ms
step:776/2110 train_time:28317ms step_avg:36.49ms
step:777/2110 train_time:28377ms step_avg:36.52ms
step:778/2110 train_time:28436ms step_avg:36.55ms
step:779/2110 train_time:28495ms step_avg:36.58ms
step:780/2110 train_time:28553ms step_avg:36.61ms
step:781/2110 train_time:28613ms step_avg:36.64ms
step:782/2110 train_time:28670ms step_avg:36.66ms
step:783/2110 train_time:28730ms step_avg:36.69ms
step:784/2110 train_time:28789ms step_avg:36.72ms
step:785/2110 train_time:28849ms step_avg:36.75ms
step:786/2110 train_time:28907ms step_avg:36.78ms
step:787/2110 train_time:28969ms step_avg:36.81ms
step:788/2110 train_time:29028ms step_avg:36.84ms
step:789/2110 train_time:29089ms step_avg:36.87ms
step:790/2110 train_time:29148ms step_avg:36.90ms
step:791/2110 train_time:29208ms step_avg:36.93ms
step:792/2110 train_time:29265ms step_avg:36.95ms
step:793/2110 train_time:29324ms step_avg:36.98ms
step:794/2110 train_time:29382ms step_avg:37.00ms
step:795/2110 train_time:29442ms step_avg:37.03ms
step:796/2110 train_time:29499ms step_avg:37.06ms
step:797/2110 train_time:29558ms step_avg:37.09ms
step:798/2110 train_time:29616ms step_avg:37.11ms
step:799/2110 train_time:29676ms step_avg:37.14ms
step:800/2110 train_time:29735ms step_avg:37.17ms
step:801/2110 train_time:29794ms step_avg:37.20ms
step:802/2110 train_time:29853ms step_avg:37.22ms
step:803/2110 train_time:29914ms step_avg:37.25ms
step:804/2110 train_time:29973ms step_avg:37.28ms
step:805/2110 train_time:30034ms step_avg:37.31ms
step:806/2110 train_time:30093ms step_avg:37.34ms
step:807/2110 train_time:30154ms step_avg:37.37ms
step:808/2110 train_time:30213ms step_avg:37.39ms
step:809/2110 train_time:30272ms step_avg:37.42ms
step:810/2110 train_time:30331ms step_avg:37.45ms
step:811/2110 train_time:30391ms step_avg:37.47ms
step:812/2110 train_time:30450ms step_avg:37.50ms
step:813/2110 train_time:30509ms step_avg:37.53ms
step:814/2110 train_time:30567ms step_avg:37.55ms
step:815/2110 train_time:30625ms step_avg:37.58ms
step:816/2110 train_time:30684ms step_avg:37.60ms
step:817/2110 train_time:30742ms step_avg:37.63ms
step:818/2110 train_time:30800ms step_avg:37.65ms
step:819/2110 train_time:30859ms step_avg:37.68ms
step:820/2110 train_time:30918ms step_avg:37.71ms
step:821/2110 train_time:30979ms step_avg:37.73ms
step:822/2110 train_time:31038ms step_avg:37.76ms
step:823/2110 train_time:31099ms step_avg:37.79ms
step:824/2110 train_time:31157ms step_avg:37.81ms
step:825/2110 train_time:31218ms step_avg:37.84ms
step:826/2110 train_time:31277ms step_avg:37.87ms
step:827/2110 train_time:31337ms step_avg:37.89ms
step:828/2110 train_time:31395ms step_avg:37.92ms
step:829/2110 train_time:31455ms step_avg:37.94ms
step:830/2110 train_time:31514ms step_avg:37.97ms
step:831/2110 train_time:31574ms step_avg:37.99ms
step:832/2110 train_time:31632ms step_avg:38.02ms
step:833/2110 train_time:31692ms step_avg:38.05ms
step:834/2110 train_time:31751ms step_avg:38.07ms
step:835/2110 train_time:31811ms step_avg:38.10ms
step:836/2110 train_time:31870ms step_avg:38.12ms
step:837/2110 train_time:31931ms step_avg:38.15ms
step:838/2110 train_time:31989ms step_avg:38.17ms
step:839/2110 train_time:32049ms step_avg:38.20ms
step:840/2110 train_time:32108ms step_avg:38.22ms
step:841/2110 train_time:32168ms step_avg:38.25ms
step:842/2110 train_time:32226ms step_avg:38.27ms
step:843/2110 train_time:32286ms step_avg:38.30ms
step:844/2110 train_time:32344ms step_avg:38.32ms
step:845/2110 train_time:32403ms step_avg:38.35ms
step:846/2110 train_time:32461ms step_avg:38.37ms
step:847/2110 train_time:32521ms step_avg:38.40ms
step:848/2110 train_time:32579ms step_avg:38.42ms
step:849/2110 train_time:32638ms step_avg:38.44ms
step:850/2110 train_time:32697ms step_avg:38.47ms
step:851/2110 train_time:32757ms step_avg:38.49ms
step:852/2110 train_time:32816ms step_avg:38.52ms
step:853/2110 train_time:32876ms step_avg:38.54ms
step:854/2110 train_time:32934ms step_avg:38.56ms
step:855/2110 train_time:32995ms step_avg:38.59ms
step:856/2110 train_time:33054ms step_avg:38.61ms
step:857/2110 train_time:33115ms step_avg:38.64ms
step:858/2110 train_time:33174ms step_avg:38.66ms
step:859/2110 train_time:33236ms step_avg:38.69ms
step:860/2110 train_time:33296ms step_avg:38.72ms
step:861/2110 train_time:33355ms step_avg:38.74ms
step:862/2110 train_time:33413ms step_avg:38.76ms
step:863/2110 train_time:33473ms step_avg:38.79ms
step:864/2110 train_time:33532ms step_avg:38.81ms
step:865/2110 train_time:33591ms step_avg:38.83ms
step:866/2110 train_time:33649ms step_avg:38.86ms
step:867/2110 train_time:33708ms step_avg:38.88ms
step:868/2110 train_time:33766ms step_avg:38.90ms
step:869/2110 train_time:33826ms step_avg:38.93ms
step:870/2110 train_time:33884ms step_avg:38.95ms
step:871/2110 train_time:33943ms step_avg:38.97ms
step:872/2110 train_time:34002ms step_avg:38.99ms
step:873/2110 train_time:34062ms step_avg:39.02ms
step:874/2110 train_time:34120ms step_avg:39.04ms
step:875/2110 train_time:34180ms step_avg:39.06ms
step:876/2110 train_time:34239ms step_avg:39.09ms
step:877/2110 train_time:34299ms step_avg:39.11ms
step:878/2110 train_time:34358ms step_avg:39.13ms
step:879/2110 train_time:34417ms step_avg:39.16ms
step:880/2110 train_time:34476ms step_avg:39.18ms
step:881/2110 train_time:34536ms step_avg:39.20ms
step:882/2110 train_time:34595ms step_avg:39.22ms
step:883/2110 train_time:34655ms step_avg:39.25ms
step:884/2110 train_time:34713ms step_avg:39.27ms
step:885/2110 train_time:34773ms step_avg:39.29ms
step:886/2110 train_time:34833ms step_avg:39.31ms
step:887/2110 train_time:34893ms step_avg:39.34ms
step:888/2110 train_time:34951ms step_avg:39.36ms
step:889/2110 train_time:35011ms step_avg:39.38ms
step:890/2110 train_time:35070ms step_avg:39.40ms
step:891/2110 train_time:35131ms step_avg:39.43ms
step:892/2110 train_time:35190ms step_avg:39.45ms
step:893/2110 train_time:35250ms step_avg:39.47ms
step:894/2110 train_time:35308ms step_avg:39.49ms
step:895/2110 train_time:35368ms step_avg:39.52ms
step:896/2110 train_time:35427ms step_avg:39.54ms
step:897/2110 train_time:35486ms step_avg:39.56ms
step:898/2110 train_time:35544ms step_avg:39.58ms
step:899/2110 train_time:35602ms step_avg:39.60ms
step:900/2110 train_time:35660ms step_avg:39.62ms
step:901/2110 train_time:35720ms step_avg:39.64ms
step:902/2110 train_time:35778ms step_avg:39.67ms
step:903/2110 train_time:35838ms step_avg:39.69ms
step:904/2110 train_time:35897ms step_avg:39.71ms
step:905/2110 train_time:35958ms step_avg:39.73ms
step:906/2110 train_time:36017ms step_avg:39.75ms
step:907/2110 train_time:36078ms step_avg:39.78ms
step:908/2110 train_time:36137ms step_avg:39.80ms
step:909/2110 train_time:36197ms step_avg:39.82ms
step:910/2110 train_time:36256ms step_avg:39.84ms
step:911/2110 train_time:36316ms step_avg:39.86ms
step:912/2110 train_time:36375ms step_avg:39.88ms
step:913/2110 train_time:36435ms step_avg:39.91ms
step:914/2110 train_time:36494ms step_avg:39.93ms
step:915/2110 train_time:36554ms step_avg:39.95ms
step:916/2110 train_time:36612ms step_avg:39.97ms
step:917/2110 train_time:36673ms step_avg:39.99ms
step:918/2110 train_time:36732ms step_avg:40.01ms
step:919/2110 train_time:36792ms step_avg:40.03ms
step:920/2110 train_time:36851ms step_avg:40.06ms
step:921/2110 train_time:36911ms step_avg:40.08ms
step:922/2110 train_time:36969ms step_avg:40.10ms
step:923/2110 train_time:37029ms step_avg:40.12ms
step:924/2110 train_time:37087ms step_avg:40.14ms
step:925/2110 train_time:37148ms step_avg:40.16ms
step:926/2110 train_time:37205ms step_avg:40.18ms
step:927/2110 train_time:37265ms step_avg:40.20ms
step:928/2110 train_time:37323ms step_avg:40.22ms
step:929/2110 train_time:37382ms step_avg:40.24ms
step:930/2110 train_time:37440ms step_avg:40.26ms
step:931/2110 train_time:37500ms step_avg:40.28ms
step:932/2110 train_time:37558ms step_avg:40.30ms
step:933/2110 train_time:37619ms step_avg:40.32ms
step:934/2110 train_time:37677ms step_avg:40.34ms
step:935/2110 train_time:37737ms step_avg:40.36ms
step:936/2110 train_time:37796ms step_avg:40.38ms
step:937/2110 train_time:37856ms step_avg:40.40ms
step:938/2110 train_time:37916ms step_avg:40.42ms
step:939/2110 train_time:37976ms step_avg:40.44ms
step:940/2110 train_time:38035ms step_avg:40.46ms
step:941/2110 train_time:38095ms step_avg:40.48ms
step:942/2110 train_time:38154ms step_avg:40.50ms
step:943/2110 train_time:38214ms step_avg:40.52ms
step:944/2110 train_time:38273ms step_avg:40.54ms
step:945/2110 train_time:38333ms step_avg:40.56ms
step:946/2110 train_time:38392ms step_avg:40.58ms
step:947/2110 train_time:38452ms step_avg:40.60ms
step:948/2110 train_time:38510ms step_avg:40.62ms
step:949/2110 train_time:38570ms step_avg:40.64ms
step:950/2110 train_time:38628ms step_avg:40.66ms
step:951/2110 train_time:38688ms step_avg:40.68ms
step:952/2110 train_time:38746ms step_avg:40.70ms
step:953/2110 train_time:38805ms step_avg:40.72ms
step:954/2110 train_time:38863ms step_avg:40.74ms
step:955/2110 train_time:38923ms step_avg:40.76ms
step:956/2110 train_time:38980ms step_avg:40.77ms
step:957/2110 train_time:39041ms step_avg:40.79ms
step:958/2110 train_time:39100ms step_avg:40.81ms
step:959/2110 train_time:39160ms step_avg:40.83ms
step:960/2110 train_time:39218ms step_avg:40.85ms
step:961/2110 train_time:39278ms step_avg:40.87ms
step:962/2110 train_time:39337ms step_avg:40.89ms
step:963/2110 train_time:39397ms step_avg:40.91ms
step:964/2110 train_time:39455ms step_avg:40.93ms
step:965/2110 train_time:39515ms step_avg:40.95ms
step:966/2110 train_time:39573ms step_avg:40.97ms
step:967/2110 train_time:39634ms step_avg:40.99ms
step:968/2110 train_time:39693ms step_avg:41.01ms
step:969/2110 train_time:39753ms step_avg:41.03ms
step:970/2110 train_time:39812ms step_avg:41.04ms
step:971/2110 train_time:39872ms step_avg:41.06ms
step:972/2110 train_time:39930ms step_avg:41.08ms
step:973/2110 train_time:39991ms step_avg:41.10ms
step:974/2110 train_time:40050ms step_avg:41.12ms
step:975/2110 train_time:40109ms step_avg:41.14ms
step:976/2110 train_time:40168ms step_avg:41.16ms
step:977/2110 train_time:40227ms step_avg:41.17ms
step:978/2110 train_time:40285ms step_avg:41.19ms
step:979/2110 train_time:40344ms step_avg:41.21ms
step:980/2110 train_time:40402ms step_avg:41.23ms
step:981/2110 train_time:40462ms step_avg:41.25ms
step:982/2110 train_time:40520ms step_avg:41.26ms
step:983/2110 train_time:40580ms step_avg:41.28ms
step:984/2110 train_time:40639ms step_avg:41.30ms
step:985/2110 train_time:40699ms step_avg:41.32ms
step:986/2110 train_time:40758ms step_avg:41.34ms
step:987/2110 train_time:40818ms step_avg:41.36ms
step:988/2110 train_time:40877ms step_avg:41.37ms
step:989/2110 train_time:40937ms step_avg:41.39ms
step:990/2110 train_time:40995ms step_avg:41.41ms
step:991/2110 train_time:41055ms step_avg:41.43ms
step:992/2110 train_time:41113ms step_avg:41.44ms
step:993/2110 train_time:41174ms step_avg:41.46ms
step:994/2110 train_time:41233ms step_avg:41.48ms
step:995/2110 train_time:41293ms step_avg:41.50ms
step:996/2110 train_time:41351ms step_avg:41.52ms
step:997/2110 train_time:41412ms step_avg:41.54ms
step:998/2110 train_time:41470ms step_avg:41.55ms
step:999/2110 train_time:41530ms step_avg:41.57ms
step:1000/2110 train_time:41589ms step_avg:41.59ms
step:1000/2110 val_loss:3.7577 train_time:41650ms step_avg:41.65ms
step:1001/2110 train_time:41682ms step_avg:41.64ms
step:1002/2110 train_time:41717ms step_avg:41.63ms
step:1003/2110 train_time:41773ms step_avg:41.65ms
step:1004/2110 train_time:41835ms step_avg:41.67ms
step:1005/2110 train_time:41897ms step_avg:41.69ms
step:1006/2110 train_time:41957ms step_avg:41.71ms
step:1007/2110 train_time:42017ms step_avg:41.73ms
step:1008/2110 train_time:42076ms step_avg:41.74ms
step:1009/2110 train_time:42135ms step_avg:41.76ms
step:1010/2110 train_time:42193ms step_avg:41.78ms
step:1011/2110 train_time:42252ms step_avg:41.79ms
step:1012/2110 train_time:42310ms step_avg:41.81ms
step:1013/2110 train_time:42369ms step_avg:41.83ms
step:1014/2110 train_time:42427ms step_avg:41.84ms
step:1015/2110 train_time:42487ms step_avg:41.86ms
step:1016/2110 train_time:42545ms step_avg:41.87ms
step:1017/2110 train_time:42605ms step_avg:41.89ms
step:1018/2110 train_time:42663ms step_avg:41.91ms
step:1019/2110 train_time:42725ms step_avg:41.93ms
step:1020/2110 train_time:42784ms step_avg:41.94ms
step:1021/2110 train_time:42844ms step_avg:41.96ms
step:1022/2110 train_time:42903ms step_avg:41.98ms
step:1023/2110 train_time:42963ms step_avg:42.00ms
step:1024/2110 train_time:43020ms step_avg:42.01ms
step:1025/2110 train_time:43080ms step_avg:42.03ms
step:1026/2110 train_time:43137ms step_avg:42.04ms
step:1027/2110 train_time:43197ms step_avg:42.06ms
step:1028/2110 train_time:43255ms step_avg:42.08ms
step:1029/2110 train_time:43314ms step_avg:42.09ms
step:1030/2110 train_time:43372ms step_avg:42.11ms
step:1031/2110 train_time:43432ms step_avg:42.13ms
step:1032/2110 train_time:43490ms step_avg:42.14ms
step:1033/2110 train_time:43550ms step_avg:42.16ms
step:1034/2110 train_time:43610ms step_avg:42.18ms
step:1035/2110 train_time:43671ms step_avg:42.19ms
step:1036/2110 train_time:43730ms step_avg:42.21ms
step:1037/2110 train_time:43791ms step_avg:42.23ms
step:1038/2110 train_time:43849ms step_avg:42.24ms
step:1039/2110 train_time:43910ms step_avg:42.26ms
step:1040/2110 train_time:43968ms step_avg:42.28ms
step:1041/2110 train_time:44028ms step_avg:42.29ms
step:1042/2110 train_time:44086ms step_avg:42.31ms
step:1043/2110 train_time:44146ms step_avg:42.33ms
step:1044/2110 train_time:44204ms step_avg:42.34ms
step:1045/2110 train_time:44263ms step_avg:42.36ms
step:1046/2110 train_time:44320ms step_avg:42.37ms
step:1047/2110 train_time:44380ms step_avg:42.39ms
step:1048/2110 train_time:44437ms step_avg:42.40ms
step:1049/2110 train_time:44497ms step_avg:42.42ms
step:1050/2110 train_time:44555ms step_avg:42.43ms
step:1051/2110 train_time:44615ms step_avg:42.45ms
step:1052/2110 train_time:44674ms step_avg:42.47ms
step:1053/2110 train_time:44735ms step_avg:42.48ms
step:1054/2110 train_time:44794ms step_avg:42.50ms
step:1055/2110 train_time:44854ms step_avg:42.52ms
step:1056/2110 train_time:44913ms step_avg:42.53ms
step:1057/2110 train_time:44973ms step_avg:42.55ms
step:1058/2110 train_time:45032ms step_avg:42.56ms
step:1059/2110 train_time:45092ms step_avg:42.58ms
step:1060/2110 train_time:45151ms step_avg:42.60ms
step:1061/2110 train_time:45211ms step_avg:42.61ms
step:1062/2110 train_time:45269ms step_avg:42.63ms
step:1063/2110 train_time:45329ms step_avg:42.64ms
step:1064/2110 train_time:45387ms step_avg:42.66ms
step:1065/2110 train_time:45447ms step_avg:42.67ms
step:1066/2110 train_time:45504ms step_avg:42.69ms
step:1067/2110 train_time:45564ms step_avg:42.70ms
step:1068/2110 train_time:45622ms step_avg:42.72ms
step:1069/2110 train_time:45682ms step_avg:42.73ms
step:1070/2110 train_time:45739ms step_avg:42.75ms
step:1071/2110 train_time:45799ms step_avg:42.76ms
step:1072/2110 train_time:45858ms step_avg:42.78ms
step:1073/2110 train_time:45919ms step_avg:42.80ms
step:1074/2110 train_time:45977ms step_avg:42.81ms
step:1075/2110 train_time:46037ms step_avg:42.83ms
step:1076/2110 train_time:46096ms step_avg:42.84ms
step:1077/2110 train_time:46156ms step_avg:42.86ms
step:1078/2110 train_time:46215ms step_avg:42.87ms
step:1079/2110 train_time:46274ms step_avg:42.89ms
step:1080/2110 train_time:46332ms step_avg:42.90ms
step:1081/2110 train_time:46392ms step_avg:42.92ms
step:1082/2110 train_time:46451ms step_avg:42.93ms
step:1083/2110 train_time:46511ms step_avg:42.95ms
step:1084/2110 train_time:46569ms step_avg:42.96ms
step:1085/2110 train_time:46629ms step_avg:42.98ms
step:1086/2110 train_time:46688ms step_avg:42.99ms
step:1087/2110 train_time:46748ms step_avg:43.01ms
step:1088/2110 train_time:46807ms step_avg:43.02ms
step:1089/2110 train_time:46868ms step_avg:43.04ms
step:1090/2110 train_time:46926ms step_avg:43.05ms
step:1091/2110 train_time:46986ms step_avg:43.07ms
step:1092/2110 train_time:47044ms step_avg:43.08ms
step:1093/2110 train_time:47104ms step_avg:43.10ms
step:1094/2110 train_time:47161ms step_avg:43.11ms
step:1095/2110 train_time:47221ms step_avg:43.12ms
step:1096/2110 train_time:47279ms step_avg:43.14ms
step:1097/2110 train_time:47339ms step_avg:43.15ms
step:1098/2110 train_time:47398ms step_avg:43.17ms
step:1099/2110 train_time:47458ms step_avg:43.18ms
step:1100/2110 train_time:47517ms step_avg:43.20ms
step:1101/2110 train_time:47577ms step_avg:43.21ms
step:1102/2110 train_time:47636ms step_avg:43.23ms
step:1103/2110 train_time:47697ms step_avg:43.24ms
step:1104/2110 train_time:47756ms step_avg:43.26ms
step:1105/2110 train_time:47817ms step_avg:43.27ms
step:1106/2110 train_time:47875ms step_avg:43.29ms
step:1107/2110 train_time:47935ms step_avg:43.30ms
step:1108/2110 train_time:47994ms step_avg:43.32ms
step:1109/2110 train_time:48054ms step_avg:43.33ms
step:1110/2110 train_time:48113ms step_avg:43.34ms
step:1111/2110 train_time:48173ms step_avg:43.36ms
step:1112/2110 train_time:48232ms step_avg:43.37ms
step:1113/2110 train_time:48291ms step_avg:43.39ms
step:1114/2110 train_time:48350ms step_avg:43.40ms
step:1115/2110 train_time:48410ms step_avg:43.42ms
step:1116/2110 train_time:48469ms step_avg:43.43ms
step:1117/2110 train_time:48529ms step_avg:43.45ms
step:1118/2110 train_time:48587ms step_avg:43.46ms
step:1119/2110 train_time:48647ms step_avg:43.47ms
step:1120/2110 train_time:48705ms step_avg:43.49ms
step:1121/2110 train_time:48765ms step_avg:43.50ms
step:1122/2110 train_time:48823ms step_avg:43.51ms
step:1123/2110 train_time:48883ms step_avg:43.53ms
step:1124/2110 train_time:48940ms step_avg:43.54ms
step:1125/2110 train_time:49000ms step_avg:43.56ms
step:1126/2110 train_time:49058ms step_avg:43.57ms
step:1127/2110 train_time:49118ms step_avg:43.58ms
step:1128/2110 train_time:49176ms step_avg:43.60ms
step:1129/2110 train_time:49236ms step_avg:43.61ms
step:1130/2110 train_time:49295ms step_avg:43.62ms
step:1131/2110 train_time:49355ms step_avg:43.64ms
step:1132/2110 train_time:49414ms step_avg:43.65ms
step:1133/2110 train_time:49474ms step_avg:43.67ms
step:1134/2110 train_time:49532ms step_avg:43.68ms
step:1135/2110 train_time:49593ms step_avg:43.69ms
step:1136/2110 train_time:49651ms step_avg:43.71ms
step:1137/2110 train_time:49712ms step_avg:43.72ms
step:1138/2110 train_time:49771ms step_avg:43.74ms
step:1139/2110 train_time:49831ms step_avg:43.75ms
step:1140/2110 train_time:49890ms step_avg:43.76ms
step:1141/2110 train_time:49951ms step_avg:43.78ms
step:1142/2110 train_time:50010ms step_avg:43.79ms
step:1143/2110 train_time:50070ms step_avg:43.81ms
step:1144/2110 train_time:50128ms step_avg:43.82ms
step:1145/2110 train_time:50189ms step_avg:43.83ms
step:1146/2110 train_time:50248ms step_avg:43.85ms
step:1147/2110 train_time:50309ms step_avg:43.86ms
step:1148/2110 train_time:50368ms step_avg:43.87ms
step:1149/2110 train_time:50429ms step_avg:43.89ms
step:1150/2110 train_time:50487ms step_avg:43.90ms
step:1151/2110 train_time:50548ms step_avg:43.92ms
step:1152/2110 train_time:50607ms step_avg:43.93ms
step:1153/2110 train_time:50668ms step_avg:43.94ms
step:1154/2110 train_time:50727ms step_avg:43.96ms
step:1155/2110 train_time:50788ms step_avg:43.97ms
step:1156/2110 train_time:50847ms step_avg:43.98ms
step:1157/2110 train_time:50907ms step_avg:44.00ms
step:1158/2110 train_time:50966ms step_avg:44.01ms
step:1159/2110 train_time:51027ms step_avg:44.03ms
step:1160/2110 train_time:51085ms step_avg:44.04ms
step:1161/2110 train_time:51145ms step_avg:44.05ms
step:1162/2110 train_time:51204ms step_avg:44.07ms
step:1163/2110 train_time:51264ms step_avg:44.08ms
step:1164/2110 train_time:51322ms step_avg:44.09ms
step:1165/2110 train_time:51382ms step_avg:44.10ms
step:1166/2110 train_time:51440ms step_avg:44.12ms
step:1167/2110 train_time:51502ms step_avg:44.13ms
step:1168/2110 train_time:51560ms step_avg:44.14ms
step:1169/2110 train_time:51620ms step_avg:44.16ms
step:1170/2110 train_time:51679ms step_avg:44.17ms
step:1171/2110 train_time:51741ms step_avg:44.19ms
step:1172/2110 train_time:51800ms step_avg:44.20ms
step:1173/2110 train_time:51860ms step_avg:44.21ms
step:1174/2110 train_time:51918ms step_avg:44.22ms
step:1175/2110 train_time:51979ms step_avg:44.24ms
step:1176/2110 train_time:52038ms step_avg:44.25ms
step:1177/2110 train_time:52099ms step_avg:44.26ms
step:1178/2110 train_time:52158ms step_avg:44.28ms
step:1179/2110 train_time:52219ms step_avg:44.29ms
step:1180/2110 train_time:52278ms step_avg:44.30ms
step:1181/2110 train_time:52338ms step_avg:44.32ms
step:1182/2110 train_time:52397ms step_avg:44.33ms
step:1183/2110 train_time:52458ms step_avg:44.34ms
step:1184/2110 train_time:52517ms step_avg:44.36ms
step:1185/2110 train_time:52578ms step_avg:44.37ms
step:1186/2110 train_time:52637ms step_avg:44.38ms
step:1187/2110 train_time:52698ms step_avg:44.40ms
step:1188/2110 train_time:52757ms step_avg:44.41ms
step:1189/2110 train_time:52818ms step_avg:44.42ms
step:1190/2110 train_time:52877ms step_avg:44.43ms
step:1191/2110 train_time:52937ms step_avg:44.45ms
step:1192/2110 train_time:52997ms step_avg:44.46ms
step:1193/2110 train_time:53058ms step_avg:44.47ms
step:1194/2110 train_time:53117ms step_avg:44.49ms
step:1195/2110 train_time:53178ms step_avg:44.50ms
step:1196/2110 train_time:53237ms step_avg:44.51ms
step:1197/2110 train_time:53297ms step_avg:44.53ms
step:1198/2110 train_time:53357ms step_avg:44.54ms
step:1199/2110 train_time:53417ms step_avg:44.55ms
step:1200/2110 train_time:53477ms step_avg:44.56ms
step:1201/2110 train_time:53537ms step_avg:44.58ms
step:1202/2110 train_time:53598ms step_avg:44.59ms
step:1203/2110 train_time:53658ms step_avg:44.60ms
step:1204/2110 train_time:53718ms step_avg:44.62ms
step:1205/2110 train_time:53778ms step_avg:44.63ms
step:1206/2110 train_time:53838ms step_avg:44.64ms
step:1207/2110 train_time:53897ms step_avg:44.65ms
step:1208/2110 train_time:53957ms step_avg:44.67ms
step:1209/2110 train_time:54018ms step_avg:44.68ms
step:1210/2110 train_time:54077ms step_avg:44.69ms
step:1211/2110 train_time:54137ms step_avg:44.70ms
step:1212/2110 train_time:54197ms step_avg:44.72ms
step:1213/2110 train_time:54257ms step_avg:44.73ms
step:1214/2110 train_time:54316ms step_avg:44.74ms
step:1215/2110 train_time:54377ms step_avg:44.75ms
step:1216/2110 train_time:54436ms step_avg:44.77ms
step:1217/2110 train_time:54496ms step_avg:44.78ms
step:1218/2110 train_time:54555ms step_avg:44.79ms
step:1219/2110 train_time:54615ms step_avg:44.80ms
step:1220/2110 train_time:54675ms step_avg:44.82ms
step:1221/2110 train_time:54735ms step_avg:44.83ms
step:1222/2110 train_time:54796ms step_avg:44.84ms
step:1223/2110 train_time:54856ms step_avg:44.85ms
step:1224/2110 train_time:54915ms step_avg:44.87ms
step:1225/2110 train_time:54976ms step_avg:44.88ms
step:1226/2110 train_time:55036ms step_avg:44.89ms
step:1227/2110 train_time:55096ms step_avg:44.90ms
step:1228/2110 train_time:55156ms step_avg:44.92ms
step:1229/2110 train_time:55216ms step_avg:44.93ms
step:1230/2110 train_time:55276ms step_avg:44.94ms
step:1231/2110 train_time:55336ms step_avg:44.95ms
step:1232/2110 train_time:55395ms step_avg:44.96ms
step:1233/2110 train_time:55456ms step_avg:44.98ms
step:1234/2110 train_time:55516ms step_avg:44.99ms
step:1235/2110 train_time:55575ms step_avg:45.00ms
step:1236/2110 train_time:55635ms step_avg:45.01ms
step:1237/2110 train_time:55696ms step_avg:45.02ms
step:1238/2110 train_time:55756ms step_avg:45.04ms
step:1239/2110 train_time:55816ms step_avg:45.05ms
step:1240/2110 train_time:55875ms step_avg:45.06ms
step:1241/2110 train_time:55934ms step_avg:45.07ms
step:1242/2110 train_time:55995ms step_avg:45.08ms
step:1243/2110 train_time:56055ms step_avg:45.10ms
step:1244/2110 train_time:56115ms step_avg:45.11ms
step:1245/2110 train_time:56175ms step_avg:45.12ms
step:1246/2110 train_time:56234ms step_avg:45.13ms
step:1247/2110 train_time:56295ms step_avg:45.14ms
step:1248/2110 train_time:56356ms step_avg:45.16ms
step:1249/2110 train_time:56414ms step_avg:45.17ms
step:1250/2110 train_time:56474ms step_avg:45.18ms
step:1250/2110 val_loss:3.5961 train_time:56535ms step_avg:45.23ms
step:1251/2110 train_time:56575ms step_avg:45.22ms
step:1252/2110 train_time:56616ms step_avg:45.22ms
step:1253/2110 train_time:56662ms step_avg:45.22ms
step:1254/2110 train_time:56723ms step_avg:45.23ms
step:1255/2110 train_time:56784ms step_avg:45.25ms
step:1256/2110 train_time:56845ms step_avg:45.26ms
step:1257/2110 train_time:56902ms step_avg:45.27ms
step:1258/2110 train_time:56960ms step_avg:45.28ms
step:1259/2110 train_time:57020ms step_avg:45.29ms
step:1260/2110 train_time:57078ms step_avg:45.30ms
step:1261/2110 train_time:57137ms step_avg:45.31ms
step:1262/2110 train_time:57195ms step_avg:45.32ms
step:1263/2110 train_time:57254ms step_avg:45.33ms
step:1264/2110 train_time:57313ms step_avg:45.34ms
step:1265/2110 train_time:57371ms step_avg:45.35ms
step:1266/2110 train_time:57430ms step_avg:45.36ms
step:1267/2110 train_time:57494ms step_avg:45.38ms
step:1268/2110 train_time:57555ms step_avg:45.39ms
step:1269/2110 train_time:57617ms step_avg:45.40ms
step:1270/2110 train_time:57677ms step_avg:45.41ms
step:1271/2110 train_time:57738ms step_avg:45.43ms
step:1272/2110 train_time:57798ms step_avg:45.44ms
step:1273/2110 train_time:57858ms step_avg:45.45ms
step:1274/2110 train_time:57917ms step_avg:45.46ms
step:1275/2110 train_time:57976ms step_avg:45.47ms
step:1276/2110 train_time:58035ms step_avg:45.48ms
step:1277/2110 train_time:58094ms step_avg:45.49ms
step:1278/2110 train_time:58153ms step_avg:45.50ms
step:1279/2110 train_time:58212ms step_avg:45.51ms
step:1280/2110 train_time:58273ms step_avg:45.53ms
step:1281/2110 train_time:58330ms step_avg:45.53ms
step:1282/2110 train_time:58389ms step_avg:45.55ms
step:1283/2110 train_time:58449ms step_avg:45.56ms
step:1284/2110 train_time:58510ms step_avg:45.57ms
step:1285/2110 train_time:58571ms step_avg:45.58ms
step:1286/2110 train_time:58633ms step_avg:45.59ms
step:1287/2110 train_time:58694ms step_avg:45.61ms
step:1288/2110 train_time:58754ms step_avg:45.62ms
step:1289/2110 train_time:58815ms step_avg:45.63ms
step:1290/2110 train_time:58876ms step_avg:45.64ms
step:1291/2110 train_time:58934ms step_avg:45.65ms
step:1292/2110 train_time:58994ms step_avg:45.66ms
step:1293/2110 train_time:59052ms step_avg:45.67ms
step:1294/2110 train_time:59111ms step_avg:45.68ms
step:1295/2110 train_time:59170ms step_avg:45.69ms
step:1296/2110 train_time:59230ms step_avg:45.70ms
step:1297/2110 train_time:59288ms step_avg:45.71ms
step:1298/2110 train_time:59348ms step_avg:45.72ms
step:1299/2110 train_time:59408ms step_avg:45.73ms
step:1300/2110 train_time:59468ms step_avg:45.74ms
step:1301/2110 train_time:59529ms step_avg:45.76ms
step:1302/2110 train_time:59590ms step_avg:45.77ms
step:1303/2110 train_time:59650ms step_avg:45.78ms
step:1304/2110 train_time:59710ms step_avg:45.79ms
step:1305/2110 train_time:59772ms step_avg:45.80ms
step:1306/2110 train_time:59832ms step_avg:45.81ms
step:1307/2110 train_time:59892ms step_avg:45.82ms
step:1308/2110 train_time:59952ms step_avg:45.83ms
step:1309/2110 train_time:60011ms step_avg:45.85ms
step:1310/2110 train_time:60070ms step_avg:45.86ms
step:1311/2110 train_time:60129ms step_avg:45.87ms
step:1312/2110 train_time:60189ms step_avg:45.88ms
step:1313/2110 train_time:60248ms step_avg:45.89ms
step:1314/2110 train_time:60308ms step_avg:45.90ms
step:1315/2110 train_time:60367ms step_avg:45.91ms
step:1316/2110 train_time:60427ms step_avg:45.92ms
step:1317/2110 train_time:60488ms step_avg:45.93ms
step:1318/2110 train_time:60548ms step_avg:45.94ms
step:1319/2110 train_time:60609ms step_avg:45.95ms
step:1320/2110 train_time:60669ms step_avg:45.96ms
step:1321/2110 train_time:60731ms step_avg:45.97ms
step:1322/2110 train_time:60791ms step_avg:45.98ms
step:1323/2110 train_time:60851ms step_avg:45.99ms
step:1324/2110 train_time:60911ms step_avg:46.01ms
step:1325/2110 train_time:60970ms step_avg:46.02ms
step:1326/2110 train_time:61030ms step_avg:46.03ms
step:1327/2110 train_time:61090ms step_avg:46.04ms
step:1328/2110 train_time:61149ms step_avg:46.05ms
step:1329/2110 train_time:61209ms step_avg:46.06ms
step:1330/2110 train_time:61269ms step_avg:46.07ms
step:1331/2110 train_time:61328ms step_avg:46.08ms
step:1332/2110 train_time:61387ms step_avg:46.09ms
step:1333/2110 train_time:61447ms step_avg:46.10ms
step:1334/2110 train_time:61507ms step_avg:46.11ms
step:1335/2110 train_time:61568ms step_avg:46.12ms
step:1336/2110 train_time:61627ms step_avg:46.13ms
step:1337/2110 train_time:61688ms step_avg:46.14ms
step:1338/2110 train_time:61749ms step_avg:46.15ms
step:1339/2110 train_time:61809ms step_avg:46.16ms
step:1340/2110 train_time:61869ms step_avg:46.17ms
step:1341/2110 train_time:61930ms step_avg:46.18ms
step:1342/2110 train_time:61990ms step_avg:46.19ms
step:1343/2110 train_time:62050ms step_avg:46.20ms
step:1344/2110 train_time:62110ms step_avg:46.21ms
step:1345/2110 train_time:62169ms step_avg:46.22ms
step:1346/2110 train_time:62228ms step_avg:46.23ms
step:1347/2110 train_time:62288ms step_avg:46.24ms
step:1348/2110 train_time:62348ms step_avg:46.25ms
step:1349/2110 train_time:62408ms step_avg:46.26ms
step:1350/2110 train_time:62467ms step_avg:46.27ms
step:1351/2110 train_time:62529ms step_avg:46.28ms
step:1352/2110 train_time:62588ms step_avg:46.29ms
step:1353/2110 train_time:62649ms step_avg:46.30ms
step:1354/2110 train_time:62708ms step_avg:46.31ms
step:1355/2110 train_time:62769ms step_avg:46.32ms
step:1356/2110 train_time:62830ms step_avg:46.33ms
step:1357/2110 train_time:62890ms step_avg:46.35ms
step:1358/2110 train_time:62950ms step_avg:46.35ms
step:1359/2110 train_time:63011ms step_avg:46.37ms
step:1360/2110 train_time:63070ms step_avg:46.37ms
step:1361/2110 train_time:63130ms step_avg:46.38ms
step:1362/2110 train_time:63188ms step_avg:46.39ms
step:1363/2110 train_time:63248ms step_avg:46.40ms
step:1364/2110 train_time:63308ms step_avg:46.41ms
step:1365/2110 train_time:63368ms step_avg:46.42ms
step:1366/2110 train_time:63427ms step_avg:46.43ms
step:1367/2110 train_time:63488ms step_avg:46.44ms
step:1368/2110 train_time:63548ms step_avg:46.45ms
step:1369/2110 train_time:63608ms step_avg:46.46ms
step:1370/2110 train_time:63668ms step_avg:46.47ms
step:1371/2110 train_time:63728ms step_avg:46.48ms
step:1372/2110 train_time:63788ms step_avg:46.49ms
step:1373/2110 train_time:63850ms step_avg:46.50ms
step:1374/2110 train_time:63909ms step_avg:46.51ms
step:1375/2110 train_time:63970ms step_avg:46.52ms
step:1376/2110 train_time:64030ms step_avg:46.53ms
step:1377/2110 train_time:64090ms step_avg:46.54ms
step:1378/2110 train_time:64149ms step_avg:46.55ms
step:1379/2110 train_time:64209ms step_avg:46.56ms
step:1380/2110 train_time:64271ms step_avg:46.57ms
step:1381/2110 train_time:64329ms step_avg:46.58ms
step:1382/2110 train_time:64415ms step_avg:46.61ms
step:1383/2110 train_time:64503ms step_avg:46.64ms
step:1384/2110 train_time:64590ms step_avg:46.67ms
step:1385/2110 train_time:64676ms step_avg:46.70ms
step:1386/2110 train_time:64764ms step_avg:46.73ms
step:1387/2110 train_time:64850ms step_avg:46.76ms
step:1388/2110 train_time:64937ms step_avg:46.78ms
step:1389/2110 train_time:65023ms step_avg:46.81ms
step:1390/2110 train_time:65108ms step_avg:46.84ms
step:1391/2110 train_time:65195ms step_avg:46.87ms
step:1392/2110 train_time:65282ms step_avg:46.90ms
step:1393/2110 train_time:65369ms step_avg:46.93ms
step:1394/2110 train_time:65455ms step_avg:46.96ms
step:1395/2110 train_time:65542ms step_avg:46.98ms
step:1396/2110 train_time:65628ms step_avg:47.01ms
step:1397/2110 train_time:65715ms step_avg:47.04ms
step:1398/2110 train_time:65802ms step_avg:47.07ms
step:1399/2110 train_time:65889ms step_avg:47.10ms
step:1400/2110 train_time:65976ms step_avg:47.13ms
step:1401/2110 train_time:66062ms step_avg:47.15ms
step:1402/2110 train_time:66148ms step_avg:47.18ms
step:1403/2110 train_time:66234ms step_avg:47.21ms
step:1404/2110 train_time:66320ms step_avg:47.24ms
step:1405/2110 train_time:66407ms step_avg:47.26ms
step:1406/2110 train_time:66494ms step_avg:47.29ms
step:1407/2110 train_time:66581ms step_avg:47.32ms
step:1408/2110 train_time:66667ms step_avg:47.35ms
step:1409/2110 train_time:66753ms step_avg:47.38ms
step:1410/2110 train_time:66841ms step_avg:47.40ms
step:1411/2110 train_time:66927ms step_avg:47.43ms
step:1412/2110 train_time:67015ms step_avg:47.46ms
step:1413/2110 train_time:67100ms step_avg:47.49ms
step:1414/2110 train_time:67186ms step_avg:47.51ms
step:1415/2110 train_time:67272ms step_avg:47.54ms
step:1416/2110 train_time:67358ms step_avg:47.57ms
step:1417/2110 train_time:67446ms step_avg:47.60ms
step:1418/2110 train_time:67533ms step_avg:47.63ms
step:1419/2110 train_time:67620ms step_avg:47.65ms
step:1420/2110 train_time:67706ms step_avg:47.68ms
step:1421/2110 train_time:67792ms step_avg:47.71ms
step:1422/2110 train_time:67879ms step_avg:47.73ms
step:1423/2110 train_time:67965ms step_avg:47.76ms
step:1424/2110 train_time:68052ms step_avg:47.79ms
step:1425/2110 train_time:68138ms step_avg:47.82ms
step:1426/2110 train_time:68226ms step_avg:47.84ms
step:1427/2110 train_time:68311ms step_avg:47.87ms
step:1428/2110 train_time:68397ms step_avg:47.90ms
step:1429/2110 train_time:68484ms step_avg:47.92ms
step:1430/2110 train_time:68570ms step_avg:47.95ms
step:1431/2110 train_time:68657ms step_avg:47.98ms
step:1432/2110 train_time:68744ms step_avg:48.01ms
step:1433/2110 train_time:68831ms step_avg:48.03ms
step:1434/2110 train_time:68917ms step_avg:48.06ms
step:1435/2110 train_time:69004ms step_avg:48.09ms
step:1436/2110 train_time:69090ms step_avg:48.11ms
step:1437/2110 train_time:69177ms step_avg:48.14ms
step:1438/2110 train_time:69265ms step_avg:48.17ms
step:1439/2110 train_time:69350ms step_avg:48.19ms
step:1440/2110 train_time:69438ms step_avg:48.22ms
step:1441/2110 train_time:69524ms step_avg:48.25ms
step:1442/2110 train_time:69609ms step_avg:48.27ms
step:1443/2110 train_time:69696ms step_avg:48.30ms
step:1444/2110 train_time:69782ms step_avg:48.33ms
step:1445/2110 train_time:69869ms step_avg:48.35ms
step:1446/2110 train_time:69956ms step_avg:48.38ms
step:1447/2110 train_time:70043ms step_avg:48.41ms
step:1448/2110 train_time:70128ms step_avg:48.43ms
step:1449/2110 train_time:70215ms step_avg:48.46ms
step:1450/2110 train_time:70302ms step_avg:48.48ms
step:1451/2110 train_time:70388ms step_avg:48.51ms
step:1452/2110 train_time:70476ms step_avg:48.54ms
step:1453/2110 train_time:70562ms step_avg:48.56ms
step:1454/2110 train_time:70648ms step_avg:48.59ms
step:1455/2110 train_time:70735ms step_avg:48.62ms
step:1456/2110 train_time:70822ms step_avg:48.64ms
step:1457/2110 train_time:70908ms step_avg:48.67ms
step:1458/2110 train_time:70994ms step_avg:48.69ms
step:1459/2110 train_time:71081ms step_avg:48.72ms
step:1460/2110 train_time:71167ms step_avg:48.74ms
step:1461/2110 train_time:71253ms step_avg:48.77ms
step:1462/2110 train_time:71341ms step_avg:48.80ms
step:1463/2110 train_time:71427ms step_avg:48.82ms
step:1464/2110 train_time:71512ms step_avg:48.85ms
step:1465/2110 train_time:71600ms step_avg:48.87ms
step:1466/2110 train_time:71686ms step_avg:48.90ms
step:1467/2110 train_time:71773ms step_avg:48.92ms
step:1468/2110 train_time:71860ms step_avg:48.95ms
step:1469/2110 train_time:71947ms step_avg:48.98ms
step:1470/2110 train_time:72034ms step_avg:49.00ms
step:1471/2110 train_time:72120ms step_avg:49.03ms
step:1472/2110 train_time:72207ms step_avg:49.05ms
step:1473/2110 train_time:72293ms step_avg:49.08ms
step:1474/2110 train_time:72380ms step_avg:49.10ms
step:1475/2110 train_time:72466ms step_avg:49.13ms
step:1476/2110 train_time:72553ms step_avg:49.15ms
step:1477/2110 train_time:72639ms step_avg:49.18ms
step:1478/2110 train_time:72726ms step_avg:49.21ms
step:1479/2110 train_time:72812ms step_avg:49.23ms
step:1480/2110 train_time:72899ms step_avg:49.26ms
step:1481/2110 train_time:72985ms step_avg:49.28ms
step:1482/2110 train_time:73073ms step_avg:49.31ms
step:1483/2110 train_time:73158ms step_avg:49.33ms
step:1484/2110 train_time:73244ms step_avg:49.36ms
step:1485/2110 train_time:73331ms step_avg:49.38ms
step:1486/2110 train_time:73417ms step_avg:49.41ms
step:1487/2110 train_time:73503ms step_avg:49.43ms
step:1488/2110 train_time:73589ms step_avg:49.46ms
step:1489/2110 train_time:73676ms step_avg:49.48ms
step:1490/2110 train_time:73763ms step_avg:49.51ms
step:1491/2110 train_time:73849ms step_avg:49.53ms
step:1492/2110 train_time:73936ms step_avg:49.56ms
step:1493/2110 train_time:74023ms step_avg:49.58ms
step:1494/2110 train_time:74109ms step_avg:49.60ms
step:1495/2110 train_time:74196ms step_avg:49.63ms
step:1496/2110 train_time:74282ms step_avg:49.65ms
step:1497/2110 train_time:74368ms step_avg:49.68ms
step:1498/2110 train_time:74455ms step_avg:49.70ms
step:1499/2110 train_time:74542ms step_avg:49.73ms
step:1500/2110 train_time:74628ms step_avg:49.75ms
step:1500/2110 val_loss:3.4952 train_time:74716ms step_avg:49.81ms
step:1501/2110 train_time:74746ms step_avg:49.80ms
step:1502/2110 train_time:74808ms step_avg:49.81ms
step:1503/2110 train_time:74904ms step_avg:49.84ms
step:1504/2110 train_time:74990ms step_avg:49.86ms
step:1505/2110 train_time:75077ms step_avg:49.89ms
step:1506/2110 train_time:75163ms step_avg:49.91ms
step:1507/2110 train_time:75249ms step_avg:49.93ms
step:1508/2110 train_time:75334ms step_avg:49.96ms
step:1509/2110 train_time:75420ms step_avg:49.98ms
step:1510/2110 train_time:75506ms step_avg:50.00ms
step:1511/2110 train_time:75591ms step_avg:50.03ms
step:1512/2110 train_time:75678ms step_avg:50.05ms
step:1513/2110 train_time:75769ms step_avg:50.08ms
step:1514/2110 train_time:75858ms step_avg:50.10ms
step:1515/2110 train_time:75949ms step_avg:50.13ms
step:1516/2110 train_time:76036ms step_avg:50.16ms
step:1517/2110 train_time:76125ms step_avg:50.18ms
step:1518/2110 train_time:76209ms step_avg:50.20ms
step:1519/2110 train_time:76294ms step_avg:50.23ms
step:1520/2110 train_time:76379ms step_avg:50.25ms
step:1521/2110 train_time:76465ms step_avg:50.27ms
step:1522/2110 train_time:76551ms step_avg:50.30ms
step:1523/2110 train_time:76637ms step_avg:50.32ms
step:1524/2110 train_time:76726ms step_avg:50.35ms
step:1525/2110 train_time:76814ms step_avg:50.37ms
step:1526/2110 train_time:76902ms step_avg:50.39ms
step:1527/2110 train_time:76989ms step_avg:50.42ms
step:1528/2110 train_time:77075ms step_avg:50.44ms
step:1529/2110 train_time:77163ms step_avg:50.47ms
step:1530/2110 train_time:77249ms step_avg:50.49ms
step:1531/2110 train_time:77333ms step_avg:50.51ms
step:1532/2110 train_time:77419ms step_avg:50.53ms
step:1533/2110 train_time:77506ms step_avg:50.56ms
step:1534/2110 train_time:77592ms step_avg:50.58ms
step:1535/2110 train_time:77678ms step_avg:50.60ms
step:1536/2110 train_time:77767ms step_avg:50.63ms
step:1537/2110 train_time:77853ms step_avg:50.65ms
step:1538/2110 train_time:77943ms step_avg:50.68ms
step:1539/2110 train_time:78028ms step_avg:50.70ms
step:1540/2110 train_time:78114ms step_avg:50.72ms
step:1541/2110 train_time:78202ms step_avg:50.75ms
step:1542/2110 train_time:78288ms step_avg:50.77ms
step:1543/2110 train_time:78373ms step_avg:50.79ms
step:1544/2110 train_time:78459ms step_avg:50.82ms
step:1545/2110 train_time:78545ms step_avg:50.84ms
step:1546/2110 train_time:78631ms step_avg:50.86ms
step:1547/2110 train_time:78718ms step_avg:50.88ms
step:1548/2110 train_time:78807ms step_avg:50.91ms
step:1549/2110 train_time:78893ms step_avg:50.93ms
step:1550/2110 train_time:78981ms step_avg:50.96ms
step:1551/2110 train_time:79068ms step_avg:50.98ms
step:1552/2110 train_time:79153ms step_avg:51.00ms
step:1553/2110 train_time:79240ms step_avg:51.02ms
step:1554/2110 train_time:79327ms step_avg:51.05ms
step:1555/2110 train_time:79413ms step_avg:51.07ms
step:1556/2110 train_time:79499ms step_avg:51.09ms
step:1557/2110 train_time:79586ms step_avg:51.11ms
step:1558/2110 train_time:79672ms step_avg:51.14ms
step:1559/2110 train_time:79759ms step_avg:51.16ms
step:1560/2110 train_time:79847ms step_avg:51.18ms
step:1561/2110 train_time:79933ms step_avg:51.21ms
step:1562/2110 train_time:80021ms step_avg:51.23ms
step:1563/2110 train_time:80108ms step_avg:51.25ms
step:1564/2110 train_time:80194ms step_avg:51.27ms
step:1565/2110 train_time:80280ms step_avg:51.30ms
step:1566/2110 train_time:80367ms step_avg:51.32ms
step:1567/2110 train_time:80453ms step_avg:51.34ms
step:1568/2110 train_time:80540ms step_avg:51.36ms
step:1569/2110 train_time:80626ms step_avg:51.39ms
step:1570/2110 train_time:80711ms step_avg:51.41ms
step:1571/2110 train_time:80800ms step_avg:51.43ms
step:1572/2110 train_time:80887ms step_avg:51.45ms
step:1573/2110 train_time:80976ms step_avg:51.48ms
step:1574/2110 train_time:81060ms step_avg:51.50ms
step:1575/2110 train_time:81148ms step_avg:51.52ms
step:1576/2110 train_time:81233ms step_avg:51.54ms
step:1577/2110 train_time:81320ms step_avg:51.57ms
step:1578/2110 train_time:81407ms step_avg:51.59ms
step:1579/2110 train_time:81493ms step_avg:51.61ms
step:1580/2110 train_time:81581ms step_avg:51.63ms
step:1581/2110 train_time:81667ms step_avg:51.66ms
step:1582/2110 train_time:81753ms step_avg:51.68ms
step:1583/2110 train_time:81840ms step_avg:51.70ms
step:1584/2110 train_time:81927ms step_avg:51.72ms
step:1585/2110 train_time:82013ms step_avg:51.74ms
step:1586/2110 train_time:82100ms step_avg:51.77ms
step:1587/2110 train_time:82186ms step_avg:51.79ms
step:1588/2110 train_time:82273ms step_avg:51.81ms
step:1589/2110 train_time:82359ms step_avg:51.83ms
step:1590/2110 train_time:82446ms step_avg:51.85ms
step:1591/2110 train_time:82532ms step_avg:51.87ms
step:1592/2110 train_time:82618ms step_avg:51.90ms
step:1593/2110 train_time:82706ms step_avg:51.92ms
step:1594/2110 train_time:82792ms step_avg:51.94ms
step:1595/2110 train_time:82880ms step_avg:51.96ms
step:1596/2110 train_time:82967ms step_avg:51.98ms
step:1597/2110 train_time:83053ms step_avg:52.01ms
step:1598/2110 train_time:83140ms step_avg:52.03ms
step:1599/2110 train_time:83227ms step_avg:52.05ms
step:1600/2110 train_time:83313ms step_avg:52.07ms
step:1601/2110 train_time:83400ms step_avg:52.09ms
step:1602/2110 train_time:83487ms step_avg:52.11ms
step:1603/2110 train_time:83573ms step_avg:52.14ms
step:1604/2110 train_time:83659ms step_avg:52.16ms
step:1605/2110 train_time:83745ms step_avg:52.18ms
step:1606/2110 train_time:83832ms step_avg:52.20ms
step:1607/2110 train_time:83919ms step_avg:52.22ms
step:1608/2110 train_time:84006ms step_avg:52.24ms
step:1609/2110 train_time:84093ms step_avg:52.26ms
step:1610/2110 train_time:84180ms step_avg:52.29ms
step:1611/2110 train_time:84267ms step_avg:52.31ms
step:1612/2110 train_time:84353ms step_avg:52.33ms
step:1613/2110 train_time:84440ms step_avg:52.35ms
step:1614/2110 train_time:84526ms step_avg:52.37ms
step:1615/2110 train_time:84613ms step_avg:52.39ms
step:1616/2110 train_time:84700ms step_avg:52.41ms
step:1617/2110 train_time:84787ms step_avg:52.43ms
step:1618/2110 train_time:84873ms step_avg:52.46ms
step:1619/2110 train_time:84960ms step_avg:52.48ms
step:1620/2110 train_time:85046ms step_avg:52.50ms
step:1621/2110 train_time:85132ms step_avg:52.52ms
step:1622/2110 train_time:85219ms step_avg:52.54ms
step:1623/2110 train_time:85306ms step_avg:52.56ms
step:1624/2110 train_time:85392ms step_avg:52.58ms
step:1625/2110 train_time:85478ms step_avg:52.60ms
step:1626/2110 train_time:85565ms step_avg:52.62ms
step:1627/2110 train_time:85651ms step_avg:52.64ms
step:1628/2110 train_time:85738ms step_avg:52.66ms
step:1629/2110 train_time:85824ms step_avg:52.69ms
step:1630/2110 train_time:85911ms step_avg:52.71ms
step:1631/2110 train_time:85998ms step_avg:52.73ms
step:1632/2110 train_time:86085ms step_avg:52.75ms
step:1633/2110 train_time:86172ms step_avg:52.77ms
step:1634/2110 train_time:86259ms step_avg:52.79ms
step:1635/2110 train_time:86346ms step_avg:52.81ms
step:1636/2110 train_time:86432ms step_avg:52.83ms
step:1637/2110 train_time:86519ms step_avg:52.85ms
step:1638/2110 train_time:86606ms step_avg:52.87ms
step:1639/2110 train_time:86692ms step_avg:52.89ms
step:1640/2110 train_time:86779ms step_avg:52.91ms
step:1641/2110 train_time:86866ms step_avg:52.93ms
step:1642/2110 train_time:86952ms step_avg:52.95ms
step:1643/2110 train_time:87038ms step_avg:52.98ms
step:1644/2110 train_time:87126ms step_avg:53.00ms
step:1645/2110 train_time:87213ms step_avg:53.02ms
step:1646/2110 train_time:87300ms step_avg:53.04ms
step:1647/2110 train_time:87386ms step_avg:53.06ms
step:1648/2110 train_time:87472ms step_avg:53.08ms
step:1649/2110 train_time:87558ms step_avg:53.10ms
step:1650/2110 train_time:87645ms step_avg:53.12ms
step:1651/2110 train_time:87732ms step_avg:53.14ms
step:1652/2110 train_time:87819ms step_avg:53.16ms
step:1653/2110 train_time:87905ms step_avg:53.18ms
step:1654/2110 train_time:87991ms step_avg:53.20ms
step:1655/2110 train_time:88079ms step_avg:53.22ms
step:1656/2110 train_time:88166ms step_avg:53.24ms
step:1657/2110 train_time:88253ms step_avg:53.26ms
step:1658/2110 train_time:88341ms step_avg:53.28ms
step:1659/2110 train_time:88430ms step_avg:53.30ms
step:1660/2110 train_time:88518ms step_avg:53.32ms
step:1661/2110 train_time:88606ms step_avg:53.34ms
step:1662/2110 train_time:88692ms step_avg:53.36ms
step:1663/2110 train_time:88781ms step_avg:53.39ms
step:1664/2110 train_time:88869ms step_avg:53.41ms
step:1665/2110 train_time:88957ms step_avg:53.43ms
step:1666/2110 train_time:89045ms step_avg:53.45ms
step:1667/2110 train_time:89134ms step_avg:53.47ms
step:1668/2110 train_time:89221ms step_avg:53.49ms
step:1669/2110 train_time:89309ms step_avg:53.51ms
step:1670/2110 train_time:89396ms step_avg:53.53ms
step:1671/2110 train_time:89485ms step_avg:53.55ms
step:1672/2110 train_time:89573ms step_avg:53.57ms
step:1673/2110 train_time:89660ms step_avg:53.59ms
step:1674/2110 train_time:89748ms step_avg:53.61ms
step:1675/2110 train_time:89835ms step_avg:53.63ms
step:1676/2110 train_time:89922ms step_avg:53.65ms
step:1677/2110 train_time:90010ms step_avg:53.67ms
step:1678/2110 train_time:90098ms step_avg:53.69ms
step:1679/2110 train_time:90186ms step_avg:53.71ms
step:1680/2110 train_time:90274ms step_avg:53.73ms
step:1681/2110 train_time:90362ms step_avg:53.76ms
step:1682/2110 train_time:90451ms step_avg:53.78ms
step:1683/2110 train_time:90538ms step_avg:53.80ms
step:1684/2110 train_time:90626ms step_avg:53.82ms
step:1685/2110 train_time:90714ms step_avg:53.84ms
step:1686/2110 train_time:90801ms step_avg:53.86ms
step:1687/2110 train_time:90889ms step_avg:53.88ms
step:1688/2110 train_time:90975ms step_avg:53.90ms
step:1689/2110 train_time:91064ms step_avg:53.92ms
step:1690/2110 train_time:91152ms step_avg:53.94ms
step:1691/2110 train_time:91241ms step_avg:53.96ms
step:1692/2110 train_time:91330ms step_avg:53.98ms
step:1693/2110 train_time:91417ms step_avg:54.00ms
step:1694/2110 train_time:91505ms step_avg:54.02ms
step:1695/2110 train_time:91592ms step_avg:54.04ms
step:1696/2110 train_time:91680ms step_avg:54.06ms
step:1697/2110 train_time:91768ms step_avg:54.08ms
step:1698/2110 train_time:91855ms step_avg:54.10ms
step:1699/2110 train_time:91944ms step_avg:54.12ms
step:1700/2110 train_time:92031ms step_avg:54.14ms
step:1701/2110 train_time:92119ms step_avg:54.16ms
step:1702/2110 train_time:92208ms step_avg:54.18ms
step:1703/2110 train_time:92295ms step_avg:54.20ms
step:1704/2110 train_time:92385ms step_avg:54.22ms
step:1705/2110 train_time:92472ms step_avg:54.24ms
step:1706/2110 train_time:92561ms step_avg:54.26ms
step:1707/2110 train_time:92649ms step_avg:54.28ms
step:1708/2110 train_time:92737ms step_avg:54.30ms
step:1709/2110 train_time:92824ms step_avg:54.31ms
step:1710/2110 train_time:92912ms step_avg:54.33ms
step:1711/2110 train_time:92999ms step_avg:54.35ms
step:1712/2110 train_time:93087ms step_avg:54.37ms
step:1713/2110 train_time:93175ms step_avg:54.39ms
step:1714/2110 train_time:93263ms step_avg:54.41ms
step:1715/2110 train_time:93351ms step_avg:54.43ms
step:1716/2110 train_time:93439ms step_avg:54.45ms
step:1717/2110 train_time:93527ms step_avg:54.47ms
step:1718/2110 train_time:93614ms step_avg:54.49ms
step:1719/2110 train_time:93703ms step_avg:54.51ms
step:1720/2110 train_time:93791ms step_avg:54.53ms
step:1721/2110 train_time:93878ms step_avg:54.55ms
step:1722/2110 train_time:93966ms step_avg:54.57ms
step:1723/2110 train_time:94054ms step_avg:54.59ms
step:1724/2110 train_time:94143ms step_avg:54.61ms
step:1725/2110 train_time:94231ms step_avg:54.63ms
step:1726/2110 train_time:94319ms step_avg:54.65ms
step:1727/2110 train_time:94407ms step_avg:54.67ms
step:1728/2110 train_time:94495ms step_avg:54.68ms
step:1729/2110 train_time:94583ms step_avg:54.70ms
step:1730/2110 train_time:94671ms step_avg:54.72ms
step:1731/2110 train_time:94759ms step_avg:54.74ms
step:1732/2110 train_time:94847ms step_avg:54.76ms
step:1733/2110 train_time:94935ms step_avg:54.78ms
step:1734/2110 train_time:95022ms step_avg:54.80ms
step:1735/2110 train_time:95110ms step_avg:54.82ms
step:1736/2110 train_time:95198ms step_avg:54.84ms
step:1737/2110 train_time:95286ms step_avg:54.86ms
step:1738/2110 train_time:95373ms step_avg:54.88ms
step:1739/2110 train_time:95462ms step_avg:54.89ms
step:1740/2110 train_time:95551ms step_avg:54.91ms
step:1741/2110 train_time:95639ms step_avg:54.93ms
step:1742/2110 train_time:95727ms step_avg:54.95ms
step:1743/2110 train_time:95814ms step_avg:54.97ms
step:1744/2110 train_time:95903ms step_avg:54.99ms
step:1745/2110 train_time:95991ms step_avg:55.01ms
step:1746/2110 train_time:96079ms step_avg:55.03ms
step:1747/2110 train_time:96166ms step_avg:55.05ms
step:1748/2110 train_time:96253ms step_avg:55.06ms
step:1749/2110 train_time:96342ms step_avg:55.08ms
step:1750/2110 train_time:96430ms step_avg:55.10ms
step:1750/2110 val_loss:3.3815 train_time:96520ms step_avg:55.15ms
step:1751/2110 train_time:96556ms step_avg:55.14ms
step:1752/2110 train_time:96619ms step_avg:55.15ms
step:1753/2110 train_time:96712ms step_avg:55.17ms
step:1754/2110 train_time:96800ms step_avg:55.19ms
step:1755/2110 train_time:96888ms step_avg:55.21ms
step:1756/2110 train_time:96974ms step_avg:55.22ms
step:1757/2110 train_time:97061ms step_avg:55.24ms
step:1758/2110 train_time:97148ms step_avg:55.26ms
step:1759/2110 train_time:97235ms step_avg:55.28ms
step:1760/2110 train_time:97321ms step_avg:55.30ms
step:1761/2110 train_time:97410ms step_avg:55.31ms
step:1762/2110 train_time:97503ms step_avg:55.34ms
step:1763/2110 train_time:97594ms step_avg:55.36ms
step:1764/2110 train_time:97685ms step_avg:55.38ms
step:1765/2110 train_time:97773ms step_avg:55.40ms
step:1766/2110 train_time:97860ms step_avg:55.41ms
step:1767/2110 train_time:97947ms step_avg:55.43ms
step:1768/2110 train_time:98034ms step_avg:55.45ms
step:1769/2110 train_time:98121ms step_avg:55.47ms
step:1770/2110 train_time:98207ms step_avg:55.48ms
step:1771/2110 train_time:98294ms step_avg:55.50ms
step:1772/2110 train_time:98382ms step_avg:55.52ms
step:1773/2110 train_time:98472ms step_avg:55.54ms
step:1774/2110 train_time:98563ms step_avg:55.56ms
step:1775/2110 train_time:98653ms step_avg:55.58ms
step:1776/2110 train_time:98743ms step_avg:55.60ms
step:1777/2110 train_time:98832ms step_avg:55.62ms
step:1778/2110 train_time:98920ms step_avg:55.64ms
step:1779/2110 train_time:99006ms step_avg:55.65ms
step:1780/2110 train_time:99093ms step_avg:55.67ms
step:1781/2110 train_time:99180ms step_avg:55.69ms
step:1782/2110 train_time:99266ms step_avg:55.71ms
step:1783/2110 train_time:99354ms step_avg:55.72ms
step:1784/2110 train_time:99443ms step_avg:55.74ms
step:1785/2110 train_time:99531ms step_avg:55.76ms
step:1786/2110 train_time:99621ms step_avg:55.78ms
step:1787/2110 train_time:99711ms step_avg:55.80ms
step:1788/2110 train_time:99799ms step_avg:55.82ms
step:1789/2110 train_time:99888ms step_avg:55.83ms
step:1790/2110 train_time:99975ms step_avg:55.85ms
step:1791/2110 train_time:100062ms step_avg:55.87ms
step:1792/2110 train_time:100148ms step_avg:55.89ms
step:1793/2110 train_time:100236ms step_avg:55.90ms
step:1794/2110 train_time:100324ms step_avg:55.92ms
step:1795/2110 train_time:100411ms step_avg:55.94ms
step:1796/2110 train_time:100499ms step_avg:55.96ms
step:1797/2110 train_time:100589ms step_avg:55.98ms
step:1798/2110 train_time:100677ms step_avg:55.99ms
step:1799/2110 train_time:100766ms step_avg:56.01ms
step:1800/2110 train_time:100854ms step_avg:56.03ms
step:1801/2110 train_time:100942ms step_avg:56.05ms
step:1802/2110 train_time:101029ms step_avg:56.06ms
step:1803/2110 train_time:101116ms step_avg:56.08ms
step:1804/2110 train_time:101204ms step_avg:56.10ms
step:1805/2110 train_time:101290ms step_avg:56.12ms
step:1806/2110 train_time:101377ms step_avg:56.13ms
step:1807/2110 train_time:101466ms step_avg:56.15ms
step:1808/2110 train_time:101555ms step_avg:56.17ms
step:1809/2110 train_time:101644ms step_avg:56.19ms
step:1810/2110 train_time:101733ms step_avg:56.21ms
step:1811/2110 train_time:101822ms step_avg:56.22ms
step:1812/2110 train_time:101909ms step_avg:56.24ms
step:1813/2110 train_time:101997ms step_avg:56.26ms
step:1814/2110 train_time:102084ms step_avg:56.28ms
step:1815/2110 train_time:102171ms step_avg:56.29ms
step:1816/2110 train_time:102258ms step_avg:56.31ms
step:1817/2110 train_time:102346ms step_avg:56.33ms
step:1818/2110 train_time:102433ms step_avg:56.34ms
step:1819/2110 train_time:102522ms step_avg:56.36ms
step:1820/2110 train_time:102610ms step_avg:56.38ms
step:1821/2110 train_time:102698ms step_avg:56.40ms
step:1822/2110 train_time:102786ms step_avg:56.41ms
step:1823/2110 train_time:102874ms step_avg:56.43ms
step:1824/2110 train_time:102962ms step_avg:56.45ms
step:1825/2110 train_time:103048ms step_avg:56.46ms
step:1826/2110 train_time:103136ms step_avg:56.48ms
step:1827/2110 train_time:103224ms step_avg:56.50ms
step:1828/2110 train_time:103310ms step_avg:56.52ms
step:1829/2110 train_time:103399ms step_avg:56.53ms
step:1830/2110 train_time:103487ms step_avg:56.55ms
step:1831/2110 train_time:103577ms step_avg:56.57ms
step:1832/2110 train_time:103666ms step_avg:56.59ms
step:1833/2110 train_time:103753ms step_avg:56.60ms
step:1834/2110 train_time:103841ms step_avg:56.62ms
step:1835/2110 train_time:103929ms step_avg:56.64ms
step:1836/2110 train_time:104017ms step_avg:56.65ms
step:1837/2110 train_time:104104ms step_avg:56.67ms
step:1838/2110 train_time:104192ms step_avg:56.69ms
step:1839/2110 train_time:104280ms step_avg:56.70ms
step:1840/2110 train_time:104367ms step_avg:56.72ms
step:1841/2110 train_time:104456ms step_avg:56.74ms
step:1842/2110 train_time:104544ms step_avg:56.76ms
step:1843/2110 train_time:104631ms step_avg:56.77ms
step:1844/2110 train_time:104720ms step_avg:56.79ms
step:1845/2110 train_time:104808ms step_avg:56.81ms
step:1846/2110 train_time:104896ms step_avg:56.82ms
step:1847/2110 train_time:104984ms step_avg:56.84ms
step:1848/2110 train_time:105072ms step_avg:56.86ms
step:1849/2110 train_time:105160ms step_avg:56.87ms
step:1850/2110 train_time:105247ms step_avg:56.89ms
step:1851/2110 train_time:105334ms step_avg:56.91ms
step:1852/2110 train_time:105424ms step_avg:56.92ms
step:1853/2110 train_time:105511ms step_avg:56.94ms
step:1854/2110 train_time:105599ms step_avg:56.96ms
step:1855/2110 train_time:105688ms step_avg:56.97ms
step:1856/2110 train_time:105776ms step_avg:56.99ms
step:1857/2110 train_time:105866ms step_avg:57.01ms
step:1858/2110 train_time:105954ms step_avg:57.03ms
step:1859/2110 train_time:106043ms step_avg:57.04ms
step:1860/2110 train_time:106131ms step_avg:57.06ms
step:1861/2110 train_time:106217ms step_avg:57.08ms
step:1862/2110 train_time:106305ms step_avg:57.09ms
step:1863/2110 train_time:106392ms step_avg:57.11ms
step:1864/2110 train_time:106480ms step_avg:57.12ms
step:1865/2110 train_time:106567ms step_avg:57.14ms
step:1866/2110 train_time:106656ms step_avg:57.16ms
step:1867/2110 train_time:106744ms step_avg:57.17ms
step:1868/2110 train_time:106833ms step_avg:57.19ms
step:1869/2110 train_time:106920ms step_avg:57.21ms
step:1870/2110 train_time:107008ms step_avg:57.22ms
step:1871/2110 train_time:107096ms step_avg:57.24ms
step:1872/2110 train_time:107183ms step_avg:57.26ms
step:1873/2110 train_time:107271ms step_avg:57.27ms
step:1874/2110 train_time:107359ms step_avg:57.29ms
step:1875/2110 train_time:107447ms step_avg:57.31ms
step:1876/2110 train_time:107535ms step_avg:57.32ms
step:1877/2110 train_time:107624ms step_avg:57.34ms
step:1878/2110 train_time:107712ms step_avg:57.35ms
step:1879/2110 train_time:107800ms step_avg:57.37ms
step:1880/2110 train_time:107886ms step_avg:57.39ms
step:1881/2110 train_time:107975ms step_avg:57.40ms
step:1882/2110 train_time:108063ms step_avg:57.42ms
step:1883/2110 train_time:108150ms step_avg:57.44ms
step:1884/2110 train_time:108238ms step_avg:57.45ms
step:1885/2110 train_time:108325ms step_avg:57.47ms
step:1886/2110 train_time:108413ms step_avg:57.48ms
step:1887/2110 train_time:108502ms step_avg:57.50ms
step:1888/2110 train_time:108590ms step_avg:57.52ms
step:1889/2110 train_time:108678ms step_avg:57.53ms
step:1890/2110 train_time:108766ms step_avg:57.55ms
step:1891/2110 train_time:108854ms step_avg:57.56ms
step:1892/2110 train_time:108942ms step_avg:57.58ms
step:1893/2110 train_time:109030ms step_avg:57.60ms
step:1894/2110 train_time:109118ms step_avg:57.61ms
step:1895/2110 train_time:109205ms step_avg:57.63ms
step:1896/2110 train_time:109293ms step_avg:57.64ms
step:1897/2110 train_time:109382ms step_avg:57.66ms
step:1898/2110 train_time:109468ms step_avg:57.68ms
step:1899/2110 train_time:109557ms step_avg:57.69ms
step:1900/2110 train_time:109645ms step_avg:57.71ms
step:1901/2110 train_time:109733ms step_avg:57.72ms
step:1902/2110 train_time:109821ms step_avg:57.74ms
step:1903/2110 train_time:109908ms step_avg:57.76ms
step:1904/2110 train_time:109997ms step_avg:57.77ms
step:1905/2110 train_time:110084ms step_avg:57.79ms
step:1906/2110 train_time:110172ms step_avg:57.80ms
step:1907/2110 train_time:110261ms step_avg:57.82ms
step:1908/2110 train_time:110349ms step_avg:57.83ms
step:1909/2110 train_time:110436ms step_avg:57.85ms
step:1910/2110 train_time:110524ms step_avg:57.87ms
step:1911/2110 train_time:110611ms step_avg:57.88ms
step:1912/2110 train_time:110699ms step_avg:57.90ms
step:1913/2110 train_time:110787ms step_avg:57.91ms
step:1914/2110 train_time:110875ms step_avg:57.93ms
step:1915/2110 train_time:110963ms step_avg:57.94ms
step:1916/2110 train_time:111051ms step_avg:57.96ms
step:1917/2110 train_time:111139ms step_avg:57.98ms
step:1918/2110 train_time:111226ms step_avg:57.99ms
step:1919/2110 train_time:111315ms step_avg:58.01ms
step:1920/2110 train_time:111403ms step_avg:58.02ms
step:1921/2110 train_time:111491ms step_avg:58.04ms
step:1922/2110 train_time:111579ms step_avg:58.05ms
step:1923/2110 train_time:111668ms step_avg:58.07ms
step:1924/2110 train_time:111756ms step_avg:58.09ms
step:1925/2110 train_time:111846ms step_avg:58.10ms
step:1926/2110 train_time:111934ms step_avg:58.12ms
step:1927/2110 train_time:112022ms step_avg:58.13ms
step:1928/2110 train_time:112108ms step_avg:58.15ms
step:1929/2110 train_time:112196ms step_avg:58.16ms
step:1930/2110 train_time:112284ms step_avg:58.18ms
step:1931/2110 train_time:112371ms step_avg:58.19ms
step:1932/2110 train_time:112459ms step_avg:58.21ms
step:1933/2110 train_time:112547ms step_avg:58.22ms
step:1934/2110 train_time:112636ms step_avg:58.24ms
step:1935/2110 train_time:112723ms step_avg:58.25ms
step:1936/2110 train_time:112811ms step_avg:58.27ms
step:1937/2110 train_time:112900ms step_avg:58.29ms
step:1938/2110 train_time:112987ms step_avg:58.30ms
step:1939/2110 train_time:113076ms step_avg:58.32ms
step:1940/2110 train_time:113163ms step_avg:58.33ms
step:1941/2110 train_time:113251ms step_avg:58.35ms
step:1942/2110 train_time:113338ms step_avg:58.36ms
step:1943/2110 train_time:113426ms step_avg:58.38ms
step:1944/2110 train_time:113515ms step_avg:58.39ms
step:1945/2110 train_time:113602ms step_avg:58.41ms
step:1946/2110 train_time:113690ms step_avg:58.42ms
step:1947/2110 train_time:113778ms step_avg:58.44ms
step:1948/2110 train_time:113865ms step_avg:58.45ms
step:1949/2110 train_time:113955ms step_avg:58.47ms
step:1950/2110 train_time:114043ms step_avg:58.48ms
step:1951/2110 train_time:114131ms step_avg:58.50ms
step:1952/2110 train_time:114219ms step_avg:58.51ms
step:1953/2110 train_time:114306ms step_avg:58.53ms
step:1954/2110 train_time:114394ms step_avg:58.54ms
step:1955/2110 train_time:114482ms step_avg:58.56ms
step:1956/2110 train_time:114570ms step_avg:58.57ms
step:1957/2110 train_time:114658ms step_avg:58.59ms
step:1958/2110 train_time:114746ms step_avg:58.60ms
step:1959/2110 train_time:114833ms step_avg:58.62ms
step:1960/2110 train_time:114922ms step_avg:58.63ms
step:1961/2110 train_time:115009ms step_avg:58.65ms
step:1962/2110 train_time:115097ms step_avg:58.66ms
step:1963/2110 train_time:115185ms step_avg:58.68ms
step:1964/2110 train_time:115273ms step_avg:58.69ms
step:1965/2110 train_time:115359ms step_avg:58.71ms
step:1966/2110 train_time:115447ms step_avg:58.72ms
step:1967/2110 train_time:115535ms step_avg:58.74ms
step:1968/2110 train_time:115623ms step_avg:58.75ms
step:1969/2110 train_time:115710ms step_avg:58.77ms
step:1970/2110 train_time:115798ms step_avg:58.78ms
step:1971/2110 train_time:115887ms step_avg:58.80ms
step:1972/2110 train_time:115976ms step_avg:58.81ms
step:1973/2110 train_time:116065ms step_avg:58.83ms
step:1974/2110 train_time:116152ms step_avg:58.84ms
step:1975/2110 train_time:116241ms step_avg:58.86ms
step:1976/2110 train_time:116328ms step_avg:58.87ms
step:1977/2110 train_time:116416ms step_avg:58.89ms
step:1978/2110 train_time:116505ms step_avg:58.90ms
step:1979/2110 train_time:116592ms step_avg:58.91ms
step:1980/2110 train_time:116680ms step_avg:58.93ms
step:1981/2110 train_time:116768ms step_avg:58.94ms
step:1982/2110 train_time:116855ms step_avg:58.96ms
step:1983/2110 train_time:116945ms step_avg:58.97ms
step:1984/2110 train_time:117033ms step_avg:58.99ms
step:1985/2110 train_time:117122ms step_avg:59.00ms
step:1986/2110 train_time:117209ms step_avg:59.02ms
step:1987/2110 train_time:117297ms step_avg:59.03ms
step:1988/2110 train_time:117385ms step_avg:59.05ms
step:1989/2110 train_time:117473ms step_avg:59.06ms
step:1990/2110 train_time:117561ms step_avg:59.08ms
step:1991/2110 train_time:117648ms step_avg:59.09ms
step:1992/2110 train_time:117737ms step_avg:59.10ms
step:1993/2110 train_time:117825ms step_avg:59.12ms
step:1994/2110 train_time:117913ms step_avg:59.13ms
step:1995/2110 train_time:118001ms step_avg:59.15ms
step:1996/2110 train_time:118089ms step_avg:59.16ms
step:1997/2110 train_time:118177ms step_avg:59.18ms
step:1998/2110 train_time:118264ms step_avg:59.19ms
step:1999/2110 train_time:118352ms step_avg:59.21ms
step:2000/2110 train_time:118441ms step_avg:59.22ms
step:2000/2110 val_loss:3.3054 train_time:118530ms step_avg:59.26ms
step:2001/2110 train_time:118566ms step_avg:59.25ms
step:2002/2110 train_time:118623ms step_avg:59.25ms
step:2003/2110 train_time:118718ms step_avg:59.27ms
step:2004/2110 train_time:118806ms step_avg:59.28ms
step:2005/2110 train_time:118895ms step_avg:59.30ms
step:2006/2110 train_time:118982ms step_avg:59.31ms
step:2007/2110 train_time:119069ms step_avg:59.33ms
step:2008/2110 train_time:119156ms step_avg:59.34ms
step:2009/2110 train_time:119242ms step_avg:59.35ms
step:2010/2110 train_time:119329ms step_avg:59.37ms
step:2011/2110 train_time:119416ms step_avg:59.38ms
step:2012/2110 train_time:119506ms step_avg:59.40ms
step:2013/2110 train_time:119597ms step_avg:59.41ms
step:2014/2110 train_time:119687ms step_avg:59.43ms
step:2015/2110 train_time:119776ms step_avg:59.44ms
step:2016/2110 train_time:119863ms step_avg:59.46ms
step:2017/2110 train_time:119953ms step_avg:59.47ms
step:2018/2110 train_time:120039ms step_avg:59.48ms
step:2019/2110 train_time:120127ms step_avg:59.50ms
step:2020/2110 train_time:120214ms step_avg:59.51ms
step:2021/2110 train_time:120300ms step_avg:59.52ms
step:2022/2110 train_time:120388ms step_avg:59.54ms
step:2023/2110 train_time:120476ms step_avg:59.55ms
step:2024/2110 train_time:120566ms step_avg:59.57ms
step:2025/2110 train_time:120655ms step_avg:59.58ms
step:2026/2110 train_time:120745ms step_avg:59.60ms
step:2027/2110 train_time:120833ms step_avg:59.61ms
step:2028/2110 train_time:120920ms step_avg:59.63ms
step:2029/2110 train_time:121008ms step_avg:59.64ms
step:2030/2110 train_time:121095ms step_avg:59.65ms
step:2031/2110 train_time:121182ms step_avg:59.67ms
step:2032/2110 train_time:121270ms step_avg:59.68ms
step:2033/2110 train_time:121357ms step_avg:59.69ms
step:2034/2110 train_time:121445ms step_avg:59.71ms
step:2035/2110 train_time:121535ms step_avg:59.72ms
step:2036/2110 train_time:121623ms step_avg:59.74ms
step:2037/2110 train_time:121712ms step_avg:59.75ms
step:2038/2110 train_time:121799ms step_avg:59.76ms
step:2039/2110 train_time:121888ms step_avg:59.78ms
step:2040/2110 train_time:121975ms step_avg:59.79ms
step:2041/2110 train_time:122063ms step_avg:59.81ms
step:2042/2110 train_time:122150ms step_avg:59.82ms
step:2043/2110 train_time:122237ms step_avg:59.83ms
step:2044/2110 train_time:122324ms step_avg:59.85ms
step:2045/2110 train_time:122412ms step_avg:59.86ms
step:2046/2110 train_time:122499ms step_avg:59.87ms
step:2047/2110 train_time:122589ms step_avg:59.89ms
step:2048/2110 train_time:122676ms step_avg:59.90ms
step:2049/2110 train_time:122765ms step_avg:59.91ms
step:2050/2110 train_time:122855ms step_avg:59.93ms
step:2051/2110 train_time:122942ms step_avg:59.94ms
step:2052/2110 train_time:123030ms step_avg:59.96ms
step:2053/2110 train_time:123118ms step_avg:59.97ms
step:2054/2110 train_time:123205ms step_avg:59.98ms
step:2055/2110 train_time:123293ms step_avg:60.00ms
step:2056/2110 train_time:123380ms step_avg:60.01ms
step:2057/2110 train_time:123468ms step_avg:60.02ms
step:2058/2110 train_time:123556ms step_avg:60.04ms
step:2059/2110 train_time:123644ms step_avg:60.05ms
step:2060/2110 train_time:123733ms step_avg:60.06ms
step:2061/2110 train_time:123820ms step_avg:60.08ms
step:2062/2110 train_time:123908ms step_avg:60.09ms
step:2063/2110 train_time:123997ms step_avg:60.11ms
step:2064/2110 train_time:124085ms step_avg:60.12ms
step:2065/2110 train_time:124172ms step_avg:60.13ms
step:2066/2110 train_time:124260ms step_avg:60.15ms
step:2067/2110 train_time:124348ms step_avg:60.16ms
step:2068/2110 train_time:124435ms step_avg:60.17ms
step:2069/2110 train_time:124525ms step_avg:60.19ms
step:2070/2110 train_time:124613ms step_avg:60.20ms
step:2071/2110 train_time:124701ms step_avg:60.21ms
step:2072/2110 train_time:124791ms step_avg:60.23ms
step:2073/2110 train_time:124878ms step_avg:60.24ms
step:2074/2110 train_time:124967ms step_avg:60.25ms
step:2075/2110 train_time:125055ms step_avg:60.27ms
step:2076/2110 train_time:125143ms step_avg:60.28ms
step:2077/2110 train_time:125231ms step_avg:60.29ms
step:2078/2110 train_time:125319ms step_avg:60.31ms
step:2079/2110 train_time:125408ms step_avg:60.32ms
step:2080/2110 train_time:125495ms step_avg:60.33ms
step:2081/2110 train_time:125583ms step_avg:60.35ms
step:2082/2110 train_time:125671ms step_avg:60.36ms
step:2083/2110 train_time:125761ms step_avg:60.37ms
step:2084/2110 train_time:125849ms step_avg:60.39ms
step:2085/2110 train_time:125937ms step_avg:60.40ms
step:2086/2110 train_time:126026ms step_avg:60.41ms
step:2087/2110 train_time:126114ms step_avg:60.43ms
step:2088/2110 train_time:126201ms step_avg:60.44ms
step:2089/2110 train_time:126290ms step_avg:60.45ms
step:2090/2110 train_time:126378ms step_avg:60.47ms
step:2091/2110 train_time:126466ms step_avg:60.48ms
step:2092/2110 train_time:126556ms step_avg:60.50ms
step:2093/2110 train_time:126642ms step_avg:60.51ms
step:2094/2110 train_time:126731ms step_avg:60.52ms
step:2095/2110 train_time:126818ms step_avg:60.53ms
step:2096/2110 train_time:126906ms step_avg:60.55ms
step:2097/2110 train_time:126994ms step_avg:60.56ms
step:2098/2110 train_time:127083ms step_avg:60.57ms
step:2099/2110 train_time:127171ms step_avg:60.59ms
step:2100/2110 train_time:127258ms step_avg:60.60ms
step:2101/2110 train_time:127348ms step_avg:60.61ms
step:2102/2110 train_time:127435ms step_avg:60.63ms
step:2103/2110 train_time:127524ms step_avg:60.64ms
step:2104/2110 train_time:127612ms step_avg:60.65ms
step:2105/2110 train_time:127700ms step_avg:60.66ms
step:2106/2110 train_time:127788ms step_avg:60.68ms
step:2107/2110 train_time:127876ms step_avg:60.69ms
step:2108/2110 train_time:127964ms step_avg:60.70ms
step:2109/2110 train_time:128052ms step_avg:60.72ms
step:2110/2110 train_time:128140ms step_avg:60.73ms
step:2110/2110 val_loss:3.2815 train_time:128230ms step_avg:60.77ms
peak memory allocated: 29892 MiB reserved: 44516 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 15 20:23:35 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   34C    P0            123W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   35C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   35C    P0            119W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   35C    P0            119W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              88      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              88      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2110 train_time:111ms step_avg:111.24ms
step:2/2110 train_time:144ms step_avg:71.90ms
step:3/2110 train_time:172ms step_avg:57.46ms
step:4/2110 train_time:202ms step_avg:50.58ms
step:5/2110 train_time:230ms step_avg:45.92ms
step:6/2110 train_time:427ms step_avg:71.23ms
step:7/2110 train_time:598ms step_avg:85.36ms
step:8/2110 train_time:630ms step_avg:78.80ms
step:9/2110 train_time:663ms step_avg:73.63ms
step:10/2110 train_time:696ms step_avg:69.56ms
step:11/2110 train_time:729ms step_avg:66.27ms
step:12/2110 train_time:764ms step_avg:63.67ms
step:13/2110 train_time:797ms step_avg:61.31ms
step:14/2110 train_time:830ms step_avg:59.31ms
step:15/2110 train_time:861ms step_avg:57.39ms
step:16/2110 train_time:894ms step_avg:55.85ms
step:17/2110 train_time:927ms step_avg:54.51ms
step:18/2110 train_time:963ms step_avg:53.49ms
step:19/2110 train_time:993ms step_avg:52.27ms
step:20/2110 train_time:1026ms step_avg:51.29ms
step:21/2110 train_time:1059ms step_avg:50.44ms
step:22/2110 train_time:1092ms step_avg:49.65ms
step:23/2110 train_time:1126ms step_avg:48.94ms
step:24/2110 train_time:1158ms step_avg:48.26ms
step:25/2110 train_time:1192ms step_avg:47.67ms
step:26/2110 train_time:1224ms step_avg:47.10ms
step:27/2110 train_time:1258ms step_avg:46.59ms
step:28/2110 train_time:1294ms step_avg:46.22ms
step:29/2110 train_time:1325ms step_avg:45.69ms
step:30/2110 train_time:1361ms step_avg:45.35ms
step:31/2110 train_time:1392ms step_avg:44.91ms
step:32/2110 train_time:1426ms step_avg:44.55ms
step:33/2110 train_time:1457ms step_avg:44.15ms
step:34/2110 train_time:1491ms step_avg:43.85ms
step:35/2110 train_time:1524ms step_avg:43.56ms
step:36/2110 train_time:1559ms step_avg:43.31ms
step:37/2110 train_time:1593ms step_avg:43.05ms
step:38/2110 train_time:1626ms step_avg:42.79ms
step:39/2110 train_time:1660ms step_avg:42.56ms
step:40/2110 train_time:1693ms step_avg:42.34ms
step:41/2110 train_time:1727ms step_avg:42.12ms
step:42/2110 train_time:1761ms step_avg:41.93ms
step:43/2110 train_time:1794ms step_avg:41.72ms
step:44/2110 train_time:1827ms step_avg:41.52ms
step:45/2110 train_time:1861ms step_avg:41.35ms
step:46/2110 train_time:1894ms step_avg:41.17ms
step:47/2110 train_time:1927ms step_avg:41.00ms
step:48/2110 train_time:1960ms step_avg:40.84ms
step:49/2110 train_time:1993ms step_avg:40.68ms
step:50/2110 train_time:2027ms step_avg:40.53ms
step:51/2110 train_time:2061ms step_avg:40.41ms
step:52/2110 train_time:2092ms step_avg:40.24ms
step:53/2110 train_time:2126ms step_avg:40.11ms
step:54/2110 train_time:2159ms step_avg:39.97ms
step:55/2110 train_time:2193ms step_avg:39.86ms
step:56/2110 train_time:2226ms step_avg:39.74ms
step:57/2110 train_time:2258ms step_avg:39.62ms
step:58/2110 train_time:2291ms step_avg:39.50ms
step:59/2110 train_time:2324ms step_avg:39.40ms
step:60/2110 train_time:2357ms step_avg:39.28ms
step:61/2110 train_time:2390ms step_avg:39.18ms
step:62/2110 train_time:2423ms step_avg:39.08ms
step:63/2110 train_time:2456ms step_avg:38.99ms
step:64/2110 train_time:2490ms step_avg:38.90ms
step:65/2110 train_time:2523ms step_avg:38.81ms
step:66/2110 train_time:2557ms step_avg:38.74ms
step:67/2110 train_time:2590ms step_avg:38.66ms
step:68/2110 train_time:2623ms step_avg:38.57ms
step:69/2110 train_time:2656ms step_avg:38.50ms
step:70/2110 train_time:2690ms step_avg:38.42ms
step:71/2110 train_time:2723ms step_avg:38.35ms
step:72/2110 train_time:2756ms step_avg:38.28ms
step:73/2110 train_time:2790ms step_avg:38.22ms
step:74/2110 train_time:2823ms step_avg:38.15ms
step:75/2110 train_time:2857ms step_avg:38.09ms
step:76/2110 train_time:2890ms step_avg:38.02ms
step:77/2110 train_time:2925ms step_avg:37.99ms
step:78/2110 train_time:2958ms step_avg:37.92ms
step:79/2110 train_time:2990ms step_avg:37.85ms
step:80/2110 train_time:3023ms step_avg:37.78ms
step:81/2110 train_time:3056ms step_avg:37.73ms
step:82/2110 train_time:3088ms step_avg:37.66ms
step:83/2110 train_time:3122ms step_avg:37.61ms
step:84/2110 train_time:3155ms step_avg:37.56ms
step:85/2110 train_time:3188ms step_avg:37.50ms
step:86/2110 train_time:3221ms step_avg:37.46ms
step:87/2110 train_time:3254ms step_avg:37.40ms
step:88/2110 train_time:3287ms step_avg:37.35ms
step:89/2110 train_time:3320ms step_avg:37.30ms
step:90/2110 train_time:3353ms step_avg:37.25ms
step:91/2110 train_time:3386ms step_avg:37.20ms
step:92/2110 train_time:3418ms step_avg:37.16ms
step:93/2110 train_time:3452ms step_avg:37.12ms
step:94/2110 train_time:3485ms step_avg:37.07ms
step:95/2110 train_time:3518ms step_avg:37.03ms
step:96/2110 train_time:3553ms step_avg:37.01ms
step:97/2110 train_time:3586ms step_avg:36.97ms
step:98/2110 train_time:3620ms step_avg:36.94ms
step:99/2110 train_time:3652ms step_avg:36.89ms
step:100/2110 train_time:3686ms step_avg:36.86ms
step:101/2110 train_time:3719ms step_avg:36.82ms
step:102/2110 train_time:3753ms step_avg:36.80ms
step:103/2110 train_time:3784ms step_avg:36.74ms
step:104/2110 train_time:3820ms step_avg:36.73ms
step:105/2110 train_time:3853ms step_avg:36.69ms
step:106/2110 train_time:3887ms step_avg:36.67ms
step:107/2110 train_time:3920ms step_avg:36.64ms
step:108/2110 train_time:3952ms step_avg:36.59ms
step:109/2110 train_time:3984ms step_avg:36.55ms
step:110/2110 train_time:4018ms step_avg:36.52ms
step:111/2110 train_time:4050ms step_avg:36.48ms
step:112/2110 train_time:4086ms step_avg:36.48ms
step:113/2110 train_time:4117ms step_avg:36.44ms
step:114/2110 train_time:4151ms step_avg:36.41ms
step:115/2110 train_time:4183ms step_avg:36.37ms
step:116/2110 train_time:4216ms step_avg:36.34ms
step:117/2110 train_time:4249ms step_avg:36.32ms
step:118/2110 train_time:4283ms step_avg:36.30ms
step:119/2110 train_time:4315ms step_avg:36.26ms
step:120/2110 train_time:4349ms step_avg:36.24ms
step:121/2110 train_time:4380ms step_avg:36.20ms
step:122/2110 train_time:4415ms step_avg:36.19ms
step:123/2110 train_time:4447ms step_avg:36.15ms
step:124/2110 train_time:4482ms step_avg:36.15ms
step:125/2110 train_time:4512ms step_avg:36.10ms
step:126/2110 train_time:4546ms step_avg:36.08ms
step:127/2110 train_time:4579ms step_avg:36.05ms
step:128/2110 train_time:4613ms step_avg:36.04ms
step:129/2110 train_time:4646ms step_avg:36.01ms
step:130/2110 train_time:4679ms step_avg:35.99ms
step:131/2110 train_time:4712ms step_avg:35.97ms
step:132/2110 train_time:4747ms step_avg:35.96ms
step:133/2110 train_time:4779ms step_avg:35.93ms
step:134/2110 train_time:4813ms step_avg:35.92ms
step:135/2110 train_time:4844ms step_avg:35.88ms
step:136/2110 train_time:4878ms step_avg:35.87ms
step:137/2110 train_time:4911ms step_avg:35.85ms
step:138/2110 train_time:4944ms step_avg:35.83ms
step:139/2110 train_time:4977ms step_avg:35.81ms
step:140/2110 train_time:5012ms step_avg:35.80ms
step:141/2110 train_time:5045ms step_avg:35.78ms
step:142/2110 train_time:5077ms step_avg:35.76ms
step:143/2110 train_time:5108ms step_avg:35.72ms
step:144/2110 train_time:5141ms step_avg:35.70ms
step:145/2110 train_time:5174ms step_avg:35.68ms
step:146/2110 train_time:5207ms step_avg:35.66ms
step:147/2110 train_time:5240ms step_avg:35.65ms
step:148/2110 train_time:5273ms step_avg:35.63ms
step:149/2110 train_time:5307ms step_avg:35.62ms
step:150/2110 train_time:5339ms step_avg:35.60ms
step:151/2110 train_time:5372ms step_avg:35.58ms
step:152/2110 train_time:5406ms step_avg:35.57ms
step:153/2110 train_time:5438ms step_avg:35.55ms
step:154/2110 train_time:5473ms step_avg:35.54ms
step:155/2110 train_time:5506ms step_avg:35.52ms
step:156/2110 train_time:5539ms step_avg:35.51ms
step:157/2110 train_time:5571ms step_avg:35.48ms
step:158/2110 train_time:5606ms step_avg:35.48ms
step:159/2110 train_time:5637ms step_avg:35.45ms
step:160/2110 train_time:5671ms step_avg:35.44ms
step:161/2110 train_time:5703ms step_avg:35.42ms
step:162/2110 train_time:5738ms step_avg:35.42ms
step:163/2110 train_time:5769ms step_avg:35.40ms
step:164/2110 train_time:5803ms step_avg:35.38ms
step:165/2110 train_time:5835ms step_avg:35.36ms
step:166/2110 train_time:5869ms step_avg:35.36ms
step:167/2110 train_time:5901ms step_avg:35.34ms
step:168/2110 train_time:5935ms step_avg:35.32ms
step:169/2110 train_time:5967ms step_avg:35.31ms
step:170/2110 train_time:6002ms step_avg:35.31ms
step:171/2110 train_time:6035ms step_avg:35.29ms
step:172/2110 train_time:6070ms step_avg:35.29ms
step:173/2110 train_time:6100ms step_avg:35.26ms
step:174/2110 train_time:6135ms step_avg:35.26ms
step:175/2110 train_time:6166ms step_avg:35.23ms
step:176/2110 train_time:6202ms step_avg:35.24ms
step:177/2110 train_time:6232ms step_avg:35.21ms
step:178/2110 train_time:6266ms step_avg:35.20ms
step:179/2110 train_time:6298ms step_avg:35.19ms
step:180/2110 train_time:6332ms step_avg:35.18ms
step:181/2110 train_time:6367ms step_avg:35.17ms
step:182/2110 train_time:6399ms step_avg:35.16ms
step:183/2110 train_time:6430ms step_avg:35.14ms
step:184/2110 train_time:6463ms step_avg:35.13ms
step:185/2110 train_time:6497ms step_avg:35.12ms
step:186/2110 train_time:6530ms step_avg:35.11ms
step:187/2110 train_time:6563ms step_avg:35.09ms
step:188/2110 train_time:6597ms step_avg:35.09ms
step:189/2110 train_time:6630ms step_avg:35.08ms
step:190/2110 train_time:6666ms step_avg:35.09ms
step:191/2110 train_time:6695ms step_avg:35.05ms
step:192/2110 train_time:6726ms step_avg:35.03ms
step:193/2110 train_time:6761ms step_avg:35.03ms
step:194/2110 train_time:6797ms step_avg:35.04ms
step:195/2110 train_time:6827ms step_avg:35.01ms
step:196/2110 train_time:6862ms step_avg:35.01ms
step:197/2110 train_time:6895ms step_avg:35.00ms
step:198/2110 train_time:6930ms step_avg:35.00ms
step:199/2110 train_time:6962ms step_avg:34.99ms
step:200/2110 train_time:6991ms step_avg:34.95ms
step:201/2110 train_time:7023ms step_avg:34.94ms
step:202/2110 train_time:7059ms step_avg:34.94ms
step:203/2110 train_time:7092ms step_avg:34.93ms
step:204/2110 train_time:7125ms step_avg:34.93ms
step:205/2110 train_time:7156ms step_avg:34.91ms
step:206/2110 train_time:7191ms step_avg:34.91ms
step:207/2110 train_time:7224ms step_avg:34.90ms
step:208/2110 train_time:7256ms step_avg:34.89ms
step:209/2110 train_time:7289ms step_avg:34.87ms
step:210/2110 train_time:7325ms step_avg:34.88ms
step:211/2110 train_time:7353ms step_avg:34.85ms
step:212/2110 train_time:7389ms step_avg:34.85ms
step:213/2110 train_time:7421ms step_avg:34.84ms
step:214/2110 train_time:7454ms step_avg:34.83ms
step:215/2110 train_time:7486ms step_avg:34.82ms
step:216/2110 train_time:7521ms step_avg:34.82ms
step:217/2110 train_time:7553ms step_avg:34.81ms
step:218/2110 train_time:7584ms step_avg:34.79ms
step:219/2110 train_time:7619ms step_avg:34.79ms
step:220/2110 train_time:7653ms step_avg:34.78ms
step:221/2110 train_time:7683ms step_avg:34.77ms
step:222/2110 train_time:7717ms step_avg:34.76ms
step:223/2110 train_time:7749ms step_avg:34.75ms
step:224/2110 train_time:7783ms step_avg:34.75ms
step:225/2110 train_time:7816ms step_avg:34.74ms
step:226/2110 train_time:7851ms step_avg:34.74ms
step:227/2110 train_time:7882ms step_avg:34.72ms
step:228/2110 train_time:7916ms step_avg:34.72ms
step:229/2110 train_time:7947ms step_avg:34.70ms
step:230/2110 train_time:7981ms step_avg:34.70ms
step:231/2110 train_time:8014ms step_avg:34.69ms
step:232/2110 train_time:8048ms step_avg:34.69ms
step:233/2110 train_time:8080ms step_avg:34.68ms
step:234/2110 train_time:8115ms step_avg:34.68ms
step:235/2110 train_time:8147ms step_avg:34.67ms
step:236/2110 train_time:8180ms step_avg:34.66ms
step:237/2110 train_time:8212ms step_avg:34.65ms
step:238/2110 train_time:8245ms step_avg:34.64ms
step:239/2110 train_time:8278ms step_avg:34.64ms
step:240/2110 train_time:8312ms step_avg:34.63ms
step:241/2110 train_time:8344ms step_avg:34.62ms
step:242/2110 train_time:8378ms step_avg:34.62ms
step:243/2110 train_time:8410ms step_avg:34.61ms
step:244/2110 train_time:8445ms step_avg:34.61ms
step:245/2110 train_time:8475ms step_avg:34.59ms
step:246/2110 train_time:8509ms step_avg:34.59ms
step:247/2110 train_time:8543ms step_avg:34.59ms
step:248/2110 train_time:8576ms step_avg:34.58ms
step:249/2110 train_time:8608ms step_avg:34.57ms
step:250/2110 train_time:8641ms step_avg:34.57ms
step:250/2110 val_loss:4.2990 train_time:8675ms step_avg:34.70ms
step:251/2110 train_time:8711ms step_avg:34.70ms
step:252/2110 train_time:8742ms step_avg:34.69ms
step:253/2110 train_time:8767ms step_avg:34.65ms
step:254/2110 train_time:8795ms step_avg:34.63ms
step:255/2110 train_time:8835ms step_avg:34.65ms
step:256/2110 train_time:8874ms step_avg:34.66ms
step:257/2110 train_time:8915ms step_avg:34.69ms
step:258/2110 train_time:8954ms step_avg:34.71ms
step:259/2110 train_time:8994ms step_avg:34.73ms
step:260/2110 train_time:9033ms step_avg:34.74ms
step:261/2110 train_time:9075ms step_avg:34.77ms
step:262/2110 train_time:9113ms step_avg:34.78ms
step:263/2110 train_time:9155ms step_avg:34.81ms
step:264/2110 train_time:9193ms step_avg:34.82ms
step:265/2110 train_time:9236ms step_avg:34.85ms
step:266/2110 train_time:9273ms step_avg:34.86ms
step:267/2110 train_time:9315ms step_avg:34.89ms
step:268/2110 train_time:9354ms step_avg:34.90ms
step:269/2110 train_time:9395ms step_avg:34.92ms
step:270/2110 train_time:9434ms step_avg:34.94ms
step:271/2110 train_time:9474ms step_avg:34.96ms
step:272/2110 train_time:9513ms step_avg:34.98ms
step:273/2110 train_time:9553ms step_avg:34.99ms
step:274/2110 train_time:9592ms step_avg:35.01ms
step:275/2110 train_time:9633ms step_avg:35.03ms
step:276/2110 train_time:9672ms step_avg:35.04ms
step:277/2110 train_time:9713ms step_avg:35.07ms
step:278/2110 train_time:9752ms step_avg:35.08ms
step:279/2110 train_time:9793ms step_avg:35.10ms
step:280/2110 train_time:9833ms step_avg:35.12ms
step:281/2110 train_time:9874ms step_avg:35.14ms
step:282/2110 train_time:9913ms step_avg:35.15ms
step:283/2110 train_time:9954ms step_avg:35.17ms
step:284/2110 train_time:9994ms step_avg:35.19ms
step:285/2110 train_time:10036ms step_avg:35.22ms
step:286/2110 train_time:10074ms step_avg:35.22ms
step:287/2110 train_time:10113ms step_avg:35.24ms
step:288/2110 train_time:10152ms step_avg:35.25ms
step:289/2110 train_time:10192ms step_avg:35.27ms
step:290/2110 train_time:10226ms step_avg:35.26ms
step:291/2110 train_time:10257ms step_avg:35.25ms
step:292/2110 train_time:10291ms step_avg:35.24ms
step:293/2110 train_time:10324ms step_avg:35.24ms
step:294/2110 train_time:10358ms step_avg:35.23ms
step:295/2110 train_time:10391ms step_avg:35.22ms
step:296/2110 train_time:10423ms step_avg:35.21ms
step:297/2110 train_time:10455ms step_avg:35.20ms
step:298/2110 train_time:10489ms step_avg:35.20ms
step:299/2110 train_time:10520ms step_avg:35.18ms
step:300/2110 train_time:10554ms step_avg:35.18ms
step:301/2110 train_time:10585ms step_avg:35.17ms
step:302/2110 train_time:10618ms step_avg:35.16ms
step:303/2110 train_time:10650ms step_avg:35.15ms
step:304/2110 train_time:10685ms step_avg:35.15ms
step:305/2110 train_time:10718ms step_avg:35.14ms
step:306/2110 train_time:10752ms step_avg:35.14ms
step:307/2110 train_time:10785ms step_avg:35.13ms
step:308/2110 train_time:10820ms step_avg:35.13ms
step:309/2110 train_time:10847ms step_avg:35.10ms
step:310/2110 train_time:10880ms step_avg:35.10ms
step:311/2110 train_time:10913ms step_avg:35.09ms
step:312/2110 train_time:10950ms step_avg:35.09ms
step:313/2110 train_time:10982ms step_avg:35.08ms
step:314/2110 train_time:11015ms step_avg:35.08ms
step:315/2110 train_time:11047ms step_avg:35.07ms
step:316/2110 train_time:11082ms step_avg:35.07ms
step:317/2110 train_time:11116ms step_avg:35.07ms
step:318/2110 train_time:11151ms step_avg:35.07ms
step:319/2110 train_time:11180ms step_avg:35.05ms
step:320/2110 train_time:11215ms step_avg:35.05ms
step:321/2110 train_time:11247ms step_avg:35.04ms
step:322/2110 train_time:11281ms step_avg:35.03ms
step:323/2110 train_time:11312ms step_avg:35.02ms
step:324/2110 train_time:11346ms step_avg:35.02ms
step:325/2110 train_time:11379ms step_avg:35.01ms
step:326/2110 train_time:11411ms step_avg:35.00ms
step:327/2110 train_time:11445ms step_avg:35.00ms
step:328/2110 train_time:11479ms step_avg:35.00ms
step:329/2110 train_time:11511ms step_avg:34.99ms
step:330/2110 train_time:11545ms step_avg:34.99ms
step:331/2110 train_time:11576ms step_avg:34.97ms
step:332/2110 train_time:11609ms step_avg:34.97ms
step:333/2110 train_time:11642ms step_avg:34.96ms
step:334/2110 train_time:11674ms step_avg:34.95ms
step:335/2110 train_time:11707ms step_avg:34.95ms
step:336/2110 train_time:11741ms step_avg:34.94ms
step:337/2110 train_time:11773ms step_avg:34.93ms
step:338/2110 train_time:11806ms step_avg:34.93ms
step:339/2110 train_time:11838ms step_avg:34.92ms
step:340/2110 train_time:11871ms step_avg:34.91ms
step:341/2110 train_time:11905ms step_avg:34.91ms
step:342/2110 train_time:11937ms step_avg:34.90ms
step:343/2110 train_time:11970ms step_avg:34.90ms
step:344/2110 train_time:12004ms step_avg:34.90ms
step:345/2110 train_time:12037ms step_avg:34.89ms
step:346/2110 train_time:12071ms step_avg:34.89ms
step:347/2110 train_time:12103ms step_avg:34.88ms
step:348/2110 train_time:12136ms step_avg:34.87ms
step:349/2110 train_time:12170ms step_avg:34.87ms
step:350/2110 train_time:12202ms step_avg:34.86ms
step:351/2110 train_time:12236ms step_avg:34.86ms
step:352/2110 train_time:12268ms step_avg:34.85ms
step:353/2110 train_time:12302ms step_avg:34.85ms
step:354/2110 train_time:12335ms step_avg:34.84ms
step:355/2110 train_time:12368ms step_avg:34.84ms
step:356/2110 train_time:12402ms step_avg:34.84ms
step:357/2110 train_time:12434ms step_avg:34.83ms
step:358/2110 train_time:12467ms step_avg:34.82ms
step:359/2110 train_time:12500ms step_avg:34.82ms
step:360/2110 train_time:12534ms step_avg:34.82ms
step:361/2110 train_time:12566ms step_avg:34.81ms
step:362/2110 train_time:12598ms step_avg:34.80ms
step:363/2110 train_time:12632ms step_avg:34.80ms
step:364/2110 train_time:12664ms step_avg:34.79ms
step:365/2110 train_time:12697ms step_avg:34.79ms
step:366/2110 train_time:12730ms step_avg:34.78ms
step:367/2110 train_time:12764ms step_avg:34.78ms
step:368/2110 train_time:12799ms step_avg:34.78ms
step:369/2110 train_time:12831ms step_avg:34.77ms
step:370/2110 train_time:12864ms step_avg:34.77ms
step:371/2110 train_time:12899ms step_avg:34.77ms
step:372/2110 train_time:12934ms step_avg:34.77ms
step:373/2110 train_time:12961ms step_avg:34.75ms
step:374/2110 train_time:12995ms step_avg:34.75ms
step:375/2110 train_time:13029ms step_avg:34.74ms
step:376/2110 train_time:13062ms step_avg:34.74ms
step:377/2110 train_time:13094ms step_avg:34.73ms
step:378/2110 train_time:13129ms step_avg:34.73ms
step:379/2110 train_time:13162ms step_avg:34.73ms
step:380/2110 train_time:13196ms step_avg:34.73ms
step:381/2110 train_time:13228ms step_avg:34.72ms
step:382/2110 train_time:13260ms step_avg:34.71ms
step:383/2110 train_time:13294ms step_avg:34.71ms
step:384/2110 train_time:13327ms step_avg:34.71ms
step:385/2110 train_time:13359ms step_avg:34.70ms
step:386/2110 train_time:13392ms step_avg:34.69ms
step:387/2110 train_time:13425ms step_avg:34.69ms
step:388/2110 train_time:13460ms step_avg:34.69ms
step:389/2110 train_time:13491ms step_avg:34.68ms
step:390/2110 train_time:13525ms step_avg:34.68ms
step:391/2110 train_time:13556ms step_avg:34.67ms
step:392/2110 train_time:13592ms step_avg:34.67ms
step:393/2110 train_time:13624ms step_avg:34.67ms
step:394/2110 train_time:13658ms step_avg:34.66ms
step:395/2110 train_time:13689ms step_avg:34.66ms
step:396/2110 train_time:13723ms step_avg:34.65ms
step:397/2110 train_time:13754ms step_avg:34.65ms
step:398/2110 train_time:13787ms step_avg:34.64ms
step:399/2110 train_time:13820ms step_avg:34.64ms
step:400/2110 train_time:13854ms step_avg:34.64ms
step:401/2110 train_time:13886ms step_avg:34.63ms
step:402/2110 train_time:13919ms step_avg:34.62ms
step:403/2110 train_time:13951ms step_avg:34.62ms
step:404/2110 train_time:13987ms step_avg:34.62ms
step:405/2110 train_time:14018ms step_avg:34.61ms
step:406/2110 train_time:14051ms step_avg:34.61ms
step:407/2110 train_time:14083ms step_avg:34.60ms
step:408/2110 train_time:14118ms step_avg:34.60ms
step:409/2110 train_time:14149ms step_avg:34.59ms
step:410/2110 train_time:14183ms step_avg:34.59ms
step:411/2110 train_time:14215ms step_avg:34.59ms
step:412/2110 train_time:14249ms step_avg:34.59ms
step:413/2110 train_time:14282ms step_avg:34.58ms
step:414/2110 train_time:14317ms step_avg:34.58ms
step:415/2110 train_time:14348ms step_avg:34.57ms
step:416/2110 train_time:14382ms step_avg:34.57ms
step:417/2110 train_time:14414ms step_avg:34.57ms
step:418/2110 train_time:14449ms step_avg:34.57ms
step:419/2110 train_time:14480ms step_avg:34.56ms
step:420/2110 train_time:14514ms step_avg:34.56ms
step:421/2110 train_time:14546ms step_avg:34.55ms
step:422/2110 train_time:14581ms step_avg:34.55ms
step:423/2110 train_time:14614ms step_avg:34.55ms
step:424/2110 train_time:14652ms step_avg:34.56ms
step:425/2110 train_time:14682ms step_avg:34.55ms
step:426/2110 train_time:14714ms step_avg:34.54ms
step:427/2110 train_time:14747ms step_avg:34.54ms
step:428/2110 train_time:14780ms step_avg:34.53ms
step:429/2110 train_time:14812ms step_avg:34.53ms
step:430/2110 train_time:14846ms step_avg:34.53ms
step:431/2110 train_time:14877ms step_avg:34.52ms
step:432/2110 train_time:14912ms step_avg:34.52ms
step:433/2110 train_time:14944ms step_avg:34.51ms
step:434/2110 train_time:14977ms step_avg:34.51ms
step:435/2110 train_time:15008ms step_avg:34.50ms
step:436/2110 train_time:15042ms step_avg:34.50ms
step:437/2110 train_time:15077ms step_avg:34.50ms
step:438/2110 train_time:15108ms step_avg:34.49ms
step:439/2110 train_time:15141ms step_avg:34.49ms
step:440/2110 train_time:15174ms step_avg:34.49ms
step:441/2110 train_time:15207ms step_avg:34.48ms
step:442/2110 train_time:15240ms step_avg:34.48ms
step:443/2110 train_time:15274ms step_avg:34.48ms
step:444/2110 train_time:15307ms step_avg:34.48ms
step:445/2110 train_time:15339ms step_avg:34.47ms
step:446/2110 train_time:15373ms step_avg:34.47ms
step:447/2110 train_time:15406ms step_avg:34.47ms
step:448/2110 train_time:15443ms step_avg:34.47ms
step:449/2110 train_time:15473ms step_avg:34.46ms
step:450/2110 train_time:15507ms step_avg:34.46ms
step:451/2110 train_time:15539ms step_avg:34.45ms
step:452/2110 train_time:15574ms step_avg:34.46ms
step:453/2110 train_time:15607ms step_avg:34.45ms
step:454/2110 train_time:15642ms step_avg:34.45ms
step:455/2110 train_time:15674ms step_avg:34.45ms
step:456/2110 train_time:15706ms step_avg:34.44ms
step:457/2110 train_time:15739ms step_avg:34.44ms
step:458/2110 train_time:15774ms step_avg:34.44ms
step:459/2110 train_time:15804ms step_avg:34.43ms
step:460/2110 train_time:15839ms step_avg:34.43ms
step:461/2110 train_time:15870ms step_avg:34.43ms
step:462/2110 train_time:15904ms step_avg:34.42ms
step:463/2110 train_time:15938ms step_avg:34.42ms
step:464/2110 train_time:15970ms step_avg:34.42ms
step:465/2110 train_time:16002ms step_avg:34.41ms
step:466/2110 train_time:16036ms step_avg:34.41ms
step:467/2110 train_time:16070ms step_avg:34.41ms
step:468/2110 train_time:16103ms step_avg:34.41ms
step:469/2110 train_time:16136ms step_avg:34.41ms
step:470/2110 train_time:16172ms step_avg:34.41ms
step:471/2110 train_time:16203ms step_avg:34.40ms
step:472/2110 train_time:16239ms step_avg:34.40ms
step:473/2110 train_time:16272ms step_avg:34.40ms
step:474/2110 train_time:16305ms step_avg:34.40ms
step:475/2110 train_time:16337ms step_avg:34.39ms
step:476/2110 train_time:16366ms step_avg:34.38ms
step:477/2110 train_time:16400ms step_avg:34.38ms
step:478/2110 train_time:16433ms step_avg:34.38ms
step:479/2110 train_time:16466ms step_avg:34.38ms
step:480/2110 train_time:16500ms step_avg:34.38ms
step:481/2110 train_time:16532ms step_avg:34.37ms
step:482/2110 train_time:16564ms step_avg:34.37ms
step:483/2110 train_time:16598ms step_avg:34.36ms
step:484/2110 train_time:16630ms step_avg:34.36ms
step:485/2110 train_time:16664ms step_avg:34.36ms
step:486/2110 train_time:16696ms step_avg:34.35ms
step:487/2110 train_time:16730ms step_avg:34.35ms
step:488/2110 train_time:16763ms step_avg:34.35ms
step:489/2110 train_time:16797ms step_avg:34.35ms
step:490/2110 train_time:16828ms step_avg:34.34ms
step:491/2110 train_time:16862ms step_avg:34.34ms
step:492/2110 train_time:16896ms step_avg:34.34ms
step:493/2110 train_time:16931ms step_avg:34.34ms
step:494/2110 train_time:16965ms step_avg:34.34ms
step:495/2110 train_time:16995ms step_avg:34.33ms
step:496/2110 train_time:17029ms step_avg:34.33ms
step:497/2110 train_time:17059ms step_avg:34.32ms
step:498/2110 train_time:17093ms step_avg:34.32ms
step:499/2110 train_time:17125ms step_avg:34.32ms
step:500/2110 train_time:17159ms step_avg:34.32ms
step:500/2110 val_loss:4.0419 train_time:17192ms step_avg:34.38ms
step:501/2110 train_time:17224ms step_avg:34.38ms
step:502/2110 train_time:17257ms step_avg:34.38ms
step:503/2110 train_time:17287ms step_avg:34.37ms
step:504/2110 train_time:17318ms step_avg:34.36ms
step:505/2110 train_time:17346ms step_avg:34.35ms
step:506/2110 train_time:17376ms step_avg:34.34ms
step:507/2110 train_time:17404ms step_avg:34.33ms
step:508/2110 train_time:17434ms step_avg:34.32ms
step:509/2110 train_time:17470ms step_avg:34.32ms
step:510/2110 train_time:17504ms step_avg:34.32ms
step:511/2110 train_time:17536ms step_avg:34.32ms
step:512/2110 train_time:17568ms step_avg:34.31ms
step:513/2110 train_time:17600ms step_avg:34.31ms
step:514/2110 train_time:17636ms step_avg:34.31ms
step:515/2110 train_time:17667ms step_avg:34.30ms
step:516/2110 train_time:17699ms step_avg:34.30ms
step:517/2110 train_time:17733ms step_avg:34.30ms
step:518/2110 train_time:17766ms step_avg:34.30ms
step:519/2110 train_time:17798ms step_avg:34.29ms
step:520/2110 train_time:17831ms step_avg:34.29ms
step:521/2110 train_time:17864ms step_avg:34.29ms
step:522/2110 train_time:17897ms step_avg:34.28ms
step:523/2110 train_time:17931ms step_avg:34.28ms
step:524/2110 train_time:17961ms step_avg:34.28ms
step:525/2110 train_time:17996ms step_avg:34.28ms
step:526/2110 train_time:18028ms step_avg:34.27ms
step:527/2110 train_time:18059ms step_avg:34.27ms
step:528/2110 train_time:18093ms step_avg:34.27ms
step:529/2110 train_time:18125ms step_avg:34.26ms
step:530/2110 train_time:18161ms step_avg:34.27ms
step:531/2110 train_time:18192ms step_avg:34.26ms
step:532/2110 train_time:18226ms step_avg:34.26ms
step:533/2110 train_time:18260ms step_avg:34.26ms
step:534/2110 train_time:18293ms step_avg:34.26ms
step:535/2110 train_time:18326ms step_avg:34.25ms
step:536/2110 train_time:18359ms step_avg:34.25ms
step:537/2110 train_time:18392ms step_avg:34.25ms
step:538/2110 train_time:18426ms step_avg:34.25ms
step:539/2110 train_time:18459ms step_avg:34.25ms
step:540/2110 train_time:18495ms step_avg:34.25ms
step:541/2110 train_time:18524ms step_avg:34.24ms
step:542/2110 train_time:18557ms step_avg:34.24ms
step:543/2110 train_time:18590ms step_avg:34.24ms
step:544/2110 train_time:18623ms step_avg:34.23ms
step:545/2110 train_time:18655ms step_avg:34.23ms
step:546/2110 train_time:18688ms step_avg:34.23ms
step:547/2110 train_time:18721ms step_avg:34.22ms
step:548/2110 train_time:18754ms step_avg:34.22ms
step:549/2110 train_time:18787ms step_avg:34.22ms
step:550/2110 train_time:18825ms step_avg:34.23ms
step:551/2110 train_time:18853ms step_avg:34.22ms
step:552/2110 train_time:18885ms step_avg:34.21ms
step:553/2110 train_time:18918ms step_avg:34.21ms
step:554/2110 train_time:18955ms step_avg:34.22ms
step:555/2110 train_time:19002ms step_avg:34.24ms
step:556/2110 train_time:19046ms step_avg:34.26ms
step:557/2110 train_time:19093ms step_avg:34.28ms
step:558/2110 train_time:19138ms step_avg:34.30ms
step:559/2110 train_time:19187ms step_avg:34.32ms
step:560/2110 train_time:19235ms step_avg:34.35ms
step:561/2110 train_time:19281ms step_avg:34.37ms
step:562/2110 train_time:19326ms step_avg:34.39ms
step:563/2110 train_time:19372ms step_avg:34.41ms
step:564/2110 train_time:19420ms step_avg:34.43ms
step:565/2110 train_time:19467ms step_avg:34.45ms
step:566/2110 train_time:19512ms step_avg:34.47ms
step:567/2110 train_time:19559ms step_avg:34.50ms
step:568/2110 train_time:19603ms step_avg:34.51ms
step:569/2110 train_time:19650ms step_avg:34.53ms
step:570/2110 train_time:19695ms step_avg:34.55ms
step:571/2110 train_time:19742ms step_avg:34.57ms
step:572/2110 train_time:19788ms step_avg:34.59ms
step:573/2110 train_time:19836ms step_avg:34.62ms
step:574/2110 train_time:19882ms step_avg:34.64ms
step:575/2110 train_time:19930ms step_avg:34.66ms
step:576/2110 train_time:19976ms step_avg:34.68ms
step:577/2110 train_time:20022ms step_avg:34.70ms
step:578/2110 train_time:20069ms step_avg:34.72ms
step:579/2110 train_time:20117ms step_avg:34.74ms
step:580/2110 train_time:20160ms step_avg:34.76ms
step:581/2110 train_time:20208ms step_avg:34.78ms
step:582/2110 train_time:20251ms step_avg:34.80ms
step:583/2110 train_time:20299ms step_avg:34.82ms
step:584/2110 train_time:20344ms step_avg:34.84ms
step:585/2110 train_time:20391ms step_avg:34.86ms
step:586/2110 train_time:20435ms step_avg:34.87ms
step:587/2110 train_time:20482ms step_avg:34.89ms
step:588/2110 train_time:20528ms step_avg:34.91ms
step:589/2110 train_time:20576ms step_avg:34.93ms
step:590/2110 train_time:20620ms step_avg:34.95ms
step:591/2110 train_time:20667ms step_avg:34.97ms
step:592/2110 train_time:20712ms step_avg:34.99ms
step:593/2110 train_time:20759ms step_avg:35.01ms
step:594/2110 train_time:20803ms step_avg:35.02ms
step:595/2110 train_time:20851ms step_avg:35.04ms
step:596/2110 train_time:20896ms step_avg:35.06ms
step:597/2110 train_time:20943ms step_avg:35.08ms
step:598/2110 train_time:20989ms step_avg:35.10ms
step:599/2110 train_time:21035ms step_avg:35.12ms
step:600/2110 train_time:21080ms step_avg:35.13ms
step:601/2110 train_time:21127ms step_avg:35.15ms
step:602/2110 train_time:21173ms step_avg:35.17ms
step:603/2110 train_time:21219ms step_avg:35.19ms
step:604/2110 train_time:21266ms step_avg:35.21ms
step:605/2110 train_time:21313ms step_avg:35.23ms
step:606/2110 train_time:21358ms step_avg:35.24ms
step:607/2110 train_time:21404ms step_avg:35.26ms
step:608/2110 train_time:21450ms step_avg:35.28ms
step:609/2110 train_time:21496ms step_avg:35.30ms
step:610/2110 train_time:21542ms step_avg:35.31ms
step:611/2110 train_time:21589ms step_avg:35.33ms
step:612/2110 train_time:21629ms step_avg:35.34ms
step:613/2110 train_time:21669ms step_avg:35.35ms
step:614/2110 train_time:21707ms step_avg:35.35ms
step:615/2110 train_time:21747ms step_avg:35.36ms
step:616/2110 train_time:21786ms step_avg:35.37ms
step:617/2110 train_time:21829ms step_avg:35.38ms
step:618/2110 train_time:21869ms step_avg:35.39ms
step:619/2110 train_time:21911ms step_avg:35.40ms
step:620/2110 train_time:21947ms step_avg:35.40ms
step:621/2110 train_time:21983ms step_avg:35.40ms
step:622/2110 train_time:22018ms step_avg:35.40ms
step:623/2110 train_time:22053ms step_avg:35.40ms
step:624/2110 train_time:22092ms step_avg:35.40ms
step:625/2110 train_time:22131ms step_avg:35.41ms
step:626/2110 train_time:22175ms step_avg:35.42ms
step:627/2110 train_time:22216ms step_avg:35.43ms
step:628/2110 train_time:22258ms step_avg:35.44ms
step:629/2110 train_time:22297ms step_avg:35.45ms
step:630/2110 train_time:22338ms step_avg:35.46ms
step:631/2110 train_time:22379ms step_avg:35.47ms
step:632/2110 train_time:22419ms step_avg:35.47ms
step:633/2110 train_time:22460ms step_avg:35.48ms
step:634/2110 train_time:22499ms step_avg:35.49ms
step:635/2110 train_time:22539ms step_avg:35.49ms
step:636/2110 train_time:22577ms step_avg:35.50ms
step:637/2110 train_time:22617ms step_avg:35.51ms
step:638/2110 train_time:22656ms step_avg:35.51ms
step:639/2110 train_time:22697ms step_avg:35.52ms
step:640/2110 train_time:22739ms step_avg:35.53ms
step:641/2110 train_time:22780ms step_avg:35.54ms
step:642/2110 train_time:22818ms step_avg:35.54ms
step:643/2110 train_time:22859ms step_avg:35.55ms
step:644/2110 train_time:22898ms step_avg:35.56ms
step:645/2110 train_time:22937ms step_avg:35.56ms
step:646/2110 train_time:22978ms step_avg:35.57ms
step:647/2110 train_time:23018ms step_avg:35.58ms
step:648/2110 train_time:23057ms step_avg:35.58ms
step:649/2110 train_time:23098ms step_avg:35.59ms
step:650/2110 train_time:23135ms step_avg:35.59ms
step:651/2110 train_time:23178ms step_avg:35.60ms
step:652/2110 train_time:23216ms step_avg:35.61ms
step:653/2110 train_time:23258ms step_avg:35.62ms
step:654/2110 train_time:23297ms step_avg:35.62ms
step:655/2110 train_time:23339ms step_avg:35.63ms
step:656/2110 train_time:23377ms step_avg:35.64ms
step:657/2110 train_time:23419ms step_avg:35.64ms
step:658/2110 train_time:23457ms step_avg:35.65ms
step:659/2110 train_time:23498ms step_avg:35.66ms
step:660/2110 train_time:23535ms step_avg:35.66ms
step:661/2110 train_time:23577ms step_avg:35.67ms
step:662/2110 train_time:23620ms step_avg:35.68ms
step:663/2110 train_time:23661ms step_avg:35.69ms
step:664/2110 train_time:23699ms step_avg:35.69ms
step:665/2110 train_time:23741ms step_avg:35.70ms
step:666/2110 train_time:23780ms step_avg:35.71ms
step:667/2110 train_time:23821ms step_avg:35.71ms
step:668/2110 train_time:23860ms step_avg:35.72ms
step:669/2110 train_time:23901ms step_avg:35.73ms
step:670/2110 train_time:23939ms step_avg:35.73ms
step:671/2110 train_time:23979ms step_avg:35.74ms
step:672/2110 train_time:24018ms step_avg:35.74ms
step:673/2110 train_time:24058ms step_avg:35.75ms
step:674/2110 train_time:24096ms step_avg:35.75ms
step:675/2110 train_time:24137ms step_avg:35.76ms
step:676/2110 train_time:24177ms step_avg:35.77ms
step:677/2110 train_time:24219ms step_avg:35.77ms
step:678/2110 train_time:24257ms step_avg:35.78ms
step:679/2110 train_time:24298ms step_avg:35.79ms
step:680/2110 train_time:24337ms step_avg:35.79ms
step:681/2110 train_time:24377ms step_avg:35.80ms
step:682/2110 train_time:24416ms step_avg:35.80ms
step:683/2110 train_time:24458ms step_avg:35.81ms
step:684/2110 train_time:24508ms step_avg:35.83ms
step:685/2110 train_time:24550ms step_avg:35.84ms
step:686/2110 train_time:24600ms step_avg:35.86ms
step:687/2110 train_time:24642ms step_avg:35.87ms
step:688/2110 train_time:24688ms step_avg:35.88ms
step:689/2110 train_time:24725ms step_avg:35.89ms
step:690/2110 train_time:24764ms step_avg:35.89ms
step:691/2110 train_time:24806ms step_avg:35.90ms
step:692/2110 train_time:24869ms step_avg:35.94ms
step:693/2110 train_time:24927ms step_avg:35.97ms
step:694/2110 train_time:24986ms step_avg:36.00ms
step:695/2110 train_time:25044ms step_avg:36.03ms
step:696/2110 train_time:25104ms step_avg:36.07ms
step:697/2110 train_time:25160ms step_avg:36.10ms
step:698/2110 train_time:25219ms step_avg:36.13ms
step:699/2110 train_time:25276ms step_avg:36.16ms
step:700/2110 train_time:25335ms step_avg:36.19ms
step:701/2110 train_time:25393ms step_avg:36.22ms
step:702/2110 train_time:25451ms step_avg:36.25ms
step:703/2110 train_time:25509ms step_avg:36.29ms
step:704/2110 train_time:25571ms step_avg:36.32ms
step:705/2110 train_time:25633ms step_avg:36.36ms
step:706/2110 train_time:25693ms step_avg:36.39ms
step:707/2110 train_time:25754ms step_avg:36.43ms
step:708/2110 train_time:25815ms step_avg:36.46ms
step:709/2110 train_time:25875ms step_avg:36.50ms
step:710/2110 train_time:25934ms step_avg:36.53ms
step:711/2110 train_time:25993ms step_avg:36.56ms
step:712/2110 train_time:26051ms step_avg:36.59ms
step:713/2110 train_time:26110ms step_avg:36.62ms
step:714/2110 train_time:26168ms step_avg:36.65ms
step:715/2110 train_time:26226ms step_avg:36.68ms
step:716/2110 train_time:26284ms step_avg:36.71ms
step:717/2110 train_time:26343ms step_avg:36.74ms
step:718/2110 train_time:26401ms step_avg:36.77ms
step:719/2110 train_time:26460ms step_avg:36.80ms
step:720/2110 train_time:26520ms step_avg:36.83ms
step:721/2110 train_time:26581ms step_avg:36.87ms
step:722/2110 train_time:26642ms step_avg:36.90ms
step:723/2110 train_time:26703ms step_avg:36.93ms
step:724/2110 train_time:26765ms step_avg:36.97ms
step:725/2110 train_time:26825ms step_avg:37.00ms
step:726/2110 train_time:26885ms step_avg:37.03ms
step:727/2110 train_time:26945ms step_avg:37.06ms
step:728/2110 train_time:27005ms step_avg:37.10ms
step:729/2110 train_time:27064ms step_avg:37.12ms
step:730/2110 train_time:27123ms step_avg:37.15ms
step:731/2110 train_time:27182ms step_avg:37.18ms
step:732/2110 train_time:27240ms step_avg:37.21ms
step:733/2110 train_time:27298ms step_avg:37.24ms
step:734/2110 train_time:27356ms step_avg:37.27ms
step:735/2110 train_time:27415ms step_avg:37.30ms
step:736/2110 train_time:27473ms step_avg:37.33ms
step:737/2110 train_time:27532ms step_avg:37.36ms
step:738/2110 train_time:27591ms step_avg:37.39ms
step:739/2110 train_time:27652ms step_avg:37.42ms
step:740/2110 train_time:27711ms step_avg:37.45ms
step:741/2110 train_time:27772ms step_avg:37.48ms
step:742/2110 train_time:27831ms step_avg:37.51ms
step:743/2110 train_time:27891ms step_avg:37.54ms
step:744/2110 train_time:27950ms step_avg:37.57ms
step:745/2110 train_time:28010ms step_avg:37.60ms
step:746/2110 train_time:28069ms step_avg:37.63ms
step:747/2110 train_time:28128ms step_avg:37.65ms
step:748/2110 train_time:28187ms step_avg:37.68ms
step:749/2110 train_time:28246ms step_avg:37.71ms
step:750/2110 train_time:28305ms step_avg:37.74ms
step:750/2110 val_loss:3.9095 train_time:28365ms step_avg:37.82ms
step:751/2110 train_time:28408ms step_avg:37.83ms
step:752/2110 train_time:28441ms step_avg:37.82ms
step:753/2110 train_time:28488ms step_avg:37.83ms
step:754/2110 train_time:28549ms step_avg:37.86ms
step:755/2110 train_time:28609ms step_avg:37.89ms
step:756/2110 train_time:28668ms step_avg:37.92ms
step:757/2110 train_time:28728ms step_avg:37.95ms
step:758/2110 train_time:28786ms step_avg:37.98ms
step:759/2110 train_time:28845ms step_avg:38.00ms
step:760/2110 train_time:28903ms step_avg:38.03ms
step:761/2110 train_time:28961ms step_avg:38.06ms
step:762/2110 train_time:29020ms step_avg:38.08ms
step:763/2110 train_time:29077ms step_avg:38.11ms
step:764/2110 train_time:29136ms step_avg:38.14ms
step:765/2110 train_time:29194ms step_avg:38.16ms
step:766/2110 train_time:29253ms step_avg:38.19ms
step:767/2110 train_time:29312ms step_avg:38.22ms
step:768/2110 train_time:29373ms step_avg:38.25ms
step:769/2110 train_time:29434ms step_avg:38.28ms
step:770/2110 train_time:29495ms step_avg:38.30ms
step:771/2110 train_time:29556ms step_avg:38.33ms
step:772/2110 train_time:29616ms step_avg:38.36ms
step:773/2110 train_time:29675ms step_avg:38.39ms
step:774/2110 train_time:29734ms step_avg:38.42ms
step:775/2110 train_time:29793ms step_avg:38.44ms
step:776/2110 train_time:29853ms step_avg:38.47ms
step:777/2110 train_time:29911ms step_avg:38.50ms
step:778/2110 train_time:29970ms step_avg:38.52ms
step:779/2110 train_time:30028ms step_avg:38.55ms
step:780/2110 train_time:30086ms step_avg:38.57ms
step:781/2110 train_time:30145ms step_avg:38.60ms
step:782/2110 train_time:30203ms step_avg:38.62ms
step:783/2110 train_time:30262ms step_avg:38.65ms
step:784/2110 train_time:30320ms step_avg:38.67ms
step:785/2110 train_time:30380ms step_avg:38.70ms
step:786/2110 train_time:30439ms step_avg:38.73ms
step:787/2110 train_time:30500ms step_avg:38.75ms
step:788/2110 train_time:30559ms step_avg:38.78ms
step:789/2110 train_time:30620ms step_avg:38.81ms
step:790/2110 train_time:30679ms step_avg:38.83ms
step:791/2110 train_time:30739ms step_avg:38.86ms
step:792/2110 train_time:30798ms step_avg:38.89ms
step:793/2110 train_time:30857ms step_avg:38.91ms
step:794/2110 train_time:30916ms step_avg:38.94ms
step:795/2110 train_time:30975ms step_avg:38.96ms
step:796/2110 train_time:31034ms step_avg:38.99ms
step:797/2110 train_time:31093ms step_avg:39.01ms
step:798/2110 train_time:31152ms step_avg:39.04ms
step:799/2110 train_time:31211ms step_avg:39.06ms
step:800/2110 train_time:31270ms step_avg:39.09ms
step:801/2110 train_time:31331ms step_avg:39.11ms
step:802/2110 train_time:31392ms step_avg:39.14ms
step:803/2110 train_time:31449ms step_avg:39.16ms
step:804/2110 train_time:31510ms step_avg:39.19ms
step:805/2110 train_time:31569ms step_avg:39.22ms
step:806/2110 train_time:31629ms step_avg:39.24ms
step:807/2110 train_time:31687ms step_avg:39.26ms
step:808/2110 train_time:31746ms step_avg:39.29ms
step:809/2110 train_time:31805ms step_avg:39.31ms
step:810/2110 train_time:31865ms step_avg:39.34ms
step:811/2110 train_time:31923ms step_avg:39.36ms
step:812/2110 train_time:31984ms step_avg:39.39ms
step:813/2110 train_time:32041ms step_avg:39.41ms
step:814/2110 train_time:32101ms step_avg:39.44ms
step:815/2110 train_time:32159ms step_avg:39.46ms
step:816/2110 train_time:32219ms step_avg:39.48ms
step:817/2110 train_time:32277ms step_avg:39.51ms
step:818/2110 train_time:32338ms step_avg:39.53ms
step:819/2110 train_time:32396ms step_avg:39.56ms
step:820/2110 train_time:32456ms step_avg:39.58ms
step:821/2110 train_time:32517ms step_avg:39.61ms
step:822/2110 train_time:32577ms step_avg:39.63ms
step:823/2110 train_time:32635ms step_avg:39.65ms
step:824/2110 train_time:32696ms step_avg:39.68ms
step:825/2110 train_time:32754ms step_avg:39.70ms
step:826/2110 train_time:32815ms step_avg:39.73ms
step:827/2110 train_time:32873ms step_avg:39.75ms
step:828/2110 train_time:32933ms step_avg:39.77ms
step:829/2110 train_time:32992ms step_avg:39.80ms
step:830/2110 train_time:33051ms step_avg:39.82ms
step:831/2110 train_time:33108ms step_avg:39.84ms
step:832/2110 train_time:33168ms step_avg:39.87ms
step:833/2110 train_time:33227ms step_avg:39.89ms
step:834/2110 train_time:33287ms step_avg:39.91ms
step:835/2110 train_time:33345ms step_avg:39.93ms
step:836/2110 train_time:33405ms step_avg:39.96ms
step:837/2110 train_time:33463ms step_avg:39.98ms
step:838/2110 train_time:33523ms step_avg:40.00ms
step:839/2110 train_time:33581ms step_avg:40.03ms
step:840/2110 train_time:33642ms step_avg:40.05ms
step:841/2110 train_time:33700ms step_avg:40.07ms
step:842/2110 train_time:33761ms step_avg:40.10ms
step:843/2110 train_time:33819ms step_avg:40.12ms
step:844/2110 train_time:33881ms step_avg:40.14ms
step:845/2110 train_time:33938ms step_avg:40.16ms
step:846/2110 train_time:33999ms step_avg:40.19ms
step:847/2110 train_time:34057ms step_avg:40.21ms
step:848/2110 train_time:34117ms step_avg:40.23ms
step:849/2110 train_time:34175ms step_avg:40.25ms
step:850/2110 train_time:34236ms step_avg:40.28ms
step:851/2110 train_time:34293ms step_avg:40.30ms
step:852/2110 train_time:34354ms step_avg:40.32ms
step:853/2110 train_time:34413ms step_avg:40.34ms
step:854/2110 train_time:34474ms step_avg:40.37ms
step:855/2110 train_time:34531ms step_avg:40.39ms
step:856/2110 train_time:34592ms step_avg:40.41ms
step:857/2110 train_time:34649ms step_avg:40.43ms
step:858/2110 train_time:34709ms step_avg:40.45ms
step:859/2110 train_time:34768ms step_avg:40.47ms
step:860/2110 train_time:34828ms step_avg:40.50ms
step:861/2110 train_time:34886ms step_avg:40.52ms
step:862/2110 train_time:34945ms step_avg:40.54ms
step:863/2110 train_time:35004ms step_avg:40.56ms
step:864/2110 train_time:35065ms step_avg:40.58ms
step:865/2110 train_time:35122ms step_avg:40.60ms
step:866/2110 train_time:35183ms step_avg:40.63ms
step:867/2110 train_time:35240ms step_avg:40.65ms
step:868/2110 train_time:35301ms step_avg:40.67ms
step:869/2110 train_time:35360ms step_avg:40.69ms
step:870/2110 train_time:35420ms step_avg:40.71ms
step:871/2110 train_time:35478ms step_avg:40.73ms
step:872/2110 train_time:35539ms step_avg:40.76ms
step:873/2110 train_time:35597ms step_avg:40.78ms
step:874/2110 train_time:35658ms step_avg:40.80ms
step:875/2110 train_time:35716ms step_avg:40.82ms
step:876/2110 train_time:35777ms step_avg:40.84ms
step:877/2110 train_time:35835ms step_avg:40.86ms
step:878/2110 train_time:35895ms step_avg:40.88ms
step:879/2110 train_time:35954ms step_avg:40.90ms
step:880/2110 train_time:36014ms step_avg:40.92ms
step:881/2110 train_time:36072ms step_avg:40.94ms
step:882/2110 train_time:36132ms step_avg:40.97ms
step:883/2110 train_time:36189ms step_avg:40.98ms
step:884/2110 train_time:36249ms step_avg:41.01ms
step:885/2110 train_time:36306ms step_avg:41.02ms
step:886/2110 train_time:36367ms step_avg:41.05ms
step:887/2110 train_time:36424ms step_avg:41.06ms
step:888/2110 train_time:36484ms step_avg:41.09ms
step:889/2110 train_time:36542ms step_avg:41.11ms
step:890/2110 train_time:36602ms step_avg:41.13ms
step:891/2110 train_time:36661ms step_avg:41.15ms
step:892/2110 train_time:36722ms step_avg:41.17ms
step:893/2110 train_time:36780ms step_avg:41.19ms
step:894/2110 train_time:36840ms step_avg:41.21ms
step:895/2110 train_time:36898ms step_avg:41.23ms
step:896/2110 train_time:36958ms step_avg:41.25ms
step:897/2110 train_time:37017ms step_avg:41.27ms
step:898/2110 train_time:37078ms step_avg:41.29ms
step:899/2110 train_time:37136ms step_avg:41.31ms
step:900/2110 train_time:37197ms step_avg:41.33ms
step:901/2110 train_time:37255ms step_avg:41.35ms
step:902/2110 train_time:37316ms step_avg:41.37ms
step:903/2110 train_time:37374ms step_avg:41.39ms
step:904/2110 train_time:37434ms step_avg:41.41ms
step:905/2110 train_time:37492ms step_avg:41.43ms
step:906/2110 train_time:37552ms step_avg:41.45ms
step:907/2110 train_time:37610ms step_avg:41.47ms
step:908/2110 train_time:37670ms step_avg:41.49ms
step:909/2110 train_time:37728ms step_avg:41.51ms
step:910/2110 train_time:37788ms step_avg:41.53ms
step:911/2110 train_time:37846ms step_avg:41.54ms
step:912/2110 train_time:37906ms step_avg:41.56ms
step:913/2110 train_time:37965ms step_avg:41.58ms
step:914/2110 train_time:38025ms step_avg:41.60ms
step:915/2110 train_time:38085ms step_avg:41.62ms
step:916/2110 train_time:38144ms step_avg:41.64ms
step:917/2110 train_time:38202ms step_avg:41.66ms
step:918/2110 train_time:38262ms step_avg:41.68ms
step:919/2110 train_time:38321ms step_avg:41.70ms
step:920/2110 train_time:38381ms step_avg:41.72ms
step:921/2110 train_time:38438ms step_avg:41.73ms
step:922/2110 train_time:38499ms step_avg:41.76ms
step:923/2110 train_time:38557ms step_avg:41.77ms
step:924/2110 train_time:38617ms step_avg:41.79ms
step:925/2110 train_time:38675ms step_avg:41.81ms
step:926/2110 train_time:38735ms step_avg:41.83ms
step:927/2110 train_time:38794ms step_avg:41.85ms
step:928/2110 train_time:38854ms step_avg:41.87ms
step:929/2110 train_time:38912ms step_avg:41.89ms
step:930/2110 train_time:38973ms step_avg:41.91ms
step:931/2110 train_time:39031ms step_avg:41.92ms
step:932/2110 train_time:39091ms step_avg:41.94ms
step:933/2110 train_time:39148ms step_avg:41.96ms
step:934/2110 train_time:39209ms step_avg:41.98ms
step:935/2110 train_time:39266ms step_avg:42.00ms
step:936/2110 train_time:39327ms step_avg:42.02ms
step:937/2110 train_time:39384ms step_avg:42.03ms
step:938/2110 train_time:39443ms step_avg:42.05ms
step:939/2110 train_time:39503ms step_avg:42.07ms
step:940/2110 train_time:39563ms step_avg:42.09ms
step:941/2110 train_time:39622ms step_avg:42.11ms
step:942/2110 train_time:39683ms step_avg:42.13ms
step:943/2110 train_time:39741ms step_avg:42.14ms
step:944/2110 train_time:39802ms step_avg:42.16ms
step:945/2110 train_time:39860ms step_avg:42.18ms
step:946/2110 train_time:39921ms step_avg:42.20ms
step:947/2110 train_time:39978ms step_avg:42.22ms
step:948/2110 train_time:40039ms step_avg:42.24ms
step:949/2110 train_time:40097ms step_avg:42.25ms
step:950/2110 train_time:40157ms step_avg:42.27ms
step:951/2110 train_time:40216ms step_avg:42.29ms
step:952/2110 train_time:40276ms step_avg:42.31ms
step:953/2110 train_time:40335ms step_avg:42.32ms
step:954/2110 train_time:40395ms step_avg:42.34ms
step:955/2110 train_time:40453ms step_avg:42.36ms
step:956/2110 train_time:40514ms step_avg:42.38ms
step:957/2110 train_time:40571ms step_avg:42.39ms
step:958/2110 train_time:40632ms step_avg:42.41ms
step:959/2110 train_time:40689ms step_avg:42.43ms
step:960/2110 train_time:40749ms step_avg:42.45ms
step:961/2110 train_time:40807ms step_avg:42.46ms
step:962/2110 train_time:40868ms step_avg:42.48ms
step:963/2110 train_time:40927ms step_avg:42.50ms
step:964/2110 train_time:40986ms step_avg:42.52ms
step:965/2110 train_time:41045ms step_avg:42.53ms
step:966/2110 train_time:41104ms step_avg:42.55ms
step:967/2110 train_time:41163ms step_avg:42.57ms
step:968/2110 train_time:41223ms step_avg:42.59ms
step:969/2110 train_time:41281ms step_avg:42.60ms
step:970/2110 train_time:41340ms step_avg:42.62ms
step:971/2110 train_time:41400ms step_avg:42.64ms
step:972/2110 train_time:41460ms step_avg:42.65ms
step:973/2110 train_time:41519ms step_avg:42.67ms
step:974/2110 train_time:41579ms step_avg:42.69ms
step:975/2110 train_time:41637ms step_avg:42.70ms
step:976/2110 train_time:41698ms step_avg:42.72ms
step:977/2110 train_time:41756ms step_avg:42.74ms
step:978/2110 train_time:41816ms step_avg:42.76ms
step:979/2110 train_time:41874ms step_avg:42.77ms
step:980/2110 train_time:41935ms step_avg:42.79ms
step:981/2110 train_time:41993ms step_avg:42.81ms
step:982/2110 train_time:42053ms step_avg:42.82ms
step:983/2110 train_time:42112ms step_avg:42.84ms
step:984/2110 train_time:42171ms step_avg:42.86ms
step:985/2110 train_time:42229ms step_avg:42.87ms
step:986/2110 train_time:42289ms step_avg:42.89ms
step:987/2110 train_time:42347ms step_avg:42.90ms
step:988/2110 train_time:42407ms step_avg:42.92ms
step:989/2110 train_time:42465ms step_avg:42.94ms
step:990/2110 train_time:42526ms step_avg:42.96ms
step:991/2110 train_time:42583ms step_avg:42.97ms
step:992/2110 train_time:42643ms step_avg:42.99ms
step:993/2110 train_time:42702ms step_avg:43.00ms
step:994/2110 train_time:42761ms step_avg:43.02ms
step:995/2110 train_time:42820ms step_avg:43.03ms
step:996/2110 train_time:42881ms step_avg:43.05ms
step:997/2110 train_time:42940ms step_avg:43.07ms
step:998/2110 train_time:43000ms step_avg:43.09ms
step:999/2110 train_time:43056ms step_avg:43.10ms
step:1000/2110 train_time:43115ms step_avg:43.11ms
step:1000/2110 val_loss:3.7581 train_time:43176ms step_avg:43.18ms
step:1001/2110 train_time:43211ms step_avg:43.17ms
step:1002/2110 train_time:43244ms step_avg:43.16ms
step:1003/2110 train_time:43304ms step_avg:43.17ms
step:1004/2110 train_time:43366ms step_avg:43.19ms
step:1005/2110 train_time:43426ms step_avg:43.21ms
step:1006/2110 train_time:43485ms step_avg:43.23ms
step:1007/2110 train_time:43544ms step_avg:43.24ms
step:1008/2110 train_time:43603ms step_avg:43.26ms
step:1009/2110 train_time:43662ms step_avg:43.27ms
step:1010/2110 train_time:43720ms step_avg:43.29ms
step:1011/2110 train_time:43778ms step_avg:43.30ms
step:1012/2110 train_time:43837ms step_avg:43.32ms
step:1013/2110 train_time:43896ms step_avg:43.33ms
step:1014/2110 train_time:43954ms step_avg:43.35ms
step:1015/2110 train_time:44012ms step_avg:43.36ms
step:1016/2110 train_time:44070ms step_avg:43.38ms
step:1017/2110 train_time:44130ms step_avg:43.39ms
step:1018/2110 train_time:44190ms step_avg:43.41ms
step:1019/2110 train_time:44251ms step_avg:43.43ms
step:1020/2110 train_time:44311ms step_avg:43.44ms
step:1021/2110 train_time:44371ms step_avg:43.46ms
step:1022/2110 train_time:44431ms step_avg:43.47ms
step:1023/2110 train_time:44492ms step_avg:43.49ms
step:1024/2110 train_time:44550ms step_avg:43.51ms
step:1025/2110 train_time:44611ms step_avg:43.52ms
step:1026/2110 train_time:44669ms step_avg:43.54ms
step:1027/2110 train_time:44729ms step_avg:43.55ms
step:1028/2110 train_time:44787ms step_avg:43.57ms
step:1029/2110 train_time:44847ms step_avg:43.58ms
step:1030/2110 train_time:44906ms step_avg:43.60ms
step:1031/2110 train_time:44964ms step_avg:43.61ms
step:1032/2110 train_time:45024ms step_avg:43.63ms
step:1033/2110 train_time:45083ms step_avg:43.64ms
step:1034/2110 train_time:45143ms step_avg:43.66ms
step:1035/2110 train_time:45203ms step_avg:43.67ms
step:1036/2110 train_time:45263ms step_avg:43.69ms
step:1037/2110 train_time:45324ms step_avg:43.71ms
step:1038/2110 train_time:45384ms step_avg:43.72ms
step:1039/2110 train_time:45444ms step_avg:43.74ms
step:1040/2110 train_time:45504ms step_avg:43.75ms
step:1041/2110 train_time:45563ms step_avg:43.77ms
step:1042/2110 train_time:45623ms step_avg:43.78ms
step:1043/2110 train_time:45681ms step_avg:43.80ms
step:1044/2110 train_time:45741ms step_avg:43.81ms
step:1045/2110 train_time:45799ms step_avg:43.83ms
step:1046/2110 train_time:45858ms step_avg:43.84ms
step:1047/2110 train_time:45917ms step_avg:43.86ms
step:1048/2110 train_time:45976ms step_avg:43.87ms
step:1049/2110 train_time:46036ms step_avg:43.89ms
step:1050/2110 train_time:46094ms step_avg:43.90ms
step:1051/2110 train_time:46153ms step_avg:43.91ms
step:1052/2110 train_time:46212ms step_avg:43.93ms
step:1053/2110 train_time:46273ms step_avg:43.94ms
step:1054/2110 train_time:46332ms step_avg:43.96ms
step:1055/2110 train_time:46393ms step_avg:43.97ms
step:1056/2110 train_time:46452ms step_avg:43.99ms
step:1057/2110 train_time:46512ms step_avg:44.00ms
step:1058/2110 train_time:46571ms step_avg:44.02ms
step:1059/2110 train_time:46631ms step_avg:44.03ms
step:1060/2110 train_time:46690ms step_avg:44.05ms
step:1061/2110 train_time:46750ms step_avg:44.06ms
step:1062/2110 train_time:46808ms step_avg:44.08ms
step:1063/2110 train_time:46868ms step_avg:44.09ms
step:1064/2110 train_time:46928ms step_avg:44.10ms
step:1065/2110 train_time:46987ms step_avg:44.12ms
step:1066/2110 train_time:47046ms step_avg:44.13ms
step:1067/2110 train_time:47106ms step_avg:44.15ms
step:1068/2110 train_time:47165ms step_avg:44.16ms
step:1069/2110 train_time:47225ms step_avg:44.18ms
step:1070/2110 train_time:47285ms step_avg:44.19ms
step:1071/2110 train_time:47346ms step_avg:44.21ms
step:1072/2110 train_time:47406ms step_avg:44.22ms
step:1073/2110 train_time:47465ms step_avg:44.24ms
step:1074/2110 train_time:47525ms step_avg:44.25ms
step:1075/2110 train_time:47584ms step_avg:44.26ms
step:1076/2110 train_time:47643ms step_avg:44.28ms
step:1077/2110 train_time:47703ms step_avg:44.29ms
step:1078/2110 train_time:47762ms step_avg:44.31ms
step:1079/2110 train_time:47821ms step_avg:44.32ms
step:1080/2110 train_time:47881ms step_avg:44.33ms
step:1081/2110 train_time:47940ms step_avg:44.35ms
step:1082/2110 train_time:48000ms step_avg:44.36ms
step:1083/2110 train_time:48058ms step_avg:44.38ms
step:1084/2110 train_time:48117ms step_avg:44.39ms
step:1085/2110 train_time:48177ms step_avg:44.40ms
step:1086/2110 train_time:48236ms step_avg:44.42ms
step:1087/2110 train_time:48295ms step_avg:44.43ms
step:1088/2110 train_time:48354ms step_avg:44.44ms
step:1089/2110 train_time:48413ms step_avg:44.46ms
step:1090/2110 train_time:48472ms step_avg:44.47ms
step:1091/2110 train_time:48533ms step_avg:44.48ms
step:1092/2110 train_time:48592ms step_avg:44.50ms
step:1093/2110 train_time:48652ms step_avg:44.51ms
step:1094/2110 train_time:48710ms step_avg:44.52ms
step:1095/2110 train_time:48770ms step_avg:44.54ms
step:1096/2110 train_time:48828ms step_avg:44.55ms
step:1097/2110 train_time:48888ms step_avg:44.56ms
step:1098/2110 train_time:48947ms step_avg:44.58ms
step:1099/2110 train_time:49006ms step_avg:44.59ms
step:1100/2110 train_time:49066ms step_avg:44.61ms
step:1101/2110 train_time:49125ms step_avg:44.62ms
step:1102/2110 train_time:49185ms step_avg:44.63ms
step:1103/2110 train_time:49244ms step_avg:44.65ms
step:1104/2110 train_time:49304ms step_avg:44.66ms
step:1105/2110 train_time:49363ms step_avg:44.67ms
step:1106/2110 train_time:49423ms step_avg:44.69ms
step:1107/2110 train_time:49483ms step_avg:44.70ms
step:1108/2110 train_time:49542ms step_avg:44.71ms
step:1109/2110 train_time:49601ms step_avg:44.73ms
step:1110/2110 train_time:49661ms step_avg:44.74ms
step:1111/2110 train_time:49720ms step_avg:44.75ms
step:1112/2110 train_time:49779ms step_avg:44.77ms
step:1113/2110 train_time:49839ms step_avg:44.78ms
step:1114/2110 train_time:49897ms step_avg:44.79ms
step:1115/2110 train_time:49957ms step_avg:44.80ms
step:1116/2110 train_time:50016ms step_avg:44.82ms
step:1117/2110 train_time:50075ms step_avg:44.83ms
step:1118/2110 train_time:50135ms step_avg:44.84ms
step:1119/2110 train_time:50193ms step_avg:44.86ms
step:1120/2110 train_time:50252ms step_avg:44.87ms
step:1121/2110 train_time:50311ms step_avg:44.88ms
step:1122/2110 train_time:50370ms step_avg:44.89ms
step:1123/2110 train_time:50430ms step_avg:44.91ms
step:1124/2110 train_time:50489ms step_avg:44.92ms
step:1125/2110 train_time:50549ms step_avg:44.93ms
step:1126/2110 train_time:50608ms step_avg:44.94ms
step:1127/2110 train_time:50667ms step_avg:44.96ms
step:1128/2110 train_time:50727ms step_avg:44.97ms
step:1129/2110 train_time:50786ms step_avg:44.98ms
step:1130/2110 train_time:50846ms step_avg:45.00ms
step:1131/2110 train_time:50905ms step_avg:45.01ms
step:1132/2110 train_time:50965ms step_avg:45.02ms
step:1133/2110 train_time:51025ms step_avg:45.03ms
step:1134/2110 train_time:51084ms step_avg:45.05ms
step:1135/2110 train_time:51144ms step_avg:45.06ms
step:1136/2110 train_time:51203ms step_avg:45.07ms
step:1137/2110 train_time:51262ms step_avg:45.09ms
step:1138/2110 train_time:51322ms step_avg:45.10ms
step:1139/2110 train_time:51381ms step_avg:45.11ms
step:1140/2110 train_time:51442ms step_avg:45.12ms
step:1141/2110 train_time:51501ms step_avg:45.14ms
step:1142/2110 train_time:51561ms step_avg:45.15ms
step:1143/2110 train_time:51621ms step_avg:45.16ms
step:1144/2110 train_time:51681ms step_avg:45.18ms
step:1145/2110 train_time:51741ms step_avg:45.19ms
step:1146/2110 train_time:51800ms step_avg:45.20ms
step:1147/2110 train_time:51860ms step_avg:45.21ms
step:1148/2110 train_time:51920ms step_avg:45.23ms
step:1149/2110 train_time:51981ms step_avg:45.24ms
step:1150/2110 train_time:52041ms step_avg:45.25ms
step:1151/2110 train_time:52101ms step_avg:45.27ms
step:1152/2110 train_time:52160ms step_avg:45.28ms
step:1153/2110 train_time:52220ms step_avg:45.29ms
step:1154/2110 train_time:52280ms step_avg:45.30ms
step:1155/2110 train_time:52339ms step_avg:45.32ms
step:1156/2110 train_time:52399ms step_avg:45.33ms
step:1157/2110 train_time:52458ms step_avg:45.34ms
step:1158/2110 train_time:52518ms step_avg:45.35ms
step:1159/2110 train_time:52577ms step_avg:45.36ms
step:1160/2110 train_time:52637ms step_avg:45.38ms
step:1161/2110 train_time:52697ms step_avg:45.39ms
step:1162/2110 train_time:52756ms step_avg:45.40ms
step:1163/2110 train_time:52816ms step_avg:45.41ms
step:1164/2110 train_time:52875ms step_avg:45.42ms
step:1165/2110 train_time:52934ms step_avg:45.44ms
step:1166/2110 train_time:52993ms step_avg:45.45ms
step:1167/2110 train_time:53054ms step_avg:45.46ms
step:1168/2110 train_time:53113ms step_avg:45.47ms
step:1169/2110 train_time:53174ms step_avg:45.49ms
step:1170/2110 train_time:53233ms step_avg:45.50ms
step:1171/2110 train_time:53293ms step_avg:45.51ms
step:1172/2110 train_time:53352ms step_avg:45.52ms
step:1173/2110 train_time:53412ms step_avg:45.53ms
step:1174/2110 train_time:53471ms step_avg:45.55ms
step:1175/2110 train_time:53532ms step_avg:45.56ms
step:1176/2110 train_time:53590ms step_avg:45.57ms
step:1177/2110 train_time:53651ms step_avg:45.58ms
step:1178/2110 train_time:53710ms step_avg:45.59ms
step:1179/2110 train_time:53770ms step_avg:45.61ms
step:1180/2110 train_time:53830ms step_avg:45.62ms
step:1181/2110 train_time:53890ms step_avg:45.63ms
step:1182/2110 train_time:53949ms step_avg:45.64ms
step:1183/2110 train_time:54009ms step_avg:45.65ms
step:1184/2110 train_time:54068ms step_avg:45.67ms
step:1185/2110 train_time:54129ms step_avg:45.68ms
step:1186/2110 train_time:54189ms step_avg:45.69ms
step:1187/2110 train_time:54250ms step_avg:45.70ms
step:1188/2110 train_time:54308ms step_avg:45.71ms
step:1189/2110 train_time:54368ms step_avg:45.73ms
step:1190/2110 train_time:54429ms step_avg:45.74ms
step:1191/2110 train_time:54489ms step_avg:45.75ms
step:1192/2110 train_time:54549ms step_avg:45.76ms
step:1193/2110 train_time:54608ms step_avg:45.77ms
step:1194/2110 train_time:54668ms step_avg:45.79ms
step:1195/2110 train_time:54729ms step_avg:45.80ms
step:1196/2110 train_time:54788ms step_avg:45.81ms
step:1197/2110 train_time:54848ms step_avg:45.82ms
step:1198/2110 train_time:54908ms step_avg:45.83ms
step:1199/2110 train_time:54969ms step_avg:45.85ms
step:1200/2110 train_time:55029ms step_avg:45.86ms
step:1201/2110 train_time:55089ms step_avg:45.87ms
step:1202/2110 train_time:55149ms step_avg:45.88ms
step:1203/2110 train_time:55208ms step_avg:45.89ms
step:1204/2110 train_time:55268ms step_avg:45.90ms
step:1205/2110 train_time:55329ms step_avg:45.92ms
step:1206/2110 train_time:55388ms step_avg:45.93ms
step:1207/2110 train_time:55449ms step_avg:45.94ms
step:1208/2110 train_time:55508ms step_avg:45.95ms
step:1209/2110 train_time:55568ms step_avg:45.96ms
step:1210/2110 train_time:55628ms step_avg:45.97ms
step:1211/2110 train_time:55688ms step_avg:45.99ms
step:1212/2110 train_time:55748ms step_avg:46.00ms
step:1213/2110 train_time:55808ms step_avg:46.01ms
step:1214/2110 train_time:55868ms step_avg:46.02ms
step:1215/2110 train_time:55929ms step_avg:46.03ms
step:1216/2110 train_time:55989ms step_avg:46.04ms
step:1217/2110 train_time:56049ms step_avg:46.05ms
step:1218/2110 train_time:56108ms step_avg:46.07ms
step:1219/2110 train_time:56169ms step_avg:46.08ms
step:1220/2110 train_time:56229ms step_avg:46.09ms
step:1221/2110 train_time:56289ms step_avg:46.10ms
step:1222/2110 train_time:56349ms step_avg:46.11ms
step:1223/2110 train_time:56409ms step_avg:46.12ms
step:1224/2110 train_time:56469ms step_avg:46.13ms
step:1225/2110 train_time:56529ms step_avg:46.15ms
step:1226/2110 train_time:56588ms step_avg:46.16ms
step:1227/2110 train_time:56649ms step_avg:46.17ms
step:1228/2110 train_time:56708ms step_avg:46.18ms
step:1229/2110 train_time:56769ms step_avg:46.19ms
step:1230/2110 train_time:56829ms step_avg:46.20ms
step:1231/2110 train_time:56890ms step_avg:46.21ms
step:1232/2110 train_time:56949ms step_avg:46.22ms
step:1233/2110 train_time:57008ms step_avg:46.24ms
step:1234/2110 train_time:57068ms step_avg:46.25ms
step:1235/2110 train_time:57129ms step_avg:46.26ms
step:1236/2110 train_time:57188ms step_avg:46.27ms
step:1237/2110 train_time:57248ms step_avg:46.28ms
step:1238/2110 train_time:57307ms step_avg:46.29ms
step:1239/2110 train_time:57367ms step_avg:46.30ms
step:1240/2110 train_time:57428ms step_avg:46.31ms
step:1241/2110 train_time:57487ms step_avg:46.32ms
step:1242/2110 train_time:57548ms step_avg:46.33ms
step:1243/2110 train_time:57607ms step_avg:46.35ms
step:1244/2110 train_time:57668ms step_avg:46.36ms
step:1245/2110 train_time:57729ms step_avg:46.37ms
step:1246/2110 train_time:57789ms step_avg:46.38ms
step:1247/2110 train_time:57848ms step_avg:46.39ms
step:1248/2110 train_time:57908ms step_avg:46.40ms
step:1249/2110 train_time:57968ms step_avg:46.41ms
step:1250/2110 train_time:58028ms step_avg:46.42ms
step:1250/2110 val_loss:3.5966 train_time:58090ms step_avg:46.47ms
step:1251/2110 train_time:58128ms step_avg:46.47ms
step:1252/2110 train_time:58165ms step_avg:46.46ms
step:1253/2110 train_time:58215ms step_avg:46.46ms
step:1254/2110 train_time:58280ms step_avg:46.48ms
step:1255/2110 train_time:58339ms step_avg:46.49ms
step:1256/2110 train_time:58399ms step_avg:46.50ms
step:1257/2110 train_time:58458ms step_avg:46.51ms
step:1258/2110 train_time:58517ms step_avg:46.52ms
step:1259/2110 train_time:58576ms step_avg:46.53ms
step:1260/2110 train_time:58635ms step_avg:46.54ms
step:1261/2110 train_time:58694ms step_avg:46.55ms
step:1262/2110 train_time:58753ms step_avg:46.56ms
step:1263/2110 train_time:58811ms step_avg:46.56ms
step:1264/2110 train_time:58870ms step_avg:46.57ms
step:1265/2110 train_time:58929ms step_avg:46.58ms
step:1266/2110 train_time:58988ms step_avg:46.59ms
step:1267/2110 train_time:59048ms step_avg:46.60ms
step:1268/2110 train_time:59108ms step_avg:46.61ms
step:1269/2110 train_time:59171ms step_avg:46.63ms
step:1270/2110 train_time:59231ms step_avg:46.64ms
step:1271/2110 train_time:59293ms step_avg:46.65ms
step:1272/2110 train_time:59353ms step_avg:46.66ms
step:1273/2110 train_time:59413ms step_avg:46.67ms
step:1274/2110 train_time:59472ms step_avg:46.68ms
step:1275/2110 train_time:59533ms step_avg:46.69ms
step:1276/2110 train_time:59592ms step_avg:46.70ms
step:1277/2110 train_time:59651ms step_avg:46.71ms
step:1278/2110 train_time:59710ms step_avg:46.72ms
step:1279/2110 train_time:59769ms step_avg:46.73ms
step:1280/2110 train_time:59828ms step_avg:46.74ms
step:1281/2110 train_time:59886ms step_avg:46.75ms
step:1282/2110 train_time:59947ms step_avg:46.76ms
step:1283/2110 train_time:60006ms step_avg:46.77ms
step:1284/2110 train_time:60067ms step_avg:46.78ms
step:1285/2110 train_time:60127ms step_avg:46.79ms
step:1286/2110 train_time:60188ms step_avg:46.80ms
step:1287/2110 train_time:60249ms step_avg:46.81ms
step:1288/2110 train_time:60309ms step_avg:46.82ms
step:1289/2110 train_time:60369ms step_avg:46.83ms
step:1290/2110 train_time:60429ms step_avg:46.84ms
step:1291/2110 train_time:60489ms step_avg:46.85ms
step:1292/2110 train_time:60549ms step_avg:46.86ms
step:1293/2110 train_time:60608ms step_avg:46.87ms
step:1294/2110 train_time:60668ms step_avg:46.88ms
step:1295/2110 train_time:60727ms step_avg:46.89ms
step:1296/2110 train_time:60786ms step_avg:46.90ms
step:1297/2110 train_time:60846ms step_avg:46.91ms
step:1298/2110 train_time:60907ms step_avg:46.92ms
step:1299/2110 train_time:60966ms step_avg:46.93ms
step:1300/2110 train_time:61026ms step_avg:46.94ms
step:1301/2110 train_time:61086ms step_avg:46.95ms
step:1302/2110 train_time:61147ms step_avg:46.96ms
step:1303/2110 train_time:61206ms step_avg:46.97ms
step:1304/2110 train_time:61267ms step_avg:46.98ms
step:1305/2110 train_time:61327ms step_avg:46.99ms
step:1306/2110 train_time:61388ms step_avg:47.00ms
step:1307/2110 train_time:61448ms step_avg:47.01ms
step:1308/2110 train_time:61508ms step_avg:47.02ms
step:1309/2110 train_time:61568ms step_avg:47.03ms
step:1310/2110 train_time:61628ms step_avg:47.04ms
step:1311/2110 train_time:61687ms step_avg:47.05ms
step:1312/2110 train_time:61747ms step_avg:47.06ms
step:1313/2110 train_time:61806ms step_avg:47.07ms
step:1314/2110 train_time:61866ms step_avg:47.08ms
step:1315/2110 train_time:61925ms step_avg:47.09ms
step:1316/2110 train_time:61984ms step_avg:47.10ms
step:1317/2110 train_time:62044ms step_avg:47.11ms
step:1318/2110 train_time:62104ms step_avg:47.12ms
step:1319/2110 train_time:62165ms step_avg:47.13ms
step:1320/2110 train_time:62226ms step_avg:47.14ms
step:1321/2110 train_time:62287ms step_avg:47.15ms
step:1322/2110 train_time:62346ms step_avg:47.16ms
step:1323/2110 train_time:62407ms step_avg:47.17ms
step:1324/2110 train_time:62468ms step_avg:47.18ms
step:1325/2110 train_time:62527ms step_avg:47.19ms
step:1326/2110 train_time:62587ms step_avg:47.20ms
step:1327/2110 train_time:62647ms step_avg:47.21ms
step:1328/2110 train_time:62707ms step_avg:47.22ms
step:1329/2110 train_time:62767ms step_avg:47.23ms
step:1330/2110 train_time:62826ms step_avg:47.24ms
step:1331/2110 train_time:62886ms step_avg:47.25ms
step:1332/2110 train_time:62946ms step_avg:47.26ms
step:1333/2110 train_time:63005ms step_avg:47.27ms
step:1334/2110 train_time:63066ms step_avg:47.28ms
step:1335/2110 train_time:63126ms step_avg:47.29ms
step:1336/2110 train_time:63187ms step_avg:47.30ms
step:1337/2110 train_time:63247ms step_avg:47.31ms
step:1338/2110 train_time:63307ms step_avg:47.31ms
step:1339/2110 train_time:63367ms step_avg:47.32ms
step:1340/2110 train_time:63427ms step_avg:47.33ms
step:1341/2110 train_time:63487ms step_avg:47.34ms
step:1342/2110 train_time:63547ms step_avg:47.35ms
step:1343/2110 train_time:63606ms step_avg:47.36ms
step:1344/2110 train_time:63667ms step_avg:47.37ms
step:1345/2110 train_time:63727ms step_avg:47.38ms
step:1346/2110 train_time:63786ms step_avg:47.39ms
step:1347/2110 train_time:63846ms step_avg:47.40ms
step:1348/2110 train_time:63906ms step_avg:47.41ms
step:1349/2110 train_time:63967ms step_avg:47.42ms
step:1350/2110 train_time:64026ms step_avg:47.43ms
step:1351/2110 train_time:64086ms step_avg:47.44ms
step:1352/2110 train_time:64145ms step_avg:47.44ms
step:1353/2110 train_time:64206ms step_avg:47.45ms
step:1354/2110 train_time:64267ms step_avg:47.46ms
step:1355/2110 train_time:64328ms step_avg:47.47ms
step:1356/2110 train_time:64388ms step_avg:47.48ms
step:1357/2110 train_time:64447ms step_avg:47.49ms
step:1358/2110 train_time:64507ms step_avg:47.50ms
step:1359/2110 train_time:64568ms step_avg:47.51ms
step:1360/2110 train_time:64627ms step_avg:47.52ms
step:1361/2110 train_time:64687ms step_avg:47.53ms
step:1362/2110 train_time:64746ms step_avg:47.54ms
step:1363/2110 train_time:64806ms step_avg:47.55ms
step:1364/2110 train_time:64866ms step_avg:47.56ms
step:1365/2110 train_time:64925ms step_avg:47.56ms
step:1366/2110 train_time:64985ms step_avg:47.57ms
step:1367/2110 train_time:65045ms step_avg:47.58ms
step:1368/2110 train_time:65105ms step_avg:47.59ms
step:1369/2110 train_time:65166ms step_avg:47.60ms
step:1370/2110 train_time:65226ms step_avg:47.61ms
step:1371/2110 train_time:65286ms step_avg:47.62ms
step:1372/2110 train_time:65346ms step_avg:47.63ms
step:1373/2110 train_time:65407ms step_avg:47.64ms
step:1374/2110 train_time:65468ms step_avg:47.65ms
step:1375/2110 train_time:65527ms step_avg:47.66ms
step:1376/2110 train_time:65587ms step_avg:47.67ms
step:1377/2110 train_time:65647ms step_avg:47.67ms
step:1378/2110 train_time:65707ms step_avg:47.68ms
step:1379/2110 train_time:65767ms step_avg:47.69ms
step:1380/2110 train_time:65826ms step_avg:47.70ms
step:1381/2110 train_time:65887ms step_avg:47.71ms
step:1382/2110 train_time:65975ms step_avg:47.74ms
step:1383/2110 train_time:66061ms step_avg:47.77ms
step:1384/2110 train_time:66148ms step_avg:47.80ms
step:1385/2110 train_time:66235ms step_avg:47.82ms
step:1386/2110 train_time:66322ms step_avg:47.85ms
step:1387/2110 train_time:66409ms step_avg:47.88ms
step:1388/2110 train_time:66497ms step_avg:47.91ms
step:1389/2110 train_time:66583ms step_avg:47.94ms
step:1390/2110 train_time:66670ms step_avg:47.96ms
step:1391/2110 train_time:66757ms step_avg:47.99ms
step:1392/2110 train_time:66843ms step_avg:48.02ms
step:1393/2110 train_time:66929ms step_avg:48.05ms
step:1394/2110 train_time:67015ms step_avg:48.07ms
step:1395/2110 train_time:67102ms step_avg:48.10ms
step:1396/2110 train_time:67190ms step_avg:48.13ms
step:1397/2110 train_time:67276ms step_avg:48.16ms
step:1398/2110 train_time:67363ms step_avg:48.19ms
step:1399/2110 train_time:67450ms step_avg:48.21ms
step:1400/2110 train_time:67537ms step_avg:48.24ms
step:1401/2110 train_time:67623ms step_avg:48.27ms
step:1402/2110 train_time:67711ms step_avg:48.30ms
step:1403/2110 train_time:67798ms step_avg:48.32ms
step:1404/2110 train_time:67883ms step_avg:48.35ms
step:1405/2110 train_time:67970ms step_avg:48.38ms
step:1406/2110 train_time:68058ms step_avg:48.41ms
step:1407/2110 train_time:68143ms step_avg:48.43ms
step:1408/2110 train_time:68231ms step_avg:48.46ms
step:1409/2110 train_time:68318ms step_avg:48.49ms
step:1410/2110 train_time:68404ms step_avg:48.51ms
step:1411/2110 train_time:68492ms step_avg:48.54ms
step:1412/2110 train_time:68578ms step_avg:48.57ms
step:1413/2110 train_time:68665ms step_avg:48.60ms
step:1414/2110 train_time:68753ms step_avg:48.62ms
step:1415/2110 train_time:68839ms step_avg:48.65ms
step:1416/2110 train_time:68925ms step_avg:48.68ms
step:1417/2110 train_time:69012ms step_avg:48.70ms
step:1418/2110 train_time:69100ms step_avg:48.73ms
step:1419/2110 train_time:69185ms step_avg:48.76ms
step:1420/2110 train_time:69273ms step_avg:48.78ms
step:1421/2110 train_time:69360ms step_avg:48.81ms
step:1422/2110 train_time:69447ms step_avg:48.84ms
step:1423/2110 train_time:69533ms step_avg:48.86ms
step:1424/2110 train_time:69621ms step_avg:48.89ms
step:1425/2110 train_time:69706ms step_avg:48.92ms
step:1426/2110 train_time:69795ms step_avg:48.94ms
step:1427/2110 train_time:69881ms step_avg:48.97ms
step:1428/2110 train_time:69968ms step_avg:49.00ms
step:1429/2110 train_time:70054ms step_avg:49.02ms
step:1430/2110 train_time:70141ms step_avg:49.05ms
step:1431/2110 train_time:70227ms step_avg:49.08ms
step:1432/2110 train_time:70315ms step_avg:49.10ms
step:1433/2110 train_time:70402ms step_avg:49.13ms
step:1434/2110 train_time:70489ms step_avg:49.16ms
step:1435/2110 train_time:70576ms step_avg:49.18ms
step:1436/2110 train_time:70664ms step_avg:49.21ms
step:1437/2110 train_time:70749ms step_avg:49.23ms
step:1438/2110 train_time:70836ms step_avg:49.26ms
step:1439/2110 train_time:70923ms step_avg:49.29ms
step:1440/2110 train_time:71009ms step_avg:49.31ms
step:1441/2110 train_time:71096ms step_avg:49.34ms
step:1442/2110 train_time:71182ms step_avg:49.36ms
step:1443/2110 train_time:71269ms step_avg:49.39ms
step:1444/2110 train_time:71358ms step_avg:49.42ms
step:1445/2110 train_time:71443ms step_avg:49.44ms
step:1446/2110 train_time:71530ms step_avg:49.47ms
step:1447/2110 train_time:71617ms step_avg:49.49ms
step:1448/2110 train_time:71703ms step_avg:49.52ms
step:1449/2110 train_time:71790ms step_avg:49.54ms
step:1450/2110 train_time:71877ms step_avg:49.57ms
step:1451/2110 train_time:71963ms step_avg:49.60ms
step:1452/2110 train_time:72050ms step_avg:49.62ms
step:1453/2110 train_time:72136ms step_avg:49.65ms
step:1454/2110 train_time:72223ms step_avg:49.67ms
step:1455/2110 train_time:72310ms step_avg:49.70ms
step:1456/2110 train_time:72399ms step_avg:49.72ms
step:1457/2110 train_time:72484ms step_avg:49.75ms
step:1458/2110 train_time:72572ms step_avg:49.77ms
step:1459/2110 train_time:72658ms step_avg:49.80ms
step:1460/2110 train_time:72744ms step_avg:49.82ms
step:1461/2110 train_time:72830ms step_avg:49.85ms
step:1462/2110 train_time:72917ms step_avg:49.87ms
step:1463/2110 train_time:73004ms step_avg:49.90ms
step:1464/2110 train_time:73091ms step_avg:49.93ms
step:1465/2110 train_time:73178ms step_avg:49.95ms
step:1466/2110 train_time:73264ms step_avg:49.98ms
step:1467/2110 train_time:73350ms step_avg:50.00ms
step:1468/2110 train_time:73439ms step_avg:50.03ms
step:1469/2110 train_time:73524ms step_avg:50.05ms
step:1470/2110 train_time:73612ms step_avg:50.08ms
step:1471/2110 train_time:73699ms step_avg:50.10ms
step:1472/2110 train_time:73786ms step_avg:50.13ms
step:1473/2110 train_time:73872ms step_avg:50.15ms
step:1474/2110 train_time:73959ms step_avg:50.18ms
step:1475/2110 train_time:74045ms step_avg:50.20ms
step:1476/2110 train_time:74132ms step_avg:50.22ms
step:1477/2110 train_time:74219ms step_avg:50.25ms
step:1478/2110 train_time:74306ms step_avg:50.27ms
step:1479/2110 train_time:74393ms step_avg:50.30ms
step:1480/2110 train_time:74481ms step_avg:50.32ms
step:1481/2110 train_time:74567ms step_avg:50.35ms
step:1482/2110 train_time:74655ms step_avg:50.37ms
step:1483/2110 train_time:74741ms step_avg:50.40ms
step:1484/2110 train_time:74827ms step_avg:50.42ms
step:1485/2110 train_time:74914ms step_avg:50.45ms
step:1486/2110 train_time:75001ms step_avg:50.47ms
step:1487/2110 train_time:75088ms step_avg:50.50ms
step:1488/2110 train_time:75175ms step_avg:50.52ms
step:1489/2110 train_time:75261ms step_avg:50.54ms
step:1490/2110 train_time:75350ms step_avg:50.57ms
step:1491/2110 train_time:75435ms step_avg:50.59ms
step:1492/2110 train_time:75523ms step_avg:50.62ms
step:1493/2110 train_time:75610ms step_avg:50.64ms
step:1494/2110 train_time:75698ms step_avg:50.67ms
step:1495/2110 train_time:75784ms step_avg:50.69ms
step:1496/2110 train_time:75872ms step_avg:50.72ms
step:1497/2110 train_time:75958ms step_avg:50.74ms
step:1498/2110 train_time:76044ms step_avg:50.76ms
step:1499/2110 train_time:76132ms step_avg:50.79ms
step:1500/2110 train_time:76219ms step_avg:50.81ms
step:1500/2110 val_loss:3.4951 train_time:76306ms step_avg:50.87ms
step:1501/2110 train_time:76342ms step_avg:50.86ms
step:1502/2110 train_time:76398ms step_avg:50.86ms
step:1503/2110 train_time:76492ms step_avg:50.89ms
step:1504/2110 train_time:76579ms step_avg:50.92ms
step:1505/2110 train_time:76666ms step_avg:50.94ms
step:1506/2110 train_time:76753ms step_avg:50.96ms
step:1507/2110 train_time:76837ms step_avg:50.99ms
step:1508/2110 train_time:76924ms step_avg:51.01ms
step:1509/2110 train_time:77009ms step_avg:51.03ms
step:1510/2110 train_time:77095ms step_avg:51.06ms
step:1511/2110 train_time:77180ms step_avg:51.08ms
step:1512/2110 train_time:77267ms step_avg:51.10ms
step:1513/2110 train_time:77359ms step_avg:51.13ms
step:1514/2110 train_time:77449ms step_avg:51.16ms
step:1515/2110 train_time:77539ms step_avg:51.18ms
step:1516/2110 train_time:77627ms step_avg:51.21ms
step:1517/2110 train_time:77713ms step_avg:51.23ms
step:1518/2110 train_time:77800ms step_avg:51.25ms
step:1519/2110 train_time:77885ms step_avg:51.27ms
step:1520/2110 train_time:77972ms step_avg:51.30ms
step:1521/2110 train_time:78057ms step_avg:51.32ms
step:1522/2110 train_time:78143ms step_avg:51.34ms
step:1523/2110 train_time:78229ms step_avg:51.37ms
step:1524/2110 train_time:78317ms step_avg:51.39ms
step:1525/2110 train_time:78406ms step_avg:51.41ms
step:1526/2110 train_time:78495ms step_avg:51.44ms
step:1527/2110 train_time:78581ms step_avg:51.46ms
step:1528/2110 train_time:78668ms step_avg:51.48ms
step:1529/2110 train_time:78756ms step_avg:51.51ms
step:1530/2110 train_time:78843ms step_avg:51.53ms
step:1531/2110 train_time:78928ms step_avg:51.55ms
step:1532/2110 train_time:79014ms step_avg:51.58ms
step:1533/2110 train_time:79101ms step_avg:51.60ms
step:1534/2110 train_time:79187ms step_avg:51.62ms
step:1535/2110 train_time:79274ms step_avg:51.64ms
step:1536/2110 train_time:79362ms step_avg:51.67ms
step:1537/2110 train_time:79449ms step_avg:51.69ms
step:1538/2110 train_time:79538ms step_avg:51.72ms
step:1539/2110 train_time:79625ms step_avg:51.74ms
step:1540/2110 train_time:79712ms step_avg:51.76ms
step:1541/2110 train_time:79798ms step_avg:51.78ms
step:1542/2110 train_time:79884ms step_avg:51.81ms
step:1543/2110 train_time:79970ms step_avg:51.83ms
step:1544/2110 train_time:80056ms step_avg:51.85ms
step:1545/2110 train_time:80142ms step_avg:51.87ms
step:1546/2110 train_time:80229ms step_avg:51.89ms
step:1547/2110 train_time:80316ms step_avg:51.92ms
step:1548/2110 train_time:80403ms step_avg:51.94ms
step:1549/2110 train_time:80490ms step_avg:51.96ms
step:1550/2110 train_time:80577ms step_avg:51.99ms
step:1551/2110 train_time:80664ms step_avg:52.01ms
step:1552/2110 train_time:80753ms step_avg:52.03ms
step:1553/2110 train_time:80839ms step_avg:52.05ms
step:1554/2110 train_time:80926ms step_avg:52.08ms
step:1555/2110 train_time:81011ms step_avg:52.10ms
step:1556/2110 train_time:81098ms step_avg:52.12ms
step:1557/2110 train_time:81184ms step_avg:52.14ms
step:1558/2110 train_time:81271ms step_avg:52.16ms
step:1559/2110 train_time:81358ms step_avg:52.19ms
step:1560/2110 train_time:81446ms step_avg:52.21ms
step:1561/2110 train_time:81533ms step_avg:52.23ms
step:1562/2110 train_time:81620ms step_avg:52.25ms
step:1563/2110 train_time:81706ms step_avg:52.28ms
step:1564/2110 train_time:81794ms step_avg:52.30ms
step:1565/2110 train_time:81882ms step_avg:52.32ms
step:1566/2110 train_time:81966ms step_avg:52.34ms
step:1567/2110 train_time:82053ms step_avg:52.36ms
step:1568/2110 train_time:82139ms step_avg:52.38ms
step:1569/2110 train_time:82226ms step_avg:52.41ms
step:1570/2110 train_time:82314ms step_avg:52.43ms
step:1571/2110 train_time:82401ms step_avg:52.45ms
step:1572/2110 train_time:82487ms step_avg:52.47ms
step:1573/2110 train_time:82577ms step_avg:52.50ms
step:1574/2110 train_time:82663ms step_avg:52.52ms
step:1575/2110 train_time:82750ms step_avg:52.54ms
step:1576/2110 train_time:82837ms step_avg:52.56ms
step:1577/2110 train_time:82923ms step_avg:52.58ms
step:1578/2110 train_time:83010ms step_avg:52.60ms
step:1579/2110 train_time:83097ms step_avg:52.63ms
step:1580/2110 train_time:83183ms step_avg:52.65ms
step:1581/2110 train_time:83270ms step_avg:52.67ms
step:1582/2110 train_time:83359ms step_avg:52.69ms
step:1583/2110 train_time:83444ms step_avg:52.71ms
step:1584/2110 train_time:83535ms step_avg:52.74ms
step:1585/2110 train_time:83619ms step_avg:52.76ms
step:1586/2110 train_time:83706ms step_avg:52.78ms
step:1587/2110 train_time:83793ms step_avg:52.80ms
step:1588/2110 train_time:83879ms step_avg:52.82ms
step:1589/2110 train_time:83965ms step_avg:52.84ms
step:1590/2110 train_time:84053ms step_avg:52.86ms
step:1591/2110 train_time:84138ms step_avg:52.88ms
step:1592/2110 train_time:84225ms step_avg:52.91ms
step:1593/2110 train_time:84313ms step_avg:52.93ms
step:1594/2110 train_time:84399ms step_avg:52.95ms
step:1595/2110 train_time:84485ms step_avg:52.97ms
step:1596/2110 train_time:84574ms step_avg:52.99ms
step:1597/2110 train_time:84660ms step_avg:53.01ms
step:1598/2110 train_time:84747ms step_avg:53.03ms
step:1599/2110 train_time:84833ms step_avg:53.05ms
step:1600/2110 train_time:84919ms step_avg:53.07ms
step:1601/2110 train_time:85007ms step_avg:53.10ms
step:1602/2110 train_time:85094ms step_avg:53.12ms
step:1603/2110 train_time:85179ms step_avg:53.14ms
step:1604/2110 train_time:85266ms step_avg:53.16ms
step:1605/2110 train_time:85353ms step_avg:53.18ms
step:1606/2110 train_time:85441ms step_avg:53.20ms
step:1607/2110 train_time:85527ms step_avg:53.22ms
step:1608/2110 train_time:85614ms step_avg:53.24ms
step:1609/2110 train_time:85700ms step_avg:53.26ms
step:1610/2110 train_time:85787ms step_avg:53.28ms
step:1611/2110 train_time:85874ms step_avg:53.31ms
step:1612/2110 train_time:85961ms step_avg:53.33ms
step:1613/2110 train_time:86047ms step_avg:53.35ms
step:1614/2110 train_time:86135ms step_avg:53.37ms
step:1615/2110 train_time:86220ms step_avg:53.39ms
step:1616/2110 train_time:86308ms step_avg:53.41ms
step:1617/2110 train_time:86395ms step_avg:53.43ms
step:1618/2110 train_time:86482ms step_avg:53.45ms
step:1619/2110 train_time:86570ms step_avg:53.47ms
step:1620/2110 train_time:86657ms step_avg:53.49ms
step:1621/2110 train_time:86743ms step_avg:53.51ms
step:1622/2110 train_time:86831ms step_avg:53.53ms
step:1623/2110 train_time:86917ms step_avg:53.55ms
step:1624/2110 train_time:87004ms step_avg:53.57ms
step:1625/2110 train_time:87090ms step_avg:53.59ms
step:1626/2110 train_time:87177ms step_avg:53.61ms
step:1627/2110 train_time:87263ms step_avg:53.63ms
step:1628/2110 train_time:87350ms step_avg:53.65ms
step:1629/2110 train_time:87437ms step_avg:53.68ms
step:1630/2110 train_time:87524ms step_avg:53.70ms
step:1631/2110 train_time:87611ms step_avg:53.72ms
step:1632/2110 train_time:87698ms step_avg:53.74ms
step:1633/2110 train_time:87784ms step_avg:53.76ms
step:1634/2110 train_time:87871ms step_avg:53.78ms
step:1635/2110 train_time:87958ms step_avg:53.80ms
step:1636/2110 train_time:88045ms step_avg:53.82ms
step:1637/2110 train_time:88132ms step_avg:53.84ms
step:1638/2110 train_time:88219ms step_avg:53.86ms
step:1639/2110 train_time:88305ms step_avg:53.88ms
step:1640/2110 train_time:88392ms step_avg:53.90ms
step:1641/2110 train_time:88478ms step_avg:53.92ms
step:1642/2110 train_time:88566ms step_avg:53.94ms
step:1643/2110 train_time:88652ms step_avg:53.96ms
step:1644/2110 train_time:88739ms step_avg:53.98ms
step:1645/2110 train_time:88826ms step_avg:54.00ms
step:1646/2110 train_time:88913ms step_avg:54.02ms
step:1647/2110 train_time:89000ms step_avg:54.04ms
step:1648/2110 train_time:89086ms step_avg:54.06ms
step:1649/2110 train_time:89173ms step_avg:54.08ms
step:1650/2110 train_time:89260ms step_avg:54.10ms
step:1651/2110 train_time:89346ms step_avg:54.12ms
step:1652/2110 train_time:89433ms step_avg:54.14ms
step:1653/2110 train_time:89520ms step_avg:54.16ms
step:1654/2110 train_time:89607ms step_avg:54.18ms
step:1655/2110 train_time:89693ms step_avg:54.20ms
step:1656/2110 train_time:89779ms step_avg:54.21ms
step:1657/2110 train_time:89867ms step_avg:54.23ms
step:1658/2110 train_time:89955ms step_avg:54.25ms
step:1659/2110 train_time:90045ms step_avg:54.28ms
step:1660/2110 train_time:90133ms step_avg:54.30ms
step:1661/2110 train_time:90221ms step_avg:54.32ms
step:1662/2110 train_time:90309ms step_avg:54.34ms
step:1663/2110 train_time:90397ms step_avg:54.36ms
step:1664/2110 train_time:90484ms step_avg:54.38ms
step:1665/2110 train_time:90573ms step_avg:54.40ms
step:1666/2110 train_time:90661ms step_avg:54.42ms
step:1667/2110 train_time:90748ms step_avg:54.44ms
step:1668/2110 train_time:90837ms step_avg:54.46ms
step:1669/2110 train_time:90924ms step_avg:54.48ms
step:1670/2110 train_time:91013ms step_avg:54.50ms
step:1671/2110 train_time:91100ms step_avg:54.52ms
step:1672/2110 train_time:91188ms step_avg:54.54ms
step:1673/2110 train_time:91277ms step_avg:54.56ms
step:1674/2110 train_time:91365ms step_avg:54.58ms
step:1675/2110 train_time:91453ms step_avg:54.60ms
step:1676/2110 train_time:91540ms step_avg:54.62ms
step:1677/2110 train_time:91628ms step_avg:54.64ms
step:1678/2110 train_time:91716ms step_avg:54.66ms
step:1679/2110 train_time:91804ms step_avg:54.68ms
step:1680/2110 train_time:91893ms step_avg:54.70ms
step:1681/2110 train_time:91980ms step_avg:54.72ms
step:1682/2110 train_time:92069ms step_avg:54.74ms
step:1683/2110 train_time:92157ms step_avg:54.76ms
step:1684/2110 train_time:92245ms step_avg:54.78ms
step:1685/2110 train_time:92333ms step_avg:54.80ms
step:1686/2110 train_time:92422ms step_avg:54.82ms
step:1687/2110 train_time:92510ms step_avg:54.84ms
step:1688/2110 train_time:92598ms step_avg:54.86ms
step:1689/2110 train_time:92685ms step_avg:54.88ms
step:1690/2110 train_time:92774ms step_avg:54.90ms
step:1691/2110 train_time:92861ms step_avg:54.91ms
step:1692/2110 train_time:92950ms step_avg:54.93ms
step:1693/2110 train_time:93038ms step_avg:54.95ms
step:1694/2110 train_time:93126ms step_avg:54.97ms
step:1695/2110 train_time:93214ms step_avg:54.99ms
step:1696/2110 train_time:93301ms step_avg:55.01ms
step:1697/2110 train_time:93389ms step_avg:55.03ms
step:1698/2110 train_time:93478ms step_avg:55.05ms
step:1699/2110 train_time:93566ms step_avg:55.07ms
step:1700/2110 train_time:93654ms step_avg:55.09ms
step:1701/2110 train_time:93742ms step_avg:55.11ms
step:1702/2110 train_time:93832ms step_avg:55.13ms
step:1703/2110 train_time:93918ms step_avg:55.15ms
step:1704/2110 train_time:94006ms step_avg:55.17ms
step:1705/2110 train_time:94093ms step_avg:55.19ms
step:1706/2110 train_time:94181ms step_avg:55.21ms
step:1707/2110 train_time:94269ms step_avg:55.22ms
step:1708/2110 train_time:94358ms step_avg:55.24ms
step:1709/2110 train_time:94448ms step_avg:55.27ms
step:1710/2110 train_time:94536ms step_avg:55.28ms
step:1711/2110 train_time:94623ms step_avg:55.30ms
step:1712/2110 train_time:94711ms step_avg:55.32ms
step:1713/2110 train_time:94798ms step_avg:55.34ms
step:1714/2110 train_time:94886ms step_avg:55.36ms
step:1715/2110 train_time:94974ms step_avg:55.38ms
step:1716/2110 train_time:95062ms step_avg:55.40ms
step:1717/2110 train_time:95150ms step_avg:55.42ms
step:1718/2110 train_time:95238ms step_avg:55.44ms
step:1719/2110 train_time:95325ms step_avg:55.45ms
step:1720/2110 train_time:95414ms step_avg:55.47ms
step:1721/2110 train_time:95502ms step_avg:55.49ms
step:1722/2110 train_time:95591ms step_avg:55.51ms
step:1723/2110 train_time:95679ms step_avg:55.53ms
step:1724/2110 train_time:95767ms step_avg:55.55ms
step:1725/2110 train_time:95856ms step_avg:55.57ms
step:1726/2110 train_time:95944ms step_avg:55.59ms
step:1727/2110 train_time:96033ms step_avg:55.61ms
step:1728/2110 train_time:96120ms step_avg:55.62ms
step:1729/2110 train_time:96208ms step_avg:55.64ms
step:1730/2110 train_time:96296ms step_avg:55.66ms
step:1731/2110 train_time:96384ms step_avg:55.68ms
step:1732/2110 train_time:96472ms step_avg:55.70ms
step:1733/2110 train_time:96561ms step_avg:55.72ms
step:1734/2110 train_time:96649ms step_avg:55.74ms
step:1735/2110 train_time:96737ms step_avg:55.76ms
step:1736/2110 train_time:96825ms step_avg:55.77ms
step:1737/2110 train_time:96914ms step_avg:55.79ms
step:1738/2110 train_time:97002ms step_avg:55.81ms
step:1739/2110 train_time:97091ms step_avg:55.83ms
step:1740/2110 train_time:97179ms step_avg:55.85ms
step:1741/2110 train_time:97267ms step_avg:55.87ms
step:1742/2110 train_time:97356ms step_avg:55.89ms
step:1743/2110 train_time:97444ms step_avg:55.91ms
step:1744/2110 train_time:97533ms step_avg:55.93ms
step:1745/2110 train_time:97620ms step_avg:55.94ms
step:1746/2110 train_time:97708ms step_avg:55.96ms
step:1747/2110 train_time:97797ms step_avg:55.98ms
step:1748/2110 train_time:97884ms step_avg:56.00ms
step:1749/2110 train_time:97973ms step_avg:56.02ms
step:1750/2110 train_time:98061ms step_avg:56.03ms
step:1750/2110 val_loss:3.3800 train_time:98151ms step_avg:56.09ms
step:1751/2110 train_time:98197ms step_avg:56.08ms
step:1752/2110 train_time:98245ms step_avg:56.08ms
step:1753/2110 train_time:98337ms step_avg:56.10ms
step:1754/2110 train_time:98425ms step_avg:56.11ms
step:1755/2110 train_time:98514ms step_avg:56.13ms
step:1756/2110 train_time:98601ms step_avg:56.15ms
step:1757/2110 train_time:98687ms step_avg:56.17ms
step:1758/2110 train_time:98775ms step_avg:56.19ms
step:1759/2110 train_time:98861ms step_avg:56.20ms
step:1760/2110 train_time:98948ms step_avg:56.22ms
step:1761/2110 train_time:99035ms step_avg:56.24ms
step:1762/2110 train_time:99124ms step_avg:56.26ms
step:1763/2110 train_time:99215ms step_avg:56.28ms
step:1764/2110 train_time:99305ms step_avg:56.30ms
step:1765/2110 train_time:99397ms step_avg:56.32ms
step:1766/2110 train_time:99485ms step_avg:56.33ms
step:1767/2110 train_time:99573ms step_avg:56.35ms
step:1768/2110 train_time:99660ms step_avg:56.37ms
step:1769/2110 train_time:99746ms step_avg:56.39ms
step:1770/2110 train_time:99833ms step_avg:56.40ms
step:1771/2110 train_time:99920ms step_avg:56.42ms
step:1772/2110 train_time:100006ms step_avg:56.44ms
step:1773/2110 train_time:100095ms step_avg:56.46ms
step:1774/2110 train_time:100184ms step_avg:56.47ms
step:1775/2110 train_time:100274ms step_avg:56.49ms
step:1776/2110 train_time:100363ms step_avg:56.51ms
step:1777/2110 train_time:100453ms step_avg:56.53ms
step:1778/2110 train_time:100542ms step_avg:56.55ms
step:1779/2110 train_time:100629ms step_avg:56.56ms
step:1780/2110 train_time:100716ms step_avg:56.58ms
step:1781/2110 train_time:100803ms step_avg:56.60ms
step:1782/2110 train_time:100890ms step_avg:56.62ms
step:1783/2110 train_time:100977ms step_avg:56.63ms
step:1784/2110 train_time:101064ms step_avg:56.65ms
step:1785/2110 train_time:101153ms step_avg:56.67ms
step:1786/2110 train_time:101241ms step_avg:56.69ms
step:1787/2110 train_time:101330ms step_avg:56.70ms
step:1788/2110 train_time:101420ms step_avg:56.72ms
step:1789/2110 train_time:101508ms step_avg:56.74ms
step:1790/2110 train_time:101596ms step_avg:56.76ms
step:1791/2110 train_time:101684ms step_avg:56.77ms
step:1792/2110 train_time:101770ms step_avg:56.79ms
step:1793/2110 train_time:101858ms step_avg:56.81ms
step:1794/2110 train_time:101946ms step_avg:56.83ms
step:1795/2110 train_time:102032ms step_avg:56.84ms
step:1796/2110 train_time:102120ms step_avg:56.86ms
step:1797/2110 train_time:102208ms step_avg:56.88ms
step:1798/2110 train_time:102297ms step_avg:56.90ms
step:1799/2110 train_time:102386ms step_avg:56.91ms
step:1800/2110 train_time:102475ms step_avg:56.93ms
step:1801/2110 train_time:102563ms step_avg:56.95ms
step:1802/2110 train_time:102652ms step_avg:56.97ms
step:1803/2110 train_time:102740ms step_avg:56.98ms
step:1804/2110 train_time:102826ms step_avg:57.00ms
step:1805/2110 train_time:102914ms step_avg:57.02ms
step:1806/2110 train_time:103001ms step_avg:57.03ms
step:1807/2110 train_time:103089ms step_avg:57.05ms
step:1808/2110 train_time:103178ms step_avg:57.07ms
step:1809/2110 train_time:103266ms step_avg:57.08ms
step:1810/2110 train_time:103355ms step_avg:57.10ms
step:1811/2110 train_time:103443ms step_avg:57.12ms
step:1812/2110 train_time:103531ms step_avg:57.14ms
step:1813/2110 train_time:103620ms step_avg:57.15ms
step:1814/2110 train_time:103708ms step_avg:57.17ms
step:1815/2110 train_time:103797ms step_avg:57.19ms
step:1816/2110 train_time:103884ms step_avg:57.21ms
step:1817/2110 train_time:103972ms step_avg:57.22ms
step:1818/2110 train_time:104060ms step_avg:57.24ms
step:1819/2110 train_time:104148ms step_avg:57.26ms
step:1820/2110 train_time:104237ms step_avg:57.27ms
step:1821/2110 train_time:104325ms step_avg:57.29ms
step:1822/2110 train_time:104413ms step_avg:57.31ms
step:1823/2110 train_time:104502ms step_avg:57.32ms
step:1824/2110 train_time:104590ms step_avg:57.34ms
step:1825/2110 train_time:104679ms step_avg:57.36ms
step:1826/2110 train_time:104765ms step_avg:57.37ms
step:1827/2110 train_time:104854ms step_avg:57.39ms
step:1828/2110 train_time:104940ms step_avg:57.41ms
step:1829/2110 train_time:105029ms step_avg:57.42ms
step:1830/2110 train_time:105116ms step_avg:57.44ms
step:1831/2110 train_time:105204ms step_avg:57.46ms
step:1832/2110 train_time:105294ms step_avg:57.47ms
step:1833/2110 train_time:105382ms step_avg:57.49ms
step:1834/2110 train_time:105470ms step_avg:57.51ms
step:1835/2110 train_time:105560ms step_avg:57.53ms
step:1836/2110 train_time:105648ms step_avg:57.54ms
step:1837/2110 train_time:105736ms step_avg:57.56ms
step:1838/2110 train_time:105823ms step_avg:57.58ms
step:1839/2110 train_time:105911ms step_avg:57.59ms
step:1840/2110 train_time:105999ms step_avg:57.61ms
step:1841/2110 train_time:106087ms step_avg:57.62ms
step:1842/2110 train_time:106175ms step_avg:57.64ms
step:1843/2110 train_time:106263ms step_avg:57.66ms
step:1844/2110 train_time:106352ms step_avg:57.67ms
step:1845/2110 train_time:106441ms step_avg:57.69ms
step:1846/2110 train_time:106530ms step_avg:57.71ms
step:1847/2110 train_time:106619ms step_avg:57.73ms
step:1848/2110 train_time:106706ms step_avg:57.74ms
step:1849/2110 train_time:106794ms step_avg:57.76ms
step:1850/2110 train_time:106882ms step_avg:57.77ms
step:1851/2110 train_time:106971ms step_avg:57.79ms
step:1852/2110 train_time:107058ms step_avg:57.81ms
step:1853/2110 train_time:107146ms step_avg:57.82ms
step:1854/2110 train_time:107234ms step_avg:57.84ms
step:1855/2110 train_time:107322ms step_avg:57.86ms
step:1856/2110 train_time:107410ms step_avg:57.87ms
step:1857/2110 train_time:107498ms step_avg:57.89ms
step:1858/2110 train_time:107586ms step_avg:57.90ms
step:1859/2110 train_time:107674ms step_avg:57.92ms
step:1860/2110 train_time:107762ms step_avg:57.94ms
step:1861/2110 train_time:107850ms step_avg:57.95ms
step:1862/2110 train_time:107938ms step_avg:57.97ms
step:1863/2110 train_time:108026ms step_avg:57.98ms
step:1864/2110 train_time:108115ms step_avg:58.00ms
step:1865/2110 train_time:108201ms step_avg:58.02ms
step:1866/2110 train_time:108289ms step_avg:58.03ms
step:1867/2110 train_time:108378ms step_avg:58.05ms
step:1868/2110 train_time:108465ms step_avg:58.06ms
step:1869/2110 train_time:108555ms step_avg:58.08ms
step:1870/2110 train_time:108642ms step_avg:58.10ms
step:1871/2110 train_time:108730ms step_avg:58.11ms
step:1872/2110 train_time:108821ms step_avg:58.13ms
step:1873/2110 train_time:108907ms step_avg:58.15ms
step:1874/2110 train_time:108994ms step_avg:58.16ms
step:1875/2110 train_time:109082ms step_avg:58.18ms
step:1876/2110 train_time:109171ms step_avg:58.19ms
step:1877/2110 train_time:109260ms step_avg:58.21ms
step:1878/2110 train_time:109349ms step_avg:58.23ms
step:1879/2110 train_time:109437ms step_avg:58.24ms
step:1880/2110 train_time:109524ms step_avg:58.26ms
step:1881/2110 train_time:109612ms step_avg:58.27ms
step:1882/2110 train_time:109699ms step_avg:58.29ms
step:1883/2110 train_time:109787ms step_avg:58.30ms
step:1884/2110 train_time:109876ms step_avg:58.32ms
step:1885/2110 train_time:109963ms step_avg:58.34ms
step:1886/2110 train_time:110050ms step_avg:58.35ms
step:1887/2110 train_time:110140ms step_avg:58.37ms
step:1888/2110 train_time:110228ms step_avg:58.38ms
step:1889/2110 train_time:110317ms step_avg:58.40ms
step:1890/2110 train_time:110404ms step_avg:58.41ms
step:1891/2110 train_time:110493ms step_avg:58.43ms
step:1892/2110 train_time:110580ms step_avg:58.45ms
step:1893/2110 train_time:110668ms step_avg:58.46ms
step:1894/2110 train_time:110758ms step_avg:58.48ms
step:1895/2110 train_time:110846ms step_avg:58.49ms
step:1896/2110 train_time:110935ms step_avg:58.51ms
step:1897/2110 train_time:111022ms step_avg:58.52ms
step:1898/2110 train_time:111110ms step_avg:58.54ms
step:1899/2110 train_time:111198ms step_avg:58.56ms
step:1900/2110 train_time:111286ms step_avg:58.57ms
step:1901/2110 train_time:111374ms step_avg:58.59ms
step:1902/2110 train_time:111462ms step_avg:58.60ms
step:1903/2110 train_time:111551ms step_avg:58.62ms
step:1904/2110 train_time:111639ms step_avg:58.63ms
step:1905/2110 train_time:111728ms step_avg:58.65ms
step:1906/2110 train_time:111814ms step_avg:58.66ms
step:1907/2110 train_time:111903ms step_avg:58.68ms
step:1908/2110 train_time:111994ms step_avg:58.70ms
step:1909/2110 train_time:112080ms step_avg:58.71ms
step:1910/2110 train_time:112168ms step_avg:58.73ms
step:1911/2110 train_time:112257ms step_avg:58.74ms
step:1912/2110 train_time:112344ms step_avg:58.76ms
step:1913/2110 train_time:112433ms step_avg:58.77ms
step:1914/2110 train_time:112521ms step_avg:58.79ms
step:1915/2110 train_time:112608ms step_avg:58.80ms
step:1916/2110 train_time:112698ms step_avg:58.82ms
step:1917/2110 train_time:112786ms step_avg:58.83ms
step:1918/2110 train_time:112874ms step_avg:58.85ms
step:1919/2110 train_time:112963ms step_avg:58.87ms
step:1920/2110 train_time:113051ms step_avg:58.88ms
step:1921/2110 train_time:113140ms step_avg:58.90ms
step:1922/2110 train_time:113228ms step_avg:58.91ms
step:1923/2110 train_time:113319ms step_avg:58.93ms
step:1924/2110 train_time:113406ms step_avg:58.94ms
step:1925/2110 train_time:113492ms step_avg:58.96ms
step:1926/2110 train_time:113580ms step_avg:58.97ms
step:1927/2110 train_time:113668ms step_avg:58.99ms
step:1928/2110 train_time:113757ms step_avg:59.00ms
step:1929/2110 train_time:113844ms step_avg:59.02ms
step:1930/2110 train_time:113933ms step_avg:59.03ms
step:1931/2110 train_time:114021ms step_avg:59.05ms
step:1932/2110 train_time:114109ms step_avg:59.06ms
step:1933/2110 train_time:114197ms step_avg:59.08ms
step:1934/2110 train_time:114285ms step_avg:59.09ms
step:1935/2110 train_time:114373ms step_avg:59.11ms
step:1936/2110 train_time:114462ms step_avg:59.12ms
step:1937/2110 train_time:114549ms step_avg:59.14ms
step:1938/2110 train_time:114639ms step_avg:59.15ms
step:1939/2110 train_time:114726ms step_avg:59.17ms
step:1940/2110 train_time:114813ms step_avg:59.18ms
step:1941/2110 train_time:114902ms step_avg:59.20ms
step:1942/2110 train_time:114992ms step_avg:59.21ms
step:1943/2110 train_time:115078ms step_avg:59.23ms
step:1944/2110 train_time:115168ms step_avg:59.24ms
step:1945/2110 train_time:115256ms step_avg:59.26ms
step:1946/2110 train_time:115343ms step_avg:59.27ms
step:1947/2110 train_time:115431ms step_avg:59.29ms
step:1948/2110 train_time:115521ms step_avg:59.30ms
step:1949/2110 train_time:115607ms step_avg:59.32ms
step:1950/2110 train_time:115695ms step_avg:59.33ms
step:1951/2110 train_time:115785ms step_avg:59.35ms
step:1952/2110 train_time:115872ms step_avg:59.36ms
step:1953/2110 train_time:115961ms step_avg:59.38ms
step:1954/2110 train_time:116049ms step_avg:59.39ms
step:1955/2110 train_time:116136ms step_avg:59.40ms
step:1956/2110 train_time:116224ms step_avg:59.42ms
step:1957/2110 train_time:116311ms step_avg:59.43ms
step:1958/2110 train_time:116400ms step_avg:59.45ms
step:1959/2110 train_time:116489ms step_avg:59.46ms
step:1960/2110 train_time:116576ms step_avg:59.48ms
step:1961/2110 train_time:116663ms step_avg:59.49ms
step:1962/2110 train_time:116753ms step_avg:59.51ms
step:1963/2110 train_time:116840ms step_avg:59.52ms
step:1964/2110 train_time:116928ms step_avg:59.54ms
step:1965/2110 train_time:117016ms step_avg:59.55ms
step:1966/2110 train_time:117104ms step_avg:59.56ms
step:1967/2110 train_time:117192ms step_avg:59.58ms
step:1968/2110 train_time:117282ms step_avg:59.59ms
step:1969/2110 train_time:117367ms step_avg:59.61ms
step:1970/2110 train_time:117457ms step_avg:59.62ms
step:1971/2110 train_time:117546ms step_avg:59.64ms
step:1972/2110 train_time:117634ms step_avg:59.65ms
step:1973/2110 train_time:117721ms step_avg:59.67ms
step:1974/2110 train_time:117808ms step_avg:59.68ms
step:1975/2110 train_time:117897ms step_avg:59.69ms
step:1976/2110 train_time:117985ms step_avg:59.71ms
step:1977/2110 train_time:118072ms step_avg:59.72ms
step:1978/2110 train_time:118160ms step_avg:59.74ms
step:1979/2110 train_time:118249ms step_avg:59.75ms
step:1980/2110 train_time:118338ms step_avg:59.77ms
step:1981/2110 train_time:118424ms step_avg:59.78ms
step:1982/2110 train_time:118514ms step_avg:59.80ms
step:1983/2110 train_time:118601ms step_avg:59.81ms
step:1984/2110 train_time:118689ms step_avg:59.82ms
step:1985/2110 train_time:118778ms step_avg:59.84ms
step:1986/2110 train_time:118865ms step_avg:59.85ms
step:1987/2110 train_time:118954ms step_avg:59.87ms
step:1988/2110 train_time:119043ms step_avg:59.88ms
step:1989/2110 train_time:119130ms step_avg:59.89ms
step:1990/2110 train_time:119219ms step_avg:59.91ms
step:1991/2110 train_time:119306ms step_avg:59.92ms
step:1992/2110 train_time:119396ms step_avg:59.94ms
step:1993/2110 train_time:119482ms step_avg:59.95ms
step:1994/2110 train_time:119571ms step_avg:59.97ms
step:1995/2110 train_time:119661ms step_avg:59.98ms
step:1996/2110 train_time:119750ms step_avg:60.00ms
step:1997/2110 train_time:119835ms step_avg:60.01ms
step:1998/2110 train_time:119923ms step_avg:60.02ms
step:1999/2110 train_time:120010ms step_avg:60.04ms
step:2000/2110 train_time:120100ms step_avg:60.05ms
step:2000/2110 val_loss:3.3046 train_time:120188ms step_avg:60.09ms
step:2001/2110 train_time:120223ms step_avg:60.08ms
step:2002/2110 train_time:120282ms step_avg:60.08ms
step:2003/2110 train_time:120377ms step_avg:60.10ms
step:2004/2110 train_time:120464ms step_avg:60.11ms
step:2005/2110 train_time:120552ms step_avg:60.13ms
step:2006/2110 train_time:120640ms step_avg:60.14ms
step:2007/2110 train_time:120728ms step_avg:60.15ms
step:2008/2110 train_time:120813ms step_avg:60.17ms
step:2009/2110 train_time:120900ms step_avg:60.18ms
step:2010/2110 train_time:120987ms step_avg:60.19ms
step:2011/2110 train_time:121073ms step_avg:60.21ms
step:2012/2110 train_time:121163ms step_avg:60.22ms
step:2013/2110 train_time:121255ms step_avg:60.24ms
step:2014/2110 train_time:121346ms step_avg:60.25ms
step:2015/2110 train_time:121435ms step_avg:60.27ms
step:2016/2110 train_time:121523ms step_avg:60.28ms
step:2017/2110 train_time:121610ms step_avg:60.29ms
step:2018/2110 train_time:121698ms step_avg:60.31ms
step:2019/2110 train_time:121787ms step_avg:60.32ms
step:2020/2110 train_time:121873ms step_avg:60.33ms
step:2021/2110 train_time:121960ms step_avg:60.35ms
step:2022/2110 train_time:122047ms step_avg:60.36ms
step:2023/2110 train_time:122134ms step_avg:60.37ms
step:2024/2110 train_time:122225ms step_avg:60.39ms
step:2025/2110 train_time:122314ms step_avg:60.40ms
step:2026/2110 train_time:122405ms step_avg:60.42ms
step:2027/2110 train_time:122492ms step_avg:60.43ms
step:2028/2110 train_time:122580ms step_avg:60.44ms
step:2029/2110 train_time:122668ms step_avg:60.46ms
step:2030/2110 train_time:122759ms step_avg:60.47ms
step:2031/2110 train_time:122844ms step_avg:60.48ms
step:2032/2110 train_time:122931ms step_avg:60.50ms
step:2033/2110 train_time:123018ms step_avg:60.51ms
step:2034/2110 train_time:123107ms step_avg:60.52ms
step:2035/2110 train_time:123195ms step_avg:60.54ms
step:2036/2110 train_time:123287ms step_avg:60.55ms
step:2037/2110 train_time:123375ms step_avg:60.57ms
step:2038/2110 train_time:123464ms step_avg:60.58ms
step:2039/2110 train_time:123551ms step_avg:60.59ms
step:2040/2110 train_time:123639ms step_avg:60.61ms
step:2041/2110 train_time:123726ms step_avg:60.62ms
step:2042/2110 train_time:123814ms step_avg:60.63ms
step:2043/2110 train_time:123900ms step_avg:60.65ms
step:2044/2110 train_time:123988ms step_avg:60.66ms
step:2045/2110 train_time:124075ms step_avg:60.67ms
step:2046/2110 train_time:124164ms step_avg:60.69ms
step:2047/2110 train_time:124253ms step_avg:60.70ms
step:2048/2110 train_time:124342ms step_avg:60.71ms
step:2049/2110 train_time:124431ms step_avg:60.73ms
step:2050/2110 train_time:124519ms step_avg:60.74ms
step:2051/2110 train_time:124608ms step_avg:60.75ms
step:2052/2110 train_time:124696ms step_avg:60.77ms
step:2053/2110 train_time:124784ms step_avg:60.78ms
step:2054/2110 train_time:124871ms step_avg:60.79ms
step:2055/2110 train_time:124960ms step_avg:60.81ms
step:2056/2110 train_time:125048ms step_avg:60.82ms
step:2057/2110 train_time:125136ms step_avg:60.83ms
step:2058/2110 train_time:125223ms step_avg:60.85ms
step:2059/2110 train_time:125312ms step_avg:60.86ms
step:2060/2110 train_time:125400ms step_avg:60.87ms
step:2061/2110 train_time:125488ms step_avg:60.89ms
step:2062/2110 train_time:125576ms step_avg:60.90ms
step:2063/2110 train_time:125664ms step_avg:60.91ms
step:2064/2110 train_time:125752ms step_avg:60.93ms
step:2065/2110 train_time:125840ms step_avg:60.94ms
step:2066/2110 train_time:125928ms step_avg:60.95ms
step:2067/2110 train_time:126016ms step_avg:60.97ms
step:2068/2110 train_time:126105ms step_avg:60.98ms
step:2069/2110 train_time:126193ms step_avg:60.99ms
step:2070/2110 train_time:126281ms step_avg:61.01ms
step:2071/2110 train_time:126370ms step_avg:61.02ms
step:2072/2110 train_time:126459ms step_avg:61.03ms
step:2073/2110 train_time:126548ms step_avg:61.05ms
step:2074/2110 train_time:126635ms step_avg:61.06ms
step:2075/2110 train_time:126724ms step_avg:61.07ms
step:2076/2110 train_time:126811ms step_avg:61.08ms
step:2077/2110 train_time:126900ms step_avg:61.10ms
step:2078/2110 train_time:126989ms step_avg:61.11ms
step:2079/2110 train_time:127078ms step_avg:61.12ms
step:2080/2110 train_time:127166ms step_avg:61.14ms
step:2081/2110 train_time:127254ms step_avg:61.15ms
step:2082/2110 train_time:127342ms step_avg:61.16ms
step:2083/2110 train_time:127430ms step_avg:61.18ms
step:2084/2110 train_time:127519ms step_avg:61.19ms
step:2085/2110 train_time:127608ms step_avg:61.20ms
step:2086/2110 train_time:127696ms step_avg:61.22ms
step:2087/2110 train_time:127784ms step_avg:61.23ms
step:2088/2110 train_time:127871ms step_avg:61.24ms
step:2089/2110 train_time:127961ms step_avg:61.25ms
step:2090/2110 train_time:128048ms step_avg:61.27ms
step:2091/2110 train_time:128137ms step_avg:61.28ms
step:2092/2110 train_time:128226ms step_avg:61.29ms
step:2093/2110 train_time:128315ms step_avg:61.31ms
step:2094/2110 train_time:128402ms step_avg:61.32ms
step:2095/2110 train_time:128491ms step_avg:61.33ms
step:2096/2110 train_time:128579ms step_avg:61.35ms
step:2097/2110 train_time:128669ms step_avg:61.36ms
step:2098/2110 train_time:128757ms step_avg:61.37ms
step:2099/2110 train_time:128847ms step_avg:61.38ms
step:2100/2110 train_time:128935ms step_avg:61.40ms
step:2101/2110 train_time:129024ms step_avg:61.41ms
step:2102/2110 train_time:129111ms step_avg:61.42ms
step:2103/2110 train_time:129200ms step_avg:61.44ms
step:2104/2110 train_time:129288ms step_avg:61.45ms
step:2105/2110 train_time:129377ms step_avg:61.46ms
step:2106/2110 train_time:129466ms step_avg:61.47ms
step:2107/2110 train_time:129554ms step_avg:61.49ms
step:2108/2110 train_time:129643ms step_avg:61.50ms
step:2109/2110 train_time:129731ms step_avg:61.51ms
step:2110/2110 train_time:129819ms step_avg:61.53ms
step:2110/2110 val_loss:3.2803 train_time:129911ms step_avg:61.57ms
peak memory allocated: 29816 MiB reserved: 43736 MiB

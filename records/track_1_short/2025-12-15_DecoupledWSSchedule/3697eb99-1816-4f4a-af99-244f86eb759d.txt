import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 15 21:47:32 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   39C    P0            126W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   43C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   43C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   42C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   42C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   37C    P0            120W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              88      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              88      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2110 train_time:117ms step_avg:117.45ms
step:2/2110 train_time:156ms step_avg:77.89ms
step:3/2110 train_time:183ms step_avg:60.84ms
step:4/2110 train_time:211ms step_avg:52.80ms
step:5/2110 train_time:237ms step_avg:47.37ms
step:6/2110 train_time:498ms step_avg:82.99ms
step:7/2110 train_time:629ms step_avg:89.91ms
step:8/2110 train_time:662ms step_avg:82.73ms
step:9/2110 train_time:694ms step_avg:77.15ms
step:10/2110 train_time:727ms step_avg:72.67ms
step:11/2110 train_time:760ms step_avg:69.11ms
step:12/2110 train_time:793ms step_avg:66.05ms
step:13/2110 train_time:826ms step_avg:63.53ms
step:14/2110 train_time:858ms step_avg:61.31ms
step:15/2110 train_time:892ms step_avg:59.45ms
step:16/2110 train_time:924ms step_avg:57.78ms
step:17/2110 train_time:958ms step_avg:56.33ms
step:18/2110 train_time:990ms step_avg:55.02ms
step:19/2110 train_time:1024ms step_avg:53.87ms
step:20/2110 train_time:1056ms step_avg:52.82ms
step:21/2110 train_time:1090ms step_avg:51.90ms
step:22/2110 train_time:1125ms step_avg:51.12ms
step:23/2110 train_time:1156ms step_avg:50.26ms
step:24/2110 train_time:1189ms step_avg:49.56ms
step:25/2110 train_time:1222ms step_avg:48.89ms
step:26/2110 train_time:1257ms step_avg:48.34ms
step:27/2110 train_time:1289ms step_avg:47.76ms
step:28/2110 train_time:1321ms step_avg:47.19ms
step:29/2110 train_time:1354ms step_avg:46.69ms
step:30/2110 train_time:1387ms step_avg:46.25ms
step:31/2110 train_time:1420ms step_avg:45.80ms
step:32/2110 train_time:1455ms step_avg:45.46ms
step:33/2110 train_time:1488ms step_avg:45.10ms
step:34/2110 train_time:1522ms step_avg:44.75ms
step:35/2110 train_time:1555ms step_avg:44.42ms
step:36/2110 train_time:1589ms step_avg:44.13ms
step:37/2110 train_time:1621ms step_avg:43.82ms
step:38/2110 train_time:1656ms step_avg:43.58ms
step:39/2110 train_time:1689ms step_avg:43.31ms
step:40/2110 train_time:1725ms step_avg:43.12ms
step:41/2110 train_time:1757ms step_avg:42.86ms
step:42/2110 train_time:1790ms step_avg:42.62ms
step:43/2110 train_time:1824ms step_avg:42.43ms
step:44/2110 train_time:1857ms step_avg:42.21ms
step:45/2110 train_time:1888ms step_avg:41.97ms
step:46/2110 train_time:1925ms step_avg:41.85ms
step:47/2110 train_time:1957ms step_avg:41.63ms
step:48/2110 train_time:1990ms step_avg:41.47ms
step:49/2110 train_time:2020ms step_avg:41.23ms
step:50/2110 train_time:2054ms step_avg:41.08ms
step:51/2110 train_time:2088ms step_avg:40.94ms
step:52/2110 train_time:2125ms step_avg:40.86ms
step:53/2110 train_time:2153ms step_avg:40.63ms
step:54/2110 train_time:2185ms step_avg:40.46ms
step:55/2110 train_time:2218ms step_avg:40.33ms
step:56/2110 train_time:2252ms step_avg:40.21ms
step:57/2110 train_time:2286ms step_avg:40.11ms
step:58/2110 train_time:2319ms step_avg:39.98ms
step:59/2110 train_time:2352ms step_avg:39.86ms
step:60/2110 train_time:2386ms step_avg:39.77ms
step:61/2110 train_time:2418ms step_avg:39.64ms
step:62/2110 train_time:2453ms step_avg:39.57ms
step:63/2110 train_time:2487ms step_avg:39.47ms
step:64/2110 train_time:2520ms step_avg:39.37ms
step:65/2110 train_time:2550ms step_avg:39.23ms
step:66/2110 train_time:2583ms step_avg:39.14ms
step:67/2110 train_time:2618ms step_avg:39.07ms
step:68/2110 train_time:2650ms step_avg:38.97ms
step:69/2110 train_time:2683ms step_avg:38.88ms
step:70/2110 train_time:2719ms step_avg:38.84ms
step:71/2110 train_time:2750ms step_avg:38.73ms
step:72/2110 train_time:2783ms step_avg:38.66ms
step:73/2110 train_time:2817ms step_avg:38.59ms
step:74/2110 train_time:2849ms step_avg:38.51ms
step:75/2110 train_time:2882ms step_avg:38.42ms
step:76/2110 train_time:2916ms step_avg:38.37ms
step:77/2110 train_time:2950ms step_avg:38.32ms
step:78/2110 train_time:2985ms step_avg:38.26ms
step:79/2110 train_time:3015ms step_avg:38.16ms
step:80/2110 train_time:3050ms step_avg:38.12ms
step:81/2110 train_time:3080ms step_avg:38.03ms
step:82/2110 train_time:3116ms step_avg:38.00ms
step:83/2110 train_time:3148ms step_avg:37.92ms
step:84/2110 train_time:3182ms step_avg:37.88ms
step:85/2110 train_time:3215ms step_avg:37.83ms
step:86/2110 train_time:3251ms step_avg:37.80ms
step:87/2110 train_time:3282ms step_avg:37.73ms
step:88/2110 train_time:3316ms step_avg:37.69ms
step:89/2110 train_time:3348ms step_avg:37.62ms
step:90/2110 train_time:3379ms step_avg:37.55ms
step:91/2110 train_time:3411ms step_avg:37.49ms
step:92/2110 train_time:3445ms step_avg:37.45ms
step:93/2110 train_time:3478ms step_avg:37.39ms
step:94/2110 train_time:3511ms step_avg:37.36ms
step:95/2110 train_time:3544ms step_avg:37.31ms
step:96/2110 train_time:3578ms step_avg:37.28ms
step:97/2110 train_time:3610ms step_avg:37.22ms
step:98/2110 train_time:3644ms step_avg:37.18ms
step:99/2110 train_time:3677ms step_avg:37.14ms
step:100/2110 train_time:3710ms step_avg:37.10ms
step:101/2110 train_time:3743ms step_avg:37.06ms
step:102/2110 train_time:3776ms step_avg:37.02ms
step:103/2110 train_time:3809ms step_avg:36.98ms
step:104/2110 train_time:3844ms step_avg:36.96ms
step:105/2110 train_time:3877ms step_avg:36.92ms
step:106/2110 train_time:3915ms step_avg:36.94ms
step:107/2110 train_time:3948ms step_avg:36.90ms
step:108/2110 train_time:3985ms step_avg:36.90ms
step:109/2110 train_time:4020ms step_avg:36.88ms
step:110/2110 train_time:4055ms step_avg:36.86ms
step:111/2110 train_time:4088ms step_avg:36.83ms
step:112/2110 train_time:4125ms step_avg:36.83ms
step:113/2110 train_time:4160ms step_avg:36.81ms
step:114/2110 train_time:4199ms step_avg:36.83ms
step:115/2110 train_time:4235ms step_avg:36.83ms
step:116/2110 train_time:4269ms step_avg:36.81ms
step:117/2110 train_time:4303ms step_avg:36.78ms
step:118/2110 train_time:4340ms step_avg:36.78ms
step:119/2110 train_time:4374ms step_avg:36.75ms
step:120/2110 train_time:4411ms step_avg:36.76ms
step:121/2110 train_time:4445ms step_avg:36.73ms
step:122/2110 train_time:4480ms step_avg:36.72ms
step:123/2110 train_time:4515ms step_avg:36.71ms
step:124/2110 train_time:4550ms step_avg:36.70ms
step:125/2110 train_time:4583ms step_avg:36.66ms
step:126/2110 train_time:4621ms step_avg:36.67ms
step:127/2110 train_time:4656ms step_avg:36.66ms
step:128/2110 train_time:4691ms step_avg:36.65ms
step:129/2110 train_time:4722ms step_avg:36.61ms
step:130/2110 train_time:4758ms step_avg:36.60ms
step:131/2110 train_time:4790ms step_avg:36.57ms
step:132/2110 train_time:4827ms step_avg:36.57ms
step:133/2110 train_time:4860ms step_avg:36.55ms
step:134/2110 train_time:4897ms step_avg:36.55ms
step:135/2110 train_time:4932ms step_avg:36.53ms
step:136/2110 train_time:4970ms step_avg:36.54ms
step:137/2110 train_time:5004ms step_avg:36.53ms
step:138/2110 train_time:5043ms step_avg:36.54ms
step:139/2110 train_time:5077ms step_avg:36.53ms
step:140/2110 train_time:5111ms step_avg:36.51ms
step:141/2110 train_time:5147ms step_avg:36.50ms
step:142/2110 train_time:5182ms step_avg:36.49ms
step:143/2110 train_time:5218ms step_avg:36.49ms
step:144/2110 train_time:5254ms step_avg:36.49ms
step:145/2110 train_time:5288ms step_avg:36.47ms
step:146/2110 train_time:5326ms step_avg:36.48ms
step:147/2110 train_time:5358ms step_avg:36.45ms
step:148/2110 train_time:5396ms step_avg:36.46ms
step:149/2110 train_time:5430ms step_avg:36.44ms
step:150/2110 train_time:5465ms step_avg:36.43ms
step:151/2110 train_time:5498ms step_avg:36.41ms
step:152/2110 train_time:5536ms step_avg:36.42ms
step:153/2110 train_time:5571ms step_avg:36.41ms
step:154/2110 train_time:5605ms step_avg:36.40ms
step:155/2110 train_time:5640ms step_avg:36.39ms
step:156/2110 train_time:5678ms step_avg:36.40ms
step:157/2110 train_time:5711ms step_avg:36.37ms
step:158/2110 train_time:5749ms step_avg:36.38ms
step:159/2110 train_time:5784ms step_avg:36.38ms
step:160/2110 train_time:5820ms step_avg:36.38ms
step:161/2110 train_time:5855ms step_avg:36.37ms
step:162/2110 train_time:5890ms step_avg:36.36ms
step:163/2110 train_time:5924ms step_avg:36.34ms
step:164/2110 train_time:5959ms step_avg:36.33ms
step:165/2110 train_time:5989ms step_avg:36.29ms
step:166/2110 train_time:6023ms step_avg:36.28ms
step:167/2110 train_time:6054ms step_avg:36.25ms
step:168/2110 train_time:6088ms step_avg:36.24ms
step:169/2110 train_time:6121ms step_avg:36.22ms
step:170/2110 train_time:6157ms step_avg:36.22ms
step:171/2110 train_time:6190ms step_avg:36.20ms
step:172/2110 train_time:6225ms step_avg:36.19ms
step:173/2110 train_time:6255ms step_avg:36.16ms
step:174/2110 train_time:6291ms step_avg:36.16ms
step:175/2110 train_time:6321ms step_avg:36.12ms
step:176/2110 train_time:6355ms step_avg:36.11ms
step:177/2110 train_time:6388ms step_avg:36.09ms
step:178/2110 train_time:6424ms step_avg:36.09ms
step:179/2110 train_time:6458ms step_avg:36.08ms
step:180/2110 train_time:6489ms step_avg:36.05ms
step:181/2110 train_time:6519ms step_avg:36.02ms
step:182/2110 train_time:6551ms step_avg:36.00ms
step:183/2110 train_time:6582ms step_avg:35.97ms
step:184/2110 train_time:6612ms step_avg:35.93ms
step:185/2110 train_time:6640ms step_avg:35.89ms
step:186/2110 train_time:6667ms step_avg:35.84ms
step:187/2110 train_time:6696ms step_avg:35.81ms
step:188/2110 train_time:6730ms step_avg:35.80ms
step:189/2110 train_time:6762ms step_avg:35.78ms
step:190/2110 train_time:6796ms step_avg:35.77ms
step:191/2110 train_time:6831ms step_avg:35.76ms
step:192/2110 train_time:6867ms step_avg:35.76ms
step:193/2110 train_time:6898ms step_avg:35.74ms
step:194/2110 train_time:6931ms step_avg:35.73ms
step:195/2110 train_time:6965ms step_avg:35.72ms
step:196/2110 train_time:6999ms step_avg:35.71ms
step:197/2110 train_time:7027ms step_avg:35.67ms
step:198/2110 train_time:7060ms step_avg:35.66ms
step:199/2110 train_time:7093ms step_avg:35.64ms
step:200/2110 train_time:7129ms step_avg:35.65ms
step:201/2110 train_time:7164ms step_avg:35.64ms
step:202/2110 train_time:7203ms step_avg:35.66ms
step:203/2110 train_time:7237ms step_avg:35.65ms
step:204/2110 train_time:7276ms step_avg:35.66ms
step:205/2110 train_time:7309ms step_avg:35.66ms
step:206/2110 train_time:7346ms step_avg:35.66ms
step:207/2110 train_time:7380ms step_avg:35.65ms
step:208/2110 train_time:7416ms step_avg:35.65ms
step:209/2110 train_time:7446ms step_avg:35.63ms
step:210/2110 train_time:7480ms step_avg:35.62ms
step:211/2110 train_time:7514ms step_avg:35.61ms
step:212/2110 train_time:7549ms step_avg:35.61ms
step:213/2110 train_time:7579ms step_avg:35.58ms
step:214/2110 train_time:7611ms step_avg:35.57ms
step:215/2110 train_time:7646ms step_avg:35.56ms
step:216/2110 train_time:7683ms step_avg:35.57ms
step:217/2110 train_time:7717ms step_avg:35.56ms
step:218/2110 train_time:7756ms step_avg:35.58ms
step:219/2110 train_time:7793ms step_avg:35.58ms
step:220/2110 train_time:7829ms step_avg:35.59ms
step:221/2110 train_time:7865ms step_avg:35.59ms
step:222/2110 train_time:7902ms step_avg:35.59ms
step:223/2110 train_time:7936ms step_avg:35.59ms
step:224/2110 train_time:7972ms step_avg:35.59ms
step:225/2110 train_time:8007ms step_avg:35.59ms
step:226/2110 train_time:8043ms step_avg:35.59ms
step:227/2110 train_time:8079ms step_avg:35.59ms
step:228/2110 train_time:8115ms step_avg:35.59ms
step:229/2110 train_time:8151ms step_avg:35.59ms
step:230/2110 train_time:8192ms step_avg:35.62ms
step:231/2110 train_time:8229ms step_avg:35.62ms
step:232/2110 train_time:8266ms step_avg:35.63ms
step:233/2110 train_time:8297ms step_avg:35.61ms
step:234/2110 train_time:8334ms step_avg:35.61ms
step:235/2110 train_time:8368ms step_avg:35.61ms
step:236/2110 train_time:8407ms step_avg:35.62ms
step:237/2110 train_time:8443ms step_avg:35.63ms
step:238/2110 train_time:8481ms step_avg:35.63ms
step:239/2110 train_time:8516ms step_avg:35.63ms
step:240/2110 train_time:8552ms step_avg:35.63ms
step:241/2110 train_time:8585ms step_avg:35.62ms
step:242/2110 train_time:8621ms step_avg:35.63ms
step:243/2110 train_time:8657ms step_avg:35.62ms
step:244/2110 train_time:8693ms step_avg:35.63ms
step:245/2110 train_time:8727ms step_avg:35.62ms
step:246/2110 train_time:8764ms step_avg:35.63ms
step:247/2110 train_time:8797ms step_avg:35.61ms
step:248/2110 train_time:8834ms step_avg:35.62ms
step:249/2110 train_time:8868ms step_avg:35.61ms
step:250/2110 train_time:8903ms step_avg:35.61ms
step:250/2110 val_loss:4.3006 train_time:8907ms step_avg:35.63ms
step:251/2110 train_time:8945ms step_avg:35.64ms
step:252/2110 train_time:8979ms step_avg:35.63ms
step:253/2110 train_time:9015ms step_avg:35.63ms
step:254/2110 train_time:9048ms step_avg:35.62ms
step:255/2110 train_time:9083ms step_avg:35.62ms
step:256/2110 train_time:9119ms step_avg:35.62ms
step:257/2110 train_time:9151ms step_avg:35.61ms
step:258/2110 train_time:9184ms step_avg:35.60ms
step:259/2110 train_time:9216ms step_avg:35.58ms
step:260/2110 train_time:9250ms step_avg:35.58ms
step:261/2110 train_time:9283ms step_avg:35.57ms
step:262/2110 train_time:9318ms step_avg:35.56ms
step:263/2110 train_time:9352ms step_avg:35.56ms
step:264/2110 train_time:9387ms step_avg:35.56ms
step:265/2110 train_time:9423ms step_avg:35.56ms
step:266/2110 train_time:9457ms step_avg:35.55ms
step:267/2110 train_time:9489ms step_avg:35.54ms
step:268/2110 train_time:9524ms step_avg:35.54ms
step:269/2110 train_time:9558ms step_avg:35.53ms
step:270/2110 train_time:9594ms step_avg:35.53ms
step:271/2110 train_time:9630ms step_avg:35.53ms
step:272/2110 train_time:9664ms step_avg:35.53ms
step:273/2110 train_time:9694ms step_avg:35.51ms
step:274/2110 train_time:9733ms step_avg:35.52ms
step:275/2110 train_time:9766ms step_avg:35.51ms
step:276/2110 train_time:9801ms step_avg:35.51ms
step:277/2110 train_time:9836ms step_avg:35.51ms
step:278/2110 train_time:9873ms step_avg:35.52ms
step:279/2110 train_time:9908ms step_avg:35.51ms
step:280/2110 train_time:9943ms step_avg:35.51ms
step:281/2110 train_time:9974ms step_avg:35.50ms
step:282/2110 train_time:10008ms step_avg:35.49ms
step:283/2110 train_time:10038ms step_avg:35.47ms
step:284/2110 train_time:10070ms step_avg:35.46ms
step:285/2110 train_time:10101ms step_avg:35.44ms
step:286/2110 train_time:10135ms step_avg:35.44ms
step:287/2110 train_time:10166ms step_avg:35.42ms
step:288/2110 train_time:10200ms step_avg:35.42ms
step:289/2110 train_time:10234ms step_avg:35.41ms
step:290/2110 train_time:10269ms step_avg:35.41ms
step:291/2110 train_time:10299ms step_avg:35.39ms
step:292/2110 train_time:10332ms step_avg:35.38ms
step:293/2110 train_time:10359ms step_avg:35.36ms
step:294/2110 train_time:10389ms step_avg:35.34ms
step:295/2110 train_time:10416ms step_avg:35.31ms
step:296/2110 train_time:10444ms step_avg:35.29ms
step:297/2110 train_time:10475ms step_avg:35.27ms
step:298/2110 train_time:10509ms step_avg:35.27ms
step:299/2110 train_time:10541ms step_avg:35.25ms
step:300/2110 train_time:10577ms step_avg:35.26ms
step:301/2110 train_time:10609ms step_avg:35.25ms
step:302/2110 train_time:10643ms step_avg:35.24ms
step:303/2110 train_time:10674ms step_avg:35.23ms
step:304/2110 train_time:10708ms step_avg:35.22ms
step:305/2110 train_time:10739ms step_avg:35.21ms
step:306/2110 train_time:10774ms step_avg:35.21ms
step:307/2110 train_time:10806ms step_avg:35.20ms
step:308/2110 train_time:10842ms step_avg:35.20ms
step:309/2110 train_time:10876ms step_avg:35.20ms
step:310/2110 train_time:10909ms step_avg:35.19ms
step:311/2110 train_time:10943ms step_avg:35.19ms
step:312/2110 train_time:10979ms step_avg:35.19ms
step:313/2110 train_time:11009ms step_avg:35.17ms
step:314/2110 train_time:11042ms step_avg:35.17ms
step:315/2110 train_time:11074ms step_avg:35.16ms
step:316/2110 train_time:11106ms step_avg:35.15ms
step:317/2110 train_time:11137ms step_avg:35.13ms
step:318/2110 train_time:11170ms step_avg:35.13ms
step:319/2110 train_time:11202ms step_avg:35.12ms
step:320/2110 train_time:11235ms step_avg:35.11ms
step:321/2110 train_time:11268ms step_avg:35.10ms
step:322/2110 train_time:11304ms step_avg:35.11ms
step:323/2110 train_time:11337ms step_avg:35.10ms
step:324/2110 train_time:11370ms step_avg:35.09ms
step:325/2110 train_time:11399ms step_avg:35.07ms
step:326/2110 train_time:11434ms step_avg:35.07ms
step:327/2110 train_time:11467ms step_avg:35.07ms
step:328/2110 train_time:11499ms step_avg:35.06ms
step:329/2110 train_time:11531ms step_avg:35.05ms
step:330/2110 train_time:11566ms step_avg:35.05ms
step:331/2110 train_time:11598ms step_avg:35.04ms
step:332/2110 train_time:11631ms step_avg:35.03ms
step:333/2110 train_time:11664ms step_avg:35.03ms
step:334/2110 train_time:11698ms step_avg:35.02ms
step:335/2110 train_time:11730ms step_avg:35.02ms
step:336/2110 train_time:11768ms step_avg:35.02ms
step:337/2110 train_time:11800ms step_avg:35.01ms
step:338/2110 train_time:11833ms step_avg:35.01ms
step:339/2110 train_time:11866ms step_avg:35.00ms
step:340/2110 train_time:11899ms step_avg:35.00ms
step:341/2110 train_time:11932ms step_avg:34.99ms
step:342/2110 train_time:11966ms step_avg:34.99ms
step:343/2110 train_time:11997ms step_avg:34.98ms
step:344/2110 train_time:12031ms step_avg:34.98ms
step:345/2110 train_time:12062ms step_avg:34.96ms
step:346/2110 train_time:12095ms step_avg:34.96ms
step:347/2110 train_time:12126ms step_avg:34.94ms
step:348/2110 train_time:12160ms step_avg:34.94ms
step:349/2110 train_time:12192ms step_avg:34.94ms
step:350/2110 train_time:12225ms step_avg:34.93ms
step:351/2110 train_time:12259ms step_avg:34.93ms
step:352/2110 train_time:12294ms step_avg:34.93ms
step:353/2110 train_time:12327ms step_avg:34.92ms
step:354/2110 train_time:12361ms step_avg:34.92ms
step:355/2110 train_time:12392ms step_avg:34.91ms
step:356/2110 train_time:12428ms step_avg:34.91ms
step:357/2110 train_time:12459ms step_avg:34.90ms
step:358/2110 train_time:12492ms step_avg:34.89ms
step:359/2110 train_time:12522ms step_avg:34.88ms
step:360/2110 train_time:12557ms step_avg:34.88ms
step:361/2110 train_time:12590ms step_avg:34.88ms
step:362/2110 train_time:12624ms step_avg:34.87ms
step:363/2110 train_time:12656ms step_avg:34.86ms
step:364/2110 train_time:12688ms step_avg:34.86ms
step:365/2110 train_time:12720ms step_avg:34.85ms
step:366/2110 train_time:12754ms step_avg:34.85ms
step:367/2110 train_time:12785ms step_avg:34.84ms
step:368/2110 train_time:12820ms step_avg:34.84ms
step:369/2110 train_time:12854ms step_avg:34.83ms
step:370/2110 train_time:12886ms step_avg:34.83ms
step:371/2110 train_time:12917ms step_avg:34.82ms
step:372/2110 train_time:12951ms step_avg:34.82ms
step:373/2110 train_time:12984ms step_avg:34.81ms
step:374/2110 train_time:13018ms step_avg:34.81ms
step:375/2110 train_time:13050ms step_avg:34.80ms
step:376/2110 train_time:13083ms step_avg:34.79ms
step:377/2110 train_time:13115ms step_avg:34.79ms
step:378/2110 train_time:13150ms step_avg:34.79ms
step:379/2110 train_time:13182ms step_avg:34.78ms
step:380/2110 train_time:13216ms step_avg:34.78ms
step:381/2110 train_time:13247ms step_avg:34.77ms
step:382/2110 train_time:13282ms step_avg:34.77ms
step:383/2110 train_time:13313ms step_avg:34.76ms
step:384/2110 train_time:13348ms step_avg:34.76ms
step:385/2110 train_time:13380ms step_avg:34.75ms
step:386/2110 train_time:13413ms step_avg:34.75ms
step:387/2110 train_time:13446ms step_avg:34.74ms
step:388/2110 train_time:13481ms step_avg:34.74ms
step:389/2110 train_time:13513ms step_avg:34.74ms
step:390/2110 train_time:13547ms step_avg:34.74ms
step:391/2110 train_time:13579ms step_avg:34.73ms
step:392/2110 train_time:13613ms step_avg:34.73ms
step:393/2110 train_time:13645ms step_avg:34.72ms
step:394/2110 train_time:13679ms step_avg:34.72ms
step:395/2110 train_time:13711ms step_avg:34.71ms
step:396/2110 train_time:13744ms step_avg:34.71ms
step:397/2110 train_time:13776ms step_avg:34.70ms
step:398/2110 train_time:13812ms step_avg:34.70ms
step:399/2110 train_time:13843ms step_avg:34.70ms
step:400/2110 train_time:13878ms step_avg:34.70ms
step:401/2110 train_time:13908ms step_avg:34.68ms
step:402/2110 train_time:13941ms step_avg:34.68ms
step:403/2110 train_time:13974ms step_avg:34.67ms
step:404/2110 train_time:14007ms step_avg:34.67ms
step:405/2110 train_time:14039ms step_avg:34.66ms
step:406/2110 train_time:14074ms step_avg:34.66ms
step:407/2110 train_time:14106ms step_avg:34.66ms
step:408/2110 train_time:14139ms step_avg:34.65ms
step:409/2110 train_time:14174ms step_avg:34.66ms
step:410/2110 train_time:14208ms step_avg:34.65ms
step:411/2110 train_time:14239ms step_avg:34.64ms
step:412/2110 train_time:14271ms step_avg:34.64ms
step:413/2110 train_time:14304ms step_avg:34.63ms
step:414/2110 train_time:14338ms step_avg:34.63ms
step:415/2110 train_time:14370ms step_avg:34.63ms
step:416/2110 train_time:14405ms step_avg:34.63ms
step:417/2110 train_time:14436ms step_avg:34.62ms
step:418/2110 train_time:14471ms step_avg:34.62ms
step:419/2110 train_time:14501ms step_avg:34.61ms
step:420/2110 train_time:14536ms step_avg:34.61ms
step:421/2110 train_time:14568ms step_avg:34.60ms
step:422/2110 train_time:14601ms step_avg:34.60ms
step:423/2110 train_time:14634ms step_avg:34.59ms
step:424/2110 train_time:14667ms step_avg:34.59ms
step:425/2110 train_time:14699ms step_avg:34.59ms
step:426/2110 train_time:14734ms step_avg:34.59ms
step:427/2110 train_time:14767ms step_avg:34.58ms
step:428/2110 train_time:14800ms step_avg:34.58ms
step:429/2110 train_time:14833ms step_avg:34.58ms
step:430/2110 train_time:14868ms step_avg:34.58ms
step:431/2110 train_time:14899ms step_avg:34.57ms
step:432/2110 train_time:14934ms step_avg:34.57ms
step:433/2110 train_time:14966ms step_avg:34.56ms
step:434/2110 train_time:14999ms step_avg:34.56ms
step:435/2110 train_time:15030ms step_avg:34.55ms
step:436/2110 train_time:15064ms step_avg:34.55ms
step:437/2110 train_time:15096ms step_avg:34.55ms
step:438/2110 train_time:15130ms step_avg:34.54ms
step:439/2110 train_time:15162ms step_avg:34.54ms
step:440/2110 train_time:15197ms step_avg:34.54ms
step:441/2110 train_time:15228ms step_avg:34.53ms
step:442/2110 train_time:15262ms step_avg:34.53ms
step:443/2110 train_time:15294ms step_avg:34.52ms
step:444/2110 train_time:15328ms step_avg:34.52ms
step:445/2110 train_time:15359ms step_avg:34.51ms
step:446/2110 train_time:15393ms step_avg:34.51ms
step:447/2110 train_time:15425ms step_avg:34.51ms
step:448/2110 train_time:15460ms step_avg:34.51ms
step:449/2110 train_time:15493ms step_avg:34.51ms
step:450/2110 train_time:15526ms step_avg:34.50ms
step:451/2110 train_time:15558ms step_avg:34.50ms
step:452/2110 train_time:15592ms step_avg:34.50ms
step:453/2110 train_time:15626ms step_avg:34.49ms
step:454/2110 train_time:15661ms step_avg:34.49ms
step:455/2110 train_time:15692ms step_avg:34.49ms
step:456/2110 train_time:15725ms step_avg:34.49ms
step:457/2110 train_time:15757ms step_avg:34.48ms
step:458/2110 train_time:15790ms step_avg:34.48ms
step:459/2110 train_time:15821ms step_avg:34.47ms
step:460/2110 train_time:15854ms step_avg:34.47ms
step:461/2110 train_time:15888ms step_avg:34.46ms
step:462/2110 train_time:15923ms step_avg:34.46ms
step:463/2110 train_time:15953ms step_avg:34.46ms
step:464/2110 train_time:15986ms step_avg:34.45ms
step:465/2110 train_time:16019ms step_avg:34.45ms
step:466/2110 train_time:16054ms step_avg:34.45ms
step:467/2110 train_time:16086ms step_avg:34.45ms
step:468/2110 train_time:16119ms step_avg:34.44ms
step:469/2110 train_time:16152ms step_avg:34.44ms
step:470/2110 train_time:16185ms step_avg:34.44ms
step:471/2110 train_time:16219ms step_avg:34.44ms
step:472/2110 train_time:16254ms step_avg:34.44ms
step:473/2110 train_time:16286ms step_avg:34.43ms
step:474/2110 train_time:16318ms step_avg:34.43ms
step:475/2110 train_time:16350ms step_avg:34.42ms
step:476/2110 train_time:16384ms step_avg:34.42ms
step:477/2110 train_time:16416ms step_avg:34.41ms
step:478/2110 train_time:16449ms step_avg:34.41ms
step:479/2110 train_time:16482ms step_avg:34.41ms
step:480/2110 train_time:16516ms step_avg:34.41ms
step:481/2110 train_time:16548ms step_avg:34.40ms
step:482/2110 train_time:16581ms step_avg:34.40ms
step:483/2110 train_time:16613ms step_avg:34.40ms
step:484/2110 train_time:16646ms step_avg:34.39ms
step:485/2110 train_time:16680ms step_avg:34.39ms
step:486/2110 train_time:16714ms step_avg:34.39ms
step:487/2110 train_time:16747ms step_avg:34.39ms
step:488/2110 train_time:16783ms step_avg:34.39ms
step:489/2110 train_time:16815ms step_avg:34.39ms
step:490/2110 train_time:16850ms step_avg:34.39ms
step:491/2110 train_time:16883ms step_avg:34.38ms
step:492/2110 train_time:16915ms step_avg:34.38ms
step:493/2110 train_time:16944ms step_avg:34.37ms
step:494/2110 train_time:16978ms step_avg:34.37ms
step:495/2110 train_time:17009ms step_avg:34.36ms
step:496/2110 train_time:17044ms step_avg:34.36ms
step:497/2110 train_time:17076ms step_avg:34.36ms
step:498/2110 train_time:17111ms step_avg:34.36ms
step:499/2110 train_time:17141ms step_avg:34.35ms
step:500/2110 train_time:17174ms step_avg:34.35ms
step:500/2110 val_loss:4.0249 train_time:17209ms step_avg:34.42ms
step:501/2110 train_time:17245ms step_avg:34.42ms
step:502/2110 train_time:17279ms step_avg:34.42ms
step:503/2110 train_time:17312ms step_avg:34.42ms
step:504/2110 train_time:17347ms step_avg:34.42ms
step:505/2110 train_time:17376ms step_avg:34.41ms
step:506/2110 train_time:17410ms step_avg:34.41ms
step:507/2110 train_time:17442ms step_avg:34.40ms
step:508/2110 train_time:17473ms step_avg:34.40ms
step:509/2110 train_time:17503ms step_avg:34.39ms
step:510/2110 train_time:17534ms step_avg:34.38ms
step:511/2110 train_time:17565ms step_avg:34.37ms
step:512/2110 train_time:17599ms step_avg:34.37ms
step:513/2110 train_time:17633ms step_avg:34.37ms
step:514/2110 train_time:17666ms step_avg:34.37ms
step:515/2110 train_time:17695ms step_avg:34.36ms
step:516/2110 train_time:17723ms step_avg:34.35ms
step:517/2110 train_time:17749ms step_avg:34.33ms
step:518/2110 train_time:17782ms step_avg:34.33ms
step:519/2110 train_time:17813ms step_avg:34.32ms
step:520/2110 train_time:17846ms step_avg:34.32ms
step:521/2110 train_time:17877ms step_avg:34.31ms
step:522/2110 train_time:17911ms step_avg:34.31ms
step:523/2110 train_time:17944ms step_avg:34.31ms
step:524/2110 train_time:17978ms step_avg:34.31ms
step:525/2110 train_time:18008ms step_avg:34.30ms
step:526/2110 train_time:18042ms step_avg:34.30ms
step:527/2110 train_time:18073ms step_avg:34.29ms
step:528/2110 train_time:18110ms step_avg:34.30ms
step:529/2110 train_time:18144ms step_avg:34.30ms
step:530/2110 train_time:18178ms step_avg:34.30ms
step:531/2110 train_time:18210ms step_avg:34.29ms
step:532/2110 train_time:18244ms step_avg:34.29ms
step:533/2110 train_time:18276ms step_avg:34.29ms
step:534/2110 train_time:18308ms step_avg:34.28ms
step:535/2110 train_time:18339ms step_avg:34.28ms
step:536/2110 train_time:18373ms step_avg:34.28ms
step:537/2110 train_time:18409ms step_avg:34.28ms
step:538/2110 train_time:18444ms step_avg:34.28ms
step:539/2110 train_time:18473ms step_avg:34.27ms
step:540/2110 train_time:18507ms step_avg:34.27ms
step:541/2110 train_time:18538ms step_avg:34.27ms
step:542/2110 train_time:18574ms step_avg:34.27ms
step:543/2110 train_time:18605ms step_avg:34.26ms
step:544/2110 train_time:18639ms step_avg:34.26ms
step:545/2110 train_time:18671ms step_avg:34.26ms
step:546/2110 train_time:18704ms step_avg:34.26ms
step:547/2110 train_time:18738ms step_avg:34.26ms
step:548/2110 train_time:18772ms step_avg:34.26ms
step:549/2110 train_time:18805ms step_avg:34.25ms
step:550/2110 train_time:18841ms step_avg:34.26ms
step:551/2110 train_time:18873ms step_avg:34.25ms
step:552/2110 train_time:18907ms step_avg:34.25ms
step:553/2110 train_time:18941ms step_avg:34.25ms
step:554/2110 train_time:18977ms step_avg:34.25ms
step:555/2110 train_time:19013ms step_avg:34.26ms
step:556/2110 train_time:19049ms step_avg:34.26ms
step:557/2110 train_time:19081ms step_avg:34.26ms
step:558/2110 train_time:19112ms step_avg:34.25ms
step:559/2110 train_time:19143ms step_avg:34.25ms
step:560/2110 train_time:19181ms step_avg:34.25ms
step:561/2110 train_time:19216ms step_avg:34.25ms
step:562/2110 train_time:19253ms step_avg:34.26ms
step:563/2110 train_time:19288ms step_avg:34.26ms
step:564/2110 train_time:19327ms step_avg:34.27ms
step:565/2110 train_time:19360ms step_avg:34.27ms
step:566/2110 train_time:19398ms step_avg:34.27ms
step:567/2110 train_time:19430ms step_avg:34.27ms
step:568/2110 train_time:19465ms step_avg:34.27ms
step:569/2110 train_time:19500ms step_avg:34.27ms
step:570/2110 train_time:19538ms step_avg:34.28ms
step:571/2110 train_time:19574ms step_avg:34.28ms
step:572/2110 train_time:19610ms step_avg:34.28ms
step:573/2110 train_time:19644ms step_avg:34.28ms
step:574/2110 train_time:19678ms step_avg:34.28ms
step:575/2110 train_time:19708ms step_avg:34.27ms
step:576/2110 train_time:19740ms step_avg:34.27ms
step:577/2110 train_time:19770ms step_avg:34.26ms
step:578/2110 train_time:19798ms step_avg:34.25ms
step:579/2110 train_time:19824ms step_avg:34.24ms
step:580/2110 train_time:19850ms step_avg:34.22ms
step:581/2110 train_time:19882ms step_avg:34.22ms
step:582/2110 train_time:19915ms step_avg:34.22ms
step:583/2110 train_time:19948ms step_avg:34.22ms
step:584/2110 train_time:19982ms step_avg:34.22ms
step:585/2110 train_time:20013ms step_avg:34.21ms
step:586/2110 train_time:20046ms step_avg:34.21ms
step:587/2110 train_time:20078ms step_avg:34.20ms
step:588/2110 train_time:20112ms step_avg:34.20ms
step:589/2110 train_time:20146ms step_avg:34.20ms
step:590/2110 train_time:20182ms step_avg:34.21ms
step:591/2110 train_time:20216ms step_avg:34.21ms
step:592/2110 train_time:20252ms step_avg:34.21ms
step:593/2110 train_time:20288ms step_avg:34.21ms
step:594/2110 train_time:20323ms step_avg:34.21ms
step:595/2110 train_time:20356ms step_avg:34.21ms
step:596/2110 train_time:20390ms step_avg:34.21ms
step:597/2110 train_time:20425ms step_avg:34.21ms
step:598/2110 train_time:20460ms step_avg:34.21ms
step:599/2110 train_time:20496ms step_avg:34.22ms
step:600/2110 train_time:20533ms step_avg:34.22ms
step:601/2110 train_time:20568ms step_avg:34.22ms
step:602/2110 train_time:20604ms step_avg:34.23ms
step:603/2110 train_time:20640ms step_avg:34.23ms
step:604/2110 train_time:20678ms step_avg:34.23ms
step:605/2110 train_time:20715ms step_avg:34.24ms
step:606/2110 train_time:20750ms step_avg:34.24ms
step:607/2110 train_time:20779ms step_avg:34.23ms
step:608/2110 train_time:20814ms step_avg:34.23ms
step:609/2110 train_time:20849ms step_avg:34.23ms
step:610/2110 train_time:20882ms step_avg:34.23ms
step:611/2110 train_time:20914ms step_avg:34.23ms
step:612/2110 train_time:20944ms step_avg:34.22ms
step:613/2110 train_time:20969ms step_avg:34.21ms
step:614/2110 train_time:20998ms step_avg:34.20ms
step:615/2110 train_time:21026ms step_avg:34.19ms
step:616/2110 train_time:21055ms step_avg:34.18ms
step:617/2110 train_time:21088ms step_avg:34.18ms
step:618/2110 train_time:21121ms step_avg:34.18ms
step:619/2110 train_time:21154ms step_avg:34.17ms
step:620/2110 train_time:21187ms step_avg:34.17ms
step:621/2110 train_time:21220ms step_avg:34.17ms
step:622/2110 train_time:21251ms step_avg:34.17ms
step:623/2110 train_time:21285ms step_avg:34.17ms
step:624/2110 train_time:21320ms step_avg:34.17ms
step:625/2110 train_time:21350ms step_avg:34.16ms
step:626/2110 train_time:21383ms step_avg:34.16ms
step:627/2110 train_time:21416ms step_avg:34.16ms
step:628/2110 train_time:21450ms step_avg:34.16ms
step:629/2110 train_time:21482ms step_avg:34.15ms
step:630/2110 train_time:21515ms step_avg:34.15ms
step:631/2110 train_time:21550ms step_avg:34.15ms
step:632/2110 train_time:21587ms step_avg:34.16ms
step:633/2110 train_time:21618ms step_avg:34.15ms
step:634/2110 train_time:21653ms step_avg:34.15ms
step:635/2110 train_time:21683ms step_avg:34.15ms
step:636/2110 train_time:21715ms step_avg:34.14ms
step:637/2110 train_time:21749ms step_avg:34.14ms
step:638/2110 train_time:21785ms step_avg:34.15ms
step:639/2110 train_time:21815ms step_avg:34.14ms
step:640/2110 train_time:21850ms step_avg:34.14ms
step:641/2110 train_time:21885ms step_avg:34.14ms
step:642/2110 train_time:21919ms step_avg:34.14ms
step:643/2110 train_time:21950ms step_avg:34.14ms
step:644/2110 train_time:21982ms step_avg:34.13ms
step:645/2110 train_time:22014ms step_avg:34.13ms
step:646/2110 train_time:22047ms step_avg:34.13ms
step:647/2110 train_time:22081ms step_avg:34.13ms
step:648/2110 train_time:22117ms step_avg:34.13ms
step:649/2110 train_time:22150ms step_avg:34.13ms
step:650/2110 train_time:22184ms step_avg:34.13ms
step:651/2110 train_time:22215ms step_avg:34.12ms
step:652/2110 train_time:22247ms step_avg:34.12ms
step:653/2110 train_time:22277ms step_avg:34.11ms
step:654/2110 train_time:22309ms step_avg:34.11ms
step:655/2110 train_time:22342ms step_avg:34.11ms
step:656/2110 train_time:22376ms step_avg:34.11ms
step:657/2110 train_time:22408ms step_avg:34.11ms
step:658/2110 train_time:22441ms step_avg:34.10ms
step:659/2110 train_time:22474ms step_avg:34.10ms
step:660/2110 train_time:22508ms step_avg:34.10ms
step:661/2110 train_time:22541ms step_avg:34.10ms
step:662/2110 train_time:22574ms step_avg:34.10ms
step:663/2110 train_time:22607ms step_avg:34.10ms
step:664/2110 train_time:22640ms step_avg:34.10ms
step:665/2110 train_time:22673ms step_avg:34.10ms
step:666/2110 train_time:22708ms step_avg:34.10ms
step:667/2110 train_time:22740ms step_avg:34.09ms
step:668/2110 train_time:22773ms step_avg:34.09ms
step:669/2110 train_time:22806ms step_avg:34.09ms
step:670/2110 train_time:22838ms step_avg:34.09ms
step:671/2110 train_time:22872ms step_avg:34.09ms
step:672/2110 train_time:22905ms step_avg:34.09ms
step:673/2110 train_time:22938ms step_avg:34.08ms
step:674/2110 train_time:22971ms step_avg:34.08ms
step:675/2110 train_time:23004ms step_avg:34.08ms
step:676/2110 train_time:23038ms step_avg:34.08ms
step:677/2110 train_time:23071ms step_avg:34.08ms
step:678/2110 train_time:23107ms step_avg:34.08ms
step:679/2110 train_time:23141ms step_avg:34.08ms
step:680/2110 train_time:23176ms step_avg:34.08ms
step:681/2110 train_time:23212ms step_avg:34.09ms
step:682/2110 train_time:23245ms step_avg:34.08ms
step:683/2110 train_time:23280ms step_avg:34.08ms
step:684/2110 train_time:23316ms step_avg:34.09ms
step:685/2110 train_time:23350ms step_avg:34.09ms
step:686/2110 train_time:23384ms step_avg:34.09ms
step:687/2110 train_time:23413ms step_avg:34.08ms
step:688/2110 train_time:23441ms step_avg:34.07ms
step:689/2110 train_time:23465ms step_avg:34.06ms
step:690/2110 train_time:23497ms step_avg:34.05ms
step:691/2110 train_time:23531ms step_avg:34.05ms
step:692/2110 train_time:23590ms step_avg:34.09ms
step:693/2110 train_time:23649ms step_avg:34.13ms
step:694/2110 train_time:23708ms step_avg:34.16ms
step:695/2110 train_time:23768ms step_avg:34.20ms
step:696/2110 train_time:23828ms step_avg:34.24ms
step:697/2110 train_time:23887ms step_avg:34.27ms
step:698/2110 train_time:23946ms step_avg:34.31ms
step:699/2110 train_time:24005ms step_avg:34.34ms
step:700/2110 train_time:24064ms step_avg:34.38ms
step:701/2110 train_time:24123ms step_avg:34.41ms
step:702/2110 train_time:24183ms step_avg:34.45ms
step:703/2110 train_time:24242ms step_avg:34.48ms
step:704/2110 train_time:24302ms step_avg:34.52ms
step:705/2110 train_time:24361ms step_avg:34.55ms
step:706/2110 train_time:24420ms step_avg:34.59ms
step:707/2110 train_time:24479ms step_avg:34.62ms
step:708/2110 train_time:24540ms step_avg:34.66ms
step:709/2110 train_time:24599ms step_avg:34.70ms
step:710/2110 train_time:24659ms step_avg:34.73ms
step:711/2110 train_time:24719ms step_avg:34.77ms
step:712/2110 train_time:24779ms step_avg:34.80ms
step:713/2110 train_time:24839ms step_avg:34.84ms
step:714/2110 train_time:24899ms step_avg:34.87ms
step:715/2110 train_time:24958ms step_avg:34.91ms
step:716/2110 train_time:25017ms step_avg:34.94ms
step:717/2110 train_time:25077ms step_avg:34.98ms
step:718/2110 train_time:25137ms step_avg:35.01ms
step:719/2110 train_time:25196ms step_avg:35.04ms
step:720/2110 train_time:25255ms step_avg:35.08ms
step:721/2110 train_time:25314ms step_avg:35.11ms
step:722/2110 train_time:25373ms step_avg:35.14ms
step:723/2110 train_time:25432ms step_avg:35.18ms
step:724/2110 train_time:25492ms step_avg:35.21ms
step:725/2110 train_time:25551ms step_avg:35.24ms
step:726/2110 train_time:25610ms step_avg:35.27ms
step:727/2110 train_time:25668ms step_avg:35.31ms
step:728/2110 train_time:25728ms step_avg:35.34ms
step:729/2110 train_time:25787ms step_avg:35.37ms
step:730/2110 train_time:25846ms step_avg:35.41ms
step:731/2110 train_time:25905ms step_avg:35.44ms
step:732/2110 train_time:25964ms step_avg:35.47ms
step:733/2110 train_time:26023ms step_avg:35.50ms
step:734/2110 train_time:26082ms step_avg:35.53ms
step:735/2110 train_time:26142ms step_avg:35.57ms
step:736/2110 train_time:26201ms step_avg:35.60ms
step:737/2110 train_time:26260ms step_avg:35.63ms
step:738/2110 train_time:26319ms step_avg:35.66ms
step:739/2110 train_time:26378ms step_avg:35.69ms
step:740/2110 train_time:26438ms step_avg:35.73ms
step:741/2110 train_time:26498ms step_avg:35.76ms
step:742/2110 train_time:26558ms step_avg:35.79ms
step:743/2110 train_time:26617ms step_avg:35.82ms
step:744/2110 train_time:26676ms step_avg:35.86ms
step:745/2110 train_time:26736ms step_avg:35.89ms
step:746/2110 train_time:26796ms step_avg:35.92ms
step:747/2110 train_time:26855ms step_avg:35.95ms
step:748/2110 train_time:26914ms step_avg:35.98ms
step:749/2110 train_time:26974ms step_avg:36.01ms
step:750/2110 train_time:27034ms step_avg:36.05ms
step:750/2110 val_loss:3.9136 train_time:27096ms step_avg:36.13ms
step:751/2110 train_time:27132ms step_avg:36.13ms
step:752/2110 train_time:27172ms step_avg:36.13ms
step:753/2110 train_time:27217ms step_avg:36.15ms
step:754/2110 train_time:27281ms step_avg:36.18ms
step:755/2110 train_time:27343ms step_avg:36.22ms
step:756/2110 train_time:27403ms step_avg:36.25ms
step:757/2110 train_time:27462ms step_avg:36.28ms
step:758/2110 train_time:27521ms step_avg:36.31ms
step:759/2110 train_time:27580ms step_avg:36.34ms
step:760/2110 train_time:27640ms step_avg:36.37ms
step:761/2110 train_time:27697ms step_avg:36.40ms
step:762/2110 train_time:27756ms step_avg:36.42ms
step:763/2110 train_time:27813ms step_avg:36.45ms
step:764/2110 train_time:27872ms step_avg:36.48ms
step:765/2110 train_time:27930ms step_avg:36.51ms
step:766/2110 train_time:27988ms step_avg:36.54ms
step:767/2110 train_time:28048ms step_avg:36.57ms
step:768/2110 train_time:28108ms step_avg:36.60ms
step:769/2110 train_time:28169ms step_avg:36.63ms
step:770/2110 train_time:28229ms step_avg:36.66ms
step:771/2110 train_time:28291ms step_avg:36.69ms
step:772/2110 train_time:28350ms step_avg:36.72ms
step:773/2110 train_time:28410ms step_avg:36.75ms
step:774/2110 train_time:28469ms step_avg:36.78ms
step:775/2110 train_time:28528ms step_avg:36.81ms
step:776/2110 train_time:28587ms step_avg:36.84ms
step:777/2110 train_time:28647ms step_avg:36.87ms
step:778/2110 train_time:28706ms step_avg:36.90ms
step:779/2110 train_time:28764ms step_avg:36.92ms
step:780/2110 train_time:28824ms step_avg:36.95ms
step:781/2110 train_time:28882ms step_avg:36.98ms
step:782/2110 train_time:28940ms step_avg:37.01ms
step:783/2110 train_time:29000ms step_avg:37.04ms
step:784/2110 train_time:29059ms step_avg:37.07ms
step:785/2110 train_time:29120ms step_avg:37.10ms
step:786/2110 train_time:29180ms step_avg:37.12ms
step:787/2110 train_time:29240ms step_avg:37.15ms
step:788/2110 train_time:29301ms step_avg:37.18ms
step:789/2110 train_time:29361ms step_avg:37.21ms
step:790/2110 train_time:29420ms step_avg:37.24ms
step:791/2110 train_time:29481ms step_avg:37.27ms
step:792/2110 train_time:29540ms step_avg:37.30ms
step:793/2110 train_time:29599ms step_avg:37.33ms
step:794/2110 train_time:29658ms step_avg:37.35ms
step:795/2110 train_time:29717ms step_avg:37.38ms
step:796/2110 train_time:29775ms step_avg:37.41ms
step:797/2110 train_time:29834ms step_avg:37.43ms
step:798/2110 train_time:29893ms step_avg:37.46ms
step:799/2110 train_time:29952ms step_avg:37.49ms
step:800/2110 train_time:30010ms step_avg:37.51ms
step:801/2110 train_time:30069ms step_avg:37.54ms
step:802/2110 train_time:30129ms step_avg:37.57ms
step:803/2110 train_time:30188ms step_avg:37.59ms
step:804/2110 train_time:30248ms step_avg:37.62ms
step:805/2110 train_time:30307ms step_avg:37.65ms
step:806/2110 train_time:30367ms step_avg:37.68ms
step:807/2110 train_time:30427ms step_avg:37.70ms
step:808/2110 train_time:30488ms step_avg:37.73ms
step:809/2110 train_time:30548ms step_avg:37.76ms
step:810/2110 train_time:30608ms step_avg:37.79ms
step:811/2110 train_time:30667ms step_avg:37.81ms
step:812/2110 train_time:30726ms step_avg:37.84ms
step:813/2110 train_time:30785ms step_avg:37.87ms
step:814/2110 train_time:30843ms step_avg:37.89ms
step:815/2110 train_time:30903ms step_avg:37.92ms
step:816/2110 train_time:30962ms step_avg:37.94ms
step:817/2110 train_time:31021ms step_avg:37.97ms
step:818/2110 train_time:31081ms step_avg:38.00ms
step:819/2110 train_time:31141ms step_avg:38.02ms
step:820/2110 train_time:31200ms step_avg:38.05ms
step:821/2110 train_time:31260ms step_avg:38.08ms
step:822/2110 train_time:31320ms step_avg:38.10ms
step:823/2110 train_time:31381ms step_avg:38.13ms
step:824/2110 train_time:31440ms step_avg:38.16ms
step:825/2110 train_time:31500ms step_avg:38.18ms
step:826/2110 train_time:31559ms step_avg:38.21ms
step:827/2110 train_time:31619ms step_avg:38.23ms
step:828/2110 train_time:31677ms step_avg:38.26ms
step:829/2110 train_time:31737ms step_avg:38.28ms
step:830/2110 train_time:31795ms step_avg:38.31ms
step:831/2110 train_time:31855ms step_avg:38.33ms
step:832/2110 train_time:31914ms step_avg:38.36ms
step:833/2110 train_time:31973ms step_avg:38.38ms
step:834/2110 train_time:32032ms step_avg:38.41ms
step:835/2110 train_time:32090ms step_avg:38.43ms
step:836/2110 train_time:32149ms step_avg:38.46ms
step:837/2110 train_time:32208ms step_avg:38.48ms
step:838/2110 train_time:32267ms step_avg:38.50ms
step:839/2110 train_time:32327ms step_avg:38.53ms
step:840/2110 train_time:32387ms step_avg:38.56ms
step:841/2110 train_time:32446ms step_avg:38.58ms
step:842/2110 train_time:32506ms step_avg:38.61ms
step:843/2110 train_time:32565ms step_avg:38.63ms
step:844/2110 train_time:32625ms step_avg:38.66ms
step:845/2110 train_time:32685ms step_avg:38.68ms
step:846/2110 train_time:32745ms step_avg:38.71ms
step:847/2110 train_time:32804ms step_avg:38.73ms
step:848/2110 train_time:32864ms step_avg:38.75ms
step:849/2110 train_time:32923ms step_avg:38.78ms
step:850/2110 train_time:32982ms step_avg:38.80ms
step:851/2110 train_time:33041ms step_avg:38.83ms
step:852/2110 train_time:33101ms step_avg:38.85ms
step:853/2110 train_time:33160ms step_avg:38.88ms
step:854/2110 train_time:33220ms step_avg:38.90ms
step:855/2110 train_time:33279ms step_avg:38.92ms
step:856/2110 train_time:33339ms step_avg:38.95ms
step:857/2110 train_time:33398ms step_avg:38.97ms
step:858/2110 train_time:33457ms step_avg:38.99ms
step:859/2110 train_time:33516ms step_avg:39.02ms
step:860/2110 train_time:33576ms step_avg:39.04ms
step:861/2110 train_time:33634ms step_avg:39.06ms
step:862/2110 train_time:33694ms step_avg:39.09ms
step:863/2110 train_time:33753ms step_avg:39.11ms
step:864/2110 train_time:33811ms step_avg:39.13ms
step:865/2110 train_time:33871ms step_avg:39.16ms
step:866/2110 train_time:33930ms step_avg:39.18ms
step:867/2110 train_time:33988ms step_avg:39.20ms
step:868/2110 train_time:34047ms step_avg:39.23ms
step:869/2110 train_time:34108ms step_avg:39.25ms
step:870/2110 train_time:34167ms step_avg:39.27ms
step:871/2110 train_time:34226ms step_avg:39.30ms
step:872/2110 train_time:34287ms step_avg:39.32ms
step:873/2110 train_time:34346ms step_avg:39.34ms
step:874/2110 train_time:34406ms step_avg:39.37ms
step:875/2110 train_time:34466ms step_avg:39.39ms
step:876/2110 train_time:34525ms step_avg:39.41ms
step:877/2110 train_time:34585ms step_avg:39.44ms
step:878/2110 train_time:34645ms step_avg:39.46ms
step:879/2110 train_time:34705ms step_avg:39.48ms
step:880/2110 train_time:34764ms step_avg:39.50ms
step:881/2110 train_time:34823ms step_avg:39.53ms
step:882/2110 train_time:34883ms step_avg:39.55ms
step:883/2110 train_time:34942ms step_avg:39.57ms
step:884/2110 train_time:35001ms step_avg:39.59ms
step:885/2110 train_time:35061ms step_avg:39.62ms
step:886/2110 train_time:35120ms step_avg:39.64ms
step:887/2110 train_time:35179ms step_avg:39.66ms
step:888/2110 train_time:35239ms step_avg:39.68ms
step:889/2110 train_time:35298ms step_avg:39.71ms
step:890/2110 train_time:35356ms step_avg:39.73ms
step:891/2110 train_time:35416ms step_avg:39.75ms
step:892/2110 train_time:35475ms step_avg:39.77ms
step:893/2110 train_time:35533ms step_avg:39.79ms
step:894/2110 train_time:35592ms step_avg:39.81ms
step:895/2110 train_time:35652ms step_avg:39.83ms
step:896/2110 train_time:35710ms step_avg:39.86ms
step:897/2110 train_time:35770ms step_avg:39.88ms
step:898/2110 train_time:35829ms step_avg:39.90ms
step:899/2110 train_time:35888ms step_avg:39.92ms
step:900/2110 train_time:35947ms step_avg:39.94ms
step:901/2110 train_time:36007ms step_avg:39.96ms
step:902/2110 train_time:36067ms step_avg:39.99ms
step:903/2110 train_time:36126ms step_avg:40.01ms
step:904/2110 train_time:36186ms step_avg:40.03ms
step:905/2110 train_time:36246ms step_avg:40.05ms
step:906/2110 train_time:36306ms step_avg:40.07ms
step:907/2110 train_time:36365ms step_avg:40.09ms
step:908/2110 train_time:36424ms step_avg:40.11ms
step:909/2110 train_time:36484ms step_avg:40.14ms
step:910/2110 train_time:36544ms step_avg:40.16ms
step:911/2110 train_time:36603ms step_avg:40.18ms
step:912/2110 train_time:36662ms step_avg:40.20ms
step:913/2110 train_time:36721ms step_avg:40.22ms
step:914/2110 train_time:36780ms step_avg:40.24ms
step:915/2110 train_time:36840ms step_avg:40.26ms
step:916/2110 train_time:36900ms step_avg:40.28ms
step:917/2110 train_time:36959ms step_avg:40.30ms
step:918/2110 train_time:37017ms step_avg:40.32ms
step:919/2110 train_time:37077ms step_avg:40.35ms
step:920/2110 train_time:37136ms step_avg:40.37ms
step:921/2110 train_time:37196ms step_avg:40.39ms
step:922/2110 train_time:37255ms step_avg:40.41ms
step:923/2110 train_time:37315ms step_avg:40.43ms
step:924/2110 train_time:37374ms step_avg:40.45ms
step:925/2110 train_time:37432ms step_avg:40.47ms
step:926/2110 train_time:37491ms step_avg:40.49ms
step:927/2110 train_time:37550ms step_avg:40.51ms
step:928/2110 train_time:37609ms step_avg:40.53ms
step:929/2110 train_time:37668ms step_avg:40.55ms
step:930/2110 train_time:37727ms step_avg:40.57ms
step:931/2110 train_time:37787ms step_avg:40.59ms
step:932/2110 train_time:37846ms step_avg:40.61ms
step:933/2110 train_time:37906ms step_avg:40.63ms
step:934/2110 train_time:37965ms step_avg:40.65ms
step:935/2110 train_time:38024ms step_avg:40.67ms
step:936/2110 train_time:38085ms step_avg:40.69ms
step:937/2110 train_time:38144ms step_avg:40.71ms
step:938/2110 train_time:38203ms step_avg:40.73ms
step:939/2110 train_time:38262ms step_avg:40.75ms
step:940/2110 train_time:38322ms step_avg:40.77ms
step:941/2110 train_time:38382ms step_avg:40.79ms
step:942/2110 train_time:38441ms step_avg:40.81ms
step:943/2110 train_time:38502ms step_avg:40.83ms
step:944/2110 train_time:38561ms step_avg:40.85ms
step:945/2110 train_time:38620ms step_avg:40.87ms
step:946/2110 train_time:38679ms step_avg:40.89ms
step:947/2110 train_time:38738ms step_avg:40.91ms
step:948/2110 train_time:38797ms step_avg:40.93ms
step:949/2110 train_time:38857ms step_avg:40.94ms
step:950/2110 train_time:38916ms step_avg:40.96ms
step:951/2110 train_time:38974ms step_avg:40.98ms
step:952/2110 train_time:39033ms step_avg:41.00ms
step:953/2110 train_time:39092ms step_avg:41.02ms
step:954/2110 train_time:39151ms step_avg:41.04ms
step:955/2110 train_time:39210ms step_avg:41.06ms
step:956/2110 train_time:39270ms step_avg:41.08ms
step:957/2110 train_time:39328ms step_avg:41.10ms
step:958/2110 train_time:39388ms step_avg:41.11ms
step:959/2110 train_time:39446ms step_avg:41.13ms
step:960/2110 train_time:39506ms step_avg:41.15ms
step:961/2110 train_time:39565ms step_avg:41.17ms
step:962/2110 train_time:39625ms step_avg:41.19ms
step:963/2110 train_time:39685ms step_avg:41.21ms
step:964/2110 train_time:39744ms step_avg:41.23ms
step:965/2110 train_time:39803ms step_avg:41.25ms
step:966/2110 train_time:39863ms step_avg:41.27ms
step:967/2110 train_time:39923ms step_avg:41.29ms
step:968/2110 train_time:39983ms step_avg:41.30ms
step:969/2110 train_time:40042ms step_avg:41.32ms
step:970/2110 train_time:40102ms step_avg:41.34ms
step:971/2110 train_time:40161ms step_avg:41.36ms
step:972/2110 train_time:40221ms step_avg:41.38ms
step:973/2110 train_time:40282ms step_avg:41.40ms
step:974/2110 train_time:40340ms step_avg:41.42ms
step:975/2110 train_time:40400ms step_avg:41.44ms
step:976/2110 train_time:40460ms step_avg:41.45ms
step:977/2110 train_time:40519ms step_avg:41.47ms
step:978/2110 train_time:40578ms step_avg:41.49ms
step:979/2110 train_time:40636ms step_avg:41.51ms
step:980/2110 train_time:40696ms step_avg:41.53ms
step:981/2110 train_time:40755ms step_avg:41.54ms
step:982/2110 train_time:40813ms step_avg:41.56ms
step:983/2110 train_time:40872ms step_avg:41.58ms
step:984/2110 train_time:40931ms step_avg:41.60ms
step:985/2110 train_time:40990ms step_avg:41.61ms
step:986/2110 train_time:41049ms step_avg:41.63ms
step:987/2110 train_time:41109ms step_avg:41.65ms
step:988/2110 train_time:41167ms step_avg:41.67ms
step:989/2110 train_time:41227ms step_avg:41.69ms
step:990/2110 train_time:41287ms step_avg:41.70ms
step:991/2110 train_time:41346ms step_avg:41.72ms
step:992/2110 train_time:41406ms step_avg:41.74ms
step:993/2110 train_time:41465ms step_avg:41.76ms
step:994/2110 train_time:41525ms step_avg:41.78ms
step:995/2110 train_time:41585ms step_avg:41.79ms
step:996/2110 train_time:41645ms step_avg:41.81ms
step:997/2110 train_time:41705ms step_avg:41.83ms
step:998/2110 train_time:41764ms step_avg:41.85ms
step:999/2110 train_time:41823ms step_avg:41.86ms
step:1000/2110 train_time:41883ms step_avg:41.88ms
step:1000/2110 val_loss:3.7563 train_time:41944ms step_avg:41.94ms
step:1001/2110 train_time:41983ms step_avg:41.94ms
step:1002/2110 train_time:42022ms step_avg:41.94ms
step:1003/2110 train_time:42069ms step_avg:41.94ms
step:1004/2110 train_time:42131ms step_avg:41.96ms
step:1005/2110 train_time:42191ms step_avg:41.98ms
step:1006/2110 train_time:42251ms step_avg:42.00ms
step:1007/2110 train_time:42310ms step_avg:42.02ms
step:1008/2110 train_time:42369ms step_avg:42.03ms
step:1009/2110 train_time:42429ms step_avg:42.05ms
step:1010/2110 train_time:42488ms step_avg:42.07ms
step:1011/2110 train_time:42546ms step_avg:42.08ms
step:1012/2110 train_time:42605ms step_avg:42.10ms
step:1013/2110 train_time:42663ms step_avg:42.12ms
step:1014/2110 train_time:42721ms step_avg:42.13ms
step:1015/2110 train_time:42779ms step_avg:42.15ms
step:1016/2110 train_time:42838ms step_avg:42.16ms
step:1017/2110 train_time:42897ms step_avg:42.18ms
step:1018/2110 train_time:42957ms step_avg:42.20ms
step:1019/2110 train_time:43019ms step_avg:42.22ms
step:1020/2110 train_time:43078ms step_avg:42.23ms
step:1021/2110 train_time:43138ms step_avg:42.25ms
step:1022/2110 train_time:43198ms step_avg:42.27ms
step:1023/2110 train_time:43257ms step_avg:42.28ms
step:1024/2110 train_time:43316ms step_avg:42.30ms
step:1025/2110 train_time:43375ms step_avg:42.32ms
step:1026/2110 train_time:43434ms step_avg:42.33ms
step:1027/2110 train_time:43492ms step_avg:42.35ms
step:1028/2110 train_time:43552ms step_avg:42.37ms
step:1029/2110 train_time:43611ms step_avg:42.38ms
step:1030/2110 train_time:43670ms step_avg:42.40ms
step:1031/2110 train_time:43729ms step_avg:42.41ms
step:1032/2110 train_time:43788ms step_avg:42.43ms
step:1033/2110 train_time:43848ms step_avg:42.45ms
step:1034/2110 train_time:43907ms step_avg:42.46ms
step:1035/2110 train_time:43968ms step_avg:42.48ms
step:1036/2110 train_time:44028ms step_avg:42.50ms
step:1037/2110 train_time:44090ms step_avg:42.52ms
step:1038/2110 train_time:44150ms step_avg:42.53ms
step:1039/2110 train_time:44210ms step_avg:42.55ms
step:1040/2110 train_time:44269ms step_avg:42.57ms
step:1041/2110 train_time:44329ms step_avg:42.58ms
step:1042/2110 train_time:44388ms step_avg:42.60ms
step:1043/2110 train_time:44448ms step_avg:42.62ms
step:1044/2110 train_time:44506ms step_avg:42.63ms
step:1045/2110 train_time:44566ms step_avg:42.65ms
step:1046/2110 train_time:44626ms step_avg:42.66ms
step:1047/2110 train_time:44684ms step_avg:42.68ms
step:1048/2110 train_time:44744ms step_avg:42.69ms
step:1049/2110 train_time:44802ms step_avg:42.71ms
step:1050/2110 train_time:44861ms step_avg:42.73ms
step:1051/2110 train_time:44921ms step_avg:42.74ms
step:1052/2110 train_time:44980ms step_avg:42.76ms
step:1053/2110 train_time:45039ms step_avg:42.77ms
step:1054/2110 train_time:45098ms step_avg:42.79ms
step:1055/2110 train_time:45158ms step_avg:42.80ms
step:1056/2110 train_time:45218ms step_avg:42.82ms
step:1057/2110 train_time:45277ms step_avg:42.84ms
step:1058/2110 train_time:45337ms step_avg:42.85ms
step:1059/2110 train_time:45395ms step_avg:42.87ms
step:1060/2110 train_time:45454ms step_avg:42.88ms
step:1061/2110 train_time:45513ms step_avg:42.90ms
step:1062/2110 train_time:45572ms step_avg:42.91ms
step:1063/2110 train_time:45632ms step_avg:42.93ms
step:1064/2110 train_time:45691ms step_avg:42.94ms
step:1065/2110 train_time:45750ms step_avg:42.96ms
step:1066/2110 train_time:45810ms step_avg:42.97ms
step:1067/2110 train_time:45870ms step_avg:42.99ms
step:1068/2110 train_time:45930ms step_avg:43.01ms
step:1069/2110 train_time:45991ms step_avg:43.02ms
step:1070/2110 train_time:46051ms step_avg:43.04ms
step:1071/2110 train_time:46111ms step_avg:43.05ms
step:1072/2110 train_time:46170ms step_avg:43.07ms
step:1073/2110 train_time:46230ms step_avg:43.09ms
step:1074/2110 train_time:46290ms step_avg:43.10ms
step:1075/2110 train_time:46349ms step_avg:43.12ms
step:1076/2110 train_time:46409ms step_avg:43.13ms
step:1077/2110 train_time:46467ms step_avg:43.15ms
step:1078/2110 train_time:46527ms step_avg:43.16ms
step:1079/2110 train_time:46586ms step_avg:43.18ms
step:1080/2110 train_time:46645ms step_avg:43.19ms
step:1081/2110 train_time:46704ms step_avg:43.20ms
step:1082/2110 train_time:46763ms step_avg:43.22ms
step:1083/2110 train_time:46823ms step_avg:43.23ms
step:1084/2110 train_time:46881ms step_avg:43.25ms
step:1085/2110 train_time:46941ms step_avg:43.26ms
step:1086/2110 train_time:47000ms step_avg:43.28ms
step:1087/2110 train_time:47060ms step_avg:43.29ms
step:1088/2110 train_time:47118ms step_avg:43.31ms
step:1089/2110 train_time:47179ms step_avg:43.32ms
step:1090/2110 train_time:47238ms step_avg:43.34ms
step:1091/2110 train_time:47297ms step_avg:43.35ms
step:1092/2110 train_time:47356ms step_avg:43.37ms
step:1093/2110 train_time:47416ms step_avg:43.38ms
step:1094/2110 train_time:47475ms step_avg:43.40ms
step:1095/2110 train_time:47534ms step_avg:43.41ms
step:1096/2110 train_time:47594ms step_avg:43.43ms
step:1097/2110 train_time:47653ms step_avg:43.44ms
step:1098/2110 train_time:47713ms step_avg:43.45ms
step:1099/2110 train_time:47772ms step_avg:43.47ms
step:1100/2110 train_time:47833ms step_avg:43.48ms
step:1101/2110 train_time:47892ms step_avg:43.50ms
step:1102/2110 train_time:47951ms step_avg:43.51ms
step:1103/2110 train_time:48011ms step_avg:43.53ms
step:1104/2110 train_time:48070ms step_avg:43.54ms
step:1105/2110 train_time:48131ms step_avg:43.56ms
step:1106/2110 train_time:48189ms step_avg:43.57ms
step:1107/2110 train_time:48250ms step_avg:43.59ms
step:1108/2110 train_time:48308ms step_avg:43.60ms
step:1109/2110 train_time:48369ms step_avg:43.62ms
step:1110/2110 train_time:48428ms step_avg:43.63ms
step:1111/2110 train_time:48488ms step_avg:43.64ms
step:1112/2110 train_time:48547ms step_avg:43.66ms
step:1113/2110 train_time:48606ms step_avg:43.67ms
step:1114/2110 train_time:48666ms step_avg:43.69ms
step:1115/2110 train_time:48725ms step_avg:43.70ms
step:1116/2110 train_time:48784ms step_avg:43.71ms
step:1117/2110 train_time:48844ms step_avg:43.73ms
step:1118/2110 train_time:48902ms step_avg:43.74ms
step:1119/2110 train_time:48961ms step_avg:43.75ms
step:1120/2110 train_time:49020ms step_avg:43.77ms
step:1121/2110 train_time:49079ms step_avg:43.78ms
step:1122/2110 train_time:49138ms step_avg:43.79ms
step:1123/2110 train_time:49197ms step_avg:43.81ms
step:1124/2110 train_time:49256ms step_avg:43.82ms
step:1125/2110 train_time:49315ms step_avg:43.84ms
step:1126/2110 train_time:49375ms step_avg:43.85ms
step:1127/2110 train_time:49434ms step_avg:43.86ms
step:1128/2110 train_time:49492ms step_avg:43.88ms
step:1129/2110 train_time:49552ms step_avg:43.89ms
step:1130/2110 train_time:49611ms step_avg:43.90ms
step:1131/2110 train_time:49671ms step_avg:43.92ms
step:1132/2110 train_time:49730ms step_avg:43.93ms
step:1133/2110 train_time:49790ms step_avg:43.95ms
step:1134/2110 train_time:49850ms step_avg:43.96ms
step:1135/2110 train_time:49909ms step_avg:43.97ms
step:1136/2110 train_time:49969ms step_avg:43.99ms
step:1137/2110 train_time:50029ms step_avg:44.00ms
step:1138/2110 train_time:50089ms step_avg:44.01ms
step:1139/2110 train_time:50149ms step_avg:44.03ms
step:1140/2110 train_time:50208ms step_avg:44.04ms
step:1141/2110 train_time:50268ms step_avg:44.06ms
step:1142/2110 train_time:50328ms step_avg:44.07ms
step:1143/2110 train_time:50389ms step_avg:44.09ms
step:1144/2110 train_time:50449ms step_avg:44.10ms
step:1145/2110 train_time:50510ms step_avg:44.11ms
step:1146/2110 train_time:50569ms step_avg:44.13ms
step:1147/2110 train_time:50630ms step_avg:44.14ms
step:1148/2110 train_time:50691ms step_avg:44.16ms
step:1149/2110 train_time:50751ms step_avg:44.17ms
step:1150/2110 train_time:50810ms step_avg:44.18ms
step:1151/2110 train_time:50871ms step_avg:44.20ms
step:1152/2110 train_time:50931ms step_avg:44.21ms
step:1153/2110 train_time:50992ms step_avg:44.23ms
step:1154/2110 train_time:51051ms step_avg:44.24ms
step:1155/2110 train_time:51112ms step_avg:44.25ms
step:1156/2110 train_time:51172ms step_avg:44.27ms
step:1157/2110 train_time:51233ms step_avg:44.28ms
step:1158/2110 train_time:51292ms step_avg:44.29ms
step:1159/2110 train_time:51353ms step_avg:44.31ms
step:1160/2110 train_time:51412ms step_avg:44.32ms
step:1161/2110 train_time:51472ms step_avg:44.33ms
step:1162/2110 train_time:51532ms step_avg:44.35ms
step:1163/2110 train_time:51592ms step_avg:44.36ms
step:1164/2110 train_time:51651ms step_avg:44.37ms
step:1165/2110 train_time:51712ms step_avg:44.39ms
step:1166/2110 train_time:51772ms step_avg:44.40ms
step:1167/2110 train_time:51833ms step_avg:44.42ms
step:1168/2110 train_time:51892ms step_avg:44.43ms
step:1169/2110 train_time:51955ms step_avg:44.44ms
step:1170/2110 train_time:52014ms step_avg:44.46ms
step:1171/2110 train_time:52074ms step_avg:44.47ms
step:1172/2110 train_time:52134ms step_avg:44.48ms
step:1173/2110 train_time:52194ms step_avg:44.50ms
step:1174/2110 train_time:52253ms step_avg:44.51ms
step:1175/2110 train_time:52313ms step_avg:44.52ms
step:1176/2110 train_time:52373ms step_avg:44.53ms
step:1177/2110 train_time:52434ms step_avg:44.55ms
step:1178/2110 train_time:52493ms step_avg:44.56ms
step:1179/2110 train_time:52554ms step_avg:44.57ms
step:1180/2110 train_time:52612ms step_avg:44.59ms
step:1181/2110 train_time:52673ms step_avg:44.60ms
step:1182/2110 train_time:52733ms step_avg:44.61ms
step:1183/2110 train_time:52793ms step_avg:44.63ms
step:1184/2110 train_time:52853ms step_avg:44.64ms
step:1185/2110 train_time:52913ms step_avg:44.65ms
step:1186/2110 train_time:52973ms step_avg:44.66ms
step:1187/2110 train_time:53034ms step_avg:44.68ms
step:1188/2110 train_time:53093ms step_avg:44.69ms
step:1189/2110 train_time:53154ms step_avg:44.70ms
step:1190/2110 train_time:53213ms step_avg:44.72ms
step:1191/2110 train_time:53274ms step_avg:44.73ms
step:1192/2110 train_time:53333ms step_avg:44.74ms
step:1193/2110 train_time:53394ms step_avg:44.76ms
step:1194/2110 train_time:53453ms step_avg:44.77ms
step:1195/2110 train_time:53513ms step_avg:44.78ms
step:1196/2110 train_time:53574ms step_avg:44.79ms
step:1197/2110 train_time:53633ms step_avg:44.81ms
step:1198/2110 train_time:53693ms step_avg:44.82ms
step:1199/2110 train_time:53754ms step_avg:44.83ms
step:1200/2110 train_time:53814ms step_avg:44.84ms
step:1201/2110 train_time:53874ms step_avg:44.86ms
step:1202/2110 train_time:53934ms step_avg:44.87ms
step:1203/2110 train_time:53994ms step_avg:44.88ms
step:1204/2110 train_time:54053ms step_avg:44.89ms
step:1205/2110 train_time:54113ms step_avg:44.91ms
step:1206/2110 train_time:54173ms step_avg:44.92ms
step:1207/2110 train_time:54234ms step_avg:44.93ms
step:1208/2110 train_time:54293ms step_avg:44.94ms
step:1209/2110 train_time:54354ms step_avg:44.96ms
step:1210/2110 train_time:54413ms step_avg:44.97ms
step:1211/2110 train_time:54473ms step_avg:44.98ms
step:1212/2110 train_time:54533ms step_avg:44.99ms
step:1213/2110 train_time:54593ms step_avg:45.01ms
step:1214/2110 train_time:54652ms step_avg:45.02ms
step:1215/2110 train_time:54713ms step_avg:45.03ms
step:1216/2110 train_time:54773ms step_avg:45.04ms
step:1217/2110 train_time:54834ms step_avg:45.06ms
step:1218/2110 train_time:54893ms step_avg:45.07ms
step:1219/2110 train_time:54954ms step_avg:45.08ms
step:1220/2110 train_time:55013ms step_avg:45.09ms
step:1221/2110 train_time:55074ms step_avg:45.11ms
step:1222/2110 train_time:55134ms step_avg:45.12ms
step:1223/2110 train_time:55194ms step_avg:45.13ms
step:1224/2110 train_time:55254ms step_avg:45.14ms
step:1225/2110 train_time:55315ms step_avg:45.15ms
step:1226/2110 train_time:55373ms step_avg:45.17ms
step:1227/2110 train_time:55433ms step_avg:45.18ms
step:1228/2110 train_time:55493ms step_avg:45.19ms
step:1229/2110 train_time:55553ms step_avg:45.20ms
step:1230/2110 train_time:55612ms step_avg:45.21ms
step:1231/2110 train_time:55673ms step_avg:45.23ms
step:1232/2110 train_time:55733ms step_avg:45.24ms
step:1233/2110 train_time:55794ms step_avg:45.25ms
step:1234/2110 train_time:55853ms step_avg:45.26ms
step:1235/2110 train_time:55913ms step_avg:45.27ms
step:1236/2110 train_time:55973ms step_avg:45.29ms
step:1237/2110 train_time:56033ms step_avg:45.30ms
step:1238/2110 train_time:56092ms step_avg:45.31ms
step:1239/2110 train_time:56153ms step_avg:45.32ms
step:1240/2110 train_time:56213ms step_avg:45.33ms
step:1241/2110 train_time:56273ms step_avg:45.34ms
step:1242/2110 train_time:56333ms step_avg:45.36ms
step:1243/2110 train_time:56393ms step_avg:45.37ms
step:1244/2110 train_time:56452ms step_avg:45.38ms
step:1245/2110 train_time:56513ms step_avg:45.39ms
step:1246/2110 train_time:56573ms step_avg:45.40ms
step:1247/2110 train_time:56633ms step_avg:45.42ms
step:1248/2110 train_time:56693ms step_avg:45.43ms
step:1249/2110 train_time:56754ms step_avg:45.44ms
step:1250/2110 train_time:56813ms step_avg:45.45ms
step:1250/2110 val_loss:3.5884 train_time:56876ms step_avg:45.50ms
step:1251/2110 train_time:56908ms step_avg:45.49ms
step:1252/2110 train_time:56938ms step_avg:45.48ms
step:1253/2110 train_time:57002ms step_avg:45.49ms
step:1254/2110 train_time:57066ms step_avg:45.51ms
step:1255/2110 train_time:57129ms step_avg:45.52ms
step:1256/2110 train_time:57188ms step_avg:45.53ms
step:1257/2110 train_time:57248ms step_avg:45.54ms
step:1258/2110 train_time:57307ms step_avg:45.55ms
step:1259/2110 train_time:57367ms step_avg:45.57ms
step:1260/2110 train_time:57426ms step_avg:45.58ms
step:1261/2110 train_time:57485ms step_avg:45.59ms
step:1262/2110 train_time:57544ms step_avg:45.60ms
step:1263/2110 train_time:57604ms step_avg:45.61ms
step:1264/2110 train_time:57663ms step_avg:45.62ms
step:1265/2110 train_time:57722ms step_avg:45.63ms
step:1266/2110 train_time:57780ms step_avg:45.64ms
step:1267/2110 train_time:57841ms step_avg:45.65ms
step:1268/2110 train_time:57901ms step_avg:45.66ms
step:1269/2110 train_time:57965ms step_avg:45.68ms
step:1270/2110 train_time:58027ms step_avg:45.69ms
step:1271/2110 train_time:58090ms step_avg:45.70ms
step:1272/2110 train_time:58149ms step_avg:45.71ms
step:1273/2110 train_time:58210ms step_avg:45.73ms
step:1274/2110 train_time:58269ms step_avg:45.74ms
step:1275/2110 train_time:58329ms step_avg:45.75ms
step:1276/2110 train_time:58389ms step_avg:45.76ms
step:1277/2110 train_time:58449ms step_avg:45.77ms
step:1278/2110 train_time:58508ms step_avg:45.78ms
step:1279/2110 train_time:58567ms step_avg:45.79ms
step:1280/2110 train_time:58626ms step_avg:45.80ms
step:1281/2110 train_time:58686ms step_avg:45.81ms
step:1282/2110 train_time:58745ms step_avg:45.82ms
step:1283/2110 train_time:58807ms step_avg:45.84ms
step:1284/2110 train_time:58868ms step_avg:45.85ms
step:1285/2110 train_time:58930ms step_avg:45.86ms
step:1286/2110 train_time:58990ms step_avg:45.87ms
step:1287/2110 train_time:59051ms step_avg:45.88ms
step:1288/2110 train_time:59110ms step_avg:45.89ms
step:1289/2110 train_time:59172ms step_avg:45.91ms
step:1290/2110 train_time:59231ms step_avg:45.92ms
step:1291/2110 train_time:59295ms step_avg:45.93ms
step:1292/2110 train_time:59351ms step_avg:45.94ms
step:1293/2110 train_time:59411ms step_avg:45.95ms
step:1294/2110 train_time:59471ms step_avg:45.96ms
step:1295/2110 train_time:59530ms step_avg:45.97ms
step:1296/2110 train_time:59589ms step_avg:45.98ms
step:1297/2110 train_time:59648ms step_avg:45.99ms
step:1298/2110 train_time:59707ms step_avg:46.00ms
step:1299/2110 train_time:59768ms step_avg:46.01ms
step:1300/2110 train_time:59828ms step_avg:46.02ms
step:1301/2110 train_time:59889ms step_avg:46.03ms
step:1302/2110 train_time:59949ms step_avg:46.04ms
step:1303/2110 train_time:60010ms step_avg:46.05ms
step:1304/2110 train_time:60071ms step_avg:46.07ms
step:1305/2110 train_time:60131ms step_avg:46.08ms
step:1306/2110 train_time:60191ms step_avg:46.09ms
step:1307/2110 train_time:60252ms step_avg:46.10ms
step:1308/2110 train_time:60311ms step_avg:46.11ms
step:1309/2110 train_time:60371ms step_avg:46.12ms
step:1310/2110 train_time:60430ms step_avg:46.13ms
step:1311/2110 train_time:60491ms step_avg:46.14ms
step:1312/2110 train_time:60550ms step_avg:46.15ms
step:1313/2110 train_time:60610ms step_avg:46.16ms
step:1314/2110 train_time:60669ms step_avg:46.17ms
step:1315/2110 train_time:60729ms step_avg:46.18ms
step:1316/2110 train_time:60789ms step_avg:46.19ms
step:1317/2110 train_time:60851ms step_avg:46.20ms
step:1318/2110 train_time:60910ms step_avg:46.21ms
step:1319/2110 train_time:60971ms step_avg:46.23ms
step:1320/2110 train_time:61031ms step_avg:46.24ms
step:1321/2110 train_time:61092ms step_avg:46.25ms
step:1322/2110 train_time:61150ms step_avg:46.26ms
step:1323/2110 train_time:61211ms step_avg:46.27ms
step:1324/2110 train_time:61271ms step_avg:46.28ms
step:1325/2110 train_time:61332ms step_avg:46.29ms
step:1326/2110 train_time:61391ms step_avg:46.30ms
step:1327/2110 train_time:61451ms step_avg:46.31ms
step:1328/2110 train_time:61511ms step_avg:46.32ms
step:1329/2110 train_time:61571ms step_avg:46.33ms
step:1330/2110 train_time:61630ms step_avg:46.34ms
step:1331/2110 train_time:61690ms step_avg:46.35ms
step:1332/2110 train_time:61749ms step_avg:46.36ms
step:1333/2110 train_time:61811ms step_avg:46.37ms
step:1334/2110 train_time:61870ms step_avg:46.38ms
step:1335/2110 train_time:61932ms step_avg:46.39ms
step:1336/2110 train_time:61991ms step_avg:46.40ms
step:1337/2110 train_time:62052ms step_avg:46.41ms
step:1338/2110 train_time:62112ms step_avg:46.42ms
step:1339/2110 train_time:62172ms step_avg:46.43ms
step:1340/2110 train_time:62231ms step_avg:46.44ms
step:1341/2110 train_time:62292ms step_avg:46.45ms
step:1342/2110 train_time:62351ms step_avg:46.46ms
step:1343/2110 train_time:62411ms step_avg:46.47ms
step:1344/2110 train_time:62470ms step_avg:46.48ms
step:1345/2110 train_time:62530ms step_avg:46.49ms
step:1346/2110 train_time:62589ms step_avg:46.50ms
step:1347/2110 train_time:62650ms step_avg:46.51ms
step:1348/2110 train_time:62709ms step_avg:46.52ms
step:1349/2110 train_time:62770ms step_avg:46.53ms
step:1350/2110 train_time:62830ms step_avg:46.54ms
step:1351/2110 train_time:62892ms step_avg:46.55ms
step:1352/2110 train_time:62951ms step_avg:46.56ms
step:1353/2110 train_time:63012ms step_avg:46.57ms
step:1354/2110 train_time:63072ms step_avg:46.58ms
step:1355/2110 train_time:63133ms step_avg:46.59ms
step:1356/2110 train_time:63192ms step_avg:46.60ms
step:1357/2110 train_time:63253ms step_avg:46.61ms
step:1358/2110 train_time:63312ms step_avg:46.62ms
step:1359/2110 train_time:63373ms step_avg:46.63ms
step:1360/2110 train_time:63432ms step_avg:46.64ms
step:1361/2110 train_time:63492ms step_avg:46.65ms
step:1362/2110 train_time:63551ms step_avg:46.66ms
step:1363/2110 train_time:63611ms step_avg:46.67ms
step:1364/2110 train_time:63670ms step_avg:46.68ms
step:1365/2110 train_time:63731ms step_avg:46.69ms
step:1366/2110 train_time:63791ms step_avg:46.70ms
step:1367/2110 train_time:63852ms step_avg:46.71ms
step:1368/2110 train_time:63911ms step_avg:46.72ms
step:1369/2110 train_time:63972ms step_avg:46.73ms
step:1370/2110 train_time:64031ms step_avg:46.74ms
step:1371/2110 train_time:64093ms step_avg:46.75ms
step:1372/2110 train_time:64152ms step_avg:46.76ms
step:1373/2110 train_time:64213ms step_avg:46.77ms
step:1374/2110 train_time:64273ms step_avg:46.78ms
step:1375/2110 train_time:64333ms step_avg:46.79ms
step:1376/2110 train_time:64392ms step_avg:46.80ms
step:1377/2110 train_time:64452ms step_avg:46.81ms
step:1378/2110 train_time:64511ms step_avg:46.81ms
step:1379/2110 train_time:64572ms step_avg:46.83ms
step:1380/2110 train_time:64631ms step_avg:46.83ms
step:1381/2110 train_time:64692ms step_avg:46.84ms
step:1382/2110 train_time:64778ms step_avg:46.87ms
step:1383/2110 train_time:64868ms step_avg:46.90ms
step:1384/2110 train_time:64954ms step_avg:46.93ms
step:1385/2110 train_time:65042ms step_avg:46.96ms
step:1386/2110 train_time:65129ms step_avg:46.99ms
step:1387/2110 train_time:65217ms step_avg:47.02ms
step:1388/2110 train_time:65304ms step_avg:47.05ms
step:1389/2110 train_time:65391ms step_avg:47.08ms
step:1390/2110 train_time:65477ms step_avg:47.11ms
step:1391/2110 train_time:65564ms step_avg:47.13ms
step:1392/2110 train_time:65649ms step_avg:47.16ms
step:1393/2110 train_time:65737ms step_avg:47.19ms
step:1394/2110 train_time:65824ms step_avg:47.22ms
step:1395/2110 train_time:65911ms step_avg:47.25ms
step:1396/2110 train_time:65998ms step_avg:47.28ms
step:1397/2110 train_time:66085ms step_avg:47.31ms
step:1398/2110 train_time:66172ms step_avg:47.33ms
step:1399/2110 train_time:66260ms step_avg:47.36ms
step:1400/2110 train_time:66346ms step_avg:47.39ms
step:1401/2110 train_time:66434ms step_avg:47.42ms
step:1402/2110 train_time:66520ms step_avg:47.45ms
step:1403/2110 train_time:66606ms step_avg:47.47ms
step:1404/2110 train_time:66692ms step_avg:47.50ms
step:1405/2110 train_time:66780ms step_avg:47.53ms
step:1406/2110 train_time:66866ms step_avg:47.56ms
step:1407/2110 train_time:66954ms step_avg:47.59ms
step:1408/2110 train_time:67041ms step_avg:47.61ms
step:1409/2110 train_time:67128ms step_avg:47.64ms
step:1410/2110 train_time:67214ms step_avg:47.67ms
step:1411/2110 train_time:67302ms step_avg:47.70ms
step:1412/2110 train_time:67387ms step_avg:47.72ms
step:1413/2110 train_time:67474ms step_avg:47.75ms
step:1414/2110 train_time:67561ms step_avg:47.78ms
step:1415/2110 train_time:67647ms step_avg:47.81ms
step:1416/2110 train_time:67733ms step_avg:47.83ms
step:1417/2110 train_time:67821ms step_avg:47.86ms
step:1418/2110 train_time:67907ms step_avg:47.89ms
step:1419/2110 train_time:67994ms step_avg:47.92ms
step:1420/2110 train_time:68081ms step_avg:47.94ms
step:1421/2110 train_time:68168ms step_avg:47.97ms
step:1422/2110 train_time:68254ms step_avg:48.00ms
step:1423/2110 train_time:68342ms step_avg:48.03ms
step:1424/2110 train_time:68428ms step_avg:48.05ms
step:1425/2110 train_time:68516ms step_avg:48.08ms
step:1426/2110 train_time:68603ms step_avg:48.11ms
step:1427/2110 train_time:68689ms step_avg:48.14ms
step:1428/2110 train_time:68775ms step_avg:48.16ms
step:1429/2110 train_time:68863ms step_avg:48.19ms
step:1430/2110 train_time:68948ms step_avg:48.22ms
step:1431/2110 train_time:69036ms step_avg:48.24ms
step:1432/2110 train_time:69123ms step_avg:48.27ms
step:1433/2110 train_time:69209ms step_avg:48.30ms
step:1434/2110 train_time:69295ms step_avg:48.32ms
step:1435/2110 train_time:69383ms step_avg:48.35ms
step:1436/2110 train_time:69469ms step_avg:48.38ms
step:1437/2110 train_time:69556ms step_avg:48.40ms
step:1438/2110 train_time:69643ms step_avg:48.43ms
step:1439/2110 train_time:69729ms step_avg:48.46ms
step:1440/2110 train_time:69815ms step_avg:48.48ms
step:1441/2110 train_time:69903ms step_avg:48.51ms
step:1442/2110 train_time:69990ms step_avg:48.54ms
step:1443/2110 train_time:70078ms step_avg:48.56ms
step:1444/2110 train_time:70163ms step_avg:48.59ms
step:1445/2110 train_time:70250ms step_avg:48.62ms
step:1446/2110 train_time:70337ms step_avg:48.64ms
step:1447/2110 train_time:70426ms step_avg:48.67ms
step:1448/2110 train_time:70513ms step_avg:48.70ms
step:1449/2110 train_time:70600ms step_avg:48.72ms
step:1450/2110 train_time:70686ms step_avg:48.75ms
step:1451/2110 train_time:70772ms step_avg:48.77ms
step:1452/2110 train_time:70859ms step_avg:48.80ms
step:1453/2110 train_time:70945ms step_avg:48.83ms
step:1454/2110 train_time:71032ms step_avg:48.85ms
step:1455/2110 train_time:71119ms step_avg:48.88ms
step:1456/2110 train_time:71205ms step_avg:48.90ms
step:1457/2110 train_time:71292ms step_avg:48.93ms
step:1458/2110 train_time:71378ms step_avg:48.96ms
step:1459/2110 train_time:71465ms step_avg:48.98ms
step:1460/2110 train_time:71551ms step_avg:49.01ms
step:1461/2110 train_time:71639ms step_avg:49.03ms
step:1462/2110 train_time:71725ms step_avg:49.06ms
step:1463/2110 train_time:71812ms step_avg:49.09ms
step:1464/2110 train_time:71899ms step_avg:49.11ms
step:1465/2110 train_time:71987ms step_avg:49.14ms
step:1466/2110 train_time:72074ms step_avg:49.16ms
step:1467/2110 train_time:72161ms step_avg:49.19ms
step:1468/2110 train_time:72247ms step_avg:49.21ms
step:1469/2110 train_time:72335ms step_avg:49.24ms
step:1470/2110 train_time:72422ms step_avg:49.27ms
step:1471/2110 train_time:72508ms step_avg:49.29ms
step:1472/2110 train_time:72596ms step_avg:49.32ms
step:1473/2110 train_time:72683ms step_avg:49.34ms
step:1474/2110 train_time:72769ms step_avg:49.37ms
step:1475/2110 train_time:72856ms step_avg:49.39ms
step:1476/2110 train_time:72942ms step_avg:49.42ms
step:1477/2110 train_time:73029ms step_avg:49.44ms
step:1478/2110 train_time:73115ms step_avg:49.47ms
step:1479/2110 train_time:73204ms step_avg:49.50ms
step:1480/2110 train_time:73290ms step_avg:49.52ms
step:1481/2110 train_time:73377ms step_avg:49.55ms
step:1482/2110 train_time:73463ms step_avg:49.57ms
step:1483/2110 train_time:73550ms step_avg:49.60ms
step:1484/2110 train_time:73636ms step_avg:49.62ms
step:1485/2110 train_time:73725ms step_avg:49.65ms
step:1486/2110 train_time:73811ms step_avg:49.67ms
step:1487/2110 train_time:73899ms step_avg:49.70ms
step:1488/2110 train_time:73985ms step_avg:49.72ms
step:1489/2110 train_time:74072ms step_avg:49.75ms
step:1490/2110 train_time:74159ms step_avg:49.77ms
step:1491/2110 train_time:74247ms step_avg:49.80ms
step:1492/2110 train_time:74334ms step_avg:49.82ms
step:1493/2110 train_time:74421ms step_avg:49.85ms
step:1494/2110 train_time:74507ms step_avg:49.87ms
step:1495/2110 train_time:74594ms step_avg:49.90ms
step:1496/2110 train_time:74681ms step_avg:49.92ms
step:1497/2110 train_time:74768ms step_avg:49.94ms
step:1498/2110 train_time:74853ms step_avg:49.97ms
step:1499/2110 train_time:74942ms step_avg:49.99ms
step:1500/2110 train_time:75027ms step_avg:50.02ms
step:1500/2110 val_loss:3.4899 train_time:75116ms step_avg:50.08ms
step:1501/2110 train_time:75148ms step_avg:50.07ms
step:1502/2110 train_time:75206ms step_avg:50.07ms
step:1503/2110 train_time:75300ms step_avg:50.10ms
step:1504/2110 train_time:75387ms step_avg:50.12ms
step:1505/2110 train_time:75474ms step_avg:50.15ms
step:1506/2110 train_time:75560ms step_avg:50.17ms
step:1507/2110 train_time:75646ms step_avg:50.20ms
step:1508/2110 train_time:75732ms step_avg:50.22ms
step:1509/2110 train_time:75818ms step_avg:50.24ms
step:1510/2110 train_time:75903ms step_avg:50.27ms
step:1511/2110 train_time:75989ms step_avg:50.29ms
step:1512/2110 train_time:76075ms step_avg:50.31ms
step:1513/2110 train_time:76167ms step_avg:50.34ms
step:1514/2110 train_time:76256ms step_avg:50.37ms
step:1515/2110 train_time:76346ms step_avg:50.39ms
step:1516/2110 train_time:76433ms step_avg:50.42ms
step:1517/2110 train_time:76520ms step_avg:50.44ms
step:1518/2110 train_time:76606ms step_avg:50.46ms
step:1519/2110 train_time:76692ms step_avg:50.49ms
step:1520/2110 train_time:76778ms step_avg:50.51ms
step:1521/2110 train_time:76864ms step_avg:50.54ms
step:1522/2110 train_time:76949ms step_avg:50.56ms
step:1523/2110 train_time:77037ms step_avg:50.58ms
step:1524/2110 train_time:77124ms step_avg:50.61ms
step:1525/2110 train_time:77213ms step_avg:50.63ms
step:1526/2110 train_time:77301ms step_avg:50.66ms
step:1527/2110 train_time:77390ms step_avg:50.68ms
step:1528/2110 train_time:77478ms step_avg:50.71ms
step:1529/2110 train_time:77564ms step_avg:50.73ms
step:1530/2110 train_time:77651ms step_avg:50.75ms
step:1531/2110 train_time:77735ms step_avg:50.77ms
step:1532/2110 train_time:77821ms step_avg:50.80ms
step:1533/2110 train_time:77908ms step_avg:50.82ms
step:1534/2110 train_time:77994ms step_avg:50.84ms
step:1535/2110 train_time:78081ms step_avg:50.87ms
step:1536/2110 train_time:78169ms step_avg:50.89ms
step:1537/2110 train_time:78257ms step_avg:50.92ms
step:1538/2110 train_time:78345ms step_avg:50.94ms
step:1539/2110 train_time:78432ms step_avg:50.96ms
step:1540/2110 train_time:78521ms step_avg:50.99ms
step:1541/2110 train_time:78607ms step_avg:51.01ms
step:1542/2110 train_time:78693ms step_avg:51.03ms
step:1543/2110 train_time:78779ms step_avg:51.06ms
step:1544/2110 train_time:78866ms step_avg:51.08ms
step:1545/2110 train_time:78952ms step_avg:51.10ms
step:1546/2110 train_time:79039ms step_avg:51.12ms
step:1547/2110 train_time:79125ms step_avg:51.15ms
step:1548/2110 train_time:79212ms step_avg:51.17ms
step:1549/2110 train_time:79301ms step_avg:51.20ms
step:1550/2110 train_time:79388ms step_avg:51.22ms
step:1551/2110 train_time:79474ms step_avg:51.24ms
step:1552/2110 train_time:79562ms step_avg:51.26ms
step:1553/2110 train_time:79648ms step_avg:51.29ms
step:1554/2110 train_time:79735ms step_avg:51.31ms
step:1555/2110 train_time:79821ms step_avg:51.33ms
step:1556/2110 train_time:79907ms step_avg:51.35ms
step:1557/2110 train_time:79993ms step_avg:51.38ms
step:1558/2110 train_time:80081ms step_avg:51.40ms
step:1559/2110 train_time:80168ms step_avg:51.42ms
step:1560/2110 train_time:80256ms step_avg:51.45ms
step:1561/2110 train_time:80344ms step_avg:51.47ms
step:1562/2110 train_time:80430ms step_avg:51.49ms
step:1563/2110 train_time:80517ms step_avg:51.51ms
step:1564/2110 train_time:80604ms step_avg:51.54ms
step:1565/2110 train_time:80690ms step_avg:51.56ms
step:1566/2110 train_time:80777ms step_avg:51.58ms
step:1567/2110 train_time:80863ms step_avg:51.60ms
step:1568/2110 train_time:80949ms step_avg:51.63ms
step:1569/2110 train_time:81036ms step_avg:51.65ms
step:1570/2110 train_time:81124ms step_avg:51.67ms
step:1571/2110 train_time:81211ms step_avg:51.69ms
step:1572/2110 train_time:81298ms step_avg:51.72ms
step:1573/2110 train_time:81385ms step_avg:51.74ms
step:1574/2110 train_time:81473ms step_avg:51.76ms
step:1575/2110 train_time:81559ms step_avg:51.78ms
step:1576/2110 train_time:81647ms step_avg:51.81ms
step:1577/2110 train_time:81733ms step_avg:51.83ms
step:1578/2110 train_time:81820ms step_avg:51.85ms
step:1579/2110 train_time:81907ms step_avg:51.87ms
step:1580/2110 train_time:81992ms step_avg:51.89ms
step:1581/2110 train_time:82079ms step_avg:51.92ms
step:1582/2110 train_time:82168ms step_avg:51.94ms
step:1583/2110 train_time:82254ms step_avg:51.96ms
step:1584/2110 train_time:82342ms step_avg:51.98ms
step:1585/2110 train_time:82428ms step_avg:52.01ms
step:1586/2110 train_time:82516ms step_avg:52.03ms
step:1587/2110 train_time:82602ms step_avg:52.05ms
step:1588/2110 train_time:82689ms step_avg:52.07ms
step:1589/2110 train_time:82775ms step_avg:52.09ms
step:1590/2110 train_time:82862ms step_avg:52.11ms
step:1591/2110 train_time:82948ms step_avg:52.14ms
step:1592/2110 train_time:83035ms step_avg:52.16ms
step:1593/2110 train_time:83122ms step_avg:52.18ms
step:1594/2110 train_time:83209ms step_avg:52.20ms
step:1595/2110 train_time:83295ms step_avg:52.22ms
step:1596/2110 train_time:83383ms step_avg:52.25ms
step:1597/2110 train_time:83470ms step_avg:52.27ms
step:1598/2110 train_time:83558ms step_avg:52.29ms
step:1599/2110 train_time:83646ms step_avg:52.31ms
step:1600/2110 train_time:83733ms step_avg:52.33ms
step:1601/2110 train_time:83820ms step_avg:52.35ms
step:1602/2110 train_time:83906ms step_avg:52.38ms
step:1603/2110 train_time:83993ms step_avg:52.40ms
step:1604/2110 train_time:84080ms step_avg:52.42ms
step:1605/2110 train_time:84167ms step_avg:52.44ms
step:1606/2110 train_time:84255ms step_avg:52.46ms
step:1607/2110 train_time:84342ms step_avg:52.48ms
step:1608/2110 train_time:84429ms step_avg:52.51ms
step:1609/2110 train_time:84516ms step_avg:52.53ms
step:1610/2110 train_time:84604ms step_avg:52.55ms
step:1611/2110 train_time:84691ms step_avg:52.57ms
step:1612/2110 train_time:84779ms step_avg:52.59ms
step:1613/2110 train_time:84866ms step_avg:52.61ms
step:1614/2110 train_time:84953ms step_avg:52.64ms
step:1615/2110 train_time:85039ms step_avg:52.66ms
step:1616/2110 train_time:85126ms step_avg:52.68ms
step:1617/2110 train_time:85213ms step_avg:52.70ms
step:1618/2110 train_time:85299ms step_avg:52.72ms
step:1619/2110 train_time:85387ms step_avg:52.74ms
step:1620/2110 train_time:85474ms step_avg:52.76ms
step:1621/2110 train_time:85561ms step_avg:52.78ms
step:1622/2110 train_time:85647ms step_avg:52.80ms
step:1623/2110 train_time:85734ms step_avg:52.82ms
step:1624/2110 train_time:85821ms step_avg:52.85ms
step:1625/2110 train_time:85907ms step_avg:52.87ms
step:1626/2110 train_time:85994ms step_avg:52.89ms
step:1627/2110 train_time:86081ms step_avg:52.91ms
step:1628/2110 train_time:86167ms step_avg:52.93ms
step:1629/2110 train_time:86254ms step_avg:52.95ms
step:1630/2110 train_time:86341ms step_avg:52.97ms
step:1631/2110 train_time:86428ms step_avg:52.99ms
step:1632/2110 train_time:86516ms step_avg:53.01ms
step:1633/2110 train_time:86603ms step_avg:53.03ms
step:1634/2110 train_time:86690ms step_avg:53.05ms
step:1635/2110 train_time:86776ms step_avg:53.07ms
step:1636/2110 train_time:86865ms step_avg:53.10ms
step:1637/2110 train_time:86951ms step_avg:53.12ms
step:1638/2110 train_time:87039ms step_avg:53.14ms
step:1639/2110 train_time:87126ms step_avg:53.16ms
step:1640/2110 train_time:87212ms step_avg:53.18ms
step:1641/2110 train_time:87300ms step_avg:53.20ms
step:1642/2110 train_time:87387ms step_avg:53.22ms
step:1643/2110 train_time:87473ms step_avg:53.24ms
step:1644/2110 train_time:87561ms step_avg:53.26ms
step:1645/2110 train_time:87648ms step_avg:53.28ms
step:1646/2110 train_time:87735ms step_avg:53.30ms
step:1647/2110 train_time:87821ms step_avg:53.32ms
step:1648/2110 train_time:87908ms step_avg:53.34ms
step:1649/2110 train_time:87994ms step_avg:53.36ms
step:1650/2110 train_time:88082ms step_avg:53.38ms
step:1651/2110 train_time:88169ms step_avg:53.40ms
step:1652/2110 train_time:88256ms step_avg:53.42ms
step:1653/2110 train_time:88343ms step_avg:53.44ms
step:1654/2110 train_time:88430ms step_avg:53.46ms
step:1655/2110 train_time:88518ms step_avg:53.49ms
step:1656/2110 train_time:88605ms step_avg:53.51ms
step:1657/2110 train_time:88691ms step_avg:53.53ms
step:1658/2110 train_time:88780ms step_avg:53.55ms
step:1659/2110 train_time:88868ms step_avg:53.57ms
step:1660/2110 train_time:88956ms step_avg:53.59ms
step:1661/2110 train_time:89044ms step_avg:53.61ms
step:1662/2110 train_time:89132ms step_avg:53.63ms
step:1663/2110 train_time:89220ms step_avg:53.65ms
step:1664/2110 train_time:89309ms step_avg:53.67ms
step:1665/2110 train_time:89396ms step_avg:53.69ms
step:1666/2110 train_time:89485ms step_avg:53.71ms
step:1667/2110 train_time:89572ms step_avg:53.73ms
step:1668/2110 train_time:89660ms step_avg:53.75ms
step:1669/2110 train_time:89749ms step_avg:53.77ms
step:1670/2110 train_time:89837ms step_avg:53.79ms
step:1671/2110 train_time:89924ms step_avg:53.81ms
step:1672/2110 train_time:90012ms step_avg:53.84ms
step:1673/2110 train_time:90102ms step_avg:53.86ms
step:1674/2110 train_time:90189ms step_avg:53.88ms
step:1675/2110 train_time:90278ms step_avg:53.90ms
step:1676/2110 train_time:90366ms step_avg:53.92ms
step:1677/2110 train_time:90454ms step_avg:53.94ms
step:1678/2110 train_time:90544ms step_avg:53.96ms
step:1679/2110 train_time:90631ms step_avg:53.98ms
step:1680/2110 train_time:90720ms step_avg:54.00ms
step:1681/2110 train_time:90808ms step_avg:54.02ms
step:1682/2110 train_time:90896ms step_avg:54.04ms
step:1683/2110 train_time:90984ms step_avg:54.06ms
step:1684/2110 train_time:91071ms step_avg:54.08ms
step:1685/2110 train_time:91160ms step_avg:54.10ms
step:1686/2110 train_time:91248ms step_avg:54.12ms
step:1687/2110 train_time:91335ms step_avg:54.14ms
step:1688/2110 train_time:91424ms step_avg:54.16ms
step:1689/2110 train_time:91511ms step_avg:54.18ms
step:1690/2110 train_time:91600ms step_avg:54.20ms
step:1691/2110 train_time:91688ms step_avg:54.22ms
step:1692/2110 train_time:91776ms step_avg:54.24ms
step:1693/2110 train_time:91864ms step_avg:54.26ms
step:1694/2110 train_time:91952ms step_avg:54.28ms
step:1695/2110 train_time:92039ms step_avg:54.30ms
step:1696/2110 train_time:92128ms step_avg:54.32ms
step:1697/2110 train_time:92216ms step_avg:54.34ms
step:1698/2110 train_time:92304ms step_avg:54.36ms
step:1699/2110 train_time:92392ms step_avg:54.38ms
step:1700/2110 train_time:92480ms step_avg:54.40ms
step:1701/2110 train_time:92568ms step_avg:54.42ms
step:1702/2110 train_time:92656ms step_avg:54.44ms
step:1703/2110 train_time:92743ms step_avg:54.46ms
step:1704/2110 train_time:92831ms step_avg:54.48ms
step:1705/2110 train_time:92919ms step_avg:54.50ms
step:1706/2110 train_time:93009ms step_avg:54.52ms
step:1707/2110 train_time:93097ms step_avg:54.54ms
step:1708/2110 train_time:93186ms step_avg:54.56ms
step:1709/2110 train_time:93273ms step_avg:54.58ms
step:1710/2110 train_time:93361ms step_avg:54.60ms
step:1711/2110 train_time:93450ms step_avg:54.62ms
step:1712/2110 train_time:93537ms step_avg:54.64ms
step:1713/2110 train_time:93626ms step_avg:54.66ms
step:1714/2110 train_time:93715ms step_avg:54.68ms
step:1715/2110 train_time:93802ms step_avg:54.70ms
step:1716/2110 train_time:93890ms step_avg:54.71ms
step:1717/2110 train_time:93979ms step_avg:54.73ms
step:1718/2110 train_time:94068ms step_avg:54.75ms
step:1719/2110 train_time:94156ms step_avg:54.77ms
step:1720/2110 train_time:94245ms step_avg:54.79ms
step:1721/2110 train_time:94332ms step_avg:54.81ms
step:1722/2110 train_time:94421ms step_avg:54.83ms
step:1723/2110 train_time:94509ms step_avg:54.85ms
step:1724/2110 train_time:94597ms step_avg:54.87ms
step:1725/2110 train_time:94685ms step_avg:54.89ms
step:1726/2110 train_time:94772ms step_avg:54.91ms
step:1727/2110 train_time:94860ms step_avg:54.93ms
step:1728/2110 train_time:94949ms step_avg:54.95ms
step:1729/2110 train_time:95037ms step_avg:54.97ms
step:1730/2110 train_time:95126ms step_avg:54.99ms
step:1731/2110 train_time:95214ms step_avg:55.01ms
step:1732/2110 train_time:95302ms step_avg:55.02ms
step:1733/2110 train_time:95390ms step_avg:55.04ms
step:1734/2110 train_time:95479ms step_avg:55.06ms
step:1735/2110 train_time:95566ms step_avg:55.08ms
step:1736/2110 train_time:95653ms step_avg:55.10ms
step:1737/2110 train_time:95743ms step_avg:55.12ms
step:1738/2110 train_time:95830ms step_avg:55.14ms
step:1739/2110 train_time:95920ms step_avg:55.16ms
step:1740/2110 train_time:96009ms step_avg:55.18ms
step:1741/2110 train_time:96096ms step_avg:55.20ms
step:1742/2110 train_time:96186ms step_avg:55.22ms
step:1743/2110 train_time:96273ms step_avg:55.23ms
step:1744/2110 train_time:96362ms step_avg:55.25ms
step:1745/2110 train_time:96450ms step_avg:55.27ms
step:1746/2110 train_time:96538ms step_avg:55.29ms
step:1747/2110 train_time:96626ms step_avg:55.31ms
step:1748/2110 train_time:96714ms step_avg:55.33ms
step:1749/2110 train_time:96803ms step_avg:55.35ms
step:1750/2110 train_time:96890ms step_avg:55.37ms
step:1750/2110 val_loss:3.3764 train_time:96981ms step_avg:55.42ms
step:1751/2110 train_time:97016ms step_avg:55.41ms
step:1752/2110 train_time:97075ms step_avg:55.41ms
step:1753/2110 train_time:97172ms step_avg:55.43ms
step:1754/2110 train_time:97260ms step_avg:55.45ms
step:1755/2110 train_time:97348ms step_avg:55.47ms
step:1756/2110 train_time:97435ms step_avg:55.49ms
step:1757/2110 train_time:97522ms step_avg:55.50ms
step:1758/2110 train_time:97609ms step_avg:55.52ms
step:1759/2110 train_time:97695ms step_avg:55.54ms
step:1760/2110 train_time:97782ms step_avg:55.56ms
step:1761/2110 train_time:97869ms step_avg:55.58ms
step:1762/2110 train_time:97957ms step_avg:55.59ms
step:1763/2110 train_time:98048ms step_avg:55.61ms
step:1764/2110 train_time:98140ms step_avg:55.64ms
step:1765/2110 train_time:98230ms step_avg:55.65ms
step:1766/2110 train_time:98320ms step_avg:55.67ms
step:1767/2110 train_time:98407ms step_avg:55.69ms
step:1768/2110 train_time:98495ms step_avg:55.71ms
step:1769/2110 train_time:98581ms step_avg:55.73ms
step:1770/2110 train_time:98668ms step_avg:55.74ms
step:1771/2110 train_time:98755ms step_avg:55.76ms
step:1772/2110 train_time:98842ms step_avg:55.78ms
step:1773/2110 train_time:98930ms step_avg:55.80ms
step:1774/2110 train_time:99020ms step_avg:55.82ms
step:1775/2110 train_time:99111ms step_avg:55.84ms
step:1776/2110 train_time:99201ms step_avg:55.86ms
step:1777/2110 train_time:99289ms step_avg:55.87ms
step:1778/2110 train_time:99378ms step_avg:55.89ms
step:1779/2110 train_time:99465ms step_avg:55.91ms
step:1780/2110 train_time:99552ms step_avg:55.93ms
step:1781/2110 train_time:99639ms step_avg:55.95ms
step:1782/2110 train_time:99726ms step_avg:55.96ms
step:1783/2110 train_time:99814ms step_avg:55.98ms
step:1784/2110 train_time:99901ms step_avg:56.00ms
step:1785/2110 train_time:99989ms step_avg:56.02ms
step:1786/2110 train_time:100080ms step_avg:56.04ms
step:1787/2110 train_time:100169ms step_avg:56.05ms
step:1788/2110 train_time:100258ms step_avg:56.07ms
step:1789/2110 train_time:100346ms step_avg:56.09ms
step:1790/2110 train_time:100434ms step_avg:56.11ms
step:1791/2110 train_time:100521ms step_avg:56.13ms
step:1792/2110 train_time:100608ms step_avg:56.14ms
step:1793/2110 train_time:100695ms step_avg:56.16ms
step:1794/2110 train_time:100783ms step_avg:56.18ms
step:1795/2110 train_time:100869ms step_avg:56.19ms
step:1796/2110 train_time:100958ms step_avg:56.21ms
step:1797/2110 train_time:101046ms step_avg:56.23ms
step:1798/2110 train_time:101135ms step_avg:56.25ms
step:1799/2110 train_time:101223ms step_avg:56.27ms
step:1800/2110 train_time:101312ms step_avg:56.28ms
step:1801/2110 train_time:101401ms step_avg:56.30ms
step:1802/2110 train_time:101489ms step_avg:56.32ms
step:1803/2110 train_time:101576ms step_avg:56.34ms
step:1804/2110 train_time:101663ms step_avg:56.35ms
step:1805/2110 train_time:101751ms step_avg:56.37ms
step:1806/2110 train_time:101839ms step_avg:56.39ms
step:1807/2110 train_time:101926ms step_avg:56.41ms
step:1808/2110 train_time:102016ms step_avg:56.42ms
step:1809/2110 train_time:102103ms step_avg:56.44ms
step:1810/2110 train_time:102192ms step_avg:56.46ms
step:1811/2110 train_time:102282ms step_avg:56.48ms
step:1812/2110 train_time:102372ms step_avg:56.50ms
step:1813/2110 train_time:102460ms step_avg:56.51ms
step:1814/2110 train_time:102549ms step_avg:56.53ms
step:1815/2110 train_time:102637ms step_avg:56.55ms
step:1816/2110 train_time:102725ms step_avg:56.57ms
step:1817/2110 train_time:102812ms step_avg:56.58ms
step:1818/2110 train_time:102900ms step_avg:56.60ms
step:1819/2110 train_time:102988ms step_avg:56.62ms
step:1820/2110 train_time:103076ms step_avg:56.64ms
step:1821/2110 train_time:103164ms step_avg:56.65ms
step:1822/2110 train_time:103252ms step_avg:56.67ms
step:1823/2110 train_time:103340ms step_avg:56.69ms
step:1824/2110 train_time:103430ms step_avg:56.70ms
step:1825/2110 train_time:103517ms step_avg:56.72ms
step:1826/2110 train_time:103604ms step_avg:56.74ms
step:1827/2110 train_time:103694ms step_avg:56.76ms
step:1828/2110 train_time:103781ms step_avg:56.77ms
step:1829/2110 train_time:103870ms step_avg:56.79ms
step:1830/2110 train_time:103958ms step_avg:56.81ms
step:1831/2110 train_time:104045ms step_avg:56.82ms
step:1832/2110 train_time:104134ms step_avg:56.84ms
step:1833/2110 train_time:104222ms step_avg:56.86ms
step:1834/2110 train_time:104310ms step_avg:56.88ms
step:1835/2110 train_time:104400ms step_avg:56.89ms
step:1836/2110 train_time:104489ms step_avg:56.91ms
step:1837/2110 train_time:104577ms step_avg:56.93ms
step:1838/2110 train_time:104665ms step_avg:56.94ms
step:1839/2110 train_time:104752ms step_avg:56.96ms
step:1840/2110 train_time:104839ms step_avg:56.98ms
step:1841/2110 train_time:104927ms step_avg:56.99ms
step:1842/2110 train_time:105016ms step_avg:57.01ms
step:1843/2110 train_time:105103ms step_avg:57.03ms
step:1844/2110 train_time:105191ms step_avg:57.05ms
step:1845/2110 train_time:105280ms step_avg:57.06ms
step:1846/2110 train_time:105368ms step_avg:57.08ms
step:1847/2110 train_time:105457ms step_avg:57.10ms
step:1848/2110 train_time:105545ms step_avg:57.11ms
step:1849/2110 train_time:105632ms step_avg:57.13ms
step:1850/2110 train_time:105721ms step_avg:57.15ms
step:1851/2110 train_time:105808ms step_avg:57.16ms
step:1852/2110 train_time:105897ms step_avg:57.18ms
step:1853/2110 train_time:105984ms step_avg:57.20ms
step:1854/2110 train_time:106072ms step_avg:57.21ms
step:1855/2110 train_time:106160ms step_avg:57.23ms
step:1856/2110 train_time:106249ms step_avg:57.25ms
step:1857/2110 train_time:106336ms step_avg:57.26ms
step:1858/2110 train_time:106424ms step_avg:57.28ms
step:1859/2110 train_time:106513ms step_avg:57.30ms
step:1860/2110 train_time:106600ms step_avg:57.31ms
step:1861/2110 train_time:106688ms step_avg:57.33ms
step:1862/2110 train_time:106777ms step_avg:57.35ms
step:1863/2110 train_time:106864ms step_avg:57.36ms
step:1864/2110 train_time:106952ms step_avg:57.38ms
step:1865/2110 train_time:107040ms step_avg:57.39ms
step:1866/2110 train_time:107129ms step_avg:57.41ms
step:1867/2110 train_time:107216ms step_avg:57.43ms
step:1868/2110 train_time:107304ms step_avg:57.44ms
step:1869/2110 train_time:107392ms step_avg:57.46ms
step:1870/2110 train_time:107481ms step_avg:57.48ms
step:1871/2110 train_time:107569ms step_avg:57.49ms
step:1872/2110 train_time:107657ms step_avg:57.51ms
step:1873/2110 train_time:107745ms step_avg:57.53ms
step:1874/2110 train_time:107832ms step_avg:57.54ms
step:1875/2110 train_time:107921ms step_avg:57.56ms
step:1876/2110 train_time:108009ms step_avg:57.57ms
step:1877/2110 train_time:108096ms step_avg:57.59ms
step:1878/2110 train_time:108184ms step_avg:57.61ms
step:1879/2110 train_time:108272ms step_avg:57.62ms
step:1880/2110 train_time:108360ms step_avg:57.64ms
step:1881/2110 train_time:108447ms step_avg:57.65ms
step:1882/2110 train_time:108536ms step_avg:57.67ms
step:1883/2110 train_time:108623ms step_avg:57.69ms
step:1884/2110 train_time:108712ms step_avg:57.70ms
step:1885/2110 train_time:108800ms step_avg:57.72ms
step:1886/2110 train_time:108888ms step_avg:57.74ms
step:1887/2110 train_time:108977ms step_avg:57.75ms
step:1888/2110 train_time:109065ms step_avg:57.77ms
step:1889/2110 train_time:109154ms step_avg:57.78ms
step:1890/2110 train_time:109243ms step_avg:57.80ms
step:1891/2110 train_time:109331ms step_avg:57.82ms
step:1892/2110 train_time:109419ms step_avg:57.83ms
step:1893/2110 train_time:109506ms step_avg:57.85ms
step:1894/2110 train_time:109595ms step_avg:57.86ms
step:1895/2110 train_time:109683ms step_avg:57.88ms
step:1896/2110 train_time:109772ms step_avg:57.90ms
step:1897/2110 train_time:109860ms step_avg:57.91ms
step:1898/2110 train_time:109949ms step_avg:57.93ms
step:1899/2110 train_time:110037ms step_avg:57.94ms
step:1900/2110 train_time:110125ms step_avg:57.96ms
step:1901/2110 train_time:110215ms step_avg:57.98ms
step:1902/2110 train_time:110302ms step_avg:57.99ms
step:1903/2110 train_time:110391ms step_avg:58.01ms
step:1904/2110 train_time:110479ms step_avg:58.02ms
step:1905/2110 train_time:110566ms step_avg:58.04ms
step:1906/2110 train_time:110654ms step_avg:58.06ms
step:1907/2110 train_time:110742ms step_avg:58.07ms
step:1908/2110 train_time:110831ms step_avg:58.09ms
step:1909/2110 train_time:110918ms step_avg:58.10ms
step:1910/2110 train_time:111007ms step_avg:58.12ms
step:1911/2110 train_time:111097ms step_avg:58.14ms
step:1912/2110 train_time:111185ms step_avg:58.15ms
step:1913/2110 train_time:111273ms step_avg:58.17ms
step:1914/2110 train_time:111360ms step_avg:58.18ms
step:1915/2110 train_time:111448ms step_avg:58.20ms
step:1916/2110 train_time:111538ms step_avg:58.21ms
step:1917/2110 train_time:111625ms step_avg:58.23ms
step:1918/2110 train_time:111713ms step_avg:58.24ms
step:1919/2110 train_time:111801ms step_avg:58.26ms
step:1920/2110 train_time:111889ms step_avg:58.28ms
step:1921/2110 train_time:111978ms step_avg:58.29ms
step:1922/2110 train_time:112066ms step_avg:58.31ms
step:1923/2110 train_time:112154ms step_avg:58.32ms
step:1924/2110 train_time:112242ms step_avg:58.34ms
step:1925/2110 train_time:112330ms step_avg:58.35ms
step:1926/2110 train_time:112418ms step_avg:58.37ms
step:1927/2110 train_time:112506ms step_avg:58.38ms
step:1928/2110 train_time:112595ms step_avg:58.40ms
step:1929/2110 train_time:112681ms step_avg:58.41ms
step:1930/2110 train_time:112771ms step_avg:58.43ms
step:1931/2110 train_time:112858ms step_avg:58.45ms
step:1932/2110 train_time:112947ms step_avg:58.46ms
step:1933/2110 train_time:113035ms step_avg:58.48ms
step:1934/2110 train_time:113123ms step_avg:58.49ms
step:1935/2110 train_time:113212ms step_avg:58.51ms
step:1936/2110 train_time:113300ms step_avg:58.52ms
step:1937/2110 train_time:113388ms step_avg:58.54ms
step:1938/2110 train_time:113477ms step_avg:58.55ms
step:1939/2110 train_time:113564ms step_avg:58.57ms
step:1940/2110 train_time:113652ms step_avg:58.58ms
step:1941/2110 train_time:113741ms step_avg:58.60ms
step:1942/2110 train_time:113829ms step_avg:58.61ms
step:1943/2110 train_time:113918ms step_avg:58.63ms
step:1944/2110 train_time:114006ms step_avg:58.65ms
step:1945/2110 train_time:114094ms step_avg:58.66ms
step:1946/2110 train_time:114183ms step_avg:58.68ms
step:1947/2110 train_time:114272ms step_avg:58.69ms
step:1948/2110 train_time:114359ms step_avg:58.71ms
step:1949/2110 train_time:114448ms step_avg:58.72ms
step:1950/2110 train_time:114536ms step_avg:58.74ms
step:1951/2110 train_time:114623ms step_avg:58.75ms
step:1952/2110 train_time:114711ms step_avg:58.77ms
step:1953/2110 train_time:114798ms step_avg:58.78ms
step:1954/2110 train_time:114887ms step_avg:58.80ms
step:1955/2110 train_time:114975ms step_avg:58.81ms
step:1956/2110 train_time:115063ms step_avg:58.83ms
step:1957/2110 train_time:115151ms step_avg:58.84ms
step:1958/2110 train_time:115240ms step_avg:58.86ms
step:1959/2110 train_time:115327ms step_avg:58.87ms
step:1960/2110 train_time:115415ms step_avg:58.89ms
step:1961/2110 train_time:115503ms step_avg:58.90ms
step:1962/2110 train_time:115591ms step_avg:58.92ms
step:1963/2110 train_time:115680ms step_avg:58.93ms
step:1964/2110 train_time:115769ms step_avg:58.95ms
step:1965/2110 train_time:115857ms step_avg:58.96ms
step:1966/2110 train_time:115945ms step_avg:58.98ms
step:1967/2110 train_time:116033ms step_avg:58.99ms
step:1968/2110 train_time:116121ms step_avg:59.00ms
step:1969/2110 train_time:116209ms step_avg:59.02ms
step:1970/2110 train_time:116299ms step_avg:59.04ms
step:1971/2110 train_time:116387ms step_avg:59.05ms
step:1972/2110 train_time:116476ms step_avg:59.06ms
step:1973/2110 train_time:116563ms step_avg:59.08ms
step:1974/2110 train_time:116652ms step_avg:59.09ms
step:1975/2110 train_time:116740ms step_avg:59.11ms
step:1976/2110 train_time:116828ms step_avg:59.12ms
step:1977/2110 train_time:116916ms step_avg:59.14ms
step:1978/2110 train_time:117003ms step_avg:59.15ms
step:1979/2110 train_time:117091ms step_avg:59.17ms
step:1980/2110 train_time:117180ms step_avg:59.18ms
step:1981/2110 train_time:117267ms step_avg:59.20ms
step:1982/2110 train_time:117355ms step_avg:59.21ms
step:1983/2110 train_time:117443ms step_avg:59.22ms
step:1984/2110 train_time:117531ms step_avg:59.24ms
step:1985/2110 train_time:117619ms step_avg:59.25ms
step:1986/2110 train_time:117707ms step_avg:59.27ms
step:1987/2110 train_time:117795ms step_avg:59.28ms
step:1988/2110 train_time:117883ms step_avg:59.30ms
step:1989/2110 train_time:117970ms step_avg:59.31ms
step:1990/2110 train_time:118059ms step_avg:59.33ms
step:1991/2110 train_time:118146ms step_avg:59.34ms
step:1992/2110 train_time:118235ms step_avg:59.36ms
step:1993/2110 train_time:118323ms step_avg:59.37ms
step:1994/2110 train_time:118411ms step_avg:59.38ms
step:1995/2110 train_time:118500ms step_avg:59.40ms
step:1996/2110 train_time:118588ms step_avg:59.41ms
step:1997/2110 train_time:118676ms step_avg:59.43ms
step:1998/2110 train_time:118764ms step_avg:59.44ms
step:1999/2110 train_time:118852ms step_avg:59.46ms
step:2000/2110 train_time:118940ms step_avg:59.47ms
step:2000/2110 val_loss:3.3020 train_time:119029ms step_avg:59.51ms
step:2001/2110 train_time:119064ms step_avg:59.50ms
step:2002/2110 train_time:119124ms step_avg:59.50ms
step:2003/2110 train_time:119218ms step_avg:59.52ms
step:2004/2110 train_time:119306ms step_avg:59.53ms
step:2005/2110 train_time:119393ms step_avg:59.55ms
step:2006/2110 train_time:119480ms step_avg:59.56ms
step:2007/2110 train_time:119566ms step_avg:59.57ms
step:2008/2110 train_time:119652ms step_avg:59.59ms
step:2009/2110 train_time:119739ms step_avg:59.60ms
step:2010/2110 train_time:119828ms step_avg:59.62ms
step:2011/2110 train_time:119914ms step_avg:59.63ms
step:2012/2110 train_time:120003ms step_avg:59.64ms
step:2013/2110 train_time:120094ms step_avg:59.66ms
step:2014/2110 train_time:120185ms step_avg:59.67ms
step:2015/2110 train_time:120276ms step_avg:59.69ms
step:2016/2110 train_time:120364ms step_avg:59.70ms
step:2017/2110 train_time:120451ms step_avg:59.72ms
step:2018/2110 train_time:120538ms step_avg:59.73ms
step:2019/2110 train_time:120625ms step_avg:59.75ms
step:2020/2110 train_time:120713ms step_avg:59.76ms
step:2021/2110 train_time:120801ms step_avg:59.77ms
step:2022/2110 train_time:120888ms step_avg:59.79ms
step:2023/2110 train_time:120975ms step_avg:59.80ms
step:2024/2110 train_time:121065ms step_avg:59.81ms
step:2025/2110 train_time:121153ms step_avg:59.83ms
step:2026/2110 train_time:121244ms step_avg:59.84ms
step:2027/2110 train_time:121331ms step_avg:59.86ms
step:2028/2110 train_time:121420ms step_avg:59.87ms
step:2029/2110 train_time:121508ms step_avg:59.89ms
step:2030/2110 train_time:121597ms step_avg:59.90ms
step:2031/2110 train_time:121684ms step_avg:59.91ms
step:2032/2110 train_time:121772ms step_avg:59.93ms
step:2033/2110 train_time:121859ms step_avg:59.94ms
step:2034/2110 train_time:121947ms step_avg:59.95ms
step:2035/2110 train_time:122035ms step_avg:59.97ms
step:2036/2110 train_time:122124ms step_avg:59.98ms
step:2037/2110 train_time:122215ms step_avg:60.00ms
step:2038/2110 train_time:122304ms step_avg:60.01ms
step:2039/2110 train_time:122391ms step_avg:60.03ms
step:2040/2110 train_time:122479ms step_avg:60.04ms
step:2041/2110 train_time:122567ms step_avg:60.05ms
step:2042/2110 train_time:122654ms step_avg:60.07ms
step:2043/2110 train_time:122741ms step_avg:60.08ms
step:2044/2110 train_time:122829ms step_avg:60.09ms
step:2045/2110 train_time:122916ms step_avg:60.11ms
step:2046/2110 train_time:123006ms step_avg:60.12ms
step:2047/2110 train_time:123094ms step_avg:60.13ms
step:2048/2110 train_time:123183ms step_avg:60.15ms
step:2049/2110 train_time:123272ms step_avg:60.16ms
step:2050/2110 train_time:123359ms step_avg:60.18ms
step:2051/2110 train_time:123448ms step_avg:60.19ms
step:2052/2110 train_time:123537ms step_avg:60.20ms
step:2053/2110 train_time:123625ms step_avg:60.22ms
step:2054/2110 train_time:123712ms step_avg:60.23ms
step:2055/2110 train_time:123800ms step_avg:60.24ms
step:2056/2110 train_time:123888ms step_avg:60.26ms
step:2057/2110 train_time:123975ms step_avg:60.27ms
step:2058/2110 train_time:124063ms step_avg:60.28ms
step:2059/2110 train_time:124152ms step_avg:60.30ms
step:2060/2110 train_time:124241ms step_avg:60.31ms
step:2061/2110 train_time:124329ms step_avg:60.32ms
step:2062/2110 train_time:124417ms step_avg:60.34ms
step:2063/2110 train_time:124505ms step_avg:60.35ms
step:2064/2110 train_time:124593ms step_avg:60.36ms
step:2065/2110 train_time:124680ms step_avg:60.38ms
step:2066/2110 train_time:124768ms step_avg:60.39ms
step:2067/2110 train_time:124856ms step_avg:60.40ms
step:2068/2110 train_time:124945ms step_avg:60.42ms
step:2069/2110 train_time:125032ms step_avg:60.43ms
step:2070/2110 train_time:125121ms step_avg:60.44ms
step:2071/2110 train_time:125210ms step_avg:60.46ms
step:2072/2110 train_time:125300ms step_avg:60.47ms
step:2073/2110 train_time:125388ms step_avg:60.49ms
step:2074/2110 train_time:125477ms step_avg:60.50ms
step:2075/2110 train_time:125564ms step_avg:60.51ms
step:2076/2110 train_time:125651ms step_avg:60.53ms
step:2077/2110 train_time:125740ms step_avg:60.54ms
step:2078/2110 train_time:125829ms step_avg:60.55ms
step:2079/2110 train_time:125917ms step_avg:60.57ms
step:2080/2110 train_time:126006ms step_avg:60.58ms
step:2081/2110 train_time:126093ms step_avg:60.59ms
step:2082/2110 train_time:126182ms step_avg:60.61ms
step:2083/2110 train_time:126271ms step_avg:60.62ms
step:2084/2110 train_time:126359ms step_avg:60.63ms
step:2085/2110 train_time:126450ms step_avg:60.65ms
step:2086/2110 train_time:126538ms step_avg:60.66ms
step:2087/2110 train_time:126626ms step_avg:60.67ms
step:2088/2110 train_time:126713ms step_avg:60.69ms
step:2089/2110 train_time:126803ms step_avg:60.70ms
step:2090/2110 train_time:126891ms step_avg:60.71ms
step:2091/2110 train_time:126980ms step_avg:60.73ms
step:2092/2110 train_time:127069ms step_avg:60.74ms
step:2093/2110 train_time:127157ms step_avg:60.75ms
step:2094/2110 train_time:127247ms step_avg:60.77ms
step:2095/2110 train_time:127336ms step_avg:60.78ms
step:2096/2110 train_time:127428ms step_avg:60.80ms
step:2097/2110 train_time:127516ms step_avg:60.81ms
step:2098/2110 train_time:127604ms step_avg:60.82ms
step:2099/2110 train_time:127692ms step_avg:60.83ms
step:2100/2110 train_time:127780ms step_avg:60.85ms
step:2101/2110 train_time:127867ms step_avg:60.86ms
step:2102/2110 train_time:127956ms step_avg:60.87ms
step:2103/2110 train_time:128046ms step_avg:60.89ms
step:2104/2110 train_time:128134ms step_avg:60.90ms
step:2105/2110 train_time:128223ms step_avg:60.91ms
step:2106/2110 train_time:128311ms step_avg:60.93ms
step:2107/2110 train_time:128399ms step_avg:60.94ms
step:2108/2110 train_time:128489ms step_avg:60.95ms
step:2109/2110 train_time:128577ms step_avg:60.97ms
step:2110/2110 train_time:128666ms step_avg:60.98ms
step:2110/2110 val_loss:3.2773 train_time:128756ms step_avg:61.02ms
peak memory allocated: 29892 MiB reserved: 44616 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 15 22:19:09 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   39C    P0            128W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   42C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   43C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   42C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   42C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   37C    P0            122W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              88      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              88      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:125ms step_avg:125.01ms
step:2/2110 train_time:162ms step_avg:81.08ms
step:3/2110 train_time:193ms step_avg:64.43ms
step:4/2110 train_time:224ms step_avg:55.99ms
step:5/2110 train_time:251ms step_avg:50.11ms
step:6/2110 train_time:450ms step_avg:74.97ms
step:7/2110 train_time:692ms step_avg:98.82ms
step:8/2110 train_time:724ms step_avg:90.50ms
step:9/2110 train_time:757ms step_avg:84.09ms
step:10/2110 train_time:790ms step_avg:78.97ms
step:11/2110 train_time:822ms step_avg:74.75ms
step:12/2110 train_time:855ms step_avg:71.24ms
step:13/2110 train_time:888ms step_avg:68.32ms
step:14/2110 train_time:921ms step_avg:65.77ms
step:15/2110 train_time:954ms step_avg:63.61ms
step:16/2110 train_time:987ms step_avg:61.67ms
step:17/2110 train_time:1020ms step_avg:59.99ms
step:18/2110 train_time:1053ms step_avg:58.48ms
step:19/2110 train_time:1086ms step_avg:57.14ms
step:20/2110 train_time:1119ms step_avg:55.93ms
step:21/2110 train_time:1152ms step_avg:54.83ms
step:22/2110 train_time:1184ms step_avg:53.83ms
step:23/2110 train_time:1218ms step_avg:52.94ms
step:24/2110 train_time:1250ms step_avg:52.08ms
step:25/2110 train_time:1283ms step_avg:51.33ms
step:26/2110 train_time:1316ms step_avg:50.61ms
step:27/2110 train_time:1349ms step_avg:49.96ms
step:28/2110 train_time:1382ms step_avg:49.35ms
step:29/2110 train_time:1415ms step_avg:48.79ms
step:30/2110 train_time:1447ms step_avg:48.25ms
step:31/2110 train_time:1481ms step_avg:47.76ms
step:32/2110 train_time:1513ms step_avg:47.29ms
step:33/2110 train_time:1547ms step_avg:46.87ms
step:34/2110 train_time:1580ms step_avg:46.47ms
step:35/2110 train_time:1616ms step_avg:46.17ms
step:36/2110 train_time:1649ms step_avg:45.81ms
step:37/2110 train_time:1683ms step_avg:45.49ms
step:38/2110 train_time:1716ms step_avg:45.17ms
step:39/2110 train_time:1750ms step_avg:44.88ms
step:40/2110 train_time:1783ms step_avg:44.59ms
step:41/2110 train_time:1817ms step_avg:44.32ms
step:42/2110 train_time:1850ms step_avg:44.04ms
step:43/2110 train_time:1883ms step_avg:43.79ms
step:44/2110 train_time:1916ms step_avg:43.54ms
step:45/2110 train_time:1950ms step_avg:43.32ms
step:46/2110 train_time:1982ms step_avg:43.09ms
step:47/2110 train_time:2016ms step_avg:42.88ms
step:48/2110 train_time:2048ms step_avg:42.67ms
step:49/2110 train_time:2081ms step_avg:42.48ms
step:50/2110 train_time:2114ms step_avg:42.28ms
step:51/2110 train_time:2148ms step_avg:42.12ms
step:52/2110 train_time:2181ms step_avg:41.94ms
step:53/2110 train_time:2214ms step_avg:41.78ms
step:54/2110 train_time:2247ms step_avg:41.60ms
step:55/2110 train_time:2280ms step_avg:41.45ms
step:56/2110 train_time:2312ms step_avg:41.29ms
step:57/2110 train_time:2346ms step_avg:41.16ms
step:58/2110 train_time:2378ms step_avg:41.01ms
step:59/2110 train_time:2412ms step_avg:40.88ms
step:60/2110 train_time:2444ms step_avg:40.74ms
step:61/2110 train_time:2477ms step_avg:40.61ms
step:62/2110 train_time:2510ms step_avg:40.49ms
step:63/2110 train_time:2544ms step_avg:40.37ms
step:64/2110 train_time:2577ms step_avg:40.26ms
step:65/2110 train_time:2610ms step_avg:40.16ms
step:66/2110 train_time:2643ms step_avg:40.05ms
step:67/2110 train_time:2677ms step_avg:39.96ms
step:68/2110 train_time:2710ms step_avg:39.86ms
step:69/2110 train_time:2744ms step_avg:39.76ms
step:70/2110 train_time:2777ms step_avg:39.67ms
step:71/2110 train_time:2810ms step_avg:39.58ms
step:72/2110 train_time:2843ms step_avg:39.49ms
step:73/2110 train_time:2877ms step_avg:39.41ms
step:74/2110 train_time:2909ms step_avg:39.31ms
step:75/2110 train_time:2943ms step_avg:39.24ms
step:76/2110 train_time:2975ms step_avg:39.15ms
step:77/2110 train_time:3009ms step_avg:39.08ms
step:78/2110 train_time:3041ms step_avg:38.99ms
step:79/2110 train_time:3075ms step_avg:38.93ms
step:80/2110 train_time:3108ms step_avg:38.85ms
step:81/2110 train_time:3141ms step_avg:38.78ms
step:82/2110 train_time:3174ms step_avg:38.71ms
step:83/2110 train_time:3208ms step_avg:38.65ms
step:84/2110 train_time:3240ms step_avg:38.57ms
step:85/2110 train_time:3274ms step_avg:38.52ms
step:86/2110 train_time:3306ms step_avg:38.45ms
step:87/2110 train_time:3340ms step_avg:38.39ms
step:88/2110 train_time:3372ms step_avg:38.32ms
step:89/2110 train_time:3406ms step_avg:38.27ms
step:90/2110 train_time:3439ms step_avg:38.21ms
step:91/2110 train_time:3472ms step_avg:38.15ms
step:92/2110 train_time:3505ms step_avg:38.09ms
step:93/2110 train_time:3538ms step_avg:38.04ms
step:94/2110 train_time:3570ms step_avg:37.98ms
step:95/2110 train_time:3604ms step_avg:37.93ms
step:96/2110 train_time:3636ms step_avg:37.88ms
step:97/2110 train_time:3670ms step_avg:37.83ms
step:98/2110 train_time:3702ms step_avg:37.78ms
step:99/2110 train_time:3736ms step_avg:37.74ms
step:100/2110 train_time:3769ms step_avg:37.69ms
step:101/2110 train_time:3802ms step_avg:37.64ms
step:102/2110 train_time:3835ms step_avg:37.60ms
step:103/2110 train_time:3868ms step_avg:37.56ms
step:104/2110 train_time:3901ms step_avg:37.51ms
step:105/2110 train_time:3934ms step_avg:37.47ms
step:106/2110 train_time:3968ms step_avg:37.43ms
step:107/2110 train_time:4001ms step_avg:37.39ms
step:108/2110 train_time:4033ms step_avg:37.35ms
step:109/2110 train_time:4066ms step_avg:37.31ms
step:110/2110 train_time:4099ms step_avg:37.27ms
step:111/2110 train_time:4133ms step_avg:37.23ms
step:112/2110 train_time:4165ms step_avg:37.19ms
step:113/2110 train_time:4199ms step_avg:37.16ms
step:114/2110 train_time:4231ms step_avg:37.11ms
step:115/2110 train_time:4265ms step_avg:37.09ms
step:116/2110 train_time:4297ms step_avg:37.05ms
step:117/2110 train_time:4331ms step_avg:37.01ms
step:118/2110 train_time:4364ms step_avg:36.98ms
step:119/2110 train_time:4396ms step_avg:36.94ms
step:120/2110 train_time:4429ms step_avg:36.91ms
step:121/2110 train_time:4462ms step_avg:36.88ms
step:122/2110 train_time:4495ms step_avg:36.84ms
step:123/2110 train_time:4528ms step_avg:36.82ms
step:124/2110 train_time:4561ms step_avg:36.78ms
step:125/2110 train_time:4594ms step_avg:36.75ms
step:126/2110 train_time:4627ms step_avg:36.72ms
step:127/2110 train_time:4660ms step_avg:36.69ms
step:128/2110 train_time:4693ms step_avg:36.66ms
step:129/2110 train_time:4726ms step_avg:36.64ms
step:130/2110 train_time:4758ms step_avg:36.60ms
step:131/2110 train_time:4792ms step_avg:36.58ms
step:132/2110 train_time:4825ms step_avg:36.55ms
step:133/2110 train_time:4858ms step_avg:36.53ms
step:134/2110 train_time:4891ms step_avg:36.50ms
step:135/2110 train_time:4924ms step_avg:36.47ms
step:136/2110 train_time:4956ms step_avg:36.44ms
step:137/2110 train_time:4990ms step_avg:36.42ms
step:138/2110 train_time:5022ms step_avg:36.39ms
step:139/2110 train_time:5056ms step_avg:36.37ms
step:140/2110 train_time:5089ms step_avg:36.35ms
step:141/2110 train_time:5121ms step_avg:36.32ms
step:142/2110 train_time:5154ms step_avg:36.30ms
step:143/2110 train_time:5187ms step_avg:36.28ms
step:144/2110 train_time:5221ms step_avg:36.25ms
step:145/2110 train_time:5254ms step_avg:36.23ms
step:146/2110 train_time:5287ms step_avg:36.21ms
step:147/2110 train_time:5320ms step_avg:36.19ms
step:148/2110 train_time:5353ms step_avg:36.17ms
step:149/2110 train_time:5386ms step_avg:36.15ms
step:150/2110 train_time:5419ms step_avg:36.12ms
step:151/2110 train_time:5452ms step_avg:36.11ms
step:152/2110 train_time:5484ms step_avg:36.08ms
step:153/2110 train_time:5518ms step_avg:36.06ms
step:154/2110 train_time:5550ms step_avg:36.04ms
step:155/2110 train_time:5584ms step_avg:36.02ms
step:156/2110 train_time:5616ms step_avg:36.00ms
step:157/2110 train_time:5650ms step_avg:35.98ms
step:158/2110 train_time:5682ms step_avg:35.96ms
step:159/2110 train_time:5716ms step_avg:35.95ms
step:160/2110 train_time:5748ms step_avg:35.93ms
step:161/2110 train_time:5782ms step_avg:35.91ms
step:162/2110 train_time:5814ms step_avg:35.89ms
step:163/2110 train_time:5847ms step_avg:35.87ms
step:164/2110 train_time:5881ms step_avg:35.86ms
step:165/2110 train_time:5914ms step_avg:35.84ms
step:166/2110 train_time:5947ms step_avg:35.82ms
step:167/2110 train_time:5980ms step_avg:35.81ms
step:168/2110 train_time:6012ms step_avg:35.79ms
step:169/2110 train_time:6045ms step_avg:35.77ms
step:170/2110 train_time:6078ms step_avg:35.75ms
step:171/2110 train_time:6111ms step_avg:35.74ms
step:172/2110 train_time:6144ms step_avg:35.72ms
step:173/2110 train_time:6178ms step_avg:35.71ms
step:174/2110 train_time:6210ms step_avg:35.69ms
step:175/2110 train_time:6244ms step_avg:35.68ms
step:176/2110 train_time:6276ms step_avg:35.66ms
step:177/2110 train_time:6310ms step_avg:35.65ms
step:178/2110 train_time:6342ms step_avg:35.63ms
step:179/2110 train_time:6376ms step_avg:35.62ms
step:180/2110 train_time:6408ms step_avg:35.60ms
step:181/2110 train_time:6442ms step_avg:35.59ms
step:182/2110 train_time:6475ms step_avg:35.57ms
step:183/2110 train_time:6508ms step_avg:35.56ms
step:184/2110 train_time:6540ms step_avg:35.54ms
step:185/2110 train_time:6574ms step_avg:35.53ms
step:186/2110 train_time:6606ms step_avg:35.52ms
step:187/2110 train_time:6640ms step_avg:35.51ms
step:188/2110 train_time:6672ms step_avg:35.49ms
step:189/2110 train_time:6706ms step_avg:35.48ms
step:190/2110 train_time:6739ms step_avg:35.47ms
step:191/2110 train_time:6772ms step_avg:35.46ms
step:192/2110 train_time:6805ms step_avg:35.44ms
step:193/2110 train_time:6838ms step_avg:35.43ms
step:194/2110 train_time:6870ms step_avg:35.41ms
step:195/2110 train_time:6904ms step_avg:35.40ms
step:196/2110 train_time:6936ms step_avg:35.39ms
step:197/2110 train_time:6970ms step_avg:35.38ms
step:198/2110 train_time:7002ms step_avg:35.36ms
step:199/2110 train_time:7036ms step_avg:35.35ms
step:200/2110 train_time:7068ms step_avg:35.34ms
step:201/2110 train_time:7101ms step_avg:35.33ms
step:202/2110 train_time:7134ms step_avg:35.32ms
step:203/2110 train_time:7168ms step_avg:35.31ms
step:204/2110 train_time:7200ms step_avg:35.29ms
step:205/2110 train_time:7233ms step_avg:35.28ms
step:206/2110 train_time:7266ms step_avg:35.27ms
step:207/2110 train_time:7299ms step_avg:35.26ms
step:208/2110 train_time:7331ms step_avg:35.25ms
step:209/2110 train_time:7365ms step_avg:35.24ms
step:210/2110 train_time:7398ms step_avg:35.23ms
step:211/2110 train_time:7431ms step_avg:35.22ms
step:212/2110 train_time:7464ms step_avg:35.21ms
step:213/2110 train_time:7497ms step_avg:35.20ms
step:214/2110 train_time:7530ms step_avg:35.19ms
step:215/2110 train_time:7563ms step_avg:35.18ms
step:216/2110 train_time:7595ms step_avg:35.16ms
step:217/2110 train_time:7629ms step_avg:35.16ms
step:218/2110 train_time:7661ms step_avg:35.14ms
step:219/2110 train_time:7695ms step_avg:35.13ms
step:220/2110 train_time:7727ms step_avg:35.12ms
step:221/2110 train_time:7761ms step_avg:35.12ms
step:222/2110 train_time:7794ms step_avg:35.11ms
step:223/2110 train_time:7827ms step_avg:35.10ms
step:224/2110 train_time:7860ms step_avg:35.09ms
step:225/2110 train_time:7893ms step_avg:35.08ms
step:226/2110 train_time:7926ms step_avg:35.07ms
step:227/2110 train_time:7959ms step_avg:35.06ms
step:228/2110 train_time:7991ms step_avg:35.05ms
step:229/2110 train_time:8024ms step_avg:35.04ms
step:230/2110 train_time:8057ms step_avg:35.03ms
step:231/2110 train_time:8090ms step_avg:35.02ms
step:232/2110 train_time:8124ms step_avg:35.02ms
step:233/2110 train_time:8156ms step_avg:35.01ms
step:234/2110 train_time:8189ms step_avg:34.99ms
step:235/2110 train_time:8222ms step_avg:34.99ms
step:236/2110 train_time:8255ms step_avg:34.98ms
step:237/2110 train_time:8288ms step_avg:34.97ms
step:238/2110 train_time:8321ms step_avg:34.96ms
step:239/2110 train_time:8354ms step_avg:34.95ms
step:240/2110 train_time:8386ms step_avg:34.94ms
step:241/2110 train_time:8419ms step_avg:34.94ms
step:242/2110 train_time:8452ms step_avg:34.92ms
step:243/2110 train_time:8485ms step_avg:34.92ms
step:244/2110 train_time:8518ms step_avg:34.91ms
step:245/2110 train_time:8551ms step_avg:34.90ms
step:246/2110 train_time:8584ms step_avg:34.89ms
step:247/2110 train_time:8617ms step_avg:34.89ms
step:248/2110 train_time:8650ms step_avg:34.88ms
step:249/2110 train_time:8683ms step_avg:34.87ms
step:250/2110 train_time:8716ms step_avg:34.86ms
step:250/2110 val_loss:4.2992 train_time:8751ms step_avg:35.00ms
step:251/2110 train_time:8781ms step_avg:34.98ms
step:252/2110 train_time:8809ms step_avg:34.96ms
step:253/2110 train_time:8838ms step_avg:34.93ms
step:254/2110 train_time:8869ms step_avg:34.92ms
step:255/2110 train_time:8899ms step_avg:34.90ms
step:256/2110 train_time:8928ms step_avg:34.88ms
step:257/2110 train_time:8957ms step_avg:34.85ms
step:258/2110 train_time:8990ms step_avg:34.84ms
step:259/2110 train_time:9024ms step_avg:34.84ms
step:260/2110 train_time:9057ms step_avg:34.83ms
step:261/2110 train_time:9092ms step_avg:34.84ms
step:262/2110 train_time:9125ms step_avg:34.83ms
step:263/2110 train_time:9158ms step_avg:34.82ms
step:264/2110 train_time:9191ms step_avg:34.81ms
step:265/2110 train_time:9224ms step_avg:34.81ms
step:266/2110 train_time:9256ms step_avg:34.80ms
step:267/2110 train_time:9290ms step_avg:34.79ms
step:268/2110 train_time:9322ms step_avg:34.79ms
step:269/2110 train_time:9355ms step_avg:34.78ms
step:270/2110 train_time:9387ms step_avg:34.77ms
step:271/2110 train_time:9421ms step_avg:34.76ms
step:272/2110 train_time:9453ms step_avg:34.75ms
step:273/2110 train_time:9486ms step_avg:34.75ms
step:274/2110 train_time:9518ms step_avg:34.74ms
step:275/2110 train_time:9551ms step_avg:34.73ms
step:276/2110 train_time:9584ms step_avg:34.72ms
step:277/2110 train_time:9616ms step_avg:34.72ms
step:278/2110 train_time:9649ms step_avg:34.71ms
step:279/2110 train_time:9682ms step_avg:34.70ms
step:280/2110 train_time:9715ms step_avg:34.70ms
step:281/2110 train_time:9748ms step_avg:34.69ms
step:282/2110 train_time:9781ms step_avg:34.68ms
step:283/2110 train_time:9815ms step_avg:34.68ms
step:284/2110 train_time:9848ms step_avg:34.68ms
step:285/2110 train_time:9882ms step_avg:34.67ms
step:286/2110 train_time:9916ms step_avg:34.67ms
step:287/2110 train_time:9950ms step_avg:34.67ms
step:288/2110 train_time:9982ms step_avg:34.66ms
step:289/2110 train_time:10016ms step_avg:34.66ms
step:290/2110 train_time:10049ms step_avg:34.65ms
step:291/2110 train_time:10082ms step_avg:34.65ms
step:292/2110 train_time:10115ms step_avg:34.64ms
step:293/2110 train_time:10148ms step_avg:34.64ms
step:294/2110 train_time:10181ms step_avg:34.63ms
step:295/2110 train_time:10214ms step_avg:34.62ms
step:296/2110 train_time:10247ms step_avg:34.62ms
step:297/2110 train_time:10280ms step_avg:34.61ms
step:298/2110 train_time:10312ms step_avg:34.60ms
step:299/2110 train_time:10345ms step_avg:34.60ms
step:300/2110 train_time:10378ms step_avg:34.59ms
step:301/2110 train_time:10411ms step_avg:34.59ms
step:302/2110 train_time:10443ms step_avg:34.58ms
step:303/2110 train_time:10476ms step_avg:34.57ms
step:304/2110 train_time:10508ms step_avg:34.57ms
step:305/2110 train_time:10541ms step_avg:34.56ms
step:306/2110 train_time:10574ms step_avg:34.56ms
step:307/2110 train_time:10607ms step_avg:34.55ms
step:308/2110 train_time:10639ms step_avg:34.54ms
step:309/2110 train_time:10672ms step_avg:34.54ms
step:310/2110 train_time:10705ms step_avg:34.53ms
step:311/2110 train_time:10738ms step_avg:34.53ms
step:312/2110 train_time:10770ms step_avg:34.52ms
step:313/2110 train_time:10804ms step_avg:34.52ms
step:314/2110 train_time:10838ms step_avg:34.52ms
step:315/2110 train_time:10871ms step_avg:34.51ms
step:316/2110 train_time:10903ms step_avg:34.50ms
step:317/2110 train_time:10937ms step_avg:34.50ms
step:318/2110 train_time:10970ms step_avg:34.50ms
step:319/2110 train_time:11003ms step_avg:34.49ms
step:320/2110 train_time:11037ms step_avg:34.49ms
step:321/2110 train_time:11069ms step_avg:34.48ms
step:322/2110 train_time:11102ms step_avg:34.48ms
step:323/2110 train_time:11135ms step_avg:34.47ms
step:324/2110 train_time:11168ms step_avg:34.47ms
step:325/2110 train_time:11201ms step_avg:34.46ms
step:326/2110 train_time:11234ms step_avg:34.46ms
step:327/2110 train_time:11267ms step_avg:34.46ms
step:328/2110 train_time:11300ms step_avg:34.45ms
step:329/2110 train_time:11333ms step_avg:34.45ms
step:330/2110 train_time:11366ms step_avg:34.44ms
step:331/2110 train_time:11399ms step_avg:34.44ms
step:332/2110 train_time:11432ms step_avg:34.43ms
step:333/2110 train_time:11465ms step_avg:34.43ms
step:334/2110 train_time:11497ms step_avg:34.42ms
step:335/2110 train_time:11531ms step_avg:34.42ms
step:336/2110 train_time:11563ms step_avg:34.41ms
step:337/2110 train_time:11596ms step_avg:34.41ms
step:338/2110 train_time:11629ms step_avg:34.40ms
step:339/2110 train_time:11662ms step_avg:34.40ms
step:340/2110 train_time:11694ms step_avg:34.39ms
step:341/2110 train_time:11728ms step_avg:34.39ms
step:342/2110 train_time:11760ms step_avg:34.39ms
step:343/2110 train_time:11793ms step_avg:34.38ms
step:344/2110 train_time:11826ms step_avg:34.38ms
step:345/2110 train_time:11860ms step_avg:34.38ms
step:346/2110 train_time:11892ms step_avg:34.37ms
step:347/2110 train_time:11926ms step_avg:34.37ms
step:348/2110 train_time:11959ms step_avg:34.36ms
step:349/2110 train_time:11992ms step_avg:34.36ms
step:350/2110 train_time:12025ms step_avg:34.36ms
step:351/2110 train_time:12059ms step_avg:34.36ms
step:352/2110 train_time:12091ms step_avg:34.35ms
step:353/2110 train_time:12125ms step_avg:34.35ms
step:354/2110 train_time:12158ms step_avg:34.34ms
step:355/2110 train_time:12192ms step_avg:34.34ms
step:356/2110 train_time:12224ms step_avg:34.34ms
step:357/2110 train_time:12257ms step_avg:34.33ms
step:358/2110 train_time:12290ms step_avg:34.33ms
step:359/2110 train_time:12323ms step_avg:34.33ms
step:360/2110 train_time:12356ms step_avg:34.32ms
step:361/2110 train_time:12389ms step_avg:34.32ms
step:362/2110 train_time:12421ms step_avg:34.31ms
step:363/2110 train_time:12455ms step_avg:34.31ms
step:364/2110 train_time:12487ms step_avg:34.30ms
step:365/2110 train_time:12520ms step_avg:34.30ms
step:366/2110 train_time:12552ms step_avg:34.30ms
step:367/2110 train_time:12586ms step_avg:34.29ms
step:368/2110 train_time:12618ms step_avg:34.29ms
step:369/2110 train_time:12651ms step_avg:34.29ms
step:370/2110 train_time:12684ms step_avg:34.28ms
step:371/2110 train_time:12717ms step_avg:34.28ms
step:372/2110 train_time:12751ms step_avg:34.28ms
step:373/2110 train_time:12783ms step_avg:34.27ms
step:374/2110 train_time:12815ms step_avg:34.27ms
step:375/2110 train_time:12849ms step_avg:34.26ms
step:376/2110 train_time:12882ms step_avg:34.26ms
step:377/2110 train_time:12915ms step_avg:34.26ms
step:378/2110 train_time:12948ms step_avg:34.25ms
step:379/2110 train_time:12981ms step_avg:34.25ms
step:380/2110 train_time:13014ms step_avg:34.25ms
step:381/2110 train_time:13047ms step_avg:34.25ms
step:382/2110 train_time:13080ms step_avg:34.24ms
step:383/2110 train_time:13114ms step_avg:34.24ms
step:384/2110 train_time:13146ms step_avg:34.23ms
step:385/2110 train_time:13179ms step_avg:34.23ms
step:386/2110 train_time:13212ms step_avg:34.23ms
step:387/2110 train_time:13245ms step_avg:34.23ms
step:388/2110 train_time:13278ms step_avg:34.22ms
step:389/2110 train_time:13311ms step_avg:34.22ms
step:390/2110 train_time:13343ms step_avg:34.21ms
step:391/2110 train_time:13377ms step_avg:34.21ms
step:392/2110 train_time:13409ms step_avg:34.21ms
step:393/2110 train_time:13443ms step_avg:34.21ms
step:394/2110 train_time:13475ms step_avg:34.20ms
step:395/2110 train_time:13509ms step_avg:34.20ms
step:396/2110 train_time:13542ms step_avg:34.20ms
step:397/2110 train_time:13575ms step_avg:34.19ms
step:398/2110 train_time:13608ms step_avg:34.19ms
step:399/2110 train_time:13641ms step_avg:34.19ms
step:400/2110 train_time:13673ms step_avg:34.18ms
step:401/2110 train_time:13707ms step_avg:34.18ms
step:402/2110 train_time:13739ms step_avg:34.18ms
step:403/2110 train_time:13772ms step_avg:34.17ms
step:404/2110 train_time:13805ms step_avg:34.17ms
step:405/2110 train_time:13838ms step_avg:34.17ms
step:406/2110 train_time:13870ms step_avg:34.16ms
step:407/2110 train_time:13904ms step_avg:34.16ms
step:408/2110 train_time:13936ms step_avg:34.16ms
step:409/2110 train_time:13970ms step_avg:34.16ms
step:410/2110 train_time:14002ms step_avg:34.15ms
step:411/2110 train_time:14036ms step_avg:34.15ms
step:412/2110 train_time:14069ms step_avg:34.15ms
step:413/2110 train_time:14102ms step_avg:34.14ms
step:414/2110 train_time:14134ms step_avg:34.14ms
step:415/2110 train_time:14168ms step_avg:34.14ms
step:416/2110 train_time:14200ms step_avg:34.14ms
step:417/2110 train_time:14234ms step_avg:34.13ms
step:418/2110 train_time:14267ms step_avg:34.13ms
step:419/2110 train_time:14300ms step_avg:34.13ms
step:420/2110 train_time:14332ms step_avg:34.12ms
step:421/2110 train_time:14366ms step_avg:34.12ms
step:422/2110 train_time:14399ms step_avg:34.12ms
step:423/2110 train_time:14432ms step_avg:34.12ms
step:424/2110 train_time:14465ms step_avg:34.12ms
step:425/2110 train_time:14498ms step_avg:34.11ms
step:426/2110 train_time:14531ms step_avg:34.11ms
step:427/2110 train_time:14564ms step_avg:34.11ms
step:428/2110 train_time:14596ms step_avg:34.10ms
step:429/2110 train_time:14630ms step_avg:34.10ms
step:430/2110 train_time:14663ms step_avg:34.10ms
step:431/2110 train_time:14696ms step_avg:34.10ms
step:432/2110 train_time:14729ms step_avg:34.09ms
step:433/2110 train_time:14762ms step_avg:34.09ms
step:434/2110 train_time:14795ms step_avg:34.09ms
step:435/2110 train_time:14828ms step_avg:34.09ms
step:436/2110 train_time:14860ms step_avg:34.08ms
step:437/2110 train_time:14894ms step_avg:34.08ms
step:438/2110 train_time:14926ms step_avg:34.08ms
step:439/2110 train_time:14959ms step_avg:34.08ms
step:440/2110 train_time:14992ms step_avg:34.07ms
step:441/2110 train_time:15025ms step_avg:34.07ms
step:442/2110 train_time:15058ms step_avg:34.07ms
step:443/2110 train_time:15091ms step_avg:34.07ms
step:444/2110 train_time:15124ms step_avg:34.06ms
step:445/2110 train_time:15157ms step_avg:34.06ms
step:446/2110 train_time:15190ms step_avg:34.06ms
step:447/2110 train_time:15223ms step_avg:34.06ms
step:448/2110 train_time:15256ms step_avg:34.05ms
step:449/2110 train_time:15290ms step_avg:34.05ms
step:450/2110 train_time:15322ms step_avg:34.05ms
step:451/2110 train_time:15356ms step_avg:34.05ms
step:452/2110 train_time:15389ms step_avg:34.05ms
step:453/2110 train_time:15422ms step_avg:34.04ms
step:454/2110 train_time:15454ms step_avg:34.04ms
step:455/2110 train_time:15488ms step_avg:34.04ms
step:456/2110 train_time:15521ms step_avg:34.04ms
step:457/2110 train_time:15554ms step_avg:34.04ms
step:458/2110 train_time:15588ms step_avg:34.04ms
step:459/2110 train_time:15620ms step_avg:34.03ms
step:460/2110 train_time:15653ms step_avg:34.03ms
step:461/2110 train_time:15686ms step_avg:34.03ms
step:462/2110 train_time:15719ms step_avg:34.02ms
step:463/2110 train_time:15752ms step_avg:34.02ms
step:464/2110 train_time:15785ms step_avg:34.02ms
step:465/2110 train_time:15818ms step_avg:34.02ms
step:466/2110 train_time:15850ms step_avg:34.01ms
step:467/2110 train_time:15884ms step_avg:34.01ms
step:468/2110 train_time:15916ms step_avg:34.01ms
step:469/2110 train_time:15950ms step_avg:34.01ms
step:470/2110 train_time:15982ms step_avg:34.00ms
step:471/2110 train_time:16016ms step_avg:34.00ms
step:472/2110 train_time:16048ms step_avg:34.00ms
step:473/2110 train_time:16081ms step_avg:34.00ms
step:474/2110 train_time:16114ms step_avg:34.00ms
step:475/2110 train_time:16147ms step_avg:33.99ms
step:476/2110 train_time:16180ms step_avg:33.99ms
step:477/2110 train_time:16213ms step_avg:33.99ms
step:478/2110 train_time:16246ms step_avg:33.99ms
step:479/2110 train_time:16279ms step_avg:33.99ms
step:480/2110 train_time:16312ms step_avg:33.98ms
step:481/2110 train_time:16346ms step_avg:33.98ms
step:482/2110 train_time:16378ms step_avg:33.98ms
step:483/2110 train_time:16412ms step_avg:33.98ms
step:484/2110 train_time:16445ms step_avg:33.98ms
step:485/2110 train_time:16478ms step_avg:33.98ms
step:486/2110 train_time:16511ms step_avg:33.97ms
step:487/2110 train_time:16544ms step_avg:33.97ms
step:488/2110 train_time:16576ms step_avg:33.97ms
step:489/2110 train_time:16610ms step_avg:33.97ms
step:490/2110 train_time:16643ms step_avg:33.97ms
step:491/2110 train_time:16676ms step_avg:33.96ms
step:492/2110 train_time:16709ms step_avg:33.96ms
step:493/2110 train_time:16742ms step_avg:33.96ms
step:494/2110 train_time:16774ms step_avg:33.96ms
step:495/2110 train_time:16808ms step_avg:33.95ms
step:496/2110 train_time:16840ms step_avg:33.95ms
step:497/2110 train_time:16874ms step_avg:33.95ms
step:498/2110 train_time:16907ms step_avg:33.95ms
step:499/2110 train_time:16939ms step_avg:33.95ms
step:500/2110 train_time:16972ms step_avg:33.94ms
step:500/2110 val_loss:4.0343 train_time:17007ms step_avg:34.01ms
step:501/2110 train_time:17038ms step_avg:34.01ms
step:502/2110 train_time:17068ms step_avg:34.00ms
step:503/2110 train_time:17097ms step_avg:33.99ms
step:504/2110 train_time:17121ms step_avg:33.97ms
step:505/2110 train_time:17146ms step_avg:33.95ms
step:506/2110 train_time:17180ms step_avg:33.95ms
step:507/2110 train_time:17213ms step_avg:33.95ms
step:508/2110 train_time:17246ms step_avg:33.95ms
step:509/2110 train_time:17279ms step_avg:33.95ms
step:510/2110 train_time:17312ms step_avg:33.95ms
step:511/2110 train_time:17345ms step_avg:33.94ms
step:512/2110 train_time:17378ms step_avg:33.94ms
step:513/2110 train_time:17411ms step_avg:33.94ms
step:514/2110 train_time:17443ms step_avg:33.94ms
step:515/2110 train_time:17476ms step_avg:33.93ms
step:516/2110 train_time:17509ms step_avg:33.93ms
step:517/2110 train_time:17542ms step_avg:33.93ms
step:518/2110 train_time:17574ms step_avg:33.93ms
step:519/2110 train_time:17607ms step_avg:33.93ms
step:520/2110 train_time:17641ms step_avg:33.92ms
step:521/2110 train_time:17673ms step_avg:33.92ms
step:522/2110 train_time:17705ms step_avg:33.92ms
step:523/2110 train_time:17738ms step_avg:33.92ms
step:524/2110 train_time:17771ms step_avg:33.91ms
step:525/2110 train_time:17804ms step_avg:33.91ms
step:526/2110 train_time:17836ms step_avg:33.91ms
step:527/2110 train_time:17869ms step_avg:33.91ms
step:528/2110 train_time:17902ms step_avg:33.91ms
step:529/2110 train_time:17935ms step_avg:33.90ms
step:530/2110 train_time:17967ms step_avg:33.90ms
step:531/2110 train_time:18002ms step_avg:33.90ms
step:532/2110 train_time:18035ms step_avg:33.90ms
step:533/2110 train_time:18069ms step_avg:33.90ms
step:534/2110 train_time:18102ms step_avg:33.90ms
step:535/2110 train_time:18136ms step_avg:33.90ms
step:536/2110 train_time:18168ms step_avg:33.90ms
step:537/2110 train_time:18202ms step_avg:33.90ms
step:538/2110 train_time:18235ms step_avg:33.89ms
step:539/2110 train_time:18268ms step_avg:33.89ms
step:540/2110 train_time:18300ms step_avg:33.89ms
step:541/2110 train_time:18334ms step_avg:33.89ms
step:542/2110 train_time:18366ms step_avg:33.89ms
step:543/2110 train_time:18400ms step_avg:33.89ms
step:544/2110 train_time:18432ms step_avg:33.88ms
step:545/2110 train_time:18466ms step_avg:33.88ms
step:546/2110 train_time:18498ms step_avg:33.88ms
step:547/2110 train_time:18532ms step_avg:33.88ms
step:548/2110 train_time:18564ms step_avg:33.88ms
step:549/2110 train_time:18597ms step_avg:33.87ms
step:550/2110 train_time:18629ms step_avg:33.87ms
step:551/2110 train_time:18662ms step_avg:33.87ms
step:552/2110 train_time:18695ms step_avg:33.87ms
step:553/2110 train_time:18728ms step_avg:33.87ms
step:554/2110 train_time:18760ms step_avg:33.86ms
step:555/2110 train_time:18793ms step_avg:33.86ms
step:556/2110 train_time:18826ms step_avg:33.86ms
step:557/2110 train_time:18859ms step_avg:33.86ms
step:558/2110 train_time:18891ms step_avg:33.86ms
step:559/2110 train_time:18925ms step_avg:33.85ms
step:560/2110 train_time:18957ms step_avg:33.85ms
step:561/2110 train_time:18991ms step_avg:33.85ms
step:562/2110 train_time:19024ms step_avg:33.85ms
step:563/2110 train_time:19057ms step_avg:33.85ms
step:564/2110 train_time:19090ms step_avg:33.85ms
step:565/2110 train_time:19124ms step_avg:33.85ms
step:566/2110 train_time:19157ms step_avg:33.85ms
step:567/2110 train_time:19190ms step_avg:33.85ms
step:568/2110 train_time:19223ms step_avg:33.84ms
step:569/2110 train_time:19257ms step_avg:33.84ms
step:570/2110 train_time:19289ms step_avg:33.84ms
step:571/2110 train_time:19323ms step_avg:33.84ms
step:572/2110 train_time:19355ms step_avg:33.84ms
step:573/2110 train_time:19388ms step_avg:33.84ms
step:574/2110 train_time:19421ms step_avg:33.83ms
step:575/2110 train_time:19455ms step_avg:33.83ms
step:576/2110 train_time:19487ms step_avg:33.83ms
step:577/2110 train_time:19520ms step_avg:33.83ms
step:578/2110 train_time:19553ms step_avg:33.83ms
step:579/2110 train_time:19586ms step_avg:33.83ms
step:580/2110 train_time:19619ms step_avg:33.83ms
step:581/2110 train_time:19652ms step_avg:33.82ms
step:582/2110 train_time:19684ms step_avg:33.82ms
step:583/2110 train_time:19717ms step_avg:33.82ms
step:584/2110 train_time:19750ms step_avg:33.82ms
step:585/2110 train_time:19783ms step_avg:33.82ms
step:586/2110 train_time:19816ms step_avg:33.82ms
step:587/2110 train_time:19849ms step_avg:33.81ms
step:588/2110 train_time:19882ms step_avg:33.81ms
step:589/2110 train_time:19915ms step_avg:33.81ms
step:590/2110 train_time:19947ms step_avg:33.81ms
step:591/2110 train_time:19980ms step_avg:33.81ms
step:592/2110 train_time:20013ms step_avg:33.81ms
step:593/2110 train_time:20046ms step_avg:33.80ms
step:594/2110 train_time:20079ms step_avg:33.80ms
step:595/2110 train_time:20113ms step_avg:33.80ms
step:596/2110 train_time:20146ms step_avg:33.80ms
step:597/2110 train_time:20179ms step_avg:33.80ms
step:598/2110 train_time:20212ms step_avg:33.80ms
step:599/2110 train_time:20245ms step_avg:33.80ms
step:600/2110 train_time:20278ms step_avg:33.80ms
step:601/2110 train_time:20311ms step_avg:33.80ms
step:602/2110 train_time:20344ms step_avg:33.79ms
step:603/2110 train_time:20377ms step_avg:33.79ms
step:604/2110 train_time:20409ms step_avg:33.79ms
step:605/2110 train_time:20443ms step_avg:33.79ms
step:606/2110 train_time:20476ms step_avg:33.79ms
step:607/2110 train_time:20509ms step_avg:33.79ms
step:608/2110 train_time:20541ms step_avg:33.79ms
step:609/2110 train_time:20575ms step_avg:33.78ms
step:610/2110 train_time:20607ms step_avg:33.78ms
step:611/2110 train_time:20640ms step_avg:33.78ms
step:612/2110 train_time:20673ms step_avg:33.78ms
step:613/2110 train_time:20706ms step_avg:33.78ms
step:614/2110 train_time:20739ms step_avg:33.78ms
step:615/2110 train_time:20772ms step_avg:33.78ms
step:616/2110 train_time:20805ms step_avg:33.77ms
step:617/2110 train_time:20838ms step_avg:33.77ms
step:618/2110 train_time:20870ms step_avg:33.77ms
step:619/2110 train_time:20904ms step_avg:33.77ms
step:620/2110 train_time:20937ms step_avg:33.77ms
step:621/2110 train_time:20970ms step_avg:33.77ms
step:622/2110 train_time:21003ms step_avg:33.77ms
step:623/2110 train_time:21036ms step_avg:33.77ms
step:624/2110 train_time:21069ms step_avg:33.76ms
step:625/2110 train_time:21102ms step_avg:33.76ms
step:626/2110 train_time:21138ms step_avg:33.77ms
step:627/2110 train_time:21180ms step_avg:33.78ms
step:628/2110 train_time:21219ms step_avg:33.79ms
step:629/2110 train_time:21259ms step_avg:33.80ms
step:630/2110 train_time:21295ms step_avg:33.80ms
step:631/2110 train_time:21336ms step_avg:33.81ms
step:632/2110 train_time:21373ms step_avg:33.82ms
step:633/2110 train_time:21412ms step_avg:33.83ms
step:634/2110 train_time:21447ms step_avg:33.83ms
step:635/2110 train_time:21485ms step_avg:33.83ms
step:636/2110 train_time:21522ms step_avg:33.84ms
step:637/2110 train_time:21558ms step_avg:33.84ms
step:638/2110 train_time:21591ms step_avg:33.84ms
step:639/2110 train_time:21624ms step_avg:33.84ms
step:640/2110 train_time:21656ms step_avg:33.84ms
step:641/2110 train_time:21689ms step_avg:33.84ms
step:642/2110 train_time:21722ms step_avg:33.83ms
step:643/2110 train_time:21755ms step_avg:33.83ms
step:644/2110 train_time:21787ms step_avg:33.83ms
step:645/2110 train_time:21820ms step_avg:33.83ms
step:646/2110 train_time:21852ms step_avg:33.83ms
step:647/2110 train_time:21885ms step_avg:33.83ms
step:648/2110 train_time:21917ms step_avg:33.82ms
step:649/2110 train_time:21950ms step_avg:33.82ms
step:650/2110 train_time:21983ms step_avg:33.82ms
step:651/2110 train_time:22015ms step_avg:33.82ms
step:652/2110 train_time:22048ms step_avg:33.82ms
step:653/2110 train_time:22081ms step_avg:33.82ms
step:654/2110 train_time:22114ms step_avg:33.81ms
step:655/2110 train_time:22147ms step_avg:33.81ms
step:656/2110 train_time:22179ms step_avg:33.81ms
step:657/2110 train_time:22212ms step_avg:33.81ms
step:658/2110 train_time:22245ms step_avg:33.81ms
step:659/2110 train_time:22279ms step_avg:33.81ms
step:660/2110 train_time:22312ms step_avg:33.81ms
step:661/2110 train_time:22346ms step_avg:33.81ms
step:662/2110 train_time:22379ms step_avg:33.81ms
step:663/2110 train_time:22412ms step_avg:33.80ms
step:664/2110 train_time:22446ms step_avg:33.80ms
step:665/2110 train_time:22479ms step_avg:33.80ms
step:666/2110 train_time:22512ms step_avg:33.80ms
step:667/2110 train_time:22546ms step_avg:33.80ms
step:668/2110 train_time:22578ms step_avg:33.80ms
step:669/2110 train_time:22611ms step_avg:33.80ms
step:670/2110 train_time:22644ms step_avg:33.80ms
step:671/2110 train_time:22677ms step_avg:33.80ms
step:672/2110 train_time:22710ms step_avg:33.79ms
step:673/2110 train_time:22743ms step_avg:33.79ms
step:674/2110 train_time:22777ms step_avg:33.79ms
step:675/2110 train_time:22809ms step_avg:33.79ms
step:676/2110 train_time:22841ms step_avg:33.79ms
step:677/2110 train_time:22875ms step_avg:33.79ms
step:678/2110 train_time:22907ms step_avg:33.79ms
step:679/2110 train_time:22940ms step_avg:33.78ms
step:680/2110 train_time:22972ms step_avg:33.78ms
step:681/2110 train_time:23006ms step_avg:33.78ms
step:682/2110 train_time:23038ms step_avg:33.78ms
step:683/2110 train_time:23071ms step_avg:33.78ms
step:684/2110 train_time:23103ms step_avg:33.78ms
step:685/2110 train_time:23136ms step_avg:33.78ms
step:686/2110 train_time:23168ms step_avg:33.77ms
step:687/2110 train_time:23202ms step_avg:33.77ms
step:688/2110 train_time:23234ms step_avg:33.77ms
step:689/2110 train_time:23267ms step_avg:33.77ms
step:690/2110 train_time:23300ms step_avg:33.77ms
step:691/2110 train_time:23335ms step_avg:33.77ms
step:692/2110 train_time:23393ms step_avg:33.80ms
step:693/2110 train_time:23454ms step_avg:33.84ms
step:694/2110 train_time:23513ms step_avg:33.88ms
step:695/2110 train_time:23573ms step_avg:33.92ms
step:696/2110 train_time:23631ms step_avg:33.95ms
step:697/2110 train_time:23691ms step_avg:33.99ms
step:698/2110 train_time:23749ms step_avg:34.02ms
step:699/2110 train_time:23809ms step_avg:34.06ms
step:700/2110 train_time:23866ms step_avg:34.09ms
step:701/2110 train_time:23925ms step_avg:34.13ms
step:702/2110 train_time:23983ms step_avg:34.16ms
step:703/2110 train_time:24043ms step_avg:34.20ms
step:704/2110 train_time:24101ms step_avg:34.23ms
step:705/2110 train_time:24161ms step_avg:34.27ms
step:706/2110 train_time:24219ms step_avg:34.30ms
step:707/2110 train_time:24279ms step_avg:34.34ms
step:708/2110 train_time:24337ms step_avg:34.37ms
step:709/2110 train_time:24398ms step_avg:34.41ms
step:710/2110 train_time:24457ms step_avg:34.45ms
step:711/2110 train_time:24518ms step_avg:34.48ms
step:712/2110 train_time:24576ms step_avg:34.52ms
step:713/2110 train_time:24637ms step_avg:34.55ms
step:714/2110 train_time:24696ms step_avg:34.59ms
step:715/2110 train_time:24757ms step_avg:34.62ms
step:716/2110 train_time:24815ms step_avg:34.66ms
step:717/2110 train_time:24875ms step_avg:34.69ms
step:718/2110 train_time:24933ms step_avg:34.73ms
step:719/2110 train_time:24994ms step_avg:34.76ms
step:720/2110 train_time:25052ms step_avg:34.79ms
step:721/2110 train_time:25112ms step_avg:34.83ms
step:722/2110 train_time:25169ms step_avg:34.86ms
step:723/2110 train_time:25229ms step_avg:34.90ms
step:724/2110 train_time:25287ms step_avg:34.93ms
step:725/2110 train_time:25347ms step_avg:34.96ms
step:726/2110 train_time:25406ms step_avg:34.99ms
step:727/2110 train_time:25466ms step_avg:35.03ms
step:728/2110 train_time:25525ms step_avg:35.06ms
step:729/2110 train_time:25586ms step_avg:35.10ms
step:730/2110 train_time:25645ms step_avg:35.13ms
step:731/2110 train_time:25706ms step_avg:35.17ms
step:732/2110 train_time:25764ms step_avg:35.20ms
step:733/2110 train_time:25824ms step_avg:35.23ms
step:734/2110 train_time:25883ms step_avg:35.26ms
step:735/2110 train_time:25943ms step_avg:35.30ms
step:736/2110 train_time:26003ms step_avg:35.33ms
step:737/2110 train_time:26062ms step_avg:35.36ms
step:738/2110 train_time:26120ms step_avg:35.39ms
step:739/2110 train_time:26180ms step_avg:35.43ms
step:740/2110 train_time:26238ms step_avg:35.46ms
step:741/2110 train_time:26298ms step_avg:35.49ms
step:742/2110 train_time:26357ms step_avg:35.52ms
step:743/2110 train_time:26418ms step_avg:35.56ms
step:744/2110 train_time:26476ms step_avg:35.59ms
step:745/2110 train_time:26537ms step_avg:35.62ms
step:746/2110 train_time:26596ms step_avg:35.65ms
step:747/2110 train_time:26656ms step_avg:35.68ms
step:748/2110 train_time:26715ms step_avg:35.72ms
step:749/2110 train_time:26775ms step_avg:35.75ms
step:750/2110 train_time:26833ms step_avg:35.78ms
step:750/2110 val_loss:3.9083 train_time:26895ms step_avg:35.86ms
step:751/2110 train_time:26931ms step_avg:35.86ms
step:752/2110 train_time:26962ms step_avg:35.85ms
step:753/2110 train_time:27017ms step_avg:35.88ms
step:754/2110 train_time:27078ms step_avg:35.91ms
step:755/2110 train_time:27138ms step_avg:35.94ms
step:756/2110 train_time:27196ms step_avg:35.97ms
step:757/2110 train_time:27255ms step_avg:36.00ms
step:758/2110 train_time:27313ms step_avg:36.03ms
step:759/2110 train_time:27373ms step_avg:36.06ms
step:760/2110 train_time:27431ms step_avg:36.09ms
step:761/2110 train_time:27490ms step_avg:36.12ms
step:762/2110 train_time:27548ms step_avg:36.15ms
step:763/2110 train_time:27607ms step_avg:36.18ms
step:764/2110 train_time:27664ms step_avg:36.21ms
step:765/2110 train_time:27723ms step_avg:36.24ms
step:766/2110 train_time:27780ms step_avg:36.27ms
step:767/2110 train_time:27841ms step_avg:36.30ms
step:768/2110 train_time:27901ms step_avg:36.33ms
step:769/2110 train_time:27963ms step_avg:36.36ms
step:770/2110 train_time:28022ms step_avg:36.39ms
step:771/2110 train_time:28084ms step_avg:36.43ms
step:772/2110 train_time:28143ms step_avg:36.46ms
step:773/2110 train_time:28204ms step_avg:36.49ms
step:774/2110 train_time:28262ms step_avg:36.51ms
step:775/2110 train_time:28322ms step_avg:36.54ms
step:776/2110 train_time:28379ms step_avg:36.57ms
step:777/2110 train_time:28439ms step_avg:36.60ms
step:778/2110 train_time:28497ms step_avg:36.63ms
step:779/2110 train_time:28557ms step_avg:36.66ms
step:780/2110 train_time:28614ms step_avg:36.68ms
step:781/2110 train_time:28673ms step_avg:36.71ms
step:782/2110 train_time:28731ms step_avg:36.74ms
step:783/2110 train_time:28791ms step_avg:36.77ms
step:784/2110 train_time:28851ms step_avg:36.80ms
step:785/2110 train_time:28912ms step_avg:36.83ms
step:786/2110 train_time:28971ms step_avg:36.86ms
step:787/2110 train_time:29031ms step_avg:36.89ms
step:788/2110 train_time:29090ms step_avg:36.92ms
step:789/2110 train_time:29152ms step_avg:36.95ms
step:790/2110 train_time:29212ms step_avg:36.98ms
step:791/2110 train_time:29271ms step_avg:37.01ms
step:792/2110 train_time:29330ms step_avg:37.03ms
step:793/2110 train_time:29390ms step_avg:37.06ms
step:794/2110 train_time:29449ms step_avg:37.09ms
step:795/2110 train_time:29508ms step_avg:37.12ms
step:796/2110 train_time:29566ms step_avg:37.14ms
step:797/2110 train_time:29625ms step_avg:37.17ms
step:798/2110 train_time:29684ms step_avg:37.20ms
step:799/2110 train_time:29744ms step_avg:37.23ms
step:800/2110 train_time:29802ms step_avg:37.25ms
step:801/2110 train_time:29862ms step_avg:37.28ms
step:802/2110 train_time:29920ms step_avg:37.31ms
step:803/2110 train_time:29981ms step_avg:37.34ms
step:804/2110 train_time:30040ms step_avg:37.36ms
step:805/2110 train_time:30101ms step_avg:37.39ms
step:806/2110 train_time:30159ms step_avg:37.42ms
step:807/2110 train_time:30220ms step_avg:37.45ms
step:808/2110 train_time:30277ms step_avg:37.47ms
step:809/2110 train_time:30337ms step_avg:37.50ms
step:810/2110 train_time:30395ms step_avg:37.52ms
step:811/2110 train_time:30455ms step_avg:37.55ms
step:812/2110 train_time:30513ms step_avg:37.58ms
step:813/2110 train_time:30573ms step_avg:37.60ms
step:814/2110 train_time:30631ms step_avg:37.63ms
step:815/2110 train_time:30691ms step_avg:37.66ms
step:816/2110 train_time:30750ms step_avg:37.68ms
step:817/2110 train_time:30810ms step_avg:37.71ms
step:818/2110 train_time:30868ms step_avg:37.74ms
step:819/2110 train_time:30928ms step_avg:37.76ms
step:820/2110 train_time:30987ms step_avg:37.79ms
step:821/2110 train_time:31048ms step_avg:37.82ms
step:822/2110 train_time:31107ms step_avg:37.84ms
step:823/2110 train_time:31168ms step_avg:37.87ms
step:824/2110 train_time:31226ms step_avg:37.90ms
step:825/2110 train_time:31287ms step_avg:37.92ms
step:826/2110 train_time:31346ms step_avg:37.95ms
step:827/2110 train_time:31406ms step_avg:37.98ms
step:828/2110 train_time:31465ms step_avg:38.00ms
step:829/2110 train_time:31524ms step_avg:38.03ms
step:830/2110 train_time:31582ms step_avg:38.05ms
step:831/2110 train_time:31642ms step_avg:38.08ms
step:832/2110 train_time:31700ms step_avg:38.10ms
step:833/2110 train_time:31760ms step_avg:38.13ms
step:834/2110 train_time:31818ms step_avg:38.15ms
step:835/2110 train_time:31879ms step_avg:38.18ms
step:836/2110 train_time:31937ms step_avg:38.20ms
step:837/2110 train_time:31997ms step_avg:38.23ms
step:838/2110 train_time:32055ms step_avg:38.25ms
step:839/2110 train_time:32116ms step_avg:38.28ms
step:840/2110 train_time:32174ms step_avg:38.30ms
step:841/2110 train_time:32235ms step_avg:38.33ms
step:842/2110 train_time:32294ms step_avg:38.35ms
step:843/2110 train_time:32355ms step_avg:38.38ms
step:844/2110 train_time:32413ms step_avg:38.40ms
step:845/2110 train_time:32473ms step_avg:38.43ms
step:846/2110 train_time:32532ms step_avg:38.45ms
step:847/2110 train_time:32592ms step_avg:38.48ms
step:848/2110 train_time:32651ms step_avg:38.50ms
step:849/2110 train_time:32711ms step_avg:38.53ms
step:850/2110 train_time:32770ms step_avg:38.55ms
step:851/2110 train_time:32830ms step_avg:38.58ms
step:852/2110 train_time:32889ms step_avg:38.60ms
step:853/2110 train_time:32949ms step_avg:38.63ms
step:854/2110 train_time:33007ms step_avg:38.65ms
step:855/2110 train_time:33067ms step_avg:38.68ms
step:856/2110 train_time:33126ms step_avg:38.70ms
step:857/2110 train_time:33187ms step_avg:38.72ms
step:858/2110 train_time:33246ms step_avg:38.75ms
step:859/2110 train_time:33307ms step_avg:38.77ms
step:860/2110 train_time:33366ms step_avg:38.80ms
step:861/2110 train_time:33426ms step_avg:38.82ms
step:862/2110 train_time:33484ms step_avg:38.84ms
step:863/2110 train_time:33544ms step_avg:38.87ms
step:864/2110 train_time:33602ms step_avg:38.89ms
step:865/2110 train_time:33662ms step_avg:38.92ms
step:866/2110 train_time:33720ms step_avg:38.94ms
step:867/2110 train_time:33780ms step_avg:38.96ms
step:868/2110 train_time:33838ms step_avg:38.98ms
step:869/2110 train_time:33898ms step_avg:39.01ms
step:870/2110 train_time:33956ms step_avg:39.03ms
step:871/2110 train_time:34016ms step_avg:39.05ms
step:872/2110 train_time:34074ms step_avg:39.08ms
step:873/2110 train_time:34134ms step_avg:39.10ms
step:874/2110 train_time:34193ms step_avg:39.12ms
step:875/2110 train_time:34252ms step_avg:39.15ms
step:876/2110 train_time:34311ms step_avg:39.17ms
step:877/2110 train_time:34372ms step_avg:39.19ms
step:878/2110 train_time:34430ms step_avg:39.21ms
step:879/2110 train_time:34491ms step_avg:39.24ms
step:880/2110 train_time:34550ms step_avg:39.26ms
step:881/2110 train_time:34611ms step_avg:39.29ms
step:882/2110 train_time:34670ms step_avg:39.31ms
step:883/2110 train_time:34730ms step_avg:39.33ms
step:884/2110 train_time:34788ms step_avg:39.35ms
step:885/2110 train_time:34848ms step_avg:39.38ms
step:886/2110 train_time:34907ms step_avg:39.40ms
step:887/2110 train_time:34967ms step_avg:39.42ms
step:888/2110 train_time:35025ms step_avg:39.44ms
step:889/2110 train_time:35085ms step_avg:39.47ms
step:890/2110 train_time:35145ms step_avg:39.49ms
step:891/2110 train_time:35205ms step_avg:39.51ms
step:892/2110 train_time:35263ms step_avg:39.53ms
step:893/2110 train_time:35323ms step_avg:39.56ms
step:894/2110 train_time:35381ms step_avg:39.58ms
step:895/2110 train_time:35442ms step_avg:39.60ms
step:896/2110 train_time:35500ms step_avg:39.62ms
step:897/2110 train_time:35560ms step_avg:39.64ms
step:898/2110 train_time:35619ms step_avg:39.66ms
step:899/2110 train_time:35679ms step_avg:39.69ms
step:900/2110 train_time:35737ms step_avg:39.71ms
step:901/2110 train_time:35796ms step_avg:39.73ms
step:902/2110 train_time:35855ms step_avg:39.75ms
step:903/2110 train_time:35914ms step_avg:39.77ms
step:904/2110 train_time:35973ms step_avg:39.79ms
step:905/2110 train_time:36033ms step_avg:39.82ms
step:906/2110 train_time:36091ms step_avg:39.84ms
step:907/2110 train_time:36152ms step_avg:39.86ms
step:908/2110 train_time:36211ms step_avg:39.88ms
step:909/2110 train_time:36271ms step_avg:39.90ms
step:910/2110 train_time:36330ms step_avg:39.92ms
step:911/2110 train_time:36391ms step_avg:39.95ms
step:912/2110 train_time:36450ms step_avg:39.97ms
step:913/2110 train_time:36510ms step_avg:39.99ms
step:914/2110 train_time:36568ms step_avg:40.01ms
step:915/2110 train_time:36628ms step_avg:40.03ms
step:916/2110 train_time:36686ms step_avg:40.05ms
step:917/2110 train_time:36747ms step_avg:40.07ms
step:918/2110 train_time:36806ms step_avg:40.09ms
step:919/2110 train_time:36865ms step_avg:40.11ms
step:920/2110 train_time:36924ms step_avg:40.13ms
step:921/2110 train_time:36984ms step_avg:40.16ms
step:922/2110 train_time:37043ms step_avg:40.18ms
step:923/2110 train_time:37104ms step_avg:40.20ms
step:924/2110 train_time:37162ms step_avg:40.22ms
step:925/2110 train_time:37222ms step_avg:40.24ms
step:926/2110 train_time:37280ms step_avg:40.26ms
step:927/2110 train_time:37340ms step_avg:40.28ms
step:928/2110 train_time:37398ms step_avg:40.30ms
step:929/2110 train_time:37458ms step_avg:40.32ms
step:930/2110 train_time:37517ms step_avg:40.34ms
step:931/2110 train_time:37577ms step_avg:40.36ms
step:932/2110 train_time:37635ms step_avg:40.38ms
step:933/2110 train_time:37695ms step_avg:40.40ms
step:934/2110 train_time:37754ms step_avg:40.42ms
step:935/2110 train_time:37813ms step_avg:40.44ms
step:936/2110 train_time:37871ms step_avg:40.46ms
step:937/2110 train_time:37931ms step_avg:40.48ms
step:938/2110 train_time:37990ms step_avg:40.50ms
step:939/2110 train_time:38050ms step_avg:40.52ms
step:940/2110 train_time:38110ms step_avg:40.54ms
step:941/2110 train_time:38170ms step_avg:40.56ms
step:942/2110 train_time:38229ms step_avg:40.58ms
step:943/2110 train_time:38289ms step_avg:40.60ms
step:944/2110 train_time:38348ms step_avg:40.62ms
step:945/2110 train_time:38408ms step_avg:40.64ms
step:946/2110 train_time:38467ms step_avg:40.66ms
step:947/2110 train_time:38526ms step_avg:40.68ms
step:948/2110 train_time:38585ms step_avg:40.70ms
step:949/2110 train_time:38645ms step_avg:40.72ms
step:950/2110 train_time:38704ms step_avg:40.74ms
step:951/2110 train_time:38765ms step_avg:40.76ms
step:952/2110 train_time:38823ms step_avg:40.78ms
step:953/2110 train_time:38884ms step_avg:40.80ms
step:954/2110 train_time:38942ms step_avg:40.82ms
step:955/2110 train_time:39003ms step_avg:40.84ms
step:956/2110 train_time:39061ms step_avg:40.86ms
step:957/2110 train_time:39122ms step_avg:40.88ms
step:958/2110 train_time:39180ms step_avg:40.90ms
step:959/2110 train_time:39240ms step_avg:40.92ms
step:960/2110 train_time:39297ms step_avg:40.93ms
step:961/2110 train_time:39357ms step_avg:40.95ms
step:962/2110 train_time:39416ms step_avg:40.97ms
step:963/2110 train_time:39476ms step_avg:40.99ms
step:964/2110 train_time:39534ms step_avg:41.01ms
step:965/2110 train_time:39594ms step_avg:41.03ms
step:966/2110 train_time:39654ms step_avg:41.05ms
step:967/2110 train_time:39714ms step_avg:41.07ms
step:968/2110 train_time:39772ms step_avg:41.09ms
step:969/2110 train_time:39832ms step_avg:41.11ms
step:970/2110 train_time:39890ms step_avg:41.12ms
step:971/2110 train_time:39950ms step_avg:41.14ms
step:972/2110 train_time:40009ms step_avg:41.16ms
step:973/2110 train_time:40070ms step_avg:41.18ms
step:974/2110 train_time:40129ms step_avg:41.20ms
step:975/2110 train_time:40189ms step_avg:41.22ms
step:976/2110 train_time:40249ms step_avg:41.24ms
step:977/2110 train_time:40309ms step_avg:41.26ms
step:978/2110 train_time:40368ms step_avg:41.28ms
step:979/2110 train_time:40428ms step_avg:41.30ms
step:980/2110 train_time:40487ms step_avg:41.31ms
step:981/2110 train_time:40547ms step_avg:41.33ms
step:982/2110 train_time:40607ms step_avg:41.35ms
step:983/2110 train_time:40667ms step_avg:41.37ms
step:984/2110 train_time:40725ms step_avg:41.39ms
step:985/2110 train_time:40785ms step_avg:41.41ms
step:986/2110 train_time:40843ms step_avg:41.42ms
step:987/2110 train_time:40903ms step_avg:41.44ms
step:988/2110 train_time:40962ms step_avg:41.46ms
step:989/2110 train_time:41021ms step_avg:41.48ms
step:990/2110 train_time:41080ms step_avg:41.49ms
step:991/2110 train_time:41141ms step_avg:41.51ms
step:992/2110 train_time:41198ms step_avg:41.53ms
step:993/2110 train_time:41259ms step_avg:41.55ms
step:994/2110 train_time:41317ms step_avg:41.57ms
step:995/2110 train_time:41377ms step_avg:41.59ms
step:996/2110 train_time:41436ms step_avg:41.60ms
step:997/2110 train_time:41497ms step_avg:41.62ms
step:998/2110 train_time:41555ms step_avg:41.64ms
step:999/2110 train_time:41615ms step_avg:41.66ms
step:1000/2110 train_time:41673ms step_avg:41.67ms
step:1000/2110 val_loss:3.7579 train_time:41735ms step_avg:41.73ms
step:1001/2110 train_time:41769ms step_avg:41.73ms
step:1002/2110 train_time:41798ms step_avg:41.71ms
step:1003/2110 train_time:41859ms step_avg:41.73ms
step:1004/2110 train_time:41921ms step_avg:41.75ms
step:1005/2110 train_time:41981ms step_avg:41.77ms
step:1006/2110 train_time:42040ms step_avg:41.79ms
step:1007/2110 train_time:42099ms step_avg:41.81ms
step:1008/2110 train_time:42157ms step_avg:41.82ms
step:1009/2110 train_time:42216ms step_avg:41.84ms
step:1010/2110 train_time:42274ms step_avg:41.86ms
step:1011/2110 train_time:42333ms step_avg:41.87ms
step:1012/2110 train_time:42392ms step_avg:41.89ms
step:1013/2110 train_time:42451ms step_avg:41.91ms
step:1014/2110 train_time:42509ms step_avg:41.92ms
step:1015/2110 train_time:42568ms step_avg:41.94ms
step:1016/2110 train_time:42625ms step_avg:41.95ms
step:1017/2110 train_time:42686ms step_avg:41.97ms
step:1018/2110 train_time:42745ms step_avg:41.99ms
step:1019/2110 train_time:42807ms step_avg:42.01ms
step:1020/2110 train_time:42867ms step_avg:42.03ms
step:1021/2110 train_time:42929ms step_avg:42.05ms
step:1022/2110 train_time:42988ms step_avg:42.06ms
step:1023/2110 train_time:43049ms step_avg:42.08ms
step:1024/2110 train_time:43107ms step_avg:42.10ms
step:1025/2110 train_time:43167ms step_avg:42.11ms
step:1026/2110 train_time:43224ms step_avg:42.13ms
step:1027/2110 train_time:43284ms step_avg:42.15ms
step:1028/2110 train_time:43341ms step_avg:42.16ms
step:1029/2110 train_time:43401ms step_avg:42.18ms
step:1030/2110 train_time:43458ms step_avg:42.19ms
step:1031/2110 train_time:43518ms step_avg:42.21ms
step:1032/2110 train_time:43576ms step_avg:42.22ms
step:1033/2110 train_time:43635ms step_avg:42.24ms
step:1034/2110 train_time:43694ms step_avg:42.26ms
step:1035/2110 train_time:43755ms step_avg:42.28ms
step:1036/2110 train_time:43815ms step_avg:42.29ms
step:1037/2110 train_time:43876ms step_avg:42.31ms
step:1038/2110 train_time:43935ms step_avg:42.33ms
step:1039/2110 train_time:43996ms step_avg:42.34ms
step:1040/2110 train_time:44055ms step_avg:42.36ms
step:1041/2110 train_time:44115ms step_avg:42.38ms
step:1042/2110 train_time:44174ms step_avg:42.39ms
step:1043/2110 train_time:44233ms step_avg:42.41ms
step:1044/2110 train_time:44292ms step_avg:42.42ms
step:1045/2110 train_time:44352ms step_avg:42.44ms
step:1046/2110 train_time:44409ms step_avg:42.46ms
step:1047/2110 train_time:44469ms step_avg:42.47ms
step:1048/2110 train_time:44527ms step_avg:42.49ms
step:1049/2110 train_time:44587ms step_avg:42.50ms
step:1050/2110 train_time:44645ms step_avg:42.52ms
step:1051/2110 train_time:44705ms step_avg:42.54ms
step:1052/2110 train_time:44763ms step_avg:42.55ms
step:1053/2110 train_time:44824ms step_avg:42.57ms
step:1054/2110 train_time:44883ms step_avg:42.58ms
step:1055/2110 train_time:44944ms step_avg:42.60ms
step:1056/2110 train_time:45002ms step_avg:42.62ms
step:1057/2110 train_time:45063ms step_avg:42.63ms
step:1058/2110 train_time:45121ms step_avg:42.65ms
step:1059/2110 train_time:45182ms step_avg:42.66ms
step:1060/2110 train_time:45240ms step_avg:42.68ms
step:1061/2110 train_time:45299ms step_avg:42.70ms
step:1062/2110 train_time:45358ms step_avg:42.71ms
step:1063/2110 train_time:45417ms step_avg:42.73ms
step:1064/2110 train_time:45476ms step_avg:42.74ms
step:1065/2110 train_time:45536ms step_avg:42.76ms
step:1066/2110 train_time:45595ms step_avg:42.77ms
step:1067/2110 train_time:45654ms step_avg:42.79ms
step:1068/2110 train_time:45713ms step_avg:42.80ms
step:1069/2110 train_time:45773ms step_avg:42.82ms
step:1070/2110 train_time:45832ms step_avg:42.83ms
step:1071/2110 train_time:45893ms step_avg:42.85ms
step:1072/2110 train_time:45952ms step_avg:42.87ms
step:1073/2110 train_time:46012ms step_avg:42.88ms
step:1074/2110 train_time:46071ms step_avg:42.90ms
step:1075/2110 train_time:46131ms step_avg:42.91ms
step:1076/2110 train_time:46191ms step_avg:42.93ms
step:1077/2110 train_time:46252ms step_avg:42.94ms
step:1078/2110 train_time:46310ms step_avg:42.96ms
step:1079/2110 train_time:46370ms step_avg:42.97ms
step:1080/2110 train_time:46428ms step_avg:42.99ms
step:1081/2110 train_time:46488ms step_avg:43.00ms
step:1082/2110 train_time:46547ms step_avg:43.02ms
step:1083/2110 train_time:46607ms step_avg:43.03ms
step:1084/2110 train_time:46664ms step_avg:43.05ms
step:1085/2110 train_time:46725ms step_avg:43.06ms
step:1086/2110 train_time:46782ms step_avg:43.08ms
step:1087/2110 train_time:46843ms step_avg:43.09ms
step:1088/2110 train_time:46901ms step_avg:43.11ms
step:1089/2110 train_time:46961ms step_avg:43.12ms
step:1090/2110 train_time:47020ms step_avg:43.14ms
step:1091/2110 train_time:47079ms step_avg:43.15ms
step:1092/2110 train_time:47138ms step_avg:43.17ms
step:1093/2110 train_time:47198ms step_avg:43.18ms
step:1094/2110 train_time:47257ms step_avg:43.20ms
step:1095/2110 train_time:47318ms step_avg:43.21ms
step:1096/2110 train_time:47376ms step_avg:43.23ms
step:1097/2110 train_time:47436ms step_avg:43.24ms
step:1098/2110 train_time:47494ms step_avg:43.25ms
step:1099/2110 train_time:47554ms step_avg:43.27ms
step:1100/2110 train_time:47612ms step_avg:43.28ms
step:1101/2110 train_time:47673ms step_avg:43.30ms
step:1102/2110 train_time:47731ms step_avg:43.31ms
step:1103/2110 train_time:47791ms step_avg:43.33ms
step:1104/2110 train_time:47850ms step_avg:43.34ms
step:1105/2110 train_time:47911ms step_avg:43.36ms
step:1106/2110 train_time:47969ms step_avg:43.37ms
step:1107/2110 train_time:48030ms step_avg:43.39ms
step:1108/2110 train_time:48089ms step_avg:43.40ms
step:1109/2110 train_time:48149ms step_avg:43.42ms
step:1110/2110 train_time:48207ms step_avg:43.43ms
step:1111/2110 train_time:48268ms step_avg:43.45ms
step:1112/2110 train_time:48326ms step_avg:43.46ms
step:1113/2110 train_time:48386ms step_avg:43.47ms
step:1114/2110 train_time:48444ms step_avg:43.49ms
step:1115/2110 train_time:48504ms step_avg:43.50ms
step:1116/2110 train_time:48562ms step_avg:43.51ms
step:1117/2110 train_time:48622ms step_avg:43.53ms
step:1118/2110 train_time:48680ms step_avg:43.54ms
step:1119/2110 train_time:48740ms step_avg:43.56ms
step:1120/2110 train_time:48799ms step_avg:43.57ms
step:1121/2110 train_time:48858ms step_avg:43.58ms
step:1122/2110 train_time:48917ms step_avg:43.60ms
step:1123/2110 train_time:48977ms step_avg:43.61ms
step:1124/2110 train_time:49036ms step_avg:43.63ms
step:1125/2110 train_time:49096ms step_avg:43.64ms
step:1126/2110 train_time:49155ms step_avg:43.65ms
step:1127/2110 train_time:49215ms step_avg:43.67ms
step:1128/2110 train_time:49274ms step_avg:43.68ms
step:1129/2110 train_time:49334ms step_avg:43.70ms
step:1130/2110 train_time:49393ms step_avg:43.71ms
step:1131/2110 train_time:49453ms step_avg:43.73ms
step:1132/2110 train_time:49512ms step_avg:43.74ms
step:1133/2110 train_time:49571ms step_avg:43.75ms
step:1134/2110 train_time:49629ms step_avg:43.76ms
step:1135/2110 train_time:49690ms step_avg:43.78ms
step:1136/2110 train_time:49748ms step_avg:43.79ms
step:1137/2110 train_time:49809ms step_avg:43.81ms
step:1138/2110 train_time:49867ms step_avg:43.82ms
step:1139/2110 train_time:49928ms step_avg:43.83ms
step:1140/2110 train_time:49986ms step_avg:43.85ms
step:1141/2110 train_time:50047ms step_avg:43.86ms
step:1142/2110 train_time:50105ms step_avg:43.88ms
step:1143/2110 train_time:50166ms step_avg:43.89ms
step:1144/2110 train_time:50224ms step_avg:43.90ms
step:1145/2110 train_time:50285ms step_avg:43.92ms
step:1146/2110 train_time:50343ms step_avg:43.93ms
step:1147/2110 train_time:50404ms step_avg:43.94ms
step:1148/2110 train_time:50462ms step_avg:43.96ms
step:1149/2110 train_time:50523ms step_avg:43.97ms
step:1150/2110 train_time:50582ms step_avg:43.98ms
step:1151/2110 train_time:50643ms step_avg:44.00ms
step:1152/2110 train_time:50701ms step_avg:44.01ms
step:1153/2110 train_time:50762ms step_avg:44.03ms
step:1154/2110 train_time:50821ms step_avg:44.04ms
step:1155/2110 train_time:50881ms step_avg:44.05ms
step:1156/2110 train_time:50941ms step_avg:44.07ms
step:1157/2110 train_time:51002ms step_avg:44.08ms
step:1158/2110 train_time:51060ms step_avg:44.09ms
step:1159/2110 train_time:51121ms step_avg:44.11ms
step:1160/2110 train_time:51180ms step_avg:44.12ms
step:1161/2110 train_time:51240ms step_avg:44.13ms
step:1162/2110 train_time:51298ms step_avg:44.15ms
step:1163/2110 train_time:51359ms step_avg:44.16ms
step:1164/2110 train_time:51417ms step_avg:44.17ms
step:1165/2110 train_time:51478ms step_avg:44.19ms
step:1166/2110 train_time:51537ms step_avg:44.20ms
step:1167/2110 train_time:51597ms step_avg:44.21ms
step:1168/2110 train_time:51657ms step_avg:44.23ms
step:1169/2110 train_time:51717ms step_avg:44.24ms
step:1170/2110 train_time:51777ms step_avg:44.25ms
step:1171/2110 train_time:51837ms step_avg:44.27ms
step:1172/2110 train_time:51897ms step_avg:44.28ms
step:1173/2110 train_time:51957ms step_avg:44.29ms
step:1174/2110 train_time:52016ms step_avg:44.31ms
step:1175/2110 train_time:52077ms step_avg:44.32ms
step:1176/2110 train_time:52136ms step_avg:44.33ms
step:1177/2110 train_time:52196ms step_avg:44.35ms
step:1178/2110 train_time:52255ms step_avg:44.36ms
step:1179/2110 train_time:52316ms step_avg:44.37ms
step:1180/2110 train_time:52375ms step_avg:44.39ms
step:1181/2110 train_time:52436ms step_avg:44.40ms
step:1182/2110 train_time:52496ms step_avg:44.41ms
step:1183/2110 train_time:52556ms step_avg:44.43ms
step:1184/2110 train_time:52615ms step_avg:44.44ms
step:1185/2110 train_time:52675ms step_avg:44.45ms
step:1186/2110 train_time:52734ms step_avg:44.46ms
step:1187/2110 train_time:52795ms step_avg:44.48ms
step:1188/2110 train_time:52855ms step_avg:44.49ms
step:1189/2110 train_time:52916ms step_avg:44.50ms
step:1190/2110 train_time:52975ms step_avg:44.52ms
step:1191/2110 train_time:53035ms step_avg:44.53ms
step:1192/2110 train_time:53094ms step_avg:44.54ms
step:1193/2110 train_time:53155ms step_avg:44.56ms
step:1194/2110 train_time:53214ms step_avg:44.57ms
step:1195/2110 train_time:53274ms step_avg:44.58ms
step:1196/2110 train_time:53333ms step_avg:44.59ms
step:1197/2110 train_time:53393ms step_avg:44.61ms
step:1198/2110 train_time:53452ms step_avg:44.62ms
step:1199/2110 train_time:53513ms step_avg:44.63ms
step:1200/2110 train_time:53572ms step_avg:44.64ms
step:1201/2110 train_time:53632ms step_avg:44.66ms
step:1202/2110 train_time:53691ms step_avg:44.67ms
step:1203/2110 train_time:53752ms step_avg:44.68ms
step:1204/2110 train_time:53810ms step_avg:44.69ms
step:1205/2110 train_time:53871ms step_avg:44.71ms
step:1206/2110 train_time:53930ms step_avg:44.72ms
step:1207/2110 train_time:53992ms step_avg:44.73ms
step:1208/2110 train_time:54050ms step_avg:44.74ms
step:1209/2110 train_time:54111ms step_avg:44.76ms
step:1210/2110 train_time:54170ms step_avg:44.77ms
step:1211/2110 train_time:54231ms step_avg:44.78ms
step:1212/2110 train_time:54290ms step_avg:44.79ms
step:1213/2110 train_time:54352ms step_avg:44.81ms
step:1214/2110 train_time:54410ms step_avg:44.82ms
step:1215/2110 train_time:54471ms step_avg:44.83ms
step:1216/2110 train_time:54529ms step_avg:44.84ms
step:1217/2110 train_time:54590ms step_avg:44.86ms
step:1218/2110 train_time:54649ms step_avg:44.87ms
step:1219/2110 train_time:54710ms step_avg:44.88ms
step:1220/2110 train_time:54768ms step_avg:44.89ms
step:1221/2110 train_time:54829ms step_avg:44.91ms
step:1222/2110 train_time:54889ms step_avg:44.92ms
step:1223/2110 train_time:54949ms step_avg:44.93ms
step:1224/2110 train_time:55009ms step_avg:44.94ms
step:1225/2110 train_time:55069ms step_avg:44.95ms
step:1226/2110 train_time:55127ms step_avg:44.97ms
step:1227/2110 train_time:55189ms step_avg:44.98ms
step:1228/2110 train_time:55248ms step_avg:44.99ms
step:1229/2110 train_time:55309ms step_avg:45.00ms
step:1230/2110 train_time:55367ms step_avg:45.01ms
step:1231/2110 train_time:55428ms step_avg:45.03ms
step:1232/2110 train_time:55487ms step_avg:45.04ms
step:1233/2110 train_time:55547ms step_avg:45.05ms
step:1234/2110 train_time:55606ms step_avg:45.06ms
step:1235/2110 train_time:55667ms step_avg:45.07ms
step:1236/2110 train_time:55725ms step_avg:45.09ms
step:1237/2110 train_time:55785ms step_avg:45.10ms
step:1238/2110 train_time:55844ms step_avg:45.11ms
step:1239/2110 train_time:55905ms step_avg:45.12ms
step:1240/2110 train_time:55964ms step_avg:45.13ms
step:1241/2110 train_time:56024ms step_avg:45.14ms
step:1242/2110 train_time:56083ms step_avg:45.16ms
step:1243/2110 train_time:56144ms step_avg:45.17ms
step:1244/2110 train_time:56203ms step_avg:45.18ms
step:1245/2110 train_time:56264ms step_avg:45.19ms
step:1246/2110 train_time:56322ms step_avg:45.20ms
step:1247/2110 train_time:56383ms step_avg:45.21ms
step:1248/2110 train_time:56441ms step_avg:45.23ms
step:1249/2110 train_time:56501ms step_avg:45.24ms
step:1250/2110 train_time:56560ms step_avg:45.25ms
step:1250/2110 val_loss:3.5936 train_time:56622ms step_avg:45.30ms
step:1251/2110 train_time:56656ms step_avg:45.29ms
step:1252/2110 train_time:56688ms step_avg:45.28ms
step:1253/2110 train_time:56751ms step_avg:45.29ms
step:1254/2110 train_time:56812ms step_avg:45.30ms
step:1255/2110 train_time:56873ms step_avg:45.32ms
step:1256/2110 train_time:56932ms step_avg:45.33ms
step:1257/2110 train_time:56993ms step_avg:45.34ms
step:1258/2110 train_time:57051ms step_avg:45.35ms
step:1259/2110 train_time:57111ms step_avg:45.36ms
step:1260/2110 train_time:57169ms step_avg:45.37ms
step:1261/2110 train_time:57229ms step_avg:45.38ms
step:1262/2110 train_time:57287ms step_avg:45.39ms
step:1263/2110 train_time:57347ms step_avg:45.41ms
step:1264/2110 train_time:57406ms step_avg:45.42ms
step:1265/2110 train_time:57465ms step_avg:45.43ms
step:1266/2110 train_time:57525ms step_avg:45.44ms
step:1267/2110 train_time:57586ms step_avg:45.45ms
step:1268/2110 train_time:57647ms step_avg:45.46ms
step:1269/2110 train_time:57709ms step_avg:45.48ms
step:1270/2110 train_time:57769ms step_avg:45.49ms
step:1271/2110 train_time:57831ms step_avg:45.50ms
step:1272/2110 train_time:57890ms step_avg:45.51ms
step:1273/2110 train_time:57951ms step_avg:45.52ms
step:1274/2110 train_time:58009ms step_avg:45.53ms
step:1275/2110 train_time:58070ms step_avg:45.54ms
step:1276/2110 train_time:58128ms step_avg:45.55ms
step:1277/2110 train_time:58189ms step_avg:45.57ms
step:1278/2110 train_time:58247ms step_avg:45.58ms
step:1279/2110 train_time:58307ms step_avg:45.59ms
step:1280/2110 train_time:58365ms step_avg:45.60ms
step:1281/2110 train_time:58425ms step_avg:45.61ms
step:1282/2110 train_time:58484ms step_avg:45.62ms
step:1283/2110 train_time:58546ms step_avg:45.63ms
step:1284/2110 train_time:58606ms step_avg:45.64ms
step:1285/2110 train_time:58667ms step_avg:45.66ms
step:1286/2110 train_time:58727ms step_avg:45.67ms
step:1287/2110 train_time:58789ms step_avg:45.68ms
step:1288/2110 train_time:58848ms step_avg:45.69ms
step:1289/2110 train_time:58910ms step_avg:45.70ms
step:1290/2110 train_time:58969ms step_avg:45.71ms
step:1291/2110 train_time:59029ms step_avg:45.72ms
step:1292/2110 train_time:59088ms step_avg:45.73ms
step:1293/2110 train_time:59148ms step_avg:45.74ms
step:1294/2110 train_time:59206ms step_avg:45.75ms
step:1295/2110 train_time:59267ms step_avg:45.77ms
step:1296/2110 train_time:59325ms step_avg:45.78ms
step:1297/2110 train_time:59385ms step_avg:45.79ms
step:1298/2110 train_time:59443ms step_avg:45.80ms
step:1299/2110 train_time:59503ms step_avg:45.81ms
step:1300/2110 train_time:59561ms step_avg:45.82ms
step:1301/2110 train_time:59623ms step_avg:45.83ms
step:1302/2110 train_time:59682ms step_avg:45.84ms
step:1303/2110 train_time:59744ms step_avg:45.85ms
step:1304/2110 train_time:59803ms step_avg:45.86ms
step:1305/2110 train_time:59865ms step_avg:45.87ms
step:1306/2110 train_time:59924ms step_avg:45.88ms
step:1307/2110 train_time:59984ms step_avg:45.89ms
step:1308/2110 train_time:60043ms step_avg:45.90ms
step:1309/2110 train_time:60104ms step_avg:45.92ms
step:1310/2110 train_time:60163ms step_avg:45.93ms
step:1311/2110 train_time:60223ms step_avg:45.94ms
step:1312/2110 train_time:60281ms step_avg:45.95ms
step:1313/2110 train_time:60341ms step_avg:45.96ms
step:1314/2110 train_time:60399ms step_avg:45.97ms
step:1315/2110 train_time:60459ms step_avg:45.98ms
step:1316/2110 train_time:60519ms step_avg:45.99ms
step:1317/2110 train_time:60579ms step_avg:46.00ms
step:1318/2110 train_time:60638ms step_avg:46.01ms
step:1319/2110 train_time:60699ms step_avg:46.02ms
step:1320/2110 train_time:60759ms step_avg:46.03ms
step:1321/2110 train_time:60820ms step_avg:46.04ms
step:1322/2110 train_time:60880ms step_avg:46.05ms
step:1323/2110 train_time:60941ms step_avg:46.06ms
step:1324/2110 train_time:61000ms step_avg:46.07ms
step:1325/2110 train_time:61062ms step_avg:46.08ms
step:1326/2110 train_time:61121ms step_avg:46.09ms
step:1327/2110 train_time:61182ms step_avg:46.11ms
step:1328/2110 train_time:61241ms step_avg:46.11ms
step:1329/2110 train_time:61300ms step_avg:46.13ms
step:1330/2110 train_time:61359ms step_avg:46.13ms
step:1331/2110 train_time:61418ms step_avg:46.14ms
step:1332/2110 train_time:61478ms step_avg:46.15ms
step:1333/2110 train_time:61537ms step_avg:46.16ms
step:1334/2110 train_time:61596ms step_avg:46.17ms
step:1335/2110 train_time:61657ms step_avg:46.19ms
step:1336/2110 train_time:61717ms step_avg:46.20ms
step:1337/2110 train_time:61777ms step_avg:46.21ms
step:1338/2110 train_time:61838ms step_avg:46.22ms
step:1339/2110 train_time:61899ms step_avg:46.23ms
step:1340/2110 train_time:61958ms step_avg:46.24ms
step:1341/2110 train_time:62019ms step_avg:46.25ms
step:1342/2110 train_time:62079ms step_avg:46.26ms
step:1343/2110 train_time:62139ms step_avg:46.27ms
step:1344/2110 train_time:62198ms step_avg:46.28ms
step:1345/2110 train_time:62259ms step_avg:46.29ms
step:1346/2110 train_time:62317ms step_avg:46.30ms
step:1347/2110 train_time:62377ms step_avg:46.31ms
step:1348/2110 train_time:62436ms step_avg:46.32ms
step:1349/2110 train_time:62495ms step_avg:46.33ms
step:1350/2110 train_time:62554ms step_avg:46.34ms
step:1351/2110 train_time:62614ms step_avg:46.35ms
step:1352/2110 train_time:62672ms step_avg:46.36ms
step:1353/2110 train_time:62733ms step_avg:46.37ms
step:1354/2110 train_time:62792ms step_avg:46.38ms
step:1355/2110 train_time:62854ms step_avg:46.39ms
step:1356/2110 train_time:62913ms step_avg:46.40ms
step:1357/2110 train_time:62974ms step_avg:46.41ms
step:1358/2110 train_time:63034ms step_avg:46.42ms
step:1359/2110 train_time:63095ms step_avg:46.43ms
step:1360/2110 train_time:63155ms step_avg:46.44ms
step:1361/2110 train_time:63215ms step_avg:46.45ms
step:1362/2110 train_time:63274ms step_avg:46.46ms
step:1363/2110 train_time:63334ms step_avg:46.47ms
step:1364/2110 train_time:63394ms step_avg:46.48ms
step:1365/2110 train_time:63454ms step_avg:46.49ms
step:1366/2110 train_time:63512ms step_avg:46.49ms
step:1367/2110 train_time:63572ms step_avg:46.51ms
step:1368/2110 train_time:63630ms step_avg:46.51ms
step:1369/2110 train_time:63692ms step_avg:46.52ms
step:1370/2110 train_time:63751ms step_avg:46.53ms
step:1371/2110 train_time:63812ms step_avg:46.54ms
step:1372/2110 train_time:63871ms step_avg:46.55ms
step:1373/2110 train_time:63932ms step_avg:46.56ms
step:1374/2110 train_time:63991ms step_avg:46.57ms
step:1375/2110 train_time:64052ms step_avg:46.58ms
step:1376/2110 train_time:64112ms step_avg:46.59ms
step:1377/2110 train_time:64172ms step_avg:46.60ms
step:1378/2110 train_time:64232ms step_avg:46.61ms
step:1379/2110 train_time:64293ms step_avg:46.62ms
step:1380/2110 train_time:64351ms step_avg:46.63ms
step:1381/2110 train_time:64412ms step_avg:46.64ms
step:1382/2110 train_time:64497ms step_avg:46.67ms
step:1383/2110 train_time:64586ms step_avg:46.70ms
step:1384/2110 train_time:64673ms step_avg:46.73ms
step:1385/2110 train_time:64760ms step_avg:46.76ms
step:1386/2110 train_time:64846ms step_avg:46.79ms
step:1387/2110 train_time:64933ms step_avg:46.82ms
step:1388/2110 train_time:65019ms step_avg:46.84ms
step:1389/2110 train_time:65107ms step_avg:46.87ms
step:1390/2110 train_time:65192ms step_avg:46.90ms
step:1391/2110 train_time:65279ms step_avg:46.93ms
step:1392/2110 train_time:65365ms step_avg:46.96ms
step:1393/2110 train_time:65452ms step_avg:46.99ms
step:1394/2110 train_time:65538ms step_avg:47.01ms
step:1395/2110 train_time:65626ms step_avg:47.04ms
step:1396/2110 train_time:65713ms step_avg:47.07ms
step:1397/2110 train_time:65800ms step_avg:47.10ms
step:1398/2110 train_time:65886ms step_avg:47.13ms
step:1399/2110 train_time:65973ms step_avg:47.16ms
step:1400/2110 train_time:66059ms step_avg:47.18ms
step:1401/2110 train_time:66146ms step_avg:47.21ms
step:1402/2110 train_time:66232ms step_avg:47.24ms
step:1403/2110 train_time:66319ms step_avg:47.27ms
step:1404/2110 train_time:66405ms step_avg:47.30ms
step:1405/2110 train_time:66493ms step_avg:47.33ms
step:1406/2110 train_time:66578ms step_avg:47.35ms
step:1407/2110 train_time:66667ms step_avg:47.38ms
step:1408/2110 train_time:66752ms step_avg:47.41ms
step:1409/2110 train_time:66839ms step_avg:47.44ms
step:1410/2110 train_time:66925ms step_avg:47.46ms
step:1411/2110 train_time:67012ms step_avg:47.49ms
step:1412/2110 train_time:67098ms step_avg:47.52ms
step:1413/2110 train_time:67186ms step_avg:47.55ms
step:1414/2110 train_time:67272ms step_avg:47.58ms
step:1415/2110 train_time:67358ms step_avg:47.60ms
step:1416/2110 train_time:67444ms step_avg:47.63ms
step:1417/2110 train_time:67531ms step_avg:47.66ms
step:1418/2110 train_time:67617ms step_avg:47.68ms
step:1419/2110 train_time:67706ms step_avg:47.71ms
step:1420/2110 train_time:67792ms step_avg:47.74ms
step:1421/2110 train_time:67879ms step_avg:47.77ms
step:1422/2110 train_time:67965ms step_avg:47.80ms
step:1423/2110 train_time:68052ms step_avg:47.82ms
step:1424/2110 train_time:68138ms step_avg:47.85ms
step:1425/2110 train_time:68225ms step_avg:47.88ms
step:1426/2110 train_time:68311ms step_avg:47.90ms
step:1427/2110 train_time:68398ms step_avg:47.93ms
step:1428/2110 train_time:68484ms step_avg:47.96ms
step:1429/2110 train_time:68572ms step_avg:47.99ms
step:1430/2110 train_time:68657ms step_avg:48.01ms
step:1431/2110 train_time:68745ms step_avg:48.04ms
step:1432/2110 train_time:68832ms step_avg:48.07ms
step:1433/2110 train_time:68919ms step_avg:48.09ms
step:1434/2110 train_time:69005ms step_avg:48.12ms
step:1435/2110 train_time:69092ms step_avg:48.15ms
step:1436/2110 train_time:69178ms step_avg:48.17ms
step:1437/2110 train_time:69266ms step_avg:48.20ms
step:1438/2110 train_time:69352ms step_avg:48.23ms
step:1439/2110 train_time:69439ms step_avg:48.26ms
step:1440/2110 train_time:69525ms step_avg:48.28ms
step:1441/2110 train_time:69613ms step_avg:48.31ms
step:1442/2110 train_time:69698ms step_avg:48.33ms
step:1443/2110 train_time:69786ms step_avg:48.36ms
step:1444/2110 train_time:69872ms step_avg:48.39ms
step:1445/2110 train_time:69959ms step_avg:48.41ms
step:1446/2110 train_time:70044ms step_avg:48.44ms
step:1447/2110 train_time:70132ms step_avg:48.47ms
step:1448/2110 train_time:70218ms step_avg:48.49ms
step:1449/2110 train_time:70306ms step_avg:48.52ms
step:1450/2110 train_time:70392ms step_avg:48.55ms
step:1451/2110 train_time:70479ms step_avg:48.57ms
step:1452/2110 train_time:70566ms step_avg:48.60ms
step:1453/2110 train_time:70654ms step_avg:48.63ms
step:1454/2110 train_time:70739ms step_avg:48.65ms
step:1455/2110 train_time:70828ms step_avg:48.68ms
step:1456/2110 train_time:70913ms step_avg:48.70ms
step:1457/2110 train_time:71001ms step_avg:48.73ms
step:1458/2110 train_time:71088ms step_avg:48.76ms
step:1459/2110 train_time:71173ms step_avg:48.78ms
step:1460/2110 train_time:71259ms step_avg:48.81ms
step:1461/2110 train_time:71346ms step_avg:48.83ms
step:1462/2110 train_time:71432ms step_avg:48.86ms
step:1463/2110 train_time:71520ms step_avg:48.89ms
step:1464/2110 train_time:71607ms step_avg:48.91ms
step:1465/2110 train_time:71694ms step_avg:48.94ms
step:1466/2110 train_time:71781ms step_avg:48.96ms
step:1467/2110 train_time:71868ms step_avg:48.99ms
step:1468/2110 train_time:71953ms step_avg:49.01ms
step:1469/2110 train_time:72041ms step_avg:49.04ms
step:1470/2110 train_time:72127ms step_avg:49.07ms
step:1471/2110 train_time:72214ms step_avg:49.09ms
step:1472/2110 train_time:72300ms step_avg:49.12ms
step:1473/2110 train_time:72387ms step_avg:49.14ms
step:1474/2110 train_time:72472ms step_avg:49.17ms
step:1475/2110 train_time:72559ms step_avg:49.19ms
step:1476/2110 train_time:72645ms step_avg:49.22ms
step:1477/2110 train_time:72733ms step_avg:49.24ms
step:1478/2110 train_time:72819ms step_avg:49.27ms
step:1479/2110 train_time:72907ms step_avg:49.29ms
step:1480/2110 train_time:72993ms step_avg:49.32ms
step:1481/2110 train_time:73080ms step_avg:49.35ms
step:1482/2110 train_time:73167ms step_avg:49.37ms
step:1483/2110 train_time:73254ms step_avg:49.40ms
step:1484/2110 train_time:73341ms step_avg:49.42ms
step:1485/2110 train_time:73428ms step_avg:49.45ms
step:1486/2110 train_time:73513ms step_avg:49.47ms
step:1487/2110 train_time:73601ms step_avg:49.50ms
step:1488/2110 train_time:73687ms step_avg:49.52ms
step:1489/2110 train_time:73774ms step_avg:49.55ms
step:1490/2110 train_time:73860ms step_avg:49.57ms
step:1491/2110 train_time:73947ms step_avg:49.60ms
step:1492/2110 train_time:74033ms step_avg:49.62ms
step:1493/2110 train_time:74120ms step_avg:49.65ms
step:1494/2110 train_time:74206ms step_avg:49.67ms
step:1495/2110 train_time:74293ms step_avg:49.69ms
step:1496/2110 train_time:74379ms step_avg:49.72ms
step:1497/2110 train_time:74467ms step_avg:49.74ms
step:1498/2110 train_time:74552ms step_avg:49.77ms
step:1499/2110 train_time:74640ms step_avg:49.79ms
step:1500/2110 train_time:74728ms step_avg:49.82ms
step:1500/2110 val_loss:3.4952 train_time:74814ms step_avg:49.88ms
step:1501/2110 train_time:74842ms step_avg:49.86ms
step:1502/2110 train_time:74905ms step_avg:49.87ms
step:1503/2110 train_time:74996ms step_avg:49.90ms
step:1504/2110 train_time:75083ms step_avg:49.92ms
step:1505/2110 train_time:75171ms step_avg:49.95ms
step:1506/2110 train_time:75256ms step_avg:49.97ms
step:1507/2110 train_time:75342ms step_avg:49.99ms
step:1508/2110 train_time:75427ms step_avg:50.02ms
step:1509/2110 train_time:75513ms step_avg:50.04ms
step:1510/2110 train_time:75600ms step_avg:50.07ms
step:1511/2110 train_time:75687ms step_avg:50.09ms
step:1512/2110 train_time:75776ms step_avg:50.12ms
step:1513/2110 train_time:75866ms step_avg:50.14ms
step:1514/2110 train_time:75954ms step_avg:50.17ms
step:1515/2110 train_time:76042ms step_avg:50.19ms
step:1516/2110 train_time:76129ms step_avg:50.22ms
step:1517/2110 train_time:76216ms step_avg:50.24ms
step:1518/2110 train_time:76301ms step_avg:50.26ms
step:1519/2110 train_time:76387ms step_avg:50.29ms
step:1520/2110 train_time:76471ms step_avg:50.31ms
step:1521/2110 train_time:76560ms step_avg:50.33ms
step:1522/2110 train_time:76645ms step_avg:50.36ms
step:1523/2110 train_time:76733ms step_avg:50.38ms
step:1524/2110 train_time:76820ms step_avg:50.41ms
step:1525/2110 train_time:76908ms step_avg:50.43ms
step:1526/2110 train_time:76995ms step_avg:50.46ms
step:1527/2110 train_time:77082ms step_avg:50.48ms
step:1528/2110 train_time:77168ms step_avg:50.50ms
step:1529/2110 train_time:77256ms step_avg:50.53ms
step:1530/2110 train_time:77341ms step_avg:50.55ms
step:1531/2110 train_time:77427ms step_avg:50.57ms
step:1532/2110 train_time:77512ms step_avg:50.60ms
step:1533/2110 train_time:77600ms step_avg:50.62ms
step:1534/2110 train_time:77687ms step_avg:50.64ms
step:1535/2110 train_time:77773ms step_avg:50.67ms
step:1536/2110 train_time:77860ms step_avg:50.69ms
step:1537/2110 train_time:77948ms step_avg:50.71ms
step:1538/2110 train_time:78035ms step_avg:50.74ms
step:1539/2110 train_time:78122ms step_avg:50.76ms
step:1540/2110 train_time:78208ms step_avg:50.78ms
step:1541/2110 train_time:78296ms step_avg:50.81ms
step:1542/2110 train_time:78381ms step_avg:50.83ms
step:1543/2110 train_time:78468ms step_avg:50.85ms
step:1544/2110 train_time:78553ms step_avg:50.88ms
step:1545/2110 train_time:78642ms step_avg:50.90ms
step:1546/2110 train_time:78727ms step_avg:50.92ms
step:1547/2110 train_time:78816ms step_avg:50.95ms
step:1548/2110 train_time:78903ms step_avg:50.97ms
step:1549/2110 train_time:78990ms step_avg:50.99ms
step:1550/2110 train_time:79076ms step_avg:51.02ms
step:1551/2110 train_time:79165ms step_avg:51.04ms
step:1552/2110 train_time:79250ms step_avg:51.06ms
step:1553/2110 train_time:79337ms step_avg:51.09ms
step:1554/2110 train_time:79424ms step_avg:51.11ms
step:1555/2110 train_time:79510ms step_avg:51.13ms
step:1556/2110 train_time:79596ms step_avg:51.15ms
step:1557/2110 train_time:79683ms step_avg:51.18ms
step:1558/2110 train_time:79769ms step_avg:51.20ms
step:1559/2110 train_time:79856ms step_avg:51.22ms
step:1560/2110 train_time:79945ms step_avg:51.25ms
step:1561/2110 train_time:80031ms step_avg:51.27ms
step:1562/2110 train_time:80118ms step_avg:51.29ms
step:1563/2110 train_time:80205ms step_avg:51.31ms
step:1564/2110 train_time:80291ms step_avg:51.34ms
step:1565/2110 train_time:80378ms step_avg:51.36ms
step:1566/2110 train_time:80464ms step_avg:51.38ms
step:1567/2110 train_time:80551ms step_avg:51.40ms
step:1568/2110 train_time:80636ms step_avg:51.43ms
step:1569/2110 train_time:80724ms step_avg:51.45ms
step:1570/2110 train_time:80810ms step_avg:51.47ms
step:1571/2110 train_time:80899ms step_avg:51.50ms
step:1572/2110 train_time:80985ms step_avg:51.52ms
step:1573/2110 train_time:81072ms step_avg:51.54ms
step:1574/2110 train_time:81159ms step_avg:51.56ms
step:1575/2110 train_time:81247ms step_avg:51.59ms
step:1576/2110 train_time:81333ms step_avg:51.61ms
step:1577/2110 train_time:81420ms step_avg:51.63ms
step:1578/2110 train_time:81507ms step_avg:51.65ms
step:1579/2110 train_time:81594ms step_avg:51.67ms
step:1580/2110 train_time:81680ms step_avg:51.70ms
step:1581/2110 train_time:81767ms step_avg:51.72ms
step:1582/2110 train_time:81853ms step_avg:51.74ms
step:1583/2110 train_time:81941ms step_avg:51.76ms
step:1584/2110 train_time:82027ms step_avg:51.78ms
step:1585/2110 train_time:82115ms step_avg:51.81ms
step:1586/2110 train_time:82201ms step_avg:51.83ms
step:1587/2110 train_time:82288ms step_avg:51.85ms
step:1588/2110 train_time:82373ms step_avg:51.87ms
step:1589/2110 train_time:82461ms step_avg:51.90ms
step:1590/2110 train_time:82547ms step_avg:51.92ms
step:1591/2110 train_time:82634ms step_avg:51.94ms
step:1592/2110 train_time:82720ms step_avg:51.96ms
step:1593/2110 train_time:82807ms step_avg:51.98ms
step:1594/2110 train_time:82893ms step_avg:52.00ms
step:1595/2110 train_time:82981ms step_avg:52.03ms
step:1596/2110 train_time:83067ms step_avg:52.05ms
step:1597/2110 train_time:83155ms step_avg:52.07ms
step:1598/2110 train_time:83242ms step_avg:52.09ms
step:1599/2110 train_time:83328ms step_avg:52.11ms
step:1600/2110 train_time:83414ms step_avg:52.13ms
step:1601/2110 train_time:83502ms step_avg:52.16ms
step:1602/2110 train_time:83588ms step_avg:52.18ms
step:1603/2110 train_time:83675ms step_avg:52.20ms
step:1604/2110 train_time:83761ms step_avg:52.22ms
step:1605/2110 train_time:83848ms step_avg:52.24ms
step:1606/2110 train_time:83934ms step_avg:52.26ms
step:1607/2110 train_time:84022ms step_avg:52.29ms
step:1608/2110 train_time:84108ms step_avg:52.31ms
step:1609/2110 train_time:84197ms step_avg:52.33ms
step:1610/2110 train_time:84283ms step_avg:52.35ms
step:1611/2110 train_time:84370ms step_avg:52.37ms
step:1612/2110 train_time:84456ms step_avg:52.39ms
step:1613/2110 train_time:84543ms step_avg:52.41ms
step:1614/2110 train_time:84629ms step_avg:52.43ms
step:1615/2110 train_time:84716ms step_avg:52.46ms
step:1616/2110 train_time:84803ms step_avg:52.48ms
step:1617/2110 train_time:84889ms step_avg:52.50ms
step:1618/2110 train_time:84975ms step_avg:52.52ms
step:1619/2110 train_time:85063ms step_avg:52.54ms
step:1620/2110 train_time:85149ms step_avg:52.56ms
step:1621/2110 train_time:85238ms step_avg:52.58ms
step:1622/2110 train_time:85325ms step_avg:52.60ms
step:1623/2110 train_time:85411ms step_avg:52.63ms
step:1624/2110 train_time:85497ms step_avg:52.65ms
step:1625/2110 train_time:85585ms step_avg:52.67ms
step:1626/2110 train_time:85671ms step_avg:52.69ms
step:1627/2110 train_time:85758ms step_avg:52.71ms
step:1628/2110 train_time:85845ms step_avg:52.73ms
step:1629/2110 train_time:85931ms step_avg:52.75ms
step:1630/2110 train_time:86017ms step_avg:52.77ms
step:1631/2110 train_time:86105ms step_avg:52.79ms
step:1632/2110 train_time:86191ms step_avg:52.81ms
step:1633/2110 train_time:86279ms step_avg:52.83ms
step:1634/2110 train_time:86365ms step_avg:52.85ms
step:1635/2110 train_time:86453ms step_avg:52.88ms
step:1636/2110 train_time:86540ms step_avg:52.90ms
step:1637/2110 train_time:86627ms step_avg:52.92ms
step:1638/2110 train_time:86713ms step_avg:52.94ms
step:1639/2110 train_time:86801ms step_avg:52.96ms
step:1640/2110 train_time:86886ms step_avg:52.98ms
step:1641/2110 train_time:86974ms step_avg:53.00ms
step:1642/2110 train_time:87061ms step_avg:53.02ms
step:1643/2110 train_time:87148ms step_avg:53.04ms
step:1644/2110 train_time:87235ms step_avg:53.06ms
step:1645/2110 train_time:87322ms step_avg:53.08ms
step:1646/2110 train_time:87408ms step_avg:53.10ms
step:1647/2110 train_time:87496ms step_avg:53.12ms
step:1648/2110 train_time:87582ms step_avg:53.14ms
step:1649/2110 train_time:87670ms step_avg:53.17ms
step:1650/2110 train_time:87756ms step_avg:53.19ms
step:1651/2110 train_time:87843ms step_avg:53.21ms
step:1652/2110 train_time:87928ms step_avg:53.23ms
step:1653/2110 train_time:88016ms step_avg:53.25ms
step:1654/2110 train_time:88102ms step_avg:53.27ms
step:1655/2110 train_time:88190ms step_avg:53.29ms
step:1656/2110 train_time:88277ms step_avg:53.31ms
step:1657/2110 train_time:88364ms step_avg:53.33ms
step:1658/2110 train_time:88451ms step_avg:53.35ms
step:1659/2110 train_time:88541ms step_avg:53.37ms
step:1660/2110 train_time:88628ms step_avg:53.39ms
step:1661/2110 train_time:88717ms step_avg:53.41ms
step:1662/2110 train_time:88804ms step_avg:53.43ms
step:1663/2110 train_time:88893ms step_avg:53.45ms
step:1664/2110 train_time:88979ms step_avg:53.47ms
step:1665/2110 train_time:89068ms step_avg:53.49ms
step:1666/2110 train_time:89156ms step_avg:53.51ms
step:1667/2110 train_time:89245ms step_avg:53.54ms
step:1668/2110 train_time:89332ms step_avg:53.56ms
step:1669/2110 train_time:89420ms step_avg:53.58ms
step:1670/2110 train_time:89508ms step_avg:53.60ms
step:1671/2110 train_time:89596ms step_avg:53.62ms
step:1672/2110 train_time:89684ms step_avg:53.64ms
step:1673/2110 train_time:89772ms step_avg:53.66ms
step:1674/2110 train_time:89859ms step_avg:53.68ms
step:1675/2110 train_time:89947ms step_avg:53.70ms
step:1676/2110 train_time:90034ms step_avg:53.72ms
step:1677/2110 train_time:90123ms step_avg:53.74ms
step:1678/2110 train_time:90210ms step_avg:53.76ms
step:1679/2110 train_time:90300ms step_avg:53.78ms
step:1680/2110 train_time:90387ms step_avg:53.80ms
step:1681/2110 train_time:90475ms step_avg:53.82ms
step:1682/2110 train_time:90562ms step_avg:53.84ms
step:1683/2110 train_time:90651ms step_avg:53.86ms
step:1684/2110 train_time:90738ms step_avg:53.88ms
step:1685/2110 train_time:90827ms step_avg:53.90ms
step:1686/2110 train_time:90915ms step_avg:53.92ms
step:1687/2110 train_time:91004ms step_avg:53.94ms
step:1688/2110 train_time:91091ms step_avg:53.96ms
step:1689/2110 train_time:91181ms step_avg:53.99ms
step:1690/2110 train_time:91268ms step_avg:54.00ms
step:1691/2110 train_time:91356ms step_avg:54.02ms
step:1692/2110 train_time:91443ms step_avg:54.04ms
step:1693/2110 train_time:91533ms step_avg:54.07ms
step:1694/2110 train_time:91620ms step_avg:54.08ms
step:1695/2110 train_time:91709ms step_avg:54.11ms
step:1696/2110 train_time:91796ms step_avg:54.13ms
step:1697/2110 train_time:91885ms step_avg:54.15ms
step:1698/2110 train_time:91972ms step_avg:54.16ms
step:1699/2110 train_time:92061ms step_avg:54.19ms
step:1700/2110 train_time:92148ms step_avg:54.20ms
step:1701/2110 train_time:92237ms step_avg:54.23ms
step:1702/2110 train_time:92324ms step_avg:54.24ms
step:1703/2110 train_time:92413ms step_avg:54.26ms
step:1704/2110 train_time:92500ms step_avg:54.28ms
step:1705/2110 train_time:92589ms step_avg:54.30ms
step:1706/2110 train_time:92677ms step_avg:54.32ms
step:1707/2110 train_time:92766ms step_avg:54.34ms
step:1708/2110 train_time:92852ms step_avg:54.36ms
step:1709/2110 train_time:92941ms step_avg:54.38ms
step:1710/2110 train_time:93029ms step_avg:54.40ms
step:1711/2110 train_time:93117ms step_avg:54.42ms
step:1712/2110 train_time:93205ms step_avg:54.44ms
step:1713/2110 train_time:93293ms step_avg:54.46ms
step:1714/2110 train_time:93381ms step_avg:54.48ms
step:1715/2110 train_time:93470ms step_avg:54.50ms
step:1716/2110 train_time:93557ms step_avg:54.52ms
step:1717/2110 train_time:93645ms step_avg:54.54ms
step:1718/2110 train_time:93733ms step_avg:54.56ms
step:1719/2110 train_time:93821ms step_avg:54.58ms
step:1720/2110 train_time:93909ms step_avg:54.60ms
step:1721/2110 train_time:93997ms step_avg:54.62ms
step:1722/2110 train_time:94084ms step_avg:54.64ms
step:1723/2110 train_time:94173ms step_avg:54.66ms
step:1724/2110 train_time:94260ms step_avg:54.67ms
step:1725/2110 train_time:94349ms step_avg:54.69ms
step:1726/2110 train_time:94436ms step_avg:54.71ms
step:1727/2110 train_time:94525ms step_avg:54.73ms
step:1728/2110 train_time:94612ms step_avg:54.75ms
step:1729/2110 train_time:94702ms step_avg:54.77ms
step:1730/2110 train_time:94788ms step_avg:54.79ms
step:1731/2110 train_time:94876ms step_avg:54.81ms
step:1732/2110 train_time:94964ms step_avg:54.83ms
step:1733/2110 train_time:95052ms step_avg:54.85ms
step:1734/2110 train_time:95140ms step_avg:54.87ms
step:1735/2110 train_time:95229ms step_avg:54.89ms
step:1736/2110 train_time:95315ms step_avg:54.91ms
step:1737/2110 train_time:95405ms step_avg:54.93ms
step:1738/2110 train_time:95492ms step_avg:54.94ms
step:1739/2110 train_time:95581ms step_avg:54.96ms
step:1740/2110 train_time:95669ms step_avg:54.98ms
step:1741/2110 train_time:95757ms step_avg:55.00ms
step:1742/2110 train_time:95845ms step_avg:55.02ms
step:1743/2110 train_time:95933ms step_avg:55.04ms
step:1744/2110 train_time:96021ms step_avg:55.06ms
step:1745/2110 train_time:96110ms step_avg:55.08ms
step:1746/2110 train_time:96198ms step_avg:55.10ms
step:1747/2110 train_time:96287ms step_avg:55.12ms
step:1748/2110 train_time:96374ms step_avg:55.13ms
step:1749/2110 train_time:96463ms step_avg:55.15ms
step:1750/2110 train_time:96549ms step_avg:55.17ms
step:1750/2110 val_loss:3.3793 train_time:96640ms step_avg:55.22ms
step:1751/2110 train_time:96666ms step_avg:55.21ms
step:1752/2110 train_time:96730ms step_avg:55.21ms
step:1753/2110 train_time:96827ms step_avg:55.24ms
step:1754/2110 train_time:96917ms step_avg:55.25ms
step:1755/2110 train_time:97005ms step_avg:55.27ms
step:1756/2110 train_time:97091ms step_avg:55.29ms
step:1757/2110 train_time:97179ms step_avg:55.31ms
step:1758/2110 train_time:97265ms step_avg:55.33ms
step:1759/2110 train_time:97354ms step_avg:55.35ms
step:1760/2110 train_time:97440ms step_avg:55.36ms
step:1761/2110 train_time:97529ms step_avg:55.38ms
step:1762/2110 train_time:97617ms step_avg:55.40ms
step:1763/2110 train_time:97708ms step_avg:55.42ms
step:1764/2110 train_time:97798ms step_avg:55.44ms
step:1765/2110 train_time:97888ms step_avg:55.46ms
step:1766/2110 train_time:97975ms step_avg:55.48ms
step:1767/2110 train_time:98063ms step_avg:55.50ms
step:1768/2110 train_time:98148ms step_avg:55.51ms
step:1769/2110 train_time:98238ms step_avg:55.53ms
step:1770/2110 train_time:98323ms step_avg:55.55ms
step:1771/2110 train_time:98411ms step_avg:55.57ms
step:1772/2110 train_time:98497ms step_avg:55.59ms
step:1773/2110 train_time:98587ms step_avg:55.60ms
step:1774/2110 train_time:98676ms step_avg:55.62ms
step:1775/2110 train_time:98766ms step_avg:55.64ms
step:1776/2110 train_time:98854ms step_avg:55.66ms
step:1777/2110 train_time:98944ms step_avg:55.68ms
step:1778/2110 train_time:99031ms step_avg:55.70ms
step:1779/2110 train_time:99119ms step_avg:55.72ms
step:1780/2110 train_time:99206ms step_avg:55.73ms
step:1781/2110 train_time:99293ms step_avg:55.75ms
step:1782/2110 train_time:99380ms step_avg:55.77ms
step:1783/2110 train_time:99468ms step_avg:55.79ms
step:1784/2110 train_time:99555ms step_avg:55.80ms
step:1785/2110 train_time:99644ms step_avg:55.82ms
step:1786/2110 train_time:99731ms step_avg:55.84ms
step:1787/2110 train_time:99820ms step_avg:55.86ms
step:1788/2110 train_time:99908ms step_avg:55.88ms
step:1789/2110 train_time:99997ms step_avg:55.90ms
step:1790/2110 train_time:100084ms step_avg:55.91ms
step:1791/2110 train_time:100172ms step_avg:55.93ms
step:1792/2110 train_time:100259ms step_avg:55.95ms
step:1793/2110 train_time:100347ms step_avg:55.97ms
step:1794/2110 train_time:100433ms step_avg:55.98ms
step:1795/2110 train_time:100522ms step_avg:56.00ms
step:1796/2110 train_time:100609ms step_avg:56.02ms
step:1797/2110 train_time:100698ms step_avg:56.04ms
step:1798/2110 train_time:100785ms step_avg:56.05ms
step:1799/2110 train_time:100874ms step_avg:56.07ms
step:1800/2110 train_time:100961ms step_avg:56.09ms
step:1801/2110 train_time:101049ms step_avg:56.11ms
step:1802/2110 train_time:101136ms step_avg:56.12ms
step:1803/2110 train_time:101224ms step_avg:56.14ms
step:1804/2110 train_time:101312ms step_avg:56.16ms
step:1805/2110 train_time:101400ms step_avg:56.18ms
step:1806/2110 train_time:101487ms step_avg:56.19ms
step:1807/2110 train_time:101576ms step_avg:56.21ms
step:1808/2110 train_time:101663ms step_avg:56.23ms
step:1809/2110 train_time:101752ms step_avg:56.25ms
step:1810/2110 train_time:101840ms step_avg:56.26ms
step:1811/2110 train_time:101928ms step_avg:56.28ms
step:1812/2110 train_time:102016ms step_avg:56.30ms
step:1813/2110 train_time:102104ms step_avg:56.32ms
step:1814/2110 train_time:102192ms step_avg:56.33ms
step:1815/2110 train_time:102281ms step_avg:56.35ms
step:1816/2110 train_time:102369ms step_avg:56.37ms
step:1817/2110 train_time:102458ms step_avg:56.39ms
step:1818/2110 train_time:102545ms step_avg:56.41ms
step:1819/2110 train_time:102634ms step_avg:56.42ms
step:1820/2110 train_time:102721ms step_avg:56.44ms
step:1821/2110 train_time:102809ms step_avg:56.46ms
step:1822/2110 train_time:102897ms step_avg:56.47ms
step:1823/2110 train_time:102986ms step_avg:56.49ms
step:1824/2110 train_time:103073ms step_avg:56.51ms
step:1825/2110 train_time:103162ms step_avg:56.53ms
step:1826/2110 train_time:103248ms step_avg:56.54ms
step:1827/2110 train_time:103338ms step_avg:56.56ms
step:1828/2110 train_time:103425ms step_avg:56.58ms
step:1829/2110 train_time:103513ms step_avg:56.60ms
step:1830/2110 train_time:103600ms step_avg:56.61ms
step:1831/2110 train_time:103689ms step_avg:56.63ms
step:1832/2110 train_time:103777ms step_avg:56.65ms
step:1833/2110 train_time:103866ms step_avg:56.66ms
step:1834/2110 train_time:103953ms step_avg:56.68ms
step:1835/2110 train_time:104042ms step_avg:56.70ms
step:1836/2110 train_time:104128ms step_avg:56.71ms
step:1837/2110 train_time:104218ms step_avg:56.73ms
step:1838/2110 train_time:104305ms step_avg:56.75ms
step:1839/2110 train_time:104394ms step_avg:56.77ms
step:1840/2110 train_time:104481ms step_avg:56.78ms
step:1841/2110 train_time:104569ms step_avg:56.80ms
step:1842/2110 train_time:104657ms step_avg:56.82ms
step:1843/2110 train_time:104746ms step_avg:56.83ms
step:1844/2110 train_time:104833ms step_avg:56.85ms
step:1845/2110 train_time:104922ms step_avg:56.87ms
step:1846/2110 train_time:105008ms step_avg:56.88ms
step:1847/2110 train_time:105097ms step_avg:56.90ms
step:1848/2110 train_time:105185ms step_avg:56.92ms
step:1849/2110 train_time:105272ms step_avg:56.93ms
step:1850/2110 train_time:105360ms step_avg:56.95ms
step:1851/2110 train_time:105448ms step_avg:56.97ms
step:1852/2110 train_time:105536ms step_avg:56.98ms
step:1853/2110 train_time:105625ms step_avg:57.00ms
step:1854/2110 train_time:105711ms step_avg:57.02ms
step:1855/2110 train_time:105800ms step_avg:57.03ms
step:1856/2110 train_time:105887ms step_avg:57.05ms
step:1857/2110 train_time:105976ms step_avg:57.07ms
step:1858/2110 train_time:106063ms step_avg:57.08ms
step:1859/2110 train_time:106151ms step_avg:57.10ms
step:1860/2110 train_time:106239ms step_avg:57.12ms
step:1861/2110 train_time:106327ms step_avg:57.13ms
step:1862/2110 train_time:106414ms step_avg:57.15ms
step:1863/2110 train_time:106503ms step_avg:57.17ms
step:1864/2110 train_time:106590ms step_avg:57.18ms
step:1865/2110 train_time:106679ms step_avg:57.20ms
step:1866/2110 train_time:106766ms step_avg:57.22ms
step:1867/2110 train_time:106855ms step_avg:57.23ms
step:1868/2110 train_time:106942ms step_avg:57.25ms
step:1869/2110 train_time:107030ms step_avg:57.27ms
step:1870/2110 train_time:107117ms step_avg:57.28ms
step:1871/2110 train_time:107207ms step_avg:57.30ms
step:1872/2110 train_time:107294ms step_avg:57.32ms
step:1873/2110 train_time:107383ms step_avg:57.33ms
step:1874/2110 train_time:107470ms step_avg:57.35ms
step:1875/2110 train_time:107559ms step_avg:57.36ms
step:1876/2110 train_time:107646ms step_avg:57.38ms
step:1877/2110 train_time:107735ms step_avg:57.40ms
step:1878/2110 train_time:107822ms step_avg:57.41ms
step:1879/2110 train_time:107910ms step_avg:57.43ms
step:1880/2110 train_time:107998ms step_avg:57.45ms
step:1881/2110 train_time:108086ms step_avg:57.46ms
step:1882/2110 train_time:108174ms step_avg:57.48ms
step:1883/2110 train_time:108262ms step_avg:57.49ms
step:1884/2110 train_time:108349ms step_avg:57.51ms
step:1885/2110 train_time:108438ms step_avg:57.53ms
step:1886/2110 train_time:108525ms step_avg:57.54ms
step:1887/2110 train_time:108613ms step_avg:57.56ms
step:1888/2110 train_time:108700ms step_avg:57.57ms
step:1889/2110 train_time:108788ms step_avg:57.59ms
step:1890/2110 train_time:108876ms step_avg:57.61ms
step:1891/2110 train_time:108964ms step_avg:57.62ms
step:1892/2110 train_time:109051ms step_avg:57.64ms
step:1893/2110 train_time:109141ms step_avg:57.65ms
step:1894/2110 train_time:109228ms step_avg:57.67ms
step:1895/2110 train_time:109316ms step_avg:57.69ms
step:1896/2110 train_time:109404ms step_avg:57.70ms
step:1897/2110 train_time:109492ms step_avg:57.72ms
step:1898/2110 train_time:109579ms step_avg:57.73ms
step:1899/2110 train_time:109667ms step_avg:57.75ms
step:1900/2110 train_time:109755ms step_avg:57.77ms
step:1901/2110 train_time:109844ms step_avg:57.78ms
step:1902/2110 train_time:109930ms step_avg:57.80ms
step:1903/2110 train_time:110019ms step_avg:57.81ms
step:1904/2110 train_time:110106ms step_avg:57.83ms
step:1905/2110 train_time:110194ms step_avg:57.84ms
step:1906/2110 train_time:110282ms step_avg:57.86ms
step:1907/2110 train_time:110370ms step_avg:57.88ms
step:1908/2110 train_time:110457ms step_avg:57.89ms
step:1909/2110 train_time:110546ms step_avg:57.91ms
step:1910/2110 train_time:110634ms step_avg:57.92ms
step:1911/2110 train_time:110722ms step_avg:57.94ms
step:1912/2110 train_time:110809ms step_avg:57.95ms
step:1913/2110 train_time:110898ms step_avg:57.97ms
step:1914/2110 train_time:110985ms step_avg:57.99ms
step:1915/2110 train_time:111075ms step_avg:58.00ms
step:1916/2110 train_time:111163ms step_avg:58.02ms
step:1917/2110 train_time:111251ms step_avg:58.03ms
step:1918/2110 train_time:111338ms step_avg:58.05ms
step:1919/2110 train_time:111428ms step_avg:58.07ms
step:1920/2110 train_time:111515ms step_avg:58.08ms
step:1921/2110 train_time:111603ms step_avg:58.10ms
step:1922/2110 train_time:111690ms step_avg:58.11ms
step:1923/2110 train_time:111779ms step_avg:58.13ms
step:1924/2110 train_time:111866ms step_avg:58.14ms
step:1925/2110 train_time:111955ms step_avg:58.16ms
step:1926/2110 train_time:112042ms step_avg:58.17ms
step:1927/2110 train_time:112130ms step_avg:58.19ms
step:1928/2110 train_time:112218ms step_avg:58.20ms
step:1929/2110 train_time:112306ms step_avg:58.22ms
step:1930/2110 train_time:112393ms step_avg:58.23ms
step:1931/2110 train_time:112482ms step_avg:58.25ms
step:1932/2110 train_time:112569ms step_avg:58.27ms
step:1933/2110 train_time:112658ms step_avg:58.28ms
step:1934/2110 train_time:112745ms step_avg:58.30ms
step:1935/2110 train_time:112833ms step_avg:58.31ms
step:1936/2110 train_time:112921ms step_avg:58.33ms
step:1937/2110 train_time:113009ms step_avg:58.34ms
step:1938/2110 train_time:113096ms step_avg:58.36ms
step:1939/2110 train_time:113185ms step_avg:58.37ms
step:1940/2110 train_time:113272ms step_avg:58.39ms
step:1941/2110 train_time:113360ms step_avg:58.40ms
step:1942/2110 train_time:113448ms step_avg:58.42ms
step:1943/2110 train_time:113537ms step_avg:58.43ms
step:1944/2110 train_time:113624ms step_avg:58.45ms
step:1945/2110 train_time:113712ms step_avg:58.46ms
step:1946/2110 train_time:113799ms step_avg:58.48ms
step:1947/2110 train_time:113888ms step_avg:58.49ms
step:1948/2110 train_time:113975ms step_avg:58.51ms
step:1949/2110 train_time:114065ms step_avg:58.52ms
step:1950/2110 train_time:114151ms step_avg:58.54ms
step:1951/2110 train_time:114240ms step_avg:58.55ms
step:1952/2110 train_time:114327ms step_avg:58.57ms
step:1953/2110 train_time:114417ms step_avg:58.59ms
step:1954/2110 train_time:114504ms step_avg:58.60ms
step:1955/2110 train_time:114592ms step_avg:58.61ms
step:1956/2110 train_time:114681ms step_avg:58.63ms
step:1957/2110 train_time:114768ms step_avg:58.65ms
step:1958/2110 train_time:114857ms step_avg:58.66ms
step:1959/2110 train_time:114945ms step_avg:58.68ms
step:1960/2110 train_time:115032ms step_avg:58.69ms
step:1961/2110 train_time:115119ms step_avg:58.70ms
step:1962/2110 train_time:115207ms step_avg:58.72ms
step:1963/2110 train_time:115296ms step_avg:58.73ms
step:1964/2110 train_time:115384ms step_avg:58.75ms
step:1965/2110 train_time:115472ms step_avg:58.76ms
step:1966/2110 train_time:115559ms step_avg:58.78ms
step:1967/2110 train_time:115649ms step_avg:58.79ms
step:1968/2110 train_time:115736ms step_avg:58.81ms
step:1969/2110 train_time:115825ms step_avg:58.82ms
step:1970/2110 train_time:115913ms step_avg:58.84ms
step:1971/2110 train_time:116002ms step_avg:58.85ms
step:1972/2110 train_time:116088ms step_avg:58.87ms
step:1973/2110 train_time:116179ms step_avg:58.88ms
step:1974/2110 train_time:116265ms step_avg:58.90ms
step:1975/2110 train_time:116355ms step_avg:58.91ms
step:1976/2110 train_time:116442ms step_avg:58.93ms
step:1977/2110 train_time:116530ms step_avg:58.94ms
step:1978/2110 train_time:116617ms step_avg:58.96ms
step:1979/2110 train_time:116705ms step_avg:58.97ms
step:1980/2110 train_time:116793ms step_avg:58.99ms
step:1981/2110 train_time:116882ms step_avg:59.00ms
step:1982/2110 train_time:116968ms step_avg:59.02ms
step:1983/2110 train_time:117057ms step_avg:59.03ms
step:1984/2110 train_time:117145ms step_avg:59.04ms
step:1985/2110 train_time:117232ms step_avg:59.06ms
step:1986/2110 train_time:117319ms step_avg:59.07ms
step:1987/2110 train_time:117409ms step_avg:59.09ms
step:1988/2110 train_time:117496ms step_avg:59.10ms
step:1989/2110 train_time:117584ms step_avg:59.12ms
step:1990/2110 train_time:117671ms step_avg:59.13ms
step:1991/2110 train_time:117760ms step_avg:59.15ms
step:1992/2110 train_time:117847ms step_avg:59.16ms
step:1993/2110 train_time:117936ms step_avg:59.18ms
step:1994/2110 train_time:118023ms step_avg:59.19ms
step:1995/2110 train_time:118112ms step_avg:59.20ms
step:1996/2110 train_time:118200ms step_avg:59.22ms
step:1997/2110 train_time:118289ms step_avg:59.23ms
step:1998/2110 train_time:118376ms step_avg:59.25ms
step:1999/2110 train_time:118464ms step_avg:59.26ms
step:2000/2110 train_time:118551ms step_avg:59.28ms
step:2000/2110 val_loss:3.3040 train_time:118641ms step_avg:59.32ms
step:2001/2110 train_time:118663ms step_avg:59.30ms
step:2002/2110 train_time:118732ms step_avg:59.31ms
step:2003/2110 train_time:118824ms step_avg:59.32ms
step:2004/2110 train_time:118914ms step_avg:59.34ms
step:2005/2110 train_time:119002ms step_avg:59.35ms
step:2006/2110 train_time:119088ms step_avg:59.37ms
step:2007/2110 train_time:119175ms step_avg:59.38ms
step:2008/2110 train_time:119262ms step_avg:59.39ms
step:2009/2110 train_time:119349ms step_avg:59.41ms
step:2010/2110 train_time:119437ms step_avg:59.42ms
step:2011/2110 train_time:119525ms step_avg:59.44ms
step:2012/2110 train_time:119613ms step_avg:59.45ms
step:2013/2110 train_time:119705ms step_avg:59.47ms
step:2014/2110 train_time:119794ms step_avg:59.48ms
step:2015/2110 train_time:119885ms step_avg:59.50ms
step:2016/2110 train_time:119972ms step_avg:59.51ms
step:2017/2110 train_time:120060ms step_avg:59.52ms
step:2018/2110 train_time:120146ms step_avg:59.54ms
step:2019/2110 train_time:120234ms step_avg:59.55ms
step:2020/2110 train_time:120321ms step_avg:59.57ms
step:2021/2110 train_time:120409ms step_avg:59.58ms
step:2022/2110 train_time:120495ms step_avg:59.59ms
step:2023/2110 train_time:120585ms step_avg:59.61ms
step:2024/2110 train_time:120673ms step_avg:59.62ms
step:2025/2110 train_time:120763ms step_avg:59.64ms
step:2026/2110 train_time:120852ms step_avg:59.65ms
step:2027/2110 train_time:120942ms step_avg:59.67ms
step:2028/2110 train_time:121029ms step_avg:59.68ms
step:2029/2110 train_time:121118ms step_avg:59.69ms
step:2030/2110 train_time:121204ms step_avg:59.71ms
step:2031/2110 train_time:121293ms step_avg:59.72ms
step:2032/2110 train_time:121380ms step_avg:59.73ms
step:2033/2110 train_time:121468ms step_avg:59.75ms
step:2034/2110 train_time:121556ms step_avg:59.76ms
step:2035/2110 train_time:121646ms step_avg:59.78ms
step:2036/2110 train_time:121734ms step_avg:59.79ms
step:2037/2110 train_time:121824ms step_avg:59.81ms
step:2038/2110 train_time:121911ms step_avg:59.82ms
step:2039/2110 train_time:122000ms step_avg:59.83ms
step:2040/2110 train_time:122087ms step_avg:59.85ms
step:2041/2110 train_time:122177ms step_avg:59.86ms
step:2042/2110 train_time:122262ms step_avg:59.87ms
step:2043/2110 train_time:122350ms step_avg:59.89ms
step:2044/2110 train_time:122438ms step_avg:59.90ms
step:2045/2110 train_time:122526ms step_avg:59.91ms
step:2046/2110 train_time:122614ms step_avg:59.93ms
step:2047/2110 train_time:122703ms step_avg:59.94ms
step:2048/2110 train_time:122792ms step_avg:59.96ms
step:2049/2110 train_time:122880ms step_avg:59.97ms
step:2050/2110 train_time:122967ms step_avg:59.98ms
step:2051/2110 train_time:123056ms step_avg:60.00ms
step:2052/2110 train_time:123143ms step_avg:60.01ms
step:2053/2110 train_time:123231ms step_avg:60.02ms
step:2054/2110 train_time:123318ms step_avg:60.04ms
step:2055/2110 train_time:123406ms step_avg:60.05ms
step:2056/2110 train_time:123493ms step_avg:60.06ms
step:2057/2110 train_time:123583ms step_avg:60.08ms
step:2058/2110 train_time:123671ms step_avg:60.09ms
step:2059/2110 train_time:123760ms step_avg:60.11ms
step:2060/2110 train_time:123847ms step_avg:60.12ms
step:2061/2110 train_time:123936ms step_avg:60.13ms
step:2062/2110 train_time:124023ms step_avg:60.15ms
step:2063/2110 train_time:124111ms step_avg:60.16ms
step:2064/2110 train_time:124198ms step_avg:60.17ms
step:2065/2110 train_time:124286ms step_avg:60.19ms
step:2066/2110 train_time:124374ms step_avg:60.20ms
step:2067/2110 train_time:124462ms step_avg:60.21ms
step:2068/2110 train_time:124550ms step_avg:60.23ms
step:2069/2110 train_time:124639ms step_avg:60.24ms
step:2070/2110 train_time:124726ms step_avg:60.25ms
step:2071/2110 train_time:124816ms step_avg:60.27ms
step:2072/2110 train_time:124903ms step_avg:60.28ms
step:2073/2110 train_time:124992ms step_avg:60.30ms
step:2074/2110 train_time:125080ms step_avg:60.31ms
step:2075/2110 train_time:125169ms step_avg:60.32ms
step:2076/2110 train_time:125255ms step_avg:60.33ms
step:2077/2110 train_time:125345ms step_avg:60.35ms
step:2078/2110 train_time:125432ms step_avg:60.36ms
step:2079/2110 train_time:125521ms step_avg:60.38ms
step:2080/2110 train_time:125608ms step_avg:60.39ms
step:2081/2110 train_time:125697ms step_avg:60.40ms
step:2082/2110 train_time:125785ms step_avg:60.42ms
step:2083/2110 train_time:125876ms step_avg:60.43ms
step:2084/2110 train_time:125963ms step_avg:60.44ms
step:2085/2110 train_time:126053ms step_avg:60.46ms
step:2086/2110 train_time:126141ms step_avg:60.47ms
step:2087/2110 train_time:126228ms step_avg:60.48ms
step:2088/2110 train_time:126315ms step_avg:60.50ms
step:2089/2110 train_time:126404ms step_avg:60.51ms
step:2090/2110 train_time:126491ms step_avg:60.52ms
step:2091/2110 train_time:126580ms step_avg:60.54ms
step:2092/2110 train_time:126668ms step_avg:60.55ms
step:2093/2110 train_time:126756ms step_avg:60.56ms
step:2094/2110 train_time:126843ms step_avg:60.57ms
step:2095/2110 train_time:126933ms step_avg:60.59ms
step:2096/2110 train_time:127021ms step_avg:60.60ms
step:2097/2110 train_time:127109ms step_avg:60.61ms
step:2098/2110 train_time:127197ms step_avg:60.63ms
step:2099/2110 train_time:127285ms step_avg:60.64ms
step:2100/2110 train_time:127372ms step_avg:60.65ms
step:2101/2110 train_time:127460ms step_avg:60.67ms
step:2102/2110 train_time:127548ms step_avg:60.68ms
step:2103/2110 train_time:127637ms step_avg:60.69ms
step:2104/2110 train_time:127724ms step_avg:60.71ms
step:2105/2110 train_time:127814ms step_avg:60.72ms
step:2106/2110 train_time:127901ms step_avg:60.73ms
step:2107/2110 train_time:127990ms step_avg:60.75ms
step:2108/2110 train_time:128077ms step_avg:60.76ms
step:2109/2110 train_time:128167ms step_avg:60.77ms
step:2110/2110 train_time:128254ms step_avg:60.78ms
step:2110/2110 val_loss:3.2800 train_time:128344ms step_avg:60.83ms
peak memory allocated: 29892 MiB reserved: 44416 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 15 20:34:01 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   39C    P0            128W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   42C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   43C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   42C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              88      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              88      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:105ms step_avg:104.99ms
step:2/2110 train_time:141ms step_avg:70.45ms
step:3/2110 train_time:174ms step_avg:57.93ms
step:4/2110 train_time:205ms step_avg:51.27ms
step:5/2110 train_time:236ms step_avg:47.14ms
step:6/2110 train_time:425ms step_avg:70.91ms
step:7/2110 train_time:609ms step_avg:86.98ms
step:8/2110 train_time:646ms step_avg:80.71ms
step:9/2110 train_time:679ms step_avg:75.40ms
step:10/2110 train_time:712ms step_avg:71.18ms
step:11/2110 train_time:742ms step_avg:67.43ms
step:12/2110 train_time:775ms step_avg:64.59ms
step:13/2110 train_time:806ms step_avg:62.03ms
step:14/2110 train_time:842ms step_avg:60.12ms
step:15/2110 train_time:873ms step_avg:58.17ms
step:16/2110 train_time:906ms step_avg:56.61ms
step:17/2110 train_time:938ms step_avg:55.17ms
step:18/2110 train_time:974ms step_avg:54.10ms
step:19/2110 train_time:1007ms step_avg:52.98ms
step:20/2110 train_time:1040ms step_avg:52.01ms
step:21/2110 train_time:1072ms step_avg:51.07ms
step:22/2110 train_time:1106ms step_avg:50.27ms
step:23/2110 train_time:1137ms step_avg:49.41ms
step:24/2110 train_time:1171ms step_avg:48.79ms
step:25/2110 train_time:1203ms step_avg:48.12ms
step:26/2110 train_time:1237ms step_avg:47.57ms
step:27/2110 train_time:1270ms step_avg:47.05ms
step:28/2110 train_time:1303ms step_avg:46.55ms
step:29/2110 train_time:1334ms step_avg:46.01ms
step:30/2110 train_time:1368ms step_avg:45.59ms
step:31/2110 train_time:1401ms step_avg:45.18ms
step:32/2110 train_time:1435ms step_avg:44.84ms
step:33/2110 train_time:1467ms step_avg:44.45ms
step:34/2110 train_time:1502ms step_avg:44.17ms
step:35/2110 train_time:1535ms step_avg:43.87ms
step:36/2110 train_time:1570ms step_avg:43.61ms
step:37/2110 train_time:1602ms step_avg:43.31ms
step:38/2110 train_time:1637ms step_avg:43.08ms
step:39/2110 train_time:1670ms step_avg:42.81ms
step:40/2110 train_time:1705ms step_avg:42.62ms
step:41/2110 train_time:1739ms step_avg:42.41ms
step:42/2110 train_time:1771ms step_avg:42.17ms
step:43/2110 train_time:1805ms step_avg:41.97ms
step:44/2110 train_time:1839ms step_avg:41.79ms
step:45/2110 train_time:1870ms step_avg:41.55ms
step:46/2110 train_time:1904ms step_avg:41.38ms
step:47/2110 train_time:1935ms step_avg:41.17ms
step:48/2110 train_time:1971ms step_avg:41.07ms
step:49/2110 train_time:2003ms step_avg:40.87ms
step:50/2110 train_time:2038ms step_avg:40.75ms
step:51/2110 train_time:2070ms step_avg:40.59ms
step:52/2110 train_time:2104ms step_avg:40.47ms
step:53/2110 train_time:2137ms step_avg:40.33ms
step:54/2110 train_time:2172ms step_avg:40.21ms
step:55/2110 train_time:2203ms step_avg:40.05ms
step:56/2110 train_time:2235ms step_avg:39.91ms
step:57/2110 train_time:2266ms step_avg:39.75ms
step:58/2110 train_time:2301ms step_avg:39.67ms
step:59/2110 train_time:2333ms step_avg:39.55ms
step:60/2110 train_time:2367ms step_avg:39.45ms
step:61/2110 train_time:2399ms step_avg:39.33ms
step:62/2110 train_time:2435ms step_avg:39.27ms
step:63/2110 train_time:2467ms step_avg:39.15ms
step:64/2110 train_time:2500ms step_avg:39.07ms
step:65/2110 train_time:2532ms step_avg:38.96ms
step:66/2110 train_time:2567ms step_avg:38.90ms
step:67/2110 train_time:2600ms step_avg:38.80ms
step:68/2110 train_time:2634ms step_avg:38.74ms
step:69/2110 train_time:2667ms step_avg:38.65ms
step:70/2110 train_time:2701ms step_avg:38.58ms
step:71/2110 train_time:2734ms step_avg:38.51ms
step:72/2110 train_time:2769ms step_avg:38.45ms
step:73/2110 train_time:2800ms step_avg:38.36ms
step:74/2110 train_time:2834ms step_avg:38.30ms
step:75/2110 train_time:2867ms step_avg:38.22ms
step:76/2110 train_time:2902ms step_avg:38.18ms
step:77/2110 train_time:2936ms step_avg:38.13ms
step:78/2110 train_time:2970ms step_avg:38.08ms
step:79/2110 train_time:3002ms step_avg:38.00ms
step:80/2110 train_time:3033ms step_avg:37.92ms
step:81/2110 train_time:3066ms step_avg:37.85ms
step:82/2110 train_time:3100ms step_avg:37.80ms
step:83/2110 train_time:3130ms step_avg:37.71ms
step:84/2110 train_time:3166ms step_avg:37.69ms
step:85/2110 train_time:3197ms step_avg:37.62ms
step:86/2110 train_time:3231ms step_avg:37.57ms
step:87/2110 train_time:3262ms step_avg:37.50ms
step:88/2110 train_time:3295ms step_avg:37.44ms
step:89/2110 train_time:3328ms step_avg:37.40ms
step:90/2110 train_time:3362ms step_avg:37.35ms
step:91/2110 train_time:3395ms step_avg:37.30ms
step:92/2110 train_time:3427ms step_avg:37.25ms
step:93/2110 train_time:3461ms step_avg:37.21ms
step:94/2110 train_time:3493ms step_avg:37.16ms
step:95/2110 train_time:3527ms step_avg:37.13ms
step:96/2110 train_time:3560ms step_avg:37.09ms
step:97/2110 train_time:3594ms step_avg:37.05ms
step:98/2110 train_time:3626ms step_avg:37.00ms
step:99/2110 train_time:3660ms step_avg:36.97ms
step:100/2110 train_time:3693ms step_avg:36.93ms
step:101/2110 train_time:3726ms step_avg:36.90ms
step:102/2110 train_time:3759ms step_avg:36.86ms
step:103/2110 train_time:3793ms step_avg:36.82ms
step:104/2110 train_time:3825ms step_avg:36.78ms
step:105/2110 train_time:3859ms step_avg:36.75ms
step:106/2110 train_time:3892ms step_avg:36.72ms
step:107/2110 train_time:3925ms step_avg:36.69ms
step:108/2110 train_time:3958ms step_avg:36.65ms
step:109/2110 train_time:3992ms step_avg:36.62ms
step:110/2110 train_time:4024ms step_avg:36.58ms
step:111/2110 train_time:4058ms step_avg:36.56ms
step:112/2110 train_time:4090ms step_avg:36.52ms
step:113/2110 train_time:4124ms step_avg:36.50ms
step:114/2110 train_time:4157ms step_avg:36.47ms
step:115/2110 train_time:4190ms step_avg:36.44ms
step:116/2110 train_time:4223ms step_avg:36.40ms
step:117/2110 train_time:4256ms step_avg:36.38ms
step:118/2110 train_time:4290ms step_avg:36.35ms
step:119/2110 train_time:4323ms step_avg:36.32ms
step:120/2110 train_time:4355ms step_avg:36.29ms
step:121/2110 train_time:4388ms step_avg:36.27ms
step:122/2110 train_time:4421ms step_avg:36.24ms
step:123/2110 train_time:4454ms step_avg:36.21ms
step:124/2110 train_time:4487ms step_avg:36.19ms
step:125/2110 train_time:4521ms step_avg:36.17ms
step:126/2110 train_time:4553ms step_avg:36.14ms
step:127/2110 train_time:4587ms step_avg:36.12ms
step:128/2110 train_time:4621ms step_avg:36.10ms
step:129/2110 train_time:4653ms step_avg:36.07ms
step:130/2110 train_time:4686ms step_avg:36.05ms
step:131/2110 train_time:4719ms step_avg:36.02ms
step:132/2110 train_time:4752ms step_avg:36.00ms
step:133/2110 train_time:4785ms step_avg:35.98ms
step:134/2110 train_time:4818ms step_avg:35.96ms
step:135/2110 train_time:4851ms step_avg:35.93ms
step:136/2110 train_time:4884ms step_avg:35.91ms
step:137/2110 train_time:4917ms step_avg:35.89ms
step:138/2110 train_time:4949ms step_avg:35.86ms
step:139/2110 train_time:4983ms step_avg:35.85ms
step:140/2110 train_time:5016ms step_avg:35.83ms
step:141/2110 train_time:5049ms step_avg:35.81ms
step:142/2110 train_time:5082ms step_avg:35.79ms
step:143/2110 train_time:5115ms step_avg:35.77ms
step:144/2110 train_time:5147ms step_avg:35.75ms
step:145/2110 train_time:5180ms step_avg:35.73ms
step:146/2110 train_time:5214ms step_avg:35.72ms
step:147/2110 train_time:5246ms step_avg:35.69ms
step:148/2110 train_time:5279ms step_avg:35.67ms
step:149/2110 train_time:5312ms step_avg:35.65ms
step:150/2110 train_time:5345ms step_avg:35.64ms
step:151/2110 train_time:5379ms step_avg:35.62ms
step:152/2110 train_time:5412ms step_avg:35.60ms
step:153/2110 train_time:5445ms step_avg:35.59ms
step:154/2110 train_time:5479ms step_avg:35.58ms
step:155/2110 train_time:5512ms step_avg:35.56ms
step:156/2110 train_time:5545ms step_avg:35.54ms
step:157/2110 train_time:5578ms step_avg:35.53ms
step:158/2110 train_time:5610ms step_avg:35.51ms
step:159/2110 train_time:5644ms step_avg:35.50ms
step:160/2110 train_time:5676ms step_avg:35.48ms
step:161/2110 train_time:5710ms step_avg:35.46ms
step:162/2110 train_time:5742ms step_avg:35.44ms
step:163/2110 train_time:5775ms step_avg:35.43ms
step:164/2110 train_time:5808ms step_avg:35.42ms
step:165/2110 train_time:5841ms step_avg:35.40ms
step:166/2110 train_time:5874ms step_avg:35.38ms
step:167/2110 train_time:5907ms step_avg:35.37ms
step:168/2110 train_time:5940ms step_avg:35.36ms
step:169/2110 train_time:5973ms step_avg:35.34ms
step:170/2110 train_time:6006ms step_avg:35.33ms
step:171/2110 train_time:6040ms step_avg:35.32ms
step:172/2110 train_time:6073ms step_avg:35.31ms
step:173/2110 train_time:6106ms step_avg:35.29ms
step:174/2110 train_time:6139ms step_avg:35.28ms
step:175/2110 train_time:6172ms step_avg:35.27ms
step:176/2110 train_time:6205ms step_avg:35.26ms
step:177/2110 train_time:6238ms step_avg:35.24ms
step:178/2110 train_time:6271ms step_avg:35.23ms
step:179/2110 train_time:6304ms step_avg:35.22ms
step:180/2110 train_time:6337ms step_avg:35.20ms
step:181/2110 train_time:6370ms step_avg:35.19ms
step:182/2110 train_time:6403ms step_avg:35.18ms
step:183/2110 train_time:6436ms step_avg:35.17ms
step:184/2110 train_time:6469ms step_avg:35.16ms
step:185/2110 train_time:6502ms step_avg:35.15ms
step:186/2110 train_time:6534ms step_avg:35.13ms
step:187/2110 train_time:6568ms step_avg:35.12ms
step:188/2110 train_time:6601ms step_avg:35.11ms
step:189/2110 train_time:6634ms step_avg:35.10ms
step:190/2110 train_time:6666ms step_avg:35.09ms
step:191/2110 train_time:6700ms step_avg:35.08ms
step:192/2110 train_time:6733ms step_avg:35.07ms
step:193/2110 train_time:6766ms step_avg:35.06ms
step:194/2110 train_time:6799ms step_avg:35.05ms
step:195/2110 train_time:6832ms step_avg:35.04ms
step:196/2110 train_time:6865ms step_avg:35.02ms
step:197/2110 train_time:6898ms step_avg:35.01ms
step:198/2110 train_time:6930ms step_avg:35.00ms
step:199/2110 train_time:6964ms step_avg:34.99ms
step:200/2110 train_time:6996ms step_avg:34.98ms
step:201/2110 train_time:7030ms step_avg:34.97ms
step:202/2110 train_time:7063ms step_avg:34.96ms
step:203/2110 train_time:7096ms step_avg:34.96ms
step:204/2110 train_time:7129ms step_avg:34.95ms
step:205/2110 train_time:7162ms step_avg:34.94ms
step:206/2110 train_time:7194ms step_avg:34.92ms
step:207/2110 train_time:7228ms step_avg:34.92ms
step:208/2110 train_time:7260ms step_avg:34.91ms
step:209/2110 train_time:7293ms step_avg:34.90ms
step:210/2110 train_time:7326ms step_avg:34.88ms
step:211/2110 train_time:7359ms step_avg:34.88ms
step:212/2110 train_time:7392ms step_avg:34.87ms
step:213/2110 train_time:7426ms step_avg:34.86ms
step:214/2110 train_time:7458ms step_avg:34.85ms
step:215/2110 train_time:7492ms step_avg:34.84ms
step:216/2110 train_time:7525ms step_avg:34.84ms
step:217/2110 train_time:7558ms step_avg:34.83ms
step:218/2110 train_time:7591ms step_avg:34.82ms
step:219/2110 train_time:7625ms step_avg:34.82ms
step:220/2110 train_time:7658ms step_avg:34.81ms
step:221/2110 train_time:7690ms step_avg:34.80ms
step:222/2110 train_time:7723ms step_avg:34.79ms
step:223/2110 train_time:7757ms step_avg:34.78ms
step:224/2110 train_time:7790ms step_avg:34.78ms
step:225/2110 train_time:7823ms step_avg:34.77ms
step:226/2110 train_time:7856ms step_avg:34.76ms
step:227/2110 train_time:7889ms step_avg:34.75ms
step:228/2110 train_time:7922ms step_avg:34.74ms
step:229/2110 train_time:7955ms step_avg:34.74ms
step:230/2110 train_time:7987ms step_avg:34.73ms
step:231/2110 train_time:8021ms step_avg:34.72ms
step:232/2110 train_time:8053ms step_avg:34.71ms
step:233/2110 train_time:8087ms step_avg:34.71ms
step:234/2110 train_time:8120ms step_avg:34.70ms
step:235/2110 train_time:8153ms step_avg:34.69ms
step:236/2110 train_time:8186ms step_avg:34.69ms
step:237/2110 train_time:8219ms step_avg:34.68ms
step:238/2110 train_time:8252ms step_avg:34.67ms
step:239/2110 train_time:8285ms step_avg:34.66ms
step:240/2110 train_time:8317ms step_avg:34.65ms
step:241/2110 train_time:8350ms step_avg:34.65ms
step:242/2110 train_time:8383ms step_avg:34.64ms
step:243/2110 train_time:8416ms step_avg:34.64ms
step:244/2110 train_time:8451ms step_avg:34.63ms
step:245/2110 train_time:8482ms step_avg:34.62ms
step:246/2110 train_time:8515ms step_avg:34.61ms
step:247/2110 train_time:8548ms step_avg:34.61ms
step:248/2110 train_time:8582ms step_avg:34.60ms
step:249/2110 train_time:8615ms step_avg:34.60ms
step:250/2110 train_time:8647ms step_avg:34.59ms
step:250/2110 val_loss:4.3038 train_time:8682ms step_avg:34.73ms
step:251/2110 train_time:8712ms step_avg:34.71ms
step:252/2110 train_time:8741ms step_avg:34.69ms
step:253/2110 train_time:8769ms step_avg:34.66ms
step:254/2110 train_time:8798ms step_avg:34.64ms
step:255/2110 train_time:8826ms step_avg:34.61ms
step:256/2110 train_time:8856ms step_avg:34.59ms
step:257/2110 train_time:8890ms step_avg:34.59ms
step:258/2110 train_time:8923ms step_avg:34.59ms
step:259/2110 train_time:8957ms step_avg:34.58ms
step:260/2110 train_time:8990ms step_avg:34.58ms
step:261/2110 train_time:9023ms step_avg:34.57ms
step:262/2110 train_time:9056ms step_avg:34.57ms
step:263/2110 train_time:9090ms step_avg:34.56ms
step:264/2110 train_time:9122ms step_avg:34.55ms
step:265/2110 train_time:9155ms step_avg:34.55ms
step:266/2110 train_time:9188ms step_avg:34.54ms
step:267/2110 train_time:9221ms step_avg:34.53ms
step:268/2110 train_time:9253ms step_avg:34.53ms
step:269/2110 train_time:9286ms step_avg:34.52ms
step:270/2110 train_time:9320ms step_avg:34.52ms
step:271/2110 train_time:9352ms step_avg:34.51ms
step:272/2110 train_time:9385ms step_avg:34.50ms
step:273/2110 train_time:9418ms step_avg:34.50ms
step:274/2110 train_time:9451ms step_avg:34.49ms
step:275/2110 train_time:9509ms step_avg:34.58ms
step:276/2110 train_time:9538ms step_avg:34.56ms
step:277/2110 train_time:9565ms step_avg:34.53ms
step:278/2110 train_time:9595ms step_avg:34.51ms
step:279/2110 train_time:9623ms step_avg:34.49ms
step:280/2110 train_time:9654ms step_avg:34.48ms
step:281/2110 train_time:9681ms step_avg:34.45ms
step:282/2110 train_time:9713ms step_avg:34.44ms
step:283/2110 train_time:9746ms step_avg:34.44ms
step:284/2110 train_time:9779ms step_avg:34.43ms
step:285/2110 train_time:9812ms step_avg:34.43ms
step:286/2110 train_time:9846ms step_avg:34.43ms
step:287/2110 train_time:9879ms step_avg:34.42ms
step:288/2110 train_time:9914ms step_avg:34.42ms
step:289/2110 train_time:9945ms step_avg:34.41ms
step:290/2110 train_time:9978ms step_avg:34.41ms
step:291/2110 train_time:10012ms step_avg:34.40ms
step:292/2110 train_time:10045ms step_avg:34.40ms
step:293/2110 train_time:10078ms step_avg:34.40ms
step:294/2110 train_time:10112ms step_avg:34.39ms
step:295/2110 train_time:10144ms step_avg:34.39ms
step:296/2110 train_time:10177ms step_avg:34.38ms
step:297/2110 train_time:10210ms step_avg:34.38ms
step:298/2110 train_time:10242ms step_avg:34.37ms
step:299/2110 train_time:10275ms step_avg:34.37ms
step:300/2110 train_time:10308ms step_avg:34.36ms
step:301/2110 train_time:10341ms step_avg:34.36ms
step:302/2110 train_time:10374ms step_avg:34.35ms
step:303/2110 train_time:10407ms step_avg:34.35ms
step:304/2110 train_time:10439ms step_avg:34.34ms
step:305/2110 train_time:10472ms step_avg:34.34ms
step:306/2110 train_time:10505ms step_avg:34.33ms
step:307/2110 train_time:10538ms step_avg:34.33ms
step:308/2110 train_time:10572ms step_avg:34.32ms
step:309/2110 train_time:10604ms step_avg:34.32ms
step:310/2110 train_time:10637ms step_avg:34.31ms
step:311/2110 train_time:10670ms step_avg:34.31ms
step:312/2110 train_time:10704ms step_avg:34.31ms
step:313/2110 train_time:10736ms step_avg:34.30ms
step:314/2110 train_time:10768ms step_avg:34.29ms
step:315/2110 train_time:10802ms step_avg:34.29ms
step:316/2110 train_time:10835ms step_avg:34.29ms
step:317/2110 train_time:10868ms step_avg:34.28ms
step:318/2110 train_time:10901ms step_avg:34.28ms
step:319/2110 train_time:10935ms step_avg:34.28ms
step:320/2110 train_time:10969ms step_avg:34.28ms
step:321/2110 train_time:11001ms step_avg:34.27ms
step:322/2110 train_time:11034ms step_avg:34.27ms
step:323/2110 train_time:11067ms step_avg:34.26ms
step:324/2110 train_time:11099ms step_avg:34.26ms
step:325/2110 train_time:11133ms step_avg:34.25ms
step:326/2110 train_time:11165ms step_avg:34.25ms
step:327/2110 train_time:11198ms step_avg:34.25ms
step:328/2110 train_time:11231ms step_avg:34.24ms
step:329/2110 train_time:11264ms step_avg:34.24ms
step:330/2110 train_time:11296ms step_avg:34.23ms
step:331/2110 train_time:11330ms step_avg:34.23ms
step:332/2110 train_time:11362ms step_avg:34.22ms
step:333/2110 train_time:11395ms step_avg:34.22ms
step:334/2110 train_time:11428ms step_avg:34.22ms
step:335/2110 train_time:11461ms step_avg:34.21ms
step:336/2110 train_time:11494ms step_avg:34.21ms
step:337/2110 train_time:11527ms step_avg:34.20ms
step:338/2110 train_time:11559ms step_avg:34.20ms
step:339/2110 train_time:11593ms step_avg:34.20ms
step:340/2110 train_time:11625ms step_avg:34.19ms
step:341/2110 train_time:11659ms step_avg:34.19ms
step:342/2110 train_time:11691ms step_avg:34.18ms
step:343/2110 train_time:11724ms step_avg:34.18ms
step:344/2110 train_time:11757ms step_avg:34.18ms
step:345/2110 train_time:11790ms step_avg:34.18ms
step:346/2110 train_time:11824ms step_avg:34.17ms
step:347/2110 train_time:11857ms step_avg:34.17ms
step:348/2110 train_time:11889ms step_avg:34.17ms
step:349/2110 train_time:11923ms step_avg:34.16ms
step:350/2110 train_time:11956ms step_avg:34.16ms
step:351/2110 train_time:11989ms step_avg:34.16ms
step:352/2110 train_time:12022ms step_avg:34.15ms
step:353/2110 train_time:12055ms step_avg:34.15ms
step:354/2110 train_time:12088ms step_avg:34.15ms
step:355/2110 train_time:12121ms step_avg:34.14ms
step:356/2110 train_time:12153ms step_avg:34.14ms
step:357/2110 train_time:12187ms step_avg:34.14ms
step:358/2110 train_time:12219ms step_avg:34.13ms
step:359/2110 train_time:12253ms step_avg:34.13ms
step:360/2110 train_time:12285ms step_avg:34.13ms
step:361/2110 train_time:12318ms step_avg:34.12ms
step:362/2110 train_time:12351ms step_avg:34.12ms
step:363/2110 train_time:12385ms step_avg:34.12ms
step:364/2110 train_time:12417ms step_avg:34.11ms
step:365/2110 train_time:12450ms step_avg:34.11ms
step:366/2110 train_time:12483ms step_avg:34.11ms
step:367/2110 train_time:12516ms step_avg:34.10ms
step:368/2110 train_time:12549ms step_avg:34.10ms
step:369/2110 train_time:12582ms step_avg:34.10ms
step:370/2110 train_time:12615ms step_avg:34.10ms
step:371/2110 train_time:12648ms step_avg:34.09ms
step:372/2110 train_time:12681ms step_avg:34.09ms
step:373/2110 train_time:12715ms step_avg:34.09ms
step:374/2110 train_time:12747ms step_avg:34.08ms
step:375/2110 train_time:12780ms step_avg:34.08ms
step:376/2110 train_time:12813ms step_avg:34.08ms
step:377/2110 train_time:12847ms step_avg:34.08ms
step:378/2110 train_time:12879ms step_avg:34.07ms
step:379/2110 train_time:12913ms step_avg:34.07ms
step:380/2110 train_time:12946ms step_avg:34.07ms
step:381/2110 train_time:12978ms step_avg:34.06ms
step:382/2110 train_time:13012ms step_avg:34.06ms
step:383/2110 train_time:13044ms step_avg:34.06ms
step:384/2110 train_time:13077ms step_avg:34.06ms
step:385/2110 train_time:13111ms step_avg:34.05ms
step:386/2110 train_time:13144ms step_avg:34.05ms
step:387/2110 train_time:13177ms step_avg:34.05ms
step:388/2110 train_time:13210ms step_avg:34.05ms
step:389/2110 train_time:13244ms step_avg:34.05ms
step:390/2110 train_time:13276ms step_avg:34.04ms
step:391/2110 train_time:13310ms step_avg:34.04ms
step:392/2110 train_time:13342ms step_avg:34.04ms
step:393/2110 train_time:13376ms step_avg:34.04ms
step:394/2110 train_time:13409ms step_avg:34.03ms
step:395/2110 train_time:13442ms step_avg:34.03ms
step:396/2110 train_time:13475ms step_avg:34.03ms
step:397/2110 train_time:13508ms step_avg:34.03ms
step:398/2110 train_time:13541ms step_avg:34.02ms
step:399/2110 train_time:13574ms step_avg:34.02ms
step:400/2110 train_time:13607ms step_avg:34.02ms
step:401/2110 train_time:13640ms step_avg:34.01ms
step:402/2110 train_time:13673ms step_avg:34.01ms
step:403/2110 train_time:13707ms step_avg:34.01ms
step:404/2110 train_time:13740ms step_avg:34.01ms
step:405/2110 train_time:13772ms step_avg:34.01ms
step:406/2110 train_time:13805ms step_avg:34.00ms
step:407/2110 train_time:13838ms step_avg:34.00ms
step:408/2110 train_time:13871ms step_avg:34.00ms
step:409/2110 train_time:13904ms step_avg:34.00ms
step:410/2110 train_time:13937ms step_avg:33.99ms
step:411/2110 train_time:13970ms step_avg:33.99ms
step:412/2110 train_time:14003ms step_avg:33.99ms
step:413/2110 train_time:14036ms step_avg:33.99ms
step:414/2110 train_time:14069ms step_avg:33.98ms
step:415/2110 train_time:14102ms step_avg:33.98ms
step:416/2110 train_time:14135ms step_avg:33.98ms
step:417/2110 train_time:14168ms step_avg:33.98ms
step:418/2110 train_time:14200ms step_avg:33.97ms
step:419/2110 train_time:14234ms step_avg:33.97ms
step:420/2110 train_time:14267ms step_avg:33.97ms
step:421/2110 train_time:14300ms step_avg:33.97ms
step:422/2110 train_time:14333ms step_avg:33.96ms
step:423/2110 train_time:14366ms step_avg:33.96ms
step:424/2110 train_time:14399ms step_avg:33.96ms
step:425/2110 train_time:14432ms step_avg:33.96ms
step:426/2110 train_time:14464ms step_avg:33.95ms
step:427/2110 train_time:14498ms step_avg:33.95ms
step:428/2110 train_time:14531ms step_avg:33.95ms
step:429/2110 train_time:14563ms step_avg:33.95ms
step:430/2110 train_time:14596ms step_avg:33.95ms
step:431/2110 train_time:14629ms step_avg:33.94ms
step:432/2110 train_time:14663ms step_avg:33.94ms
step:433/2110 train_time:14696ms step_avg:33.94ms
step:434/2110 train_time:14729ms step_avg:33.94ms
step:435/2110 train_time:14762ms step_avg:33.94ms
step:436/2110 train_time:14795ms step_avg:33.93ms
step:437/2110 train_time:14828ms step_avg:33.93ms
step:438/2110 train_time:14861ms step_avg:33.93ms
step:439/2110 train_time:14894ms step_avg:33.93ms
step:440/2110 train_time:14926ms step_avg:33.92ms
step:441/2110 train_time:14960ms step_avg:33.92ms
step:442/2110 train_time:14993ms step_avg:33.92ms
step:443/2110 train_time:15026ms step_avg:33.92ms
step:444/2110 train_time:15059ms step_avg:33.92ms
step:445/2110 train_time:15092ms step_avg:33.91ms
step:446/2110 train_time:15124ms step_avg:33.91ms
step:447/2110 train_time:15158ms step_avg:33.91ms
step:448/2110 train_time:15191ms step_avg:33.91ms
step:449/2110 train_time:15223ms step_avg:33.91ms
step:450/2110 train_time:15256ms step_avg:33.90ms
step:451/2110 train_time:15290ms step_avg:33.90ms
step:452/2110 train_time:15322ms step_avg:33.90ms
step:453/2110 train_time:15356ms step_avg:33.90ms
step:454/2110 train_time:15388ms step_avg:33.89ms
step:455/2110 train_time:15422ms step_avg:33.89ms
step:456/2110 train_time:15456ms step_avg:33.89ms
step:457/2110 train_time:15487ms step_avg:33.89ms
step:458/2110 train_time:15521ms step_avg:33.89ms
step:459/2110 train_time:15554ms step_avg:33.89ms
step:460/2110 train_time:15587ms step_avg:33.88ms
step:461/2110 train_time:15619ms step_avg:33.88ms
step:462/2110 train_time:15652ms step_avg:33.88ms
step:463/2110 train_time:15685ms step_avg:33.88ms
step:464/2110 train_time:15719ms step_avg:33.88ms
step:465/2110 train_time:15751ms step_avg:33.87ms
step:466/2110 train_time:15784ms step_avg:33.87ms
step:467/2110 train_time:15817ms step_avg:33.87ms
step:468/2110 train_time:15850ms step_avg:33.87ms
step:469/2110 train_time:15883ms step_avg:33.86ms
step:470/2110 train_time:15916ms step_avg:33.86ms
step:471/2110 train_time:15949ms step_avg:33.86ms
step:472/2110 train_time:15983ms step_avg:33.86ms
step:473/2110 train_time:16017ms step_avg:33.86ms
step:474/2110 train_time:16050ms step_avg:33.86ms
step:475/2110 train_time:16082ms step_avg:33.86ms
step:476/2110 train_time:16115ms step_avg:33.85ms
step:477/2110 train_time:16147ms step_avg:33.85ms
step:478/2110 train_time:16180ms step_avg:33.85ms
step:479/2110 train_time:16213ms step_avg:33.85ms
step:480/2110 train_time:16246ms step_avg:33.85ms
step:481/2110 train_time:16279ms step_avg:33.84ms
step:482/2110 train_time:16312ms step_avg:33.84ms
step:483/2110 train_time:16345ms step_avg:33.84ms
step:484/2110 train_time:16377ms step_avg:33.84ms
step:485/2110 train_time:16411ms step_avg:33.84ms
step:486/2110 train_time:16443ms step_avg:33.83ms
step:487/2110 train_time:16477ms step_avg:33.83ms
step:488/2110 train_time:16510ms step_avg:33.83ms
step:489/2110 train_time:16543ms step_avg:33.83ms
step:490/2110 train_time:16576ms step_avg:33.83ms
step:491/2110 train_time:16609ms step_avg:33.83ms
step:492/2110 train_time:16641ms step_avg:33.82ms
step:493/2110 train_time:16675ms step_avg:33.82ms
step:494/2110 train_time:16708ms step_avg:33.82ms
step:495/2110 train_time:16741ms step_avg:33.82ms
step:496/2110 train_time:16775ms step_avg:33.82ms
step:497/2110 train_time:16807ms step_avg:33.82ms
step:498/2110 train_time:16840ms step_avg:33.81ms
step:499/2110 train_time:16873ms step_avg:33.81ms
step:500/2110 train_time:16905ms step_avg:33.81ms
step:500/2110 val_loss:4.0339 train_time:16941ms step_avg:33.88ms
step:501/2110 train_time:16970ms step_avg:33.87ms
step:502/2110 train_time:16999ms step_avg:33.86ms
step:503/2110 train_time:17026ms step_avg:33.85ms
step:504/2110 train_time:17055ms step_avg:33.84ms
step:505/2110 train_time:17081ms step_avg:33.82ms
step:506/2110 train_time:17113ms step_avg:33.82ms
step:507/2110 train_time:17148ms step_avg:33.82ms
step:508/2110 train_time:17180ms step_avg:33.82ms
step:509/2110 train_time:17214ms step_avg:33.82ms
step:510/2110 train_time:17246ms step_avg:33.82ms
step:511/2110 train_time:17279ms step_avg:33.81ms
step:512/2110 train_time:17312ms step_avg:33.81ms
step:513/2110 train_time:17345ms step_avg:33.81ms
step:514/2110 train_time:17378ms step_avg:33.81ms
step:515/2110 train_time:17411ms step_avg:33.81ms
step:516/2110 train_time:17444ms step_avg:33.81ms
step:517/2110 train_time:17476ms step_avg:33.80ms
step:518/2110 train_time:17509ms step_avg:33.80ms
step:519/2110 train_time:17542ms step_avg:33.80ms
step:520/2110 train_time:17575ms step_avg:33.80ms
step:521/2110 train_time:17607ms step_avg:33.80ms
step:522/2110 train_time:17640ms step_avg:33.79ms
step:523/2110 train_time:17673ms step_avg:33.79ms
step:524/2110 train_time:17706ms step_avg:33.79ms
step:525/2110 train_time:17738ms step_avg:33.79ms
step:526/2110 train_time:17770ms step_avg:33.78ms
step:527/2110 train_time:17803ms step_avg:33.78ms
step:528/2110 train_time:17836ms step_avg:33.78ms
step:529/2110 train_time:17869ms step_avg:33.78ms
step:530/2110 train_time:17901ms step_avg:33.78ms
step:531/2110 train_time:17934ms step_avg:33.77ms
step:532/2110 train_time:17967ms step_avg:33.77ms
step:533/2110 train_time:18001ms step_avg:33.77ms
step:534/2110 train_time:18034ms step_avg:33.77ms
step:535/2110 train_time:18067ms step_avg:33.77ms
step:536/2110 train_time:18100ms step_avg:33.77ms
step:537/2110 train_time:18134ms step_avg:33.77ms
step:538/2110 train_time:18167ms step_avg:33.77ms
step:539/2110 train_time:18201ms step_avg:33.77ms
step:540/2110 train_time:18235ms step_avg:33.77ms
step:541/2110 train_time:18267ms step_avg:33.77ms
step:542/2110 train_time:18300ms step_avg:33.76ms
step:543/2110 train_time:18333ms step_avg:33.76ms
step:544/2110 train_time:18367ms step_avg:33.76ms
step:545/2110 train_time:18400ms step_avg:33.76ms
step:546/2110 train_time:18433ms step_avg:33.76ms
step:547/2110 train_time:18465ms step_avg:33.76ms
step:548/2110 train_time:18498ms step_avg:33.76ms
step:549/2110 train_time:18531ms step_avg:33.75ms
step:550/2110 train_time:18564ms step_avg:33.75ms
step:551/2110 train_time:18597ms step_avg:33.75ms
step:552/2110 train_time:18631ms step_avg:33.75ms
step:553/2110 train_time:18663ms step_avg:33.75ms
step:554/2110 train_time:18696ms step_avg:33.75ms
step:555/2110 train_time:18730ms step_avg:33.75ms
step:556/2110 train_time:18762ms step_avg:33.74ms
step:557/2110 train_time:18795ms step_avg:33.74ms
step:558/2110 train_time:18828ms step_avg:33.74ms
step:559/2110 train_time:18861ms step_avg:33.74ms
step:560/2110 train_time:18893ms step_avg:33.74ms
step:561/2110 train_time:18926ms step_avg:33.74ms
step:562/2110 train_time:18959ms step_avg:33.74ms
step:563/2110 train_time:18992ms step_avg:33.73ms
step:564/2110 train_time:19026ms step_avg:33.73ms
step:565/2110 train_time:19059ms step_avg:33.73ms
step:566/2110 train_time:19094ms step_avg:33.73ms
step:567/2110 train_time:19126ms step_avg:33.73ms
step:568/2110 train_time:19160ms step_avg:33.73ms
step:569/2110 train_time:19191ms step_avg:33.73ms
step:570/2110 train_time:19224ms step_avg:33.73ms
step:571/2110 train_time:19257ms step_avg:33.73ms
step:572/2110 train_time:19292ms step_avg:33.73ms
step:573/2110 train_time:19325ms step_avg:33.73ms
step:574/2110 train_time:19357ms step_avg:33.72ms
step:575/2110 train_time:19389ms step_avg:33.72ms
step:576/2110 train_time:19423ms step_avg:33.72ms
step:577/2110 train_time:19455ms step_avg:33.72ms
step:578/2110 train_time:19488ms step_avg:33.72ms
step:579/2110 train_time:19521ms step_avg:33.71ms
step:580/2110 train_time:19554ms step_avg:33.71ms
step:581/2110 train_time:19587ms step_avg:33.71ms
step:582/2110 train_time:19620ms step_avg:33.71ms
step:583/2110 train_time:19652ms step_avg:33.71ms
step:584/2110 train_time:19686ms step_avg:33.71ms
step:585/2110 train_time:19718ms step_avg:33.71ms
step:586/2110 train_time:19752ms step_avg:33.71ms
step:587/2110 train_time:19784ms step_avg:33.70ms
step:588/2110 train_time:19817ms step_avg:33.70ms
step:589/2110 train_time:19850ms step_avg:33.70ms
step:590/2110 train_time:19883ms step_avg:33.70ms
step:591/2110 train_time:19915ms step_avg:33.70ms
step:592/2110 train_time:19952ms step_avg:33.70ms
step:593/2110 train_time:19983ms step_avg:33.70ms
step:594/2110 train_time:20019ms step_avg:33.70ms
step:595/2110 train_time:20053ms step_avg:33.70ms
step:596/2110 train_time:20086ms step_avg:33.70ms
step:597/2110 train_time:20115ms step_avg:33.69ms
step:598/2110 train_time:20147ms step_avg:33.69ms
step:599/2110 train_time:20180ms step_avg:33.69ms
step:600/2110 train_time:20214ms step_avg:33.69ms
step:601/2110 train_time:20246ms step_avg:33.69ms
step:602/2110 train_time:20281ms step_avg:33.69ms
step:603/2110 train_time:20312ms step_avg:33.69ms
step:604/2110 train_time:20348ms step_avg:33.69ms
step:605/2110 train_time:20381ms step_avg:33.69ms
step:606/2110 train_time:20414ms step_avg:33.69ms
step:607/2110 train_time:20445ms step_avg:33.68ms
step:608/2110 train_time:20477ms step_avg:33.68ms
step:609/2110 train_time:20510ms step_avg:33.68ms
step:610/2110 train_time:20545ms step_avg:33.68ms
step:611/2110 train_time:20576ms step_avg:33.68ms
step:612/2110 train_time:20609ms step_avg:33.68ms
step:613/2110 train_time:20642ms step_avg:33.67ms
step:614/2110 train_time:20676ms step_avg:33.67ms
step:615/2110 train_time:20708ms step_avg:33.67ms
step:616/2110 train_time:20741ms step_avg:33.67ms
step:617/2110 train_time:20774ms step_avg:33.67ms
step:618/2110 train_time:20808ms step_avg:33.67ms
step:619/2110 train_time:20841ms step_avg:33.67ms
step:620/2110 train_time:20874ms step_avg:33.67ms
step:621/2110 train_time:20907ms step_avg:33.67ms
step:622/2110 train_time:20940ms step_avg:33.67ms
step:623/2110 train_time:20973ms step_avg:33.66ms
step:624/2110 train_time:21008ms step_avg:33.67ms
step:625/2110 train_time:21039ms step_avg:33.66ms
step:626/2110 train_time:21072ms step_avg:33.66ms
step:627/2110 train_time:21105ms step_avg:33.66ms
step:628/2110 train_time:21139ms step_avg:33.66ms
step:629/2110 train_time:21172ms step_avg:33.66ms
step:630/2110 train_time:21205ms step_avg:33.66ms
step:631/2110 train_time:21237ms step_avg:33.66ms
step:632/2110 train_time:21271ms step_avg:33.66ms
step:633/2110 train_time:21303ms step_avg:33.65ms
step:634/2110 train_time:21336ms step_avg:33.65ms
step:635/2110 train_time:21369ms step_avg:33.65ms
step:636/2110 train_time:21402ms step_avg:33.65ms
step:637/2110 train_time:21435ms step_avg:33.65ms
step:638/2110 train_time:21469ms step_avg:33.65ms
step:639/2110 train_time:21503ms step_avg:33.65ms
step:640/2110 train_time:21535ms step_avg:33.65ms
step:641/2110 train_time:21567ms step_avg:33.65ms
step:642/2110 train_time:21600ms step_avg:33.65ms
step:643/2110 train_time:21633ms step_avg:33.64ms
step:644/2110 train_time:21666ms step_avg:33.64ms
step:645/2110 train_time:21698ms step_avg:33.64ms
step:646/2110 train_time:21731ms step_avg:33.64ms
step:647/2110 train_time:21764ms step_avg:33.64ms
step:648/2110 train_time:21797ms step_avg:33.64ms
step:649/2110 train_time:21830ms step_avg:33.64ms
step:650/2110 train_time:21863ms step_avg:33.64ms
step:651/2110 train_time:21896ms step_avg:33.63ms
step:652/2110 train_time:21929ms step_avg:33.63ms
step:653/2110 train_time:21962ms step_avg:33.63ms
step:654/2110 train_time:21995ms step_avg:33.63ms
step:655/2110 train_time:22028ms step_avg:33.63ms
step:656/2110 train_time:22062ms step_avg:33.63ms
step:657/2110 train_time:22094ms step_avg:33.63ms
step:658/2110 train_time:22127ms step_avg:33.63ms
step:659/2110 train_time:22160ms step_avg:33.63ms
step:660/2110 train_time:22193ms step_avg:33.63ms
step:661/2110 train_time:22227ms step_avg:33.63ms
step:662/2110 train_time:22260ms step_avg:33.63ms
step:663/2110 train_time:22293ms step_avg:33.62ms
step:664/2110 train_time:22327ms step_avg:33.63ms
step:665/2110 train_time:22359ms step_avg:33.62ms
step:666/2110 train_time:22392ms step_avg:33.62ms
step:667/2110 train_time:22425ms step_avg:33.62ms
step:668/2110 train_time:22458ms step_avg:33.62ms
step:669/2110 train_time:22490ms step_avg:33.62ms
step:670/2110 train_time:22523ms step_avg:33.62ms
step:671/2110 train_time:22556ms step_avg:33.62ms
step:672/2110 train_time:22590ms step_avg:33.62ms
step:673/2110 train_time:22622ms step_avg:33.61ms
step:674/2110 train_time:22655ms step_avg:33.61ms
step:675/2110 train_time:22688ms step_avg:33.61ms
step:676/2110 train_time:22721ms step_avg:33.61ms
step:677/2110 train_time:22754ms step_avg:33.61ms
step:678/2110 train_time:22787ms step_avg:33.61ms
step:679/2110 train_time:22821ms step_avg:33.61ms
step:680/2110 train_time:22855ms step_avg:33.61ms
step:681/2110 train_time:22887ms step_avg:33.61ms
step:682/2110 train_time:22920ms step_avg:33.61ms
step:683/2110 train_time:22953ms step_avg:33.61ms
step:684/2110 train_time:22987ms step_avg:33.61ms
step:685/2110 train_time:23019ms step_avg:33.60ms
step:686/2110 train_time:23053ms step_avg:33.60ms
step:687/2110 train_time:23085ms step_avg:33.60ms
step:688/2110 train_time:23117ms step_avg:33.60ms
step:689/2110 train_time:23151ms step_avg:33.60ms
step:690/2110 train_time:23183ms step_avg:33.60ms
step:691/2110 train_time:23217ms step_avg:33.60ms
step:692/2110 train_time:23275ms step_avg:33.63ms
step:693/2110 train_time:23335ms step_avg:33.67ms
step:694/2110 train_time:23393ms step_avg:33.71ms
step:695/2110 train_time:23454ms step_avg:33.75ms
step:696/2110 train_time:23512ms step_avg:33.78ms
step:697/2110 train_time:23572ms step_avg:33.82ms
step:698/2110 train_time:23630ms step_avg:33.85ms
step:699/2110 train_time:23691ms step_avg:33.89ms
step:700/2110 train_time:23750ms step_avg:33.93ms
step:701/2110 train_time:23810ms step_avg:33.97ms
step:702/2110 train_time:23869ms step_avg:34.00ms
step:703/2110 train_time:23928ms step_avg:34.04ms
step:704/2110 train_time:23987ms step_avg:34.07ms
step:705/2110 train_time:24047ms step_avg:34.11ms
step:706/2110 train_time:24106ms step_avg:34.14ms
step:707/2110 train_time:24165ms step_avg:34.18ms
step:708/2110 train_time:24224ms step_avg:34.21ms
step:709/2110 train_time:24283ms step_avg:34.25ms
step:710/2110 train_time:24341ms step_avg:34.28ms
step:711/2110 train_time:24400ms step_avg:34.32ms
step:712/2110 train_time:24458ms step_avg:34.35ms
step:713/2110 train_time:24518ms step_avg:34.39ms
step:714/2110 train_time:24576ms step_avg:34.42ms
step:715/2110 train_time:24637ms step_avg:34.46ms
step:716/2110 train_time:24695ms step_avg:34.49ms
step:717/2110 train_time:24756ms step_avg:34.53ms
step:718/2110 train_time:24814ms step_avg:34.56ms
step:719/2110 train_time:24875ms step_avg:34.60ms
step:720/2110 train_time:24934ms step_avg:34.63ms
step:721/2110 train_time:24994ms step_avg:34.67ms
step:722/2110 train_time:25053ms step_avg:34.70ms
step:723/2110 train_time:25113ms step_avg:34.73ms
step:724/2110 train_time:25172ms step_avg:34.77ms
step:725/2110 train_time:25232ms step_avg:34.80ms
step:726/2110 train_time:25291ms step_avg:34.84ms
step:727/2110 train_time:25351ms step_avg:34.87ms
step:728/2110 train_time:25409ms step_avg:34.90ms
step:729/2110 train_time:25468ms step_avg:34.94ms
step:730/2110 train_time:25527ms step_avg:34.97ms
step:731/2110 train_time:25586ms step_avg:35.00ms
step:732/2110 train_time:25645ms step_avg:35.03ms
step:733/2110 train_time:25704ms step_avg:35.07ms
step:734/2110 train_time:25763ms step_avg:35.10ms
step:735/2110 train_time:25823ms step_avg:35.13ms
step:736/2110 train_time:25881ms step_avg:35.16ms
step:737/2110 train_time:25941ms step_avg:35.20ms
step:738/2110 train_time:26000ms step_avg:35.23ms
step:739/2110 train_time:26060ms step_avg:35.26ms
step:740/2110 train_time:26119ms step_avg:35.30ms
step:741/2110 train_time:26179ms step_avg:35.33ms
step:742/2110 train_time:26237ms step_avg:35.36ms
step:743/2110 train_time:26297ms step_avg:35.39ms
step:744/2110 train_time:26355ms step_avg:35.42ms
step:745/2110 train_time:26415ms step_avg:35.46ms
step:746/2110 train_time:26474ms step_avg:35.49ms
step:747/2110 train_time:26534ms step_avg:35.52ms
step:748/2110 train_time:26593ms step_avg:35.55ms
step:749/2110 train_time:26653ms step_avg:35.58ms
step:750/2110 train_time:26713ms step_avg:35.62ms
step:750/2110 val_loss:3.9132 train_time:26775ms step_avg:35.70ms
step:751/2110 train_time:26805ms step_avg:35.69ms
step:752/2110 train_time:26836ms step_avg:35.69ms
step:753/2110 train_time:26896ms step_avg:35.72ms
step:754/2110 train_time:26958ms step_avg:35.75ms
step:755/2110 train_time:27019ms step_avg:35.79ms
step:756/2110 train_time:27079ms step_avg:35.82ms
step:757/2110 train_time:27139ms step_avg:35.85ms
step:758/2110 train_time:27196ms step_avg:35.88ms
step:759/2110 train_time:27255ms step_avg:35.91ms
step:760/2110 train_time:27313ms step_avg:35.94ms
step:761/2110 train_time:27372ms step_avg:35.97ms
step:762/2110 train_time:27429ms step_avg:36.00ms
step:763/2110 train_time:27488ms step_avg:36.03ms
step:764/2110 train_time:27547ms step_avg:36.06ms
step:765/2110 train_time:27606ms step_avg:36.09ms
step:766/2110 train_time:27664ms step_avg:36.11ms
step:767/2110 train_time:27724ms step_avg:36.15ms
step:768/2110 train_time:27784ms step_avg:36.18ms
step:769/2110 train_time:27845ms step_avg:36.21ms
step:770/2110 train_time:27905ms step_avg:36.24ms
step:771/2110 train_time:27966ms step_avg:36.27ms
step:772/2110 train_time:28025ms step_avg:36.30ms
step:773/2110 train_time:28088ms step_avg:36.34ms
step:774/2110 train_time:28146ms step_avg:36.36ms
step:775/2110 train_time:28207ms step_avg:36.40ms
step:776/2110 train_time:28265ms step_avg:36.42ms
step:777/2110 train_time:28324ms step_avg:36.45ms
step:778/2110 train_time:28383ms step_avg:36.48ms
step:779/2110 train_time:28442ms step_avg:36.51ms
step:780/2110 train_time:28500ms step_avg:36.54ms
step:781/2110 train_time:28559ms step_avg:36.57ms
step:782/2110 train_time:28617ms step_avg:36.59ms
step:783/2110 train_time:28676ms step_avg:36.62ms
step:784/2110 train_time:28734ms step_avg:36.65ms
step:785/2110 train_time:28795ms step_avg:36.68ms
step:786/2110 train_time:28853ms step_avg:36.71ms
step:787/2110 train_time:28913ms step_avg:36.74ms
step:788/2110 train_time:28972ms step_avg:36.77ms
step:789/2110 train_time:29034ms step_avg:36.80ms
step:790/2110 train_time:29093ms step_avg:36.83ms
step:791/2110 train_time:29152ms step_avg:36.86ms
step:792/2110 train_time:29211ms step_avg:36.88ms
step:793/2110 train_time:29271ms step_avg:36.91ms
step:794/2110 train_time:29329ms step_avg:36.94ms
step:795/2110 train_time:29389ms step_avg:36.97ms
step:796/2110 train_time:29447ms step_avg:36.99ms
step:797/2110 train_time:29507ms step_avg:37.02ms
step:798/2110 train_time:29565ms step_avg:37.05ms
step:799/2110 train_time:29624ms step_avg:37.08ms
step:800/2110 train_time:29683ms step_avg:37.10ms
step:801/2110 train_time:29744ms step_avg:37.13ms
step:802/2110 train_time:29804ms step_avg:37.16ms
step:803/2110 train_time:29864ms step_avg:37.19ms
step:804/2110 train_time:29924ms step_avg:37.22ms
step:805/2110 train_time:29985ms step_avg:37.25ms
step:806/2110 train_time:30044ms step_avg:37.28ms
step:807/2110 train_time:30104ms step_avg:37.30ms
step:808/2110 train_time:30162ms step_avg:37.33ms
step:809/2110 train_time:30223ms step_avg:37.36ms
step:810/2110 train_time:30282ms step_avg:37.38ms
step:811/2110 train_time:30341ms step_avg:37.41ms
step:812/2110 train_time:30399ms step_avg:37.44ms
step:813/2110 train_time:30459ms step_avg:37.47ms
step:814/2110 train_time:30517ms step_avg:37.49ms
step:815/2110 train_time:30577ms step_avg:37.52ms
step:816/2110 train_time:30635ms step_avg:37.54ms
step:817/2110 train_time:30695ms step_avg:37.57ms
step:818/2110 train_time:30753ms step_avg:37.59ms
step:819/2110 train_time:30812ms step_avg:37.62ms
step:820/2110 train_time:30871ms step_avg:37.65ms
step:821/2110 train_time:30932ms step_avg:37.68ms
step:822/2110 train_time:30991ms step_avg:37.70ms
step:823/2110 train_time:31051ms step_avg:37.73ms
step:824/2110 train_time:31109ms step_avg:37.75ms
step:825/2110 train_time:31169ms step_avg:37.78ms
step:826/2110 train_time:31227ms step_avg:37.81ms
step:827/2110 train_time:31287ms step_avg:37.83ms
step:828/2110 train_time:31346ms step_avg:37.86ms
step:829/2110 train_time:31405ms step_avg:37.88ms
step:830/2110 train_time:31464ms step_avg:37.91ms
step:831/2110 train_time:31523ms step_avg:37.93ms
step:832/2110 train_time:31582ms step_avg:37.96ms
step:833/2110 train_time:31641ms step_avg:37.98ms
step:834/2110 train_time:31700ms step_avg:38.01ms
step:835/2110 train_time:31760ms step_avg:38.04ms
step:836/2110 train_time:31819ms step_avg:38.06ms
step:837/2110 train_time:31880ms step_avg:38.09ms
step:838/2110 train_time:31939ms step_avg:38.11ms
step:839/2110 train_time:32000ms step_avg:38.14ms
step:840/2110 train_time:32058ms step_avg:38.16ms
step:841/2110 train_time:32118ms step_avg:38.19ms
step:842/2110 train_time:32177ms step_avg:38.21ms
step:843/2110 train_time:32236ms step_avg:38.24ms
step:844/2110 train_time:32295ms step_avg:38.26ms
step:845/2110 train_time:32354ms step_avg:38.29ms
step:846/2110 train_time:32413ms step_avg:38.31ms
step:847/2110 train_time:32473ms step_avg:38.34ms
step:848/2110 train_time:32531ms step_avg:38.36ms
step:849/2110 train_time:32591ms step_avg:38.39ms
step:850/2110 train_time:32649ms step_avg:38.41ms
step:851/2110 train_time:32709ms step_avg:38.44ms
step:852/2110 train_time:32767ms step_avg:38.46ms
step:853/2110 train_time:32828ms step_avg:38.49ms
step:854/2110 train_time:32887ms step_avg:38.51ms
step:855/2110 train_time:32947ms step_avg:38.54ms
step:856/2110 train_time:33006ms step_avg:38.56ms
step:857/2110 train_time:33066ms step_avg:38.58ms
step:858/2110 train_time:33125ms step_avg:38.61ms
step:859/2110 train_time:33185ms step_avg:38.63ms
step:860/2110 train_time:33244ms step_avg:38.66ms
step:861/2110 train_time:33304ms step_avg:38.68ms
step:862/2110 train_time:33363ms step_avg:38.70ms
step:863/2110 train_time:33423ms step_avg:38.73ms
step:864/2110 train_time:33482ms step_avg:38.75ms
step:865/2110 train_time:33542ms step_avg:38.78ms
step:866/2110 train_time:33601ms step_avg:38.80ms
step:867/2110 train_time:33660ms step_avg:38.82ms
step:868/2110 train_time:33720ms step_avg:38.85ms
step:869/2110 train_time:33779ms step_avg:38.87ms
step:870/2110 train_time:33838ms step_avg:38.89ms
step:871/2110 train_time:33898ms step_avg:38.92ms
step:872/2110 train_time:33956ms step_avg:38.94ms
step:873/2110 train_time:34016ms step_avg:38.96ms
step:874/2110 train_time:34075ms step_avg:38.99ms
step:875/2110 train_time:34134ms step_avg:39.01ms
step:876/2110 train_time:34194ms step_avg:39.03ms
step:877/2110 train_time:34253ms step_avg:39.06ms
step:878/2110 train_time:34313ms step_avg:39.08ms
step:879/2110 train_time:34371ms step_avg:39.10ms
step:880/2110 train_time:34430ms step_avg:39.12ms
step:881/2110 train_time:34490ms step_avg:39.15ms
step:882/2110 train_time:34548ms step_avg:39.17ms
step:883/2110 train_time:34608ms step_avg:39.19ms
step:884/2110 train_time:34666ms step_avg:39.22ms
step:885/2110 train_time:34727ms step_avg:39.24ms
step:886/2110 train_time:34788ms step_avg:39.26ms
step:887/2110 train_time:34847ms step_avg:39.29ms
step:888/2110 train_time:34907ms step_avg:39.31ms
step:889/2110 train_time:34967ms step_avg:39.33ms
step:890/2110 train_time:35027ms step_avg:39.36ms
step:891/2110 train_time:35086ms step_avg:39.38ms
step:892/2110 train_time:35146ms step_avg:39.40ms
step:893/2110 train_time:35206ms step_avg:39.42ms
step:894/2110 train_time:35265ms step_avg:39.45ms
step:895/2110 train_time:35324ms step_avg:39.47ms
step:896/2110 train_time:35384ms step_avg:39.49ms
step:897/2110 train_time:35443ms step_avg:39.51ms
step:898/2110 train_time:35503ms step_avg:39.54ms
step:899/2110 train_time:35561ms step_avg:39.56ms
step:900/2110 train_time:35621ms step_avg:39.58ms
step:901/2110 train_time:35680ms step_avg:39.60ms
step:902/2110 train_time:35739ms step_avg:39.62ms
step:903/2110 train_time:35798ms step_avg:39.64ms
step:904/2110 train_time:35856ms step_avg:39.66ms
step:905/2110 train_time:35916ms step_avg:39.69ms
step:906/2110 train_time:35975ms step_avg:39.71ms
step:907/2110 train_time:36034ms step_avg:39.73ms
step:908/2110 train_time:36093ms step_avg:39.75ms
step:909/2110 train_time:36153ms step_avg:39.77ms
step:910/2110 train_time:36211ms step_avg:39.79ms
step:911/2110 train_time:36271ms step_avg:39.81ms
step:912/2110 train_time:36330ms step_avg:39.84ms
step:913/2110 train_time:36389ms step_avg:39.86ms
step:914/2110 train_time:36448ms step_avg:39.88ms
step:915/2110 train_time:36508ms step_avg:39.90ms
step:916/2110 train_time:36567ms step_avg:39.92ms
step:917/2110 train_time:36627ms step_avg:39.94ms
step:918/2110 train_time:36686ms step_avg:39.96ms
step:919/2110 train_time:36746ms step_avg:39.98ms
step:920/2110 train_time:36804ms step_avg:40.00ms
step:921/2110 train_time:36865ms step_avg:40.03ms
step:922/2110 train_time:36924ms step_avg:40.05ms
step:923/2110 train_time:36983ms step_avg:40.07ms
step:924/2110 train_time:37043ms step_avg:40.09ms
step:925/2110 train_time:37104ms step_avg:40.11ms
step:926/2110 train_time:37163ms step_avg:40.13ms
step:927/2110 train_time:37223ms step_avg:40.15ms
step:928/2110 train_time:37283ms step_avg:40.18ms
step:929/2110 train_time:37342ms step_avg:40.20ms
step:930/2110 train_time:37401ms step_avg:40.22ms
step:931/2110 train_time:37461ms step_avg:40.24ms
step:932/2110 train_time:37519ms step_avg:40.26ms
step:933/2110 train_time:37578ms step_avg:40.28ms
step:934/2110 train_time:37637ms step_avg:40.30ms
step:935/2110 train_time:37696ms step_avg:40.32ms
step:936/2110 train_time:37754ms step_avg:40.34ms
step:937/2110 train_time:37813ms step_avg:40.36ms
step:938/2110 train_time:37871ms step_avg:40.37ms
step:939/2110 train_time:37931ms step_avg:40.40ms
step:940/2110 train_time:37989ms step_avg:40.41ms
step:941/2110 train_time:38049ms step_avg:40.43ms
step:942/2110 train_time:38108ms step_avg:40.45ms
step:943/2110 train_time:38167ms step_avg:40.47ms
step:944/2110 train_time:38226ms step_avg:40.49ms
step:945/2110 train_time:38286ms step_avg:40.51ms
step:946/2110 train_time:38346ms step_avg:40.53ms
step:947/2110 train_time:38405ms step_avg:40.55ms
step:948/2110 train_time:38464ms step_avg:40.57ms
step:949/2110 train_time:38524ms step_avg:40.59ms
step:950/2110 train_time:38584ms step_avg:40.61ms
step:951/2110 train_time:38643ms step_avg:40.63ms
step:952/2110 train_time:38702ms step_avg:40.65ms
step:953/2110 train_time:38762ms step_avg:40.67ms
step:954/2110 train_time:38821ms step_avg:40.69ms
step:955/2110 train_time:38880ms step_avg:40.71ms
step:956/2110 train_time:38939ms step_avg:40.73ms
step:957/2110 train_time:38999ms step_avg:40.75ms
step:958/2110 train_time:39059ms step_avg:40.77ms
step:959/2110 train_time:39118ms step_avg:40.79ms
step:960/2110 train_time:39177ms step_avg:40.81ms
step:961/2110 train_time:39236ms step_avg:40.83ms
step:962/2110 train_time:39295ms step_avg:40.85ms
step:963/2110 train_time:39354ms step_avg:40.87ms
step:964/2110 train_time:39412ms step_avg:40.88ms
step:965/2110 train_time:39473ms step_avg:40.90ms
step:966/2110 train_time:39531ms step_avg:40.92ms
step:967/2110 train_time:39591ms step_avg:40.94ms
step:968/2110 train_time:39650ms step_avg:40.96ms
step:969/2110 train_time:39710ms step_avg:40.98ms
step:970/2110 train_time:39768ms step_avg:41.00ms
step:971/2110 train_time:39829ms step_avg:41.02ms
step:972/2110 train_time:39886ms step_avg:41.04ms
step:973/2110 train_time:39947ms step_avg:41.06ms
step:974/2110 train_time:40005ms step_avg:41.07ms
step:975/2110 train_time:40066ms step_avg:41.09ms
step:976/2110 train_time:40125ms step_avg:41.11ms
step:977/2110 train_time:40185ms step_avg:41.13ms
step:978/2110 train_time:40245ms step_avg:41.15ms
step:979/2110 train_time:40305ms step_avg:41.17ms
step:980/2110 train_time:40364ms step_avg:41.19ms
step:981/2110 train_time:40424ms step_avg:41.21ms
step:982/2110 train_time:40484ms step_avg:41.23ms
step:983/2110 train_time:40544ms step_avg:41.25ms
step:984/2110 train_time:40603ms step_avg:41.26ms
step:985/2110 train_time:40662ms step_avg:41.28ms
step:986/2110 train_time:40721ms step_avg:41.30ms
step:987/2110 train_time:40780ms step_avg:41.32ms
step:988/2110 train_time:40839ms step_avg:41.33ms
step:989/2110 train_time:40898ms step_avg:41.35ms
step:990/2110 train_time:40957ms step_avg:41.37ms
step:991/2110 train_time:41017ms step_avg:41.39ms
step:992/2110 train_time:41075ms step_avg:41.41ms
step:993/2110 train_time:41134ms step_avg:41.42ms
step:994/2110 train_time:41193ms step_avg:41.44ms
step:995/2110 train_time:41252ms step_avg:41.46ms
step:996/2110 train_time:41311ms step_avg:41.48ms
step:997/2110 train_time:41370ms step_avg:41.49ms
step:998/2110 train_time:41429ms step_avg:41.51ms
step:999/2110 train_time:41489ms step_avg:41.53ms
step:1000/2110 train_time:41547ms step_avg:41.55ms
step:1000/2110 val_loss:3.7558 train_time:41609ms step_avg:41.61ms
step:1001/2110 train_time:41648ms step_avg:41.61ms
step:1002/2110 train_time:41685ms step_avg:41.60ms
step:1003/2110 train_time:41733ms step_avg:41.61ms
step:1004/2110 train_time:41795ms step_avg:41.63ms
step:1005/2110 train_time:41854ms step_avg:41.65ms
step:1006/2110 train_time:41913ms step_avg:41.66ms
step:1007/2110 train_time:41972ms step_avg:41.68ms
step:1008/2110 train_time:42030ms step_avg:41.70ms
step:1009/2110 train_time:42090ms step_avg:41.71ms
step:1010/2110 train_time:42148ms step_avg:41.73ms
step:1011/2110 train_time:42207ms step_avg:41.75ms
step:1012/2110 train_time:42265ms step_avg:41.76ms
step:1013/2110 train_time:42324ms step_avg:41.78ms
step:1014/2110 train_time:42382ms step_avg:41.80ms
step:1015/2110 train_time:42441ms step_avg:41.81ms
step:1016/2110 train_time:42499ms step_avg:41.83ms
step:1017/2110 train_time:42559ms step_avg:41.85ms
step:1018/2110 train_time:42618ms step_avg:41.86ms
step:1019/2110 train_time:42680ms step_avg:41.88ms
step:1020/2110 train_time:42741ms step_avg:41.90ms
step:1021/2110 train_time:42801ms step_avg:41.92ms
step:1022/2110 train_time:42860ms step_avg:41.94ms
step:1023/2110 train_time:42920ms step_avg:41.95ms
step:1024/2110 train_time:42978ms step_avg:41.97ms
step:1025/2110 train_time:43037ms step_avg:41.99ms
step:1026/2110 train_time:43095ms step_avg:42.00ms
step:1027/2110 train_time:43155ms step_avg:42.02ms
step:1028/2110 train_time:43213ms step_avg:42.04ms
step:1029/2110 train_time:43272ms step_avg:42.05ms
step:1030/2110 train_time:43330ms step_avg:42.07ms
step:1031/2110 train_time:43390ms step_avg:42.09ms
step:1032/2110 train_time:43448ms step_avg:42.10ms
step:1033/2110 train_time:43508ms step_avg:42.12ms
step:1034/2110 train_time:43567ms step_avg:42.13ms
step:1035/2110 train_time:43628ms step_avg:42.15ms
step:1036/2110 train_time:43688ms step_avg:42.17ms
step:1037/2110 train_time:43750ms step_avg:42.19ms
step:1038/2110 train_time:43809ms step_avg:42.21ms
step:1039/2110 train_time:43870ms step_avg:42.22ms
step:1040/2110 train_time:43930ms step_avg:42.24ms
step:1041/2110 train_time:43991ms step_avg:42.26ms
step:1042/2110 train_time:44049ms step_avg:42.27ms
step:1043/2110 train_time:44109ms step_avg:42.29ms
step:1044/2110 train_time:44167ms step_avg:42.31ms
step:1045/2110 train_time:44227ms step_avg:42.32ms
step:1046/2110 train_time:44285ms step_avg:42.34ms
step:1047/2110 train_time:44344ms step_avg:42.35ms
step:1048/2110 train_time:44402ms step_avg:42.37ms
step:1049/2110 train_time:44461ms step_avg:42.38ms
step:1050/2110 train_time:44520ms step_avg:42.40ms
step:1051/2110 train_time:44579ms step_avg:42.42ms
step:1052/2110 train_time:44639ms step_avg:42.43ms
step:1053/2110 train_time:44699ms step_avg:42.45ms
step:1054/2110 train_time:44757ms step_avg:42.46ms
step:1055/2110 train_time:44817ms step_avg:42.48ms
step:1056/2110 train_time:44876ms step_avg:42.50ms
step:1057/2110 train_time:44936ms step_avg:42.51ms
step:1058/2110 train_time:44994ms step_avg:42.53ms
step:1059/2110 train_time:45055ms step_avg:42.54ms
step:1060/2110 train_time:45113ms step_avg:42.56ms
step:1061/2110 train_time:45173ms step_avg:42.58ms
step:1062/2110 train_time:45231ms step_avg:42.59ms
step:1063/2110 train_time:45291ms step_avg:42.61ms
step:1064/2110 train_time:45349ms step_avg:42.62ms
step:1065/2110 train_time:45409ms step_avg:42.64ms
step:1066/2110 train_time:45467ms step_avg:42.65ms
step:1067/2110 train_time:45527ms step_avg:42.67ms
step:1068/2110 train_time:45588ms step_avg:42.68ms
step:1069/2110 train_time:45647ms step_avg:42.70ms
step:1070/2110 train_time:45707ms step_avg:42.72ms
step:1071/2110 train_time:45768ms step_avg:42.73ms
step:1072/2110 train_time:45828ms step_avg:42.75ms
step:1073/2110 train_time:45888ms step_avg:42.77ms
step:1074/2110 train_time:45947ms step_avg:42.78ms
step:1075/2110 train_time:46007ms step_avg:42.80ms
step:1076/2110 train_time:46065ms step_avg:42.81ms
step:1077/2110 train_time:46125ms step_avg:42.83ms
step:1078/2110 train_time:46184ms step_avg:42.84ms
step:1079/2110 train_time:46243ms step_avg:42.86ms
step:1080/2110 train_time:46301ms step_avg:42.87ms
step:1081/2110 train_time:46360ms step_avg:42.89ms
step:1082/2110 train_time:46418ms step_avg:42.90ms
step:1083/2110 train_time:46477ms step_avg:42.92ms
step:1084/2110 train_time:46535ms step_avg:42.93ms
step:1085/2110 train_time:46595ms step_avg:42.94ms
step:1086/2110 train_time:46654ms step_avg:42.96ms
step:1087/2110 train_time:46714ms step_avg:42.98ms
step:1088/2110 train_time:46773ms step_avg:42.99ms
step:1089/2110 train_time:46833ms step_avg:43.01ms
step:1090/2110 train_time:46892ms step_avg:43.02ms
step:1091/2110 train_time:46952ms step_avg:43.04ms
step:1092/2110 train_time:47011ms step_avg:43.05ms
step:1093/2110 train_time:47071ms step_avg:43.07ms
step:1094/2110 train_time:47130ms step_avg:43.08ms
step:1095/2110 train_time:47190ms step_avg:43.10ms
step:1096/2110 train_time:47250ms step_avg:43.11ms
step:1097/2110 train_time:47309ms step_avg:43.13ms
step:1098/2110 train_time:47368ms step_avg:43.14ms
step:1099/2110 train_time:47427ms step_avg:43.15ms
step:1100/2110 train_time:47486ms step_avg:43.17ms
step:1101/2110 train_time:47547ms step_avg:43.19ms
step:1102/2110 train_time:47606ms step_avg:43.20ms
step:1103/2110 train_time:47666ms step_avg:43.21ms
step:1104/2110 train_time:47725ms step_avg:43.23ms
step:1105/2110 train_time:47786ms step_avg:43.24ms
step:1106/2110 train_time:47845ms step_avg:43.26ms
step:1107/2110 train_time:47905ms step_avg:43.27ms
step:1108/2110 train_time:47964ms step_avg:43.29ms
step:1109/2110 train_time:48024ms step_avg:43.30ms
step:1110/2110 train_time:48083ms step_avg:43.32ms
step:1111/2110 train_time:48142ms step_avg:43.33ms
step:1112/2110 train_time:48201ms step_avg:43.35ms
step:1113/2110 train_time:48260ms step_avg:43.36ms
step:1114/2110 train_time:48318ms step_avg:43.37ms
step:1115/2110 train_time:48377ms step_avg:43.39ms
step:1116/2110 train_time:48436ms step_avg:43.40ms
step:1117/2110 train_time:48495ms step_avg:43.42ms
step:1118/2110 train_time:48553ms step_avg:43.43ms
step:1119/2110 train_time:48614ms step_avg:43.44ms
step:1120/2110 train_time:48672ms step_avg:43.46ms
step:1121/2110 train_time:48733ms step_avg:43.47ms
step:1122/2110 train_time:48791ms step_avg:43.49ms
step:1123/2110 train_time:48852ms step_avg:43.50ms
step:1124/2110 train_time:48910ms step_avg:43.51ms
step:1125/2110 train_time:48971ms step_avg:43.53ms
step:1126/2110 train_time:49030ms step_avg:43.54ms
step:1127/2110 train_time:49090ms step_avg:43.56ms
step:1128/2110 train_time:49150ms step_avg:43.57ms
step:1129/2110 train_time:49209ms step_avg:43.59ms
step:1130/2110 train_time:49268ms step_avg:43.60ms
step:1131/2110 train_time:49328ms step_avg:43.61ms
step:1132/2110 train_time:49387ms step_avg:43.63ms
step:1133/2110 train_time:49447ms step_avg:43.64ms
step:1134/2110 train_time:49505ms step_avg:43.66ms
step:1135/2110 train_time:49565ms step_avg:43.67ms
step:1136/2110 train_time:49625ms step_avg:43.68ms
step:1137/2110 train_time:49686ms step_avg:43.70ms
step:1138/2110 train_time:49745ms step_avg:43.71ms
step:1139/2110 train_time:49805ms step_avg:43.73ms
step:1140/2110 train_time:49864ms step_avg:43.74ms
step:1141/2110 train_time:49925ms step_avg:43.76ms
step:1142/2110 train_time:49984ms step_avg:43.77ms
step:1143/2110 train_time:50045ms step_avg:43.78ms
step:1144/2110 train_time:50105ms step_avg:43.80ms
step:1145/2110 train_time:50166ms step_avg:43.81ms
step:1146/2110 train_time:50225ms step_avg:43.83ms
step:1147/2110 train_time:50286ms step_avg:43.84ms
step:1148/2110 train_time:50347ms step_avg:43.86ms
step:1149/2110 train_time:50408ms step_avg:43.87ms
step:1150/2110 train_time:50467ms step_avg:43.88ms
step:1151/2110 train_time:50528ms step_avg:43.90ms
step:1152/2110 train_time:50587ms step_avg:43.91ms
step:1153/2110 train_time:50649ms step_avg:43.93ms
step:1154/2110 train_time:50708ms step_avg:43.94ms
step:1155/2110 train_time:50768ms step_avg:43.96ms
step:1156/2110 train_time:50828ms step_avg:43.97ms
step:1157/2110 train_time:50889ms step_avg:43.98ms
step:1158/2110 train_time:50948ms step_avg:44.00ms
step:1159/2110 train_time:51009ms step_avg:44.01ms
step:1160/2110 train_time:51069ms step_avg:44.03ms
step:1161/2110 train_time:51130ms step_avg:44.04ms
step:1162/2110 train_time:51190ms step_avg:44.05ms
step:1163/2110 train_time:51250ms step_avg:44.07ms
step:1164/2110 train_time:51310ms step_avg:44.08ms
step:1165/2110 train_time:51370ms step_avg:44.09ms
step:1166/2110 train_time:51431ms step_avg:44.11ms
step:1167/2110 train_time:51491ms step_avg:44.12ms
step:1168/2110 train_time:51550ms step_avg:44.14ms
step:1169/2110 train_time:51611ms step_avg:44.15ms
step:1170/2110 train_time:51671ms step_avg:44.16ms
step:1171/2110 train_time:51732ms step_avg:44.18ms
step:1172/2110 train_time:51791ms step_avg:44.19ms
step:1173/2110 train_time:51851ms step_avg:44.20ms
step:1174/2110 train_time:51910ms step_avg:44.22ms
step:1175/2110 train_time:51971ms step_avg:44.23ms
step:1176/2110 train_time:52030ms step_avg:44.24ms
step:1177/2110 train_time:52091ms step_avg:44.26ms
step:1178/2110 train_time:52150ms step_avg:44.27ms
step:1179/2110 train_time:52211ms step_avg:44.28ms
step:1180/2110 train_time:52270ms step_avg:44.30ms
step:1181/2110 train_time:52330ms step_avg:44.31ms
step:1182/2110 train_time:52390ms step_avg:44.32ms
step:1183/2110 train_time:52450ms step_avg:44.34ms
step:1184/2110 train_time:52510ms step_avg:44.35ms
step:1185/2110 train_time:52570ms step_avg:44.36ms
step:1186/2110 train_time:52630ms step_avg:44.38ms
step:1187/2110 train_time:52690ms step_avg:44.39ms
step:1188/2110 train_time:52750ms step_avg:44.40ms
step:1189/2110 train_time:52810ms step_avg:44.42ms
step:1190/2110 train_time:52869ms step_avg:44.43ms
step:1191/2110 train_time:52930ms step_avg:44.44ms
step:1192/2110 train_time:52990ms step_avg:44.45ms
step:1193/2110 train_time:53050ms step_avg:44.47ms
step:1194/2110 train_time:53109ms step_avg:44.48ms
step:1195/2110 train_time:53169ms step_avg:44.49ms
step:1196/2110 train_time:53230ms step_avg:44.51ms
step:1197/2110 train_time:53291ms step_avg:44.52ms
step:1198/2110 train_time:53350ms step_avg:44.53ms
step:1199/2110 train_time:53410ms step_avg:44.55ms
step:1200/2110 train_time:53470ms step_avg:44.56ms
step:1201/2110 train_time:53530ms step_avg:44.57ms
step:1202/2110 train_time:53590ms step_avg:44.58ms
step:1203/2110 train_time:53651ms step_avg:44.60ms
step:1204/2110 train_time:53711ms step_avg:44.61ms
step:1205/2110 train_time:53772ms step_avg:44.62ms
step:1206/2110 train_time:53831ms step_avg:44.64ms
step:1207/2110 train_time:53892ms step_avg:44.65ms
step:1208/2110 train_time:53951ms step_avg:44.66ms
step:1209/2110 train_time:54012ms step_avg:44.67ms
step:1210/2110 train_time:54071ms step_avg:44.69ms
step:1211/2110 train_time:54132ms step_avg:44.70ms
step:1212/2110 train_time:54192ms step_avg:44.71ms
step:1213/2110 train_time:54253ms step_avg:44.73ms
step:1214/2110 train_time:54312ms step_avg:44.74ms
step:1215/2110 train_time:54372ms step_avg:44.75ms
step:1216/2110 train_time:54432ms step_avg:44.76ms
step:1217/2110 train_time:54493ms step_avg:44.78ms
step:1218/2110 train_time:54552ms step_avg:44.79ms
step:1219/2110 train_time:54612ms step_avg:44.80ms
step:1220/2110 train_time:54672ms step_avg:44.81ms
step:1221/2110 train_time:54732ms step_avg:44.83ms
step:1222/2110 train_time:54791ms step_avg:44.84ms
step:1223/2110 train_time:54852ms step_avg:44.85ms
step:1224/2110 train_time:54911ms step_avg:44.86ms
step:1225/2110 train_time:54971ms step_avg:44.87ms
step:1226/2110 train_time:55031ms step_avg:44.89ms
step:1227/2110 train_time:55092ms step_avg:44.90ms
step:1228/2110 train_time:55151ms step_avg:44.91ms
step:1229/2110 train_time:55212ms step_avg:44.92ms
step:1230/2110 train_time:55271ms step_avg:44.94ms
step:1231/2110 train_time:55333ms step_avg:44.95ms
step:1232/2110 train_time:55392ms step_avg:44.96ms
step:1233/2110 train_time:55452ms step_avg:44.97ms
step:1234/2110 train_time:55512ms step_avg:44.99ms
step:1235/2110 train_time:55572ms step_avg:45.00ms
step:1236/2110 train_time:55631ms step_avg:45.01ms
step:1237/2110 train_time:55692ms step_avg:45.02ms
step:1238/2110 train_time:55751ms step_avg:45.03ms
step:1239/2110 train_time:55812ms step_avg:45.05ms
step:1240/2110 train_time:55871ms step_avg:45.06ms
step:1241/2110 train_time:55932ms step_avg:45.07ms
step:1242/2110 train_time:55991ms step_avg:45.08ms
step:1243/2110 train_time:56051ms step_avg:45.09ms
step:1244/2110 train_time:56110ms step_avg:45.10ms
step:1245/2110 train_time:56171ms step_avg:45.12ms
step:1246/2110 train_time:56232ms step_avg:45.13ms
step:1247/2110 train_time:56292ms step_avg:45.14ms
step:1248/2110 train_time:56351ms step_avg:45.15ms
step:1249/2110 train_time:56411ms step_avg:45.16ms
step:1250/2110 train_time:56471ms step_avg:45.18ms
step:1250/2110 val_loss:3.5919 train_time:56533ms step_avg:45.23ms
step:1251/2110 train_time:56562ms step_avg:45.21ms
step:1252/2110 train_time:56594ms step_avg:45.20ms
step:1253/2110 train_time:56657ms step_avg:45.22ms
step:1254/2110 train_time:56718ms step_avg:45.23ms
step:1255/2110 train_time:56778ms step_avg:45.24ms
step:1256/2110 train_time:56838ms step_avg:45.25ms
step:1257/2110 train_time:56897ms step_avg:45.26ms
step:1258/2110 train_time:56956ms step_avg:45.27ms
step:1259/2110 train_time:57014ms step_avg:45.29ms
step:1260/2110 train_time:57072ms step_avg:45.30ms
step:1261/2110 train_time:57131ms step_avg:45.31ms
step:1262/2110 train_time:57189ms step_avg:45.32ms
step:1263/2110 train_time:57250ms step_avg:45.33ms
step:1264/2110 train_time:57308ms step_avg:45.34ms
step:1265/2110 train_time:57368ms step_avg:45.35ms
step:1266/2110 train_time:57427ms step_avg:45.36ms
step:1267/2110 train_time:57488ms step_avg:45.37ms
step:1268/2110 train_time:57549ms step_avg:45.39ms
step:1269/2110 train_time:57611ms step_avg:45.40ms
step:1270/2110 train_time:57672ms step_avg:45.41ms
step:1271/2110 train_time:57733ms step_avg:45.42ms
step:1272/2110 train_time:57792ms step_avg:45.43ms
step:1273/2110 train_time:57852ms step_avg:45.45ms
step:1274/2110 train_time:57910ms step_avg:45.46ms
step:1275/2110 train_time:57971ms step_avg:45.47ms
step:1276/2110 train_time:58029ms step_avg:45.48ms
step:1277/2110 train_time:58089ms step_avg:45.49ms
step:1278/2110 train_time:58148ms step_avg:45.50ms
step:1279/2110 train_time:58207ms step_avg:45.51ms
step:1280/2110 train_time:58265ms step_avg:45.52ms
step:1281/2110 train_time:58325ms step_avg:45.53ms
step:1282/2110 train_time:58384ms step_avg:45.54ms
step:1283/2110 train_time:58445ms step_avg:45.55ms
step:1284/2110 train_time:58504ms step_avg:45.56ms
step:1285/2110 train_time:58566ms step_avg:45.58ms
step:1286/2110 train_time:58627ms step_avg:45.59ms
step:1287/2110 train_time:58689ms step_avg:45.60ms
step:1288/2110 train_time:58748ms step_avg:45.61ms
step:1289/2110 train_time:58809ms step_avg:45.62ms
step:1290/2110 train_time:58869ms step_avg:45.63ms
step:1291/2110 train_time:58930ms step_avg:45.65ms
step:1292/2110 train_time:58988ms step_avg:45.66ms
step:1293/2110 train_time:59049ms step_avg:45.67ms
step:1294/2110 train_time:59107ms step_avg:45.68ms
step:1295/2110 train_time:59167ms step_avg:45.69ms
step:1296/2110 train_time:59225ms step_avg:45.70ms
step:1297/2110 train_time:59285ms step_avg:45.71ms
step:1298/2110 train_time:59344ms step_avg:45.72ms
step:1299/2110 train_time:59404ms step_avg:45.73ms
step:1300/2110 train_time:59463ms step_avg:45.74ms
step:1301/2110 train_time:59523ms step_avg:45.75ms
step:1302/2110 train_time:59583ms step_avg:45.76ms
step:1303/2110 train_time:59645ms step_avg:45.78ms
step:1304/2110 train_time:59705ms step_avg:45.79ms
step:1305/2110 train_time:59766ms step_avg:45.80ms
step:1306/2110 train_time:59826ms step_avg:45.81ms
step:1307/2110 train_time:59887ms step_avg:45.82ms
step:1308/2110 train_time:59946ms step_avg:45.83ms
step:1309/2110 train_time:60006ms step_avg:45.84ms
step:1310/2110 train_time:60065ms step_avg:45.85ms
step:1311/2110 train_time:60125ms step_avg:45.86ms
step:1312/2110 train_time:60183ms step_avg:45.87ms
step:1313/2110 train_time:60243ms step_avg:45.88ms
step:1314/2110 train_time:60302ms step_avg:45.89ms
step:1315/2110 train_time:60362ms step_avg:45.90ms
step:1316/2110 train_time:60421ms step_avg:45.91ms
step:1317/2110 train_time:60481ms step_avg:45.92ms
step:1318/2110 train_time:60541ms step_avg:45.93ms
step:1319/2110 train_time:60601ms step_avg:45.95ms
step:1320/2110 train_time:60662ms step_avg:45.96ms
step:1321/2110 train_time:60724ms step_avg:45.97ms
step:1322/2110 train_time:60784ms step_avg:45.98ms
step:1323/2110 train_time:60845ms step_avg:45.99ms
step:1324/2110 train_time:60904ms step_avg:46.00ms
step:1325/2110 train_time:60964ms step_avg:46.01ms
step:1326/2110 train_time:61023ms step_avg:46.02ms
step:1327/2110 train_time:61083ms step_avg:46.03ms
step:1328/2110 train_time:61142ms step_avg:46.04ms
step:1329/2110 train_time:61202ms step_avg:46.05ms
step:1330/2110 train_time:61261ms step_avg:46.06ms
step:1331/2110 train_time:61321ms step_avg:46.07ms
step:1332/2110 train_time:61380ms step_avg:46.08ms
step:1333/2110 train_time:61440ms step_avg:46.09ms
step:1334/2110 train_time:61499ms step_avg:46.10ms
step:1335/2110 train_time:61560ms step_avg:46.11ms
step:1336/2110 train_time:61620ms step_avg:46.12ms
step:1337/2110 train_time:61681ms step_avg:46.13ms
step:1338/2110 train_time:61741ms step_avg:46.14ms
step:1339/2110 train_time:61802ms step_avg:46.16ms
step:1340/2110 train_time:61862ms step_avg:46.17ms
step:1341/2110 train_time:61923ms step_avg:46.18ms
step:1342/2110 train_time:61983ms step_avg:46.19ms
step:1343/2110 train_time:62043ms step_avg:46.20ms
step:1344/2110 train_time:62101ms step_avg:46.21ms
step:1345/2110 train_time:62161ms step_avg:46.22ms
step:1346/2110 train_time:62220ms step_avg:46.23ms
step:1347/2110 train_time:62280ms step_avg:46.24ms
step:1348/2110 train_time:62339ms step_avg:46.25ms
step:1349/2110 train_time:62398ms step_avg:46.25ms
step:1350/2110 train_time:62456ms step_avg:46.26ms
step:1351/2110 train_time:62516ms step_avg:46.27ms
step:1352/2110 train_time:62576ms step_avg:46.28ms
step:1353/2110 train_time:62636ms step_avg:46.29ms
step:1354/2110 train_time:62695ms step_avg:46.30ms
step:1355/2110 train_time:62756ms step_avg:46.31ms
step:1356/2110 train_time:62815ms step_avg:46.32ms
step:1357/2110 train_time:62876ms step_avg:46.33ms
step:1358/2110 train_time:62935ms step_avg:46.34ms
step:1359/2110 train_time:62996ms step_avg:46.35ms
step:1360/2110 train_time:63056ms step_avg:46.36ms
step:1361/2110 train_time:63115ms step_avg:46.37ms
step:1362/2110 train_time:63174ms step_avg:46.38ms
step:1363/2110 train_time:63233ms step_avg:46.39ms
step:1364/2110 train_time:63292ms step_avg:46.40ms
step:1365/2110 train_time:63351ms step_avg:46.41ms
step:1366/2110 train_time:63412ms step_avg:46.42ms
step:1367/2110 train_time:63470ms step_avg:46.43ms
step:1368/2110 train_time:63529ms step_avg:46.44ms
step:1369/2110 train_time:63590ms step_avg:46.45ms
step:1370/2110 train_time:63651ms step_avg:46.46ms
step:1371/2110 train_time:63710ms step_avg:46.47ms
step:1372/2110 train_time:63771ms step_avg:46.48ms
step:1373/2110 train_time:63831ms step_avg:46.49ms
step:1374/2110 train_time:63891ms step_avg:46.50ms
step:1375/2110 train_time:63950ms step_avg:46.51ms
step:1376/2110 train_time:64010ms step_avg:46.52ms
step:1377/2110 train_time:64069ms step_avg:46.53ms
step:1378/2110 train_time:64129ms step_avg:46.54ms
step:1379/2110 train_time:64190ms step_avg:46.55ms
step:1380/2110 train_time:64250ms step_avg:46.56ms
step:1381/2110 train_time:64309ms step_avg:46.57ms
step:1382/2110 train_time:64396ms step_avg:46.60ms
step:1383/2110 train_time:64482ms step_avg:46.62ms
step:1384/2110 train_time:64568ms step_avg:46.65ms
step:1385/2110 train_time:64654ms step_avg:46.68ms
step:1386/2110 train_time:64741ms step_avg:46.71ms
step:1387/2110 train_time:64829ms step_avg:46.74ms
step:1388/2110 train_time:64916ms step_avg:46.77ms
step:1389/2110 train_time:65005ms step_avg:46.80ms
step:1390/2110 train_time:65092ms step_avg:46.83ms
step:1391/2110 train_time:65177ms step_avg:46.86ms
step:1392/2110 train_time:65264ms step_avg:46.89ms
step:1393/2110 train_time:65348ms step_avg:46.91ms
step:1394/2110 train_time:65435ms step_avg:46.94ms
step:1395/2110 train_time:65521ms step_avg:46.97ms
step:1396/2110 train_time:65607ms step_avg:47.00ms
step:1397/2110 train_time:65693ms step_avg:47.02ms
step:1398/2110 train_time:65780ms step_avg:47.05ms
step:1399/2110 train_time:65867ms step_avg:47.08ms
step:1400/2110 train_time:65953ms step_avg:47.11ms
step:1401/2110 train_time:66041ms step_avg:47.14ms
step:1402/2110 train_time:66128ms step_avg:47.17ms
step:1403/2110 train_time:66214ms step_avg:47.19ms
step:1404/2110 train_time:66301ms step_avg:47.22ms
step:1405/2110 train_time:66387ms step_avg:47.25ms
step:1406/2110 train_time:66473ms step_avg:47.28ms
step:1407/2110 train_time:66561ms step_avg:47.31ms
step:1408/2110 train_time:66646ms step_avg:47.33ms
step:1409/2110 train_time:66733ms step_avg:47.36ms
step:1410/2110 train_time:66821ms step_avg:47.39ms
step:1411/2110 train_time:66907ms step_avg:47.42ms
step:1412/2110 train_time:66995ms step_avg:47.45ms
step:1413/2110 train_time:67081ms step_avg:47.47ms
step:1414/2110 train_time:67167ms step_avg:47.50ms
step:1415/2110 train_time:67254ms step_avg:47.53ms
step:1416/2110 train_time:67341ms step_avg:47.56ms
step:1417/2110 train_time:67427ms step_avg:47.58ms
step:1418/2110 train_time:67514ms step_avg:47.61ms
step:1419/2110 train_time:67600ms step_avg:47.64ms
step:1420/2110 train_time:67686ms step_avg:47.67ms
step:1421/2110 train_time:67773ms step_avg:47.69ms
step:1422/2110 train_time:67860ms step_avg:47.72ms
step:1423/2110 train_time:67947ms step_avg:47.75ms
step:1424/2110 train_time:68034ms step_avg:47.78ms
step:1425/2110 train_time:68121ms step_avg:47.80ms
step:1426/2110 train_time:68207ms step_avg:47.83ms
step:1427/2110 train_time:68293ms step_avg:47.86ms
step:1428/2110 train_time:68381ms step_avg:47.89ms
step:1429/2110 train_time:68466ms step_avg:47.91ms
step:1430/2110 train_time:68553ms step_avg:47.94ms
step:1431/2110 train_time:68640ms step_avg:47.97ms
step:1432/2110 train_time:68726ms step_avg:47.99ms
step:1433/2110 train_time:68812ms step_avg:48.02ms
step:1434/2110 train_time:68900ms step_avg:48.05ms
step:1435/2110 train_time:68986ms step_avg:48.07ms
step:1436/2110 train_time:69074ms step_avg:48.10ms
step:1437/2110 train_time:69161ms step_avg:48.13ms
step:1438/2110 train_time:69246ms step_avg:48.15ms
step:1439/2110 train_time:69333ms step_avg:48.18ms
step:1440/2110 train_time:69420ms step_avg:48.21ms
step:1441/2110 train_time:69506ms step_avg:48.23ms
step:1442/2110 train_time:69594ms step_avg:48.26ms
step:1443/2110 train_time:69680ms step_avg:48.29ms
step:1444/2110 train_time:69766ms step_avg:48.31ms
step:1445/2110 train_time:69852ms step_avg:48.34ms
step:1446/2110 train_time:69939ms step_avg:48.37ms
step:1447/2110 train_time:70025ms step_avg:48.39ms
step:1448/2110 train_time:70114ms step_avg:48.42ms
step:1449/2110 train_time:70201ms step_avg:48.45ms
step:1450/2110 train_time:70286ms step_avg:48.47ms
step:1451/2110 train_time:70372ms step_avg:48.50ms
step:1452/2110 train_time:70460ms step_avg:48.53ms
step:1453/2110 train_time:70545ms step_avg:48.55ms
step:1454/2110 train_time:70633ms step_avg:48.58ms
step:1455/2110 train_time:70718ms step_avg:48.60ms
step:1456/2110 train_time:70805ms step_avg:48.63ms
step:1457/2110 train_time:70891ms step_avg:48.66ms
step:1458/2110 train_time:70977ms step_avg:48.68ms
step:1459/2110 train_time:71065ms step_avg:48.71ms
step:1460/2110 train_time:71153ms step_avg:48.74ms
step:1461/2110 train_time:71239ms step_avg:48.76ms
step:1462/2110 train_time:71344ms step_avg:48.80ms
step:1463/2110 train_time:71410ms step_avg:48.81ms
step:1464/2110 train_time:71499ms step_avg:48.84ms
step:1465/2110 train_time:71585ms step_avg:48.86ms
step:1466/2110 train_time:71671ms step_avg:48.89ms
step:1467/2110 train_time:71757ms step_avg:48.91ms
step:1468/2110 train_time:71843ms step_avg:48.94ms
step:1469/2110 train_time:71929ms step_avg:48.96ms
step:1470/2110 train_time:72016ms step_avg:48.99ms
step:1471/2110 train_time:72103ms step_avg:49.02ms
step:1472/2110 train_time:72190ms step_avg:49.04ms
step:1473/2110 train_time:72276ms step_avg:49.07ms
step:1474/2110 train_time:72362ms step_avg:49.09ms
step:1475/2110 train_time:72449ms step_avg:49.12ms
step:1476/2110 train_time:72536ms step_avg:49.14ms
step:1477/2110 train_time:72624ms step_avg:49.17ms
step:1478/2110 train_time:72709ms step_avg:49.19ms
step:1479/2110 train_time:72797ms step_avg:49.22ms
step:1480/2110 train_time:72882ms step_avg:49.24ms
step:1481/2110 train_time:72969ms step_avg:49.27ms
step:1482/2110 train_time:73056ms step_avg:49.30ms
step:1483/2110 train_time:73142ms step_avg:49.32ms
step:1484/2110 train_time:73228ms step_avg:49.35ms
step:1485/2110 train_time:73315ms step_avg:49.37ms
step:1486/2110 train_time:73402ms step_avg:49.40ms
step:1487/2110 train_time:73487ms step_avg:49.42ms
step:1488/2110 train_time:73574ms step_avg:49.44ms
step:1489/2110 train_time:73661ms step_avg:49.47ms
step:1490/2110 train_time:73747ms step_avg:49.49ms
step:1491/2110 train_time:73833ms step_avg:49.52ms
step:1492/2110 train_time:73920ms step_avg:49.54ms
step:1493/2110 train_time:74006ms step_avg:49.57ms
step:1494/2110 train_time:74095ms step_avg:49.60ms
step:1495/2110 train_time:74180ms step_avg:49.62ms
step:1496/2110 train_time:74267ms step_avg:49.64ms
step:1497/2110 train_time:74353ms step_avg:49.67ms
step:1498/2110 train_time:74440ms step_avg:49.69ms
step:1499/2110 train_time:74527ms step_avg:49.72ms
step:1500/2110 train_time:74612ms step_avg:49.74ms
step:1500/2110 val_loss:3.4939 train_time:74700ms step_avg:49.80ms
step:1501/2110 train_time:74743ms step_avg:49.80ms
step:1502/2110 train_time:74791ms step_avg:49.79ms
step:1503/2110 train_time:74883ms step_avg:49.82ms
step:1504/2110 train_time:74971ms step_avg:49.85ms
step:1505/2110 train_time:75057ms step_avg:49.87ms
step:1506/2110 train_time:75144ms step_avg:49.90ms
step:1507/2110 train_time:75228ms step_avg:49.92ms
step:1508/2110 train_time:75313ms step_avg:49.94ms
step:1509/2110 train_time:75398ms step_avg:49.97ms
step:1510/2110 train_time:75483ms step_avg:49.99ms
step:1511/2110 train_time:75568ms step_avg:50.01ms
step:1512/2110 train_time:75660ms step_avg:50.04ms
step:1513/2110 train_time:75751ms step_avg:50.07ms
step:1514/2110 train_time:75838ms step_avg:50.09ms
step:1515/2110 train_time:75926ms step_avg:50.12ms
step:1516/2110 train_time:76013ms step_avg:50.14ms
step:1517/2110 train_time:76099ms step_avg:50.16ms
step:1518/2110 train_time:76185ms step_avg:50.19ms
step:1519/2110 train_time:76270ms step_avg:50.21ms
step:1520/2110 train_time:76356ms step_avg:50.23ms
step:1521/2110 train_time:76442ms step_avg:50.26ms
step:1522/2110 train_time:76527ms step_avg:50.28ms
step:1523/2110 train_time:76616ms step_avg:50.31ms
step:1524/2110 train_time:76703ms step_avg:50.33ms
step:1525/2110 train_time:76793ms step_avg:50.36ms
step:1526/2110 train_time:76880ms step_avg:50.38ms
step:1527/2110 train_time:76967ms step_avg:50.40ms
step:1528/2110 train_time:77054ms step_avg:50.43ms
step:1529/2110 train_time:77140ms step_avg:50.45ms
step:1530/2110 train_time:77226ms step_avg:50.47ms
step:1531/2110 train_time:77311ms step_avg:50.50ms
step:1532/2110 train_time:77397ms step_avg:50.52ms
step:1533/2110 train_time:77483ms step_avg:50.54ms
step:1534/2110 train_time:77568ms step_avg:50.57ms
step:1535/2110 train_time:77657ms step_avg:50.59ms
step:1536/2110 train_time:77744ms step_avg:50.61ms
step:1537/2110 train_time:77832ms step_avg:50.64ms
step:1538/2110 train_time:77919ms step_avg:50.66ms
step:1539/2110 train_time:78006ms step_avg:50.69ms
step:1540/2110 train_time:78093ms step_avg:50.71ms
step:1541/2110 train_time:78178ms step_avg:50.73ms
step:1542/2110 train_time:78265ms step_avg:50.76ms
step:1543/2110 train_time:78350ms step_avg:50.78ms
step:1544/2110 train_time:78436ms step_avg:50.80ms
step:1545/2110 train_time:78522ms step_avg:50.82ms
step:1546/2110 train_time:78608ms step_avg:50.85ms
step:1547/2110 train_time:78696ms step_avg:50.87ms
step:1548/2110 train_time:78782ms step_avg:50.89ms
step:1549/2110 train_time:78870ms step_avg:50.92ms
step:1550/2110 train_time:78957ms step_avg:50.94ms
step:1551/2110 train_time:79044ms step_avg:50.96ms
step:1552/2110 train_time:79132ms step_avg:50.99ms
step:1553/2110 train_time:79218ms step_avg:51.01ms
step:1554/2110 train_time:79305ms step_avg:51.03ms
step:1555/2110 train_time:79390ms step_avg:51.05ms
step:1556/2110 train_time:79476ms step_avg:51.08ms
step:1557/2110 train_time:79562ms step_avg:51.10ms
step:1558/2110 train_time:79648ms step_avg:51.12ms
step:1559/2110 train_time:79735ms step_avg:51.14ms
step:1560/2110 train_time:79822ms step_avg:51.17ms
step:1561/2110 train_time:79910ms step_avg:51.19ms
step:1562/2110 train_time:79997ms step_avg:51.21ms
step:1563/2110 train_time:80085ms step_avg:51.24ms
step:1564/2110 train_time:80172ms step_avg:51.26ms
step:1565/2110 train_time:80258ms step_avg:51.28ms
step:1566/2110 train_time:80344ms step_avg:51.31ms
step:1567/2110 train_time:80429ms step_avg:51.33ms
step:1568/2110 train_time:80516ms step_avg:51.35ms
step:1569/2110 train_time:80603ms step_avg:51.37ms
step:1570/2110 train_time:80688ms step_avg:51.39ms
step:1571/2110 train_time:80776ms step_avg:51.42ms
step:1572/2110 train_time:80863ms step_avg:51.44ms
step:1573/2110 train_time:80949ms step_avg:51.46ms
step:1574/2110 train_time:81036ms step_avg:51.48ms
step:1575/2110 train_time:81124ms step_avg:51.51ms
step:1576/2110 train_time:81211ms step_avg:51.53ms
step:1577/2110 train_time:81298ms step_avg:51.55ms
step:1578/2110 train_time:81384ms step_avg:51.57ms
step:1579/2110 train_time:81470ms step_avg:51.60ms
step:1580/2110 train_time:81557ms step_avg:51.62ms
step:1581/2110 train_time:81643ms step_avg:51.64ms
step:1582/2110 train_time:81729ms step_avg:51.66ms
step:1583/2110 train_time:81817ms step_avg:51.68ms
step:1584/2110 train_time:81904ms step_avg:51.71ms
step:1585/2110 train_time:81991ms step_avg:51.73ms
step:1586/2110 train_time:82077ms step_avg:51.75ms
step:1587/2110 train_time:82164ms step_avg:51.77ms
step:1588/2110 train_time:82250ms step_avg:51.79ms
step:1589/2110 train_time:82336ms step_avg:51.82ms
step:1590/2110 train_time:82424ms step_avg:51.84ms
step:1591/2110 train_time:82509ms step_avg:51.86ms
step:1592/2110 train_time:82596ms step_avg:51.88ms
step:1593/2110 train_time:82683ms step_avg:51.90ms
step:1594/2110 train_time:82769ms step_avg:51.93ms
step:1595/2110 train_time:82855ms step_avg:51.95ms
step:1596/2110 train_time:82943ms step_avg:51.97ms
step:1597/2110 train_time:83029ms step_avg:51.99ms
step:1598/2110 train_time:83117ms step_avg:52.01ms
step:1599/2110 train_time:83204ms step_avg:52.03ms
step:1600/2110 train_time:83290ms step_avg:52.06ms
step:1601/2110 train_time:83377ms step_avg:52.08ms
step:1602/2110 train_time:83463ms step_avg:52.10ms
step:1603/2110 train_time:83550ms step_avg:52.12ms
step:1604/2110 train_time:83637ms step_avg:52.14ms
step:1605/2110 train_time:83724ms step_avg:52.16ms
step:1606/2110 train_time:83810ms step_avg:52.19ms
step:1607/2110 train_time:83897ms step_avg:52.21ms
step:1608/2110 train_time:83984ms step_avg:52.23ms
step:1609/2110 train_time:84070ms step_avg:52.25ms
step:1610/2110 train_time:84157ms step_avg:52.27ms
step:1611/2110 train_time:84244ms step_avg:52.29ms
step:1612/2110 train_time:84331ms step_avg:52.31ms
step:1613/2110 train_time:84417ms step_avg:52.34ms
step:1614/2110 train_time:84503ms step_avg:52.36ms
step:1615/2110 train_time:84590ms step_avg:52.38ms
step:1616/2110 train_time:84676ms step_avg:52.40ms
step:1617/2110 train_time:84763ms step_avg:52.42ms
step:1618/2110 train_time:84849ms step_avg:52.44ms
step:1619/2110 train_time:84936ms step_avg:52.46ms
step:1620/2110 train_time:85024ms step_avg:52.48ms
step:1621/2110 train_time:85110ms step_avg:52.50ms
step:1622/2110 train_time:85196ms step_avg:52.53ms
step:1623/2110 train_time:85284ms step_avg:52.55ms
step:1624/2110 train_time:85371ms step_avg:52.57ms
step:1625/2110 train_time:85458ms step_avg:52.59ms
step:1626/2110 train_time:85546ms step_avg:52.61ms
step:1627/2110 train_time:85630ms step_avg:52.63ms
step:1628/2110 train_time:85718ms step_avg:52.65ms
step:1629/2110 train_time:85804ms step_avg:52.67ms
step:1630/2110 train_time:85891ms step_avg:52.69ms
step:1631/2110 train_time:85977ms step_avg:52.71ms
step:1632/2110 train_time:86064ms step_avg:52.74ms
step:1633/2110 train_time:86150ms step_avg:52.76ms
step:1634/2110 train_time:86237ms step_avg:52.78ms
step:1635/2110 train_time:86324ms step_avg:52.80ms
step:1636/2110 train_time:86410ms step_avg:52.82ms
step:1637/2110 train_time:86497ms step_avg:52.84ms
step:1638/2110 train_time:86583ms step_avg:52.86ms
step:1639/2110 train_time:86670ms step_avg:52.88ms
step:1640/2110 train_time:86758ms step_avg:52.90ms
step:1641/2110 train_time:86845ms step_avg:52.92ms
step:1642/2110 train_time:86931ms step_avg:52.94ms
step:1643/2110 train_time:87017ms step_avg:52.96ms
step:1644/2110 train_time:87104ms step_avg:52.98ms
step:1645/2110 train_time:87190ms step_avg:53.00ms
step:1646/2110 train_time:87277ms step_avg:53.02ms
step:1647/2110 train_time:87364ms step_avg:53.04ms
step:1648/2110 train_time:87450ms step_avg:53.06ms
step:1649/2110 train_time:87537ms step_avg:53.08ms
step:1650/2110 train_time:87623ms step_avg:53.10ms
step:1651/2110 train_time:87710ms step_avg:53.13ms
step:1652/2110 train_time:87797ms step_avg:53.15ms
step:1653/2110 train_time:87884ms step_avg:53.17ms
step:1654/2110 train_time:87970ms step_avg:53.19ms
step:1655/2110 train_time:88057ms step_avg:53.21ms
step:1656/2110 train_time:88145ms step_avg:53.23ms
step:1657/2110 train_time:88231ms step_avg:53.25ms
step:1658/2110 train_time:88318ms step_avg:53.27ms
step:1659/2110 train_time:88407ms step_avg:53.29ms
step:1660/2110 train_time:88494ms step_avg:53.31ms
step:1661/2110 train_time:88583ms step_avg:53.33ms
step:1662/2110 train_time:88669ms step_avg:53.35ms
step:1663/2110 train_time:88757ms step_avg:53.37ms
step:1664/2110 train_time:88845ms step_avg:53.39ms
step:1665/2110 train_time:88933ms step_avg:53.41ms
step:1666/2110 train_time:89021ms step_avg:53.43ms
step:1667/2110 train_time:89108ms step_avg:53.45ms
step:1668/2110 train_time:89196ms step_avg:53.48ms
step:1669/2110 train_time:89284ms step_avg:53.50ms
step:1670/2110 train_time:89372ms step_avg:53.52ms
step:1671/2110 train_time:89459ms step_avg:53.54ms
step:1672/2110 train_time:89547ms step_avg:53.56ms
step:1673/2110 train_time:89634ms step_avg:53.58ms
step:1674/2110 train_time:89722ms step_avg:53.60ms
step:1675/2110 train_time:89810ms step_avg:53.62ms
step:1676/2110 train_time:89898ms step_avg:53.64ms
step:1677/2110 train_time:89986ms step_avg:53.66ms
step:1678/2110 train_time:90073ms step_avg:53.68ms
step:1679/2110 train_time:90162ms step_avg:53.70ms
step:1680/2110 train_time:90249ms step_avg:53.72ms
step:1681/2110 train_time:90338ms step_avg:53.74ms
step:1682/2110 train_time:90426ms step_avg:53.76ms
step:1683/2110 train_time:90513ms step_avg:53.78ms
step:1684/2110 train_time:90602ms step_avg:53.80ms
step:1685/2110 train_time:90690ms step_avg:53.82ms
step:1686/2110 train_time:90777ms step_avg:53.84ms
step:1687/2110 train_time:90865ms step_avg:53.86ms
step:1688/2110 train_time:90953ms step_avg:53.88ms
step:1689/2110 train_time:91042ms step_avg:53.90ms
step:1690/2110 train_time:91129ms step_avg:53.92ms
step:1691/2110 train_time:91217ms step_avg:53.94ms
step:1692/2110 train_time:91305ms step_avg:53.96ms
step:1693/2110 train_time:91392ms step_avg:53.98ms
step:1694/2110 train_time:91479ms step_avg:54.00ms
step:1695/2110 train_time:91568ms step_avg:54.02ms
step:1696/2110 train_time:91656ms step_avg:54.04ms
step:1697/2110 train_time:91745ms step_avg:54.06ms
step:1698/2110 train_time:91833ms step_avg:54.08ms
step:1699/2110 train_time:91920ms step_avg:54.10ms
step:1700/2110 train_time:92008ms step_avg:54.12ms
step:1701/2110 train_time:92096ms step_avg:54.14ms
step:1702/2110 train_time:92185ms step_avg:54.16ms
step:1703/2110 train_time:92272ms step_avg:54.18ms
step:1704/2110 train_time:92360ms step_avg:54.20ms
step:1705/2110 train_time:92447ms step_avg:54.22ms
step:1706/2110 train_time:92536ms step_avg:54.24ms
step:1707/2110 train_time:92624ms step_avg:54.26ms
step:1708/2110 train_time:92712ms step_avg:54.28ms
step:1709/2110 train_time:92799ms step_avg:54.30ms
step:1710/2110 train_time:92887ms step_avg:54.32ms
step:1711/2110 train_time:92975ms step_avg:54.34ms
step:1712/2110 train_time:93063ms step_avg:54.36ms
step:1713/2110 train_time:93151ms step_avg:54.38ms
step:1714/2110 train_time:93240ms step_avg:54.40ms
step:1715/2110 train_time:93327ms step_avg:54.42ms
step:1716/2110 train_time:93416ms step_avg:54.44ms
step:1717/2110 train_time:93503ms step_avg:54.46ms
step:1718/2110 train_time:93591ms step_avg:54.48ms
step:1719/2110 train_time:93678ms step_avg:54.50ms
step:1720/2110 train_time:93766ms step_avg:54.51ms
step:1721/2110 train_time:93854ms step_avg:54.53ms
step:1722/2110 train_time:93942ms step_avg:54.55ms
step:1723/2110 train_time:94031ms step_avg:54.57ms
step:1724/2110 train_time:94119ms step_avg:54.59ms
step:1725/2110 train_time:94207ms step_avg:54.61ms
step:1726/2110 train_time:94295ms step_avg:54.63ms
step:1727/2110 train_time:94383ms step_avg:54.65ms
step:1728/2110 train_time:94470ms step_avg:54.67ms
step:1729/2110 train_time:94558ms step_avg:54.69ms
step:1730/2110 train_time:94646ms step_avg:54.71ms
step:1731/2110 train_time:94734ms step_avg:54.73ms
step:1732/2110 train_time:94823ms step_avg:54.75ms
step:1733/2110 train_time:94909ms step_avg:54.77ms
step:1734/2110 train_time:94998ms step_avg:54.79ms
step:1735/2110 train_time:95086ms step_avg:54.80ms
step:1736/2110 train_time:95173ms step_avg:54.82ms
step:1737/2110 train_time:95261ms step_avg:54.84ms
step:1738/2110 train_time:95349ms step_avg:54.86ms
step:1739/2110 train_time:95446ms step_avg:54.89ms
step:1740/2110 train_time:95527ms step_avg:54.90ms
step:1741/2110 train_time:95613ms step_avg:54.92ms
step:1742/2110 train_time:95701ms step_avg:54.94ms
step:1743/2110 train_time:95790ms step_avg:54.96ms
step:1744/2110 train_time:95878ms step_avg:54.98ms
step:1745/2110 train_time:95966ms step_avg:54.99ms
step:1746/2110 train_time:96054ms step_avg:55.01ms
step:1747/2110 train_time:96143ms step_avg:55.03ms
step:1748/2110 train_time:96230ms step_avg:55.05ms
step:1749/2110 train_time:96317ms step_avg:55.07ms
step:1750/2110 train_time:96405ms step_avg:55.09ms
step:1750/2110 val_loss:3.3776 train_time:96493ms step_avg:55.14ms
step:1751/2110 train_time:96538ms step_avg:55.13ms
step:1752/2110 train_time:96589ms step_avg:55.13ms
step:1753/2110 train_time:96681ms step_avg:55.15ms
step:1754/2110 train_time:96767ms step_avg:55.17ms
step:1755/2110 train_time:96854ms step_avg:55.19ms
step:1756/2110 train_time:96941ms step_avg:55.21ms
step:1757/2110 train_time:97027ms step_avg:55.22ms
step:1758/2110 train_time:97115ms step_avg:55.24ms
step:1759/2110 train_time:97200ms step_avg:55.26ms
step:1760/2110 train_time:97288ms step_avg:55.28ms
step:1761/2110 train_time:97386ms step_avg:55.30ms
step:1762/2110 train_time:97473ms step_avg:55.32ms
step:1763/2110 train_time:97563ms step_avg:55.34ms
step:1764/2110 train_time:97654ms step_avg:55.36ms
step:1765/2110 train_time:97741ms step_avg:55.38ms
step:1766/2110 train_time:97829ms step_avg:55.40ms
step:1767/2110 train_time:97915ms step_avg:55.41ms
step:1768/2110 train_time:98003ms step_avg:55.43ms
step:1769/2110 train_time:98089ms step_avg:55.45ms
step:1770/2110 train_time:98177ms step_avg:55.47ms
step:1771/2110 train_time:98262ms step_avg:55.48ms
step:1772/2110 train_time:98351ms step_avg:55.50ms
step:1773/2110 train_time:98443ms step_avg:55.52ms
step:1774/2110 train_time:98535ms step_avg:55.54ms
step:1775/2110 train_time:98624ms step_avg:55.56ms
step:1776/2110 train_time:98713ms step_avg:55.58ms
step:1777/2110 train_time:98800ms step_avg:55.60ms
step:1778/2110 train_time:98887ms step_avg:55.62ms
step:1779/2110 train_time:98974ms step_avg:55.63ms
step:1780/2110 train_time:99060ms step_avg:55.65ms
step:1781/2110 train_time:99147ms step_avg:55.67ms
step:1782/2110 train_time:99235ms step_avg:55.69ms
step:1783/2110 train_time:99323ms step_avg:55.71ms
step:1784/2110 train_time:99413ms step_avg:55.72ms
step:1785/2110 train_time:99504ms step_avg:55.74ms
step:1786/2110 train_time:99594ms step_avg:55.76ms
step:1787/2110 train_time:99685ms step_avg:55.78ms
step:1788/2110 train_time:99773ms step_avg:55.80ms
step:1789/2110 train_time:99860ms step_avg:55.82ms
step:1790/2110 train_time:99948ms step_avg:55.84ms
step:1791/2110 train_time:100035ms step_avg:55.85ms
step:1792/2110 train_time:100122ms step_avg:55.87ms
step:1793/2110 train_time:100210ms step_avg:55.89ms
step:1794/2110 train_time:100298ms step_avg:55.91ms
step:1795/2110 train_time:100387ms step_avg:55.93ms
step:1796/2110 train_time:100475ms step_avg:55.94ms
step:1797/2110 train_time:100564ms step_avg:55.96ms
step:1798/2110 train_time:100655ms step_avg:55.98ms
step:1799/2110 train_time:100742ms step_avg:56.00ms
step:1800/2110 train_time:100831ms step_avg:56.02ms
step:1801/2110 train_time:100917ms step_avg:56.03ms
step:1802/2110 train_time:101004ms step_avg:56.05ms
step:1803/2110 train_time:101091ms step_avg:56.07ms
step:1804/2110 train_time:101178ms step_avg:56.09ms
step:1805/2110 train_time:101267ms step_avg:56.10ms
step:1806/2110 train_time:101355ms step_avg:56.12ms
step:1807/2110 train_time:101443ms step_avg:56.14ms
step:1808/2110 train_time:101533ms step_avg:56.16ms
step:1809/2110 train_time:101621ms step_avg:56.18ms
step:1810/2110 train_time:101711ms step_avg:56.19ms
step:1811/2110 train_time:101798ms step_avg:56.21ms
step:1812/2110 train_time:101886ms step_avg:56.23ms
step:1813/2110 train_time:101972ms step_avg:56.25ms
step:1814/2110 train_time:102062ms step_avg:56.26ms
step:1815/2110 train_time:102148ms step_avg:56.28ms
step:1816/2110 train_time:102236ms step_avg:56.30ms
step:1817/2110 train_time:102324ms step_avg:56.31ms
step:1818/2110 train_time:102415ms step_avg:56.33ms
step:1819/2110 train_time:102501ms step_avg:56.35ms
step:1820/2110 train_time:102589ms step_avg:56.37ms
step:1821/2110 train_time:102678ms step_avg:56.39ms
step:1822/2110 train_time:102767ms step_avg:56.40ms
step:1823/2110 train_time:102853ms step_avg:56.42ms
step:1824/2110 train_time:102942ms step_avg:56.44ms
step:1825/2110 train_time:103029ms step_avg:56.45ms
step:1826/2110 train_time:103117ms step_avg:56.47ms
step:1827/2110 train_time:103204ms step_avg:56.49ms
step:1828/2110 train_time:103293ms step_avg:56.51ms
step:1829/2110 train_time:103380ms step_avg:56.52ms
step:1830/2110 train_time:103468ms step_avg:56.54ms
step:1831/2110 train_time:103556ms step_avg:56.56ms
step:1832/2110 train_time:103646ms step_avg:56.58ms
step:1833/2110 train_time:103734ms step_avg:56.59ms
step:1834/2110 train_time:103822ms step_avg:56.61ms
step:1835/2110 train_time:103910ms step_avg:56.63ms
step:1836/2110 train_time:103998ms step_avg:56.64ms
step:1837/2110 train_time:104086ms step_avg:56.66ms
step:1838/2110 train_time:104174ms step_avg:56.68ms
step:1839/2110 train_time:104261ms step_avg:56.69ms
step:1840/2110 train_time:104351ms step_avg:56.71ms
step:1841/2110 train_time:104437ms step_avg:56.73ms
step:1842/2110 train_time:104526ms step_avg:56.75ms
step:1843/2110 train_time:104615ms step_avg:56.76ms
step:1844/2110 train_time:104705ms step_avg:56.78ms
step:1845/2110 train_time:104793ms step_avg:56.80ms
step:1846/2110 train_time:104880ms step_avg:56.81ms
step:1847/2110 train_time:104968ms step_avg:56.83ms
step:1848/2110 train_time:105056ms step_avg:56.85ms
step:1849/2110 train_time:105143ms step_avg:56.86ms
step:1850/2110 train_time:105231ms step_avg:56.88ms
step:1851/2110 train_time:105318ms step_avg:56.90ms
step:1852/2110 train_time:105408ms step_avg:56.92ms
step:1853/2110 train_time:105495ms step_avg:56.93ms
step:1854/2110 train_time:105585ms step_avg:56.95ms
step:1855/2110 train_time:105675ms step_avg:56.97ms
step:1856/2110 train_time:105763ms step_avg:56.98ms
step:1857/2110 train_time:105851ms step_avg:57.00ms
step:1858/2110 train_time:105938ms step_avg:57.02ms
step:1859/2110 train_time:106027ms step_avg:57.03ms
step:1860/2110 train_time:106126ms step_avg:57.06ms
step:1861/2110 train_time:106205ms step_avg:57.07ms
step:1862/2110 train_time:106299ms step_avg:57.09ms
step:1863/2110 train_time:106378ms step_avg:57.10ms
step:1864/2110 train_time:106468ms step_avg:57.12ms
step:1865/2110 train_time:106555ms step_avg:57.13ms
step:1866/2110 train_time:106645ms step_avg:57.15ms
step:1867/2110 train_time:106732ms step_avg:57.17ms
step:1868/2110 train_time:106820ms step_avg:57.18ms
step:1869/2110 train_time:106907ms step_avg:57.20ms
step:1870/2110 train_time:106995ms step_avg:57.22ms
step:1871/2110 train_time:107083ms step_avg:57.23ms
step:1872/2110 train_time:107171ms step_avg:57.25ms
step:1873/2110 train_time:107259ms step_avg:57.27ms
step:1874/2110 train_time:107347ms step_avg:57.28ms
step:1875/2110 train_time:107435ms step_avg:57.30ms
step:1876/2110 train_time:107524ms step_avg:57.32ms
step:1877/2110 train_time:107613ms step_avg:57.33ms
step:1878/2110 train_time:107701ms step_avg:57.35ms
step:1879/2110 train_time:107789ms step_avg:57.37ms
step:1880/2110 train_time:107877ms step_avg:57.38ms
step:1881/2110 train_time:107964ms step_avg:57.40ms
step:1882/2110 train_time:108053ms step_avg:57.41ms
step:1883/2110 train_time:108140ms step_avg:57.43ms
step:1884/2110 train_time:108228ms step_avg:57.45ms
step:1885/2110 train_time:108315ms step_avg:57.46ms
step:1886/2110 train_time:108403ms step_avg:57.48ms
step:1887/2110 train_time:108491ms step_avg:57.49ms
step:1888/2110 train_time:108580ms step_avg:57.51ms
step:1889/2110 train_time:108668ms step_avg:57.53ms
step:1890/2110 train_time:108756ms step_avg:57.54ms
step:1891/2110 train_time:108843ms step_avg:57.56ms
step:1892/2110 train_time:108933ms step_avg:57.58ms
step:1893/2110 train_time:109020ms step_avg:57.59ms
step:1894/2110 train_time:109109ms step_avg:57.61ms
step:1895/2110 train_time:109196ms step_avg:57.62ms
step:1896/2110 train_time:109286ms step_avg:57.64ms
step:1897/2110 train_time:109374ms step_avg:57.66ms
step:1898/2110 train_time:109463ms step_avg:57.67ms
step:1899/2110 train_time:109552ms step_avg:57.69ms
step:1900/2110 train_time:109640ms step_avg:57.71ms
step:1901/2110 train_time:109728ms step_avg:57.72ms
step:1902/2110 train_time:109816ms step_avg:57.74ms
step:1903/2110 train_time:109904ms step_avg:57.75ms
step:1904/2110 train_time:109992ms step_avg:57.77ms
step:1905/2110 train_time:110079ms step_avg:57.78ms
step:1906/2110 train_time:110167ms step_avg:57.80ms
step:1907/2110 train_time:110256ms step_avg:57.82ms
step:1908/2110 train_time:110344ms step_avg:57.83ms
step:1909/2110 train_time:110433ms step_avg:57.85ms
step:1910/2110 train_time:110521ms step_avg:57.86ms
step:1911/2110 train_time:110609ms step_avg:57.88ms
step:1912/2110 train_time:110697ms step_avg:57.90ms
step:1913/2110 train_time:110785ms step_avg:57.91ms
step:1914/2110 train_time:110873ms step_avg:57.93ms
step:1915/2110 train_time:110960ms step_avg:57.94ms
step:1916/2110 train_time:111049ms step_avg:57.96ms
step:1917/2110 train_time:111136ms step_avg:57.97ms
step:1918/2110 train_time:111242ms step_avg:58.00ms
step:1919/2110 train_time:111314ms step_avg:58.01ms
step:1920/2110 train_time:111404ms step_avg:58.02ms
step:1921/2110 train_time:111491ms step_avg:58.04ms
step:1922/2110 train_time:111579ms step_avg:58.05ms
step:1923/2110 train_time:111667ms step_avg:58.07ms
step:1924/2110 train_time:111755ms step_avg:58.08ms
step:1925/2110 train_time:111842ms step_avg:58.10ms
step:1926/2110 train_time:111931ms step_avg:58.12ms
step:1927/2110 train_time:112018ms step_avg:58.13ms
step:1928/2110 train_time:112107ms step_avg:58.15ms
step:1929/2110 train_time:112195ms step_avg:58.16ms
step:1930/2110 train_time:112284ms step_avg:58.18ms
step:1931/2110 train_time:112372ms step_avg:58.19ms
step:1932/2110 train_time:112460ms step_avg:58.21ms
step:1933/2110 train_time:112549ms step_avg:58.22ms
step:1934/2110 train_time:112637ms step_avg:58.24ms
step:1935/2110 train_time:112726ms step_avg:58.26ms
step:1936/2110 train_time:112815ms step_avg:58.27ms
step:1937/2110 train_time:112901ms step_avg:58.29ms
step:1938/2110 train_time:112990ms step_avg:58.30ms
step:1939/2110 train_time:113078ms step_avg:58.32ms
step:1940/2110 train_time:113166ms step_avg:58.33ms
step:1941/2110 train_time:113255ms step_avg:58.35ms
step:1942/2110 train_time:113343ms step_avg:58.36ms
step:1943/2110 train_time:113431ms step_avg:58.38ms
step:1944/2110 train_time:113520ms step_avg:58.39ms
step:1945/2110 train_time:113607ms step_avg:58.41ms
step:1946/2110 train_time:113696ms step_avg:58.43ms
step:1947/2110 train_time:113784ms step_avg:58.44ms
step:1948/2110 train_time:113873ms step_avg:58.46ms
step:1949/2110 train_time:113961ms step_avg:58.47ms
step:1950/2110 train_time:114051ms step_avg:58.49ms
step:1951/2110 train_time:114137ms step_avg:58.50ms
step:1952/2110 train_time:114226ms step_avg:58.52ms
step:1953/2110 train_time:114313ms step_avg:58.53ms
step:1954/2110 train_time:114402ms step_avg:58.55ms
step:1955/2110 train_time:114490ms step_avg:58.56ms
step:1956/2110 train_time:114578ms step_avg:58.58ms
step:1957/2110 train_time:114665ms step_avg:58.59ms
step:1958/2110 train_time:114754ms step_avg:58.61ms
step:1959/2110 train_time:114842ms step_avg:58.62ms
step:1960/2110 train_time:114930ms step_avg:58.64ms
step:1961/2110 train_time:115017ms step_avg:58.65ms
step:1962/2110 train_time:115105ms step_avg:58.67ms
step:1963/2110 train_time:115193ms step_avg:58.68ms
step:1964/2110 train_time:115282ms step_avg:58.70ms
step:1965/2110 train_time:115370ms step_avg:58.71ms
step:1966/2110 train_time:115458ms step_avg:58.73ms
step:1967/2110 train_time:115546ms step_avg:58.74ms
step:1968/2110 train_time:115634ms step_avg:58.76ms
step:1969/2110 train_time:115723ms step_avg:58.77ms
step:1970/2110 train_time:115812ms step_avg:58.79ms
step:1971/2110 train_time:115900ms step_avg:58.80ms
step:1972/2110 train_time:115989ms step_avg:58.82ms
step:1973/2110 train_time:116076ms step_avg:58.83ms
step:1974/2110 train_time:116165ms step_avg:58.85ms
step:1975/2110 train_time:116254ms step_avg:58.86ms
step:1976/2110 train_time:116342ms step_avg:58.88ms
step:1977/2110 train_time:116430ms step_avg:58.89ms
step:1978/2110 train_time:116518ms step_avg:58.91ms
step:1979/2110 train_time:116606ms step_avg:58.92ms
step:1980/2110 train_time:116694ms step_avg:58.94ms
step:1981/2110 train_time:116782ms step_avg:58.95ms
step:1982/2110 train_time:116871ms step_avg:58.97ms
step:1983/2110 train_time:116958ms step_avg:58.98ms
step:1984/2110 train_time:117047ms step_avg:59.00ms
step:1985/2110 train_time:117135ms step_avg:59.01ms
step:1986/2110 train_time:117224ms step_avg:59.03ms
step:1987/2110 train_time:117312ms step_avg:59.04ms
step:1988/2110 train_time:117400ms step_avg:59.05ms
step:1989/2110 train_time:117487ms step_avg:59.07ms
step:1990/2110 train_time:117575ms step_avg:59.08ms
step:1991/2110 train_time:117662ms step_avg:59.10ms
step:1992/2110 train_time:117752ms step_avg:59.11ms
step:1993/2110 train_time:117839ms step_avg:59.13ms
step:1994/2110 train_time:117927ms step_avg:59.14ms
step:1995/2110 train_time:118015ms step_avg:59.16ms
step:1996/2110 train_time:118103ms step_avg:59.17ms
step:1997/2110 train_time:118191ms step_avg:59.18ms
step:1998/2110 train_time:118279ms step_avg:59.20ms
step:1999/2110 train_time:118366ms step_avg:59.21ms
step:2000/2110 train_time:118455ms step_avg:59.23ms
step:2000/2110 val_loss:3.3024 train_time:118543ms step_avg:59.27ms
step:2001/2110 train_time:118587ms step_avg:59.26ms
step:2002/2110 train_time:118638ms step_avg:59.26ms
step:2003/2110 train_time:118729ms step_avg:59.28ms
step:2004/2110 train_time:118818ms step_avg:59.29ms
step:2005/2110 train_time:118906ms step_avg:59.30ms
step:2006/2110 train_time:118993ms step_avg:59.32ms
step:2007/2110 train_time:119080ms step_avg:59.33ms
step:2008/2110 train_time:119167ms step_avg:59.35ms
step:2009/2110 train_time:119253ms step_avg:59.36ms
step:2010/2110 train_time:119342ms step_avg:59.37ms
step:2011/2110 train_time:119428ms step_avg:59.39ms
step:2012/2110 train_time:119518ms step_avg:59.40ms
step:2013/2110 train_time:119611ms step_avg:59.42ms
step:2014/2110 train_time:119701ms step_avg:59.43ms
step:2015/2110 train_time:119788ms step_avg:59.45ms
step:2016/2110 train_time:119877ms step_avg:59.46ms
step:2017/2110 train_time:119964ms step_avg:59.48ms
step:2018/2110 train_time:120052ms step_avg:59.49ms
step:2019/2110 train_time:120139ms step_avg:59.50ms
step:2020/2110 train_time:120226ms step_avg:59.52ms
step:2021/2110 train_time:120313ms step_avg:59.53ms
step:2022/2110 train_time:120401ms step_avg:59.55ms
step:2023/2110 train_time:120489ms step_avg:59.56ms
step:2024/2110 train_time:120580ms step_avg:59.57ms
step:2025/2110 train_time:120669ms step_avg:59.59ms
step:2026/2110 train_time:120758ms step_avg:59.60ms
step:2027/2110 train_time:120847ms step_avg:59.62ms
step:2028/2110 train_time:120935ms step_avg:59.63ms
step:2029/2110 train_time:121023ms step_avg:59.65ms
step:2030/2110 train_time:121111ms step_avg:59.66ms
step:2031/2110 train_time:121201ms step_avg:59.68ms
step:2032/2110 train_time:121285ms step_avg:59.69ms
step:2033/2110 train_time:121372ms step_avg:59.70ms
step:2034/2110 train_time:121463ms step_avg:59.72ms
step:2035/2110 train_time:121550ms step_avg:59.73ms
step:2036/2110 train_time:121639ms step_avg:59.74ms
step:2037/2110 train_time:121728ms step_avg:59.76ms
step:2038/2110 train_time:121817ms step_avg:59.77ms
step:2039/2110 train_time:121905ms step_avg:59.79ms
step:2040/2110 train_time:121993ms step_avg:59.80ms
step:2041/2110 train_time:122080ms step_avg:59.81ms
step:2042/2110 train_time:122168ms step_avg:59.83ms
step:2043/2110 train_time:122254ms step_avg:59.84ms
step:2044/2110 train_time:122343ms step_avg:59.85ms
step:2045/2110 train_time:122430ms step_avg:59.87ms
step:2046/2110 train_time:122518ms step_avg:59.88ms
step:2047/2110 train_time:122607ms step_avg:59.90ms
step:2048/2110 train_time:122696ms step_avg:59.91ms
step:2049/2110 train_time:122785ms step_avg:59.92ms
step:2050/2110 train_time:122873ms step_avg:59.94ms
step:2051/2110 train_time:122962ms step_avg:59.95ms
step:2052/2110 train_time:123050ms step_avg:59.97ms
step:2053/2110 train_time:123138ms step_avg:59.98ms
step:2054/2110 train_time:123226ms step_avg:59.99ms
step:2055/2110 train_time:123312ms step_avg:60.01ms
step:2056/2110 train_time:123400ms step_avg:60.02ms
step:2057/2110 train_time:123488ms step_avg:60.03ms
step:2058/2110 train_time:123576ms step_avg:60.05ms
step:2059/2110 train_time:123666ms step_avg:60.06ms
step:2060/2110 train_time:123756ms step_avg:60.08ms
step:2061/2110 train_time:123845ms step_avg:60.09ms
step:2062/2110 train_time:123933ms step_avg:60.10ms
step:2063/2110 train_time:124021ms step_avg:60.12ms
step:2064/2110 train_time:124108ms step_avg:60.13ms
step:2065/2110 train_time:124196ms step_avg:60.14ms
step:2066/2110 train_time:124284ms step_avg:60.16ms
step:2067/2110 train_time:124371ms step_avg:60.17ms
step:2068/2110 train_time:124459ms step_avg:60.18ms
step:2069/2110 train_time:124547ms step_avg:60.20ms
step:2070/2110 train_time:124637ms step_avg:60.21ms
step:2071/2110 train_time:124725ms step_avg:60.22ms
step:2072/2110 train_time:124815ms step_avg:60.24ms
step:2073/2110 train_time:124903ms step_avg:60.25ms
step:2074/2110 train_time:124991ms step_avg:60.27ms
step:2075/2110 train_time:125079ms step_avg:60.28ms
step:2076/2110 train_time:125167ms step_avg:60.29ms
step:2077/2110 train_time:125256ms step_avg:60.31ms
step:2078/2110 train_time:125355ms step_avg:60.32ms
step:2079/2110 train_time:125431ms step_avg:60.33ms
step:2080/2110 train_time:125521ms step_avg:60.35ms
step:2081/2110 train_time:125609ms step_avg:60.36ms
step:2082/2110 train_time:125696ms step_avg:60.37ms
step:2083/2110 train_time:125788ms step_avg:60.39ms
step:2084/2110 train_time:125875ms step_avg:60.40ms
step:2085/2110 train_time:125963ms step_avg:60.41ms
step:2086/2110 train_time:126055ms step_avg:60.43ms
step:2087/2110 train_time:126139ms step_avg:60.44ms
step:2088/2110 train_time:126227ms step_avg:60.45ms
step:2089/2110 train_time:126317ms step_avg:60.47ms
step:2090/2110 train_time:126404ms step_avg:60.48ms
step:2091/2110 train_time:126492ms step_avg:60.49ms
step:2092/2110 train_time:126581ms step_avg:60.51ms
step:2093/2110 train_time:126668ms step_avg:60.52ms
step:2094/2110 train_time:126757ms step_avg:60.53ms
step:2095/2110 train_time:126847ms step_avg:60.55ms
step:2096/2110 train_time:126937ms step_avg:60.56ms
step:2097/2110 train_time:127025ms step_avg:60.57ms
step:2098/2110 train_time:127113ms step_avg:60.59ms
step:2099/2110 train_time:127202ms step_avg:60.60ms
step:2100/2110 train_time:127289ms step_avg:60.61ms
step:2101/2110 train_time:127378ms step_avg:60.63ms
step:2102/2110 train_time:127466ms step_avg:60.64ms
step:2103/2110 train_time:127554ms step_avg:60.65ms
step:2104/2110 train_time:127644ms step_avg:60.67ms
step:2105/2110 train_time:127731ms step_avg:60.68ms
step:2106/2110 train_time:127820ms step_avg:60.69ms
step:2107/2110 train_time:127908ms step_avg:60.71ms
step:2108/2110 train_time:127997ms step_avg:60.72ms
step:2109/2110 train_time:128087ms step_avg:60.73ms
step:2110/2110 train_time:128176ms step_avg:60.75ms
step:2110/2110 val_loss:3.2782 train_time:128266ms step_avg:60.79ms
peak memory allocated: 29244 MiB reserved: 43716 MiB

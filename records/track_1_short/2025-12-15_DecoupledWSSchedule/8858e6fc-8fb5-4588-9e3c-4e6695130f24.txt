import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Tue Dec 16 00:51:50 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   39C    P0            127W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   42C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   43C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   42C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   42C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   38C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              80      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              81      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:113ms step_avg:112.82ms
step:2/2110 train_time:150ms step_avg:75.23ms
step:3/2110 train_time:185ms step_avg:61.63ms
step:4/2110 train_time:221ms step_avg:55.21ms
step:5/2110 train_time:252ms step_avg:50.36ms
step:6/2110 train_time:454ms step_avg:75.63ms
step:7/2110 train_time:620ms step_avg:88.64ms
step:8/2110 train_time:655ms step_avg:81.87ms
step:9/2110 train_time:686ms step_avg:76.24ms
step:10/2110 train_time:720ms step_avg:72.05ms
step:11/2110 train_time:753ms step_avg:68.46ms
step:12/2110 train_time:789ms step_avg:65.76ms
step:13/2110 train_time:823ms step_avg:63.31ms
step:14/2110 train_time:858ms step_avg:61.28ms
step:15/2110 train_time:890ms step_avg:59.35ms
step:16/2110 train_time:924ms step_avg:57.78ms
step:17/2110 train_time:954ms step_avg:56.13ms
step:18/2110 train_time:987ms step_avg:54.81ms
step:19/2110 train_time:1019ms step_avg:53.64ms
step:20/2110 train_time:1052ms step_avg:52.60ms
step:21/2110 train_time:1081ms step_avg:51.46ms
step:22/2110 train_time:1116ms step_avg:50.74ms
step:23/2110 train_time:1146ms step_avg:49.84ms
step:24/2110 train_time:1179ms step_avg:49.14ms
step:25/2110 train_time:1212ms step_avg:48.48ms
step:26/2110 train_time:1247ms step_avg:47.97ms
step:27/2110 train_time:1279ms step_avg:47.36ms
step:28/2110 train_time:1313ms step_avg:46.91ms
step:29/2110 train_time:1344ms step_avg:46.36ms
step:30/2110 train_time:1379ms step_avg:45.96ms
step:31/2110 train_time:1410ms step_avg:45.48ms
step:32/2110 train_time:1444ms step_avg:45.13ms
step:33/2110 train_time:1476ms step_avg:44.74ms
step:34/2110 train_time:1510ms step_avg:44.40ms
step:35/2110 train_time:1544ms step_avg:44.11ms
step:36/2110 train_time:1580ms step_avg:43.88ms
step:37/2110 train_time:1612ms step_avg:43.56ms
step:38/2110 train_time:1646ms step_avg:43.32ms
step:39/2110 train_time:1679ms step_avg:43.06ms
step:40/2110 train_time:1713ms step_avg:42.83ms
step:41/2110 train_time:1746ms step_avg:42.59ms
step:42/2110 train_time:1780ms step_avg:42.37ms
step:43/2110 train_time:1813ms step_avg:42.15ms
step:44/2110 train_time:1846ms step_avg:41.95ms
step:45/2110 train_time:1880ms step_avg:41.78ms
step:46/2110 train_time:1913ms step_avg:41.59ms
step:47/2110 train_time:1946ms step_avg:41.41ms
step:48/2110 train_time:1979ms step_avg:41.24ms
step:49/2110 train_time:2011ms step_avg:41.04ms
step:50/2110 train_time:2046ms step_avg:40.92ms
step:51/2110 train_time:2077ms step_avg:40.73ms
step:52/2110 train_time:2111ms step_avg:40.60ms
step:53/2110 train_time:2144ms step_avg:40.45ms
step:54/2110 train_time:2178ms step_avg:40.34ms
step:55/2110 train_time:2209ms step_avg:40.17ms
step:56/2110 train_time:2243ms step_avg:40.05ms
step:57/2110 train_time:2276ms step_avg:39.93ms
step:58/2110 train_time:2310ms step_avg:39.83ms
step:59/2110 train_time:2342ms step_avg:39.69ms
step:60/2110 train_time:2374ms step_avg:39.57ms
step:61/2110 train_time:2407ms step_avg:39.47ms
step:62/2110 train_time:2442ms step_avg:39.39ms
step:63/2110 train_time:2474ms step_avg:39.27ms
step:64/2110 train_time:2509ms step_avg:39.20ms
step:65/2110 train_time:2541ms step_avg:39.09ms
step:66/2110 train_time:2575ms step_avg:39.01ms
step:67/2110 train_time:2608ms step_avg:38.92ms
step:68/2110 train_time:2643ms step_avg:38.86ms
step:69/2110 train_time:2675ms step_avg:38.76ms
step:70/2110 train_time:2708ms step_avg:38.69ms
step:71/2110 train_time:2741ms step_avg:38.60ms
step:72/2110 train_time:2774ms step_avg:38.53ms
step:73/2110 train_time:2807ms step_avg:38.45ms
step:74/2110 train_time:2842ms step_avg:38.40ms
step:75/2110 train_time:2875ms step_avg:38.33ms
step:76/2110 train_time:2913ms step_avg:38.33ms
step:77/2110 train_time:2946ms step_avg:38.26ms
step:78/2110 train_time:2977ms step_avg:38.16ms
step:79/2110 train_time:3009ms step_avg:38.09ms
step:80/2110 train_time:3043ms step_avg:38.04ms
step:81/2110 train_time:3074ms step_avg:37.95ms
step:82/2110 train_time:3106ms step_avg:37.88ms
step:83/2110 train_time:3138ms step_avg:37.81ms
step:84/2110 train_time:3172ms step_avg:37.76ms
step:85/2110 train_time:3204ms step_avg:37.69ms
step:86/2110 train_time:3238ms step_avg:37.66ms
step:87/2110 train_time:3270ms step_avg:37.59ms
step:88/2110 train_time:3302ms step_avg:37.53ms
step:89/2110 train_time:3336ms step_avg:37.48ms
step:90/2110 train_time:3370ms step_avg:37.44ms
step:91/2110 train_time:3402ms step_avg:37.38ms
step:92/2110 train_time:3436ms step_avg:37.35ms
step:93/2110 train_time:3468ms step_avg:37.29ms
step:94/2110 train_time:3503ms step_avg:37.26ms
step:95/2110 train_time:3535ms step_avg:37.21ms
step:96/2110 train_time:3567ms step_avg:37.16ms
step:97/2110 train_time:3599ms step_avg:37.11ms
step:98/2110 train_time:3636ms step_avg:37.10ms
step:99/2110 train_time:3667ms step_avg:37.04ms
step:100/2110 train_time:3700ms step_avg:37.00ms
step:101/2110 train_time:3734ms step_avg:36.97ms
step:102/2110 train_time:3767ms step_avg:36.93ms
step:103/2110 train_time:3799ms step_avg:36.88ms
step:104/2110 train_time:3833ms step_avg:36.86ms
step:105/2110 train_time:3865ms step_avg:36.81ms
step:106/2110 train_time:3899ms step_avg:36.78ms
step:107/2110 train_time:3932ms step_avg:36.74ms
step:108/2110 train_time:3964ms step_avg:36.71ms
step:109/2110 train_time:3997ms step_avg:36.67ms
step:110/2110 train_time:4031ms step_avg:36.65ms
step:111/2110 train_time:4063ms step_avg:36.61ms
step:112/2110 train_time:4098ms step_avg:36.59ms
step:113/2110 train_time:4129ms step_avg:36.54ms
step:114/2110 train_time:4163ms step_avg:36.52ms
step:115/2110 train_time:4195ms step_avg:36.48ms
step:116/2110 train_time:4229ms step_avg:36.45ms
step:117/2110 train_time:4261ms step_avg:36.41ms
step:118/2110 train_time:4295ms step_avg:36.39ms
step:119/2110 train_time:4327ms step_avg:36.36ms
step:120/2110 train_time:4362ms step_avg:36.35ms
step:121/2110 train_time:4394ms step_avg:36.31ms
step:122/2110 train_time:4427ms step_avg:36.29ms
step:123/2110 train_time:4458ms step_avg:36.25ms
step:124/2110 train_time:4494ms step_avg:36.24ms
step:125/2110 train_time:4528ms step_avg:36.22ms
step:126/2110 train_time:4561ms step_avg:36.20ms
step:127/2110 train_time:4593ms step_avg:36.16ms
step:128/2110 train_time:4627ms step_avg:36.15ms
step:129/2110 train_time:4658ms step_avg:36.11ms
step:130/2110 train_time:4692ms step_avg:36.09ms
step:131/2110 train_time:4723ms step_avg:36.05ms
step:132/2110 train_time:4757ms step_avg:36.04ms
step:133/2110 train_time:4789ms step_avg:36.01ms
step:134/2110 train_time:4823ms step_avg:36.00ms
step:135/2110 train_time:4856ms step_avg:35.97ms
step:136/2110 train_time:4888ms step_avg:35.94ms
step:137/2110 train_time:4923ms step_avg:35.94ms
step:138/2110 train_time:4961ms step_avg:35.95ms
step:139/2110 train_time:4997ms step_avg:35.95ms
step:140/2110 train_time:5034ms step_avg:35.96ms
step:141/2110 train_time:5066ms step_avg:35.93ms
step:142/2110 train_time:5100ms step_avg:35.92ms
step:143/2110 train_time:5131ms step_avg:35.88ms
step:144/2110 train_time:5165ms step_avg:35.87ms
step:145/2110 train_time:5196ms step_avg:35.84ms
step:146/2110 train_time:5228ms step_avg:35.81ms
step:147/2110 train_time:5261ms step_avg:35.79ms
step:148/2110 train_time:5293ms step_avg:35.76ms
step:149/2110 train_time:5323ms step_avg:35.73ms
step:150/2110 train_time:5355ms step_avg:35.70ms
step:151/2110 train_time:5385ms step_avg:35.66ms
step:152/2110 train_time:5419ms step_avg:35.65ms
step:153/2110 train_time:5449ms step_avg:35.62ms
step:154/2110 train_time:5483ms step_avg:35.61ms
step:155/2110 train_time:5515ms step_avg:35.58ms
step:156/2110 train_time:5550ms step_avg:35.57ms
step:157/2110 train_time:5580ms step_avg:35.54ms
step:158/2110 train_time:5615ms step_avg:35.54ms
step:159/2110 train_time:5648ms step_avg:35.52ms
step:160/2110 train_time:5681ms step_avg:35.51ms
step:161/2110 train_time:5712ms step_avg:35.48ms
step:162/2110 train_time:5748ms step_avg:35.48ms
step:163/2110 train_time:5779ms step_avg:35.46ms
step:164/2110 train_time:5813ms step_avg:35.44ms
step:165/2110 train_time:5845ms step_avg:35.42ms
step:166/2110 train_time:5879ms step_avg:35.42ms
step:167/2110 train_time:5911ms step_avg:35.39ms
step:168/2110 train_time:5944ms step_avg:35.38ms
step:169/2110 train_time:5977ms step_avg:35.37ms
step:170/2110 train_time:6010ms step_avg:35.35ms
step:171/2110 train_time:6043ms step_avg:35.34ms
step:172/2110 train_time:6077ms step_avg:35.33ms
step:173/2110 train_time:6109ms step_avg:35.31ms
step:174/2110 train_time:6142ms step_avg:35.30ms
step:175/2110 train_time:6176ms step_avg:35.29ms
step:176/2110 train_time:6208ms step_avg:35.27ms
step:177/2110 train_time:6241ms step_avg:35.26ms
step:178/2110 train_time:6276ms step_avg:35.26ms
step:179/2110 train_time:6308ms step_avg:35.24ms
step:180/2110 train_time:6339ms step_avg:35.22ms
step:181/2110 train_time:6373ms step_avg:35.21ms
step:182/2110 train_time:6406ms step_avg:35.20ms
step:183/2110 train_time:6438ms step_avg:35.18ms
step:184/2110 train_time:6472ms step_avg:35.18ms
step:185/2110 train_time:6504ms step_avg:35.15ms
step:186/2110 train_time:6537ms step_avg:35.15ms
step:187/2110 train_time:6569ms step_avg:35.13ms
step:188/2110 train_time:6603ms step_avg:35.12ms
step:189/2110 train_time:6636ms step_avg:35.11ms
step:190/2110 train_time:6672ms step_avg:35.12ms
step:191/2110 train_time:6703ms step_avg:35.09ms
step:192/2110 train_time:6738ms step_avg:35.09ms
step:193/2110 train_time:6767ms step_avg:35.06ms
step:194/2110 train_time:6803ms step_avg:35.06ms
step:195/2110 train_time:6834ms step_avg:35.05ms
step:196/2110 train_time:6868ms step_avg:35.04ms
step:197/2110 train_time:6900ms step_avg:35.02ms
step:198/2110 train_time:6934ms step_avg:35.02ms
step:199/2110 train_time:6965ms step_avg:35.00ms
step:200/2110 train_time:7000ms step_avg:35.00ms
step:201/2110 train_time:7033ms step_avg:34.99ms
step:202/2110 train_time:7069ms step_avg:35.00ms
step:203/2110 train_time:7104ms step_avg:34.99ms
step:204/2110 train_time:7135ms step_avg:34.97ms
step:205/2110 train_time:7166ms step_avg:34.95ms
step:206/2110 train_time:7200ms step_avg:34.95ms
step:207/2110 train_time:7232ms step_avg:34.94ms
step:208/2110 train_time:7265ms step_avg:34.93ms
step:209/2110 train_time:7298ms step_avg:34.92ms
step:210/2110 train_time:7331ms step_avg:34.91ms
step:211/2110 train_time:7364ms step_avg:34.90ms
step:212/2110 train_time:7396ms step_avg:34.89ms
step:213/2110 train_time:7428ms step_avg:34.87ms
step:214/2110 train_time:7462ms step_avg:34.87ms
step:215/2110 train_time:7494ms step_avg:34.85ms
step:216/2110 train_time:7526ms step_avg:34.84ms
step:217/2110 train_time:7560ms step_avg:34.84ms
step:218/2110 train_time:7594ms step_avg:34.83ms
step:219/2110 train_time:7626ms step_avg:34.82ms
step:220/2110 train_time:7659ms step_avg:34.81ms
step:221/2110 train_time:7691ms step_avg:34.80ms
step:222/2110 train_time:7725ms step_avg:34.80ms
step:223/2110 train_time:7757ms step_avg:34.79ms
step:224/2110 train_time:7792ms step_avg:34.79ms
step:225/2110 train_time:7823ms step_avg:34.77ms
step:226/2110 train_time:7858ms step_avg:34.77ms
step:227/2110 train_time:7891ms step_avg:34.76ms
step:228/2110 train_time:7923ms step_avg:34.75ms
step:229/2110 train_time:7955ms step_avg:34.74ms
step:230/2110 train_time:7989ms step_avg:34.74ms
step:231/2110 train_time:8021ms step_avg:34.72ms
step:232/2110 train_time:8055ms step_avg:34.72ms
step:233/2110 train_time:8087ms step_avg:34.71ms
step:234/2110 train_time:8120ms step_avg:34.70ms
step:235/2110 train_time:8153ms step_avg:34.69ms
step:236/2110 train_time:8187ms step_avg:34.69ms
step:237/2110 train_time:8219ms step_avg:34.68ms
step:238/2110 train_time:8253ms step_avg:34.67ms
step:239/2110 train_time:8284ms step_avg:34.66ms
step:240/2110 train_time:8319ms step_avg:34.66ms
step:241/2110 train_time:8350ms step_avg:34.65ms
step:242/2110 train_time:8385ms step_avg:34.65ms
step:243/2110 train_time:8416ms step_avg:34.63ms
step:244/2110 train_time:8450ms step_avg:34.63ms
step:245/2110 train_time:8481ms step_avg:34.62ms
step:246/2110 train_time:8516ms step_avg:34.62ms
step:247/2110 train_time:8547ms step_avg:34.60ms
step:248/2110 train_time:8581ms step_avg:34.60ms
step:249/2110 train_time:8614ms step_avg:34.59ms
step:250/2110 train_time:8647ms step_avg:34.59ms
step:250/2110 val_loss:4.2965 train_time:8681ms step_avg:34.72ms
step:251/2110 train_time:8717ms step_avg:34.73ms
step:252/2110 train_time:8751ms step_avg:34.73ms
step:253/2110 train_time:8784ms step_avg:34.72ms
step:254/2110 train_time:8818ms step_avg:34.72ms
step:255/2110 train_time:8849ms step_avg:34.70ms
step:256/2110 train_time:8882ms step_avg:34.69ms
step:257/2110 train_time:8914ms step_avg:34.69ms
step:258/2110 train_time:8947ms step_avg:34.68ms
step:259/2110 train_time:8978ms step_avg:34.66ms
step:260/2110 train_time:9011ms step_avg:34.66ms
step:261/2110 train_time:9042ms step_avg:34.64ms
step:262/2110 train_time:9074ms step_avg:34.63ms
step:263/2110 train_time:9104ms step_avg:34.61ms
step:264/2110 train_time:9137ms step_avg:34.61ms
step:265/2110 train_time:9164ms step_avg:34.58ms
step:266/2110 train_time:9198ms step_avg:34.58ms
step:267/2110 train_time:9228ms step_avg:34.56ms
step:268/2110 train_time:9260ms step_avg:34.55ms
step:269/2110 train_time:9290ms step_avg:34.54ms
step:270/2110 train_time:9323ms step_avg:34.53ms
step:271/2110 train_time:9354ms step_avg:34.52ms
step:272/2110 train_time:9386ms step_avg:34.51ms
step:273/2110 train_time:9417ms step_avg:34.49ms
step:274/2110 train_time:9449ms step_avg:34.48ms
step:275/2110 train_time:9481ms step_avg:34.48ms
step:276/2110 train_time:9516ms step_avg:34.48ms
step:277/2110 train_time:9547ms step_avg:34.47ms
step:278/2110 train_time:9580ms step_avg:34.46ms
step:279/2110 train_time:9611ms step_avg:34.45ms
step:280/2110 train_time:9646ms step_avg:34.45ms
step:281/2110 train_time:9678ms step_avg:34.44ms
step:282/2110 train_time:9712ms step_avg:34.44ms
step:283/2110 train_time:9745ms step_avg:34.43ms
step:284/2110 train_time:9777ms step_avg:34.43ms
step:285/2110 train_time:9810ms step_avg:34.42ms
step:286/2110 train_time:9846ms step_avg:34.43ms
step:287/2110 train_time:9878ms step_avg:34.42ms
step:288/2110 train_time:9910ms step_avg:34.41ms
step:289/2110 train_time:9943ms step_avg:34.40ms
step:290/2110 train_time:9976ms step_avg:34.40ms
step:291/2110 train_time:10009ms step_avg:34.40ms
step:292/2110 train_time:10044ms step_avg:34.40ms
step:293/2110 train_time:10075ms step_avg:34.39ms
step:294/2110 train_time:10109ms step_avg:34.38ms
step:295/2110 train_time:10142ms step_avg:34.38ms
step:296/2110 train_time:10178ms step_avg:34.39ms
step:297/2110 train_time:10216ms step_avg:34.40ms
step:298/2110 train_time:10257ms step_avg:34.42ms
step:299/2110 train_time:10292ms step_avg:34.42ms
step:300/2110 train_time:10326ms step_avg:34.42ms
step:301/2110 train_time:10361ms step_avg:34.42ms
step:302/2110 train_time:10398ms step_avg:34.43ms
step:303/2110 train_time:10433ms step_avg:34.43ms
step:304/2110 train_time:10467ms step_avg:34.43ms
step:305/2110 train_time:10497ms step_avg:34.42ms
step:306/2110 train_time:10528ms step_avg:34.41ms
step:307/2110 train_time:10559ms step_avg:34.39ms
step:308/2110 train_time:10591ms step_avg:34.39ms
step:309/2110 train_time:10622ms step_avg:34.37ms
step:310/2110 train_time:10655ms step_avg:34.37ms
step:311/2110 train_time:10686ms step_avg:34.36ms
step:312/2110 train_time:10718ms step_avg:34.35ms
step:313/2110 train_time:10751ms step_avg:34.35ms
step:314/2110 train_time:10784ms step_avg:34.34ms
step:315/2110 train_time:10818ms step_avg:34.34ms
step:316/2110 train_time:10854ms step_avg:34.35ms
step:317/2110 train_time:10886ms step_avg:34.34ms
step:318/2110 train_time:10919ms step_avg:34.34ms
step:319/2110 train_time:10951ms step_avg:34.33ms
step:320/2110 train_time:10985ms step_avg:34.33ms
step:321/2110 train_time:11018ms step_avg:34.32ms
step:322/2110 train_time:11051ms step_avg:34.32ms
step:323/2110 train_time:11082ms step_avg:34.31ms
step:324/2110 train_time:11116ms step_avg:34.31ms
step:325/2110 train_time:11147ms step_avg:34.30ms
step:326/2110 train_time:11181ms step_avg:34.30ms
step:327/2110 train_time:11212ms step_avg:34.29ms
step:328/2110 train_time:11247ms step_avg:34.29ms
step:329/2110 train_time:11278ms step_avg:34.28ms
step:330/2110 train_time:11313ms step_avg:34.28ms
step:331/2110 train_time:11344ms step_avg:34.27ms
step:332/2110 train_time:11378ms step_avg:34.27ms
step:333/2110 train_time:11410ms step_avg:34.26ms
step:334/2110 train_time:11444ms step_avg:34.26ms
step:335/2110 train_time:11476ms step_avg:34.26ms
step:336/2110 train_time:11508ms step_avg:34.25ms
step:337/2110 train_time:11541ms step_avg:34.25ms
step:338/2110 train_time:11575ms step_avg:34.24ms
step:339/2110 train_time:11607ms step_avg:34.24ms
step:340/2110 train_time:11642ms step_avg:34.24ms
step:341/2110 train_time:11674ms step_avg:34.23ms
step:342/2110 train_time:11706ms step_avg:34.23ms
step:343/2110 train_time:11740ms step_avg:34.23ms
step:344/2110 train_time:11773ms step_avg:34.22ms
step:345/2110 train_time:11806ms step_avg:34.22ms
step:346/2110 train_time:11840ms step_avg:34.22ms
step:347/2110 train_time:11873ms step_avg:34.21ms
step:348/2110 train_time:11907ms step_avg:34.22ms
step:349/2110 train_time:11937ms step_avg:34.20ms
step:350/2110 train_time:11971ms step_avg:34.20ms
step:351/2110 train_time:12004ms step_avg:34.20ms
step:352/2110 train_time:12036ms step_avg:34.19ms
step:353/2110 train_time:12070ms step_avg:34.19ms
step:354/2110 train_time:12103ms step_avg:34.19ms
step:355/2110 train_time:12135ms step_avg:34.18ms
step:356/2110 train_time:12169ms step_avg:34.18ms
step:357/2110 train_time:12202ms step_avg:34.18ms
step:358/2110 train_time:12234ms step_avg:34.17ms
step:359/2110 train_time:12267ms step_avg:34.17ms
step:360/2110 train_time:12300ms step_avg:34.17ms
step:361/2110 train_time:12332ms step_avg:34.16ms
step:362/2110 train_time:12367ms step_avg:34.16ms
step:363/2110 train_time:12400ms step_avg:34.16ms
step:364/2110 train_time:12431ms step_avg:34.15ms
step:365/2110 train_time:12465ms step_avg:34.15ms
step:366/2110 train_time:12502ms step_avg:34.16ms
step:367/2110 train_time:12536ms step_avg:34.16ms
step:368/2110 train_time:12569ms step_avg:34.16ms
step:369/2110 train_time:12601ms step_avg:34.15ms
step:370/2110 train_time:12632ms step_avg:34.14ms
step:371/2110 train_time:12664ms step_avg:34.14ms
step:372/2110 train_time:12697ms step_avg:34.13ms
step:373/2110 train_time:12729ms step_avg:34.13ms
step:374/2110 train_time:12762ms step_avg:34.12ms
step:375/2110 train_time:12793ms step_avg:34.12ms
step:376/2110 train_time:12826ms step_avg:34.11ms
step:377/2110 train_time:12859ms step_avg:34.11ms
step:378/2110 train_time:12894ms step_avg:34.11ms
step:379/2110 train_time:12925ms step_avg:34.10ms
step:380/2110 train_time:12960ms step_avg:34.11ms
step:381/2110 train_time:12992ms step_avg:34.10ms
step:382/2110 train_time:13025ms step_avg:34.10ms
step:383/2110 train_time:13058ms step_avg:34.09ms
step:384/2110 train_time:13092ms step_avg:34.09ms
step:385/2110 train_time:13123ms step_avg:34.09ms
step:386/2110 train_time:13157ms step_avg:34.09ms
step:387/2110 train_time:13190ms step_avg:34.08ms
step:388/2110 train_time:13223ms step_avg:34.08ms
step:389/2110 train_time:13254ms step_avg:34.07ms
step:390/2110 train_time:13288ms step_avg:34.07ms
step:391/2110 train_time:13321ms step_avg:34.07ms
step:392/2110 train_time:13354ms step_avg:34.07ms
step:393/2110 train_time:13386ms step_avg:34.06ms
step:394/2110 train_time:13419ms step_avg:34.06ms
step:395/2110 train_time:13452ms step_avg:34.06ms
step:396/2110 train_time:13485ms step_avg:34.05ms
step:397/2110 train_time:13519ms step_avg:34.05ms
step:398/2110 train_time:13553ms step_avg:34.05ms
step:399/2110 train_time:13585ms step_avg:34.05ms
step:400/2110 train_time:13617ms step_avg:34.04ms
step:401/2110 train_time:13650ms step_avg:34.04ms
step:402/2110 train_time:13684ms step_avg:34.04ms
step:403/2110 train_time:13715ms step_avg:34.03ms
step:404/2110 train_time:13750ms step_avg:34.03ms
step:405/2110 train_time:13781ms step_avg:34.03ms
step:406/2110 train_time:13815ms step_avg:34.03ms
step:407/2110 train_time:13847ms step_avg:34.02ms
step:408/2110 train_time:13882ms step_avg:34.02ms
step:409/2110 train_time:13915ms step_avg:34.02ms
step:410/2110 train_time:13946ms step_avg:34.01ms
step:411/2110 train_time:13980ms step_avg:34.01ms
step:412/2110 train_time:14012ms step_avg:34.01ms
step:413/2110 train_time:14046ms step_avg:34.01ms
step:414/2110 train_time:14081ms step_avg:34.01ms
step:415/2110 train_time:14113ms step_avg:34.01ms
step:416/2110 train_time:14145ms step_avg:34.00ms
step:417/2110 train_time:14177ms step_avg:34.00ms
step:418/2110 train_time:14211ms step_avg:34.00ms
step:419/2110 train_time:14244ms step_avg:33.99ms
step:420/2110 train_time:14278ms step_avg:33.99ms
step:421/2110 train_time:14309ms step_avg:33.99ms
step:422/2110 train_time:14342ms step_avg:33.99ms
step:423/2110 train_time:14375ms step_avg:33.98ms
step:424/2110 train_time:14409ms step_avg:33.98ms
step:425/2110 train_time:14441ms step_avg:33.98ms
step:426/2110 train_time:14474ms step_avg:33.98ms
step:427/2110 train_time:14506ms step_avg:33.97ms
step:428/2110 train_time:14541ms step_avg:33.98ms
step:429/2110 train_time:14576ms step_avg:33.98ms
step:430/2110 train_time:14612ms step_avg:33.98ms
step:431/2110 train_time:14640ms step_avg:33.97ms
step:432/2110 train_time:14675ms step_avg:33.97ms
step:433/2110 train_time:14706ms step_avg:33.96ms
step:434/2110 train_time:14737ms step_avg:33.96ms
step:435/2110 train_time:14771ms step_avg:33.96ms
step:436/2110 train_time:14806ms step_avg:33.96ms
step:437/2110 train_time:14836ms step_avg:33.95ms
step:438/2110 train_time:14869ms step_avg:33.95ms
step:439/2110 train_time:14903ms step_avg:33.95ms
step:440/2110 train_time:14936ms step_avg:33.95ms
step:441/2110 train_time:14967ms step_avg:33.94ms
step:442/2110 train_time:15001ms step_avg:33.94ms
step:443/2110 train_time:15033ms step_avg:33.93ms
step:444/2110 train_time:15067ms step_avg:33.94ms
step:445/2110 train_time:15099ms step_avg:33.93ms
step:446/2110 train_time:15133ms step_avg:33.93ms
step:447/2110 train_time:15165ms step_avg:33.93ms
step:448/2110 train_time:15199ms step_avg:33.93ms
step:449/2110 train_time:15231ms step_avg:33.92ms
step:450/2110 train_time:15264ms step_avg:33.92ms
step:451/2110 train_time:15297ms step_avg:33.92ms
step:452/2110 train_time:15330ms step_avg:33.92ms
step:453/2110 train_time:15363ms step_avg:33.91ms
step:454/2110 train_time:15396ms step_avg:33.91ms
step:455/2110 train_time:15429ms step_avg:33.91ms
step:456/2110 train_time:15463ms step_avg:33.91ms
step:457/2110 train_time:15496ms step_avg:33.91ms
step:458/2110 train_time:15530ms step_avg:33.91ms
step:459/2110 train_time:15560ms step_avg:33.90ms
step:460/2110 train_time:15594ms step_avg:33.90ms
step:461/2110 train_time:15626ms step_avg:33.90ms
step:462/2110 train_time:15660ms step_avg:33.90ms
step:463/2110 train_time:15692ms step_avg:33.89ms
step:464/2110 train_time:15725ms step_avg:33.89ms
step:465/2110 train_time:15757ms step_avg:33.89ms
step:466/2110 train_time:15791ms step_avg:33.89ms
step:467/2110 train_time:15824ms step_avg:33.88ms
step:468/2110 train_time:15858ms step_avg:33.88ms
step:469/2110 train_time:15890ms step_avg:33.88ms
step:470/2110 train_time:15926ms step_avg:33.89ms
step:471/2110 train_time:15959ms step_avg:33.88ms
step:472/2110 train_time:15994ms step_avg:33.88ms
step:473/2110 train_time:16025ms step_avg:33.88ms
step:474/2110 train_time:16066ms step_avg:33.89ms
step:475/2110 train_time:16105ms step_avg:33.90ms
step:476/2110 train_time:16139ms step_avg:33.91ms
step:477/2110 train_time:16173ms step_avg:33.91ms
step:478/2110 train_time:16208ms step_avg:33.91ms
step:479/2110 train_time:16238ms step_avg:33.90ms
step:480/2110 train_time:16273ms step_avg:33.90ms
step:481/2110 train_time:16305ms step_avg:33.90ms
step:482/2110 train_time:16341ms step_avg:33.90ms
step:483/2110 train_time:16373ms step_avg:33.90ms
step:484/2110 train_time:16407ms step_avg:33.90ms
step:485/2110 train_time:16439ms step_avg:33.90ms
step:486/2110 train_time:16475ms step_avg:33.90ms
step:487/2110 train_time:16506ms step_avg:33.89ms
step:488/2110 train_time:16549ms step_avg:33.91ms
step:489/2110 train_time:16593ms step_avg:33.93ms
step:490/2110 train_time:16634ms step_avg:33.95ms
step:491/2110 train_time:16678ms step_avg:33.97ms
step:492/2110 train_time:16720ms step_avg:33.98ms
step:493/2110 train_time:16763ms step_avg:34.00ms
step:494/2110 train_time:16803ms step_avg:34.01ms
step:495/2110 train_time:16847ms step_avg:34.03ms
step:496/2110 train_time:16887ms step_avg:34.05ms
step:497/2110 train_time:16930ms step_avg:34.06ms
step:498/2110 train_time:16970ms step_avg:34.08ms
step:499/2110 train_time:17014ms step_avg:34.10ms
step:500/2110 train_time:17055ms step_avg:34.11ms
step:500/2110 val_loss:4.0266 train_time:17098ms step_avg:34.20ms
step:501/2110 train_time:17133ms step_avg:34.20ms
step:502/2110 train_time:17169ms step_avg:34.20ms
step:503/2110 train_time:17201ms step_avg:34.20ms
step:504/2110 train_time:17236ms step_avg:34.20ms
step:505/2110 train_time:17268ms step_avg:34.19ms
step:506/2110 train_time:17301ms step_avg:34.19ms
step:507/2110 train_time:17333ms step_avg:34.19ms
step:508/2110 train_time:17369ms step_avg:34.19ms
step:509/2110 train_time:17400ms step_avg:34.18ms
step:510/2110 train_time:17434ms step_avg:34.18ms
step:511/2110 train_time:17468ms step_avg:34.18ms
step:512/2110 train_time:17503ms step_avg:34.19ms
step:513/2110 train_time:17536ms step_avg:34.18ms
step:514/2110 train_time:17572ms step_avg:34.19ms
step:515/2110 train_time:17609ms step_avg:34.19ms
step:516/2110 train_time:17644ms step_avg:34.19ms
step:517/2110 train_time:17680ms step_avg:34.20ms
step:518/2110 train_time:17715ms step_avg:34.20ms
step:519/2110 train_time:17752ms step_avg:34.20ms
step:520/2110 train_time:17788ms step_avg:34.21ms
step:521/2110 train_time:17825ms step_avg:34.21ms
step:522/2110 train_time:17860ms step_avg:34.22ms
step:523/2110 train_time:17896ms step_avg:34.22ms
step:524/2110 train_time:17931ms step_avg:34.22ms
step:525/2110 train_time:17971ms step_avg:34.23ms
step:526/2110 train_time:18007ms step_avg:34.23ms
step:527/2110 train_time:18043ms step_avg:34.24ms
step:528/2110 train_time:18078ms step_avg:34.24ms
step:529/2110 train_time:18113ms step_avg:34.24ms
step:530/2110 train_time:18153ms step_avg:34.25ms
step:531/2110 train_time:18190ms step_avg:34.26ms
step:532/2110 train_time:18227ms step_avg:34.26ms
step:533/2110 train_time:18263ms step_avg:34.26ms
step:534/2110 train_time:18297ms step_avg:34.26ms
step:535/2110 train_time:18334ms step_avg:34.27ms
step:536/2110 train_time:18369ms step_avg:34.27ms
step:537/2110 train_time:18406ms step_avg:34.28ms
step:538/2110 train_time:18441ms step_avg:34.28ms
step:539/2110 train_time:18478ms step_avg:34.28ms
step:540/2110 train_time:18512ms step_avg:34.28ms
step:541/2110 train_time:18550ms step_avg:34.29ms
step:542/2110 train_time:18585ms step_avg:34.29ms
step:543/2110 train_time:18620ms step_avg:34.29ms
step:544/2110 train_time:18656ms step_avg:34.29ms
step:545/2110 train_time:18694ms step_avg:34.30ms
step:546/2110 train_time:18727ms step_avg:34.30ms
step:547/2110 train_time:18763ms step_avg:34.30ms
step:548/2110 train_time:18798ms step_avg:34.30ms
step:549/2110 train_time:18836ms step_avg:34.31ms
step:550/2110 train_time:18872ms step_avg:34.31ms
step:551/2110 train_time:18907ms step_avg:34.31ms
step:552/2110 train_time:18941ms step_avg:34.31ms
step:553/2110 train_time:18977ms step_avg:34.32ms
step:554/2110 train_time:19011ms step_avg:34.32ms
step:555/2110 train_time:19049ms step_avg:34.32ms
step:556/2110 train_time:19084ms step_avg:34.32ms
step:557/2110 train_time:19120ms step_avg:34.33ms
step:558/2110 train_time:19155ms step_avg:34.33ms
step:559/2110 train_time:19191ms step_avg:34.33ms
step:560/2110 train_time:19229ms step_avg:34.34ms
step:561/2110 train_time:19263ms step_avg:34.34ms
step:562/2110 train_time:19297ms step_avg:34.34ms
step:563/2110 train_time:19334ms step_avg:34.34ms
step:564/2110 train_time:19373ms step_avg:34.35ms
step:565/2110 train_time:19407ms step_avg:34.35ms
step:566/2110 train_time:19441ms step_avg:34.35ms
step:567/2110 train_time:19481ms step_avg:34.36ms
step:568/2110 train_time:19517ms step_avg:34.36ms
step:569/2110 train_time:19551ms step_avg:34.36ms
step:570/2110 train_time:19585ms step_avg:34.36ms
step:571/2110 train_time:19622ms step_avg:34.36ms
step:572/2110 train_time:19656ms step_avg:34.36ms
step:573/2110 train_time:19691ms step_avg:34.37ms
step:574/2110 train_time:19727ms step_avg:34.37ms
step:575/2110 train_time:19763ms step_avg:34.37ms
step:576/2110 train_time:19798ms step_avg:34.37ms
step:577/2110 train_time:19832ms step_avg:34.37ms
step:578/2110 train_time:19867ms step_avg:34.37ms
step:579/2110 train_time:19903ms step_avg:34.37ms
step:580/2110 train_time:19937ms step_avg:34.37ms
step:581/2110 train_time:19974ms step_avg:34.38ms
step:582/2110 train_time:20009ms step_avg:34.38ms
step:583/2110 train_time:20044ms step_avg:34.38ms
step:584/2110 train_time:20079ms step_avg:34.38ms
step:585/2110 train_time:20116ms step_avg:34.39ms
step:586/2110 train_time:20158ms step_avg:34.40ms
step:587/2110 train_time:20200ms step_avg:34.41ms
step:588/2110 train_time:20239ms step_avg:34.42ms
step:589/2110 train_time:20282ms step_avg:34.43ms
step:590/2110 train_time:20324ms step_avg:34.45ms
step:591/2110 train_time:20367ms step_avg:34.46ms
step:592/2110 train_time:20408ms step_avg:34.47ms
step:593/2110 train_time:20451ms step_avg:34.49ms
step:594/2110 train_time:20492ms step_avg:34.50ms
step:595/2110 train_time:20536ms step_avg:34.51ms
step:596/2110 train_time:20578ms step_avg:34.53ms
step:597/2110 train_time:20621ms step_avg:34.54ms
step:598/2110 train_time:20662ms step_avg:34.55ms
step:599/2110 train_time:20706ms step_avg:34.57ms
step:600/2110 train_time:20747ms step_avg:34.58ms
step:601/2110 train_time:20791ms step_avg:34.59ms
step:602/2110 train_time:20832ms step_avg:34.60ms
step:603/2110 train_time:20877ms step_avg:34.62ms
step:604/2110 train_time:20918ms step_avg:34.63ms
step:605/2110 train_time:20961ms step_avg:34.65ms
step:606/2110 train_time:20995ms step_avg:34.65ms
step:607/2110 train_time:21029ms step_avg:34.64ms
step:608/2110 train_time:21062ms step_avg:34.64ms
step:609/2110 train_time:21096ms step_avg:34.64ms
step:610/2110 train_time:21131ms step_avg:34.64ms
step:611/2110 train_time:21162ms step_avg:34.64ms
step:612/2110 train_time:21197ms step_avg:34.64ms
step:613/2110 train_time:21229ms step_avg:34.63ms
step:614/2110 train_time:21262ms step_avg:34.63ms
step:615/2110 train_time:21294ms step_avg:34.62ms
step:616/2110 train_time:21327ms step_avg:34.62ms
step:617/2110 train_time:21369ms step_avg:34.63ms
step:618/2110 train_time:21407ms step_avg:34.64ms
step:619/2110 train_time:21446ms step_avg:34.65ms
step:620/2110 train_time:21491ms step_avg:34.66ms
step:621/2110 train_time:21529ms step_avg:34.67ms
step:622/2110 train_time:21563ms step_avg:34.67ms
step:623/2110 train_time:21621ms step_avg:34.71ms
step:624/2110 train_time:21665ms step_avg:34.72ms
step:625/2110 train_time:21705ms step_avg:34.73ms
step:626/2110 train_time:21746ms step_avg:34.74ms
step:627/2110 train_time:21785ms step_avg:34.74ms
step:628/2110 train_time:21824ms step_avg:34.75ms
step:629/2110 train_time:21860ms step_avg:34.75ms
step:630/2110 train_time:21900ms step_avg:34.76ms
step:631/2110 train_time:21937ms step_avg:34.77ms
step:632/2110 train_time:21976ms step_avg:34.77ms
step:633/2110 train_time:22013ms step_avg:34.78ms
step:634/2110 train_time:22050ms step_avg:34.78ms
step:635/2110 train_time:22087ms step_avg:34.78ms
step:636/2110 train_time:22125ms step_avg:34.79ms
step:637/2110 train_time:22156ms step_avg:34.78ms
step:638/2110 train_time:22192ms step_avg:34.78ms
step:639/2110 train_time:22225ms step_avg:34.78ms
step:640/2110 train_time:22258ms step_avg:34.78ms
step:641/2110 train_time:22290ms step_avg:34.77ms
step:642/2110 train_time:22325ms step_avg:34.77ms
step:643/2110 train_time:22356ms step_avg:34.77ms
step:644/2110 train_time:22392ms step_avg:34.77ms
step:645/2110 train_time:22422ms step_avg:34.76ms
step:646/2110 train_time:22457ms step_avg:34.76ms
step:647/2110 train_time:22490ms step_avg:34.76ms
step:648/2110 train_time:22523ms step_avg:34.76ms
step:649/2110 train_time:22557ms step_avg:34.76ms
step:650/2110 train_time:22590ms step_avg:34.75ms
step:651/2110 train_time:22622ms step_avg:34.75ms
step:652/2110 train_time:22657ms step_avg:34.75ms
step:653/2110 train_time:22689ms step_avg:34.75ms
step:654/2110 train_time:22724ms step_avg:34.75ms
step:655/2110 train_time:22756ms step_avg:34.74ms
step:656/2110 train_time:22789ms step_avg:34.74ms
step:657/2110 train_time:22823ms step_avg:34.74ms
step:658/2110 train_time:22857ms step_avg:34.74ms
step:659/2110 train_time:22888ms step_avg:34.73ms
step:660/2110 train_time:22924ms step_avg:34.73ms
step:661/2110 train_time:22956ms step_avg:34.73ms
step:662/2110 train_time:22991ms step_avg:34.73ms
step:663/2110 train_time:23021ms step_avg:34.72ms
step:664/2110 train_time:23056ms step_avg:34.72ms
step:665/2110 train_time:23089ms step_avg:34.72ms
step:666/2110 train_time:23123ms step_avg:34.72ms
step:667/2110 train_time:23155ms step_avg:34.71ms
step:668/2110 train_time:23189ms step_avg:34.71ms
step:669/2110 train_time:23220ms step_avg:34.71ms
step:670/2110 train_time:23254ms step_avg:34.71ms
step:671/2110 train_time:23286ms step_avg:34.70ms
step:672/2110 train_time:23320ms step_avg:34.70ms
step:673/2110 train_time:23351ms step_avg:34.70ms
step:674/2110 train_time:23384ms step_avg:34.69ms
step:675/2110 train_time:23415ms step_avg:34.69ms
step:676/2110 train_time:23451ms step_avg:34.69ms
step:677/2110 train_time:23490ms step_avg:34.70ms
step:678/2110 train_time:23525ms step_avg:34.70ms
step:679/2110 train_time:23558ms step_avg:34.69ms
step:680/2110 train_time:23590ms step_avg:34.69ms
step:681/2110 train_time:23621ms step_avg:34.69ms
step:682/2110 train_time:23654ms step_avg:34.68ms
step:683/2110 train_time:23685ms step_avg:34.68ms
step:684/2110 train_time:23719ms step_avg:34.68ms
step:685/2110 train_time:23748ms step_avg:34.67ms
step:686/2110 train_time:23783ms step_avg:34.67ms
step:687/2110 train_time:23814ms step_avg:34.66ms
step:688/2110 train_time:23847ms step_avg:34.66ms
step:689/2110 train_time:23877ms step_avg:34.66ms
step:690/2110 train_time:23910ms step_avg:34.65ms
step:691/2110 train_time:23943ms step_avg:34.65ms
step:692/2110 train_time:23980ms step_avg:34.65ms
step:693/2110 train_time:24040ms step_avg:34.69ms
step:694/2110 train_time:24100ms step_avg:34.73ms
step:695/2110 train_time:24159ms step_avg:34.76ms
step:696/2110 train_time:24219ms step_avg:34.80ms
step:697/2110 train_time:24279ms step_avg:34.83ms
step:698/2110 train_time:24338ms step_avg:34.87ms
step:699/2110 train_time:24397ms step_avg:34.90ms
step:700/2110 train_time:24456ms step_avg:34.94ms
step:701/2110 train_time:24516ms step_avg:34.97ms
step:702/2110 train_time:24575ms step_avg:35.01ms
step:703/2110 train_time:24634ms step_avg:35.04ms
step:704/2110 train_time:24693ms step_avg:35.08ms
step:705/2110 train_time:24752ms step_avg:35.11ms
step:706/2110 train_time:24811ms step_avg:35.14ms
step:707/2110 train_time:24870ms step_avg:35.18ms
step:708/2110 train_time:24930ms step_avg:35.21ms
step:709/2110 train_time:24989ms step_avg:35.24ms
step:710/2110 train_time:25048ms step_avg:35.28ms
step:711/2110 train_time:25107ms step_avg:35.31ms
step:712/2110 train_time:25165ms step_avg:35.34ms
step:713/2110 train_time:25224ms step_avg:35.38ms
step:714/2110 train_time:25283ms step_avg:35.41ms
step:715/2110 train_time:25344ms step_avg:35.45ms
step:716/2110 train_time:25402ms step_avg:35.48ms
step:717/2110 train_time:25462ms step_avg:35.51ms
step:718/2110 train_time:25521ms step_avg:35.54ms
step:719/2110 train_time:25581ms step_avg:35.58ms
step:720/2110 train_time:25640ms step_avg:35.61ms
step:721/2110 train_time:25700ms step_avg:35.64ms
step:722/2110 train_time:25758ms step_avg:35.68ms
step:723/2110 train_time:25819ms step_avg:35.71ms
step:724/2110 train_time:25878ms step_avg:35.74ms
step:725/2110 train_time:25938ms step_avg:35.78ms
step:726/2110 train_time:25997ms step_avg:35.81ms
step:727/2110 train_time:26057ms step_avg:35.84ms
step:728/2110 train_time:26117ms step_avg:35.87ms
step:729/2110 train_time:26177ms step_avg:35.91ms
step:730/2110 train_time:26237ms step_avg:35.94ms
step:731/2110 train_time:26296ms step_avg:35.97ms
step:732/2110 train_time:26355ms step_avg:36.00ms
step:733/2110 train_time:26415ms step_avg:36.04ms
step:734/2110 train_time:26475ms step_avg:36.07ms
step:735/2110 train_time:26533ms step_avg:36.10ms
step:736/2110 train_time:26592ms step_avg:36.13ms
step:737/2110 train_time:26651ms step_avg:36.16ms
step:738/2110 train_time:26709ms step_avg:36.19ms
step:739/2110 train_time:26768ms step_avg:36.22ms
step:740/2110 train_time:26827ms step_avg:36.25ms
step:741/2110 train_time:26886ms step_avg:36.28ms
step:742/2110 train_time:26945ms step_avg:36.31ms
step:743/2110 train_time:27005ms step_avg:36.35ms
step:744/2110 train_time:27063ms step_avg:36.38ms
step:745/2110 train_time:27123ms step_avg:36.41ms
step:746/2110 train_time:27181ms step_avg:36.44ms
step:747/2110 train_time:27241ms step_avg:36.47ms
step:748/2110 train_time:27300ms step_avg:36.50ms
step:749/2110 train_time:27360ms step_avg:36.53ms
step:750/2110 train_time:27420ms step_avg:36.56ms
step:750/2110 val_loss:3.9112 train_time:27480ms step_avg:36.64ms
step:751/2110 train_time:27517ms step_avg:36.64ms
step:752/2110 train_time:27553ms step_avg:36.64ms
step:753/2110 train_time:27604ms step_avg:36.66ms
step:754/2110 train_time:27664ms step_avg:36.69ms
step:755/2110 train_time:27725ms step_avg:36.72ms
step:756/2110 train_time:27784ms step_avg:36.75ms
step:757/2110 train_time:27843ms step_avg:36.78ms
step:758/2110 train_time:27901ms step_avg:36.81ms
step:759/2110 train_time:27960ms step_avg:36.84ms
step:760/2110 train_time:28018ms step_avg:36.87ms
step:761/2110 train_time:28076ms step_avg:36.89ms
step:762/2110 train_time:28134ms step_avg:36.92ms
step:763/2110 train_time:28193ms step_avg:36.95ms
step:764/2110 train_time:28251ms step_avg:36.98ms
step:765/2110 train_time:28309ms step_avg:37.01ms
step:766/2110 train_time:28367ms step_avg:37.03ms
step:767/2110 train_time:28426ms step_avg:37.06ms
step:768/2110 train_time:28486ms step_avg:37.09ms
step:769/2110 train_time:28547ms step_avg:37.12ms
step:770/2110 train_time:28607ms step_avg:37.15ms
step:771/2110 train_time:28668ms step_avg:37.18ms
step:772/2110 train_time:28729ms step_avg:37.21ms
step:773/2110 train_time:28788ms step_avg:37.24ms
step:774/2110 train_time:28847ms step_avg:37.27ms
step:775/2110 train_time:28906ms step_avg:37.30ms
step:776/2110 train_time:28965ms step_avg:37.33ms
step:777/2110 train_time:29023ms step_avg:37.35ms
step:778/2110 train_time:29081ms step_avg:37.38ms
step:779/2110 train_time:29140ms step_avg:37.41ms
step:780/2110 train_time:29198ms step_avg:37.43ms
step:781/2110 train_time:29257ms step_avg:37.46ms
step:782/2110 train_time:29315ms step_avg:37.49ms
step:783/2110 train_time:29374ms step_avg:37.52ms
step:784/2110 train_time:29433ms step_avg:37.54ms
step:785/2110 train_time:29494ms step_avg:37.57ms
step:786/2110 train_time:29554ms step_avg:37.60ms
step:787/2110 train_time:29614ms step_avg:37.63ms
step:788/2110 train_time:29674ms step_avg:37.66ms
step:789/2110 train_time:29734ms step_avg:37.69ms
step:790/2110 train_time:29794ms step_avg:37.71ms
step:791/2110 train_time:29854ms step_avg:37.74ms
step:792/2110 train_time:29913ms step_avg:37.77ms
step:793/2110 train_time:29973ms step_avg:37.80ms
step:794/2110 train_time:30033ms step_avg:37.82ms
step:795/2110 train_time:30092ms step_avg:37.85ms
step:796/2110 train_time:30150ms step_avg:37.88ms
step:797/2110 train_time:30209ms step_avg:37.90ms
step:798/2110 train_time:30267ms step_avg:37.93ms
step:799/2110 train_time:30326ms step_avg:37.95ms
step:800/2110 train_time:30385ms step_avg:37.98ms
step:801/2110 train_time:30443ms step_avg:38.01ms
step:802/2110 train_time:30502ms step_avg:38.03ms
step:803/2110 train_time:30561ms step_avg:38.06ms
step:804/2110 train_time:30620ms step_avg:38.08ms
step:805/2110 train_time:30680ms step_avg:38.11ms
step:806/2110 train_time:30739ms step_avg:38.14ms
step:807/2110 train_time:30800ms step_avg:38.17ms
step:808/2110 train_time:30858ms step_avg:38.19ms
step:809/2110 train_time:30919ms step_avg:38.22ms
step:810/2110 train_time:30977ms step_avg:38.24ms
step:811/2110 train_time:31037ms step_avg:38.27ms
step:812/2110 train_time:31095ms step_avg:38.29ms
step:813/2110 train_time:31154ms step_avg:38.32ms
step:814/2110 train_time:31213ms step_avg:38.34ms
step:815/2110 train_time:31273ms step_avg:38.37ms
step:816/2110 train_time:31331ms step_avg:38.40ms
step:817/2110 train_time:31391ms step_avg:38.42ms
step:818/2110 train_time:31450ms step_avg:38.45ms
step:819/2110 train_time:31510ms step_avg:38.47ms
step:820/2110 train_time:31571ms step_avg:38.50ms
step:821/2110 train_time:31631ms step_avg:38.53ms
step:822/2110 train_time:31691ms step_avg:38.55ms
step:823/2110 train_time:31751ms step_avg:38.58ms
step:824/2110 train_time:31811ms step_avg:38.61ms
step:825/2110 train_time:31870ms step_avg:38.63ms
step:826/2110 train_time:31930ms step_avg:38.66ms
step:827/2110 train_time:31989ms step_avg:38.68ms
step:828/2110 train_time:32047ms step_avg:38.70ms
step:829/2110 train_time:32106ms step_avg:38.73ms
step:830/2110 train_time:32164ms step_avg:38.75ms
step:831/2110 train_time:32223ms step_avg:38.78ms
step:832/2110 train_time:32282ms step_avg:38.80ms
step:833/2110 train_time:32341ms step_avg:38.82ms
step:834/2110 train_time:32399ms step_avg:38.85ms
step:835/2110 train_time:32459ms step_avg:38.87ms
step:836/2110 train_time:32518ms step_avg:38.90ms
step:837/2110 train_time:32578ms step_avg:38.92ms
step:838/2110 train_time:32637ms step_avg:38.95ms
step:839/2110 train_time:32698ms step_avg:38.97ms
step:840/2110 train_time:32756ms step_avg:39.00ms
step:841/2110 train_time:32817ms step_avg:39.02ms
step:842/2110 train_time:32876ms step_avg:39.05ms
step:843/2110 train_time:32936ms step_avg:39.07ms
step:844/2110 train_time:32994ms step_avg:39.09ms
step:845/2110 train_time:33054ms step_avg:39.12ms
step:846/2110 train_time:33112ms step_avg:39.14ms
step:847/2110 train_time:33172ms step_avg:39.16ms
step:848/2110 train_time:33230ms step_avg:39.19ms
step:849/2110 train_time:33290ms step_avg:39.21ms
step:850/2110 train_time:33349ms step_avg:39.23ms
step:851/2110 train_time:33408ms step_avg:39.26ms
step:852/2110 train_time:33467ms step_avg:39.28ms
step:853/2110 train_time:33526ms step_avg:39.30ms
step:854/2110 train_time:33586ms step_avg:39.33ms
step:855/2110 train_time:33645ms step_avg:39.35ms
step:856/2110 train_time:33705ms step_avg:39.37ms
step:857/2110 train_time:33764ms step_avg:39.40ms
step:858/2110 train_time:33823ms step_avg:39.42ms
step:859/2110 train_time:33882ms step_avg:39.44ms
step:860/2110 train_time:33941ms step_avg:39.47ms
step:861/2110 train_time:34000ms step_avg:39.49ms
step:862/2110 train_time:34059ms step_avg:39.51ms
step:863/2110 train_time:34119ms step_avg:39.53ms
step:864/2110 train_time:34177ms step_avg:39.56ms
step:865/2110 train_time:34236ms step_avg:39.58ms
step:866/2110 train_time:34294ms step_avg:39.60ms
step:867/2110 train_time:34354ms step_avg:39.62ms
step:868/2110 train_time:34413ms step_avg:39.65ms
step:869/2110 train_time:34473ms step_avg:39.67ms
step:870/2110 train_time:34533ms step_avg:39.69ms
step:871/2110 train_time:34592ms step_avg:39.72ms
step:872/2110 train_time:34651ms step_avg:39.74ms
step:873/2110 train_time:34711ms step_avg:39.76ms
step:874/2110 train_time:34771ms step_avg:39.78ms
step:875/2110 train_time:34831ms step_avg:39.81ms
step:876/2110 train_time:34890ms step_avg:39.83ms
step:877/2110 train_time:34950ms step_avg:39.85ms
step:878/2110 train_time:35009ms step_avg:39.87ms
step:879/2110 train_time:35068ms step_avg:39.89ms
step:880/2110 train_time:35127ms step_avg:39.92ms
step:881/2110 train_time:35186ms step_avg:39.94ms
step:882/2110 train_time:35244ms step_avg:39.96ms
step:883/2110 train_time:35303ms step_avg:39.98ms
step:884/2110 train_time:35361ms step_avg:40.00ms
step:885/2110 train_time:35421ms step_avg:40.02ms
step:886/2110 train_time:35480ms step_avg:40.04ms
step:887/2110 train_time:35539ms step_avg:40.07ms
step:888/2110 train_time:35598ms step_avg:40.09ms
step:889/2110 train_time:35658ms step_avg:40.11ms
step:890/2110 train_time:35717ms step_avg:40.13ms
step:891/2110 train_time:35777ms step_avg:40.15ms
step:892/2110 train_time:35835ms step_avg:40.17ms
step:893/2110 train_time:35896ms step_avg:40.20ms
step:894/2110 train_time:35955ms step_avg:40.22ms
step:895/2110 train_time:36016ms step_avg:40.24ms
step:896/2110 train_time:36074ms step_avg:40.26ms
step:897/2110 train_time:36133ms step_avg:40.28ms
step:898/2110 train_time:36193ms step_avg:40.30ms
step:899/2110 train_time:36252ms step_avg:40.32ms
step:900/2110 train_time:36311ms step_avg:40.35ms
step:901/2110 train_time:36371ms step_avg:40.37ms
step:902/2110 train_time:36431ms step_avg:40.39ms
step:903/2110 train_time:36490ms step_avg:40.41ms
step:904/2110 train_time:36549ms step_avg:40.43ms
step:905/2110 train_time:36610ms step_avg:40.45ms
step:906/2110 train_time:36670ms step_avg:40.47ms
step:907/2110 train_time:36728ms step_avg:40.49ms
step:908/2110 train_time:36787ms step_avg:40.51ms
step:909/2110 train_time:36846ms step_avg:40.53ms
step:910/2110 train_time:36906ms step_avg:40.56ms
step:911/2110 train_time:36965ms step_avg:40.58ms
step:912/2110 train_time:37023ms step_avg:40.60ms
step:913/2110 train_time:37083ms step_avg:40.62ms
step:914/2110 train_time:37141ms step_avg:40.64ms
step:915/2110 train_time:37201ms step_avg:40.66ms
step:916/2110 train_time:37259ms step_avg:40.68ms
step:917/2110 train_time:37319ms step_avg:40.70ms
step:918/2110 train_time:37377ms step_avg:40.72ms
step:919/2110 train_time:37437ms step_avg:40.74ms
step:920/2110 train_time:37496ms step_avg:40.76ms
step:921/2110 train_time:37556ms step_avg:40.78ms
step:922/2110 train_time:37615ms step_avg:40.80ms
step:923/2110 train_time:37675ms step_avg:40.82ms
step:924/2110 train_time:37734ms step_avg:40.84ms
step:925/2110 train_time:37794ms step_avg:40.86ms
step:926/2110 train_time:37853ms step_avg:40.88ms
step:927/2110 train_time:37913ms step_avg:40.90ms
step:928/2110 train_time:37972ms step_avg:40.92ms
step:929/2110 train_time:38032ms step_avg:40.94ms
step:930/2110 train_time:38091ms step_avg:40.96ms
step:931/2110 train_time:38151ms step_avg:40.98ms
step:932/2110 train_time:38211ms step_avg:41.00ms
step:933/2110 train_time:38270ms step_avg:41.02ms
step:934/2110 train_time:38329ms step_avg:41.04ms
step:935/2110 train_time:38388ms step_avg:41.06ms
step:936/2110 train_time:38446ms step_avg:41.08ms
step:937/2110 train_time:38505ms step_avg:41.09ms
step:938/2110 train_time:38564ms step_avg:41.11ms
step:939/2110 train_time:38624ms step_avg:41.13ms
step:940/2110 train_time:38684ms step_avg:41.15ms
step:941/2110 train_time:38742ms step_avg:41.17ms
step:942/2110 train_time:38801ms step_avg:41.19ms
step:943/2110 train_time:38862ms step_avg:41.21ms
step:944/2110 train_time:38921ms step_avg:41.23ms
step:945/2110 train_time:38980ms step_avg:41.25ms
step:946/2110 train_time:39038ms step_avg:41.27ms
step:947/2110 train_time:39098ms step_avg:41.29ms
step:948/2110 train_time:39157ms step_avg:41.30ms
step:949/2110 train_time:39217ms step_avg:41.32ms
step:950/2110 train_time:39276ms step_avg:41.34ms
step:951/2110 train_time:39336ms step_avg:41.36ms
step:952/2110 train_time:39395ms step_avg:41.38ms
step:953/2110 train_time:39454ms step_avg:41.40ms
step:954/2110 train_time:39513ms step_avg:41.42ms
step:955/2110 train_time:39574ms step_avg:41.44ms
step:956/2110 train_time:39634ms step_avg:41.46ms
step:957/2110 train_time:39693ms step_avg:41.48ms
step:958/2110 train_time:39752ms step_avg:41.49ms
step:959/2110 train_time:39811ms step_avg:41.51ms
step:960/2110 train_time:39871ms step_avg:41.53ms
step:961/2110 train_time:39930ms step_avg:41.55ms
step:962/2110 train_time:39990ms step_avg:41.57ms
step:963/2110 train_time:40049ms step_avg:41.59ms
step:964/2110 train_time:40109ms step_avg:41.61ms
step:965/2110 train_time:40168ms step_avg:41.62ms
step:966/2110 train_time:40228ms step_avg:41.64ms
step:967/2110 train_time:40286ms step_avg:41.66ms
step:968/2110 train_time:40345ms step_avg:41.68ms
step:969/2110 train_time:40404ms step_avg:41.70ms
step:970/2110 train_time:40463ms step_avg:41.71ms
step:971/2110 train_time:40522ms step_avg:41.73ms
step:972/2110 train_time:40581ms step_avg:41.75ms
step:973/2110 train_time:40642ms step_avg:41.77ms
step:974/2110 train_time:40701ms step_avg:41.79ms
step:975/2110 train_time:40762ms step_avg:41.81ms
step:976/2110 train_time:40820ms step_avg:41.82ms
step:977/2110 train_time:40880ms step_avg:41.84ms
step:978/2110 train_time:40939ms step_avg:41.86ms
step:979/2110 train_time:41000ms step_avg:41.88ms
step:980/2110 train_time:41058ms step_avg:41.90ms
step:981/2110 train_time:41118ms step_avg:41.91ms
step:982/2110 train_time:41177ms step_avg:41.93ms
step:983/2110 train_time:41236ms step_avg:41.95ms
step:984/2110 train_time:41294ms step_avg:41.97ms
step:985/2110 train_time:41354ms step_avg:41.98ms
step:986/2110 train_time:41413ms step_avg:42.00ms
step:987/2110 train_time:41474ms step_avg:42.02ms
step:988/2110 train_time:41534ms step_avg:42.04ms
step:989/2110 train_time:41593ms step_avg:42.06ms
step:990/2110 train_time:41652ms step_avg:42.07ms
step:991/2110 train_time:41713ms step_avg:42.09ms
step:992/2110 train_time:41773ms step_avg:42.11ms
step:993/2110 train_time:41832ms step_avg:42.13ms
step:994/2110 train_time:41892ms step_avg:42.15ms
step:995/2110 train_time:41952ms step_avg:42.16ms
step:996/2110 train_time:42012ms step_avg:42.18ms
step:997/2110 train_time:42071ms step_avg:42.20ms
step:998/2110 train_time:42130ms step_avg:42.21ms
step:999/2110 train_time:42189ms step_avg:42.23ms
step:1000/2110 train_time:42248ms step_avg:42.25ms
step:1000/2110 val_loss:3.7506 train_time:42308ms step_avg:42.31ms
step:1001/2110 train_time:42344ms step_avg:42.30ms
step:1002/2110 train_time:42379ms step_avg:42.29ms
step:1003/2110 train_time:42432ms step_avg:42.30ms
step:1004/2110 train_time:42493ms step_avg:42.32ms
step:1005/2110 train_time:42553ms step_avg:42.34ms
step:1006/2110 train_time:42613ms step_avg:42.36ms
step:1007/2110 train_time:42672ms step_avg:42.38ms
step:1008/2110 train_time:42730ms step_avg:42.39ms
step:1009/2110 train_time:42790ms step_avg:42.41ms
step:1010/2110 train_time:42848ms step_avg:42.42ms
step:1011/2110 train_time:42907ms step_avg:42.44ms
step:1012/2110 train_time:42965ms step_avg:42.46ms
step:1013/2110 train_time:43025ms step_avg:42.47ms
step:1014/2110 train_time:43083ms step_avg:42.49ms
step:1015/2110 train_time:43141ms step_avg:42.50ms
step:1016/2110 train_time:43199ms step_avg:42.52ms
step:1017/2110 train_time:43258ms step_avg:42.53ms
step:1018/2110 train_time:43317ms step_avg:42.55ms
step:1019/2110 train_time:43377ms step_avg:42.57ms
step:1020/2110 train_time:43438ms step_avg:42.59ms
step:1021/2110 train_time:43498ms step_avg:42.60ms
step:1022/2110 train_time:43558ms step_avg:42.62ms
step:1023/2110 train_time:43618ms step_avg:42.64ms
step:1024/2110 train_time:43676ms step_avg:42.65ms
step:1025/2110 train_time:43735ms step_avg:42.67ms
step:1026/2110 train_time:43793ms step_avg:42.68ms
step:1027/2110 train_time:43853ms step_avg:42.70ms
step:1028/2110 train_time:43911ms step_avg:42.72ms
step:1029/2110 train_time:43971ms step_avg:42.73ms
step:1030/2110 train_time:44029ms step_avg:42.75ms
step:1031/2110 train_time:44088ms step_avg:42.76ms
step:1032/2110 train_time:44146ms step_avg:42.78ms
step:1033/2110 train_time:44207ms step_avg:42.79ms
step:1034/2110 train_time:44265ms step_avg:42.81ms
step:1035/2110 train_time:44326ms step_avg:42.83ms
step:1036/2110 train_time:44386ms step_avg:42.84ms
step:1037/2110 train_time:44446ms step_avg:42.86ms
step:1038/2110 train_time:44506ms step_avg:42.88ms
step:1039/2110 train_time:44568ms step_avg:42.89ms
step:1040/2110 train_time:44628ms step_avg:42.91ms
step:1041/2110 train_time:44687ms step_avg:42.93ms
step:1042/2110 train_time:44746ms step_avg:42.94ms
step:1043/2110 train_time:44805ms step_avg:42.96ms
step:1044/2110 train_time:44864ms step_avg:42.97ms
step:1045/2110 train_time:44924ms step_avg:42.99ms
step:1046/2110 train_time:44982ms step_avg:43.00ms
step:1047/2110 train_time:45041ms step_avg:43.02ms
step:1048/2110 train_time:45100ms step_avg:43.03ms
step:1049/2110 train_time:45158ms step_avg:43.05ms
step:1050/2110 train_time:45216ms step_avg:43.06ms
step:1051/2110 train_time:45275ms step_avg:43.08ms
step:1052/2110 train_time:45334ms step_avg:43.09ms
step:1053/2110 train_time:45393ms step_avg:43.11ms
step:1054/2110 train_time:45452ms step_avg:43.12ms
step:1055/2110 train_time:45513ms step_avg:43.14ms
step:1056/2110 train_time:45572ms step_avg:43.16ms
step:1057/2110 train_time:45632ms step_avg:43.17ms
step:1058/2110 train_time:45691ms step_avg:43.19ms
step:1059/2110 train_time:45751ms step_avg:43.20ms
step:1060/2110 train_time:45809ms step_avg:43.22ms
step:1061/2110 train_time:45869ms step_avg:43.23ms
step:1062/2110 train_time:45927ms step_avg:43.25ms
step:1063/2110 train_time:45987ms step_avg:43.26ms
step:1064/2110 train_time:46045ms step_avg:43.28ms
step:1065/2110 train_time:46105ms step_avg:43.29ms
step:1066/2110 train_time:46164ms step_avg:43.31ms
step:1067/2110 train_time:46223ms step_avg:43.32ms
step:1068/2110 train_time:46281ms step_avg:43.33ms
step:1069/2110 train_time:46340ms step_avg:43.35ms
step:1070/2110 train_time:46400ms step_avg:43.36ms
step:1071/2110 train_time:46459ms step_avg:43.38ms
step:1072/2110 train_time:46519ms step_avg:43.39ms
step:1073/2110 train_time:46578ms step_avg:43.41ms
step:1074/2110 train_time:46638ms step_avg:43.42ms
step:1075/2110 train_time:46697ms step_avg:43.44ms
step:1076/2110 train_time:46756ms step_avg:43.45ms
step:1077/2110 train_time:46816ms step_avg:43.47ms
step:1078/2110 train_time:46874ms step_avg:43.48ms
step:1079/2110 train_time:46934ms step_avg:43.50ms
step:1080/2110 train_time:46992ms step_avg:43.51ms
step:1081/2110 train_time:47051ms step_avg:43.53ms
step:1082/2110 train_time:47109ms step_avg:43.54ms
step:1083/2110 train_time:47169ms step_avg:43.55ms
step:1084/2110 train_time:47228ms step_avg:43.57ms
step:1085/2110 train_time:47288ms step_avg:43.58ms
step:1086/2110 train_time:47346ms step_avg:43.60ms
step:1087/2110 train_time:47406ms step_avg:43.61ms
step:1088/2110 train_time:47466ms step_avg:43.63ms
step:1089/2110 train_time:47526ms step_avg:43.64ms
step:1090/2110 train_time:47585ms step_avg:43.66ms
step:1091/2110 train_time:47645ms step_avg:43.67ms
step:1092/2110 train_time:47704ms step_avg:43.69ms
step:1093/2110 train_time:47765ms step_avg:43.70ms
step:1094/2110 train_time:47824ms step_avg:43.72ms
step:1095/2110 train_time:47883ms step_avg:43.73ms
step:1096/2110 train_time:47942ms step_avg:43.74ms
step:1097/2110 train_time:48002ms step_avg:43.76ms
step:1098/2110 train_time:48060ms step_avg:43.77ms
step:1099/2110 train_time:48119ms step_avg:43.78ms
step:1100/2110 train_time:48178ms step_avg:43.80ms
step:1101/2110 train_time:48238ms step_avg:43.81ms
step:1102/2110 train_time:48296ms step_avg:43.83ms
step:1103/2110 train_time:48356ms step_avg:43.84ms
step:1104/2110 train_time:48415ms step_avg:43.85ms
step:1105/2110 train_time:48476ms step_avg:43.87ms
step:1106/2110 train_time:48534ms step_avg:43.88ms
step:1107/2110 train_time:48594ms step_avg:43.90ms
step:1108/2110 train_time:48654ms step_avg:43.91ms
step:1109/2110 train_time:48714ms step_avg:43.93ms
step:1110/2110 train_time:48773ms step_avg:43.94ms
step:1111/2110 train_time:48833ms step_avg:43.95ms
step:1112/2110 train_time:48891ms step_avg:43.97ms
step:1113/2110 train_time:48950ms step_avg:43.98ms
step:1114/2110 train_time:49009ms step_avg:43.99ms
step:1115/2110 train_time:49069ms step_avg:44.01ms
step:1116/2110 train_time:49127ms step_avg:44.02ms
step:1117/2110 train_time:49188ms step_avg:44.04ms
step:1118/2110 train_time:49246ms step_avg:44.05ms
step:1119/2110 train_time:49307ms step_avg:44.06ms
step:1120/2110 train_time:49366ms step_avg:44.08ms
step:1121/2110 train_time:49426ms step_avg:44.09ms
step:1122/2110 train_time:49486ms step_avg:44.11ms
step:1123/2110 train_time:49545ms step_avg:44.12ms
step:1124/2110 train_time:49604ms step_avg:44.13ms
step:1125/2110 train_time:49664ms step_avg:44.15ms
step:1126/2110 train_time:49724ms step_avg:44.16ms
step:1127/2110 train_time:49783ms step_avg:44.17ms
step:1128/2110 train_time:49842ms step_avg:44.19ms
step:1129/2110 train_time:49902ms step_avg:44.20ms
step:1130/2110 train_time:49960ms step_avg:44.21ms
step:1131/2110 train_time:50019ms step_avg:44.23ms
step:1132/2110 train_time:50078ms step_avg:44.24ms
step:1133/2110 train_time:50138ms step_avg:44.25ms
step:1134/2110 train_time:50195ms step_avg:44.26ms
step:1135/2110 train_time:50256ms step_avg:44.28ms
step:1136/2110 train_time:50314ms step_avg:44.29ms
step:1137/2110 train_time:50374ms step_avg:44.30ms
step:1138/2110 train_time:50432ms step_avg:44.32ms
step:1139/2110 train_time:50492ms step_avg:44.33ms
step:1140/2110 train_time:50551ms step_avg:44.34ms
step:1141/2110 train_time:50612ms step_avg:44.36ms
step:1142/2110 train_time:50671ms step_avg:44.37ms
step:1143/2110 train_time:50731ms step_avg:44.38ms
step:1144/2110 train_time:50790ms step_avg:44.40ms
step:1145/2110 train_time:50851ms step_avg:44.41ms
step:1146/2110 train_time:50909ms step_avg:44.42ms
step:1147/2110 train_time:50970ms step_avg:44.44ms
step:1148/2110 train_time:51029ms step_avg:44.45ms
step:1149/2110 train_time:51089ms step_avg:44.46ms
step:1150/2110 train_time:51148ms step_avg:44.48ms
step:1151/2110 train_time:51208ms step_avg:44.49ms
step:1152/2110 train_time:51267ms step_avg:44.50ms
step:1153/2110 train_time:51328ms step_avg:44.52ms
step:1154/2110 train_time:51387ms step_avg:44.53ms
step:1155/2110 train_time:51448ms step_avg:44.54ms
step:1156/2110 train_time:51507ms step_avg:44.56ms
step:1157/2110 train_time:51568ms step_avg:44.57ms
step:1158/2110 train_time:51627ms step_avg:44.58ms
step:1159/2110 train_time:51688ms step_avg:44.60ms
step:1160/2110 train_time:51748ms step_avg:44.61ms
step:1161/2110 train_time:51808ms step_avg:44.62ms
step:1162/2110 train_time:51867ms step_avg:44.64ms
step:1163/2110 train_time:51928ms step_avg:44.65ms
step:1164/2110 train_time:51987ms step_avg:44.66ms
step:1165/2110 train_time:52047ms step_avg:44.68ms
step:1166/2110 train_time:52106ms step_avg:44.69ms
step:1167/2110 train_time:52167ms step_avg:44.70ms
step:1168/2110 train_time:52226ms step_avg:44.71ms
step:1169/2110 train_time:52287ms step_avg:44.73ms
step:1170/2110 train_time:52346ms step_avg:44.74ms
step:1171/2110 train_time:52408ms step_avg:44.75ms
step:1172/2110 train_time:52468ms step_avg:44.77ms
step:1173/2110 train_time:52527ms step_avg:44.78ms
step:1174/2110 train_time:52586ms step_avg:44.79ms
step:1175/2110 train_time:52646ms step_avg:44.81ms
step:1176/2110 train_time:52705ms step_avg:44.82ms
step:1177/2110 train_time:52767ms step_avg:44.83ms
step:1178/2110 train_time:52827ms step_avg:44.84ms
step:1179/2110 train_time:52887ms step_avg:44.86ms
step:1180/2110 train_time:52946ms step_avg:44.87ms
step:1181/2110 train_time:53007ms step_avg:44.88ms
step:1182/2110 train_time:53066ms step_avg:44.90ms
step:1183/2110 train_time:53127ms step_avg:44.91ms
step:1184/2110 train_time:53186ms step_avg:44.92ms
step:1185/2110 train_time:53246ms step_avg:44.93ms
step:1186/2110 train_time:53305ms step_avg:44.95ms
step:1187/2110 train_time:53366ms step_avg:44.96ms
step:1188/2110 train_time:53426ms step_avg:44.97ms
step:1189/2110 train_time:53486ms step_avg:44.98ms
step:1190/2110 train_time:53546ms step_avg:45.00ms
step:1191/2110 train_time:53606ms step_avg:45.01ms
step:1192/2110 train_time:53666ms step_avg:45.02ms
step:1193/2110 train_time:53726ms step_avg:45.03ms
step:1194/2110 train_time:53785ms step_avg:45.05ms
step:1195/2110 train_time:53845ms step_avg:45.06ms
step:1196/2110 train_time:53904ms step_avg:45.07ms
step:1197/2110 train_time:53964ms step_avg:45.08ms
step:1198/2110 train_time:54024ms step_avg:45.10ms
step:1199/2110 train_time:54084ms step_avg:45.11ms
step:1200/2110 train_time:54144ms step_avg:45.12ms
step:1201/2110 train_time:54204ms step_avg:45.13ms
step:1202/2110 train_time:54264ms step_avg:45.14ms
step:1203/2110 train_time:54325ms step_avg:45.16ms
step:1204/2110 train_time:54384ms step_avg:45.17ms
step:1205/2110 train_time:54443ms step_avg:45.18ms
step:1206/2110 train_time:54503ms step_avg:45.19ms
step:1207/2110 train_time:54563ms step_avg:45.21ms
step:1208/2110 train_time:54622ms step_avg:45.22ms
step:1209/2110 train_time:54681ms step_avg:45.23ms
step:1210/2110 train_time:54741ms step_avg:45.24ms
step:1211/2110 train_time:54800ms step_avg:45.25ms
step:1212/2110 train_time:54860ms step_avg:45.26ms
step:1213/2110 train_time:54920ms step_avg:45.28ms
step:1214/2110 train_time:54979ms step_avg:45.29ms
step:1215/2110 train_time:55039ms step_avg:45.30ms
step:1216/2110 train_time:55097ms step_avg:45.31ms
step:1217/2110 train_time:55158ms step_avg:45.32ms
step:1218/2110 train_time:55217ms step_avg:45.33ms
step:1219/2110 train_time:55277ms step_avg:45.35ms
step:1220/2110 train_time:55336ms step_avg:45.36ms
step:1221/2110 train_time:55397ms step_avg:45.37ms
step:1222/2110 train_time:55456ms step_avg:45.38ms
step:1223/2110 train_time:55516ms step_avg:45.39ms
step:1224/2110 train_time:55575ms step_avg:45.40ms
step:1225/2110 train_time:55636ms step_avg:45.42ms
step:1226/2110 train_time:55694ms step_avg:45.43ms
step:1227/2110 train_time:55755ms step_avg:45.44ms
step:1228/2110 train_time:55813ms step_avg:45.45ms
step:1229/2110 train_time:55874ms step_avg:45.46ms
step:1230/2110 train_time:55933ms step_avg:45.47ms
step:1231/2110 train_time:55993ms step_avg:45.49ms
step:1232/2110 train_time:56052ms step_avg:45.50ms
step:1233/2110 train_time:56113ms step_avg:45.51ms
step:1234/2110 train_time:56172ms step_avg:45.52ms
step:1235/2110 train_time:56232ms step_avg:45.53ms
step:1236/2110 train_time:56291ms step_avg:45.54ms
step:1237/2110 train_time:56352ms step_avg:45.56ms
step:1238/2110 train_time:56411ms step_avg:45.57ms
step:1239/2110 train_time:56472ms step_avg:45.58ms
step:1240/2110 train_time:56531ms step_avg:45.59ms
step:1241/2110 train_time:56591ms step_avg:45.60ms
step:1242/2110 train_time:56650ms step_avg:45.61ms
step:1243/2110 train_time:56711ms step_avg:45.62ms
step:1244/2110 train_time:56770ms step_avg:45.63ms
step:1245/2110 train_time:56830ms step_avg:45.65ms
step:1246/2110 train_time:56889ms step_avg:45.66ms
step:1247/2110 train_time:56949ms step_avg:45.67ms
step:1248/2110 train_time:57009ms step_avg:45.68ms
step:1249/2110 train_time:57069ms step_avg:45.69ms
step:1250/2110 train_time:57128ms step_avg:45.70ms
step:1250/2110 val_loss:3.5892 train_time:57190ms step_avg:45.75ms
step:1251/2110 train_time:57226ms step_avg:45.74ms
step:1252/2110 train_time:57262ms step_avg:45.74ms
step:1253/2110 train_time:57315ms step_avg:45.74ms
step:1254/2110 train_time:57376ms step_avg:45.75ms
step:1255/2110 train_time:57437ms step_avg:45.77ms
step:1256/2110 train_time:57497ms step_avg:45.78ms
step:1257/2110 train_time:57557ms step_avg:45.79ms
step:1258/2110 train_time:57616ms step_avg:45.80ms
step:1259/2110 train_time:57675ms step_avg:45.81ms
step:1260/2110 train_time:57734ms step_avg:45.82ms
step:1261/2110 train_time:57794ms step_avg:45.83ms
step:1262/2110 train_time:57852ms step_avg:45.84ms
step:1263/2110 train_time:57912ms step_avg:45.85ms
step:1264/2110 train_time:57972ms step_avg:45.86ms
step:1265/2110 train_time:58030ms step_avg:45.87ms
step:1266/2110 train_time:58089ms step_avg:45.88ms
step:1267/2110 train_time:58150ms step_avg:45.90ms
step:1268/2110 train_time:58212ms step_avg:45.91ms
step:1269/2110 train_time:58274ms step_avg:45.92ms
step:1270/2110 train_time:58335ms step_avg:45.93ms
step:1271/2110 train_time:58397ms step_avg:45.95ms
step:1272/2110 train_time:58457ms step_avg:45.96ms
step:1273/2110 train_time:58517ms step_avg:45.97ms
step:1274/2110 train_time:58578ms step_avg:45.98ms
step:1275/2110 train_time:58636ms step_avg:45.99ms
step:1276/2110 train_time:58695ms step_avg:46.00ms
step:1277/2110 train_time:58755ms step_avg:46.01ms
step:1278/2110 train_time:58814ms step_avg:46.02ms
step:1279/2110 train_time:58873ms step_avg:46.03ms
step:1280/2110 train_time:58932ms step_avg:46.04ms
step:1281/2110 train_time:58991ms step_avg:46.05ms
step:1282/2110 train_time:59050ms step_avg:46.06ms
step:1283/2110 train_time:59111ms step_avg:46.07ms
step:1284/2110 train_time:59171ms step_avg:46.08ms
step:1285/2110 train_time:59232ms step_avg:46.10ms
step:1286/2110 train_time:59293ms step_avg:46.11ms
step:1287/2110 train_time:59354ms step_avg:46.12ms
step:1288/2110 train_time:59415ms step_avg:46.13ms
step:1289/2110 train_time:59475ms step_avg:46.14ms
step:1290/2110 train_time:59534ms step_avg:46.15ms
step:1291/2110 train_time:59595ms step_avg:46.16ms
step:1292/2110 train_time:59655ms step_avg:46.17ms
step:1293/2110 train_time:59714ms step_avg:46.18ms
step:1294/2110 train_time:59774ms step_avg:46.19ms
step:1295/2110 train_time:59833ms step_avg:46.20ms
step:1296/2110 train_time:59892ms step_avg:46.21ms
step:1297/2110 train_time:59951ms step_avg:46.22ms
step:1298/2110 train_time:60010ms step_avg:46.23ms
step:1299/2110 train_time:60071ms step_avg:46.24ms
step:1300/2110 train_time:60131ms step_avg:46.25ms
step:1301/2110 train_time:60190ms step_avg:46.26ms
step:1302/2110 train_time:60250ms step_avg:46.28ms
step:1303/2110 train_time:60311ms step_avg:46.29ms
step:1304/2110 train_time:60373ms step_avg:46.30ms
step:1305/2110 train_time:60434ms step_avg:46.31ms
step:1306/2110 train_time:60494ms step_avg:46.32ms
step:1307/2110 train_time:60554ms step_avg:46.33ms
step:1308/2110 train_time:60614ms step_avg:46.34ms
step:1309/2110 train_time:60673ms step_avg:46.35ms
step:1310/2110 train_time:60733ms step_avg:46.36ms
step:1311/2110 train_time:60792ms step_avg:46.37ms
step:1312/2110 train_time:60851ms step_avg:46.38ms
step:1313/2110 train_time:60911ms step_avg:46.39ms
step:1314/2110 train_time:60971ms step_avg:46.40ms
step:1315/2110 train_time:61030ms step_avg:46.41ms
step:1316/2110 train_time:61090ms step_avg:46.42ms
step:1317/2110 train_time:61149ms step_avg:46.43ms
step:1318/2110 train_time:61210ms step_avg:46.44ms
step:1319/2110 train_time:61271ms step_avg:46.45ms
step:1320/2110 train_time:61331ms step_avg:46.46ms
step:1321/2110 train_time:61392ms step_avg:46.47ms
step:1322/2110 train_time:61451ms step_avg:46.48ms
step:1323/2110 train_time:61512ms step_avg:46.49ms
step:1324/2110 train_time:61573ms step_avg:46.50ms
step:1325/2110 train_time:61632ms step_avg:46.51ms
step:1326/2110 train_time:61692ms step_avg:46.52ms
step:1327/2110 train_time:61752ms step_avg:46.53ms
step:1328/2110 train_time:61811ms step_avg:46.54ms
step:1329/2110 train_time:61871ms step_avg:46.55ms
step:1330/2110 train_time:61931ms step_avg:46.56ms
step:1331/2110 train_time:61990ms step_avg:46.57ms
step:1332/2110 train_time:62049ms step_avg:46.58ms
step:1333/2110 train_time:62109ms step_avg:46.59ms
step:1334/2110 train_time:62169ms step_avg:46.60ms
step:1335/2110 train_time:62229ms step_avg:46.61ms
step:1336/2110 train_time:62291ms step_avg:46.62ms
step:1337/2110 train_time:62351ms step_avg:46.64ms
step:1338/2110 train_time:62412ms step_avg:46.65ms
step:1339/2110 train_time:62472ms step_avg:46.66ms
step:1340/2110 train_time:62533ms step_avg:46.67ms
step:1341/2110 train_time:62593ms step_avg:46.68ms
step:1342/2110 train_time:62653ms step_avg:46.69ms
step:1343/2110 train_time:62713ms step_avg:46.70ms
step:1344/2110 train_time:62773ms step_avg:46.71ms
step:1345/2110 train_time:62832ms step_avg:46.72ms
step:1346/2110 train_time:62892ms step_avg:46.73ms
step:1347/2110 train_time:62952ms step_avg:46.73ms
step:1348/2110 train_time:63011ms step_avg:46.74ms
step:1349/2110 train_time:63071ms step_avg:46.75ms
step:1350/2110 train_time:63131ms step_avg:46.76ms
step:1351/2110 train_time:63191ms step_avg:46.77ms
step:1352/2110 train_time:63250ms step_avg:46.78ms
step:1353/2110 train_time:63310ms step_avg:46.79ms
step:1354/2110 train_time:63371ms step_avg:46.80ms
step:1355/2110 train_time:63431ms step_avg:46.81ms
step:1356/2110 train_time:63492ms step_avg:46.82ms
step:1357/2110 train_time:63551ms step_avg:46.83ms
step:1358/2110 train_time:63611ms step_avg:46.84ms
step:1359/2110 train_time:63672ms step_avg:46.85ms
step:1360/2110 train_time:63732ms step_avg:46.86ms
step:1361/2110 train_time:63791ms step_avg:46.87ms
step:1362/2110 train_time:63850ms step_avg:46.88ms
step:1363/2110 train_time:63910ms step_avg:46.89ms
step:1364/2110 train_time:63971ms step_avg:46.90ms
step:1365/2110 train_time:64030ms step_avg:46.91ms
step:1366/2110 train_time:64090ms step_avg:46.92ms
step:1367/2110 train_time:64151ms step_avg:46.93ms
step:1368/2110 train_time:64211ms step_avg:46.94ms
step:1369/2110 train_time:64271ms step_avg:46.95ms
step:1370/2110 train_time:64331ms step_avg:46.96ms
step:1371/2110 train_time:64392ms step_avg:46.97ms
step:1372/2110 train_time:64451ms step_avg:46.98ms
step:1373/2110 train_time:64512ms step_avg:46.99ms
step:1374/2110 train_time:64573ms step_avg:47.00ms
step:1375/2110 train_time:64634ms step_avg:47.01ms
step:1376/2110 train_time:64694ms step_avg:47.02ms
step:1377/2110 train_time:64753ms step_avg:47.02ms
step:1378/2110 train_time:64812ms step_avg:47.03ms
step:1379/2110 train_time:64872ms step_avg:47.04ms
step:1380/2110 train_time:64932ms step_avg:47.05ms
step:1381/2110 train_time:64992ms step_avg:47.06ms
step:1382/2110 train_time:65080ms step_avg:47.09ms
step:1383/2110 train_time:65167ms step_avg:47.12ms
step:1384/2110 train_time:65253ms step_avg:47.15ms
step:1385/2110 train_time:65341ms step_avg:47.18ms
step:1386/2110 train_time:65428ms step_avg:47.21ms
step:1387/2110 train_time:65514ms step_avg:47.23ms
step:1388/2110 train_time:65602ms step_avg:47.26ms
step:1389/2110 train_time:65689ms step_avg:47.29ms
step:1390/2110 train_time:65776ms step_avg:47.32ms
step:1391/2110 train_time:65862ms step_avg:47.35ms
step:1392/2110 train_time:65949ms step_avg:47.38ms
step:1393/2110 train_time:66036ms step_avg:47.41ms
step:1394/2110 train_time:66123ms step_avg:47.43ms
step:1395/2110 train_time:66210ms step_avg:47.46ms
step:1396/2110 train_time:66298ms step_avg:47.49ms
step:1397/2110 train_time:66384ms step_avg:47.52ms
step:1398/2110 train_time:66471ms step_avg:47.55ms
step:1399/2110 train_time:66558ms step_avg:47.58ms
step:1400/2110 train_time:66646ms step_avg:47.60ms
step:1401/2110 train_time:66732ms step_avg:47.63ms
step:1402/2110 train_time:66820ms step_avg:47.66ms
step:1403/2110 train_time:66906ms step_avg:47.69ms
step:1404/2110 train_time:66993ms step_avg:47.72ms
step:1405/2110 train_time:67078ms step_avg:47.74ms
step:1406/2110 train_time:67165ms step_avg:47.77ms
step:1407/2110 train_time:67252ms step_avg:47.80ms
step:1408/2110 train_time:67339ms step_avg:47.83ms
step:1409/2110 train_time:67426ms step_avg:47.85ms
step:1410/2110 train_time:67512ms step_avg:47.88ms
step:1411/2110 train_time:67599ms step_avg:47.91ms
step:1412/2110 train_time:67686ms step_avg:47.94ms
step:1413/2110 train_time:67772ms step_avg:47.96ms
step:1414/2110 train_time:67859ms step_avg:47.99ms
step:1415/2110 train_time:67946ms step_avg:48.02ms
step:1416/2110 train_time:68032ms step_avg:48.05ms
step:1417/2110 train_time:68120ms step_avg:48.07ms
step:1418/2110 train_time:68207ms step_avg:48.10ms
step:1419/2110 train_time:68293ms step_avg:48.13ms
step:1420/2110 train_time:68380ms step_avg:48.15ms
step:1421/2110 train_time:68467ms step_avg:48.18ms
step:1422/2110 train_time:68553ms step_avg:48.21ms
step:1423/2110 train_time:68641ms step_avg:48.24ms
step:1424/2110 train_time:68727ms step_avg:48.26ms
step:1425/2110 train_time:68813ms step_avg:48.29ms
step:1426/2110 train_time:68900ms step_avg:48.32ms
step:1427/2110 train_time:68987ms step_avg:48.34ms
step:1428/2110 train_time:69073ms step_avg:48.37ms
step:1429/2110 train_time:69160ms step_avg:48.40ms
step:1430/2110 train_time:69248ms step_avg:48.42ms
step:1431/2110 train_time:69333ms step_avg:48.45ms
step:1432/2110 train_time:69421ms step_avg:48.48ms
step:1433/2110 train_time:69508ms step_avg:48.51ms
step:1434/2110 train_time:69595ms step_avg:48.53ms
step:1435/2110 train_time:69681ms step_avg:48.56ms
step:1436/2110 train_time:69768ms step_avg:48.58ms
step:1437/2110 train_time:69854ms step_avg:48.61ms
step:1438/2110 train_time:69942ms step_avg:48.64ms
step:1439/2110 train_time:70028ms step_avg:48.66ms
step:1440/2110 train_time:70115ms step_avg:48.69ms
step:1441/2110 train_time:70202ms step_avg:48.72ms
step:1442/2110 train_time:70288ms step_avg:48.74ms
step:1443/2110 train_time:70376ms step_avg:48.77ms
step:1444/2110 train_time:70463ms step_avg:48.80ms
step:1445/2110 train_time:70549ms step_avg:48.82ms
step:1446/2110 train_time:70636ms step_avg:48.85ms
step:1447/2110 train_time:70723ms step_avg:48.88ms
step:1448/2110 train_time:70810ms step_avg:48.90ms
step:1449/2110 train_time:70896ms step_avg:48.93ms
step:1450/2110 train_time:70982ms step_avg:48.95ms
step:1451/2110 train_time:71069ms step_avg:48.98ms
step:1452/2110 train_time:71155ms step_avg:49.01ms
step:1453/2110 train_time:71243ms step_avg:49.03ms
step:1454/2110 train_time:71329ms step_avg:49.06ms
step:1455/2110 train_time:71417ms step_avg:49.08ms
step:1456/2110 train_time:71504ms step_avg:49.11ms
step:1457/2110 train_time:71590ms step_avg:49.14ms
step:1458/2110 train_time:71677ms step_avg:49.16ms
step:1459/2110 train_time:71764ms step_avg:49.19ms
step:1460/2110 train_time:71850ms step_avg:49.21ms
step:1461/2110 train_time:71936ms step_avg:49.24ms
step:1462/2110 train_time:72023ms step_avg:49.26ms
step:1463/2110 train_time:72109ms step_avg:49.29ms
step:1464/2110 train_time:72196ms step_avg:49.31ms
step:1465/2110 train_time:72282ms step_avg:49.34ms
step:1466/2110 train_time:72368ms step_avg:49.36ms
step:1467/2110 train_time:72455ms step_avg:49.39ms
step:1468/2110 train_time:72543ms step_avg:49.42ms
step:1469/2110 train_time:72630ms step_avg:49.44ms
step:1470/2110 train_time:72716ms step_avg:49.47ms
step:1471/2110 train_time:72803ms step_avg:49.49ms
step:1472/2110 train_time:72889ms step_avg:49.52ms
step:1473/2110 train_time:72976ms step_avg:49.54ms
step:1474/2110 train_time:73064ms step_avg:49.57ms
step:1475/2110 train_time:73150ms step_avg:49.59ms
step:1476/2110 train_time:73236ms step_avg:49.62ms
step:1477/2110 train_time:73323ms step_avg:49.64ms
step:1478/2110 train_time:73410ms step_avg:49.67ms
step:1479/2110 train_time:73497ms step_avg:49.69ms
step:1480/2110 train_time:73584ms step_avg:49.72ms
step:1481/2110 train_time:73672ms step_avg:49.74ms
step:1482/2110 train_time:73758ms step_avg:49.77ms
step:1483/2110 train_time:73845ms step_avg:49.79ms
step:1484/2110 train_time:73931ms step_avg:49.82ms
step:1485/2110 train_time:74018ms step_avg:49.84ms
step:1486/2110 train_time:74105ms step_avg:49.87ms
step:1487/2110 train_time:74191ms step_avg:49.89ms
step:1488/2110 train_time:74278ms step_avg:49.92ms
step:1489/2110 train_time:74365ms step_avg:49.94ms
step:1490/2110 train_time:74451ms step_avg:49.97ms
step:1491/2110 train_time:74538ms step_avg:49.99ms
step:1492/2110 train_time:74626ms step_avg:50.02ms
step:1493/2110 train_time:74712ms step_avg:50.04ms
step:1494/2110 train_time:74798ms step_avg:50.07ms
step:1495/2110 train_time:74885ms step_avg:50.09ms
step:1496/2110 train_time:74971ms step_avg:50.11ms
step:1497/2110 train_time:75059ms step_avg:50.14ms
step:1498/2110 train_time:75146ms step_avg:50.16ms
step:1499/2110 train_time:75232ms step_avg:50.19ms
step:1500/2110 train_time:75320ms step_avg:50.21ms
step:1500/2110 val_loss:3.4896 train_time:75407ms step_avg:50.27ms
step:1501/2110 train_time:75443ms step_avg:50.26ms
step:1502/2110 train_time:75498ms step_avg:50.27ms
step:1503/2110 train_time:75589ms step_avg:50.29ms
step:1504/2110 train_time:75676ms step_avg:50.32ms
step:1505/2110 train_time:75762ms step_avg:50.34ms
step:1506/2110 train_time:75848ms step_avg:50.36ms
step:1507/2110 train_time:75934ms step_avg:50.39ms
step:1508/2110 train_time:76019ms step_avg:50.41ms
step:1509/2110 train_time:76105ms step_avg:50.43ms
step:1510/2110 train_time:76193ms step_avg:50.46ms
step:1511/2110 train_time:76278ms step_avg:50.48ms
step:1512/2110 train_time:76365ms step_avg:50.51ms
step:1513/2110 train_time:76454ms step_avg:50.53ms
step:1514/2110 train_time:76543ms step_avg:50.56ms
step:1515/2110 train_time:76631ms step_avg:50.58ms
step:1516/2110 train_time:76718ms step_avg:50.61ms
step:1517/2110 train_time:76805ms step_avg:50.63ms
step:1518/2110 train_time:76891ms step_avg:50.65ms
step:1519/2110 train_time:76977ms step_avg:50.68ms
step:1520/2110 train_time:77063ms step_avg:50.70ms
step:1521/2110 train_time:77148ms step_avg:50.72ms
step:1522/2110 train_time:77235ms step_avg:50.75ms
step:1523/2110 train_time:77321ms step_avg:50.77ms
step:1524/2110 train_time:77408ms step_avg:50.79ms
step:1525/2110 train_time:77497ms step_avg:50.82ms
step:1526/2110 train_time:77586ms step_avg:50.84ms
step:1527/2110 train_time:77673ms step_avg:50.87ms
step:1528/2110 train_time:77759ms step_avg:50.89ms
step:1529/2110 train_time:77845ms step_avg:50.91ms
step:1530/2110 train_time:77932ms step_avg:50.94ms
step:1531/2110 train_time:78018ms step_avg:50.96ms
step:1532/2110 train_time:78104ms step_avg:50.98ms
step:1533/2110 train_time:78190ms step_avg:51.00ms
step:1534/2110 train_time:78277ms step_avg:51.03ms
step:1535/2110 train_time:78363ms step_avg:51.05ms
step:1536/2110 train_time:78451ms step_avg:51.08ms
step:1537/2110 train_time:78538ms step_avg:51.10ms
step:1538/2110 train_time:78625ms step_avg:51.12ms
step:1539/2110 train_time:78713ms step_avg:51.15ms
step:1540/2110 train_time:78799ms step_avg:51.17ms
step:1541/2110 train_time:78887ms step_avg:51.19ms
step:1542/2110 train_time:78974ms step_avg:51.22ms
step:1543/2110 train_time:79059ms step_avg:51.24ms
step:1544/2110 train_time:79145ms step_avg:51.26ms
step:1545/2110 train_time:79232ms step_avg:51.28ms
step:1546/2110 train_time:79318ms step_avg:51.31ms
step:1547/2110 train_time:79405ms step_avg:51.33ms
step:1548/2110 train_time:79494ms step_avg:51.35ms
step:1549/2110 train_time:79580ms step_avg:51.37ms
step:1550/2110 train_time:79667ms step_avg:51.40ms
step:1551/2110 train_time:79754ms step_avg:51.42ms
step:1552/2110 train_time:79841ms step_avg:51.44ms
step:1553/2110 train_time:79927ms step_avg:51.47ms
step:1554/2110 train_time:80014ms step_avg:51.49ms
step:1555/2110 train_time:80100ms step_avg:51.51ms
step:1556/2110 train_time:80187ms step_avg:51.53ms
step:1557/2110 train_time:80273ms step_avg:51.56ms
step:1558/2110 train_time:80360ms step_avg:51.58ms
step:1559/2110 train_time:80446ms step_avg:51.60ms
step:1560/2110 train_time:80534ms step_avg:51.62ms
step:1561/2110 train_time:80621ms step_avg:51.65ms
step:1562/2110 train_time:80708ms step_avg:51.67ms
step:1563/2110 train_time:80795ms step_avg:51.69ms
step:1564/2110 train_time:80882ms step_avg:51.72ms
step:1565/2110 train_time:80969ms step_avg:51.74ms
step:1566/2110 train_time:81055ms step_avg:51.76ms
step:1567/2110 train_time:81141ms step_avg:51.78ms
step:1568/2110 train_time:81227ms step_avg:51.80ms
step:1569/2110 train_time:81313ms step_avg:51.82ms
step:1570/2110 train_time:81400ms step_avg:51.85ms
step:1571/2110 train_time:81487ms step_avg:51.87ms
step:1572/2110 train_time:81575ms step_avg:51.89ms
step:1573/2110 train_time:81661ms step_avg:51.91ms
step:1574/2110 train_time:81749ms step_avg:51.94ms
step:1575/2110 train_time:81836ms step_avg:51.96ms
step:1576/2110 train_time:81923ms step_avg:51.98ms
step:1577/2110 train_time:82009ms step_avg:52.00ms
step:1578/2110 train_time:82095ms step_avg:52.02ms
step:1579/2110 train_time:82182ms step_avg:52.05ms
step:1580/2110 train_time:82269ms step_avg:52.07ms
step:1581/2110 train_time:82356ms step_avg:52.09ms
step:1582/2110 train_time:82443ms step_avg:52.11ms
step:1583/2110 train_time:82529ms step_avg:52.13ms
step:1584/2110 train_time:82616ms step_avg:52.16ms
step:1585/2110 train_time:82702ms step_avg:52.18ms
step:1586/2110 train_time:82789ms step_avg:52.20ms
step:1587/2110 train_time:82876ms step_avg:52.22ms
step:1588/2110 train_time:82963ms step_avg:52.24ms
step:1589/2110 train_time:83049ms step_avg:52.26ms
step:1590/2110 train_time:83135ms step_avg:52.29ms
step:1591/2110 train_time:83223ms step_avg:52.31ms
step:1592/2110 train_time:83310ms step_avg:52.33ms
step:1593/2110 train_time:83396ms step_avg:52.35ms
step:1594/2110 train_time:83483ms step_avg:52.37ms
step:1595/2110 train_time:83569ms step_avg:52.39ms
step:1596/2110 train_time:83656ms step_avg:52.42ms
step:1597/2110 train_time:83742ms step_avg:52.44ms
step:1598/2110 train_time:83829ms step_avg:52.46ms
step:1599/2110 train_time:83916ms step_avg:52.48ms
step:1600/2110 train_time:84003ms step_avg:52.50ms
step:1601/2110 train_time:84089ms step_avg:52.52ms
step:1602/2110 train_time:84176ms step_avg:52.54ms
step:1603/2110 train_time:84262ms step_avg:52.57ms
step:1604/2110 train_time:84348ms step_avg:52.59ms
step:1605/2110 train_time:84435ms step_avg:52.61ms
step:1606/2110 train_time:84523ms step_avg:52.63ms
step:1607/2110 train_time:84609ms step_avg:52.65ms
step:1608/2110 train_time:84696ms step_avg:52.67ms
step:1609/2110 train_time:84782ms step_avg:52.69ms
step:1610/2110 train_time:84869ms step_avg:52.71ms
step:1611/2110 train_time:84956ms step_avg:52.74ms
step:1612/2110 train_time:85043ms step_avg:52.76ms
step:1613/2110 train_time:85131ms step_avg:52.78ms
step:1614/2110 train_time:85216ms step_avg:52.80ms
step:1615/2110 train_time:85303ms step_avg:52.82ms
step:1616/2110 train_time:85390ms step_avg:52.84ms
step:1617/2110 train_time:85476ms step_avg:52.86ms
step:1618/2110 train_time:85564ms step_avg:52.88ms
step:1619/2110 train_time:85651ms step_avg:52.90ms
step:1620/2110 train_time:85737ms step_avg:52.92ms
step:1621/2110 train_time:85824ms step_avg:52.95ms
step:1622/2110 train_time:85912ms step_avg:52.97ms
step:1623/2110 train_time:85998ms step_avg:52.99ms
step:1624/2110 train_time:86086ms step_avg:53.01ms
step:1625/2110 train_time:86173ms step_avg:53.03ms
step:1626/2110 train_time:86259ms step_avg:53.05ms
step:1627/2110 train_time:86345ms step_avg:53.07ms
step:1628/2110 train_time:86432ms step_avg:53.09ms
step:1629/2110 train_time:86519ms step_avg:53.11ms
step:1630/2110 train_time:86605ms step_avg:53.13ms
step:1631/2110 train_time:86692ms step_avg:53.15ms
step:1632/2110 train_time:86779ms step_avg:53.17ms
step:1633/2110 train_time:86866ms step_avg:53.19ms
step:1634/2110 train_time:86954ms step_avg:53.22ms
step:1635/2110 train_time:87040ms step_avg:53.24ms
step:1636/2110 train_time:87128ms step_avg:53.26ms
step:1637/2110 train_time:87215ms step_avg:53.28ms
step:1638/2110 train_time:87301ms step_avg:53.30ms
step:1639/2110 train_time:87388ms step_avg:53.32ms
step:1640/2110 train_time:87476ms step_avg:53.34ms
step:1641/2110 train_time:87561ms step_avg:53.36ms
step:1642/2110 train_time:87649ms step_avg:53.38ms
step:1643/2110 train_time:87735ms step_avg:53.40ms
step:1644/2110 train_time:87823ms step_avg:53.42ms
step:1645/2110 train_time:87909ms step_avg:53.44ms
step:1646/2110 train_time:87996ms step_avg:53.46ms
step:1647/2110 train_time:88083ms step_avg:53.48ms
step:1648/2110 train_time:88171ms step_avg:53.50ms
step:1649/2110 train_time:88256ms step_avg:53.52ms
step:1650/2110 train_time:88344ms step_avg:53.54ms
step:1651/2110 train_time:88430ms step_avg:53.56ms
step:1652/2110 train_time:88517ms step_avg:53.58ms
step:1653/2110 train_time:88603ms step_avg:53.60ms
step:1654/2110 train_time:88690ms step_avg:53.62ms
step:1655/2110 train_time:88776ms step_avg:53.64ms
step:1656/2110 train_time:88864ms step_avg:53.66ms
step:1657/2110 train_time:88951ms step_avg:53.68ms
step:1658/2110 train_time:89039ms step_avg:53.70ms
step:1659/2110 train_time:89127ms step_avg:53.72ms
step:1660/2110 train_time:89215ms step_avg:53.74ms
step:1661/2110 train_time:89304ms step_avg:53.76ms
step:1662/2110 train_time:89392ms step_avg:53.79ms
step:1663/2110 train_time:89479ms step_avg:53.81ms
step:1664/2110 train_time:89567ms step_avg:53.83ms
step:1665/2110 train_time:89656ms step_avg:53.85ms
step:1666/2110 train_time:89745ms step_avg:53.87ms
step:1667/2110 train_time:89834ms step_avg:53.89ms
step:1668/2110 train_time:89921ms step_avg:53.91ms
step:1669/2110 train_time:90009ms step_avg:53.93ms
step:1670/2110 train_time:90097ms step_avg:53.95ms
step:1671/2110 train_time:90187ms step_avg:53.97ms
step:1672/2110 train_time:90275ms step_avg:53.99ms
step:1673/2110 train_time:90362ms step_avg:54.01ms
step:1674/2110 train_time:90451ms step_avg:54.03ms
step:1675/2110 train_time:90538ms step_avg:54.05ms
step:1676/2110 train_time:90626ms step_avg:54.07ms
step:1677/2110 train_time:90714ms step_avg:54.09ms
step:1678/2110 train_time:90801ms step_avg:54.11ms
step:1679/2110 train_time:90890ms step_avg:54.13ms
step:1680/2110 train_time:90978ms step_avg:54.15ms
step:1681/2110 train_time:91066ms step_avg:54.17ms
step:1682/2110 train_time:91154ms step_avg:54.19ms
step:1683/2110 train_time:91241ms step_avg:54.21ms
step:1684/2110 train_time:91328ms step_avg:54.23ms
step:1685/2110 train_time:91417ms step_avg:54.25ms
step:1686/2110 train_time:91505ms step_avg:54.27ms
step:1687/2110 train_time:91593ms step_avg:54.29ms
step:1688/2110 train_time:91681ms step_avg:54.31ms
step:1689/2110 train_time:91770ms step_avg:54.33ms
step:1690/2110 train_time:91857ms step_avg:54.35ms
step:1691/2110 train_time:91945ms step_avg:54.37ms
step:1692/2110 train_time:92035ms step_avg:54.39ms
step:1693/2110 train_time:92122ms step_avg:54.41ms
step:1694/2110 train_time:92211ms step_avg:54.43ms
step:1695/2110 train_time:92298ms step_avg:54.45ms
step:1696/2110 train_time:92386ms step_avg:54.47ms
step:1697/2110 train_time:92473ms step_avg:54.49ms
step:1698/2110 train_time:92561ms step_avg:54.51ms
step:1699/2110 train_time:92650ms step_avg:54.53ms
step:1700/2110 train_time:92737ms step_avg:54.55ms
step:1701/2110 train_time:92827ms step_avg:54.57ms
step:1702/2110 train_time:92915ms step_avg:54.59ms
step:1703/2110 train_time:93002ms step_avg:54.61ms
step:1704/2110 train_time:93090ms step_avg:54.63ms
step:1705/2110 train_time:93178ms step_avg:54.65ms
step:1706/2110 train_time:93267ms step_avg:54.67ms
step:1707/2110 train_time:93354ms step_avg:54.69ms
step:1708/2110 train_time:93443ms step_avg:54.71ms
step:1709/2110 train_time:93531ms step_avg:54.73ms
step:1710/2110 train_time:93619ms step_avg:54.75ms
step:1711/2110 train_time:93707ms step_avg:54.77ms
step:1712/2110 train_time:93794ms step_avg:54.79ms
step:1713/2110 train_time:93883ms step_avg:54.81ms
step:1714/2110 train_time:93972ms step_avg:54.83ms
step:1715/2110 train_time:94059ms step_avg:54.84ms
step:1716/2110 train_time:94147ms step_avg:54.86ms
step:1717/2110 train_time:94236ms step_avg:54.88ms
step:1718/2110 train_time:94324ms step_avg:54.90ms
step:1719/2110 train_time:94411ms step_avg:54.92ms
step:1720/2110 train_time:94498ms step_avg:54.94ms
step:1721/2110 train_time:94586ms step_avg:54.96ms
step:1722/2110 train_time:94674ms step_avg:54.98ms
step:1723/2110 train_time:94762ms step_avg:55.00ms
step:1724/2110 train_time:94851ms step_avg:55.02ms
step:1725/2110 train_time:94939ms step_avg:55.04ms
step:1726/2110 train_time:95028ms step_avg:55.06ms
step:1727/2110 train_time:95116ms step_avg:55.08ms
step:1728/2110 train_time:95204ms step_avg:55.10ms
step:1729/2110 train_time:95292ms step_avg:55.11ms
step:1730/2110 train_time:95380ms step_avg:55.13ms
step:1731/2110 train_time:95467ms step_avg:55.15ms
step:1732/2110 train_time:95556ms step_avg:55.17ms
step:1733/2110 train_time:95643ms step_avg:55.19ms
step:1734/2110 train_time:95731ms step_avg:55.21ms
step:1735/2110 train_time:95819ms step_avg:55.23ms
step:1736/2110 train_time:95907ms step_avg:55.25ms
step:1737/2110 train_time:95995ms step_avg:55.27ms
step:1738/2110 train_time:96084ms step_avg:55.28ms
step:1739/2110 train_time:96172ms step_avg:55.30ms
step:1740/2110 train_time:96260ms step_avg:55.32ms
step:1741/2110 train_time:96350ms step_avg:55.34ms
step:1742/2110 train_time:96438ms step_avg:55.36ms
step:1743/2110 train_time:96526ms step_avg:55.38ms
step:1744/2110 train_time:96615ms step_avg:55.40ms
step:1745/2110 train_time:96702ms step_avg:55.42ms
step:1746/2110 train_time:96789ms step_avg:55.43ms
step:1747/2110 train_time:96878ms step_avg:55.45ms
step:1748/2110 train_time:96967ms step_avg:55.47ms
step:1749/2110 train_time:97055ms step_avg:55.49ms
step:1750/2110 train_time:97143ms step_avg:55.51ms
step:1750/2110 val_loss:3.3762 train_time:97232ms step_avg:55.56ms
step:1751/2110 train_time:97269ms step_avg:55.55ms
step:1752/2110 train_time:97323ms step_avg:55.55ms
step:1753/2110 train_time:97418ms step_avg:55.57ms
step:1754/2110 train_time:97508ms step_avg:55.59ms
step:1755/2110 train_time:97596ms step_avg:55.61ms
step:1756/2110 train_time:97683ms step_avg:55.63ms
step:1757/2110 train_time:97769ms step_avg:55.65ms
step:1758/2110 train_time:97855ms step_avg:55.66ms
step:1759/2110 train_time:97942ms step_avg:55.68ms
step:1760/2110 train_time:98029ms step_avg:55.70ms
step:1761/2110 train_time:98116ms step_avg:55.72ms
step:1762/2110 train_time:98204ms step_avg:55.73ms
step:1763/2110 train_time:98296ms step_avg:55.76ms
step:1764/2110 train_time:98388ms step_avg:55.78ms
step:1765/2110 train_time:98476ms step_avg:55.79ms
step:1766/2110 train_time:98564ms step_avg:55.81ms
step:1767/2110 train_time:98652ms step_avg:55.83ms
step:1768/2110 train_time:98739ms step_avg:55.85ms
step:1769/2110 train_time:98825ms step_avg:55.86ms
step:1770/2110 train_time:98912ms step_avg:55.88ms
step:1771/2110 train_time:98998ms step_avg:55.90ms
step:1772/2110 train_time:99084ms step_avg:55.92ms
step:1773/2110 train_time:99173ms step_avg:55.94ms
step:1774/2110 train_time:99263ms step_avg:55.95ms
step:1775/2110 train_time:99354ms step_avg:55.97ms
step:1776/2110 train_time:99443ms step_avg:55.99ms
step:1777/2110 train_time:99532ms step_avg:56.01ms
step:1778/2110 train_time:99620ms step_avg:56.03ms
step:1779/2110 train_time:99707ms step_avg:56.05ms
step:1780/2110 train_time:99794ms step_avg:56.06ms
step:1781/2110 train_time:99881ms step_avg:56.08ms
step:1782/2110 train_time:99968ms step_avg:56.10ms
step:1783/2110 train_time:100054ms step_avg:56.12ms
step:1784/2110 train_time:100141ms step_avg:56.13ms
step:1785/2110 train_time:100231ms step_avg:56.15ms
step:1786/2110 train_time:100319ms step_avg:56.17ms
step:1787/2110 train_time:100409ms step_avg:56.19ms
step:1788/2110 train_time:100497ms step_avg:56.21ms
step:1789/2110 train_time:100585ms step_avg:56.22ms
step:1790/2110 train_time:100672ms step_avg:56.24ms
step:1791/2110 train_time:100761ms step_avg:56.26ms
step:1792/2110 train_time:100847ms step_avg:56.28ms
step:1793/2110 train_time:100935ms step_avg:56.29ms
step:1794/2110 train_time:101022ms step_avg:56.31ms
step:1795/2110 train_time:101109ms step_avg:56.33ms
step:1796/2110 train_time:101197ms step_avg:56.35ms
step:1797/2110 train_time:101286ms step_avg:56.36ms
step:1798/2110 train_time:101374ms step_avg:56.38ms
step:1799/2110 train_time:101464ms step_avg:56.40ms
step:1800/2110 train_time:101552ms step_avg:56.42ms
step:1801/2110 train_time:101640ms step_avg:56.44ms
step:1802/2110 train_time:101729ms step_avg:56.45ms
step:1803/2110 train_time:101815ms step_avg:56.47ms
step:1804/2110 train_time:101903ms step_avg:56.49ms
step:1805/2110 train_time:101990ms step_avg:56.50ms
step:1806/2110 train_time:102077ms step_avg:56.52ms
step:1807/2110 train_time:102165ms step_avg:56.54ms
step:1808/2110 train_time:102253ms step_avg:56.56ms
step:1809/2110 train_time:102343ms step_avg:56.57ms
step:1810/2110 train_time:102431ms step_avg:56.59ms
step:1811/2110 train_time:102520ms step_avg:56.61ms
step:1812/2110 train_time:102609ms step_avg:56.63ms
step:1813/2110 train_time:102697ms step_avg:56.64ms
step:1814/2110 train_time:102786ms step_avg:56.66ms
step:1815/2110 train_time:102873ms step_avg:56.68ms
step:1816/2110 train_time:102960ms step_avg:56.70ms
step:1817/2110 train_time:103047ms step_avg:56.71ms
step:1818/2110 train_time:103135ms step_avg:56.73ms
step:1819/2110 train_time:103224ms step_avg:56.75ms
step:1820/2110 train_time:103312ms step_avg:56.76ms
step:1821/2110 train_time:103401ms step_avg:56.78ms
step:1822/2110 train_time:103490ms step_avg:56.80ms
step:1823/2110 train_time:103579ms step_avg:56.82ms
step:1824/2110 train_time:103667ms step_avg:56.83ms
step:1825/2110 train_time:103754ms step_avg:56.85ms
step:1826/2110 train_time:103841ms step_avg:56.87ms
step:1827/2110 train_time:103929ms step_avg:56.89ms
step:1828/2110 train_time:104016ms step_avg:56.90ms
step:1829/2110 train_time:104104ms step_avg:56.92ms
step:1830/2110 train_time:104192ms step_avg:56.94ms
step:1831/2110 train_time:104280ms step_avg:56.95ms
step:1832/2110 train_time:104368ms step_avg:56.97ms
step:1833/2110 train_time:104456ms step_avg:56.99ms
step:1834/2110 train_time:104544ms step_avg:57.00ms
step:1835/2110 train_time:104633ms step_avg:57.02ms
step:1836/2110 train_time:104720ms step_avg:57.04ms
step:1837/2110 train_time:104808ms step_avg:57.05ms
step:1838/2110 train_time:104894ms step_avg:57.07ms
step:1839/2110 train_time:104983ms step_avg:57.09ms
step:1840/2110 train_time:105071ms step_avg:57.10ms
step:1841/2110 train_time:105159ms step_avg:57.12ms
step:1842/2110 train_time:105247ms step_avg:57.14ms
step:1843/2110 train_time:105334ms step_avg:57.15ms
step:1844/2110 train_time:105422ms step_avg:57.17ms
step:1845/2110 train_time:105511ms step_avg:57.19ms
step:1846/2110 train_time:105598ms step_avg:57.20ms
step:1847/2110 train_time:105687ms step_avg:57.22ms
step:1848/2110 train_time:105774ms step_avg:57.24ms
step:1849/2110 train_time:105862ms step_avg:57.25ms
step:1850/2110 train_time:105949ms step_avg:57.27ms
step:1851/2110 train_time:106037ms step_avg:57.29ms
step:1852/2110 train_time:106124ms step_avg:57.30ms
step:1853/2110 train_time:106214ms step_avg:57.32ms
step:1854/2110 train_time:106302ms step_avg:57.34ms
step:1855/2110 train_time:106391ms step_avg:57.35ms
step:1856/2110 train_time:106478ms step_avg:57.37ms
step:1857/2110 train_time:106567ms step_avg:57.39ms
step:1858/2110 train_time:106654ms step_avg:57.40ms
step:1859/2110 train_time:106743ms step_avg:57.42ms
step:1860/2110 train_time:106830ms step_avg:57.44ms
step:1861/2110 train_time:106918ms step_avg:57.45ms
step:1862/2110 train_time:107006ms step_avg:57.47ms
step:1863/2110 train_time:107094ms step_avg:57.48ms
step:1864/2110 train_time:107182ms step_avg:57.50ms
step:1865/2110 train_time:107271ms step_avg:57.52ms
step:1866/2110 train_time:107358ms step_avg:57.53ms
step:1867/2110 train_time:107447ms step_avg:57.55ms
step:1868/2110 train_time:107534ms step_avg:57.57ms
step:1869/2110 train_time:107622ms step_avg:57.58ms
step:1870/2110 train_time:107709ms step_avg:57.60ms
step:1871/2110 train_time:107798ms step_avg:57.61ms
step:1872/2110 train_time:107886ms step_avg:57.63ms
step:1873/2110 train_time:107974ms step_avg:57.65ms
step:1874/2110 train_time:108062ms step_avg:57.66ms
step:1875/2110 train_time:108149ms step_avg:57.68ms
step:1876/2110 train_time:108236ms step_avg:57.70ms
step:1877/2110 train_time:108324ms step_avg:57.71ms
step:1878/2110 train_time:108412ms step_avg:57.73ms
step:1879/2110 train_time:108501ms step_avg:57.74ms
step:1880/2110 train_time:108589ms step_avg:57.76ms
step:1881/2110 train_time:108677ms step_avg:57.78ms
step:1882/2110 train_time:108765ms step_avg:57.79ms
step:1883/2110 train_time:108852ms step_avg:57.81ms
step:1884/2110 train_time:108940ms step_avg:57.82ms
step:1885/2110 train_time:109027ms step_avg:57.84ms
step:1886/2110 train_time:109114ms step_avg:57.85ms
step:1887/2110 train_time:109202ms step_avg:57.87ms
step:1888/2110 train_time:109291ms step_avg:57.89ms
step:1889/2110 train_time:109378ms step_avg:57.90ms
step:1890/2110 train_time:109467ms step_avg:57.92ms
step:1891/2110 train_time:109554ms step_avg:57.93ms
step:1892/2110 train_time:109642ms step_avg:57.95ms
step:1893/2110 train_time:109730ms step_avg:57.97ms
step:1894/2110 train_time:109817ms step_avg:57.98ms
step:1895/2110 train_time:109906ms step_avg:58.00ms
step:1896/2110 train_time:109992ms step_avg:58.01ms
step:1897/2110 train_time:110081ms step_avg:58.03ms
step:1898/2110 train_time:110169ms step_avg:58.04ms
step:1899/2110 train_time:110256ms step_avg:58.06ms
step:1900/2110 train_time:110344ms step_avg:58.08ms
step:1901/2110 train_time:110433ms step_avg:58.09ms
step:1902/2110 train_time:110521ms step_avg:58.11ms
step:1903/2110 train_time:110609ms step_avg:58.12ms
step:1904/2110 train_time:110696ms step_avg:58.14ms
step:1905/2110 train_time:110783ms step_avg:58.15ms
step:1906/2110 train_time:110872ms step_avg:58.17ms
step:1907/2110 train_time:110961ms step_avg:58.19ms
step:1908/2110 train_time:111048ms step_avg:58.20ms
step:1909/2110 train_time:111137ms step_avg:58.22ms
step:1910/2110 train_time:111223ms step_avg:58.23ms
step:1911/2110 train_time:111311ms step_avg:58.25ms
step:1912/2110 train_time:111399ms step_avg:58.26ms
step:1913/2110 train_time:111487ms step_avg:58.28ms
step:1914/2110 train_time:111575ms step_avg:58.29ms
step:1915/2110 train_time:111663ms step_avg:58.31ms
step:1916/2110 train_time:111751ms step_avg:58.33ms
step:1917/2110 train_time:111839ms step_avg:58.34ms
step:1918/2110 train_time:111927ms step_avg:58.36ms
step:1919/2110 train_time:112015ms step_avg:58.37ms
step:1920/2110 train_time:112104ms step_avg:58.39ms
step:1921/2110 train_time:112192ms step_avg:58.40ms
step:1922/2110 train_time:112279ms step_avg:58.42ms
step:1923/2110 train_time:112368ms step_avg:58.43ms
step:1924/2110 train_time:112456ms step_avg:58.45ms
step:1925/2110 train_time:112544ms step_avg:58.46ms
step:1926/2110 train_time:112632ms step_avg:58.48ms
step:1927/2110 train_time:112719ms step_avg:58.49ms
step:1928/2110 train_time:112807ms step_avg:58.51ms
step:1929/2110 train_time:112895ms step_avg:58.53ms
step:1930/2110 train_time:112983ms step_avg:58.54ms
step:1931/2110 train_time:113070ms step_avg:58.56ms
step:1932/2110 train_time:113159ms step_avg:58.57ms
step:1933/2110 train_time:113246ms step_avg:58.59ms
step:1934/2110 train_time:113334ms step_avg:58.60ms
step:1935/2110 train_time:113422ms step_avg:58.62ms
step:1936/2110 train_time:113511ms step_avg:58.63ms
step:1937/2110 train_time:113598ms step_avg:58.65ms
step:1938/2110 train_time:113686ms step_avg:58.66ms
step:1939/2110 train_time:113773ms step_avg:58.68ms
step:1940/2110 train_time:113861ms step_avg:58.69ms
step:1941/2110 train_time:113948ms step_avg:58.71ms
step:1942/2110 train_time:114035ms step_avg:58.72ms
step:1943/2110 train_time:114124ms step_avg:58.74ms
step:1944/2110 train_time:114212ms step_avg:58.75ms
step:1945/2110 train_time:114299ms step_avg:58.77ms
step:1946/2110 train_time:114388ms step_avg:58.78ms
step:1947/2110 train_time:114476ms step_avg:58.80ms
step:1948/2110 train_time:114564ms step_avg:58.81ms
step:1949/2110 train_time:114654ms step_avg:58.83ms
step:1950/2110 train_time:114741ms step_avg:58.84ms
step:1951/2110 train_time:114830ms step_avg:58.86ms
step:1952/2110 train_time:114917ms step_avg:58.87ms
step:1953/2110 train_time:115004ms step_avg:58.89ms
step:1954/2110 train_time:115092ms step_avg:58.90ms
step:1955/2110 train_time:115180ms step_avg:58.92ms
step:1956/2110 train_time:115271ms step_avg:58.93ms
step:1957/2110 train_time:115358ms step_avg:58.95ms
step:1958/2110 train_time:115445ms step_avg:58.96ms
step:1959/2110 train_time:115532ms step_avg:58.98ms
step:1960/2110 train_time:115621ms step_avg:58.99ms
step:1961/2110 train_time:115709ms step_avg:59.01ms
step:1962/2110 train_time:115795ms step_avg:59.02ms
step:1963/2110 train_time:115883ms step_avg:59.03ms
step:1964/2110 train_time:115972ms step_avg:59.05ms
step:1965/2110 train_time:116059ms step_avg:59.06ms
step:1966/2110 train_time:116147ms step_avg:59.08ms
step:1967/2110 train_time:116235ms step_avg:59.09ms
step:1968/2110 train_time:116323ms step_avg:59.11ms
step:1969/2110 train_time:116411ms step_avg:59.12ms
step:1970/2110 train_time:116498ms step_avg:59.14ms
step:1971/2110 train_time:116587ms step_avg:59.15ms
step:1972/2110 train_time:116674ms step_avg:59.17ms
step:1973/2110 train_time:116763ms step_avg:59.18ms
step:1974/2110 train_time:116850ms step_avg:59.19ms
step:1975/2110 train_time:116940ms step_avg:59.21ms
step:1976/2110 train_time:117029ms step_avg:59.23ms
step:1977/2110 train_time:117116ms step_avg:59.24ms
step:1978/2110 train_time:117203ms step_avg:59.25ms
step:1979/2110 train_time:117291ms step_avg:59.27ms
step:1980/2110 train_time:117379ms step_avg:59.28ms
step:1981/2110 train_time:117467ms step_avg:59.30ms
step:1982/2110 train_time:117554ms step_avg:59.31ms
step:1983/2110 train_time:117641ms step_avg:59.32ms
step:1984/2110 train_time:117729ms step_avg:59.34ms
step:1985/2110 train_time:117816ms step_avg:59.35ms
step:1986/2110 train_time:117904ms step_avg:59.37ms
step:1987/2110 train_time:117994ms step_avg:59.38ms
step:1988/2110 train_time:118082ms step_avg:59.40ms
step:1989/2110 train_time:118169ms step_avg:59.41ms
step:1990/2110 train_time:118256ms step_avg:59.42ms
step:1991/2110 train_time:118344ms step_avg:59.44ms
step:1992/2110 train_time:118432ms step_avg:59.45ms
step:1993/2110 train_time:118521ms step_avg:59.47ms
step:1994/2110 train_time:118609ms step_avg:59.48ms
step:1995/2110 train_time:118697ms step_avg:59.50ms
step:1996/2110 train_time:118785ms step_avg:59.51ms
step:1997/2110 train_time:118873ms step_avg:59.53ms
step:1998/2110 train_time:118960ms step_avg:59.54ms
step:1999/2110 train_time:119049ms step_avg:59.55ms
step:2000/2110 train_time:119136ms step_avg:59.57ms
step:2000/2110 val_loss:3.3014 train_time:119226ms step_avg:59.61ms
step:2001/2110 train_time:119262ms step_avg:59.60ms
step:2002/2110 train_time:119318ms step_avg:59.60ms
step:2003/2110 train_time:119413ms step_avg:59.62ms
step:2004/2110 train_time:119502ms step_avg:59.63ms
step:2005/2110 train_time:119589ms step_avg:59.65ms
step:2006/2110 train_time:119676ms step_avg:59.66ms
step:2007/2110 train_time:119763ms step_avg:59.67ms
step:2008/2110 train_time:119850ms step_avg:59.69ms
step:2009/2110 train_time:119937ms step_avg:59.70ms
step:2010/2110 train_time:120025ms step_avg:59.71ms
step:2011/2110 train_time:120111ms step_avg:59.73ms
step:2012/2110 train_time:120200ms step_avg:59.74ms
step:2013/2110 train_time:120290ms step_avg:59.76ms
step:2014/2110 train_time:120381ms step_avg:59.77ms
step:2015/2110 train_time:120470ms step_avg:59.79ms
step:2016/2110 train_time:120558ms step_avg:59.80ms
step:2017/2110 train_time:120646ms step_avg:59.81ms
step:2018/2110 train_time:120734ms step_avg:59.83ms
step:2019/2110 train_time:120821ms step_avg:59.84ms
step:2020/2110 train_time:120908ms step_avg:59.86ms
step:2021/2110 train_time:120996ms step_avg:59.87ms
step:2022/2110 train_time:121083ms step_avg:59.88ms
step:2023/2110 train_time:121171ms step_avg:59.90ms
step:2024/2110 train_time:121260ms step_avg:59.91ms
step:2025/2110 train_time:121349ms step_avg:59.93ms
step:2026/2110 train_time:121439ms step_avg:59.94ms
step:2027/2110 train_time:121527ms step_avg:59.95ms
step:2028/2110 train_time:121615ms step_avg:59.97ms
step:2029/2110 train_time:121703ms step_avg:59.98ms
step:2030/2110 train_time:121791ms step_avg:60.00ms
step:2031/2110 train_time:121878ms step_avg:60.01ms
step:2032/2110 train_time:121967ms step_avg:60.02ms
step:2033/2110 train_time:122054ms step_avg:60.04ms
step:2034/2110 train_time:122142ms step_avg:60.05ms
step:2035/2110 train_time:122232ms step_avg:60.06ms
step:2036/2110 train_time:122321ms step_avg:60.08ms
step:2037/2110 train_time:122410ms step_avg:60.09ms
step:2038/2110 train_time:122499ms step_avg:60.11ms
step:2039/2110 train_time:122588ms step_avg:60.12ms
step:2040/2110 train_time:122678ms step_avg:60.14ms
step:2041/2110 train_time:122763ms step_avg:60.15ms
step:2042/2110 train_time:122850ms step_avg:60.16ms
step:2043/2110 train_time:122937ms step_avg:60.17ms
step:2044/2110 train_time:123027ms step_avg:60.19ms
step:2045/2110 train_time:123114ms step_avg:60.20ms
step:2046/2110 train_time:123205ms step_avg:60.22ms
step:2047/2110 train_time:123290ms step_avg:60.23ms
step:2048/2110 train_time:123378ms step_avg:60.24ms
step:2049/2110 train_time:123468ms step_avg:60.26ms
step:2050/2110 train_time:123557ms step_avg:60.27ms
step:2051/2110 train_time:123646ms step_avg:60.29ms
step:2052/2110 train_time:123734ms step_avg:60.30ms
step:2053/2110 train_time:123822ms step_avg:60.31ms
step:2054/2110 train_time:123910ms step_avg:60.33ms
step:2055/2110 train_time:123997ms step_avg:60.34ms
step:2056/2110 train_time:124084ms step_avg:60.35ms
step:2057/2110 train_time:124173ms step_avg:60.37ms
step:2058/2110 train_time:124261ms step_avg:60.38ms
step:2059/2110 train_time:124349ms step_avg:60.39ms
step:2060/2110 train_time:124437ms step_avg:60.41ms
step:2061/2110 train_time:124526ms step_avg:60.42ms
step:2062/2110 train_time:124613ms step_avg:60.43ms
step:2063/2110 train_time:124702ms step_avg:60.45ms
step:2064/2110 train_time:124789ms step_avg:60.46ms
step:2065/2110 train_time:124877ms step_avg:60.47ms
step:2066/2110 train_time:124966ms step_avg:60.49ms
step:2067/2110 train_time:125053ms step_avg:60.50ms
step:2068/2110 train_time:125141ms step_avg:60.51ms
step:2069/2110 train_time:125230ms step_avg:60.53ms
step:2070/2110 train_time:125318ms step_avg:60.54ms
step:2071/2110 train_time:125406ms step_avg:60.55ms
step:2072/2110 train_time:125494ms step_avg:60.57ms
step:2073/2110 train_time:125583ms step_avg:60.58ms
step:2074/2110 train_time:125671ms step_avg:60.59ms
step:2075/2110 train_time:125759ms step_avg:60.61ms
step:2076/2110 train_time:125848ms step_avg:60.62ms
step:2077/2110 train_time:125935ms step_avg:60.63ms
step:2078/2110 train_time:126025ms step_avg:60.65ms
step:2079/2110 train_time:126112ms step_avg:60.66ms
step:2080/2110 train_time:126200ms step_avg:60.67ms
step:2081/2110 train_time:126287ms step_avg:60.69ms
step:2082/2110 train_time:126376ms step_avg:60.70ms
step:2083/2110 train_time:126464ms step_avg:60.71ms
step:2084/2110 train_time:126552ms step_avg:60.73ms
step:2085/2110 train_time:126641ms step_avg:60.74ms
step:2086/2110 train_time:126730ms step_avg:60.75ms
step:2087/2110 train_time:126817ms step_avg:60.77ms
step:2088/2110 train_time:126905ms step_avg:60.78ms
step:2089/2110 train_time:126993ms step_avg:60.79ms
step:2090/2110 train_time:127083ms step_avg:60.81ms
step:2091/2110 train_time:127170ms step_avg:60.82ms
step:2092/2110 train_time:127260ms step_avg:60.83ms
step:2093/2110 train_time:127349ms step_avg:60.85ms
step:2094/2110 train_time:127437ms step_avg:60.86ms
step:2095/2110 train_time:127528ms step_avg:60.87ms
step:2096/2110 train_time:127616ms step_avg:60.89ms
step:2097/2110 train_time:127703ms step_avg:60.90ms
step:2098/2110 train_time:127790ms step_avg:60.91ms
step:2099/2110 train_time:127879ms step_avg:60.92ms
step:2100/2110 train_time:127967ms step_avg:60.94ms
step:2101/2110 train_time:128055ms step_avg:60.95ms
step:2102/2110 train_time:128144ms step_avg:60.96ms
step:2103/2110 train_time:128232ms step_avg:60.98ms
step:2104/2110 train_time:128320ms step_avg:60.99ms
step:2105/2110 train_time:128410ms step_avg:61.00ms
step:2106/2110 train_time:128498ms step_avg:61.02ms
step:2107/2110 train_time:128587ms step_avg:61.03ms
step:2108/2110 train_time:128676ms step_avg:61.04ms
step:2109/2110 train_time:128764ms step_avg:61.05ms
step:2110/2110 train_time:128852ms step_avg:61.07ms
step:2110/2110 val_loss:3.2766 train_time:128941ms step_avg:61.11ms
peak memory allocated: 29892 MiB reserved: 44736 MiB

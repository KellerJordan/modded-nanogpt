import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 15 23:58:28 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   33C    P0            122W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   34C    P0            117W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   33C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   34C    P0            119W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              80      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              81      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              84      C   /usr/local/bin/python                  1592MiB |
|    5   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:100ms step_avg:100.11ms
step:2/2110 train_time:131ms step_avg:65.39ms
step:3/2110 train_time:161ms step_avg:53.56ms
step:4/2110 train_time:201ms step_avg:50.26ms
step:5/2110 train_time:239ms step_avg:47.87ms
step:6/2110 train_time:439ms step_avg:73.18ms
step:7/2110 train_time:631ms step_avg:90.08ms
step:8/2110 train_time:663ms step_avg:82.89ms
step:9/2110 train_time:696ms step_avg:77.32ms
step:10/2110 train_time:729ms step_avg:72.86ms
step:11/2110 train_time:762ms step_avg:69.27ms
step:12/2110 train_time:795ms step_avg:66.23ms
step:13/2110 train_time:828ms step_avg:63.70ms
step:14/2110 train_time:861ms step_avg:61.53ms
step:15/2110 train_time:894ms step_avg:59.61ms
step:16/2110 train_time:927ms step_avg:57.94ms
step:17/2110 train_time:960ms step_avg:56.48ms
step:18/2110 train_time:993ms step_avg:55.19ms
step:19/2110 train_time:1027ms step_avg:54.04ms
step:20/2110 train_time:1060ms step_avg:52.98ms
step:21/2110 train_time:1093ms step_avg:52.05ms
step:22/2110 train_time:1126ms step_avg:51.19ms
step:23/2110 train_time:1159ms step_avg:50.40ms
step:24/2110 train_time:1192ms step_avg:49.66ms
step:25/2110 train_time:1225ms step_avg:49.02ms
step:26/2110 train_time:1258ms step_avg:48.40ms
step:27/2110 train_time:1292ms step_avg:47.84ms
step:28/2110 train_time:1325ms step_avg:47.33ms
step:29/2110 train_time:1359ms step_avg:46.86ms
step:30/2110 train_time:1391ms step_avg:46.36ms
step:31/2110 train_time:1424ms step_avg:45.93ms
step:32/2110 train_time:1457ms step_avg:45.52ms
step:33/2110 train_time:1490ms step_avg:45.15ms
step:34/2110 train_time:1523ms step_avg:44.80ms
step:35/2110 train_time:1558ms step_avg:44.50ms
step:36/2110 train_time:1591ms step_avg:44.20ms
step:37/2110 train_time:1626ms step_avg:43.93ms
step:38/2110 train_time:1659ms step_avg:43.65ms
step:39/2110 train_time:1693ms step_avg:43.42ms
step:40/2110 train_time:1726ms step_avg:43.16ms
step:41/2110 train_time:1760ms step_avg:42.92ms
step:42/2110 train_time:1793ms step_avg:42.69ms
step:43/2110 train_time:1827ms step_avg:42.48ms
step:44/2110 train_time:1860ms step_avg:42.27ms
step:45/2110 train_time:1893ms step_avg:42.07ms
step:46/2110 train_time:1927ms step_avg:41.88ms
step:47/2110 train_time:1960ms step_avg:41.70ms
step:48/2110 train_time:1993ms step_avg:41.51ms
step:49/2110 train_time:2026ms step_avg:41.35ms
step:50/2110 train_time:2059ms step_avg:41.18ms
step:51/2110 train_time:2092ms step_avg:41.02ms
step:52/2110 train_time:2125ms step_avg:40.87ms
step:53/2110 train_time:2158ms step_avg:40.72ms
step:54/2110 train_time:2191ms step_avg:40.58ms
step:55/2110 train_time:2224ms step_avg:40.44ms
step:56/2110 train_time:2257ms step_avg:40.30ms
step:57/2110 train_time:2290ms step_avg:40.18ms
step:58/2110 train_time:2323ms step_avg:40.05ms
step:59/2110 train_time:2357ms step_avg:39.94ms
step:60/2110 train_time:2390ms step_avg:39.83ms
step:61/2110 train_time:2422ms step_avg:39.71ms
step:62/2110 train_time:2455ms step_avg:39.60ms
step:63/2110 train_time:2488ms step_avg:39.50ms
step:64/2110 train_time:2522ms step_avg:39.40ms
step:65/2110 train_time:2555ms step_avg:39.31ms
step:66/2110 train_time:2588ms step_avg:39.21ms
step:67/2110 train_time:2622ms step_avg:39.13ms
step:68/2110 train_time:2655ms step_avg:39.04ms
step:69/2110 train_time:2688ms step_avg:38.96ms
step:70/2110 train_time:2721ms step_avg:38.87ms
step:71/2110 train_time:2755ms step_avg:38.80ms
step:72/2110 train_time:2788ms step_avg:38.72ms
step:73/2110 train_time:2821ms step_avg:38.65ms
step:74/2110 train_time:2854ms step_avg:38.57ms
step:75/2110 train_time:2888ms step_avg:38.51ms
step:76/2110 train_time:2921ms step_avg:38.43ms
step:77/2110 train_time:2954ms step_avg:38.36ms
step:78/2110 train_time:2987ms step_avg:38.29ms
step:79/2110 train_time:3021ms step_avg:38.25ms
step:80/2110 train_time:3053ms step_avg:38.17ms
step:81/2110 train_time:3086ms step_avg:38.10ms
step:82/2110 train_time:3120ms step_avg:38.05ms
step:83/2110 train_time:3153ms step_avg:37.98ms
step:84/2110 train_time:3186ms step_avg:37.93ms
step:85/2110 train_time:3219ms step_avg:37.87ms
step:86/2110 train_time:3252ms step_avg:37.81ms
step:87/2110 train_time:3285ms step_avg:37.76ms
step:88/2110 train_time:3318ms step_avg:37.70ms
step:89/2110 train_time:3351ms step_avg:37.65ms
step:90/2110 train_time:3384ms step_avg:37.60ms
step:91/2110 train_time:3417ms step_avg:37.55ms
step:92/2110 train_time:3452ms step_avg:37.52ms
step:93/2110 train_time:3484ms step_avg:37.46ms
step:94/2110 train_time:3518ms step_avg:37.42ms
step:95/2110 train_time:3550ms step_avg:37.37ms
step:96/2110 train_time:3583ms step_avg:37.32ms
step:97/2110 train_time:3616ms step_avg:37.28ms
step:98/2110 train_time:3649ms step_avg:37.24ms
step:99/2110 train_time:3683ms step_avg:37.20ms
step:100/2110 train_time:3716ms step_avg:37.16ms
step:101/2110 train_time:3749ms step_avg:37.12ms
step:102/2110 train_time:3782ms step_avg:37.08ms
step:103/2110 train_time:3816ms step_avg:37.05ms
step:104/2110 train_time:3849ms step_avg:37.01ms
step:105/2110 train_time:3882ms step_avg:36.97ms
step:106/2110 train_time:3915ms step_avg:36.94ms
step:107/2110 train_time:3948ms step_avg:36.90ms
step:108/2110 train_time:3981ms step_avg:36.87ms
step:109/2110 train_time:4014ms step_avg:36.83ms
step:110/2110 train_time:4047ms step_avg:36.79ms
step:111/2110 train_time:4080ms step_avg:36.76ms
step:112/2110 train_time:4113ms step_avg:36.72ms
step:113/2110 train_time:4146ms step_avg:36.69ms
step:114/2110 train_time:4179ms step_avg:36.66ms
step:115/2110 train_time:4213ms step_avg:36.63ms
step:116/2110 train_time:4246ms step_avg:36.60ms
step:117/2110 train_time:4279ms step_avg:36.57ms
step:118/2110 train_time:4312ms step_avg:36.54ms
step:119/2110 train_time:4345ms step_avg:36.51ms
step:120/2110 train_time:4378ms step_avg:36.49ms
step:121/2110 train_time:4411ms step_avg:36.45ms
step:122/2110 train_time:4443ms step_avg:36.42ms
step:123/2110 train_time:4478ms step_avg:36.40ms
step:124/2110 train_time:4510ms step_avg:36.37ms
step:125/2110 train_time:4543ms step_avg:36.34ms
step:126/2110 train_time:4576ms step_avg:36.31ms
step:127/2110 train_time:4609ms step_avg:36.29ms
step:128/2110 train_time:4642ms step_avg:36.26ms
step:129/2110 train_time:4675ms step_avg:36.24ms
step:130/2110 train_time:4707ms step_avg:36.21ms
step:131/2110 train_time:4741ms step_avg:36.19ms
step:132/2110 train_time:4774ms step_avg:36.17ms
step:133/2110 train_time:4808ms step_avg:36.15ms
step:134/2110 train_time:4841ms step_avg:36.12ms
step:135/2110 train_time:4874ms step_avg:36.10ms
step:136/2110 train_time:4907ms step_avg:36.08ms
step:137/2110 train_time:4940ms step_avg:36.06ms
step:138/2110 train_time:4973ms step_avg:36.04ms
step:139/2110 train_time:5006ms step_avg:36.01ms
step:140/2110 train_time:5039ms step_avg:35.99ms
step:141/2110 train_time:5072ms step_avg:35.97ms
step:142/2110 train_time:5105ms step_avg:35.95ms
step:143/2110 train_time:5138ms step_avg:35.93ms
step:144/2110 train_time:5171ms step_avg:35.91ms
step:145/2110 train_time:5204ms step_avg:35.89ms
step:146/2110 train_time:5237ms step_avg:35.87ms
step:147/2110 train_time:5270ms step_avg:35.85ms
step:148/2110 train_time:5303ms step_avg:35.83ms
step:149/2110 train_time:5336ms step_avg:35.81ms
step:150/2110 train_time:5369ms step_avg:35.79ms
step:151/2110 train_time:5402ms step_avg:35.77ms
step:152/2110 train_time:5434ms step_avg:35.75ms
step:153/2110 train_time:5468ms step_avg:35.74ms
step:154/2110 train_time:5501ms step_avg:35.72ms
step:155/2110 train_time:5534ms step_avg:35.70ms
step:156/2110 train_time:5567ms step_avg:35.68ms
step:157/2110 train_time:5600ms step_avg:35.67ms
step:158/2110 train_time:5634ms step_avg:35.66ms
step:159/2110 train_time:5667ms step_avg:35.64ms
step:160/2110 train_time:5700ms step_avg:35.62ms
step:161/2110 train_time:5733ms step_avg:35.61ms
step:162/2110 train_time:5766ms step_avg:35.59ms
step:163/2110 train_time:5799ms step_avg:35.58ms
step:164/2110 train_time:5833ms step_avg:35.57ms
step:165/2110 train_time:5865ms step_avg:35.54ms
step:166/2110 train_time:5898ms step_avg:35.53ms
step:167/2110 train_time:5931ms step_avg:35.51ms
step:168/2110 train_time:5964ms step_avg:35.50ms
step:169/2110 train_time:5997ms step_avg:35.49ms
step:170/2110 train_time:6030ms step_avg:35.47ms
step:171/2110 train_time:6063ms step_avg:35.46ms
step:172/2110 train_time:6096ms step_avg:35.44ms
step:173/2110 train_time:6129ms step_avg:35.43ms
step:174/2110 train_time:6162ms step_avg:35.41ms
step:175/2110 train_time:6195ms step_avg:35.40ms
step:176/2110 train_time:6228ms step_avg:35.39ms
step:177/2110 train_time:6262ms step_avg:35.38ms
step:178/2110 train_time:6294ms step_avg:35.36ms
step:179/2110 train_time:6327ms step_avg:35.35ms
step:180/2110 train_time:6360ms step_avg:35.34ms
step:181/2110 train_time:6393ms step_avg:35.32ms
step:182/2110 train_time:6426ms step_avg:35.31ms
step:183/2110 train_time:6459ms step_avg:35.30ms
step:184/2110 train_time:6492ms step_avg:35.28ms
step:185/2110 train_time:6526ms step_avg:35.27ms
step:186/2110 train_time:6559ms step_avg:35.26ms
step:187/2110 train_time:6592ms step_avg:35.25ms
step:188/2110 train_time:6625ms step_avg:35.24ms
step:189/2110 train_time:6658ms step_avg:35.23ms
step:190/2110 train_time:6691ms step_avg:35.22ms
step:191/2110 train_time:6724ms step_avg:35.20ms
step:192/2110 train_time:6757ms step_avg:35.19ms
step:193/2110 train_time:6790ms step_avg:35.18ms
step:194/2110 train_time:6823ms step_avg:35.17ms
step:195/2110 train_time:6856ms step_avg:35.16ms
step:196/2110 train_time:6888ms step_avg:35.14ms
step:197/2110 train_time:6921ms step_avg:35.13ms
step:198/2110 train_time:6954ms step_avg:35.12ms
step:199/2110 train_time:6987ms step_avg:35.11ms
step:200/2110 train_time:7020ms step_avg:35.10ms
step:201/2110 train_time:7053ms step_avg:35.09ms
step:202/2110 train_time:7086ms step_avg:35.08ms
step:203/2110 train_time:7119ms step_avg:35.07ms
step:204/2110 train_time:7152ms step_avg:35.06ms
step:205/2110 train_time:7186ms step_avg:35.05ms
step:206/2110 train_time:7219ms step_avg:35.04ms
step:207/2110 train_time:7252ms step_avg:35.03ms
step:208/2110 train_time:7286ms step_avg:35.03ms
step:209/2110 train_time:7318ms step_avg:35.02ms
step:210/2110 train_time:7351ms step_avg:35.01ms
step:211/2110 train_time:7385ms step_avg:35.00ms
step:212/2110 train_time:7417ms step_avg:34.99ms
step:213/2110 train_time:7451ms step_avg:34.98ms
step:214/2110 train_time:7483ms step_avg:34.97ms
step:215/2110 train_time:7517ms step_avg:34.96ms
step:216/2110 train_time:7550ms step_avg:34.95ms
step:217/2110 train_time:7583ms step_avg:34.94ms
step:218/2110 train_time:7616ms step_avg:34.94ms
step:219/2110 train_time:7649ms step_avg:34.93ms
step:220/2110 train_time:7682ms step_avg:34.92ms
step:221/2110 train_time:7715ms step_avg:34.91ms
step:222/2110 train_time:7748ms step_avg:34.90ms
step:223/2110 train_time:7781ms step_avg:34.89ms
step:224/2110 train_time:7813ms step_avg:34.88ms
step:225/2110 train_time:7847ms step_avg:34.87ms
step:226/2110 train_time:7879ms step_avg:34.86ms
step:227/2110 train_time:7913ms step_avg:34.86ms
step:228/2110 train_time:7946ms step_avg:34.85ms
step:229/2110 train_time:7979ms step_avg:34.84ms
step:230/2110 train_time:8013ms step_avg:34.84ms
step:231/2110 train_time:8047ms step_avg:34.83ms
step:232/2110 train_time:8080ms step_avg:34.83ms
step:233/2110 train_time:8113ms step_avg:34.82ms
step:234/2110 train_time:8145ms step_avg:34.81ms
step:235/2110 train_time:8179ms step_avg:34.80ms
step:236/2110 train_time:8213ms step_avg:34.80ms
step:237/2110 train_time:8245ms step_avg:34.79ms
step:238/2110 train_time:8278ms step_avg:34.78ms
step:239/2110 train_time:8311ms step_avg:34.77ms
step:240/2110 train_time:8345ms step_avg:34.77ms
step:241/2110 train_time:8375ms step_avg:34.75ms
step:242/2110 train_time:8410ms step_avg:34.75ms
step:243/2110 train_time:8446ms step_avg:34.76ms
step:244/2110 train_time:8475ms step_avg:34.73ms
step:245/2110 train_time:8508ms step_avg:34.72ms
step:246/2110 train_time:8540ms step_avg:34.72ms
step:247/2110 train_time:8573ms step_avg:34.71ms
step:248/2110 train_time:8607ms step_avg:34.71ms
step:249/2110 train_time:8640ms step_avg:34.70ms
step:250/2110 train_time:8675ms step_avg:34.70ms
step:250/2110 val_loss:4.3019 train_time:8708ms step_avg:34.83ms
step:251/2110 train_time:8742ms step_avg:34.83ms
step:252/2110 train_time:8771ms step_avg:34.80ms
step:253/2110 train_time:8795ms step_avg:34.76ms
step:254/2110 train_time:8824ms step_avg:34.74ms
step:255/2110 train_time:8852ms step_avg:34.71ms
step:256/2110 train_time:8887ms step_avg:34.71ms
step:257/2110 train_time:8917ms step_avg:34.70ms
step:258/2110 train_time:8952ms step_avg:34.70ms
step:259/2110 train_time:8982ms step_avg:34.68ms
step:260/2110 train_time:9018ms step_avg:34.69ms
step:261/2110 train_time:9049ms step_avg:34.67ms
step:262/2110 train_time:9085ms step_avg:34.68ms
step:263/2110 train_time:9123ms step_avg:34.69ms
step:264/2110 train_time:9157ms step_avg:34.69ms
step:265/2110 train_time:9187ms step_avg:34.67ms
step:266/2110 train_time:9221ms step_avg:34.67ms
step:267/2110 train_time:9251ms step_avg:34.65ms
step:268/2110 train_time:9282ms step_avg:34.64ms
step:269/2110 train_time:9314ms step_avg:34.62ms
step:270/2110 train_time:9349ms step_avg:34.63ms
step:271/2110 train_time:9383ms step_avg:34.62ms
step:272/2110 train_time:9417ms step_avg:34.62ms
step:273/2110 train_time:9452ms step_avg:34.62ms
step:274/2110 train_time:9486ms step_avg:34.62ms
step:275/2110 train_time:9515ms step_avg:34.60ms
step:276/2110 train_time:9550ms step_avg:34.60ms
step:277/2110 train_time:9583ms step_avg:34.60ms
step:278/2110 train_time:9619ms step_avg:34.60ms
step:279/2110 train_time:9654ms step_avg:34.60ms
step:280/2110 train_time:9690ms step_avg:34.61ms
step:281/2110 train_time:9722ms step_avg:34.60ms
step:282/2110 train_time:9758ms step_avg:34.60ms
step:283/2110 train_time:9791ms step_avg:34.60ms
step:284/2110 train_time:9826ms step_avg:34.60ms
step:285/2110 train_time:9861ms step_avg:34.60ms
step:286/2110 train_time:9896ms step_avg:34.60ms
step:287/2110 train_time:9928ms step_avg:34.59ms
step:288/2110 train_time:9961ms step_avg:34.59ms
step:289/2110 train_time:9996ms step_avg:34.59ms
step:290/2110 train_time:10031ms step_avg:34.59ms
step:291/2110 train_time:10067ms step_avg:34.59ms
step:292/2110 train_time:10101ms step_avg:34.59ms
step:293/2110 train_time:10135ms step_avg:34.59ms
step:294/2110 train_time:10171ms step_avg:34.59ms
step:295/2110 train_time:10205ms step_avg:34.59ms
step:296/2110 train_time:10241ms step_avg:34.60ms
step:297/2110 train_time:10278ms step_avg:34.61ms
step:298/2110 train_time:10314ms step_avg:34.61ms
step:299/2110 train_time:10346ms step_avg:34.60ms
step:300/2110 train_time:10381ms step_avg:34.60ms
step:301/2110 train_time:10412ms step_avg:34.59ms
step:302/2110 train_time:10446ms step_avg:34.59ms
step:303/2110 train_time:10476ms step_avg:34.57ms
step:304/2110 train_time:10509ms step_avg:34.57ms
step:305/2110 train_time:10541ms step_avg:34.56ms
step:306/2110 train_time:10574ms step_avg:34.56ms
step:307/2110 train_time:10605ms step_avg:34.54ms
step:308/2110 train_time:10637ms step_avg:34.54ms
step:309/2110 train_time:10665ms step_avg:34.51ms
step:310/2110 train_time:10696ms step_avg:34.50ms
step:311/2110 train_time:10722ms step_avg:34.47ms
step:312/2110 train_time:10751ms step_avg:34.46ms
step:313/2110 train_time:10781ms step_avg:34.44ms
step:314/2110 train_time:10814ms step_avg:34.44ms
step:315/2110 train_time:10845ms step_avg:34.43ms
step:316/2110 train_time:10879ms step_avg:34.43ms
step:317/2110 train_time:10909ms step_avg:34.41ms
step:318/2110 train_time:10943ms step_avg:34.41ms
step:319/2110 train_time:10974ms step_avg:34.40ms
step:320/2110 train_time:11011ms step_avg:34.41ms
step:321/2110 train_time:11045ms step_avg:34.41ms
step:322/2110 train_time:11078ms step_avg:34.40ms
step:323/2110 train_time:11110ms step_avg:34.40ms
step:324/2110 train_time:11143ms step_avg:34.39ms
step:325/2110 train_time:11174ms step_avg:34.38ms
step:326/2110 train_time:11207ms step_avg:34.38ms
step:327/2110 train_time:11238ms step_avg:34.37ms
step:328/2110 train_time:11274ms step_avg:34.37ms
step:329/2110 train_time:11308ms step_avg:34.37ms
step:330/2110 train_time:11346ms step_avg:34.38ms
step:331/2110 train_time:11378ms step_avg:34.38ms
step:332/2110 train_time:11414ms step_avg:34.38ms
step:333/2110 train_time:11445ms step_avg:34.37ms
step:334/2110 train_time:11480ms step_avg:34.37ms
step:335/2110 train_time:11515ms step_avg:34.37ms
step:336/2110 train_time:11550ms step_avg:34.38ms
step:337/2110 train_time:11587ms step_avg:34.38ms
step:338/2110 train_time:11620ms step_avg:34.38ms
step:339/2110 train_time:11655ms step_avg:34.38ms
step:340/2110 train_time:11688ms step_avg:34.38ms
step:341/2110 train_time:11720ms step_avg:34.37ms
step:342/2110 train_time:11757ms step_avg:34.38ms
step:343/2110 train_time:11790ms step_avg:34.37ms
step:344/2110 train_time:11827ms step_avg:34.38ms
step:345/2110 train_time:11861ms step_avg:34.38ms
step:346/2110 train_time:11894ms step_avg:34.38ms
step:347/2110 train_time:11927ms step_avg:34.37ms
step:348/2110 train_time:11964ms step_avg:34.38ms
step:349/2110 train_time:12000ms step_avg:34.38ms
step:350/2110 train_time:12035ms step_avg:34.39ms
step:351/2110 train_time:12070ms step_avg:34.39ms
step:352/2110 train_time:12105ms step_avg:34.39ms
step:353/2110 train_time:12133ms step_avg:34.37ms
step:354/2110 train_time:12162ms step_avg:34.36ms
step:355/2110 train_time:12190ms step_avg:34.34ms
step:356/2110 train_time:12220ms step_avg:34.33ms
step:357/2110 train_time:12249ms step_avg:34.31ms
step:358/2110 train_time:12277ms step_avg:34.29ms
step:359/2110 train_time:12305ms step_avg:34.27ms
step:360/2110 train_time:12336ms step_avg:34.27ms
step:361/2110 train_time:12369ms step_avg:34.26ms
step:362/2110 train_time:12401ms step_avg:34.26ms
step:363/2110 train_time:12435ms step_avg:34.26ms
step:364/2110 train_time:12467ms step_avg:34.25ms
step:365/2110 train_time:12500ms step_avg:34.25ms
step:366/2110 train_time:12533ms step_avg:34.24ms
step:367/2110 train_time:12567ms step_avg:34.24ms
step:368/2110 train_time:12599ms step_avg:34.24ms
step:369/2110 train_time:12632ms step_avg:34.23ms
step:370/2110 train_time:12664ms step_avg:34.23ms
step:371/2110 train_time:12698ms step_avg:34.23ms
step:372/2110 train_time:12730ms step_avg:34.22ms
step:373/2110 train_time:12763ms step_avg:34.22ms
step:374/2110 train_time:12796ms step_avg:34.21ms
step:375/2110 train_time:12830ms step_avg:34.21ms
step:376/2110 train_time:12862ms step_avg:34.21ms
step:377/2110 train_time:12895ms step_avg:34.20ms
step:378/2110 train_time:12928ms step_avg:34.20ms
step:379/2110 train_time:12962ms step_avg:34.20ms
step:380/2110 train_time:12995ms step_avg:34.20ms
step:381/2110 train_time:13028ms step_avg:34.19ms
step:382/2110 train_time:13062ms step_avg:34.19ms
step:383/2110 train_time:13095ms step_avg:34.19ms
step:384/2110 train_time:13127ms step_avg:34.19ms
step:385/2110 train_time:13161ms step_avg:34.18ms
step:386/2110 train_time:13194ms step_avg:34.18ms
step:387/2110 train_time:13227ms step_avg:34.18ms
step:388/2110 train_time:13260ms step_avg:34.18ms
step:389/2110 train_time:13294ms step_avg:34.17ms
step:390/2110 train_time:13326ms step_avg:34.17ms
step:391/2110 train_time:13359ms step_avg:34.17ms
step:392/2110 train_time:13392ms step_avg:34.16ms
step:393/2110 train_time:13425ms step_avg:34.16ms
step:394/2110 train_time:13458ms step_avg:34.16ms
step:395/2110 train_time:13491ms step_avg:34.15ms
step:396/2110 train_time:13525ms step_avg:34.15ms
step:397/2110 train_time:13557ms step_avg:34.15ms
step:398/2110 train_time:13590ms step_avg:34.15ms
step:399/2110 train_time:13623ms step_avg:34.14ms
step:400/2110 train_time:13656ms step_avg:34.14ms
step:401/2110 train_time:13689ms step_avg:34.14ms
step:402/2110 train_time:13722ms step_avg:34.13ms
step:403/2110 train_time:13755ms step_avg:34.13ms
step:404/2110 train_time:13788ms step_avg:34.13ms
step:405/2110 train_time:13821ms step_avg:34.13ms
step:406/2110 train_time:13853ms step_avg:34.12ms
step:407/2110 train_time:13887ms step_avg:34.12ms
step:408/2110 train_time:13919ms step_avg:34.12ms
step:409/2110 train_time:13952ms step_avg:34.11ms
step:410/2110 train_time:13987ms step_avg:34.11ms
step:411/2110 train_time:14019ms step_avg:34.11ms
step:412/2110 train_time:14052ms step_avg:34.11ms
step:413/2110 train_time:14085ms step_avg:34.10ms
step:414/2110 train_time:14118ms step_avg:34.10ms
step:415/2110 train_time:14151ms step_avg:34.10ms
step:416/2110 train_time:14184ms step_avg:34.10ms
step:417/2110 train_time:14217ms step_avg:34.09ms
step:418/2110 train_time:14250ms step_avg:34.09ms
step:419/2110 train_time:14283ms step_avg:34.09ms
step:420/2110 train_time:14316ms step_avg:34.09ms
step:421/2110 train_time:14349ms step_avg:34.08ms
step:422/2110 train_time:14382ms step_avg:34.08ms
step:423/2110 train_time:14415ms step_avg:34.08ms
step:424/2110 train_time:14448ms step_avg:34.08ms
step:425/2110 train_time:14481ms step_avg:34.07ms
step:426/2110 train_time:14514ms step_avg:34.07ms
step:427/2110 train_time:14547ms step_avg:34.07ms
step:428/2110 train_time:14580ms step_avg:34.07ms
step:429/2110 train_time:14613ms step_avg:34.06ms
step:430/2110 train_time:14646ms step_avg:34.06ms
step:431/2110 train_time:14679ms step_avg:34.06ms
step:432/2110 train_time:14712ms step_avg:34.06ms
step:433/2110 train_time:14747ms step_avg:34.06ms
step:434/2110 train_time:14779ms step_avg:34.05ms
step:435/2110 train_time:14812ms step_avg:34.05ms
step:436/2110 train_time:14845ms step_avg:34.05ms
step:437/2110 train_time:14877ms step_avg:34.04ms
step:438/2110 train_time:14910ms step_avg:34.04ms
step:439/2110 train_time:14944ms step_avg:34.04ms
step:440/2110 train_time:14978ms step_avg:34.04ms
step:441/2110 train_time:15011ms step_avg:34.04ms
step:442/2110 train_time:15054ms step_avg:34.06ms
step:443/2110 train_time:15090ms step_avg:34.06ms
step:444/2110 train_time:15125ms step_avg:34.06ms
step:445/2110 train_time:15157ms step_avg:34.06ms
step:446/2110 train_time:15190ms step_avg:34.06ms
step:447/2110 train_time:15220ms step_avg:34.05ms
step:448/2110 train_time:15254ms step_avg:34.05ms
step:449/2110 train_time:15286ms step_avg:34.04ms
step:450/2110 train_time:15323ms step_avg:34.05ms
step:451/2110 train_time:15357ms step_avg:34.05ms
step:452/2110 train_time:15393ms step_avg:34.06ms
step:453/2110 train_time:15428ms step_avg:34.06ms
step:454/2110 train_time:15461ms step_avg:34.06ms
step:455/2110 train_time:15493ms step_avg:34.05ms
step:456/2110 train_time:15526ms step_avg:34.05ms
step:457/2110 train_time:15559ms step_avg:34.05ms
step:458/2110 train_time:15592ms step_avg:34.04ms
step:459/2110 train_time:15623ms step_avg:34.04ms
step:460/2110 train_time:15661ms step_avg:34.05ms
step:461/2110 train_time:15695ms step_avg:34.05ms
step:462/2110 train_time:15728ms step_avg:34.04ms
step:463/2110 train_time:15760ms step_avg:34.04ms
step:464/2110 train_time:15791ms step_avg:34.03ms
step:465/2110 train_time:15821ms step_avg:34.02ms
step:466/2110 train_time:15855ms step_avg:34.02ms
step:467/2110 train_time:15887ms step_avg:34.02ms
step:468/2110 train_time:15923ms step_avg:34.02ms
step:469/2110 train_time:15958ms step_avg:34.03ms
step:470/2110 train_time:15991ms step_avg:34.02ms
step:471/2110 train_time:16023ms step_avg:34.02ms
step:472/2110 train_time:16059ms step_avg:34.02ms
step:473/2110 train_time:16090ms step_avg:34.02ms
step:474/2110 train_time:16126ms step_avg:34.02ms
step:475/2110 train_time:16162ms step_avg:34.02ms
step:476/2110 train_time:16196ms step_avg:34.02ms
step:477/2110 train_time:16227ms step_avg:34.02ms
step:478/2110 train_time:16256ms step_avg:34.01ms
step:479/2110 train_time:16283ms step_avg:33.99ms
step:480/2110 train_time:16314ms step_avg:33.99ms
step:481/2110 train_time:16343ms step_avg:33.98ms
step:482/2110 train_time:16372ms step_avg:33.97ms
step:483/2110 train_time:16399ms step_avg:33.95ms
step:484/2110 train_time:16434ms step_avg:33.95ms
step:485/2110 train_time:16466ms step_avg:33.95ms
step:486/2110 train_time:16499ms step_avg:33.95ms
step:487/2110 train_time:16532ms step_avg:33.95ms
step:488/2110 train_time:16565ms step_avg:33.94ms
step:489/2110 train_time:16598ms step_avg:33.94ms
step:490/2110 train_time:16631ms step_avg:33.94ms
step:491/2110 train_time:16664ms step_avg:33.94ms
step:492/2110 train_time:16697ms step_avg:33.94ms
step:493/2110 train_time:16729ms step_avg:33.93ms
step:494/2110 train_time:16762ms step_avg:33.93ms
step:495/2110 train_time:16795ms step_avg:33.93ms
step:496/2110 train_time:16828ms step_avg:33.93ms
step:497/2110 train_time:16861ms step_avg:33.93ms
step:498/2110 train_time:16894ms step_avg:33.92ms
step:499/2110 train_time:16927ms step_avg:33.92ms
step:500/2110 train_time:16961ms step_avg:33.92ms
step:500/2110 val_loss:4.0318 train_time:16995ms step_avg:33.99ms
step:501/2110 train_time:17019ms step_avg:33.97ms
step:502/2110 train_time:17047ms step_avg:33.96ms
step:503/2110 train_time:17071ms step_avg:33.94ms
step:504/2110 train_time:17098ms step_avg:33.93ms
step:505/2110 train_time:17135ms step_avg:33.93ms
step:506/2110 train_time:17169ms step_avg:33.93ms
step:507/2110 train_time:17203ms step_avg:33.93ms
step:508/2110 train_time:17237ms step_avg:33.93ms
step:509/2110 train_time:17270ms step_avg:33.93ms
step:510/2110 train_time:17303ms step_avg:33.93ms
step:511/2110 train_time:17336ms step_avg:33.93ms
step:512/2110 train_time:17368ms step_avg:33.92ms
step:513/2110 train_time:17402ms step_avg:33.92ms
step:514/2110 train_time:17434ms step_avg:33.92ms
step:515/2110 train_time:17468ms step_avg:33.92ms
step:516/2110 train_time:17500ms step_avg:33.92ms
step:517/2110 train_time:17533ms step_avg:33.91ms
step:518/2110 train_time:17565ms step_avg:33.91ms
step:519/2110 train_time:17598ms step_avg:33.91ms
step:520/2110 train_time:17630ms step_avg:33.90ms
step:521/2110 train_time:17663ms step_avg:33.90ms
step:522/2110 train_time:17696ms step_avg:33.90ms
step:523/2110 train_time:17729ms step_avg:33.90ms
step:524/2110 train_time:17761ms step_avg:33.90ms
step:525/2110 train_time:17794ms step_avg:33.89ms
step:526/2110 train_time:17827ms step_avg:33.89ms
step:527/2110 train_time:17860ms step_avg:33.89ms
step:528/2110 train_time:17894ms step_avg:33.89ms
step:529/2110 train_time:17926ms step_avg:33.89ms
step:530/2110 train_time:17960ms step_avg:33.89ms
step:531/2110 train_time:17992ms step_avg:33.88ms
step:532/2110 train_time:18026ms step_avg:33.88ms
step:533/2110 train_time:18059ms step_avg:33.88ms
step:534/2110 train_time:18092ms step_avg:33.88ms
step:535/2110 train_time:18126ms step_avg:33.88ms
step:536/2110 train_time:18159ms step_avg:33.88ms
step:537/2110 train_time:18193ms step_avg:33.88ms
step:538/2110 train_time:18226ms step_avg:33.88ms
step:539/2110 train_time:18260ms step_avg:33.88ms
step:540/2110 train_time:18293ms step_avg:33.88ms
step:541/2110 train_time:18326ms step_avg:33.87ms
step:542/2110 train_time:18358ms step_avg:33.87ms
step:543/2110 train_time:18392ms step_avg:33.87ms
step:544/2110 train_time:18424ms step_avg:33.87ms
step:545/2110 train_time:18458ms step_avg:33.87ms
step:546/2110 train_time:18490ms step_avg:33.86ms
step:547/2110 train_time:18524ms step_avg:33.86ms
step:548/2110 train_time:18557ms step_avg:33.86ms
step:549/2110 train_time:18590ms step_avg:33.86ms
step:550/2110 train_time:18624ms step_avg:33.86ms
step:551/2110 train_time:18655ms step_avg:33.86ms
step:552/2110 train_time:18688ms step_avg:33.85ms
step:553/2110 train_time:18721ms step_avg:33.85ms
step:554/2110 train_time:18754ms step_avg:33.85ms
step:555/2110 train_time:18787ms step_avg:33.85ms
step:556/2110 train_time:18819ms step_avg:33.85ms
step:557/2110 train_time:18852ms step_avg:33.85ms
step:558/2110 train_time:18885ms step_avg:33.84ms
step:559/2110 train_time:18918ms step_avg:33.84ms
step:560/2110 train_time:18950ms step_avg:33.84ms
step:561/2110 train_time:18983ms step_avg:33.84ms
step:562/2110 train_time:19016ms step_avg:33.84ms
step:563/2110 train_time:19049ms step_avg:33.84ms
step:564/2110 train_time:19083ms step_avg:33.84ms
step:565/2110 train_time:19116ms step_avg:33.83ms
step:566/2110 train_time:19149ms step_avg:33.83ms
step:567/2110 train_time:19183ms step_avg:33.83ms
step:568/2110 train_time:19216ms step_avg:33.83ms
step:569/2110 train_time:19249ms step_avg:33.83ms
step:570/2110 train_time:19282ms step_avg:33.83ms
step:571/2110 train_time:19315ms step_avg:33.83ms
step:572/2110 train_time:19348ms step_avg:33.82ms
step:573/2110 train_time:19381ms step_avg:33.82ms
step:574/2110 train_time:19414ms step_avg:33.82ms
step:575/2110 train_time:19447ms step_avg:33.82ms
step:576/2110 train_time:19479ms step_avg:33.82ms
step:577/2110 train_time:19513ms step_avg:33.82ms
step:578/2110 train_time:19546ms step_avg:33.82ms
step:579/2110 train_time:19579ms step_avg:33.82ms
step:580/2110 train_time:19612ms step_avg:33.81ms
step:581/2110 train_time:19645ms step_avg:33.81ms
step:582/2110 train_time:19677ms step_avg:33.81ms
step:583/2110 train_time:19711ms step_avg:33.81ms
step:584/2110 train_time:19744ms step_avg:33.81ms
step:585/2110 train_time:19777ms step_avg:33.81ms
step:586/2110 train_time:19809ms step_avg:33.80ms
step:587/2110 train_time:19843ms step_avg:33.80ms
step:588/2110 train_time:19876ms step_avg:33.80ms
step:589/2110 train_time:19909ms step_avg:33.80ms
step:590/2110 train_time:19941ms step_avg:33.80ms
step:591/2110 train_time:19975ms step_avg:33.80ms
step:592/2110 train_time:20008ms step_avg:33.80ms
step:593/2110 train_time:20041ms step_avg:33.80ms
step:594/2110 train_time:20074ms step_avg:33.80ms
step:595/2110 train_time:20107ms step_avg:33.79ms
step:596/2110 train_time:20140ms step_avg:33.79ms
step:597/2110 train_time:20173ms step_avg:33.79ms
step:598/2110 train_time:20206ms step_avg:33.79ms
step:599/2110 train_time:20240ms step_avg:33.79ms
step:600/2110 train_time:20272ms step_avg:33.79ms
step:601/2110 train_time:20306ms step_avg:33.79ms
step:602/2110 train_time:20340ms step_avg:33.79ms
step:603/2110 train_time:20373ms step_avg:33.79ms
step:604/2110 train_time:20406ms step_avg:33.78ms
step:605/2110 train_time:20439ms step_avg:33.78ms
step:606/2110 train_time:20471ms step_avg:33.78ms
step:607/2110 train_time:20505ms step_avg:33.78ms
step:608/2110 train_time:20539ms step_avg:33.78ms
step:609/2110 train_time:20571ms step_avg:33.78ms
step:610/2110 train_time:20604ms step_avg:33.78ms
step:611/2110 train_time:20636ms step_avg:33.77ms
step:612/2110 train_time:20669ms step_avg:33.77ms
step:613/2110 train_time:20703ms step_avg:33.77ms
step:614/2110 train_time:20735ms step_avg:33.77ms
step:615/2110 train_time:20768ms step_avg:33.77ms
step:616/2110 train_time:20802ms step_avg:33.77ms
step:617/2110 train_time:20834ms step_avg:33.77ms
step:618/2110 train_time:20868ms step_avg:33.77ms
step:619/2110 train_time:20900ms step_avg:33.76ms
step:620/2110 train_time:20933ms step_avg:33.76ms
step:621/2110 train_time:20966ms step_avg:33.76ms
step:622/2110 train_time:20999ms step_avg:33.76ms
step:623/2110 train_time:21033ms step_avg:33.76ms
step:624/2110 train_time:21068ms step_avg:33.76ms
step:625/2110 train_time:21101ms step_avg:33.76ms
step:626/2110 train_time:21137ms step_avg:33.76ms
step:627/2110 train_time:21166ms step_avg:33.76ms
step:628/2110 train_time:21199ms step_avg:33.76ms
step:629/2110 train_time:21230ms step_avg:33.75ms
step:630/2110 train_time:21263ms step_avg:33.75ms
step:631/2110 train_time:21296ms step_avg:33.75ms
step:632/2110 train_time:21330ms step_avg:33.75ms
step:633/2110 train_time:21363ms step_avg:33.75ms
step:634/2110 train_time:21396ms step_avg:33.75ms
step:635/2110 train_time:21429ms step_avg:33.75ms
step:636/2110 train_time:21462ms step_avg:33.75ms
step:637/2110 train_time:21495ms step_avg:33.74ms
step:638/2110 train_time:21528ms step_avg:33.74ms
step:639/2110 train_time:21561ms step_avg:33.74ms
step:640/2110 train_time:21594ms step_avg:33.74ms
step:641/2110 train_time:21627ms step_avg:33.74ms
step:642/2110 train_time:21660ms step_avg:33.74ms
step:643/2110 train_time:21693ms step_avg:33.74ms
step:644/2110 train_time:21726ms step_avg:33.74ms
step:645/2110 train_time:21759ms step_avg:33.74ms
step:646/2110 train_time:21792ms step_avg:33.73ms
step:647/2110 train_time:21825ms step_avg:33.73ms
step:648/2110 train_time:21857ms step_avg:33.73ms
step:649/2110 train_time:21891ms step_avg:33.73ms
step:650/2110 train_time:21923ms step_avg:33.73ms
step:651/2110 train_time:21956ms step_avg:33.73ms
step:652/2110 train_time:21989ms step_avg:33.73ms
step:653/2110 train_time:22023ms step_avg:33.73ms
step:654/2110 train_time:22056ms step_avg:33.72ms
step:655/2110 train_time:22089ms step_avg:33.72ms
step:656/2110 train_time:22123ms step_avg:33.72ms
step:657/2110 train_time:22155ms step_avg:33.72ms
step:658/2110 train_time:22188ms step_avg:33.72ms
step:659/2110 train_time:22221ms step_avg:33.72ms
step:660/2110 train_time:22254ms step_avg:33.72ms
step:661/2110 train_time:22287ms step_avg:33.72ms
step:662/2110 train_time:22323ms step_avg:33.72ms
step:663/2110 train_time:22357ms step_avg:33.72ms
step:664/2110 train_time:22391ms step_avg:33.72ms
step:665/2110 train_time:22423ms step_avg:33.72ms
step:666/2110 train_time:22457ms step_avg:33.72ms
step:667/2110 train_time:22487ms step_avg:33.71ms
step:668/2110 train_time:22520ms step_avg:33.71ms
step:669/2110 train_time:22551ms step_avg:33.71ms
step:670/2110 train_time:22584ms step_avg:33.71ms
step:671/2110 train_time:22617ms step_avg:33.71ms
step:672/2110 train_time:22650ms step_avg:33.71ms
step:673/2110 train_time:22683ms step_avg:33.70ms
step:674/2110 train_time:22716ms step_avg:33.70ms
step:675/2110 train_time:22749ms step_avg:33.70ms
step:676/2110 train_time:22783ms step_avg:33.70ms
step:677/2110 train_time:22815ms step_avg:33.70ms
step:678/2110 train_time:22850ms step_avg:33.70ms
step:679/2110 train_time:22881ms step_avg:33.70ms
step:680/2110 train_time:22913ms step_avg:33.70ms
step:681/2110 train_time:22947ms step_avg:33.70ms
step:682/2110 train_time:22980ms step_avg:33.69ms
step:683/2110 train_time:23013ms step_avg:33.69ms
step:684/2110 train_time:23045ms step_avg:33.69ms
step:685/2110 train_time:23079ms step_avg:33.69ms
step:686/2110 train_time:23112ms step_avg:33.69ms
step:687/2110 train_time:23145ms step_avg:33.69ms
step:688/2110 train_time:23178ms step_avg:33.69ms
step:689/2110 train_time:23211ms step_avg:33.69ms
step:690/2110 train_time:23244ms step_avg:33.69ms
step:691/2110 train_time:23278ms step_avg:33.69ms
step:692/2110 train_time:23335ms step_avg:33.72ms
step:693/2110 train_time:23396ms step_avg:33.76ms
step:694/2110 train_time:23456ms step_avg:33.80ms
step:695/2110 train_time:23515ms step_avg:33.83ms
step:696/2110 train_time:23574ms step_avg:33.87ms
step:697/2110 train_time:23633ms step_avg:33.91ms
step:698/2110 train_time:23693ms step_avg:33.94ms
step:699/2110 train_time:23753ms step_avg:33.98ms
step:700/2110 train_time:23811ms step_avg:34.02ms
step:701/2110 train_time:23871ms step_avg:34.05ms
step:702/2110 train_time:23930ms step_avg:34.09ms
step:703/2110 train_time:23989ms step_avg:34.12ms
step:704/2110 train_time:24048ms step_avg:34.16ms
step:705/2110 train_time:24108ms step_avg:34.20ms
step:706/2110 train_time:24167ms step_avg:34.23ms
step:707/2110 train_time:24227ms step_avg:34.27ms
step:708/2110 train_time:24286ms step_avg:34.30ms
step:709/2110 train_time:24345ms step_avg:34.34ms
step:710/2110 train_time:24404ms step_avg:34.37ms
step:711/2110 train_time:24464ms step_avg:34.41ms
step:712/2110 train_time:24522ms step_avg:34.44ms
step:713/2110 train_time:24583ms step_avg:34.48ms
step:714/2110 train_time:24641ms step_avg:34.51ms
step:715/2110 train_time:24702ms step_avg:34.55ms
step:716/2110 train_time:24761ms step_avg:34.58ms
step:717/2110 train_time:24821ms step_avg:34.62ms
step:718/2110 train_time:24880ms step_avg:34.65ms
step:719/2110 train_time:24940ms step_avg:34.69ms
step:720/2110 train_time:24998ms step_avg:34.72ms
step:721/2110 train_time:25058ms step_avg:34.75ms
step:722/2110 train_time:25116ms step_avg:34.79ms
step:723/2110 train_time:25177ms step_avg:34.82ms
step:724/2110 train_time:25235ms step_avg:34.85ms
step:725/2110 train_time:25295ms step_avg:34.89ms
step:726/2110 train_time:25354ms step_avg:34.92ms
step:727/2110 train_time:25414ms step_avg:34.96ms
step:728/2110 train_time:25474ms step_avg:34.99ms
step:729/2110 train_time:25534ms step_avg:35.03ms
step:730/2110 train_time:25594ms step_avg:35.06ms
step:731/2110 train_time:25654ms step_avg:35.09ms
step:732/2110 train_time:25716ms step_avg:35.13ms
step:733/2110 train_time:25774ms step_avg:35.16ms
step:734/2110 train_time:25833ms step_avg:35.20ms
step:735/2110 train_time:25893ms step_avg:35.23ms
step:736/2110 train_time:25952ms step_avg:35.26ms
step:737/2110 train_time:26012ms step_avg:35.29ms
step:738/2110 train_time:26070ms step_avg:35.32ms
step:739/2110 train_time:26129ms step_avg:35.36ms
step:740/2110 train_time:26188ms step_avg:35.39ms
step:741/2110 train_time:26248ms step_avg:35.42ms
step:742/2110 train_time:26306ms step_avg:35.45ms
step:743/2110 train_time:26365ms step_avg:35.49ms
step:744/2110 train_time:26423ms step_avg:35.52ms
step:745/2110 train_time:26484ms step_avg:35.55ms
step:746/2110 train_time:26542ms step_avg:35.58ms
step:747/2110 train_time:26603ms step_avg:35.61ms
step:748/2110 train_time:26663ms step_avg:35.65ms
step:749/2110 train_time:26722ms step_avg:35.68ms
step:750/2110 train_time:26781ms step_avg:35.71ms
step:750/2110 val_loss:3.9091 train_time:26843ms step_avg:35.79ms
step:751/2110 train_time:26882ms step_avg:35.80ms
step:752/2110 train_time:26920ms step_avg:35.80ms
step:753/2110 train_time:26969ms step_avg:35.82ms
step:754/2110 train_time:27032ms step_avg:35.85ms
step:755/2110 train_time:27091ms step_avg:35.88ms
step:756/2110 train_time:27151ms step_avg:35.91ms
step:757/2110 train_time:27209ms step_avg:35.94ms
step:758/2110 train_time:27268ms step_avg:35.97ms
step:759/2110 train_time:27326ms step_avg:36.00ms
step:760/2110 train_time:27385ms step_avg:36.03ms
step:761/2110 train_time:27443ms step_avg:36.06ms
step:762/2110 train_time:27502ms step_avg:36.09ms
step:763/2110 train_time:27560ms step_avg:36.12ms
step:764/2110 train_time:27619ms step_avg:36.15ms
step:765/2110 train_time:27677ms step_avg:36.18ms
step:766/2110 train_time:27735ms step_avg:36.21ms
step:767/2110 train_time:27794ms step_avg:36.24ms
step:768/2110 train_time:27854ms step_avg:36.27ms
step:769/2110 train_time:27915ms step_avg:36.30ms
step:770/2110 train_time:27976ms step_avg:36.33ms
step:771/2110 train_time:28036ms step_avg:36.36ms
step:772/2110 train_time:28097ms step_avg:36.39ms
step:773/2110 train_time:28155ms step_avg:36.42ms
step:774/2110 train_time:28214ms step_avg:36.45ms
step:775/2110 train_time:28273ms step_avg:36.48ms
step:776/2110 train_time:28330ms step_avg:36.51ms
step:777/2110 train_time:28390ms step_avg:36.54ms
step:778/2110 train_time:28448ms step_avg:36.57ms
step:779/2110 train_time:28507ms step_avg:36.59ms
step:780/2110 train_time:28565ms step_avg:36.62ms
step:781/2110 train_time:28625ms step_avg:36.65ms
step:782/2110 train_time:28683ms step_avg:36.68ms
step:783/2110 train_time:28742ms step_avg:36.71ms
step:784/2110 train_time:28802ms step_avg:36.74ms
step:785/2110 train_time:28862ms step_avg:36.77ms
step:786/2110 train_time:28923ms step_avg:36.80ms
step:787/2110 train_time:28984ms step_avg:36.83ms
step:788/2110 train_time:29044ms step_avg:36.86ms
step:789/2110 train_time:29105ms step_avg:36.89ms
step:790/2110 train_time:29164ms step_avg:36.92ms
step:791/2110 train_time:29224ms step_avg:36.95ms
step:792/2110 train_time:29283ms step_avg:36.97ms
step:793/2110 train_time:29344ms step_avg:37.00ms
step:794/2110 train_time:29403ms step_avg:37.03ms
step:795/2110 train_time:29461ms step_avg:37.06ms
step:796/2110 train_time:29520ms step_avg:37.09ms
step:797/2110 train_time:29579ms step_avg:37.11ms
step:798/2110 train_time:29639ms step_avg:37.14ms
step:799/2110 train_time:29696ms step_avg:37.17ms
step:800/2110 train_time:29754ms step_avg:37.19ms
step:801/2110 train_time:29813ms step_avg:37.22ms
step:802/2110 train_time:29872ms step_avg:37.25ms
step:803/2110 train_time:29933ms step_avg:37.28ms
step:804/2110 train_time:29992ms step_avg:37.30ms
step:805/2110 train_time:30052ms step_avg:37.33ms
step:806/2110 train_time:30112ms step_avg:37.36ms
step:807/2110 train_time:30171ms step_avg:37.39ms
step:808/2110 train_time:30230ms step_avg:37.41ms
step:809/2110 train_time:30290ms step_avg:37.44ms
step:810/2110 train_time:30349ms step_avg:37.47ms
step:811/2110 train_time:30408ms step_avg:37.49ms
step:812/2110 train_time:30468ms step_avg:37.52ms
step:813/2110 train_time:30526ms step_avg:37.55ms
step:814/2110 train_time:30585ms step_avg:37.57ms
step:815/2110 train_time:30644ms step_avg:37.60ms
step:816/2110 train_time:30704ms step_avg:37.63ms
step:817/2110 train_time:30763ms step_avg:37.65ms
step:818/2110 train_time:30823ms step_avg:37.68ms
step:819/2110 train_time:30882ms step_avg:37.71ms
step:820/2110 train_time:30942ms step_avg:37.73ms
step:821/2110 train_time:31002ms step_avg:37.76ms
step:822/2110 train_time:31062ms step_avg:37.79ms
step:823/2110 train_time:31122ms step_avg:37.82ms
step:824/2110 train_time:31182ms step_avg:37.84ms
step:825/2110 train_time:31242ms step_avg:37.87ms
step:826/2110 train_time:31302ms step_avg:37.90ms
step:827/2110 train_time:31361ms step_avg:37.92ms
step:828/2110 train_time:31421ms step_avg:37.95ms
step:829/2110 train_time:31479ms step_avg:37.97ms
step:830/2110 train_time:31539ms step_avg:38.00ms
step:831/2110 train_time:31596ms step_avg:38.02ms
step:832/2110 train_time:31656ms step_avg:38.05ms
step:833/2110 train_time:31714ms step_avg:38.07ms
step:834/2110 train_time:31773ms step_avg:38.10ms
step:835/2110 train_time:31832ms step_avg:38.12ms
step:836/2110 train_time:31892ms step_avg:38.15ms
step:837/2110 train_time:31953ms step_avg:38.18ms
step:838/2110 train_time:32012ms step_avg:38.20ms
step:839/2110 train_time:32071ms step_avg:38.23ms
step:840/2110 train_time:32131ms step_avg:38.25ms
step:841/2110 train_time:32192ms step_avg:38.28ms
step:842/2110 train_time:32250ms step_avg:38.30ms
step:843/2110 train_time:32310ms step_avg:38.33ms
step:844/2110 train_time:32369ms step_avg:38.35ms
step:845/2110 train_time:32428ms step_avg:38.38ms
step:846/2110 train_time:32487ms step_avg:38.40ms
step:847/2110 train_time:32546ms step_avg:38.43ms
step:848/2110 train_time:32607ms step_avg:38.45ms
step:849/2110 train_time:32666ms step_avg:38.48ms
step:850/2110 train_time:32725ms step_avg:38.50ms
step:851/2110 train_time:32784ms step_avg:38.52ms
step:852/2110 train_time:32844ms step_avg:38.55ms
step:853/2110 train_time:32903ms step_avg:38.57ms
step:854/2110 train_time:32963ms step_avg:38.60ms
step:855/2110 train_time:33022ms step_avg:38.62ms
step:856/2110 train_time:33082ms step_avg:38.65ms
step:857/2110 train_time:33141ms step_avg:38.67ms
step:858/2110 train_time:33200ms step_avg:38.70ms
step:859/2110 train_time:33259ms step_avg:38.72ms
step:860/2110 train_time:33318ms step_avg:38.74ms
step:861/2110 train_time:33378ms step_avg:38.77ms
step:862/2110 train_time:33436ms step_avg:38.79ms
step:863/2110 train_time:33495ms step_avg:38.81ms
step:864/2110 train_time:33555ms step_avg:38.84ms
step:865/2110 train_time:33614ms step_avg:38.86ms
step:866/2110 train_time:33672ms step_avg:38.88ms
step:867/2110 train_time:33732ms step_avg:38.91ms
step:868/2110 train_time:33791ms step_avg:38.93ms
step:869/2110 train_time:33850ms step_avg:38.95ms
step:870/2110 train_time:33910ms step_avg:38.98ms
step:871/2110 train_time:33970ms step_avg:39.00ms
step:872/2110 train_time:34029ms step_avg:39.02ms
step:873/2110 train_time:34089ms step_avg:39.05ms
step:874/2110 train_time:34148ms step_avg:39.07ms
step:875/2110 train_time:34208ms step_avg:39.10ms
step:876/2110 train_time:34267ms step_avg:39.12ms
step:877/2110 train_time:34326ms step_avg:39.14ms
step:878/2110 train_time:34386ms step_avg:39.16ms
step:879/2110 train_time:34445ms step_avg:39.19ms
step:880/2110 train_time:34505ms step_avg:39.21ms
step:881/2110 train_time:34563ms step_avg:39.23ms
step:882/2110 train_time:34623ms step_avg:39.25ms
step:883/2110 train_time:34682ms step_avg:39.28ms
step:884/2110 train_time:34741ms step_avg:39.30ms
step:885/2110 train_time:34801ms step_avg:39.32ms
step:886/2110 train_time:34861ms step_avg:39.35ms
step:887/2110 train_time:34920ms step_avg:39.37ms
step:888/2110 train_time:34980ms step_avg:39.39ms
step:889/2110 train_time:35039ms step_avg:39.41ms
step:890/2110 train_time:35099ms step_avg:39.44ms
step:891/2110 train_time:35159ms step_avg:39.46ms
step:892/2110 train_time:35219ms step_avg:39.48ms
step:893/2110 train_time:35278ms step_avg:39.50ms
step:894/2110 train_time:35337ms step_avg:39.53ms
step:895/2110 train_time:35397ms step_avg:39.55ms
step:896/2110 train_time:35456ms step_avg:39.57ms
step:897/2110 train_time:35515ms step_avg:39.59ms
step:898/2110 train_time:35574ms step_avg:39.61ms
step:899/2110 train_time:35633ms step_avg:39.64ms
step:900/2110 train_time:35692ms step_avg:39.66ms
step:901/2110 train_time:35752ms step_avg:39.68ms
step:902/2110 train_time:35811ms step_avg:39.70ms
step:903/2110 train_time:35870ms step_avg:39.72ms
step:904/2110 train_time:35929ms step_avg:39.74ms
step:905/2110 train_time:35989ms step_avg:39.77ms
step:906/2110 train_time:36049ms step_avg:39.79ms
step:907/2110 train_time:36109ms step_avg:39.81ms
step:908/2110 train_time:36168ms step_avg:39.83ms
step:909/2110 train_time:36227ms step_avg:39.85ms
step:910/2110 train_time:36286ms step_avg:39.88ms
step:911/2110 train_time:36346ms step_avg:39.90ms
step:912/2110 train_time:36405ms step_avg:39.92ms
step:913/2110 train_time:36464ms step_avg:39.94ms
step:914/2110 train_time:36523ms step_avg:39.96ms
step:915/2110 train_time:36583ms step_avg:39.98ms
step:916/2110 train_time:36643ms step_avg:40.00ms
step:917/2110 train_time:36703ms step_avg:40.03ms
step:918/2110 train_time:36762ms step_avg:40.05ms
step:919/2110 train_time:36821ms step_avg:40.07ms
step:920/2110 train_time:36880ms step_avg:40.09ms
step:921/2110 train_time:36940ms step_avg:40.11ms
step:922/2110 train_time:36999ms step_avg:40.13ms
step:923/2110 train_time:37059ms step_avg:40.15ms
step:924/2110 train_time:37120ms step_avg:40.17ms
step:925/2110 train_time:37179ms step_avg:40.19ms
step:926/2110 train_time:37238ms step_avg:40.21ms
step:927/2110 train_time:37297ms step_avg:40.23ms
step:928/2110 train_time:37356ms step_avg:40.25ms
step:929/2110 train_time:37415ms step_avg:40.27ms
step:930/2110 train_time:37473ms step_avg:40.29ms
step:931/2110 train_time:37534ms step_avg:40.32ms
step:932/2110 train_time:37593ms step_avg:40.34ms
step:933/2110 train_time:37653ms step_avg:40.36ms
step:934/2110 train_time:37712ms step_avg:40.38ms
step:935/2110 train_time:37770ms step_avg:40.40ms
step:936/2110 train_time:37829ms step_avg:40.42ms
step:937/2110 train_time:37889ms step_avg:40.44ms
step:938/2110 train_time:37948ms step_avg:40.46ms
step:939/2110 train_time:38007ms step_avg:40.48ms
step:940/2110 train_time:38067ms step_avg:40.50ms
step:941/2110 train_time:38126ms step_avg:40.52ms
step:942/2110 train_time:38186ms step_avg:40.54ms
step:943/2110 train_time:38245ms step_avg:40.56ms
step:944/2110 train_time:38305ms step_avg:40.58ms
step:945/2110 train_time:38365ms step_avg:40.60ms
step:946/2110 train_time:38424ms step_avg:40.62ms
step:947/2110 train_time:38483ms step_avg:40.64ms
step:948/2110 train_time:38543ms step_avg:40.66ms
step:949/2110 train_time:38602ms step_avg:40.68ms
step:950/2110 train_time:38662ms step_avg:40.70ms
step:951/2110 train_time:38721ms step_avg:40.72ms
step:952/2110 train_time:38781ms step_avg:40.74ms
step:953/2110 train_time:38840ms step_avg:40.76ms
step:954/2110 train_time:38900ms step_avg:40.78ms
step:955/2110 train_time:38959ms step_avg:40.80ms
step:956/2110 train_time:39019ms step_avg:40.81ms
step:957/2110 train_time:39078ms step_avg:40.83ms
step:958/2110 train_time:39137ms step_avg:40.85ms
step:959/2110 train_time:39196ms step_avg:40.87ms
step:960/2110 train_time:39256ms step_avg:40.89ms
step:961/2110 train_time:39315ms step_avg:40.91ms
step:962/2110 train_time:39374ms step_avg:40.93ms
step:963/2110 train_time:39434ms step_avg:40.95ms
step:964/2110 train_time:39493ms step_avg:40.97ms
step:965/2110 train_time:39552ms step_avg:40.99ms
step:966/2110 train_time:39611ms step_avg:41.01ms
step:967/2110 train_time:39671ms step_avg:41.02ms
step:968/2110 train_time:39729ms step_avg:41.04ms
step:969/2110 train_time:39789ms step_avg:41.06ms
step:970/2110 train_time:39848ms step_avg:41.08ms
step:971/2110 train_time:39909ms step_avg:41.10ms
step:972/2110 train_time:39967ms step_avg:41.12ms
step:973/2110 train_time:40028ms step_avg:41.14ms
step:974/2110 train_time:40085ms step_avg:41.16ms
step:975/2110 train_time:40146ms step_avg:41.17ms
step:976/2110 train_time:40206ms step_avg:41.19ms
step:977/2110 train_time:40266ms step_avg:41.21ms
step:978/2110 train_time:40325ms step_avg:41.23ms
step:979/2110 train_time:40385ms step_avg:41.25ms
step:980/2110 train_time:40444ms step_avg:41.27ms
step:981/2110 train_time:40504ms step_avg:41.29ms
step:982/2110 train_time:40562ms step_avg:41.31ms
step:983/2110 train_time:40622ms step_avg:41.32ms
step:984/2110 train_time:40680ms step_avg:41.34ms
step:985/2110 train_time:40740ms step_avg:41.36ms
step:986/2110 train_time:40798ms step_avg:41.38ms
step:987/2110 train_time:40858ms step_avg:41.40ms
step:988/2110 train_time:40917ms step_avg:41.41ms
step:989/2110 train_time:40976ms step_avg:41.43ms
step:990/2110 train_time:41035ms step_avg:41.45ms
step:991/2110 train_time:41095ms step_avg:41.47ms
step:992/2110 train_time:41153ms step_avg:41.49ms
step:993/2110 train_time:41213ms step_avg:41.50ms
step:994/2110 train_time:41271ms step_avg:41.52ms
step:995/2110 train_time:41332ms step_avg:41.54ms
step:996/2110 train_time:41391ms step_avg:41.56ms
step:997/2110 train_time:41451ms step_avg:41.58ms
step:998/2110 train_time:41509ms step_avg:41.59ms
step:999/2110 train_time:41569ms step_avg:41.61ms
step:1000/2110 train_time:41627ms step_avg:41.63ms
step:1000/2110 val_loss:3.7654 train_time:41689ms step_avg:41.69ms
step:1001/2110 train_time:41716ms step_avg:41.67ms
step:1002/2110 train_time:41749ms step_avg:41.67ms
step:1003/2110 train_time:41811ms step_avg:41.69ms
step:1004/2110 train_time:41874ms step_avg:41.71ms
step:1005/2110 train_time:41935ms step_avg:41.73ms
step:1006/2110 train_time:41994ms step_avg:41.74ms
step:1007/2110 train_time:42054ms step_avg:41.76ms
step:1008/2110 train_time:42111ms step_avg:41.78ms
step:1009/2110 train_time:42170ms step_avg:41.79ms
step:1010/2110 train_time:42228ms step_avg:41.81ms
step:1011/2110 train_time:42286ms step_avg:41.83ms
step:1012/2110 train_time:42344ms step_avg:41.84ms
step:1013/2110 train_time:42403ms step_avg:41.86ms
step:1014/2110 train_time:42460ms step_avg:41.87ms
step:1015/2110 train_time:42519ms step_avg:41.89ms
step:1016/2110 train_time:42577ms step_avg:41.91ms
step:1017/2110 train_time:42637ms step_avg:41.92ms
step:1018/2110 train_time:42696ms step_avg:41.94ms
step:1019/2110 train_time:42757ms step_avg:41.96ms
step:1020/2110 train_time:42817ms step_avg:41.98ms
step:1021/2110 train_time:42879ms step_avg:42.00ms
step:1022/2110 train_time:42938ms step_avg:42.01ms
step:1023/2110 train_time:43000ms step_avg:42.03ms
step:1024/2110 train_time:43059ms step_avg:42.05ms
step:1025/2110 train_time:43118ms step_avg:42.07ms
step:1026/2110 train_time:43177ms step_avg:42.08ms
step:1027/2110 train_time:43236ms step_avg:42.10ms
step:1028/2110 train_time:43294ms step_avg:42.12ms
step:1029/2110 train_time:43354ms step_avg:42.13ms
step:1030/2110 train_time:43412ms step_avg:42.15ms
step:1031/2110 train_time:43470ms step_avg:42.16ms
step:1032/2110 train_time:43528ms step_avg:42.18ms
step:1033/2110 train_time:43586ms step_avg:42.19ms
step:1034/2110 train_time:43645ms step_avg:42.21ms
step:1035/2110 train_time:43704ms step_avg:42.23ms
step:1036/2110 train_time:43764ms step_avg:42.24ms
step:1037/2110 train_time:43825ms step_avg:42.26ms
step:1038/2110 train_time:43885ms step_avg:42.28ms
step:1039/2110 train_time:43945ms step_avg:42.30ms
step:1040/2110 train_time:44004ms step_avg:42.31ms
step:1041/2110 train_time:44065ms step_avg:42.33ms
step:1042/2110 train_time:44123ms step_avg:42.34ms
step:1043/2110 train_time:44184ms step_avg:42.36ms
step:1044/2110 train_time:44242ms step_avg:42.38ms
step:1045/2110 train_time:44302ms step_avg:42.39ms
step:1046/2110 train_time:44359ms step_avg:42.41ms
step:1047/2110 train_time:44420ms step_avg:42.43ms
step:1048/2110 train_time:44477ms step_avg:42.44ms
step:1049/2110 train_time:44536ms step_avg:42.46ms
step:1050/2110 train_time:44595ms step_avg:42.47ms
step:1051/2110 train_time:44654ms step_avg:42.49ms
step:1052/2110 train_time:44713ms step_avg:42.50ms
step:1053/2110 train_time:44773ms step_avg:42.52ms
step:1054/2110 train_time:44832ms step_avg:42.53ms
step:1055/2110 train_time:44892ms step_avg:42.55ms
step:1056/2110 train_time:44952ms step_avg:42.57ms
step:1057/2110 train_time:45011ms step_avg:42.58ms
step:1058/2110 train_time:45070ms step_avg:42.60ms
step:1059/2110 train_time:45131ms step_avg:42.62ms
step:1060/2110 train_time:45190ms step_avg:42.63ms
step:1061/2110 train_time:45249ms step_avg:42.65ms
step:1062/2110 train_time:45307ms step_avg:42.66ms
step:1063/2110 train_time:45367ms step_avg:42.68ms
step:1064/2110 train_time:45424ms step_avg:42.69ms
step:1065/2110 train_time:45485ms step_avg:42.71ms
step:1066/2110 train_time:45543ms step_avg:42.72ms
step:1067/2110 train_time:45603ms step_avg:42.74ms
step:1068/2110 train_time:45661ms step_avg:42.75ms
step:1069/2110 train_time:45721ms step_avg:42.77ms
step:1070/2110 train_time:45779ms step_avg:42.78ms
step:1071/2110 train_time:45839ms step_avg:42.80ms
step:1072/2110 train_time:45898ms step_avg:42.82ms
step:1073/2110 train_time:45959ms step_avg:42.83ms
step:1074/2110 train_time:46018ms step_avg:42.85ms
step:1075/2110 train_time:46078ms step_avg:42.86ms
step:1076/2110 train_time:46137ms step_avg:42.88ms
step:1077/2110 train_time:46197ms step_avg:42.89ms
step:1078/2110 train_time:46257ms step_avg:42.91ms
step:1079/2110 train_time:46317ms step_avg:42.93ms
step:1080/2110 train_time:46376ms step_avg:42.94ms
step:1081/2110 train_time:46435ms step_avg:42.96ms
step:1082/2110 train_time:46493ms step_avg:42.97ms
step:1083/2110 train_time:46552ms step_avg:42.98ms
step:1084/2110 train_time:46610ms step_avg:43.00ms
step:1085/2110 train_time:46670ms step_avg:43.01ms
step:1086/2110 train_time:46728ms step_avg:43.03ms
step:1087/2110 train_time:46788ms step_avg:43.04ms
step:1088/2110 train_time:46846ms step_avg:43.06ms
step:1089/2110 train_time:46906ms step_avg:43.07ms
step:1090/2110 train_time:46965ms step_avg:43.09ms
step:1091/2110 train_time:47025ms step_avg:43.10ms
step:1092/2110 train_time:47085ms step_avg:43.12ms
step:1093/2110 train_time:47145ms step_avg:43.13ms
step:1094/2110 train_time:47204ms step_avg:43.15ms
step:1095/2110 train_time:47264ms step_avg:43.16ms
step:1096/2110 train_time:47322ms step_avg:43.18ms
step:1097/2110 train_time:47382ms step_avg:43.19ms
step:1098/2110 train_time:47440ms step_avg:43.21ms
step:1099/2110 train_time:47500ms step_avg:43.22ms
step:1100/2110 train_time:47558ms step_avg:43.23ms
step:1101/2110 train_time:47618ms step_avg:43.25ms
step:1102/2110 train_time:47678ms step_avg:43.26ms
step:1103/2110 train_time:47737ms step_avg:43.28ms
step:1104/2110 train_time:47796ms step_avg:43.29ms
step:1105/2110 train_time:47856ms step_avg:43.31ms
step:1106/2110 train_time:47915ms step_avg:43.32ms
step:1107/2110 train_time:47974ms step_avg:43.34ms
step:1108/2110 train_time:48033ms step_avg:43.35ms
step:1109/2110 train_time:48093ms step_avg:43.37ms
step:1110/2110 train_time:48153ms step_avg:43.38ms
step:1111/2110 train_time:48213ms step_avg:43.40ms
step:1112/2110 train_time:48272ms step_avg:43.41ms
step:1113/2110 train_time:48331ms step_avg:43.42ms
step:1114/2110 train_time:48390ms step_avg:43.44ms
step:1115/2110 train_time:48449ms step_avg:43.45ms
step:1116/2110 train_time:48507ms step_avg:43.46ms
step:1117/2110 train_time:48567ms step_avg:43.48ms
step:1118/2110 train_time:48625ms step_avg:43.49ms
step:1119/2110 train_time:48685ms step_avg:43.51ms
step:1120/2110 train_time:48744ms step_avg:43.52ms
step:1121/2110 train_time:48804ms step_avg:43.54ms
step:1122/2110 train_time:48862ms step_avg:43.55ms
step:1123/2110 train_time:48922ms step_avg:43.56ms
step:1124/2110 train_time:48980ms step_avg:43.58ms
step:1125/2110 train_time:49041ms step_avg:43.59ms
step:1126/2110 train_time:49099ms step_avg:43.61ms
step:1127/2110 train_time:49160ms step_avg:43.62ms
step:1128/2110 train_time:49218ms step_avg:43.63ms
step:1129/2110 train_time:49279ms step_avg:43.65ms
step:1130/2110 train_time:49337ms step_avg:43.66ms
step:1131/2110 train_time:49397ms step_avg:43.68ms
step:1132/2110 train_time:49457ms step_avg:43.69ms
step:1133/2110 train_time:49518ms step_avg:43.70ms
step:1134/2110 train_time:49576ms step_avg:43.72ms
step:1135/2110 train_time:49636ms step_avg:43.73ms
step:1136/2110 train_time:49694ms step_avg:43.75ms
step:1137/2110 train_time:49755ms step_avg:43.76ms
step:1138/2110 train_time:49814ms step_avg:43.77ms
step:1139/2110 train_time:49873ms step_avg:43.79ms
step:1140/2110 train_time:49932ms step_avg:43.80ms
step:1141/2110 train_time:49993ms step_avg:43.81ms
step:1142/2110 train_time:50052ms step_avg:43.83ms
step:1143/2110 train_time:50113ms step_avg:43.84ms
step:1144/2110 train_time:50172ms step_avg:43.86ms
step:1145/2110 train_time:50233ms step_avg:43.87ms
step:1146/2110 train_time:50292ms step_avg:43.88ms
step:1147/2110 train_time:50352ms step_avg:43.90ms
step:1148/2110 train_time:50412ms step_avg:43.91ms
step:1149/2110 train_time:50472ms step_avg:43.93ms
step:1150/2110 train_time:50530ms step_avg:43.94ms
step:1151/2110 train_time:50591ms step_avg:43.95ms
step:1152/2110 train_time:50649ms step_avg:43.97ms
step:1153/2110 train_time:50710ms step_avg:43.98ms
step:1154/2110 train_time:50768ms step_avg:43.99ms
step:1155/2110 train_time:50829ms step_avg:44.01ms
step:1156/2110 train_time:50887ms step_avg:44.02ms
step:1157/2110 train_time:50948ms step_avg:44.03ms
step:1158/2110 train_time:51006ms step_avg:44.05ms
step:1159/2110 train_time:51067ms step_avg:44.06ms
step:1160/2110 train_time:51125ms step_avg:44.07ms
step:1161/2110 train_time:51186ms step_avg:44.09ms
step:1162/2110 train_time:51245ms step_avg:44.10ms
step:1163/2110 train_time:51306ms step_avg:44.11ms
step:1164/2110 train_time:51364ms step_avg:44.13ms
step:1165/2110 train_time:51425ms step_avg:44.14ms
step:1166/2110 train_time:51485ms step_avg:44.16ms
step:1167/2110 train_time:51547ms step_avg:44.17ms
step:1168/2110 train_time:51606ms step_avg:44.18ms
step:1169/2110 train_time:51666ms step_avg:44.20ms
step:1170/2110 train_time:51725ms step_avg:44.21ms
step:1171/2110 train_time:51785ms step_avg:44.22ms
step:1172/2110 train_time:51844ms step_avg:44.24ms
step:1173/2110 train_time:51905ms step_avg:44.25ms
step:1174/2110 train_time:51963ms step_avg:44.26ms
step:1175/2110 train_time:52024ms step_avg:44.28ms
step:1176/2110 train_time:52082ms step_avg:44.29ms
step:1177/2110 train_time:52144ms step_avg:44.30ms
step:1178/2110 train_time:52202ms step_avg:44.31ms
step:1179/2110 train_time:52262ms step_avg:44.33ms
step:1180/2110 train_time:52321ms step_avg:44.34ms
step:1181/2110 train_time:52381ms step_avg:44.35ms
step:1182/2110 train_time:52440ms step_avg:44.37ms
step:1183/2110 train_time:52501ms step_avg:44.38ms
step:1184/2110 train_time:52560ms step_avg:44.39ms
step:1185/2110 train_time:52621ms step_avg:44.41ms
step:1186/2110 train_time:52680ms step_avg:44.42ms
step:1187/2110 train_time:52741ms step_avg:44.43ms
step:1188/2110 train_time:52800ms step_avg:44.44ms
step:1189/2110 train_time:52861ms step_avg:44.46ms
step:1190/2110 train_time:52919ms step_avg:44.47ms
step:1191/2110 train_time:52980ms step_avg:44.48ms
step:1192/2110 train_time:53039ms step_avg:44.50ms
step:1193/2110 train_time:53100ms step_avg:44.51ms
step:1194/2110 train_time:53159ms step_avg:44.52ms
step:1195/2110 train_time:53219ms step_avg:44.54ms
step:1196/2110 train_time:53279ms step_avg:44.55ms
step:1197/2110 train_time:53339ms step_avg:44.56ms
step:1198/2110 train_time:53398ms step_avg:44.57ms
step:1199/2110 train_time:53458ms step_avg:44.59ms
step:1200/2110 train_time:53518ms step_avg:44.60ms
step:1201/2110 train_time:53578ms step_avg:44.61ms
step:1202/2110 train_time:53638ms step_avg:44.62ms
step:1203/2110 train_time:53698ms step_avg:44.64ms
step:1204/2110 train_time:53758ms step_avg:44.65ms
step:1205/2110 train_time:53817ms step_avg:44.66ms
step:1206/2110 train_time:53877ms step_avg:44.67ms
step:1207/2110 train_time:53936ms step_avg:44.69ms
step:1208/2110 train_time:53996ms step_avg:44.70ms
step:1209/2110 train_time:54055ms step_avg:44.71ms
step:1210/2110 train_time:54114ms step_avg:44.72ms
step:1211/2110 train_time:54174ms step_avg:44.73ms
step:1212/2110 train_time:54234ms step_avg:44.75ms
step:1213/2110 train_time:54294ms step_avg:44.76ms
step:1214/2110 train_time:54354ms step_avg:44.77ms
step:1215/2110 train_time:54414ms step_avg:44.79ms
step:1216/2110 train_time:54474ms step_avg:44.80ms
step:1217/2110 train_time:54534ms step_avg:44.81ms
step:1218/2110 train_time:54594ms step_avg:44.82ms
step:1219/2110 train_time:54654ms step_avg:44.84ms
step:1220/2110 train_time:54714ms step_avg:44.85ms
step:1221/2110 train_time:54774ms step_avg:44.86ms
step:1222/2110 train_time:54833ms step_avg:44.87ms
step:1223/2110 train_time:54893ms step_avg:44.88ms
step:1224/2110 train_time:54952ms step_avg:44.90ms
step:1225/2110 train_time:55012ms step_avg:44.91ms
step:1226/2110 train_time:55072ms step_avg:44.92ms
step:1227/2110 train_time:55131ms step_avg:44.93ms
step:1228/2110 train_time:55190ms step_avg:44.94ms
step:1229/2110 train_time:55250ms step_avg:44.96ms
step:1230/2110 train_time:55310ms step_avg:44.97ms
step:1231/2110 train_time:55371ms step_avg:44.98ms
step:1232/2110 train_time:55429ms step_avg:44.99ms
step:1233/2110 train_time:55490ms step_avg:45.00ms
step:1234/2110 train_time:55549ms step_avg:45.02ms
step:1235/2110 train_time:55609ms step_avg:45.03ms
step:1236/2110 train_time:55668ms step_avg:45.04ms
step:1237/2110 train_time:55728ms step_avg:45.05ms
step:1238/2110 train_time:55788ms step_avg:45.06ms
step:1239/2110 train_time:55848ms step_avg:45.08ms
step:1240/2110 train_time:55907ms step_avg:45.09ms
step:1241/2110 train_time:55968ms step_avg:45.10ms
step:1242/2110 train_time:56026ms step_avg:45.11ms
step:1243/2110 train_time:56087ms step_avg:45.12ms
step:1244/2110 train_time:56146ms step_avg:45.13ms
step:1245/2110 train_time:56206ms step_avg:45.15ms
step:1246/2110 train_time:56265ms step_avg:45.16ms
step:1247/2110 train_time:56325ms step_avg:45.17ms
step:1248/2110 train_time:56384ms step_avg:45.18ms
step:1249/2110 train_time:56445ms step_avg:45.19ms
step:1250/2110 train_time:56505ms step_avg:45.20ms
step:1250/2110 val_loss:3.5949 train_time:56566ms step_avg:45.25ms
step:1251/2110 train_time:56604ms step_avg:45.25ms
step:1252/2110 train_time:56637ms step_avg:45.24ms
step:1253/2110 train_time:56690ms step_avg:45.24ms
step:1254/2110 train_time:56750ms step_avg:45.26ms
step:1255/2110 train_time:56811ms step_avg:45.27ms
step:1256/2110 train_time:56871ms step_avg:45.28ms
step:1257/2110 train_time:56931ms step_avg:45.29ms
step:1258/2110 train_time:56990ms step_avg:45.30ms
step:1259/2110 train_time:57048ms step_avg:45.31ms
step:1260/2110 train_time:57107ms step_avg:45.32ms
step:1261/2110 train_time:57166ms step_avg:45.33ms
step:1262/2110 train_time:57225ms step_avg:45.34ms
step:1263/2110 train_time:57283ms step_avg:45.36ms
step:1264/2110 train_time:57342ms step_avg:45.37ms
step:1265/2110 train_time:57401ms step_avg:45.38ms
step:1266/2110 train_time:57460ms step_avg:45.39ms
step:1267/2110 train_time:57524ms step_avg:45.40ms
step:1268/2110 train_time:57585ms step_avg:45.41ms
step:1269/2110 train_time:57648ms step_avg:45.43ms
step:1270/2110 train_time:57708ms step_avg:45.44ms
step:1271/2110 train_time:57769ms step_avg:45.45ms
step:1272/2110 train_time:57828ms step_avg:45.46ms
step:1273/2110 train_time:57889ms step_avg:45.47ms
step:1274/2110 train_time:57948ms step_avg:45.48ms
step:1275/2110 train_time:58008ms step_avg:45.50ms
step:1276/2110 train_time:58067ms step_avg:45.51ms
step:1277/2110 train_time:58126ms step_avg:45.52ms
step:1278/2110 train_time:58184ms step_avg:45.53ms
step:1279/2110 train_time:58244ms step_avg:45.54ms
step:1280/2110 train_time:58303ms step_avg:45.55ms
step:1281/2110 train_time:58362ms step_avg:45.56ms
step:1282/2110 train_time:58420ms step_avg:45.57ms
step:1283/2110 train_time:58481ms step_avg:45.58ms
step:1284/2110 train_time:58541ms step_avg:45.59ms
step:1285/2110 train_time:58602ms step_avg:45.60ms
step:1286/2110 train_time:58662ms step_avg:45.62ms
step:1287/2110 train_time:58725ms step_avg:45.63ms
step:1288/2110 train_time:58785ms step_avg:45.64ms
step:1289/2110 train_time:58846ms step_avg:45.65ms
step:1290/2110 train_time:58905ms step_avg:45.66ms
step:1291/2110 train_time:58966ms step_avg:45.67ms
step:1292/2110 train_time:59025ms step_avg:45.69ms
step:1293/2110 train_time:59084ms step_avg:45.70ms
step:1294/2110 train_time:59143ms step_avg:45.71ms
step:1295/2110 train_time:59202ms step_avg:45.72ms
step:1296/2110 train_time:59261ms step_avg:45.73ms
step:1297/2110 train_time:59320ms step_avg:45.74ms
step:1298/2110 train_time:59379ms step_avg:45.75ms
step:1299/2110 train_time:59439ms step_avg:45.76ms
step:1300/2110 train_time:59499ms step_avg:45.77ms
step:1301/2110 train_time:59559ms step_avg:45.78ms
step:1302/2110 train_time:59620ms step_avg:45.79ms
step:1303/2110 train_time:59680ms step_avg:45.80ms
step:1304/2110 train_time:59741ms step_avg:45.81ms
step:1305/2110 train_time:59802ms step_avg:45.83ms
step:1306/2110 train_time:59861ms step_avg:45.84ms
step:1307/2110 train_time:59922ms step_avg:45.85ms
step:1308/2110 train_time:59982ms step_avg:45.86ms
step:1309/2110 train_time:60042ms step_avg:45.87ms
step:1310/2110 train_time:60101ms step_avg:45.88ms
step:1311/2110 train_time:60161ms step_avg:45.89ms
step:1312/2110 train_time:60219ms step_avg:45.90ms
step:1313/2110 train_time:60279ms step_avg:45.91ms
step:1314/2110 train_time:60338ms step_avg:45.92ms
step:1315/2110 train_time:60398ms step_avg:45.93ms
step:1316/2110 train_time:60457ms step_avg:45.94ms
step:1317/2110 train_time:60517ms step_avg:45.95ms
step:1318/2110 train_time:60577ms step_avg:45.96ms
step:1319/2110 train_time:60637ms step_avg:45.97ms
step:1320/2110 train_time:60699ms step_avg:45.98ms
step:1321/2110 train_time:60760ms step_avg:46.00ms
step:1322/2110 train_time:60820ms step_avg:46.01ms
step:1323/2110 train_time:60880ms step_avg:46.02ms
step:1324/2110 train_time:60940ms step_avg:46.03ms
step:1325/2110 train_time:61000ms step_avg:46.04ms
step:1326/2110 train_time:61060ms step_avg:46.05ms
step:1327/2110 train_time:61119ms step_avg:46.06ms
step:1328/2110 train_time:61178ms step_avg:46.07ms
step:1329/2110 train_time:61238ms step_avg:46.08ms
step:1330/2110 train_time:61298ms step_avg:46.09ms
step:1331/2110 train_time:61357ms step_avg:46.10ms
step:1332/2110 train_time:61417ms step_avg:46.11ms
step:1333/2110 train_time:61476ms step_avg:46.12ms
step:1334/2110 train_time:61536ms step_avg:46.13ms
step:1335/2110 train_time:61596ms step_avg:46.14ms
step:1336/2110 train_time:61656ms step_avg:46.15ms
step:1337/2110 train_time:61716ms step_avg:46.16ms
step:1338/2110 train_time:61776ms step_avg:46.17ms
step:1339/2110 train_time:61836ms step_avg:46.18ms
step:1340/2110 train_time:61897ms step_avg:46.19ms
step:1341/2110 train_time:61957ms step_avg:46.20ms
step:1342/2110 train_time:62016ms step_avg:46.21ms
step:1343/2110 train_time:62076ms step_avg:46.22ms
step:1344/2110 train_time:62136ms step_avg:46.23ms
step:1345/2110 train_time:62195ms step_avg:46.24ms
step:1346/2110 train_time:62256ms step_avg:46.25ms
step:1347/2110 train_time:62315ms step_avg:46.26ms
step:1348/2110 train_time:62374ms step_avg:46.27ms
step:1349/2110 train_time:62434ms step_avg:46.28ms
step:1350/2110 train_time:62494ms step_avg:46.29ms
step:1351/2110 train_time:62553ms step_avg:46.30ms
step:1352/2110 train_time:62613ms step_avg:46.31ms
step:1353/2110 train_time:62673ms step_avg:46.32ms
step:1354/2110 train_time:62734ms step_avg:46.33ms
step:1355/2110 train_time:62793ms step_avg:46.34ms
step:1356/2110 train_time:62853ms step_avg:46.35ms
step:1357/2110 train_time:62913ms step_avg:46.36ms
step:1358/2110 train_time:62972ms step_avg:46.37ms
step:1359/2110 train_time:63032ms step_avg:46.38ms
step:1360/2110 train_time:63091ms step_avg:46.39ms
step:1361/2110 train_time:63151ms step_avg:46.40ms
step:1362/2110 train_time:63210ms step_avg:46.41ms
step:1363/2110 train_time:63270ms step_avg:46.42ms
step:1364/2110 train_time:63329ms step_avg:46.43ms
step:1365/2110 train_time:63388ms step_avg:46.44ms
step:1366/2110 train_time:63446ms step_avg:46.45ms
step:1367/2110 train_time:63508ms step_avg:46.46ms
step:1368/2110 train_time:63566ms step_avg:46.47ms
step:1369/2110 train_time:63627ms step_avg:46.48ms
step:1370/2110 train_time:63686ms step_avg:46.49ms
step:1371/2110 train_time:63747ms step_avg:46.50ms
step:1372/2110 train_time:63808ms step_avg:46.51ms
step:1373/2110 train_time:63866ms step_avg:46.52ms
step:1374/2110 train_time:63926ms step_avg:46.53ms
step:1375/2110 train_time:63988ms step_avg:46.54ms
step:1376/2110 train_time:64048ms step_avg:46.55ms
step:1377/2110 train_time:64107ms step_avg:46.56ms
step:1378/2110 train_time:64166ms step_avg:46.56ms
step:1379/2110 train_time:64226ms step_avg:46.57ms
step:1380/2110 train_time:64286ms step_avg:46.58ms
step:1381/2110 train_time:64346ms step_avg:46.59ms
step:1382/2110 train_time:64432ms step_avg:46.62ms
step:1383/2110 train_time:64519ms step_avg:46.65ms
step:1384/2110 train_time:64606ms step_avg:46.68ms
step:1385/2110 train_time:64692ms step_avg:46.71ms
step:1386/2110 train_time:64779ms step_avg:46.74ms
step:1387/2110 train_time:64867ms step_avg:46.77ms
step:1388/2110 train_time:64954ms step_avg:46.80ms
step:1389/2110 train_time:65039ms step_avg:46.82ms
step:1390/2110 train_time:65126ms step_avg:46.85ms
step:1391/2110 train_time:65213ms step_avg:46.88ms
step:1392/2110 train_time:65300ms step_avg:46.91ms
step:1393/2110 train_time:65386ms step_avg:46.94ms
step:1394/2110 train_time:65473ms step_avg:46.97ms
step:1395/2110 train_time:65560ms step_avg:47.00ms
step:1396/2110 train_time:65646ms step_avg:47.02ms
step:1397/2110 train_time:65732ms step_avg:47.05ms
step:1398/2110 train_time:65821ms step_avg:47.08ms
step:1399/2110 train_time:65906ms step_avg:47.11ms
step:1400/2110 train_time:65993ms step_avg:47.14ms
step:1401/2110 train_time:66081ms step_avg:47.17ms
step:1402/2110 train_time:66168ms step_avg:47.20ms
step:1403/2110 train_time:66253ms step_avg:47.22ms
step:1404/2110 train_time:66339ms step_avg:47.25ms
step:1405/2110 train_time:66427ms step_avg:47.28ms
step:1406/2110 train_time:66514ms step_avg:47.31ms
step:1407/2110 train_time:66599ms step_avg:47.33ms
step:1408/2110 train_time:66686ms step_avg:47.36ms
step:1409/2110 train_time:66776ms step_avg:47.39ms
step:1410/2110 train_time:66859ms step_avg:47.42ms
step:1411/2110 train_time:66945ms step_avg:47.45ms
step:1412/2110 train_time:67033ms step_avg:47.47ms
step:1413/2110 train_time:67121ms step_avg:47.50ms
step:1414/2110 train_time:67206ms step_avg:47.53ms
step:1415/2110 train_time:67293ms step_avg:47.56ms
step:1416/2110 train_time:67380ms step_avg:47.58ms
step:1417/2110 train_time:67466ms step_avg:47.61ms
step:1418/2110 train_time:67553ms step_avg:47.64ms
step:1419/2110 train_time:67640ms step_avg:47.67ms
step:1420/2110 train_time:67726ms step_avg:47.69ms
step:1421/2110 train_time:67813ms step_avg:47.72ms
step:1422/2110 train_time:67898ms step_avg:47.75ms
step:1423/2110 train_time:67987ms step_avg:47.78ms
step:1424/2110 train_time:68072ms step_avg:47.80ms
step:1425/2110 train_time:68158ms step_avg:47.83ms
step:1426/2110 train_time:68246ms step_avg:47.86ms
step:1427/2110 train_time:68331ms step_avg:47.88ms
step:1428/2110 train_time:68418ms step_avg:47.91ms
step:1429/2110 train_time:68505ms step_avg:47.94ms
step:1430/2110 train_time:68593ms step_avg:47.97ms
step:1431/2110 train_time:68678ms step_avg:47.99ms
step:1432/2110 train_time:68766ms step_avg:48.02ms
step:1433/2110 train_time:68853ms step_avg:48.05ms
step:1434/2110 train_time:68938ms step_avg:48.07ms
step:1435/2110 train_time:69025ms step_avg:48.10ms
step:1436/2110 train_time:69111ms step_avg:48.13ms
step:1437/2110 train_time:69199ms step_avg:48.16ms
step:1438/2110 train_time:69285ms step_avg:48.18ms
step:1439/2110 train_time:69372ms step_avg:48.21ms
step:1440/2110 train_time:69458ms step_avg:48.23ms
step:1441/2110 train_time:69545ms step_avg:48.26ms
step:1442/2110 train_time:69632ms step_avg:48.29ms
step:1443/2110 train_time:69720ms step_avg:48.32ms
step:1444/2110 train_time:69806ms step_avg:48.34ms
step:1445/2110 train_time:69893ms step_avg:48.37ms
step:1446/2110 train_time:69979ms step_avg:48.40ms
step:1447/2110 train_time:70067ms step_avg:48.42ms
step:1448/2110 train_time:70153ms step_avg:48.45ms
step:1449/2110 train_time:70241ms step_avg:48.48ms
step:1450/2110 train_time:70326ms step_avg:48.50ms
step:1451/2110 train_time:70414ms step_avg:48.53ms
step:1452/2110 train_time:70501ms step_avg:48.55ms
step:1453/2110 train_time:70588ms step_avg:48.58ms
step:1454/2110 train_time:70674ms step_avg:48.61ms
step:1455/2110 train_time:70760ms step_avg:48.63ms
step:1456/2110 train_time:70848ms step_avg:48.66ms
step:1457/2110 train_time:70934ms step_avg:48.68ms
step:1458/2110 train_time:71020ms step_avg:48.71ms
step:1459/2110 train_time:71107ms step_avg:48.74ms
step:1460/2110 train_time:71194ms step_avg:48.76ms
step:1461/2110 train_time:71280ms step_avg:48.79ms
step:1462/2110 train_time:71368ms step_avg:48.82ms
step:1463/2110 train_time:71454ms step_avg:48.84ms
step:1464/2110 train_time:71541ms step_avg:48.87ms
step:1465/2110 train_time:71627ms step_avg:48.89ms
step:1466/2110 train_time:71713ms step_avg:48.92ms
step:1467/2110 train_time:71801ms step_avg:48.94ms
step:1468/2110 train_time:71888ms step_avg:48.97ms
step:1469/2110 train_time:71974ms step_avg:49.00ms
step:1470/2110 train_time:72063ms step_avg:49.02ms
step:1471/2110 train_time:72148ms step_avg:49.05ms
step:1472/2110 train_time:72235ms step_avg:49.07ms
step:1473/2110 train_time:72320ms step_avg:49.10ms
step:1474/2110 train_time:72405ms step_avg:49.12ms
step:1475/2110 train_time:72493ms step_avg:49.15ms
step:1476/2110 train_time:72580ms step_avg:49.17ms
step:1477/2110 train_time:72667ms step_avg:49.20ms
step:1478/2110 train_time:72752ms step_avg:49.22ms
step:1479/2110 train_time:72840ms step_avg:49.25ms
step:1480/2110 train_time:72928ms step_avg:49.28ms
step:1481/2110 train_time:73014ms step_avg:49.30ms
step:1482/2110 train_time:73101ms step_avg:49.33ms
step:1483/2110 train_time:73187ms step_avg:49.35ms
step:1484/2110 train_time:73274ms step_avg:49.38ms
step:1485/2110 train_time:73360ms step_avg:49.40ms
step:1486/2110 train_time:73445ms step_avg:49.42ms
step:1487/2110 train_time:73534ms step_avg:49.45ms
step:1488/2110 train_time:73621ms step_avg:49.48ms
step:1489/2110 train_time:73707ms step_avg:49.50ms
step:1490/2110 train_time:73794ms step_avg:49.53ms
step:1491/2110 train_time:73882ms step_avg:49.55ms
step:1492/2110 train_time:73968ms step_avg:49.58ms
step:1493/2110 train_time:74055ms step_avg:49.60ms
step:1494/2110 train_time:74142ms step_avg:49.63ms
step:1495/2110 train_time:74228ms step_avg:49.65ms
step:1496/2110 train_time:74315ms step_avg:49.68ms
step:1497/2110 train_time:74402ms step_avg:49.70ms
step:1498/2110 train_time:74487ms step_avg:49.72ms
step:1499/2110 train_time:74575ms step_avg:49.75ms
step:1500/2110 train_time:74660ms step_avg:49.77ms
step:1500/2110 val_loss:3.4976 train_time:74748ms step_avg:49.83ms
step:1501/2110 train_time:74783ms step_avg:49.82ms
step:1502/2110 train_time:74837ms step_avg:49.82ms
step:1503/2110 train_time:74927ms step_avg:49.85ms
step:1504/2110 train_time:75015ms step_avg:49.88ms
step:1505/2110 train_time:75102ms step_avg:49.90ms
step:1506/2110 train_time:75188ms step_avg:49.93ms
step:1507/2110 train_time:75274ms step_avg:49.95ms
step:1508/2110 train_time:75359ms step_avg:49.97ms
step:1509/2110 train_time:75445ms step_avg:50.00ms
step:1510/2110 train_time:75531ms step_avg:50.02ms
step:1511/2110 train_time:75617ms step_avg:50.04ms
step:1512/2110 train_time:75704ms step_avg:50.07ms
step:1513/2110 train_time:75794ms step_avg:50.09ms
step:1514/2110 train_time:75882ms step_avg:50.12ms
step:1515/2110 train_time:75971ms step_avg:50.15ms
step:1516/2110 train_time:76057ms step_avg:50.17ms
step:1517/2110 train_time:76145ms step_avg:50.19ms
step:1518/2110 train_time:76230ms step_avg:50.22ms
step:1519/2110 train_time:76317ms step_avg:50.24ms
step:1520/2110 train_time:76401ms step_avg:50.26ms
step:1521/2110 train_time:76488ms step_avg:50.29ms
step:1522/2110 train_time:76576ms step_avg:50.31ms
step:1523/2110 train_time:76661ms step_avg:50.34ms
step:1524/2110 train_time:76749ms step_avg:50.36ms
step:1525/2110 train_time:76836ms step_avg:50.38ms
step:1526/2110 train_time:76923ms step_avg:50.41ms
step:1527/2110 train_time:77011ms step_avg:50.43ms
step:1528/2110 train_time:77097ms step_avg:50.46ms
step:1529/2110 train_time:77185ms step_avg:50.48ms
step:1530/2110 train_time:77272ms step_avg:50.50ms
step:1531/2110 train_time:77358ms step_avg:50.53ms
step:1532/2110 train_time:77445ms step_avg:50.55ms
step:1533/2110 train_time:77531ms step_avg:50.57ms
step:1534/2110 train_time:77617ms step_avg:50.60ms
step:1535/2110 train_time:77706ms step_avg:50.62ms
step:1536/2110 train_time:77793ms step_avg:50.65ms
step:1537/2110 train_time:77879ms step_avg:50.67ms
step:1538/2110 train_time:77968ms step_avg:50.69ms
step:1539/2110 train_time:78053ms step_avg:50.72ms
step:1540/2110 train_time:78139ms step_avg:50.74ms
step:1541/2110 train_time:78227ms step_avg:50.76ms
step:1542/2110 train_time:78313ms step_avg:50.79ms
step:1543/2110 train_time:78399ms step_avg:50.81ms
step:1544/2110 train_time:78487ms step_avg:50.83ms
step:1545/2110 train_time:78571ms step_avg:50.86ms
step:1546/2110 train_time:78658ms step_avg:50.88ms
step:1547/2110 train_time:78745ms step_avg:50.90ms
step:1548/2110 train_time:78830ms step_avg:50.92ms
step:1549/2110 train_time:78919ms step_avg:50.95ms
step:1550/2110 train_time:79007ms step_avg:50.97ms
step:1551/2110 train_time:79092ms step_avg:50.99ms
step:1552/2110 train_time:79179ms step_avg:51.02ms
step:1553/2110 train_time:79266ms step_avg:51.04ms
step:1554/2110 train_time:79352ms step_avg:51.06ms
step:1555/2110 train_time:79438ms step_avg:51.09ms
step:1556/2110 train_time:79525ms step_avg:51.11ms
step:1557/2110 train_time:79610ms step_avg:51.13ms
step:1558/2110 train_time:79697ms step_avg:51.15ms
step:1559/2110 train_time:79783ms step_avg:51.18ms
step:1560/2110 train_time:79871ms step_avg:51.20ms
step:1561/2110 train_time:79957ms step_avg:51.22ms
step:1562/2110 train_time:80043ms step_avg:51.24ms
step:1563/2110 train_time:80133ms step_avg:51.27ms
step:1564/2110 train_time:80217ms step_avg:51.29ms
step:1565/2110 train_time:80305ms step_avg:51.31ms
step:1566/2110 train_time:80394ms step_avg:51.34ms
step:1567/2110 train_time:80478ms step_avg:51.36ms
step:1568/2110 train_time:80565ms step_avg:51.38ms
step:1569/2110 train_time:80653ms step_avg:51.40ms
step:1570/2110 train_time:80738ms step_avg:51.43ms
step:1571/2110 train_time:80825ms step_avg:51.45ms
step:1572/2110 train_time:80914ms step_avg:51.47ms
step:1573/2110 train_time:81000ms step_avg:51.49ms
step:1574/2110 train_time:81087ms step_avg:51.52ms
step:1575/2110 train_time:81173ms step_avg:51.54ms
step:1576/2110 train_time:81259ms step_avg:51.56ms
step:1577/2110 train_time:81347ms step_avg:51.58ms
step:1578/2110 train_time:81433ms step_avg:51.61ms
step:1579/2110 train_time:81520ms step_avg:51.63ms
step:1580/2110 train_time:81606ms step_avg:51.65ms
step:1581/2110 train_time:81692ms step_avg:51.67ms
step:1582/2110 train_time:81781ms step_avg:51.69ms
step:1583/2110 train_time:81867ms step_avg:51.72ms
step:1584/2110 train_time:81953ms step_avg:51.74ms
step:1585/2110 train_time:82041ms step_avg:51.76ms
step:1586/2110 train_time:82127ms step_avg:51.78ms
step:1587/2110 train_time:82214ms step_avg:51.80ms
step:1588/2110 train_time:82301ms step_avg:51.83ms
step:1589/2110 train_time:82386ms step_avg:51.85ms
step:1590/2110 train_time:82474ms step_avg:51.87ms
step:1591/2110 train_time:82560ms step_avg:51.89ms
step:1592/2110 train_time:82649ms step_avg:51.92ms
step:1593/2110 train_time:82733ms step_avg:51.94ms
step:1594/2110 train_time:82820ms step_avg:51.96ms
step:1595/2110 train_time:82909ms step_avg:51.98ms
step:1596/2110 train_time:82994ms step_avg:52.00ms
step:1597/2110 train_time:83080ms step_avg:52.02ms
step:1598/2110 train_time:83168ms step_avg:52.04ms
step:1599/2110 train_time:83254ms step_avg:52.07ms
step:1600/2110 train_time:83341ms step_avg:52.09ms
step:1601/2110 train_time:83428ms step_avg:52.11ms
step:1602/2110 train_time:83517ms step_avg:52.13ms
step:1603/2110 train_time:83602ms step_avg:52.15ms
step:1604/2110 train_time:83690ms step_avg:52.18ms
step:1605/2110 train_time:83778ms step_avg:52.20ms
step:1606/2110 train_time:83863ms step_avg:52.22ms
step:1607/2110 train_time:83950ms step_avg:52.24ms
step:1608/2110 train_time:84038ms step_avg:52.26ms
step:1609/2110 train_time:84124ms step_avg:52.28ms
step:1610/2110 train_time:84211ms step_avg:52.30ms
step:1611/2110 train_time:84298ms step_avg:52.33ms
step:1612/2110 train_time:84385ms step_avg:52.35ms
step:1613/2110 train_time:84471ms step_avg:52.37ms
step:1614/2110 train_time:84558ms step_avg:52.39ms
step:1615/2110 train_time:84644ms step_avg:52.41ms
step:1616/2110 train_time:84732ms step_avg:52.43ms
step:1617/2110 train_time:84819ms step_avg:52.45ms
step:1618/2110 train_time:84907ms step_avg:52.48ms
step:1619/2110 train_time:84992ms step_avg:52.50ms
step:1620/2110 train_time:85079ms step_avg:52.52ms
step:1621/2110 train_time:85166ms step_avg:52.54ms
step:1622/2110 train_time:85251ms step_avg:52.56ms
step:1623/2110 train_time:85338ms step_avg:52.58ms
step:1624/2110 train_time:85425ms step_avg:52.60ms
step:1625/2110 train_time:85512ms step_avg:52.62ms
step:1626/2110 train_time:85599ms step_avg:52.64ms
step:1627/2110 train_time:85686ms step_avg:52.66ms
step:1628/2110 train_time:85772ms step_avg:52.69ms
step:1629/2110 train_time:85859ms step_avg:52.71ms
step:1630/2110 train_time:85946ms step_avg:52.73ms
step:1631/2110 train_time:86034ms step_avg:52.75ms
step:1632/2110 train_time:86120ms step_avg:52.77ms
step:1633/2110 train_time:86207ms step_avg:52.79ms
step:1634/2110 train_time:86293ms step_avg:52.81ms
step:1635/2110 train_time:86381ms step_avg:52.83ms
step:1636/2110 train_time:86467ms step_avg:52.85ms
step:1637/2110 train_time:86553ms step_avg:52.87ms
step:1638/2110 train_time:86641ms step_avg:52.89ms
step:1639/2110 train_time:86727ms step_avg:52.91ms
step:1640/2110 train_time:86813ms step_avg:52.93ms
step:1641/2110 train_time:86900ms step_avg:52.96ms
step:1642/2110 train_time:86987ms step_avg:52.98ms
step:1643/2110 train_time:87074ms step_avg:53.00ms
step:1644/2110 train_time:87160ms step_avg:53.02ms
step:1645/2110 train_time:87249ms step_avg:53.04ms
step:1646/2110 train_time:87336ms step_avg:53.06ms
step:1647/2110 train_time:87422ms step_avg:53.08ms
step:1648/2110 train_time:87510ms step_avg:53.10ms
step:1649/2110 train_time:87595ms step_avg:53.12ms
step:1650/2110 train_time:87682ms step_avg:53.14ms
step:1651/2110 train_time:87770ms step_avg:53.16ms
step:1652/2110 train_time:87856ms step_avg:53.18ms
step:1653/2110 train_time:87943ms step_avg:53.20ms
step:1654/2110 train_time:88030ms step_avg:53.22ms
step:1655/2110 train_time:88116ms step_avg:53.24ms
step:1656/2110 train_time:88202ms step_avg:53.26ms
step:1657/2110 train_time:88291ms step_avg:53.28ms
step:1658/2110 train_time:88379ms step_avg:53.30ms
step:1659/2110 train_time:88466ms step_avg:53.32ms
step:1660/2110 train_time:88554ms step_avg:53.35ms
step:1661/2110 train_time:88643ms step_avg:53.37ms
step:1662/2110 train_time:88729ms step_avg:53.39ms
step:1663/2110 train_time:88818ms step_avg:53.41ms
step:1664/2110 train_time:88906ms step_avg:53.43ms
step:1665/2110 train_time:88994ms step_avg:53.45ms
step:1666/2110 train_time:89082ms step_avg:53.47ms
step:1667/2110 train_time:89172ms step_avg:53.49ms
step:1668/2110 train_time:89259ms step_avg:53.51ms
step:1669/2110 train_time:89346ms step_avg:53.53ms
step:1670/2110 train_time:89434ms step_avg:53.55ms
step:1671/2110 train_time:89522ms step_avg:53.57ms
step:1672/2110 train_time:89609ms step_avg:53.59ms
step:1673/2110 train_time:89698ms step_avg:53.61ms
step:1674/2110 train_time:89784ms step_avg:53.63ms
step:1675/2110 train_time:89873ms step_avg:53.66ms
step:1676/2110 train_time:89961ms step_avg:53.68ms
step:1677/2110 train_time:90049ms step_avg:53.70ms
step:1678/2110 train_time:90137ms step_avg:53.72ms
step:1679/2110 train_time:90227ms step_avg:53.74ms
step:1680/2110 train_time:90314ms step_avg:53.76ms
step:1681/2110 train_time:90402ms step_avg:53.78ms
step:1682/2110 train_time:90490ms step_avg:53.80ms
step:1683/2110 train_time:90579ms step_avg:53.82ms
step:1684/2110 train_time:90665ms step_avg:53.84ms
step:1685/2110 train_time:90753ms step_avg:53.86ms
step:1686/2110 train_time:90841ms step_avg:53.88ms
step:1687/2110 train_time:90929ms step_avg:53.90ms
step:1688/2110 train_time:91017ms step_avg:53.92ms
step:1689/2110 train_time:91107ms step_avg:53.94ms
step:1690/2110 train_time:91193ms step_avg:53.96ms
step:1691/2110 train_time:91281ms step_avg:53.98ms
step:1692/2110 train_time:91372ms step_avg:54.00ms
step:1693/2110 train_time:91458ms step_avg:54.02ms
step:1694/2110 train_time:91546ms step_avg:54.04ms
step:1695/2110 train_time:91634ms step_avg:54.06ms
step:1696/2110 train_time:91721ms step_avg:54.08ms
step:1697/2110 train_time:91808ms step_avg:54.10ms
step:1698/2110 train_time:91897ms step_avg:54.12ms
step:1699/2110 train_time:91985ms step_avg:54.14ms
step:1700/2110 train_time:92072ms step_avg:54.16ms
step:1701/2110 train_time:92160ms step_avg:54.18ms
step:1702/2110 train_time:92248ms step_avg:54.20ms
step:1703/2110 train_time:92336ms step_avg:54.22ms
step:1704/2110 train_time:92423ms step_avg:54.24ms
step:1705/2110 train_time:92513ms step_avg:54.26ms
step:1706/2110 train_time:92601ms step_avg:54.28ms
step:1707/2110 train_time:92687ms step_avg:54.30ms
step:1708/2110 train_time:92777ms step_avg:54.32ms
step:1709/2110 train_time:92863ms step_avg:54.34ms
step:1710/2110 train_time:92951ms step_avg:54.36ms
step:1711/2110 train_time:93040ms step_avg:54.38ms
step:1712/2110 train_time:93127ms step_avg:54.40ms
step:1713/2110 train_time:93215ms step_avg:54.42ms
step:1714/2110 train_time:93303ms step_avg:54.44ms
step:1715/2110 train_time:93391ms step_avg:54.46ms
step:1716/2110 train_time:93479ms step_avg:54.47ms
step:1717/2110 train_time:93566ms step_avg:54.49ms
step:1718/2110 train_time:93656ms step_avg:54.51ms
step:1719/2110 train_time:93742ms step_avg:54.53ms
step:1720/2110 train_time:93830ms step_avg:54.55ms
step:1721/2110 train_time:93919ms step_avg:54.57ms
step:1722/2110 train_time:94005ms step_avg:54.59ms
step:1723/2110 train_time:94094ms step_avg:54.61ms
step:1724/2110 train_time:94182ms step_avg:54.63ms
step:1725/2110 train_time:94270ms step_avg:54.65ms
step:1726/2110 train_time:94358ms step_avg:54.67ms
step:1727/2110 train_time:94448ms step_avg:54.69ms
step:1728/2110 train_time:94535ms step_avg:54.71ms
step:1729/2110 train_time:94623ms step_avg:54.73ms
step:1730/2110 train_time:94712ms step_avg:54.75ms
step:1731/2110 train_time:94797ms step_avg:54.76ms
step:1732/2110 train_time:94885ms step_avg:54.78ms
step:1733/2110 train_time:94974ms step_avg:54.80ms
step:1734/2110 train_time:95061ms step_avg:54.82ms
step:1735/2110 train_time:95148ms step_avg:54.84ms
step:1736/2110 train_time:95237ms step_avg:54.86ms
step:1737/2110 train_time:95324ms step_avg:54.88ms
step:1738/2110 train_time:95412ms step_avg:54.90ms
step:1739/2110 train_time:95500ms step_avg:54.92ms
step:1740/2110 train_time:95589ms step_avg:54.94ms
step:1741/2110 train_time:95677ms step_avg:54.96ms
step:1742/2110 train_time:95764ms step_avg:54.97ms
step:1743/2110 train_time:95853ms step_avg:54.99ms
step:1744/2110 train_time:95940ms step_avg:55.01ms
step:1745/2110 train_time:96028ms step_avg:55.03ms
step:1746/2110 train_time:96118ms step_avg:55.05ms
step:1747/2110 train_time:96206ms step_avg:55.07ms
step:1748/2110 train_time:96293ms step_avg:55.09ms
step:1749/2110 train_time:96382ms step_avg:55.11ms
step:1750/2110 train_time:96469ms step_avg:55.13ms
step:1750/2110 val_loss:3.3813 train_time:96558ms step_avg:55.18ms
step:1751/2110 train_time:96590ms step_avg:55.16ms
step:1752/2110 train_time:96651ms step_avg:55.17ms
step:1753/2110 train_time:96745ms step_avg:55.19ms
step:1754/2110 train_time:96834ms step_avg:55.21ms
step:1755/2110 train_time:96924ms step_avg:55.23ms
step:1756/2110 train_time:97009ms step_avg:55.24ms
step:1757/2110 train_time:97097ms step_avg:55.26ms
step:1758/2110 train_time:97184ms step_avg:55.28ms
step:1759/2110 train_time:97270ms step_avg:55.30ms
step:1760/2110 train_time:97356ms step_avg:55.32ms
step:1761/2110 train_time:97445ms step_avg:55.34ms
step:1762/2110 train_time:97534ms step_avg:55.35ms
step:1763/2110 train_time:97624ms step_avg:55.37ms
step:1764/2110 train_time:97716ms step_avg:55.39ms
step:1765/2110 train_time:97804ms step_avg:55.41ms
step:1766/2110 train_time:97892ms step_avg:55.43ms
step:1767/2110 train_time:97980ms step_avg:55.45ms
step:1768/2110 train_time:98065ms step_avg:55.47ms
step:1769/2110 train_time:98152ms step_avg:55.48ms
step:1770/2110 train_time:98239ms step_avg:55.50ms
step:1771/2110 train_time:98327ms step_avg:55.52ms
step:1772/2110 train_time:98413ms step_avg:55.54ms
step:1773/2110 train_time:98502ms step_avg:55.56ms
step:1774/2110 train_time:98592ms step_avg:55.58ms
step:1775/2110 train_time:98681ms step_avg:55.59ms
step:1776/2110 train_time:98770ms step_avg:55.61ms
step:1777/2110 train_time:98858ms step_avg:55.63ms
step:1778/2110 train_time:98946ms step_avg:55.65ms
step:1779/2110 train_time:99033ms step_avg:55.67ms
step:1780/2110 train_time:99120ms step_avg:55.69ms
step:1781/2110 train_time:99207ms step_avg:55.70ms
step:1782/2110 train_time:99293ms step_avg:55.72ms
step:1783/2110 train_time:99382ms step_avg:55.74ms
step:1784/2110 train_time:99469ms step_avg:55.76ms
step:1785/2110 train_time:99557ms step_avg:55.77ms
step:1786/2110 train_time:99646ms step_avg:55.79ms
step:1787/2110 train_time:99736ms step_avg:55.81ms
step:1788/2110 train_time:99825ms step_avg:55.83ms
step:1789/2110 train_time:99913ms step_avg:55.85ms
step:1790/2110 train_time:100001ms step_avg:55.87ms
step:1791/2110 train_time:100088ms step_avg:55.88ms
step:1792/2110 train_time:100174ms step_avg:55.90ms
step:1793/2110 train_time:100261ms step_avg:55.92ms
step:1794/2110 train_time:100350ms step_avg:55.94ms
step:1795/2110 train_time:100437ms step_avg:55.95ms
step:1796/2110 train_time:100525ms step_avg:55.97ms
step:1797/2110 train_time:100615ms step_avg:55.99ms
step:1798/2110 train_time:100702ms step_avg:56.01ms
step:1799/2110 train_time:100792ms step_avg:56.03ms
step:1800/2110 train_time:100883ms step_avg:56.05ms
step:1801/2110 train_time:100970ms step_avg:56.06ms
step:1802/2110 train_time:101057ms step_avg:56.08ms
step:1803/2110 train_time:101145ms step_avg:56.10ms
step:1804/2110 train_time:101232ms step_avg:56.12ms
step:1805/2110 train_time:101318ms step_avg:56.13ms
step:1806/2110 train_time:101406ms step_avg:56.15ms
step:1807/2110 train_time:101494ms step_avg:56.17ms
step:1808/2110 train_time:101583ms step_avg:56.19ms
step:1809/2110 train_time:101671ms step_avg:56.20ms
step:1810/2110 train_time:101761ms step_avg:56.22ms
step:1811/2110 train_time:101848ms step_avg:56.24ms
step:1812/2110 train_time:101936ms step_avg:56.26ms
step:1813/2110 train_time:102025ms step_avg:56.27ms
step:1814/2110 train_time:102113ms step_avg:56.29ms
step:1815/2110 train_time:102200ms step_avg:56.31ms
step:1816/2110 train_time:102288ms step_avg:56.33ms
step:1817/2110 train_time:102374ms step_avg:56.34ms
step:1818/2110 train_time:102461ms step_avg:56.36ms
step:1819/2110 train_time:102551ms step_avg:56.38ms
step:1820/2110 train_time:102637ms step_avg:56.39ms
step:1821/2110 train_time:102727ms step_avg:56.41ms
step:1822/2110 train_time:102816ms step_avg:56.43ms
step:1823/2110 train_time:102904ms step_avg:56.45ms
step:1824/2110 train_time:102992ms step_avg:56.46ms
step:1825/2110 train_time:103078ms step_avg:56.48ms
step:1826/2110 train_time:103166ms step_avg:56.50ms
step:1827/2110 train_time:103254ms step_avg:56.52ms
step:1828/2110 train_time:103342ms step_avg:56.53ms
step:1829/2110 train_time:103430ms step_avg:56.55ms
step:1830/2110 train_time:103517ms step_avg:56.57ms
step:1831/2110 train_time:103606ms step_avg:56.58ms
step:1832/2110 train_time:103694ms step_avg:56.60ms
step:1833/2110 train_time:103782ms step_avg:56.62ms
step:1834/2110 train_time:103871ms step_avg:56.64ms
step:1835/2110 train_time:103959ms step_avg:56.65ms
step:1836/2110 train_time:104047ms step_avg:56.67ms
step:1837/2110 train_time:104135ms step_avg:56.69ms
step:1838/2110 train_time:104222ms step_avg:56.70ms
step:1839/2110 train_time:104310ms step_avg:56.72ms
step:1840/2110 train_time:104397ms step_avg:56.74ms
step:1841/2110 train_time:104486ms step_avg:56.75ms
step:1842/2110 train_time:104573ms step_avg:56.77ms
step:1843/2110 train_time:104663ms step_avg:56.79ms
step:1844/2110 train_time:104751ms step_avg:56.81ms
step:1845/2110 train_time:104839ms step_avg:56.82ms
step:1846/2110 train_time:104927ms step_avg:56.84ms
step:1847/2110 train_time:105015ms step_avg:56.86ms
step:1848/2110 train_time:105104ms step_avg:56.87ms
step:1849/2110 train_time:105190ms step_avg:56.89ms
step:1850/2110 train_time:105278ms step_avg:56.91ms
step:1851/2110 train_time:105366ms step_avg:56.92ms
step:1852/2110 train_time:105453ms step_avg:56.94ms
step:1853/2110 train_time:105542ms step_avg:56.96ms
step:1854/2110 train_time:105629ms step_avg:56.97ms
step:1855/2110 train_time:105718ms step_avg:56.99ms
step:1856/2110 train_time:105807ms step_avg:57.01ms
step:1857/2110 train_time:105894ms step_avg:57.02ms
step:1858/2110 train_time:105983ms step_avg:57.04ms
step:1859/2110 train_time:106070ms step_avg:57.06ms
step:1860/2110 train_time:106157ms step_avg:57.07ms
step:1861/2110 train_time:106245ms step_avg:57.09ms
step:1862/2110 train_time:106333ms step_avg:57.11ms
step:1863/2110 train_time:106421ms step_avg:57.12ms
step:1864/2110 train_time:106511ms step_avg:57.14ms
step:1865/2110 train_time:106598ms step_avg:57.16ms
step:1866/2110 train_time:106687ms step_avg:57.17ms
step:1867/2110 train_time:106774ms step_avg:57.19ms
step:1868/2110 train_time:106862ms step_avg:57.21ms
step:1869/2110 train_time:106949ms step_avg:57.22ms
step:1870/2110 train_time:107037ms step_avg:57.24ms
step:1871/2110 train_time:107125ms step_avg:57.26ms
step:1872/2110 train_time:107212ms step_avg:57.27ms
step:1873/2110 train_time:107300ms step_avg:57.29ms
step:1874/2110 train_time:107388ms step_avg:57.30ms
step:1875/2110 train_time:107477ms step_avg:57.32ms
step:1876/2110 train_time:107564ms step_avg:57.34ms
step:1877/2110 train_time:107655ms step_avg:57.35ms
step:1878/2110 train_time:107743ms step_avg:57.37ms
step:1879/2110 train_time:107829ms step_avg:57.39ms
step:1880/2110 train_time:107916ms step_avg:57.40ms
step:1881/2110 train_time:108006ms step_avg:57.42ms
step:1882/2110 train_time:108093ms step_avg:57.43ms
step:1883/2110 train_time:108181ms step_avg:57.45ms
step:1884/2110 train_time:108270ms step_avg:57.47ms
step:1885/2110 train_time:108357ms step_avg:57.48ms
step:1886/2110 train_time:108445ms step_avg:57.50ms
step:1887/2110 train_time:108533ms step_avg:57.52ms
step:1888/2110 train_time:108622ms step_avg:57.53ms
step:1889/2110 train_time:108710ms step_avg:57.55ms
step:1890/2110 train_time:108799ms step_avg:57.57ms
step:1891/2110 train_time:108886ms step_avg:57.58ms
step:1892/2110 train_time:108974ms step_avg:57.60ms
step:1893/2110 train_time:109062ms step_avg:57.61ms
step:1894/2110 train_time:109150ms step_avg:57.63ms
step:1895/2110 train_time:109239ms step_avg:57.65ms
step:1896/2110 train_time:109326ms step_avg:57.66ms
step:1897/2110 train_time:109416ms step_avg:57.68ms
step:1898/2110 train_time:109504ms step_avg:57.69ms
step:1899/2110 train_time:109592ms step_avg:57.71ms
step:1900/2110 train_time:109680ms step_avg:57.73ms
step:1901/2110 train_time:109768ms step_avg:57.74ms
step:1902/2110 train_time:109855ms step_avg:57.76ms
step:1903/2110 train_time:109944ms step_avg:57.77ms
step:1904/2110 train_time:110032ms step_avg:57.79ms
step:1905/2110 train_time:110118ms step_avg:57.80ms
step:1906/2110 train_time:110207ms step_avg:57.82ms
step:1907/2110 train_time:110295ms step_avg:57.84ms
step:1908/2110 train_time:110383ms step_avg:57.85ms
step:1909/2110 train_time:110471ms step_avg:57.87ms
step:1910/2110 train_time:110559ms step_avg:57.88ms
step:1911/2110 train_time:110647ms step_avg:57.90ms
step:1912/2110 train_time:110735ms step_avg:57.92ms
step:1913/2110 train_time:110822ms step_avg:57.93ms
step:1914/2110 train_time:110911ms step_avg:57.95ms
step:1915/2110 train_time:110999ms step_avg:57.96ms
step:1916/2110 train_time:111088ms step_avg:57.98ms
step:1917/2110 train_time:111175ms step_avg:57.99ms
step:1918/2110 train_time:111262ms step_avg:58.01ms
step:1919/2110 train_time:111349ms step_avg:58.02ms
step:1920/2110 train_time:111437ms step_avg:58.04ms
step:1921/2110 train_time:111529ms step_avg:58.06ms
step:1922/2110 train_time:111616ms step_avg:58.07ms
step:1923/2110 train_time:111705ms step_avg:58.09ms
step:1924/2110 train_time:111792ms step_avg:58.10ms
step:1925/2110 train_time:111882ms step_avg:58.12ms
step:1926/2110 train_time:111968ms step_avg:58.14ms
step:1927/2110 train_time:112056ms step_avg:58.15ms
step:1928/2110 train_time:112145ms step_avg:58.17ms
step:1929/2110 train_time:112232ms step_avg:58.18ms
step:1930/2110 train_time:112320ms step_avg:58.20ms
step:1931/2110 train_time:112407ms step_avg:58.21ms
step:1932/2110 train_time:112497ms step_avg:58.23ms
step:1933/2110 train_time:112583ms step_avg:58.24ms
step:1934/2110 train_time:112672ms step_avg:58.26ms
step:1935/2110 train_time:112761ms step_avg:58.27ms
step:1936/2110 train_time:112849ms step_avg:58.29ms
step:1937/2110 train_time:112937ms step_avg:58.31ms
step:1938/2110 train_time:113025ms step_avg:58.32ms
step:1939/2110 train_time:113112ms step_avg:58.34ms
step:1940/2110 train_time:113200ms step_avg:58.35ms
step:1941/2110 train_time:113287ms step_avg:58.37ms
step:1942/2110 train_time:113375ms step_avg:58.38ms
step:1943/2110 train_time:113463ms step_avg:58.40ms
step:1944/2110 train_time:113552ms step_avg:58.41ms
step:1945/2110 train_time:113640ms step_avg:58.43ms
step:1946/2110 train_time:113727ms step_avg:58.44ms
step:1947/2110 train_time:113814ms step_avg:58.46ms
step:1948/2110 train_time:113902ms step_avg:58.47ms
step:1949/2110 train_time:113991ms step_avg:58.49ms
step:1950/2110 train_time:114079ms step_avg:58.50ms
step:1951/2110 train_time:114166ms step_avg:58.52ms
step:1952/2110 train_time:114255ms step_avg:58.53ms
step:1953/2110 train_time:114341ms step_avg:58.55ms
step:1954/2110 train_time:114430ms step_avg:58.56ms
step:1955/2110 train_time:114519ms step_avg:58.58ms
step:1956/2110 train_time:114607ms step_avg:58.59ms
step:1957/2110 train_time:114695ms step_avg:58.61ms
step:1958/2110 train_time:114783ms step_avg:58.62ms
step:1959/2110 train_time:114872ms step_avg:58.64ms
step:1960/2110 train_time:114958ms step_avg:58.65ms
step:1961/2110 train_time:115046ms step_avg:58.67ms
step:1962/2110 train_time:115135ms step_avg:58.68ms
step:1963/2110 train_time:115223ms step_avg:58.70ms
step:1964/2110 train_time:115311ms step_avg:58.71ms
step:1965/2110 train_time:115398ms step_avg:58.73ms
step:1966/2110 train_time:115486ms step_avg:58.74ms
step:1967/2110 train_time:115575ms step_avg:58.76ms
step:1968/2110 train_time:115662ms step_avg:58.77ms
step:1969/2110 train_time:115750ms step_avg:58.79ms
step:1970/2110 train_time:115838ms step_avg:58.80ms
step:1971/2110 train_time:115927ms step_avg:58.82ms
step:1972/2110 train_time:116014ms step_avg:58.83ms
step:1973/2110 train_time:116103ms step_avg:58.85ms
step:1974/2110 train_time:116191ms step_avg:58.86ms
step:1975/2110 train_time:116277ms step_avg:58.87ms
step:1976/2110 train_time:116367ms step_avg:58.89ms
step:1977/2110 train_time:116453ms step_avg:58.90ms
step:1978/2110 train_time:116541ms step_avg:58.92ms
step:1979/2110 train_time:116628ms step_avg:58.93ms
step:1980/2110 train_time:116716ms step_avg:58.95ms
step:1981/2110 train_time:116808ms step_avg:58.96ms
step:1982/2110 train_time:116893ms step_avg:58.98ms
step:1983/2110 train_time:116980ms step_avg:58.99ms
step:1984/2110 train_time:117069ms step_avg:59.01ms
step:1985/2110 train_time:117156ms step_avg:59.02ms
step:1986/2110 train_time:117244ms step_avg:59.04ms
step:1987/2110 train_time:117333ms step_avg:59.05ms
step:1988/2110 train_time:117420ms step_avg:59.06ms
step:1989/2110 train_time:117506ms step_avg:59.08ms
step:1990/2110 train_time:117594ms step_avg:59.09ms
step:1991/2110 train_time:117682ms step_avg:59.11ms
step:1992/2110 train_time:117770ms step_avg:59.12ms
step:1993/2110 train_time:117858ms step_avg:59.14ms
step:1994/2110 train_time:117946ms step_avg:59.15ms
step:1995/2110 train_time:118034ms step_avg:59.16ms
step:1996/2110 train_time:118123ms step_avg:59.18ms
step:1997/2110 train_time:118210ms step_avg:59.19ms
step:1998/2110 train_time:118298ms step_avg:59.21ms
step:1999/2110 train_time:118386ms step_avg:59.22ms
step:2000/2110 train_time:118474ms step_avg:59.24ms
step:2000/2110 val_loss:3.3047 train_time:118563ms step_avg:59.28ms
step:2001/2110 train_time:118600ms step_avg:59.27ms
step:2002/2110 train_time:118656ms step_avg:59.27ms
step:2003/2110 train_time:118747ms step_avg:59.28ms
step:2004/2110 train_time:118836ms step_avg:59.30ms
step:2005/2110 train_time:118926ms step_avg:59.31ms
step:2006/2110 train_time:119011ms step_avg:59.33ms
step:2007/2110 train_time:119098ms step_avg:59.34ms
step:2008/2110 train_time:119185ms step_avg:59.35ms
step:2009/2110 train_time:119271ms step_avg:59.37ms
step:2010/2110 train_time:119359ms step_avg:59.38ms
step:2011/2110 train_time:119446ms step_avg:59.40ms
step:2012/2110 train_time:119536ms step_avg:59.41ms
step:2013/2110 train_time:119627ms step_avg:59.43ms
step:2014/2110 train_time:119716ms step_avg:59.44ms
step:2015/2110 train_time:119805ms step_avg:59.46ms
step:2016/2110 train_time:119892ms step_avg:59.47ms
step:2017/2110 train_time:119980ms step_avg:59.48ms
step:2018/2110 train_time:120067ms step_avg:59.50ms
step:2019/2110 train_time:120155ms step_avg:59.51ms
step:2020/2110 train_time:120242ms step_avg:59.53ms
step:2021/2110 train_time:120328ms step_avg:59.54ms
step:2022/2110 train_time:120417ms step_avg:59.55ms
step:2023/2110 train_time:120503ms step_avg:59.57ms
step:2024/2110 train_time:120592ms step_avg:59.58ms
step:2025/2110 train_time:120682ms step_avg:59.60ms
step:2026/2110 train_time:120771ms step_avg:59.61ms
step:2027/2110 train_time:120858ms step_avg:59.62ms
step:2028/2110 train_time:120948ms step_avg:59.64ms
step:2029/2110 train_time:121036ms step_avg:59.65ms
step:2030/2110 train_time:121121ms step_avg:59.67ms
step:2031/2110 train_time:121209ms step_avg:59.68ms
step:2032/2110 train_time:121297ms step_avg:59.69ms
step:2033/2110 train_time:121383ms step_avg:59.71ms
step:2034/2110 train_time:121472ms step_avg:59.72ms
step:2035/2110 train_time:121563ms step_avg:59.74ms
step:2036/2110 train_time:121649ms step_avg:59.75ms
step:2037/2110 train_time:121739ms step_avg:59.76ms
step:2038/2110 train_time:121828ms step_avg:59.78ms
step:2039/2110 train_time:121917ms step_avg:59.79ms
step:2040/2110 train_time:122005ms step_avg:59.81ms
step:2041/2110 train_time:122093ms step_avg:59.82ms
step:2042/2110 train_time:122179ms step_avg:59.83ms
step:2043/2110 train_time:122266ms step_avg:59.85ms
step:2044/2110 train_time:122355ms step_avg:59.86ms
step:2045/2110 train_time:122441ms step_avg:59.87ms
step:2046/2110 train_time:122529ms step_avg:59.89ms
step:2047/2110 train_time:122617ms step_avg:59.90ms
step:2048/2110 train_time:122707ms step_avg:59.92ms
step:2049/2110 train_time:122793ms step_avg:59.93ms
step:2050/2110 train_time:122881ms step_avg:59.94ms
step:2051/2110 train_time:122971ms step_avg:59.96ms
step:2052/2110 train_time:123058ms step_avg:59.97ms
step:2053/2110 train_time:123145ms step_avg:59.98ms
step:2054/2110 train_time:123233ms step_avg:60.00ms
step:2055/2110 train_time:123321ms step_avg:60.01ms
step:2056/2110 train_time:123408ms step_avg:60.02ms
step:2057/2110 train_time:123498ms step_avg:60.04ms
step:2058/2110 train_time:123585ms step_avg:60.05ms
step:2059/2110 train_time:123673ms step_avg:60.06ms
step:2060/2110 train_time:123762ms step_avg:60.08ms
step:2061/2110 train_time:123850ms step_avg:60.09ms
step:2062/2110 train_time:123938ms step_avg:60.11ms
step:2063/2110 train_time:124025ms step_avg:60.12ms
step:2064/2110 train_time:124112ms step_avg:60.13ms
step:2065/2110 train_time:124201ms step_avg:60.15ms
step:2066/2110 train_time:124289ms step_avg:60.16ms
step:2067/2110 train_time:124378ms step_avg:60.17ms
step:2068/2110 train_time:124464ms step_avg:60.19ms
step:2069/2110 train_time:124553ms step_avg:60.20ms
step:2070/2110 train_time:124643ms step_avg:60.21ms
step:2071/2110 train_time:124730ms step_avg:60.23ms
step:2072/2110 train_time:124819ms step_avg:60.24ms
step:2073/2110 train_time:124908ms step_avg:60.25ms
step:2074/2110 train_time:124995ms step_avg:60.27ms
step:2075/2110 train_time:125083ms step_avg:60.28ms
step:2076/2110 train_time:125170ms step_avg:60.29ms
step:2077/2110 train_time:125260ms step_avg:60.31ms
step:2078/2110 train_time:125348ms step_avg:60.32ms
step:2079/2110 train_time:125437ms step_avg:60.34ms
step:2080/2110 train_time:125524ms step_avg:60.35ms
step:2081/2110 train_time:125613ms step_avg:60.36ms
step:2082/2110 train_time:125702ms step_avg:60.38ms
step:2083/2110 train_time:125789ms step_avg:60.39ms
step:2084/2110 train_time:125878ms step_avg:60.40ms
step:2085/2110 train_time:125966ms step_avg:60.42ms
step:2086/2110 train_time:126055ms step_avg:60.43ms
step:2087/2110 train_time:126142ms step_avg:60.44ms
step:2088/2110 train_time:126229ms step_avg:60.45ms
step:2089/2110 train_time:126318ms step_avg:60.47ms
step:2090/2110 train_time:126407ms step_avg:60.48ms
step:2091/2110 train_time:126495ms step_avg:60.50ms
step:2092/2110 train_time:126583ms step_avg:60.51ms
step:2093/2110 train_time:126672ms step_avg:60.52ms
step:2094/2110 train_time:126760ms step_avg:60.53ms
step:2095/2110 train_time:126848ms step_avg:60.55ms
step:2096/2110 train_time:126937ms step_avg:60.56ms
step:2097/2110 train_time:127029ms step_avg:60.58ms
step:2098/2110 train_time:127114ms step_avg:60.59ms
step:2099/2110 train_time:127202ms step_avg:60.60ms
step:2100/2110 train_time:127290ms step_avg:60.61ms
step:2101/2110 train_time:127378ms step_avg:60.63ms
step:2102/2110 train_time:127469ms step_avg:60.64ms
step:2103/2110 train_time:127556ms step_avg:60.65ms
step:2104/2110 train_time:127644ms step_avg:60.67ms
step:2105/2110 train_time:127734ms step_avg:60.68ms
step:2106/2110 train_time:127821ms step_avg:60.69ms
step:2107/2110 train_time:127909ms step_avg:60.71ms
step:2108/2110 train_time:127998ms step_avg:60.72ms
step:2109/2110 train_time:128086ms step_avg:60.73ms
step:2110/2110 train_time:128174ms step_avg:60.75ms
step:2110/2110 val_loss:3.2806 train_time:128263ms step_avg:60.79ms
peak memory allocated: 29816 MiB reserved: 44036 MiB

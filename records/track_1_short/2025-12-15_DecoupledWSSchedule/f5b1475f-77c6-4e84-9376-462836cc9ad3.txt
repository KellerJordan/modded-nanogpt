import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 15 22:51:08 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   38C    P0            126W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   42C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   43C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   37C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   42C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   42C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              88      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              88      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:114ms step_avg:114.27ms
step:2/2110 train_time:154ms step_avg:76.91ms
step:3/2110 train_time:190ms step_avg:63.27ms
step:4/2110 train_time:231ms step_avg:57.73ms
step:5/2110 train_time:269ms step_avg:53.82ms
step:6/2110 train_time:473ms step_avg:78.87ms
step:7/2110 train_time:631ms step_avg:90.08ms
step:8/2110 train_time:667ms step_avg:83.35ms
step:9/2110 train_time:703ms step_avg:78.13ms
step:10/2110 train_time:741ms step_avg:74.05ms
step:11/2110 train_time:777ms step_avg:70.63ms
step:12/2110 train_time:815ms step_avg:67.93ms
step:13/2110 train_time:851ms step_avg:65.47ms
step:14/2110 train_time:889ms step_avg:63.53ms
step:15/2110 train_time:926ms step_avg:61.70ms
step:16/2110 train_time:963ms step_avg:60.20ms
step:17/2110 train_time:999ms step_avg:58.79ms
step:18/2110 train_time:1037ms step_avg:57.60ms
step:19/2110 train_time:1073ms step_avg:56.48ms
step:20/2110 train_time:1109ms step_avg:55.47ms
step:21/2110 train_time:1145ms step_avg:54.51ms
step:22/2110 train_time:1183ms step_avg:53.76ms
step:23/2110 train_time:1218ms step_avg:52.94ms
step:24/2110 train_time:1256ms step_avg:52.33ms
step:25/2110 train_time:1292ms step_avg:51.69ms
step:26/2110 train_time:1329ms step_avg:51.11ms
step:27/2110 train_time:1366ms step_avg:50.59ms
step:28/2110 train_time:1403ms step_avg:50.10ms
step:29/2110 train_time:1439ms step_avg:49.63ms
step:30/2110 train_time:1478ms step_avg:49.26ms
step:31/2110 train_time:1513ms step_avg:48.80ms
step:32/2110 train_time:1552ms step_avg:48.49ms
step:33/2110 train_time:1587ms step_avg:48.09ms
step:34/2110 train_time:1624ms step_avg:47.76ms
step:35/2110 train_time:1659ms step_avg:47.39ms
step:36/2110 train_time:1695ms step_avg:47.08ms
step:37/2110 train_time:1731ms step_avg:46.78ms
step:38/2110 train_time:1769ms step_avg:46.55ms
step:39/2110 train_time:1804ms step_avg:46.26ms
step:40/2110 train_time:1843ms step_avg:46.08ms
step:41/2110 train_time:1879ms step_avg:45.82ms
step:42/2110 train_time:1918ms step_avg:45.67ms
step:43/2110 train_time:1954ms step_avg:45.45ms
step:44/2110 train_time:1992ms step_avg:45.28ms
step:45/2110 train_time:2028ms step_avg:45.06ms
step:46/2110 train_time:2066ms step_avg:44.92ms
step:47/2110 train_time:2102ms step_avg:44.71ms
step:48/2110 train_time:2138ms step_avg:44.55ms
step:49/2110 train_time:2174ms step_avg:44.37ms
step:50/2110 train_time:2213ms step_avg:44.25ms
step:51/2110 train_time:2248ms step_avg:44.08ms
step:52/2110 train_time:2288ms step_avg:44.00ms
step:53/2110 train_time:2324ms step_avg:43.85ms
step:54/2110 train_time:2362ms step_avg:43.73ms
step:55/2110 train_time:2399ms step_avg:43.61ms
step:56/2110 train_time:2434ms step_avg:43.47ms
step:57/2110 train_time:2470ms step_avg:43.33ms
step:58/2110 train_time:2508ms step_avg:43.24ms
step:59/2110 train_time:2543ms step_avg:43.11ms
step:60/2110 train_time:2581ms step_avg:43.02ms
step:61/2110 train_time:2616ms step_avg:42.88ms
step:62/2110 train_time:2654ms step_avg:42.81ms
step:63/2110 train_time:2689ms step_avg:42.69ms
step:64/2110 train_time:2728ms step_avg:42.63ms
step:65/2110 train_time:2763ms step_avg:42.51ms
step:66/2110 train_time:2800ms step_avg:42.42ms
step:67/2110 train_time:2834ms step_avg:42.30ms
step:68/2110 train_time:2874ms step_avg:42.26ms
step:69/2110 train_time:2910ms step_avg:42.17ms
step:70/2110 train_time:2947ms step_avg:42.10ms
step:71/2110 train_time:2983ms step_avg:42.01ms
step:72/2110 train_time:3019ms step_avg:41.94ms
step:73/2110 train_time:3056ms step_avg:41.86ms
step:74/2110 train_time:3094ms step_avg:41.81ms
step:75/2110 train_time:3130ms step_avg:41.73ms
step:76/2110 train_time:3168ms step_avg:41.68ms
step:77/2110 train_time:3203ms step_avg:41.59ms
step:78/2110 train_time:3239ms step_avg:41.53ms
step:79/2110 train_time:3273ms step_avg:41.43ms
step:80/2110 train_time:3312ms step_avg:41.40ms
step:81/2110 train_time:3346ms step_avg:41.31ms
step:82/2110 train_time:3385ms step_avg:41.28ms
step:83/2110 train_time:3422ms step_avg:41.23ms
step:84/2110 train_time:3459ms step_avg:41.17ms
step:85/2110 train_time:3496ms step_avg:41.13ms
step:86/2110 train_time:3533ms step_avg:41.08ms
step:87/2110 train_time:3569ms step_avg:41.02ms
step:88/2110 train_time:3605ms step_avg:40.97ms
step:89/2110 train_time:3640ms step_avg:40.90ms
step:90/2110 train_time:3678ms step_avg:40.86ms
step:91/2110 train_time:3714ms step_avg:40.82ms
step:92/2110 train_time:3752ms step_avg:40.78ms
step:93/2110 train_time:3789ms step_avg:40.74ms
step:94/2110 train_time:3826ms step_avg:40.70ms
step:95/2110 train_time:3861ms step_avg:40.64ms
step:96/2110 train_time:3899ms step_avg:40.62ms
step:97/2110 train_time:3937ms step_avg:40.58ms
step:98/2110 train_time:3975ms step_avg:40.56ms
step:99/2110 train_time:4009ms step_avg:40.49ms
step:100/2110 train_time:4048ms step_avg:40.48ms
step:101/2110 train_time:4084ms step_avg:40.44ms
step:102/2110 train_time:4124ms step_avg:40.43ms
step:103/2110 train_time:4160ms step_avg:40.39ms
step:104/2110 train_time:4197ms step_avg:40.36ms
step:105/2110 train_time:4232ms step_avg:40.31ms
step:106/2110 train_time:4270ms step_avg:40.28ms
step:107/2110 train_time:4305ms step_avg:40.23ms
step:108/2110 train_time:4343ms step_avg:40.21ms
step:109/2110 train_time:4380ms step_avg:40.18ms
step:110/2110 train_time:4418ms step_avg:40.16ms
step:111/2110 train_time:4453ms step_avg:40.12ms
step:112/2110 train_time:4492ms step_avg:40.11ms
step:113/2110 train_time:4527ms step_avg:40.06ms
step:114/2110 train_time:4566ms step_avg:40.05ms
step:115/2110 train_time:4600ms step_avg:40.00ms
step:116/2110 train_time:4636ms step_avg:39.96ms
step:117/2110 train_time:4672ms step_avg:39.93ms
step:118/2110 train_time:4711ms step_avg:39.92ms
step:119/2110 train_time:4748ms step_avg:39.90ms
step:120/2110 train_time:4786ms step_avg:39.89ms
step:121/2110 train_time:4822ms step_avg:39.85ms
step:122/2110 train_time:4862ms step_avg:39.86ms
step:123/2110 train_time:4898ms step_avg:39.82ms
step:124/2110 train_time:4935ms step_avg:39.80ms
step:125/2110 train_time:4972ms step_avg:39.78ms
step:126/2110 train_time:5009ms step_avg:39.76ms
step:127/2110 train_time:5045ms step_avg:39.73ms
step:128/2110 train_time:5084ms step_avg:39.72ms
step:129/2110 train_time:5120ms step_avg:39.69ms
step:130/2110 train_time:5158ms step_avg:39.67ms
step:131/2110 train_time:5195ms step_avg:39.65ms
step:132/2110 train_time:5233ms step_avg:39.64ms
step:133/2110 train_time:5269ms step_avg:39.62ms
step:134/2110 train_time:5307ms step_avg:39.60ms
step:135/2110 train_time:5345ms step_avg:39.59ms
step:136/2110 train_time:5381ms step_avg:39.57ms
step:137/2110 train_time:5416ms step_avg:39.54ms
step:138/2110 train_time:5453ms step_avg:39.51ms
step:139/2110 train_time:5488ms step_avg:39.48ms
step:140/2110 train_time:5524ms step_avg:39.46ms
step:141/2110 train_time:5561ms step_avg:39.44ms
step:142/2110 train_time:5600ms step_avg:39.44ms
step:143/2110 train_time:5634ms step_avg:39.40ms
step:144/2110 train_time:5668ms step_avg:39.36ms
step:145/2110 train_time:5706ms step_avg:39.35ms
step:146/2110 train_time:5741ms step_avg:39.32ms
step:147/2110 train_time:5776ms step_avg:39.29ms
step:148/2110 train_time:5813ms step_avg:39.28ms
step:149/2110 train_time:5848ms step_avg:39.25ms
step:150/2110 train_time:5884ms step_avg:39.23ms
step:151/2110 train_time:5921ms step_avg:39.21ms
step:152/2110 train_time:5958ms step_avg:39.19ms
step:153/2110 train_time:5994ms step_avg:39.18ms
step:154/2110 train_time:6032ms step_avg:39.17ms
step:155/2110 train_time:6068ms step_avg:39.15ms
step:156/2110 train_time:6106ms step_avg:39.14ms
step:157/2110 train_time:6143ms step_avg:39.13ms
step:158/2110 train_time:6181ms step_avg:39.12ms
step:159/2110 train_time:6217ms step_avg:39.10ms
step:160/2110 train_time:6255ms step_avg:39.10ms
step:161/2110 train_time:6294ms step_avg:39.09ms
step:162/2110 train_time:6334ms step_avg:39.10ms
step:163/2110 train_time:6371ms step_avg:39.08ms
step:164/2110 train_time:6411ms step_avg:39.09ms
step:165/2110 train_time:6450ms step_avg:39.09ms
step:166/2110 train_time:6491ms step_avg:39.10ms
step:167/2110 train_time:6530ms step_avg:39.10ms
step:168/2110 train_time:6570ms step_avg:39.11ms
step:169/2110 train_time:6608ms step_avg:39.10ms
step:170/2110 train_time:6648ms step_avg:39.11ms
step:171/2110 train_time:6687ms step_avg:39.11ms
step:172/2110 train_time:6727ms step_avg:39.11ms
step:173/2110 train_time:6764ms step_avg:39.10ms
step:174/2110 train_time:6803ms step_avg:39.10ms
step:175/2110 train_time:6840ms step_avg:39.09ms
step:176/2110 train_time:6880ms step_avg:39.09ms
step:177/2110 train_time:6916ms step_avg:39.07ms
step:178/2110 train_time:6955ms step_avg:39.07ms
step:179/2110 train_time:6992ms step_avg:39.06ms
step:180/2110 train_time:7031ms step_avg:39.06ms
step:181/2110 train_time:7069ms step_avg:39.05ms
step:182/2110 train_time:7108ms step_avg:39.06ms
step:183/2110 train_time:7145ms step_avg:39.04ms
step:184/2110 train_time:7185ms step_avg:39.05ms
step:185/2110 train_time:7221ms step_avg:39.03ms
step:186/2110 train_time:7259ms step_avg:39.03ms
step:187/2110 train_time:7295ms step_avg:39.01ms
step:188/2110 train_time:7334ms step_avg:39.01ms
step:189/2110 train_time:7372ms step_avg:39.01ms
step:190/2110 train_time:7411ms step_avg:39.00ms
step:191/2110 train_time:7447ms step_avg:38.99ms
step:192/2110 train_time:7485ms step_avg:38.98ms
step:193/2110 train_time:7522ms step_avg:38.97ms
step:194/2110 train_time:7563ms step_avg:38.98ms
step:195/2110 train_time:7601ms step_avg:38.98ms
step:196/2110 train_time:7641ms step_avg:38.98ms
step:197/2110 train_time:7678ms step_avg:38.97ms
step:198/2110 train_time:7717ms step_avg:38.97ms
step:199/2110 train_time:7758ms step_avg:38.98ms
step:200/2110 train_time:7802ms step_avg:39.01ms
step:201/2110 train_time:7854ms step_avg:39.08ms
step:202/2110 train_time:7901ms step_avg:39.11ms
step:203/2110 train_time:7950ms step_avg:39.16ms
step:204/2110 train_time:7996ms step_avg:39.19ms
step:205/2110 train_time:8043ms step_avg:39.23ms
step:206/2110 train_time:8088ms step_avg:39.26ms
step:207/2110 train_time:8139ms step_avg:39.32ms
step:208/2110 train_time:8185ms step_avg:39.35ms
step:209/2110 train_time:8233ms step_avg:39.39ms
step:210/2110 train_time:8278ms step_avg:39.42ms
step:211/2110 train_time:8327ms step_avg:39.47ms
step:212/2110 train_time:8373ms step_avg:39.49ms
step:213/2110 train_time:8421ms step_avg:39.54ms
step:214/2110 train_time:8466ms step_avg:39.56ms
step:215/2110 train_time:8514ms step_avg:39.60ms
step:216/2110 train_time:8560ms step_avg:39.63ms
step:217/2110 train_time:8608ms step_avg:39.67ms
step:218/2110 train_time:8654ms step_avg:39.70ms
step:219/2110 train_time:8704ms step_avg:39.74ms
step:220/2110 train_time:8751ms step_avg:39.78ms
step:221/2110 train_time:8799ms step_avg:39.81ms
step:222/2110 train_time:8841ms step_avg:39.82ms
step:223/2110 train_time:8888ms step_avg:39.85ms
step:224/2110 train_time:8932ms step_avg:39.88ms
step:225/2110 train_time:8981ms step_avg:39.92ms
step:226/2110 train_time:9029ms step_avg:39.95ms
step:227/2110 train_time:9080ms step_avg:40.00ms
step:228/2110 train_time:9127ms step_avg:40.03ms
step:229/2110 train_time:9176ms step_avg:40.07ms
step:230/2110 train_time:9220ms step_avg:40.09ms
step:231/2110 train_time:9267ms step_avg:40.12ms
step:232/2110 train_time:9309ms step_avg:40.13ms
step:233/2110 train_time:9357ms step_avg:40.16ms
step:234/2110 train_time:9401ms step_avg:40.17ms
step:235/2110 train_time:9447ms step_avg:40.20ms
step:236/2110 train_time:9489ms step_avg:40.21ms
step:237/2110 train_time:9534ms step_avg:40.23ms
step:238/2110 train_time:9577ms step_avg:40.24ms
step:239/2110 train_time:9624ms step_avg:40.27ms
step:240/2110 train_time:9667ms step_avg:40.28ms
step:241/2110 train_time:9714ms step_avg:40.31ms
step:242/2110 train_time:9757ms step_avg:40.32ms
step:243/2110 train_time:9803ms step_avg:40.34ms
step:244/2110 train_time:9847ms step_avg:40.36ms
step:245/2110 train_time:9895ms step_avg:40.39ms
step:246/2110 train_time:9938ms step_avg:40.40ms
step:247/2110 train_time:9986ms step_avg:40.43ms
step:248/2110 train_time:10029ms step_avg:40.44ms
step:249/2110 train_time:10076ms step_avg:40.47ms
step:250/2110 train_time:10120ms step_avg:40.48ms
step:250/2110 val_loss:4.2882 train_time:10175ms step_avg:40.70ms
step:251/2110 train_time:10215ms step_avg:40.70ms
step:252/2110 train_time:10255ms step_avg:40.70ms
step:253/2110 train_time:10293ms step_avg:40.68ms
step:254/2110 train_time:10331ms step_avg:40.67ms
step:255/2110 train_time:10368ms step_avg:40.66ms
step:256/2110 train_time:10405ms step_avg:40.64ms
step:257/2110 train_time:10451ms step_avg:40.67ms
step:258/2110 train_time:10494ms step_avg:40.68ms
step:259/2110 train_time:10539ms step_avg:40.69ms
step:260/2110 train_time:10581ms step_avg:40.70ms
step:261/2110 train_time:10624ms step_avg:40.71ms
step:262/2110 train_time:10667ms step_avg:40.71ms
step:263/2110 train_time:10712ms step_avg:40.73ms
step:264/2110 train_time:10755ms step_avg:40.74ms
step:265/2110 train_time:10802ms step_avg:40.76ms
step:266/2110 train_time:10844ms step_avg:40.77ms
step:267/2110 train_time:10890ms step_avg:40.79ms
step:268/2110 train_time:10932ms step_avg:40.79ms
step:269/2110 train_time:10978ms step_avg:40.81ms
step:270/2110 train_time:11022ms step_avg:40.82ms
step:271/2110 train_time:11067ms step_avg:40.84ms
step:272/2110 train_time:11109ms step_avg:40.84ms
step:273/2110 train_time:11154ms step_avg:40.86ms
step:274/2110 train_time:11198ms step_avg:40.87ms
step:275/2110 train_time:11245ms step_avg:40.89ms
step:276/2110 train_time:11289ms step_avg:40.90ms
step:277/2110 train_time:11336ms step_avg:40.92ms
step:278/2110 train_time:11377ms step_avg:40.93ms
step:279/2110 train_time:11425ms step_avg:40.95ms
step:280/2110 train_time:11469ms step_avg:40.96ms
step:281/2110 train_time:11517ms step_avg:40.98ms
step:282/2110 train_time:11560ms step_avg:40.99ms
step:283/2110 train_time:11605ms step_avg:41.01ms
step:284/2110 train_time:11650ms step_avg:41.02ms
step:285/2110 train_time:11694ms step_avg:41.03ms
step:286/2110 train_time:11738ms step_avg:41.04ms
step:287/2110 train_time:11782ms step_avg:41.05ms
step:288/2110 train_time:11825ms step_avg:41.06ms
step:289/2110 train_time:11869ms step_avg:41.07ms
step:290/2110 train_time:11912ms step_avg:41.08ms
step:291/2110 train_time:11956ms step_avg:41.09ms
step:292/2110 train_time:11998ms step_avg:41.09ms
step:293/2110 train_time:12041ms step_avg:41.10ms
step:294/2110 train_time:12080ms step_avg:41.09ms
step:295/2110 train_time:12122ms step_avg:41.09ms
step:296/2110 train_time:12161ms step_avg:41.08ms
step:297/2110 train_time:12202ms step_avg:41.08ms
step:298/2110 train_time:12241ms step_avg:41.08ms
step:299/2110 train_time:12284ms step_avg:41.08ms
step:300/2110 train_time:12323ms step_avg:41.08ms
step:301/2110 train_time:12365ms step_avg:41.08ms
step:302/2110 train_time:12404ms step_avg:41.07ms
step:303/2110 train_time:12447ms step_avg:41.08ms
step:304/2110 train_time:12486ms step_avg:41.07ms
step:305/2110 train_time:12529ms step_avg:41.08ms
step:306/2110 train_time:12568ms step_avg:41.07ms
step:307/2110 train_time:12610ms step_avg:41.08ms
step:308/2110 train_time:12649ms step_avg:41.07ms
step:309/2110 train_time:12692ms step_avg:41.08ms
step:310/2110 train_time:12732ms step_avg:41.07ms
step:311/2110 train_time:12777ms step_avg:41.08ms
step:312/2110 train_time:12816ms step_avg:41.08ms
step:313/2110 train_time:12859ms step_avg:41.08ms
step:314/2110 train_time:12898ms step_avg:41.08ms
step:315/2110 train_time:12939ms step_avg:41.08ms
step:316/2110 train_time:12979ms step_avg:41.07ms
step:317/2110 train_time:13022ms step_avg:41.08ms
step:318/2110 train_time:13061ms step_avg:41.07ms
step:319/2110 train_time:13102ms step_avg:41.07ms
step:320/2110 train_time:13144ms step_avg:41.07ms
step:321/2110 train_time:13185ms step_avg:41.08ms
step:322/2110 train_time:13226ms step_avg:41.07ms
step:323/2110 train_time:13269ms step_avg:41.08ms
step:324/2110 train_time:13308ms step_avg:41.07ms
step:325/2110 train_time:13352ms step_avg:41.08ms
step:326/2110 train_time:13393ms step_avg:41.08ms
step:327/2110 train_time:13434ms step_avg:41.08ms
step:328/2110 train_time:13474ms step_avg:41.08ms
step:329/2110 train_time:13517ms step_avg:41.08ms
step:330/2110 train_time:13556ms step_avg:41.08ms
step:331/2110 train_time:13597ms step_avg:41.08ms
step:332/2110 train_time:13636ms step_avg:41.07ms
step:333/2110 train_time:13678ms step_avg:41.08ms
step:334/2110 train_time:13718ms step_avg:41.07ms
step:335/2110 train_time:13760ms step_avg:41.08ms
step:336/2110 train_time:13799ms step_avg:41.07ms
step:337/2110 train_time:13845ms step_avg:41.08ms
step:338/2110 train_time:13885ms step_avg:41.08ms
step:339/2110 train_time:13928ms step_avg:41.08ms
step:340/2110 train_time:13968ms step_avg:41.08ms
step:341/2110 train_time:14010ms step_avg:41.08ms
step:342/2110 train_time:14049ms step_avg:41.08ms
step:343/2110 train_time:14091ms step_avg:41.08ms
step:344/2110 train_time:14131ms step_avg:41.08ms
step:345/2110 train_time:14173ms step_avg:41.08ms
step:346/2110 train_time:14213ms step_avg:41.08ms
step:347/2110 train_time:14255ms step_avg:41.08ms
step:348/2110 train_time:14295ms step_avg:41.08ms
step:349/2110 train_time:14337ms step_avg:41.08ms
step:350/2110 train_time:14377ms step_avg:41.08ms
step:351/2110 train_time:14419ms step_avg:41.08ms
step:352/2110 train_time:14459ms step_avg:41.08ms
step:353/2110 train_time:14501ms step_avg:41.08ms
step:354/2110 train_time:14540ms step_avg:41.07ms
step:355/2110 train_time:14583ms step_avg:41.08ms
step:356/2110 train_time:14623ms step_avg:41.08ms
step:357/2110 train_time:14666ms step_avg:41.08ms
step:358/2110 train_time:14705ms step_avg:41.07ms
step:359/2110 train_time:14748ms step_avg:41.08ms
step:360/2110 train_time:14788ms step_avg:41.08ms
step:361/2110 train_time:14831ms step_avg:41.08ms
step:362/2110 train_time:14874ms step_avg:41.09ms
step:363/2110 train_time:14916ms step_avg:41.09ms
step:364/2110 train_time:14956ms step_avg:41.09ms
step:365/2110 train_time:14997ms step_avg:41.09ms
step:366/2110 train_time:15038ms step_avg:41.09ms
step:367/2110 train_time:15080ms step_avg:41.09ms
step:368/2110 train_time:15119ms step_avg:41.09ms
step:369/2110 train_time:15161ms step_avg:41.09ms
step:370/2110 train_time:15201ms step_avg:41.08ms
step:371/2110 train_time:15244ms step_avg:41.09ms
step:372/2110 train_time:15284ms step_avg:41.09ms
step:373/2110 train_time:15328ms step_avg:41.09ms
step:374/2110 train_time:15367ms step_avg:41.09ms
step:375/2110 train_time:15410ms step_avg:41.09ms
step:376/2110 train_time:15450ms step_avg:41.09ms
step:377/2110 train_time:15492ms step_avg:41.09ms
step:378/2110 train_time:15531ms step_avg:41.09ms
step:379/2110 train_time:15573ms step_avg:41.09ms
step:380/2110 train_time:15612ms step_avg:41.09ms
step:381/2110 train_time:15655ms step_avg:41.09ms
step:382/2110 train_time:15694ms step_avg:41.08ms
step:383/2110 train_time:15736ms step_avg:41.09ms
step:384/2110 train_time:15776ms step_avg:41.08ms
step:385/2110 train_time:15818ms step_avg:41.08ms
step:386/2110 train_time:15857ms step_avg:41.08ms
step:387/2110 train_time:15900ms step_avg:41.09ms
step:388/2110 train_time:15939ms step_avg:41.08ms
step:389/2110 train_time:15983ms step_avg:41.09ms
step:390/2110 train_time:16022ms step_avg:41.08ms
step:391/2110 train_time:16064ms step_avg:41.09ms
step:392/2110 train_time:16099ms step_avg:41.07ms
step:393/2110 train_time:16138ms step_avg:41.06ms
step:394/2110 train_time:16175ms step_avg:41.05ms
step:395/2110 train_time:16210ms step_avg:41.04ms
step:396/2110 train_time:16247ms step_avg:41.03ms
step:397/2110 train_time:16283ms step_avg:41.01ms
step:398/2110 train_time:16321ms step_avg:41.01ms
step:399/2110 train_time:16358ms step_avg:41.00ms
step:400/2110 train_time:16394ms step_avg:40.99ms
step:401/2110 train_time:16429ms step_avg:40.97ms
step:402/2110 train_time:16467ms step_avg:40.96ms
step:403/2110 train_time:16504ms step_avg:40.95ms
step:404/2110 train_time:16542ms step_avg:40.94ms
step:405/2110 train_time:16578ms step_avg:40.93ms
step:406/2110 train_time:16615ms step_avg:40.92ms
step:407/2110 train_time:16652ms step_avg:40.91ms
step:408/2110 train_time:16691ms step_avg:40.91ms
step:409/2110 train_time:16726ms step_avg:40.90ms
step:410/2110 train_time:16763ms step_avg:40.89ms
step:411/2110 train_time:16800ms step_avg:40.87ms
step:412/2110 train_time:16837ms step_avg:40.87ms
step:413/2110 train_time:16874ms step_avg:40.86ms
step:414/2110 train_time:16910ms step_avg:40.85ms
step:415/2110 train_time:16945ms step_avg:40.83ms
step:416/2110 train_time:16982ms step_avg:40.82ms
step:417/2110 train_time:17017ms step_avg:40.81ms
step:418/2110 train_time:17055ms step_avg:40.80ms
step:419/2110 train_time:17092ms step_avg:40.79ms
step:420/2110 train_time:17131ms step_avg:40.79ms
step:421/2110 train_time:17168ms step_avg:40.78ms
step:422/2110 train_time:17208ms step_avg:40.78ms
step:423/2110 train_time:17245ms step_avg:40.77ms
step:424/2110 train_time:17283ms step_avg:40.76ms
step:425/2110 train_time:17319ms step_avg:40.75ms
step:426/2110 train_time:17357ms step_avg:40.74ms
step:427/2110 train_time:17392ms step_avg:40.73ms
step:428/2110 train_time:17430ms step_avg:40.72ms
step:429/2110 train_time:17465ms step_avg:40.71ms
step:430/2110 train_time:17503ms step_avg:40.71ms
step:431/2110 train_time:17539ms step_avg:40.69ms
step:432/2110 train_time:17577ms step_avg:40.69ms
step:433/2110 train_time:17614ms step_avg:40.68ms
step:434/2110 train_time:17651ms step_avg:40.67ms
step:435/2110 train_time:17687ms step_avg:40.66ms
step:436/2110 train_time:17725ms step_avg:40.65ms
step:437/2110 train_time:17763ms step_avg:40.65ms
step:438/2110 train_time:17800ms step_avg:40.64ms
step:439/2110 train_time:17835ms step_avg:40.63ms
step:440/2110 train_time:17874ms step_avg:40.62ms
step:441/2110 train_time:17910ms step_avg:40.61ms
step:442/2110 train_time:17948ms step_avg:40.61ms
step:443/2110 train_time:17985ms step_avg:40.60ms
step:444/2110 train_time:18024ms step_avg:40.59ms
step:445/2110 train_time:18059ms step_avg:40.58ms
step:446/2110 train_time:18094ms step_avg:40.57ms
step:447/2110 train_time:18131ms step_avg:40.56ms
step:448/2110 train_time:18167ms step_avg:40.55ms
step:449/2110 train_time:18203ms step_avg:40.54ms
step:450/2110 train_time:18241ms step_avg:40.54ms
step:451/2110 train_time:18275ms step_avg:40.52ms
step:452/2110 train_time:18312ms step_avg:40.51ms
step:453/2110 train_time:18348ms step_avg:40.50ms
step:454/2110 train_time:18386ms step_avg:40.50ms
step:455/2110 train_time:18432ms step_avg:40.51ms
step:456/2110 train_time:18471ms step_avg:40.51ms
step:457/2110 train_time:18505ms step_avg:40.49ms
step:458/2110 train_time:18543ms step_avg:40.49ms
step:459/2110 train_time:18579ms step_avg:40.48ms
step:460/2110 train_time:18616ms step_avg:40.47ms
step:461/2110 train_time:18652ms step_avg:40.46ms
step:462/2110 train_time:18691ms step_avg:40.46ms
step:463/2110 train_time:18730ms step_avg:40.45ms
step:464/2110 train_time:18766ms step_avg:40.44ms
step:465/2110 train_time:18802ms step_avg:40.44ms
step:466/2110 train_time:18838ms step_avg:40.43ms
step:467/2110 train_time:18880ms step_avg:40.43ms
step:468/2110 train_time:18920ms step_avg:40.43ms
step:469/2110 train_time:18964ms step_avg:40.43ms
step:470/2110 train_time:19007ms step_avg:40.44ms
step:471/2110 train_time:19050ms step_avg:40.45ms
step:472/2110 train_time:19089ms step_avg:40.44ms
step:473/2110 train_time:19129ms step_avg:40.44ms
step:474/2110 train_time:19170ms step_avg:40.44ms
step:475/2110 train_time:19213ms step_avg:40.45ms
step:476/2110 train_time:19257ms step_avg:40.45ms
step:477/2110 train_time:19302ms step_avg:40.47ms
step:478/2110 train_time:19341ms step_avg:40.46ms
step:479/2110 train_time:19379ms step_avg:40.46ms
step:480/2110 train_time:19418ms step_avg:40.45ms
step:481/2110 train_time:19461ms step_avg:40.46ms
step:482/2110 train_time:19501ms step_avg:40.46ms
step:483/2110 train_time:19546ms step_avg:40.47ms
step:484/2110 train_time:19587ms step_avg:40.47ms
step:485/2110 train_time:19632ms step_avg:40.48ms
step:486/2110 train_time:19673ms step_avg:40.48ms
step:487/2110 train_time:19715ms step_avg:40.48ms
step:488/2110 train_time:19757ms step_avg:40.49ms
step:489/2110 train_time:19801ms step_avg:40.49ms
step:490/2110 train_time:19844ms step_avg:40.50ms
step:491/2110 train_time:19886ms step_avg:40.50ms
step:492/2110 train_time:19923ms step_avg:40.49ms
step:493/2110 train_time:19961ms step_avg:40.49ms
step:494/2110 train_time:20001ms step_avg:40.49ms
step:495/2110 train_time:20044ms step_avg:40.49ms
step:496/2110 train_time:20084ms step_avg:40.49ms
step:497/2110 train_time:20129ms step_avg:40.50ms
step:498/2110 train_time:20173ms step_avg:40.51ms
step:499/2110 train_time:20214ms step_avg:40.51ms
step:500/2110 train_time:20253ms step_avg:40.51ms
step:500/2110 val_loss:4.0252 train_time:20302ms step_avg:40.60ms
step:501/2110 train_time:20341ms step_avg:40.60ms
step:502/2110 train_time:20379ms step_avg:40.60ms
step:503/2110 train_time:20417ms step_avg:40.59ms
step:504/2110 train_time:20457ms step_avg:40.59ms
step:505/2110 train_time:20494ms step_avg:40.58ms
step:506/2110 train_time:20532ms step_avg:40.58ms
step:507/2110 train_time:20568ms step_avg:40.57ms
step:508/2110 train_time:20606ms step_avg:40.56ms
step:509/2110 train_time:20643ms step_avg:40.56ms
step:510/2110 train_time:20682ms step_avg:40.55ms
step:511/2110 train_time:20718ms step_avg:40.54ms
step:512/2110 train_time:20758ms step_avg:40.54ms
step:513/2110 train_time:20799ms step_avg:40.54ms
step:514/2110 train_time:20839ms step_avg:40.54ms
step:515/2110 train_time:20881ms step_avg:40.55ms
step:516/2110 train_time:20921ms step_avg:40.55ms
step:517/2110 train_time:20968ms step_avg:40.56ms
step:518/2110 train_time:21010ms step_avg:40.56ms
step:519/2110 train_time:21047ms step_avg:40.55ms
step:520/2110 train_time:21083ms step_avg:40.54ms
step:521/2110 train_time:21120ms step_avg:40.54ms
step:522/2110 train_time:21160ms step_avg:40.54ms
step:523/2110 train_time:21202ms step_avg:40.54ms
step:524/2110 train_time:21242ms step_avg:40.54ms
step:525/2110 train_time:21286ms step_avg:40.54ms
step:526/2110 train_time:21322ms step_avg:40.54ms
step:527/2110 train_time:21359ms step_avg:40.53ms
step:528/2110 train_time:21397ms step_avg:40.52ms
step:529/2110 train_time:21438ms step_avg:40.53ms
step:530/2110 train_time:21478ms step_avg:40.53ms
step:531/2110 train_time:21524ms step_avg:40.53ms
step:532/2110 train_time:21565ms step_avg:40.54ms
step:533/2110 train_time:21603ms step_avg:40.53ms
step:534/2110 train_time:21641ms step_avg:40.53ms
step:535/2110 train_time:21681ms step_avg:40.53ms
step:536/2110 train_time:21720ms step_avg:40.52ms
step:537/2110 train_time:21763ms step_avg:40.53ms
step:538/2110 train_time:21806ms step_avg:40.53ms
step:539/2110 train_time:21849ms step_avg:40.54ms
step:540/2110 train_time:21884ms step_avg:40.53ms
step:541/2110 train_time:21923ms step_avg:40.52ms
step:542/2110 train_time:21964ms step_avg:40.52ms
step:543/2110 train_time:22008ms step_avg:40.53ms
step:544/2110 train_time:22048ms step_avg:40.53ms
step:545/2110 train_time:22091ms step_avg:40.53ms
step:546/2110 train_time:22131ms step_avg:40.53ms
step:547/2110 train_time:22177ms step_avg:40.54ms
step:548/2110 train_time:22218ms step_avg:40.54ms
step:549/2110 train_time:22254ms step_avg:40.54ms
step:550/2110 train_time:22290ms step_avg:40.53ms
step:551/2110 train_time:22332ms step_avg:40.53ms
step:552/2110 train_time:22373ms step_avg:40.53ms
step:553/2110 train_time:22417ms step_avg:40.54ms
step:554/2110 train_time:22462ms step_avg:40.55ms
step:555/2110 train_time:22504ms step_avg:40.55ms
step:556/2110 train_time:22539ms step_avg:40.54ms
step:557/2110 train_time:22576ms step_avg:40.53ms
step:558/2110 train_time:22611ms step_avg:40.52ms
step:559/2110 train_time:22651ms step_avg:40.52ms
step:560/2110 train_time:22694ms step_avg:40.53ms
step:561/2110 train_time:22735ms step_avg:40.53ms
step:562/2110 train_time:22769ms step_avg:40.52ms
step:563/2110 train_time:22805ms step_avg:40.51ms
step:564/2110 train_time:22845ms step_avg:40.51ms
step:565/2110 train_time:22883ms step_avg:40.50ms
step:566/2110 train_time:22923ms step_avg:40.50ms
step:567/2110 train_time:22970ms step_avg:40.51ms
step:568/2110 train_time:23008ms step_avg:40.51ms
step:569/2110 train_time:23054ms step_avg:40.52ms
step:570/2110 train_time:23094ms step_avg:40.52ms
step:571/2110 train_time:23136ms step_avg:40.52ms
step:572/2110 train_time:23175ms step_avg:40.52ms
step:573/2110 train_time:23221ms step_avg:40.53ms
step:574/2110 train_time:23259ms step_avg:40.52ms
step:575/2110 train_time:23299ms step_avg:40.52ms
step:576/2110 train_time:23338ms step_avg:40.52ms
step:577/2110 train_time:23381ms step_avg:40.52ms
step:578/2110 train_time:23422ms step_avg:40.52ms
step:579/2110 train_time:23472ms step_avg:40.54ms
step:580/2110 train_time:23509ms step_avg:40.53ms
step:581/2110 train_time:23545ms step_avg:40.52ms
step:582/2110 train_time:23581ms step_avg:40.52ms
step:583/2110 train_time:23616ms step_avg:40.51ms
step:584/2110 train_time:23654ms step_avg:40.50ms
step:585/2110 train_time:23691ms step_avg:40.50ms
step:586/2110 train_time:23729ms step_avg:40.49ms
step:587/2110 train_time:23765ms step_avg:40.49ms
step:588/2110 train_time:23802ms step_avg:40.48ms
step:589/2110 train_time:23838ms step_avg:40.47ms
step:590/2110 train_time:23876ms step_avg:40.47ms
step:591/2110 train_time:23913ms step_avg:40.46ms
step:592/2110 train_time:23949ms step_avg:40.45ms
step:593/2110 train_time:23983ms step_avg:40.44ms
step:594/2110 train_time:24021ms step_avg:40.44ms
step:595/2110 train_time:24057ms step_avg:40.43ms
step:596/2110 train_time:24094ms step_avg:40.43ms
step:597/2110 train_time:24130ms step_avg:40.42ms
step:598/2110 train_time:24168ms step_avg:40.42ms
step:599/2110 train_time:24203ms step_avg:40.41ms
step:600/2110 train_time:24241ms step_avg:40.40ms
step:601/2110 train_time:24283ms step_avg:40.40ms
step:602/2110 train_time:24327ms step_avg:40.41ms
step:603/2110 train_time:24370ms step_avg:40.41ms
step:604/2110 train_time:24405ms step_avg:40.41ms
step:605/2110 train_time:24443ms step_avg:40.40ms
step:606/2110 train_time:24479ms step_avg:40.39ms
step:607/2110 train_time:24515ms step_avg:40.39ms
step:608/2110 train_time:24553ms step_avg:40.38ms
step:609/2110 train_time:24597ms step_avg:40.39ms
step:610/2110 train_time:24636ms step_avg:40.39ms
step:611/2110 train_time:24670ms step_avg:40.38ms
step:612/2110 train_time:24708ms step_avg:40.37ms
step:613/2110 train_time:24744ms step_avg:40.36ms
step:614/2110 train_time:24782ms step_avg:40.36ms
step:615/2110 train_time:24825ms step_avg:40.37ms
step:616/2110 train_time:24870ms step_avg:40.37ms
step:617/2110 train_time:24910ms step_avg:40.37ms
step:618/2110 train_time:24946ms step_avg:40.37ms
step:619/2110 train_time:24981ms step_avg:40.36ms
step:620/2110 train_time:25019ms step_avg:40.35ms
step:621/2110 train_time:25056ms step_avg:40.35ms
step:622/2110 train_time:25094ms step_avg:40.34ms
step:623/2110 train_time:25135ms step_avg:40.34ms
step:624/2110 train_time:25183ms step_avg:40.36ms
step:625/2110 train_time:25224ms step_avg:40.36ms
step:626/2110 train_time:25263ms step_avg:40.36ms
step:627/2110 train_time:25309ms step_avg:40.36ms
step:628/2110 train_time:25350ms step_avg:40.37ms
step:629/2110 train_time:25394ms step_avg:40.37ms
step:630/2110 train_time:25435ms step_avg:40.37ms
step:631/2110 train_time:25482ms step_avg:40.38ms
step:632/2110 train_time:25523ms step_avg:40.38ms
step:633/2110 train_time:25567ms step_avg:40.39ms
step:634/2110 train_time:25608ms step_avg:40.39ms
step:635/2110 train_time:25652ms step_avg:40.40ms
step:636/2110 train_time:25692ms step_avg:40.40ms
step:637/2110 train_time:25736ms step_avg:40.40ms
step:638/2110 train_time:25778ms step_avg:40.40ms
step:639/2110 train_time:25820ms step_avg:40.41ms
step:640/2110 train_time:25859ms step_avg:40.40ms
step:641/2110 train_time:25903ms step_avg:40.41ms
step:642/2110 train_time:25944ms step_avg:40.41ms
step:643/2110 train_time:25987ms step_avg:40.42ms
step:644/2110 train_time:26027ms step_avg:40.41ms
step:645/2110 train_time:26071ms step_avg:40.42ms
step:646/2110 train_time:26111ms step_avg:40.42ms
step:647/2110 train_time:26154ms step_avg:40.42ms
step:648/2110 train_time:26196ms step_avg:40.43ms
step:649/2110 train_time:26241ms step_avg:40.43ms
step:650/2110 train_time:26282ms step_avg:40.43ms
step:651/2110 train_time:26326ms step_avg:40.44ms
step:652/2110 train_time:26367ms step_avg:40.44ms
step:653/2110 train_time:26411ms step_avg:40.45ms
step:654/2110 train_time:26453ms step_avg:40.45ms
step:655/2110 train_time:26496ms step_avg:40.45ms
step:656/2110 train_time:26538ms step_avg:40.45ms
step:657/2110 train_time:26582ms step_avg:40.46ms
step:658/2110 train_time:26623ms step_avg:40.46ms
step:659/2110 train_time:26664ms step_avg:40.46ms
step:660/2110 train_time:26704ms step_avg:40.46ms
step:661/2110 train_time:26746ms step_avg:40.46ms
step:662/2110 train_time:26786ms step_avg:40.46ms
step:663/2110 train_time:26830ms step_avg:40.47ms
step:664/2110 train_time:26871ms step_avg:40.47ms
step:665/2110 train_time:26915ms step_avg:40.47ms
step:666/2110 train_time:26957ms step_avg:40.48ms
step:667/2110 train_time:27002ms step_avg:40.48ms
step:668/2110 train_time:27043ms step_avg:40.48ms
step:669/2110 train_time:27087ms step_avg:40.49ms
step:670/2110 train_time:27128ms step_avg:40.49ms
step:671/2110 train_time:27170ms step_avg:40.49ms
step:672/2110 train_time:27210ms step_avg:40.49ms
step:673/2110 train_time:27255ms step_avg:40.50ms
step:674/2110 train_time:27295ms step_avg:40.50ms
step:675/2110 train_time:27340ms step_avg:40.50ms
step:676/2110 train_time:27383ms step_avg:40.51ms
step:677/2110 train_time:27428ms step_avg:40.51ms
step:678/2110 train_time:27471ms step_avg:40.52ms
step:679/2110 train_time:27516ms step_avg:40.52ms
step:680/2110 train_time:27557ms step_avg:40.53ms
step:681/2110 train_time:27601ms step_avg:40.53ms
step:682/2110 train_time:27640ms step_avg:40.53ms
step:683/2110 train_time:27685ms step_avg:40.53ms
step:684/2110 train_time:27724ms step_avg:40.53ms
step:685/2110 train_time:27767ms step_avg:40.54ms
step:686/2110 train_time:27808ms step_avg:40.54ms
step:687/2110 train_time:27853ms step_avg:40.54ms
step:688/2110 train_time:27895ms step_avg:40.54ms
step:689/2110 train_time:27939ms step_avg:40.55ms
step:690/2110 train_time:27980ms step_avg:40.55ms
step:691/2110 train_time:28026ms step_avg:40.56ms
step:692/2110 train_time:28088ms step_avg:40.59ms
step:693/2110 train_time:28147ms step_avg:40.62ms
step:694/2110 train_time:28205ms step_avg:40.64ms
step:695/2110 train_time:28263ms step_avg:40.67ms
step:696/2110 train_time:28321ms step_avg:40.69ms
step:697/2110 train_time:28380ms step_avg:40.72ms
step:698/2110 train_time:28438ms step_avg:40.74ms
step:699/2110 train_time:28496ms step_avg:40.77ms
step:700/2110 train_time:28553ms step_avg:40.79ms
step:701/2110 train_time:28612ms step_avg:40.82ms
step:702/2110 train_time:28670ms step_avg:40.84ms
step:703/2110 train_time:28729ms step_avg:40.87ms
step:704/2110 train_time:28788ms step_avg:40.89ms
step:705/2110 train_time:28852ms step_avg:40.93ms
step:706/2110 train_time:28913ms step_avg:40.95ms
step:707/2110 train_time:28976ms step_avg:40.98ms
step:708/2110 train_time:29035ms step_avg:41.01ms
step:709/2110 train_time:29096ms step_avg:41.04ms
step:710/2110 train_time:29155ms step_avg:41.06ms
step:711/2110 train_time:29215ms step_avg:41.09ms
step:712/2110 train_time:29273ms step_avg:41.11ms
step:713/2110 train_time:29332ms step_avg:41.14ms
step:714/2110 train_time:29389ms step_avg:41.16ms
step:715/2110 train_time:29448ms step_avg:41.19ms
step:716/2110 train_time:29505ms step_avg:41.21ms
step:717/2110 train_time:29564ms step_avg:41.23ms
step:718/2110 train_time:29623ms step_avg:41.26ms
step:719/2110 train_time:29682ms step_avg:41.28ms
step:720/2110 train_time:29741ms step_avg:41.31ms
step:721/2110 train_time:29802ms step_avg:41.33ms
step:722/2110 train_time:29863ms step_avg:41.36ms
step:723/2110 train_time:29925ms step_avg:41.39ms
step:724/2110 train_time:29985ms step_avg:41.42ms
step:725/2110 train_time:30046ms step_avg:41.44ms
step:726/2110 train_time:30105ms step_avg:41.47ms
step:727/2110 train_time:30166ms step_avg:41.49ms
step:728/2110 train_time:30224ms step_avg:41.52ms
step:729/2110 train_time:30283ms step_avg:41.54ms
step:730/2110 train_time:30342ms step_avg:41.56ms
step:731/2110 train_time:30401ms step_avg:41.59ms
step:732/2110 train_time:30459ms step_avg:41.61ms
step:733/2110 train_time:30518ms step_avg:41.63ms
step:734/2110 train_time:30576ms step_avg:41.66ms
step:735/2110 train_time:30634ms step_avg:41.68ms
step:736/2110 train_time:30692ms step_avg:41.70ms
step:737/2110 train_time:30752ms step_avg:41.73ms
step:738/2110 train_time:30810ms step_avg:41.75ms
step:739/2110 train_time:30871ms step_avg:41.77ms
step:740/2110 train_time:30930ms step_avg:41.80ms
step:741/2110 train_time:30991ms step_avg:41.82ms
step:742/2110 train_time:31050ms step_avg:41.85ms
step:743/2110 train_time:31111ms step_avg:41.87ms
step:744/2110 train_time:31170ms step_avg:41.90ms
step:745/2110 train_time:31230ms step_avg:41.92ms
step:746/2110 train_time:31288ms step_avg:41.94ms
step:747/2110 train_time:31347ms step_avg:41.96ms
step:748/2110 train_time:31406ms step_avg:41.99ms
step:749/2110 train_time:31465ms step_avg:42.01ms
step:750/2110 train_time:31524ms step_avg:42.03ms
step:750/2110 val_loss:3.8999 train_time:31585ms step_avg:42.11ms
step:751/2110 train_time:31625ms step_avg:42.11ms
step:752/2110 train_time:31664ms step_avg:42.11ms
step:753/2110 train_time:31707ms step_avg:42.11ms
step:754/2110 train_time:31767ms step_avg:42.13ms
step:755/2110 train_time:31827ms step_avg:42.16ms
step:756/2110 train_time:31886ms step_avg:42.18ms
step:757/2110 train_time:31945ms step_avg:42.20ms
step:758/2110 train_time:32003ms step_avg:42.22ms
step:759/2110 train_time:32062ms step_avg:42.24ms
step:760/2110 train_time:32120ms step_avg:42.26ms
step:761/2110 train_time:32179ms step_avg:42.29ms
step:762/2110 train_time:32237ms step_avg:42.31ms
step:763/2110 train_time:32296ms step_avg:42.33ms
step:764/2110 train_time:32354ms step_avg:42.35ms
step:765/2110 train_time:32413ms step_avg:42.37ms
step:766/2110 train_time:32471ms step_avg:42.39ms
step:767/2110 train_time:32533ms step_avg:42.42ms
step:768/2110 train_time:32592ms step_avg:42.44ms
step:769/2110 train_time:32654ms step_avg:42.46ms
step:770/2110 train_time:32713ms step_avg:42.48ms
step:771/2110 train_time:32774ms step_avg:42.51ms
step:772/2110 train_time:32834ms step_avg:42.53ms
step:773/2110 train_time:32894ms step_avg:42.55ms
step:774/2110 train_time:32953ms step_avg:42.58ms
step:775/2110 train_time:33014ms step_avg:42.60ms
step:776/2110 train_time:33072ms step_avg:42.62ms
step:777/2110 train_time:33132ms step_avg:42.64ms
step:778/2110 train_time:33190ms step_avg:42.66ms
step:779/2110 train_time:33249ms step_avg:42.68ms
step:780/2110 train_time:33308ms step_avg:42.70ms
step:781/2110 train_time:33367ms step_avg:42.72ms
step:782/2110 train_time:33426ms step_avg:42.74ms
step:783/2110 train_time:33486ms step_avg:42.77ms
step:784/2110 train_time:33544ms step_avg:42.79ms
step:785/2110 train_time:33605ms step_avg:42.81ms
step:786/2110 train_time:33664ms step_avg:42.83ms
step:787/2110 train_time:33725ms step_avg:42.85ms
step:788/2110 train_time:33783ms step_avg:42.87ms
step:789/2110 train_time:33844ms step_avg:42.90ms
step:790/2110 train_time:33904ms step_avg:42.92ms
step:791/2110 train_time:33964ms step_avg:42.94ms
step:792/2110 train_time:34022ms step_avg:42.96ms
step:793/2110 train_time:34082ms step_avg:42.98ms
step:794/2110 train_time:34140ms step_avg:43.00ms
step:795/2110 train_time:34200ms step_avg:43.02ms
step:796/2110 train_time:34257ms step_avg:43.04ms
step:797/2110 train_time:34317ms step_avg:43.06ms
step:798/2110 train_time:34375ms step_avg:43.08ms
step:799/2110 train_time:34434ms step_avg:43.10ms
step:800/2110 train_time:34493ms step_avg:43.12ms
step:801/2110 train_time:34553ms step_avg:43.14ms
step:802/2110 train_time:34612ms step_avg:43.16ms
step:803/2110 train_time:34672ms step_avg:43.18ms
step:804/2110 train_time:34732ms step_avg:43.20ms
step:805/2110 train_time:34792ms step_avg:43.22ms
step:806/2110 train_time:34851ms step_avg:43.24ms
step:807/2110 train_time:34910ms step_avg:43.26ms
step:808/2110 train_time:34969ms step_avg:43.28ms
step:809/2110 train_time:35029ms step_avg:43.30ms
step:810/2110 train_time:35088ms step_avg:43.32ms
step:811/2110 train_time:35148ms step_avg:43.34ms
step:812/2110 train_time:35207ms step_avg:43.36ms
step:813/2110 train_time:35266ms step_avg:43.38ms
step:814/2110 train_time:35324ms step_avg:43.40ms
step:815/2110 train_time:35384ms step_avg:43.42ms
step:816/2110 train_time:35442ms step_avg:43.43ms
step:817/2110 train_time:35502ms step_avg:43.45ms
step:818/2110 train_time:35560ms step_avg:43.47ms
step:819/2110 train_time:35620ms step_avg:43.49ms
step:820/2110 train_time:35678ms step_avg:43.51ms
step:821/2110 train_time:35739ms step_avg:43.53ms
step:822/2110 train_time:35798ms step_avg:43.55ms
step:823/2110 train_time:35859ms step_avg:43.57ms
step:824/2110 train_time:35917ms step_avg:43.59ms
step:825/2110 train_time:35977ms step_avg:43.61ms
step:826/2110 train_time:36036ms step_avg:43.63ms
step:827/2110 train_time:36096ms step_avg:43.65ms
step:828/2110 train_time:36155ms step_avg:43.67ms
step:829/2110 train_time:36215ms step_avg:43.68ms
step:830/2110 train_time:36273ms step_avg:43.70ms
step:831/2110 train_time:36333ms step_avg:43.72ms
step:832/2110 train_time:36391ms step_avg:43.74ms
step:833/2110 train_time:36451ms step_avg:43.76ms
step:834/2110 train_time:36511ms step_avg:43.78ms
step:835/2110 train_time:36570ms step_avg:43.80ms
step:836/2110 train_time:36628ms step_avg:43.81ms
step:837/2110 train_time:36689ms step_avg:43.83ms
step:838/2110 train_time:36747ms step_avg:43.85ms
step:839/2110 train_time:36808ms step_avg:43.87ms
step:840/2110 train_time:36867ms step_avg:43.89ms
step:841/2110 train_time:36927ms step_avg:43.91ms
step:842/2110 train_time:36985ms step_avg:43.93ms
step:843/2110 train_time:37045ms step_avg:43.94ms
step:844/2110 train_time:37103ms step_avg:43.96ms
step:845/2110 train_time:37164ms step_avg:43.98ms
step:846/2110 train_time:37222ms step_avg:44.00ms
step:847/2110 train_time:37281ms step_avg:44.02ms
step:848/2110 train_time:37339ms step_avg:44.03ms
step:849/2110 train_time:37399ms step_avg:44.05ms
step:850/2110 train_time:37457ms step_avg:44.07ms
step:851/2110 train_time:37517ms step_avg:44.09ms
step:852/2110 train_time:37575ms step_avg:44.10ms
step:853/2110 train_time:37635ms step_avg:44.12ms
step:854/2110 train_time:37693ms step_avg:44.14ms
step:855/2110 train_time:37754ms step_avg:44.16ms
step:856/2110 train_time:37813ms step_avg:44.17ms
step:857/2110 train_time:37873ms step_avg:44.19ms
step:858/2110 train_time:37931ms step_avg:44.21ms
step:859/2110 train_time:37992ms step_avg:44.23ms
step:860/2110 train_time:38051ms step_avg:44.24ms
step:861/2110 train_time:38110ms step_avg:44.26ms
step:862/2110 train_time:38169ms step_avg:44.28ms
step:863/2110 train_time:38229ms step_avg:44.30ms
step:864/2110 train_time:38288ms step_avg:44.31ms
step:865/2110 train_time:38348ms step_avg:44.33ms
step:866/2110 train_time:38407ms step_avg:44.35ms
step:867/2110 train_time:38466ms step_avg:44.37ms
step:868/2110 train_time:38525ms step_avg:44.38ms
step:869/2110 train_time:38585ms step_avg:44.40ms
step:870/2110 train_time:38643ms step_avg:44.42ms
step:871/2110 train_time:38704ms step_avg:44.44ms
step:872/2110 train_time:38763ms step_avg:44.45ms
step:873/2110 train_time:38823ms step_avg:44.47ms
step:874/2110 train_time:38882ms step_avg:44.49ms
step:875/2110 train_time:38942ms step_avg:44.51ms
step:876/2110 train_time:39000ms step_avg:44.52ms
step:877/2110 train_time:39060ms step_avg:44.54ms
step:878/2110 train_time:39118ms step_avg:44.55ms
step:879/2110 train_time:39178ms step_avg:44.57ms
step:880/2110 train_time:39236ms step_avg:44.59ms
step:881/2110 train_time:39296ms step_avg:44.60ms
step:882/2110 train_time:39355ms step_avg:44.62ms
step:883/2110 train_time:39415ms step_avg:44.64ms
step:884/2110 train_time:39474ms step_avg:44.65ms
step:885/2110 train_time:39533ms step_avg:44.67ms
step:886/2110 train_time:39592ms step_avg:44.69ms
step:887/2110 train_time:39652ms step_avg:44.70ms
step:888/2110 train_time:39711ms step_avg:44.72ms
step:889/2110 train_time:39770ms step_avg:44.74ms
step:890/2110 train_time:39830ms step_avg:44.75ms
step:891/2110 train_time:39889ms step_avg:44.77ms
step:892/2110 train_time:39947ms step_avg:44.78ms
step:893/2110 train_time:40007ms step_avg:44.80ms
step:894/2110 train_time:40067ms step_avg:44.82ms
step:895/2110 train_time:40127ms step_avg:44.83ms
step:896/2110 train_time:40185ms step_avg:44.85ms
step:897/2110 train_time:40244ms step_avg:44.87ms
step:898/2110 train_time:40302ms step_avg:44.88ms
step:899/2110 train_time:40363ms step_avg:44.90ms
step:900/2110 train_time:40422ms step_avg:44.91ms
step:901/2110 train_time:40480ms step_avg:44.93ms
step:902/2110 train_time:40539ms step_avg:44.94ms
step:903/2110 train_time:40600ms step_avg:44.96ms
step:904/2110 train_time:40658ms step_avg:44.98ms
step:905/2110 train_time:40718ms step_avg:44.99ms
step:906/2110 train_time:40776ms step_avg:45.01ms
step:907/2110 train_time:40836ms step_avg:45.02ms
step:908/2110 train_time:40895ms step_avg:45.04ms
step:909/2110 train_time:40955ms step_avg:45.05ms
step:910/2110 train_time:41014ms step_avg:45.07ms
step:911/2110 train_time:41074ms step_avg:45.09ms
step:912/2110 train_time:41132ms step_avg:45.10ms
step:913/2110 train_time:41192ms step_avg:45.12ms
step:914/2110 train_time:41251ms step_avg:45.13ms
step:915/2110 train_time:41311ms step_avg:45.15ms
step:916/2110 train_time:41370ms step_avg:45.16ms
step:917/2110 train_time:41430ms step_avg:45.18ms
step:918/2110 train_time:41489ms step_avg:45.19ms
step:919/2110 train_time:41549ms step_avg:45.21ms
step:920/2110 train_time:41609ms step_avg:45.23ms
step:921/2110 train_time:41669ms step_avg:45.24ms
step:922/2110 train_time:41728ms step_avg:45.26ms
step:923/2110 train_time:41787ms step_avg:45.27ms
step:924/2110 train_time:41846ms step_avg:45.29ms
step:925/2110 train_time:41906ms step_avg:45.30ms
step:926/2110 train_time:41964ms step_avg:45.32ms
step:927/2110 train_time:42025ms step_avg:45.33ms
step:928/2110 train_time:42083ms step_avg:45.35ms
step:929/2110 train_time:42143ms step_avg:45.36ms
step:930/2110 train_time:42201ms step_avg:45.38ms
step:931/2110 train_time:42260ms step_avg:45.39ms
step:932/2110 train_time:42318ms step_avg:45.41ms
step:933/2110 train_time:42379ms step_avg:45.42ms
step:934/2110 train_time:42438ms step_avg:45.44ms
step:935/2110 train_time:42498ms step_avg:45.45ms
step:936/2110 train_time:42557ms step_avg:45.47ms
step:937/2110 train_time:42617ms step_avg:45.48ms
step:938/2110 train_time:42675ms step_avg:45.50ms
step:939/2110 train_time:42735ms step_avg:45.51ms
step:940/2110 train_time:42793ms step_avg:45.52ms
step:941/2110 train_time:42853ms step_avg:45.54ms
step:942/2110 train_time:42913ms step_avg:45.55ms
step:943/2110 train_time:42972ms step_avg:45.57ms
step:944/2110 train_time:43031ms step_avg:45.58ms
step:945/2110 train_time:43091ms step_avg:45.60ms
step:946/2110 train_time:43150ms step_avg:45.61ms
step:947/2110 train_time:43210ms step_avg:45.63ms
step:948/2110 train_time:43268ms step_avg:45.64ms
step:949/2110 train_time:43328ms step_avg:45.66ms
step:950/2110 train_time:43387ms step_avg:45.67ms
step:951/2110 train_time:43447ms step_avg:45.69ms
step:952/2110 train_time:43505ms step_avg:45.70ms
step:953/2110 train_time:43566ms step_avg:45.71ms
step:954/2110 train_time:43624ms step_avg:45.73ms
step:955/2110 train_time:43684ms step_avg:45.74ms
step:956/2110 train_time:43743ms step_avg:45.76ms
step:957/2110 train_time:43804ms step_avg:45.77ms
step:958/2110 train_time:43863ms step_avg:45.79ms
step:959/2110 train_time:43922ms step_avg:45.80ms
step:960/2110 train_time:43981ms step_avg:45.81ms
step:961/2110 train_time:44041ms step_avg:45.83ms
step:962/2110 train_time:44099ms step_avg:45.84ms
step:963/2110 train_time:44159ms step_avg:45.86ms
step:964/2110 train_time:44218ms step_avg:45.87ms
step:965/2110 train_time:44277ms step_avg:45.88ms
step:966/2110 train_time:44335ms step_avg:45.90ms
step:967/2110 train_time:44396ms step_avg:45.91ms
step:968/2110 train_time:44455ms step_avg:45.92ms
step:969/2110 train_time:44515ms step_avg:45.94ms
step:970/2110 train_time:44574ms step_avg:45.95ms
step:971/2110 train_time:44634ms step_avg:45.97ms
step:972/2110 train_time:44692ms step_avg:45.98ms
step:973/2110 train_time:44753ms step_avg:45.99ms
step:974/2110 train_time:44812ms step_avg:46.01ms
step:975/2110 train_time:44872ms step_avg:46.02ms
step:976/2110 train_time:44931ms step_avg:46.04ms
step:977/2110 train_time:44990ms step_avg:46.05ms
step:978/2110 train_time:45049ms step_avg:46.06ms
step:979/2110 train_time:45109ms step_avg:46.08ms
step:980/2110 train_time:45167ms step_avg:46.09ms
step:981/2110 train_time:45228ms step_avg:46.10ms
step:982/2110 train_time:45286ms step_avg:46.12ms
step:983/2110 train_time:45346ms step_avg:46.13ms
step:984/2110 train_time:45405ms step_avg:46.14ms
step:985/2110 train_time:45466ms step_avg:46.16ms
step:986/2110 train_time:45525ms step_avg:46.17ms
step:987/2110 train_time:45584ms step_avg:46.18ms
step:988/2110 train_time:45642ms step_avg:46.20ms
step:989/2110 train_time:45703ms step_avg:46.21ms
step:990/2110 train_time:45761ms step_avg:46.22ms
step:991/2110 train_time:45821ms step_avg:46.24ms
step:992/2110 train_time:45879ms step_avg:46.25ms
step:993/2110 train_time:45939ms step_avg:46.26ms
step:994/2110 train_time:45998ms step_avg:46.28ms
step:995/2110 train_time:46058ms step_avg:46.29ms
step:996/2110 train_time:46117ms step_avg:46.30ms
step:997/2110 train_time:46175ms step_avg:46.31ms
step:998/2110 train_time:46234ms step_avg:46.33ms
step:999/2110 train_time:46294ms step_avg:46.34ms
step:1000/2110 train_time:46353ms step_avg:46.35ms
step:1000/2110 val_loss:3.7570 train_time:46415ms step_avg:46.41ms
step:1001/2110 train_time:46452ms step_avg:46.41ms
step:1002/2110 train_time:46491ms step_avg:46.40ms
step:1003/2110 train_time:46541ms step_avg:46.40ms
step:1004/2110 train_time:46602ms step_avg:46.42ms
step:1005/2110 train_time:46663ms step_avg:46.43ms
step:1006/2110 train_time:46723ms step_avg:46.44ms
step:1007/2110 train_time:46783ms step_avg:46.46ms
step:1008/2110 train_time:46840ms step_avg:46.47ms
step:1009/2110 train_time:46900ms step_avg:46.48ms
step:1010/2110 train_time:46958ms step_avg:46.49ms
step:1011/2110 train_time:47017ms step_avg:46.51ms
step:1012/2110 train_time:47075ms step_avg:46.52ms
step:1013/2110 train_time:47134ms step_avg:46.53ms
step:1014/2110 train_time:47192ms step_avg:46.54ms
step:1015/2110 train_time:47251ms step_avg:46.55ms
step:1016/2110 train_time:47308ms step_avg:46.56ms
step:1017/2110 train_time:47368ms step_avg:46.58ms
step:1018/2110 train_time:47427ms step_avg:46.59ms
step:1019/2110 train_time:47490ms step_avg:46.60ms
step:1020/2110 train_time:47551ms step_avg:46.62ms
step:1021/2110 train_time:47612ms step_avg:46.63ms
step:1022/2110 train_time:47672ms step_avg:46.65ms
step:1023/2110 train_time:47732ms step_avg:46.66ms
step:1024/2110 train_time:47790ms step_avg:46.67ms
step:1025/2110 train_time:47850ms step_avg:46.68ms
step:1026/2110 train_time:47908ms step_avg:46.69ms
step:1027/2110 train_time:47968ms step_avg:46.71ms
step:1028/2110 train_time:48026ms step_avg:46.72ms
step:1029/2110 train_time:48085ms step_avg:46.73ms
step:1030/2110 train_time:48142ms step_avg:46.74ms
step:1031/2110 train_time:48201ms step_avg:46.75ms
step:1032/2110 train_time:48260ms step_avg:46.76ms
step:1033/2110 train_time:48320ms step_avg:46.78ms
step:1034/2110 train_time:48380ms step_avg:46.79ms
step:1035/2110 train_time:48440ms step_avg:46.80ms
step:1036/2110 train_time:48499ms step_avg:46.81ms
step:1037/2110 train_time:48560ms step_avg:46.83ms
step:1038/2110 train_time:48620ms step_avg:46.84ms
step:1039/2110 train_time:48681ms step_avg:46.85ms
step:1040/2110 train_time:48739ms step_avg:46.86ms
step:1041/2110 train_time:48799ms step_avg:46.88ms
step:1042/2110 train_time:48858ms step_avg:46.89ms
step:1043/2110 train_time:48918ms step_avg:46.90ms
step:1044/2110 train_time:48976ms step_avg:46.91ms
step:1045/2110 train_time:49035ms step_avg:46.92ms
step:1046/2110 train_time:49092ms step_avg:46.93ms
step:1047/2110 train_time:49151ms step_avg:46.95ms
step:1048/2110 train_time:49209ms step_avg:46.96ms
step:1049/2110 train_time:49269ms step_avg:46.97ms
step:1050/2110 train_time:49327ms step_avg:46.98ms
step:1051/2110 train_time:49388ms step_avg:46.99ms
step:1052/2110 train_time:49447ms step_avg:47.00ms
step:1053/2110 train_time:49508ms step_avg:47.02ms
step:1054/2110 train_time:49568ms step_avg:47.03ms
step:1055/2110 train_time:49628ms step_avg:47.04ms
step:1056/2110 train_time:49686ms step_avg:47.05ms
step:1057/2110 train_time:49746ms step_avg:47.06ms
step:1058/2110 train_time:49804ms step_avg:47.07ms
step:1059/2110 train_time:49865ms step_avg:47.09ms
step:1060/2110 train_time:49924ms step_avg:47.10ms
step:1061/2110 train_time:49983ms step_avg:47.11ms
step:1062/2110 train_time:50041ms step_avg:47.12ms
step:1063/2110 train_time:50101ms step_avg:47.13ms
step:1064/2110 train_time:50160ms step_avg:47.14ms
step:1065/2110 train_time:50219ms step_avg:47.15ms
step:1066/2110 train_time:50277ms step_avg:47.16ms
step:1067/2110 train_time:50338ms step_avg:47.18ms
step:1068/2110 train_time:50396ms step_avg:47.19ms
step:1069/2110 train_time:50458ms step_avg:47.20ms
step:1070/2110 train_time:50516ms step_avg:47.21ms
step:1071/2110 train_time:50577ms step_avg:47.22ms
step:1072/2110 train_time:50636ms step_avg:47.24ms
step:1073/2110 train_time:50696ms step_avg:47.25ms
step:1074/2110 train_time:50755ms step_avg:47.26ms
step:1075/2110 train_time:50816ms step_avg:47.27ms
step:1076/2110 train_time:50875ms step_avg:47.28ms
step:1077/2110 train_time:50934ms step_avg:47.29ms
step:1078/2110 train_time:50992ms step_avg:47.30ms
step:1079/2110 train_time:51052ms step_avg:47.31ms
step:1080/2110 train_time:51110ms step_avg:47.32ms
step:1081/2110 train_time:51170ms step_avg:47.34ms
step:1082/2110 train_time:51228ms step_avg:47.35ms
step:1083/2110 train_time:51288ms step_avg:47.36ms
step:1084/2110 train_time:51346ms step_avg:47.37ms
step:1085/2110 train_time:51406ms step_avg:47.38ms
step:1086/2110 train_time:51465ms step_avg:47.39ms
step:1087/2110 train_time:51525ms step_avg:47.40ms
step:1088/2110 train_time:51583ms step_avg:47.41ms
step:1089/2110 train_time:51643ms step_avg:47.42ms
step:1090/2110 train_time:51702ms step_avg:47.43ms
step:1091/2110 train_time:51762ms step_avg:47.44ms
step:1092/2110 train_time:51821ms step_avg:47.46ms
step:1093/2110 train_time:51881ms step_avg:47.47ms
step:1094/2110 train_time:51939ms step_avg:47.48ms
step:1095/2110 train_time:51999ms step_avg:47.49ms
step:1096/2110 train_time:52057ms step_avg:47.50ms
step:1097/2110 train_time:52118ms step_avg:47.51ms
step:1098/2110 train_time:52176ms step_avg:47.52ms
step:1099/2110 train_time:52236ms step_avg:47.53ms
step:1100/2110 train_time:52294ms step_avg:47.54ms
step:1101/2110 train_time:52355ms step_avg:47.55ms
step:1102/2110 train_time:52414ms step_avg:47.56ms
step:1103/2110 train_time:52475ms step_avg:47.57ms
step:1104/2110 train_time:52532ms step_avg:47.58ms
step:1105/2110 train_time:52592ms step_avg:47.59ms
step:1106/2110 train_time:52651ms step_avg:47.60ms
step:1107/2110 train_time:52711ms step_avg:47.62ms
step:1108/2110 train_time:52770ms step_avg:47.63ms
step:1109/2110 train_time:52830ms step_avg:47.64ms
step:1110/2110 train_time:52889ms step_avg:47.65ms
step:1111/2110 train_time:52950ms step_avg:47.66ms
step:1112/2110 train_time:53008ms step_avg:47.67ms
step:1113/2110 train_time:53068ms step_avg:47.68ms
step:1114/2110 train_time:53127ms step_avg:47.69ms
step:1115/2110 train_time:53186ms step_avg:47.70ms
step:1116/2110 train_time:53244ms step_avg:47.71ms
step:1117/2110 train_time:53305ms step_avg:47.72ms
step:1118/2110 train_time:53364ms step_avg:47.73ms
step:1119/2110 train_time:53424ms step_avg:47.74ms
step:1120/2110 train_time:53483ms step_avg:47.75ms
step:1121/2110 train_time:53542ms step_avg:47.76ms
step:1122/2110 train_time:53601ms step_avg:47.77ms
step:1123/2110 train_time:53661ms step_avg:47.78ms
step:1124/2110 train_time:53720ms step_avg:47.79ms
step:1125/2110 train_time:53780ms step_avg:47.80ms
step:1126/2110 train_time:53838ms step_avg:47.81ms
step:1127/2110 train_time:53899ms step_avg:47.83ms
step:1128/2110 train_time:53957ms step_avg:47.83ms
step:1129/2110 train_time:54017ms step_avg:47.85ms
step:1130/2110 train_time:54076ms step_avg:47.85ms
step:1131/2110 train_time:54136ms step_avg:47.87ms
step:1132/2110 train_time:54194ms step_avg:47.87ms
step:1133/2110 train_time:54255ms step_avg:47.89ms
step:1134/2110 train_time:54313ms step_avg:47.90ms
step:1135/2110 train_time:54374ms step_avg:47.91ms
step:1136/2110 train_time:54432ms step_avg:47.92ms
step:1137/2110 train_time:54493ms step_avg:47.93ms
step:1138/2110 train_time:54550ms step_avg:47.94ms
step:1139/2110 train_time:54610ms step_avg:47.95ms
step:1140/2110 train_time:54669ms step_avg:47.96ms
step:1141/2110 train_time:54730ms step_avg:47.97ms
step:1142/2110 train_time:54789ms step_avg:47.98ms
step:1143/2110 train_time:54850ms step_avg:47.99ms
step:1144/2110 train_time:54909ms step_avg:48.00ms
step:1145/2110 train_time:54969ms step_avg:48.01ms
step:1146/2110 train_time:55028ms step_avg:48.02ms
step:1147/2110 train_time:55089ms step_avg:48.03ms
step:1148/2110 train_time:55148ms step_avg:48.04ms
step:1149/2110 train_time:55208ms step_avg:48.05ms
step:1150/2110 train_time:55268ms step_avg:48.06ms
step:1151/2110 train_time:55328ms step_avg:48.07ms
step:1152/2110 train_time:55387ms step_avg:48.08ms
step:1153/2110 train_time:55448ms step_avg:48.09ms
step:1154/2110 train_time:55507ms step_avg:48.10ms
step:1155/2110 train_time:55568ms step_avg:48.11ms
step:1156/2110 train_time:55627ms step_avg:48.12ms
step:1157/2110 train_time:55688ms step_avg:48.13ms
step:1158/2110 train_time:55747ms step_avg:48.14ms
step:1159/2110 train_time:55809ms step_avg:48.15ms
step:1160/2110 train_time:55867ms step_avg:48.16ms
step:1161/2110 train_time:55928ms step_avg:48.17ms
step:1162/2110 train_time:55987ms step_avg:48.18ms
step:1163/2110 train_time:56047ms step_avg:48.19ms
step:1164/2110 train_time:56105ms step_avg:48.20ms
step:1165/2110 train_time:56166ms step_avg:48.21ms
step:1166/2110 train_time:56225ms step_avg:48.22ms
step:1167/2110 train_time:56287ms step_avg:48.23ms
step:1168/2110 train_time:56347ms step_avg:48.24ms
step:1169/2110 train_time:56407ms step_avg:48.25ms
step:1170/2110 train_time:56467ms step_avg:48.26ms
step:1171/2110 train_time:56527ms step_avg:48.27ms
step:1172/2110 train_time:56586ms step_avg:48.28ms
step:1173/2110 train_time:56647ms step_avg:48.29ms
step:1174/2110 train_time:56706ms step_avg:48.30ms
step:1175/2110 train_time:56767ms step_avg:48.31ms
step:1176/2110 train_time:56827ms step_avg:48.32ms
step:1177/2110 train_time:56887ms step_avg:48.33ms
step:1178/2110 train_time:56946ms step_avg:48.34ms
step:1179/2110 train_time:57006ms step_avg:48.35ms
step:1180/2110 train_time:57065ms step_avg:48.36ms
step:1181/2110 train_time:57125ms step_avg:48.37ms
step:1182/2110 train_time:57184ms step_avg:48.38ms
step:1183/2110 train_time:57245ms step_avg:48.39ms
step:1184/2110 train_time:57304ms step_avg:48.40ms
step:1185/2110 train_time:57365ms step_avg:48.41ms
step:1186/2110 train_time:57424ms step_avg:48.42ms
step:1187/2110 train_time:57484ms step_avg:48.43ms
step:1188/2110 train_time:57543ms step_avg:48.44ms
step:1189/2110 train_time:57603ms step_avg:48.45ms
step:1190/2110 train_time:57663ms step_avg:48.46ms
step:1191/2110 train_time:57724ms step_avg:48.47ms
step:1192/2110 train_time:57783ms step_avg:48.48ms
step:1193/2110 train_time:57844ms step_avg:48.49ms
step:1194/2110 train_time:57903ms step_avg:48.50ms
step:1195/2110 train_time:57964ms step_avg:48.51ms
step:1196/2110 train_time:58023ms step_avg:48.51ms
step:1197/2110 train_time:58083ms step_avg:48.52ms
step:1198/2110 train_time:58142ms step_avg:48.53ms
step:1199/2110 train_time:58202ms step_avg:48.54ms
step:1200/2110 train_time:58262ms step_avg:48.55ms
step:1201/2110 train_time:58322ms step_avg:48.56ms
step:1202/2110 train_time:58382ms step_avg:48.57ms
step:1203/2110 train_time:58442ms step_avg:48.58ms
step:1204/2110 train_time:58501ms step_avg:48.59ms
step:1205/2110 train_time:58561ms step_avg:48.60ms
step:1206/2110 train_time:58621ms step_avg:48.61ms
step:1207/2110 train_time:58682ms step_avg:48.62ms
step:1208/2110 train_time:58742ms step_avg:48.63ms
step:1209/2110 train_time:58802ms step_avg:48.64ms
step:1210/2110 train_time:58861ms step_avg:48.65ms
step:1211/2110 train_time:58922ms step_avg:48.66ms
step:1212/2110 train_time:58981ms step_avg:48.66ms
step:1213/2110 train_time:59041ms step_avg:48.67ms
step:1214/2110 train_time:59100ms step_avg:48.68ms
step:1215/2110 train_time:59161ms step_avg:48.69ms
step:1216/2110 train_time:59220ms step_avg:48.70ms
step:1217/2110 train_time:59281ms step_avg:48.71ms
step:1218/2110 train_time:59341ms step_avg:48.72ms
step:1219/2110 train_time:59401ms step_avg:48.73ms
step:1220/2110 train_time:59460ms step_avg:48.74ms
step:1221/2110 train_time:59521ms step_avg:48.75ms
step:1222/2110 train_time:59580ms step_avg:48.76ms
step:1223/2110 train_time:59640ms step_avg:48.77ms
step:1224/2110 train_time:59699ms step_avg:48.77ms
step:1225/2110 train_time:59759ms step_avg:48.78ms
step:1226/2110 train_time:59819ms step_avg:48.79ms
step:1227/2110 train_time:59879ms step_avg:48.80ms
step:1228/2110 train_time:59939ms step_avg:48.81ms
step:1229/2110 train_time:60000ms step_avg:48.82ms
step:1230/2110 train_time:60059ms step_avg:48.83ms
step:1231/2110 train_time:60120ms step_avg:48.84ms
step:1232/2110 train_time:60179ms step_avg:48.85ms
step:1233/2110 train_time:60240ms step_avg:48.86ms
step:1234/2110 train_time:60299ms step_avg:48.86ms
step:1235/2110 train_time:60359ms step_avg:48.87ms
step:1236/2110 train_time:60418ms step_avg:48.88ms
step:1237/2110 train_time:60478ms step_avg:48.89ms
step:1238/2110 train_time:60537ms step_avg:48.90ms
step:1239/2110 train_time:60598ms step_avg:48.91ms
step:1240/2110 train_time:60657ms step_avg:48.92ms
step:1241/2110 train_time:60717ms step_avg:48.93ms
step:1242/2110 train_time:60777ms step_avg:48.93ms
step:1243/2110 train_time:60837ms step_avg:48.94ms
step:1244/2110 train_time:60895ms step_avg:48.95ms
step:1245/2110 train_time:60957ms step_avg:48.96ms
step:1246/2110 train_time:61016ms step_avg:48.97ms
step:1247/2110 train_time:61077ms step_avg:48.98ms
step:1248/2110 train_time:61136ms step_avg:48.99ms
step:1249/2110 train_time:61196ms step_avg:49.00ms
step:1250/2110 train_time:61255ms step_avg:49.00ms
step:1250/2110 val_loss:3.5910 train_time:61318ms step_avg:49.05ms
step:1251/2110 train_time:61358ms step_avg:49.05ms
step:1252/2110 train_time:61393ms step_avg:49.04ms
step:1253/2110 train_time:61441ms step_avg:49.04ms
step:1254/2110 train_time:61503ms step_avg:49.05ms
step:1255/2110 train_time:61564ms step_avg:49.05ms
step:1256/2110 train_time:61624ms step_avg:49.06ms
step:1257/2110 train_time:61682ms step_avg:49.07ms
step:1258/2110 train_time:61741ms step_avg:49.08ms
step:1259/2110 train_time:61801ms step_avg:49.09ms
step:1260/2110 train_time:61859ms step_avg:49.09ms
step:1261/2110 train_time:61918ms step_avg:49.10ms
step:1262/2110 train_time:61976ms step_avg:49.11ms
step:1263/2110 train_time:62035ms step_avg:49.12ms
step:1264/2110 train_time:62094ms step_avg:49.12ms
step:1265/2110 train_time:62154ms step_avg:49.13ms
step:1266/2110 train_time:62214ms step_avg:49.14ms
step:1267/2110 train_time:62274ms step_avg:49.15ms
step:1268/2110 train_time:62334ms step_avg:49.16ms
step:1269/2110 train_time:62397ms step_avg:49.17ms
step:1270/2110 train_time:62458ms step_avg:49.18ms
step:1271/2110 train_time:62521ms step_avg:49.19ms
step:1272/2110 train_time:62581ms step_avg:49.20ms
step:1273/2110 train_time:62641ms step_avg:49.21ms
step:1274/2110 train_time:62700ms step_avg:49.21ms
step:1275/2110 train_time:62761ms step_avg:49.22ms
step:1276/2110 train_time:62820ms step_avg:49.23ms
step:1277/2110 train_time:62878ms step_avg:49.24ms
step:1278/2110 train_time:62937ms step_avg:49.25ms
step:1279/2110 train_time:62996ms step_avg:49.25ms
step:1280/2110 train_time:63054ms step_avg:49.26ms
step:1281/2110 train_time:63114ms step_avg:49.27ms
step:1282/2110 train_time:63173ms step_avg:49.28ms
step:1283/2110 train_time:63234ms step_avg:49.29ms
step:1284/2110 train_time:63293ms step_avg:49.29ms
step:1285/2110 train_time:63354ms step_avg:49.30ms
step:1286/2110 train_time:63414ms step_avg:49.31ms
step:1287/2110 train_time:63476ms step_avg:49.32ms
step:1288/2110 train_time:63537ms step_avg:49.33ms
step:1289/2110 train_time:63598ms step_avg:49.34ms
step:1290/2110 train_time:63657ms step_avg:49.35ms
step:1291/2110 train_time:63717ms step_avg:49.35ms
step:1292/2110 train_time:63776ms step_avg:49.36ms
step:1293/2110 train_time:63835ms step_avg:49.37ms
step:1294/2110 train_time:63894ms step_avg:49.38ms
step:1295/2110 train_time:63953ms step_avg:49.38ms
step:1296/2110 train_time:64012ms step_avg:49.39ms
step:1297/2110 train_time:64071ms step_avg:49.40ms
step:1298/2110 train_time:64130ms step_avg:49.41ms
step:1299/2110 train_time:64189ms step_avg:49.41ms
step:1300/2110 train_time:64248ms step_avg:49.42ms
step:1301/2110 train_time:64309ms step_avg:49.43ms
step:1302/2110 train_time:64370ms step_avg:49.44ms
step:1303/2110 train_time:64431ms step_avg:49.45ms
step:1304/2110 train_time:64491ms step_avg:49.46ms
step:1305/2110 train_time:64553ms step_avg:49.47ms
step:1306/2110 train_time:64613ms step_avg:49.47ms
step:1307/2110 train_time:64673ms step_avg:49.48ms
step:1308/2110 train_time:64733ms step_avg:49.49ms
step:1309/2110 train_time:64793ms step_avg:49.50ms
step:1310/2110 train_time:64852ms step_avg:49.51ms
step:1311/2110 train_time:64912ms step_avg:49.51ms
step:1312/2110 train_time:64971ms step_avg:49.52ms
step:1313/2110 train_time:65030ms step_avg:49.53ms
step:1314/2110 train_time:65089ms step_avg:49.53ms
step:1315/2110 train_time:65148ms step_avg:49.54ms
step:1316/2110 train_time:65208ms step_avg:49.55ms
step:1317/2110 train_time:65267ms step_avg:49.56ms
step:1318/2110 train_time:65328ms step_avg:49.57ms
step:1319/2110 train_time:65387ms step_avg:49.57ms
step:1320/2110 train_time:65447ms step_avg:49.58ms
step:1321/2110 train_time:65508ms step_avg:49.59ms
step:1322/2110 train_time:65569ms step_avg:49.60ms
step:1323/2110 train_time:65630ms step_avg:49.61ms
step:1324/2110 train_time:65690ms step_avg:49.61ms
step:1325/2110 train_time:65749ms step_avg:49.62ms
step:1326/2110 train_time:65809ms step_avg:49.63ms
step:1327/2110 train_time:65868ms step_avg:49.64ms
step:1328/2110 train_time:65927ms step_avg:49.64ms
step:1329/2110 train_time:65986ms step_avg:49.65ms
step:1330/2110 train_time:66046ms step_avg:49.66ms
step:1331/2110 train_time:66105ms step_avg:49.67ms
step:1332/2110 train_time:66165ms step_avg:49.67ms
step:1333/2110 train_time:66224ms step_avg:49.68ms
step:1334/2110 train_time:66283ms step_avg:49.69ms
step:1335/2110 train_time:66344ms step_avg:49.70ms
step:1336/2110 train_time:66403ms step_avg:49.70ms
step:1337/2110 train_time:66465ms step_avg:49.71ms
step:1338/2110 train_time:66525ms step_avg:49.72ms
step:1339/2110 train_time:66585ms step_avg:49.73ms
step:1340/2110 train_time:66646ms step_avg:49.74ms
step:1341/2110 train_time:66706ms step_avg:49.74ms
step:1342/2110 train_time:66765ms step_avg:49.75ms
step:1343/2110 train_time:66825ms step_avg:49.76ms
step:1344/2110 train_time:66885ms step_avg:49.77ms
step:1345/2110 train_time:66943ms step_avg:49.77ms
step:1346/2110 train_time:67003ms step_avg:49.78ms
step:1347/2110 train_time:67062ms step_avg:49.79ms
step:1348/2110 train_time:67121ms step_avg:49.79ms
step:1349/2110 train_time:67180ms step_avg:49.80ms
step:1350/2110 train_time:67240ms step_avg:49.81ms
step:1351/2110 train_time:67299ms step_avg:49.81ms
step:1352/2110 train_time:67358ms step_avg:49.82ms
step:1353/2110 train_time:67419ms step_avg:49.83ms
step:1354/2110 train_time:67478ms step_avg:49.84ms
step:1355/2110 train_time:67538ms step_avg:49.84ms
step:1356/2110 train_time:67598ms step_avg:49.85ms
step:1357/2110 train_time:67659ms step_avg:49.86ms
step:1358/2110 train_time:67718ms step_avg:49.87ms
step:1359/2110 train_time:67778ms step_avg:49.87ms
step:1360/2110 train_time:67838ms step_avg:49.88ms
step:1361/2110 train_time:67898ms step_avg:49.89ms
step:1362/2110 train_time:67957ms step_avg:49.89ms
step:1363/2110 train_time:68016ms step_avg:49.90ms
step:1364/2110 train_time:68075ms step_avg:49.91ms
step:1365/2110 train_time:68135ms step_avg:49.92ms
step:1366/2110 train_time:68194ms step_avg:49.92ms
step:1367/2110 train_time:68255ms step_avg:49.93ms
step:1368/2110 train_time:68314ms step_avg:49.94ms
step:1369/2110 train_time:68374ms step_avg:49.94ms
step:1370/2110 train_time:68433ms step_avg:49.95ms
step:1371/2110 train_time:68493ms step_avg:49.96ms
step:1372/2110 train_time:68553ms step_avg:49.97ms
step:1373/2110 train_time:68614ms step_avg:49.97ms
step:1374/2110 train_time:68674ms step_avg:49.98ms
step:1375/2110 train_time:68736ms step_avg:49.99ms
step:1376/2110 train_time:68795ms step_avg:50.00ms
step:1377/2110 train_time:68856ms step_avg:50.00ms
step:1378/2110 train_time:68914ms step_avg:50.01ms
step:1379/2110 train_time:68975ms step_avg:50.02ms
step:1380/2110 train_time:69034ms step_avg:50.02ms
step:1381/2110 train_time:69095ms step_avg:50.03ms
step:1382/2110 train_time:69181ms step_avg:50.06ms
step:1383/2110 train_time:69268ms step_avg:50.09ms
step:1384/2110 train_time:69354ms step_avg:50.11ms
step:1385/2110 train_time:69442ms step_avg:50.14ms
step:1386/2110 train_time:69528ms step_avg:50.16ms
step:1387/2110 train_time:69615ms step_avg:50.19ms
step:1388/2110 train_time:69702ms step_avg:50.22ms
step:1389/2110 train_time:69789ms step_avg:50.24ms
step:1390/2110 train_time:69875ms step_avg:50.27ms
step:1391/2110 train_time:69962ms step_avg:50.30ms
step:1392/2110 train_time:70047ms step_avg:50.32ms
step:1393/2110 train_time:70134ms step_avg:50.35ms
step:1394/2110 train_time:70220ms step_avg:50.37ms
step:1395/2110 train_time:70307ms step_avg:50.40ms
step:1396/2110 train_time:70393ms step_avg:50.42ms
step:1397/2110 train_time:70479ms step_avg:50.45ms
step:1398/2110 train_time:70566ms step_avg:50.48ms
step:1399/2110 train_time:70654ms step_avg:50.50ms
step:1400/2110 train_time:70739ms step_avg:50.53ms
step:1401/2110 train_time:70827ms step_avg:50.55ms
step:1402/2110 train_time:70914ms step_avg:50.58ms
step:1403/2110 train_time:71000ms step_avg:50.61ms
step:1404/2110 train_time:71086ms step_avg:50.63ms
step:1405/2110 train_time:71173ms step_avg:50.66ms
step:1406/2110 train_time:71259ms step_avg:50.68ms
step:1407/2110 train_time:71345ms step_avg:50.71ms
step:1408/2110 train_time:71431ms step_avg:50.73ms
step:1409/2110 train_time:71519ms step_avg:50.76ms
step:1410/2110 train_time:71604ms step_avg:50.78ms
step:1411/2110 train_time:71692ms step_avg:50.81ms
step:1412/2110 train_time:71779ms step_avg:50.83ms
step:1413/2110 train_time:71865ms step_avg:50.86ms
step:1414/2110 train_time:71951ms step_avg:50.88ms
step:1415/2110 train_time:72038ms step_avg:50.91ms
step:1416/2110 train_time:72124ms step_avg:50.93ms
step:1417/2110 train_time:72211ms step_avg:50.96ms
step:1418/2110 train_time:72297ms step_avg:50.98ms
step:1419/2110 train_time:72384ms step_avg:51.01ms
step:1420/2110 train_time:72470ms step_avg:51.04ms
step:1421/2110 train_time:72557ms step_avg:51.06ms
step:1422/2110 train_time:72644ms step_avg:51.09ms
step:1423/2110 train_time:72730ms step_avg:51.11ms
step:1424/2110 train_time:72818ms step_avg:51.14ms
step:1425/2110 train_time:72905ms step_avg:51.16ms
step:1426/2110 train_time:72991ms step_avg:51.19ms
step:1427/2110 train_time:73078ms step_avg:51.21ms
step:1428/2110 train_time:73165ms step_avg:51.24ms
step:1429/2110 train_time:73251ms step_avg:51.26ms
step:1430/2110 train_time:73337ms step_avg:51.28ms
step:1431/2110 train_time:73424ms step_avg:51.31ms
step:1432/2110 train_time:73510ms step_avg:51.33ms
step:1433/2110 train_time:73596ms step_avg:51.36ms
step:1434/2110 train_time:73683ms step_avg:51.38ms
step:1435/2110 train_time:73770ms step_avg:51.41ms
step:1436/2110 train_time:73857ms step_avg:51.43ms
step:1437/2110 train_time:73943ms step_avg:51.46ms
step:1438/2110 train_time:74029ms step_avg:51.48ms
step:1439/2110 train_time:74116ms step_avg:51.51ms
step:1440/2110 train_time:74203ms step_avg:51.53ms
step:1441/2110 train_time:74290ms step_avg:51.55ms
step:1442/2110 train_time:74376ms step_avg:51.58ms
step:1443/2110 train_time:74464ms step_avg:51.60ms
step:1444/2110 train_time:74550ms step_avg:51.63ms
step:1445/2110 train_time:74637ms step_avg:51.65ms
step:1446/2110 train_time:74722ms step_avg:51.68ms
step:1447/2110 train_time:74810ms step_avg:51.70ms
step:1448/2110 train_time:74897ms step_avg:51.72ms
step:1449/2110 train_time:74984ms step_avg:51.75ms
step:1450/2110 train_time:75070ms step_avg:51.77ms
step:1451/2110 train_time:75157ms step_avg:51.80ms
step:1452/2110 train_time:75244ms step_avg:51.82ms
step:1453/2110 train_time:75333ms step_avg:51.85ms
step:1454/2110 train_time:75419ms step_avg:51.87ms
step:1455/2110 train_time:75505ms step_avg:51.89ms
step:1456/2110 train_time:75591ms step_avg:51.92ms
step:1457/2110 train_time:75678ms step_avg:51.94ms
step:1458/2110 train_time:75764ms step_avg:51.96ms
step:1459/2110 train_time:75851ms step_avg:51.99ms
step:1460/2110 train_time:75937ms step_avg:52.01ms
step:1461/2110 train_time:76023ms step_avg:52.04ms
step:1462/2110 train_time:76109ms step_avg:52.06ms
step:1463/2110 train_time:76196ms step_avg:52.08ms
step:1464/2110 train_time:76283ms step_avg:52.11ms
step:1465/2110 train_time:76370ms step_avg:52.13ms
step:1466/2110 train_time:76457ms step_avg:52.15ms
step:1467/2110 train_time:76543ms step_avg:52.18ms
step:1468/2110 train_time:76629ms step_avg:52.20ms
step:1469/2110 train_time:76716ms step_avg:52.22ms
step:1470/2110 train_time:76804ms step_avg:52.25ms
step:1471/2110 train_time:76890ms step_avg:52.27ms
step:1472/2110 train_time:76977ms step_avg:52.29ms
step:1473/2110 train_time:77063ms step_avg:52.32ms
step:1474/2110 train_time:77149ms step_avg:52.34ms
step:1475/2110 train_time:77236ms step_avg:52.36ms
step:1476/2110 train_time:77321ms step_avg:52.39ms
step:1477/2110 train_time:77409ms step_avg:52.41ms
step:1478/2110 train_time:77496ms step_avg:52.43ms
step:1479/2110 train_time:77583ms step_avg:52.46ms
step:1480/2110 train_time:77668ms step_avg:52.48ms
step:1481/2110 train_time:77755ms step_avg:52.50ms
step:1482/2110 train_time:77842ms step_avg:52.52ms
step:1483/2110 train_time:77928ms step_avg:52.55ms
step:1484/2110 train_time:78014ms step_avg:52.57ms
step:1485/2110 train_time:78101ms step_avg:52.59ms
step:1486/2110 train_time:78187ms step_avg:52.62ms
step:1487/2110 train_time:78275ms step_avg:52.64ms
step:1488/2110 train_time:78361ms step_avg:52.66ms
step:1489/2110 train_time:78448ms step_avg:52.69ms
step:1490/2110 train_time:78535ms step_avg:52.71ms
step:1491/2110 train_time:78622ms step_avg:52.73ms
step:1492/2110 train_time:78708ms step_avg:52.75ms
step:1493/2110 train_time:78795ms step_avg:52.78ms
step:1494/2110 train_time:78881ms step_avg:52.80ms
step:1495/2110 train_time:78967ms step_avg:52.82ms
step:1496/2110 train_time:79054ms step_avg:52.84ms
step:1497/2110 train_time:79140ms step_avg:52.87ms
step:1498/2110 train_time:79226ms step_avg:52.89ms
step:1499/2110 train_time:79313ms step_avg:52.91ms
step:1500/2110 train_time:79399ms step_avg:52.93ms
step:1500/2110 val_loss:3.4942 train_time:79488ms step_avg:52.99ms
step:1501/2110 train_time:79519ms step_avg:52.98ms
step:1502/2110 train_time:79577ms step_avg:52.98ms
step:1503/2110 train_time:79671ms step_avg:53.01ms
step:1504/2110 train_time:79761ms step_avg:53.03ms
step:1505/2110 train_time:79849ms step_avg:53.06ms
step:1506/2110 train_time:79934ms step_avg:53.08ms
step:1507/2110 train_time:80021ms step_avg:53.10ms
step:1508/2110 train_time:80106ms step_avg:53.12ms
step:1509/2110 train_time:80191ms step_avg:53.14ms
step:1510/2110 train_time:80277ms step_avg:53.16ms
step:1511/2110 train_time:80363ms step_avg:53.19ms
step:1512/2110 train_time:80450ms step_avg:53.21ms
step:1513/2110 train_time:80540ms step_avg:53.23ms
step:1514/2110 train_time:80629ms step_avg:53.26ms
step:1515/2110 train_time:80717ms step_avg:53.28ms
step:1516/2110 train_time:80803ms step_avg:53.30ms
step:1517/2110 train_time:80891ms step_avg:53.32ms
step:1518/2110 train_time:80977ms step_avg:53.34ms
step:1519/2110 train_time:81064ms step_avg:53.37ms
step:1520/2110 train_time:81149ms step_avg:53.39ms
step:1521/2110 train_time:81235ms step_avg:53.41ms
step:1522/2110 train_time:81320ms step_avg:53.43ms
step:1523/2110 train_time:81407ms step_avg:53.45ms
step:1524/2110 train_time:81494ms step_avg:53.47ms
step:1525/2110 train_time:81584ms step_avg:53.50ms
step:1526/2110 train_time:81671ms step_avg:53.52ms
step:1527/2110 train_time:81760ms step_avg:53.54ms
step:1528/2110 train_time:81845ms step_avg:53.56ms
step:1529/2110 train_time:81933ms step_avg:53.59ms
step:1530/2110 train_time:82019ms step_avg:53.61ms
step:1531/2110 train_time:82105ms step_avg:53.63ms
step:1532/2110 train_time:82190ms step_avg:53.65ms
step:1533/2110 train_time:82277ms step_avg:53.67ms
step:1534/2110 train_time:82363ms step_avg:53.69ms
step:1535/2110 train_time:82450ms step_avg:53.71ms
step:1536/2110 train_time:82536ms step_avg:53.73ms
step:1537/2110 train_time:82624ms step_avg:53.76ms
step:1538/2110 train_time:82711ms step_avg:53.78ms
step:1539/2110 train_time:82799ms step_avg:53.80ms
step:1540/2110 train_time:82885ms step_avg:53.82ms
step:1541/2110 train_time:82973ms step_avg:53.84ms
step:1542/2110 train_time:83059ms step_avg:53.86ms
step:1543/2110 train_time:83145ms step_avg:53.89ms
step:1544/2110 train_time:83230ms step_avg:53.91ms
step:1545/2110 train_time:83317ms step_avg:53.93ms
step:1546/2110 train_time:83404ms step_avg:53.95ms
step:1547/2110 train_time:83490ms step_avg:53.97ms
step:1548/2110 train_time:83578ms step_avg:53.99ms
step:1549/2110 train_time:83665ms step_avg:54.01ms
step:1550/2110 train_time:83750ms step_avg:54.03ms
step:1551/2110 train_time:83838ms step_avg:54.05ms
step:1552/2110 train_time:83925ms step_avg:54.08ms
step:1553/2110 train_time:84012ms step_avg:54.10ms
step:1554/2110 train_time:84098ms step_avg:54.12ms
step:1555/2110 train_time:84185ms step_avg:54.14ms
step:1556/2110 train_time:84270ms step_avg:54.16ms
step:1557/2110 train_time:84357ms step_avg:54.18ms
step:1558/2110 train_time:84443ms step_avg:54.20ms
step:1559/2110 train_time:84530ms step_avg:54.22ms
step:1560/2110 train_time:84616ms step_avg:54.24ms
step:1561/2110 train_time:84704ms step_avg:54.26ms
step:1562/2110 train_time:84790ms step_avg:54.28ms
step:1563/2110 train_time:84877ms step_avg:54.30ms
step:1564/2110 train_time:84963ms step_avg:54.32ms
step:1565/2110 train_time:85049ms step_avg:54.34ms
step:1566/2110 train_time:85136ms step_avg:54.36ms
step:1567/2110 train_time:85222ms step_avg:54.39ms
step:1568/2110 train_time:85308ms step_avg:54.41ms
step:1569/2110 train_time:85395ms step_avg:54.43ms
step:1570/2110 train_time:85483ms step_avg:54.45ms
step:1571/2110 train_time:85570ms step_avg:54.47ms
step:1572/2110 train_time:85655ms step_avg:54.49ms
step:1573/2110 train_time:85744ms step_avg:54.51ms
step:1574/2110 train_time:85830ms step_avg:54.53ms
step:1575/2110 train_time:85918ms step_avg:54.55ms
step:1576/2110 train_time:86004ms step_avg:54.57ms
step:1577/2110 train_time:86090ms step_avg:54.59ms
step:1578/2110 train_time:86177ms step_avg:54.61ms
step:1579/2110 train_time:86264ms step_avg:54.63ms
step:1580/2110 train_time:86350ms step_avg:54.65ms
step:1581/2110 train_time:86438ms step_avg:54.67ms
step:1582/2110 train_time:86525ms step_avg:54.69ms
step:1583/2110 train_time:86612ms step_avg:54.71ms
step:1584/2110 train_time:86698ms step_avg:54.73ms
step:1585/2110 train_time:86785ms step_avg:54.75ms
step:1586/2110 train_time:86872ms step_avg:54.77ms
step:1587/2110 train_time:86959ms step_avg:54.79ms
step:1588/2110 train_time:87045ms step_avg:54.81ms
step:1589/2110 train_time:87132ms step_avg:54.83ms
step:1590/2110 train_time:87218ms step_avg:54.85ms
step:1591/2110 train_time:87305ms step_avg:54.87ms
step:1592/2110 train_time:87392ms step_avg:54.89ms
step:1593/2110 train_time:87479ms step_avg:54.91ms
step:1594/2110 train_time:87565ms step_avg:54.93ms
step:1595/2110 train_time:87652ms step_avg:54.95ms
step:1596/2110 train_time:87738ms step_avg:54.97ms
step:1597/2110 train_time:87826ms step_avg:54.99ms
step:1598/2110 train_time:87911ms step_avg:55.01ms
step:1599/2110 train_time:87999ms step_avg:55.03ms
step:1600/2110 train_time:88085ms step_avg:55.05ms
step:1601/2110 train_time:88172ms step_avg:55.07ms
step:1602/2110 train_time:88258ms step_avg:55.09ms
step:1603/2110 train_time:88345ms step_avg:55.11ms
step:1604/2110 train_time:88430ms step_avg:55.13ms
step:1605/2110 train_time:88518ms step_avg:55.15ms
step:1606/2110 train_time:88604ms step_avg:55.17ms
step:1607/2110 train_time:88692ms step_avg:55.19ms
step:1608/2110 train_time:88779ms step_avg:55.21ms
step:1609/2110 train_time:88865ms step_avg:55.23ms
step:1610/2110 train_time:88951ms step_avg:55.25ms
step:1611/2110 train_time:89038ms step_avg:55.27ms
step:1612/2110 train_time:89124ms step_avg:55.29ms
step:1613/2110 train_time:89211ms step_avg:55.31ms
step:1614/2110 train_time:89298ms step_avg:55.33ms
step:1615/2110 train_time:89386ms step_avg:55.35ms
step:1616/2110 train_time:89472ms step_avg:55.37ms
step:1617/2110 train_time:89559ms step_avg:55.39ms
step:1618/2110 train_time:89645ms step_avg:55.41ms
step:1619/2110 train_time:89733ms step_avg:55.43ms
step:1620/2110 train_time:89820ms step_avg:55.44ms
step:1621/2110 train_time:89908ms step_avg:55.46ms
step:1622/2110 train_time:89994ms step_avg:55.48ms
step:1623/2110 train_time:90082ms step_avg:55.50ms
step:1624/2110 train_time:90167ms step_avg:55.52ms
step:1625/2110 train_time:90254ms step_avg:55.54ms
step:1626/2110 train_time:90341ms step_avg:55.56ms
step:1627/2110 train_time:90428ms step_avg:55.58ms
step:1628/2110 train_time:90514ms step_avg:55.60ms
step:1629/2110 train_time:90601ms step_avg:55.62ms
step:1630/2110 train_time:90687ms step_avg:55.64ms
step:1631/2110 train_time:90775ms step_avg:55.66ms
step:1632/2110 train_time:90862ms step_avg:55.68ms
step:1633/2110 train_time:90949ms step_avg:55.69ms
step:1634/2110 train_time:91036ms step_avg:55.71ms
step:1635/2110 train_time:91123ms step_avg:55.73ms
step:1636/2110 train_time:91208ms step_avg:55.75ms
step:1637/2110 train_time:91296ms step_avg:55.77ms
step:1638/2110 train_time:91382ms step_avg:55.79ms
step:1639/2110 train_time:91469ms step_avg:55.81ms
step:1640/2110 train_time:91556ms step_avg:55.83ms
step:1641/2110 train_time:91643ms step_avg:55.85ms
step:1642/2110 train_time:91728ms step_avg:55.86ms
step:1643/2110 train_time:91817ms step_avg:55.88ms
step:1644/2110 train_time:91902ms step_avg:55.90ms
step:1645/2110 train_time:91990ms step_avg:55.92ms
step:1646/2110 train_time:92076ms step_avg:55.94ms
step:1647/2110 train_time:92164ms step_avg:55.96ms
step:1648/2110 train_time:92250ms step_avg:55.98ms
step:1649/2110 train_time:92337ms step_avg:56.00ms
step:1650/2110 train_time:92424ms step_avg:56.01ms
step:1651/2110 train_time:92511ms step_avg:56.03ms
step:1652/2110 train_time:92597ms step_avg:56.05ms
step:1653/2110 train_time:92684ms step_avg:56.07ms
step:1654/2110 train_time:92770ms step_avg:56.09ms
step:1655/2110 train_time:92858ms step_avg:56.11ms
step:1656/2110 train_time:92945ms step_avg:56.13ms
step:1657/2110 train_time:93033ms step_avg:56.15ms
step:1658/2110 train_time:93120ms step_avg:56.16ms
step:1659/2110 train_time:93209ms step_avg:56.18ms
step:1660/2110 train_time:93297ms step_avg:56.20ms
step:1661/2110 train_time:93386ms step_avg:56.22ms
step:1662/2110 train_time:93472ms step_avg:56.24ms
step:1663/2110 train_time:93561ms step_avg:56.26ms
step:1664/2110 train_time:93648ms step_avg:56.28ms
step:1665/2110 train_time:93737ms step_avg:56.30ms
step:1666/2110 train_time:93825ms step_avg:56.32ms
step:1667/2110 train_time:93915ms step_avg:56.34ms
step:1668/2110 train_time:94001ms step_avg:56.36ms
step:1669/2110 train_time:94090ms step_avg:56.38ms
step:1670/2110 train_time:94177ms step_avg:56.39ms
step:1671/2110 train_time:94265ms step_avg:56.41ms
step:1672/2110 train_time:94352ms step_avg:56.43ms
step:1673/2110 train_time:94442ms step_avg:56.45ms
step:1674/2110 train_time:94528ms step_avg:56.47ms
step:1675/2110 train_time:94618ms step_avg:56.49ms
step:1676/2110 train_time:94705ms step_avg:56.51ms
step:1677/2110 train_time:94793ms step_avg:56.53ms
step:1678/2110 train_time:94880ms step_avg:56.54ms
step:1679/2110 train_time:94969ms step_avg:56.56ms
step:1680/2110 train_time:95057ms step_avg:56.58ms
step:1681/2110 train_time:95146ms step_avg:56.60ms
step:1682/2110 train_time:95233ms step_avg:56.62ms
step:1683/2110 train_time:95322ms step_avg:56.64ms
step:1684/2110 train_time:95409ms step_avg:56.66ms
step:1685/2110 train_time:95498ms step_avg:56.68ms
step:1686/2110 train_time:95585ms step_avg:56.69ms
step:1687/2110 train_time:95675ms step_avg:56.71ms
step:1688/2110 train_time:95761ms step_avg:56.73ms
step:1689/2110 train_time:95850ms step_avg:56.75ms
step:1690/2110 train_time:95937ms step_avg:56.77ms
step:1691/2110 train_time:96026ms step_avg:56.79ms
step:1692/2110 train_time:96113ms step_avg:56.80ms
step:1693/2110 train_time:96202ms step_avg:56.82ms
step:1694/2110 train_time:96290ms step_avg:56.84ms
step:1695/2110 train_time:96379ms step_avg:56.86ms
step:1696/2110 train_time:96466ms step_avg:56.88ms
step:1697/2110 train_time:96554ms step_avg:56.90ms
step:1698/2110 train_time:96641ms step_avg:56.91ms
step:1699/2110 train_time:96730ms step_avg:56.93ms
step:1700/2110 train_time:96817ms step_avg:56.95ms
step:1701/2110 train_time:96906ms step_avg:56.97ms
step:1702/2110 train_time:96993ms step_avg:56.99ms
step:1703/2110 train_time:97082ms step_avg:57.01ms
step:1704/2110 train_time:97170ms step_avg:57.02ms
step:1705/2110 train_time:97259ms step_avg:57.04ms
step:1706/2110 train_time:97347ms step_avg:57.06ms
step:1707/2110 train_time:97435ms step_avg:57.08ms
step:1708/2110 train_time:97522ms step_avg:57.10ms
step:1709/2110 train_time:97609ms step_avg:57.11ms
step:1710/2110 train_time:97697ms step_avg:57.13ms
step:1711/2110 train_time:97785ms step_avg:57.15ms
step:1712/2110 train_time:97872ms step_avg:57.17ms
step:1713/2110 train_time:97960ms step_avg:57.19ms
step:1714/2110 train_time:98048ms step_avg:57.20ms
step:1715/2110 train_time:98137ms step_avg:57.22ms
step:1716/2110 train_time:98224ms step_avg:57.24ms
step:1717/2110 train_time:98314ms step_avg:57.26ms
step:1718/2110 train_time:98401ms step_avg:57.28ms
step:1719/2110 train_time:98490ms step_avg:57.29ms
step:1720/2110 train_time:98577ms step_avg:57.31ms
step:1721/2110 train_time:98665ms step_avg:57.33ms
step:1722/2110 train_time:98752ms step_avg:57.35ms
step:1723/2110 train_time:98841ms step_avg:57.37ms
step:1724/2110 train_time:98929ms step_avg:57.38ms
step:1725/2110 train_time:99018ms step_avg:57.40ms
step:1726/2110 train_time:99106ms step_avg:57.42ms
step:1727/2110 train_time:99194ms step_avg:57.44ms
step:1728/2110 train_time:99282ms step_avg:57.45ms
step:1729/2110 train_time:99370ms step_avg:57.47ms
step:1730/2110 train_time:99459ms step_avg:57.49ms
step:1731/2110 train_time:99547ms step_avg:57.51ms
step:1732/2110 train_time:99633ms step_avg:57.52ms
step:1733/2110 train_time:99721ms step_avg:57.54ms
step:1734/2110 train_time:99809ms step_avg:57.56ms
step:1735/2110 train_time:99897ms step_avg:57.58ms
step:1736/2110 train_time:99985ms step_avg:57.59ms
step:1737/2110 train_time:100073ms step_avg:57.61ms
step:1738/2110 train_time:100160ms step_avg:57.63ms
step:1739/2110 train_time:100248ms step_avg:57.65ms
step:1740/2110 train_time:100336ms step_avg:57.66ms
step:1741/2110 train_time:100425ms step_avg:57.68ms
step:1742/2110 train_time:100513ms step_avg:57.70ms
step:1743/2110 train_time:100601ms step_avg:57.72ms
step:1744/2110 train_time:100688ms step_avg:57.73ms
step:1745/2110 train_time:100777ms step_avg:57.75ms
step:1746/2110 train_time:100865ms step_avg:57.77ms
step:1747/2110 train_time:100953ms step_avg:57.79ms
step:1748/2110 train_time:101040ms step_avg:57.80ms
step:1749/2110 train_time:101129ms step_avg:57.82ms
step:1750/2110 train_time:101216ms step_avg:57.84ms
step:1750/2110 val_loss:3.3783 train_time:101306ms step_avg:57.89ms
step:1751/2110 train_time:101337ms step_avg:57.87ms
step:1752/2110 train_time:101400ms step_avg:57.88ms
step:1753/2110 train_time:101492ms step_avg:57.90ms
step:1754/2110 train_time:101580ms step_avg:57.91ms
step:1755/2110 train_time:101669ms step_avg:57.93ms
step:1756/2110 train_time:101755ms step_avg:57.95ms
step:1757/2110 train_time:101842ms step_avg:57.96ms
step:1758/2110 train_time:101928ms step_avg:57.98ms
step:1759/2110 train_time:102015ms step_avg:58.00ms
step:1760/2110 train_time:102101ms step_avg:58.01ms
step:1761/2110 train_time:102188ms step_avg:58.03ms
step:1762/2110 train_time:102276ms step_avg:58.05ms
step:1763/2110 train_time:102369ms step_avg:58.07ms
step:1764/2110 train_time:102458ms step_avg:58.08ms
step:1765/2110 train_time:102548ms step_avg:58.10ms
step:1766/2110 train_time:102636ms step_avg:58.12ms
step:1767/2110 train_time:102723ms step_avg:58.13ms
step:1768/2110 train_time:102811ms step_avg:58.15ms
step:1769/2110 train_time:102898ms step_avg:58.17ms
step:1770/2110 train_time:102984ms step_avg:58.18ms
step:1771/2110 train_time:103071ms step_avg:58.20ms
step:1772/2110 train_time:103157ms step_avg:58.22ms
step:1773/2110 train_time:103248ms step_avg:58.23ms
step:1774/2110 train_time:103336ms step_avg:58.25ms
step:1775/2110 train_time:103426ms step_avg:58.27ms
step:1776/2110 train_time:103515ms step_avg:58.29ms
step:1777/2110 train_time:103606ms step_avg:58.30ms
step:1778/2110 train_time:103692ms step_avg:58.32ms
step:1779/2110 train_time:103780ms step_avg:58.34ms
step:1780/2110 train_time:103867ms step_avg:58.35ms
step:1781/2110 train_time:103955ms step_avg:58.37ms
step:1782/2110 train_time:104041ms step_avg:58.38ms
step:1783/2110 train_time:104129ms step_avg:58.40ms
step:1784/2110 train_time:104216ms step_avg:58.42ms
step:1785/2110 train_time:104305ms step_avg:58.43ms
step:1786/2110 train_time:104393ms step_avg:58.45ms
step:1787/2110 train_time:104484ms step_avg:58.47ms
step:1788/2110 train_time:104572ms step_avg:58.49ms
step:1789/2110 train_time:104661ms step_avg:58.50ms
step:1790/2110 train_time:104748ms step_avg:58.52ms
step:1791/2110 train_time:104836ms step_avg:58.54ms
step:1792/2110 train_time:104922ms step_avg:58.55ms
step:1793/2110 train_time:105010ms step_avg:58.57ms
step:1794/2110 train_time:105096ms step_avg:58.58ms
step:1795/2110 train_time:105185ms step_avg:58.60ms
step:1796/2110 train_time:105272ms step_avg:58.61ms
step:1797/2110 train_time:105361ms step_avg:58.63ms
step:1798/2110 train_time:105449ms step_avg:58.65ms
step:1799/2110 train_time:105539ms step_avg:58.67ms
step:1800/2110 train_time:105627ms step_avg:58.68ms
step:1801/2110 train_time:105715ms step_avg:58.70ms
step:1802/2110 train_time:105802ms step_avg:58.71ms
step:1803/2110 train_time:105890ms step_avg:58.73ms
step:1804/2110 train_time:105976ms step_avg:58.75ms
step:1805/2110 train_time:106064ms step_avg:58.76ms
step:1806/2110 train_time:106151ms step_avg:58.78ms
step:1807/2110 train_time:106240ms step_avg:58.79ms
step:1808/2110 train_time:106328ms step_avg:58.81ms
step:1809/2110 train_time:106417ms step_avg:58.83ms
step:1810/2110 train_time:106505ms step_avg:58.84ms
step:1811/2110 train_time:106594ms step_avg:58.86ms
step:1812/2110 train_time:106682ms step_avg:58.88ms
step:1813/2110 train_time:106770ms step_avg:58.89ms
step:1814/2110 train_time:106857ms step_avg:58.91ms
step:1815/2110 train_time:106946ms step_avg:58.92ms
step:1816/2110 train_time:107033ms step_avg:58.94ms
step:1817/2110 train_time:107122ms step_avg:58.96ms
step:1818/2110 train_time:107210ms step_avg:58.97ms
step:1819/2110 train_time:107299ms step_avg:58.99ms
step:1820/2110 train_time:107386ms step_avg:59.00ms
step:1821/2110 train_time:107475ms step_avg:59.02ms
step:1822/2110 train_time:107563ms step_avg:59.04ms
step:1823/2110 train_time:107651ms step_avg:59.05ms
step:1824/2110 train_time:107738ms step_avg:59.07ms
step:1825/2110 train_time:107827ms step_avg:59.08ms
step:1826/2110 train_time:107914ms step_avg:59.10ms
step:1827/2110 train_time:108002ms step_avg:59.11ms
step:1828/2110 train_time:108090ms step_avg:59.13ms
step:1829/2110 train_time:108178ms step_avg:59.15ms
step:1830/2110 train_time:108265ms step_avg:59.16ms
step:1831/2110 train_time:108355ms step_avg:59.18ms
step:1832/2110 train_time:108443ms step_avg:59.19ms
step:1833/2110 train_time:108531ms step_avg:59.21ms
step:1834/2110 train_time:108619ms step_avg:59.23ms
step:1835/2110 train_time:108707ms step_avg:59.24ms
step:1836/2110 train_time:108795ms step_avg:59.26ms
step:1837/2110 train_time:108883ms step_avg:59.27ms
step:1838/2110 train_time:108970ms step_avg:59.29ms
step:1839/2110 train_time:109058ms step_avg:59.30ms
step:1840/2110 train_time:109145ms step_avg:59.32ms
step:1841/2110 train_time:109234ms step_avg:59.33ms
step:1842/2110 train_time:109320ms step_avg:59.35ms
step:1843/2110 train_time:109410ms step_avg:59.37ms
step:1844/2110 train_time:109497ms step_avg:59.38ms
step:1845/2110 train_time:109587ms step_avg:59.40ms
step:1846/2110 train_time:109674ms step_avg:59.41ms
step:1847/2110 train_time:109763ms step_avg:59.43ms
step:1848/2110 train_time:109851ms step_avg:59.44ms
step:1849/2110 train_time:109939ms step_avg:59.46ms
step:1850/2110 train_time:110026ms step_avg:59.47ms
step:1851/2110 train_time:110114ms step_avg:59.49ms
step:1852/2110 train_time:110201ms step_avg:59.50ms
step:1853/2110 train_time:110290ms step_avg:59.52ms
step:1854/2110 train_time:110377ms step_avg:59.53ms
step:1855/2110 train_time:110467ms step_avg:59.55ms
step:1856/2110 train_time:110554ms step_avg:59.57ms
step:1857/2110 train_time:110642ms step_avg:59.58ms
step:1858/2110 train_time:110730ms step_avg:59.60ms
step:1859/2110 train_time:110818ms step_avg:59.61ms
step:1860/2110 train_time:110906ms step_avg:59.63ms
step:1861/2110 train_time:110994ms step_avg:59.64ms
step:1862/2110 train_time:111081ms step_avg:59.66ms
step:1863/2110 train_time:111170ms step_avg:59.67ms
step:1864/2110 train_time:111256ms step_avg:59.69ms
step:1865/2110 train_time:111346ms step_avg:59.70ms
step:1866/2110 train_time:111433ms step_avg:59.72ms
step:1867/2110 train_time:111521ms step_avg:59.73ms
step:1868/2110 train_time:111610ms step_avg:59.75ms
step:1869/2110 train_time:111698ms step_avg:59.76ms
step:1870/2110 train_time:111785ms step_avg:59.78ms
step:1871/2110 train_time:111873ms step_avg:59.79ms
step:1872/2110 train_time:111960ms step_avg:59.81ms
step:1873/2110 train_time:112049ms step_avg:59.82ms
step:1874/2110 train_time:112137ms step_avg:59.84ms
step:1875/2110 train_time:112226ms step_avg:59.85ms
step:1876/2110 train_time:112313ms step_avg:59.87ms
step:1877/2110 train_time:112402ms step_avg:59.88ms
step:1878/2110 train_time:112489ms step_avg:59.90ms
step:1879/2110 train_time:112578ms step_avg:59.91ms
step:1880/2110 train_time:112665ms step_avg:59.93ms
step:1881/2110 train_time:112754ms step_avg:59.94ms
step:1882/2110 train_time:112841ms step_avg:59.96ms
step:1883/2110 train_time:112930ms step_avg:59.97ms
step:1884/2110 train_time:113018ms step_avg:59.99ms
step:1885/2110 train_time:113105ms step_avg:60.00ms
step:1886/2110 train_time:113193ms step_avg:60.02ms
step:1887/2110 train_time:113282ms step_avg:60.03ms
step:1888/2110 train_time:113370ms step_avg:60.05ms
step:1889/2110 train_time:113460ms step_avg:60.06ms
step:1890/2110 train_time:113546ms step_avg:60.08ms
step:1891/2110 train_time:113635ms step_avg:60.09ms
step:1892/2110 train_time:113723ms step_avg:60.11ms
step:1893/2110 train_time:113811ms step_avg:60.12ms
step:1894/2110 train_time:113898ms step_avg:60.14ms
step:1895/2110 train_time:113986ms step_avg:60.15ms
step:1896/2110 train_time:114074ms step_avg:60.17ms
step:1897/2110 train_time:114161ms step_avg:60.18ms
step:1898/2110 train_time:114250ms step_avg:60.19ms
step:1899/2110 train_time:114338ms step_avg:60.21ms
step:1900/2110 train_time:114426ms step_avg:60.22ms
step:1901/2110 train_time:114514ms step_avg:60.24ms
step:1902/2110 train_time:114601ms step_avg:60.25ms
step:1903/2110 train_time:114689ms step_avg:60.27ms
step:1904/2110 train_time:114777ms step_avg:60.28ms
step:1905/2110 train_time:114865ms step_avg:60.30ms
step:1906/2110 train_time:114953ms step_avg:60.31ms
step:1907/2110 train_time:115042ms step_avg:60.33ms
step:1908/2110 train_time:115129ms step_avg:60.34ms
step:1909/2110 train_time:115216ms step_avg:60.35ms
step:1910/2110 train_time:115304ms step_avg:60.37ms
step:1911/2110 train_time:115393ms step_avg:60.38ms
step:1912/2110 train_time:115479ms step_avg:60.40ms
step:1913/2110 train_time:115568ms step_avg:60.41ms
step:1914/2110 train_time:115656ms step_avg:60.43ms
step:1915/2110 train_time:115745ms step_avg:60.44ms
step:1916/2110 train_time:115833ms step_avg:60.46ms
step:1917/2110 train_time:115921ms step_avg:60.47ms
step:1918/2110 train_time:116009ms step_avg:60.48ms
step:1919/2110 train_time:116096ms step_avg:60.50ms
step:1920/2110 train_time:116184ms step_avg:60.51ms
step:1921/2110 train_time:116272ms step_avg:60.53ms
step:1922/2110 train_time:116359ms step_avg:60.54ms
step:1923/2110 train_time:116448ms step_avg:60.56ms
step:1924/2110 train_time:116535ms step_avg:60.57ms
step:1925/2110 train_time:116625ms step_avg:60.58ms
step:1926/2110 train_time:116713ms step_avg:60.60ms
step:1927/2110 train_time:116802ms step_avg:60.61ms
step:1928/2110 train_time:116889ms step_avg:60.63ms
step:1929/2110 train_time:116977ms step_avg:60.64ms
step:1930/2110 train_time:117065ms step_avg:60.66ms
step:1931/2110 train_time:117153ms step_avg:60.67ms
step:1932/2110 train_time:117241ms step_avg:60.68ms
step:1933/2110 train_time:117329ms step_avg:60.70ms
step:1934/2110 train_time:117418ms step_avg:60.71ms
step:1935/2110 train_time:117505ms step_avg:60.73ms
step:1936/2110 train_time:117594ms step_avg:60.74ms
step:1937/2110 train_time:117681ms step_avg:60.75ms
step:1938/2110 train_time:117769ms step_avg:60.77ms
step:1939/2110 train_time:117857ms step_avg:60.78ms
step:1940/2110 train_time:117945ms step_avg:60.80ms
step:1941/2110 train_time:118033ms step_avg:60.81ms
step:1942/2110 train_time:118120ms step_avg:60.82ms
step:1943/2110 train_time:118208ms step_avg:60.84ms
step:1944/2110 train_time:118296ms step_avg:60.85ms
step:1945/2110 train_time:118384ms step_avg:60.87ms
step:1946/2110 train_time:118473ms step_avg:60.88ms
step:1947/2110 train_time:118560ms step_avg:60.89ms
step:1948/2110 train_time:118648ms step_avg:60.91ms
step:1949/2110 train_time:118736ms step_avg:60.92ms
step:1950/2110 train_time:118824ms step_avg:60.94ms
step:1951/2110 train_time:118912ms step_avg:60.95ms
step:1952/2110 train_time:118999ms step_avg:60.96ms
step:1953/2110 train_time:119087ms step_avg:60.98ms
step:1954/2110 train_time:119175ms step_avg:60.99ms
step:1955/2110 train_time:119263ms step_avg:61.00ms
step:1956/2110 train_time:119351ms step_avg:61.02ms
step:1957/2110 train_time:119438ms step_avg:61.03ms
step:1958/2110 train_time:119527ms step_avg:61.05ms
step:1959/2110 train_time:119614ms step_avg:61.06ms
step:1960/2110 train_time:119702ms step_avg:61.07ms
step:1961/2110 train_time:119789ms step_avg:61.09ms
step:1962/2110 train_time:119877ms step_avg:61.10ms
step:1963/2110 train_time:119966ms step_avg:61.11ms
step:1964/2110 train_time:120054ms step_avg:61.13ms
step:1965/2110 train_time:120142ms step_avg:61.14ms
step:1966/2110 train_time:120231ms step_avg:61.15ms
step:1967/2110 train_time:120319ms step_avg:61.17ms
step:1968/2110 train_time:120407ms step_avg:61.18ms
step:1969/2110 train_time:120495ms step_avg:61.20ms
step:1970/2110 train_time:120583ms step_avg:61.21ms
step:1971/2110 train_time:120671ms step_avg:61.22ms
step:1972/2110 train_time:120759ms step_avg:61.24ms
step:1973/2110 train_time:120848ms step_avg:61.25ms
step:1974/2110 train_time:120935ms step_avg:61.26ms
step:1975/2110 train_time:121024ms step_avg:61.28ms
step:1976/2110 train_time:121111ms step_avg:61.29ms
step:1977/2110 train_time:121200ms step_avg:61.30ms
step:1978/2110 train_time:121287ms step_avg:61.32ms
step:1979/2110 train_time:121376ms step_avg:61.33ms
step:1980/2110 train_time:121464ms step_avg:61.35ms
step:1981/2110 train_time:121552ms step_avg:61.36ms
step:1982/2110 train_time:121639ms step_avg:61.37ms
step:1983/2110 train_time:121728ms step_avg:61.39ms
step:1984/2110 train_time:121816ms step_avg:61.40ms
step:1985/2110 train_time:121905ms step_avg:61.41ms
step:1986/2110 train_time:121994ms step_avg:61.43ms
step:1987/2110 train_time:122081ms step_avg:61.44ms
step:1988/2110 train_time:122168ms step_avg:61.45ms
step:1989/2110 train_time:122256ms step_avg:61.47ms
step:1990/2110 train_time:122344ms step_avg:61.48ms
step:1991/2110 train_time:122432ms step_avg:61.49ms
step:1992/2110 train_time:122520ms step_avg:61.51ms
step:1993/2110 train_time:122607ms step_avg:61.52ms
step:1994/2110 train_time:122696ms step_avg:61.53ms
step:1995/2110 train_time:122783ms step_avg:61.55ms
step:1996/2110 train_time:122872ms step_avg:61.56ms
step:1997/2110 train_time:122959ms step_avg:61.57ms
step:1998/2110 train_time:123047ms step_avg:61.59ms
step:1999/2110 train_time:123135ms step_avg:61.60ms
step:2000/2110 train_time:123223ms step_avg:61.61ms
step:2000/2110 val_loss:3.3023 train_time:123312ms step_avg:61.66ms
step:2001/2110 train_time:123347ms step_avg:61.64ms
step:2002/2110 train_time:123402ms step_avg:61.64ms
step:2003/2110 train_time:123498ms step_avg:61.66ms
step:2004/2110 train_time:123585ms step_avg:61.67ms
step:2005/2110 train_time:123674ms step_avg:61.68ms
step:2006/2110 train_time:123761ms step_avg:61.70ms
step:2007/2110 train_time:123847ms step_avg:61.71ms
step:2008/2110 train_time:123935ms step_avg:61.72ms
step:2009/2110 train_time:124021ms step_avg:61.73ms
step:2010/2110 train_time:124108ms step_avg:61.75ms
step:2011/2110 train_time:124195ms step_avg:61.76ms
step:2012/2110 train_time:124284ms step_avg:61.77ms
step:2013/2110 train_time:124376ms step_avg:61.79ms
step:2014/2110 train_time:124467ms step_avg:61.80ms
step:2015/2110 train_time:124557ms step_avg:61.82ms
step:2016/2110 train_time:124645ms step_avg:61.83ms
step:2017/2110 train_time:124734ms step_avg:61.84ms
step:2018/2110 train_time:124821ms step_avg:61.85ms
step:2019/2110 train_time:124907ms step_avg:61.87ms
step:2020/2110 train_time:124995ms step_avg:61.88ms
step:2021/2110 train_time:125081ms step_avg:61.89ms
step:2022/2110 train_time:125168ms step_avg:61.90ms
step:2023/2110 train_time:125256ms step_avg:61.92ms
step:2024/2110 train_time:125345ms step_avg:61.93ms
step:2025/2110 train_time:125436ms step_avg:61.94ms
step:2026/2110 train_time:125524ms step_avg:61.96ms
step:2027/2110 train_time:125612ms step_avg:61.97ms
step:2028/2110 train_time:125700ms step_avg:61.98ms
step:2029/2110 train_time:125789ms step_avg:62.00ms
step:2030/2110 train_time:125877ms step_avg:62.01ms
step:2031/2110 train_time:125964ms step_avg:62.02ms
step:2032/2110 train_time:126051ms step_avg:62.03ms
step:2033/2110 train_time:126138ms step_avg:62.05ms
step:2034/2110 train_time:126225ms step_avg:62.06ms
step:2035/2110 train_time:126314ms step_avg:62.07ms
step:2036/2110 train_time:126404ms step_avg:62.08ms
step:2037/2110 train_time:126494ms step_avg:62.10ms
step:2038/2110 train_time:126582ms step_avg:62.11ms
step:2039/2110 train_time:126671ms step_avg:62.12ms
step:2040/2110 train_time:126759ms step_avg:62.14ms
step:2041/2110 train_time:126846ms step_avg:62.15ms
step:2042/2110 train_time:126934ms step_avg:62.16ms
step:2043/2110 train_time:127021ms step_avg:62.17ms
step:2044/2110 train_time:127109ms step_avg:62.19ms
step:2045/2110 train_time:127197ms step_avg:62.20ms
step:2046/2110 train_time:127283ms step_avg:62.21ms
step:2047/2110 train_time:127373ms step_avg:62.22ms
step:2048/2110 train_time:127462ms step_avg:62.24ms
step:2049/2110 train_time:127549ms step_avg:62.25ms
step:2050/2110 train_time:127638ms step_avg:62.26ms
step:2051/2110 train_time:127726ms step_avg:62.27ms
step:2052/2110 train_time:127814ms step_avg:62.29ms
step:2053/2110 train_time:127901ms step_avg:62.30ms
step:2054/2110 train_time:127988ms step_avg:62.31ms
step:2055/2110 train_time:128076ms step_avg:62.32ms
step:2056/2110 train_time:128163ms step_avg:62.34ms
step:2057/2110 train_time:128251ms step_avg:62.35ms
step:2058/2110 train_time:128340ms step_avg:62.36ms
step:2059/2110 train_time:128429ms step_avg:62.37ms
step:2060/2110 train_time:128517ms step_avg:62.39ms
step:2061/2110 train_time:128606ms step_avg:62.40ms
step:2062/2110 train_time:128694ms step_avg:62.41ms
step:2063/2110 train_time:128782ms step_avg:62.42ms
step:2064/2110 train_time:128870ms step_avg:62.44ms
step:2065/2110 train_time:128957ms step_avg:62.45ms
step:2066/2110 train_time:129045ms step_avg:62.46ms
step:2067/2110 train_time:129132ms step_avg:62.47ms
step:2068/2110 train_time:129221ms step_avg:62.49ms
step:2069/2110 train_time:129308ms step_avg:62.50ms
step:2070/2110 train_time:129397ms step_avg:62.51ms
step:2071/2110 train_time:129487ms step_avg:62.52ms
step:2072/2110 train_time:129574ms step_avg:62.54ms
step:2073/2110 train_time:129662ms step_avg:62.55ms
step:2074/2110 train_time:129751ms step_avg:62.56ms
step:2075/2110 train_time:129839ms step_avg:62.57ms
step:2076/2110 train_time:129927ms step_avg:62.59ms
step:2077/2110 train_time:130014ms step_avg:62.60ms
step:2078/2110 train_time:130102ms step_avg:62.61ms
step:2079/2110 train_time:130191ms step_avg:62.62ms
step:2080/2110 train_time:130280ms step_avg:62.63ms
step:2081/2110 train_time:130367ms step_avg:62.65ms
step:2082/2110 train_time:130455ms step_avg:62.66ms
step:2083/2110 train_time:130544ms step_avg:62.67ms
step:2084/2110 train_time:130632ms step_avg:62.68ms
step:2085/2110 train_time:130721ms step_avg:62.70ms
step:2086/2110 train_time:130809ms step_avg:62.71ms
step:2087/2110 train_time:130897ms step_avg:62.72ms
step:2088/2110 train_time:130984ms step_avg:62.73ms
step:2089/2110 train_time:131073ms step_avg:62.74ms
step:2090/2110 train_time:131163ms step_avg:62.76ms
step:2091/2110 train_time:131252ms step_avg:62.77ms
step:2092/2110 train_time:131340ms step_avg:62.78ms
step:2093/2110 train_time:131427ms step_avg:62.79ms
step:2094/2110 train_time:131516ms step_avg:62.81ms
step:2095/2110 train_time:131604ms step_avg:62.82ms
step:2096/2110 train_time:131692ms step_avg:62.83ms
step:2097/2110 train_time:131781ms step_avg:62.84ms
step:2098/2110 train_time:131870ms step_avg:62.86ms
step:2099/2110 train_time:131957ms step_avg:62.87ms
step:2100/2110 train_time:132044ms step_avg:62.88ms
step:2101/2110 train_time:132134ms step_avg:62.89ms
step:2102/2110 train_time:132221ms step_avg:62.90ms
step:2103/2110 train_time:132310ms step_avg:62.91ms
step:2104/2110 train_time:132399ms step_avg:62.93ms
step:2105/2110 train_time:132486ms step_avg:62.94ms
step:2106/2110 train_time:132574ms step_avg:62.95ms
step:2107/2110 train_time:132664ms step_avg:62.96ms
step:2108/2110 train_time:132752ms step_avg:62.98ms
step:2109/2110 train_time:132840ms step_avg:62.99ms
step:2110/2110 train_time:132929ms step_avg:63.00ms
step:2110/2110 val_loss:3.2784 train_time:133018ms step_avg:63.04ms
peak memory allocated: 29244 MiB reserved: 43576 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 15 23:12:59 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   35C    P0            123W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   33C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   38C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              88      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              88      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:91ms step_avg:90.62ms
step:2/2110 train_time:119ms step_avg:59.73ms
step:3/2110 train_time:147ms step_avg:49.01ms
step:4/2110 train_time:178ms step_avg:44.62ms
step:5/2110 train_time:212ms step_avg:42.30ms
step:6/2110 train_time:600ms step_avg:99.94ms
step:7/2110 train_time:676ms step_avg:96.56ms
step:8/2110 train_time:709ms step_avg:88.59ms
step:9/2110 train_time:741ms step_avg:82.37ms
step:10/2110 train_time:774ms step_avg:77.35ms
step:11/2110 train_time:807ms step_avg:73.35ms
step:12/2110 train_time:839ms step_avg:69.95ms
step:13/2110 train_time:872ms step_avg:67.11ms
step:14/2110 train_time:905ms step_avg:64.64ms
step:15/2110 train_time:939ms step_avg:62.57ms
step:16/2110 train_time:971ms step_avg:60.71ms
step:17/2110 train_time:1004ms step_avg:59.09ms
step:18/2110 train_time:1037ms step_avg:57.62ms
step:19/2110 train_time:1070ms step_avg:56.33ms
step:20/2110 train_time:1103ms step_avg:55.14ms
step:21/2110 train_time:1136ms step_avg:54.10ms
step:22/2110 train_time:1169ms step_avg:53.14ms
step:23/2110 train_time:1202ms step_avg:52.28ms
step:24/2110 train_time:1235ms step_avg:51.46ms
step:25/2110 train_time:1268ms step_avg:50.73ms
step:26/2110 train_time:1301ms step_avg:50.03ms
step:27/2110 train_time:1334ms step_avg:49.42ms
step:28/2110 train_time:1367ms step_avg:48.83ms
step:29/2110 train_time:1401ms step_avg:48.30ms
step:30/2110 train_time:1434ms step_avg:47.80ms
step:31/2110 train_time:1467ms step_avg:47.32ms
step:32/2110 train_time:1500ms step_avg:46.86ms
step:33/2110 train_time:1533ms step_avg:46.44ms
step:34/2110 train_time:1566ms step_avg:46.06ms
step:35/2110 train_time:1600ms step_avg:45.73ms
step:36/2110 train_time:1634ms step_avg:45.39ms
step:37/2110 train_time:1669ms step_avg:45.10ms
step:38/2110 train_time:1702ms step_avg:44.79ms
step:39/2110 train_time:1736ms step_avg:44.51ms
step:40/2110 train_time:1769ms step_avg:44.23ms
step:41/2110 train_time:1803ms step_avg:43.96ms
step:42/2110 train_time:1835ms step_avg:43.70ms
step:43/2110 train_time:1869ms step_avg:43.46ms
step:44/2110 train_time:1901ms step_avg:43.22ms
step:45/2110 train_time:1935ms step_avg:43.00ms
step:46/2110 train_time:1968ms step_avg:42.77ms
step:47/2110 train_time:2001ms step_avg:42.58ms
step:48/2110 train_time:2034ms step_avg:42.37ms
step:49/2110 train_time:2067ms step_avg:42.19ms
step:50/2110 train_time:2100ms step_avg:41.99ms
step:51/2110 train_time:2133ms step_avg:41.82ms
step:52/2110 train_time:2165ms step_avg:41.64ms
step:53/2110 train_time:2199ms step_avg:41.49ms
step:54/2110 train_time:2231ms step_avg:41.32ms
step:55/2110 train_time:2265ms step_avg:41.18ms
step:56/2110 train_time:2298ms step_avg:41.03ms
step:57/2110 train_time:2331ms step_avg:40.89ms
step:58/2110 train_time:2363ms step_avg:40.75ms
step:59/2110 train_time:2397ms step_avg:40.63ms
step:60/2110 train_time:2429ms step_avg:40.49ms
step:61/2110 train_time:2463ms step_avg:40.37ms
step:62/2110 train_time:2495ms step_avg:40.25ms
step:63/2110 train_time:2529ms step_avg:40.14ms
step:64/2110 train_time:2562ms step_avg:40.02ms
step:65/2110 train_time:2595ms step_avg:39.93ms
step:66/2110 train_time:2628ms step_avg:39.82ms
step:67/2110 train_time:2662ms step_avg:39.73ms
step:68/2110 train_time:2695ms step_avg:39.63ms
step:69/2110 train_time:2728ms step_avg:39.54ms
step:70/2110 train_time:2761ms step_avg:39.45ms
step:71/2110 train_time:2795ms step_avg:39.37ms
step:72/2110 train_time:2828ms step_avg:39.27ms
step:73/2110 train_time:2861ms step_avg:39.20ms
step:74/2110 train_time:2894ms step_avg:39.11ms
step:75/2110 train_time:2928ms step_avg:39.04ms
step:76/2110 train_time:2962ms step_avg:38.97ms
step:77/2110 train_time:2994ms step_avg:38.88ms
step:78/2110 train_time:3027ms step_avg:38.80ms
step:79/2110 train_time:3060ms step_avg:38.73ms
step:80/2110 train_time:3092ms step_avg:38.66ms
step:81/2110 train_time:3126ms step_avg:38.59ms
step:82/2110 train_time:3159ms step_avg:38.52ms
step:83/2110 train_time:3192ms step_avg:38.46ms
step:84/2110 train_time:3225ms step_avg:38.39ms
step:85/2110 train_time:3258ms step_avg:38.33ms
step:86/2110 train_time:3290ms step_avg:38.26ms
step:87/2110 train_time:3324ms step_avg:38.20ms
step:88/2110 train_time:3356ms step_avg:38.14ms
step:89/2110 train_time:3390ms step_avg:38.08ms
step:90/2110 train_time:3422ms step_avg:38.02ms
step:91/2110 train_time:3455ms step_avg:37.97ms
step:92/2110 train_time:3488ms step_avg:37.92ms
step:93/2110 train_time:3521ms step_avg:37.86ms
step:94/2110 train_time:3554ms step_avg:37.81ms
step:95/2110 train_time:3588ms step_avg:37.76ms
step:96/2110 train_time:3620ms step_avg:37.71ms
step:97/2110 train_time:3654ms step_avg:37.67ms
step:98/2110 train_time:3686ms step_avg:37.61ms
step:99/2110 train_time:3719ms step_avg:37.57ms
step:100/2110 train_time:3753ms step_avg:37.53ms
step:101/2110 train_time:3786ms step_avg:37.48ms
step:102/2110 train_time:3819ms step_avg:37.44ms
step:103/2110 train_time:3852ms step_avg:37.40ms
step:104/2110 train_time:3884ms step_avg:37.35ms
step:105/2110 train_time:3918ms step_avg:37.31ms
step:106/2110 train_time:3951ms step_avg:37.27ms
step:107/2110 train_time:3984ms step_avg:37.23ms
step:108/2110 train_time:4018ms step_avg:37.21ms
step:109/2110 train_time:4050ms step_avg:37.16ms
step:110/2110 train_time:4083ms step_avg:37.12ms
step:111/2110 train_time:4116ms step_avg:37.08ms
step:112/2110 train_time:4149ms step_avg:37.04ms
step:113/2110 train_time:4182ms step_avg:37.01ms
step:114/2110 train_time:4215ms step_avg:36.97ms
step:115/2110 train_time:4248ms step_avg:36.94ms
step:116/2110 train_time:4281ms step_avg:36.90ms
step:117/2110 train_time:4314ms step_avg:36.87ms
step:118/2110 train_time:4347ms step_avg:36.84ms
step:119/2110 train_time:4380ms step_avg:36.81ms
step:120/2110 train_time:4413ms step_avg:36.78ms
step:121/2110 train_time:4447ms step_avg:36.75ms
step:122/2110 train_time:4480ms step_avg:36.72ms
step:123/2110 train_time:4513ms step_avg:36.69ms
step:124/2110 train_time:4545ms step_avg:36.66ms
step:125/2110 train_time:4578ms step_avg:36.63ms
step:126/2110 train_time:4611ms step_avg:36.59ms
step:127/2110 train_time:4644ms step_avg:36.57ms
step:128/2110 train_time:4677ms step_avg:36.54ms
step:129/2110 train_time:4710ms step_avg:36.51ms
step:130/2110 train_time:4743ms step_avg:36.48ms
step:131/2110 train_time:4776ms step_avg:36.46ms
step:132/2110 train_time:4809ms step_avg:36.43ms
step:133/2110 train_time:4842ms step_avg:36.41ms
step:134/2110 train_time:4875ms step_avg:36.38ms
step:135/2110 train_time:4908ms step_avg:36.36ms
step:136/2110 train_time:4941ms step_avg:36.33ms
step:137/2110 train_time:4974ms step_avg:36.31ms
step:138/2110 train_time:5007ms step_avg:36.28ms
step:139/2110 train_time:5040ms step_avg:36.26ms
step:140/2110 train_time:5073ms step_avg:36.23ms
step:141/2110 train_time:5106ms step_avg:36.21ms
step:142/2110 train_time:5139ms step_avg:36.19ms
step:143/2110 train_time:5172ms step_avg:36.17ms
step:144/2110 train_time:5205ms step_avg:36.14ms
step:145/2110 train_time:5238ms step_avg:36.13ms
step:146/2110 train_time:5271ms step_avg:36.10ms
step:147/2110 train_time:5304ms step_avg:36.08ms
step:148/2110 train_time:5337ms step_avg:36.06ms
step:149/2110 train_time:5370ms step_avg:36.04ms
step:150/2110 train_time:5403ms step_avg:36.02ms
step:151/2110 train_time:5436ms step_avg:36.00ms
step:152/2110 train_time:5469ms step_avg:35.98ms
step:153/2110 train_time:5502ms step_avg:35.96ms
step:154/2110 train_time:5535ms step_avg:35.94ms
step:155/2110 train_time:5568ms step_avg:35.92ms
step:156/2110 train_time:5602ms step_avg:35.91ms
step:157/2110 train_time:5634ms step_avg:35.88ms
step:158/2110 train_time:5666ms step_avg:35.86ms
step:159/2110 train_time:5700ms step_avg:35.85ms
step:160/2110 train_time:5733ms step_avg:35.83ms
step:161/2110 train_time:5766ms step_avg:35.81ms
step:162/2110 train_time:5799ms step_avg:35.79ms
step:163/2110 train_time:5832ms step_avg:35.78ms
step:164/2110 train_time:5865ms step_avg:35.76ms
step:165/2110 train_time:5898ms step_avg:35.74ms
step:166/2110 train_time:5930ms step_avg:35.72ms
step:167/2110 train_time:5963ms step_avg:35.71ms
step:168/2110 train_time:5996ms step_avg:35.69ms
step:169/2110 train_time:6030ms step_avg:35.68ms
step:170/2110 train_time:6062ms step_avg:35.66ms
step:171/2110 train_time:6096ms step_avg:35.65ms
step:172/2110 train_time:6130ms step_avg:35.64ms
step:173/2110 train_time:6162ms step_avg:35.62ms
step:174/2110 train_time:6196ms step_avg:35.61ms
step:175/2110 train_time:6228ms step_avg:35.59ms
step:176/2110 train_time:6261ms step_avg:35.57ms
step:177/2110 train_time:6294ms step_avg:35.56ms
step:178/2110 train_time:6327ms step_avg:35.54ms
step:179/2110 train_time:6360ms step_avg:35.53ms
step:180/2110 train_time:6393ms step_avg:35.52ms
step:181/2110 train_time:6426ms step_avg:35.50ms
step:182/2110 train_time:6459ms step_avg:35.49ms
step:183/2110 train_time:6492ms step_avg:35.47ms
step:184/2110 train_time:6524ms step_avg:35.46ms
step:185/2110 train_time:6558ms step_avg:35.45ms
step:186/2110 train_time:6592ms step_avg:35.44ms
step:187/2110 train_time:6624ms step_avg:35.42ms
step:188/2110 train_time:6656ms step_avg:35.41ms
step:189/2110 train_time:6690ms step_avg:35.39ms
step:190/2110 train_time:6722ms step_avg:35.38ms
step:191/2110 train_time:6755ms step_avg:35.37ms
step:192/2110 train_time:6788ms step_avg:35.35ms
step:193/2110 train_time:6821ms step_avg:35.34ms
step:194/2110 train_time:6854ms step_avg:35.33ms
step:195/2110 train_time:6887ms step_avg:35.32ms
step:196/2110 train_time:6920ms step_avg:35.30ms
step:197/2110 train_time:6953ms step_avg:35.29ms
step:198/2110 train_time:6986ms step_avg:35.28ms
step:199/2110 train_time:7019ms step_avg:35.27ms
step:200/2110 train_time:7051ms step_avg:35.26ms
step:201/2110 train_time:7085ms step_avg:35.25ms
step:202/2110 train_time:7117ms step_avg:35.23ms
step:203/2110 train_time:7150ms step_avg:35.22ms
step:204/2110 train_time:7183ms step_avg:35.21ms
step:205/2110 train_time:7216ms step_avg:35.20ms
step:206/2110 train_time:7249ms step_avg:35.19ms
step:207/2110 train_time:7282ms step_avg:35.18ms
step:208/2110 train_time:7315ms step_avg:35.17ms
step:209/2110 train_time:7348ms step_avg:35.16ms
step:210/2110 train_time:7381ms step_avg:35.15ms
step:211/2110 train_time:7414ms step_avg:35.14ms
step:212/2110 train_time:7447ms step_avg:35.13ms
step:213/2110 train_time:7480ms step_avg:35.12ms
step:214/2110 train_time:7513ms step_avg:35.11ms
step:215/2110 train_time:7547ms step_avg:35.10ms
step:216/2110 train_time:7579ms step_avg:35.09ms
step:217/2110 train_time:7613ms step_avg:35.08ms
step:218/2110 train_time:7645ms step_avg:35.07ms
step:219/2110 train_time:7679ms step_avg:35.06ms
step:220/2110 train_time:7712ms step_avg:35.05ms
step:221/2110 train_time:7744ms step_avg:35.04ms
step:222/2110 train_time:7777ms step_avg:35.03ms
step:223/2110 train_time:7810ms step_avg:35.02ms
step:224/2110 train_time:7843ms step_avg:35.01ms
step:225/2110 train_time:7876ms step_avg:35.00ms
step:226/2110 train_time:7908ms step_avg:34.99ms
step:227/2110 train_time:7942ms step_avg:34.99ms
step:228/2110 train_time:7974ms step_avg:34.98ms
step:229/2110 train_time:8007ms step_avg:34.97ms
step:230/2110 train_time:8040ms step_avg:34.96ms
step:231/2110 train_time:8073ms step_avg:34.95ms
step:232/2110 train_time:8106ms step_avg:34.94ms
step:233/2110 train_time:8139ms step_avg:34.93ms
step:234/2110 train_time:8173ms step_avg:34.93ms
step:235/2110 train_time:8206ms step_avg:34.92ms
step:236/2110 train_time:8239ms step_avg:34.91ms
step:237/2110 train_time:8272ms step_avg:34.90ms
step:238/2110 train_time:8304ms step_avg:34.89ms
step:239/2110 train_time:8339ms step_avg:34.89ms
step:240/2110 train_time:8370ms step_avg:34.88ms
step:241/2110 train_time:8404ms step_avg:34.87ms
step:242/2110 train_time:8436ms step_avg:34.86ms
step:243/2110 train_time:8470ms step_avg:34.85ms
step:244/2110 train_time:8502ms step_avg:34.85ms
step:245/2110 train_time:8536ms step_avg:34.84ms
step:246/2110 train_time:8568ms step_avg:34.83ms
step:247/2110 train_time:8602ms step_avg:34.82ms
step:248/2110 train_time:8634ms step_avg:34.82ms
step:249/2110 train_time:8668ms step_avg:34.81ms
step:250/2110 train_time:8700ms step_avg:34.80ms
step:250/2110 val_loss:4.2995 train_time:8735ms step_avg:34.94ms
step:251/2110 train_time:8764ms step_avg:34.92ms
step:252/2110 train_time:8788ms step_avg:34.87ms
step:253/2110 train_time:8811ms step_avg:34.82ms
step:254/2110 train_time:8841ms step_avg:34.81ms
step:255/2110 train_time:8877ms step_avg:34.81ms
step:256/2110 train_time:8911ms step_avg:34.81ms
step:257/2110 train_time:8944ms step_avg:34.80ms
step:258/2110 train_time:8977ms step_avg:34.80ms
step:259/2110 train_time:9011ms step_avg:34.79ms
step:260/2110 train_time:9045ms step_avg:34.79ms
step:261/2110 train_time:9078ms step_avg:34.78ms
step:262/2110 train_time:9111ms step_avg:34.78ms
step:263/2110 train_time:9144ms step_avg:34.77ms
step:264/2110 train_time:9176ms step_avg:34.76ms
step:265/2110 train_time:9210ms step_avg:34.75ms
step:266/2110 train_time:9242ms step_avg:34.75ms
step:267/2110 train_time:9275ms step_avg:34.74ms
step:268/2110 train_time:9308ms step_avg:34.73ms
step:269/2110 train_time:9341ms step_avg:34.72ms
step:270/2110 train_time:9374ms step_avg:34.72ms
step:271/2110 train_time:9408ms step_avg:34.72ms
step:272/2110 train_time:9440ms step_avg:34.71ms
step:273/2110 train_time:9474ms step_avg:34.70ms
step:274/2110 train_time:9506ms step_avg:34.69ms
step:275/2110 train_time:9541ms step_avg:34.69ms
step:276/2110 train_time:9574ms step_avg:34.69ms
step:277/2110 train_time:9607ms step_avg:34.68ms
step:278/2110 train_time:9640ms step_avg:34.68ms
step:279/2110 train_time:9677ms step_avg:34.69ms
step:280/2110 train_time:9710ms step_avg:34.68ms
step:281/2110 train_time:9743ms step_avg:34.67ms
step:282/2110 train_time:9777ms step_avg:34.67ms
step:283/2110 train_time:9810ms step_avg:34.66ms
step:284/2110 train_time:9843ms step_avg:34.66ms
step:285/2110 train_time:9876ms step_avg:34.65ms
step:286/2110 train_time:9909ms step_avg:34.65ms
step:287/2110 train_time:9943ms step_avg:34.64ms
step:288/2110 train_time:9975ms step_avg:34.64ms
step:289/2110 train_time:10010ms step_avg:34.64ms
step:290/2110 train_time:10042ms step_avg:34.63ms
step:291/2110 train_time:10075ms step_avg:34.62ms
step:292/2110 train_time:10108ms step_avg:34.62ms
step:293/2110 train_time:10142ms step_avg:34.61ms
step:294/2110 train_time:10176ms step_avg:34.61ms
step:295/2110 train_time:10208ms step_avg:34.60ms
step:296/2110 train_time:10241ms step_avg:34.60ms
step:297/2110 train_time:10275ms step_avg:34.60ms
step:298/2110 train_time:10307ms step_avg:34.59ms
step:299/2110 train_time:10340ms step_avg:34.58ms
step:300/2110 train_time:10373ms step_avg:34.58ms
step:301/2110 train_time:10406ms step_avg:34.57ms
step:302/2110 train_time:10439ms step_avg:34.57ms
step:303/2110 train_time:10476ms step_avg:34.57ms
step:304/2110 train_time:10512ms step_avg:34.58ms
step:305/2110 train_time:10548ms step_avg:34.58ms
step:306/2110 train_time:10581ms step_avg:34.58ms
step:307/2110 train_time:10614ms step_avg:34.57ms
step:308/2110 train_time:10647ms step_avg:34.57ms
step:309/2110 train_time:10680ms step_avg:34.56ms
step:310/2110 train_time:10712ms step_avg:34.56ms
step:311/2110 train_time:10745ms step_avg:34.55ms
step:312/2110 train_time:10778ms step_avg:34.55ms
step:313/2110 train_time:10812ms step_avg:34.54ms
step:314/2110 train_time:10844ms step_avg:34.54ms
step:315/2110 train_time:10878ms step_avg:34.53ms
step:316/2110 train_time:10911ms step_avg:34.53ms
step:317/2110 train_time:10944ms step_avg:34.52ms
step:318/2110 train_time:10977ms step_avg:34.52ms
step:319/2110 train_time:11011ms step_avg:34.52ms
step:320/2110 train_time:11043ms step_avg:34.51ms
step:321/2110 train_time:11076ms step_avg:34.51ms
step:322/2110 train_time:11109ms step_avg:34.50ms
step:323/2110 train_time:11142ms step_avg:34.50ms
step:324/2110 train_time:11177ms step_avg:34.50ms
step:325/2110 train_time:11211ms step_avg:34.50ms
step:326/2110 train_time:11244ms step_avg:34.49ms
step:327/2110 train_time:11277ms step_avg:34.49ms
step:328/2110 train_time:11309ms step_avg:34.48ms
step:329/2110 train_time:11342ms step_avg:34.48ms
step:330/2110 train_time:11375ms step_avg:34.47ms
step:331/2110 train_time:11408ms step_avg:34.47ms
step:332/2110 train_time:11441ms step_avg:34.46ms
step:333/2110 train_time:11474ms step_avg:34.46ms
step:334/2110 train_time:11507ms step_avg:34.45ms
step:335/2110 train_time:11540ms step_avg:34.45ms
step:336/2110 train_time:11573ms step_avg:34.44ms
step:337/2110 train_time:11606ms step_avg:34.44ms
step:338/2110 train_time:11638ms step_avg:34.43ms
step:339/2110 train_time:11672ms step_avg:34.43ms
step:340/2110 train_time:11704ms step_avg:34.42ms
step:341/2110 train_time:11737ms step_avg:34.42ms
step:342/2110 train_time:11770ms step_avg:34.42ms
step:343/2110 train_time:11803ms step_avg:34.41ms
step:344/2110 train_time:11836ms step_avg:34.41ms
step:345/2110 train_time:11870ms step_avg:34.40ms
step:346/2110 train_time:11902ms step_avg:34.40ms
step:347/2110 train_time:11935ms step_avg:34.39ms
step:348/2110 train_time:11968ms step_avg:34.39ms
step:349/2110 train_time:12001ms step_avg:34.39ms
step:350/2110 train_time:12033ms step_avg:34.38ms
step:351/2110 train_time:12067ms step_avg:34.38ms
step:352/2110 train_time:12100ms step_avg:34.37ms
step:353/2110 train_time:12133ms step_avg:34.37ms
step:354/2110 train_time:12166ms step_avg:34.37ms
step:355/2110 train_time:12199ms step_avg:34.36ms
step:356/2110 train_time:12232ms step_avg:34.36ms
step:357/2110 train_time:12266ms step_avg:34.36ms
step:358/2110 train_time:12299ms step_avg:34.36ms
step:359/2110 train_time:12332ms step_avg:34.35ms
step:360/2110 train_time:12365ms step_avg:34.35ms
step:361/2110 train_time:12398ms step_avg:34.34ms
step:362/2110 train_time:12431ms step_avg:34.34ms
step:363/2110 train_time:12464ms step_avg:34.34ms
step:364/2110 train_time:12496ms step_avg:34.33ms
step:365/2110 train_time:12530ms step_avg:34.33ms
step:366/2110 train_time:12563ms step_avg:34.32ms
step:367/2110 train_time:12599ms step_avg:34.33ms
step:368/2110 train_time:12637ms step_avg:34.34ms
step:369/2110 train_time:12670ms step_avg:34.34ms
step:370/2110 train_time:12703ms step_avg:34.33ms
step:371/2110 train_time:12736ms step_avg:34.33ms
step:372/2110 train_time:12769ms step_avg:34.33ms
step:373/2110 train_time:12802ms step_avg:34.32ms
step:374/2110 train_time:12835ms step_avg:34.32ms
step:375/2110 train_time:12867ms step_avg:34.31ms
step:376/2110 train_time:12901ms step_avg:34.31ms
step:377/2110 train_time:12933ms step_avg:34.31ms
step:378/2110 train_time:12966ms step_avg:34.30ms
step:379/2110 train_time:12999ms step_avg:34.30ms
step:380/2110 train_time:13031ms step_avg:34.29ms
step:381/2110 train_time:13065ms step_avg:34.29ms
step:382/2110 train_time:13098ms step_avg:34.29ms
step:383/2110 train_time:13131ms step_avg:34.28ms
step:384/2110 train_time:13164ms step_avg:34.28ms
step:385/2110 train_time:13197ms step_avg:34.28ms
step:386/2110 train_time:13230ms step_avg:34.27ms
step:387/2110 train_time:13263ms step_avg:34.27ms
step:388/2110 train_time:13297ms step_avg:34.27ms
step:389/2110 train_time:13329ms step_avg:34.26ms
step:390/2110 train_time:13363ms step_avg:34.26ms
step:391/2110 train_time:13395ms step_avg:34.26ms
step:392/2110 train_time:13428ms step_avg:34.25ms
step:393/2110 train_time:13461ms step_avg:34.25ms
step:394/2110 train_time:13494ms step_avg:34.25ms
step:395/2110 train_time:13527ms step_avg:34.25ms
step:396/2110 train_time:13560ms step_avg:34.24ms
step:397/2110 train_time:13594ms step_avg:34.24ms
step:398/2110 train_time:13627ms step_avg:34.24ms
step:399/2110 train_time:13660ms step_avg:34.24ms
step:400/2110 train_time:13692ms step_avg:34.23ms
step:401/2110 train_time:13726ms step_avg:34.23ms
step:402/2110 train_time:13758ms step_avg:34.22ms
step:403/2110 train_time:13791ms step_avg:34.22ms
step:404/2110 train_time:13824ms step_avg:34.22ms
step:405/2110 train_time:13858ms step_avg:34.22ms
step:406/2110 train_time:13890ms step_avg:34.21ms
step:407/2110 train_time:13923ms step_avg:34.21ms
step:408/2110 train_time:13956ms step_avg:34.20ms
step:409/2110 train_time:13989ms step_avg:34.20ms
step:410/2110 train_time:14022ms step_avg:34.20ms
step:411/2110 train_time:14055ms step_avg:34.20ms
step:412/2110 train_time:14088ms step_avg:34.19ms
step:413/2110 train_time:14121ms step_avg:34.19ms
step:414/2110 train_time:14154ms step_avg:34.19ms
step:415/2110 train_time:14187ms step_avg:34.19ms
step:416/2110 train_time:14220ms step_avg:34.18ms
step:417/2110 train_time:14252ms step_avg:34.18ms
step:418/2110 train_time:14285ms step_avg:34.17ms
step:419/2110 train_time:14319ms step_avg:34.17ms
step:420/2110 train_time:14352ms step_avg:34.17ms
step:421/2110 train_time:14385ms step_avg:34.17ms
step:422/2110 train_time:14418ms step_avg:34.16ms
step:423/2110 train_time:14451ms step_avg:34.16ms
step:424/2110 train_time:14484ms step_avg:34.16ms
step:425/2110 train_time:14517ms step_avg:34.16ms
step:426/2110 train_time:14550ms step_avg:34.15ms
step:427/2110 train_time:14583ms step_avg:34.15ms
step:428/2110 train_time:14615ms step_avg:34.15ms
step:429/2110 train_time:14649ms step_avg:34.15ms
step:430/2110 train_time:14681ms step_avg:34.14ms
step:431/2110 train_time:14715ms step_avg:34.14ms
step:432/2110 train_time:14748ms step_avg:34.14ms
step:433/2110 train_time:14780ms step_avg:34.13ms
step:434/2110 train_time:14814ms step_avg:34.13ms
step:435/2110 train_time:14846ms step_avg:34.13ms
step:436/2110 train_time:14879ms step_avg:34.13ms
step:437/2110 train_time:14912ms step_avg:34.12ms
step:438/2110 train_time:14944ms step_avg:34.12ms
step:439/2110 train_time:14978ms step_avg:34.12ms
step:440/2110 train_time:15010ms step_avg:34.11ms
step:441/2110 train_time:15044ms step_avg:34.11ms
step:442/2110 train_time:15076ms step_avg:34.11ms
step:443/2110 train_time:15110ms step_avg:34.11ms
step:444/2110 train_time:15142ms step_avg:34.10ms
step:445/2110 train_time:15175ms step_avg:34.10ms
step:446/2110 train_time:15208ms step_avg:34.10ms
step:447/2110 train_time:15242ms step_avg:34.10ms
step:448/2110 train_time:15275ms step_avg:34.10ms
step:449/2110 train_time:15308ms step_avg:34.09ms
step:450/2110 train_time:15340ms step_avg:34.09ms
step:451/2110 train_time:15374ms step_avg:34.09ms
step:452/2110 train_time:15406ms step_avg:34.09ms
step:453/2110 train_time:15440ms step_avg:34.08ms
step:454/2110 train_time:15473ms step_avg:34.08ms
step:455/2110 train_time:15506ms step_avg:34.08ms
step:456/2110 train_time:15538ms step_avg:34.07ms
step:457/2110 train_time:15571ms step_avg:34.07ms
step:458/2110 train_time:15604ms step_avg:34.07ms
step:459/2110 train_time:15637ms step_avg:34.07ms
step:460/2110 train_time:15670ms step_avg:34.06ms
step:461/2110 train_time:15703ms step_avg:34.06ms
step:462/2110 train_time:15736ms step_avg:34.06ms
step:463/2110 train_time:15769ms step_avg:34.06ms
step:464/2110 train_time:15802ms step_avg:34.06ms
step:465/2110 train_time:15835ms step_avg:34.05ms
step:466/2110 train_time:15868ms step_avg:34.05ms
step:467/2110 train_time:15901ms step_avg:34.05ms
step:468/2110 train_time:15935ms step_avg:34.05ms
step:469/2110 train_time:15967ms step_avg:34.05ms
step:470/2110 train_time:16000ms step_avg:34.04ms
step:471/2110 train_time:16033ms step_avg:34.04ms
step:472/2110 train_time:16067ms step_avg:34.04ms
step:473/2110 train_time:16100ms step_avg:34.04ms
step:474/2110 train_time:16133ms step_avg:34.03ms
step:475/2110 train_time:16166ms step_avg:34.03ms
step:476/2110 train_time:16198ms step_avg:34.03ms
step:477/2110 train_time:16231ms step_avg:34.03ms
step:478/2110 train_time:16265ms step_avg:34.03ms
step:479/2110 train_time:16298ms step_avg:34.02ms
step:480/2110 train_time:16331ms step_avg:34.02ms
step:481/2110 train_time:16364ms step_avg:34.02ms
step:482/2110 train_time:16396ms step_avg:34.02ms
step:483/2110 train_time:16430ms step_avg:34.02ms
step:484/2110 train_time:16463ms step_avg:34.01ms
step:485/2110 train_time:16496ms step_avg:34.01ms
step:486/2110 train_time:16530ms step_avg:34.01ms
step:487/2110 train_time:16562ms step_avg:34.01ms
step:488/2110 train_time:16595ms step_avg:34.01ms
step:489/2110 train_time:16628ms step_avg:34.00ms
step:490/2110 train_time:16661ms step_avg:34.00ms
step:491/2110 train_time:16694ms step_avg:34.00ms
step:492/2110 train_time:16727ms step_avg:34.00ms
step:493/2110 train_time:16760ms step_avg:34.00ms
step:494/2110 train_time:16793ms step_avg:33.99ms
step:495/2110 train_time:16826ms step_avg:33.99ms
step:496/2110 train_time:16859ms step_avg:33.99ms
step:497/2110 train_time:16892ms step_avg:33.99ms
step:498/2110 train_time:16925ms step_avg:33.99ms
step:499/2110 train_time:16958ms step_avg:33.98ms
step:500/2110 train_time:16991ms step_avg:33.98ms
step:500/2110 val_loss:4.0332 train_time:17026ms step_avg:34.05ms
step:501/2110 train_time:17053ms step_avg:34.04ms
step:502/2110 train_time:17079ms step_avg:34.02ms
step:503/2110 train_time:17100ms step_avg:34.00ms
step:504/2110 train_time:17133ms step_avg:33.99ms
step:505/2110 train_time:17169ms step_avg:34.00ms
step:506/2110 train_time:17202ms step_avg:34.00ms
step:507/2110 train_time:17237ms step_avg:34.00ms
step:508/2110 train_time:17269ms step_avg:33.99ms
step:509/2110 train_time:17303ms step_avg:33.99ms
step:510/2110 train_time:17336ms step_avg:33.99ms
step:511/2110 train_time:17370ms step_avg:33.99ms
step:512/2110 train_time:17402ms step_avg:33.99ms
step:513/2110 train_time:17435ms step_avg:33.99ms
step:514/2110 train_time:17468ms step_avg:33.98ms
step:515/2110 train_time:17501ms step_avg:33.98ms
step:516/2110 train_time:17534ms step_avg:33.98ms
step:517/2110 train_time:17567ms step_avg:33.98ms
step:518/2110 train_time:17599ms step_avg:33.97ms
step:519/2110 train_time:17632ms step_avg:33.97ms
step:520/2110 train_time:17664ms step_avg:33.97ms
step:521/2110 train_time:17697ms step_avg:33.97ms
step:522/2110 train_time:17729ms step_avg:33.96ms
step:523/2110 train_time:17763ms step_avg:33.96ms
step:524/2110 train_time:17795ms step_avg:33.96ms
step:525/2110 train_time:17828ms step_avg:33.96ms
step:526/2110 train_time:17861ms step_avg:33.96ms
step:527/2110 train_time:17894ms step_avg:33.95ms
step:528/2110 train_time:17926ms step_avg:33.95ms
step:529/2110 train_time:17959ms step_avg:33.95ms
step:530/2110 train_time:17993ms step_avg:33.95ms
step:531/2110 train_time:18025ms step_avg:33.95ms
step:532/2110 train_time:18059ms step_avg:33.94ms
step:533/2110 train_time:18092ms step_avg:33.94ms
step:534/2110 train_time:18125ms step_avg:33.94ms
step:535/2110 train_time:18159ms step_avg:33.94ms
step:536/2110 train_time:18192ms step_avg:33.94ms
step:537/2110 train_time:18226ms step_avg:33.94ms
step:538/2110 train_time:18259ms step_avg:33.94ms
step:539/2110 train_time:18293ms step_avg:33.94ms
step:540/2110 train_time:18326ms step_avg:33.94ms
step:541/2110 train_time:18359ms step_avg:33.93ms
step:542/2110 train_time:18392ms step_avg:33.93ms
step:543/2110 train_time:18425ms step_avg:33.93ms
step:544/2110 train_time:18458ms step_avg:33.93ms
step:545/2110 train_time:18491ms step_avg:33.93ms
step:546/2110 train_time:18524ms step_avg:33.93ms
step:547/2110 train_time:18557ms step_avg:33.92ms
step:548/2110 train_time:18589ms step_avg:33.92ms
step:549/2110 train_time:18622ms step_avg:33.92ms
step:550/2110 train_time:18655ms step_avg:33.92ms
step:551/2110 train_time:18688ms step_avg:33.92ms
step:552/2110 train_time:18721ms step_avg:33.91ms
step:553/2110 train_time:18753ms step_avg:33.91ms
step:554/2110 train_time:18786ms step_avg:33.91ms
step:555/2110 train_time:18819ms step_avg:33.91ms
step:556/2110 train_time:18851ms step_avg:33.90ms
step:557/2110 train_time:18884ms step_avg:33.90ms
step:558/2110 train_time:18917ms step_avg:33.90ms
step:559/2110 train_time:18950ms step_avg:33.90ms
step:560/2110 train_time:18982ms step_avg:33.90ms
step:561/2110 train_time:19016ms step_avg:33.90ms
step:562/2110 train_time:19048ms step_avg:33.89ms
step:563/2110 train_time:19082ms step_avg:33.89ms
step:564/2110 train_time:19115ms step_avg:33.89ms
step:565/2110 train_time:19148ms step_avg:33.89ms
step:566/2110 train_time:19181ms step_avg:33.89ms
step:567/2110 train_time:19214ms step_avg:33.89ms
step:568/2110 train_time:19247ms step_avg:33.88ms
step:569/2110 train_time:19281ms step_avg:33.89ms
step:570/2110 train_time:19314ms step_avg:33.88ms
step:571/2110 train_time:19347ms step_avg:33.88ms
step:572/2110 train_time:19380ms step_avg:33.88ms
step:573/2110 train_time:19413ms step_avg:33.88ms
step:574/2110 train_time:19447ms step_avg:33.88ms
step:575/2110 train_time:19479ms step_avg:33.88ms
step:576/2110 train_time:19512ms step_avg:33.88ms
step:577/2110 train_time:19545ms step_avg:33.87ms
step:578/2110 train_time:19578ms step_avg:33.87ms
step:579/2110 train_time:19611ms step_avg:33.87ms
step:580/2110 train_time:19644ms step_avg:33.87ms
step:581/2110 train_time:19677ms step_avg:33.87ms
step:582/2110 train_time:19710ms step_avg:33.87ms
step:583/2110 train_time:19743ms step_avg:33.86ms
step:584/2110 train_time:19776ms step_avg:33.86ms
step:585/2110 train_time:19809ms step_avg:33.86ms
step:586/2110 train_time:19842ms step_avg:33.86ms
step:587/2110 train_time:19875ms step_avg:33.86ms
step:588/2110 train_time:19909ms step_avg:33.86ms
step:589/2110 train_time:19940ms step_avg:33.85ms
step:590/2110 train_time:19973ms step_avg:33.85ms
step:591/2110 train_time:20006ms step_avg:33.85ms
step:592/2110 train_time:20039ms step_avg:33.85ms
step:593/2110 train_time:20072ms step_avg:33.85ms
step:594/2110 train_time:20105ms step_avg:33.85ms
step:595/2110 train_time:20138ms step_avg:33.85ms
step:596/2110 train_time:20171ms step_avg:33.84ms
step:597/2110 train_time:20204ms step_avg:33.84ms
step:598/2110 train_time:20237ms step_avg:33.84ms
step:599/2110 train_time:20271ms step_avg:33.84ms
step:600/2110 train_time:20304ms step_avg:33.84ms
step:601/2110 train_time:20337ms step_avg:33.84ms
step:602/2110 train_time:20370ms step_avg:33.84ms
step:603/2110 train_time:20403ms step_avg:33.84ms
step:604/2110 train_time:20436ms step_avg:33.84ms
step:605/2110 train_time:20469ms step_avg:33.83ms
step:606/2110 train_time:20502ms step_avg:33.83ms
step:607/2110 train_time:20535ms step_avg:33.83ms
step:608/2110 train_time:20568ms step_avg:33.83ms
step:609/2110 train_time:20601ms step_avg:33.83ms
step:610/2110 train_time:20634ms step_avg:33.83ms
step:611/2110 train_time:20667ms step_avg:33.82ms
step:612/2110 train_time:20699ms step_avg:33.82ms
step:613/2110 train_time:20733ms step_avg:33.82ms
step:614/2110 train_time:20766ms step_avg:33.82ms
step:615/2110 train_time:20799ms step_avg:33.82ms
step:616/2110 train_time:20832ms step_avg:33.82ms
step:617/2110 train_time:20865ms step_avg:33.82ms
step:618/2110 train_time:20897ms step_avg:33.81ms
step:619/2110 train_time:20930ms step_avg:33.81ms
step:620/2110 train_time:20963ms step_avg:33.81ms
step:621/2110 train_time:20996ms step_avg:33.81ms
step:622/2110 train_time:21029ms step_avg:33.81ms
step:623/2110 train_time:21062ms step_avg:33.81ms
step:624/2110 train_time:21095ms step_avg:33.81ms
step:625/2110 train_time:21128ms step_avg:33.80ms
step:626/2110 train_time:21161ms step_avg:33.80ms
step:627/2110 train_time:21194ms step_avg:33.80ms
step:628/2110 train_time:21227ms step_avg:33.80ms
step:629/2110 train_time:21260ms step_avg:33.80ms
step:630/2110 train_time:21292ms step_avg:33.80ms
step:631/2110 train_time:21325ms step_avg:33.80ms
step:632/2110 train_time:21359ms step_avg:33.80ms
step:633/2110 train_time:21392ms step_avg:33.79ms
step:634/2110 train_time:21425ms step_avg:33.79ms
step:635/2110 train_time:21458ms step_avg:33.79ms
step:636/2110 train_time:21491ms step_avg:33.79ms
step:637/2110 train_time:21524ms step_avg:33.79ms
step:638/2110 train_time:21557ms step_avg:33.79ms
step:639/2110 train_time:21590ms step_avg:33.79ms
step:640/2110 train_time:21624ms step_avg:33.79ms
step:641/2110 train_time:21656ms step_avg:33.79ms
step:642/2110 train_time:21689ms step_avg:33.78ms
step:643/2110 train_time:21722ms step_avg:33.78ms
step:644/2110 train_time:21755ms step_avg:33.78ms
step:645/2110 train_time:21788ms step_avg:33.78ms
step:646/2110 train_time:21820ms step_avg:33.78ms
step:647/2110 train_time:21854ms step_avg:33.78ms
step:648/2110 train_time:21887ms step_avg:33.78ms
step:649/2110 train_time:21919ms step_avg:33.77ms
step:650/2110 train_time:21952ms step_avg:33.77ms
step:651/2110 train_time:21985ms step_avg:33.77ms
step:652/2110 train_time:22018ms step_avg:33.77ms
step:653/2110 train_time:22052ms step_avg:33.77ms
step:654/2110 train_time:22084ms step_avg:33.77ms
step:655/2110 train_time:22117ms step_avg:33.77ms
step:656/2110 train_time:22151ms step_avg:33.77ms
step:657/2110 train_time:22183ms step_avg:33.76ms
step:658/2110 train_time:22215ms step_avg:33.76ms
step:659/2110 train_time:22249ms step_avg:33.76ms
step:660/2110 train_time:22281ms step_avg:33.76ms
step:661/2110 train_time:22315ms step_avg:33.76ms
step:662/2110 train_time:22347ms step_avg:33.76ms
step:663/2110 train_time:22381ms step_avg:33.76ms
step:664/2110 train_time:22413ms step_avg:33.75ms
step:665/2110 train_time:22447ms step_avg:33.75ms
step:666/2110 train_time:22480ms step_avg:33.75ms
step:667/2110 train_time:22513ms step_avg:33.75ms
step:668/2110 train_time:22545ms step_avg:33.75ms
step:669/2110 train_time:22579ms step_avg:33.75ms
step:670/2110 train_time:22611ms step_avg:33.75ms
step:671/2110 train_time:22645ms step_avg:33.75ms
step:672/2110 train_time:22678ms step_avg:33.75ms
step:673/2110 train_time:22711ms step_avg:33.75ms
step:674/2110 train_time:22744ms step_avg:33.75ms
step:675/2110 train_time:22777ms step_avg:33.74ms
step:676/2110 train_time:22810ms step_avg:33.74ms
step:677/2110 train_time:22843ms step_avg:33.74ms
step:678/2110 train_time:22876ms step_avg:33.74ms
step:679/2110 train_time:22909ms step_avg:33.74ms
step:680/2110 train_time:22941ms step_avg:33.74ms
step:681/2110 train_time:22975ms step_avg:33.74ms
step:682/2110 train_time:23008ms step_avg:33.74ms
step:683/2110 train_time:23041ms step_avg:33.73ms
step:684/2110 train_time:23074ms step_avg:33.73ms
step:685/2110 train_time:23107ms step_avg:33.73ms
step:686/2110 train_time:23139ms step_avg:33.73ms
step:687/2110 train_time:23173ms step_avg:33.73ms
step:688/2110 train_time:23206ms step_avg:33.73ms
step:689/2110 train_time:23240ms step_avg:33.73ms
step:690/2110 train_time:23272ms step_avg:33.73ms
step:691/2110 train_time:23306ms step_avg:33.73ms
step:692/2110 train_time:23363ms step_avg:33.76ms
step:693/2110 train_time:23423ms step_avg:33.80ms
step:694/2110 train_time:23483ms step_avg:33.84ms
step:695/2110 train_time:23543ms step_avg:33.88ms
step:696/2110 train_time:23602ms step_avg:33.91ms
step:697/2110 train_time:23662ms step_avg:33.95ms
step:698/2110 train_time:23720ms step_avg:33.98ms
step:699/2110 train_time:23780ms step_avg:34.02ms
step:700/2110 train_time:23839ms step_avg:34.06ms
step:701/2110 train_time:23898ms step_avg:34.09ms
step:702/2110 train_time:23957ms step_avg:34.13ms
step:703/2110 train_time:24017ms step_avg:34.16ms
step:704/2110 train_time:24075ms step_avg:34.20ms
step:705/2110 train_time:24136ms step_avg:34.23ms
step:706/2110 train_time:24193ms step_avg:34.27ms
step:707/2110 train_time:24253ms step_avg:34.30ms
step:708/2110 train_time:24311ms step_avg:34.34ms
step:709/2110 train_time:24372ms step_avg:34.38ms
step:710/2110 train_time:24431ms step_avg:34.41ms
step:711/2110 train_time:24491ms step_avg:34.45ms
step:712/2110 train_time:24550ms step_avg:34.48ms
step:713/2110 train_time:24609ms step_avg:34.52ms
step:714/2110 train_time:24668ms step_avg:34.55ms
step:715/2110 train_time:24728ms step_avg:34.58ms
step:716/2110 train_time:24786ms step_avg:34.62ms
step:717/2110 train_time:24846ms step_avg:34.65ms
step:718/2110 train_time:24905ms step_avg:34.69ms
step:719/2110 train_time:24965ms step_avg:34.72ms
step:720/2110 train_time:25023ms step_avg:34.75ms
step:721/2110 train_time:25083ms step_avg:34.79ms
step:722/2110 train_time:25142ms step_avg:34.82ms
step:723/2110 train_time:25202ms step_avg:34.86ms
step:724/2110 train_time:25261ms step_avg:34.89ms
step:725/2110 train_time:25321ms step_avg:34.93ms
step:726/2110 train_time:25379ms step_avg:34.96ms
step:727/2110 train_time:25440ms step_avg:34.99ms
step:728/2110 train_time:25499ms step_avg:35.03ms
step:729/2110 train_time:25559ms step_avg:35.06ms
step:730/2110 train_time:25617ms step_avg:35.09ms
step:731/2110 train_time:25678ms step_avg:35.13ms
step:732/2110 train_time:25736ms step_avg:35.16ms
step:733/2110 train_time:25796ms step_avg:35.19ms
step:734/2110 train_time:25855ms step_avg:35.22ms
step:735/2110 train_time:25916ms step_avg:35.26ms
step:736/2110 train_time:25974ms step_avg:35.29ms
step:737/2110 train_time:26034ms step_avg:35.32ms
step:738/2110 train_time:26091ms step_avg:35.35ms
step:739/2110 train_time:26152ms step_avg:35.39ms
step:740/2110 train_time:26210ms step_avg:35.42ms
step:741/2110 train_time:26271ms step_avg:35.45ms
step:742/2110 train_time:26329ms step_avg:35.48ms
step:743/2110 train_time:26390ms step_avg:35.52ms
step:744/2110 train_time:26448ms step_avg:35.55ms
step:745/2110 train_time:26508ms step_avg:35.58ms
step:746/2110 train_time:26566ms step_avg:35.61ms
step:747/2110 train_time:26626ms step_avg:35.64ms
step:748/2110 train_time:26685ms step_avg:35.67ms
step:749/2110 train_time:26744ms step_avg:35.71ms
step:750/2110 train_time:26804ms step_avg:35.74ms
step:750/2110 val_loss:3.9046 train_time:26866ms step_avg:35.82ms
step:751/2110 train_time:26894ms step_avg:35.81ms
step:752/2110 train_time:26924ms step_avg:35.80ms
step:753/2110 train_time:26988ms step_avg:35.84ms
step:754/2110 train_time:27049ms step_avg:35.87ms
step:755/2110 train_time:27110ms step_avg:35.91ms
step:756/2110 train_time:27168ms step_avg:35.94ms
step:757/2110 train_time:27227ms step_avg:35.97ms
step:758/2110 train_time:27285ms step_avg:36.00ms
step:759/2110 train_time:27345ms step_avg:36.03ms
step:760/2110 train_time:27402ms step_avg:36.06ms
step:761/2110 train_time:27461ms step_avg:36.09ms
step:762/2110 train_time:27519ms step_avg:36.11ms
step:763/2110 train_time:27578ms step_avg:36.14ms
step:764/2110 train_time:27635ms step_avg:36.17ms
step:765/2110 train_time:27694ms step_avg:36.20ms
step:766/2110 train_time:27752ms step_avg:36.23ms
step:767/2110 train_time:27814ms step_avg:36.26ms
step:768/2110 train_time:27875ms step_avg:36.30ms
step:769/2110 train_time:27937ms step_avg:36.33ms
step:770/2110 train_time:27997ms step_avg:36.36ms
step:771/2110 train_time:28058ms step_avg:36.39ms
step:772/2110 train_time:28117ms step_avg:36.42ms
step:773/2110 train_time:28178ms step_avg:36.45ms
step:774/2110 train_time:28237ms step_avg:36.48ms
step:775/2110 train_time:28297ms step_avg:36.51ms
step:776/2110 train_time:28355ms step_avg:36.54ms
step:777/2110 train_time:28415ms step_avg:36.57ms
step:778/2110 train_time:28473ms step_avg:36.60ms
step:779/2110 train_time:28532ms step_avg:36.63ms
step:780/2110 train_time:28590ms step_avg:36.65ms
step:781/2110 train_time:28649ms step_avg:36.68ms
step:782/2110 train_time:28706ms step_avg:36.71ms
step:783/2110 train_time:28766ms step_avg:36.74ms
step:784/2110 train_time:28824ms step_avg:36.77ms
step:785/2110 train_time:28886ms step_avg:36.80ms
step:786/2110 train_time:28945ms step_avg:36.83ms
step:787/2110 train_time:29006ms step_avg:36.86ms
step:788/2110 train_time:29065ms step_avg:36.88ms
step:789/2110 train_time:29125ms step_avg:36.91ms
step:790/2110 train_time:29185ms step_avg:36.94ms
step:791/2110 train_time:29245ms step_avg:36.97ms
step:792/2110 train_time:29303ms step_avg:37.00ms
step:793/2110 train_time:29363ms step_avg:37.03ms
step:794/2110 train_time:29421ms step_avg:37.05ms
step:795/2110 train_time:29481ms step_avg:37.08ms
step:796/2110 train_time:29538ms step_avg:37.11ms
step:797/2110 train_time:29597ms step_avg:37.14ms
step:798/2110 train_time:29655ms step_avg:37.16ms
step:799/2110 train_time:29714ms step_avg:37.19ms
step:800/2110 train_time:29772ms step_avg:37.21ms
step:801/2110 train_time:29833ms step_avg:37.24ms
step:802/2110 train_time:29893ms step_avg:37.27ms
step:803/2110 train_time:29954ms step_avg:37.30ms
step:804/2110 train_time:30014ms step_avg:37.33ms
step:805/2110 train_time:30075ms step_avg:37.36ms
step:806/2110 train_time:30134ms step_avg:37.39ms
step:807/2110 train_time:30195ms step_avg:37.42ms
step:808/2110 train_time:30254ms step_avg:37.44ms
step:809/2110 train_time:30313ms step_avg:37.47ms
step:810/2110 train_time:30372ms step_avg:37.50ms
step:811/2110 train_time:30432ms step_avg:37.52ms
step:812/2110 train_time:30490ms step_avg:37.55ms
step:813/2110 train_time:30549ms step_avg:37.58ms
step:814/2110 train_time:30608ms step_avg:37.60ms
step:815/2110 train_time:30667ms step_avg:37.63ms
step:816/2110 train_time:30726ms step_avg:37.65ms
step:817/2110 train_time:30785ms step_avg:37.68ms
step:818/2110 train_time:30843ms step_avg:37.71ms
step:819/2110 train_time:30903ms step_avg:37.73ms
step:820/2110 train_time:30961ms step_avg:37.76ms
step:821/2110 train_time:31022ms step_avg:37.79ms
step:822/2110 train_time:31082ms step_avg:37.81ms
step:823/2110 train_time:31143ms step_avg:37.84ms
step:824/2110 train_time:31201ms step_avg:37.87ms
step:825/2110 train_time:31261ms step_avg:37.89ms
step:826/2110 train_time:31320ms step_avg:37.92ms
step:827/2110 train_time:31380ms step_avg:37.94ms
step:828/2110 train_time:31437ms step_avg:37.97ms
step:829/2110 train_time:31497ms step_avg:37.99ms
step:830/2110 train_time:31555ms step_avg:38.02ms
step:831/2110 train_time:31615ms step_avg:38.04ms
step:832/2110 train_time:31673ms step_avg:38.07ms
step:833/2110 train_time:31733ms step_avg:38.09ms
step:834/2110 train_time:31791ms step_avg:38.12ms
step:835/2110 train_time:31851ms step_avg:38.14ms
step:836/2110 train_time:31910ms step_avg:38.17ms
step:837/2110 train_time:31971ms step_avg:38.20ms
step:838/2110 train_time:32030ms step_avg:38.22ms
step:839/2110 train_time:32091ms step_avg:38.25ms
step:840/2110 train_time:32150ms step_avg:38.27ms
step:841/2110 train_time:32210ms step_avg:38.30ms
step:842/2110 train_time:32270ms step_avg:38.32ms
step:843/2110 train_time:32329ms step_avg:38.35ms
step:844/2110 train_time:32388ms step_avg:38.37ms
step:845/2110 train_time:32449ms step_avg:38.40ms
step:846/2110 train_time:32507ms step_avg:38.42ms
step:847/2110 train_time:32566ms step_avg:38.45ms
step:848/2110 train_time:32624ms step_avg:38.47ms
step:849/2110 train_time:32684ms step_avg:38.50ms
step:850/2110 train_time:32742ms step_avg:38.52ms
step:851/2110 train_time:32802ms step_avg:38.54ms
step:852/2110 train_time:32860ms step_avg:38.57ms
step:853/2110 train_time:32921ms step_avg:38.59ms
step:854/2110 train_time:32979ms step_avg:38.62ms
step:855/2110 train_time:33040ms step_avg:38.64ms
step:856/2110 train_time:33098ms step_avg:38.67ms
step:857/2110 train_time:33159ms step_avg:38.69ms
step:858/2110 train_time:33217ms step_avg:38.71ms
step:859/2110 train_time:33278ms step_avg:38.74ms
step:860/2110 train_time:33336ms step_avg:38.76ms
step:861/2110 train_time:33397ms step_avg:38.79ms
step:862/2110 train_time:33455ms step_avg:38.81ms
step:863/2110 train_time:33514ms step_avg:38.83ms
step:864/2110 train_time:33573ms step_avg:38.86ms
step:865/2110 train_time:33633ms step_avg:38.88ms
step:866/2110 train_time:33692ms step_avg:38.91ms
step:867/2110 train_time:33752ms step_avg:38.93ms
step:868/2110 train_time:33811ms step_avg:38.95ms
step:869/2110 train_time:33871ms step_avg:38.98ms
step:870/2110 train_time:33930ms step_avg:39.00ms
step:871/2110 train_time:33990ms step_avg:39.02ms
step:872/2110 train_time:34049ms step_avg:39.05ms
step:873/2110 train_time:34109ms step_avg:39.07ms
step:874/2110 train_time:34168ms step_avg:39.09ms
step:875/2110 train_time:34228ms step_avg:39.12ms
step:876/2110 train_time:34286ms step_avg:39.14ms
step:877/2110 train_time:34348ms step_avg:39.16ms
step:878/2110 train_time:34406ms step_avg:39.19ms
step:879/2110 train_time:34466ms step_avg:39.21ms
step:880/2110 train_time:34523ms step_avg:39.23ms
step:881/2110 train_time:34583ms step_avg:39.25ms
step:882/2110 train_time:34641ms step_avg:39.28ms
step:883/2110 train_time:34701ms step_avg:39.30ms
step:884/2110 train_time:34760ms step_avg:39.32ms
step:885/2110 train_time:34820ms step_avg:39.34ms
step:886/2110 train_time:34878ms step_avg:39.37ms
step:887/2110 train_time:34938ms step_avg:39.39ms
step:888/2110 train_time:34997ms step_avg:39.41ms
step:889/2110 train_time:35057ms step_avg:39.43ms
step:890/2110 train_time:35116ms step_avg:39.46ms
step:891/2110 train_time:35175ms step_avg:39.48ms
step:892/2110 train_time:35235ms step_avg:39.50ms
step:893/2110 train_time:35295ms step_avg:39.52ms
step:894/2110 train_time:35354ms step_avg:39.55ms
step:895/2110 train_time:35414ms step_avg:39.57ms
step:896/2110 train_time:35473ms step_avg:39.59ms
step:897/2110 train_time:35533ms step_avg:39.61ms
step:898/2110 train_time:35592ms step_avg:39.63ms
step:899/2110 train_time:35652ms step_avg:39.66ms
step:900/2110 train_time:35710ms step_avg:39.68ms
step:901/2110 train_time:35770ms step_avg:39.70ms
step:902/2110 train_time:35829ms step_avg:39.72ms
step:903/2110 train_time:35889ms step_avg:39.74ms
step:904/2110 train_time:35947ms step_avg:39.76ms
step:905/2110 train_time:36008ms step_avg:39.79ms
step:906/2110 train_time:36066ms step_avg:39.81ms
step:907/2110 train_time:36126ms step_avg:39.83ms
step:908/2110 train_time:36185ms step_avg:39.85ms
step:909/2110 train_time:36246ms step_avg:39.87ms
step:910/2110 train_time:36304ms step_avg:39.89ms
step:911/2110 train_time:36365ms step_avg:39.92ms
step:912/2110 train_time:36422ms step_avg:39.94ms
step:913/2110 train_time:36483ms step_avg:39.96ms
step:914/2110 train_time:36541ms step_avg:39.98ms
step:915/2110 train_time:36601ms step_avg:40.00ms
step:916/2110 train_time:36659ms step_avg:40.02ms
step:917/2110 train_time:36720ms step_avg:40.04ms
step:918/2110 train_time:36778ms step_avg:40.06ms
step:919/2110 train_time:36838ms step_avg:40.08ms
step:920/2110 train_time:36897ms step_avg:40.10ms
step:921/2110 train_time:36956ms step_avg:40.13ms
step:922/2110 train_time:37015ms step_avg:40.15ms
step:923/2110 train_time:37075ms step_avg:40.17ms
step:924/2110 train_time:37134ms step_avg:40.19ms
step:925/2110 train_time:37195ms step_avg:40.21ms
step:926/2110 train_time:37255ms step_avg:40.23ms
step:927/2110 train_time:37315ms step_avg:40.25ms
step:928/2110 train_time:37373ms step_avg:40.27ms
step:929/2110 train_time:37433ms step_avg:40.29ms
step:930/2110 train_time:37492ms step_avg:40.31ms
step:931/2110 train_time:37552ms step_avg:40.34ms
step:932/2110 train_time:37611ms step_avg:40.36ms
step:933/2110 train_time:37671ms step_avg:40.38ms
step:934/2110 train_time:37729ms step_avg:40.40ms
step:935/2110 train_time:37789ms step_avg:40.42ms
step:936/2110 train_time:37848ms step_avg:40.44ms
step:937/2110 train_time:37908ms step_avg:40.46ms
step:938/2110 train_time:37966ms step_avg:40.48ms
step:939/2110 train_time:38026ms step_avg:40.50ms
step:940/2110 train_time:38085ms step_avg:40.52ms
step:941/2110 train_time:38146ms step_avg:40.54ms
step:942/2110 train_time:38205ms step_avg:40.56ms
step:943/2110 train_time:38265ms step_avg:40.58ms
step:944/2110 train_time:38323ms step_avg:40.60ms
step:945/2110 train_time:38384ms step_avg:40.62ms
step:946/2110 train_time:38442ms step_avg:40.64ms
step:947/2110 train_time:38502ms step_avg:40.66ms
step:948/2110 train_time:38560ms step_avg:40.68ms
step:949/2110 train_time:38620ms step_avg:40.70ms
step:950/2110 train_time:38678ms step_avg:40.71ms
step:951/2110 train_time:38738ms step_avg:40.73ms
step:952/2110 train_time:38796ms step_avg:40.75ms
step:953/2110 train_time:38856ms step_avg:40.77ms
step:954/2110 train_time:38915ms step_avg:40.79ms
step:955/2110 train_time:38975ms step_avg:40.81ms
step:956/2110 train_time:39034ms step_avg:40.83ms
step:957/2110 train_time:39094ms step_avg:40.85ms
step:958/2110 train_time:39153ms step_avg:40.87ms
step:959/2110 train_time:39213ms step_avg:40.89ms
step:960/2110 train_time:39271ms step_avg:40.91ms
step:961/2110 train_time:39332ms step_avg:40.93ms
step:962/2110 train_time:39391ms step_avg:40.95ms
step:963/2110 train_time:39451ms step_avg:40.97ms
step:964/2110 train_time:39509ms step_avg:40.98ms
step:965/2110 train_time:39570ms step_avg:41.00ms
step:966/2110 train_time:39628ms step_avg:41.02ms
step:967/2110 train_time:39688ms step_avg:41.04ms
step:968/2110 train_time:39746ms step_avg:41.06ms
step:969/2110 train_time:39806ms step_avg:41.08ms
step:970/2110 train_time:39864ms step_avg:41.10ms
step:971/2110 train_time:39925ms step_avg:41.12ms
step:972/2110 train_time:39983ms step_avg:41.13ms
step:973/2110 train_time:40043ms step_avg:41.15ms
step:974/2110 train_time:40101ms step_avg:41.17ms
step:975/2110 train_time:40161ms step_avg:41.19ms
step:976/2110 train_time:40220ms step_avg:41.21ms
step:977/2110 train_time:40281ms step_avg:41.23ms
step:978/2110 train_time:40339ms step_avg:41.25ms
step:979/2110 train_time:40399ms step_avg:41.27ms
step:980/2110 train_time:40457ms step_avg:41.28ms
step:981/2110 train_time:40517ms step_avg:41.30ms
step:982/2110 train_time:40575ms step_avg:41.32ms
step:983/2110 train_time:40635ms step_avg:41.34ms
step:984/2110 train_time:40694ms step_avg:41.36ms
step:985/2110 train_time:40754ms step_avg:41.37ms
step:986/2110 train_time:40813ms step_avg:41.39ms
step:987/2110 train_time:40873ms step_avg:41.41ms
step:988/2110 train_time:40932ms step_avg:41.43ms
step:989/2110 train_time:40992ms step_avg:41.45ms
step:990/2110 train_time:41051ms step_avg:41.47ms
step:991/2110 train_time:41110ms step_avg:41.48ms
step:992/2110 train_time:41170ms step_avg:41.50ms
step:993/2110 train_time:41229ms step_avg:41.52ms
step:994/2110 train_time:41289ms step_avg:41.54ms
step:995/2110 train_time:41348ms step_avg:41.56ms
step:996/2110 train_time:41407ms step_avg:41.57ms
step:997/2110 train_time:41467ms step_avg:41.59ms
step:998/2110 train_time:41525ms step_avg:41.61ms
step:999/2110 train_time:41586ms step_avg:41.63ms
step:1000/2110 train_time:41645ms step_avg:41.64ms
step:1000/2110 val_loss:3.7548 train_time:41707ms step_avg:41.71ms
step:1001/2110 train_time:41735ms step_avg:41.69ms
step:1002/2110 train_time:41767ms step_avg:41.68ms
step:1003/2110 train_time:41829ms step_avg:41.70ms
step:1004/2110 train_time:41893ms step_avg:41.73ms
step:1005/2110 train_time:41955ms step_avg:41.75ms
step:1006/2110 train_time:42013ms step_avg:41.76ms
step:1007/2110 train_time:42072ms step_avg:41.78ms
step:1008/2110 train_time:42129ms step_avg:41.79ms
step:1009/2110 train_time:42189ms step_avg:41.81ms
step:1010/2110 train_time:42247ms step_avg:41.83ms
step:1011/2110 train_time:42306ms step_avg:41.85ms
step:1012/2110 train_time:42364ms step_avg:41.86ms
step:1013/2110 train_time:42422ms step_avg:41.88ms
step:1014/2110 train_time:42480ms step_avg:41.89ms
step:1015/2110 train_time:42539ms step_avg:41.91ms
step:1016/2110 train_time:42597ms step_avg:41.93ms
step:1017/2110 train_time:42658ms step_avg:41.94ms
step:1018/2110 train_time:42717ms step_avg:41.96ms
step:1019/2110 train_time:42779ms step_avg:41.98ms
step:1020/2110 train_time:42840ms step_avg:42.00ms
step:1021/2110 train_time:42902ms step_avg:42.02ms
step:1022/2110 train_time:42963ms step_avg:42.04ms
step:1023/2110 train_time:43022ms step_avg:42.06ms
step:1024/2110 train_time:43081ms step_avg:42.07ms
step:1025/2110 train_time:43141ms step_avg:42.09ms
step:1026/2110 train_time:43200ms step_avg:42.11ms
step:1027/2110 train_time:43260ms step_avg:42.12ms
step:1028/2110 train_time:43317ms step_avg:42.14ms
step:1029/2110 train_time:43376ms step_avg:42.15ms
step:1030/2110 train_time:43434ms step_avg:42.17ms
step:1031/2110 train_time:43494ms step_avg:42.19ms
step:1032/2110 train_time:43552ms step_avg:42.20ms
step:1033/2110 train_time:43611ms step_avg:42.22ms
step:1034/2110 train_time:43669ms step_avg:42.23ms
step:1035/2110 train_time:43730ms step_avg:42.25ms
step:1036/2110 train_time:43789ms step_avg:42.27ms
step:1037/2110 train_time:43850ms step_avg:42.29ms
step:1038/2110 train_time:43909ms step_avg:42.30ms
step:1039/2110 train_time:43970ms step_avg:42.32ms
step:1040/2110 train_time:44029ms step_avg:42.34ms
step:1041/2110 train_time:44089ms step_avg:42.35ms
step:1042/2110 train_time:44147ms step_avg:42.37ms
step:1043/2110 train_time:44207ms step_avg:42.38ms
step:1044/2110 train_time:44266ms step_avg:42.40ms
step:1045/2110 train_time:44325ms step_avg:42.42ms
step:1046/2110 train_time:44383ms step_avg:42.43ms
step:1047/2110 train_time:44443ms step_avg:42.45ms
step:1048/2110 train_time:44501ms step_avg:42.46ms
step:1049/2110 train_time:44560ms step_avg:42.48ms
step:1050/2110 train_time:44619ms step_avg:42.49ms
step:1051/2110 train_time:44679ms step_avg:42.51ms
step:1052/2110 train_time:44739ms step_avg:42.53ms
step:1053/2110 train_time:44799ms step_avg:42.54ms
step:1054/2110 train_time:44859ms step_avg:42.56ms
step:1055/2110 train_time:44919ms step_avg:42.58ms
step:1056/2110 train_time:44978ms step_avg:42.59ms
step:1057/2110 train_time:45039ms step_avg:42.61ms
step:1058/2110 train_time:45098ms step_avg:42.63ms
step:1059/2110 train_time:45157ms step_avg:42.64ms
step:1060/2110 train_time:45216ms step_avg:42.66ms
step:1061/2110 train_time:45276ms step_avg:42.67ms
step:1062/2110 train_time:45336ms step_avg:42.69ms
step:1063/2110 train_time:45396ms step_avg:42.71ms
step:1064/2110 train_time:45454ms step_avg:42.72ms
step:1065/2110 train_time:45513ms step_avg:42.74ms
step:1066/2110 train_time:45571ms step_avg:42.75ms
step:1067/2110 train_time:45631ms step_avg:42.77ms
step:1068/2110 train_time:45690ms step_avg:42.78ms
step:1069/2110 train_time:45750ms step_avg:42.80ms
step:1070/2110 train_time:45810ms step_avg:42.81ms
step:1071/2110 train_time:45868ms step_avg:42.83ms
step:1072/2110 train_time:45928ms step_avg:42.84ms
step:1073/2110 train_time:45988ms step_avg:42.86ms
step:1074/2110 train_time:46047ms step_avg:42.87ms
step:1075/2110 train_time:46108ms step_avg:42.89ms
step:1076/2110 train_time:46167ms step_avg:42.91ms
step:1077/2110 train_time:46227ms step_avg:42.92ms
step:1078/2110 train_time:46286ms step_avg:42.94ms
step:1079/2110 train_time:46346ms step_avg:42.95ms
step:1080/2110 train_time:46405ms step_avg:42.97ms
step:1081/2110 train_time:46464ms step_avg:42.98ms
step:1082/2110 train_time:46524ms step_avg:43.00ms
step:1083/2110 train_time:46582ms step_avg:43.01ms
step:1084/2110 train_time:46642ms step_avg:43.03ms
step:1085/2110 train_time:46702ms step_avg:43.04ms
step:1086/2110 train_time:46760ms step_avg:43.06ms
step:1087/2110 train_time:46820ms step_avg:43.07ms
step:1088/2110 train_time:46879ms step_avg:43.09ms
step:1089/2110 train_time:46940ms step_avg:43.10ms
step:1090/2110 train_time:46998ms step_avg:43.12ms
step:1091/2110 train_time:47059ms step_avg:43.13ms
step:1092/2110 train_time:47118ms step_avg:43.15ms
step:1093/2110 train_time:47178ms step_avg:43.16ms
step:1094/2110 train_time:47237ms step_avg:43.18ms
step:1095/2110 train_time:47297ms step_avg:43.19ms
step:1096/2110 train_time:47356ms step_avg:43.21ms
step:1097/2110 train_time:47416ms step_avg:43.22ms
step:1098/2110 train_time:47473ms step_avg:43.24ms
step:1099/2110 train_time:47533ms step_avg:43.25ms
step:1100/2110 train_time:47592ms step_avg:43.27ms
step:1101/2110 train_time:47652ms step_avg:43.28ms
step:1102/2110 train_time:47710ms step_avg:43.29ms
step:1103/2110 train_time:47770ms step_avg:43.31ms
step:1104/2110 train_time:47828ms step_avg:43.32ms
step:1105/2110 train_time:47888ms step_avg:43.34ms
step:1106/2110 train_time:47947ms step_avg:43.35ms
step:1107/2110 train_time:48007ms step_avg:43.37ms
step:1108/2110 train_time:48066ms step_avg:43.38ms
step:1109/2110 train_time:48126ms step_avg:43.40ms
step:1110/2110 train_time:48185ms step_avg:43.41ms
step:1111/2110 train_time:48246ms step_avg:43.43ms
step:1112/2110 train_time:48305ms step_avg:43.44ms
step:1113/2110 train_time:48365ms step_avg:43.45ms
step:1114/2110 train_time:48423ms step_avg:43.47ms
step:1115/2110 train_time:48484ms step_avg:43.48ms
step:1116/2110 train_time:48543ms step_avg:43.50ms
step:1117/2110 train_time:48602ms step_avg:43.51ms
step:1118/2110 train_time:48661ms step_avg:43.53ms
step:1119/2110 train_time:48720ms step_avg:43.54ms
step:1120/2110 train_time:48779ms step_avg:43.55ms
step:1121/2110 train_time:48839ms step_avg:43.57ms
step:1122/2110 train_time:48897ms step_avg:43.58ms
step:1123/2110 train_time:48957ms step_avg:43.59ms
step:1124/2110 train_time:49015ms step_avg:43.61ms
step:1125/2110 train_time:49075ms step_avg:43.62ms
step:1126/2110 train_time:49134ms step_avg:43.64ms
step:1127/2110 train_time:49194ms step_avg:43.65ms
step:1128/2110 train_time:49252ms step_avg:43.66ms
step:1129/2110 train_time:49312ms step_avg:43.68ms
step:1130/2110 train_time:49371ms step_avg:43.69ms
step:1131/2110 train_time:49432ms step_avg:43.71ms
step:1132/2110 train_time:49490ms step_avg:43.72ms
step:1133/2110 train_time:49550ms step_avg:43.73ms
step:1134/2110 train_time:49608ms step_avg:43.75ms
step:1135/2110 train_time:49669ms step_avg:43.76ms
step:1136/2110 train_time:49727ms step_avg:43.77ms
step:1137/2110 train_time:49788ms step_avg:43.79ms
step:1138/2110 train_time:49847ms step_avg:43.80ms
step:1139/2110 train_time:49907ms step_avg:43.82ms
step:1140/2110 train_time:49966ms step_avg:43.83ms
step:1141/2110 train_time:50025ms step_avg:43.84ms
step:1142/2110 train_time:50085ms step_avg:43.86ms
step:1143/2110 train_time:50145ms step_avg:43.87ms
step:1144/2110 train_time:50204ms step_avg:43.88ms
step:1145/2110 train_time:50265ms step_avg:43.90ms
step:1146/2110 train_time:50325ms step_avg:43.91ms
step:1147/2110 train_time:50385ms step_avg:43.93ms
step:1148/2110 train_time:50444ms step_avg:43.94ms
step:1149/2110 train_time:50504ms step_avg:43.95ms
step:1150/2110 train_time:50563ms step_avg:43.97ms
step:1151/2110 train_time:50623ms step_avg:43.98ms
step:1152/2110 train_time:50682ms step_avg:43.99ms
step:1153/2110 train_time:50743ms step_avg:44.01ms
step:1154/2110 train_time:50802ms step_avg:44.02ms
step:1155/2110 train_time:50862ms step_avg:44.04ms
step:1156/2110 train_time:50921ms step_avg:44.05ms
step:1157/2110 train_time:50981ms step_avg:44.06ms
step:1158/2110 train_time:51042ms step_avg:44.08ms
step:1159/2110 train_time:51103ms step_avg:44.09ms
step:1160/2110 train_time:51162ms step_avg:44.11ms
step:1161/2110 train_time:51222ms step_avg:44.12ms
step:1162/2110 train_time:51282ms step_avg:44.13ms
step:1163/2110 train_time:51343ms step_avg:44.15ms
step:1164/2110 train_time:51402ms step_avg:44.16ms
step:1165/2110 train_time:51463ms step_avg:44.17ms
step:1166/2110 train_time:51522ms step_avg:44.19ms
step:1167/2110 train_time:51583ms step_avg:44.20ms
step:1168/2110 train_time:51643ms step_avg:44.21ms
step:1169/2110 train_time:51704ms step_avg:44.23ms
step:1170/2110 train_time:51764ms step_avg:44.24ms
step:1171/2110 train_time:51824ms step_avg:44.26ms
step:1172/2110 train_time:51883ms step_avg:44.27ms
step:1173/2110 train_time:51943ms step_avg:44.28ms
step:1174/2110 train_time:52002ms step_avg:44.30ms
step:1175/2110 train_time:52063ms step_avg:44.31ms
step:1176/2110 train_time:52122ms step_avg:44.32ms
step:1177/2110 train_time:52183ms step_avg:44.34ms
step:1178/2110 train_time:52242ms step_avg:44.35ms
step:1179/2110 train_time:52303ms step_avg:44.36ms
step:1180/2110 train_time:52363ms step_avg:44.38ms
step:1181/2110 train_time:52423ms step_avg:44.39ms
step:1182/2110 train_time:52483ms step_avg:44.40ms
step:1183/2110 train_time:52543ms step_avg:44.42ms
step:1184/2110 train_time:52602ms step_avg:44.43ms
step:1185/2110 train_time:52663ms step_avg:44.44ms
step:1186/2110 train_time:52723ms step_avg:44.45ms
step:1187/2110 train_time:52784ms step_avg:44.47ms
step:1188/2110 train_time:52844ms step_avg:44.48ms
step:1189/2110 train_time:52904ms step_avg:44.49ms
step:1190/2110 train_time:52963ms step_avg:44.51ms
step:1191/2110 train_time:53024ms step_avg:44.52ms
step:1192/2110 train_time:53082ms step_avg:44.53ms
step:1193/2110 train_time:53143ms step_avg:44.55ms
step:1194/2110 train_time:53203ms step_avg:44.56ms
step:1195/2110 train_time:53263ms step_avg:44.57ms
step:1196/2110 train_time:53323ms step_avg:44.58ms
step:1197/2110 train_time:53383ms step_avg:44.60ms
step:1198/2110 train_time:53443ms step_avg:44.61ms
step:1199/2110 train_time:53504ms step_avg:44.62ms
step:1200/2110 train_time:53563ms step_avg:44.64ms
step:1201/2110 train_time:53623ms step_avg:44.65ms
step:1202/2110 train_time:53683ms step_avg:44.66ms
step:1203/2110 train_time:53744ms step_avg:44.67ms
step:1204/2110 train_time:53802ms step_avg:44.69ms
step:1205/2110 train_time:53863ms step_avg:44.70ms
step:1206/2110 train_time:53923ms step_avg:44.71ms
step:1207/2110 train_time:53983ms step_avg:44.72ms
step:1208/2110 train_time:54042ms step_avg:44.74ms
step:1209/2110 train_time:54103ms step_avg:44.75ms
step:1210/2110 train_time:54162ms step_avg:44.76ms
step:1211/2110 train_time:54224ms step_avg:44.78ms
step:1212/2110 train_time:54284ms step_avg:44.79ms
step:1213/2110 train_time:54344ms step_avg:44.80ms
step:1214/2110 train_time:54404ms step_avg:44.81ms
step:1215/2110 train_time:54465ms step_avg:44.83ms
step:1216/2110 train_time:54525ms step_avg:44.84ms
step:1217/2110 train_time:54584ms step_avg:44.85ms
step:1218/2110 train_time:54644ms step_avg:44.86ms
step:1219/2110 train_time:54704ms step_avg:44.88ms
step:1220/2110 train_time:54763ms step_avg:44.89ms
step:1221/2110 train_time:54824ms step_avg:44.90ms
step:1222/2110 train_time:54883ms step_avg:44.91ms
step:1223/2110 train_time:54945ms step_avg:44.93ms
step:1224/2110 train_time:55005ms step_avg:44.94ms
step:1225/2110 train_time:55065ms step_avg:44.95ms
step:1226/2110 train_time:55125ms step_avg:44.96ms
step:1227/2110 train_time:55186ms step_avg:44.98ms
step:1228/2110 train_time:55245ms step_avg:44.99ms
step:1229/2110 train_time:55306ms step_avg:45.00ms
step:1230/2110 train_time:55365ms step_avg:45.01ms
step:1231/2110 train_time:55425ms step_avg:45.02ms
step:1232/2110 train_time:55485ms step_avg:45.04ms
step:1233/2110 train_time:55546ms step_avg:45.05ms
step:1234/2110 train_time:55605ms step_avg:45.06ms
step:1235/2110 train_time:55666ms step_avg:45.07ms
step:1236/2110 train_time:55725ms step_avg:45.08ms
step:1237/2110 train_time:55785ms step_avg:45.10ms
step:1238/2110 train_time:55845ms step_avg:45.11ms
step:1239/2110 train_time:55905ms step_avg:45.12ms
step:1240/2110 train_time:55964ms step_avg:45.13ms
step:1241/2110 train_time:56025ms step_avg:45.14ms
step:1242/2110 train_time:56084ms step_avg:45.16ms
step:1243/2110 train_time:56145ms step_avg:45.17ms
step:1244/2110 train_time:56205ms step_avg:45.18ms
step:1245/2110 train_time:56266ms step_avg:45.19ms
step:1246/2110 train_time:56325ms step_avg:45.20ms
step:1247/2110 train_time:56386ms step_avg:45.22ms
step:1248/2110 train_time:56447ms step_avg:45.23ms
step:1249/2110 train_time:56506ms step_avg:45.24ms
step:1250/2110 train_time:56566ms step_avg:45.25ms
step:1250/2110 val_loss:3.5897 train_time:56628ms step_avg:45.30ms
step:1251/2110 train_time:56654ms step_avg:45.29ms
step:1252/2110 train_time:56688ms step_avg:45.28ms
step:1253/2110 train_time:56752ms step_avg:45.29ms
step:1254/2110 train_time:56814ms step_avg:45.31ms
step:1255/2110 train_time:56875ms step_avg:45.32ms
step:1256/2110 train_time:56935ms step_avg:45.33ms
step:1257/2110 train_time:56994ms step_avg:45.34ms
step:1258/2110 train_time:57052ms step_avg:45.35ms
step:1259/2110 train_time:57112ms step_avg:45.36ms
step:1260/2110 train_time:57170ms step_avg:45.37ms
step:1261/2110 train_time:57231ms step_avg:45.39ms
step:1262/2110 train_time:57289ms step_avg:45.40ms
step:1263/2110 train_time:57348ms step_avg:45.41ms
step:1264/2110 train_time:57406ms step_avg:45.42ms
step:1265/2110 train_time:57466ms step_avg:45.43ms
step:1266/2110 train_time:57524ms step_avg:45.44ms
step:1267/2110 train_time:57587ms step_avg:45.45ms
step:1268/2110 train_time:57647ms step_avg:45.46ms
step:1269/2110 train_time:57710ms step_avg:45.48ms
step:1270/2110 train_time:57772ms step_avg:45.49ms
step:1271/2110 train_time:57834ms step_avg:45.50ms
step:1272/2110 train_time:57893ms step_avg:45.51ms
step:1273/2110 train_time:57954ms step_avg:45.53ms
step:1274/2110 train_time:58012ms step_avg:45.54ms
step:1275/2110 train_time:58072ms step_avg:45.55ms
step:1276/2110 train_time:58130ms step_avg:45.56ms
step:1277/2110 train_time:58191ms step_avg:45.57ms
step:1278/2110 train_time:58249ms step_avg:45.58ms
step:1279/2110 train_time:58308ms step_avg:45.59ms
step:1280/2110 train_time:58367ms step_avg:45.60ms
step:1281/2110 train_time:58427ms step_avg:45.61ms
step:1282/2110 train_time:58485ms step_avg:45.62ms
step:1283/2110 train_time:58546ms step_avg:45.63ms
step:1284/2110 train_time:58605ms step_avg:45.64ms
step:1285/2110 train_time:58667ms step_avg:45.65ms
step:1286/2110 train_time:58727ms step_avg:45.67ms
step:1287/2110 train_time:58789ms step_avg:45.68ms
step:1288/2110 train_time:58849ms step_avg:45.69ms
step:1289/2110 train_time:58911ms step_avg:45.70ms
step:1290/2110 train_time:58970ms step_avg:45.71ms
step:1291/2110 train_time:59031ms step_avg:45.73ms
step:1292/2110 train_time:59092ms step_avg:45.74ms
step:1293/2110 train_time:59150ms step_avg:45.75ms
step:1294/2110 train_time:59209ms step_avg:45.76ms
step:1295/2110 train_time:59268ms step_avg:45.77ms
step:1296/2110 train_time:59327ms step_avg:45.78ms
step:1297/2110 train_time:59386ms step_avg:45.79ms
step:1298/2110 train_time:59445ms step_avg:45.80ms
step:1299/2110 train_time:59506ms step_avg:45.81ms
step:1300/2110 train_time:59564ms step_avg:45.82ms
step:1301/2110 train_time:59625ms step_avg:45.83ms
step:1302/2110 train_time:59685ms step_avg:45.84ms
step:1303/2110 train_time:59746ms step_avg:45.85ms
step:1304/2110 train_time:59806ms step_avg:45.86ms
step:1305/2110 train_time:59867ms step_avg:45.88ms
step:1306/2110 train_time:59927ms step_avg:45.89ms
step:1307/2110 train_time:59988ms step_avg:45.90ms
step:1308/2110 train_time:60047ms step_avg:45.91ms
step:1309/2110 train_time:60108ms step_avg:45.92ms
step:1310/2110 train_time:60166ms step_avg:45.93ms
step:1311/2110 train_time:60226ms step_avg:45.94ms
step:1312/2110 train_time:60284ms step_avg:45.95ms
step:1313/2110 train_time:60344ms step_avg:45.96ms
step:1314/2110 train_time:60403ms step_avg:45.97ms
step:1315/2110 train_time:60462ms step_avg:45.98ms
step:1316/2110 train_time:60521ms step_avg:45.99ms
step:1317/2110 train_time:60581ms step_avg:46.00ms
step:1318/2110 train_time:60641ms step_avg:46.01ms
step:1319/2110 train_time:60702ms step_avg:46.02ms
step:1320/2110 train_time:60762ms step_avg:46.03ms
step:1321/2110 train_time:60823ms step_avg:46.04ms
step:1322/2110 train_time:60884ms step_avg:46.05ms
step:1323/2110 train_time:60945ms step_avg:46.07ms
step:1324/2110 train_time:61004ms step_avg:46.08ms
step:1325/2110 train_time:61065ms step_avg:46.09ms
step:1326/2110 train_time:61123ms step_avg:46.10ms
step:1327/2110 train_time:61184ms step_avg:46.11ms
step:1328/2110 train_time:61242ms step_avg:46.12ms
step:1329/2110 train_time:61302ms step_avg:46.13ms
step:1330/2110 train_time:61360ms step_avg:46.14ms
step:1331/2110 train_time:61420ms step_avg:46.15ms
step:1332/2110 train_time:61480ms step_avg:46.16ms
step:1333/2110 train_time:61541ms step_avg:46.17ms
step:1334/2110 train_time:61600ms step_avg:46.18ms
step:1335/2110 train_time:61660ms step_avg:46.19ms
step:1336/2110 train_time:61720ms step_avg:46.20ms
step:1337/2110 train_time:61781ms step_avg:46.21ms
step:1338/2110 train_time:61841ms step_avg:46.22ms
step:1339/2110 train_time:61901ms step_avg:46.23ms
step:1340/2110 train_time:61961ms step_avg:46.24ms
step:1341/2110 train_time:62022ms step_avg:46.25ms
step:1342/2110 train_time:62082ms step_avg:46.26ms
step:1343/2110 train_time:62142ms step_avg:46.27ms
step:1344/2110 train_time:62201ms step_avg:46.28ms
step:1345/2110 train_time:62262ms step_avg:46.29ms
step:1346/2110 train_time:62321ms step_avg:46.30ms
step:1347/2110 train_time:62381ms step_avg:46.31ms
step:1348/2110 train_time:62440ms step_avg:46.32ms
step:1349/2110 train_time:62500ms step_avg:46.33ms
step:1350/2110 train_time:62559ms step_avg:46.34ms
step:1351/2110 train_time:62619ms step_avg:46.35ms
step:1352/2110 train_time:62680ms step_avg:46.36ms
step:1353/2110 train_time:62741ms step_avg:46.37ms
step:1354/2110 train_time:62801ms step_avg:46.38ms
step:1355/2110 train_time:62862ms step_avg:46.39ms
step:1356/2110 train_time:62921ms step_avg:46.40ms
step:1357/2110 train_time:62982ms step_avg:46.41ms
step:1358/2110 train_time:63042ms step_avg:46.42ms
step:1359/2110 train_time:63102ms step_avg:46.43ms
step:1360/2110 train_time:63160ms step_avg:46.44ms
step:1361/2110 train_time:63221ms step_avg:46.45ms
step:1362/2110 train_time:63281ms step_avg:46.46ms
step:1363/2110 train_time:63341ms step_avg:46.47ms
step:1364/2110 train_time:63400ms step_avg:46.48ms
step:1365/2110 train_time:63460ms step_avg:46.49ms
step:1366/2110 train_time:63520ms step_avg:46.50ms
step:1367/2110 train_time:63580ms step_avg:46.51ms
step:1368/2110 train_time:63639ms step_avg:46.52ms
step:1369/2110 train_time:63699ms step_avg:46.53ms
step:1370/2110 train_time:63758ms step_avg:46.54ms
step:1371/2110 train_time:63819ms step_avg:46.55ms
step:1372/2110 train_time:63880ms step_avg:46.56ms
step:1373/2110 train_time:63941ms step_avg:46.57ms
step:1374/2110 train_time:64000ms step_avg:46.58ms
step:1375/2110 train_time:64061ms step_avg:46.59ms
step:1376/2110 train_time:64120ms step_avg:46.60ms
step:1377/2110 train_time:64181ms step_avg:46.61ms
step:1378/2110 train_time:64240ms step_avg:46.62ms
step:1379/2110 train_time:64301ms step_avg:46.63ms
step:1380/2110 train_time:64360ms step_avg:46.64ms
step:1381/2110 train_time:64420ms step_avg:46.65ms
step:1382/2110 train_time:64508ms step_avg:46.68ms
step:1383/2110 train_time:64595ms step_avg:46.71ms
step:1384/2110 train_time:64681ms step_avg:46.74ms
step:1385/2110 train_time:64769ms step_avg:46.76ms
step:1386/2110 train_time:64856ms step_avg:46.79ms
step:1387/2110 train_time:64943ms step_avg:46.82ms
step:1388/2110 train_time:65030ms step_avg:46.85ms
step:1389/2110 train_time:65117ms step_avg:46.88ms
step:1390/2110 train_time:65204ms step_avg:46.91ms
step:1391/2110 train_time:65291ms step_avg:46.94ms
step:1392/2110 train_time:65377ms step_avg:46.97ms
step:1393/2110 train_time:65464ms step_avg:46.99ms
step:1394/2110 train_time:65551ms step_avg:47.02ms
step:1395/2110 train_time:65638ms step_avg:47.05ms
step:1396/2110 train_time:65724ms step_avg:47.08ms
step:1397/2110 train_time:65812ms step_avg:47.11ms
step:1398/2110 train_time:65899ms step_avg:47.14ms
step:1399/2110 train_time:65985ms step_avg:47.17ms
step:1400/2110 train_time:66072ms step_avg:47.19ms
step:1401/2110 train_time:66159ms step_avg:47.22ms
step:1402/2110 train_time:66245ms step_avg:47.25ms
step:1403/2110 train_time:66333ms step_avg:47.28ms
step:1404/2110 train_time:66418ms step_avg:47.31ms
step:1405/2110 train_time:66506ms step_avg:47.34ms
step:1406/2110 train_time:66593ms step_avg:47.36ms
step:1407/2110 train_time:66680ms step_avg:47.39ms
step:1408/2110 train_time:66766ms step_avg:47.42ms
step:1409/2110 train_time:66853ms step_avg:47.45ms
step:1410/2110 train_time:66939ms step_avg:47.47ms
step:1411/2110 train_time:67027ms step_avg:47.50ms
step:1412/2110 train_time:67114ms step_avg:47.53ms
step:1413/2110 train_time:67201ms step_avg:47.56ms
step:1414/2110 train_time:67288ms step_avg:47.59ms
step:1415/2110 train_time:67376ms step_avg:47.62ms
step:1416/2110 train_time:67461ms step_avg:47.64ms
step:1417/2110 train_time:67548ms step_avg:47.67ms
step:1418/2110 train_time:67635ms step_avg:47.70ms
step:1419/2110 train_time:67722ms step_avg:47.73ms
step:1420/2110 train_time:67809ms step_avg:47.75ms
step:1421/2110 train_time:67897ms step_avg:47.78ms
step:1422/2110 train_time:67983ms step_avg:47.81ms
step:1423/2110 train_time:68070ms step_avg:47.84ms
step:1424/2110 train_time:68156ms step_avg:47.86ms
step:1425/2110 train_time:68243ms step_avg:47.89ms
step:1426/2110 train_time:68330ms step_avg:47.92ms
step:1427/2110 train_time:68417ms step_avg:47.94ms
step:1428/2110 train_time:68503ms step_avg:47.97ms
step:1429/2110 train_time:68590ms step_avg:48.00ms
step:1430/2110 train_time:68676ms step_avg:48.03ms
step:1431/2110 train_time:68764ms step_avg:48.05ms
step:1432/2110 train_time:68850ms step_avg:48.08ms
step:1433/2110 train_time:68938ms step_avg:48.11ms
step:1434/2110 train_time:69025ms step_avg:48.13ms
step:1435/2110 train_time:69113ms step_avg:48.16ms
step:1436/2110 train_time:69199ms step_avg:48.19ms
step:1437/2110 train_time:69287ms step_avg:48.22ms
step:1438/2110 train_time:69374ms step_avg:48.24ms
step:1439/2110 train_time:69460ms step_avg:48.27ms
step:1440/2110 train_time:69547ms step_avg:48.30ms
step:1441/2110 train_time:69636ms step_avg:48.32ms
step:1442/2110 train_time:69721ms step_avg:48.35ms
step:1443/2110 train_time:69809ms step_avg:48.38ms
step:1444/2110 train_time:69895ms step_avg:48.40ms
step:1445/2110 train_time:69982ms step_avg:48.43ms
step:1446/2110 train_time:70069ms step_avg:48.46ms
step:1447/2110 train_time:70157ms step_avg:48.48ms
step:1448/2110 train_time:70242ms step_avg:48.51ms
step:1449/2110 train_time:70330ms step_avg:48.54ms
step:1450/2110 train_time:70416ms step_avg:48.56ms
step:1451/2110 train_time:70504ms step_avg:48.59ms
step:1452/2110 train_time:70591ms step_avg:48.62ms
step:1453/2110 train_time:70678ms step_avg:48.64ms
step:1454/2110 train_time:70763ms step_avg:48.67ms
step:1455/2110 train_time:70851ms step_avg:48.69ms
step:1456/2110 train_time:70937ms step_avg:48.72ms
step:1457/2110 train_time:71025ms step_avg:48.75ms
step:1458/2110 train_time:71111ms step_avg:48.77ms
step:1459/2110 train_time:71198ms step_avg:48.80ms
step:1460/2110 train_time:71284ms step_avg:48.82ms
step:1461/2110 train_time:71372ms step_avg:48.85ms
step:1462/2110 train_time:71457ms step_avg:48.88ms
step:1463/2110 train_time:71545ms step_avg:48.90ms
step:1464/2110 train_time:71633ms step_avg:48.93ms
step:1465/2110 train_time:71719ms step_avg:48.96ms
step:1466/2110 train_time:71807ms step_avg:48.98ms
step:1467/2110 train_time:71893ms step_avg:49.01ms
step:1468/2110 train_time:71978ms step_avg:49.03ms
step:1469/2110 train_time:72066ms step_avg:49.06ms
step:1470/2110 train_time:72153ms step_avg:49.08ms
step:1471/2110 train_time:72240ms step_avg:49.11ms
step:1472/2110 train_time:72327ms step_avg:49.14ms
step:1473/2110 train_time:72414ms step_avg:49.16ms
step:1474/2110 train_time:72499ms step_avg:49.19ms
step:1475/2110 train_time:72587ms step_avg:49.21ms
step:1476/2110 train_time:72674ms step_avg:49.24ms
step:1477/2110 train_time:72761ms step_avg:49.26ms
step:1478/2110 train_time:72847ms step_avg:49.29ms
step:1479/2110 train_time:72935ms step_avg:49.31ms
step:1480/2110 train_time:73022ms step_avg:49.34ms
step:1481/2110 train_time:73110ms step_avg:49.37ms
step:1482/2110 train_time:73197ms step_avg:49.39ms
step:1483/2110 train_time:73283ms step_avg:49.42ms
step:1484/2110 train_time:73369ms step_avg:49.44ms
step:1485/2110 train_time:73457ms step_avg:49.47ms
step:1486/2110 train_time:73543ms step_avg:49.49ms
step:1487/2110 train_time:73630ms step_avg:49.52ms
step:1488/2110 train_time:73716ms step_avg:49.54ms
step:1489/2110 train_time:73803ms step_avg:49.57ms
step:1490/2110 train_time:73889ms step_avg:49.59ms
step:1491/2110 train_time:73976ms step_avg:49.62ms
step:1492/2110 train_time:74062ms step_avg:49.64ms
step:1493/2110 train_time:74150ms step_avg:49.66ms
step:1494/2110 train_time:74236ms step_avg:49.69ms
step:1495/2110 train_time:74323ms step_avg:49.71ms
step:1496/2110 train_time:74410ms step_avg:49.74ms
step:1497/2110 train_time:74497ms step_avg:49.76ms
step:1498/2110 train_time:74583ms step_avg:49.79ms
step:1499/2110 train_time:74670ms step_avg:49.81ms
step:1500/2110 train_time:74756ms step_avg:49.84ms
step:1500/2110 val_loss:3.4906 train_time:74844ms step_avg:49.90ms
step:1501/2110 train_time:74874ms step_avg:49.88ms
step:1502/2110 train_time:74933ms step_avg:49.89ms
step:1503/2110 train_time:75027ms step_avg:49.92ms
step:1504/2110 train_time:75113ms step_avg:49.94ms
step:1505/2110 train_time:75201ms step_avg:49.97ms
step:1506/2110 train_time:75286ms step_avg:49.99ms
step:1507/2110 train_time:75372ms step_avg:50.01ms
step:1508/2110 train_time:75458ms step_avg:50.04ms
step:1509/2110 train_time:75543ms step_avg:50.06ms
step:1510/2110 train_time:75629ms step_avg:50.09ms
step:1511/2110 train_time:75715ms step_avg:50.11ms
step:1512/2110 train_time:75802ms step_avg:50.13ms
step:1513/2110 train_time:75891ms step_avg:50.16ms
step:1514/2110 train_time:75980ms step_avg:50.18ms
step:1515/2110 train_time:76069ms step_avg:50.21ms
step:1516/2110 train_time:76155ms step_avg:50.23ms
step:1517/2110 train_time:76243ms step_avg:50.26ms
step:1518/2110 train_time:76328ms step_avg:50.28ms
step:1519/2110 train_time:76416ms step_avg:50.31ms
step:1520/2110 train_time:76500ms step_avg:50.33ms
step:1521/2110 train_time:76588ms step_avg:50.35ms
step:1522/2110 train_time:76673ms step_avg:50.38ms
step:1523/2110 train_time:76760ms step_avg:50.40ms
step:1524/2110 train_time:76848ms step_avg:50.42ms
step:1525/2110 train_time:76936ms step_avg:50.45ms
step:1526/2110 train_time:77024ms step_avg:50.47ms
step:1527/2110 train_time:77112ms step_avg:50.50ms
step:1528/2110 train_time:77199ms step_avg:50.52ms
step:1529/2110 train_time:77287ms step_avg:50.55ms
step:1530/2110 train_time:77372ms step_avg:50.57ms
step:1531/2110 train_time:77459ms step_avg:50.59ms
step:1532/2110 train_time:77544ms step_avg:50.62ms
step:1533/2110 train_time:77632ms step_avg:50.64ms
step:1534/2110 train_time:77717ms step_avg:50.66ms
step:1535/2110 train_time:77804ms step_avg:50.69ms
step:1536/2110 train_time:77891ms step_avg:50.71ms
step:1537/2110 train_time:77980ms step_avg:50.74ms
step:1538/2110 train_time:78067ms step_avg:50.76ms
step:1539/2110 train_time:78155ms step_avg:50.78ms
step:1540/2110 train_time:78241ms step_avg:50.81ms
step:1541/2110 train_time:78329ms step_avg:50.83ms
step:1542/2110 train_time:78415ms step_avg:50.85ms
step:1543/2110 train_time:78501ms step_avg:50.88ms
step:1544/2110 train_time:78587ms step_avg:50.90ms
step:1545/2110 train_time:78674ms step_avg:50.92ms
step:1546/2110 train_time:78759ms step_avg:50.94ms
step:1547/2110 train_time:78848ms step_avg:50.97ms
step:1548/2110 train_time:78935ms step_avg:50.99ms
step:1549/2110 train_time:79022ms step_avg:51.01ms
step:1550/2110 train_time:79108ms step_avg:51.04ms
step:1551/2110 train_time:79195ms step_avg:51.06ms
step:1552/2110 train_time:79280ms step_avg:51.08ms
step:1553/2110 train_time:79368ms step_avg:51.11ms
step:1554/2110 train_time:79454ms step_avg:51.13ms
step:1555/2110 train_time:79540ms step_avg:51.15ms
step:1556/2110 train_time:79625ms step_avg:51.17ms
step:1557/2110 train_time:79713ms step_avg:51.20ms
step:1558/2110 train_time:79800ms step_avg:51.22ms
step:1559/2110 train_time:79888ms step_avg:51.24ms
step:1560/2110 train_time:79974ms step_avg:51.27ms
step:1561/2110 train_time:80061ms step_avg:51.29ms
step:1562/2110 train_time:80147ms step_avg:51.31ms
step:1563/2110 train_time:80235ms step_avg:51.33ms
step:1564/2110 train_time:80321ms step_avg:51.36ms
step:1565/2110 train_time:80409ms step_avg:51.38ms
step:1566/2110 train_time:80496ms step_avg:51.40ms
step:1567/2110 train_time:80583ms step_avg:51.42ms
step:1568/2110 train_time:80669ms step_avg:51.45ms
step:1569/2110 train_time:80755ms step_avg:51.47ms
step:1570/2110 train_time:80841ms step_avg:51.49ms
step:1571/2110 train_time:80929ms step_avg:51.51ms
step:1572/2110 train_time:81015ms step_avg:51.54ms
step:1573/2110 train_time:81103ms step_avg:51.56ms
step:1574/2110 train_time:81189ms step_avg:51.58ms
step:1575/2110 train_time:81276ms step_avg:51.60ms
step:1576/2110 train_time:81362ms step_avg:51.63ms
step:1577/2110 train_time:81450ms step_avg:51.65ms
step:1578/2110 train_time:81536ms step_avg:51.67ms
step:1579/2110 train_time:81623ms step_avg:51.69ms
step:1580/2110 train_time:81709ms step_avg:51.71ms
step:1581/2110 train_time:81796ms step_avg:51.74ms
step:1582/2110 train_time:81883ms step_avg:51.76ms
step:1583/2110 train_time:81971ms step_avg:51.78ms
step:1584/2110 train_time:82057ms step_avg:51.80ms
step:1585/2110 train_time:82145ms step_avg:51.83ms
step:1586/2110 train_time:82231ms step_avg:51.85ms
step:1587/2110 train_time:82318ms step_avg:51.87ms
step:1588/2110 train_time:82404ms step_avg:51.89ms
step:1589/2110 train_time:82491ms step_avg:51.91ms
step:1590/2110 train_time:82577ms step_avg:51.94ms
step:1591/2110 train_time:82663ms step_avg:51.96ms
step:1592/2110 train_time:82749ms step_avg:51.98ms
step:1593/2110 train_time:82836ms step_avg:52.00ms
step:1594/2110 train_time:82922ms step_avg:52.02ms
step:1595/2110 train_time:83010ms step_avg:52.04ms
step:1596/2110 train_time:83096ms step_avg:52.07ms
step:1597/2110 train_time:83184ms step_avg:52.09ms
step:1598/2110 train_time:83271ms step_avg:52.11ms
step:1599/2110 train_time:83358ms step_avg:52.13ms
step:1600/2110 train_time:83443ms step_avg:52.15ms
step:1601/2110 train_time:83532ms step_avg:52.17ms
step:1602/2110 train_time:83618ms step_avg:52.20ms
step:1603/2110 train_time:83705ms step_avg:52.22ms
step:1604/2110 train_time:83791ms step_avg:52.24ms
step:1605/2110 train_time:83877ms step_avg:52.26ms
step:1606/2110 train_time:83963ms step_avg:52.28ms
step:1607/2110 train_time:84051ms step_avg:52.30ms
step:1608/2110 train_time:84137ms step_avg:52.32ms
step:1609/2110 train_time:84225ms step_avg:52.35ms
step:1610/2110 train_time:84311ms step_avg:52.37ms
step:1611/2110 train_time:84399ms step_avg:52.39ms
step:1612/2110 train_time:84484ms step_avg:52.41ms
step:1613/2110 train_time:84572ms step_avg:52.43ms
step:1614/2110 train_time:84658ms step_avg:52.45ms
step:1615/2110 train_time:84745ms step_avg:52.47ms
step:1616/2110 train_time:84831ms step_avg:52.49ms
step:1617/2110 train_time:84918ms step_avg:52.52ms
step:1618/2110 train_time:85003ms step_avg:52.54ms
step:1619/2110 train_time:85092ms step_avg:52.56ms
step:1620/2110 train_time:85177ms step_avg:52.58ms
step:1621/2110 train_time:85265ms step_avg:52.60ms
step:1622/2110 train_time:85351ms step_avg:52.62ms
step:1623/2110 train_time:85437ms step_avg:52.64ms
step:1624/2110 train_time:85523ms step_avg:52.66ms
step:1625/2110 train_time:85611ms step_avg:52.68ms
step:1626/2110 train_time:85697ms step_avg:52.70ms
step:1627/2110 train_time:85784ms step_avg:52.73ms
step:1628/2110 train_time:85870ms step_avg:52.75ms
step:1629/2110 train_time:85957ms step_avg:52.77ms
step:1630/2110 train_time:86043ms step_avg:52.79ms
step:1631/2110 train_time:86131ms step_avg:52.81ms
step:1632/2110 train_time:86216ms step_avg:52.83ms
step:1633/2110 train_time:86304ms step_avg:52.85ms
step:1634/2110 train_time:86389ms step_avg:52.87ms
step:1635/2110 train_time:86477ms step_avg:52.89ms
step:1636/2110 train_time:86563ms step_avg:52.91ms
step:1637/2110 train_time:86651ms step_avg:52.93ms
step:1638/2110 train_time:86737ms step_avg:52.95ms
step:1639/2110 train_time:86824ms step_avg:52.97ms
step:1640/2110 train_time:86910ms step_avg:52.99ms
step:1641/2110 train_time:86998ms step_avg:53.02ms
step:1642/2110 train_time:87084ms step_avg:53.04ms
step:1643/2110 train_time:87172ms step_avg:53.06ms
step:1644/2110 train_time:87258ms step_avg:53.08ms
step:1645/2110 train_time:87345ms step_avg:53.10ms
step:1646/2110 train_time:87431ms step_avg:53.12ms
step:1647/2110 train_time:87518ms step_avg:53.14ms
step:1648/2110 train_time:87604ms step_avg:53.16ms
step:1649/2110 train_time:87692ms step_avg:53.18ms
step:1650/2110 train_time:87778ms step_avg:53.20ms
step:1651/2110 train_time:87865ms step_avg:53.22ms
step:1652/2110 train_time:87951ms step_avg:53.24ms
step:1653/2110 train_time:88038ms step_avg:53.26ms
step:1654/2110 train_time:88123ms step_avg:53.28ms
step:1655/2110 train_time:88211ms step_avg:53.30ms
step:1656/2110 train_time:88297ms step_avg:53.32ms
step:1657/2110 train_time:88385ms step_avg:53.34ms
step:1658/2110 train_time:88473ms step_avg:53.36ms
step:1659/2110 train_time:88560ms step_avg:53.38ms
step:1660/2110 train_time:88648ms step_avg:53.40ms
step:1661/2110 train_time:88737ms step_avg:53.42ms
step:1662/2110 train_time:88824ms step_avg:53.44ms
step:1663/2110 train_time:88914ms step_avg:53.47ms
step:1664/2110 train_time:89000ms step_avg:53.49ms
step:1665/2110 train_time:89090ms step_avg:53.51ms
step:1666/2110 train_time:89178ms step_avg:53.53ms
step:1667/2110 train_time:89267ms step_avg:53.55ms
step:1668/2110 train_time:89354ms step_avg:53.57ms
step:1669/2110 train_time:89442ms step_avg:53.59ms
step:1670/2110 train_time:89529ms step_avg:53.61ms
step:1671/2110 train_time:89618ms step_avg:53.63ms
step:1672/2110 train_time:89705ms step_avg:53.65ms
step:1673/2110 train_time:89794ms step_avg:53.67ms
step:1674/2110 train_time:89880ms step_avg:53.69ms
step:1675/2110 train_time:89969ms step_avg:53.71ms
step:1676/2110 train_time:90056ms step_avg:53.73ms
step:1677/2110 train_time:90145ms step_avg:53.75ms
step:1678/2110 train_time:90231ms step_avg:53.77ms
step:1679/2110 train_time:90320ms step_avg:53.79ms
step:1680/2110 train_time:90408ms step_avg:53.81ms
step:1681/2110 train_time:90497ms step_avg:53.83ms
step:1682/2110 train_time:90583ms step_avg:53.85ms
step:1683/2110 train_time:90672ms step_avg:53.88ms
step:1684/2110 train_time:90760ms step_avg:53.90ms
step:1685/2110 train_time:90848ms step_avg:53.92ms
step:1686/2110 train_time:90935ms step_avg:53.94ms
step:1687/2110 train_time:91023ms step_avg:53.96ms
step:1688/2110 train_time:91111ms step_avg:53.98ms
step:1689/2110 train_time:91199ms step_avg:54.00ms
step:1690/2110 train_time:91286ms step_avg:54.02ms
step:1691/2110 train_time:91376ms step_avg:54.04ms
step:1692/2110 train_time:91462ms step_avg:54.06ms
step:1693/2110 train_time:91551ms step_avg:54.08ms
step:1694/2110 train_time:91637ms step_avg:54.10ms
step:1695/2110 train_time:91726ms step_avg:54.12ms
step:1696/2110 train_time:91813ms step_avg:54.13ms
step:1697/2110 train_time:91902ms step_avg:54.16ms
step:1698/2110 train_time:91988ms step_avg:54.17ms
step:1699/2110 train_time:92077ms step_avg:54.19ms
step:1700/2110 train_time:92163ms step_avg:54.21ms
step:1701/2110 train_time:92252ms step_avg:54.23ms
step:1702/2110 train_time:92340ms step_avg:54.25ms
step:1703/2110 train_time:92428ms step_avg:54.27ms
step:1704/2110 train_time:92515ms step_avg:54.29ms
step:1705/2110 train_time:92603ms step_avg:54.31ms
step:1706/2110 train_time:92691ms step_avg:54.33ms
step:1707/2110 train_time:92779ms step_avg:54.35ms
step:1708/2110 train_time:92866ms step_avg:54.37ms
step:1709/2110 train_time:92955ms step_avg:54.39ms
step:1710/2110 train_time:93041ms step_avg:54.41ms
step:1711/2110 train_time:93130ms step_avg:54.43ms
step:1712/2110 train_time:93218ms step_avg:54.45ms
step:1713/2110 train_time:93306ms step_avg:54.47ms
step:1714/2110 train_time:93393ms step_avg:54.49ms
step:1715/2110 train_time:93482ms step_avg:54.51ms
step:1716/2110 train_time:93569ms step_avg:54.53ms
step:1717/2110 train_time:93657ms step_avg:54.55ms
step:1718/2110 train_time:93744ms step_avg:54.57ms
step:1719/2110 train_time:93832ms step_avg:54.59ms
step:1720/2110 train_time:93919ms step_avg:54.60ms
step:1721/2110 train_time:94008ms step_avg:54.62ms
step:1722/2110 train_time:94096ms step_avg:54.64ms
step:1723/2110 train_time:94184ms step_avg:54.66ms
step:1724/2110 train_time:94271ms step_avg:54.68ms
step:1725/2110 train_time:94360ms step_avg:54.70ms
step:1726/2110 train_time:94447ms step_avg:54.72ms
step:1727/2110 train_time:94536ms step_avg:54.74ms
step:1728/2110 train_time:94623ms step_avg:54.76ms
step:1729/2110 train_time:94711ms step_avg:54.78ms
step:1730/2110 train_time:94798ms step_avg:54.80ms
step:1731/2110 train_time:94887ms step_avg:54.82ms
step:1732/2110 train_time:94974ms step_avg:54.83ms
step:1733/2110 train_time:95062ms step_avg:54.85ms
step:1734/2110 train_time:95149ms step_avg:54.87ms
step:1735/2110 train_time:95238ms step_avg:54.89ms
step:1736/2110 train_time:95326ms step_avg:54.91ms
step:1737/2110 train_time:95416ms step_avg:54.93ms
step:1738/2110 train_time:95503ms step_avg:54.95ms
step:1739/2110 train_time:95592ms step_avg:54.97ms
step:1740/2110 train_time:95680ms step_avg:54.99ms
step:1741/2110 train_time:95768ms step_avg:55.01ms
step:1742/2110 train_time:95856ms step_avg:55.03ms
step:1743/2110 train_time:95944ms step_avg:55.05ms
step:1744/2110 train_time:96032ms step_avg:55.06ms
step:1745/2110 train_time:96120ms step_avg:55.08ms
step:1746/2110 train_time:96207ms step_avg:55.10ms
step:1747/2110 train_time:96296ms step_avg:55.12ms
step:1748/2110 train_time:96383ms step_avg:55.14ms
step:1749/2110 train_time:96472ms step_avg:55.16ms
step:1750/2110 train_time:96560ms step_avg:55.18ms
step:1750/2110 val_loss:3.3763 train_time:96650ms step_avg:55.23ms
step:1751/2110 train_time:96675ms step_avg:55.21ms
step:1752/2110 train_time:96742ms step_avg:55.22ms
step:1753/2110 train_time:96836ms step_avg:55.24ms
step:1754/2110 train_time:96924ms step_avg:55.26ms
step:1755/2110 train_time:97011ms step_avg:55.28ms
step:1756/2110 train_time:97098ms step_avg:55.29ms
step:1757/2110 train_time:97185ms step_avg:55.31ms
step:1758/2110 train_time:97271ms step_avg:55.33ms
step:1759/2110 train_time:97359ms step_avg:55.35ms
step:1760/2110 train_time:97447ms step_avg:55.37ms
step:1761/2110 train_time:97534ms step_avg:55.39ms
step:1762/2110 train_time:97623ms step_avg:55.40ms
step:1763/2110 train_time:97716ms step_avg:55.43ms
step:1764/2110 train_time:97807ms step_avg:55.45ms
step:1765/2110 train_time:97897ms step_avg:55.47ms
step:1766/2110 train_time:97985ms step_avg:55.48ms
step:1767/2110 train_time:98072ms step_avg:55.50ms
step:1768/2110 train_time:98158ms step_avg:55.52ms
step:1769/2110 train_time:98246ms step_avg:55.54ms
step:1770/2110 train_time:98332ms step_avg:55.55ms
step:1771/2110 train_time:98420ms step_avg:55.57ms
step:1772/2110 train_time:98507ms step_avg:55.59ms
step:1773/2110 train_time:98597ms step_avg:55.61ms
step:1774/2110 train_time:98686ms step_avg:55.63ms
step:1775/2110 train_time:98776ms step_avg:55.65ms
step:1776/2110 train_time:98864ms step_avg:55.67ms
step:1777/2110 train_time:98954ms step_avg:55.69ms
step:1778/2110 train_time:99041ms step_avg:55.70ms
step:1779/2110 train_time:99128ms step_avg:55.72ms
step:1780/2110 train_time:99215ms step_avg:55.74ms
step:1781/2110 train_time:99303ms step_avg:55.76ms
step:1782/2110 train_time:99390ms step_avg:55.77ms
step:1783/2110 train_time:99478ms step_avg:55.79ms
step:1784/2110 train_time:99565ms step_avg:55.81ms
step:1785/2110 train_time:99654ms step_avg:55.83ms
step:1786/2110 train_time:99744ms step_avg:55.85ms
step:1787/2110 train_time:99833ms step_avg:55.87ms
step:1788/2110 train_time:99922ms step_avg:55.88ms
step:1789/2110 train_time:100010ms step_avg:55.90ms
step:1790/2110 train_time:100097ms step_avg:55.92ms
step:1791/2110 train_time:100185ms step_avg:55.94ms
step:1792/2110 train_time:100272ms step_avg:55.96ms
step:1793/2110 train_time:100360ms step_avg:55.97ms
step:1794/2110 train_time:100447ms step_avg:55.99ms
step:1795/2110 train_time:100535ms step_avg:56.01ms
step:1796/2110 train_time:100623ms step_avg:56.03ms
step:1797/2110 train_time:100713ms step_avg:56.04ms
step:1798/2110 train_time:100802ms step_avg:56.06ms
step:1799/2110 train_time:100890ms step_avg:56.08ms
step:1800/2110 train_time:100978ms step_avg:56.10ms
step:1801/2110 train_time:101066ms step_avg:56.12ms
step:1802/2110 train_time:101155ms step_avg:56.13ms
step:1803/2110 train_time:101243ms step_avg:56.15ms
step:1804/2110 train_time:101329ms step_avg:56.17ms
step:1805/2110 train_time:101417ms step_avg:56.19ms
step:1806/2110 train_time:101504ms step_avg:56.20ms
step:1807/2110 train_time:101594ms step_avg:56.22ms
step:1808/2110 train_time:101681ms step_avg:56.24ms
step:1809/2110 train_time:101771ms step_avg:56.26ms
step:1810/2110 train_time:101858ms step_avg:56.28ms
step:1811/2110 train_time:101950ms step_avg:56.29ms
step:1812/2110 train_time:102034ms step_avg:56.31ms
step:1813/2110 train_time:102123ms step_avg:56.33ms
step:1814/2110 train_time:102211ms step_avg:56.35ms
step:1815/2110 train_time:102298ms step_avg:56.36ms
step:1816/2110 train_time:102386ms step_avg:56.38ms
step:1817/2110 train_time:102473ms step_avg:56.40ms
step:1818/2110 train_time:102561ms step_avg:56.41ms
step:1819/2110 train_time:102650ms step_avg:56.43ms
step:1820/2110 train_time:102738ms step_avg:56.45ms
step:1821/2110 train_time:102827ms step_avg:56.47ms
step:1822/2110 train_time:102915ms step_avg:56.48ms
step:1823/2110 train_time:103003ms step_avg:56.50ms
step:1824/2110 train_time:103090ms step_avg:56.52ms
step:1825/2110 train_time:103179ms step_avg:56.54ms
step:1826/2110 train_time:103265ms step_avg:56.55ms
step:1827/2110 train_time:103354ms step_avg:56.57ms
step:1828/2110 train_time:103442ms step_avg:56.59ms
step:1829/2110 train_time:103530ms step_avg:56.60ms
step:1830/2110 train_time:103617ms step_avg:56.62ms
step:1831/2110 train_time:103706ms step_avg:56.64ms
step:1832/2110 train_time:103794ms step_avg:56.66ms
step:1833/2110 train_time:103882ms step_avg:56.67ms
step:1834/2110 train_time:103969ms step_avg:56.69ms
step:1835/2110 train_time:104058ms step_avg:56.71ms
step:1836/2110 train_time:104145ms step_avg:56.72ms
step:1837/2110 train_time:104233ms step_avg:56.74ms
step:1838/2110 train_time:104322ms step_avg:56.76ms
step:1839/2110 train_time:104410ms step_avg:56.78ms
step:1840/2110 train_time:104498ms step_avg:56.79ms
step:1841/2110 train_time:104587ms step_avg:56.81ms
step:1842/2110 train_time:104675ms step_avg:56.83ms
step:1843/2110 train_time:104763ms step_avg:56.84ms
step:1844/2110 train_time:104851ms step_avg:56.86ms
step:1845/2110 train_time:104940ms step_avg:56.88ms
step:1846/2110 train_time:105027ms step_avg:56.89ms
step:1847/2110 train_time:105116ms step_avg:56.91ms
step:1848/2110 train_time:105204ms step_avg:56.93ms
step:1849/2110 train_time:105293ms step_avg:56.95ms
step:1850/2110 train_time:105381ms step_avg:56.96ms
step:1851/2110 train_time:105469ms step_avg:56.98ms
step:1852/2110 train_time:105557ms step_avg:57.00ms
step:1853/2110 train_time:105645ms step_avg:57.01ms
step:1854/2110 train_time:105733ms step_avg:57.03ms
step:1855/2110 train_time:105822ms step_avg:57.05ms
step:1856/2110 train_time:105909ms step_avg:57.06ms
step:1857/2110 train_time:105998ms step_avg:57.08ms
step:1858/2110 train_time:106086ms step_avg:57.10ms
step:1859/2110 train_time:106175ms step_avg:57.11ms
step:1860/2110 train_time:106263ms step_avg:57.13ms
step:1861/2110 train_time:106351ms step_avg:57.15ms
step:1862/2110 train_time:106438ms step_avg:57.16ms
step:1863/2110 train_time:106526ms step_avg:57.18ms
step:1864/2110 train_time:106615ms step_avg:57.20ms
step:1865/2110 train_time:106703ms step_avg:57.21ms
step:1866/2110 train_time:106791ms step_avg:57.23ms
step:1867/2110 train_time:106879ms step_avg:57.25ms
step:1868/2110 train_time:106966ms step_avg:57.26ms
step:1869/2110 train_time:107055ms step_avg:57.28ms
step:1870/2110 train_time:107144ms step_avg:57.30ms
step:1871/2110 train_time:107231ms step_avg:57.31ms
step:1872/2110 train_time:107319ms step_avg:57.33ms
step:1873/2110 train_time:107408ms step_avg:57.35ms
step:1874/2110 train_time:107496ms step_avg:57.36ms
step:1875/2110 train_time:107586ms step_avg:57.38ms
step:1876/2110 train_time:107673ms step_avg:57.39ms
step:1877/2110 train_time:107761ms step_avg:57.41ms
step:1878/2110 train_time:107848ms step_avg:57.43ms
step:1879/2110 train_time:107937ms step_avg:57.44ms
step:1880/2110 train_time:108024ms step_avg:57.46ms
step:1881/2110 train_time:108114ms step_avg:57.48ms
step:1882/2110 train_time:108202ms step_avg:57.49ms
step:1883/2110 train_time:108290ms step_avg:57.51ms
step:1884/2110 train_time:108378ms step_avg:57.53ms
step:1885/2110 train_time:108465ms step_avg:57.54ms
step:1886/2110 train_time:108553ms step_avg:57.56ms
step:1887/2110 train_time:108641ms step_avg:57.57ms
step:1888/2110 train_time:108728ms step_avg:57.59ms
step:1889/2110 train_time:108817ms step_avg:57.61ms
step:1890/2110 train_time:108905ms step_avg:57.62ms
step:1891/2110 train_time:108993ms step_avg:57.64ms
step:1892/2110 train_time:109080ms step_avg:57.65ms
step:1893/2110 train_time:109168ms step_avg:57.67ms
step:1894/2110 train_time:109256ms step_avg:57.69ms
step:1895/2110 train_time:109345ms step_avg:57.70ms
step:1896/2110 train_time:109432ms step_avg:57.72ms
step:1897/2110 train_time:109521ms step_avg:57.73ms
step:1898/2110 train_time:109608ms step_avg:57.75ms
step:1899/2110 train_time:109697ms step_avg:57.77ms
step:1900/2110 train_time:109786ms step_avg:57.78ms
step:1901/2110 train_time:109874ms step_avg:57.80ms
step:1902/2110 train_time:109962ms step_avg:57.81ms
step:1903/2110 train_time:110050ms step_avg:57.83ms
step:1904/2110 train_time:110138ms step_avg:57.85ms
step:1905/2110 train_time:110227ms step_avg:57.86ms
step:1906/2110 train_time:110314ms step_avg:57.88ms
step:1907/2110 train_time:110403ms step_avg:57.89ms
step:1908/2110 train_time:110490ms step_avg:57.91ms
step:1909/2110 train_time:110579ms step_avg:57.92ms
step:1910/2110 train_time:110666ms step_avg:57.94ms
step:1911/2110 train_time:110755ms step_avg:57.96ms
step:1912/2110 train_time:110843ms step_avg:57.97ms
step:1913/2110 train_time:110931ms step_avg:57.99ms
step:1914/2110 train_time:111019ms step_avg:58.00ms
step:1915/2110 train_time:111107ms step_avg:58.02ms
step:1916/2110 train_time:111196ms step_avg:58.04ms
step:1917/2110 train_time:111285ms step_avg:58.05ms
step:1918/2110 train_time:111373ms step_avg:58.07ms
step:1919/2110 train_time:111461ms step_avg:58.08ms
step:1920/2110 train_time:111548ms step_avg:58.10ms
step:1921/2110 train_time:111637ms step_avg:58.11ms
step:1922/2110 train_time:111724ms step_avg:58.13ms
step:1923/2110 train_time:111813ms step_avg:58.14ms
step:1924/2110 train_time:111901ms step_avg:58.16ms
step:1925/2110 train_time:111989ms step_avg:58.18ms
step:1926/2110 train_time:112076ms step_avg:58.19ms
step:1927/2110 train_time:112165ms step_avg:58.21ms
step:1928/2110 train_time:112253ms step_avg:58.22ms
step:1929/2110 train_time:112340ms step_avg:58.24ms
step:1930/2110 train_time:112427ms step_avg:58.25ms
step:1931/2110 train_time:112516ms step_avg:58.27ms
step:1932/2110 train_time:112604ms step_avg:58.28ms
step:1933/2110 train_time:112692ms step_avg:58.30ms
step:1934/2110 train_time:112781ms step_avg:58.31ms
step:1935/2110 train_time:112869ms step_avg:58.33ms
step:1936/2110 train_time:112958ms step_avg:58.35ms
step:1937/2110 train_time:113046ms step_avg:58.36ms
step:1938/2110 train_time:113133ms step_avg:58.38ms
step:1939/2110 train_time:113222ms step_avg:58.39ms
step:1940/2110 train_time:113309ms step_avg:58.41ms
step:1941/2110 train_time:113398ms step_avg:58.42ms
step:1942/2110 train_time:113485ms step_avg:58.44ms
step:1943/2110 train_time:113575ms step_avg:58.45ms
step:1944/2110 train_time:113662ms step_avg:58.47ms
step:1945/2110 train_time:113751ms step_avg:58.48ms
step:1946/2110 train_time:113838ms step_avg:58.50ms
step:1947/2110 train_time:113926ms step_avg:58.51ms
step:1948/2110 train_time:114013ms step_avg:58.53ms
step:1949/2110 train_time:114103ms step_avg:58.54ms
step:1950/2110 train_time:114192ms step_avg:58.56ms
step:1951/2110 train_time:114281ms step_avg:58.58ms
step:1952/2110 train_time:114367ms step_avg:58.59ms
step:1953/2110 train_time:114456ms step_avg:58.61ms
step:1954/2110 train_time:114544ms step_avg:58.62ms
step:1955/2110 train_time:114633ms step_avg:58.64ms
step:1956/2110 train_time:114721ms step_avg:58.65ms
step:1957/2110 train_time:114809ms step_avg:58.67ms
step:1958/2110 train_time:114896ms step_avg:58.68ms
step:1959/2110 train_time:114984ms step_avg:58.70ms
step:1960/2110 train_time:115072ms step_avg:58.71ms
step:1961/2110 train_time:115161ms step_avg:58.73ms
step:1962/2110 train_time:115249ms step_avg:58.74ms
step:1963/2110 train_time:115337ms step_avg:58.76ms
step:1964/2110 train_time:115425ms step_avg:58.77ms
step:1965/2110 train_time:115514ms step_avg:58.79ms
step:1966/2110 train_time:115602ms step_avg:58.80ms
step:1967/2110 train_time:115690ms step_avg:58.82ms
step:1968/2110 train_time:115777ms step_avg:58.83ms
step:1969/2110 train_time:115866ms step_avg:58.85ms
step:1970/2110 train_time:115953ms step_avg:58.86ms
step:1971/2110 train_time:116042ms step_avg:58.87ms
step:1972/2110 train_time:116129ms step_avg:58.89ms
step:1973/2110 train_time:116218ms step_avg:58.90ms
step:1974/2110 train_time:116305ms step_avg:58.92ms
step:1975/2110 train_time:116394ms step_avg:58.93ms
step:1976/2110 train_time:116482ms step_avg:58.95ms
step:1977/2110 train_time:116572ms step_avg:58.96ms
step:1978/2110 train_time:116660ms step_avg:58.98ms
step:1979/2110 train_time:116748ms step_avg:58.99ms
step:1980/2110 train_time:116836ms step_avg:59.01ms
step:1981/2110 train_time:116924ms step_avg:59.02ms
step:1982/2110 train_time:117011ms step_avg:59.04ms
step:1983/2110 train_time:117100ms step_avg:59.05ms
step:1984/2110 train_time:117187ms step_avg:59.07ms
step:1985/2110 train_time:117275ms step_avg:59.08ms
step:1986/2110 train_time:117363ms step_avg:59.10ms
step:1987/2110 train_time:117452ms step_avg:59.11ms
step:1988/2110 train_time:117539ms step_avg:59.12ms
step:1989/2110 train_time:117627ms step_avg:59.14ms
step:1990/2110 train_time:117715ms step_avg:59.15ms
step:1991/2110 train_time:117803ms step_avg:59.17ms
step:1992/2110 train_time:117890ms step_avg:59.18ms
step:1993/2110 train_time:117979ms step_avg:59.20ms
step:1994/2110 train_time:118066ms step_avg:59.21ms
step:1995/2110 train_time:118155ms step_avg:59.23ms
step:1996/2110 train_time:118244ms step_avg:59.24ms
step:1997/2110 train_time:118332ms step_avg:59.25ms
step:1998/2110 train_time:118420ms step_avg:59.27ms
step:1999/2110 train_time:118509ms step_avg:59.28ms
step:2000/2110 train_time:118597ms step_avg:59.30ms
step:2000/2110 val_loss:3.3009 train_time:118687ms step_avg:59.34ms
step:2001/2110 train_time:118718ms step_avg:59.33ms
step:2002/2110 train_time:118780ms step_avg:59.33ms
step:2003/2110 train_time:118873ms step_avg:59.35ms
step:2004/2110 train_time:118962ms step_avg:59.36ms
step:2005/2110 train_time:119050ms step_avg:59.38ms
step:2006/2110 train_time:119137ms step_avg:59.39ms
step:2007/2110 train_time:119225ms step_avg:59.40ms
step:2008/2110 train_time:119310ms step_avg:59.42ms
step:2009/2110 train_time:119397ms step_avg:59.43ms
step:2010/2110 train_time:119484ms step_avg:59.44ms
step:2011/2110 train_time:119571ms step_avg:59.46ms
step:2012/2110 train_time:119660ms step_avg:59.47ms
step:2013/2110 train_time:119752ms step_avg:59.49ms
step:2014/2110 train_time:119841ms step_avg:59.50ms
step:2015/2110 train_time:119932ms step_avg:59.52ms
step:2016/2110 train_time:120019ms step_avg:59.53ms
step:2017/2110 train_time:120108ms step_avg:59.55ms
step:2018/2110 train_time:120195ms step_avg:59.56ms
step:2019/2110 train_time:120283ms step_avg:59.58ms
step:2020/2110 train_time:120369ms step_avg:59.59ms
step:2021/2110 train_time:120457ms step_avg:59.60ms
step:2022/2110 train_time:120544ms step_avg:59.62ms
step:2023/2110 train_time:120632ms step_avg:59.63ms
step:2024/2110 train_time:120722ms step_avg:59.65ms
step:2025/2110 train_time:120813ms step_avg:59.66ms
step:2026/2110 train_time:120902ms step_avg:59.68ms
step:2027/2110 train_time:120991ms step_avg:59.69ms
step:2028/2110 train_time:121079ms step_avg:59.70ms
step:2029/2110 train_time:121168ms step_avg:59.72ms
step:2030/2110 train_time:121255ms step_avg:59.73ms
step:2031/2110 train_time:121343ms step_avg:59.75ms
step:2032/2110 train_time:121430ms step_avg:59.76ms
step:2033/2110 train_time:121519ms step_avg:59.77ms
step:2034/2110 train_time:121607ms step_avg:59.79ms
step:2035/2110 train_time:121696ms step_avg:59.80ms
step:2036/2110 train_time:121787ms step_avg:59.82ms
step:2037/2110 train_time:121875ms step_avg:59.83ms
step:2038/2110 train_time:121963ms step_avg:59.84ms
step:2039/2110 train_time:122051ms step_avg:59.86ms
step:2040/2110 train_time:122139ms step_avg:59.87ms
step:2041/2110 train_time:122226ms step_avg:59.89ms
step:2042/2110 train_time:122313ms step_avg:59.90ms
step:2043/2110 train_time:122401ms step_avg:59.91ms
step:2044/2110 train_time:122488ms step_avg:59.93ms
step:2045/2110 train_time:122576ms step_avg:59.94ms
step:2046/2110 train_time:122664ms step_avg:59.95ms
step:2047/2110 train_time:122754ms step_avg:59.97ms
step:2048/2110 train_time:122842ms step_avg:59.98ms
step:2049/2110 train_time:122932ms step_avg:60.00ms
step:2050/2110 train_time:123019ms step_avg:60.01ms
step:2051/2110 train_time:123109ms step_avg:60.02ms
step:2052/2110 train_time:123196ms step_avg:60.04ms
step:2053/2110 train_time:123285ms step_avg:60.05ms
step:2054/2110 train_time:123373ms step_avg:60.06ms
step:2055/2110 train_time:123460ms step_avg:60.08ms
step:2056/2110 train_time:123546ms step_avg:60.09ms
step:2057/2110 train_time:123636ms step_avg:60.10ms
step:2058/2110 train_time:123725ms step_avg:60.12ms
step:2059/2110 train_time:123813ms step_avg:60.13ms
step:2060/2110 train_time:123902ms step_avg:60.15ms
step:2061/2110 train_time:123993ms step_avg:60.16ms
step:2062/2110 train_time:124078ms step_avg:60.17ms
step:2063/2110 train_time:124166ms step_avg:60.19ms
step:2064/2110 train_time:124256ms step_avg:60.20ms
step:2065/2110 train_time:124343ms step_avg:60.21ms
step:2066/2110 train_time:124431ms step_avg:60.23ms
step:2067/2110 train_time:124518ms step_avg:60.24ms
step:2068/2110 train_time:124606ms step_avg:60.25ms
step:2069/2110 train_time:124695ms step_avg:60.27ms
step:2070/2110 train_time:124782ms step_avg:60.28ms
step:2071/2110 train_time:124871ms step_avg:60.30ms
step:2072/2110 train_time:124959ms step_avg:60.31ms
step:2073/2110 train_time:125049ms step_avg:60.32ms
step:2074/2110 train_time:125137ms step_avg:60.34ms
step:2075/2110 train_time:125225ms step_avg:60.35ms
step:2076/2110 train_time:125313ms step_avg:60.36ms
step:2077/2110 train_time:125402ms step_avg:60.38ms
step:2078/2110 train_time:125490ms step_avg:60.39ms
step:2079/2110 train_time:125578ms step_avg:60.40ms
step:2080/2110 train_time:125665ms step_avg:60.42ms
step:2081/2110 train_time:125754ms step_avg:60.43ms
step:2082/2110 train_time:125842ms step_avg:60.44ms
step:2083/2110 train_time:125931ms step_avg:60.46ms
step:2084/2110 train_time:126018ms step_avg:60.47ms
step:2085/2110 train_time:126107ms step_avg:60.48ms
step:2086/2110 train_time:126195ms step_avg:60.50ms
step:2087/2110 train_time:126282ms step_avg:60.51ms
step:2088/2110 train_time:126370ms step_avg:60.52ms
step:2089/2110 train_time:126459ms step_avg:60.54ms
step:2090/2110 train_time:126547ms step_avg:60.55ms
step:2091/2110 train_time:126636ms step_avg:60.56ms
step:2092/2110 train_time:126723ms step_avg:60.58ms
step:2093/2110 train_time:126812ms step_avg:60.59ms
step:2094/2110 train_time:126901ms step_avg:60.60ms
step:2095/2110 train_time:126990ms step_avg:60.62ms
step:2096/2110 train_time:127077ms step_avg:60.63ms
step:2097/2110 train_time:127167ms step_avg:60.64ms
step:2098/2110 train_time:127254ms step_avg:60.66ms
step:2099/2110 train_time:127344ms step_avg:60.67ms
step:2100/2110 train_time:127431ms step_avg:60.68ms
step:2101/2110 train_time:127519ms step_avg:60.69ms
step:2102/2110 train_time:127607ms step_avg:60.71ms
step:2103/2110 train_time:127697ms step_avg:60.72ms
step:2104/2110 train_time:127784ms step_avg:60.73ms
step:2105/2110 train_time:127875ms step_avg:60.75ms
step:2106/2110 train_time:127962ms step_avg:60.76ms
step:2107/2110 train_time:128051ms step_avg:60.77ms
step:2108/2110 train_time:128138ms step_avg:60.79ms
step:2109/2110 train_time:128228ms step_avg:60.80ms
step:2110/2110 train_time:128316ms step_avg:60.81ms
step:2110/2110 val_loss:3.2762 train_time:128407ms step_avg:60.86ms
peak memory allocated: 30540 MiB reserved: 44176 MiB

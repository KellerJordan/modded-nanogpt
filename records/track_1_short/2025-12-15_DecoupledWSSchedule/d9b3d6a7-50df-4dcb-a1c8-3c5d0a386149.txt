import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 15 22:29:30 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   38C    P0            125W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   42C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            120W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   36C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   37C    P0            120W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              88      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              88      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2110 train_time:110ms step_avg:109.96ms
step:2/2110 train_time:146ms step_avg:73.18ms
step:3/2110 train_time:178ms step_avg:59.46ms
step:4/2110 train_time:210ms step_avg:52.58ms
step:5/2110 train_time:238ms step_avg:47.65ms
step:6/2110 train_time:441ms step_avg:73.44ms
step:7/2110 train_time:702ms step_avg:100.24ms
step:8/2110 train_time:734ms step_avg:91.76ms
step:9/2110 train_time:767ms step_avg:85.21ms
step:10/2110 train_time:799ms step_avg:79.94ms
step:11/2110 train_time:832ms step_avg:75.67ms
step:12/2110 train_time:865ms step_avg:72.10ms
step:13/2110 train_time:898ms step_avg:69.10ms
step:14/2110 train_time:931ms step_avg:66.51ms
step:15/2110 train_time:964ms step_avg:64.30ms
step:16/2110 train_time:997ms step_avg:62.33ms
step:17/2110 train_time:1031ms step_avg:60.63ms
step:18/2110 train_time:1063ms step_avg:59.08ms
step:19/2110 train_time:1097ms step_avg:57.72ms
step:20/2110 train_time:1130ms step_avg:56.48ms
step:21/2110 train_time:1162ms step_avg:55.35ms
step:22/2110 train_time:1195ms step_avg:54.33ms
step:23/2110 train_time:1229ms step_avg:53.43ms
step:24/2110 train_time:1262ms step_avg:52.57ms
step:25/2110 train_time:1295ms step_avg:51.81ms
step:26/2110 train_time:1328ms step_avg:51.07ms
step:27/2110 train_time:1361ms step_avg:50.41ms
step:28/2110 train_time:1394ms step_avg:49.77ms
step:29/2110 train_time:1427ms step_avg:49.20ms
step:30/2110 train_time:1459ms step_avg:48.65ms
step:31/2110 train_time:1493ms step_avg:48.17ms
step:32/2110 train_time:1526ms step_avg:47.69ms
step:33/2110 train_time:1559ms step_avg:47.25ms
step:34/2110 train_time:1593ms step_avg:46.84ms
step:35/2110 train_time:1627ms step_avg:46.47ms
step:36/2110 train_time:1660ms step_avg:46.12ms
step:37/2110 train_time:1694ms step_avg:45.80ms
step:38/2110 train_time:1728ms step_avg:45.46ms
step:39/2110 train_time:1761ms step_avg:45.16ms
step:40/2110 train_time:1794ms step_avg:44.86ms
step:41/2110 train_time:1828ms step_avg:44.58ms
step:42/2110 train_time:1861ms step_avg:44.30ms
step:43/2110 train_time:1894ms step_avg:44.05ms
step:44/2110 train_time:1927ms step_avg:43.79ms
step:45/2110 train_time:1960ms step_avg:43.56ms
step:46/2110 train_time:1993ms step_avg:43.32ms
step:47/2110 train_time:2026ms step_avg:43.12ms
step:48/2110 train_time:2059ms step_avg:42.90ms
step:49/2110 train_time:2093ms step_avg:42.71ms
step:50/2110 train_time:2125ms step_avg:42.50ms
step:51/2110 train_time:2159ms step_avg:42.34ms
step:52/2110 train_time:2192ms step_avg:42.15ms
step:53/2110 train_time:2225ms step_avg:41.99ms
step:54/2110 train_time:2258ms step_avg:41.81ms
step:55/2110 train_time:2291ms step_avg:41.66ms
step:56/2110 train_time:2324ms step_avg:41.50ms
step:57/2110 train_time:2357ms step_avg:41.36ms
step:58/2110 train_time:2390ms step_avg:41.21ms
step:59/2110 train_time:2423ms step_avg:41.07ms
step:60/2110 train_time:2456ms step_avg:40.94ms
step:61/2110 train_time:2490ms step_avg:40.82ms
step:62/2110 train_time:2523ms step_avg:40.69ms
step:63/2110 train_time:2557ms step_avg:40.58ms
step:64/2110 train_time:2589ms step_avg:40.46ms
step:65/2110 train_time:2623ms step_avg:40.36ms
step:66/2110 train_time:2657ms step_avg:40.25ms
step:67/2110 train_time:2691ms step_avg:40.16ms
step:68/2110 train_time:2724ms step_avg:40.07ms
step:69/2110 train_time:2757ms step_avg:39.96ms
step:70/2110 train_time:2790ms step_avg:39.86ms
step:71/2110 train_time:2824ms step_avg:39.77ms
step:72/2110 train_time:2857ms step_avg:39.68ms
step:73/2110 train_time:2891ms step_avg:39.60ms
step:74/2110 train_time:2924ms step_avg:39.51ms
step:75/2110 train_time:2957ms step_avg:39.43ms
step:76/2110 train_time:2991ms step_avg:39.36ms
step:77/2110 train_time:3023ms step_avg:39.27ms
step:78/2110 train_time:3056ms step_avg:39.18ms
step:79/2110 train_time:3089ms step_avg:39.11ms
step:80/2110 train_time:3122ms step_avg:39.03ms
step:81/2110 train_time:3156ms step_avg:38.96ms
step:82/2110 train_time:3188ms step_avg:38.88ms
step:83/2110 train_time:3222ms step_avg:38.82ms
step:84/2110 train_time:3255ms step_avg:38.75ms
step:85/2110 train_time:3288ms step_avg:38.68ms
step:86/2110 train_time:3321ms step_avg:38.61ms
step:87/2110 train_time:3354ms step_avg:38.55ms
step:88/2110 train_time:3387ms step_avg:38.48ms
step:89/2110 train_time:3420ms step_avg:38.42ms
step:90/2110 train_time:3452ms step_avg:38.36ms
step:91/2110 train_time:3486ms step_avg:38.31ms
step:92/2110 train_time:3519ms step_avg:38.25ms
step:93/2110 train_time:3552ms step_avg:38.19ms
step:94/2110 train_time:3585ms step_avg:38.14ms
step:95/2110 train_time:3618ms step_avg:38.09ms
step:96/2110 train_time:3651ms step_avg:38.03ms
step:97/2110 train_time:3685ms step_avg:37.99ms
step:98/2110 train_time:3718ms step_avg:37.94ms
step:99/2110 train_time:3752ms step_avg:37.89ms
step:100/2110 train_time:3784ms step_avg:37.84ms
step:101/2110 train_time:3818ms step_avg:37.80ms
step:102/2110 train_time:3851ms step_avg:37.75ms
step:103/2110 train_time:3884ms step_avg:37.71ms
step:104/2110 train_time:3917ms step_avg:37.66ms
step:105/2110 train_time:3950ms step_avg:37.62ms
step:106/2110 train_time:3983ms step_avg:37.57ms
step:107/2110 train_time:4016ms step_avg:37.54ms
step:108/2110 train_time:4049ms step_avg:37.49ms
step:109/2110 train_time:4082ms step_avg:37.45ms
step:110/2110 train_time:4115ms step_avg:37.41ms
step:111/2110 train_time:4148ms step_avg:37.37ms
step:112/2110 train_time:4181ms step_avg:37.33ms
step:113/2110 train_time:4214ms step_avg:37.29ms
step:114/2110 train_time:4246ms step_avg:37.25ms
step:115/2110 train_time:4280ms step_avg:37.21ms
step:116/2110 train_time:4313ms step_avg:37.18ms
step:117/2110 train_time:4346ms step_avg:37.15ms
step:118/2110 train_time:4379ms step_avg:37.11ms
step:119/2110 train_time:4414ms step_avg:37.09ms
step:120/2110 train_time:4446ms step_avg:37.05ms
step:121/2110 train_time:4480ms step_avg:37.02ms
step:122/2110 train_time:4512ms step_avg:36.99ms
step:123/2110 train_time:4546ms step_avg:36.96ms
step:124/2110 train_time:4581ms step_avg:36.94ms
step:125/2110 train_time:4618ms step_avg:36.94ms
step:126/2110 train_time:4651ms step_avg:36.91ms
step:127/2110 train_time:4684ms step_avg:36.88ms
step:128/2110 train_time:4717ms step_avg:36.85ms
step:129/2110 train_time:4753ms step_avg:36.85ms
step:130/2110 train_time:4786ms step_avg:36.81ms
step:131/2110 train_time:4820ms step_avg:36.80ms
step:132/2110 train_time:4853ms step_avg:36.77ms
step:133/2110 train_time:4887ms step_avg:36.74ms
step:134/2110 train_time:4921ms step_avg:36.72ms
step:135/2110 train_time:4961ms step_avg:36.75ms
step:136/2110 train_time:4997ms step_avg:36.74ms
step:137/2110 train_time:5033ms step_avg:36.74ms
step:138/2110 train_time:5065ms step_avg:36.70ms
step:139/2110 train_time:5099ms step_avg:36.68ms
step:140/2110 train_time:5132ms step_avg:36.66ms
step:141/2110 train_time:5165ms step_avg:36.63ms
step:142/2110 train_time:5198ms step_avg:36.60ms
step:143/2110 train_time:5231ms step_avg:36.58ms
step:144/2110 train_time:5263ms step_avg:36.55ms
step:145/2110 train_time:5297ms step_avg:36.53ms
step:146/2110 train_time:5330ms step_avg:36.50ms
step:147/2110 train_time:5362ms step_avg:36.48ms
step:148/2110 train_time:5394ms step_avg:36.45ms
step:149/2110 train_time:5428ms step_avg:36.43ms
step:150/2110 train_time:5461ms step_avg:36.41ms
step:151/2110 train_time:5494ms step_avg:36.38ms
step:152/2110 train_time:5526ms step_avg:36.36ms
step:153/2110 train_time:5559ms step_avg:36.34ms
step:154/2110 train_time:5592ms step_avg:36.31ms
step:155/2110 train_time:5625ms step_avg:36.29ms
step:156/2110 train_time:5658ms step_avg:36.27ms
step:157/2110 train_time:5692ms step_avg:36.25ms
step:158/2110 train_time:5724ms step_avg:36.23ms
step:159/2110 train_time:5758ms step_avg:36.21ms
step:160/2110 train_time:5790ms step_avg:36.19ms
step:161/2110 train_time:5824ms step_avg:36.17ms
step:162/2110 train_time:5857ms step_avg:36.15ms
step:163/2110 train_time:5890ms step_avg:36.14ms
step:164/2110 train_time:5923ms step_avg:36.11ms
step:165/2110 train_time:5957ms step_avg:36.10ms
step:166/2110 train_time:5990ms step_avg:36.09ms
step:167/2110 train_time:6023ms step_avg:36.07ms
step:168/2110 train_time:6057ms step_avg:36.05ms
step:169/2110 train_time:6090ms step_avg:36.04ms
step:170/2110 train_time:6123ms step_avg:36.02ms
step:171/2110 train_time:6156ms step_avg:36.00ms
step:172/2110 train_time:6189ms step_avg:35.98ms
step:173/2110 train_time:6222ms step_avg:35.97ms
step:174/2110 train_time:6255ms step_avg:35.95ms
step:175/2110 train_time:6288ms step_avg:35.93ms
step:176/2110 train_time:6320ms step_avg:35.91ms
step:177/2110 train_time:6354ms step_avg:35.90ms
step:178/2110 train_time:6386ms step_avg:35.88ms
step:179/2110 train_time:6419ms step_avg:35.86ms
step:180/2110 train_time:6452ms step_avg:35.84ms
step:181/2110 train_time:6485ms step_avg:35.83ms
step:182/2110 train_time:6518ms step_avg:35.81ms
step:183/2110 train_time:6551ms step_avg:35.80ms
step:184/2110 train_time:6585ms step_avg:35.79ms
step:185/2110 train_time:6617ms step_avg:35.77ms
step:186/2110 train_time:6649ms step_avg:35.75ms
step:187/2110 train_time:6683ms step_avg:35.74ms
step:188/2110 train_time:6715ms step_avg:35.72ms
step:189/2110 train_time:6749ms step_avg:35.71ms
step:190/2110 train_time:6781ms step_avg:35.69ms
step:191/2110 train_time:6815ms step_avg:35.68ms
step:192/2110 train_time:6847ms step_avg:35.66ms
step:193/2110 train_time:6881ms step_avg:35.65ms
step:194/2110 train_time:6914ms step_avg:35.64ms
step:195/2110 train_time:6947ms step_avg:35.63ms
step:196/2110 train_time:6980ms step_avg:35.61ms
step:197/2110 train_time:7014ms step_avg:35.60ms
step:198/2110 train_time:7047ms step_avg:35.59ms
step:199/2110 train_time:7080ms step_avg:35.58ms
step:200/2110 train_time:7113ms step_avg:35.56ms
step:201/2110 train_time:7146ms step_avg:35.55ms
step:202/2110 train_time:7178ms step_avg:35.54ms
step:203/2110 train_time:7212ms step_avg:35.53ms
step:204/2110 train_time:7244ms step_avg:35.51ms
step:205/2110 train_time:7277ms step_avg:35.50ms
step:206/2110 train_time:7310ms step_avg:35.49ms
step:207/2110 train_time:7343ms step_avg:35.48ms
step:208/2110 train_time:7376ms step_avg:35.46ms
step:209/2110 train_time:7409ms step_avg:35.45ms
step:210/2110 train_time:7442ms step_avg:35.44ms
step:211/2110 train_time:7475ms step_avg:35.43ms
step:212/2110 train_time:7507ms step_avg:35.41ms
step:213/2110 train_time:7541ms step_avg:35.40ms
step:214/2110 train_time:7573ms step_avg:35.39ms
step:215/2110 train_time:7607ms step_avg:35.38ms
step:216/2110 train_time:7639ms step_avg:35.37ms
step:217/2110 train_time:7673ms step_avg:35.36ms
step:218/2110 train_time:7705ms step_avg:35.35ms
step:219/2110 train_time:7739ms step_avg:35.34ms
step:220/2110 train_time:7772ms step_avg:35.33ms
step:221/2110 train_time:7805ms step_avg:35.32ms
step:222/2110 train_time:7837ms step_avg:35.30ms
step:223/2110 train_time:7870ms step_avg:35.29ms
step:224/2110 train_time:7903ms step_avg:35.28ms
step:225/2110 train_time:7937ms step_avg:35.27ms
step:226/2110 train_time:7970ms step_avg:35.26ms
step:227/2110 train_time:8003ms step_avg:35.25ms
step:228/2110 train_time:8035ms step_avg:35.24ms
step:229/2110 train_time:8069ms step_avg:35.23ms
step:230/2110 train_time:8101ms step_avg:35.22ms
step:231/2110 train_time:8135ms step_avg:35.22ms
step:232/2110 train_time:8167ms step_avg:35.20ms
step:233/2110 train_time:8201ms step_avg:35.20ms
step:234/2110 train_time:8233ms step_avg:35.18ms
step:235/2110 train_time:8266ms step_avg:35.18ms
step:236/2110 train_time:8299ms step_avg:35.16ms
step:237/2110 train_time:8332ms step_avg:35.16ms
step:238/2110 train_time:8365ms step_avg:35.15ms
step:239/2110 train_time:8398ms step_avg:35.14ms
step:240/2110 train_time:8431ms step_avg:35.13ms
step:241/2110 train_time:8464ms step_avg:35.12ms
step:242/2110 train_time:8497ms step_avg:35.11ms
step:243/2110 train_time:8530ms step_avg:35.10ms
step:244/2110 train_time:8562ms step_avg:35.09ms
step:245/2110 train_time:8596ms step_avg:35.08ms
step:246/2110 train_time:8628ms step_avg:35.07ms
step:247/2110 train_time:8661ms step_avg:35.07ms
step:248/2110 train_time:8694ms step_avg:35.06ms
step:249/2110 train_time:8728ms step_avg:35.05ms
step:250/2110 train_time:8760ms step_avg:35.04ms
step:250/2110 val_loss:4.2969 train_time:8796ms step_avg:35.18ms
step:251/2110 train_time:8820ms step_avg:35.14ms
step:252/2110 train_time:8842ms step_avg:35.09ms
step:253/2110 train_time:8867ms step_avg:35.05ms
step:254/2110 train_time:8899ms step_avg:35.04ms
step:255/2110 train_time:8935ms step_avg:35.04ms
step:256/2110 train_time:8969ms step_avg:35.04ms
step:257/2110 train_time:9003ms step_avg:35.03ms
step:258/2110 train_time:9036ms step_avg:35.02ms
step:259/2110 train_time:9069ms step_avg:35.02ms
step:260/2110 train_time:9102ms step_avg:35.01ms
step:261/2110 train_time:9136ms step_avg:35.00ms
step:262/2110 train_time:9169ms step_avg:35.00ms
step:263/2110 train_time:9202ms step_avg:34.99ms
step:264/2110 train_time:9235ms step_avg:34.98ms
step:265/2110 train_time:9268ms step_avg:34.97ms
step:266/2110 train_time:9300ms step_avg:34.96ms
step:267/2110 train_time:9333ms step_avg:34.96ms
step:268/2110 train_time:9366ms step_avg:34.95ms
step:269/2110 train_time:9399ms step_avg:34.94ms
step:270/2110 train_time:9431ms step_avg:34.93ms
step:271/2110 train_time:9464ms step_avg:34.92ms
step:272/2110 train_time:9497ms step_avg:34.92ms
step:273/2110 train_time:9530ms step_avg:34.91ms
step:274/2110 train_time:9562ms step_avg:34.90ms
step:275/2110 train_time:9595ms step_avg:34.89ms
step:276/2110 train_time:9628ms step_avg:34.89ms
step:277/2110 train_time:9661ms step_avg:34.88ms
step:278/2110 train_time:9693ms step_avg:34.87ms
step:279/2110 train_time:9726ms step_avg:34.86ms
step:280/2110 train_time:9759ms step_avg:34.85ms
step:281/2110 train_time:9792ms step_avg:34.85ms
step:282/2110 train_time:9825ms step_avg:34.84ms
step:283/2110 train_time:9859ms step_avg:34.84ms
step:284/2110 train_time:9892ms step_avg:34.83ms
step:285/2110 train_time:9926ms step_avg:34.83ms
step:286/2110 train_time:9959ms step_avg:34.82ms
step:287/2110 train_time:9993ms step_avg:34.82ms
step:288/2110 train_time:10026ms step_avg:34.81ms
step:289/2110 train_time:10059ms step_avg:34.81ms
step:290/2110 train_time:10092ms step_avg:34.80ms
step:291/2110 train_time:10126ms step_avg:34.80ms
step:292/2110 train_time:10158ms step_avg:34.79ms
step:293/2110 train_time:10192ms step_avg:34.78ms
step:294/2110 train_time:10225ms step_avg:34.78ms
step:295/2110 train_time:10258ms step_avg:34.77ms
step:296/2110 train_time:10290ms step_avg:34.76ms
step:297/2110 train_time:10324ms step_avg:34.76ms
step:298/2110 train_time:10356ms step_avg:34.75ms
step:299/2110 train_time:10389ms step_avg:34.75ms
step:300/2110 train_time:10422ms step_avg:34.74ms
step:301/2110 train_time:10455ms step_avg:34.73ms
step:302/2110 train_time:10487ms step_avg:34.73ms
step:303/2110 train_time:10521ms step_avg:34.72ms
step:304/2110 train_time:10553ms step_avg:34.71ms
step:305/2110 train_time:10586ms step_avg:34.71ms
step:306/2110 train_time:10619ms step_avg:34.70ms
step:307/2110 train_time:10652ms step_avg:34.70ms
step:308/2110 train_time:10684ms step_avg:34.69ms
step:309/2110 train_time:10717ms step_avg:34.68ms
step:310/2110 train_time:10750ms step_avg:34.68ms
step:311/2110 train_time:10783ms step_avg:34.67ms
step:312/2110 train_time:10816ms step_avg:34.67ms
step:313/2110 train_time:10849ms step_avg:34.66ms
step:314/2110 train_time:10882ms step_avg:34.66ms
step:315/2110 train_time:10916ms step_avg:34.65ms
step:316/2110 train_time:10948ms step_avg:34.65ms
step:317/2110 train_time:10982ms step_avg:34.64ms
step:318/2110 train_time:11015ms step_avg:34.64ms
step:319/2110 train_time:11049ms step_avg:34.64ms
step:320/2110 train_time:11081ms step_avg:34.63ms
step:321/2110 train_time:11115ms step_avg:34.63ms
step:322/2110 train_time:11148ms step_avg:34.62ms
step:323/2110 train_time:11181ms step_avg:34.62ms
step:324/2110 train_time:11214ms step_avg:34.61ms
step:325/2110 train_time:11247ms step_avg:34.61ms
step:326/2110 train_time:11279ms step_avg:34.60ms
step:327/2110 train_time:11313ms step_avg:34.59ms
step:328/2110 train_time:11345ms step_avg:34.59ms
step:329/2110 train_time:11378ms step_avg:34.58ms
step:330/2110 train_time:11411ms step_avg:34.58ms
step:331/2110 train_time:11445ms step_avg:34.58ms
step:332/2110 train_time:11477ms step_avg:34.57ms
step:333/2110 train_time:11510ms step_avg:34.57ms
step:334/2110 train_time:11543ms step_avg:34.56ms
step:335/2110 train_time:11576ms step_avg:34.56ms
step:336/2110 train_time:11609ms step_avg:34.55ms
step:337/2110 train_time:11642ms step_avg:34.55ms
step:338/2110 train_time:11675ms step_avg:34.54ms
step:339/2110 train_time:11708ms step_avg:34.54ms
step:340/2110 train_time:11740ms step_avg:34.53ms
step:341/2110 train_time:11774ms step_avg:34.53ms
step:342/2110 train_time:11807ms step_avg:34.52ms
step:343/2110 train_time:11840ms step_avg:34.52ms
step:344/2110 train_time:11872ms step_avg:34.51ms
step:345/2110 train_time:11906ms step_avg:34.51ms
step:346/2110 train_time:11938ms step_avg:34.50ms
step:347/2110 train_time:11972ms step_avg:34.50ms
step:348/2110 train_time:12005ms step_avg:34.50ms
step:349/2110 train_time:12038ms step_avg:34.49ms
step:350/2110 train_time:12071ms step_avg:34.49ms
step:351/2110 train_time:12105ms step_avg:34.49ms
step:352/2110 train_time:12137ms step_avg:34.48ms
step:353/2110 train_time:12170ms step_avg:34.48ms
step:354/2110 train_time:12203ms step_avg:34.47ms
step:355/2110 train_time:12236ms step_avg:34.47ms
step:356/2110 train_time:12269ms step_avg:34.46ms
step:357/2110 train_time:12303ms step_avg:34.46ms
step:358/2110 train_time:12336ms step_avg:34.46ms
step:359/2110 train_time:12369ms step_avg:34.45ms
step:360/2110 train_time:12402ms step_avg:34.45ms
step:361/2110 train_time:12435ms step_avg:34.45ms
step:362/2110 train_time:12467ms step_avg:34.44ms
step:363/2110 train_time:12500ms step_avg:34.44ms
step:364/2110 train_time:12533ms step_avg:34.43ms
step:365/2110 train_time:12566ms step_avg:34.43ms
step:366/2110 train_time:12599ms step_avg:34.42ms
step:367/2110 train_time:12632ms step_avg:34.42ms
step:368/2110 train_time:12665ms step_avg:34.41ms
step:369/2110 train_time:12698ms step_avg:34.41ms
step:370/2110 train_time:12730ms step_avg:34.41ms
step:371/2110 train_time:12764ms step_avg:34.40ms
step:372/2110 train_time:12797ms step_avg:34.40ms
step:373/2110 train_time:12830ms step_avg:34.40ms
step:374/2110 train_time:12863ms step_avg:34.39ms
step:375/2110 train_time:12896ms step_avg:34.39ms
step:376/2110 train_time:12929ms step_avg:34.39ms
step:377/2110 train_time:12963ms step_avg:34.38ms
step:378/2110 train_time:12995ms step_avg:34.38ms
step:379/2110 train_time:13028ms step_avg:34.38ms
step:380/2110 train_time:13061ms step_avg:34.37ms
step:381/2110 train_time:13094ms step_avg:34.37ms
step:382/2110 train_time:13127ms step_avg:34.36ms
step:383/2110 train_time:13160ms step_avg:34.36ms
step:384/2110 train_time:13193ms step_avg:34.36ms
step:385/2110 train_time:13227ms step_avg:34.35ms
step:386/2110 train_time:13259ms step_avg:34.35ms
step:387/2110 train_time:13293ms step_avg:34.35ms
step:388/2110 train_time:13325ms step_avg:34.34ms
step:389/2110 train_time:13359ms step_avg:34.34ms
step:390/2110 train_time:13391ms step_avg:34.34ms
step:391/2110 train_time:13425ms step_avg:34.33ms
step:392/2110 train_time:13457ms step_avg:34.33ms
step:393/2110 train_time:13491ms step_avg:34.33ms
step:394/2110 train_time:13524ms step_avg:34.32ms
step:395/2110 train_time:13557ms step_avg:34.32ms
step:396/2110 train_time:13590ms step_avg:34.32ms
step:397/2110 train_time:13623ms step_avg:34.31ms
step:398/2110 train_time:13655ms step_avg:34.31ms
step:399/2110 train_time:13689ms step_avg:34.31ms
step:400/2110 train_time:13722ms step_avg:34.31ms
step:401/2110 train_time:13755ms step_avg:34.30ms
step:402/2110 train_time:13787ms step_avg:34.30ms
step:403/2110 train_time:13820ms step_avg:34.29ms
step:404/2110 train_time:13853ms step_avg:34.29ms
step:405/2110 train_time:13886ms step_avg:34.29ms
step:406/2110 train_time:13919ms step_avg:34.28ms
step:407/2110 train_time:13952ms step_avg:34.28ms
step:408/2110 train_time:13985ms step_avg:34.28ms
step:409/2110 train_time:14018ms step_avg:34.27ms
step:410/2110 train_time:14051ms step_avg:34.27ms
step:411/2110 train_time:14084ms step_avg:34.27ms
step:412/2110 train_time:14117ms step_avg:34.27ms
step:413/2110 train_time:14150ms step_avg:34.26ms
step:414/2110 train_time:14183ms step_avg:34.26ms
step:415/2110 train_time:14216ms step_avg:34.26ms
step:416/2110 train_time:14249ms step_avg:34.25ms
step:417/2110 train_time:14282ms step_avg:34.25ms
step:418/2110 train_time:14315ms step_avg:34.25ms
step:419/2110 train_time:14348ms step_avg:34.24ms
step:420/2110 train_time:14381ms step_avg:34.24ms
step:421/2110 train_time:14414ms step_avg:34.24ms
step:422/2110 train_time:14447ms step_avg:34.23ms
step:423/2110 train_time:14480ms step_avg:34.23ms
step:424/2110 train_time:14513ms step_avg:34.23ms
step:425/2110 train_time:14546ms step_avg:34.23ms
step:426/2110 train_time:14579ms step_avg:34.22ms
step:427/2110 train_time:14612ms step_avg:34.22ms
step:428/2110 train_time:14645ms step_avg:34.22ms
step:429/2110 train_time:14678ms step_avg:34.21ms
step:430/2110 train_time:14710ms step_avg:34.21ms
step:431/2110 train_time:14744ms step_avg:34.21ms
step:432/2110 train_time:14776ms step_avg:34.20ms
step:433/2110 train_time:14810ms step_avg:34.20ms
step:434/2110 train_time:14843ms step_avg:34.20ms
step:435/2110 train_time:14876ms step_avg:34.20ms
step:436/2110 train_time:14909ms step_avg:34.20ms
step:437/2110 train_time:14943ms step_avg:34.19ms
step:438/2110 train_time:14975ms step_avg:34.19ms
step:439/2110 train_time:15009ms step_avg:34.19ms
step:440/2110 train_time:15042ms step_avg:34.19ms
step:441/2110 train_time:15075ms step_avg:34.18ms
step:442/2110 train_time:15108ms step_avg:34.18ms
step:443/2110 train_time:15141ms step_avg:34.18ms
step:444/2110 train_time:15174ms step_avg:34.18ms
step:445/2110 train_time:15208ms step_avg:34.17ms
step:446/2110 train_time:15241ms step_avg:34.17ms
step:447/2110 train_time:15274ms step_avg:34.17ms
step:448/2110 train_time:15308ms step_avg:34.17ms
step:449/2110 train_time:15340ms step_avg:34.16ms
step:450/2110 train_time:15372ms step_avg:34.16ms
step:451/2110 train_time:15406ms step_avg:34.16ms
step:452/2110 train_time:15439ms step_avg:34.16ms
step:453/2110 train_time:15472ms step_avg:34.15ms
step:454/2110 train_time:15504ms step_avg:34.15ms
step:455/2110 train_time:15538ms step_avg:34.15ms
step:456/2110 train_time:15570ms step_avg:34.15ms
step:457/2110 train_time:15603ms step_avg:34.14ms
step:458/2110 train_time:15636ms step_avg:34.14ms
step:459/2110 train_time:15669ms step_avg:34.14ms
step:460/2110 train_time:15701ms step_avg:34.13ms
step:461/2110 train_time:15735ms step_avg:34.13ms
step:462/2110 train_time:15768ms step_avg:34.13ms
step:463/2110 train_time:15801ms step_avg:34.13ms
step:464/2110 train_time:15834ms step_avg:34.12ms
step:465/2110 train_time:15867ms step_avg:34.12ms
step:466/2110 train_time:15900ms step_avg:34.12ms
step:467/2110 train_time:15933ms step_avg:34.12ms
step:468/2110 train_time:15966ms step_avg:34.11ms
step:469/2110 train_time:15999ms step_avg:34.11ms
step:470/2110 train_time:16032ms step_avg:34.11ms
step:471/2110 train_time:16065ms step_avg:34.11ms
step:472/2110 train_time:16098ms step_avg:34.11ms
step:473/2110 train_time:16132ms step_avg:34.10ms
step:474/2110 train_time:16164ms step_avg:34.10ms
step:475/2110 train_time:16198ms step_avg:34.10ms
step:476/2110 train_time:16230ms step_avg:34.10ms
step:477/2110 train_time:16264ms step_avg:34.10ms
step:478/2110 train_time:16297ms step_avg:34.09ms
step:479/2110 train_time:16329ms step_avg:34.09ms
step:480/2110 train_time:16362ms step_avg:34.09ms
step:481/2110 train_time:16395ms step_avg:34.09ms
step:482/2110 train_time:16428ms step_avg:34.08ms
step:483/2110 train_time:16462ms step_avg:34.08ms
step:484/2110 train_time:16494ms step_avg:34.08ms
step:485/2110 train_time:16528ms step_avg:34.08ms
step:486/2110 train_time:16560ms step_avg:34.07ms
step:487/2110 train_time:16594ms step_avg:34.07ms
step:488/2110 train_time:16626ms step_avg:34.07ms
step:489/2110 train_time:16659ms step_avg:34.07ms
step:490/2110 train_time:16692ms step_avg:34.06ms
step:491/2110 train_time:16725ms step_avg:34.06ms
step:492/2110 train_time:16758ms step_avg:34.06ms
step:493/2110 train_time:16791ms step_avg:34.06ms
step:494/2110 train_time:16824ms step_avg:34.06ms
step:495/2110 train_time:16857ms step_avg:34.06ms
step:496/2110 train_time:16890ms step_avg:34.05ms
step:497/2110 train_time:16923ms step_avg:34.05ms
step:498/2110 train_time:16956ms step_avg:34.05ms
step:499/2110 train_time:16989ms step_avg:34.05ms
step:500/2110 train_time:17022ms step_avg:34.04ms
step:500/2110 val_loss:4.0413 train_time:17058ms step_avg:34.12ms
step:501/2110 train_time:17081ms step_avg:34.09ms
step:502/2110 train_time:17105ms step_avg:34.07ms
step:503/2110 train_time:17137ms step_avg:34.07ms
step:504/2110 train_time:17165ms step_avg:34.06ms
step:505/2110 train_time:17196ms step_avg:34.05ms
step:506/2110 train_time:17229ms step_avg:34.05ms
step:507/2110 train_time:17263ms step_avg:34.05ms
step:508/2110 train_time:17296ms step_avg:34.05ms
step:509/2110 train_time:17330ms step_avg:34.05ms
step:510/2110 train_time:17363ms step_avg:34.04ms
step:511/2110 train_time:17396ms step_avg:34.04ms
step:512/2110 train_time:17429ms step_avg:34.04ms
step:513/2110 train_time:17462ms step_avg:34.04ms
step:514/2110 train_time:17494ms step_avg:34.04ms
step:515/2110 train_time:17527ms step_avg:34.03ms
step:516/2110 train_time:17560ms step_avg:34.03ms
step:517/2110 train_time:17593ms step_avg:34.03ms
step:518/2110 train_time:17625ms step_avg:34.03ms
step:519/2110 train_time:17658ms step_avg:34.02ms
step:520/2110 train_time:17691ms step_avg:34.02ms
step:521/2110 train_time:17723ms step_avg:34.02ms
step:522/2110 train_time:17756ms step_avg:34.02ms
step:523/2110 train_time:17789ms step_avg:34.01ms
step:524/2110 train_time:17821ms step_avg:34.01ms
step:525/2110 train_time:17855ms step_avg:34.01ms
step:526/2110 train_time:17887ms step_avg:34.01ms
step:527/2110 train_time:17920ms step_avg:34.00ms
step:528/2110 train_time:17952ms step_avg:34.00ms
step:529/2110 train_time:17986ms step_avg:34.00ms
step:530/2110 train_time:18020ms step_avg:34.00ms
step:531/2110 train_time:18052ms step_avg:34.00ms
step:532/2110 train_time:18086ms step_avg:34.00ms
step:533/2110 train_time:18120ms step_avg:34.00ms
step:534/2110 train_time:18154ms step_avg:34.00ms
step:535/2110 train_time:18186ms step_avg:33.99ms
step:536/2110 train_time:18221ms step_avg:34.00ms
step:537/2110 train_time:18253ms step_avg:33.99ms
step:538/2110 train_time:18287ms step_avg:33.99ms
step:539/2110 train_time:18320ms step_avg:33.99ms
step:540/2110 train_time:18354ms step_avg:33.99ms
step:541/2110 train_time:18386ms step_avg:33.98ms
step:542/2110 train_time:18421ms step_avg:33.99ms
step:543/2110 train_time:18452ms step_avg:33.98ms
step:544/2110 train_time:18486ms step_avg:33.98ms
step:545/2110 train_time:18518ms step_avg:33.98ms
step:546/2110 train_time:18552ms step_avg:33.98ms
step:547/2110 train_time:18584ms step_avg:33.97ms
step:548/2110 train_time:18618ms step_avg:33.97ms
step:549/2110 train_time:18650ms step_avg:33.97ms
step:550/2110 train_time:18684ms step_avg:33.97ms
step:551/2110 train_time:18717ms step_avg:33.97ms
step:552/2110 train_time:18749ms step_avg:33.97ms
step:553/2110 train_time:18782ms step_avg:33.96ms
step:554/2110 train_time:18814ms step_avg:33.96ms
step:555/2110 train_time:18847ms step_avg:33.96ms
step:556/2110 train_time:18881ms step_avg:33.96ms
step:557/2110 train_time:18913ms step_avg:33.95ms
step:558/2110 train_time:18946ms step_avg:33.95ms
step:559/2110 train_time:18978ms step_avg:33.95ms
step:560/2110 train_time:19012ms step_avg:33.95ms
step:561/2110 train_time:19044ms step_avg:33.95ms
step:562/2110 train_time:19078ms step_avg:33.95ms
step:563/2110 train_time:19111ms step_avg:33.94ms
step:564/2110 train_time:19145ms step_avg:33.94ms
step:565/2110 train_time:19178ms step_avg:33.94ms
step:566/2110 train_time:19212ms step_avg:33.94ms
step:567/2110 train_time:19244ms step_avg:33.94ms
step:568/2110 train_time:19277ms step_avg:33.94ms
step:569/2110 train_time:19310ms step_avg:33.94ms
step:570/2110 train_time:19345ms step_avg:33.94ms
step:571/2110 train_time:19377ms step_avg:33.94ms
step:572/2110 train_time:19410ms step_avg:33.93ms
step:573/2110 train_time:19442ms step_avg:33.93ms
step:574/2110 train_time:19476ms step_avg:33.93ms
step:575/2110 train_time:19509ms step_avg:33.93ms
step:576/2110 train_time:19542ms step_avg:33.93ms
step:577/2110 train_time:19575ms step_avg:33.92ms
step:578/2110 train_time:19608ms step_avg:33.92ms
step:579/2110 train_time:19641ms step_avg:33.92ms
step:580/2110 train_time:19674ms step_avg:33.92ms
step:581/2110 train_time:19708ms step_avg:33.92ms
step:582/2110 train_time:19740ms step_avg:33.92ms
step:583/2110 train_time:19774ms step_avg:33.92ms
step:584/2110 train_time:19807ms step_avg:33.92ms
step:585/2110 train_time:19839ms step_avg:33.91ms
step:586/2110 train_time:19871ms step_avg:33.91ms
step:587/2110 train_time:19904ms step_avg:33.91ms
step:588/2110 train_time:19937ms step_avg:33.91ms
step:589/2110 train_time:19970ms step_avg:33.90ms
step:590/2110 train_time:20003ms step_avg:33.90ms
step:591/2110 train_time:20036ms step_avg:33.90ms
step:592/2110 train_time:20070ms step_avg:33.90ms
step:593/2110 train_time:20102ms step_avg:33.90ms
step:594/2110 train_time:20134ms step_avg:33.90ms
step:595/2110 train_time:20167ms step_avg:33.89ms
step:596/2110 train_time:20201ms step_avg:33.89ms
step:597/2110 train_time:20235ms step_avg:33.89ms
step:598/2110 train_time:20269ms step_avg:33.89ms
step:599/2110 train_time:20301ms step_avg:33.89ms
step:600/2110 train_time:20334ms step_avg:33.89ms
step:601/2110 train_time:20367ms step_avg:33.89ms
step:602/2110 train_time:20402ms step_avg:33.89ms
step:603/2110 train_time:20434ms step_avg:33.89ms
step:604/2110 train_time:20467ms step_avg:33.88ms
step:605/2110 train_time:20500ms step_avg:33.88ms
step:606/2110 train_time:20533ms step_avg:33.88ms
step:607/2110 train_time:20565ms step_avg:33.88ms
step:608/2110 train_time:20599ms step_avg:33.88ms
step:609/2110 train_time:20631ms step_avg:33.88ms
step:610/2110 train_time:20664ms step_avg:33.88ms
step:611/2110 train_time:20698ms step_avg:33.87ms
step:612/2110 train_time:20731ms step_avg:33.87ms
step:613/2110 train_time:20763ms step_avg:33.87ms
step:614/2110 train_time:20796ms step_avg:33.87ms
step:615/2110 train_time:20829ms step_avg:33.87ms
step:616/2110 train_time:20963ms step_avg:34.03ms
step:617/2110 train_time:20997ms step_avg:34.03ms
step:618/2110 train_time:21032ms step_avg:34.03ms
step:619/2110 train_time:21062ms step_avg:34.03ms
step:620/2110 train_time:21097ms step_avg:34.03ms
step:621/2110 train_time:21127ms step_avg:34.02ms
step:622/2110 train_time:21162ms step_avg:34.02ms
step:623/2110 train_time:21208ms step_avg:34.04ms
step:624/2110 train_time:21244ms step_avg:34.05ms
step:625/2110 train_time:21275ms step_avg:34.04ms
step:626/2110 train_time:21308ms step_avg:34.04ms
step:627/2110 train_time:21339ms step_avg:34.03ms
step:628/2110 train_time:21373ms step_avg:34.03ms
step:629/2110 train_time:21404ms step_avg:34.03ms
step:630/2110 train_time:21440ms step_avg:34.03ms
step:631/2110 train_time:21470ms step_avg:34.03ms
step:632/2110 train_time:21505ms step_avg:34.03ms
step:633/2110 train_time:21535ms step_avg:34.02ms
step:634/2110 train_time:21569ms step_avg:34.02ms
step:635/2110 train_time:21601ms step_avg:34.02ms
step:636/2110 train_time:21634ms step_avg:34.02ms
step:637/2110 train_time:21666ms step_avg:34.01ms
step:638/2110 train_time:21699ms step_avg:34.01ms
step:639/2110 train_time:21732ms step_avg:34.01ms
step:640/2110 train_time:21764ms step_avg:34.01ms
step:641/2110 train_time:21796ms step_avg:34.00ms
step:642/2110 train_time:21829ms step_avg:34.00ms
step:643/2110 train_time:21862ms step_avg:34.00ms
step:644/2110 train_time:21896ms step_avg:34.00ms
step:645/2110 train_time:21929ms step_avg:34.00ms
step:646/2110 train_time:21962ms step_avg:34.00ms
step:647/2110 train_time:21996ms step_avg:34.00ms
step:648/2110 train_time:22028ms step_avg:33.99ms
step:649/2110 train_time:22061ms step_avg:33.99ms
step:650/2110 train_time:22095ms step_avg:33.99ms
step:651/2110 train_time:22127ms step_avg:33.99ms
step:652/2110 train_time:22161ms step_avg:33.99ms
step:653/2110 train_time:22193ms step_avg:33.99ms
step:654/2110 train_time:22227ms step_avg:33.99ms
step:655/2110 train_time:22261ms step_avg:33.99ms
step:656/2110 train_time:22295ms step_avg:33.99ms
step:657/2110 train_time:22328ms step_avg:33.98ms
step:658/2110 train_time:22361ms step_avg:33.98ms
step:659/2110 train_time:22394ms step_avg:33.98ms
step:660/2110 train_time:22427ms step_avg:33.98ms
step:661/2110 train_time:22461ms step_avg:33.98ms
step:662/2110 train_time:22494ms step_avg:33.98ms
step:663/2110 train_time:22525ms step_avg:33.97ms
step:664/2110 train_time:22558ms step_avg:33.97ms
step:665/2110 train_time:22591ms step_avg:33.97ms
step:666/2110 train_time:22624ms step_avg:33.97ms
step:667/2110 train_time:22656ms step_avg:33.97ms
step:668/2110 train_time:22689ms step_avg:33.97ms
step:669/2110 train_time:22721ms step_avg:33.96ms
step:670/2110 train_time:22756ms step_avg:33.96ms
step:671/2110 train_time:22787ms step_avg:33.96ms
step:672/2110 train_time:22821ms step_avg:33.96ms
step:673/2110 train_time:22853ms step_avg:33.96ms
step:674/2110 train_time:22887ms step_avg:33.96ms
step:675/2110 train_time:22920ms step_avg:33.96ms
step:676/2110 train_time:22954ms step_avg:33.96ms
step:677/2110 train_time:22986ms step_avg:33.95ms
step:678/2110 train_time:23020ms step_avg:33.95ms
step:679/2110 train_time:23052ms step_avg:33.95ms
step:680/2110 train_time:23087ms step_avg:33.95ms
step:681/2110 train_time:23118ms step_avg:33.95ms
step:682/2110 train_time:23151ms step_avg:33.95ms
step:683/2110 train_time:23184ms step_avg:33.94ms
step:684/2110 train_time:23218ms step_avg:33.94ms
step:685/2110 train_time:23251ms step_avg:33.94ms
step:686/2110 train_time:23285ms step_avg:33.94ms
step:687/2110 train_time:23318ms step_avg:33.94ms
step:688/2110 train_time:23351ms step_avg:33.94ms
step:689/2110 train_time:23383ms step_avg:33.94ms
step:690/2110 train_time:23418ms step_avg:33.94ms
step:691/2110 train_time:23450ms step_avg:33.94ms
step:692/2110 train_time:23508ms step_avg:33.97ms
step:693/2110 train_time:23567ms step_avg:34.01ms
step:694/2110 train_time:23627ms step_avg:34.04ms
step:695/2110 train_time:23685ms step_avg:34.08ms
step:696/2110 train_time:23744ms step_avg:34.12ms
step:697/2110 train_time:23803ms step_avg:34.15ms
step:698/2110 train_time:23862ms step_avg:34.19ms
step:699/2110 train_time:23922ms step_avg:34.22ms
step:700/2110 train_time:23982ms step_avg:34.26ms
step:701/2110 train_time:24041ms step_avg:34.30ms
step:702/2110 train_time:24101ms step_avg:34.33ms
step:703/2110 train_time:24160ms step_avg:34.37ms
step:704/2110 train_time:24219ms step_avg:34.40ms
step:705/2110 train_time:24279ms step_avg:34.44ms
step:706/2110 train_time:24338ms step_avg:34.47ms
step:707/2110 train_time:24398ms step_avg:34.51ms
step:708/2110 train_time:24457ms step_avg:34.54ms
step:709/2110 train_time:24516ms step_avg:34.58ms
step:710/2110 train_time:24574ms step_avg:34.61ms
step:711/2110 train_time:24634ms step_avg:34.65ms
step:712/2110 train_time:24693ms step_avg:34.68ms
step:713/2110 train_time:24752ms step_avg:34.71ms
step:714/2110 train_time:24811ms step_avg:34.75ms
step:715/2110 train_time:24870ms step_avg:34.78ms
step:716/2110 train_time:24929ms step_avg:34.82ms
step:717/2110 train_time:24988ms step_avg:34.85ms
step:718/2110 train_time:25048ms step_avg:34.89ms
step:719/2110 train_time:25107ms step_avg:34.92ms
step:720/2110 train_time:25167ms step_avg:34.95ms
step:721/2110 train_time:25227ms step_avg:34.99ms
step:722/2110 train_time:25287ms step_avg:35.02ms
step:723/2110 train_time:25347ms step_avg:35.06ms
step:724/2110 train_time:25407ms step_avg:35.09ms
step:725/2110 train_time:25466ms step_avg:35.13ms
step:726/2110 train_time:25525ms step_avg:35.16ms
step:727/2110 train_time:25585ms step_avg:35.19ms
step:728/2110 train_time:25644ms step_avg:35.23ms
step:729/2110 train_time:25703ms step_avg:35.26ms
step:730/2110 train_time:25762ms step_avg:35.29ms
step:731/2110 train_time:25822ms step_avg:35.32ms
step:732/2110 train_time:25881ms step_avg:35.36ms
step:733/2110 train_time:25940ms step_avg:35.39ms
step:734/2110 train_time:25999ms step_avg:35.42ms
step:735/2110 train_time:26059ms step_avg:35.45ms
step:736/2110 train_time:26117ms step_avg:35.49ms
step:737/2110 train_time:26178ms step_avg:35.52ms
step:738/2110 train_time:26237ms step_avg:35.55ms
step:739/2110 train_time:26296ms step_avg:35.58ms
step:740/2110 train_time:26354ms step_avg:35.61ms
step:741/2110 train_time:26414ms step_avg:35.65ms
step:742/2110 train_time:26473ms step_avg:35.68ms
step:743/2110 train_time:26533ms step_avg:35.71ms
step:744/2110 train_time:26592ms step_avg:35.74ms
step:745/2110 train_time:26651ms step_avg:35.77ms
step:746/2110 train_time:26710ms step_avg:35.80ms
step:747/2110 train_time:26769ms step_avg:35.84ms
step:748/2110 train_time:26829ms step_avg:35.87ms
step:749/2110 train_time:26889ms step_avg:35.90ms
step:750/2110 train_time:26948ms step_avg:35.93ms
step:750/2110 val_loss:3.9057 train_time:27010ms step_avg:36.01ms
step:751/2110 train_time:27041ms step_avg:36.01ms
step:752/2110 train_time:27073ms step_avg:36.00ms
step:753/2110 train_time:27132ms step_avg:36.03ms
step:754/2110 train_time:27194ms step_avg:36.07ms
step:755/2110 train_time:27255ms step_avg:36.10ms
step:756/2110 train_time:27314ms step_avg:36.13ms
step:757/2110 train_time:27374ms step_avg:36.16ms
step:758/2110 train_time:27433ms step_avg:36.19ms
step:759/2110 train_time:27491ms step_avg:36.22ms
step:760/2110 train_time:27550ms step_avg:36.25ms
step:761/2110 train_time:27608ms step_avg:36.28ms
step:762/2110 train_time:27666ms step_avg:36.31ms
step:763/2110 train_time:27724ms step_avg:36.34ms
step:764/2110 train_time:27783ms step_avg:36.36ms
step:765/2110 train_time:27841ms step_avg:36.39ms
step:766/2110 train_time:27900ms step_avg:36.42ms
step:767/2110 train_time:27961ms step_avg:36.45ms
step:768/2110 train_time:28018ms step_avg:36.48ms
step:769/2110 train_time:28080ms step_avg:36.51ms
step:770/2110 train_time:28142ms step_avg:36.55ms
step:771/2110 train_time:28203ms step_avg:36.58ms
step:772/2110 train_time:28264ms step_avg:36.61ms
step:773/2110 train_time:28323ms step_avg:36.64ms
step:774/2110 train_time:28382ms step_avg:36.67ms
step:775/2110 train_time:28441ms step_avg:36.70ms
step:776/2110 train_time:28501ms step_avg:36.73ms
step:777/2110 train_time:28560ms step_avg:36.76ms
step:778/2110 train_time:28619ms step_avg:36.79ms
step:779/2110 train_time:28677ms step_avg:36.81ms
step:780/2110 train_time:28736ms step_avg:36.84ms
step:781/2110 train_time:28794ms step_avg:36.87ms
step:782/2110 train_time:28852ms step_avg:36.90ms
step:783/2110 train_time:28912ms step_avg:36.92ms
step:784/2110 train_time:28970ms step_avg:36.95ms
step:785/2110 train_time:29031ms step_avg:36.98ms
step:786/2110 train_time:29091ms step_avg:37.01ms
step:787/2110 train_time:29152ms step_avg:37.04ms
step:788/2110 train_time:29211ms step_avg:37.07ms
step:789/2110 train_time:29271ms step_avg:37.10ms
step:790/2110 train_time:29330ms step_avg:37.13ms
step:791/2110 train_time:29390ms step_avg:37.16ms
step:792/2110 train_time:29448ms step_avg:37.18ms
step:793/2110 train_time:29507ms step_avg:37.21ms
step:794/2110 train_time:29566ms step_avg:37.24ms
step:795/2110 train_time:29625ms step_avg:37.26ms
step:796/2110 train_time:29684ms step_avg:37.29ms
step:797/2110 train_time:29742ms step_avg:37.32ms
step:798/2110 train_time:29802ms step_avg:37.35ms
step:799/2110 train_time:29861ms step_avg:37.37ms
step:800/2110 train_time:29920ms step_avg:37.40ms
step:801/2110 train_time:29979ms step_avg:37.43ms
step:802/2110 train_time:30040ms step_avg:37.46ms
step:803/2110 train_time:30100ms step_avg:37.48ms
step:804/2110 train_time:30161ms step_avg:37.51ms
step:805/2110 train_time:30220ms step_avg:37.54ms
step:806/2110 train_time:30280ms step_avg:37.57ms
step:807/2110 train_time:30339ms step_avg:37.60ms
step:808/2110 train_time:30398ms step_avg:37.62ms
step:809/2110 train_time:30458ms step_avg:37.65ms
step:810/2110 train_time:30517ms step_avg:37.68ms
step:811/2110 train_time:30576ms step_avg:37.70ms
step:812/2110 train_time:30635ms step_avg:37.73ms
step:813/2110 train_time:30695ms step_avg:37.76ms
step:814/2110 train_time:30754ms step_avg:37.78ms
step:815/2110 train_time:30813ms step_avg:37.81ms
step:816/2110 train_time:30872ms step_avg:37.83ms
step:817/2110 train_time:30931ms step_avg:37.86ms
step:818/2110 train_time:30990ms step_avg:37.89ms
step:819/2110 train_time:31050ms step_avg:37.91ms
step:820/2110 train_time:31108ms step_avg:37.94ms
step:821/2110 train_time:31169ms step_avg:37.96ms
step:822/2110 train_time:31228ms step_avg:37.99ms
step:823/2110 train_time:31288ms step_avg:38.02ms
step:824/2110 train_time:31347ms step_avg:38.04ms
step:825/2110 train_time:31407ms step_avg:38.07ms
step:826/2110 train_time:31465ms step_avg:38.09ms
step:827/2110 train_time:31526ms step_avg:38.12ms
step:828/2110 train_time:31584ms step_avg:38.15ms
step:829/2110 train_time:31644ms step_avg:38.17ms
step:830/2110 train_time:31704ms step_avg:38.20ms
step:831/2110 train_time:31763ms step_avg:38.22ms
step:832/2110 train_time:31822ms step_avg:38.25ms
step:833/2110 train_time:31881ms step_avg:38.27ms
step:834/2110 train_time:31941ms step_avg:38.30ms
step:835/2110 train_time:32000ms step_avg:38.32ms
step:836/2110 train_time:32059ms step_avg:38.35ms
step:837/2110 train_time:32119ms step_avg:38.37ms
step:838/2110 train_time:32179ms step_avg:38.40ms
step:839/2110 train_time:32239ms step_avg:38.43ms
step:840/2110 train_time:32298ms step_avg:38.45ms
step:841/2110 train_time:32358ms step_avg:38.48ms
step:842/2110 train_time:32418ms step_avg:38.50ms
step:843/2110 train_time:32476ms step_avg:38.52ms
step:844/2110 train_time:32536ms step_avg:38.55ms
step:845/2110 train_time:32596ms step_avg:38.58ms
step:846/2110 train_time:32655ms step_avg:38.60ms
step:847/2110 train_time:32714ms step_avg:38.62ms
step:848/2110 train_time:32773ms step_avg:38.65ms
step:849/2110 train_time:32832ms step_avg:38.67ms
step:850/2110 train_time:32892ms step_avg:38.70ms
step:851/2110 train_time:32951ms step_avg:38.72ms
step:852/2110 train_time:33010ms step_avg:38.74ms
step:853/2110 train_time:33069ms step_avg:38.77ms
step:854/2110 train_time:33128ms step_avg:38.79ms
step:855/2110 train_time:33188ms step_avg:38.82ms
step:856/2110 train_time:33247ms step_avg:38.84ms
step:857/2110 train_time:33306ms step_avg:38.86ms
step:858/2110 train_time:33365ms step_avg:38.89ms
step:859/2110 train_time:33425ms step_avg:38.91ms
step:860/2110 train_time:33484ms step_avg:38.93ms
step:861/2110 train_time:33544ms step_avg:38.96ms
step:862/2110 train_time:33603ms step_avg:38.98ms
step:863/2110 train_time:33662ms step_avg:39.01ms
step:864/2110 train_time:33722ms step_avg:39.03ms
step:865/2110 train_time:33782ms step_avg:39.05ms
step:866/2110 train_time:33842ms step_avg:39.08ms
step:867/2110 train_time:33901ms step_avg:39.10ms
step:868/2110 train_time:33961ms step_avg:39.13ms
step:869/2110 train_time:34021ms step_avg:39.15ms
step:870/2110 train_time:34080ms step_avg:39.17ms
step:871/2110 train_time:34140ms step_avg:39.20ms
step:872/2110 train_time:34199ms step_avg:39.22ms
step:873/2110 train_time:34258ms step_avg:39.24ms
step:874/2110 train_time:34318ms step_avg:39.27ms
step:875/2110 train_time:34376ms step_avg:39.29ms
step:876/2110 train_time:34435ms step_avg:39.31ms
step:877/2110 train_time:34496ms step_avg:39.33ms
step:878/2110 train_time:34554ms step_avg:39.36ms
step:879/2110 train_time:34614ms step_avg:39.38ms
step:880/2110 train_time:34673ms step_avg:39.40ms
step:881/2110 train_time:34732ms step_avg:39.42ms
step:882/2110 train_time:34792ms step_avg:39.45ms
step:883/2110 train_time:34852ms step_avg:39.47ms
step:884/2110 train_time:34910ms step_avg:39.49ms
step:885/2110 train_time:34970ms step_avg:39.51ms
step:886/2110 train_time:35029ms step_avg:39.54ms
step:887/2110 train_time:35088ms step_avg:39.56ms
step:888/2110 train_time:35148ms step_avg:39.58ms
step:889/2110 train_time:35206ms step_avg:39.60ms
step:890/2110 train_time:35265ms step_avg:39.62ms
step:891/2110 train_time:35326ms step_avg:39.65ms
step:892/2110 train_time:35385ms step_avg:39.67ms
step:893/2110 train_time:35445ms step_avg:39.69ms
step:894/2110 train_time:35504ms step_avg:39.71ms
step:895/2110 train_time:35563ms step_avg:39.74ms
step:896/2110 train_time:35623ms step_avg:39.76ms
step:897/2110 train_time:35682ms step_avg:39.78ms
step:898/2110 train_time:35742ms step_avg:39.80ms
step:899/2110 train_time:35802ms step_avg:39.82ms
step:900/2110 train_time:35861ms step_avg:39.85ms
step:901/2110 train_time:35920ms step_avg:39.87ms
step:902/2110 train_time:35980ms step_avg:39.89ms
step:903/2110 train_time:36039ms step_avg:39.91ms
step:904/2110 train_time:36100ms step_avg:39.93ms
step:905/2110 train_time:36158ms step_avg:39.95ms
step:906/2110 train_time:36217ms step_avg:39.97ms
step:907/2110 train_time:36277ms step_avg:40.00ms
step:908/2110 train_time:36337ms step_avg:40.02ms
step:909/2110 train_time:36396ms step_avg:40.04ms
step:910/2110 train_time:36456ms step_avg:40.06ms
step:911/2110 train_time:36515ms step_avg:40.08ms
step:912/2110 train_time:36574ms step_avg:40.10ms
step:913/2110 train_time:36634ms step_avg:40.12ms
step:914/2110 train_time:36693ms step_avg:40.15ms
step:915/2110 train_time:36752ms step_avg:40.17ms
step:916/2110 train_time:36811ms step_avg:40.19ms
step:917/2110 train_time:36870ms step_avg:40.21ms
step:918/2110 train_time:36929ms step_avg:40.23ms
step:919/2110 train_time:36988ms step_avg:40.25ms
step:920/2110 train_time:37048ms step_avg:40.27ms
step:921/2110 train_time:37106ms step_avg:40.29ms
step:922/2110 train_time:37165ms step_avg:40.31ms
step:923/2110 train_time:37224ms step_avg:40.33ms
step:924/2110 train_time:37284ms step_avg:40.35ms
step:925/2110 train_time:37343ms step_avg:40.37ms
step:926/2110 train_time:37402ms step_avg:40.39ms
step:927/2110 train_time:37461ms step_avg:40.41ms
step:928/2110 train_time:37521ms step_avg:40.43ms
step:929/2110 train_time:37580ms step_avg:40.45ms
step:930/2110 train_time:37640ms step_avg:40.47ms
step:931/2110 train_time:37699ms step_avg:40.49ms
step:932/2110 train_time:37759ms step_avg:40.51ms
step:933/2110 train_time:37818ms step_avg:40.53ms
step:934/2110 train_time:37877ms step_avg:40.55ms
step:935/2110 train_time:37937ms step_avg:40.57ms
step:936/2110 train_time:37997ms step_avg:40.60ms
step:937/2110 train_time:38057ms step_avg:40.62ms
step:938/2110 train_time:38116ms step_avg:40.64ms
step:939/2110 train_time:38175ms step_avg:40.66ms
step:940/2110 train_time:38235ms step_avg:40.68ms
step:941/2110 train_time:38295ms step_avg:40.70ms
step:942/2110 train_time:38354ms step_avg:40.72ms
step:943/2110 train_time:38414ms step_avg:40.74ms
step:944/2110 train_time:38474ms step_avg:40.76ms
step:945/2110 train_time:38533ms step_avg:40.78ms
step:946/2110 train_time:38592ms step_avg:40.79ms
step:947/2110 train_time:38652ms step_avg:40.82ms
step:948/2110 train_time:38711ms step_avg:40.83ms
step:949/2110 train_time:38770ms step_avg:40.85ms
step:950/2110 train_time:38828ms step_avg:40.87ms
step:951/2110 train_time:38888ms step_avg:40.89ms
step:952/2110 train_time:38947ms step_avg:40.91ms
step:953/2110 train_time:39006ms step_avg:40.93ms
step:954/2110 train_time:39065ms step_avg:40.95ms
step:955/2110 train_time:39124ms step_avg:40.97ms
step:956/2110 train_time:39184ms step_avg:40.99ms
step:957/2110 train_time:39244ms step_avg:41.01ms
step:958/2110 train_time:39303ms step_avg:41.03ms
step:959/2110 train_time:39362ms step_avg:41.05ms
step:960/2110 train_time:39422ms step_avg:41.06ms
step:961/2110 train_time:39481ms step_avg:41.08ms
step:962/2110 train_time:39541ms step_avg:41.10ms
step:963/2110 train_time:39601ms step_avg:41.12ms
step:964/2110 train_time:39660ms step_avg:41.14ms
step:965/2110 train_time:39720ms step_avg:41.16ms
step:966/2110 train_time:39779ms step_avg:41.18ms
step:967/2110 train_time:39838ms step_avg:41.20ms
step:968/2110 train_time:39898ms step_avg:41.22ms
step:969/2110 train_time:39957ms step_avg:41.24ms
step:970/2110 train_time:40016ms step_avg:41.25ms
step:971/2110 train_time:40075ms step_avg:41.27ms
step:972/2110 train_time:40134ms step_avg:41.29ms
step:973/2110 train_time:40194ms step_avg:41.31ms
step:974/2110 train_time:40254ms step_avg:41.33ms
step:975/2110 train_time:40312ms step_avg:41.35ms
step:976/2110 train_time:40371ms step_avg:41.36ms
step:977/2110 train_time:40432ms step_avg:41.38ms
step:978/2110 train_time:40490ms step_avg:41.40ms
step:979/2110 train_time:40550ms step_avg:41.42ms
step:980/2110 train_time:40608ms step_avg:41.44ms
step:981/2110 train_time:40668ms step_avg:41.46ms
step:982/2110 train_time:40726ms step_avg:41.47ms
step:983/2110 train_time:40785ms step_avg:41.49ms
step:984/2110 train_time:40844ms step_avg:41.51ms
step:985/2110 train_time:40904ms step_avg:41.53ms
step:986/2110 train_time:40963ms step_avg:41.54ms
step:987/2110 train_time:41022ms step_avg:41.56ms
step:988/2110 train_time:41081ms step_avg:41.58ms
step:989/2110 train_time:41141ms step_avg:41.60ms
step:990/2110 train_time:41201ms step_avg:41.62ms
step:991/2110 train_time:41260ms step_avg:41.63ms
step:992/2110 train_time:41319ms step_avg:41.65ms
step:993/2110 train_time:41379ms step_avg:41.67ms
step:994/2110 train_time:41439ms step_avg:41.69ms
step:995/2110 train_time:41498ms step_avg:41.71ms
step:996/2110 train_time:41558ms step_avg:41.73ms
step:997/2110 train_time:41617ms step_avg:41.74ms
step:998/2110 train_time:41677ms step_avg:41.76ms
step:999/2110 train_time:41737ms step_avg:41.78ms
step:1000/2110 train_time:41796ms step_avg:41.80ms
step:1000/2110 val_loss:3.7601 train_time:41856ms step_avg:41.86ms
step:1001/2110 train_time:41889ms step_avg:41.85ms
step:1002/2110 train_time:41921ms step_avg:41.84ms
step:1003/2110 train_time:41981ms step_avg:41.86ms
step:1004/2110 train_time:42044ms step_avg:41.88ms
step:1005/2110 train_time:42104ms step_avg:41.89ms
step:1006/2110 train_time:42163ms step_avg:41.91ms
step:1007/2110 train_time:42222ms step_avg:41.93ms
step:1008/2110 train_time:42280ms step_avg:41.94ms
step:1009/2110 train_time:42339ms step_avg:41.96ms
step:1010/2110 train_time:42398ms step_avg:41.98ms
step:1011/2110 train_time:42456ms step_avg:41.99ms
step:1012/2110 train_time:42514ms step_avg:42.01ms
step:1013/2110 train_time:42572ms step_avg:42.03ms
step:1014/2110 train_time:42632ms step_avg:42.04ms
step:1015/2110 train_time:42690ms step_avg:42.06ms
step:1016/2110 train_time:42747ms step_avg:42.07ms
step:1017/2110 train_time:42806ms step_avg:42.09ms
step:1018/2110 train_time:42866ms step_avg:42.11ms
step:1019/2110 train_time:42928ms step_avg:42.13ms
step:1020/2110 train_time:42987ms step_avg:42.14ms
step:1021/2110 train_time:43050ms step_avg:42.16ms
step:1022/2110 train_time:43110ms step_avg:42.18ms
step:1023/2110 train_time:43170ms step_avg:42.20ms
step:1024/2110 train_time:43230ms step_avg:42.22ms
step:1025/2110 train_time:43289ms step_avg:42.23ms
step:1026/2110 train_time:43348ms step_avg:42.25ms
step:1027/2110 train_time:43407ms step_avg:42.27ms
step:1028/2110 train_time:43465ms step_avg:42.28ms
step:1029/2110 train_time:43525ms step_avg:42.30ms
step:1030/2110 train_time:43582ms step_avg:42.31ms
step:1031/2110 train_time:43642ms step_avg:42.33ms
step:1032/2110 train_time:43700ms step_avg:42.34ms
step:1033/2110 train_time:43760ms step_avg:42.36ms
step:1034/2110 train_time:43818ms step_avg:42.38ms
step:1035/2110 train_time:43878ms step_avg:42.39ms
step:1036/2110 train_time:43937ms step_avg:42.41ms
step:1037/2110 train_time:43998ms step_avg:42.43ms
step:1038/2110 train_time:44057ms step_avg:42.44ms
step:1039/2110 train_time:44119ms step_avg:42.46ms
step:1040/2110 train_time:44178ms step_avg:42.48ms
step:1041/2110 train_time:44237ms step_avg:42.49ms
step:1042/2110 train_time:44296ms step_avg:42.51ms
step:1043/2110 train_time:44355ms step_avg:42.53ms
step:1044/2110 train_time:44414ms step_avg:42.54ms
step:1045/2110 train_time:44473ms step_avg:42.56ms
step:1046/2110 train_time:44532ms step_avg:42.57ms
step:1047/2110 train_time:44591ms step_avg:42.59ms
step:1048/2110 train_time:44650ms step_avg:42.60ms
step:1049/2110 train_time:44709ms step_avg:42.62ms
step:1050/2110 train_time:44768ms step_avg:42.64ms
step:1051/2110 train_time:44828ms step_avg:42.65ms
step:1052/2110 train_time:44887ms step_avg:42.67ms
step:1053/2110 train_time:44947ms step_avg:42.68ms
step:1054/2110 train_time:45006ms step_avg:42.70ms
step:1055/2110 train_time:45066ms step_avg:42.72ms
step:1056/2110 train_time:45125ms step_avg:42.73ms
step:1057/2110 train_time:45185ms step_avg:42.75ms
step:1058/2110 train_time:45244ms step_avg:42.76ms
step:1059/2110 train_time:45304ms step_avg:42.78ms
step:1060/2110 train_time:45363ms step_avg:42.79ms
step:1061/2110 train_time:45422ms step_avg:42.81ms
step:1062/2110 train_time:45480ms step_avg:42.82ms
step:1063/2110 train_time:45540ms step_avg:42.84ms
step:1064/2110 train_time:45598ms step_avg:42.86ms
step:1065/2110 train_time:45658ms step_avg:42.87ms
step:1066/2110 train_time:45716ms step_avg:42.89ms
step:1067/2110 train_time:45777ms step_avg:42.90ms
step:1068/2110 train_time:45835ms step_avg:42.92ms
step:1069/2110 train_time:45895ms step_avg:42.93ms
step:1070/2110 train_time:45954ms step_avg:42.95ms
step:1071/2110 train_time:46015ms step_avg:42.96ms
step:1072/2110 train_time:46075ms step_avg:42.98ms
step:1073/2110 train_time:46134ms step_avg:43.00ms
step:1074/2110 train_time:46194ms step_avg:43.01ms
step:1075/2110 train_time:46254ms step_avg:43.03ms
step:1076/2110 train_time:46314ms step_avg:43.04ms
step:1077/2110 train_time:46374ms step_avg:43.06ms
step:1078/2110 train_time:46433ms step_avg:43.07ms
step:1079/2110 train_time:46492ms step_avg:43.09ms
step:1080/2110 train_time:46551ms step_avg:43.10ms
step:1081/2110 train_time:46609ms step_avg:43.12ms
step:1082/2110 train_time:46668ms step_avg:43.13ms
step:1083/2110 train_time:46727ms step_avg:43.15ms
step:1084/2110 train_time:46786ms step_avg:43.16ms
step:1085/2110 train_time:46847ms step_avg:43.18ms
step:1086/2110 train_time:46906ms step_avg:43.19ms
step:1087/2110 train_time:46966ms step_avg:43.21ms
step:1088/2110 train_time:47025ms step_avg:43.22ms
step:1089/2110 train_time:47086ms step_avg:43.24ms
step:1090/2110 train_time:47144ms step_avg:43.25ms
step:1091/2110 train_time:47205ms step_avg:43.27ms
step:1092/2110 train_time:47263ms step_avg:43.28ms
step:1093/2110 train_time:47323ms step_avg:43.30ms
step:1094/2110 train_time:47382ms step_avg:43.31ms
step:1095/2110 train_time:47442ms step_avg:43.33ms
step:1096/2110 train_time:47500ms step_avg:43.34ms
step:1097/2110 train_time:47559ms step_avg:43.35ms
step:1098/2110 train_time:47618ms step_avg:43.37ms
step:1099/2110 train_time:47677ms step_avg:43.38ms
step:1100/2110 train_time:47736ms step_avg:43.40ms
step:1101/2110 train_time:47795ms step_avg:43.41ms
step:1102/2110 train_time:47854ms step_avg:43.43ms
step:1103/2110 train_time:47914ms step_avg:43.44ms
step:1104/2110 train_time:47973ms step_avg:43.45ms
step:1105/2110 train_time:48033ms step_avg:43.47ms
step:1106/2110 train_time:48093ms step_avg:43.48ms
step:1107/2110 train_time:48152ms step_avg:43.50ms
step:1108/2110 train_time:48212ms step_avg:43.51ms
step:1109/2110 train_time:48272ms step_avg:43.53ms
step:1110/2110 train_time:48332ms step_avg:43.54ms
step:1111/2110 train_time:48392ms step_avg:43.56ms
step:1112/2110 train_time:48452ms step_avg:43.57ms
step:1113/2110 train_time:48511ms step_avg:43.59ms
step:1114/2110 train_time:48570ms step_avg:43.60ms
step:1115/2110 train_time:48629ms step_avg:43.61ms
step:1116/2110 train_time:48688ms step_avg:43.63ms
step:1117/2110 train_time:48747ms step_avg:43.64ms
step:1118/2110 train_time:48806ms step_avg:43.65ms
step:1119/2110 train_time:48866ms step_avg:43.67ms
step:1120/2110 train_time:48925ms step_avg:43.68ms
step:1121/2110 train_time:48986ms step_avg:43.70ms
step:1122/2110 train_time:49045ms step_avg:43.71ms
step:1123/2110 train_time:49104ms step_avg:43.73ms
step:1124/2110 train_time:49162ms step_avg:43.74ms
step:1125/2110 train_time:49223ms step_avg:43.75ms
step:1126/2110 train_time:49282ms step_avg:43.77ms
step:1127/2110 train_time:49342ms step_avg:43.78ms
step:1128/2110 train_time:49400ms step_avg:43.79ms
step:1129/2110 train_time:49460ms step_avg:43.81ms
step:1130/2110 train_time:49519ms step_avg:43.82ms
step:1131/2110 train_time:49578ms step_avg:43.84ms
step:1132/2110 train_time:49636ms step_avg:43.85ms
step:1133/2110 train_time:49696ms step_avg:43.86ms
step:1134/2110 train_time:49755ms step_avg:43.88ms
step:1135/2110 train_time:49815ms step_avg:43.89ms
step:1136/2110 train_time:49873ms step_avg:43.90ms
step:1137/2110 train_time:49933ms step_avg:43.92ms
step:1138/2110 train_time:49992ms step_avg:43.93ms
step:1139/2110 train_time:50051ms step_avg:43.94ms
step:1140/2110 train_time:50111ms step_avg:43.96ms
step:1141/2110 train_time:50170ms step_avg:43.97ms
step:1142/2110 train_time:50231ms step_avg:43.98ms
step:1143/2110 train_time:50291ms step_avg:44.00ms
step:1144/2110 train_time:50352ms step_avg:44.01ms
step:1145/2110 train_time:50412ms step_avg:44.03ms
step:1146/2110 train_time:50471ms step_avg:44.04ms
step:1147/2110 train_time:50530ms step_avg:44.05ms
step:1148/2110 train_time:50589ms step_avg:44.07ms
step:1149/2110 train_time:50650ms step_avg:44.08ms
step:1150/2110 train_time:50709ms step_avg:44.09ms
step:1151/2110 train_time:50769ms step_avg:44.11ms
step:1152/2110 train_time:50828ms step_avg:44.12ms
step:1153/2110 train_time:50889ms step_avg:44.14ms
step:1154/2110 train_time:50949ms step_avg:44.15ms
step:1155/2110 train_time:51008ms step_avg:44.16ms
step:1156/2110 train_time:51068ms step_avg:44.18ms
step:1157/2110 train_time:51128ms step_avg:44.19ms
step:1158/2110 train_time:51188ms step_avg:44.20ms
step:1159/2110 train_time:51248ms step_avg:44.22ms
step:1160/2110 train_time:51309ms step_avg:44.23ms
step:1161/2110 train_time:51369ms step_avg:44.25ms
step:1162/2110 train_time:51428ms step_avg:44.26ms
step:1163/2110 train_time:51489ms step_avg:44.27ms
step:1164/2110 train_time:51548ms step_avg:44.29ms
step:1165/2110 train_time:51609ms step_avg:44.30ms
step:1166/2110 train_time:51668ms step_avg:44.31ms
step:1167/2110 train_time:51729ms step_avg:44.33ms
step:1168/2110 train_time:51789ms step_avg:44.34ms
step:1169/2110 train_time:51848ms step_avg:44.35ms
step:1170/2110 train_time:51908ms step_avg:44.37ms
step:1171/2110 train_time:51968ms step_avg:44.38ms
step:1172/2110 train_time:52027ms step_avg:44.39ms
step:1173/2110 train_time:52087ms step_avg:44.41ms
step:1174/2110 train_time:52147ms step_avg:44.42ms
step:1175/2110 train_time:52208ms step_avg:44.43ms
step:1176/2110 train_time:52268ms step_avg:44.45ms
step:1177/2110 train_time:52328ms step_avg:44.46ms
step:1178/2110 train_time:52389ms step_avg:44.47ms
step:1179/2110 train_time:52449ms step_avg:44.49ms
step:1180/2110 train_time:52508ms step_avg:44.50ms
step:1181/2110 train_time:52568ms step_avg:44.51ms
step:1182/2110 train_time:52628ms step_avg:44.52ms
step:1183/2110 train_time:52687ms step_avg:44.54ms
step:1184/2110 train_time:52747ms step_avg:44.55ms
step:1185/2110 train_time:52807ms step_avg:44.56ms
step:1186/2110 train_time:52867ms step_avg:44.58ms
step:1187/2110 train_time:52926ms step_avg:44.59ms
step:1188/2110 train_time:52985ms step_avg:44.60ms
step:1189/2110 train_time:53046ms step_avg:44.61ms
step:1190/2110 train_time:53106ms step_avg:44.63ms
step:1191/2110 train_time:53165ms step_avg:44.64ms
step:1192/2110 train_time:53224ms step_avg:44.65ms
step:1193/2110 train_time:53284ms step_avg:44.66ms
step:1194/2110 train_time:53344ms step_avg:44.68ms
step:1195/2110 train_time:53404ms step_avg:44.69ms
step:1196/2110 train_time:53464ms step_avg:44.70ms
step:1197/2110 train_time:53524ms step_avg:44.72ms
step:1198/2110 train_time:53583ms step_avg:44.73ms
step:1199/2110 train_time:53644ms step_avg:44.74ms
step:1200/2110 train_time:53703ms step_avg:44.75ms
step:1201/2110 train_time:53763ms step_avg:44.77ms
step:1202/2110 train_time:53822ms step_avg:44.78ms
step:1203/2110 train_time:53883ms step_avg:44.79ms
step:1204/2110 train_time:53942ms step_avg:44.80ms
step:1205/2110 train_time:54002ms step_avg:44.81ms
step:1206/2110 train_time:54060ms step_avg:44.83ms
step:1207/2110 train_time:54121ms step_avg:44.84ms
step:1208/2110 train_time:54181ms step_avg:44.85ms
step:1209/2110 train_time:54241ms step_avg:44.86ms
step:1210/2110 train_time:54301ms step_avg:44.88ms
step:1211/2110 train_time:54361ms step_avg:44.89ms
step:1212/2110 train_time:54420ms step_avg:44.90ms
step:1213/2110 train_time:54480ms step_avg:44.91ms
step:1214/2110 train_time:54539ms step_avg:44.93ms
step:1215/2110 train_time:54600ms step_avg:44.94ms
step:1216/2110 train_time:54658ms step_avg:44.95ms
step:1217/2110 train_time:54720ms step_avg:44.96ms
step:1218/2110 train_time:54778ms step_avg:44.97ms
step:1219/2110 train_time:54839ms step_avg:44.99ms
step:1220/2110 train_time:54898ms step_avg:45.00ms
step:1221/2110 train_time:54958ms step_avg:45.01ms
step:1222/2110 train_time:55016ms step_avg:45.02ms
step:1223/2110 train_time:55077ms step_avg:45.03ms
step:1224/2110 train_time:55136ms step_avg:45.05ms
step:1225/2110 train_time:55195ms step_avg:45.06ms
step:1226/2110 train_time:55256ms step_avg:45.07ms
step:1227/2110 train_time:55316ms step_avg:45.08ms
step:1228/2110 train_time:55376ms step_avg:45.09ms
step:1229/2110 train_time:55436ms step_avg:45.11ms
step:1230/2110 train_time:55496ms step_avg:45.12ms
step:1231/2110 train_time:55555ms step_avg:45.13ms
step:1232/2110 train_time:55615ms step_avg:45.14ms
step:1233/2110 train_time:55675ms step_avg:45.15ms
step:1234/2110 train_time:55735ms step_avg:45.17ms
step:1235/2110 train_time:55796ms step_avg:45.18ms
step:1236/2110 train_time:55855ms step_avg:45.19ms
step:1237/2110 train_time:55915ms step_avg:45.20ms
step:1238/2110 train_time:55974ms step_avg:45.21ms
step:1239/2110 train_time:56034ms step_avg:45.23ms
step:1240/2110 train_time:56095ms step_avg:45.24ms
step:1241/2110 train_time:56154ms step_avg:45.25ms
step:1242/2110 train_time:56214ms step_avg:45.26ms
step:1243/2110 train_time:56274ms step_avg:45.27ms
step:1244/2110 train_time:56334ms step_avg:45.28ms
step:1245/2110 train_time:56395ms step_avg:45.30ms
step:1246/2110 train_time:56455ms step_avg:45.31ms
step:1247/2110 train_time:56514ms step_avg:45.32ms
step:1248/2110 train_time:56573ms step_avg:45.33ms
step:1249/2110 train_time:56633ms step_avg:45.34ms
step:1250/2110 train_time:56693ms step_avg:45.35ms
step:1250/2110 val_loss:3.5939 train_time:56754ms step_avg:45.40ms
step:1251/2110 train_time:56786ms step_avg:45.39ms
step:1252/2110 train_time:56817ms step_avg:45.38ms
step:1253/2110 train_time:56880ms step_avg:45.40ms
step:1254/2110 train_time:56944ms step_avg:45.41ms
step:1255/2110 train_time:57004ms step_avg:45.42ms
step:1256/2110 train_time:57063ms step_avg:45.43ms
step:1257/2110 train_time:57124ms step_avg:45.44ms
step:1258/2110 train_time:57182ms step_avg:45.45ms
step:1259/2110 train_time:57241ms step_avg:45.47ms
step:1260/2110 train_time:57299ms step_avg:45.48ms
step:1261/2110 train_time:57359ms step_avg:45.49ms
step:1262/2110 train_time:57417ms step_avg:45.50ms
step:1263/2110 train_time:57477ms step_avg:45.51ms
step:1264/2110 train_time:57536ms step_avg:45.52ms
step:1265/2110 train_time:57595ms step_avg:45.53ms
step:1266/2110 train_time:57654ms step_avg:45.54ms
step:1267/2110 train_time:57716ms step_avg:45.55ms
step:1268/2110 train_time:57777ms step_avg:45.57ms
step:1269/2110 train_time:57839ms step_avg:45.58ms
step:1270/2110 train_time:57900ms step_avg:45.59ms
step:1271/2110 train_time:57962ms step_avg:45.60ms
step:1272/2110 train_time:58022ms step_avg:45.61ms
step:1273/2110 train_time:58082ms step_avg:45.63ms
step:1274/2110 train_time:58141ms step_avg:45.64ms
step:1275/2110 train_time:58200ms step_avg:45.65ms
step:1276/2110 train_time:58259ms step_avg:45.66ms
step:1277/2110 train_time:58319ms step_avg:45.67ms
step:1278/2110 train_time:58378ms step_avg:45.68ms
step:1279/2110 train_time:58437ms step_avg:45.69ms
step:1280/2110 train_time:58496ms step_avg:45.70ms
step:1281/2110 train_time:58554ms step_avg:45.71ms
step:1282/2110 train_time:58613ms step_avg:45.72ms
step:1283/2110 train_time:58673ms step_avg:45.73ms
step:1284/2110 train_time:58734ms step_avg:45.74ms
step:1285/2110 train_time:58795ms step_avg:45.75ms
step:1286/2110 train_time:58856ms step_avg:45.77ms
step:1287/2110 train_time:58918ms step_avg:45.78ms
step:1288/2110 train_time:58978ms step_avg:45.79ms
step:1289/2110 train_time:59040ms step_avg:45.80ms
step:1290/2110 train_time:59099ms step_avg:45.81ms
step:1291/2110 train_time:59159ms step_avg:45.82ms
step:1292/2110 train_time:59219ms step_avg:45.83ms
step:1293/2110 train_time:59278ms step_avg:45.85ms
step:1294/2110 train_time:59337ms step_avg:45.86ms
step:1295/2110 train_time:59396ms step_avg:45.87ms
step:1296/2110 train_time:59456ms step_avg:45.88ms
step:1297/2110 train_time:59514ms step_avg:45.89ms
step:1298/2110 train_time:59574ms step_avg:45.90ms
step:1299/2110 train_time:59634ms step_avg:45.91ms
step:1300/2110 train_time:59694ms step_avg:45.92ms
step:1301/2110 train_time:59753ms step_avg:45.93ms
step:1302/2110 train_time:59813ms step_avg:45.94ms
step:1303/2110 train_time:59874ms step_avg:45.95ms
step:1304/2110 train_time:59936ms step_avg:45.96ms
step:1305/2110 train_time:59997ms step_avg:45.97ms
step:1306/2110 train_time:60058ms step_avg:45.99ms
step:1307/2110 train_time:60119ms step_avg:46.00ms
step:1308/2110 train_time:60178ms step_avg:46.01ms
step:1309/2110 train_time:60238ms step_avg:46.02ms
step:1310/2110 train_time:60297ms step_avg:46.03ms
step:1311/2110 train_time:60356ms step_avg:46.04ms
step:1312/2110 train_time:60415ms step_avg:46.05ms
step:1313/2110 train_time:60474ms step_avg:46.06ms
step:1314/2110 train_time:60535ms step_avg:46.07ms
step:1315/2110 train_time:60594ms step_avg:46.08ms
step:1316/2110 train_time:60653ms step_avg:46.09ms
step:1317/2110 train_time:60713ms step_avg:46.10ms
step:1318/2110 train_time:60773ms step_avg:46.11ms
step:1319/2110 train_time:60833ms step_avg:46.12ms
step:1320/2110 train_time:60893ms step_avg:46.13ms
step:1321/2110 train_time:60954ms step_avg:46.14ms
step:1322/2110 train_time:61015ms step_avg:46.15ms
step:1323/2110 train_time:61076ms step_avg:46.16ms
step:1324/2110 train_time:61137ms step_avg:46.18ms
step:1325/2110 train_time:61197ms step_avg:46.19ms
step:1326/2110 train_time:61256ms step_avg:46.20ms
step:1327/2110 train_time:61315ms step_avg:46.21ms
step:1328/2110 train_time:61375ms step_avg:46.22ms
step:1329/2110 train_time:61434ms step_avg:46.23ms
step:1330/2110 train_time:61494ms step_avg:46.24ms
step:1331/2110 train_time:61553ms step_avg:46.25ms
step:1332/2110 train_time:61612ms step_avg:46.26ms
step:1333/2110 train_time:61672ms step_avg:46.27ms
step:1334/2110 train_time:61733ms step_avg:46.28ms
step:1335/2110 train_time:61792ms step_avg:46.29ms
step:1336/2110 train_time:61852ms step_avg:46.30ms
step:1337/2110 train_time:61912ms step_avg:46.31ms
step:1338/2110 train_time:61972ms step_avg:46.32ms
step:1339/2110 train_time:62034ms step_avg:46.33ms
step:1340/2110 train_time:62094ms step_avg:46.34ms
step:1341/2110 train_time:62153ms step_avg:46.35ms
step:1342/2110 train_time:62214ms step_avg:46.36ms
step:1343/2110 train_time:62274ms step_avg:46.37ms
step:1344/2110 train_time:62334ms step_avg:46.38ms
step:1345/2110 train_time:62394ms step_avg:46.39ms
step:1346/2110 train_time:62454ms step_avg:46.40ms
step:1347/2110 train_time:62514ms step_avg:46.41ms
step:1348/2110 train_time:62573ms step_avg:46.42ms
step:1349/2110 train_time:62633ms step_avg:46.43ms
step:1350/2110 train_time:62693ms step_avg:46.44ms
step:1351/2110 train_time:62753ms step_avg:46.45ms
step:1352/2110 train_time:62813ms step_avg:46.46ms
step:1353/2110 train_time:62873ms step_avg:46.47ms
step:1354/2110 train_time:62933ms step_avg:46.48ms
step:1355/2110 train_time:62994ms step_avg:46.49ms
step:1356/2110 train_time:63053ms step_avg:46.50ms
step:1357/2110 train_time:63113ms step_avg:46.51ms
step:1358/2110 train_time:63174ms step_avg:46.52ms
step:1359/2110 train_time:63234ms step_avg:46.53ms
step:1360/2110 train_time:63297ms step_avg:46.54ms
step:1361/2110 train_time:63354ms step_avg:46.55ms
step:1362/2110 train_time:63414ms step_avg:46.56ms
step:1363/2110 train_time:63474ms step_avg:46.57ms
step:1364/2110 train_time:63535ms step_avg:46.58ms
step:1365/2110 train_time:63595ms step_avg:46.59ms
step:1366/2110 train_time:63655ms step_avg:46.60ms
step:1367/2110 train_time:63714ms step_avg:46.61ms
step:1368/2110 train_time:63774ms step_avg:46.62ms
step:1369/2110 train_time:63834ms step_avg:46.63ms
step:1370/2110 train_time:63896ms step_avg:46.64ms
step:1371/2110 train_time:63956ms step_avg:46.65ms
step:1372/2110 train_time:64016ms step_avg:46.66ms
step:1373/2110 train_time:64075ms step_avg:46.67ms
step:1374/2110 train_time:64136ms step_avg:46.68ms
step:1375/2110 train_time:64196ms step_avg:46.69ms
step:1376/2110 train_time:64256ms step_avg:46.70ms
step:1377/2110 train_time:64316ms step_avg:46.71ms
step:1378/2110 train_time:64376ms step_avg:46.72ms
step:1379/2110 train_time:64437ms step_avg:46.73ms
step:1380/2110 train_time:64497ms step_avg:46.74ms
step:1381/2110 train_time:64557ms step_avg:46.75ms
step:1382/2110 train_time:64645ms step_avg:46.78ms
step:1383/2110 train_time:64731ms step_avg:46.80ms
step:1384/2110 train_time:64818ms step_avg:46.83ms
step:1385/2110 train_time:64905ms step_avg:46.86ms
step:1386/2110 train_time:64992ms step_avg:46.89ms
step:1387/2110 train_time:65078ms step_avg:46.92ms
step:1388/2110 train_time:65165ms step_avg:46.95ms
step:1389/2110 train_time:65251ms step_avg:46.98ms
step:1390/2110 train_time:65338ms step_avg:47.01ms
step:1391/2110 train_time:65426ms step_avg:47.04ms
step:1392/2110 train_time:65512ms step_avg:47.06ms
step:1393/2110 train_time:65599ms step_avg:47.09ms
step:1394/2110 train_time:65685ms step_avg:47.12ms
step:1395/2110 train_time:65772ms step_avg:47.15ms
step:1396/2110 train_time:65859ms step_avg:47.18ms
step:1397/2110 train_time:65945ms step_avg:47.21ms
step:1398/2110 train_time:66032ms step_avg:47.23ms
step:1399/2110 train_time:66120ms step_avg:47.26ms
step:1400/2110 train_time:66208ms step_avg:47.29ms
step:1401/2110 train_time:66294ms step_avg:47.32ms
step:1402/2110 train_time:66382ms step_avg:47.35ms
step:1403/2110 train_time:66468ms step_avg:47.38ms
step:1404/2110 train_time:66555ms step_avg:47.40ms
step:1405/2110 train_time:66641ms step_avg:47.43ms
step:1406/2110 train_time:66728ms step_avg:47.46ms
step:1407/2110 train_time:66814ms step_avg:47.49ms
step:1408/2110 train_time:66902ms step_avg:47.52ms
step:1409/2110 train_time:66989ms step_avg:47.54ms
step:1410/2110 train_time:67076ms step_avg:47.57ms
step:1411/2110 train_time:67162ms step_avg:47.60ms
step:1412/2110 train_time:67250ms step_avg:47.63ms
step:1413/2110 train_time:67336ms step_avg:47.65ms
step:1414/2110 train_time:67423ms step_avg:47.68ms
step:1415/2110 train_time:67510ms step_avg:47.71ms
step:1416/2110 train_time:67597ms step_avg:47.74ms
step:1417/2110 train_time:67683ms step_avg:47.77ms
step:1418/2110 train_time:67770ms step_avg:47.79ms
step:1419/2110 train_time:67857ms step_avg:47.82ms
step:1420/2110 train_time:67944ms step_avg:47.85ms
step:1421/2110 train_time:68030ms step_avg:47.87ms
step:1422/2110 train_time:68117ms step_avg:47.90ms
step:1423/2110 train_time:68204ms step_avg:47.93ms
step:1424/2110 train_time:68291ms step_avg:47.96ms
step:1425/2110 train_time:68377ms step_avg:47.98ms
step:1426/2110 train_time:68466ms step_avg:48.01ms
step:1427/2110 train_time:68551ms step_avg:48.04ms
step:1428/2110 train_time:68640ms step_avg:48.07ms
step:1429/2110 train_time:68727ms step_avg:48.09ms
step:1430/2110 train_time:68813ms step_avg:48.12ms
step:1431/2110 train_time:68901ms step_avg:48.15ms
step:1432/2110 train_time:68989ms step_avg:48.18ms
step:1433/2110 train_time:69075ms step_avg:48.20ms
step:1434/2110 train_time:69162ms step_avg:48.23ms
step:1435/2110 train_time:69249ms step_avg:48.26ms
step:1436/2110 train_time:69335ms step_avg:48.28ms
step:1437/2110 train_time:69422ms step_avg:48.31ms
step:1438/2110 train_time:69509ms step_avg:48.34ms
step:1439/2110 train_time:69596ms step_avg:48.36ms
step:1440/2110 train_time:69682ms step_avg:48.39ms
step:1441/2110 train_time:69769ms step_avg:48.42ms
step:1442/2110 train_time:69855ms step_avg:48.44ms
step:1443/2110 train_time:69942ms step_avg:48.47ms
step:1444/2110 train_time:70029ms step_avg:48.50ms
step:1445/2110 train_time:70115ms step_avg:48.52ms
step:1446/2110 train_time:70202ms step_avg:48.55ms
step:1447/2110 train_time:70289ms step_avg:48.58ms
step:1448/2110 train_time:70377ms step_avg:48.60ms
step:1449/2110 train_time:70463ms step_avg:48.63ms
step:1450/2110 train_time:70549ms step_avg:48.65ms
step:1451/2110 train_time:70636ms step_avg:48.68ms
step:1452/2110 train_time:70723ms step_avg:48.71ms
step:1453/2110 train_time:70810ms step_avg:48.73ms
step:1454/2110 train_time:70898ms step_avg:48.76ms
step:1455/2110 train_time:70985ms step_avg:48.79ms
step:1456/2110 train_time:71072ms step_avg:48.81ms
step:1457/2110 train_time:71158ms step_avg:48.84ms
step:1458/2110 train_time:71246ms step_avg:48.87ms
step:1459/2110 train_time:71332ms step_avg:48.89ms
step:1460/2110 train_time:71419ms step_avg:48.92ms
step:1461/2110 train_time:71506ms step_avg:48.94ms
step:1462/2110 train_time:71592ms step_avg:48.97ms
step:1463/2110 train_time:71680ms step_avg:48.99ms
step:1464/2110 train_time:71767ms step_avg:49.02ms
step:1465/2110 train_time:71854ms step_avg:49.05ms
step:1466/2110 train_time:71941ms step_avg:49.07ms
step:1467/2110 train_time:72027ms step_avg:49.10ms
step:1468/2110 train_time:72113ms step_avg:49.12ms
step:1469/2110 train_time:72201ms step_avg:49.15ms
step:1470/2110 train_time:72289ms step_avg:49.18ms
step:1471/2110 train_time:72375ms step_avg:49.20ms
step:1472/2110 train_time:72463ms step_avg:49.23ms
step:1473/2110 train_time:72549ms step_avg:49.25ms
step:1474/2110 train_time:72637ms step_avg:49.28ms
step:1475/2110 train_time:72723ms step_avg:49.30ms
step:1476/2110 train_time:72810ms step_avg:49.33ms
step:1477/2110 train_time:72896ms step_avg:49.35ms
step:1478/2110 train_time:72984ms step_avg:49.38ms
step:1479/2110 train_time:73071ms step_avg:49.41ms
step:1480/2110 train_time:73158ms step_avg:49.43ms
step:1481/2110 train_time:73244ms step_avg:49.46ms
step:1482/2110 train_time:73331ms step_avg:49.48ms
step:1483/2110 train_time:73417ms step_avg:49.51ms
step:1484/2110 train_time:73505ms step_avg:49.53ms
step:1485/2110 train_time:73591ms step_avg:49.56ms
step:1486/2110 train_time:73678ms step_avg:49.58ms
step:1487/2110 train_time:73765ms step_avg:49.61ms
step:1488/2110 train_time:73852ms step_avg:49.63ms
step:1489/2110 train_time:73938ms step_avg:49.66ms
step:1490/2110 train_time:74026ms step_avg:49.68ms
step:1491/2110 train_time:74112ms step_avg:49.71ms
step:1492/2110 train_time:74199ms step_avg:49.73ms
step:1493/2110 train_time:74285ms step_avg:49.76ms
step:1494/2110 train_time:74372ms step_avg:49.78ms
step:1495/2110 train_time:74458ms step_avg:49.80ms
step:1496/2110 train_time:74545ms step_avg:49.83ms
step:1497/2110 train_time:74632ms step_avg:49.85ms
step:1498/2110 train_time:74719ms step_avg:49.88ms
step:1499/2110 train_time:74805ms step_avg:49.90ms
step:1500/2110 train_time:74893ms step_avg:49.93ms
step:1500/2110 val_loss:3.4930 train_time:74980ms step_avg:49.99ms
step:1501/2110 train_time:75013ms step_avg:49.98ms
step:1502/2110 train_time:75070ms step_avg:49.98ms
step:1503/2110 train_time:75166ms step_avg:50.01ms
step:1504/2110 train_time:75253ms step_avg:50.04ms
step:1505/2110 train_time:75339ms step_avg:50.06ms
step:1506/2110 train_time:75426ms step_avg:50.08ms
step:1507/2110 train_time:75511ms step_avg:50.11ms
step:1508/2110 train_time:75596ms step_avg:50.13ms
step:1509/2110 train_time:75683ms step_avg:50.15ms
step:1510/2110 train_time:75767ms step_avg:50.18ms
step:1511/2110 train_time:75852ms step_avg:50.20ms
step:1512/2110 train_time:75939ms step_avg:50.22ms
step:1513/2110 train_time:76029ms step_avg:50.25ms
step:1514/2110 train_time:76117ms step_avg:50.28ms
step:1515/2110 train_time:76206ms step_avg:50.30ms
step:1516/2110 train_time:76293ms step_avg:50.33ms
step:1517/2110 train_time:76380ms step_avg:50.35ms
step:1518/2110 train_time:76467ms step_avg:50.37ms
step:1519/2110 train_time:76552ms step_avg:50.40ms
step:1520/2110 train_time:76638ms step_avg:50.42ms
step:1521/2110 train_time:76724ms step_avg:50.44ms
step:1522/2110 train_time:76810ms step_avg:50.47ms
step:1523/2110 train_time:76896ms step_avg:50.49ms
step:1524/2110 train_time:76983ms step_avg:50.51ms
step:1525/2110 train_time:77072ms step_avg:50.54ms
step:1526/2110 train_time:77160ms step_avg:50.56ms
step:1527/2110 train_time:77247ms step_avg:50.59ms
step:1528/2110 train_time:77333ms step_avg:50.61ms
step:1529/2110 train_time:77420ms step_avg:50.63ms
step:1530/2110 train_time:77507ms step_avg:50.66ms
step:1531/2110 train_time:77593ms step_avg:50.68ms
step:1532/2110 train_time:77678ms step_avg:50.70ms
step:1533/2110 train_time:77764ms step_avg:50.73ms
step:1534/2110 train_time:77850ms step_avg:50.75ms
step:1535/2110 train_time:77937ms step_avg:50.77ms
step:1536/2110 train_time:78023ms step_avg:50.80ms
step:1537/2110 train_time:78111ms step_avg:50.82ms
step:1538/2110 train_time:78198ms step_avg:50.84ms
step:1539/2110 train_time:78287ms step_avg:50.87ms
step:1540/2110 train_time:78373ms step_avg:50.89ms
step:1541/2110 train_time:78460ms step_avg:50.91ms
step:1542/2110 train_time:78547ms step_avg:50.94ms
step:1543/2110 train_time:78633ms step_avg:50.96ms
step:1544/2110 train_time:78721ms step_avg:50.98ms
step:1545/2110 train_time:78807ms step_avg:51.01ms
step:1546/2110 train_time:78893ms step_avg:51.03ms
step:1547/2110 train_time:78981ms step_avg:51.05ms
step:1548/2110 train_time:79068ms step_avg:51.08ms
step:1549/2110 train_time:79154ms step_avg:51.10ms
step:1550/2110 train_time:79243ms step_avg:51.12ms
step:1551/2110 train_time:79330ms step_avg:51.15ms
step:1552/2110 train_time:79418ms step_avg:51.17ms
step:1553/2110 train_time:79505ms step_avg:51.19ms
step:1554/2110 train_time:79591ms step_avg:51.22ms
step:1555/2110 train_time:79677ms step_avg:51.24ms
step:1556/2110 train_time:79764ms step_avg:51.26ms
step:1557/2110 train_time:79850ms step_avg:51.28ms
step:1558/2110 train_time:79936ms step_avg:51.31ms
step:1559/2110 train_time:80023ms step_avg:51.33ms
step:1560/2110 train_time:80111ms step_avg:51.35ms
step:1561/2110 train_time:80198ms step_avg:51.38ms
step:1562/2110 train_time:80285ms step_avg:51.40ms
step:1563/2110 train_time:80371ms step_avg:51.42ms
step:1564/2110 train_time:80459ms step_avg:51.44ms
step:1565/2110 train_time:80548ms step_avg:51.47ms
step:1566/2110 train_time:80633ms step_avg:51.49ms
step:1567/2110 train_time:80719ms step_avg:51.51ms
step:1568/2110 train_time:80807ms step_avg:51.54ms
step:1569/2110 train_time:80893ms step_avg:51.56ms
step:1570/2110 train_time:80980ms step_avg:51.58ms
step:1571/2110 train_time:81067ms step_avg:51.60ms
step:1572/2110 train_time:81153ms step_avg:51.62ms
step:1573/2110 train_time:81241ms step_avg:51.65ms
step:1574/2110 train_time:81329ms step_avg:51.67ms
step:1575/2110 train_time:81415ms step_avg:51.69ms
step:1576/2110 train_time:81502ms step_avg:51.71ms
step:1577/2110 train_time:81589ms step_avg:51.74ms
step:1578/2110 train_time:81675ms step_avg:51.76ms
step:1579/2110 train_time:81762ms step_avg:51.78ms
step:1580/2110 train_time:81847ms step_avg:51.80ms
step:1581/2110 train_time:81935ms step_avg:51.82ms
step:1582/2110 train_time:82021ms step_avg:51.85ms
step:1583/2110 train_time:82109ms step_avg:51.87ms
step:1584/2110 train_time:82196ms step_avg:51.89ms
step:1585/2110 train_time:82284ms step_avg:51.91ms
step:1586/2110 train_time:82370ms step_avg:51.94ms
step:1587/2110 train_time:82458ms step_avg:51.96ms
step:1588/2110 train_time:82547ms step_avg:51.98ms
step:1589/2110 train_time:82633ms step_avg:52.00ms
step:1590/2110 train_time:82719ms step_avg:52.02ms
step:1591/2110 train_time:82806ms step_avg:52.05ms
step:1592/2110 train_time:82892ms step_avg:52.07ms
step:1593/2110 train_time:82980ms step_avg:52.09ms
step:1594/2110 train_time:83067ms step_avg:52.11ms
step:1595/2110 train_time:83153ms step_avg:52.13ms
step:1596/2110 train_time:83240ms step_avg:52.16ms
step:1597/2110 train_time:83328ms step_avg:52.18ms
step:1598/2110 train_time:83413ms step_avg:52.20ms
step:1599/2110 train_time:83501ms step_avg:52.22ms
step:1600/2110 train_time:83588ms step_avg:52.24ms
step:1601/2110 train_time:83675ms step_avg:52.26ms
step:1602/2110 train_time:83761ms step_avg:52.29ms
step:1603/2110 train_time:83848ms step_avg:52.31ms
step:1604/2110 train_time:83934ms step_avg:52.33ms
step:1605/2110 train_time:84022ms step_avg:52.35ms
step:1606/2110 train_time:84108ms step_avg:52.37ms
step:1607/2110 train_time:84195ms step_avg:52.39ms
step:1608/2110 train_time:84283ms step_avg:52.41ms
step:1609/2110 train_time:84370ms step_avg:52.44ms
step:1610/2110 train_time:84456ms step_avg:52.46ms
step:1611/2110 train_time:84545ms step_avg:52.48ms
step:1612/2110 train_time:84631ms step_avg:52.50ms
step:1613/2110 train_time:84717ms step_avg:52.52ms
step:1614/2110 train_time:84805ms step_avg:52.54ms
step:1615/2110 train_time:84892ms step_avg:52.56ms
step:1616/2110 train_time:84978ms step_avg:52.59ms
step:1617/2110 train_time:85065ms step_avg:52.61ms
step:1618/2110 train_time:85151ms step_avg:52.63ms
step:1619/2110 train_time:85240ms step_avg:52.65ms
step:1620/2110 train_time:85327ms step_avg:52.67ms
step:1621/2110 train_time:85414ms step_avg:52.69ms
step:1622/2110 train_time:85501ms step_avg:52.71ms
step:1623/2110 train_time:85589ms step_avg:52.74ms
step:1624/2110 train_time:85676ms step_avg:52.76ms
step:1625/2110 train_time:85762ms step_avg:52.78ms
step:1626/2110 train_time:85849ms step_avg:52.80ms
step:1627/2110 train_time:85936ms step_avg:52.82ms
step:1628/2110 train_time:86022ms step_avg:52.84ms
step:1629/2110 train_time:86110ms step_avg:52.86ms
step:1630/2110 train_time:86197ms step_avg:52.88ms
step:1631/2110 train_time:86285ms step_avg:52.90ms
step:1632/2110 train_time:86371ms step_avg:52.92ms
step:1633/2110 train_time:86460ms step_avg:52.95ms
step:1634/2110 train_time:86546ms step_avg:52.97ms
step:1635/2110 train_time:86634ms step_avg:52.99ms
step:1636/2110 train_time:86720ms step_avg:53.01ms
step:1637/2110 train_time:86809ms step_avg:53.03ms
step:1638/2110 train_time:86894ms step_avg:53.05ms
step:1639/2110 train_time:86982ms step_avg:53.07ms
step:1640/2110 train_time:87068ms step_avg:53.09ms
step:1641/2110 train_time:87155ms step_avg:53.11ms
step:1642/2110 train_time:87243ms step_avg:53.13ms
step:1643/2110 train_time:87331ms step_avg:53.15ms
step:1644/2110 train_time:87418ms step_avg:53.17ms
step:1645/2110 train_time:87505ms step_avg:53.19ms
step:1646/2110 train_time:87591ms step_avg:53.21ms
step:1647/2110 train_time:87679ms step_avg:53.24ms
step:1648/2110 train_time:87766ms step_avg:53.26ms
step:1649/2110 train_time:87852ms step_avg:53.28ms
step:1650/2110 train_time:87939ms step_avg:53.30ms
step:1651/2110 train_time:88026ms step_avg:53.32ms
step:1652/2110 train_time:88112ms step_avg:53.34ms
step:1653/2110 train_time:88201ms step_avg:53.36ms
step:1654/2110 train_time:88287ms step_avg:53.38ms
step:1655/2110 train_time:88373ms step_avg:53.40ms
step:1656/2110 train_time:88459ms step_avg:53.42ms
step:1657/2110 train_time:88547ms step_avg:53.44ms
step:1658/2110 train_time:88635ms step_avg:53.46ms
step:1659/2110 train_time:88723ms step_avg:53.48ms
step:1660/2110 train_time:88811ms step_avg:53.50ms
step:1661/2110 train_time:88899ms step_avg:53.52ms
step:1662/2110 train_time:88987ms step_avg:53.54ms
step:1663/2110 train_time:89075ms step_avg:53.56ms
step:1664/2110 train_time:89163ms step_avg:53.58ms
step:1665/2110 train_time:89253ms step_avg:53.61ms
step:1666/2110 train_time:89341ms step_avg:53.63ms
step:1667/2110 train_time:89430ms step_avg:53.65ms
step:1668/2110 train_time:89518ms step_avg:53.67ms
step:1669/2110 train_time:89607ms step_avg:53.69ms
step:1670/2110 train_time:89693ms step_avg:53.71ms
step:1671/2110 train_time:89783ms step_avg:53.73ms
step:1672/2110 train_time:89870ms step_avg:53.75ms
step:1673/2110 train_time:89958ms step_avg:53.77ms
step:1674/2110 train_time:90046ms step_avg:53.79ms
step:1675/2110 train_time:90134ms step_avg:53.81ms
step:1676/2110 train_time:90222ms step_avg:53.83ms
step:1677/2110 train_time:90311ms step_avg:53.85ms
step:1678/2110 train_time:90399ms step_avg:53.87ms
step:1679/2110 train_time:90487ms step_avg:53.89ms
step:1680/2110 train_time:90574ms step_avg:53.91ms
step:1681/2110 train_time:90663ms step_avg:53.93ms
step:1682/2110 train_time:90750ms step_avg:53.95ms
step:1683/2110 train_time:90839ms step_avg:53.97ms
step:1684/2110 train_time:90927ms step_avg:53.99ms
step:1685/2110 train_time:91015ms step_avg:54.01ms
step:1686/2110 train_time:91102ms step_avg:54.03ms
step:1687/2110 train_time:91191ms step_avg:54.06ms
step:1688/2110 train_time:91280ms step_avg:54.08ms
step:1689/2110 train_time:91368ms step_avg:54.10ms
step:1690/2110 train_time:91455ms step_avg:54.12ms
step:1691/2110 train_time:91545ms step_avg:54.14ms
step:1692/2110 train_time:91632ms step_avg:54.16ms
step:1693/2110 train_time:91720ms step_avg:54.18ms
step:1694/2110 train_time:91807ms step_avg:54.20ms
step:1695/2110 train_time:91896ms step_avg:54.22ms
step:1696/2110 train_time:91983ms step_avg:54.24ms
step:1697/2110 train_time:92072ms step_avg:54.26ms
step:1698/2110 train_time:92159ms step_avg:54.28ms
step:1699/2110 train_time:92250ms step_avg:54.30ms
step:1700/2110 train_time:92338ms step_avg:54.32ms
step:1701/2110 train_time:92427ms step_avg:54.34ms
step:1702/2110 train_time:92514ms step_avg:54.36ms
step:1703/2110 train_time:92603ms step_avg:54.38ms
step:1704/2110 train_time:92690ms step_avg:54.40ms
step:1705/2110 train_time:92778ms step_avg:54.42ms
step:1706/2110 train_time:92866ms step_avg:54.44ms
step:1707/2110 train_time:92954ms step_avg:54.45ms
step:1708/2110 train_time:93042ms step_avg:54.47ms
step:1709/2110 train_time:93131ms step_avg:54.49ms
step:1710/2110 train_time:93219ms step_avg:54.51ms
step:1711/2110 train_time:93309ms step_avg:54.53ms
step:1712/2110 train_time:93396ms step_avg:54.55ms
step:1713/2110 train_time:93484ms step_avg:54.57ms
step:1714/2110 train_time:93572ms step_avg:54.59ms
step:1715/2110 train_time:93660ms step_avg:54.61ms
step:1716/2110 train_time:93748ms step_avg:54.63ms
step:1717/2110 train_time:93837ms step_avg:54.65ms
step:1718/2110 train_time:93924ms step_avg:54.67ms
step:1719/2110 train_time:94013ms step_avg:54.69ms
step:1720/2110 train_time:94101ms step_avg:54.71ms
step:1721/2110 train_time:94191ms step_avg:54.73ms
step:1722/2110 train_time:94279ms step_avg:54.75ms
step:1723/2110 train_time:94367ms step_avg:54.77ms
step:1724/2110 train_time:94455ms step_avg:54.79ms
step:1725/2110 train_time:94544ms step_avg:54.81ms
step:1726/2110 train_time:94632ms step_avg:54.83ms
step:1727/2110 train_time:94721ms step_avg:54.85ms
step:1728/2110 train_time:94808ms step_avg:54.87ms
step:1729/2110 train_time:94897ms step_avg:54.89ms
step:1730/2110 train_time:94985ms step_avg:54.90ms
step:1731/2110 train_time:95073ms step_avg:54.92ms
step:1732/2110 train_time:95161ms step_avg:54.94ms
step:1733/2110 train_time:95251ms step_avg:54.96ms
step:1734/2110 train_time:95339ms step_avg:54.98ms
step:1735/2110 train_time:95427ms step_avg:55.00ms
step:1736/2110 train_time:95515ms step_avg:55.02ms
step:1737/2110 train_time:95605ms step_avg:55.04ms
step:1738/2110 train_time:95692ms step_avg:55.06ms
step:1739/2110 train_time:95780ms step_avg:55.08ms
step:1740/2110 train_time:95868ms step_avg:55.10ms
step:1741/2110 train_time:95956ms step_avg:55.12ms
step:1742/2110 train_time:96044ms step_avg:55.13ms
step:1743/2110 train_time:96133ms step_avg:55.15ms
step:1744/2110 train_time:96222ms step_avg:55.17ms
step:1745/2110 train_time:96310ms step_avg:55.19ms
step:1746/2110 train_time:96399ms step_avg:55.21ms
step:1747/2110 train_time:96486ms step_avg:55.23ms
step:1748/2110 train_time:96573ms step_avg:55.25ms
step:1749/2110 train_time:96662ms step_avg:55.27ms
step:1750/2110 train_time:96750ms step_avg:55.29ms
step:1750/2110 val_loss:3.3796 train_time:96840ms step_avg:55.34ms
step:1751/2110 train_time:96868ms step_avg:55.32ms
step:1752/2110 train_time:96930ms step_avg:55.33ms
step:1753/2110 train_time:97023ms step_avg:55.35ms
step:1754/2110 train_time:97110ms step_avg:55.36ms
step:1755/2110 train_time:97199ms step_avg:55.38ms
step:1756/2110 train_time:97284ms step_avg:55.40ms
step:1757/2110 train_time:97373ms step_avg:55.42ms
step:1758/2110 train_time:97459ms step_avg:55.44ms
step:1759/2110 train_time:97547ms step_avg:55.46ms
step:1760/2110 train_time:97634ms step_avg:55.47ms
step:1761/2110 train_time:97722ms step_avg:55.49ms
step:1762/2110 train_time:97812ms step_avg:55.51ms
step:1763/2110 train_time:97903ms step_avg:55.53ms
step:1764/2110 train_time:97991ms step_avg:55.55ms
step:1765/2110 train_time:98080ms step_avg:55.57ms
step:1766/2110 train_time:98168ms step_avg:55.59ms
step:1767/2110 train_time:98256ms step_avg:55.61ms
step:1768/2110 train_time:98342ms step_avg:55.62ms
step:1769/2110 train_time:98431ms step_avg:55.64ms
step:1770/2110 train_time:98517ms step_avg:55.66ms
step:1771/2110 train_time:98605ms step_avg:55.68ms
step:1772/2110 train_time:98691ms step_avg:55.69ms
step:1773/2110 train_time:98781ms step_avg:55.71ms
step:1774/2110 train_time:98870ms step_avg:55.73ms
step:1775/2110 train_time:98959ms step_avg:55.75ms
step:1776/2110 train_time:99046ms step_avg:55.77ms
step:1777/2110 train_time:99136ms step_avg:55.79ms
step:1778/2110 train_time:99222ms step_avg:55.81ms
step:1779/2110 train_time:99311ms step_avg:55.82ms
step:1780/2110 train_time:99397ms step_avg:55.84ms
step:1781/2110 train_time:99484ms step_avg:55.86ms
step:1782/2110 train_time:99571ms step_avg:55.88ms
step:1783/2110 train_time:99660ms step_avg:55.89ms
step:1784/2110 train_time:99747ms step_avg:55.91ms
step:1785/2110 train_time:99835ms step_avg:55.93ms
step:1786/2110 train_time:99923ms step_avg:55.95ms
step:1787/2110 train_time:100013ms step_avg:55.97ms
step:1788/2110 train_time:100100ms step_avg:55.98ms
step:1789/2110 train_time:100188ms step_avg:56.00ms
step:1790/2110 train_time:100275ms step_avg:56.02ms
step:1791/2110 train_time:100363ms step_avg:56.04ms
step:1792/2110 train_time:100450ms step_avg:56.05ms
step:1793/2110 train_time:100538ms step_avg:56.07ms
step:1794/2110 train_time:100625ms step_avg:56.09ms
step:1795/2110 train_time:100714ms step_avg:56.11ms
step:1796/2110 train_time:100802ms step_avg:56.13ms
step:1797/2110 train_time:100891ms step_avg:56.14ms
step:1798/2110 train_time:100979ms step_avg:56.16ms
step:1799/2110 train_time:101068ms step_avg:56.18ms
step:1800/2110 train_time:101156ms step_avg:56.20ms
step:1801/2110 train_time:101244ms step_avg:56.22ms
step:1802/2110 train_time:101331ms step_avg:56.23ms
step:1803/2110 train_time:101419ms step_avg:56.25ms
step:1804/2110 train_time:101505ms step_avg:56.27ms
step:1805/2110 train_time:101594ms step_avg:56.28ms
step:1806/2110 train_time:101681ms step_avg:56.30ms
step:1807/2110 train_time:101769ms step_avg:56.32ms
step:1808/2110 train_time:101856ms step_avg:56.34ms
step:1809/2110 train_time:101946ms step_avg:56.36ms
step:1810/2110 train_time:102034ms step_avg:56.37ms
step:1811/2110 train_time:102123ms step_avg:56.39ms
step:1812/2110 train_time:102211ms step_avg:56.41ms
step:1813/2110 train_time:102299ms step_avg:56.43ms
step:1814/2110 train_time:102385ms step_avg:56.44ms
step:1815/2110 train_time:102474ms step_avg:56.46ms
step:1816/2110 train_time:102562ms step_avg:56.48ms
step:1817/2110 train_time:102651ms step_avg:56.49ms
step:1818/2110 train_time:102739ms step_avg:56.51ms
step:1819/2110 train_time:102827ms step_avg:56.53ms
step:1820/2110 train_time:102914ms step_avg:56.55ms
step:1821/2110 train_time:103004ms step_avg:56.56ms
step:1822/2110 train_time:103092ms step_avg:56.58ms
step:1823/2110 train_time:103181ms step_avg:56.60ms
step:1824/2110 train_time:103268ms step_avg:56.62ms
step:1825/2110 train_time:103355ms step_avg:56.63ms
step:1826/2110 train_time:103442ms step_avg:56.65ms
step:1827/2110 train_time:103530ms step_avg:56.67ms
step:1828/2110 train_time:103617ms step_avg:56.68ms
step:1829/2110 train_time:103706ms step_avg:56.70ms
step:1830/2110 train_time:103794ms step_avg:56.72ms
step:1831/2110 train_time:103883ms step_avg:56.74ms
step:1832/2110 train_time:103970ms step_avg:56.75ms
step:1833/2110 train_time:104059ms step_avg:56.77ms
step:1834/2110 train_time:104145ms step_avg:56.79ms
step:1835/2110 train_time:104234ms step_avg:56.80ms
step:1836/2110 train_time:104321ms step_avg:56.82ms
step:1837/2110 train_time:104410ms step_avg:56.84ms
step:1838/2110 train_time:104497ms step_avg:56.85ms
step:1839/2110 train_time:104585ms step_avg:56.87ms
step:1840/2110 train_time:104672ms step_avg:56.89ms
step:1841/2110 train_time:104762ms step_avg:56.90ms
step:1842/2110 train_time:104849ms step_avg:56.92ms
step:1843/2110 train_time:104938ms step_avg:56.94ms
step:1844/2110 train_time:105025ms step_avg:56.95ms
step:1845/2110 train_time:105114ms step_avg:56.97ms
step:1846/2110 train_time:105201ms step_avg:56.99ms
step:1847/2110 train_time:105289ms step_avg:57.01ms
step:1848/2110 train_time:105377ms step_avg:57.02ms
step:1849/2110 train_time:105464ms step_avg:57.04ms
step:1850/2110 train_time:105552ms step_avg:57.06ms
step:1851/2110 train_time:105640ms step_avg:57.07ms
step:1852/2110 train_time:105728ms step_avg:57.09ms
step:1853/2110 train_time:105816ms step_avg:57.11ms
step:1854/2110 train_time:105904ms step_avg:57.12ms
step:1855/2110 train_time:105993ms step_avg:57.14ms
step:1856/2110 train_time:106080ms step_avg:57.16ms
step:1857/2110 train_time:106169ms step_avg:57.17ms
step:1858/2110 train_time:106256ms step_avg:57.19ms
step:1859/2110 train_time:106345ms step_avg:57.21ms
step:1860/2110 train_time:106432ms step_avg:57.22ms
step:1861/2110 train_time:106521ms step_avg:57.24ms
step:1862/2110 train_time:106608ms step_avg:57.25ms
step:1863/2110 train_time:106697ms step_avg:57.27ms
step:1864/2110 train_time:106784ms step_avg:57.29ms
step:1865/2110 train_time:106875ms step_avg:57.31ms
step:1866/2110 train_time:106963ms step_avg:57.32ms
step:1867/2110 train_time:107052ms step_avg:57.34ms
step:1868/2110 train_time:107139ms step_avg:57.36ms
step:1869/2110 train_time:107228ms step_avg:57.37ms
step:1870/2110 train_time:107314ms step_avg:57.39ms
step:1871/2110 train_time:107404ms step_avg:57.40ms
step:1872/2110 train_time:107491ms step_avg:57.42ms
step:1873/2110 train_time:107581ms step_avg:57.44ms
step:1874/2110 train_time:107672ms step_avg:57.46ms
step:1875/2110 train_time:107756ms step_avg:57.47ms
step:1876/2110 train_time:107843ms step_avg:57.49ms
step:1877/2110 train_time:107932ms step_avg:57.50ms
step:1878/2110 train_time:108019ms step_avg:57.52ms
step:1879/2110 train_time:108107ms step_avg:57.53ms
step:1880/2110 train_time:108194ms step_avg:57.55ms
step:1881/2110 train_time:108284ms step_avg:57.57ms
step:1882/2110 train_time:108371ms step_avg:57.58ms
step:1883/2110 train_time:108459ms step_avg:57.60ms
step:1884/2110 train_time:108546ms step_avg:57.61ms
step:1885/2110 train_time:108635ms step_avg:57.63ms
step:1886/2110 train_time:108722ms step_avg:57.65ms
step:1887/2110 train_time:108811ms step_avg:57.66ms
step:1888/2110 train_time:108899ms step_avg:57.68ms
step:1889/2110 train_time:108987ms step_avg:57.70ms
step:1890/2110 train_time:109075ms step_avg:57.71ms
step:1891/2110 train_time:109164ms step_avg:57.73ms
step:1892/2110 train_time:109252ms step_avg:57.74ms
step:1893/2110 train_time:109341ms step_avg:57.76ms
step:1894/2110 train_time:109429ms step_avg:57.78ms
step:1895/2110 train_time:109517ms step_avg:57.79ms
step:1896/2110 train_time:109604ms step_avg:57.81ms
step:1897/2110 train_time:109692ms step_avg:57.82ms
step:1898/2110 train_time:109780ms step_avg:57.84ms
step:1899/2110 train_time:109869ms step_avg:57.86ms
step:1900/2110 train_time:109957ms step_avg:57.87ms
step:1901/2110 train_time:110046ms step_avg:57.89ms
step:1902/2110 train_time:110133ms step_avg:57.90ms
step:1903/2110 train_time:110221ms step_avg:57.92ms
step:1904/2110 train_time:110309ms step_avg:57.94ms
step:1905/2110 train_time:110397ms step_avg:57.95ms
step:1906/2110 train_time:110484ms step_avg:57.97ms
step:1907/2110 train_time:110574ms step_avg:57.98ms
step:1908/2110 train_time:110662ms step_avg:58.00ms
step:1909/2110 train_time:110749ms step_avg:58.01ms
step:1910/2110 train_time:110836ms step_avg:58.03ms
step:1911/2110 train_time:110924ms step_avg:58.05ms
step:1912/2110 train_time:111012ms step_avg:58.06ms
step:1913/2110 train_time:111101ms step_avg:58.08ms
step:1914/2110 train_time:111187ms step_avg:58.09ms
step:1915/2110 train_time:111276ms step_avg:58.11ms
step:1916/2110 train_time:111363ms step_avg:58.12ms
step:1917/2110 train_time:111452ms step_avg:58.14ms
step:1918/2110 train_time:111540ms step_avg:58.15ms
step:1919/2110 train_time:111629ms step_avg:58.17ms
step:1920/2110 train_time:111716ms step_avg:58.19ms
step:1921/2110 train_time:111805ms step_avg:58.20ms
step:1922/2110 train_time:111892ms step_avg:58.22ms
step:1923/2110 train_time:111982ms step_avg:58.23ms
step:1924/2110 train_time:112069ms step_avg:58.25ms
step:1925/2110 train_time:112158ms step_avg:58.26ms
step:1926/2110 train_time:112245ms step_avg:58.28ms
step:1927/2110 train_time:112335ms step_avg:58.30ms
step:1928/2110 train_time:112422ms step_avg:58.31ms
step:1929/2110 train_time:112510ms step_avg:58.33ms
step:1930/2110 train_time:112597ms step_avg:58.34ms
step:1931/2110 train_time:112686ms step_avg:58.36ms
step:1932/2110 train_time:112774ms step_avg:58.37ms
step:1933/2110 train_time:112862ms step_avg:58.39ms
step:1934/2110 train_time:112949ms step_avg:58.40ms
step:1935/2110 train_time:113037ms step_avg:58.42ms
step:1936/2110 train_time:113124ms step_avg:58.43ms
step:1937/2110 train_time:113213ms step_avg:58.45ms
step:1938/2110 train_time:113302ms step_avg:58.46ms
step:1939/2110 train_time:113389ms step_avg:58.48ms
step:1940/2110 train_time:113477ms step_avg:58.49ms
step:1941/2110 train_time:113565ms step_avg:58.51ms
step:1942/2110 train_time:113652ms step_avg:58.52ms
step:1943/2110 train_time:113741ms step_avg:58.54ms
step:1944/2110 train_time:113829ms step_avg:58.55ms
step:1945/2110 train_time:113917ms step_avg:58.57ms
step:1946/2110 train_time:114004ms step_avg:58.58ms
step:1947/2110 train_time:114093ms step_avg:58.60ms
step:1948/2110 train_time:114180ms step_avg:58.61ms
step:1949/2110 train_time:114269ms step_avg:58.63ms
step:1950/2110 train_time:114357ms step_avg:58.64ms
step:1951/2110 train_time:114445ms step_avg:58.66ms
step:1952/2110 train_time:114532ms step_avg:58.67ms
step:1953/2110 train_time:114621ms step_avg:58.69ms
step:1954/2110 train_time:114707ms step_avg:58.70ms
step:1955/2110 train_time:114795ms step_avg:58.72ms
step:1956/2110 train_time:114883ms step_avg:58.73ms
step:1957/2110 train_time:114970ms step_avg:58.75ms
step:1958/2110 train_time:115058ms step_avg:58.76ms
step:1959/2110 train_time:115146ms step_avg:58.78ms
step:1960/2110 train_time:115233ms step_avg:58.79ms
step:1961/2110 train_time:115322ms step_avg:58.81ms
step:1962/2110 train_time:115411ms step_avg:58.82ms
step:1963/2110 train_time:115498ms step_avg:58.84ms
step:1964/2110 train_time:115586ms step_avg:58.85ms
step:1965/2110 train_time:115674ms step_avg:58.87ms
step:1966/2110 train_time:115763ms step_avg:58.88ms
step:1967/2110 train_time:115850ms step_avg:58.90ms
step:1968/2110 train_time:115939ms step_avg:58.91ms
step:1969/2110 train_time:116027ms step_avg:58.93ms
step:1970/2110 train_time:116115ms step_avg:58.94ms
step:1971/2110 train_time:116202ms step_avg:58.96ms
step:1972/2110 train_time:116290ms step_avg:58.97ms
step:1973/2110 train_time:116378ms step_avg:58.99ms
step:1974/2110 train_time:116465ms step_avg:59.00ms
step:1975/2110 train_time:116554ms step_avg:59.01ms
step:1976/2110 train_time:116642ms step_avg:59.03ms
step:1977/2110 train_time:116730ms step_avg:59.04ms
step:1978/2110 train_time:116818ms step_avg:59.06ms
step:1979/2110 train_time:116906ms step_avg:59.07ms
step:1980/2110 train_time:116994ms step_avg:59.09ms
step:1981/2110 train_time:117081ms step_avg:59.10ms
step:1982/2110 train_time:117170ms step_avg:59.12ms
step:1983/2110 train_time:117257ms step_avg:59.13ms
step:1984/2110 train_time:117344ms step_avg:59.15ms
step:1985/2110 train_time:117433ms step_avg:59.16ms
step:1986/2110 train_time:117521ms step_avg:59.17ms
step:1987/2110 train_time:117609ms step_avg:59.19ms
step:1988/2110 train_time:117696ms step_avg:59.20ms
step:1989/2110 train_time:117784ms step_avg:59.22ms
step:1990/2110 train_time:117871ms step_avg:59.23ms
step:1991/2110 train_time:117959ms step_avg:59.25ms
step:1992/2110 train_time:118046ms step_avg:59.26ms
step:1993/2110 train_time:118135ms step_avg:59.27ms
step:1994/2110 train_time:118223ms step_avg:59.29ms
step:1995/2110 train_time:118310ms step_avg:59.30ms
step:1996/2110 train_time:118398ms step_avg:59.32ms
step:1997/2110 train_time:118486ms step_avg:59.33ms
step:1998/2110 train_time:118574ms step_avg:59.35ms
step:1999/2110 train_time:118661ms step_avg:59.36ms
step:2000/2110 train_time:118750ms step_avg:59.37ms
step:2000/2110 val_loss:3.3044 train_time:118839ms step_avg:59.42ms
step:2001/2110 train_time:118874ms step_avg:59.41ms
step:2002/2110 train_time:118931ms step_avg:59.41ms
step:2003/2110 train_time:119024ms step_avg:59.42ms
step:2004/2110 train_time:119112ms step_avg:59.44ms
step:2005/2110 train_time:119199ms step_avg:59.45ms
step:2006/2110 train_time:119287ms step_avg:59.47ms
step:2007/2110 train_time:119374ms step_avg:59.48ms
step:2008/2110 train_time:119461ms step_avg:59.49ms
step:2009/2110 train_time:119547ms step_avg:59.51ms
step:2010/2110 train_time:119636ms step_avg:59.52ms
step:2011/2110 train_time:119724ms step_avg:59.53ms
step:2012/2110 train_time:119812ms step_avg:59.55ms
step:2013/2110 train_time:119903ms step_avg:59.56ms
step:2014/2110 train_time:119992ms step_avg:59.58ms
step:2015/2110 train_time:120081ms step_avg:59.59ms
step:2016/2110 train_time:120169ms step_avg:59.61ms
step:2017/2110 train_time:120256ms step_avg:59.62ms
step:2018/2110 train_time:120344ms step_avg:59.64ms
step:2019/2110 train_time:120431ms step_avg:59.65ms
step:2020/2110 train_time:120518ms step_avg:59.66ms
step:2021/2110 train_time:120604ms step_avg:59.68ms
step:2022/2110 train_time:120693ms step_avg:59.69ms
step:2023/2110 train_time:120781ms step_avg:59.70ms
step:2024/2110 train_time:120870ms step_avg:59.72ms
step:2025/2110 train_time:120960ms step_avg:59.73ms
step:2026/2110 train_time:121048ms step_avg:59.75ms
step:2027/2110 train_time:121137ms step_avg:59.76ms
step:2028/2110 train_time:121224ms step_avg:59.78ms
step:2029/2110 train_time:121312ms step_avg:59.79ms
step:2030/2110 train_time:121399ms step_avg:59.80ms
step:2031/2110 train_time:121487ms step_avg:59.82ms
step:2032/2110 train_time:121575ms step_avg:59.83ms
step:2033/2110 train_time:121662ms step_avg:59.84ms
step:2034/2110 train_time:121750ms step_avg:59.86ms
step:2035/2110 train_time:121841ms step_avg:59.87ms
step:2036/2110 train_time:121928ms step_avg:59.89ms
step:2037/2110 train_time:122018ms step_avg:59.90ms
step:2038/2110 train_time:122106ms step_avg:59.91ms
step:2039/2110 train_time:122195ms step_avg:59.93ms
step:2040/2110 train_time:122282ms step_avg:59.94ms
step:2041/2110 train_time:122369ms step_avg:59.96ms
step:2042/2110 train_time:122456ms step_avg:59.97ms
step:2043/2110 train_time:122543ms step_avg:59.98ms
step:2044/2110 train_time:122631ms step_avg:60.00ms
step:2045/2110 train_time:122718ms step_avg:60.01ms
step:2046/2110 train_time:122807ms step_avg:60.02ms
step:2047/2110 train_time:122896ms step_avg:60.04ms
step:2048/2110 train_time:122984ms step_avg:60.05ms
step:2049/2110 train_time:123074ms step_avg:60.07ms
step:2050/2110 train_time:123161ms step_avg:60.08ms
step:2051/2110 train_time:123250ms step_avg:60.09ms
step:2052/2110 train_time:123337ms step_avg:60.11ms
step:2053/2110 train_time:123425ms step_avg:60.12ms
step:2054/2110 train_time:123514ms step_avg:60.13ms
step:2055/2110 train_time:123601ms step_avg:60.15ms
step:2056/2110 train_time:123689ms step_avg:60.16ms
step:2057/2110 train_time:123777ms step_avg:60.17ms
step:2058/2110 train_time:123865ms step_avg:60.19ms
step:2059/2110 train_time:123954ms step_avg:60.20ms
step:2060/2110 train_time:124043ms step_avg:60.21ms
step:2061/2110 train_time:124131ms step_avg:60.23ms
step:2062/2110 train_time:124218ms step_avg:60.24ms
step:2063/2110 train_time:124308ms step_avg:60.26ms
step:2064/2110 train_time:124395ms step_avg:60.27ms
step:2065/2110 train_time:124483ms step_avg:60.28ms
step:2066/2110 train_time:124571ms step_avg:60.30ms
step:2067/2110 train_time:124660ms step_avg:60.31ms
step:2068/2110 train_time:124748ms step_avg:60.32ms
step:2069/2110 train_time:124835ms step_avg:60.34ms
step:2070/2110 train_time:124924ms step_avg:60.35ms
step:2071/2110 train_time:125012ms step_avg:60.36ms
step:2072/2110 train_time:125100ms step_avg:60.38ms
step:2073/2110 train_time:125189ms step_avg:60.39ms
step:2074/2110 train_time:125277ms step_avg:60.40ms
step:2075/2110 train_time:125367ms step_avg:60.42ms
step:2076/2110 train_time:125454ms step_avg:60.43ms
step:2077/2110 train_time:125542ms step_avg:60.44ms
step:2078/2110 train_time:125630ms step_avg:60.46ms
step:2079/2110 train_time:125719ms step_avg:60.47ms
step:2080/2110 train_time:125807ms step_avg:60.48ms
step:2081/2110 train_time:125895ms step_avg:60.50ms
step:2082/2110 train_time:125984ms step_avg:60.51ms
step:2083/2110 train_time:126071ms step_avg:60.52ms
step:2084/2110 train_time:126159ms step_avg:60.54ms
step:2085/2110 train_time:126248ms step_avg:60.55ms
step:2086/2110 train_time:126336ms step_avg:60.56ms
step:2087/2110 train_time:126425ms step_avg:60.58ms
step:2088/2110 train_time:126512ms step_avg:60.59ms
step:2089/2110 train_time:126601ms step_avg:60.60ms
step:2090/2110 train_time:126689ms step_avg:60.62ms
step:2091/2110 train_time:126777ms step_avg:60.63ms
step:2092/2110 train_time:126867ms step_avg:60.64ms
step:2093/2110 train_time:126956ms step_avg:60.66ms
step:2094/2110 train_time:127044ms step_avg:60.67ms
step:2095/2110 train_time:127133ms step_avg:60.68ms
step:2096/2110 train_time:127220ms step_avg:60.70ms
step:2097/2110 train_time:127310ms step_avg:60.71ms
step:2098/2110 train_time:127397ms step_avg:60.72ms
step:2099/2110 train_time:127487ms step_avg:60.74ms
step:2100/2110 train_time:127575ms step_avg:60.75ms
step:2101/2110 train_time:127663ms step_avg:60.76ms
step:2102/2110 train_time:127751ms step_avg:60.78ms
step:2103/2110 train_time:127839ms step_avg:60.79ms
step:2104/2110 train_time:127927ms step_avg:60.80ms
step:2105/2110 train_time:128016ms step_avg:60.82ms
step:2106/2110 train_time:128105ms step_avg:60.83ms
step:2107/2110 train_time:128193ms step_avg:60.84ms
step:2108/2110 train_time:128281ms step_avg:60.85ms
step:2109/2110 train_time:128370ms step_avg:60.87ms
step:2110/2110 train_time:128458ms step_avg:60.88ms
step:2110/2110 val_loss:3.2799 train_time:128549ms step_avg:60.92ms
peak memory allocated: 29892 MiB reserved: 43896 MiB

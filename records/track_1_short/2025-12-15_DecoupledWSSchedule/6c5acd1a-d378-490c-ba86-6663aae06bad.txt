import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Tue Dec 16 01:02:24 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   38C    P0            126W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   42C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   43C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            122W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              80      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              81      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:98ms step_avg:98.43ms
step:2/2110 train_time:130ms step_avg:64.84ms
step:3/2110 train_time:159ms step_avg:52.86ms
step:4/2110 train_time:189ms step_avg:47.20ms
step:5/2110 train_time:232ms step_avg:46.35ms
step:6/2110 train_time:398ms step_avg:66.31ms
step:7/2110 train_time:684ms step_avg:97.74ms
step:8/2110 train_time:717ms step_avg:89.57ms
step:9/2110 train_time:753ms step_avg:83.70ms
step:10/2110 train_time:787ms step_avg:78.74ms
step:11/2110 train_time:823ms step_avg:74.84ms
step:12/2110 train_time:856ms step_avg:71.33ms
step:13/2110 train_time:891ms step_avg:68.50ms
step:14/2110 train_time:925ms step_avg:66.07ms
step:15/2110 train_time:958ms step_avg:63.88ms
step:16/2110 train_time:991ms step_avg:61.93ms
step:17/2110 train_time:1024ms step_avg:60.25ms
step:18/2110 train_time:1057ms step_avg:58.75ms
step:19/2110 train_time:1090ms step_avg:57.38ms
step:20/2110 train_time:1123ms step_avg:56.15ms
step:21/2110 train_time:1156ms step_avg:55.06ms
step:22/2110 train_time:1190ms step_avg:54.10ms
step:23/2110 train_time:1222ms step_avg:53.14ms
step:24/2110 train_time:1255ms step_avg:52.29ms
step:25/2110 train_time:1290ms step_avg:51.59ms
step:26/2110 train_time:1321ms step_avg:50.82ms
step:27/2110 train_time:1355ms step_avg:50.20ms
step:28/2110 train_time:1390ms step_avg:49.64ms
step:29/2110 train_time:1428ms step_avg:49.23ms
step:30/2110 train_time:1461ms step_avg:48.70ms
step:31/2110 train_time:1497ms step_avg:48.29ms
step:32/2110 train_time:1533ms step_avg:47.91ms
step:33/2110 train_time:1570ms step_avg:47.58ms
step:34/2110 train_time:1604ms step_avg:47.17ms
step:35/2110 train_time:1640ms step_avg:46.85ms
step:36/2110 train_time:1673ms step_avg:46.47ms
step:37/2110 train_time:1710ms step_avg:46.21ms
step:38/2110 train_time:1745ms step_avg:45.92ms
step:39/2110 train_time:1790ms step_avg:45.89ms
step:40/2110 train_time:1824ms step_avg:45.60ms
step:41/2110 train_time:1860ms step_avg:45.37ms
step:42/2110 train_time:1901ms step_avg:45.27ms
step:43/2110 train_time:1939ms step_avg:45.09ms
step:44/2110 train_time:1973ms step_avg:44.85ms
step:45/2110 train_time:2010ms step_avg:44.68ms
step:46/2110 train_time:2046ms step_avg:44.48ms
step:47/2110 train_time:2085ms step_avg:44.36ms
step:48/2110 train_time:2119ms step_avg:44.15ms
step:49/2110 train_time:2155ms step_avg:43.98ms
step:50/2110 train_time:2190ms step_avg:43.81ms
step:51/2110 train_time:2229ms step_avg:43.71ms
step:52/2110 train_time:2262ms step_avg:43.49ms
step:53/2110 train_time:2300ms step_avg:43.39ms
step:54/2110 train_time:2334ms step_avg:43.22ms
step:55/2110 train_time:2369ms step_avg:43.08ms
step:56/2110 train_time:2403ms step_avg:42.91ms
step:57/2110 train_time:2439ms step_avg:42.80ms
step:58/2110 train_time:2473ms step_avg:42.63ms
step:59/2110 train_time:2505ms step_avg:42.47ms
step:60/2110 train_time:2538ms step_avg:42.30ms
step:61/2110 train_time:2572ms step_avg:42.16ms
step:62/2110 train_time:2605ms step_avg:42.01ms
step:63/2110 train_time:2638ms step_avg:41.87ms
step:64/2110 train_time:2674ms step_avg:41.78ms
step:65/2110 train_time:2709ms step_avg:41.68ms
step:66/2110 train_time:2745ms step_avg:41.59ms
step:67/2110 train_time:2783ms step_avg:41.54ms
step:68/2110 train_time:2818ms step_avg:41.44ms
step:69/2110 train_time:2853ms step_avg:41.35ms
step:70/2110 train_time:2887ms step_avg:41.24ms
step:71/2110 train_time:2923ms step_avg:41.17ms
step:72/2110 train_time:2957ms step_avg:41.06ms
step:73/2110 train_time:2993ms step_avg:41.00ms
step:74/2110 train_time:3028ms step_avg:40.91ms
step:75/2110 train_time:3064ms step_avg:40.85ms
step:76/2110 train_time:3098ms step_avg:40.76ms
step:77/2110 train_time:3133ms step_avg:40.69ms
step:78/2110 train_time:3166ms step_avg:40.59ms
step:79/2110 train_time:3202ms step_avg:40.54ms
step:80/2110 train_time:3237ms step_avg:40.47ms
step:81/2110 train_time:3273ms step_avg:40.41ms
step:82/2110 train_time:3306ms step_avg:40.32ms
step:83/2110 train_time:3342ms step_avg:40.26ms
step:84/2110 train_time:3375ms step_avg:40.18ms
step:85/2110 train_time:3411ms step_avg:40.13ms
step:86/2110 train_time:3445ms step_avg:40.06ms
step:87/2110 train_time:3482ms step_avg:40.02ms
step:88/2110 train_time:3517ms step_avg:39.97ms
step:89/2110 train_time:3552ms step_avg:39.90ms
step:90/2110 train_time:3586ms step_avg:39.84ms
step:91/2110 train_time:3621ms step_avg:39.79ms
step:92/2110 train_time:3655ms step_avg:39.73ms
step:93/2110 train_time:3690ms step_avg:39.68ms
step:94/2110 train_time:3724ms step_avg:39.62ms
step:95/2110 train_time:3759ms step_avg:39.57ms
step:96/2110 train_time:3794ms step_avg:39.52ms
step:97/2110 train_time:3828ms step_avg:39.47ms
step:98/2110 train_time:3862ms step_avg:39.41ms
step:99/2110 train_time:3898ms step_avg:39.38ms
step:100/2110 train_time:3932ms step_avg:39.32ms
step:101/2110 train_time:3968ms step_avg:39.28ms
step:102/2110 train_time:4001ms step_avg:39.23ms
step:103/2110 train_time:4038ms step_avg:39.20ms
step:104/2110 train_time:4071ms step_avg:39.15ms
step:105/2110 train_time:4107ms step_avg:39.12ms
step:106/2110 train_time:4141ms step_avg:39.06ms
step:107/2110 train_time:4177ms step_avg:39.04ms
step:108/2110 train_time:4210ms step_avg:38.98ms
step:109/2110 train_time:4246ms step_avg:38.95ms
step:110/2110 train_time:4279ms step_avg:38.90ms
step:111/2110 train_time:4316ms step_avg:38.88ms
step:112/2110 train_time:4350ms step_avg:38.84ms
step:113/2110 train_time:4386ms step_avg:38.81ms
step:114/2110 train_time:4418ms step_avg:38.76ms
step:115/2110 train_time:4455ms step_avg:38.74ms
step:116/2110 train_time:4489ms step_avg:38.70ms
step:117/2110 train_time:4524ms step_avg:38.67ms
step:118/2110 train_time:4557ms step_avg:38.62ms
step:119/2110 train_time:4593ms step_avg:38.59ms
step:120/2110 train_time:4627ms step_avg:38.56ms
step:121/2110 train_time:4662ms step_avg:38.53ms
step:122/2110 train_time:4696ms step_avg:38.49ms
step:123/2110 train_time:4733ms step_avg:38.48ms
step:124/2110 train_time:4766ms step_avg:38.43ms
step:125/2110 train_time:4803ms step_avg:38.42ms
step:126/2110 train_time:4835ms step_avg:38.37ms
step:127/2110 train_time:4871ms step_avg:38.35ms
step:128/2110 train_time:4903ms step_avg:38.31ms
step:129/2110 train_time:4939ms step_avg:38.28ms
step:130/2110 train_time:4973ms step_avg:38.25ms
step:131/2110 train_time:5010ms step_avg:38.24ms
step:132/2110 train_time:5042ms step_avg:38.20ms
step:133/2110 train_time:5078ms step_avg:38.18ms
step:134/2110 train_time:5111ms step_avg:38.14ms
step:135/2110 train_time:5147ms step_avg:38.12ms
step:136/2110 train_time:5181ms step_avg:38.09ms
step:137/2110 train_time:5216ms step_avg:38.08ms
step:138/2110 train_time:5250ms step_avg:38.04ms
step:139/2110 train_time:5285ms step_avg:38.02ms
step:140/2110 train_time:5318ms step_avg:37.98ms
step:141/2110 train_time:5354ms step_avg:37.97ms
step:142/2110 train_time:5386ms step_avg:37.93ms
step:143/2110 train_time:5422ms step_avg:37.91ms
step:144/2110 train_time:5454ms step_avg:37.88ms
step:145/2110 train_time:5490ms step_avg:37.86ms
step:146/2110 train_time:5524ms step_avg:37.83ms
step:147/2110 train_time:5559ms step_avg:37.82ms
step:148/2110 train_time:5593ms step_avg:37.79ms
step:149/2110 train_time:5627ms step_avg:37.77ms
step:150/2110 train_time:5661ms step_avg:37.74ms
step:151/2110 train_time:5697ms step_avg:37.73ms
step:152/2110 train_time:5731ms step_avg:37.71ms
step:153/2110 train_time:5767ms step_avg:37.69ms
step:154/2110 train_time:5801ms step_avg:37.67ms
step:155/2110 train_time:5836ms step_avg:37.65ms
step:156/2110 train_time:5869ms step_avg:37.62ms
step:157/2110 train_time:5905ms step_avg:37.61ms
step:158/2110 train_time:5939ms step_avg:37.59ms
step:159/2110 train_time:5975ms step_avg:37.58ms
step:160/2110 train_time:6009ms step_avg:37.56ms
step:161/2110 train_time:6045ms step_avg:37.55ms
step:162/2110 train_time:6079ms step_avg:37.52ms
step:163/2110 train_time:6117ms step_avg:37.53ms
step:164/2110 train_time:6150ms step_avg:37.50ms
step:165/2110 train_time:6186ms step_avg:37.49ms
step:166/2110 train_time:6220ms step_avg:37.47ms
step:167/2110 train_time:6255ms step_avg:37.45ms
step:168/2110 train_time:6289ms step_avg:37.44ms
step:169/2110 train_time:6324ms step_avg:37.42ms
step:170/2110 train_time:6358ms step_avg:37.40ms
step:171/2110 train_time:6394ms step_avg:37.39ms
step:172/2110 train_time:6428ms step_avg:37.37ms
step:173/2110 train_time:6463ms step_avg:37.36ms
step:174/2110 train_time:6496ms step_avg:37.33ms
step:175/2110 train_time:6532ms step_avg:37.33ms
step:176/2110 train_time:6566ms step_avg:37.31ms
step:177/2110 train_time:6602ms step_avg:37.30ms
step:178/2110 train_time:6637ms step_avg:37.29ms
step:179/2110 train_time:6671ms step_avg:37.27ms
step:180/2110 train_time:6706ms step_avg:37.26ms
step:181/2110 train_time:6740ms step_avg:37.24ms
step:182/2110 train_time:6775ms step_avg:37.23ms
step:183/2110 train_time:6812ms step_avg:37.22ms
step:184/2110 train_time:6845ms step_avg:37.20ms
step:185/2110 train_time:6881ms step_avg:37.20ms
step:186/2110 train_time:6916ms step_avg:37.19ms
step:187/2110 train_time:6948ms step_avg:37.16ms
step:188/2110 train_time:6982ms step_avg:37.14ms
step:189/2110 train_time:7018ms step_avg:37.13ms
step:190/2110 train_time:7051ms step_avg:37.11ms
step:191/2110 train_time:7086ms step_avg:37.10ms
step:192/2110 train_time:7119ms step_avg:37.08ms
step:193/2110 train_time:7155ms step_avg:37.07ms
step:194/2110 train_time:7190ms step_avg:37.06ms
step:195/2110 train_time:7225ms step_avg:37.05ms
step:196/2110 train_time:7265ms step_avg:37.07ms
step:197/2110 train_time:7301ms step_avg:37.06ms
step:198/2110 train_time:7336ms step_avg:37.05ms
step:199/2110 train_time:7367ms step_avg:37.02ms
step:200/2110 train_time:7403ms step_avg:37.02ms
step:201/2110 train_time:7433ms step_avg:36.98ms
step:202/2110 train_time:7465ms step_avg:36.96ms
step:203/2110 train_time:7502ms step_avg:36.95ms
step:204/2110 train_time:7540ms step_avg:36.96ms
step:205/2110 train_time:7570ms step_avg:36.93ms
step:206/2110 train_time:7604ms step_avg:36.91ms
step:207/2110 train_time:7639ms step_avg:36.91ms
step:208/2110 train_time:7680ms step_avg:36.92ms
step:209/2110 train_time:7710ms step_avg:36.89ms
step:210/2110 train_time:7745ms step_avg:36.88ms
step:211/2110 train_time:7781ms step_avg:36.87ms
step:212/2110 train_time:7817ms step_avg:36.87ms
step:213/2110 train_time:7851ms step_avg:36.86ms
step:214/2110 train_time:7885ms step_avg:36.85ms
step:215/2110 train_time:7919ms step_avg:36.83ms
step:216/2110 train_time:7950ms step_avg:36.81ms
step:217/2110 train_time:7984ms step_avg:36.79ms
step:218/2110 train_time:8019ms step_avg:36.79ms
step:219/2110 train_time:8056ms step_avg:36.78ms
step:220/2110 train_time:8091ms step_avg:36.78ms
step:221/2110 train_time:8123ms step_avg:36.76ms
step:222/2110 train_time:8158ms step_avg:36.75ms
step:223/2110 train_time:8191ms step_avg:36.73ms
step:224/2110 train_time:8230ms step_avg:36.74ms
step:225/2110 train_time:8270ms step_avg:36.76ms
step:226/2110 train_time:8312ms step_avg:36.78ms
step:227/2110 train_time:8348ms step_avg:36.78ms
step:228/2110 train_time:8388ms step_avg:36.79ms
step:229/2110 train_time:8424ms step_avg:36.79ms
step:230/2110 train_time:8464ms step_avg:36.80ms
step:231/2110 train_time:8492ms step_avg:36.76ms
step:232/2110 train_time:8523ms step_avg:36.74ms
step:233/2110 train_time:8550ms step_avg:36.69ms
step:234/2110 train_time:8581ms step_avg:36.67ms
step:235/2110 train_time:8618ms step_avg:36.67ms
step:236/2110 train_time:8652ms step_avg:36.66ms
step:237/2110 train_time:8687ms step_avg:36.65ms
step:238/2110 train_time:8721ms step_avg:36.64ms
step:239/2110 train_time:8756ms step_avg:36.64ms
step:240/2110 train_time:8790ms step_avg:36.63ms
step:241/2110 train_time:8826ms step_avg:36.62ms
step:242/2110 train_time:8859ms step_avg:36.61ms
step:243/2110 train_time:8894ms step_avg:36.60ms
step:244/2110 train_time:8928ms step_avg:36.59ms
step:245/2110 train_time:8964ms step_avg:36.59ms
step:246/2110 train_time:8998ms step_avg:36.58ms
step:247/2110 train_time:9034ms step_avg:36.58ms
step:248/2110 train_time:9067ms step_avg:36.56ms
step:249/2110 train_time:9102ms step_avg:36.56ms
step:250/2110 train_time:9135ms step_avg:36.54ms
step:250/2110 val_loss:4.2919 train_time:9173ms step_avg:36.69ms
step:251/2110 train_time:9209ms step_avg:36.69ms
step:252/2110 train_time:9242ms step_avg:36.67ms
step:253/2110 train_time:9271ms step_avg:36.64ms
step:254/2110 train_time:9298ms step_avg:36.61ms
step:255/2110 train_time:9325ms step_avg:36.57ms
step:256/2110 train_time:9356ms step_avg:36.55ms
step:257/2110 train_time:9390ms step_avg:36.54ms
step:258/2110 train_time:9423ms step_avg:36.52ms
step:259/2110 train_time:9457ms step_avg:36.51ms
step:260/2110 train_time:9490ms step_avg:36.50ms
step:261/2110 train_time:9526ms step_avg:36.50ms
step:262/2110 train_time:9559ms step_avg:36.48ms
step:263/2110 train_time:9595ms step_avg:36.48ms
step:264/2110 train_time:9628ms step_avg:36.47ms
step:265/2110 train_time:9669ms step_avg:36.49ms
step:266/2110 train_time:9702ms step_avg:36.47ms
step:267/2110 train_time:9738ms step_avg:36.47ms
step:268/2110 train_time:9772ms step_avg:36.46ms
step:269/2110 train_time:9807ms step_avg:36.46ms
step:270/2110 train_time:9841ms step_avg:36.45ms
step:271/2110 train_time:9877ms step_avg:36.45ms
step:272/2110 train_time:9912ms step_avg:36.44ms
step:273/2110 train_time:9946ms step_avg:36.43ms
step:274/2110 train_time:9982ms step_avg:36.43ms
step:275/2110 train_time:10017ms step_avg:36.42ms
step:276/2110 train_time:10051ms step_avg:36.42ms
step:277/2110 train_time:10088ms step_avg:36.42ms
step:278/2110 train_time:10122ms step_avg:36.41ms
step:279/2110 train_time:10158ms step_avg:36.41ms
step:280/2110 train_time:10192ms step_avg:36.40ms
step:281/2110 train_time:10228ms step_avg:36.40ms
step:282/2110 train_time:10262ms step_avg:36.39ms
step:283/2110 train_time:10299ms step_avg:36.39ms
step:284/2110 train_time:10332ms step_avg:36.38ms
step:285/2110 train_time:10369ms step_avg:36.38ms
step:286/2110 train_time:10402ms step_avg:36.37ms
step:287/2110 train_time:10437ms step_avg:36.36ms
step:288/2110 train_time:10469ms step_avg:36.35ms
step:289/2110 train_time:10506ms step_avg:36.35ms
step:290/2110 train_time:10539ms step_avg:36.34ms
step:291/2110 train_time:10576ms step_avg:36.34ms
step:292/2110 train_time:10609ms step_avg:36.33ms
step:293/2110 train_time:10645ms step_avg:36.33ms
step:294/2110 train_time:10678ms step_avg:36.32ms
step:295/2110 train_time:10714ms step_avg:36.32ms
step:296/2110 train_time:10747ms step_avg:36.31ms
step:297/2110 train_time:10785ms step_avg:36.31ms
step:298/2110 train_time:10818ms step_avg:36.30ms
step:299/2110 train_time:10854ms step_avg:36.30ms
step:300/2110 train_time:10887ms step_avg:36.29ms
step:301/2110 train_time:10923ms step_avg:36.29ms
step:302/2110 train_time:10956ms step_avg:36.28ms
step:303/2110 train_time:10994ms step_avg:36.28ms
step:304/2110 train_time:11027ms step_avg:36.27ms
step:305/2110 train_time:11065ms step_avg:36.28ms
step:306/2110 train_time:11099ms step_avg:36.27ms
step:307/2110 train_time:11134ms step_avg:36.27ms
step:308/2110 train_time:11167ms step_avg:36.26ms
step:309/2110 train_time:11203ms step_avg:36.25ms
step:310/2110 train_time:11236ms step_avg:36.25ms
step:311/2110 train_time:11272ms step_avg:36.24ms
step:312/2110 train_time:11306ms step_avg:36.24ms
step:313/2110 train_time:11342ms step_avg:36.24ms
step:314/2110 train_time:11376ms step_avg:36.23ms
step:315/2110 train_time:11411ms step_avg:36.23ms
step:316/2110 train_time:11444ms step_avg:36.22ms
step:317/2110 train_time:11480ms step_avg:36.21ms
step:318/2110 train_time:11513ms step_avg:36.20ms
step:319/2110 train_time:11549ms step_avg:36.20ms
step:320/2110 train_time:11582ms step_avg:36.19ms
step:321/2110 train_time:11617ms step_avg:36.19ms
step:322/2110 train_time:11651ms step_avg:36.18ms
step:323/2110 train_time:11687ms step_avg:36.18ms
step:324/2110 train_time:11721ms step_avg:36.18ms
step:325/2110 train_time:11757ms step_avg:36.17ms
step:326/2110 train_time:11790ms step_avg:36.17ms
step:327/2110 train_time:11828ms step_avg:36.17ms
step:328/2110 train_time:11861ms step_avg:36.16ms
step:329/2110 train_time:11896ms step_avg:36.16ms
step:330/2110 train_time:11932ms step_avg:36.16ms
step:331/2110 train_time:11967ms step_avg:36.15ms
step:332/2110 train_time:12001ms step_avg:36.15ms
step:333/2110 train_time:12036ms step_avg:36.14ms
step:334/2110 train_time:12069ms step_avg:36.13ms
step:335/2110 train_time:12104ms step_avg:36.13ms
step:336/2110 train_time:12137ms step_avg:36.12ms
step:337/2110 train_time:12170ms step_avg:36.11ms
step:338/2110 train_time:12203ms step_avg:36.10ms
step:339/2110 train_time:12236ms step_avg:36.09ms
step:340/2110 train_time:12268ms step_avg:36.08ms
step:341/2110 train_time:12301ms step_avg:36.07ms
step:342/2110 train_time:12333ms step_avg:36.06ms
step:343/2110 train_time:12367ms step_avg:36.05ms
step:344/2110 train_time:12399ms step_avg:36.04ms
step:345/2110 train_time:12432ms step_avg:36.03ms
step:346/2110 train_time:12465ms step_avg:36.03ms
step:347/2110 train_time:12497ms step_avg:36.02ms
step:348/2110 train_time:12531ms step_avg:36.01ms
step:349/2110 train_time:12563ms step_avg:36.00ms
step:350/2110 train_time:12597ms step_avg:35.99ms
step:351/2110 train_time:12630ms step_avg:35.98ms
step:352/2110 train_time:12663ms step_avg:35.97ms
step:353/2110 train_time:12695ms step_avg:35.96ms
step:354/2110 train_time:12728ms step_avg:35.96ms
step:355/2110 train_time:12761ms step_avg:35.95ms
step:356/2110 train_time:12795ms step_avg:35.94ms
step:357/2110 train_time:12828ms step_avg:35.93ms
step:358/2110 train_time:12861ms step_avg:35.92ms
step:359/2110 train_time:12895ms step_avg:35.92ms
step:360/2110 train_time:12929ms step_avg:35.91ms
step:361/2110 train_time:12962ms step_avg:35.90ms
step:362/2110 train_time:12998ms step_avg:35.91ms
step:363/2110 train_time:13040ms step_avg:35.92ms
step:364/2110 train_time:13081ms step_avg:35.94ms
step:365/2110 train_time:13124ms step_avg:35.96ms
step:366/2110 train_time:13169ms step_avg:35.98ms
step:367/2110 train_time:13213ms step_avg:36.00ms
step:368/2110 train_time:13256ms step_avg:36.02ms
step:369/2110 train_time:13298ms step_avg:36.04ms
step:370/2110 train_time:13338ms step_avg:36.05ms
step:371/2110 train_time:13382ms step_avg:36.07ms
step:372/2110 train_time:13425ms step_avg:36.09ms
step:373/2110 train_time:13468ms step_avg:36.11ms
step:374/2110 train_time:13510ms step_avg:36.12ms
step:375/2110 train_time:13554ms step_avg:36.14ms
step:376/2110 train_time:13597ms step_avg:36.16ms
step:377/2110 train_time:13640ms step_avg:36.18ms
step:378/2110 train_time:13682ms step_avg:36.20ms
step:379/2110 train_time:13728ms step_avg:36.22ms
step:380/2110 train_time:13767ms step_avg:36.23ms
step:381/2110 train_time:13809ms step_avg:36.24ms
step:382/2110 train_time:13850ms step_avg:36.26ms
step:383/2110 train_time:13891ms step_avg:36.27ms
step:384/2110 train_time:13934ms step_avg:36.29ms
step:385/2110 train_time:13977ms step_avg:36.30ms
step:386/2110 train_time:14020ms step_avg:36.32ms
step:387/2110 train_time:14061ms step_avg:36.33ms
step:388/2110 train_time:14103ms step_avg:36.35ms
step:389/2110 train_time:14144ms step_avg:36.36ms
step:390/2110 train_time:14184ms step_avg:36.37ms
step:391/2110 train_time:14229ms step_avg:36.39ms
step:392/2110 train_time:14273ms step_avg:36.41ms
step:393/2110 train_time:14319ms step_avg:36.43ms
step:394/2110 train_time:14361ms step_avg:36.45ms
step:395/2110 train_time:14405ms step_avg:36.47ms
step:396/2110 train_time:14446ms step_avg:36.48ms
step:397/2110 train_time:14480ms step_avg:36.47ms
step:398/2110 train_time:14512ms step_avg:36.46ms
step:399/2110 train_time:14545ms step_avg:36.45ms
step:400/2110 train_time:14578ms step_avg:36.44ms
step:401/2110 train_time:14610ms step_avg:36.43ms
step:402/2110 train_time:14644ms step_avg:36.43ms
step:403/2110 train_time:14680ms step_avg:36.43ms
step:404/2110 train_time:14716ms step_avg:36.43ms
step:405/2110 train_time:14755ms step_avg:36.43ms
step:406/2110 train_time:14799ms step_avg:36.45ms
step:407/2110 train_time:14836ms step_avg:36.45ms
step:408/2110 train_time:14868ms step_avg:36.44ms
step:409/2110 train_time:14901ms step_avg:36.43ms
step:410/2110 train_time:14934ms step_avg:36.42ms
step:411/2110 train_time:14966ms step_avg:36.41ms
step:412/2110 train_time:14999ms step_avg:36.41ms
step:413/2110 train_time:15032ms step_avg:36.40ms
step:414/2110 train_time:15065ms step_avg:36.39ms
step:415/2110 train_time:15097ms step_avg:36.38ms
step:416/2110 train_time:15130ms step_avg:36.37ms
step:417/2110 train_time:15163ms step_avg:36.36ms
step:418/2110 train_time:15196ms step_avg:36.35ms
step:419/2110 train_time:15228ms step_avg:36.34ms
step:420/2110 train_time:15261ms step_avg:36.34ms
step:421/2110 train_time:15294ms step_avg:36.33ms
step:422/2110 train_time:15326ms step_avg:36.32ms
step:423/2110 train_time:15359ms step_avg:36.31ms
step:424/2110 train_time:15392ms step_avg:36.30ms
step:425/2110 train_time:15425ms step_avg:36.29ms
step:426/2110 train_time:15457ms step_avg:36.28ms
step:427/2110 train_time:15490ms step_avg:36.28ms
step:428/2110 train_time:15523ms step_avg:36.27ms
step:429/2110 train_time:15556ms step_avg:36.26ms
step:430/2110 train_time:15589ms step_avg:36.25ms
step:431/2110 train_time:15622ms step_avg:36.25ms
step:432/2110 train_time:15656ms step_avg:36.24ms
step:433/2110 train_time:15689ms step_avg:36.23ms
step:434/2110 train_time:15724ms step_avg:36.23ms
step:435/2110 train_time:15756ms step_avg:36.22ms
step:436/2110 train_time:15790ms step_avg:36.22ms
step:437/2110 train_time:15823ms step_avg:36.21ms
step:438/2110 train_time:15859ms step_avg:36.21ms
step:439/2110 train_time:15890ms step_avg:36.19ms
step:440/2110 train_time:15922ms step_avg:36.19ms
step:441/2110 train_time:15955ms step_avg:36.18ms
step:442/2110 train_time:15988ms step_avg:36.17ms
step:443/2110 train_time:16021ms step_avg:36.17ms
step:444/2110 train_time:16054ms step_avg:36.16ms
step:445/2110 train_time:16087ms step_avg:36.15ms
step:446/2110 train_time:16120ms step_avg:36.14ms
step:447/2110 train_time:16153ms step_avg:36.14ms
step:448/2110 train_time:16185ms step_avg:36.13ms
step:449/2110 train_time:16218ms step_avg:36.12ms
step:450/2110 train_time:16251ms step_avg:36.11ms
step:451/2110 train_time:16284ms step_avg:36.11ms
step:452/2110 train_time:16316ms step_avg:36.10ms
step:453/2110 train_time:16350ms step_avg:36.09ms
step:454/2110 train_time:16382ms step_avg:36.08ms
step:455/2110 train_time:16416ms step_avg:36.08ms
step:456/2110 train_time:16449ms step_avg:36.07ms
step:457/2110 train_time:16481ms step_avg:36.06ms
step:458/2110 train_time:16514ms step_avg:36.06ms
step:459/2110 train_time:16547ms step_avg:36.05ms
step:460/2110 train_time:16580ms step_avg:36.04ms
step:461/2110 train_time:16613ms step_avg:36.04ms
step:462/2110 train_time:16647ms step_avg:36.03ms
step:463/2110 train_time:16680ms step_avg:36.03ms
step:464/2110 train_time:16713ms step_avg:36.02ms
step:465/2110 train_time:16747ms step_avg:36.01ms
step:466/2110 train_time:16780ms step_avg:36.01ms
step:467/2110 train_time:16814ms step_avg:36.00ms
step:468/2110 train_time:16847ms step_avg:36.00ms
step:469/2110 train_time:16880ms step_avg:35.99ms
step:470/2110 train_time:16912ms step_avg:35.98ms
step:471/2110 train_time:16946ms step_avg:35.98ms
step:472/2110 train_time:16979ms step_avg:35.97ms
step:473/2110 train_time:17012ms step_avg:35.97ms
step:474/2110 train_time:17046ms step_avg:35.96ms
step:475/2110 train_time:17079ms step_avg:35.95ms
step:476/2110 train_time:17112ms step_avg:35.95ms
step:477/2110 train_time:17144ms step_avg:35.94ms
step:478/2110 train_time:17177ms step_avg:35.94ms
step:479/2110 train_time:17210ms step_avg:35.93ms
step:480/2110 train_time:17242ms step_avg:35.92ms
step:481/2110 train_time:17276ms step_avg:35.92ms
step:482/2110 train_time:17308ms step_avg:35.91ms
step:483/2110 train_time:17341ms step_avg:35.90ms
step:484/2110 train_time:17374ms step_avg:35.90ms
step:485/2110 train_time:17407ms step_avg:35.89ms
step:486/2110 train_time:17440ms step_avg:35.89ms
step:487/2110 train_time:17473ms step_avg:35.88ms
step:488/2110 train_time:17506ms step_avg:35.87ms
step:489/2110 train_time:17539ms step_avg:35.87ms
step:490/2110 train_time:17572ms step_avg:35.86ms
step:491/2110 train_time:17606ms step_avg:35.86ms
step:492/2110 train_time:17639ms step_avg:35.85ms
step:493/2110 train_time:17673ms step_avg:35.85ms
step:494/2110 train_time:17706ms step_avg:35.84ms
step:495/2110 train_time:17739ms step_avg:35.84ms
step:496/2110 train_time:17772ms step_avg:35.83ms
step:497/2110 train_time:17805ms step_avg:35.83ms
step:498/2110 train_time:17838ms step_avg:35.82ms
step:499/2110 train_time:17871ms step_avg:35.81ms
step:500/2110 train_time:17905ms step_avg:35.81ms
step:500/2110 val_loss:4.0270 train_time:17941ms step_avg:35.88ms
step:501/2110 train_time:17968ms step_avg:35.86ms
step:502/2110 train_time:17994ms step_avg:35.84ms
step:503/2110 train_time:18021ms step_avg:35.83ms
step:504/2110 train_time:18054ms step_avg:35.82ms
step:505/2110 train_time:18088ms step_avg:35.82ms
step:506/2110 train_time:18122ms step_avg:35.82ms
step:507/2110 train_time:18157ms step_avg:35.81ms
step:508/2110 train_time:18189ms step_avg:35.81ms
step:509/2110 train_time:18222ms step_avg:35.80ms
step:510/2110 train_time:18255ms step_avg:35.79ms
step:511/2110 train_time:18288ms step_avg:35.79ms
step:512/2110 train_time:18321ms step_avg:35.78ms
step:513/2110 train_time:18354ms step_avg:35.78ms
step:514/2110 train_time:18386ms step_avg:35.77ms
step:515/2110 train_time:18421ms step_avg:35.77ms
step:516/2110 train_time:18453ms step_avg:35.76ms
step:517/2110 train_time:18485ms step_avg:35.75ms
step:518/2110 train_time:18518ms step_avg:35.75ms
step:519/2110 train_time:18551ms step_avg:35.74ms
step:520/2110 train_time:18584ms step_avg:35.74ms
step:521/2110 train_time:18617ms step_avg:35.73ms
step:522/2110 train_time:18649ms step_avg:35.73ms
step:523/2110 train_time:18682ms step_avg:35.72ms
step:524/2110 train_time:18715ms step_avg:35.72ms
step:525/2110 train_time:18748ms step_avg:35.71ms
step:526/2110 train_time:18781ms step_avg:35.70ms
step:527/2110 train_time:18813ms step_avg:35.70ms
step:528/2110 train_time:18845ms step_avg:35.69ms
step:529/2110 train_time:18878ms step_avg:35.69ms
step:530/2110 train_time:18911ms step_avg:35.68ms
step:531/2110 train_time:18945ms step_avg:35.68ms
step:532/2110 train_time:18978ms step_avg:35.67ms
step:533/2110 train_time:19012ms step_avg:35.67ms
step:534/2110 train_time:19045ms step_avg:35.66ms
step:535/2110 train_time:19078ms step_avg:35.66ms
step:536/2110 train_time:19112ms step_avg:35.66ms
step:537/2110 train_time:19145ms step_avg:35.65ms
step:538/2110 train_time:19179ms step_avg:35.65ms
step:539/2110 train_time:19212ms step_avg:35.64ms
step:540/2110 train_time:19245ms step_avg:35.64ms
step:541/2110 train_time:19278ms step_avg:35.63ms
step:542/2110 train_time:19311ms step_avg:35.63ms
step:543/2110 train_time:19343ms step_avg:35.62ms
step:544/2110 train_time:19376ms step_avg:35.62ms
step:545/2110 train_time:19409ms step_avg:35.61ms
step:546/2110 train_time:19442ms step_avg:35.61ms
step:547/2110 train_time:19475ms step_avg:35.60ms
step:548/2110 train_time:19508ms step_avg:35.60ms
step:549/2110 train_time:19541ms step_avg:35.59ms
step:550/2110 train_time:19574ms step_avg:35.59ms
step:551/2110 train_time:19607ms step_avg:35.58ms
step:552/2110 train_time:19640ms step_avg:35.58ms
step:553/2110 train_time:19673ms step_avg:35.57ms
step:554/2110 train_time:19706ms step_avg:35.57ms
step:555/2110 train_time:19738ms step_avg:35.56ms
step:556/2110 train_time:19772ms step_avg:35.56ms
step:557/2110 train_time:19804ms step_avg:35.56ms
step:558/2110 train_time:19837ms step_avg:35.55ms
step:559/2110 train_time:19870ms step_avg:35.55ms
step:560/2110 train_time:19903ms step_avg:35.54ms
step:561/2110 train_time:19936ms step_avg:35.54ms
step:562/2110 train_time:19969ms step_avg:35.53ms
step:563/2110 train_time:20003ms step_avg:35.53ms
step:564/2110 train_time:20036ms step_avg:35.52ms
step:565/2110 train_time:20069ms step_avg:35.52ms
step:566/2110 train_time:20102ms step_avg:35.52ms
step:567/2110 train_time:20135ms step_avg:35.51ms
step:568/2110 train_time:20168ms step_avg:35.51ms
step:569/2110 train_time:20202ms step_avg:35.50ms
step:570/2110 train_time:20235ms step_avg:35.50ms
step:571/2110 train_time:20268ms step_avg:35.50ms
step:572/2110 train_time:20301ms step_avg:35.49ms
step:573/2110 train_time:20334ms step_avg:35.49ms
step:574/2110 train_time:20367ms step_avg:35.48ms
step:575/2110 train_time:20400ms step_avg:35.48ms
step:576/2110 train_time:20434ms step_avg:35.48ms
step:577/2110 train_time:20467ms step_avg:35.47ms
step:578/2110 train_time:20500ms step_avg:35.47ms
step:579/2110 train_time:20533ms step_avg:35.46ms
step:580/2110 train_time:20566ms step_avg:35.46ms
step:581/2110 train_time:20602ms step_avg:35.46ms
step:582/2110 train_time:20640ms step_avg:35.46ms
step:583/2110 train_time:20669ms step_avg:35.45ms
step:584/2110 train_time:20699ms step_avg:35.44ms
step:585/2110 train_time:20731ms step_avg:35.44ms
step:586/2110 train_time:20765ms step_avg:35.43ms
step:587/2110 train_time:20797ms step_avg:35.43ms
step:588/2110 train_time:20830ms step_avg:35.43ms
step:589/2110 train_time:20866ms step_avg:35.43ms
step:590/2110 train_time:20909ms step_avg:35.44ms
step:591/2110 train_time:20941ms step_avg:35.43ms
step:592/2110 train_time:20976ms step_avg:35.43ms
step:593/2110 train_time:21007ms step_avg:35.43ms
step:594/2110 train_time:21048ms step_avg:35.43ms
step:595/2110 train_time:21087ms step_avg:35.44ms
step:596/2110 train_time:21125ms step_avg:35.44ms
step:597/2110 train_time:21161ms step_avg:35.44ms
step:598/2110 train_time:21201ms step_avg:35.45ms
step:599/2110 train_time:21231ms step_avg:35.44ms
step:600/2110 train_time:21268ms step_avg:35.45ms
step:601/2110 train_time:21303ms step_avg:35.45ms
step:602/2110 train_time:21341ms step_avg:35.45ms
step:603/2110 train_time:21373ms step_avg:35.44ms
step:604/2110 train_time:21410ms step_avg:35.45ms
step:605/2110 train_time:21441ms step_avg:35.44ms
step:606/2110 train_time:21474ms step_avg:35.44ms
step:607/2110 train_time:21505ms step_avg:35.43ms
step:608/2110 train_time:21539ms step_avg:35.43ms
step:609/2110 train_time:21573ms step_avg:35.42ms
step:610/2110 train_time:21606ms step_avg:35.42ms
step:611/2110 train_time:21637ms step_avg:35.41ms
step:612/2110 train_time:21674ms step_avg:35.41ms
step:613/2110 train_time:21710ms step_avg:35.42ms
step:614/2110 train_time:21745ms step_avg:35.42ms
step:615/2110 train_time:21781ms step_avg:35.42ms
step:616/2110 train_time:21819ms step_avg:35.42ms
step:617/2110 train_time:21850ms step_avg:35.41ms
step:618/2110 train_time:21883ms step_avg:35.41ms
step:619/2110 train_time:21917ms step_avg:35.41ms
step:620/2110 train_time:21953ms step_avg:35.41ms
step:621/2110 train_time:21983ms step_avg:35.40ms
step:622/2110 train_time:22015ms step_avg:35.39ms
step:623/2110 train_time:22052ms step_avg:35.40ms
step:624/2110 train_time:22089ms step_avg:35.40ms
step:625/2110 train_time:22125ms step_avg:35.40ms
step:626/2110 train_time:22161ms step_avg:35.40ms
step:627/2110 train_time:22196ms step_avg:35.40ms
step:628/2110 train_time:22231ms step_avg:35.40ms
step:629/2110 train_time:22262ms step_avg:35.39ms
step:630/2110 train_time:22296ms step_avg:35.39ms
step:631/2110 train_time:22327ms step_avg:35.38ms
step:632/2110 train_time:22365ms step_avg:35.39ms
step:633/2110 train_time:22402ms step_avg:35.39ms
step:634/2110 train_time:22439ms step_avg:35.39ms
step:635/2110 train_time:22473ms step_avg:35.39ms
step:636/2110 train_time:22506ms step_avg:35.39ms
step:637/2110 train_time:22536ms step_avg:35.38ms
step:638/2110 train_time:22568ms step_avg:35.37ms
step:639/2110 train_time:22601ms step_avg:35.37ms
step:640/2110 train_time:22641ms step_avg:35.38ms
step:641/2110 train_time:22674ms step_avg:35.37ms
step:642/2110 train_time:22711ms step_avg:35.38ms
step:643/2110 train_time:22743ms step_avg:35.37ms
step:644/2110 train_time:22777ms step_avg:35.37ms
step:645/2110 train_time:22807ms step_avg:35.36ms
step:646/2110 train_time:22844ms step_avg:35.36ms
step:647/2110 train_time:22878ms step_avg:35.36ms
step:648/2110 train_time:22914ms step_avg:35.36ms
step:649/2110 train_time:22950ms step_avg:35.36ms
step:650/2110 train_time:22987ms step_avg:35.36ms
step:651/2110 train_time:23018ms step_avg:35.36ms
step:652/2110 train_time:23052ms step_avg:35.36ms
step:653/2110 train_time:23082ms step_avg:35.35ms
step:654/2110 train_time:23118ms step_avg:35.35ms
step:655/2110 train_time:23154ms step_avg:35.35ms
step:656/2110 train_time:23193ms step_avg:35.35ms
step:657/2110 train_time:23227ms step_avg:35.35ms
step:658/2110 train_time:23263ms step_avg:35.35ms
step:659/2110 train_time:23294ms step_avg:35.35ms
step:660/2110 train_time:23329ms step_avg:35.35ms
step:661/2110 train_time:23361ms step_avg:35.34ms
step:662/2110 train_time:23400ms step_avg:35.35ms
step:663/2110 train_time:23437ms step_avg:35.35ms
step:664/2110 train_time:23478ms step_avg:35.36ms
step:665/2110 train_time:23513ms step_avg:35.36ms
step:666/2110 train_time:23550ms step_avg:35.36ms
step:667/2110 train_time:23583ms step_avg:35.36ms
step:668/2110 train_time:23618ms step_avg:35.36ms
step:669/2110 train_time:23649ms step_avg:35.35ms
step:670/2110 train_time:23684ms step_avg:35.35ms
step:671/2110 train_time:23720ms step_avg:35.35ms
step:672/2110 train_time:23760ms step_avg:35.36ms
step:673/2110 train_time:23796ms step_avg:35.36ms
step:674/2110 train_time:23830ms step_avg:35.36ms
step:675/2110 train_time:23863ms step_avg:35.35ms
step:676/2110 train_time:23899ms step_avg:35.35ms
step:677/2110 train_time:23931ms step_avg:35.35ms
step:678/2110 train_time:23969ms step_avg:35.35ms
step:679/2110 train_time:24005ms step_avg:35.35ms
step:680/2110 train_time:24040ms step_avg:35.35ms
step:681/2110 train_time:24076ms step_avg:35.35ms
step:682/2110 train_time:24110ms step_avg:35.35ms
step:683/2110 train_time:24142ms step_avg:35.35ms
step:684/2110 train_time:24174ms step_avg:35.34ms
step:685/2110 train_time:24206ms step_avg:35.34ms
step:686/2110 train_time:24242ms step_avg:35.34ms
step:687/2110 train_time:24278ms step_avg:35.34ms
step:688/2110 train_time:24318ms step_avg:35.35ms
step:689/2110 train_time:24351ms step_avg:35.34ms
step:690/2110 train_time:24382ms step_avg:35.34ms
step:691/2110 train_time:24419ms step_avg:35.34ms
step:692/2110 train_time:24454ms step_avg:35.34ms
step:693/2110 train_time:24502ms step_avg:35.36ms
step:694/2110 train_time:24563ms step_avg:35.39ms
step:695/2110 train_time:24620ms step_avg:35.43ms
step:696/2110 train_time:24680ms step_avg:35.46ms
step:697/2110 train_time:24739ms step_avg:35.49ms
step:698/2110 train_time:24801ms step_avg:35.53ms
step:699/2110 train_time:24859ms step_avg:35.56ms
step:700/2110 train_time:24920ms step_avg:35.60ms
step:701/2110 train_time:24979ms step_avg:35.63ms
step:702/2110 train_time:25041ms step_avg:35.67ms
step:703/2110 train_time:25100ms step_avg:35.70ms
step:704/2110 train_time:25159ms step_avg:35.74ms
step:705/2110 train_time:25220ms step_avg:35.77ms
step:706/2110 train_time:25281ms step_avg:35.81ms
step:707/2110 train_time:25341ms step_avg:35.84ms
step:708/2110 train_time:25399ms step_avg:35.87ms
step:709/2110 train_time:25457ms step_avg:35.91ms
step:710/2110 train_time:25516ms step_avg:35.94ms
step:711/2110 train_time:25576ms step_avg:35.97ms
step:712/2110 train_time:25634ms step_avg:36.00ms
step:713/2110 train_time:25693ms step_avg:36.03ms
step:714/2110 train_time:25753ms step_avg:36.07ms
step:715/2110 train_time:25812ms step_avg:36.10ms
step:716/2110 train_time:25872ms step_avg:36.13ms
step:717/2110 train_time:25930ms step_avg:36.16ms
step:718/2110 train_time:25988ms step_avg:36.20ms
step:719/2110 train_time:26048ms step_avg:36.23ms
step:720/2110 train_time:26108ms step_avg:36.26ms
step:721/2110 train_time:26167ms step_avg:36.29ms
step:722/2110 train_time:26226ms step_avg:36.32ms
step:723/2110 train_time:26286ms step_avg:36.36ms
step:724/2110 train_time:26344ms step_avg:36.39ms
step:725/2110 train_time:26405ms step_avg:36.42ms
step:726/2110 train_time:26463ms step_avg:36.45ms
step:727/2110 train_time:26523ms step_avg:36.48ms
step:728/2110 train_time:26582ms step_avg:36.51ms
step:729/2110 train_time:26642ms step_avg:36.55ms
step:730/2110 train_time:26701ms step_avg:36.58ms
step:731/2110 train_time:26761ms step_avg:36.61ms
step:732/2110 train_time:26821ms step_avg:36.64ms
step:733/2110 train_time:26882ms step_avg:36.67ms
step:734/2110 train_time:26941ms step_avg:36.70ms
step:735/2110 train_time:27001ms step_avg:36.74ms
step:736/2110 train_time:27061ms step_avg:36.77ms
step:737/2110 train_time:27121ms step_avg:36.80ms
step:738/2110 train_time:27182ms step_avg:36.83ms
step:739/2110 train_time:27240ms step_avg:36.86ms
step:740/2110 train_time:27298ms step_avg:36.89ms
step:741/2110 train_time:27358ms step_avg:36.92ms
step:742/2110 train_time:27419ms step_avg:36.95ms
step:743/2110 train_time:27477ms step_avg:36.98ms
step:744/2110 train_time:27535ms step_avg:37.01ms
step:745/2110 train_time:27595ms step_avg:37.04ms
step:746/2110 train_time:27654ms step_avg:37.07ms
step:747/2110 train_time:27713ms step_avg:37.10ms
step:748/2110 train_time:27771ms step_avg:37.13ms
step:749/2110 train_time:27830ms step_avg:37.16ms
step:750/2110 train_time:27889ms step_avg:37.19ms
step:750/2110 val_loss:3.9072 train_time:27950ms step_avg:37.27ms
step:751/2110 train_time:27990ms step_avg:37.27ms
step:752/2110 train_time:28025ms step_avg:37.27ms
step:753/2110 train_time:28074ms step_avg:37.28ms
step:754/2110 train_time:28139ms step_avg:37.32ms
step:755/2110 train_time:28200ms step_avg:37.35ms
step:756/2110 train_time:28259ms step_avg:37.38ms
step:757/2110 train_time:28318ms step_avg:37.41ms
step:758/2110 train_time:28376ms step_avg:37.44ms
step:759/2110 train_time:28435ms step_avg:37.46ms
step:760/2110 train_time:28495ms step_avg:37.49ms
step:761/2110 train_time:28553ms step_avg:37.52ms
step:762/2110 train_time:28611ms step_avg:37.55ms
step:763/2110 train_time:28669ms step_avg:37.57ms
step:764/2110 train_time:28727ms step_avg:37.60ms
step:765/2110 train_time:28787ms step_avg:37.63ms
step:766/2110 train_time:28844ms step_avg:37.66ms
step:767/2110 train_time:28903ms step_avg:37.68ms
step:768/2110 train_time:28961ms step_avg:37.71ms
step:769/2110 train_time:29022ms step_avg:37.74ms
step:770/2110 train_time:29084ms step_avg:37.77ms
step:771/2110 train_time:29145ms step_avg:37.80ms
step:772/2110 train_time:29206ms step_avg:37.83ms
step:773/2110 train_time:29268ms step_avg:37.86ms
step:774/2110 train_time:29327ms step_avg:37.89ms
step:775/2110 train_time:29386ms step_avg:37.92ms
step:776/2110 train_time:29445ms step_avg:37.94ms
step:777/2110 train_time:29504ms step_avg:37.97ms
step:778/2110 train_time:29564ms step_avg:38.00ms
step:779/2110 train_time:29623ms step_avg:38.03ms
step:780/2110 train_time:29681ms step_avg:38.05ms
step:781/2110 train_time:29739ms step_avg:38.08ms
step:782/2110 train_time:29797ms step_avg:38.10ms
step:783/2110 train_time:29855ms step_avg:38.13ms
step:784/2110 train_time:29915ms step_avg:38.16ms
step:785/2110 train_time:29975ms step_avg:38.18ms
step:786/2110 train_time:30034ms step_avg:38.21ms
step:787/2110 train_time:30095ms step_avg:38.24ms
step:788/2110 train_time:30156ms step_avg:38.27ms
step:789/2110 train_time:30214ms step_avg:38.29ms
step:790/2110 train_time:30274ms step_avg:38.32ms
step:791/2110 train_time:30332ms step_avg:38.35ms
step:792/2110 train_time:30391ms step_avg:38.37ms
step:793/2110 train_time:30452ms step_avg:38.40ms
step:794/2110 train_time:30511ms step_avg:38.43ms
step:795/2110 train_time:30570ms step_avg:38.45ms
step:796/2110 train_time:30629ms step_avg:38.48ms
step:797/2110 train_time:30689ms step_avg:38.51ms
step:798/2110 train_time:30748ms step_avg:38.53ms
step:799/2110 train_time:30806ms step_avg:38.56ms
step:800/2110 train_time:30865ms step_avg:38.58ms
step:801/2110 train_time:30926ms step_avg:38.61ms
step:802/2110 train_time:30985ms step_avg:38.63ms
step:803/2110 train_time:31045ms step_avg:38.66ms
step:804/2110 train_time:31105ms step_avg:38.69ms
step:805/2110 train_time:31166ms step_avg:38.71ms
step:806/2110 train_time:31225ms step_avg:38.74ms
step:807/2110 train_time:31284ms step_avg:38.77ms
step:808/2110 train_time:31343ms step_avg:38.79ms
step:809/2110 train_time:31403ms step_avg:38.82ms
step:810/2110 train_time:31462ms step_avg:38.84ms
step:811/2110 train_time:31520ms step_avg:38.87ms
step:812/2110 train_time:31579ms step_avg:38.89ms
step:813/2110 train_time:31639ms step_avg:38.92ms
step:814/2110 train_time:31697ms step_avg:38.94ms
step:815/2110 train_time:31756ms step_avg:38.96ms
step:816/2110 train_time:31814ms step_avg:38.99ms
step:817/2110 train_time:31874ms step_avg:39.01ms
step:818/2110 train_time:31934ms step_avg:39.04ms
step:819/2110 train_time:31992ms step_avg:39.06ms
step:820/2110 train_time:32051ms step_avg:39.09ms
step:821/2110 train_time:32110ms step_avg:39.11ms
step:822/2110 train_time:32172ms step_avg:39.14ms
step:823/2110 train_time:32230ms step_avg:39.16ms
step:824/2110 train_time:32290ms step_avg:39.19ms
step:825/2110 train_time:32349ms step_avg:39.21ms
step:826/2110 train_time:32410ms step_avg:39.24ms
step:827/2110 train_time:32469ms step_avg:39.26ms
step:828/2110 train_time:32530ms step_avg:39.29ms
step:829/2110 train_time:32589ms step_avg:39.31ms
step:830/2110 train_time:32648ms step_avg:39.33ms
step:831/2110 train_time:32707ms step_avg:39.36ms
step:832/2110 train_time:32768ms step_avg:39.38ms
step:833/2110 train_time:32826ms step_avg:39.41ms
step:834/2110 train_time:32885ms step_avg:39.43ms
step:835/2110 train_time:32944ms step_avg:39.45ms
step:836/2110 train_time:33004ms step_avg:39.48ms
step:837/2110 train_time:33063ms step_avg:39.50ms
step:838/2110 train_time:33121ms step_avg:39.52ms
step:839/2110 train_time:33180ms step_avg:39.55ms
step:840/2110 train_time:33239ms step_avg:39.57ms
step:841/2110 train_time:33299ms step_avg:39.59ms
step:842/2110 train_time:33358ms step_avg:39.62ms
step:843/2110 train_time:33418ms step_avg:39.64ms
step:844/2110 train_time:33477ms step_avg:39.66ms
step:845/2110 train_time:33536ms step_avg:39.69ms
step:846/2110 train_time:33595ms step_avg:39.71ms
step:847/2110 train_time:33654ms step_avg:39.73ms
step:848/2110 train_time:33712ms step_avg:39.76ms
step:849/2110 train_time:33772ms step_avg:39.78ms
step:850/2110 train_time:33833ms step_avg:39.80ms
step:851/2110 train_time:33891ms step_avg:39.82ms
step:852/2110 train_time:33950ms step_avg:39.85ms
step:853/2110 train_time:34010ms step_avg:39.87ms
step:854/2110 train_time:34070ms step_avg:39.89ms
step:855/2110 train_time:34130ms step_avg:39.92ms
step:856/2110 train_time:34189ms step_avg:39.94ms
step:857/2110 train_time:34248ms step_avg:39.96ms
step:858/2110 train_time:34308ms step_avg:39.99ms
step:859/2110 train_time:34369ms step_avg:40.01ms
step:860/2110 train_time:34428ms step_avg:40.03ms
step:861/2110 train_time:34487ms step_avg:40.06ms
step:862/2110 train_time:34546ms step_avg:40.08ms
step:863/2110 train_time:34607ms step_avg:40.10ms
step:864/2110 train_time:34668ms step_avg:40.12ms
step:865/2110 train_time:34725ms step_avg:40.14ms
step:866/2110 train_time:34784ms step_avg:40.17ms
step:867/2110 train_time:34843ms step_avg:40.19ms
step:868/2110 train_time:34904ms step_avg:40.21ms
step:869/2110 train_time:34962ms step_avg:40.23ms
step:870/2110 train_time:35021ms step_avg:40.25ms
step:871/2110 train_time:35080ms step_avg:40.28ms
step:872/2110 train_time:35140ms step_avg:40.30ms
step:873/2110 train_time:35199ms step_avg:40.32ms
step:874/2110 train_time:35258ms step_avg:40.34ms
step:875/2110 train_time:35316ms step_avg:40.36ms
step:876/2110 train_time:35376ms step_avg:40.38ms
step:877/2110 train_time:35435ms step_avg:40.40ms
step:878/2110 train_time:35493ms step_avg:40.42ms
step:879/2110 train_time:35553ms step_avg:40.45ms
step:880/2110 train_time:35612ms step_avg:40.47ms
step:881/2110 train_time:35672ms step_avg:40.49ms
step:882/2110 train_time:35730ms step_avg:40.51ms
step:883/2110 train_time:35789ms step_avg:40.53ms
step:884/2110 train_time:35848ms step_avg:40.55ms
step:885/2110 train_time:35908ms step_avg:40.57ms
step:886/2110 train_time:35968ms step_avg:40.60ms
step:887/2110 train_time:36027ms step_avg:40.62ms
step:888/2110 train_time:36088ms step_avg:40.64ms
step:889/2110 train_time:36146ms step_avg:40.66ms
step:890/2110 train_time:36205ms step_avg:40.68ms
step:891/2110 train_time:36266ms step_avg:40.70ms
step:892/2110 train_time:36325ms step_avg:40.72ms
step:893/2110 train_time:36386ms step_avg:40.75ms
step:894/2110 train_time:36445ms step_avg:40.77ms
step:895/2110 train_time:36504ms step_avg:40.79ms
step:896/2110 train_time:36564ms step_avg:40.81ms
step:897/2110 train_time:36623ms step_avg:40.83ms
step:898/2110 train_time:36682ms step_avg:40.85ms
step:899/2110 train_time:36740ms step_avg:40.87ms
step:900/2110 train_time:36799ms step_avg:40.89ms
step:901/2110 train_time:36859ms step_avg:40.91ms
step:902/2110 train_time:36917ms step_avg:40.93ms
step:903/2110 train_time:36976ms step_avg:40.95ms
step:904/2110 train_time:37035ms step_avg:40.97ms
step:905/2110 train_time:37094ms step_avg:40.99ms
step:906/2110 train_time:37154ms step_avg:41.01ms
step:907/2110 train_time:37212ms step_avg:41.03ms
step:908/2110 train_time:37272ms step_avg:41.05ms
step:909/2110 train_time:37331ms step_avg:41.07ms
step:910/2110 train_time:37392ms step_avg:41.09ms
step:911/2110 train_time:37451ms step_avg:41.11ms
step:912/2110 train_time:37511ms step_avg:41.13ms
step:913/2110 train_time:37571ms step_avg:41.15ms
step:914/2110 train_time:37630ms step_avg:41.17ms
step:915/2110 train_time:37689ms step_avg:41.19ms
step:916/2110 train_time:37749ms step_avg:41.21ms
step:917/2110 train_time:37809ms step_avg:41.23ms
step:918/2110 train_time:37869ms step_avg:41.25ms
step:919/2110 train_time:37927ms step_avg:41.27ms
step:920/2110 train_time:37988ms step_avg:41.29ms
step:921/2110 train_time:38045ms step_avg:41.31ms
step:922/2110 train_time:38105ms step_avg:41.33ms
step:923/2110 train_time:38164ms step_avg:41.35ms
step:924/2110 train_time:38224ms step_avg:41.37ms
step:925/2110 train_time:38283ms step_avg:41.39ms
step:926/2110 train_time:38341ms step_avg:41.41ms
step:927/2110 train_time:38401ms step_avg:41.42ms
step:928/2110 train_time:38459ms step_avg:41.44ms
step:929/2110 train_time:38520ms step_avg:41.46ms
step:930/2110 train_time:38578ms step_avg:41.48ms
step:931/2110 train_time:38637ms step_avg:41.50ms
step:932/2110 train_time:38695ms step_avg:41.52ms
step:933/2110 train_time:38755ms step_avg:41.54ms
step:934/2110 train_time:38815ms step_avg:41.56ms
step:935/2110 train_time:38874ms step_avg:41.58ms
step:936/2110 train_time:38933ms step_avg:41.59ms
step:937/2110 train_time:38992ms step_avg:41.61ms
step:938/2110 train_time:39053ms step_avg:41.63ms
step:939/2110 train_time:39111ms step_avg:41.65ms
step:940/2110 train_time:39170ms step_avg:41.67ms
step:941/2110 train_time:39229ms step_avg:41.69ms
step:942/2110 train_time:39290ms step_avg:41.71ms
step:943/2110 train_time:39349ms step_avg:41.73ms
step:944/2110 train_time:39408ms step_avg:41.75ms
step:945/2110 train_time:39469ms step_avg:41.77ms
step:946/2110 train_time:39528ms step_avg:41.78ms
step:947/2110 train_time:39588ms step_avg:41.80ms
step:948/2110 train_time:39646ms step_avg:41.82ms
step:949/2110 train_time:39706ms step_avg:41.84ms
step:950/2110 train_time:39766ms step_avg:41.86ms
step:951/2110 train_time:39825ms step_avg:41.88ms
step:952/2110 train_time:39886ms step_avg:41.90ms
step:953/2110 train_time:39943ms step_avg:41.91ms
step:954/2110 train_time:40003ms step_avg:41.93ms
step:955/2110 train_time:40061ms step_avg:41.95ms
step:956/2110 train_time:40120ms step_avg:41.97ms
step:957/2110 train_time:40181ms step_avg:41.99ms
step:958/2110 train_time:40238ms step_avg:42.00ms
step:959/2110 train_time:40299ms step_avg:42.02ms
step:960/2110 train_time:40358ms step_avg:42.04ms
step:961/2110 train_time:40419ms step_avg:42.06ms
step:962/2110 train_time:40477ms step_avg:42.08ms
step:963/2110 train_time:40537ms step_avg:42.09ms
step:964/2110 train_time:40595ms step_avg:42.11ms
step:965/2110 train_time:40655ms step_avg:42.13ms
step:966/2110 train_time:40715ms step_avg:42.15ms
step:967/2110 train_time:40775ms step_avg:42.17ms
step:968/2110 train_time:40834ms step_avg:42.18ms
step:969/2110 train_time:40894ms step_avg:42.20ms
step:970/2110 train_time:40953ms step_avg:42.22ms
step:971/2110 train_time:41012ms step_avg:42.24ms
step:972/2110 train_time:41071ms step_avg:42.25ms
step:973/2110 train_time:41131ms step_avg:42.27ms
step:974/2110 train_time:41192ms step_avg:42.29ms
step:975/2110 train_time:41251ms step_avg:42.31ms
step:976/2110 train_time:41310ms step_avg:42.33ms
step:977/2110 train_time:41370ms step_avg:42.34ms
step:978/2110 train_time:41430ms step_avg:42.36ms
step:979/2110 train_time:41490ms step_avg:42.38ms
step:980/2110 train_time:41549ms step_avg:42.40ms
step:981/2110 train_time:41609ms step_avg:42.41ms
step:982/2110 train_time:41669ms step_avg:42.43ms
step:983/2110 train_time:41729ms step_avg:42.45ms
step:984/2110 train_time:41787ms step_avg:42.47ms
step:985/2110 train_time:41845ms step_avg:42.48ms
step:986/2110 train_time:41904ms step_avg:42.50ms
step:987/2110 train_time:41964ms step_avg:42.52ms
step:988/2110 train_time:42024ms step_avg:42.53ms
step:989/2110 train_time:42082ms step_avg:42.55ms
step:990/2110 train_time:42141ms step_avg:42.57ms
step:991/2110 train_time:42200ms step_avg:42.58ms
step:992/2110 train_time:42260ms step_avg:42.60ms
step:993/2110 train_time:42319ms step_avg:42.62ms
step:994/2110 train_time:42377ms step_avg:42.63ms
step:995/2110 train_time:42436ms step_avg:42.65ms
step:996/2110 train_time:42495ms step_avg:42.67ms
step:997/2110 train_time:42556ms step_avg:42.68ms
step:998/2110 train_time:42616ms step_avg:42.70ms
step:999/2110 train_time:42675ms step_avg:42.72ms
step:1000/2110 train_time:42734ms step_avg:42.73ms
step:1000/2110 val_loss:3.7621 train_time:42795ms step_avg:42.79ms
step:1001/2110 train_time:42830ms step_avg:42.79ms
step:1002/2110 train_time:42871ms step_avg:42.79ms
step:1003/2110 train_time:42919ms step_avg:42.79ms
step:1004/2110 train_time:42981ms step_avg:42.81ms
step:1005/2110 train_time:43041ms step_avg:42.83ms
step:1006/2110 train_time:43102ms step_avg:42.84ms
step:1007/2110 train_time:43160ms step_avg:42.86ms
step:1008/2110 train_time:43218ms step_avg:42.88ms
step:1009/2110 train_time:43277ms step_avg:42.89ms
step:1010/2110 train_time:43335ms step_avg:42.91ms
step:1011/2110 train_time:43394ms step_avg:42.92ms
step:1012/2110 train_time:43452ms step_avg:42.94ms
step:1013/2110 train_time:43511ms step_avg:42.95ms
step:1014/2110 train_time:43569ms step_avg:42.97ms
step:1015/2110 train_time:43629ms step_avg:42.98ms
step:1016/2110 train_time:43687ms step_avg:43.00ms
step:1017/2110 train_time:43745ms step_avg:43.01ms
step:1018/2110 train_time:43804ms step_avg:43.03ms
step:1019/2110 train_time:43866ms step_avg:43.05ms
step:1020/2110 train_time:43928ms step_avg:43.07ms
step:1021/2110 train_time:43987ms step_avg:43.08ms
step:1022/2110 train_time:44046ms step_avg:43.10ms
step:1023/2110 train_time:44106ms step_avg:43.11ms
step:1024/2110 train_time:44164ms step_avg:43.13ms
step:1025/2110 train_time:44224ms step_avg:43.15ms
step:1026/2110 train_time:44282ms step_avg:43.16ms
step:1027/2110 train_time:44342ms step_avg:43.18ms
step:1028/2110 train_time:44400ms step_avg:43.19ms
step:1029/2110 train_time:44459ms step_avg:43.21ms
step:1030/2110 train_time:44518ms step_avg:43.22ms
step:1031/2110 train_time:44576ms step_avg:43.24ms
step:1032/2110 train_time:44635ms step_avg:43.25ms
step:1033/2110 train_time:44694ms step_avg:43.27ms
step:1034/2110 train_time:44755ms step_avg:43.28ms
step:1035/2110 train_time:44813ms step_avg:43.30ms
step:1036/2110 train_time:44874ms step_avg:43.31ms
step:1037/2110 train_time:44934ms step_avg:43.33ms
step:1038/2110 train_time:44996ms step_avg:43.35ms
step:1039/2110 train_time:45056ms step_avg:43.37ms
step:1040/2110 train_time:45116ms step_avg:43.38ms
step:1041/2110 train_time:45175ms step_avg:43.40ms
step:1042/2110 train_time:45234ms step_avg:43.41ms
step:1043/2110 train_time:45294ms step_avg:43.43ms
step:1044/2110 train_time:45352ms step_avg:43.44ms
step:1045/2110 train_time:45411ms step_avg:43.46ms
step:1046/2110 train_time:45469ms step_avg:43.47ms
step:1047/2110 train_time:45529ms step_avg:43.48ms
step:1048/2110 train_time:45587ms step_avg:43.50ms
step:1049/2110 train_time:45645ms step_avg:43.51ms
step:1050/2110 train_time:45703ms step_avg:43.53ms
step:1051/2110 train_time:45763ms step_avg:43.54ms
step:1052/2110 train_time:45823ms step_avg:43.56ms
step:1053/2110 train_time:45882ms step_avg:43.57ms
step:1054/2110 train_time:45942ms step_avg:43.59ms
step:1055/2110 train_time:46004ms step_avg:43.61ms
step:1056/2110 train_time:46063ms step_avg:43.62ms
step:1057/2110 train_time:46124ms step_avg:43.64ms
step:1058/2110 train_time:46181ms step_avg:43.65ms
step:1059/2110 train_time:46241ms step_avg:43.66ms
step:1060/2110 train_time:46299ms step_avg:43.68ms
step:1061/2110 train_time:46359ms step_avg:43.69ms
step:1062/2110 train_time:46419ms step_avg:43.71ms
step:1063/2110 train_time:46477ms step_avg:43.72ms
step:1064/2110 train_time:46536ms step_avg:43.74ms
step:1065/2110 train_time:46594ms step_avg:43.75ms
step:1066/2110 train_time:46653ms step_avg:43.76ms
step:1067/2110 train_time:46713ms step_avg:43.78ms
step:1068/2110 train_time:46772ms step_avg:43.79ms
step:1069/2110 train_time:46832ms step_avg:43.81ms
step:1070/2110 train_time:46892ms step_avg:43.82ms
step:1071/2110 train_time:46952ms step_avg:43.84ms
step:1072/2110 train_time:47012ms step_avg:43.85ms
step:1073/2110 train_time:47072ms step_avg:43.87ms
step:1074/2110 train_time:47132ms step_avg:43.88ms
step:1075/2110 train_time:47191ms step_avg:43.90ms
step:1076/2110 train_time:47251ms step_avg:43.91ms
step:1077/2110 train_time:47309ms step_avg:43.93ms
step:1078/2110 train_time:47367ms step_avg:43.94ms
step:1079/2110 train_time:47426ms step_avg:43.95ms
step:1080/2110 train_time:47485ms step_avg:43.97ms
step:1081/2110 train_time:47544ms step_avg:43.98ms
step:1082/2110 train_time:47602ms step_avg:43.99ms
step:1083/2110 train_time:47662ms step_avg:44.01ms
step:1084/2110 train_time:47720ms step_avg:44.02ms
step:1085/2110 train_time:47781ms step_avg:44.04ms
step:1086/2110 train_time:47838ms step_avg:44.05ms
step:1087/2110 train_time:47899ms step_avg:44.07ms
step:1088/2110 train_time:47958ms step_avg:44.08ms
step:1089/2110 train_time:48019ms step_avg:44.09ms
step:1090/2110 train_time:48078ms step_avg:44.11ms
step:1091/2110 train_time:48137ms step_avg:44.12ms
step:1092/2110 train_time:48196ms step_avg:44.14ms
step:1093/2110 train_time:48256ms step_avg:44.15ms
step:1094/2110 train_time:48317ms step_avg:44.17ms
step:1095/2110 train_time:48376ms step_avg:44.18ms
step:1096/2110 train_time:48434ms step_avg:44.19ms
step:1097/2110 train_time:48494ms step_avg:44.21ms
step:1098/2110 train_time:48555ms step_avg:44.22ms
step:1099/2110 train_time:48613ms step_avg:44.23ms
step:1100/2110 train_time:48672ms step_avg:44.25ms
step:1101/2110 train_time:48730ms step_avg:44.26ms
step:1102/2110 train_time:48789ms step_avg:44.27ms
step:1103/2110 train_time:48849ms step_avg:44.29ms
step:1104/2110 train_time:48907ms step_avg:44.30ms
step:1105/2110 train_time:48967ms step_avg:44.31ms
step:1106/2110 train_time:49026ms step_avg:44.33ms
step:1107/2110 train_time:49086ms step_avg:44.34ms
step:1108/2110 train_time:49147ms step_avg:44.36ms
step:1109/2110 train_time:49207ms step_avg:44.37ms
step:1110/2110 train_time:49265ms step_avg:44.38ms
step:1111/2110 train_time:49325ms step_avg:44.40ms
step:1112/2110 train_time:49384ms step_avg:44.41ms
step:1113/2110 train_time:49444ms step_avg:44.42ms
step:1114/2110 train_time:49501ms step_avg:44.44ms
step:1115/2110 train_time:49561ms step_avg:44.45ms
step:1116/2110 train_time:49620ms step_avg:44.46ms
step:1117/2110 train_time:49679ms step_avg:44.48ms
step:1118/2110 train_time:49739ms step_avg:44.49ms
step:1119/2110 train_time:49798ms step_avg:44.50ms
step:1120/2110 train_time:49856ms step_avg:44.51ms
step:1121/2110 train_time:49916ms step_avg:44.53ms
step:1122/2110 train_time:49976ms step_avg:44.54ms
step:1123/2110 train_time:50036ms step_avg:44.56ms
step:1124/2110 train_time:50095ms step_avg:44.57ms
step:1125/2110 train_time:50155ms step_avg:44.58ms
step:1126/2110 train_time:50215ms step_avg:44.60ms
step:1127/2110 train_time:50275ms step_avg:44.61ms
step:1128/2110 train_time:50333ms step_avg:44.62ms
step:1129/2110 train_time:50392ms step_avg:44.63ms
step:1130/2110 train_time:50451ms step_avg:44.65ms
step:1131/2110 train_time:50511ms step_avg:44.66ms
step:1132/2110 train_time:50570ms step_avg:44.67ms
step:1133/2110 train_time:50630ms step_avg:44.69ms
step:1134/2110 train_time:50687ms step_avg:44.70ms
step:1135/2110 train_time:50747ms step_avg:44.71ms
step:1136/2110 train_time:50806ms step_avg:44.72ms
step:1137/2110 train_time:50866ms step_avg:44.74ms
step:1138/2110 train_time:50926ms step_avg:44.75ms
step:1139/2110 train_time:50985ms step_avg:44.76ms
step:1140/2110 train_time:51044ms step_avg:44.78ms
step:1141/2110 train_time:51105ms step_avg:44.79ms
step:1142/2110 train_time:51165ms step_avg:44.80ms
step:1143/2110 train_time:51225ms step_avg:44.82ms
step:1144/2110 train_time:51284ms step_avg:44.83ms
step:1145/2110 train_time:51345ms step_avg:44.84ms
step:1146/2110 train_time:51404ms step_avg:44.86ms
step:1147/2110 train_time:51465ms step_avg:44.87ms
step:1148/2110 train_time:51523ms step_avg:44.88ms
step:1149/2110 train_time:51583ms step_avg:44.89ms
step:1150/2110 train_time:51641ms step_avg:44.91ms
step:1151/2110 train_time:51702ms step_avg:44.92ms
step:1152/2110 train_time:51761ms step_avg:44.93ms
step:1153/2110 train_time:51821ms step_avg:44.94ms
step:1154/2110 train_time:51880ms step_avg:44.96ms
step:1155/2110 train_time:51940ms step_avg:44.97ms
step:1156/2110 train_time:52001ms step_avg:44.98ms
step:1157/2110 train_time:52061ms step_avg:45.00ms
step:1158/2110 train_time:52120ms step_avg:45.01ms
step:1159/2110 train_time:52180ms step_avg:45.02ms
step:1160/2110 train_time:52241ms step_avg:45.04ms
step:1161/2110 train_time:52301ms step_avg:45.05ms
step:1162/2110 train_time:52359ms step_avg:45.06ms
step:1163/2110 train_time:52420ms step_avg:45.07ms
step:1164/2110 train_time:52479ms step_avg:45.09ms
step:1165/2110 train_time:52540ms step_avg:45.10ms
step:1166/2110 train_time:52599ms step_avg:45.11ms
step:1167/2110 train_time:52659ms step_avg:45.12ms
step:1168/2110 train_time:52719ms step_avg:45.14ms
step:1169/2110 train_time:52779ms step_avg:45.15ms
step:1170/2110 train_time:52838ms step_avg:45.16ms
step:1171/2110 train_time:52897ms step_avg:45.17ms
step:1172/2110 train_time:52957ms step_avg:45.19ms
step:1173/2110 train_time:53018ms step_avg:45.20ms
step:1174/2110 train_time:53079ms step_avg:45.21ms
step:1175/2110 train_time:53137ms step_avg:45.22ms
step:1176/2110 train_time:53197ms step_avg:45.24ms
step:1177/2110 train_time:53260ms step_avg:45.25ms
step:1178/2110 train_time:53321ms step_avg:45.26ms
step:1179/2110 train_time:53381ms step_avg:45.28ms
step:1180/2110 train_time:53441ms step_avg:45.29ms
step:1181/2110 train_time:53500ms step_avg:45.30ms
step:1182/2110 train_time:53561ms step_avg:45.31ms
step:1183/2110 train_time:53620ms step_avg:45.33ms
step:1184/2110 train_time:53681ms step_avg:45.34ms
step:1185/2110 train_time:53739ms step_avg:45.35ms
step:1186/2110 train_time:53799ms step_avg:45.36ms
step:1187/2110 train_time:53858ms step_avg:45.37ms
step:1188/2110 train_time:53920ms step_avg:45.39ms
step:1189/2110 train_time:53977ms step_avg:45.40ms
step:1190/2110 train_time:54039ms step_avg:45.41ms
step:1191/2110 train_time:54098ms step_avg:45.42ms
step:1192/2110 train_time:54159ms step_avg:45.44ms
step:1193/2110 train_time:54219ms step_avg:45.45ms
step:1194/2110 train_time:54280ms step_avg:45.46ms
step:1195/2110 train_time:54339ms step_avg:45.47ms
step:1196/2110 train_time:54400ms step_avg:45.49ms
step:1197/2110 train_time:54459ms step_avg:45.50ms
step:1198/2110 train_time:54520ms step_avg:45.51ms
step:1199/2110 train_time:54579ms step_avg:45.52ms
step:1200/2110 train_time:54640ms step_avg:45.53ms
step:1201/2110 train_time:54699ms step_avg:45.54ms
step:1202/2110 train_time:54760ms step_avg:45.56ms
step:1203/2110 train_time:54819ms step_avg:45.57ms
step:1204/2110 train_time:54880ms step_avg:45.58ms
step:1205/2110 train_time:54938ms step_avg:45.59ms
step:1206/2110 train_time:54999ms step_avg:45.60ms
step:1207/2110 train_time:55057ms step_avg:45.62ms
step:1208/2110 train_time:55119ms step_avg:45.63ms
step:1209/2110 train_time:55178ms step_avg:45.64ms
step:1210/2110 train_time:55240ms step_avg:45.65ms
step:1211/2110 train_time:55298ms step_avg:45.66ms
step:1212/2110 train_time:55360ms step_avg:45.68ms
step:1213/2110 train_time:55419ms step_avg:45.69ms
step:1214/2110 train_time:55480ms step_avg:45.70ms
step:1215/2110 train_time:55538ms step_avg:45.71ms
step:1216/2110 train_time:55599ms step_avg:45.72ms
step:1217/2110 train_time:55658ms step_avg:45.73ms
step:1218/2110 train_time:55720ms step_avg:45.75ms
step:1219/2110 train_time:55778ms step_avg:45.76ms
step:1220/2110 train_time:55839ms step_avg:45.77ms
step:1221/2110 train_time:55897ms step_avg:45.78ms
step:1222/2110 train_time:55959ms step_avg:45.79ms
step:1223/2110 train_time:56017ms step_avg:45.80ms
step:1224/2110 train_time:56079ms step_avg:45.82ms
step:1225/2110 train_time:56138ms step_avg:45.83ms
step:1226/2110 train_time:56198ms step_avg:45.84ms
step:1227/2110 train_time:56257ms step_avg:45.85ms
step:1228/2110 train_time:56319ms step_avg:45.86ms
step:1229/2110 train_time:56378ms step_avg:45.87ms
step:1230/2110 train_time:56439ms step_avg:45.89ms
step:1231/2110 train_time:56498ms step_avg:45.90ms
step:1232/2110 train_time:56559ms step_avg:45.91ms
step:1233/2110 train_time:56619ms step_avg:45.92ms
step:1234/2110 train_time:56679ms step_avg:45.93ms
step:1235/2110 train_time:56738ms step_avg:45.94ms
step:1236/2110 train_time:56799ms step_avg:45.95ms
step:1237/2110 train_time:56858ms step_avg:45.96ms
step:1238/2110 train_time:56919ms step_avg:45.98ms
step:1239/2110 train_time:56977ms step_avg:45.99ms
step:1240/2110 train_time:57039ms step_avg:46.00ms
step:1241/2110 train_time:57097ms step_avg:46.01ms
step:1242/2110 train_time:57158ms step_avg:46.02ms
step:1243/2110 train_time:57217ms step_avg:46.03ms
step:1244/2110 train_time:57278ms step_avg:46.04ms
step:1245/2110 train_time:57337ms step_avg:46.05ms
step:1246/2110 train_time:57399ms step_avg:46.07ms
step:1247/2110 train_time:57458ms step_avg:46.08ms
step:1248/2110 train_time:57520ms step_avg:46.09ms
step:1249/2110 train_time:57578ms step_avg:46.10ms
step:1250/2110 train_time:57638ms step_avg:46.11ms
step:1250/2110 val_loss:3.5938 train_time:57697ms step_avg:46.16ms
step:1251/2110 train_time:57734ms step_avg:46.15ms
step:1252/2110 train_time:57769ms step_avg:46.14ms
step:1253/2110 train_time:57821ms step_avg:46.15ms
step:1254/2110 train_time:57883ms step_avg:46.16ms
step:1255/2110 train_time:57943ms step_avg:46.17ms
step:1256/2110 train_time:58004ms step_avg:46.18ms
step:1257/2110 train_time:58062ms step_avg:46.19ms
step:1258/2110 train_time:58121ms step_avg:46.20ms
step:1259/2110 train_time:58180ms step_avg:46.21ms
step:1260/2110 train_time:58238ms step_avg:46.22ms
step:1261/2110 train_time:58297ms step_avg:46.23ms
step:1262/2110 train_time:58356ms step_avg:46.24ms
step:1263/2110 train_time:58415ms step_avg:46.25ms
step:1264/2110 train_time:58475ms step_avg:46.26ms
step:1265/2110 train_time:58532ms step_avg:46.27ms
step:1266/2110 train_time:58592ms step_avg:46.28ms
step:1267/2110 train_time:58654ms step_avg:46.29ms
step:1268/2110 train_time:58715ms step_avg:46.31ms
step:1269/2110 train_time:58778ms step_avg:46.32ms
step:1270/2110 train_time:58839ms step_avg:46.33ms
step:1271/2110 train_time:58900ms step_avg:46.34ms
step:1272/2110 train_time:58960ms step_avg:46.35ms
step:1273/2110 train_time:59021ms step_avg:46.36ms
step:1274/2110 train_time:59080ms step_avg:46.37ms
step:1275/2110 train_time:59139ms step_avg:46.38ms
step:1276/2110 train_time:59199ms step_avg:46.39ms
step:1277/2110 train_time:59257ms step_avg:46.40ms
step:1278/2110 train_time:59316ms step_avg:46.41ms
step:1279/2110 train_time:59375ms step_avg:46.42ms
step:1280/2110 train_time:59434ms step_avg:46.43ms
step:1281/2110 train_time:59493ms step_avg:46.44ms
step:1282/2110 train_time:59552ms step_avg:46.45ms
step:1283/2110 train_time:59612ms step_avg:46.46ms
step:1284/2110 train_time:59673ms step_avg:46.47ms
step:1285/2110 train_time:59735ms step_avg:46.49ms
step:1286/2110 train_time:59795ms step_avg:46.50ms
step:1287/2110 train_time:59856ms step_avg:46.51ms
step:1288/2110 train_time:59916ms step_avg:46.52ms
step:1289/2110 train_time:59977ms step_avg:46.53ms
step:1290/2110 train_time:60037ms step_avg:46.54ms
step:1291/2110 train_time:60097ms step_avg:46.55ms
step:1292/2110 train_time:60157ms step_avg:46.56ms
step:1293/2110 train_time:60215ms step_avg:46.57ms
step:1294/2110 train_time:60275ms step_avg:46.58ms
step:1295/2110 train_time:60333ms step_avg:46.59ms
step:1296/2110 train_time:60392ms step_avg:46.60ms
step:1297/2110 train_time:60452ms step_avg:46.61ms
step:1298/2110 train_time:60511ms step_avg:46.62ms
step:1299/2110 train_time:60570ms step_avg:46.63ms
step:1300/2110 train_time:60630ms step_avg:46.64ms
step:1301/2110 train_time:60691ms step_avg:46.65ms
step:1302/2110 train_time:60751ms step_avg:46.66ms
step:1303/2110 train_time:60813ms step_avg:46.67ms
step:1304/2110 train_time:60873ms step_avg:46.68ms
step:1305/2110 train_time:60935ms step_avg:46.69ms
step:1306/2110 train_time:60995ms step_avg:46.70ms
step:1307/2110 train_time:61056ms step_avg:46.71ms
step:1308/2110 train_time:61115ms step_avg:46.72ms
step:1309/2110 train_time:61175ms step_avg:46.73ms
step:1310/2110 train_time:61234ms step_avg:46.74ms
step:1311/2110 train_time:61293ms step_avg:46.75ms
step:1312/2110 train_time:61352ms step_avg:46.76ms
step:1313/2110 train_time:61411ms step_avg:46.77ms
step:1314/2110 train_time:61471ms step_avg:46.78ms
step:1315/2110 train_time:61530ms step_avg:46.79ms
step:1316/2110 train_time:61590ms step_avg:46.80ms
step:1317/2110 train_time:61651ms step_avg:46.81ms
step:1318/2110 train_time:61711ms step_avg:46.82ms
step:1319/2110 train_time:61772ms step_avg:46.83ms
step:1320/2110 train_time:61833ms step_avg:46.84ms
step:1321/2110 train_time:61895ms step_avg:46.85ms
step:1322/2110 train_time:61955ms step_avg:46.86ms
step:1323/2110 train_time:62015ms step_avg:46.87ms
step:1324/2110 train_time:62075ms step_avg:46.88ms
step:1325/2110 train_time:62135ms step_avg:46.89ms
step:1326/2110 train_time:62194ms step_avg:46.90ms
step:1327/2110 train_time:62255ms step_avg:46.91ms
step:1328/2110 train_time:62314ms step_avg:46.92ms
step:1329/2110 train_time:62374ms step_avg:46.93ms
step:1330/2110 train_time:62433ms step_avg:46.94ms
step:1331/2110 train_time:62494ms step_avg:46.95ms
step:1332/2110 train_time:62554ms step_avg:46.96ms
step:1333/2110 train_time:62614ms step_avg:46.97ms
step:1334/2110 train_time:62673ms step_avg:46.98ms
step:1335/2110 train_time:62734ms step_avg:46.99ms
step:1336/2110 train_time:62794ms step_avg:47.00ms
step:1337/2110 train_time:62855ms step_avg:47.01ms
step:1338/2110 train_time:62915ms step_avg:47.02ms
step:1339/2110 train_time:62975ms step_avg:47.03ms
step:1340/2110 train_time:63035ms step_avg:47.04ms
step:1341/2110 train_time:63096ms step_avg:47.05ms
step:1342/2110 train_time:63156ms step_avg:47.06ms
step:1343/2110 train_time:63215ms step_avg:47.07ms
step:1344/2110 train_time:63274ms step_avg:47.08ms
step:1345/2110 train_time:63334ms step_avg:47.09ms
step:1346/2110 train_time:63394ms step_avg:47.10ms
step:1347/2110 train_time:63453ms step_avg:47.11ms
step:1348/2110 train_time:63513ms step_avg:47.12ms
step:1349/2110 train_time:63573ms step_avg:47.13ms
step:1350/2110 train_time:63632ms step_avg:47.14ms
step:1351/2110 train_time:63694ms step_avg:47.15ms
step:1352/2110 train_time:63754ms step_avg:47.16ms
step:1353/2110 train_time:63814ms step_avg:47.16ms
step:1354/2110 train_time:63874ms step_avg:47.17ms
step:1355/2110 train_time:63934ms step_avg:47.18ms
step:1356/2110 train_time:63994ms step_avg:47.19ms
step:1357/2110 train_time:64055ms step_avg:47.20ms
step:1358/2110 train_time:64114ms step_avg:47.21ms
step:1359/2110 train_time:64174ms step_avg:47.22ms
step:1360/2110 train_time:64233ms step_avg:47.23ms
step:1361/2110 train_time:64293ms step_avg:47.24ms
step:1362/2110 train_time:64353ms step_avg:47.25ms
step:1363/2110 train_time:64412ms step_avg:47.26ms
step:1364/2110 train_time:64473ms step_avg:47.27ms
step:1365/2110 train_time:64532ms step_avg:47.28ms
step:1366/2110 train_time:64592ms step_avg:47.29ms
step:1367/2110 train_time:64652ms step_avg:47.29ms
step:1368/2110 train_time:64712ms step_avg:47.30ms
step:1369/2110 train_time:64772ms step_avg:47.31ms
step:1370/2110 train_time:64832ms step_avg:47.32ms
step:1371/2110 train_time:64892ms step_avg:47.33ms
step:1372/2110 train_time:64953ms step_avg:47.34ms
step:1373/2110 train_time:65013ms step_avg:47.35ms
step:1374/2110 train_time:65073ms step_avg:47.36ms
step:1375/2110 train_time:65133ms step_avg:47.37ms
step:1376/2110 train_time:65193ms step_avg:47.38ms
step:1377/2110 train_time:65254ms step_avg:47.39ms
step:1378/2110 train_time:65313ms step_avg:47.40ms
step:1379/2110 train_time:65373ms step_avg:47.41ms
step:1380/2110 train_time:65433ms step_avg:47.42ms
step:1381/2110 train_time:65493ms step_avg:47.42ms
step:1382/2110 train_time:65581ms step_avg:47.45ms
step:1383/2110 train_time:65668ms step_avg:47.48ms
step:1384/2110 train_time:65756ms step_avg:47.51ms
step:1385/2110 train_time:65843ms step_avg:47.54ms
step:1386/2110 train_time:65929ms step_avg:47.57ms
step:1387/2110 train_time:66015ms step_avg:47.60ms
step:1388/2110 train_time:66103ms step_avg:47.62ms
step:1389/2110 train_time:66190ms step_avg:47.65ms
step:1390/2110 train_time:66277ms step_avg:47.68ms
step:1391/2110 train_time:66364ms step_avg:47.71ms
step:1392/2110 train_time:66451ms step_avg:47.74ms
step:1393/2110 train_time:66537ms step_avg:47.77ms
step:1394/2110 train_time:66625ms step_avg:47.79ms
step:1395/2110 train_time:66711ms step_avg:47.82ms
step:1396/2110 train_time:66798ms step_avg:47.85ms
step:1397/2110 train_time:66886ms step_avg:47.88ms
step:1398/2110 train_time:66973ms step_avg:47.91ms
step:1399/2110 train_time:67059ms step_avg:47.93ms
step:1400/2110 train_time:67147ms step_avg:47.96ms
step:1401/2110 train_time:67233ms step_avg:47.99ms
step:1402/2110 train_time:67320ms step_avg:48.02ms
step:1403/2110 train_time:67407ms step_avg:48.04ms
step:1404/2110 train_time:67495ms step_avg:48.07ms
step:1405/2110 train_time:67581ms step_avg:48.10ms
step:1406/2110 train_time:67668ms step_avg:48.13ms
step:1407/2110 train_time:67754ms step_avg:48.16ms
step:1408/2110 train_time:67842ms step_avg:48.18ms
step:1409/2110 train_time:67929ms step_avg:48.21ms
step:1410/2110 train_time:68016ms step_avg:48.24ms
step:1411/2110 train_time:68103ms step_avg:48.27ms
step:1412/2110 train_time:68190ms step_avg:48.29ms
step:1413/2110 train_time:68276ms step_avg:48.32ms
step:1414/2110 train_time:68364ms step_avg:48.35ms
step:1415/2110 train_time:68449ms step_avg:48.37ms
step:1416/2110 train_time:68537ms step_avg:48.40ms
step:1417/2110 train_time:68624ms step_avg:48.43ms
step:1418/2110 train_time:68710ms step_avg:48.46ms
step:1419/2110 train_time:68797ms step_avg:48.48ms
step:1420/2110 train_time:68885ms step_avg:48.51ms
step:1421/2110 train_time:68972ms step_avg:48.54ms
step:1422/2110 train_time:69059ms step_avg:48.56ms
step:1423/2110 train_time:69146ms step_avg:48.59ms
step:1424/2110 train_time:69232ms step_avg:48.62ms
step:1425/2110 train_time:69319ms step_avg:48.64ms
step:1426/2110 train_time:69406ms step_avg:48.67ms
step:1427/2110 train_time:69491ms step_avg:48.70ms
step:1428/2110 train_time:69579ms step_avg:48.72ms
step:1429/2110 train_time:69667ms step_avg:48.75ms
step:1430/2110 train_time:69753ms step_avg:48.78ms
step:1431/2110 train_time:69840ms step_avg:48.81ms
step:1432/2110 train_time:69928ms step_avg:48.83ms
step:1433/2110 train_time:70013ms step_avg:48.86ms
step:1434/2110 train_time:70102ms step_avg:48.89ms
step:1435/2110 train_time:70188ms step_avg:48.91ms
step:1436/2110 train_time:70276ms step_avg:48.94ms
step:1437/2110 train_time:70363ms step_avg:48.96ms
step:1438/2110 train_time:70448ms step_avg:48.99ms
step:1439/2110 train_time:70536ms step_avg:49.02ms
step:1440/2110 train_time:70624ms step_avg:49.04ms
step:1441/2110 train_time:70711ms step_avg:49.07ms
step:1442/2110 train_time:70797ms step_avg:49.10ms
step:1443/2110 train_time:70884ms step_avg:49.12ms
step:1444/2110 train_time:70971ms step_avg:49.15ms
step:1445/2110 train_time:71058ms step_avg:49.17ms
step:1446/2110 train_time:71145ms step_avg:49.20ms
step:1447/2110 train_time:71232ms step_avg:49.23ms
step:1448/2110 train_time:71319ms step_avg:49.25ms
step:1449/2110 train_time:71405ms step_avg:49.28ms
step:1450/2110 train_time:71491ms step_avg:49.30ms
step:1451/2110 train_time:71578ms step_avg:49.33ms
step:1452/2110 train_time:71665ms step_avg:49.36ms
step:1453/2110 train_time:71751ms step_avg:49.38ms
step:1454/2110 train_time:71838ms step_avg:49.41ms
step:1455/2110 train_time:71926ms step_avg:49.43ms
step:1456/2110 train_time:72013ms step_avg:49.46ms
step:1457/2110 train_time:72101ms step_avg:49.49ms
step:1458/2110 train_time:72188ms step_avg:49.51ms
step:1459/2110 train_time:72275ms step_avg:49.54ms
step:1460/2110 train_time:72362ms step_avg:49.56ms
step:1461/2110 train_time:72448ms step_avg:49.59ms
step:1462/2110 train_time:72535ms step_avg:49.61ms
step:1463/2110 train_time:72622ms step_avg:49.64ms
step:1464/2110 train_time:72708ms step_avg:49.66ms
step:1465/2110 train_time:72794ms step_avg:49.69ms
step:1466/2110 train_time:72882ms step_avg:49.71ms
step:1467/2110 train_time:72968ms step_avg:49.74ms
step:1468/2110 train_time:73054ms step_avg:49.76ms
step:1469/2110 train_time:73141ms step_avg:49.79ms
step:1470/2110 train_time:73228ms step_avg:49.82ms
step:1471/2110 train_time:73315ms step_avg:49.84ms
step:1472/2110 train_time:73402ms step_avg:49.87ms
step:1473/2110 train_time:73487ms step_avg:49.89ms
step:1474/2110 train_time:73575ms step_avg:49.92ms
step:1475/2110 train_time:73662ms step_avg:49.94ms
step:1476/2110 train_time:73748ms step_avg:49.96ms
step:1477/2110 train_time:73835ms step_avg:49.99ms
step:1478/2110 train_time:73923ms step_avg:50.02ms
step:1479/2110 train_time:74009ms step_avg:50.04ms
step:1480/2110 train_time:74096ms step_avg:50.06ms
step:1481/2110 train_time:74184ms step_avg:50.09ms
step:1482/2110 train_time:74271ms step_avg:50.12ms
step:1483/2110 train_time:74358ms step_avg:50.14ms
step:1484/2110 train_time:74445ms step_avg:50.17ms
step:1485/2110 train_time:74531ms step_avg:50.19ms
step:1486/2110 train_time:74618ms step_avg:50.21ms
step:1487/2110 train_time:74706ms step_avg:50.24ms
step:1488/2110 train_time:74793ms step_avg:50.26ms
step:1489/2110 train_time:74880ms step_avg:50.29ms
step:1490/2110 train_time:74967ms step_avg:50.31ms
step:1491/2110 train_time:75053ms step_avg:50.34ms
step:1492/2110 train_time:75142ms step_avg:50.36ms
step:1493/2110 train_time:75228ms step_avg:50.39ms
step:1494/2110 train_time:75316ms step_avg:50.41ms
step:1495/2110 train_time:75402ms step_avg:50.44ms
step:1496/2110 train_time:75488ms step_avg:50.46ms
step:1497/2110 train_time:75574ms step_avg:50.48ms
step:1498/2110 train_time:75662ms step_avg:50.51ms
step:1499/2110 train_time:75749ms step_avg:50.53ms
step:1500/2110 train_time:75837ms step_avg:50.56ms
step:1500/2110 val_loss:3.4946 train_time:75923ms step_avg:50.62ms
step:1501/2110 train_time:75955ms step_avg:50.60ms
step:1502/2110 train_time:76016ms step_avg:50.61ms
step:1503/2110 train_time:76108ms step_avg:50.64ms
step:1504/2110 train_time:76195ms step_avg:50.66ms
step:1505/2110 train_time:76282ms step_avg:50.69ms
step:1506/2110 train_time:76369ms step_avg:50.71ms
step:1507/2110 train_time:76454ms step_avg:50.73ms
step:1508/2110 train_time:76539ms step_avg:50.76ms
step:1509/2110 train_time:76624ms step_avg:50.78ms
step:1510/2110 train_time:76709ms step_avg:50.80ms
step:1511/2110 train_time:76795ms step_avg:50.82ms
step:1512/2110 train_time:76882ms step_avg:50.85ms
step:1513/2110 train_time:76973ms step_avg:50.87ms
step:1514/2110 train_time:77062ms step_avg:50.90ms
step:1515/2110 train_time:77152ms step_avg:50.93ms
step:1516/2110 train_time:77239ms step_avg:50.95ms
step:1517/2110 train_time:77326ms step_avg:50.97ms
step:1518/2110 train_time:77411ms step_avg:51.00ms
step:1519/2110 train_time:77497ms step_avg:51.02ms
step:1520/2110 train_time:77583ms step_avg:51.04ms
step:1521/2110 train_time:77669ms step_avg:51.06ms
step:1522/2110 train_time:77754ms step_avg:51.09ms
step:1523/2110 train_time:77841ms step_avg:51.11ms
step:1524/2110 train_time:77929ms step_avg:51.13ms
step:1525/2110 train_time:78018ms step_avg:51.16ms
step:1526/2110 train_time:78105ms step_avg:51.18ms
step:1527/2110 train_time:78192ms step_avg:51.21ms
step:1528/2110 train_time:78279ms step_avg:51.23ms
step:1529/2110 train_time:78366ms step_avg:51.25ms
step:1530/2110 train_time:78451ms step_avg:51.28ms
step:1531/2110 train_time:78538ms step_avg:51.30ms
step:1532/2110 train_time:78624ms step_avg:51.32ms
step:1533/2110 train_time:78710ms step_avg:51.34ms
step:1534/2110 train_time:78796ms step_avg:51.37ms
step:1535/2110 train_time:78883ms step_avg:51.39ms
step:1536/2110 train_time:78971ms step_avg:51.41ms
step:1537/2110 train_time:79059ms step_avg:51.44ms
step:1538/2110 train_time:79147ms step_avg:51.46ms
step:1539/2110 train_time:79235ms step_avg:51.48ms
step:1540/2110 train_time:79321ms step_avg:51.51ms
step:1541/2110 train_time:79409ms step_avg:51.53ms
step:1542/2110 train_time:79496ms step_avg:51.55ms
step:1543/2110 train_time:79582ms step_avg:51.58ms
step:1544/2110 train_time:79668ms step_avg:51.60ms
step:1545/2110 train_time:79753ms step_avg:51.62ms
step:1546/2110 train_time:79840ms step_avg:51.64ms
step:1547/2110 train_time:79928ms step_avg:51.67ms
step:1548/2110 train_time:80015ms step_avg:51.69ms
step:1549/2110 train_time:80103ms step_avg:51.71ms
step:1550/2110 train_time:80190ms step_avg:51.74ms
step:1551/2110 train_time:80277ms step_avg:51.76ms
step:1552/2110 train_time:80364ms step_avg:51.78ms
step:1553/2110 train_time:80451ms step_avg:51.80ms
step:1554/2110 train_time:80537ms step_avg:51.83ms
step:1555/2110 train_time:80624ms step_avg:51.85ms
step:1556/2110 train_time:80710ms step_avg:51.87ms
step:1557/2110 train_time:80797ms step_avg:51.89ms
step:1558/2110 train_time:80884ms step_avg:51.92ms
step:1559/2110 train_time:80972ms step_avg:51.94ms
step:1560/2110 train_time:81059ms step_avg:51.96ms
step:1561/2110 train_time:81146ms step_avg:51.98ms
step:1562/2110 train_time:81232ms step_avg:52.01ms
step:1563/2110 train_time:81320ms step_avg:52.03ms
step:1564/2110 train_time:81407ms step_avg:52.05ms
step:1565/2110 train_time:81493ms step_avg:52.07ms
step:1566/2110 train_time:81580ms step_avg:52.09ms
step:1567/2110 train_time:81668ms step_avg:52.12ms
step:1568/2110 train_time:81753ms step_avg:52.14ms
step:1569/2110 train_time:81840ms step_avg:52.16ms
step:1570/2110 train_time:81928ms step_avg:52.18ms
step:1571/2110 train_time:82016ms step_avg:52.21ms
step:1572/2110 train_time:82102ms step_avg:52.23ms
step:1573/2110 train_time:82190ms step_avg:52.25ms
step:1574/2110 train_time:82276ms step_avg:52.27ms
step:1575/2110 train_time:82364ms step_avg:52.29ms
step:1576/2110 train_time:82449ms step_avg:52.32ms
step:1577/2110 train_time:82536ms step_avg:52.34ms
step:1578/2110 train_time:82623ms step_avg:52.36ms
step:1579/2110 train_time:82710ms step_avg:52.38ms
step:1580/2110 train_time:82796ms step_avg:52.40ms
step:1581/2110 train_time:82884ms step_avg:52.43ms
step:1582/2110 train_time:82971ms step_avg:52.45ms
step:1583/2110 train_time:83058ms step_avg:52.47ms
step:1584/2110 train_time:83145ms step_avg:52.49ms
step:1585/2110 train_time:83232ms step_avg:52.51ms
step:1586/2110 train_time:83319ms step_avg:52.53ms
step:1587/2110 train_time:83406ms step_avg:52.56ms
step:1588/2110 train_time:83492ms step_avg:52.58ms
step:1589/2110 train_time:83579ms step_avg:52.60ms
step:1590/2110 train_time:83666ms step_avg:52.62ms
step:1591/2110 train_time:83752ms step_avg:52.64ms
step:1592/2110 train_time:83838ms step_avg:52.66ms
step:1593/2110 train_time:83925ms step_avg:52.68ms
step:1594/2110 train_time:84011ms step_avg:52.70ms
step:1595/2110 train_time:84099ms step_avg:52.73ms
step:1596/2110 train_time:84186ms step_avg:52.75ms
step:1597/2110 train_time:84273ms step_avg:52.77ms
step:1598/2110 train_time:84359ms step_avg:52.79ms
step:1599/2110 train_time:84447ms step_avg:52.81ms
step:1600/2110 train_time:84532ms step_avg:52.83ms
step:1601/2110 train_time:84619ms step_avg:52.85ms
step:1602/2110 train_time:84705ms step_avg:52.87ms
step:1603/2110 train_time:84793ms step_avg:52.90ms
step:1604/2110 train_time:84880ms step_avg:52.92ms
step:1605/2110 train_time:84967ms step_avg:52.94ms
step:1606/2110 train_time:85053ms step_avg:52.96ms
step:1607/2110 train_time:85141ms step_avg:52.98ms
step:1608/2110 train_time:85227ms step_avg:53.00ms
step:1609/2110 train_time:85314ms step_avg:53.02ms
step:1610/2110 train_time:85401ms step_avg:53.04ms
step:1611/2110 train_time:85488ms step_avg:53.07ms
step:1612/2110 train_time:85574ms step_avg:53.09ms
step:1613/2110 train_time:85661ms step_avg:53.11ms
step:1614/2110 train_time:85747ms step_avg:53.13ms
step:1615/2110 train_time:85835ms step_avg:53.15ms
step:1616/2110 train_time:85922ms step_avg:53.17ms
step:1617/2110 train_time:86011ms step_avg:53.19ms
step:1618/2110 train_time:86097ms step_avg:53.21ms
step:1619/2110 train_time:86184ms step_avg:53.23ms
step:1620/2110 train_time:86270ms step_avg:53.25ms
step:1621/2110 train_time:86358ms step_avg:53.27ms
step:1622/2110 train_time:86444ms step_avg:53.29ms
step:1623/2110 train_time:86531ms step_avg:53.32ms
step:1624/2110 train_time:86617ms step_avg:53.34ms
step:1625/2110 train_time:86704ms step_avg:53.36ms
step:1626/2110 train_time:86790ms step_avg:53.38ms
step:1627/2110 train_time:86877ms step_avg:53.40ms
step:1628/2110 train_time:86965ms step_avg:53.42ms
step:1629/2110 train_time:87052ms step_avg:53.44ms
step:1630/2110 train_time:87138ms step_avg:53.46ms
step:1631/2110 train_time:87226ms step_avg:53.48ms
step:1632/2110 train_time:87311ms step_avg:53.50ms
step:1633/2110 train_time:87398ms step_avg:53.52ms
step:1634/2110 train_time:87485ms step_avg:53.54ms
step:1635/2110 train_time:87572ms step_avg:53.56ms
step:1636/2110 train_time:87660ms step_avg:53.58ms
step:1637/2110 train_time:87748ms step_avg:53.60ms
step:1638/2110 train_time:87834ms step_avg:53.62ms
step:1639/2110 train_time:87922ms step_avg:53.64ms
step:1640/2110 train_time:88009ms step_avg:53.66ms
step:1641/2110 train_time:88096ms step_avg:53.68ms
step:1642/2110 train_time:88184ms step_avg:53.71ms
step:1643/2110 train_time:88271ms step_avg:53.73ms
step:1644/2110 train_time:88358ms step_avg:53.75ms
step:1645/2110 train_time:88444ms step_avg:53.77ms
step:1646/2110 train_time:88531ms step_avg:53.79ms
step:1647/2110 train_time:88618ms step_avg:53.81ms
step:1648/2110 train_time:88705ms step_avg:53.83ms
step:1649/2110 train_time:88792ms step_avg:53.85ms
step:1650/2110 train_time:88878ms step_avg:53.87ms
step:1651/2110 train_time:88966ms step_avg:53.89ms
step:1652/2110 train_time:89052ms step_avg:53.91ms
step:1653/2110 train_time:89140ms step_avg:53.93ms
step:1654/2110 train_time:89227ms step_avg:53.95ms
step:1655/2110 train_time:89313ms step_avg:53.97ms
step:1656/2110 train_time:89399ms step_avg:53.99ms
step:1657/2110 train_time:89487ms step_avg:54.01ms
step:1658/2110 train_time:89574ms step_avg:54.03ms
step:1659/2110 train_time:89663ms step_avg:54.05ms
step:1660/2110 train_time:89751ms step_avg:54.07ms
step:1661/2110 train_time:89838ms step_avg:54.09ms
step:1662/2110 train_time:89926ms step_avg:54.11ms
step:1663/2110 train_time:90016ms step_avg:54.13ms
step:1664/2110 train_time:90103ms step_avg:54.15ms
step:1665/2110 train_time:90193ms step_avg:54.17ms
step:1666/2110 train_time:90281ms step_avg:54.19ms
step:1667/2110 train_time:90369ms step_avg:54.21ms
step:1668/2110 train_time:90457ms step_avg:54.23ms
step:1669/2110 train_time:90546ms step_avg:54.25ms
step:1670/2110 train_time:90633ms step_avg:54.27ms
step:1671/2110 train_time:90722ms step_avg:54.29ms
step:1672/2110 train_time:90810ms step_avg:54.31ms
step:1673/2110 train_time:90898ms step_avg:54.33ms
step:1674/2110 train_time:90985ms step_avg:54.35ms
step:1675/2110 train_time:91074ms step_avg:54.37ms
step:1676/2110 train_time:91161ms step_avg:54.39ms
step:1677/2110 train_time:91252ms step_avg:54.41ms
step:1678/2110 train_time:91339ms step_avg:54.43ms
step:1679/2110 train_time:91429ms step_avg:54.45ms
step:1680/2110 train_time:91517ms step_avg:54.47ms
step:1681/2110 train_time:91605ms step_avg:54.49ms
step:1682/2110 train_time:91692ms step_avg:54.51ms
step:1683/2110 train_time:91781ms step_avg:54.53ms
step:1684/2110 train_time:91868ms step_avg:54.55ms
step:1685/2110 train_time:91957ms step_avg:54.57ms
step:1686/2110 train_time:92044ms step_avg:54.59ms
step:1687/2110 train_time:92132ms step_avg:54.61ms
step:1688/2110 train_time:92221ms step_avg:54.63ms
step:1689/2110 train_time:92309ms step_avg:54.65ms
step:1690/2110 train_time:92397ms step_avg:54.67ms
step:1691/2110 train_time:92486ms step_avg:54.69ms
step:1692/2110 train_time:92573ms step_avg:54.71ms
step:1693/2110 train_time:92662ms step_avg:54.73ms
step:1694/2110 train_time:92748ms step_avg:54.75ms
step:1695/2110 train_time:92837ms step_avg:54.77ms
step:1696/2110 train_time:92925ms step_avg:54.79ms
step:1697/2110 train_time:93013ms step_avg:54.81ms
step:1698/2110 train_time:93102ms step_avg:54.83ms
step:1699/2110 train_time:93190ms step_avg:54.85ms
step:1700/2110 train_time:93277ms step_avg:54.87ms
step:1701/2110 train_time:93367ms step_avg:54.89ms
step:1702/2110 train_time:93454ms step_avg:54.91ms
step:1703/2110 train_time:93543ms step_avg:54.93ms
step:1704/2110 train_time:93631ms step_avg:54.95ms
step:1705/2110 train_time:93719ms step_avg:54.97ms
step:1706/2110 train_time:93808ms step_avg:54.99ms
step:1707/2110 train_time:93897ms step_avg:55.01ms
step:1708/2110 train_time:93984ms step_avg:55.03ms
step:1709/2110 train_time:94073ms step_avg:55.05ms
step:1710/2110 train_time:94161ms step_avg:55.06ms
step:1711/2110 train_time:94249ms step_avg:55.08ms
step:1712/2110 train_time:94337ms step_avg:55.10ms
step:1713/2110 train_time:94426ms step_avg:55.12ms
step:1714/2110 train_time:94512ms step_avg:55.14ms
step:1715/2110 train_time:94601ms step_avg:55.16ms
step:1716/2110 train_time:94691ms step_avg:55.18ms
step:1717/2110 train_time:94777ms step_avg:55.20ms
step:1718/2110 train_time:94865ms step_avg:55.22ms
step:1719/2110 train_time:94953ms step_avg:55.24ms
step:1720/2110 train_time:95041ms step_avg:55.26ms
step:1721/2110 train_time:95130ms step_avg:55.28ms
step:1722/2110 train_time:95217ms step_avg:55.29ms
step:1723/2110 train_time:95306ms step_avg:55.31ms
step:1724/2110 train_time:95393ms step_avg:55.33ms
step:1725/2110 train_time:95482ms step_avg:55.35ms
step:1726/2110 train_time:95570ms step_avg:55.37ms
step:1727/2110 train_time:95659ms step_avg:55.39ms
step:1728/2110 train_time:95746ms step_avg:55.41ms
step:1729/2110 train_time:95835ms step_avg:55.43ms
step:1730/2110 train_time:95923ms step_avg:55.45ms
step:1731/2110 train_time:96011ms step_avg:55.47ms
step:1732/2110 train_time:96100ms step_avg:55.48ms
step:1733/2110 train_time:96188ms step_avg:55.50ms
step:1734/2110 train_time:96275ms step_avg:55.52ms
step:1735/2110 train_time:96364ms step_avg:55.54ms
step:1736/2110 train_time:96451ms step_avg:55.56ms
step:1737/2110 train_time:96540ms step_avg:55.58ms
step:1738/2110 train_time:96628ms step_avg:55.60ms
step:1739/2110 train_time:96717ms step_avg:55.62ms
step:1740/2110 train_time:96804ms step_avg:55.63ms
step:1741/2110 train_time:96892ms step_avg:55.65ms
step:1742/2110 train_time:96980ms step_avg:55.67ms
step:1743/2110 train_time:97069ms step_avg:55.69ms
step:1744/2110 train_time:97157ms step_avg:55.71ms
step:1745/2110 train_time:97244ms step_avg:55.73ms
step:1746/2110 train_time:97332ms step_avg:55.75ms
step:1747/2110 train_time:97421ms step_avg:55.76ms
step:1748/2110 train_time:97510ms step_avg:55.78ms
step:1749/2110 train_time:97599ms step_avg:55.80ms
step:1750/2110 train_time:97688ms step_avg:55.82ms
step:1750/2110 val_loss:3.3808 train_time:97779ms step_avg:55.87ms
step:1751/2110 train_time:97810ms step_avg:55.86ms
step:1752/2110 train_time:97872ms step_avg:55.86ms
step:1753/2110 train_time:97966ms step_avg:55.88ms
step:1754/2110 train_time:98055ms step_avg:55.90ms
step:1755/2110 train_time:98144ms step_avg:55.92ms
step:1756/2110 train_time:98230ms step_avg:55.94ms
step:1757/2110 train_time:98317ms step_avg:55.96ms
step:1758/2110 train_time:98404ms step_avg:55.97ms
step:1759/2110 train_time:98491ms step_avg:55.99ms
step:1760/2110 train_time:98578ms step_avg:56.01ms
step:1761/2110 train_time:98666ms step_avg:56.03ms
step:1762/2110 train_time:98755ms step_avg:56.05ms
step:1763/2110 train_time:98847ms step_avg:56.07ms
step:1764/2110 train_time:98937ms step_avg:56.09ms
step:1765/2110 train_time:99027ms step_avg:56.11ms
step:1766/2110 train_time:99115ms step_avg:56.12ms
step:1767/2110 train_time:99204ms step_avg:56.14ms
step:1768/2110 train_time:99291ms step_avg:56.16ms
step:1769/2110 train_time:99379ms step_avg:56.18ms
step:1770/2110 train_time:99465ms step_avg:56.20ms
step:1771/2110 train_time:99551ms step_avg:56.21ms
step:1772/2110 train_time:99638ms step_avg:56.23ms
step:1773/2110 train_time:99727ms step_avg:56.25ms
step:1774/2110 train_time:99816ms step_avg:56.27ms
step:1775/2110 train_time:99907ms step_avg:56.29ms
step:1776/2110 train_time:99996ms step_avg:56.30ms
step:1777/2110 train_time:100085ms step_avg:56.32ms
step:1778/2110 train_time:100172ms step_avg:56.34ms
step:1779/2110 train_time:100261ms step_avg:56.36ms
step:1780/2110 train_time:100347ms step_avg:56.37ms
step:1781/2110 train_time:100436ms step_avg:56.39ms
step:1782/2110 train_time:100522ms step_avg:56.41ms
step:1783/2110 train_time:100610ms step_avg:56.43ms
step:1784/2110 train_time:100697ms step_avg:56.44ms
step:1785/2110 train_time:100786ms step_avg:56.46ms
step:1786/2110 train_time:100874ms step_avg:56.48ms
step:1787/2110 train_time:100963ms step_avg:56.50ms
step:1788/2110 train_time:101052ms step_avg:56.52ms
step:1789/2110 train_time:101140ms step_avg:56.53ms
step:1790/2110 train_time:101227ms step_avg:56.55ms
step:1791/2110 train_time:101316ms step_avg:56.57ms
step:1792/2110 train_time:101403ms step_avg:56.59ms
step:1793/2110 train_time:101491ms step_avg:56.60ms
step:1794/2110 train_time:101578ms step_avg:56.62ms
step:1795/2110 train_time:101666ms step_avg:56.64ms
step:1796/2110 train_time:101753ms step_avg:56.66ms
step:1797/2110 train_time:101842ms step_avg:56.67ms
step:1798/2110 train_time:101930ms step_avg:56.69ms
step:1799/2110 train_time:102020ms step_avg:56.71ms
step:1800/2110 train_time:102108ms step_avg:56.73ms
step:1801/2110 train_time:102197ms step_avg:56.74ms
step:1802/2110 train_time:102284ms step_avg:56.76ms
step:1803/2110 train_time:102372ms step_avg:56.78ms
step:1804/2110 train_time:102461ms step_avg:56.80ms
step:1805/2110 train_time:102548ms step_avg:56.81ms
step:1806/2110 train_time:102636ms step_avg:56.83ms
step:1807/2110 train_time:102724ms step_avg:56.85ms
step:1808/2110 train_time:102811ms step_avg:56.86ms
step:1809/2110 train_time:102900ms step_avg:56.88ms
step:1810/2110 train_time:102988ms step_avg:56.90ms
step:1811/2110 train_time:103077ms step_avg:56.92ms
step:1812/2110 train_time:103165ms step_avg:56.93ms
step:1813/2110 train_time:103253ms step_avg:56.95ms
step:1814/2110 train_time:103341ms step_avg:56.97ms
step:1815/2110 train_time:103429ms step_avg:56.99ms
step:1816/2110 train_time:103516ms step_avg:57.00ms
step:1817/2110 train_time:103604ms step_avg:57.02ms
step:1818/2110 train_time:103692ms step_avg:57.04ms
step:1819/2110 train_time:103779ms step_avg:57.05ms
step:1820/2110 train_time:103866ms step_avg:57.07ms
step:1821/2110 train_time:103956ms step_avg:57.09ms
step:1822/2110 train_time:104044ms step_avg:57.10ms
step:1823/2110 train_time:104133ms step_avg:57.12ms
step:1824/2110 train_time:104220ms step_avg:57.14ms
step:1825/2110 train_time:104308ms step_avg:57.16ms
step:1826/2110 train_time:104395ms step_avg:57.17ms
step:1827/2110 train_time:104484ms step_avg:57.19ms
step:1828/2110 train_time:104572ms step_avg:57.21ms
step:1829/2110 train_time:104660ms step_avg:57.22ms
step:1830/2110 train_time:104748ms step_avg:57.24ms
step:1831/2110 train_time:104835ms step_avg:57.26ms
step:1832/2110 train_time:104923ms step_avg:57.27ms
step:1833/2110 train_time:105012ms step_avg:57.29ms
step:1834/2110 train_time:105101ms step_avg:57.31ms
step:1835/2110 train_time:105189ms step_avg:57.32ms
step:1836/2110 train_time:105276ms step_avg:57.34ms
step:1837/2110 train_time:105364ms step_avg:57.36ms
step:1838/2110 train_time:105452ms step_avg:57.37ms
step:1839/2110 train_time:105540ms step_avg:57.39ms
step:1840/2110 train_time:105627ms step_avg:57.41ms
step:1841/2110 train_time:105716ms step_avg:57.42ms
step:1842/2110 train_time:105804ms step_avg:57.44ms
step:1843/2110 train_time:105892ms step_avg:57.46ms
step:1844/2110 train_time:105980ms step_avg:57.47ms
step:1845/2110 train_time:106069ms step_avg:57.49ms
step:1846/2110 train_time:106158ms step_avg:57.51ms
step:1847/2110 train_time:106246ms step_avg:57.52ms
step:1848/2110 train_time:106333ms step_avg:57.54ms
step:1849/2110 train_time:106422ms step_avg:57.56ms
step:1850/2110 train_time:106509ms step_avg:57.57ms
step:1851/2110 train_time:106597ms step_avg:57.59ms
step:1852/2110 train_time:106684ms step_avg:57.60ms
step:1853/2110 train_time:106773ms step_avg:57.62ms
step:1854/2110 train_time:106862ms step_avg:57.64ms
step:1855/2110 train_time:106951ms step_avg:57.66ms
step:1856/2110 train_time:107040ms step_avg:57.67ms
step:1857/2110 train_time:107128ms step_avg:57.69ms
step:1858/2110 train_time:107216ms step_avg:57.70ms
step:1859/2110 train_time:107304ms step_avg:57.72ms
step:1860/2110 train_time:107391ms step_avg:57.74ms
step:1861/2110 train_time:107479ms step_avg:57.75ms
step:1862/2110 train_time:107567ms step_avg:57.77ms
step:1863/2110 train_time:107655ms step_avg:57.79ms
step:1864/2110 train_time:107742ms step_avg:57.80ms
step:1865/2110 train_time:107831ms step_avg:57.82ms
step:1866/2110 train_time:107919ms step_avg:57.83ms
step:1867/2110 train_time:108008ms step_avg:57.85ms
step:1868/2110 train_time:108096ms step_avg:57.87ms
step:1869/2110 train_time:108184ms step_avg:57.88ms
step:1870/2110 train_time:108272ms step_avg:57.90ms
step:1871/2110 train_time:108361ms step_avg:57.92ms
step:1872/2110 train_time:108448ms step_avg:57.93ms
step:1873/2110 train_time:108537ms step_avg:57.95ms
step:1874/2110 train_time:108624ms step_avg:57.96ms
step:1875/2110 train_time:108713ms step_avg:57.98ms
step:1876/2110 train_time:108801ms step_avg:58.00ms
step:1877/2110 train_time:108889ms step_avg:58.01ms
step:1878/2110 train_time:108977ms step_avg:58.03ms
step:1879/2110 train_time:109065ms step_avg:58.04ms
step:1880/2110 train_time:109153ms step_avg:58.06ms
step:1881/2110 train_time:109242ms step_avg:58.08ms
step:1882/2110 train_time:109330ms step_avg:58.09ms
step:1883/2110 train_time:109418ms step_avg:58.11ms
step:1884/2110 train_time:109505ms step_avg:58.12ms
step:1885/2110 train_time:109593ms step_avg:58.14ms
step:1886/2110 train_time:109681ms step_avg:58.16ms
step:1887/2110 train_time:109770ms step_avg:58.17ms
step:1888/2110 train_time:109858ms step_avg:58.19ms
step:1889/2110 train_time:109947ms step_avg:58.20ms
step:1890/2110 train_time:110034ms step_avg:58.22ms
step:1891/2110 train_time:110122ms step_avg:58.24ms
step:1892/2110 train_time:110210ms step_avg:58.25ms
step:1893/2110 train_time:110300ms step_avg:58.27ms
step:1894/2110 train_time:110387ms step_avg:58.28ms
step:1895/2110 train_time:110475ms step_avg:58.30ms
step:1896/2110 train_time:110563ms step_avg:58.31ms
step:1897/2110 train_time:110651ms step_avg:58.33ms
step:1898/2110 train_time:110740ms step_avg:58.35ms
step:1899/2110 train_time:110828ms step_avg:58.36ms
step:1900/2110 train_time:110916ms step_avg:58.38ms
step:1901/2110 train_time:111005ms step_avg:58.39ms
step:1902/2110 train_time:111093ms step_avg:58.41ms
step:1903/2110 train_time:111181ms step_avg:58.42ms
step:1904/2110 train_time:111269ms step_avg:58.44ms
step:1905/2110 train_time:111357ms step_avg:58.46ms
step:1906/2110 train_time:111443ms step_avg:58.47ms
step:1907/2110 train_time:111532ms step_avg:58.49ms
step:1908/2110 train_time:111621ms step_avg:58.50ms
step:1909/2110 train_time:111708ms step_avg:58.52ms
step:1910/2110 train_time:111797ms step_avg:58.53ms
step:1911/2110 train_time:111885ms step_avg:58.55ms
step:1912/2110 train_time:111974ms step_avg:58.56ms
step:1913/2110 train_time:112063ms step_avg:58.58ms
step:1914/2110 train_time:112149ms step_avg:58.59ms
step:1915/2110 train_time:112238ms step_avg:58.61ms
step:1916/2110 train_time:112326ms step_avg:58.63ms
step:1917/2110 train_time:112414ms step_avg:58.64ms
step:1918/2110 train_time:112502ms step_avg:58.66ms
step:1919/2110 train_time:112590ms step_avg:58.67ms
step:1920/2110 train_time:112677ms step_avg:58.69ms
step:1921/2110 train_time:112765ms step_avg:58.70ms
step:1922/2110 train_time:112853ms step_avg:58.72ms
step:1923/2110 train_time:112942ms step_avg:58.73ms
step:1924/2110 train_time:113030ms step_avg:58.75ms
step:1925/2110 train_time:113118ms step_avg:58.76ms
step:1926/2110 train_time:113206ms step_avg:58.78ms
step:1927/2110 train_time:113294ms step_avg:58.79ms
step:1928/2110 train_time:113382ms step_avg:58.81ms
step:1929/2110 train_time:113469ms step_avg:58.82ms
step:1930/2110 train_time:113558ms step_avg:58.84ms
step:1931/2110 train_time:113645ms step_avg:58.85ms
step:1932/2110 train_time:113734ms step_avg:58.87ms
step:1933/2110 train_time:113822ms step_avg:58.88ms
step:1934/2110 train_time:113909ms step_avg:58.90ms
step:1935/2110 train_time:113997ms step_avg:58.91ms
step:1936/2110 train_time:114084ms step_avg:58.93ms
step:1937/2110 train_time:114173ms step_avg:58.94ms
step:1938/2110 train_time:114262ms step_avg:58.96ms
step:1939/2110 train_time:114349ms step_avg:58.97ms
step:1940/2110 train_time:114437ms step_avg:58.99ms
step:1941/2110 train_time:114525ms step_avg:59.00ms
step:1942/2110 train_time:114613ms step_avg:59.02ms
step:1943/2110 train_time:114702ms step_avg:59.03ms
step:1944/2110 train_time:114790ms step_avg:59.05ms
step:1945/2110 train_time:114879ms step_avg:59.06ms
step:1946/2110 train_time:114966ms step_avg:59.08ms
step:1947/2110 train_time:115055ms step_avg:59.09ms
step:1948/2110 train_time:115142ms step_avg:59.11ms
step:1949/2110 train_time:115232ms step_avg:59.12ms
step:1950/2110 train_time:115320ms step_avg:59.14ms
step:1951/2110 train_time:115408ms step_avg:59.15ms
step:1952/2110 train_time:115495ms step_avg:59.17ms
step:1953/2110 train_time:115583ms step_avg:59.18ms
step:1954/2110 train_time:115671ms step_avg:59.20ms
step:1955/2110 train_time:115761ms step_avg:59.21ms
step:1956/2110 train_time:115847ms step_avg:59.23ms
step:1957/2110 train_time:115935ms step_avg:59.24ms
step:1958/2110 train_time:116022ms step_avg:59.26ms
step:1959/2110 train_time:116111ms step_avg:59.27ms
step:1960/2110 train_time:116201ms step_avg:59.29ms
step:1961/2110 train_time:116289ms step_avg:59.30ms
step:1962/2110 train_time:116377ms step_avg:59.32ms
step:1963/2110 train_time:116464ms step_avg:59.33ms
step:1964/2110 train_time:116554ms step_avg:59.35ms
step:1965/2110 train_time:116642ms step_avg:59.36ms
step:1966/2110 train_time:116731ms step_avg:59.37ms
step:1967/2110 train_time:116819ms step_avg:59.39ms
step:1968/2110 train_time:116906ms step_avg:59.40ms
step:1969/2110 train_time:116996ms step_avg:59.42ms
step:1970/2110 train_time:117084ms step_avg:59.43ms
step:1971/2110 train_time:117172ms step_avg:59.45ms
step:1972/2110 train_time:117261ms step_avg:59.46ms
step:1973/2110 train_time:117349ms step_avg:59.48ms
step:1974/2110 train_time:117436ms step_avg:59.49ms
step:1975/2110 train_time:117524ms step_avg:59.51ms
step:1976/2110 train_time:117612ms step_avg:59.52ms
step:1977/2110 train_time:117701ms step_avg:59.54ms
step:1978/2110 train_time:117789ms step_avg:59.55ms
step:1979/2110 train_time:117877ms step_avg:59.56ms
step:1980/2110 train_time:117965ms step_avg:59.58ms
step:1981/2110 train_time:118053ms step_avg:59.59ms
step:1982/2110 train_time:118140ms step_avg:59.61ms
step:1983/2110 train_time:118230ms step_avg:59.62ms
step:1984/2110 train_time:118317ms step_avg:59.64ms
step:1985/2110 train_time:118405ms step_avg:59.65ms
step:1986/2110 train_time:118493ms step_avg:59.66ms
step:1987/2110 train_time:118582ms step_avg:59.68ms
step:1988/2110 train_time:118670ms step_avg:59.69ms
step:1989/2110 train_time:118759ms step_avg:59.71ms
step:1990/2110 train_time:118846ms step_avg:59.72ms
step:1991/2110 train_time:118935ms step_avg:59.74ms
step:1992/2110 train_time:119022ms step_avg:59.75ms
step:1993/2110 train_time:119112ms step_avg:59.77ms
step:1994/2110 train_time:119200ms step_avg:59.78ms
step:1995/2110 train_time:119289ms step_avg:59.79ms
step:1996/2110 train_time:119377ms step_avg:59.81ms
step:1997/2110 train_time:119465ms step_avg:59.82ms
step:1998/2110 train_time:119552ms step_avg:59.84ms
step:1999/2110 train_time:119641ms step_avg:59.85ms
step:2000/2110 train_time:119729ms step_avg:59.86ms
step:2000/2110 val_loss:3.3052 train_time:119820ms step_avg:59.91ms
step:2001/2110 train_time:119847ms step_avg:59.89ms
step:2002/2110 train_time:119912ms step_avg:59.90ms
step:2003/2110 train_time:120004ms step_avg:59.91ms
step:2004/2110 train_time:120092ms step_avg:59.93ms
step:2005/2110 train_time:120181ms step_avg:59.94ms
step:2006/2110 train_time:120267ms step_avg:59.95ms
step:2007/2110 train_time:120355ms step_avg:59.97ms
step:2008/2110 train_time:120441ms step_avg:59.98ms
step:2009/2110 train_time:120529ms step_avg:59.99ms
step:2010/2110 train_time:120616ms step_avg:60.01ms
step:2011/2110 train_time:120704ms step_avg:60.02ms
step:2012/2110 train_time:120792ms step_avg:60.04ms
step:2013/2110 train_time:120884ms step_avg:60.05ms
step:2014/2110 train_time:120974ms step_avg:60.07ms
step:2015/2110 train_time:121065ms step_avg:60.08ms
step:2016/2110 train_time:121152ms step_avg:60.10ms
step:2017/2110 train_time:121241ms step_avg:60.11ms
step:2018/2110 train_time:121328ms step_avg:60.12ms
step:2019/2110 train_time:121416ms step_avg:60.14ms
step:2020/2110 train_time:121503ms step_avg:60.15ms
step:2021/2110 train_time:121591ms step_avg:60.16ms
step:2022/2110 train_time:121678ms step_avg:60.18ms
step:2023/2110 train_time:121766ms step_avg:60.19ms
step:2024/2110 train_time:121855ms step_avg:60.21ms
step:2025/2110 train_time:121946ms step_avg:60.22ms
step:2026/2110 train_time:122036ms step_avg:60.24ms
step:2027/2110 train_time:122125ms step_avg:60.25ms
step:2028/2110 train_time:122213ms step_avg:60.26ms
step:2029/2110 train_time:122300ms step_avg:60.28ms
step:2030/2110 train_time:122388ms step_avg:60.29ms
step:2031/2110 train_time:122477ms step_avg:60.30ms
step:2032/2110 train_time:122565ms step_avg:60.32ms
step:2033/2110 train_time:122653ms step_avg:60.33ms
step:2034/2110 train_time:122740ms step_avg:60.34ms
step:2035/2110 train_time:122830ms step_avg:60.36ms
step:2036/2110 train_time:122920ms step_avg:60.37ms
step:2037/2110 train_time:123009ms step_avg:60.39ms
step:2038/2110 train_time:123098ms step_avg:60.40ms
step:2039/2110 train_time:123187ms step_avg:60.42ms
step:2040/2110 train_time:123274ms step_avg:60.43ms
step:2041/2110 train_time:123361ms step_avg:60.44ms
step:2042/2110 train_time:123450ms step_avg:60.46ms
step:2043/2110 train_time:123537ms step_avg:60.47ms
step:2044/2110 train_time:123625ms step_avg:60.48ms
step:2045/2110 train_time:123714ms step_avg:60.50ms
step:2046/2110 train_time:123801ms step_avg:60.51ms
step:2047/2110 train_time:123890ms step_avg:60.52ms
step:2048/2110 train_time:123979ms step_avg:60.54ms
step:2049/2110 train_time:124068ms step_avg:60.55ms
step:2050/2110 train_time:124156ms step_avg:60.56ms
step:2051/2110 train_time:124246ms step_avg:60.58ms
step:2052/2110 train_time:124334ms step_avg:60.59ms
step:2053/2110 train_time:124422ms step_avg:60.60ms
step:2054/2110 train_time:124510ms step_avg:60.62ms
step:2055/2110 train_time:124599ms step_avg:60.63ms
step:2056/2110 train_time:124686ms step_avg:60.64ms
step:2057/2110 train_time:124773ms step_avg:60.66ms
step:2058/2110 train_time:124861ms step_avg:60.67ms
step:2059/2110 train_time:124950ms step_avg:60.68ms
step:2060/2110 train_time:125039ms step_avg:60.70ms
step:2061/2110 train_time:125127ms step_avg:60.71ms
step:2062/2110 train_time:125215ms step_avg:60.73ms
step:2063/2110 train_time:125303ms step_avg:60.74ms
step:2064/2110 train_time:125392ms step_avg:60.75ms
step:2065/2110 train_time:125480ms step_avg:60.77ms
step:2066/2110 train_time:125568ms step_avg:60.78ms
step:2067/2110 train_time:125655ms step_avg:60.79ms
step:2068/2110 train_time:125742ms step_avg:60.80ms
step:2069/2110 train_time:125830ms step_avg:60.82ms
step:2070/2110 train_time:125920ms step_avg:60.83ms
step:2071/2110 train_time:126008ms step_avg:60.84ms
step:2072/2110 train_time:126098ms step_avg:60.86ms
step:2073/2110 train_time:126186ms step_avg:60.87ms
step:2074/2110 train_time:126274ms step_avg:60.88ms
step:2075/2110 train_time:126361ms step_avg:60.90ms
step:2076/2110 train_time:126449ms step_avg:60.91ms
step:2077/2110 train_time:126540ms step_avg:60.92ms
step:2078/2110 train_time:126629ms step_avg:60.94ms
step:2079/2110 train_time:126717ms step_avg:60.95ms
step:2080/2110 train_time:126806ms step_avg:60.96ms
step:2081/2110 train_time:126893ms step_avg:60.98ms
step:2082/2110 train_time:126981ms step_avg:60.99ms
step:2083/2110 train_time:127070ms step_avg:61.00ms
step:2084/2110 train_time:127158ms step_avg:61.02ms
step:2085/2110 train_time:127247ms step_avg:61.03ms
step:2086/2110 train_time:127336ms step_avg:61.04ms
step:2087/2110 train_time:127423ms step_avg:61.06ms
step:2088/2110 train_time:127511ms step_avg:61.07ms
step:2089/2110 train_time:127600ms step_avg:61.08ms
step:2090/2110 train_time:127690ms step_avg:61.10ms
step:2091/2110 train_time:127779ms step_avg:61.11ms
step:2092/2110 train_time:127868ms step_avg:61.12ms
step:2093/2110 train_time:127956ms step_avg:61.14ms
step:2094/2110 train_time:128044ms step_avg:61.15ms
step:2095/2110 train_time:128134ms step_avg:61.16ms
step:2096/2110 train_time:128222ms step_avg:61.17ms
step:2097/2110 train_time:128310ms step_avg:61.19ms
step:2098/2110 train_time:128398ms step_avg:61.20ms
step:2099/2110 train_time:128486ms step_avg:61.21ms
step:2100/2110 train_time:128574ms step_avg:61.23ms
step:2101/2110 train_time:128663ms step_avg:61.24ms
step:2102/2110 train_time:128751ms step_avg:61.25ms
step:2103/2110 train_time:128841ms step_avg:61.27ms
step:2104/2110 train_time:128929ms step_avg:61.28ms
step:2105/2110 train_time:129018ms step_avg:61.29ms
step:2106/2110 train_time:129108ms step_avg:61.30ms
step:2107/2110 train_time:129196ms step_avg:61.32ms
step:2108/2110 train_time:129284ms step_avg:61.33ms
step:2109/2110 train_time:129372ms step_avg:61.34ms
step:2110/2110 train_time:129460ms step_avg:61.36ms
step:2110/2110 val_loss:3.2805 train_time:129550ms step_avg:61.40ms
peak memory allocated: 29892 MiB reserved: 35436 MiB

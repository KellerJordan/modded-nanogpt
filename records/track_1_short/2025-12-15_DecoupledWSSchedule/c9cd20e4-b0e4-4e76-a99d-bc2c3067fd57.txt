import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Tue Dec 16 01:23:33 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   39C    P0            126W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   43C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   44C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   37C    P0            122W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   43C    P0            127W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   43C    P0            127W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   38C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              80      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              81      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:108ms step_avg:108.37ms
step:2/2110 train_time:147ms step_avg:73.68ms
step:3/2110 train_time:180ms step_avg:59.94ms
step:4/2110 train_time:212ms step_avg:52.99ms
step:5/2110 train_time:242ms step_avg:48.30ms
step:6/2110 train_time:434ms step_avg:72.31ms
step:7/2110 train_time:635ms step_avg:90.78ms
step:8/2110 train_time:668ms step_avg:83.48ms
step:9/2110 train_time:701ms step_avg:77.86ms
step:10/2110 train_time:733ms step_avg:73.30ms
step:11/2110 train_time:766ms step_avg:69.66ms
step:12/2110 train_time:799ms step_avg:66.60ms
step:13/2110 train_time:832ms step_avg:63.99ms
step:14/2110 train_time:865ms step_avg:61.75ms
step:15/2110 train_time:898ms step_avg:59.87ms
step:16/2110 train_time:931ms step_avg:58.18ms
step:17/2110 train_time:964ms step_avg:56.71ms
step:18/2110 train_time:997ms step_avg:55.39ms
step:19/2110 train_time:1030ms step_avg:54.23ms
step:20/2110 train_time:1063ms step_avg:53.15ms
step:21/2110 train_time:1097ms step_avg:52.23ms
step:22/2110 train_time:1129ms step_avg:51.34ms
step:23/2110 train_time:1163ms step_avg:50.56ms
step:24/2110 train_time:1196ms step_avg:49.81ms
step:25/2110 train_time:1229ms step_avg:49.15ms
step:26/2110 train_time:1262ms step_avg:48.52ms
step:27/2110 train_time:1295ms step_avg:47.96ms
step:28/2110 train_time:1328ms step_avg:47.42ms
step:29/2110 train_time:1361ms step_avg:46.92ms
step:30/2110 train_time:1393ms step_avg:46.44ms
step:31/2110 train_time:1427ms step_avg:46.03ms
step:32/2110 train_time:1460ms step_avg:45.61ms
step:33/2110 train_time:1494ms step_avg:45.28ms
step:34/2110 train_time:1528ms step_avg:44.93ms
step:35/2110 train_time:1563ms step_avg:44.65ms
step:36/2110 train_time:1596ms step_avg:44.33ms
step:37/2110 train_time:1630ms step_avg:44.05ms
step:38/2110 train_time:1663ms step_avg:43.77ms
step:39/2110 train_time:1697ms step_avg:43.51ms
step:40/2110 train_time:1730ms step_avg:43.25ms
step:41/2110 train_time:1764ms step_avg:43.01ms
step:42/2110 train_time:1798ms step_avg:42.80ms
step:43/2110 train_time:1830ms step_avg:42.56ms
step:44/2110 train_time:1863ms step_avg:42.35ms
step:45/2110 train_time:1897ms step_avg:42.15ms
step:46/2110 train_time:1930ms step_avg:41.95ms
step:47/2110 train_time:1963ms step_avg:41.76ms
step:48/2110 train_time:1996ms step_avg:41.58ms
step:49/2110 train_time:2030ms step_avg:41.43ms
step:50/2110 train_time:2062ms step_avg:41.25ms
step:51/2110 train_time:2095ms step_avg:41.07ms
step:52/2110 train_time:2129ms step_avg:40.95ms
step:53/2110 train_time:2160ms step_avg:40.76ms
step:54/2110 train_time:2193ms step_avg:40.61ms
step:55/2110 train_time:2226ms step_avg:40.48ms
step:56/2110 train_time:2259ms step_avg:40.34ms
step:57/2110 train_time:2292ms step_avg:40.22ms
step:58/2110 train_time:2325ms step_avg:40.09ms
step:59/2110 train_time:2359ms step_avg:39.98ms
step:60/2110 train_time:2393ms step_avg:39.88ms
step:61/2110 train_time:2425ms step_avg:39.75ms
step:62/2110 train_time:2458ms step_avg:39.64ms
step:63/2110 train_time:2491ms step_avg:39.54ms
step:64/2110 train_time:2524ms step_avg:39.43ms
step:65/2110 train_time:2558ms step_avg:39.35ms
step:66/2110 train_time:2591ms step_avg:39.25ms
step:67/2110 train_time:2624ms step_avg:39.17ms
step:68/2110 train_time:2657ms step_avg:39.08ms
step:69/2110 train_time:2691ms step_avg:39.00ms
step:70/2110 train_time:2724ms step_avg:38.91ms
step:71/2110 train_time:2757ms step_avg:38.83ms
step:72/2110 train_time:2790ms step_avg:38.75ms
step:73/2110 train_time:2823ms step_avg:38.67ms
step:74/2110 train_time:2856ms step_avg:38.59ms
step:75/2110 train_time:2889ms step_avg:38.52ms
step:76/2110 train_time:2922ms step_avg:38.44ms
step:77/2110 train_time:2955ms step_avg:38.38ms
step:78/2110 train_time:2988ms step_avg:38.31ms
step:79/2110 train_time:3021ms step_avg:38.25ms
step:80/2110 train_time:3054ms step_avg:38.18ms
step:81/2110 train_time:3088ms step_avg:38.12ms
step:82/2110 train_time:3120ms step_avg:38.05ms
step:83/2110 train_time:3154ms step_avg:38.00ms
step:84/2110 train_time:3187ms step_avg:37.94ms
step:85/2110 train_time:3220ms step_avg:37.88ms
step:86/2110 train_time:3253ms step_avg:37.83ms
step:87/2110 train_time:3286ms step_avg:37.77ms
step:88/2110 train_time:3319ms step_avg:37.72ms
step:89/2110 train_time:3352ms step_avg:37.66ms
step:90/2110 train_time:3386ms step_avg:37.62ms
step:91/2110 train_time:3418ms step_avg:37.56ms
step:92/2110 train_time:3451ms step_avg:37.51ms
step:93/2110 train_time:3484ms step_avg:37.47ms
step:94/2110 train_time:3517ms step_avg:37.41ms
step:95/2110 train_time:3550ms step_avg:37.37ms
step:96/2110 train_time:3583ms step_avg:37.32ms
step:97/2110 train_time:3616ms step_avg:37.28ms
step:98/2110 train_time:3649ms step_avg:37.24ms
step:99/2110 train_time:3683ms step_avg:37.20ms
step:100/2110 train_time:3717ms step_avg:37.17ms
step:101/2110 train_time:3749ms step_avg:37.12ms
step:102/2110 train_time:3782ms step_avg:37.08ms
step:103/2110 train_time:3816ms step_avg:37.05ms
step:104/2110 train_time:3848ms step_avg:37.00ms
step:105/2110 train_time:3882ms step_avg:36.97ms
step:106/2110 train_time:3916ms step_avg:36.94ms
step:107/2110 train_time:3948ms step_avg:36.90ms
step:108/2110 train_time:3981ms step_avg:36.86ms
step:109/2110 train_time:4014ms step_avg:36.83ms
step:110/2110 train_time:4047ms step_avg:36.79ms
step:111/2110 train_time:4080ms step_avg:36.76ms
step:112/2110 train_time:4113ms step_avg:36.72ms
step:113/2110 train_time:4146ms step_avg:36.69ms
step:114/2110 train_time:4179ms step_avg:36.65ms
step:115/2110 train_time:4212ms step_avg:36.63ms
step:116/2110 train_time:4245ms step_avg:36.60ms
step:117/2110 train_time:4279ms step_avg:36.57ms
step:118/2110 train_time:4312ms step_avg:36.54ms
step:119/2110 train_time:4345ms step_avg:36.51ms
step:120/2110 train_time:4377ms step_avg:36.48ms
step:121/2110 train_time:4410ms step_avg:36.45ms
step:122/2110 train_time:4443ms step_avg:36.42ms
step:123/2110 train_time:4476ms step_avg:36.39ms
step:124/2110 train_time:4510ms step_avg:36.37ms
step:125/2110 train_time:4542ms step_avg:36.34ms
step:126/2110 train_time:4576ms step_avg:36.32ms
step:127/2110 train_time:4608ms step_avg:36.29ms
step:128/2110 train_time:4642ms step_avg:36.26ms
step:129/2110 train_time:4675ms step_avg:36.24ms
step:130/2110 train_time:4708ms step_avg:36.22ms
step:131/2110 train_time:4742ms step_avg:36.20ms
step:132/2110 train_time:4774ms step_avg:36.17ms
step:133/2110 train_time:4808ms step_avg:36.15ms
step:134/2110 train_time:4841ms step_avg:36.12ms
step:135/2110 train_time:4874ms step_avg:36.10ms
step:136/2110 train_time:4907ms step_avg:36.08ms
step:137/2110 train_time:4940ms step_avg:36.06ms
step:138/2110 train_time:4973ms step_avg:36.03ms
step:139/2110 train_time:5006ms step_avg:36.01ms
step:140/2110 train_time:5039ms step_avg:35.99ms
step:141/2110 train_time:5072ms step_avg:35.97ms
step:142/2110 train_time:5105ms step_avg:35.95ms
step:143/2110 train_time:5139ms step_avg:35.93ms
step:144/2110 train_time:5197ms step_avg:36.09ms
step:145/2110 train_time:5221ms step_avg:36.01ms
step:146/2110 train_time:5254ms step_avg:35.99ms
step:147/2110 train_time:5282ms step_avg:35.93ms
step:148/2110 train_time:5307ms step_avg:35.86ms
step:149/2110 train_time:5337ms step_avg:35.82ms
step:150/2110 train_time:5370ms step_avg:35.80ms
step:151/2110 train_time:5403ms step_avg:35.78ms
step:152/2110 train_time:5435ms step_avg:35.76ms
step:153/2110 train_time:5469ms step_avg:35.74ms
step:154/2110 train_time:5502ms step_avg:35.73ms
step:155/2110 train_time:5535ms step_avg:35.71ms
step:156/2110 train_time:5568ms step_avg:35.69ms
step:157/2110 train_time:5601ms step_avg:35.67ms
step:158/2110 train_time:5633ms step_avg:35.65ms
step:159/2110 train_time:5667ms step_avg:35.64ms
step:160/2110 train_time:5700ms step_avg:35.63ms
step:161/2110 train_time:5733ms step_avg:35.61ms
step:162/2110 train_time:5766ms step_avg:35.59ms
step:163/2110 train_time:5799ms step_avg:35.58ms
step:164/2110 train_time:5832ms step_avg:35.56ms
step:165/2110 train_time:5865ms step_avg:35.55ms
step:166/2110 train_time:5898ms step_avg:35.53ms
step:167/2110 train_time:5931ms step_avg:35.52ms
step:168/2110 train_time:5965ms step_avg:35.50ms
step:169/2110 train_time:5997ms step_avg:35.49ms
step:170/2110 train_time:6031ms step_avg:35.48ms
step:171/2110 train_time:6063ms step_avg:35.46ms
step:172/2110 train_time:6096ms step_avg:35.44ms
step:173/2110 train_time:6129ms step_avg:35.43ms
step:174/2110 train_time:6162ms step_avg:35.41ms
step:175/2110 train_time:6195ms step_avg:35.40ms
step:176/2110 train_time:6228ms step_avg:35.39ms
step:177/2110 train_time:6261ms step_avg:35.37ms
step:178/2110 train_time:6293ms step_avg:35.36ms
step:179/2110 train_time:6327ms step_avg:35.34ms
step:180/2110 train_time:6359ms step_avg:35.33ms
step:181/2110 train_time:6393ms step_avg:35.32ms
step:182/2110 train_time:6426ms step_avg:35.31ms
step:183/2110 train_time:6459ms step_avg:35.29ms
step:184/2110 train_time:6491ms step_avg:35.28ms
step:185/2110 train_time:6524ms step_avg:35.27ms
step:186/2110 train_time:6557ms step_avg:35.25ms
step:187/2110 train_time:6590ms step_avg:35.24ms
step:188/2110 train_time:6623ms step_avg:35.23ms
step:189/2110 train_time:6657ms step_avg:35.22ms
step:190/2110 train_time:6689ms step_avg:35.21ms
step:191/2110 train_time:6723ms step_avg:35.20ms
step:192/2110 train_time:6755ms step_avg:35.18ms
step:193/2110 train_time:6789ms step_avg:35.18ms
step:194/2110 train_time:6821ms step_avg:35.16ms
step:195/2110 train_time:6855ms step_avg:35.15ms
step:196/2110 train_time:6888ms step_avg:35.14ms
step:197/2110 train_time:6921ms step_avg:35.13ms
step:198/2110 train_time:6954ms step_avg:35.12ms
step:199/2110 train_time:6987ms step_avg:35.11ms
step:200/2110 train_time:7020ms step_avg:35.10ms
step:201/2110 train_time:7053ms step_avg:35.09ms
step:202/2110 train_time:7086ms step_avg:35.08ms
step:203/2110 train_time:7120ms step_avg:35.07ms
step:204/2110 train_time:7152ms step_avg:35.06ms
step:205/2110 train_time:7185ms step_avg:35.05ms
step:206/2110 train_time:7218ms step_avg:35.04ms
step:207/2110 train_time:7251ms step_avg:35.03ms
step:208/2110 train_time:7284ms step_avg:35.02ms
step:209/2110 train_time:7317ms step_avg:35.01ms
step:210/2110 train_time:7350ms step_avg:35.00ms
step:211/2110 train_time:7383ms step_avg:34.99ms
step:212/2110 train_time:7416ms step_avg:34.98ms
step:213/2110 train_time:7449ms step_avg:34.97ms
step:214/2110 train_time:7482ms step_avg:34.96ms
step:215/2110 train_time:7515ms step_avg:34.95ms
step:216/2110 train_time:7547ms step_avg:34.94ms
step:217/2110 train_time:7581ms step_avg:34.93ms
step:218/2110 train_time:7613ms step_avg:34.92ms
step:219/2110 train_time:7647ms step_avg:34.92ms
step:220/2110 train_time:7680ms step_avg:34.91ms
step:221/2110 train_time:7713ms step_avg:34.90ms
step:222/2110 train_time:7745ms step_avg:34.89ms
step:223/2110 train_time:7779ms step_avg:34.88ms
step:224/2110 train_time:7812ms step_avg:34.87ms
step:225/2110 train_time:7845ms step_avg:34.87ms
step:226/2110 train_time:7878ms step_avg:34.86ms
step:227/2110 train_time:7911ms step_avg:34.85ms
step:228/2110 train_time:7944ms step_avg:34.84ms
step:229/2110 train_time:7978ms step_avg:34.84ms
step:230/2110 train_time:8011ms step_avg:34.83ms
step:231/2110 train_time:8044ms step_avg:34.82ms
step:232/2110 train_time:8076ms step_avg:34.81ms
step:233/2110 train_time:8110ms step_avg:34.80ms
step:234/2110 train_time:8142ms step_avg:34.79ms
step:235/2110 train_time:8175ms step_avg:34.79ms
step:236/2110 train_time:8208ms step_avg:34.78ms
step:237/2110 train_time:8242ms step_avg:34.78ms
step:238/2110 train_time:8274ms step_avg:34.77ms
step:239/2110 train_time:8308ms step_avg:34.76ms
step:240/2110 train_time:8340ms step_avg:34.75ms
step:241/2110 train_time:8373ms step_avg:34.74ms
step:242/2110 train_time:8406ms step_avg:34.74ms
step:243/2110 train_time:8439ms step_avg:34.73ms
step:244/2110 train_time:8472ms step_avg:34.72ms
step:245/2110 train_time:8505ms step_avg:34.71ms
step:246/2110 train_time:8538ms step_avg:34.71ms
step:247/2110 train_time:8571ms step_avg:34.70ms
step:248/2110 train_time:8604ms step_avg:34.69ms
step:249/2110 train_time:8637ms step_avg:34.69ms
step:250/2110 train_time:8670ms step_avg:34.68ms
step:250/2110 val_loss:4.2995 train_time:8706ms step_avg:34.82ms
step:251/2110 train_time:8728ms step_avg:34.77ms
step:252/2110 train_time:8751ms step_avg:34.73ms
step:253/2110 train_time:8776ms step_avg:34.69ms
step:254/2110 train_time:8809ms step_avg:34.68ms
step:255/2110 train_time:8844ms step_avg:34.68ms
step:256/2110 train_time:8878ms step_avg:34.68ms
step:257/2110 train_time:8912ms step_avg:34.68ms
step:258/2110 train_time:8947ms step_avg:34.68ms
step:259/2110 train_time:8979ms step_avg:34.67ms
step:260/2110 train_time:9012ms step_avg:34.66ms
step:261/2110 train_time:9047ms step_avg:34.66ms
step:262/2110 train_time:9078ms step_avg:34.65ms
step:263/2110 train_time:9111ms step_avg:34.64ms
step:264/2110 train_time:9143ms step_avg:34.63ms
step:265/2110 train_time:9177ms step_avg:34.63ms
step:266/2110 train_time:9209ms step_avg:34.62ms
step:267/2110 train_time:9242ms step_avg:34.62ms
step:268/2110 train_time:9275ms step_avg:34.61ms
step:269/2110 train_time:9308ms step_avg:34.60ms
step:270/2110 train_time:9340ms step_avg:34.59ms
step:271/2110 train_time:9374ms step_avg:34.59ms
step:272/2110 train_time:9406ms step_avg:34.58ms
step:273/2110 train_time:9439ms step_avg:34.57ms
step:274/2110 train_time:9472ms step_avg:34.57ms
step:275/2110 train_time:9505ms step_avg:34.56ms
step:276/2110 train_time:9537ms step_avg:34.55ms
step:277/2110 train_time:9570ms step_avg:34.55ms
step:278/2110 train_time:9602ms step_avg:34.54ms
step:279/2110 train_time:9636ms step_avg:34.54ms
step:280/2110 train_time:9668ms step_avg:34.53ms
step:281/2110 train_time:9701ms step_avg:34.52ms
step:282/2110 train_time:9734ms step_avg:34.52ms
step:283/2110 train_time:9768ms step_avg:34.52ms
step:284/2110 train_time:9801ms step_avg:34.51ms
step:285/2110 train_time:9835ms step_avg:34.51ms
step:286/2110 train_time:9868ms step_avg:34.50ms
step:287/2110 train_time:9902ms step_avg:34.50ms
step:288/2110 train_time:9935ms step_avg:34.50ms
step:289/2110 train_time:9968ms step_avg:34.49ms
step:290/2110 train_time:10001ms step_avg:34.49ms
step:291/2110 train_time:10035ms step_avg:34.48ms
step:292/2110 train_time:10068ms step_avg:34.48ms
step:293/2110 train_time:10101ms step_avg:34.47ms
step:294/2110 train_time:10133ms step_avg:34.47ms
step:295/2110 train_time:10167ms step_avg:34.46ms
step:296/2110 train_time:10199ms step_avg:34.46ms
step:297/2110 train_time:10233ms step_avg:34.46ms
step:298/2110 train_time:10266ms step_avg:34.45ms
step:299/2110 train_time:10298ms step_avg:34.44ms
step:300/2110 train_time:10331ms step_avg:34.44ms
step:301/2110 train_time:10364ms step_avg:34.43ms
step:302/2110 train_time:10397ms step_avg:34.43ms
step:303/2110 train_time:10430ms step_avg:34.42ms
step:304/2110 train_time:10463ms step_avg:34.42ms
step:305/2110 train_time:10496ms step_avg:34.41ms
step:306/2110 train_time:10528ms step_avg:34.41ms
step:307/2110 train_time:10561ms step_avg:34.40ms
step:308/2110 train_time:10595ms step_avg:34.40ms
step:309/2110 train_time:10627ms step_avg:34.39ms
step:310/2110 train_time:10660ms step_avg:34.39ms
step:311/2110 train_time:10693ms step_avg:34.38ms
step:312/2110 train_time:10726ms step_avg:34.38ms
step:313/2110 train_time:10759ms step_avg:34.37ms
step:314/2110 train_time:10792ms step_avg:34.37ms
step:315/2110 train_time:10826ms step_avg:34.37ms
step:316/2110 train_time:10859ms step_avg:34.36ms
step:317/2110 train_time:10892ms step_avg:34.36ms
step:318/2110 train_time:10925ms step_avg:34.35ms
step:319/2110 train_time:10958ms step_avg:34.35ms
step:320/2110 train_time:10991ms step_avg:34.35ms
step:321/2110 train_time:11024ms step_avg:34.34ms
step:322/2110 train_time:11057ms step_avg:34.34ms
step:323/2110 train_time:11091ms step_avg:34.34ms
step:324/2110 train_time:11123ms step_avg:34.33ms
step:325/2110 train_time:11157ms step_avg:34.33ms
step:326/2110 train_time:11189ms step_avg:34.32ms
step:327/2110 train_time:11223ms step_avg:34.32ms
step:328/2110 train_time:11255ms step_avg:34.32ms
step:329/2110 train_time:11289ms step_avg:34.31ms
step:330/2110 train_time:11321ms step_avg:34.31ms
step:331/2110 train_time:11354ms step_avg:34.30ms
step:332/2110 train_time:11387ms step_avg:34.30ms
step:333/2110 train_time:11420ms step_avg:34.30ms
step:334/2110 train_time:11454ms step_avg:34.29ms
step:335/2110 train_time:11487ms step_avg:34.29ms
step:336/2110 train_time:11520ms step_avg:34.28ms
step:337/2110 train_time:11552ms step_avg:34.28ms
step:338/2110 train_time:11585ms step_avg:34.27ms
step:339/2110 train_time:11618ms step_avg:34.27ms
step:340/2110 train_time:11652ms step_avg:34.27ms
step:341/2110 train_time:11685ms step_avg:34.27ms
step:342/2110 train_time:11718ms step_avg:34.26ms
step:343/2110 train_time:11751ms step_avg:34.26ms
step:344/2110 train_time:11784ms step_avg:34.26ms
step:345/2110 train_time:11817ms step_avg:34.25ms
step:346/2110 train_time:11850ms step_avg:34.25ms
step:347/2110 train_time:11884ms step_avg:34.25ms
step:348/2110 train_time:11916ms step_avg:34.24ms
step:349/2110 train_time:11950ms step_avg:34.24ms
step:350/2110 train_time:11982ms step_avg:34.24ms
step:351/2110 train_time:12016ms step_avg:34.23ms
step:352/2110 train_time:12049ms step_avg:34.23ms
step:353/2110 train_time:12082ms step_avg:34.23ms
step:354/2110 train_time:12115ms step_avg:34.22ms
step:355/2110 train_time:12149ms step_avg:34.22ms
step:356/2110 train_time:12181ms step_avg:34.22ms
step:357/2110 train_time:12215ms step_avg:34.22ms
step:358/2110 train_time:12248ms step_avg:34.21ms
step:359/2110 train_time:12281ms step_avg:34.21ms
step:360/2110 train_time:12314ms step_avg:34.21ms
step:361/2110 train_time:12347ms step_avg:34.20ms
step:362/2110 train_time:12379ms step_avg:34.20ms
step:363/2110 train_time:12413ms step_avg:34.19ms
step:364/2110 train_time:12445ms step_avg:34.19ms
step:365/2110 train_time:12479ms step_avg:34.19ms
step:366/2110 train_time:12512ms step_avg:34.18ms
step:367/2110 train_time:12545ms step_avg:34.18ms
step:368/2110 train_time:12577ms step_avg:34.18ms
step:369/2110 train_time:12611ms step_avg:34.17ms
step:370/2110 train_time:12643ms step_avg:34.17ms
step:371/2110 train_time:12676ms step_avg:34.17ms
step:372/2110 train_time:12709ms step_avg:34.16ms
step:373/2110 train_time:12742ms step_avg:34.16ms
step:374/2110 train_time:12775ms step_avg:34.16ms
step:375/2110 train_time:12809ms step_avg:34.16ms
step:376/2110 train_time:12842ms step_avg:34.15ms
step:377/2110 train_time:12875ms step_avg:34.15ms
step:378/2110 train_time:12907ms step_avg:34.15ms
step:379/2110 train_time:12941ms step_avg:34.14ms
step:380/2110 train_time:12973ms step_avg:34.14ms
step:381/2110 train_time:13007ms step_avg:34.14ms
step:382/2110 train_time:13039ms step_avg:34.13ms
step:383/2110 train_time:13073ms step_avg:34.13ms
step:384/2110 train_time:13105ms step_avg:34.13ms
step:385/2110 train_time:13139ms step_avg:34.13ms
step:386/2110 train_time:13172ms step_avg:34.12ms
step:387/2110 train_time:13205ms step_avg:34.12ms
step:388/2110 train_time:13238ms step_avg:34.12ms
step:389/2110 train_time:13271ms step_avg:34.12ms
step:390/2110 train_time:13304ms step_avg:34.11ms
step:391/2110 train_time:13337ms step_avg:34.11ms
step:392/2110 train_time:13370ms step_avg:34.11ms
step:393/2110 train_time:13403ms step_avg:34.10ms
step:394/2110 train_time:13435ms step_avg:34.10ms
step:395/2110 train_time:13469ms step_avg:34.10ms
step:396/2110 train_time:13501ms step_avg:34.09ms
step:397/2110 train_time:13535ms step_avg:34.09ms
step:398/2110 train_time:13567ms step_avg:34.09ms
step:399/2110 train_time:13600ms step_avg:34.09ms
step:400/2110 train_time:13633ms step_avg:34.08ms
step:401/2110 train_time:13666ms step_avg:34.08ms
step:402/2110 train_time:13699ms step_avg:34.08ms
step:403/2110 train_time:13732ms step_avg:34.08ms
step:404/2110 train_time:13765ms step_avg:34.07ms
step:405/2110 train_time:13799ms step_avg:34.07ms
step:406/2110 train_time:13831ms step_avg:34.07ms
step:407/2110 train_time:13865ms step_avg:34.07ms
step:408/2110 train_time:13898ms step_avg:34.06ms
step:409/2110 train_time:13931ms step_avg:34.06ms
step:410/2110 train_time:13964ms step_avg:34.06ms
step:411/2110 train_time:13997ms step_avg:34.06ms
step:412/2110 train_time:14030ms step_avg:34.05ms
step:413/2110 train_time:14063ms step_avg:34.05ms
step:414/2110 train_time:14096ms step_avg:34.05ms
step:415/2110 train_time:14129ms step_avg:34.05ms
step:416/2110 train_time:14162ms step_avg:34.04ms
step:417/2110 train_time:14195ms step_avg:34.04ms
step:418/2110 train_time:14228ms step_avg:34.04ms
step:419/2110 train_time:14261ms step_avg:34.04ms
step:420/2110 train_time:14294ms step_avg:34.03ms
step:421/2110 train_time:14327ms step_avg:34.03ms
step:422/2110 train_time:14360ms step_avg:34.03ms
step:423/2110 train_time:14393ms step_avg:34.03ms
step:424/2110 train_time:14426ms step_avg:34.02ms
step:425/2110 train_time:14459ms step_avg:34.02ms
step:426/2110 train_time:14493ms step_avg:34.02ms
step:427/2110 train_time:14525ms step_avg:34.02ms
step:428/2110 train_time:14558ms step_avg:34.01ms
step:429/2110 train_time:14591ms step_avg:34.01ms
step:430/2110 train_time:14624ms step_avg:34.01ms
step:431/2110 train_time:14657ms step_avg:34.01ms
step:432/2110 train_time:14690ms step_avg:34.01ms
step:433/2110 train_time:14723ms step_avg:34.00ms
step:434/2110 train_time:14756ms step_avg:34.00ms
step:435/2110 train_time:14789ms step_avg:34.00ms
step:436/2110 train_time:14821ms step_avg:33.99ms
step:437/2110 train_time:14855ms step_avg:33.99ms
step:438/2110 train_time:14888ms step_avg:33.99ms
step:439/2110 train_time:14921ms step_avg:33.99ms
step:440/2110 train_time:14954ms step_avg:33.99ms
step:441/2110 train_time:14988ms step_avg:33.99ms
step:442/2110 train_time:15021ms step_avg:33.98ms
step:443/2110 train_time:15054ms step_avg:33.98ms
step:444/2110 train_time:15087ms step_avg:33.98ms
step:445/2110 train_time:15120ms step_avg:33.98ms
step:446/2110 train_time:15152ms step_avg:33.97ms
step:447/2110 train_time:15186ms step_avg:33.97ms
step:448/2110 train_time:15219ms step_avg:33.97ms
step:449/2110 train_time:15252ms step_avg:33.97ms
step:450/2110 train_time:15285ms step_avg:33.97ms
step:451/2110 train_time:15318ms step_avg:33.96ms
step:452/2110 train_time:15351ms step_avg:33.96ms
step:453/2110 train_time:15384ms step_avg:33.96ms
step:454/2110 train_time:15417ms step_avg:33.96ms
step:455/2110 train_time:15450ms step_avg:33.96ms
step:456/2110 train_time:15482ms step_avg:33.95ms
step:457/2110 train_time:15515ms step_avg:33.95ms
step:458/2110 train_time:15548ms step_avg:33.95ms
step:459/2110 train_time:15582ms step_avg:33.95ms
step:460/2110 train_time:15615ms step_avg:33.95ms
step:461/2110 train_time:15648ms step_avg:33.94ms
step:462/2110 train_time:15681ms step_avg:33.94ms
step:463/2110 train_time:15714ms step_avg:33.94ms
step:464/2110 train_time:15746ms step_avg:33.94ms
step:465/2110 train_time:15780ms step_avg:33.93ms
step:466/2110 train_time:15813ms step_avg:33.93ms
step:467/2110 train_time:15846ms step_avg:33.93ms
step:468/2110 train_time:15879ms step_avg:33.93ms
step:469/2110 train_time:15913ms step_avg:33.93ms
step:470/2110 train_time:15945ms step_avg:33.93ms
step:471/2110 train_time:15979ms step_avg:33.92ms
step:472/2110 train_time:16012ms step_avg:33.92ms
step:473/2110 train_time:16045ms step_avg:33.92ms
step:474/2110 train_time:16077ms step_avg:33.92ms
step:475/2110 train_time:16111ms step_avg:33.92ms
step:476/2110 train_time:16143ms step_avg:33.91ms
step:477/2110 train_time:16177ms step_avg:33.91ms
step:478/2110 train_time:16209ms step_avg:33.91ms
step:479/2110 train_time:16243ms step_avg:33.91ms
step:480/2110 train_time:16276ms step_avg:33.91ms
step:481/2110 train_time:16309ms step_avg:33.91ms
step:482/2110 train_time:16342ms step_avg:33.90ms
step:483/2110 train_time:16375ms step_avg:33.90ms
step:484/2110 train_time:16407ms step_avg:33.90ms
step:485/2110 train_time:16441ms step_avg:33.90ms
step:486/2110 train_time:16473ms step_avg:33.90ms
step:487/2110 train_time:16507ms step_avg:33.90ms
step:488/2110 train_time:16540ms step_avg:33.89ms
step:489/2110 train_time:16573ms step_avg:33.89ms
step:490/2110 train_time:16606ms step_avg:33.89ms
step:491/2110 train_time:16639ms step_avg:33.89ms
step:492/2110 train_time:16671ms step_avg:33.89ms
step:493/2110 train_time:16705ms step_avg:33.88ms
step:494/2110 train_time:16738ms step_avg:33.88ms
step:495/2110 train_time:16771ms step_avg:33.88ms
step:496/2110 train_time:16804ms step_avg:33.88ms
step:497/2110 train_time:16837ms step_avg:33.88ms
step:498/2110 train_time:16870ms step_avg:33.88ms
step:499/2110 train_time:16903ms step_avg:33.87ms
step:500/2110 train_time:16936ms step_avg:33.87ms
step:500/2110 val_loss:4.0292 train_time:16972ms step_avg:33.94ms
step:501/2110 train_time:16997ms step_avg:33.93ms
step:502/2110 train_time:17020ms step_avg:33.90ms
step:503/2110 train_time:17042ms step_avg:33.88ms
step:504/2110 train_time:17075ms step_avg:33.88ms
step:505/2110 train_time:17110ms step_avg:33.88ms
step:506/2110 train_time:17143ms step_avg:33.88ms
step:507/2110 train_time:17177ms step_avg:33.88ms
step:508/2110 train_time:17210ms step_avg:33.88ms
step:509/2110 train_time:17244ms step_avg:33.88ms
step:510/2110 train_time:17277ms step_avg:33.88ms
step:511/2110 train_time:17310ms step_avg:33.87ms
step:512/2110 train_time:17342ms step_avg:33.87ms
step:513/2110 train_time:17376ms step_avg:33.87ms
step:514/2110 train_time:17408ms step_avg:33.87ms
step:515/2110 train_time:17441ms step_avg:33.87ms
step:516/2110 train_time:17474ms step_avg:33.86ms
step:517/2110 train_time:17507ms step_avg:33.86ms
step:518/2110 train_time:17539ms step_avg:33.86ms
step:519/2110 train_time:17572ms step_avg:33.86ms
step:520/2110 train_time:17605ms step_avg:33.86ms
step:521/2110 train_time:17638ms step_avg:33.85ms
step:522/2110 train_time:17670ms step_avg:33.85ms
step:523/2110 train_time:17703ms step_avg:33.85ms
step:524/2110 train_time:17736ms step_avg:33.85ms
step:525/2110 train_time:17769ms step_avg:33.84ms
step:526/2110 train_time:17801ms step_avg:33.84ms
step:527/2110 train_time:17834ms step_avg:33.84ms
step:528/2110 train_time:17867ms step_avg:33.84ms
step:529/2110 train_time:17900ms step_avg:33.84ms
step:530/2110 train_time:17933ms step_avg:33.84ms
step:531/2110 train_time:17966ms step_avg:33.84ms
step:532/2110 train_time:18000ms step_avg:33.83ms
step:533/2110 train_time:18033ms step_avg:33.83ms
step:534/2110 train_time:18066ms step_avg:33.83ms
step:535/2110 train_time:18100ms step_avg:33.83ms
step:536/2110 train_time:18135ms step_avg:33.83ms
step:537/2110 train_time:18167ms step_avg:33.83ms
step:538/2110 train_time:18200ms step_avg:33.83ms
step:539/2110 train_time:18233ms step_avg:33.83ms
step:540/2110 train_time:18266ms step_avg:33.83ms
step:541/2110 train_time:18299ms step_avg:33.82ms
step:542/2110 train_time:18332ms step_avg:33.82ms
step:543/2110 train_time:18365ms step_avg:33.82ms
step:544/2110 train_time:18399ms step_avg:33.82ms
step:545/2110 train_time:18432ms step_avg:33.82ms
step:546/2110 train_time:18464ms step_avg:33.82ms
step:547/2110 train_time:18497ms step_avg:33.82ms
step:548/2110 train_time:18530ms step_avg:33.81ms
step:549/2110 train_time:18563ms step_avg:33.81ms
step:550/2110 train_time:18595ms step_avg:33.81ms
step:551/2110 train_time:18629ms step_avg:33.81ms
step:552/2110 train_time:18661ms step_avg:33.81ms
step:553/2110 train_time:18694ms step_avg:33.81ms
step:554/2110 train_time:18727ms step_avg:33.80ms
step:555/2110 train_time:18760ms step_avg:33.80ms
step:556/2110 train_time:18793ms step_avg:33.80ms
step:557/2110 train_time:18826ms step_avg:33.80ms
step:558/2110 train_time:18859ms step_avg:33.80ms
step:559/2110 train_time:18892ms step_avg:33.80ms
step:560/2110 train_time:18925ms step_avg:33.79ms
step:561/2110 train_time:18958ms step_avg:33.79ms
step:562/2110 train_time:18991ms step_avg:33.79ms
step:563/2110 train_time:19025ms step_avg:33.79ms
step:564/2110 train_time:19058ms step_avg:33.79ms
step:565/2110 train_time:19091ms step_avg:33.79ms
step:566/2110 train_time:19124ms step_avg:33.79ms
step:567/2110 train_time:19158ms step_avg:33.79ms
step:568/2110 train_time:19190ms step_avg:33.79ms
step:569/2110 train_time:19224ms step_avg:33.79ms
step:570/2110 train_time:19257ms step_avg:33.78ms
step:571/2110 train_time:19290ms step_avg:33.78ms
step:572/2110 train_time:19323ms step_avg:33.78ms
step:573/2110 train_time:19356ms step_avg:33.78ms
step:574/2110 train_time:19389ms step_avg:33.78ms
step:575/2110 train_time:19422ms step_avg:33.78ms
step:576/2110 train_time:19455ms step_avg:33.78ms
step:577/2110 train_time:19488ms step_avg:33.78ms
step:578/2110 train_time:19521ms step_avg:33.77ms
step:579/2110 train_time:19554ms step_avg:33.77ms
step:580/2110 train_time:19587ms step_avg:33.77ms
step:581/2110 train_time:19620ms step_avg:33.77ms
step:582/2110 train_time:19653ms step_avg:33.77ms
step:583/2110 train_time:19686ms step_avg:33.77ms
step:584/2110 train_time:19718ms step_avg:33.76ms
step:585/2110 train_time:19751ms step_avg:33.76ms
step:586/2110 train_time:19784ms step_avg:33.76ms
step:587/2110 train_time:19817ms step_avg:33.76ms
step:588/2110 train_time:19850ms step_avg:33.76ms
step:589/2110 train_time:19883ms step_avg:33.76ms
step:590/2110 train_time:19915ms step_avg:33.76ms
step:591/2110 train_time:19949ms step_avg:33.75ms
step:592/2110 train_time:19982ms step_avg:33.75ms
step:593/2110 train_time:20015ms step_avg:33.75ms
step:594/2110 train_time:20048ms step_avg:33.75ms
step:595/2110 train_time:20081ms step_avg:33.75ms
step:596/2110 train_time:20114ms step_avg:33.75ms
step:597/2110 train_time:20147ms step_avg:33.75ms
step:598/2110 train_time:20180ms step_avg:33.75ms
step:599/2110 train_time:20213ms step_avg:33.75ms
step:600/2110 train_time:20246ms step_avg:33.74ms
step:601/2110 train_time:20280ms step_avg:33.74ms
step:602/2110 train_time:20313ms step_avg:33.74ms
step:603/2110 train_time:20346ms step_avg:33.74ms
step:604/2110 train_time:20379ms step_avg:33.74ms
step:605/2110 train_time:20412ms step_avg:33.74ms
step:606/2110 train_time:20445ms step_avg:33.74ms
step:607/2110 train_time:20478ms step_avg:33.74ms
step:608/2110 train_time:20511ms step_avg:33.73ms
step:609/2110 train_time:20544ms step_avg:33.73ms
step:610/2110 train_time:20577ms step_avg:33.73ms
step:611/2110 train_time:20610ms step_avg:33.73ms
step:612/2110 train_time:20643ms step_avg:33.73ms
step:613/2110 train_time:20676ms step_avg:33.73ms
step:614/2110 train_time:20708ms step_avg:33.73ms
step:615/2110 train_time:20741ms step_avg:33.73ms
step:616/2110 train_time:20774ms step_avg:33.72ms
step:617/2110 train_time:20807ms step_avg:33.72ms
step:618/2110 train_time:20840ms step_avg:33.72ms
step:619/2110 train_time:20873ms step_avg:33.72ms
step:620/2110 train_time:20906ms step_avg:33.72ms
step:621/2110 train_time:20939ms step_avg:33.72ms
step:622/2110 train_time:20972ms step_avg:33.72ms
step:623/2110 train_time:21018ms step_avg:33.74ms
step:624/2110 train_time:21048ms step_avg:33.73ms
step:625/2110 train_time:21076ms step_avg:33.72ms
step:626/2110 train_time:21109ms step_avg:33.72ms
step:627/2110 train_time:21148ms step_avg:33.73ms
step:628/2110 train_time:21184ms step_avg:33.73ms
step:629/2110 train_time:21220ms step_avg:33.74ms
step:630/2110 train_time:21255ms step_avg:33.74ms
step:631/2110 train_time:21290ms step_avg:33.74ms
step:632/2110 train_time:21324ms step_avg:33.74ms
step:633/2110 train_time:21359ms step_avg:33.74ms
step:634/2110 train_time:21394ms step_avg:33.74ms
step:635/2110 train_time:21433ms step_avg:33.75ms
step:636/2110 train_time:21469ms step_avg:33.76ms
step:637/2110 train_time:21508ms step_avg:33.76ms
step:638/2110 train_time:21543ms step_avg:33.77ms
step:639/2110 train_time:21577ms step_avg:33.77ms
step:640/2110 train_time:21610ms step_avg:33.76ms
step:641/2110 train_time:21643ms step_avg:33.76ms
step:642/2110 train_time:21675ms step_avg:33.76ms
step:643/2110 train_time:21708ms step_avg:33.76ms
step:644/2110 train_time:21741ms step_avg:33.76ms
step:645/2110 train_time:21774ms step_avg:33.76ms
step:646/2110 train_time:21806ms step_avg:33.76ms
step:647/2110 train_time:21839ms step_avg:33.75ms
step:648/2110 train_time:21872ms step_avg:33.75ms
step:649/2110 train_time:21905ms step_avg:33.75ms
step:650/2110 train_time:21937ms step_avg:33.75ms
step:651/2110 train_time:21970ms step_avg:33.75ms
step:652/2110 train_time:22003ms step_avg:33.75ms
step:653/2110 train_time:22036ms step_avg:33.75ms
step:654/2110 train_time:22069ms step_avg:33.74ms
step:655/2110 train_time:22102ms step_avg:33.74ms
step:656/2110 train_time:22135ms step_avg:33.74ms
step:657/2110 train_time:22168ms step_avg:33.74ms
step:658/2110 train_time:22201ms step_avg:33.74ms
step:659/2110 train_time:22234ms step_avg:33.74ms
step:660/2110 train_time:22267ms step_avg:33.74ms
step:661/2110 train_time:22301ms step_avg:33.74ms
step:662/2110 train_time:22334ms step_avg:33.74ms
step:663/2110 train_time:22367ms step_avg:33.74ms
step:664/2110 train_time:22401ms step_avg:33.74ms
step:665/2110 train_time:22435ms step_avg:33.74ms
step:666/2110 train_time:22468ms step_avg:33.74ms
step:667/2110 train_time:22501ms step_avg:33.73ms
step:668/2110 train_time:22534ms step_avg:33.73ms
step:669/2110 train_time:22567ms step_avg:33.73ms
step:670/2110 train_time:22600ms step_avg:33.73ms
step:671/2110 train_time:22634ms step_avg:33.73ms
step:672/2110 train_time:22667ms step_avg:33.73ms
step:673/2110 train_time:22700ms step_avg:33.73ms
step:674/2110 train_time:22732ms step_avg:33.73ms
step:675/2110 train_time:22766ms step_avg:33.73ms
step:676/2110 train_time:22798ms step_avg:33.73ms
step:677/2110 train_time:22832ms step_avg:33.72ms
step:678/2110 train_time:22864ms step_avg:33.72ms
step:679/2110 train_time:22898ms step_avg:33.72ms
step:680/2110 train_time:22930ms step_avg:33.72ms
step:681/2110 train_time:22964ms step_avg:33.72ms
step:682/2110 train_time:22997ms step_avg:33.72ms
step:683/2110 train_time:23030ms step_avg:33.72ms
step:684/2110 train_time:23062ms step_avg:33.72ms
step:685/2110 train_time:23095ms step_avg:33.72ms
step:686/2110 train_time:23128ms step_avg:33.71ms
step:687/2110 train_time:23161ms step_avg:33.71ms
step:688/2110 train_time:23195ms step_avg:33.71ms
step:689/2110 train_time:23228ms step_avg:33.71ms
step:690/2110 train_time:23260ms step_avg:33.71ms
step:691/2110 train_time:23294ms step_avg:33.71ms
step:692/2110 train_time:23352ms step_avg:33.75ms
step:693/2110 train_time:23412ms step_avg:33.78ms
step:694/2110 train_time:23477ms step_avg:33.83ms
step:695/2110 train_time:23531ms step_avg:33.86ms
step:696/2110 train_time:23590ms step_avg:33.89ms
step:697/2110 train_time:23650ms step_avg:33.93ms
step:698/2110 train_time:23709ms step_avg:33.97ms
step:699/2110 train_time:23768ms step_avg:34.00ms
step:700/2110 train_time:23827ms step_avg:34.04ms
step:701/2110 train_time:23886ms step_avg:34.07ms
step:702/2110 train_time:23944ms step_avg:34.11ms
step:703/2110 train_time:24004ms step_avg:34.14ms
step:704/2110 train_time:24062ms step_avg:34.18ms
step:705/2110 train_time:24123ms step_avg:34.22ms
step:706/2110 train_time:24182ms step_avg:34.25ms
step:707/2110 train_time:24243ms step_avg:34.29ms
step:708/2110 train_time:24302ms step_avg:34.32ms
step:709/2110 train_time:24363ms step_avg:34.36ms
step:710/2110 train_time:24423ms step_avg:34.40ms
step:711/2110 train_time:24483ms step_avg:34.43ms
step:712/2110 train_time:24542ms step_avg:34.47ms
step:713/2110 train_time:24603ms step_avg:34.51ms
step:714/2110 train_time:24663ms step_avg:34.54ms
step:715/2110 train_time:24723ms step_avg:34.58ms
step:716/2110 train_time:24782ms step_avg:34.61ms
step:717/2110 train_time:24841ms step_avg:34.65ms
step:718/2110 train_time:24900ms step_avg:34.68ms
step:719/2110 train_time:24960ms step_avg:34.71ms
step:720/2110 train_time:25018ms step_avg:34.75ms
step:721/2110 train_time:25078ms step_avg:34.78ms
step:722/2110 train_time:25137ms step_avg:34.82ms
step:723/2110 train_time:25196ms step_avg:34.85ms
step:724/2110 train_time:25255ms step_avg:34.88ms
step:725/2110 train_time:25315ms step_avg:34.92ms
step:726/2110 train_time:25373ms step_avg:34.95ms
step:727/2110 train_time:25433ms step_avg:34.98ms
step:728/2110 train_time:25492ms step_avg:35.02ms
step:729/2110 train_time:25552ms step_avg:35.05ms
step:730/2110 train_time:25611ms step_avg:35.08ms
step:731/2110 train_time:25671ms step_avg:35.12ms
step:732/2110 train_time:25729ms step_avg:35.15ms
step:733/2110 train_time:25789ms step_avg:35.18ms
step:734/2110 train_time:25847ms step_avg:35.21ms
step:735/2110 train_time:25908ms step_avg:35.25ms
step:736/2110 train_time:25966ms step_avg:35.28ms
step:737/2110 train_time:26027ms step_avg:35.31ms
step:738/2110 train_time:26085ms step_avg:35.35ms
step:739/2110 train_time:26145ms step_avg:35.38ms
step:740/2110 train_time:26204ms step_avg:35.41ms
step:741/2110 train_time:26265ms step_avg:35.45ms
step:742/2110 train_time:26324ms step_avg:35.48ms
step:743/2110 train_time:26385ms step_avg:35.51ms
step:744/2110 train_time:26443ms step_avg:35.54ms
step:745/2110 train_time:26503ms step_avg:35.58ms
step:746/2110 train_time:26563ms step_avg:35.61ms
step:747/2110 train_time:26624ms step_avg:35.64ms
step:748/2110 train_time:26683ms step_avg:35.67ms
step:749/2110 train_time:26743ms step_avg:35.71ms
step:750/2110 train_time:26802ms step_avg:35.74ms
step:750/2110 val_loss:3.9064 train_time:26863ms step_avg:35.82ms
step:751/2110 train_time:26887ms step_avg:35.80ms
step:752/2110 train_time:26922ms step_avg:35.80ms
step:753/2110 train_time:26986ms step_avg:35.84ms
step:754/2110 train_time:27051ms step_avg:35.88ms
step:755/2110 train_time:27110ms step_avg:35.91ms
step:756/2110 train_time:27168ms step_avg:35.94ms
step:757/2110 train_time:27228ms step_avg:35.97ms
step:758/2110 train_time:27286ms step_avg:36.00ms
step:759/2110 train_time:27345ms step_avg:36.03ms
step:760/2110 train_time:27403ms step_avg:36.06ms
step:761/2110 train_time:27462ms step_avg:36.09ms
step:762/2110 train_time:27520ms step_avg:36.12ms
step:763/2110 train_time:27579ms step_avg:36.15ms
step:764/2110 train_time:27638ms step_avg:36.17ms
step:765/2110 train_time:27697ms step_avg:36.21ms
step:766/2110 train_time:27755ms step_avg:36.23ms
step:767/2110 train_time:27815ms step_avg:36.27ms
step:768/2110 train_time:27874ms step_avg:36.29ms
step:769/2110 train_time:27936ms step_avg:36.33ms
step:770/2110 train_time:27995ms step_avg:36.36ms
step:771/2110 train_time:28057ms step_avg:36.39ms
step:772/2110 train_time:28117ms step_avg:36.42ms
step:773/2110 train_time:28177ms step_avg:36.45ms
step:774/2110 train_time:28236ms step_avg:36.48ms
step:775/2110 train_time:28296ms step_avg:36.51ms
step:776/2110 train_time:28354ms step_avg:36.54ms
step:777/2110 train_time:28413ms step_avg:36.57ms
step:778/2110 train_time:28471ms step_avg:36.60ms
step:779/2110 train_time:28530ms step_avg:36.62ms
step:780/2110 train_time:28588ms step_avg:36.65ms
step:781/2110 train_time:28647ms step_avg:36.68ms
step:782/2110 train_time:28705ms step_avg:36.71ms
step:783/2110 train_time:28765ms step_avg:36.74ms
step:784/2110 train_time:28824ms step_avg:36.76ms
step:785/2110 train_time:28884ms step_avg:36.80ms
step:786/2110 train_time:28943ms step_avg:36.82ms
step:787/2110 train_time:29005ms step_avg:36.85ms
step:788/2110 train_time:29064ms step_avg:36.88ms
step:789/2110 train_time:29125ms step_avg:36.91ms
step:790/2110 train_time:29184ms step_avg:36.94ms
step:791/2110 train_time:29244ms step_avg:36.97ms
step:792/2110 train_time:29302ms step_avg:37.00ms
step:793/2110 train_time:29362ms step_avg:37.03ms
step:794/2110 train_time:29421ms step_avg:37.05ms
step:795/2110 train_time:29481ms step_avg:37.08ms
step:796/2110 train_time:29540ms step_avg:37.11ms
step:797/2110 train_time:29599ms step_avg:37.14ms
step:798/2110 train_time:29657ms step_avg:37.16ms
step:799/2110 train_time:29716ms step_avg:37.19ms
step:800/2110 train_time:29776ms step_avg:37.22ms
step:801/2110 train_time:29835ms step_avg:37.25ms
step:802/2110 train_time:29893ms step_avg:37.27ms
step:803/2110 train_time:29953ms step_avg:37.30ms
step:804/2110 train_time:30012ms step_avg:37.33ms
step:805/2110 train_time:30072ms step_avg:37.36ms
step:806/2110 train_time:30130ms step_avg:37.38ms
step:807/2110 train_time:30191ms step_avg:37.41ms
step:808/2110 train_time:30250ms step_avg:37.44ms
step:809/2110 train_time:30310ms step_avg:37.47ms
step:810/2110 train_time:30368ms step_avg:37.49ms
step:811/2110 train_time:30428ms step_avg:37.52ms
step:812/2110 train_time:30486ms step_avg:37.54ms
step:813/2110 train_time:30545ms step_avg:37.57ms
step:814/2110 train_time:30603ms step_avg:37.60ms
step:815/2110 train_time:30663ms step_avg:37.62ms
step:816/2110 train_time:30721ms step_avg:37.65ms
step:817/2110 train_time:30782ms step_avg:37.68ms
step:818/2110 train_time:30841ms step_avg:37.70ms
step:819/2110 train_time:30901ms step_avg:37.73ms
step:820/2110 train_time:30961ms step_avg:37.76ms
step:821/2110 train_time:31022ms step_avg:37.79ms
step:822/2110 train_time:31082ms step_avg:37.81ms
step:823/2110 train_time:31142ms step_avg:37.84ms
step:824/2110 train_time:31200ms step_avg:37.86ms
step:825/2110 train_time:31261ms step_avg:37.89ms
step:826/2110 train_time:31320ms step_avg:37.92ms
step:827/2110 train_time:31380ms step_avg:37.94ms
step:828/2110 train_time:31438ms step_avg:37.97ms
step:829/2110 train_time:31498ms step_avg:37.99ms
step:830/2110 train_time:31556ms step_avg:38.02ms
step:831/2110 train_time:31616ms step_avg:38.05ms
step:832/2110 train_time:31674ms step_avg:38.07ms
step:833/2110 train_time:31734ms step_avg:38.10ms
step:834/2110 train_time:31793ms step_avg:38.12ms
step:835/2110 train_time:31852ms step_avg:38.15ms
step:836/2110 train_time:31910ms step_avg:38.17ms
step:837/2110 train_time:31970ms step_avg:38.20ms
step:838/2110 train_time:32029ms step_avg:38.22ms
step:839/2110 train_time:32088ms step_avg:38.25ms
step:840/2110 train_time:32147ms step_avg:38.27ms
step:841/2110 train_time:32207ms step_avg:38.30ms
step:842/2110 train_time:32266ms step_avg:38.32ms
step:843/2110 train_time:32326ms step_avg:38.35ms
step:844/2110 train_time:32384ms step_avg:38.37ms
step:845/2110 train_time:32445ms step_avg:38.40ms
step:846/2110 train_time:32503ms step_avg:38.42ms
step:847/2110 train_time:32562ms step_avg:38.44ms
step:848/2110 train_time:32621ms step_avg:38.47ms
step:849/2110 train_time:32681ms step_avg:38.49ms
step:850/2110 train_time:32739ms step_avg:38.52ms
step:851/2110 train_time:32800ms step_avg:38.54ms
step:852/2110 train_time:32859ms step_avg:38.57ms
step:853/2110 train_time:32921ms step_avg:38.59ms
step:854/2110 train_time:32980ms step_avg:38.62ms
step:855/2110 train_time:33039ms step_avg:38.64ms
step:856/2110 train_time:33098ms step_avg:38.67ms
step:857/2110 train_time:33158ms step_avg:38.69ms
step:858/2110 train_time:33217ms step_avg:38.71ms
step:859/2110 train_time:33277ms step_avg:38.74ms
step:860/2110 train_time:33337ms step_avg:38.76ms
step:861/2110 train_time:33397ms step_avg:38.79ms
step:862/2110 train_time:33455ms step_avg:38.81ms
step:863/2110 train_time:33514ms step_avg:38.83ms
step:864/2110 train_time:33572ms step_avg:38.86ms
step:865/2110 train_time:33632ms step_avg:38.88ms
step:866/2110 train_time:33691ms step_avg:38.90ms
step:867/2110 train_time:33751ms step_avg:38.93ms
step:868/2110 train_time:33809ms step_avg:38.95ms
step:869/2110 train_time:33869ms step_avg:38.97ms
step:870/2110 train_time:33927ms step_avg:39.00ms
step:871/2110 train_time:33987ms step_avg:39.02ms
step:872/2110 train_time:34045ms step_avg:39.04ms
step:873/2110 train_time:34105ms step_avg:39.07ms
step:874/2110 train_time:34164ms step_avg:39.09ms
step:875/2110 train_time:34225ms step_avg:39.11ms
step:876/2110 train_time:34283ms step_avg:39.14ms
step:877/2110 train_time:34343ms step_avg:39.16ms
step:878/2110 train_time:34403ms step_avg:39.18ms
step:879/2110 train_time:34462ms step_avg:39.21ms
step:880/2110 train_time:34521ms step_avg:39.23ms
step:881/2110 train_time:34581ms step_avg:39.25ms
step:882/2110 train_time:34640ms step_avg:39.27ms
step:883/2110 train_time:34701ms step_avg:39.30ms
step:884/2110 train_time:34759ms step_avg:39.32ms
step:885/2110 train_time:34819ms step_avg:39.34ms
step:886/2110 train_time:34879ms step_avg:39.37ms
step:887/2110 train_time:34939ms step_avg:39.39ms
step:888/2110 train_time:34998ms step_avg:39.41ms
step:889/2110 train_time:35058ms step_avg:39.44ms
step:890/2110 train_time:35117ms step_avg:39.46ms
step:891/2110 train_time:35177ms step_avg:39.48ms
step:892/2110 train_time:35236ms step_avg:39.50ms
step:893/2110 train_time:35296ms step_avg:39.53ms
step:894/2110 train_time:35354ms step_avg:39.55ms
step:895/2110 train_time:35414ms step_avg:39.57ms
step:896/2110 train_time:35473ms step_avg:39.59ms
step:897/2110 train_time:35533ms step_avg:39.61ms
step:898/2110 train_time:35592ms step_avg:39.63ms
step:899/2110 train_time:35651ms step_avg:39.66ms
step:900/2110 train_time:35710ms step_avg:39.68ms
step:901/2110 train_time:35769ms step_avg:39.70ms
step:902/2110 train_time:35828ms step_avg:39.72ms
step:903/2110 train_time:35887ms step_avg:39.74ms
step:904/2110 train_time:35945ms step_avg:39.76ms
step:905/2110 train_time:36006ms step_avg:39.79ms
step:906/2110 train_time:36064ms step_avg:39.81ms
step:907/2110 train_time:36124ms step_avg:39.83ms
step:908/2110 train_time:36183ms step_avg:39.85ms
step:909/2110 train_time:36244ms step_avg:39.87ms
step:910/2110 train_time:36302ms step_avg:39.89ms
step:911/2110 train_time:36363ms step_avg:39.91ms
step:912/2110 train_time:36421ms step_avg:39.94ms
step:913/2110 train_time:36482ms step_avg:39.96ms
step:914/2110 train_time:36540ms step_avg:39.98ms
step:915/2110 train_time:36601ms step_avg:40.00ms
step:916/2110 train_time:36660ms step_avg:40.02ms
step:917/2110 train_time:36720ms step_avg:40.04ms
step:918/2110 train_time:36779ms step_avg:40.06ms
step:919/2110 train_time:36840ms step_avg:40.09ms
step:920/2110 train_time:36898ms step_avg:40.11ms
step:921/2110 train_time:36958ms step_avg:40.13ms
step:922/2110 train_time:37017ms step_avg:40.15ms
step:923/2110 train_time:37077ms step_avg:40.17ms
step:924/2110 train_time:37136ms step_avg:40.19ms
step:925/2110 train_time:37196ms step_avg:40.21ms
step:926/2110 train_time:37254ms step_avg:40.23ms
step:927/2110 train_time:37314ms step_avg:40.25ms
step:928/2110 train_time:37372ms step_avg:40.27ms
step:929/2110 train_time:37433ms step_avg:40.29ms
step:930/2110 train_time:37491ms step_avg:40.31ms
step:931/2110 train_time:37551ms step_avg:40.33ms
step:932/2110 train_time:37608ms step_avg:40.35ms
step:933/2110 train_time:37669ms step_avg:40.37ms
step:934/2110 train_time:37727ms step_avg:40.39ms
step:935/2110 train_time:37787ms step_avg:40.41ms
step:936/2110 train_time:37845ms step_avg:40.43ms
step:937/2110 train_time:37906ms step_avg:40.45ms
step:938/2110 train_time:37964ms step_avg:40.47ms
step:939/2110 train_time:38025ms step_avg:40.50ms
step:940/2110 train_time:38084ms step_avg:40.51ms
step:941/2110 train_time:38144ms step_avg:40.54ms
step:942/2110 train_time:38202ms step_avg:40.55ms
step:943/2110 train_time:38262ms step_avg:40.58ms
step:944/2110 train_time:38321ms step_avg:40.59ms
step:945/2110 train_time:38382ms step_avg:40.62ms
step:946/2110 train_time:38441ms step_avg:40.63ms
step:947/2110 train_time:38501ms step_avg:40.66ms
step:948/2110 train_time:38560ms step_avg:40.67ms
step:949/2110 train_time:38620ms step_avg:40.70ms
step:950/2110 train_time:38679ms step_avg:40.72ms
step:951/2110 train_time:38739ms step_avg:40.74ms
step:952/2110 train_time:38798ms step_avg:40.75ms
step:953/2110 train_time:38858ms step_avg:40.77ms
step:954/2110 train_time:38917ms step_avg:40.79ms
step:955/2110 train_time:38977ms step_avg:40.81ms
step:956/2110 train_time:39035ms step_avg:40.83ms
step:957/2110 train_time:39095ms step_avg:40.85ms
step:958/2110 train_time:39153ms step_avg:40.87ms
step:959/2110 train_time:39213ms step_avg:40.89ms
step:960/2110 train_time:39271ms step_avg:40.91ms
step:961/2110 train_time:39331ms step_avg:40.93ms
step:962/2110 train_time:39389ms step_avg:40.95ms
step:963/2110 train_time:39449ms step_avg:40.97ms
step:964/2110 train_time:39508ms step_avg:40.98ms
step:965/2110 train_time:39568ms step_avg:41.00ms
step:966/2110 train_time:39626ms step_avg:41.02ms
step:967/2110 train_time:39687ms step_avg:41.04ms
step:968/2110 train_time:39744ms step_avg:41.06ms
step:969/2110 train_time:39804ms step_avg:41.08ms
step:970/2110 train_time:39863ms step_avg:41.10ms
step:971/2110 train_time:39924ms step_avg:41.12ms
step:972/2110 train_time:39982ms step_avg:41.13ms
step:973/2110 train_time:40043ms step_avg:41.15ms
step:974/2110 train_time:40102ms step_avg:41.17ms
step:975/2110 train_time:40183ms step_avg:41.21ms
step:976/2110 train_time:40221ms step_avg:41.21ms
step:977/2110 train_time:40281ms step_avg:41.23ms
step:978/2110 train_time:40340ms step_avg:41.25ms
step:979/2110 train_time:40401ms step_avg:41.27ms
step:980/2110 train_time:40460ms step_avg:41.29ms
step:981/2110 train_time:40521ms step_avg:41.31ms
step:982/2110 train_time:40579ms step_avg:41.32ms
step:983/2110 train_time:40639ms step_avg:41.34ms
step:984/2110 train_time:40699ms step_avg:41.36ms
step:985/2110 train_time:40758ms step_avg:41.38ms
step:986/2110 train_time:40817ms step_avg:41.40ms
step:987/2110 train_time:40877ms step_avg:41.42ms
step:988/2110 train_time:40936ms step_avg:41.43ms
step:989/2110 train_time:40995ms step_avg:41.45ms
step:990/2110 train_time:41054ms step_avg:41.47ms
step:991/2110 train_time:41114ms step_avg:41.49ms
step:992/2110 train_time:41172ms step_avg:41.50ms
step:993/2110 train_time:41233ms step_avg:41.52ms
step:994/2110 train_time:41292ms step_avg:41.54ms
step:995/2110 train_time:41351ms step_avg:41.56ms
step:996/2110 train_time:41409ms step_avg:41.58ms
step:997/2110 train_time:41469ms step_avg:41.59ms
step:998/2110 train_time:41527ms step_avg:41.61ms
step:999/2110 train_time:41588ms step_avg:41.63ms
step:1000/2110 train_time:41646ms step_avg:41.65ms
step:1000/2110 val_loss:3.7574 train_time:41708ms step_avg:41.71ms
step:1001/2110 train_time:41733ms step_avg:41.69ms
step:1002/2110 train_time:41767ms step_avg:41.68ms
step:1003/2110 train_time:41830ms step_avg:41.70ms
step:1004/2110 train_time:41890ms step_avg:41.72ms
step:1005/2110 train_time:41951ms step_avg:41.74ms
step:1006/2110 train_time:42010ms step_avg:41.76ms
step:1007/2110 train_time:42070ms step_avg:41.78ms
step:1008/2110 train_time:42127ms step_avg:41.79ms
step:1009/2110 train_time:42187ms step_avg:41.81ms
step:1010/2110 train_time:42245ms step_avg:41.83ms
step:1011/2110 train_time:42304ms step_avg:41.84ms
step:1012/2110 train_time:42361ms step_avg:41.86ms
step:1013/2110 train_time:42420ms step_avg:41.88ms
step:1014/2110 train_time:42478ms step_avg:41.89ms
step:1015/2110 train_time:42538ms step_avg:41.91ms
step:1016/2110 train_time:42595ms step_avg:41.92ms
step:1017/2110 train_time:42657ms step_avg:41.94ms
step:1018/2110 train_time:42716ms step_avg:41.96ms
step:1019/2110 train_time:42778ms step_avg:41.98ms
step:1020/2110 train_time:42838ms step_avg:42.00ms
step:1021/2110 train_time:42900ms step_avg:42.02ms
step:1022/2110 train_time:42959ms step_avg:42.03ms
step:1023/2110 train_time:43019ms step_avg:42.05ms
step:1024/2110 train_time:43077ms step_avg:42.07ms
step:1025/2110 train_time:43137ms step_avg:42.08ms
step:1026/2110 train_time:43195ms step_avg:42.10ms
step:1027/2110 train_time:43255ms step_avg:42.12ms
step:1028/2110 train_time:43313ms step_avg:42.13ms
step:1029/2110 train_time:43373ms step_avg:42.15ms
step:1030/2110 train_time:43431ms step_avg:42.17ms
step:1031/2110 train_time:43491ms step_avg:42.18ms
step:1032/2110 train_time:43549ms step_avg:42.20ms
step:1033/2110 train_time:43609ms step_avg:42.22ms
step:1034/2110 train_time:43668ms step_avg:42.23ms
step:1035/2110 train_time:43729ms step_avg:42.25ms
step:1036/2110 train_time:43789ms step_avg:42.27ms
step:1037/2110 train_time:43851ms step_avg:42.29ms
step:1038/2110 train_time:43910ms step_avg:42.30ms
step:1039/2110 train_time:43971ms step_avg:42.32ms
step:1040/2110 train_time:44030ms step_avg:42.34ms
step:1041/2110 train_time:44090ms step_avg:42.35ms
step:1042/2110 train_time:44148ms step_avg:42.37ms
step:1043/2110 train_time:44208ms step_avg:42.39ms
step:1044/2110 train_time:44266ms step_avg:42.40ms
step:1045/2110 train_time:44326ms step_avg:42.42ms
step:1046/2110 train_time:44384ms step_avg:42.43ms
step:1047/2110 train_time:44444ms step_avg:42.45ms
step:1048/2110 train_time:44501ms step_avg:42.46ms
step:1049/2110 train_time:44561ms step_avg:42.48ms
step:1050/2110 train_time:44619ms step_avg:42.49ms
step:1051/2110 train_time:44680ms step_avg:42.51ms
step:1052/2110 train_time:44738ms step_avg:42.53ms
step:1053/2110 train_time:44799ms step_avg:42.54ms
step:1054/2110 train_time:44859ms step_avg:42.56ms
step:1055/2110 train_time:44920ms step_avg:42.58ms
step:1056/2110 train_time:44979ms step_avg:42.59ms
step:1057/2110 train_time:45039ms step_avg:42.61ms
step:1058/2110 train_time:45098ms step_avg:42.63ms
step:1059/2110 train_time:45158ms step_avg:42.64ms
step:1060/2110 train_time:45216ms step_avg:42.66ms
step:1061/2110 train_time:45276ms step_avg:42.67ms
step:1062/2110 train_time:45335ms step_avg:42.69ms
step:1063/2110 train_time:45396ms step_avg:42.71ms
step:1064/2110 train_time:45454ms step_avg:42.72ms
step:1065/2110 train_time:45514ms step_avg:42.74ms
step:1066/2110 train_time:45573ms step_avg:42.75ms
step:1067/2110 train_time:45633ms step_avg:42.77ms
step:1068/2110 train_time:45693ms step_avg:42.78ms
step:1069/2110 train_time:45753ms step_avg:42.80ms
step:1070/2110 train_time:45812ms step_avg:42.82ms
step:1071/2110 train_time:45872ms step_avg:42.83ms
step:1072/2110 train_time:45931ms step_avg:42.85ms
step:1073/2110 train_time:45992ms step_avg:42.86ms
step:1074/2110 train_time:46051ms step_avg:42.88ms
step:1075/2110 train_time:46111ms step_avg:42.89ms
step:1076/2110 train_time:46169ms step_avg:42.91ms
step:1077/2110 train_time:46230ms step_avg:42.92ms
step:1078/2110 train_time:46289ms step_avg:42.94ms
step:1079/2110 train_time:46349ms step_avg:42.96ms
step:1080/2110 train_time:46408ms step_avg:42.97ms
step:1081/2110 train_time:46469ms step_avg:42.99ms
step:1082/2110 train_time:46527ms step_avg:43.00ms
step:1083/2110 train_time:46587ms step_avg:43.02ms
step:1084/2110 train_time:46647ms step_avg:43.03ms
step:1085/2110 train_time:46706ms step_avg:43.05ms
step:1086/2110 train_time:46765ms step_avg:43.06ms
step:1087/2110 train_time:46825ms step_avg:43.08ms
step:1088/2110 train_time:46884ms step_avg:43.09ms
step:1089/2110 train_time:46945ms step_avg:43.11ms
step:1090/2110 train_time:47003ms step_avg:43.12ms
step:1091/2110 train_time:47064ms step_avg:43.14ms
step:1092/2110 train_time:47122ms step_avg:43.15ms
step:1093/2110 train_time:47182ms step_avg:43.17ms
step:1094/2110 train_time:47240ms step_avg:43.18ms
step:1095/2110 train_time:47300ms step_avg:43.20ms
step:1096/2110 train_time:47357ms step_avg:43.21ms
step:1097/2110 train_time:47418ms step_avg:43.22ms
step:1098/2110 train_time:47476ms step_avg:43.24ms
step:1099/2110 train_time:47536ms step_avg:43.25ms
step:1100/2110 train_time:47595ms step_avg:43.27ms
step:1101/2110 train_time:47655ms step_avg:43.28ms
step:1102/2110 train_time:47714ms step_avg:43.30ms
step:1103/2110 train_time:47775ms step_avg:43.31ms
step:1104/2110 train_time:47835ms step_avg:43.33ms
step:1105/2110 train_time:47896ms step_avg:43.34ms
step:1106/2110 train_time:47955ms step_avg:43.36ms
step:1107/2110 train_time:48015ms step_avg:43.37ms
step:1108/2110 train_time:48074ms step_avg:43.39ms
step:1109/2110 train_time:48134ms step_avg:43.40ms
step:1110/2110 train_time:48192ms step_avg:43.42ms
step:1111/2110 train_time:48253ms step_avg:43.43ms
step:1112/2110 train_time:48311ms step_avg:43.44ms
step:1113/2110 train_time:48371ms step_avg:43.46ms
step:1114/2110 train_time:48429ms step_avg:43.47ms
step:1115/2110 train_time:48489ms step_avg:43.49ms
step:1116/2110 train_time:48548ms step_avg:43.50ms
step:1117/2110 train_time:48608ms step_avg:43.52ms
step:1118/2110 train_time:48666ms step_avg:43.53ms
step:1119/2110 train_time:48726ms step_avg:43.54ms
step:1120/2110 train_time:48785ms step_avg:43.56ms
step:1121/2110 train_time:48846ms step_avg:43.57ms
step:1122/2110 train_time:48904ms step_avg:43.59ms
step:1123/2110 train_time:48965ms step_avg:43.60ms
step:1124/2110 train_time:49024ms step_avg:43.62ms
step:1125/2110 train_time:49084ms step_avg:43.63ms
step:1126/2110 train_time:49142ms step_avg:43.64ms
step:1127/2110 train_time:49203ms step_avg:43.66ms
step:1128/2110 train_time:49262ms step_avg:43.67ms
step:1129/2110 train_time:49321ms step_avg:43.69ms
step:1130/2110 train_time:49380ms step_avg:43.70ms
step:1131/2110 train_time:49440ms step_avg:43.71ms
step:1132/2110 train_time:49497ms step_avg:43.73ms
step:1133/2110 train_time:49558ms step_avg:43.74ms
step:1134/2110 train_time:49617ms step_avg:43.75ms
step:1135/2110 train_time:49678ms step_avg:43.77ms
step:1136/2110 train_time:49736ms step_avg:43.78ms
step:1137/2110 train_time:49797ms step_avg:43.80ms
step:1138/2110 train_time:49856ms step_avg:43.81ms
step:1139/2110 train_time:49916ms step_avg:43.82ms
step:1140/2110 train_time:49976ms step_avg:43.84ms
step:1141/2110 train_time:50037ms step_avg:43.85ms
step:1142/2110 train_time:50096ms step_avg:43.87ms
step:1143/2110 train_time:50156ms step_avg:43.88ms
step:1144/2110 train_time:50215ms step_avg:43.89ms
step:1145/2110 train_time:50275ms step_avg:43.91ms
step:1146/2110 train_time:50335ms step_avg:43.92ms
step:1147/2110 train_time:50395ms step_avg:43.94ms
step:1148/2110 train_time:50454ms step_avg:43.95ms
step:1149/2110 train_time:50515ms step_avg:43.96ms
step:1150/2110 train_time:50575ms step_avg:43.98ms
step:1151/2110 train_time:50636ms step_avg:43.99ms
step:1152/2110 train_time:50695ms step_avg:44.01ms
step:1153/2110 train_time:50756ms step_avg:44.02ms
step:1154/2110 train_time:50815ms step_avg:44.03ms
step:1155/2110 train_time:50877ms step_avg:44.05ms
step:1156/2110 train_time:50937ms step_avg:44.06ms
step:1157/2110 train_time:50996ms step_avg:44.08ms
step:1158/2110 train_time:51056ms step_avg:44.09ms
step:1159/2110 train_time:51116ms step_avg:44.10ms
step:1160/2110 train_time:51176ms step_avg:44.12ms
step:1161/2110 train_time:51237ms step_avg:44.13ms
step:1162/2110 train_time:51296ms step_avg:44.14ms
step:1163/2110 train_time:51356ms step_avg:44.16ms
step:1164/2110 train_time:51415ms step_avg:44.17ms
step:1165/2110 train_time:51475ms step_avg:44.18ms
step:1166/2110 train_time:51535ms step_avg:44.20ms
step:1167/2110 train_time:51596ms step_avg:44.21ms
step:1168/2110 train_time:51656ms step_avg:44.23ms
step:1169/2110 train_time:51717ms step_avg:44.24ms
step:1170/2110 train_time:51776ms step_avg:44.25ms
step:1171/2110 train_time:51838ms step_avg:44.27ms
step:1172/2110 train_time:51896ms step_avg:44.28ms
step:1173/2110 train_time:51957ms step_avg:44.29ms
step:1174/2110 train_time:52016ms step_avg:44.31ms
step:1175/2110 train_time:52077ms step_avg:44.32ms
step:1176/2110 train_time:52136ms step_avg:44.33ms
step:1177/2110 train_time:52197ms step_avg:44.35ms
step:1178/2110 train_time:52256ms step_avg:44.36ms
step:1179/2110 train_time:52316ms step_avg:44.37ms
step:1180/2110 train_time:52375ms step_avg:44.39ms
step:1181/2110 train_time:52436ms step_avg:44.40ms
step:1182/2110 train_time:52495ms step_avg:44.41ms
step:1183/2110 train_time:52556ms step_avg:44.43ms
step:1184/2110 train_time:52615ms step_avg:44.44ms
step:1185/2110 train_time:52677ms step_avg:44.45ms
step:1186/2110 train_time:52737ms step_avg:44.47ms
step:1187/2110 train_time:52797ms step_avg:44.48ms
step:1188/2110 train_time:52856ms step_avg:44.49ms
step:1189/2110 train_time:52917ms step_avg:44.51ms
step:1190/2110 train_time:52976ms step_avg:44.52ms
step:1191/2110 train_time:53037ms step_avg:44.53ms
step:1192/2110 train_time:53097ms step_avg:44.54ms
step:1193/2110 train_time:53157ms step_avg:44.56ms
step:1194/2110 train_time:53216ms step_avg:44.57ms
step:1195/2110 train_time:53277ms step_avg:44.58ms
step:1196/2110 train_time:53336ms step_avg:44.60ms
step:1197/2110 train_time:53396ms step_avg:44.61ms
step:1198/2110 train_time:53455ms step_avg:44.62ms
step:1199/2110 train_time:53516ms step_avg:44.63ms
step:1200/2110 train_time:53576ms step_avg:44.65ms
step:1201/2110 train_time:53637ms step_avg:44.66ms
step:1202/2110 train_time:53696ms step_avg:44.67ms
step:1203/2110 train_time:53756ms step_avg:44.69ms
step:1204/2110 train_time:53815ms step_avg:44.70ms
step:1205/2110 train_time:53876ms step_avg:44.71ms
step:1206/2110 train_time:53935ms step_avg:44.72ms
step:1207/2110 train_time:53996ms step_avg:44.74ms
step:1208/2110 train_time:54055ms step_avg:44.75ms
step:1209/2110 train_time:54116ms step_avg:44.76ms
step:1210/2110 train_time:54176ms step_avg:44.77ms
step:1211/2110 train_time:54237ms step_avg:44.79ms
step:1212/2110 train_time:54296ms step_avg:44.80ms
step:1213/2110 train_time:54356ms step_avg:44.81ms
step:1214/2110 train_time:54415ms step_avg:44.82ms
step:1215/2110 train_time:54476ms step_avg:44.84ms
step:1216/2110 train_time:54535ms step_avg:44.85ms
step:1217/2110 train_time:54595ms step_avg:44.86ms
step:1218/2110 train_time:54655ms step_avg:44.87ms
step:1219/2110 train_time:54716ms step_avg:44.89ms
step:1220/2110 train_time:54776ms step_avg:44.90ms
step:1221/2110 train_time:54837ms step_avg:44.91ms
step:1222/2110 train_time:54896ms step_avg:44.92ms
step:1223/2110 train_time:54957ms step_avg:44.94ms
step:1224/2110 train_time:55016ms step_avg:44.95ms
step:1225/2110 train_time:55077ms step_avg:44.96ms
step:1226/2110 train_time:55136ms step_avg:44.97ms
step:1227/2110 train_time:55196ms step_avg:44.98ms
step:1228/2110 train_time:55255ms step_avg:45.00ms
step:1229/2110 train_time:55316ms step_avg:45.01ms
step:1230/2110 train_time:55375ms step_avg:45.02ms
step:1231/2110 train_time:55436ms step_avg:45.03ms
step:1232/2110 train_time:55495ms step_avg:45.04ms
step:1233/2110 train_time:55555ms step_avg:45.06ms
step:1234/2110 train_time:55615ms step_avg:45.07ms
step:1235/2110 train_time:55676ms step_avg:45.08ms
step:1236/2110 train_time:55735ms step_avg:45.09ms
step:1237/2110 train_time:55797ms step_avg:45.11ms
step:1238/2110 train_time:55855ms step_avg:45.12ms
step:1239/2110 train_time:55917ms step_avg:45.13ms
step:1240/2110 train_time:55976ms step_avg:45.14ms
step:1241/2110 train_time:56037ms step_avg:45.15ms
step:1242/2110 train_time:56096ms step_avg:45.17ms
step:1243/2110 train_time:56156ms step_avg:45.18ms
step:1244/2110 train_time:56215ms step_avg:45.19ms
step:1245/2110 train_time:56276ms step_avg:45.20ms
step:1246/2110 train_time:56336ms step_avg:45.21ms
step:1247/2110 train_time:56396ms step_avg:45.23ms
step:1248/2110 train_time:56455ms step_avg:45.24ms
step:1249/2110 train_time:56516ms step_avg:45.25ms
step:1250/2110 train_time:56575ms step_avg:45.26ms
step:1250/2110 val_loss:3.5928 train_time:56637ms step_avg:45.31ms
step:1251/2110 train_time:56661ms step_avg:45.29ms
step:1252/2110 train_time:56699ms step_avg:45.29ms
step:1253/2110 train_time:56764ms step_avg:45.30ms
step:1254/2110 train_time:56824ms step_avg:45.31ms
step:1255/2110 train_time:56885ms step_avg:45.33ms
step:1256/2110 train_time:56944ms step_avg:45.34ms
step:1257/2110 train_time:57004ms step_avg:45.35ms
step:1258/2110 train_time:57062ms step_avg:45.36ms
step:1259/2110 train_time:57122ms step_avg:45.37ms
step:1260/2110 train_time:57180ms step_avg:45.38ms
step:1261/2110 train_time:57240ms step_avg:45.39ms
step:1262/2110 train_time:57298ms step_avg:45.40ms
step:1263/2110 train_time:57357ms step_avg:45.41ms
step:1264/2110 train_time:57415ms step_avg:45.42ms
step:1265/2110 train_time:57475ms step_avg:45.43ms
step:1266/2110 train_time:57534ms step_avg:45.45ms
step:1267/2110 train_time:57598ms step_avg:45.46ms
step:1268/2110 train_time:57659ms step_avg:45.47ms
step:1269/2110 train_time:57722ms step_avg:45.49ms
step:1270/2110 train_time:57782ms step_avg:45.50ms
step:1271/2110 train_time:57843ms step_avg:45.51ms
step:1272/2110 train_time:57902ms step_avg:45.52ms
step:1273/2110 train_time:57962ms step_avg:45.53ms
step:1274/2110 train_time:58020ms step_avg:45.54ms
step:1275/2110 train_time:58081ms step_avg:45.55ms
step:1276/2110 train_time:58140ms step_avg:45.56ms
step:1277/2110 train_time:58199ms step_avg:45.57ms
step:1278/2110 train_time:58257ms step_avg:45.58ms
step:1279/2110 train_time:58316ms step_avg:45.60ms
step:1280/2110 train_time:58375ms step_avg:45.61ms
step:1281/2110 train_time:58434ms step_avg:45.62ms
step:1282/2110 train_time:58492ms step_avg:45.63ms
step:1283/2110 train_time:58554ms step_avg:45.64ms
step:1284/2110 train_time:58614ms step_avg:45.65ms
step:1285/2110 train_time:58676ms step_avg:45.66ms
step:1286/2110 train_time:58736ms step_avg:45.67ms
step:1287/2110 train_time:58798ms step_avg:45.69ms
step:1288/2110 train_time:58857ms step_avg:45.70ms
step:1289/2110 train_time:58918ms step_avg:45.71ms
step:1290/2110 train_time:58977ms step_avg:45.72ms
step:1291/2110 train_time:59038ms step_avg:45.73ms
step:1292/2110 train_time:59097ms step_avg:45.74ms
step:1293/2110 train_time:59157ms step_avg:45.75ms
step:1294/2110 train_time:59215ms step_avg:45.76ms
step:1295/2110 train_time:59275ms step_avg:45.77ms
step:1296/2110 train_time:59334ms step_avg:45.78ms
step:1297/2110 train_time:59393ms step_avg:45.79ms
step:1298/2110 train_time:59452ms step_avg:45.80ms
step:1299/2110 train_time:59512ms step_avg:45.81ms
step:1300/2110 train_time:59572ms step_avg:45.82ms
step:1301/2110 train_time:59633ms step_avg:45.84ms
step:1302/2110 train_time:59693ms step_avg:45.85ms
step:1303/2110 train_time:59755ms step_avg:45.86ms
step:1304/2110 train_time:59815ms step_avg:45.87ms
step:1305/2110 train_time:59876ms step_avg:45.88ms
step:1306/2110 train_time:59935ms step_avg:45.89ms
step:1307/2110 train_time:59996ms step_avg:45.90ms
step:1308/2110 train_time:60055ms step_avg:45.91ms
step:1309/2110 train_time:60116ms step_avg:45.92ms
step:1310/2110 train_time:60175ms step_avg:45.93ms
step:1311/2110 train_time:60235ms step_avg:45.95ms
step:1312/2110 train_time:60293ms step_avg:45.95ms
step:1313/2110 train_time:60353ms step_avg:45.97ms
step:1314/2110 train_time:60413ms step_avg:45.98ms
step:1315/2110 train_time:60473ms step_avg:45.99ms
step:1316/2110 train_time:60533ms step_avg:46.00ms
step:1317/2110 train_time:60593ms step_avg:46.01ms
step:1318/2110 train_time:60653ms step_avg:46.02ms
step:1319/2110 train_time:60714ms step_avg:46.03ms
step:1320/2110 train_time:60774ms step_avg:46.04ms
step:1321/2110 train_time:60835ms step_avg:46.05ms
step:1322/2110 train_time:60894ms step_avg:46.06ms
step:1323/2110 train_time:60956ms step_avg:46.07ms
step:1324/2110 train_time:61015ms step_avg:46.08ms
step:1325/2110 train_time:61076ms step_avg:46.09ms
step:1326/2110 train_time:61134ms step_avg:46.10ms
step:1327/2110 train_time:61194ms step_avg:46.11ms
step:1328/2110 train_time:61253ms step_avg:46.12ms
step:1329/2110 train_time:61314ms step_avg:46.14ms
step:1330/2110 train_time:61373ms step_avg:46.15ms
step:1331/2110 train_time:61434ms step_avg:46.16ms
step:1332/2110 train_time:61493ms step_avg:46.17ms
step:1333/2110 train_time:61553ms step_avg:46.18ms
step:1334/2110 train_time:61613ms step_avg:46.19ms
step:1335/2110 train_time:61674ms step_avg:46.20ms
step:1336/2110 train_time:61733ms step_avg:46.21ms
step:1337/2110 train_time:61795ms step_avg:46.22ms
step:1338/2110 train_time:61854ms step_avg:46.23ms
step:1339/2110 train_time:61915ms step_avg:46.24ms
step:1340/2110 train_time:61975ms step_avg:46.25ms
step:1341/2110 train_time:62035ms step_avg:46.26ms
step:1342/2110 train_time:62094ms step_avg:46.27ms
step:1343/2110 train_time:62154ms step_avg:46.28ms
step:1344/2110 train_time:62213ms step_avg:46.29ms
step:1345/2110 train_time:62273ms step_avg:46.30ms
step:1346/2110 train_time:62331ms step_avg:46.31ms
step:1347/2110 train_time:62392ms step_avg:46.32ms
step:1348/2110 train_time:62451ms step_avg:46.33ms
step:1349/2110 train_time:62512ms step_avg:46.34ms
step:1350/2110 train_time:62571ms step_avg:46.35ms
step:1351/2110 train_time:62632ms step_avg:46.36ms
step:1352/2110 train_time:62692ms step_avg:46.37ms
step:1353/2110 train_time:62754ms step_avg:46.38ms
step:1354/2110 train_time:62814ms step_avg:46.39ms
step:1355/2110 train_time:62875ms step_avg:46.40ms
step:1356/2110 train_time:62934ms step_avg:46.41ms
step:1357/2110 train_time:62994ms step_avg:46.42ms
step:1358/2110 train_time:63053ms step_avg:46.43ms
step:1359/2110 train_time:63114ms step_avg:46.44ms
step:1360/2110 train_time:63174ms step_avg:46.45ms
step:1361/2110 train_time:63234ms step_avg:46.46ms
step:1362/2110 train_time:63292ms step_avg:46.47ms
step:1363/2110 train_time:63353ms step_avg:46.48ms
step:1364/2110 train_time:63412ms step_avg:46.49ms
step:1365/2110 train_time:63473ms step_avg:46.50ms
step:1366/2110 train_time:63533ms step_avg:46.51ms
step:1367/2110 train_time:63593ms step_avg:46.52ms
step:1368/2110 train_time:63651ms step_avg:46.53ms
step:1369/2110 train_time:63713ms step_avg:46.54ms
step:1370/2110 train_time:63772ms step_avg:46.55ms
step:1371/2110 train_time:63834ms step_avg:46.56ms
step:1372/2110 train_time:63894ms step_avg:46.57ms
step:1373/2110 train_time:63955ms step_avg:46.58ms
step:1374/2110 train_time:64014ms step_avg:46.59ms
step:1375/2110 train_time:64075ms step_avg:46.60ms
step:1376/2110 train_time:64134ms step_avg:46.61ms
step:1377/2110 train_time:64194ms step_avg:46.62ms
step:1378/2110 train_time:64252ms step_avg:46.63ms
step:1379/2110 train_time:64313ms step_avg:46.64ms
step:1380/2110 train_time:64373ms step_avg:46.65ms
step:1381/2110 train_time:64433ms step_avg:46.66ms
step:1382/2110 train_time:64519ms step_avg:46.68ms
step:1383/2110 train_time:64607ms step_avg:46.71ms
step:1384/2110 train_time:64692ms step_avg:46.74ms
step:1385/2110 train_time:64781ms step_avg:46.77ms
step:1386/2110 train_time:64867ms step_avg:46.80ms
step:1387/2110 train_time:64954ms step_avg:46.83ms
step:1388/2110 train_time:65041ms step_avg:46.86ms
step:1389/2110 train_time:65129ms step_avg:46.89ms
step:1390/2110 train_time:65215ms step_avg:46.92ms
step:1391/2110 train_time:65303ms step_avg:46.95ms
step:1392/2110 train_time:65388ms step_avg:46.97ms
step:1393/2110 train_time:65475ms step_avg:47.00ms
step:1394/2110 train_time:65561ms step_avg:47.03ms
step:1395/2110 train_time:65649ms step_avg:47.06ms
step:1396/2110 train_time:65736ms step_avg:47.09ms
step:1397/2110 train_time:65823ms step_avg:47.12ms
step:1398/2110 train_time:65909ms step_avg:47.15ms
step:1399/2110 train_time:65996ms step_avg:47.17ms
step:1400/2110 train_time:66083ms step_avg:47.20ms
step:1401/2110 train_time:66169ms step_avg:47.23ms
step:1402/2110 train_time:66255ms step_avg:47.26ms
step:1403/2110 train_time:66343ms step_avg:47.29ms
step:1404/2110 train_time:66429ms step_avg:47.31ms
step:1405/2110 train_time:66516ms step_avg:47.34ms
step:1406/2110 train_time:66602ms step_avg:47.37ms
step:1407/2110 train_time:66689ms step_avg:47.40ms
step:1408/2110 train_time:66775ms step_avg:47.43ms
step:1409/2110 train_time:66862ms step_avg:47.45ms
step:1410/2110 train_time:66948ms step_avg:47.48ms
step:1411/2110 train_time:67036ms step_avg:47.51ms
step:1412/2110 train_time:67123ms step_avg:47.54ms
step:1413/2110 train_time:67210ms step_avg:47.57ms
step:1414/2110 train_time:67295ms step_avg:47.59ms
step:1415/2110 train_time:67383ms step_avg:47.62ms
step:1416/2110 train_time:67468ms step_avg:47.65ms
step:1417/2110 train_time:67556ms step_avg:47.68ms
step:1418/2110 train_time:67644ms step_avg:47.70ms
step:1419/2110 train_time:67731ms step_avg:47.73ms
step:1420/2110 train_time:67817ms step_avg:47.76ms
step:1421/2110 train_time:67904ms step_avg:47.79ms
step:1422/2110 train_time:67990ms step_avg:47.81ms
step:1423/2110 train_time:68077ms step_avg:47.84ms
step:1424/2110 train_time:68165ms step_avg:47.87ms
step:1425/2110 train_time:68251ms step_avg:47.90ms
step:1426/2110 train_time:68338ms step_avg:47.92ms
step:1427/2110 train_time:68424ms step_avg:47.95ms
step:1428/2110 train_time:68511ms step_avg:47.98ms
step:1429/2110 train_time:68597ms step_avg:48.00ms
step:1430/2110 train_time:68685ms step_avg:48.03ms
step:1431/2110 train_time:68771ms step_avg:48.06ms
step:1432/2110 train_time:68858ms step_avg:48.09ms
step:1433/2110 train_time:68946ms step_avg:48.11ms
step:1434/2110 train_time:69032ms step_avg:48.14ms
step:1435/2110 train_time:69119ms step_avg:48.17ms
step:1436/2110 train_time:69206ms step_avg:48.19ms
step:1437/2110 train_time:69292ms step_avg:48.22ms
step:1438/2110 train_time:69379ms step_avg:48.25ms
step:1439/2110 train_time:69465ms step_avg:48.27ms
step:1440/2110 train_time:69551ms step_avg:48.30ms
step:1441/2110 train_time:69639ms step_avg:48.33ms
step:1442/2110 train_time:69727ms step_avg:48.35ms
step:1443/2110 train_time:69812ms step_avg:48.38ms
step:1444/2110 train_time:69900ms step_avg:48.41ms
step:1445/2110 train_time:69987ms step_avg:48.43ms
step:1446/2110 train_time:70073ms step_avg:48.46ms
step:1447/2110 train_time:70159ms step_avg:48.49ms
step:1448/2110 train_time:70247ms step_avg:48.51ms
step:1449/2110 train_time:70333ms step_avg:48.54ms
step:1450/2110 train_time:70420ms step_avg:48.57ms
step:1451/2110 train_time:70506ms step_avg:48.59ms
step:1452/2110 train_time:70593ms step_avg:48.62ms
step:1453/2110 train_time:70680ms step_avg:48.64ms
step:1454/2110 train_time:70768ms step_avg:48.67ms
step:1455/2110 train_time:70854ms step_avg:48.70ms
step:1456/2110 train_time:70942ms step_avg:48.72ms
step:1457/2110 train_time:71028ms step_avg:48.75ms
step:1458/2110 train_time:71116ms step_avg:48.78ms
step:1459/2110 train_time:71202ms step_avg:48.80ms
step:1460/2110 train_time:71289ms step_avg:48.83ms
step:1461/2110 train_time:71376ms step_avg:48.85ms
step:1462/2110 train_time:71463ms step_avg:48.88ms
step:1463/2110 train_time:71549ms step_avg:48.91ms
step:1464/2110 train_time:71636ms step_avg:48.93ms
step:1465/2110 train_time:71723ms step_avg:48.96ms
step:1466/2110 train_time:71810ms step_avg:48.98ms
step:1467/2110 train_time:71897ms step_avg:49.01ms
step:1468/2110 train_time:71984ms step_avg:49.04ms
step:1469/2110 train_time:72070ms step_avg:49.06ms
step:1470/2110 train_time:72157ms step_avg:49.09ms
step:1471/2110 train_time:72244ms step_avg:49.11ms
step:1472/2110 train_time:72330ms step_avg:49.14ms
step:1473/2110 train_time:72418ms step_avg:49.16ms
step:1474/2110 train_time:72506ms step_avg:49.19ms
step:1475/2110 train_time:72591ms step_avg:49.21ms
step:1476/2110 train_time:72680ms step_avg:49.24ms
step:1477/2110 train_time:72767ms step_avg:49.27ms
step:1478/2110 train_time:72854ms step_avg:49.29ms
step:1479/2110 train_time:72941ms step_avg:49.32ms
step:1480/2110 train_time:73028ms step_avg:49.34ms
step:1481/2110 train_time:73114ms step_avg:49.37ms
step:1482/2110 train_time:73202ms step_avg:49.39ms
step:1483/2110 train_time:73288ms step_avg:49.42ms
step:1484/2110 train_time:73375ms step_avg:49.44ms
step:1485/2110 train_time:73461ms step_avg:49.47ms
step:1486/2110 train_time:73548ms step_avg:49.49ms
step:1487/2110 train_time:73634ms step_avg:49.52ms
step:1488/2110 train_time:73722ms step_avg:49.54ms
step:1489/2110 train_time:73809ms step_avg:49.57ms
step:1490/2110 train_time:73897ms step_avg:49.59ms
step:1491/2110 train_time:73983ms step_avg:49.62ms
step:1492/2110 train_time:74070ms step_avg:49.64ms
step:1493/2110 train_time:74156ms step_avg:49.67ms
step:1494/2110 train_time:74244ms step_avg:49.69ms
step:1495/2110 train_time:74330ms step_avg:49.72ms
step:1496/2110 train_time:74417ms step_avg:49.74ms
step:1497/2110 train_time:74503ms step_avg:49.77ms
step:1498/2110 train_time:74590ms step_avg:49.79ms
step:1499/2110 train_time:74677ms step_avg:49.82ms
step:1500/2110 train_time:74765ms step_avg:49.84ms
step:1500/2110 val_loss:3.4956 train_time:74852ms step_avg:49.90ms
step:1501/2110 train_time:74891ms step_avg:49.89ms
step:1502/2110 train_time:74944ms step_avg:49.90ms
step:1503/2110 train_time:75034ms step_avg:49.92ms
step:1504/2110 train_time:75122ms step_avg:49.95ms
step:1505/2110 train_time:75208ms step_avg:49.97ms
step:1506/2110 train_time:75293ms step_avg:50.00ms
step:1507/2110 train_time:75379ms step_avg:50.02ms
step:1508/2110 train_time:75465ms step_avg:50.04ms
step:1509/2110 train_time:75550ms step_avg:50.07ms
step:1510/2110 train_time:75638ms step_avg:50.09ms
step:1511/2110 train_time:75723ms step_avg:50.11ms
step:1512/2110 train_time:75812ms step_avg:50.14ms
step:1513/2110 train_time:75902ms step_avg:50.17ms
step:1514/2110 train_time:75990ms step_avg:50.19ms
step:1515/2110 train_time:76079ms step_avg:50.22ms
step:1516/2110 train_time:76166ms step_avg:50.24ms
step:1517/2110 train_time:76251ms step_avg:50.26ms
step:1518/2110 train_time:76339ms step_avg:50.29ms
step:1519/2110 train_time:76425ms step_avg:50.31ms
step:1520/2110 train_time:76511ms step_avg:50.34ms
step:1521/2110 train_time:76596ms step_avg:50.36ms
step:1522/2110 train_time:76682ms step_avg:50.38ms
step:1523/2110 train_time:76770ms step_avg:50.41ms
step:1524/2110 train_time:76858ms step_avg:50.43ms
step:1525/2110 train_time:76945ms step_avg:50.46ms
step:1526/2110 train_time:77034ms step_avg:50.48ms
step:1527/2110 train_time:77121ms step_avg:50.51ms
step:1528/2110 train_time:77208ms step_avg:50.53ms
step:1529/2110 train_time:77294ms step_avg:50.55ms
step:1530/2110 train_time:77382ms step_avg:50.58ms
step:1531/2110 train_time:77467ms step_avg:50.60ms
step:1532/2110 train_time:77553ms step_avg:50.62ms
step:1533/2110 train_time:77640ms step_avg:50.65ms
step:1534/2110 train_time:77726ms step_avg:50.67ms
step:1535/2110 train_time:77813ms step_avg:50.69ms
step:1536/2110 train_time:77902ms step_avg:50.72ms
step:1537/2110 train_time:77990ms step_avg:50.74ms
step:1538/2110 train_time:78077ms step_avg:50.77ms
step:1539/2110 train_time:78165ms step_avg:50.79ms
step:1540/2110 train_time:78252ms step_avg:50.81ms
step:1541/2110 train_time:78339ms step_avg:50.84ms
step:1542/2110 train_time:78425ms step_avg:50.86ms
step:1543/2110 train_time:78511ms step_avg:50.88ms
step:1544/2110 train_time:78597ms step_avg:50.90ms
step:1545/2110 train_time:78684ms step_avg:50.93ms
step:1546/2110 train_time:78771ms step_avg:50.95ms
step:1547/2110 train_time:78857ms step_avg:50.97ms
step:1548/2110 train_time:78945ms step_avg:51.00ms
step:1549/2110 train_time:79032ms step_avg:51.02ms
step:1550/2110 train_time:79120ms step_avg:51.05ms
step:1551/2110 train_time:79207ms step_avg:51.07ms
step:1552/2110 train_time:79294ms step_avg:51.09ms
step:1553/2110 train_time:79380ms step_avg:51.11ms
step:1554/2110 train_time:79467ms step_avg:51.14ms
step:1555/2110 train_time:79553ms step_avg:51.16ms
step:1556/2110 train_time:79640ms step_avg:51.18ms
step:1557/2110 train_time:79726ms step_avg:51.21ms
step:1558/2110 train_time:79814ms step_avg:51.23ms
step:1559/2110 train_time:79901ms step_avg:51.25ms
step:1560/2110 train_time:79988ms step_avg:51.27ms
step:1561/2110 train_time:80077ms step_avg:51.30ms
step:1562/2110 train_time:80164ms step_avg:51.32ms
step:1563/2110 train_time:80250ms step_avg:51.34ms
step:1564/2110 train_time:80337ms step_avg:51.37ms
step:1565/2110 train_time:80424ms step_avg:51.39ms
step:1566/2110 train_time:80511ms step_avg:51.41ms
step:1567/2110 train_time:80597ms step_avg:51.43ms
step:1568/2110 train_time:80685ms step_avg:51.46ms
step:1569/2110 train_time:80771ms step_avg:51.48ms
step:1570/2110 train_time:80858ms step_avg:51.50ms
step:1571/2110 train_time:80946ms step_avg:51.52ms
step:1572/2110 train_time:81034ms step_avg:51.55ms
step:1573/2110 train_time:81120ms step_avg:51.57ms
step:1574/2110 train_time:81207ms step_avg:51.59ms
step:1575/2110 train_time:81293ms step_avg:51.61ms
step:1576/2110 train_time:81381ms step_avg:51.64ms
step:1577/2110 train_time:81467ms step_avg:51.66ms
step:1578/2110 train_time:81554ms step_avg:51.68ms
step:1579/2110 train_time:81640ms step_avg:51.70ms
step:1580/2110 train_time:81727ms step_avg:51.73ms
step:1581/2110 train_time:81813ms step_avg:51.75ms
step:1582/2110 train_time:81900ms step_avg:51.77ms
step:1583/2110 train_time:81987ms step_avg:51.79ms
step:1584/2110 train_time:82073ms step_avg:51.81ms
step:1585/2110 train_time:82162ms step_avg:51.84ms
step:1586/2110 train_time:82248ms step_avg:51.86ms
step:1587/2110 train_time:82335ms step_avg:51.88ms
step:1588/2110 train_time:82422ms step_avg:51.90ms
step:1589/2110 train_time:82509ms step_avg:51.92ms
step:1590/2110 train_time:82596ms step_avg:51.95ms
step:1591/2110 train_time:82684ms step_avg:51.97ms
step:1592/2110 train_time:82770ms step_avg:51.99ms
step:1593/2110 train_time:82857ms step_avg:52.01ms
step:1594/2110 train_time:82946ms step_avg:52.04ms
step:1595/2110 train_time:83032ms step_avg:52.06ms
step:1596/2110 train_time:83119ms step_avg:52.08ms
step:1597/2110 train_time:83207ms step_avg:52.10ms
step:1598/2110 train_time:83292ms step_avg:52.12ms
step:1599/2110 train_time:83379ms step_avg:52.14ms
step:1600/2110 train_time:83466ms step_avg:52.17ms
step:1601/2110 train_time:83553ms step_avg:52.19ms
step:1602/2110 train_time:83642ms step_avg:52.21ms
step:1603/2110 train_time:83728ms step_avg:52.23ms
step:1604/2110 train_time:83815ms step_avg:52.25ms
step:1605/2110 train_time:83902ms step_avg:52.28ms
step:1606/2110 train_time:83988ms step_avg:52.30ms
step:1607/2110 train_time:84077ms step_avg:52.32ms
step:1608/2110 train_time:84165ms step_avg:52.34ms
step:1609/2110 train_time:84251ms step_avg:52.36ms
step:1610/2110 train_time:84339ms step_avg:52.38ms
step:1611/2110 train_time:84425ms step_avg:52.41ms
step:1612/2110 train_time:84511ms step_avg:52.43ms
step:1613/2110 train_time:84598ms step_avg:52.45ms
step:1614/2110 train_time:84687ms step_avg:52.47ms
step:1615/2110 train_time:84772ms step_avg:52.49ms
step:1616/2110 train_time:84860ms step_avg:52.51ms
step:1617/2110 train_time:84946ms step_avg:52.53ms
step:1618/2110 train_time:85033ms step_avg:52.55ms
step:1619/2110 train_time:85120ms step_avg:52.58ms
step:1620/2110 train_time:85207ms step_avg:52.60ms
step:1621/2110 train_time:85294ms step_avg:52.62ms
step:1622/2110 train_time:85382ms step_avg:52.64ms
step:1623/2110 train_time:85468ms step_avg:52.66ms
step:1624/2110 train_time:85555ms step_avg:52.68ms
step:1625/2110 train_time:85642ms step_avg:52.70ms
step:1626/2110 train_time:85729ms step_avg:52.72ms
step:1627/2110 train_time:85815ms step_avg:52.74ms
step:1628/2110 train_time:85902ms step_avg:52.77ms
step:1629/2110 train_time:85989ms step_avg:52.79ms
step:1630/2110 train_time:86077ms step_avg:52.81ms
step:1631/2110 train_time:86163ms step_avg:52.83ms
step:1632/2110 train_time:86250ms step_avg:52.85ms
step:1633/2110 train_time:86336ms step_avg:52.87ms
step:1634/2110 train_time:86424ms step_avg:52.89ms
step:1635/2110 train_time:86510ms step_avg:52.91ms
step:1636/2110 train_time:86598ms step_avg:52.93ms
step:1637/2110 train_time:86684ms step_avg:52.95ms
step:1638/2110 train_time:86771ms step_avg:52.97ms
step:1639/2110 train_time:86858ms step_avg:52.99ms
step:1640/2110 train_time:86945ms step_avg:53.02ms
step:1641/2110 train_time:87032ms step_avg:53.04ms
step:1642/2110 train_time:87119ms step_avg:53.06ms
step:1643/2110 train_time:87206ms step_avg:53.08ms
step:1644/2110 train_time:87293ms step_avg:53.10ms
step:1645/2110 train_time:87380ms step_avg:53.12ms
step:1646/2110 train_time:87466ms step_avg:53.14ms
step:1647/2110 train_time:87554ms step_avg:53.16ms
step:1648/2110 train_time:87642ms step_avg:53.18ms
step:1649/2110 train_time:87728ms step_avg:53.20ms
step:1650/2110 train_time:87815ms step_avg:53.22ms
step:1651/2110 train_time:87902ms step_avg:53.24ms
step:1652/2110 train_time:87988ms step_avg:53.26ms
step:1653/2110 train_time:88075ms step_avg:53.28ms
step:1654/2110 train_time:88162ms step_avg:53.30ms
step:1655/2110 train_time:88248ms step_avg:53.32ms
step:1656/2110 train_time:88336ms step_avg:53.34ms
step:1657/2110 train_time:88423ms step_avg:53.36ms
step:1658/2110 train_time:88511ms step_avg:53.38ms
step:1659/2110 train_time:88599ms step_avg:53.41ms
step:1660/2110 train_time:88687ms step_avg:53.43ms
step:1661/2110 train_time:88775ms step_avg:53.45ms
step:1662/2110 train_time:88864ms step_avg:53.47ms
step:1663/2110 train_time:88951ms step_avg:53.49ms
step:1664/2110 train_time:89040ms step_avg:53.51ms
step:1665/2110 train_time:89128ms step_avg:53.53ms
step:1666/2110 train_time:89216ms step_avg:53.55ms
step:1667/2110 train_time:89305ms step_avg:53.57ms
step:1668/2110 train_time:89393ms step_avg:53.59ms
step:1669/2110 train_time:89481ms step_avg:53.61ms
step:1670/2110 train_time:89569ms step_avg:53.63ms
step:1671/2110 train_time:89657ms step_avg:53.65ms
step:1672/2110 train_time:89746ms step_avg:53.68ms
step:1673/2110 train_time:89833ms step_avg:53.70ms
step:1674/2110 train_time:89921ms step_avg:53.72ms
step:1675/2110 train_time:90009ms step_avg:53.74ms
step:1676/2110 train_time:90097ms step_avg:53.76ms
step:1677/2110 train_time:90185ms step_avg:53.78ms
step:1678/2110 train_time:90272ms step_avg:53.80ms
step:1679/2110 train_time:90360ms step_avg:53.82ms
step:1680/2110 train_time:90448ms step_avg:53.84ms
step:1681/2110 train_time:90536ms step_avg:53.86ms
step:1682/2110 train_time:90624ms step_avg:53.88ms
step:1683/2110 train_time:90712ms step_avg:53.90ms
step:1684/2110 train_time:90801ms step_avg:53.92ms
step:1685/2110 train_time:90888ms step_avg:53.94ms
step:1686/2110 train_time:90977ms step_avg:53.96ms
step:1687/2110 train_time:91066ms step_avg:53.98ms
step:1688/2110 train_time:91155ms step_avg:54.00ms
step:1689/2110 train_time:91243ms step_avg:54.02ms
step:1690/2110 train_time:91331ms step_avg:54.04ms
step:1691/2110 train_time:91419ms step_avg:54.06ms
step:1692/2110 train_time:91507ms step_avg:54.08ms
step:1693/2110 train_time:91595ms step_avg:54.10ms
step:1694/2110 train_time:91683ms step_avg:54.12ms
step:1695/2110 train_time:91771ms step_avg:54.14ms
step:1696/2110 train_time:91860ms step_avg:54.16ms
step:1697/2110 train_time:91948ms step_avg:54.18ms
step:1698/2110 train_time:92036ms step_avg:54.20ms
step:1699/2110 train_time:92123ms step_avg:54.22ms
step:1700/2110 train_time:92211ms step_avg:54.24ms
step:1701/2110 train_time:92300ms step_avg:54.26ms
step:1702/2110 train_time:92387ms step_avg:54.28ms
step:1703/2110 train_time:92477ms step_avg:54.30ms
step:1704/2110 train_time:92565ms step_avg:54.32ms
step:1705/2110 train_time:92653ms step_avg:54.34ms
step:1706/2110 train_time:92742ms step_avg:54.36ms
step:1707/2110 train_time:92830ms step_avg:54.38ms
step:1708/2110 train_time:92918ms step_avg:54.40ms
step:1709/2110 train_time:93007ms step_avg:54.42ms
step:1710/2110 train_time:93096ms step_avg:54.44ms
step:1711/2110 train_time:93184ms step_avg:54.46ms
step:1712/2110 train_time:93272ms step_avg:54.48ms
step:1713/2110 train_time:93359ms step_avg:54.50ms
step:1714/2110 train_time:93448ms step_avg:54.52ms
step:1715/2110 train_time:93535ms step_avg:54.54ms
step:1716/2110 train_time:93623ms step_avg:54.56ms
step:1717/2110 train_time:93712ms step_avg:54.58ms
step:1718/2110 train_time:93801ms step_avg:54.60ms
step:1719/2110 train_time:93889ms step_avg:54.62ms
step:1720/2110 train_time:93978ms step_avg:54.64ms
step:1721/2110 train_time:94066ms step_avg:54.66ms
step:1722/2110 train_time:94155ms step_avg:54.68ms
step:1723/2110 train_time:94243ms step_avg:54.70ms
step:1724/2110 train_time:94331ms step_avg:54.72ms
step:1725/2110 train_time:94419ms step_avg:54.74ms
step:1726/2110 train_time:94507ms step_avg:54.75ms
step:1727/2110 train_time:94596ms step_avg:54.77ms
step:1728/2110 train_time:94684ms step_avg:54.79ms
step:1729/2110 train_time:94772ms step_avg:54.81ms
step:1730/2110 train_time:94860ms step_avg:54.83ms
step:1731/2110 train_time:94948ms step_avg:54.85ms
step:1732/2110 train_time:95037ms step_avg:54.87ms
step:1733/2110 train_time:95126ms step_avg:54.89ms
step:1734/2110 train_time:95215ms step_avg:54.91ms
step:1735/2110 train_time:95303ms step_avg:54.93ms
step:1736/2110 train_time:95391ms step_avg:54.95ms
step:1737/2110 train_time:95479ms step_avg:54.97ms
step:1738/2110 train_time:95567ms step_avg:54.99ms
step:1739/2110 train_time:95655ms step_avg:55.01ms
step:1740/2110 train_time:95744ms step_avg:55.03ms
step:1741/2110 train_time:95832ms step_avg:55.04ms
step:1742/2110 train_time:95920ms step_avg:55.06ms
step:1743/2110 train_time:96007ms step_avg:55.08ms
step:1744/2110 train_time:96096ms step_avg:55.10ms
step:1745/2110 train_time:96186ms step_avg:55.12ms
step:1746/2110 train_time:96275ms step_avg:55.14ms
step:1747/2110 train_time:96362ms step_avg:55.16ms
step:1748/2110 train_time:96450ms step_avg:55.18ms
step:1749/2110 train_time:96538ms step_avg:55.20ms
step:1750/2110 train_time:96626ms step_avg:55.21ms
step:1750/2110 val_loss:3.3787 train_time:96715ms step_avg:55.27ms
step:1751/2110 train_time:96765ms step_avg:55.26ms
step:1752/2110 train_time:96814ms step_avg:55.26ms
step:1753/2110 train_time:96903ms step_avg:55.28ms
step:1754/2110 train_time:96992ms step_avg:55.30ms
step:1755/2110 train_time:97079ms step_avg:55.32ms
step:1756/2110 train_time:97166ms step_avg:55.33ms
step:1757/2110 train_time:97253ms step_avg:55.35ms
step:1758/2110 train_time:97340ms step_avg:55.37ms
step:1759/2110 train_time:97426ms step_avg:55.39ms
step:1760/2110 train_time:97512ms step_avg:55.40ms
step:1761/2110 train_time:97600ms step_avg:55.42ms
step:1762/2110 train_time:97689ms step_avg:55.44ms
step:1763/2110 train_time:97781ms step_avg:55.46ms
step:1764/2110 train_time:97872ms step_avg:55.48ms
step:1765/2110 train_time:97964ms step_avg:55.50ms
step:1766/2110 train_time:98053ms step_avg:55.52ms
step:1767/2110 train_time:98140ms step_avg:55.54ms
step:1768/2110 train_time:98230ms step_avg:55.56ms
step:1769/2110 train_time:98316ms step_avg:55.58ms
step:1770/2110 train_time:98404ms step_avg:55.60ms
step:1771/2110 train_time:98490ms step_avg:55.61ms
step:1772/2110 train_time:98577ms step_avg:55.63ms
step:1773/2110 train_time:98666ms step_avg:55.65ms
step:1774/2110 train_time:98755ms step_avg:55.67ms
step:1775/2110 train_time:98845ms step_avg:55.69ms
step:1776/2110 train_time:98935ms step_avg:55.71ms
step:1777/2110 train_time:99024ms step_avg:55.73ms
step:1778/2110 train_time:99113ms step_avg:55.74ms
step:1779/2110 train_time:99199ms step_avg:55.76ms
step:1780/2110 train_time:99289ms step_avg:55.78ms
step:1781/2110 train_time:99376ms step_avg:55.80ms
step:1782/2110 train_time:99464ms step_avg:55.82ms
step:1783/2110 train_time:99550ms step_avg:55.83ms
step:1784/2110 train_time:99639ms step_avg:55.85ms
step:1785/2110 train_time:99726ms step_avg:55.87ms
step:1786/2110 train_time:99816ms step_avg:55.89ms
step:1787/2110 train_time:99904ms step_avg:55.91ms
step:1788/2110 train_time:99994ms step_avg:55.92ms
step:1789/2110 train_time:100081ms step_avg:55.94ms
step:1790/2110 train_time:100171ms step_avg:55.96ms
step:1791/2110 train_time:100258ms step_avg:55.98ms
step:1792/2110 train_time:100346ms step_avg:56.00ms
step:1793/2110 train_time:100433ms step_avg:56.01ms
step:1794/2110 train_time:100521ms step_avg:56.03ms
step:1795/2110 train_time:100608ms step_avg:56.05ms
step:1796/2110 train_time:100697ms step_avg:56.07ms
step:1797/2110 train_time:100784ms step_avg:56.08ms
step:1798/2110 train_time:100875ms step_avg:56.10ms
step:1799/2110 train_time:100962ms step_avg:56.12ms
step:1800/2110 train_time:101052ms step_avg:56.14ms
step:1801/2110 train_time:101138ms step_avg:56.16ms
step:1802/2110 train_time:101228ms step_avg:56.18ms
step:1803/2110 train_time:101314ms step_avg:56.19ms
step:1804/2110 train_time:101402ms step_avg:56.21ms
step:1805/2110 train_time:101488ms step_avg:56.23ms
step:1806/2110 train_time:101579ms step_avg:56.25ms
step:1807/2110 train_time:101664ms step_avg:56.26ms
step:1808/2110 train_time:101753ms step_avg:56.28ms
step:1809/2110 train_time:101841ms step_avg:56.30ms
step:1810/2110 train_time:101931ms step_avg:56.32ms
step:1811/2110 train_time:102019ms step_avg:56.33ms
step:1812/2110 train_time:102108ms step_avg:56.35ms
step:1813/2110 train_time:102195ms step_avg:56.37ms
step:1814/2110 train_time:102285ms step_avg:56.39ms
step:1815/2110 train_time:102371ms step_avg:56.40ms
step:1816/2110 train_time:102459ms step_avg:56.42ms
step:1817/2110 train_time:102546ms step_avg:56.44ms
step:1818/2110 train_time:102635ms step_avg:56.45ms
step:1819/2110 train_time:102722ms step_avg:56.47ms
step:1820/2110 train_time:102811ms step_avg:56.49ms
step:1821/2110 train_time:102897ms step_avg:56.51ms
step:1822/2110 train_time:102985ms step_avg:56.52ms
step:1823/2110 train_time:103073ms step_avg:56.54ms
step:1824/2110 train_time:103161ms step_avg:56.56ms
step:1825/2110 train_time:103249ms step_avg:56.57ms
step:1826/2110 train_time:103336ms step_avg:56.59ms
step:1827/2110 train_time:103424ms step_avg:56.61ms
step:1828/2110 train_time:103512ms step_avg:56.63ms
step:1829/2110 train_time:103599ms step_avg:56.64ms
step:1830/2110 train_time:103687ms step_avg:56.66ms
step:1831/2110 train_time:103776ms step_avg:56.68ms
step:1832/2110 train_time:103865ms step_avg:56.69ms
step:1833/2110 train_time:103953ms step_avg:56.71ms
step:1834/2110 train_time:104041ms step_avg:56.73ms
step:1835/2110 train_time:104129ms step_avg:56.75ms
step:1836/2110 train_time:104216ms step_avg:56.76ms
step:1837/2110 train_time:104306ms step_avg:56.78ms
step:1838/2110 train_time:104394ms step_avg:56.80ms
step:1839/2110 train_time:104481ms step_avg:56.81ms
step:1840/2110 train_time:104570ms step_avg:56.83ms
step:1841/2110 train_time:104657ms step_avg:56.85ms
step:1842/2110 train_time:104746ms step_avg:56.87ms
step:1843/2110 train_time:104833ms step_avg:56.88ms
step:1844/2110 train_time:104921ms step_avg:56.90ms
step:1845/2110 train_time:105010ms step_avg:56.92ms
step:1846/2110 train_time:105101ms step_avg:56.93ms
step:1847/2110 train_time:105186ms step_avg:56.95ms
step:1848/2110 train_time:105276ms step_avg:56.97ms
step:1849/2110 train_time:105363ms step_avg:56.98ms
step:1850/2110 train_time:105451ms step_avg:57.00ms
step:1851/2110 train_time:105538ms step_avg:57.02ms
step:1852/2110 train_time:105625ms step_avg:57.03ms
step:1853/2110 train_time:105713ms step_avg:57.05ms
step:1854/2110 train_time:105801ms step_avg:57.07ms
step:1855/2110 train_time:105889ms step_avg:57.08ms
step:1856/2110 train_time:105978ms step_avg:57.10ms
step:1857/2110 train_time:106066ms step_avg:57.12ms
step:1858/2110 train_time:106155ms step_avg:57.13ms
step:1859/2110 train_time:106243ms step_avg:57.15ms
step:1860/2110 train_time:106332ms step_avg:57.17ms
step:1861/2110 train_time:106419ms step_avg:57.18ms
step:1862/2110 train_time:106508ms step_avg:57.20ms
step:1863/2110 train_time:106597ms step_avg:57.22ms
step:1864/2110 train_time:106685ms step_avg:57.23ms
step:1865/2110 train_time:106773ms step_avg:57.25ms
step:1866/2110 train_time:106860ms step_avg:57.27ms
step:1867/2110 train_time:106949ms step_avg:57.28ms
step:1868/2110 train_time:107039ms step_avg:57.30ms
step:1869/2110 train_time:107125ms step_avg:57.32ms
step:1870/2110 train_time:107215ms step_avg:57.33ms
step:1871/2110 train_time:107303ms step_avg:57.35ms
step:1872/2110 train_time:107391ms step_avg:57.37ms
step:1873/2110 train_time:107479ms step_avg:57.38ms
step:1874/2110 train_time:107568ms step_avg:57.40ms
step:1875/2110 train_time:107656ms step_avg:57.42ms
step:1876/2110 train_time:107745ms step_avg:57.43ms
step:1877/2110 train_time:107833ms step_avg:57.45ms
step:1878/2110 train_time:107920ms step_avg:57.47ms
step:1879/2110 train_time:108011ms step_avg:57.48ms
step:1880/2110 train_time:108099ms step_avg:57.50ms
step:1881/2110 train_time:108188ms step_avg:57.52ms
step:1882/2110 train_time:108278ms step_avg:57.53ms
step:1883/2110 train_time:108364ms step_avg:57.55ms
step:1884/2110 train_time:108453ms step_avg:57.57ms
step:1885/2110 train_time:108539ms step_avg:57.58ms
step:1886/2110 train_time:108628ms step_avg:57.60ms
step:1887/2110 train_time:108716ms step_avg:57.61ms
step:1888/2110 train_time:108805ms step_avg:57.63ms
step:1889/2110 train_time:108892ms step_avg:57.65ms
step:1890/2110 train_time:108981ms step_avg:57.66ms
step:1891/2110 train_time:109069ms step_avg:57.68ms
step:1892/2110 train_time:109158ms step_avg:57.69ms
step:1893/2110 train_time:109245ms step_avg:57.71ms
step:1894/2110 train_time:109333ms step_avg:57.73ms
step:1895/2110 train_time:109421ms step_avg:57.74ms
step:1896/2110 train_time:109510ms step_avg:57.76ms
step:1897/2110 train_time:109597ms step_avg:57.77ms
step:1898/2110 train_time:109686ms step_avg:57.79ms
step:1899/2110 train_time:109772ms step_avg:57.81ms
step:1900/2110 train_time:109861ms step_avg:57.82ms
step:1901/2110 train_time:109950ms step_avg:57.84ms
step:1902/2110 train_time:110040ms step_avg:57.85ms
step:1903/2110 train_time:110127ms step_avg:57.87ms
step:1904/2110 train_time:110216ms step_avg:57.89ms
step:1905/2110 train_time:110303ms step_avg:57.90ms
step:1906/2110 train_time:110392ms step_avg:57.92ms
step:1907/2110 train_time:110480ms step_avg:57.93ms
step:1908/2110 train_time:110569ms step_avg:57.95ms
step:1909/2110 train_time:110657ms step_avg:57.97ms
step:1910/2110 train_time:110744ms step_avg:57.98ms
step:1911/2110 train_time:110831ms step_avg:58.00ms
step:1912/2110 train_time:110919ms step_avg:58.01ms
step:1913/2110 train_time:111008ms step_avg:58.03ms
step:1914/2110 train_time:111097ms step_avg:58.04ms
step:1915/2110 train_time:111186ms step_avg:58.06ms
step:1916/2110 train_time:111276ms step_avg:58.08ms
step:1917/2110 train_time:111362ms step_avg:58.09ms
step:1918/2110 train_time:111450ms step_avg:58.11ms
step:1919/2110 train_time:111539ms step_avg:58.12ms
step:1920/2110 train_time:111628ms step_avg:58.14ms
step:1921/2110 train_time:111715ms step_avg:58.15ms
step:1922/2110 train_time:111804ms step_avg:58.17ms
step:1923/2110 train_time:111891ms step_avg:58.19ms
step:1924/2110 train_time:111980ms step_avg:58.20ms
step:1925/2110 train_time:112067ms step_avg:58.22ms
step:1926/2110 train_time:112157ms step_avg:58.23ms
step:1927/2110 train_time:112244ms step_avg:58.25ms
step:1928/2110 train_time:112333ms step_avg:58.26ms
step:1929/2110 train_time:112419ms step_avg:58.28ms
step:1930/2110 train_time:112508ms step_avg:58.29ms
step:1931/2110 train_time:112595ms step_avg:58.31ms
step:1932/2110 train_time:112684ms step_avg:58.33ms
step:1933/2110 train_time:112771ms step_avg:58.34ms
step:1934/2110 train_time:112860ms step_avg:58.36ms
step:1935/2110 train_time:112947ms step_avg:58.37ms
step:1936/2110 train_time:113036ms step_avg:58.39ms
step:1937/2110 train_time:113123ms step_avg:58.40ms
step:1938/2110 train_time:113214ms step_avg:58.42ms
step:1939/2110 train_time:113300ms step_avg:58.43ms
step:1940/2110 train_time:113390ms step_avg:58.45ms
step:1941/2110 train_time:113478ms step_avg:58.46ms
step:1942/2110 train_time:113567ms step_avg:58.48ms
step:1943/2110 train_time:113654ms step_avg:58.49ms
step:1944/2110 train_time:113743ms step_avg:58.51ms
step:1945/2110 train_time:113829ms step_avg:58.52ms
step:1946/2110 train_time:113919ms step_avg:58.54ms
step:1947/2110 train_time:114005ms step_avg:58.55ms
step:1948/2110 train_time:114094ms step_avg:58.57ms
step:1949/2110 train_time:114182ms step_avg:58.58ms
step:1950/2110 train_time:114272ms step_avg:58.60ms
step:1951/2110 train_time:114359ms step_avg:58.62ms
step:1952/2110 train_time:114448ms step_avg:58.63ms
step:1953/2110 train_time:114535ms step_avg:58.65ms
step:1954/2110 train_time:114623ms step_avg:58.66ms
step:1955/2110 train_time:114711ms step_avg:58.68ms
step:1956/2110 train_time:114801ms step_avg:58.69ms
step:1957/2110 train_time:114888ms step_avg:58.71ms
step:1958/2110 train_time:114977ms step_avg:58.72ms
step:1959/2110 train_time:115064ms step_avg:58.74ms
step:1960/2110 train_time:115150ms step_avg:58.75ms
step:1961/2110 train_time:115237ms step_avg:58.76ms
step:1962/2110 train_time:115326ms step_avg:58.78ms
step:1963/2110 train_time:115414ms step_avg:58.79ms
step:1964/2110 train_time:115503ms step_avg:58.81ms
step:1965/2110 train_time:115591ms step_avg:58.83ms
step:1966/2110 train_time:115679ms step_avg:58.84ms
step:1967/2110 train_time:115767ms step_avg:58.85ms
step:1968/2110 train_time:115855ms step_avg:58.87ms
step:1969/2110 train_time:115943ms step_avg:58.88ms
step:1970/2110 train_time:116031ms step_avg:58.90ms
step:1971/2110 train_time:116119ms step_avg:58.91ms
step:1972/2110 train_time:116208ms step_avg:58.93ms
step:1973/2110 train_time:116296ms step_avg:58.94ms
step:1974/2110 train_time:116386ms step_avg:58.96ms
step:1975/2110 train_time:116474ms step_avg:58.97ms
step:1976/2110 train_time:116562ms step_avg:58.99ms
step:1977/2110 train_time:116650ms step_avg:59.00ms
step:1978/2110 train_time:116737ms step_avg:59.02ms
step:1979/2110 train_time:116826ms step_avg:59.03ms
step:1980/2110 train_time:116915ms step_avg:59.05ms
step:1981/2110 train_time:117003ms step_avg:59.06ms
step:1982/2110 train_time:117092ms step_avg:59.08ms
step:1983/2110 train_time:117179ms step_avg:59.09ms
step:1984/2110 train_time:117266ms step_avg:59.11ms
step:1985/2110 train_time:117355ms step_avg:59.12ms
step:1986/2110 train_time:117444ms step_avg:59.14ms
step:1987/2110 train_time:117533ms step_avg:59.15ms
step:1988/2110 train_time:117622ms step_avg:59.17ms
step:1989/2110 train_time:117710ms step_avg:59.18ms
step:1990/2110 train_time:117797ms step_avg:59.19ms
step:1991/2110 train_time:117885ms step_avg:59.21ms
step:1992/2110 train_time:117973ms step_avg:59.22ms
step:1993/2110 train_time:118061ms step_avg:59.24ms
step:1994/2110 train_time:118149ms step_avg:59.25ms
step:1995/2110 train_time:118237ms step_avg:59.27ms
step:1996/2110 train_time:118325ms step_avg:59.28ms
step:1997/2110 train_time:118415ms step_avg:59.30ms
step:1998/2110 train_time:118502ms step_avg:59.31ms
step:1999/2110 train_time:118592ms step_avg:59.33ms
step:2000/2110 train_time:118679ms step_avg:59.34ms
step:2000/2110 val_loss:3.3036 train_time:118770ms step_avg:59.38ms
step:2001/2110 train_time:118802ms step_avg:59.37ms
step:2002/2110 train_time:118859ms step_avg:59.37ms
step:2003/2110 train_time:118949ms step_avg:59.39ms
step:2004/2110 train_time:119038ms step_avg:59.40ms
step:2005/2110 train_time:119126ms step_avg:59.41ms
step:2006/2110 train_time:119213ms step_avg:59.43ms
step:2007/2110 train_time:119301ms step_avg:59.44ms
step:2008/2110 train_time:119387ms step_avg:59.46ms
step:2009/2110 train_time:119475ms step_avg:59.47ms
step:2010/2110 train_time:119563ms step_avg:59.48ms
step:2011/2110 train_time:119651ms step_avg:59.50ms
step:2012/2110 train_time:119740ms step_avg:59.51ms
step:2013/2110 train_time:119832ms step_avg:59.53ms
step:2014/2110 train_time:119920ms step_avg:59.54ms
step:2015/2110 train_time:120010ms step_avg:59.56ms
step:2016/2110 train_time:120098ms step_avg:59.57ms
step:2017/2110 train_time:120186ms step_avg:59.59ms
step:2018/2110 train_time:120274ms step_avg:59.60ms
step:2019/2110 train_time:120362ms step_avg:59.61ms
step:2020/2110 train_time:120448ms step_avg:59.63ms
step:2021/2110 train_time:120537ms step_avg:59.64ms
step:2022/2110 train_time:120624ms step_avg:59.66ms
step:2023/2110 train_time:120713ms step_avg:59.67ms
step:2024/2110 train_time:120802ms step_avg:59.68ms
step:2025/2110 train_time:120893ms step_avg:59.70ms
step:2026/2110 train_time:120981ms step_avg:59.71ms
step:2027/2110 train_time:121069ms step_avg:59.73ms
step:2028/2110 train_time:121156ms step_avg:59.74ms
step:2029/2110 train_time:121245ms step_avg:59.76ms
step:2030/2110 train_time:121332ms step_avg:59.77ms
step:2031/2110 train_time:121421ms step_avg:59.78ms
step:2032/2110 train_time:121509ms step_avg:59.80ms
step:2033/2110 train_time:121598ms step_avg:59.81ms
step:2034/2110 train_time:121685ms step_avg:59.83ms
step:2035/2110 train_time:121774ms step_avg:59.84ms
step:2036/2110 train_time:121862ms step_avg:59.85ms
step:2037/2110 train_time:121953ms step_avg:59.87ms
step:2038/2110 train_time:122041ms step_avg:59.88ms
step:2039/2110 train_time:122129ms step_avg:59.90ms
step:2040/2110 train_time:122216ms step_avg:59.91ms
step:2041/2110 train_time:122304ms step_avg:59.92ms
step:2042/2110 train_time:122392ms step_avg:59.94ms
step:2043/2110 train_time:122480ms step_avg:59.95ms
step:2044/2110 train_time:122567ms step_avg:59.96ms
step:2045/2110 train_time:122656ms step_avg:59.98ms
step:2046/2110 train_time:122744ms step_avg:59.99ms
step:2047/2110 train_time:122833ms step_avg:60.01ms
step:2048/2110 train_time:122922ms step_avg:60.02ms
step:2049/2110 train_time:123012ms step_avg:60.03ms
step:2050/2110 train_time:123099ms step_avg:60.05ms
step:2051/2110 train_time:123187ms step_avg:60.06ms
step:2052/2110 train_time:123275ms step_avg:60.08ms
step:2053/2110 train_time:123363ms step_avg:60.09ms
step:2054/2110 train_time:123450ms step_avg:60.10ms
step:2055/2110 train_time:123539ms step_avg:60.12ms
step:2056/2110 train_time:123626ms step_avg:60.13ms
step:2057/2110 train_time:123715ms step_avg:60.14ms
step:2058/2110 train_time:123803ms step_avg:60.16ms
step:2059/2110 train_time:123892ms step_avg:60.17ms
step:2060/2110 train_time:123980ms step_avg:60.18ms
step:2061/2110 train_time:124068ms step_avg:60.20ms
step:2062/2110 train_time:124155ms step_avg:60.21ms
step:2063/2110 train_time:124244ms step_avg:60.23ms
step:2064/2110 train_time:124332ms step_avg:60.24ms
step:2065/2110 train_time:124420ms step_avg:60.25ms
step:2066/2110 train_time:124508ms step_avg:60.27ms
step:2067/2110 train_time:124598ms step_avg:60.28ms
step:2068/2110 train_time:124685ms step_avg:60.29ms
step:2069/2110 train_time:124774ms step_avg:60.31ms
step:2070/2110 train_time:124862ms step_avg:60.32ms
step:2071/2110 train_time:124950ms step_avg:60.33ms
step:2072/2110 train_time:125039ms step_avg:60.35ms
step:2073/2110 train_time:125127ms step_avg:60.36ms
step:2074/2110 train_time:125215ms step_avg:60.37ms
step:2075/2110 train_time:125305ms step_avg:60.39ms
step:2076/2110 train_time:125392ms step_avg:60.40ms
step:2077/2110 train_time:125481ms step_avg:60.41ms
step:2078/2110 train_time:125568ms step_avg:60.43ms
step:2079/2110 train_time:125657ms step_avg:60.44ms
step:2080/2110 train_time:125745ms step_avg:60.45ms
step:2081/2110 train_time:125834ms step_avg:60.47ms
step:2082/2110 train_time:125922ms step_avg:60.48ms
step:2083/2110 train_time:126010ms step_avg:60.49ms
step:2084/2110 train_time:126098ms step_avg:60.51ms
step:2085/2110 train_time:126187ms step_avg:60.52ms
step:2086/2110 train_time:126276ms step_avg:60.53ms
step:2087/2110 train_time:126364ms step_avg:60.55ms
step:2088/2110 train_time:126451ms step_avg:60.56ms
step:2089/2110 train_time:126540ms step_avg:60.57ms
step:2090/2110 train_time:126628ms step_avg:60.59ms
step:2091/2110 train_time:126719ms step_avg:60.60ms
step:2092/2110 train_time:126807ms step_avg:60.62ms
step:2093/2110 train_time:126896ms step_avg:60.63ms
step:2094/2110 train_time:126984ms step_avg:60.64ms
step:2095/2110 train_time:127073ms step_avg:60.66ms
step:2096/2110 train_time:127161ms step_avg:60.67ms
step:2097/2110 train_time:127250ms step_avg:60.68ms
step:2098/2110 train_time:127338ms step_avg:60.70ms
step:2099/2110 train_time:127427ms step_avg:60.71ms
step:2100/2110 train_time:127515ms step_avg:60.72ms
step:2101/2110 train_time:127604ms step_avg:60.74ms
step:2102/2110 train_time:127692ms step_avg:60.75ms
step:2103/2110 train_time:127781ms step_avg:60.76ms
step:2104/2110 train_time:127869ms step_avg:60.77ms
step:2105/2110 train_time:127958ms step_avg:60.79ms
step:2106/2110 train_time:128045ms step_avg:60.80ms
step:2107/2110 train_time:128133ms step_avg:60.81ms
step:2108/2110 train_time:128221ms step_avg:60.83ms
step:2109/2110 train_time:128310ms step_avg:60.84ms
step:2110/2110 train_time:128399ms step_avg:60.85ms
step:2110/2110 val_loss:3.2794 train_time:128489ms step_avg:60.90ms
peak memory allocated: 29244 MiB reserved: 44696 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 15 22:40:17 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   38C    P0            126W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   42C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            120W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   36C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   42C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   37C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              80      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              81      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:100ms step_avg:100.10ms
step:2/2110 train_time:136ms step_avg:67.85ms
step:3/2110 train_time:167ms step_avg:55.74ms
step:4/2110 train_time:198ms step_avg:49.47ms
step:5/2110 train_time:223ms step_avg:44.58ms
step:6/2110 train_time:406ms step_avg:67.64ms
step:7/2110 train_time:609ms step_avg:86.96ms
step:8/2110 train_time:649ms step_avg:81.11ms
step:9/2110 train_time:680ms step_avg:75.56ms
step:10/2110 train_time:715ms step_avg:71.52ms
step:11/2110 train_time:745ms step_avg:67.71ms
step:12/2110 train_time:773ms step_avg:64.38ms
step:13/2110 train_time:806ms step_avg:61.97ms
step:14/2110 train_time:842ms step_avg:60.14ms
step:15/2110 train_time:873ms step_avg:58.19ms
step:16/2110 train_time:909ms step_avg:56.79ms
step:17/2110 train_time:941ms step_avg:55.37ms
step:18/2110 train_time:975ms step_avg:54.15ms
step:19/2110 train_time:1009ms step_avg:53.09ms
step:20/2110 train_time:1042ms step_avg:52.11ms
step:21/2110 train_time:1074ms step_avg:51.14ms
step:22/2110 train_time:1105ms step_avg:50.23ms
step:23/2110 train_time:1137ms step_avg:49.44ms
step:24/2110 train_time:1171ms step_avg:48.77ms
step:25/2110 train_time:1203ms step_avg:48.14ms
step:26/2110 train_time:1236ms step_avg:47.54ms
step:27/2110 train_time:1269ms step_avg:47.00ms
step:28/2110 train_time:1304ms step_avg:46.56ms
step:29/2110 train_time:1336ms step_avg:46.08ms
step:30/2110 train_time:1368ms step_avg:45.61ms
step:31/2110 train_time:1402ms step_avg:45.22ms
step:32/2110 train_time:1436ms step_avg:44.86ms
step:33/2110 train_time:1467ms step_avg:44.46ms
step:34/2110 train_time:1502ms step_avg:44.18ms
step:35/2110 train_time:1537ms step_avg:43.92ms
step:36/2110 train_time:1572ms step_avg:43.66ms
step:37/2110 train_time:1604ms step_avg:43.35ms
step:38/2110 train_time:1638ms step_avg:43.11ms
step:39/2110 train_time:1672ms step_avg:42.86ms
step:40/2110 train_time:1706ms step_avg:42.66ms
step:41/2110 train_time:1739ms step_avg:42.42ms
step:42/2110 train_time:1773ms step_avg:42.21ms
step:43/2110 train_time:1806ms step_avg:41.99ms
step:44/2110 train_time:1839ms step_avg:41.80ms
step:45/2110 train_time:1871ms step_avg:41.59ms
step:46/2110 train_time:1906ms step_avg:41.43ms
step:47/2110 train_time:1940ms step_avg:41.27ms
step:48/2110 train_time:1972ms step_avg:41.09ms
step:49/2110 train_time:2004ms step_avg:40.89ms
step:50/2110 train_time:2038ms step_avg:40.76ms
step:51/2110 train_time:2071ms step_avg:40.60ms
step:52/2110 train_time:2103ms step_avg:40.45ms
step:53/2110 train_time:2136ms step_avg:40.31ms
step:54/2110 train_time:2170ms step_avg:40.19ms
step:55/2110 train_time:2203ms step_avg:40.05ms
step:56/2110 train_time:2235ms step_avg:39.90ms
step:57/2110 train_time:2269ms step_avg:39.80ms
step:58/2110 train_time:2301ms step_avg:39.67ms
step:59/2110 train_time:2335ms step_avg:39.58ms
step:60/2110 train_time:2369ms step_avg:39.49ms
step:61/2110 train_time:2400ms step_avg:39.35ms
step:62/2110 train_time:2433ms step_avg:39.24ms
step:63/2110 train_time:2467ms step_avg:39.16ms
step:64/2110 train_time:2501ms step_avg:39.08ms
step:65/2110 train_time:2534ms step_avg:38.98ms
step:66/2110 train_time:2568ms step_avg:38.91ms
step:67/2110 train_time:2600ms step_avg:38.80ms
step:68/2110 train_time:2634ms step_avg:38.74ms
step:69/2110 train_time:2668ms step_avg:38.66ms
step:70/2110 train_time:2701ms step_avg:38.59ms
step:71/2110 train_time:2734ms step_avg:38.50ms
step:72/2110 train_time:2767ms step_avg:38.44ms
step:73/2110 train_time:2799ms step_avg:38.34ms
step:74/2110 train_time:2835ms step_avg:38.30ms
step:75/2110 train_time:2866ms step_avg:38.22ms
step:76/2110 train_time:2900ms step_avg:38.15ms
step:77/2110 train_time:2931ms step_avg:38.07ms
step:78/2110 train_time:2965ms step_avg:38.01ms
step:79/2110 train_time:2997ms step_avg:37.94ms
step:80/2110 train_time:3032ms step_avg:37.90ms
step:81/2110 train_time:3064ms step_avg:37.83ms
step:82/2110 train_time:3099ms step_avg:37.79ms
step:83/2110 train_time:3130ms step_avg:37.72ms
step:84/2110 train_time:3166ms step_avg:37.69ms
step:85/2110 train_time:3195ms step_avg:37.59ms
step:86/2110 train_time:3229ms step_avg:37.54ms
step:87/2110 train_time:3261ms step_avg:37.48ms
step:88/2110 train_time:3294ms step_avg:37.43ms
step:89/2110 train_time:3327ms step_avg:37.38ms
step:90/2110 train_time:3360ms step_avg:37.34ms
step:91/2110 train_time:3393ms step_avg:37.29ms
step:92/2110 train_time:3430ms step_avg:37.28ms
step:93/2110 train_time:3460ms step_avg:37.20ms
step:94/2110 train_time:3493ms step_avg:37.16ms
step:95/2110 train_time:3526ms step_avg:37.12ms
step:96/2110 train_time:3561ms step_avg:37.09ms
step:97/2110 train_time:3592ms step_avg:37.03ms
step:98/2110 train_time:3625ms step_avg:36.99ms
step:99/2110 train_time:3659ms step_avg:36.96ms
step:100/2110 train_time:3692ms step_avg:36.92ms
step:101/2110 train_time:3724ms step_avg:36.87ms
step:102/2110 train_time:3759ms step_avg:36.85ms
step:103/2110 train_time:3791ms step_avg:36.80ms
step:104/2110 train_time:3825ms step_avg:36.78ms
step:105/2110 train_time:3858ms step_avg:36.74ms
step:106/2110 train_time:3890ms step_avg:36.70ms
step:107/2110 train_time:3923ms step_avg:36.66ms
step:108/2110 train_time:3957ms step_avg:36.64ms
step:109/2110 train_time:3989ms step_avg:36.60ms
step:110/2110 train_time:4023ms step_avg:36.57ms
step:111/2110 train_time:4055ms step_avg:36.53ms
step:112/2110 train_time:4089ms step_avg:36.51ms
step:113/2110 train_time:4121ms step_avg:36.47ms
step:114/2110 train_time:4155ms step_avg:36.45ms
step:115/2110 train_time:4189ms step_avg:36.43ms
step:116/2110 train_time:4222ms step_avg:36.39ms
step:117/2110 train_time:4254ms step_avg:36.36ms
step:118/2110 train_time:4288ms step_avg:36.34ms
step:119/2110 train_time:4319ms step_avg:36.30ms
step:120/2110 train_time:4353ms step_avg:36.28ms
step:121/2110 train_time:4386ms step_avg:36.24ms
step:122/2110 train_time:4420ms step_avg:36.23ms
step:123/2110 train_time:4452ms step_avg:36.20ms
step:124/2110 train_time:4486ms step_avg:36.18ms
step:125/2110 train_time:4519ms step_avg:36.15ms
step:126/2110 train_time:4553ms step_avg:36.13ms
step:127/2110 train_time:4585ms step_avg:36.10ms
step:128/2110 train_time:4618ms step_avg:36.08ms
step:129/2110 train_time:4652ms step_avg:36.06ms
step:130/2110 train_time:4686ms step_avg:36.05ms
step:131/2110 train_time:4717ms step_avg:36.01ms
step:132/2110 train_time:4752ms step_avg:36.00ms
step:133/2110 train_time:4784ms step_avg:35.97ms
step:134/2110 train_time:4818ms step_avg:35.96ms
step:135/2110 train_time:4850ms step_avg:35.92ms
step:136/2110 train_time:4884ms step_avg:35.91ms
step:137/2110 train_time:4915ms step_avg:35.88ms
step:138/2110 train_time:4949ms step_avg:35.87ms
step:139/2110 train_time:4982ms step_avg:35.84ms
step:140/2110 train_time:5017ms step_avg:35.84ms
step:141/2110 train_time:5048ms step_avg:35.80ms
step:142/2110 train_time:5083ms step_avg:35.79ms
step:143/2110 train_time:5114ms step_avg:35.76ms
step:144/2110 train_time:5149ms step_avg:35.76ms
step:145/2110 train_time:5181ms step_avg:35.73ms
step:146/2110 train_time:5215ms step_avg:35.72ms
step:147/2110 train_time:5246ms step_avg:35.69ms
step:148/2110 train_time:5280ms step_avg:35.68ms
step:149/2110 train_time:5313ms step_avg:35.66ms
step:150/2110 train_time:5346ms step_avg:35.64ms
step:151/2110 train_time:5378ms step_avg:35.62ms
step:152/2110 train_time:5412ms step_avg:35.60ms
step:153/2110 train_time:5444ms step_avg:35.58ms
step:154/2110 train_time:5479ms step_avg:35.58ms
step:155/2110 train_time:5513ms step_avg:35.56ms
step:156/2110 train_time:5546ms step_avg:35.55ms
step:157/2110 train_time:5578ms step_avg:35.53ms
step:158/2110 train_time:5613ms step_avg:35.52ms
step:159/2110 train_time:5645ms step_avg:35.50ms
step:160/2110 train_time:5679ms step_avg:35.49ms
step:161/2110 train_time:5709ms step_avg:35.46ms
step:162/2110 train_time:5745ms step_avg:35.46ms
step:163/2110 train_time:5778ms step_avg:35.44ms
step:164/2110 train_time:5810ms step_avg:35.43ms
step:165/2110 train_time:5843ms step_avg:35.41ms
step:166/2110 train_time:5879ms step_avg:35.42ms
step:167/2110 train_time:5909ms step_avg:35.38ms
step:168/2110 train_time:5943ms step_avg:35.37ms
step:169/2110 train_time:5977ms step_avg:35.37ms
step:170/2110 train_time:6010ms step_avg:35.35ms
step:171/2110 train_time:6040ms step_avg:35.32ms
step:172/2110 train_time:6077ms step_avg:35.33ms
step:173/2110 train_time:6108ms step_avg:35.30ms
step:174/2110 train_time:6143ms step_avg:35.30ms
step:175/2110 train_time:6172ms step_avg:35.27ms
step:176/2110 train_time:6207ms step_avg:35.27ms
step:177/2110 train_time:6240ms step_avg:35.26ms
step:178/2110 train_time:6274ms step_avg:35.25ms
step:179/2110 train_time:6305ms step_avg:35.22ms
step:180/2110 train_time:6338ms step_avg:35.21ms
step:181/2110 train_time:6372ms step_avg:35.20ms
step:182/2110 train_time:6404ms step_avg:35.19ms
step:183/2110 train_time:6436ms step_avg:35.17ms
step:184/2110 train_time:6471ms step_avg:35.17ms
step:185/2110 train_time:6503ms step_avg:35.15ms
step:186/2110 train_time:6536ms step_avg:35.14ms
step:187/2110 train_time:6569ms step_avg:35.13ms
step:188/2110 train_time:6603ms step_avg:35.12ms
step:189/2110 train_time:6634ms step_avg:35.10ms
step:190/2110 train_time:6669ms step_avg:35.10ms
step:191/2110 train_time:6699ms step_avg:35.07ms
step:192/2110 train_time:6734ms step_avg:35.07ms
step:193/2110 train_time:6765ms step_avg:35.05ms
step:194/2110 train_time:6802ms step_avg:35.06ms
step:195/2110 train_time:6832ms step_avg:35.04ms
step:196/2110 train_time:6866ms step_avg:35.03ms
step:197/2110 train_time:6900ms step_avg:35.03ms
step:198/2110 train_time:6933ms step_avg:35.02ms
step:199/2110 train_time:6965ms step_avg:35.00ms
step:200/2110 train_time:6998ms step_avg:34.99ms
step:201/2110 train_time:7030ms step_avg:34.98ms
step:202/2110 train_time:7064ms step_avg:34.97ms
step:203/2110 train_time:7096ms step_avg:34.96ms
step:204/2110 train_time:7130ms step_avg:34.95ms
step:205/2110 train_time:7162ms step_avg:34.94ms
step:206/2110 train_time:7198ms step_avg:34.94ms
step:207/2110 train_time:7228ms step_avg:34.92ms
step:208/2110 train_time:7261ms step_avg:34.91ms
step:209/2110 train_time:7293ms step_avg:34.90ms
step:210/2110 train_time:7328ms step_avg:34.89ms
step:211/2110 train_time:7360ms step_avg:34.88ms
step:212/2110 train_time:7395ms step_avg:34.88ms
step:213/2110 train_time:7427ms step_avg:34.87ms
step:214/2110 train_time:7459ms step_avg:34.86ms
step:215/2110 train_time:7495ms step_avg:34.86ms
step:216/2110 train_time:7528ms step_avg:34.85ms
step:217/2110 train_time:7559ms step_avg:34.84ms
step:218/2110 train_time:7595ms step_avg:34.84ms
step:219/2110 train_time:7626ms step_avg:34.82ms
step:220/2110 train_time:7659ms step_avg:34.82ms
step:221/2110 train_time:7692ms step_avg:34.80ms
step:222/2110 train_time:7725ms step_avg:34.80ms
step:223/2110 train_time:7756ms step_avg:34.78ms
step:224/2110 train_time:7790ms step_avg:34.78ms
step:225/2110 train_time:7822ms step_avg:34.76ms
step:226/2110 train_time:7857ms step_avg:34.77ms
step:227/2110 train_time:7888ms step_avg:34.75ms
step:228/2110 train_time:7921ms step_avg:34.74ms
step:229/2110 train_time:7954ms step_avg:34.73ms
step:230/2110 train_time:7988ms step_avg:34.73ms
step:231/2110 train_time:8020ms step_avg:34.72ms
step:232/2110 train_time:8053ms step_avg:34.71ms
step:233/2110 train_time:8086ms step_avg:34.70ms
step:234/2110 train_time:8120ms step_avg:34.70ms
step:235/2110 train_time:8153ms step_avg:34.69ms
step:236/2110 train_time:8189ms step_avg:34.70ms
step:237/2110 train_time:8221ms step_avg:34.69ms
step:238/2110 train_time:8253ms step_avg:34.67ms
step:239/2110 train_time:8285ms step_avg:34.66ms
step:240/2110 train_time:8317ms step_avg:34.66ms
step:241/2110 train_time:8353ms step_avg:34.66ms
step:242/2110 train_time:8387ms step_avg:34.66ms
step:243/2110 train_time:8416ms step_avg:34.63ms
step:244/2110 train_time:8451ms step_avg:34.64ms
step:245/2110 train_time:8483ms step_avg:34.63ms
step:246/2110 train_time:8516ms step_avg:34.62ms
step:247/2110 train_time:8548ms step_avg:34.61ms
step:248/2110 train_time:8583ms step_avg:34.61ms
step:249/2110 train_time:8614ms step_avg:34.59ms
step:250/2110 train_time:8649ms step_avg:34.59ms
step:250/2110 val_loss:4.2913 train_time:8682ms step_avg:34.73ms
step:251/2110 train_time:8717ms step_avg:34.73ms
step:252/2110 train_time:8751ms step_avg:34.73ms
step:253/2110 train_time:8781ms step_avg:34.71ms
step:254/2110 train_time:8808ms step_avg:34.68ms
step:255/2110 train_time:8832ms step_avg:34.64ms
step:256/2110 train_time:8861ms step_avg:34.61ms
step:257/2110 train_time:8893ms step_avg:34.60ms
step:258/2110 train_time:8927ms step_avg:34.60ms
step:259/2110 train_time:8959ms step_avg:34.59ms
step:260/2110 train_time:8994ms step_avg:34.59ms
step:261/2110 train_time:9024ms step_avg:34.57ms
step:262/2110 train_time:9060ms step_avg:34.58ms
step:263/2110 train_time:9091ms step_avg:34.57ms
step:264/2110 train_time:9125ms step_avg:34.56ms
step:265/2110 train_time:9157ms step_avg:34.55ms
step:266/2110 train_time:9191ms step_avg:34.55ms
step:267/2110 train_time:9223ms step_avg:34.54ms
step:268/2110 train_time:9257ms step_avg:34.54ms
step:269/2110 train_time:9288ms step_avg:34.53ms
step:270/2110 train_time:9321ms step_avg:34.52ms
step:271/2110 train_time:9352ms step_avg:34.51ms
step:272/2110 train_time:9386ms step_avg:34.51ms
step:273/2110 train_time:9417ms step_avg:34.50ms
step:274/2110 train_time:9452ms step_avg:34.50ms
step:275/2110 train_time:9484ms step_avg:34.49ms
step:276/2110 train_time:9516ms step_avg:34.48ms
step:277/2110 train_time:9550ms step_avg:34.48ms
step:278/2110 train_time:9583ms step_avg:34.47ms
step:279/2110 train_time:9615ms step_avg:34.46ms
step:280/2110 train_time:9648ms step_avg:34.46ms
step:281/2110 train_time:9680ms step_avg:34.45ms
step:282/2110 train_time:9714ms step_avg:34.45ms
step:283/2110 train_time:9748ms step_avg:34.44ms
step:284/2110 train_time:9782ms step_avg:34.44ms
step:285/2110 train_time:9814ms step_avg:34.44ms
step:286/2110 train_time:9850ms step_avg:34.44ms
step:287/2110 train_time:9883ms step_avg:34.43ms
step:288/2110 train_time:9917ms step_avg:34.44ms
step:289/2110 train_time:9948ms step_avg:34.42ms
step:290/2110 train_time:9982ms step_avg:34.42ms
step:291/2110 train_time:10015ms step_avg:34.41ms
step:292/2110 train_time:10050ms step_avg:34.42ms
step:293/2110 train_time:10082ms step_avg:34.41ms
step:294/2110 train_time:10115ms step_avg:34.40ms
step:295/2110 train_time:10147ms step_avg:34.40ms
step:296/2110 train_time:10180ms step_avg:34.39ms
step:297/2110 train_time:10212ms step_avg:34.38ms
step:298/2110 train_time:10247ms step_avg:34.39ms
step:299/2110 train_time:10278ms step_avg:34.37ms
step:300/2110 train_time:10312ms step_avg:34.37ms
step:301/2110 train_time:10345ms step_avg:34.37ms
step:302/2110 train_time:10378ms step_avg:34.37ms
step:303/2110 train_time:10409ms step_avg:34.35ms
step:304/2110 train_time:10443ms step_avg:34.35ms
step:305/2110 train_time:10476ms step_avg:34.35ms
step:306/2110 train_time:10509ms step_avg:34.34ms
step:307/2110 train_time:10541ms step_avg:34.34ms
step:308/2110 train_time:10575ms step_avg:34.33ms
step:309/2110 train_time:10606ms step_avg:34.32ms
step:310/2110 train_time:10640ms step_avg:34.32ms
step:311/2110 train_time:10674ms step_avg:34.32ms
step:312/2110 train_time:10707ms step_avg:34.32ms
step:313/2110 train_time:10738ms step_avg:34.31ms
step:314/2110 train_time:10773ms step_avg:34.31ms
step:315/2110 train_time:10805ms step_avg:34.30ms
step:316/2110 train_time:10839ms step_avg:34.30ms
step:317/2110 train_time:10871ms step_avg:34.29ms
step:318/2110 train_time:10905ms step_avg:34.29ms
step:319/2110 train_time:10938ms step_avg:34.29ms
step:320/2110 train_time:10971ms step_avg:34.29ms
step:321/2110 train_time:11004ms step_avg:34.28ms
step:322/2110 train_time:11039ms step_avg:34.28ms
step:323/2110 train_time:11070ms step_avg:34.27ms
step:324/2110 train_time:11104ms step_avg:34.27ms
step:325/2110 train_time:11137ms step_avg:34.27ms
step:326/2110 train_time:11170ms step_avg:34.26ms
step:327/2110 train_time:11203ms step_avg:34.26ms
step:328/2110 train_time:11237ms step_avg:34.26ms
step:329/2110 train_time:11268ms step_avg:34.25ms
step:330/2110 train_time:11303ms step_avg:34.25ms
step:331/2110 train_time:11335ms step_avg:34.24ms
step:332/2110 train_time:11368ms step_avg:34.24ms
step:333/2110 train_time:11400ms step_avg:34.23ms
step:334/2110 train_time:11433ms step_avg:34.23ms
step:335/2110 train_time:11467ms step_avg:34.23ms
step:336/2110 train_time:11501ms step_avg:34.23ms
step:337/2110 train_time:11532ms step_avg:34.22ms
step:338/2110 train_time:11565ms step_avg:34.22ms
step:339/2110 train_time:11599ms step_avg:34.22ms
step:340/2110 train_time:11632ms step_avg:34.21ms
step:341/2110 train_time:11664ms step_avg:34.21ms
step:342/2110 train_time:11698ms step_avg:34.20ms
step:343/2110 train_time:11729ms step_avg:34.19ms
step:344/2110 train_time:11763ms step_avg:34.20ms
step:345/2110 train_time:11796ms step_avg:34.19ms
step:346/2110 train_time:11829ms step_avg:34.19ms
step:347/2110 train_time:11861ms step_avg:34.18ms
step:348/2110 train_time:11896ms step_avg:34.18ms
step:349/2110 train_time:12027ms step_avg:34.46ms
step:350/2110 train_time:12062ms step_avg:34.46ms
step:351/2110 train_time:12100ms step_avg:34.47ms
step:352/2110 train_time:12134ms step_avg:34.47ms
step:353/2110 train_time:12167ms step_avg:34.47ms
step:354/2110 train_time:12200ms step_avg:34.46ms
step:355/2110 train_time:12231ms step_avg:34.45ms
step:356/2110 train_time:12265ms step_avg:34.45ms
step:357/2110 train_time:12296ms step_avg:34.44ms
step:358/2110 train_time:12330ms step_avg:34.44ms
step:359/2110 train_time:12362ms step_avg:34.44ms
step:360/2110 train_time:12397ms step_avg:34.44ms
step:361/2110 train_time:12428ms step_avg:34.43ms
step:362/2110 train_time:12461ms step_avg:34.42ms
step:363/2110 train_time:12493ms step_avg:34.42ms
step:364/2110 train_time:12526ms step_avg:34.41ms
step:365/2110 train_time:12558ms step_avg:34.40ms
step:366/2110 train_time:12593ms step_avg:34.41ms
step:367/2110 train_time:12624ms step_avg:34.40ms
step:368/2110 train_time:12657ms step_avg:34.39ms
step:369/2110 train_time:12689ms step_avg:34.39ms
step:370/2110 train_time:12722ms step_avg:34.38ms
step:371/2110 train_time:12755ms step_avg:34.38ms
step:372/2110 train_time:12789ms step_avg:34.38ms
step:373/2110 train_time:12821ms step_avg:34.37ms
step:374/2110 train_time:12854ms step_avg:34.37ms
step:375/2110 train_time:12885ms step_avg:34.36ms
step:376/2110 train_time:12919ms step_avg:34.36ms
step:377/2110 train_time:12952ms step_avg:34.36ms
step:378/2110 train_time:12987ms step_avg:34.36ms
step:379/2110 train_time:13019ms step_avg:34.35ms
step:380/2110 train_time:13055ms step_avg:34.36ms
step:381/2110 train_time:13086ms step_avg:34.35ms
step:382/2110 train_time:13120ms step_avg:34.35ms
step:383/2110 train_time:13152ms step_avg:34.34ms
step:384/2110 train_time:13187ms step_avg:34.34ms
step:385/2110 train_time:13219ms step_avg:34.33ms
step:386/2110 train_time:13253ms step_avg:34.33ms
step:387/2110 train_time:13285ms step_avg:34.33ms
step:388/2110 train_time:13319ms step_avg:34.33ms
step:389/2110 train_time:13351ms step_avg:34.32ms
step:390/2110 train_time:13384ms step_avg:34.32ms
step:391/2110 train_time:13415ms step_avg:34.31ms
step:392/2110 train_time:13450ms step_avg:34.31ms
step:393/2110 train_time:13482ms step_avg:34.31ms
step:394/2110 train_time:13515ms step_avg:34.30ms
step:395/2110 train_time:13547ms step_avg:34.30ms
step:396/2110 train_time:13582ms step_avg:34.30ms
step:397/2110 train_time:13614ms step_avg:34.29ms
step:398/2110 train_time:13648ms step_avg:34.29ms
step:399/2110 train_time:13680ms step_avg:34.29ms
step:400/2110 train_time:13714ms step_avg:34.29ms
step:401/2110 train_time:13746ms step_avg:34.28ms
step:402/2110 train_time:13780ms step_avg:34.28ms
step:403/2110 train_time:13811ms step_avg:34.27ms
step:404/2110 train_time:13846ms step_avg:34.27ms
step:405/2110 train_time:13878ms step_avg:34.27ms
step:406/2110 train_time:13911ms step_avg:34.26ms
step:407/2110 train_time:13943ms step_avg:34.26ms
step:408/2110 train_time:13977ms step_avg:34.26ms
step:409/2110 train_time:14010ms step_avg:34.25ms
step:410/2110 train_time:14045ms step_avg:34.26ms
step:411/2110 train_time:14078ms step_avg:34.25ms
step:412/2110 train_time:14113ms step_avg:34.25ms
step:413/2110 train_time:14142ms step_avg:34.24ms
step:414/2110 train_time:14178ms step_avg:34.25ms
step:415/2110 train_time:14209ms step_avg:34.24ms
step:416/2110 train_time:14243ms step_avg:34.24ms
step:417/2110 train_time:14274ms step_avg:34.23ms
step:418/2110 train_time:14311ms step_avg:34.24ms
step:419/2110 train_time:14341ms step_avg:34.23ms
step:420/2110 train_time:14375ms step_avg:34.23ms
step:421/2110 train_time:14407ms step_avg:34.22ms
step:422/2110 train_time:14440ms step_avg:34.22ms
step:423/2110 train_time:14473ms step_avg:34.22ms
step:424/2110 train_time:14509ms step_avg:34.22ms
step:425/2110 train_time:14539ms step_avg:34.21ms
step:426/2110 train_time:14575ms step_avg:34.21ms
step:427/2110 train_time:14604ms step_avg:34.20ms
step:428/2110 train_time:14637ms step_avg:34.20ms
step:429/2110 train_time:14671ms step_avg:34.20ms
step:430/2110 train_time:14705ms step_avg:34.20ms
step:431/2110 train_time:14735ms step_avg:34.19ms
step:432/2110 train_time:14771ms step_avg:34.19ms
step:433/2110 train_time:14802ms step_avg:34.18ms
step:434/2110 train_time:14835ms step_avg:34.18ms
step:435/2110 train_time:14867ms step_avg:34.18ms
step:436/2110 train_time:14902ms step_avg:34.18ms
step:437/2110 train_time:14933ms step_avg:34.17ms
step:438/2110 train_time:14967ms step_avg:34.17ms
step:439/2110 train_time:15000ms step_avg:34.17ms
step:440/2110 train_time:15033ms step_avg:34.17ms
step:441/2110 train_time:15065ms step_avg:34.16ms
step:442/2110 train_time:15100ms step_avg:34.16ms
step:443/2110 train_time:15133ms step_avg:34.16ms
step:444/2110 train_time:15166ms step_avg:34.16ms
step:445/2110 train_time:15199ms step_avg:34.16ms
step:446/2110 train_time:15232ms step_avg:34.15ms
step:447/2110 train_time:15264ms step_avg:34.15ms
step:448/2110 train_time:15299ms step_avg:34.15ms
step:449/2110 train_time:15329ms step_avg:34.14ms
step:450/2110 train_time:15364ms step_avg:34.14ms
step:451/2110 train_time:15396ms step_avg:34.14ms
step:452/2110 train_time:15431ms step_avg:34.14ms
step:453/2110 train_time:15461ms step_avg:34.13ms
step:454/2110 train_time:15495ms step_avg:34.13ms
step:455/2110 train_time:15529ms step_avg:34.13ms
step:456/2110 train_time:15561ms step_avg:34.13ms
step:457/2110 train_time:15594ms step_avg:34.12ms
step:458/2110 train_time:15629ms step_avg:34.12ms
step:459/2110 train_time:15662ms step_avg:34.12ms
step:460/2110 train_time:15694ms step_avg:34.12ms
step:461/2110 train_time:15727ms step_avg:34.11ms
step:462/2110 train_time:15759ms step_avg:34.11ms
step:463/2110 train_time:15790ms step_avg:34.10ms
step:464/2110 train_time:15826ms step_avg:34.11ms
step:465/2110 train_time:15856ms step_avg:34.10ms
step:466/2110 train_time:15890ms step_avg:34.10ms
step:467/2110 train_time:15923ms step_avg:34.10ms
step:468/2110 train_time:15956ms step_avg:34.09ms
step:469/2110 train_time:15990ms step_avg:34.09ms
step:470/2110 train_time:16025ms step_avg:34.09ms
step:471/2110 train_time:16055ms step_avg:34.09ms
step:472/2110 train_time:16089ms step_avg:34.09ms
step:473/2110 train_time:16123ms step_avg:34.09ms
step:474/2110 train_time:16157ms step_avg:34.09ms
step:475/2110 train_time:16188ms step_avg:34.08ms
step:476/2110 train_time:16222ms step_avg:34.08ms
step:477/2110 train_time:16254ms step_avg:34.08ms
step:478/2110 train_time:16290ms step_avg:34.08ms
step:479/2110 train_time:16320ms step_avg:34.07ms
step:480/2110 train_time:16353ms step_avg:34.07ms
step:481/2110 train_time:16386ms step_avg:34.07ms
step:482/2110 train_time:16419ms step_avg:34.06ms
step:483/2110 train_time:16452ms step_avg:34.06ms
step:484/2110 train_time:16485ms step_avg:34.06ms
step:485/2110 train_time:16518ms step_avg:34.06ms
step:486/2110 train_time:16552ms step_avg:34.06ms
step:487/2110 train_time:16584ms step_avg:34.05ms
step:488/2110 train_time:16618ms step_avg:34.05ms
step:489/2110 train_time:16649ms step_avg:34.05ms
step:490/2110 train_time:16684ms step_avg:34.05ms
step:491/2110 train_time:16716ms step_avg:34.04ms
step:492/2110 train_time:16749ms step_avg:34.04ms
step:493/2110 train_time:16783ms step_avg:34.04ms
step:494/2110 train_time:16816ms step_avg:34.04ms
step:495/2110 train_time:16847ms step_avg:34.03ms
step:496/2110 train_time:16881ms step_avg:34.03ms
step:497/2110 train_time:16913ms step_avg:34.03ms
step:498/2110 train_time:16946ms step_avg:34.03ms
step:499/2110 train_time:16979ms step_avg:34.03ms
step:500/2110 train_time:17012ms step_avg:34.02ms
step:500/2110 val_loss:4.0291 train_time:17046ms step_avg:34.09ms
step:501/2110 train_time:17080ms step_avg:34.09ms
step:502/2110 train_time:17113ms step_avg:34.09ms
step:503/2110 train_time:17142ms step_avg:34.08ms
step:504/2110 train_time:17177ms step_avg:34.08ms
step:505/2110 train_time:17205ms step_avg:34.07ms
step:506/2110 train_time:17234ms step_avg:34.06ms
step:507/2110 train_time:17260ms step_avg:34.04ms
step:508/2110 train_time:17288ms step_avg:34.03ms
step:509/2110 train_time:17321ms step_avg:34.03ms
step:510/2110 train_time:17354ms step_avg:34.03ms
step:511/2110 train_time:17386ms step_avg:34.02ms
step:512/2110 train_time:17419ms step_avg:34.02ms
step:513/2110 train_time:17451ms step_avg:34.02ms
step:514/2110 train_time:17485ms step_avg:34.02ms
step:515/2110 train_time:17517ms step_avg:34.01ms
step:516/2110 train_time:17551ms step_avg:34.01ms
step:517/2110 train_time:17584ms step_avg:34.01ms
step:518/2110 train_time:17618ms step_avg:34.01ms
step:519/2110 train_time:17648ms step_avg:34.00ms
step:520/2110 train_time:17683ms step_avg:34.01ms
step:521/2110 train_time:17715ms step_avg:34.00ms
step:522/2110 train_time:17747ms step_avg:34.00ms
step:523/2110 train_time:17779ms step_avg:33.99ms
step:524/2110 train_time:17812ms step_avg:33.99ms
step:525/2110 train_time:17845ms step_avg:33.99ms
step:526/2110 train_time:17877ms step_avg:33.99ms
step:527/2110 train_time:17909ms step_avg:33.98ms
step:528/2110 train_time:17943ms step_avg:33.98ms
step:529/2110 train_time:17976ms step_avg:33.98ms
step:530/2110 train_time:18009ms step_avg:33.98ms
step:531/2110 train_time:18042ms step_avg:33.98ms
step:532/2110 train_time:18076ms step_avg:33.98ms
step:533/2110 train_time:18109ms step_avg:33.98ms
step:534/2110 train_time:18144ms step_avg:33.98ms
step:535/2110 train_time:18176ms step_avg:33.97ms
step:536/2110 train_time:18209ms step_avg:33.97ms
step:537/2110 train_time:18242ms step_avg:33.97ms
step:538/2110 train_time:18276ms step_avg:33.97ms
step:539/2110 train_time:18309ms step_avg:33.97ms
step:540/2110 train_time:18342ms step_avg:33.97ms
step:541/2110 train_time:18375ms step_avg:33.96ms
step:542/2110 train_time:18408ms step_avg:33.96ms
step:543/2110 train_time:18441ms step_avg:33.96ms
step:544/2110 train_time:18475ms step_avg:33.96ms
step:545/2110 train_time:18507ms step_avg:33.96ms
step:546/2110 train_time:18540ms step_avg:33.96ms
step:547/2110 train_time:18573ms step_avg:33.95ms
step:548/2110 train_time:18606ms step_avg:33.95ms
step:549/2110 train_time:18638ms step_avg:33.95ms
step:550/2110 train_time:18671ms step_avg:33.95ms
step:551/2110 train_time:18704ms step_avg:33.95ms
step:552/2110 train_time:18737ms step_avg:33.94ms
step:553/2110 train_time:18769ms step_avg:33.94ms
step:554/2110 train_time:18804ms step_avg:33.94ms
step:555/2110 train_time:18835ms step_avg:33.94ms
step:556/2110 train_time:18869ms step_avg:33.94ms
step:557/2110 train_time:18901ms step_avg:33.93ms
step:558/2110 train_time:18936ms step_avg:33.94ms
step:559/2110 train_time:18969ms step_avg:33.93ms
step:560/2110 train_time:19002ms step_avg:33.93ms
step:561/2110 train_time:19033ms step_avg:33.93ms
step:562/2110 train_time:19067ms step_avg:33.93ms
step:563/2110 train_time:19099ms step_avg:33.92ms
step:564/2110 train_time:19134ms step_avg:33.93ms
step:565/2110 train_time:19166ms step_avg:33.92ms
step:566/2110 train_time:19200ms step_avg:33.92ms
step:567/2110 train_time:19232ms step_avg:33.92ms
step:568/2110 train_time:19265ms step_avg:33.92ms
step:569/2110 train_time:19299ms step_avg:33.92ms
step:570/2110 train_time:19333ms step_avg:33.92ms
step:571/2110 train_time:19365ms step_avg:33.91ms
step:572/2110 train_time:19399ms step_avg:33.91ms
step:573/2110 train_time:19432ms step_avg:33.91ms
step:574/2110 train_time:19465ms step_avg:33.91ms
step:575/2110 train_time:19497ms step_avg:33.91ms
step:576/2110 train_time:19533ms step_avg:33.91ms
step:577/2110 train_time:19563ms step_avg:33.90ms
step:578/2110 train_time:19597ms step_avg:33.90ms
step:579/2110 train_time:19629ms step_avg:33.90ms
step:580/2110 train_time:19663ms step_avg:33.90ms
step:581/2110 train_time:19695ms step_avg:33.90ms
step:582/2110 train_time:19730ms step_avg:33.90ms
step:583/2110 train_time:19761ms step_avg:33.90ms
step:584/2110 train_time:19794ms step_avg:33.89ms
step:585/2110 train_time:19827ms step_avg:33.89ms
step:586/2110 train_time:19860ms step_avg:33.89ms
step:587/2110 train_time:19893ms step_avg:33.89ms
step:588/2110 train_time:19927ms step_avg:33.89ms
step:589/2110 train_time:19959ms step_avg:33.89ms
step:590/2110 train_time:19993ms step_avg:33.89ms
step:591/2110 train_time:20026ms step_avg:33.88ms
step:592/2110 train_time:20060ms step_avg:33.88ms
step:593/2110 train_time:20092ms step_avg:33.88ms
step:594/2110 train_time:20125ms step_avg:33.88ms
step:595/2110 train_time:20157ms step_avg:33.88ms
step:596/2110 train_time:20191ms step_avg:33.88ms
step:597/2110 train_time:20223ms step_avg:33.87ms
step:598/2110 train_time:20257ms step_avg:33.88ms
step:599/2110 train_time:20289ms step_avg:33.87ms
step:600/2110 train_time:20323ms step_avg:33.87ms
step:601/2110 train_time:20356ms step_avg:33.87ms
step:602/2110 train_time:20390ms step_avg:33.87ms
step:603/2110 train_time:20422ms step_avg:33.87ms
step:604/2110 train_time:20455ms step_avg:33.87ms
step:605/2110 train_time:20489ms step_avg:33.87ms
step:606/2110 train_time:20523ms step_avg:33.87ms
step:607/2110 train_time:20556ms step_avg:33.86ms
step:608/2110 train_time:20588ms step_avg:33.86ms
step:609/2110 train_time:20622ms step_avg:33.86ms
step:610/2110 train_time:20655ms step_avg:33.86ms
step:611/2110 train_time:20686ms step_avg:33.86ms
step:612/2110 train_time:20719ms step_avg:33.86ms
step:613/2110 train_time:20753ms step_avg:33.85ms
step:614/2110 train_time:20785ms step_avg:33.85ms
step:615/2110 train_time:20818ms step_avg:33.85ms
step:616/2110 train_time:20851ms step_avg:33.85ms
step:617/2110 train_time:20884ms step_avg:33.85ms
step:618/2110 train_time:20917ms step_avg:33.85ms
step:619/2110 train_time:20950ms step_avg:33.84ms
step:620/2110 train_time:20984ms step_avg:33.85ms
step:621/2110 train_time:21017ms step_avg:33.84ms
step:622/2110 train_time:21052ms step_avg:33.85ms
step:623/2110 train_time:21090ms step_avg:33.85ms
step:624/2110 train_time:21132ms step_avg:33.87ms
step:625/2110 train_time:21166ms step_avg:33.87ms
step:626/2110 train_time:21199ms step_avg:33.86ms
step:627/2110 train_time:21231ms step_avg:33.86ms
step:628/2110 train_time:21265ms step_avg:33.86ms
step:629/2110 train_time:21297ms step_avg:33.86ms
step:630/2110 train_time:21331ms step_avg:33.86ms
step:631/2110 train_time:21362ms step_avg:33.85ms
step:632/2110 train_time:21395ms step_avg:33.85ms
step:633/2110 train_time:21426ms step_avg:33.85ms
step:634/2110 train_time:21461ms step_avg:33.85ms
step:635/2110 train_time:21493ms step_avg:33.85ms
step:636/2110 train_time:21529ms step_avg:33.85ms
step:637/2110 train_time:21559ms step_avg:33.84ms
step:638/2110 train_time:21593ms step_avg:33.85ms
step:639/2110 train_time:21625ms step_avg:33.84ms
step:640/2110 train_time:21659ms step_avg:33.84ms
step:641/2110 train_time:21689ms step_avg:33.84ms
step:642/2110 train_time:21723ms step_avg:33.84ms
step:643/2110 train_time:21754ms step_avg:33.83ms
step:644/2110 train_time:21789ms step_avg:33.83ms
step:645/2110 train_time:21820ms step_avg:33.83ms
step:646/2110 train_time:21853ms step_avg:33.83ms
step:647/2110 train_time:21882ms step_avg:33.82ms
step:648/2110 train_time:21914ms step_avg:33.82ms
step:649/2110 train_time:21946ms step_avg:33.82ms
step:650/2110 train_time:21978ms step_avg:33.81ms
step:651/2110 train_time:22009ms step_avg:33.81ms
step:652/2110 train_time:22041ms step_avg:33.81ms
step:653/2110 train_time:22073ms step_avg:33.80ms
step:654/2110 train_time:22106ms step_avg:33.80ms
step:655/2110 train_time:22139ms step_avg:33.80ms
step:656/2110 train_time:22172ms step_avg:33.80ms
step:657/2110 train_time:22205ms step_avg:33.80ms
step:658/2110 train_time:22240ms step_avg:33.80ms
step:659/2110 train_time:22271ms step_avg:33.80ms
step:660/2110 train_time:22306ms step_avg:33.80ms
step:661/2110 train_time:22338ms step_avg:33.79ms
step:662/2110 train_time:22373ms step_avg:33.80ms
step:663/2110 train_time:22404ms step_avg:33.79ms
step:664/2110 train_time:22439ms step_avg:33.79ms
step:665/2110 train_time:22470ms step_avg:33.79ms
step:666/2110 train_time:22503ms step_avg:33.79ms
step:667/2110 train_time:22535ms step_avg:33.79ms
step:668/2110 train_time:22570ms step_avg:33.79ms
step:669/2110 train_time:22603ms step_avg:33.79ms
step:670/2110 train_time:22636ms step_avg:33.79ms
step:671/2110 train_time:22669ms step_avg:33.78ms
step:672/2110 train_time:22704ms step_avg:33.79ms
step:673/2110 train_time:22736ms step_avg:33.78ms
step:674/2110 train_time:22768ms step_avg:33.78ms
step:675/2110 train_time:22802ms step_avg:33.78ms
step:676/2110 train_time:22837ms step_avg:33.78ms
step:677/2110 train_time:22868ms step_avg:33.78ms
step:678/2110 train_time:22902ms step_avg:33.78ms
step:679/2110 train_time:22934ms step_avg:33.78ms
step:680/2110 train_time:22969ms step_avg:33.78ms
step:681/2110 train_time:23000ms step_avg:33.77ms
step:682/2110 train_time:23035ms step_avg:33.78ms
step:683/2110 train_time:23066ms step_avg:33.77ms
step:684/2110 train_time:23098ms step_avg:33.77ms
step:685/2110 train_time:23131ms step_avg:33.77ms
step:686/2110 train_time:23171ms step_avg:33.78ms
step:687/2110 train_time:23206ms step_avg:33.78ms
step:688/2110 train_time:23239ms step_avg:33.78ms
step:689/2110 train_time:23271ms step_avg:33.77ms
step:690/2110 train_time:23304ms step_avg:33.77ms
step:691/2110 train_time:23343ms step_avg:33.78ms
step:692/2110 train_time:23388ms step_avg:33.80ms
step:693/2110 train_time:23447ms step_avg:33.83ms
step:694/2110 train_time:23506ms step_avg:33.87ms
step:695/2110 train_time:23566ms step_avg:33.91ms
step:696/2110 train_time:23625ms step_avg:33.94ms
step:697/2110 train_time:23686ms step_avg:33.98ms
step:698/2110 train_time:23747ms step_avg:34.02ms
step:699/2110 train_time:23805ms step_avg:34.06ms
step:700/2110 train_time:23865ms step_avg:34.09ms
step:701/2110 train_time:23924ms step_avg:34.13ms
step:702/2110 train_time:23984ms step_avg:34.17ms
step:703/2110 train_time:24044ms step_avg:34.20ms
step:704/2110 train_time:24103ms step_avg:34.24ms
step:705/2110 train_time:24163ms step_avg:34.27ms
step:706/2110 train_time:24221ms step_avg:34.31ms
step:707/2110 train_time:24280ms step_avg:34.34ms
step:708/2110 train_time:24338ms step_avg:34.38ms
step:709/2110 train_time:24398ms step_avg:34.41ms
step:710/2110 train_time:24456ms step_avg:34.45ms
step:711/2110 train_time:24516ms step_avg:34.48ms
step:712/2110 train_time:24575ms step_avg:34.52ms
step:713/2110 train_time:24634ms step_avg:34.55ms
step:714/2110 train_time:24693ms step_avg:34.58ms
step:715/2110 train_time:24754ms step_avg:34.62ms
step:716/2110 train_time:24814ms step_avg:34.66ms
step:717/2110 train_time:24874ms step_avg:34.69ms
step:718/2110 train_time:24933ms step_avg:34.73ms
step:719/2110 train_time:24994ms step_avg:34.76ms
step:720/2110 train_time:25053ms step_avg:34.80ms
step:721/2110 train_time:25112ms step_avg:34.83ms
step:722/2110 train_time:25170ms step_avg:34.86ms
step:723/2110 train_time:25231ms step_avg:34.90ms
step:724/2110 train_time:25289ms step_avg:34.93ms
step:725/2110 train_time:25349ms step_avg:34.96ms
step:726/2110 train_time:25409ms step_avg:35.00ms
step:727/2110 train_time:25468ms step_avg:35.03ms
step:728/2110 train_time:25528ms step_avg:35.07ms
step:729/2110 train_time:25587ms step_avg:35.10ms
step:730/2110 train_time:25646ms step_avg:35.13ms
step:731/2110 train_time:25706ms step_avg:35.16ms
step:732/2110 train_time:25766ms step_avg:35.20ms
step:733/2110 train_time:25825ms step_avg:35.23ms
step:734/2110 train_time:25885ms step_avg:35.27ms
step:735/2110 train_time:25945ms step_avg:35.30ms
step:736/2110 train_time:26005ms step_avg:35.33ms
step:737/2110 train_time:26064ms step_avg:35.37ms
step:738/2110 train_time:26123ms step_avg:35.40ms
step:739/2110 train_time:26182ms step_avg:35.43ms
step:740/2110 train_time:26241ms step_avg:35.46ms
step:741/2110 train_time:26300ms step_avg:35.49ms
step:742/2110 train_time:26359ms step_avg:35.52ms
step:743/2110 train_time:26419ms step_avg:35.56ms
step:744/2110 train_time:26477ms step_avg:35.59ms
step:745/2110 train_time:26538ms step_avg:35.62ms
step:746/2110 train_time:26596ms step_avg:35.65ms
step:747/2110 train_time:26656ms step_avg:35.68ms
step:748/2110 train_time:26715ms step_avg:35.72ms
step:749/2110 train_time:26775ms step_avg:35.75ms
step:750/2110 train_time:26834ms step_avg:35.78ms
step:750/2110 val_loss:3.9037 train_time:26896ms step_avg:35.86ms
step:751/2110 train_time:26930ms step_avg:35.86ms
step:752/2110 train_time:26966ms step_avg:35.86ms
step:753/2110 train_time:27020ms step_avg:35.88ms
step:754/2110 train_time:27083ms step_avg:35.92ms
step:755/2110 train_time:27142ms step_avg:35.95ms
step:756/2110 train_time:27201ms step_avg:35.98ms
step:757/2110 train_time:27259ms step_avg:36.01ms
step:758/2110 train_time:27318ms step_avg:36.04ms
step:759/2110 train_time:27377ms step_avg:36.07ms
step:760/2110 train_time:27436ms step_avg:36.10ms
step:761/2110 train_time:27495ms step_avg:36.13ms
step:762/2110 train_time:27554ms step_avg:36.16ms
step:763/2110 train_time:27611ms step_avg:36.19ms
step:764/2110 train_time:27670ms step_avg:36.22ms
step:765/2110 train_time:27728ms step_avg:36.25ms
step:766/2110 train_time:27787ms step_avg:36.27ms
step:767/2110 train_time:27846ms step_avg:36.31ms
step:768/2110 train_time:27905ms step_avg:36.33ms
step:769/2110 train_time:27968ms step_avg:36.37ms
step:770/2110 train_time:28028ms step_avg:36.40ms
step:771/2110 train_time:28090ms step_avg:36.43ms
step:772/2110 train_time:28150ms step_avg:36.46ms
step:773/2110 train_time:28210ms step_avg:36.49ms
step:774/2110 train_time:28269ms step_avg:36.52ms
step:775/2110 train_time:28327ms step_avg:36.55ms
step:776/2110 train_time:28386ms step_avg:36.58ms
step:777/2110 train_time:28445ms step_avg:36.61ms
step:778/2110 train_time:28503ms step_avg:36.64ms
step:779/2110 train_time:28562ms step_avg:36.66ms
step:780/2110 train_time:28622ms step_avg:36.69ms
step:781/2110 train_time:28679ms step_avg:36.72ms
step:782/2110 train_time:28738ms step_avg:36.75ms
step:783/2110 train_time:28798ms step_avg:36.78ms
step:784/2110 train_time:28858ms step_avg:36.81ms
step:785/2110 train_time:28919ms step_avg:36.84ms
step:786/2110 train_time:28979ms step_avg:36.87ms
step:787/2110 train_time:29040ms step_avg:36.90ms
step:788/2110 train_time:29100ms step_avg:36.93ms
step:789/2110 train_time:29160ms step_avg:36.96ms
step:790/2110 train_time:29220ms step_avg:36.99ms
step:791/2110 train_time:29279ms step_avg:37.02ms
step:792/2110 train_time:29339ms step_avg:37.04ms
step:793/2110 train_time:29397ms step_avg:37.07ms
step:794/2110 train_time:29457ms step_avg:37.10ms
step:795/2110 train_time:29515ms step_avg:37.13ms
step:796/2110 train_time:29574ms step_avg:37.15ms
step:797/2110 train_time:29633ms step_avg:37.18ms
step:798/2110 train_time:29693ms step_avg:37.21ms
step:799/2110 train_time:29751ms step_avg:37.23ms
step:800/2110 train_time:29812ms step_avg:37.27ms
step:801/2110 train_time:29869ms step_avg:37.29ms
step:802/2110 train_time:29929ms step_avg:37.32ms
step:803/2110 train_time:29990ms step_avg:37.35ms
step:804/2110 train_time:30049ms step_avg:37.37ms
step:805/2110 train_time:30109ms step_avg:37.40ms
step:806/2110 train_time:30169ms step_avg:37.43ms
step:807/2110 train_time:30229ms step_avg:37.46ms
step:808/2110 train_time:30287ms step_avg:37.48ms
step:809/2110 train_time:30348ms step_avg:37.51ms
step:810/2110 train_time:30406ms step_avg:37.54ms
step:811/2110 train_time:30466ms step_avg:37.57ms
step:812/2110 train_time:30525ms step_avg:37.59ms
step:813/2110 train_time:30584ms step_avg:37.62ms
step:814/2110 train_time:30642ms step_avg:37.64ms
step:815/2110 train_time:30701ms step_avg:37.67ms
step:816/2110 train_time:30760ms step_avg:37.70ms
step:817/2110 train_time:30820ms step_avg:37.72ms
step:818/2110 train_time:30879ms step_avg:37.75ms
step:819/2110 train_time:30940ms step_avg:37.78ms
step:820/2110 train_time:30999ms step_avg:37.80ms
step:821/2110 train_time:31060ms step_avg:37.83ms
step:822/2110 train_time:31119ms step_avg:37.86ms
step:823/2110 train_time:31179ms step_avg:37.89ms
step:824/2110 train_time:31239ms step_avg:37.91ms
step:825/2110 train_time:31298ms step_avg:37.94ms
step:826/2110 train_time:31358ms step_avg:37.96ms
step:827/2110 train_time:31417ms step_avg:37.99ms
step:828/2110 train_time:31475ms step_avg:38.01ms
step:829/2110 train_time:31535ms step_avg:38.04ms
step:830/2110 train_time:31593ms step_avg:38.06ms
step:831/2110 train_time:31653ms step_avg:38.09ms
step:832/2110 train_time:31711ms step_avg:38.11ms
step:833/2110 train_time:31770ms step_avg:38.14ms
step:834/2110 train_time:31828ms step_avg:38.16ms
step:835/2110 train_time:31890ms step_avg:38.19ms
step:836/2110 train_time:31949ms step_avg:38.22ms
step:837/2110 train_time:32008ms step_avg:38.24ms
step:838/2110 train_time:32067ms step_avg:38.27ms
step:839/2110 train_time:32127ms step_avg:38.29ms
step:840/2110 train_time:32186ms step_avg:38.32ms
step:841/2110 train_time:32246ms step_avg:38.34ms
step:842/2110 train_time:32304ms step_avg:38.37ms
step:843/2110 train_time:32365ms step_avg:38.39ms
step:844/2110 train_time:32423ms step_avg:38.42ms
step:845/2110 train_time:32484ms step_avg:38.44ms
step:846/2110 train_time:32542ms step_avg:38.47ms
step:847/2110 train_time:32602ms step_avg:38.49ms
step:848/2110 train_time:32660ms step_avg:38.51ms
step:849/2110 train_time:32720ms step_avg:38.54ms
step:850/2110 train_time:32778ms step_avg:38.56ms
step:851/2110 train_time:32838ms step_avg:38.59ms
step:852/2110 train_time:32897ms step_avg:38.61ms
step:853/2110 train_time:32957ms step_avg:38.64ms
step:854/2110 train_time:33016ms step_avg:38.66ms
step:855/2110 train_time:33077ms step_avg:38.69ms
step:856/2110 train_time:33136ms step_avg:38.71ms
step:857/2110 train_time:33196ms step_avg:38.74ms
step:858/2110 train_time:33256ms step_avg:38.76ms
step:859/2110 train_time:33316ms step_avg:38.78ms
step:860/2110 train_time:33376ms step_avg:38.81ms
step:861/2110 train_time:33436ms step_avg:38.83ms
step:862/2110 train_time:33494ms step_avg:38.86ms
step:863/2110 train_time:33554ms step_avg:38.88ms
step:864/2110 train_time:33612ms step_avg:38.90ms
step:865/2110 train_time:33672ms step_avg:38.93ms
step:866/2110 train_time:33730ms step_avg:38.95ms
step:867/2110 train_time:33790ms step_avg:38.97ms
step:868/2110 train_time:33849ms step_avg:39.00ms
step:869/2110 train_time:33909ms step_avg:39.02ms
step:870/2110 train_time:33967ms step_avg:39.04ms
step:871/2110 train_time:34028ms step_avg:39.07ms
step:872/2110 train_time:34086ms step_avg:39.09ms
step:873/2110 train_time:34147ms step_avg:39.11ms
step:874/2110 train_time:34206ms step_avg:39.14ms
step:875/2110 train_time:34266ms step_avg:39.16ms
step:876/2110 train_time:34325ms step_avg:39.18ms
step:877/2110 train_time:34385ms step_avg:39.21ms
step:878/2110 train_time:34444ms step_avg:39.23ms
step:879/2110 train_time:34504ms step_avg:39.25ms
step:880/2110 train_time:34562ms step_avg:39.28ms
step:881/2110 train_time:34622ms step_avg:39.30ms
step:882/2110 train_time:34680ms step_avg:39.32ms
step:883/2110 train_time:34740ms step_avg:39.34ms
step:884/2110 train_time:34798ms step_avg:39.36ms
step:885/2110 train_time:34858ms step_avg:39.39ms
step:886/2110 train_time:34917ms step_avg:39.41ms
step:887/2110 train_time:34977ms step_avg:39.43ms
step:888/2110 train_time:35036ms step_avg:39.46ms
step:889/2110 train_time:35096ms step_avg:39.48ms
step:890/2110 train_time:35155ms step_avg:39.50ms
step:891/2110 train_time:35215ms step_avg:39.52ms
step:892/2110 train_time:35273ms step_avg:39.54ms
step:893/2110 train_time:35334ms step_avg:39.57ms
step:894/2110 train_time:35392ms step_avg:39.59ms
step:895/2110 train_time:35452ms step_avg:39.61ms
step:896/2110 train_time:35511ms step_avg:39.63ms
step:897/2110 train_time:35571ms step_avg:39.66ms
step:898/2110 train_time:35630ms step_avg:39.68ms
step:899/2110 train_time:35690ms step_avg:39.70ms
step:900/2110 train_time:35749ms step_avg:39.72ms
step:901/2110 train_time:35809ms step_avg:39.74ms
step:902/2110 train_time:35867ms step_avg:39.76ms
step:903/2110 train_time:35927ms step_avg:39.79ms
step:904/2110 train_time:35986ms step_avg:39.81ms
step:905/2110 train_time:36047ms step_avg:39.83ms
step:906/2110 train_time:36106ms step_avg:39.85ms
step:907/2110 train_time:36166ms step_avg:39.87ms
step:908/2110 train_time:36225ms step_avg:39.89ms
step:909/2110 train_time:36285ms step_avg:39.92ms
step:910/2110 train_time:36343ms step_avg:39.94ms
step:911/2110 train_time:36403ms step_avg:39.96ms
step:912/2110 train_time:36461ms step_avg:39.98ms
step:913/2110 train_time:36522ms step_avg:40.00ms
step:914/2110 train_time:36581ms step_avg:40.02ms
step:915/2110 train_time:36641ms step_avg:40.04ms
step:916/2110 train_time:36699ms step_avg:40.06ms
step:917/2110 train_time:36759ms step_avg:40.09ms
step:918/2110 train_time:36817ms step_avg:40.11ms
step:919/2110 train_time:36878ms step_avg:40.13ms
step:920/2110 train_time:36937ms step_avg:40.15ms
step:921/2110 train_time:36997ms step_avg:40.17ms
step:922/2110 train_time:37056ms step_avg:40.19ms
step:923/2110 train_time:37116ms step_avg:40.21ms
step:924/2110 train_time:37176ms step_avg:40.23ms
step:925/2110 train_time:37236ms step_avg:40.26ms
step:926/2110 train_time:37295ms step_avg:40.28ms
step:927/2110 train_time:37355ms step_avg:40.30ms
step:928/2110 train_time:37413ms step_avg:40.32ms
step:929/2110 train_time:37473ms step_avg:40.34ms
step:930/2110 train_time:37532ms step_avg:40.36ms
step:931/2110 train_time:37592ms step_avg:40.38ms
step:932/2110 train_time:37650ms step_avg:40.40ms
step:933/2110 train_time:37710ms step_avg:40.42ms
step:934/2110 train_time:37769ms step_avg:40.44ms
step:935/2110 train_time:37829ms step_avg:40.46ms
step:936/2110 train_time:37888ms step_avg:40.48ms
step:937/2110 train_time:37948ms step_avg:40.50ms
step:938/2110 train_time:38007ms step_avg:40.52ms
step:939/2110 train_time:38068ms step_avg:40.54ms
step:940/2110 train_time:38127ms step_avg:40.56ms
step:941/2110 train_time:38187ms step_avg:40.58ms
step:942/2110 train_time:38245ms step_avg:40.60ms
step:943/2110 train_time:38305ms step_avg:40.62ms
step:944/2110 train_time:38363ms step_avg:40.64ms
step:945/2110 train_time:38424ms step_avg:40.66ms
step:946/2110 train_time:38483ms step_avg:40.68ms
step:947/2110 train_time:38544ms step_avg:40.70ms
step:948/2110 train_time:38602ms step_avg:40.72ms
step:949/2110 train_time:38662ms step_avg:40.74ms
step:950/2110 train_time:38720ms step_avg:40.76ms
step:951/2110 train_time:38781ms step_avg:40.78ms
step:952/2110 train_time:38839ms step_avg:40.80ms
step:953/2110 train_time:38899ms step_avg:40.82ms
step:954/2110 train_time:38958ms step_avg:40.84ms
step:955/2110 train_time:39018ms step_avg:40.86ms
step:956/2110 train_time:39077ms step_avg:40.88ms
step:957/2110 train_time:39139ms step_avg:40.90ms
step:958/2110 train_time:39198ms step_avg:40.92ms
step:959/2110 train_time:39258ms step_avg:40.94ms
step:960/2110 train_time:39317ms step_avg:40.96ms
step:961/2110 train_time:39377ms step_avg:40.97ms
step:962/2110 train_time:39437ms step_avg:40.99ms
step:963/2110 train_time:39496ms step_avg:41.01ms
step:964/2110 train_time:39556ms step_avg:41.03ms
step:965/2110 train_time:39615ms step_avg:41.05ms
step:966/2110 train_time:39674ms step_avg:41.07ms
step:967/2110 train_time:39734ms step_avg:41.09ms
step:968/2110 train_time:39792ms step_avg:41.11ms
step:969/2110 train_time:39852ms step_avg:41.13ms
step:970/2110 train_time:39910ms step_avg:41.14ms
step:971/2110 train_time:39971ms step_avg:41.16ms
step:972/2110 train_time:40030ms step_avg:41.18ms
step:973/2110 train_time:40090ms step_avg:41.20ms
step:974/2110 train_time:40150ms step_avg:41.22ms
step:975/2110 train_time:40210ms step_avg:41.24ms
step:976/2110 train_time:40269ms step_avg:41.26ms
step:977/2110 train_time:40330ms step_avg:41.28ms
step:978/2110 train_time:40389ms step_avg:41.30ms
step:979/2110 train_time:40449ms step_avg:41.32ms
step:980/2110 train_time:40508ms step_avg:41.33ms
step:981/2110 train_time:40568ms step_avg:41.35ms
step:982/2110 train_time:40626ms step_avg:41.37ms
step:983/2110 train_time:40686ms step_avg:41.39ms
step:984/2110 train_time:40745ms step_avg:41.41ms
step:985/2110 train_time:40804ms step_avg:41.43ms
step:986/2110 train_time:40862ms step_avg:41.44ms
step:987/2110 train_time:40922ms step_avg:41.46ms
step:988/2110 train_time:40981ms step_avg:41.48ms
step:989/2110 train_time:41042ms step_avg:41.50ms
step:990/2110 train_time:41101ms step_avg:41.52ms
step:991/2110 train_time:41162ms step_avg:41.54ms
step:992/2110 train_time:41220ms step_avg:41.55ms
step:993/2110 train_time:41281ms step_avg:41.57ms
step:994/2110 train_time:41339ms step_avg:41.59ms
step:995/2110 train_time:41399ms step_avg:41.61ms
step:996/2110 train_time:41457ms step_avg:41.62ms
step:997/2110 train_time:41517ms step_avg:41.64ms
step:998/2110 train_time:41576ms step_avg:41.66ms
step:999/2110 train_time:41637ms step_avg:41.68ms
step:1000/2110 train_time:41696ms step_avg:41.70ms
step:1000/2110 val_loss:3.7650 train_time:41758ms step_avg:41.76ms
step:1001/2110 train_time:41779ms step_avg:41.74ms
step:1002/2110 train_time:41818ms step_avg:41.73ms
step:1003/2110 train_time:41881ms step_avg:41.76ms
step:1004/2110 train_time:41942ms step_avg:41.77ms
step:1005/2110 train_time:42003ms step_avg:41.79ms
step:1006/2110 train_time:42061ms step_avg:41.81ms
step:1007/2110 train_time:42121ms step_avg:41.83ms
step:1008/2110 train_time:42178ms step_avg:41.84ms
step:1009/2110 train_time:42238ms step_avg:41.86ms
step:1010/2110 train_time:42296ms step_avg:41.88ms
step:1011/2110 train_time:42354ms step_avg:41.89ms
step:1012/2110 train_time:42412ms step_avg:41.91ms
step:1013/2110 train_time:42470ms step_avg:41.93ms
step:1014/2110 train_time:42528ms step_avg:41.94ms
step:1015/2110 train_time:42587ms step_avg:41.96ms
step:1016/2110 train_time:42645ms step_avg:41.97ms
step:1017/2110 train_time:42705ms step_avg:41.99ms
step:1018/2110 train_time:42765ms step_avg:42.01ms
step:1019/2110 train_time:42826ms step_avg:42.03ms
step:1020/2110 train_time:42886ms step_avg:42.04ms
step:1021/2110 train_time:42948ms step_avg:42.06ms
step:1022/2110 train_time:43008ms step_avg:42.08ms
step:1023/2110 train_time:43069ms step_avg:42.10ms
step:1024/2110 train_time:43127ms step_avg:42.12ms
step:1025/2110 train_time:43186ms step_avg:42.13ms
step:1026/2110 train_time:43245ms step_avg:42.15ms
step:1027/2110 train_time:43304ms step_avg:42.17ms
step:1028/2110 train_time:43362ms step_avg:42.18ms
step:1029/2110 train_time:43421ms step_avg:42.20ms
step:1030/2110 train_time:43478ms step_avg:42.21ms
step:1031/2110 train_time:43537ms step_avg:42.23ms
step:1032/2110 train_time:43595ms step_avg:42.24ms
step:1033/2110 train_time:43654ms step_avg:42.26ms
step:1034/2110 train_time:43712ms step_avg:42.27ms
step:1035/2110 train_time:43772ms step_avg:42.29ms
step:1036/2110 train_time:43832ms step_avg:42.31ms
step:1037/2110 train_time:43893ms step_avg:42.33ms
step:1038/2110 train_time:43952ms step_avg:42.34ms
step:1039/2110 train_time:44013ms step_avg:42.36ms
step:1040/2110 train_time:44072ms step_avg:42.38ms
step:1041/2110 train_time:44133ms step_avg:42.39ms
step:1042/2110 train_time:44192ms step_avg:42.41ms
step:1043/2110 train_time:44252ms step_avg:42.43ms
step:1044/2110 train_time:44311ms step_avg:42.44ms
step:1045/2110 train_time:44371ms step_avg:42.46ms
step:1046/2110 train_time:44429ms step_avg:42.47ms
step:1047/2110 train_time:44488ms step_avg:42.49ms
step:1048/2110 train_time:44546ms step_avg:42.51ms
step:1049/2110 train_time:44606ms step_avg:42.52ms
step:1050/2110 train_time:44664ms step_avg:42.54ms
step:1051/2110 train_time:44724ms step_avg:42.55ms
step:1052/2110 train_time:44782ms step_avg:42.57ms
step:1053/2110 train_time:44843ms step_avg:42.59ms
step:1054/2110 train_time:44902ms step_avg:42.60ms
step:1055/2110 train_time:44963ms step_avg:42.62ms
step:1056/2110 train_time:45021ms step_avg:42.63ms
step:1057/2110 train_time:45082ms step_avg:42.65ms
step:1058/2110 train_time:45140ms step_avg:42.67ms
step:1059/2110 train_time:45200ms step_avg:42.68ms
step:1060/2110 train_time:45258ms step_avg:42.70ms
step:1061/2110 train_time:45319ms step_avg:42.71ms
step:1062/2110 train_time:45376ms step_avg:42.73ms
step:1063/2110 train_time:45436ms step_avg:42.74ms
step:1064/2110 train_time:45495ms step_avg:42.76ms
step:1065/2110 train_time:45554ms step_avg:42.77ms
step:1066/2110 train_time:45612ms step_avg:42.79ms
step:1067/2110 train_time:45672ms step_avg:42.80ms
step:1068/2110 train_time:45730ms step_avg:42.82ms
step:1069/2110 train_time:45791ms step_avg:42.84ms
step:1070/2110 train_time:45850ms step_avg:42.85ms
step:1071/2110 train_time:45910ms step_avg:42.87ms
step:1072/2110 train_time:45969ms step_avg:42.88ms
step:1073/2110 train_time:46030ms step_avg:42.90ms
step:1074/2110 train_time:46089ms step_avg:42.91ms
step:1075/2110 train_time:46149ms step_avg:42.93ms
step:1076/2110 train_time:46208ms step_avg:42.94ms
step:1077/2110 train_time:46267ms step_avg:42.96ms
step:1078/2110 train_time:46327ms step_avg:42.97ms
step:1079/2110 train_time:46389ms step_avg:42.99ms
step:1080/2110 train_time:46449ms step_avg:43.01ms
step:1081/2110 train_time:46507ms step_avg:43.02ms
step:1082/2110 train_time:46567ms step_avg:43.04ms
step:1083/2110 train_time:46623ms step_avg:43.05ms
step:1084/2110 train_time:46681ms step_avg:43.06ms
step:1085/2110 train_time:46742ms step_avg:43.08ms
step:1086/2110 train_time:46800ms step_avg:43.09ms
step:1087/2110 train_time:46860ms step_avg:43.11ms
step:1088/2110 train_time:46919ms step_avg:43.12ms
step:1089/2110 train_time:46980ms step_avg:43.14ms
step:1090/2110 train_time:47039ms step_avg:43.16ms
step:1091/2110 train_time:47099ms step_avg:43.17ms
step:1092/2110 train_time:47157ms step_avg:43.18ms
step:1093/2110 train_time:47217ms step_avg:43.20ms
step:1094/2110 train_time:47276ms step_avg:43.21ms
step:1095/2110 train_time:47336ms step_avg:43.23ms
step:1096/2110 train_time:47394ms step_avg:43.24ms
step:1097/2110 train_time:47455ms step_avg:43.26ms
step:1098/2110 train_time:47513ms step_avg:43.27ms
step:1099/2110 train_time:47573ms step_avg:43.29ms
step:1100/2110 train_time:47632ms step_avg:43.30ms
step:1101/2110 train_time:47691ms step_avg:43.32ms
step:1102/2110 train_time:47750ms step_avg:43.33ms
step:1103/2110 train_time:47809ms step_avg:43.34ms
step:1104/2110 train_time:47868ms step_avg:43.36ms
step:1105/2110 train_time:47928ms step_avg:43.37ms
step:1106/2110 train_time:47987ms step_avg:43.39ms
step:1107/2110 train_time:48047ms step_avg:43.40ms
step:1108/2110 train_time:48106ms step_avg:43.42ms
step:1109/2110 train_time:48166ms step_avg:43.43ms
step:1110/2110 train_time:48225ms step_avg:43.45ms
step:1111/2110 train_time:48284ms step_avg:43.46ms
step:1112/2110 train_time:48343ms step_avg:43.47ms
step:1113/2110 train_time:48403ms step_avg:43.49ms
step:1114/2110 train_time:48461ms step_avg:43.50ms
step:1115/2110 train_time:48521ms step_avg:43.52ms
step:1116/2110 train_time:48580ms step_avg:43.53ms
step:1117/2110 train_time:48640ms step_avg:43.55ms
step:1118/2110 train_time:48699ms step_avg:43.56ms
step:1119/2110 train_time:48758ms step_avg:43.57ms
step:1120/2110 train_time:48816ms step_avg:43.59ms
step:1121/2110 train_time:48876ms step_avg:43.60ms
step:1122/2110 train_time:48935ms step_avg:43.61ms
step:1123/2110 train_time:48996ms step_avg:43.63ms
step:1124/2110 train_time:49054ms step_avg:43.64ms
step:1125/2110 train_time:49114ms step_avg:43.66ms
step:1126/2110 train_time:49173ms step_avg:43.67ms
step:1127/2110 train_time:49233ms step_avg:43.68ms
step:1128/2110 train_time:49291ms step_avg:43.70ms
step:1129/2110 train_time:49351ms step_avg:43.71ms
step:1130/2110 train_time:49409ms step_avg:43.73ms
step:1131/2110 train_time:49469ms step_avg:43.74ms
step:1132/2110 train_time:49527ms step_avg:43.75ms
step:1133/2110 train_time:49587ms step_avg:43.77ms
step:1134/2110 train_time:49645ms step_avg:43.78ms
step:1135/2110 train_time:49705ms step_avg:43.79ms
step:1136/2110 train_time:49764ms step_avg:43.81ms
step:1137/2110 train_time:49824ms step_avg:43.82ms
step:1138/2110 train_time:49882ms step_avg:43.83ms
step:1139/2110 train_time:49943ms step_avg:43.85ms
step:1140/2110 train_time:50002ms step_avg:43.86ms
step:1141/2110 train_time:50062ms step_avg:43.88ms
step:1142/2110 train_time:50121ms step_avg:43.89ms
step:1143/2110 train_time:50182ms step_avg:43.90ms
step:1144/2110 train_time:50241ms step_avg:43.92ms
step:1145/2110 train_time:50302ms step_avg:43.93ms
step:1146/2110 train_time:50362ms step_avg:43.95ms
step:1147/2110 train_time:50421ms step_avg:43.96ms
step:1148/2110 train_time:50480ms step_avg:43.97ms
step:1149/2110 train_time:50541ms step_avg:43.99ms
step:1150/2110 train_time:50600ms step_avg:44.00ms
step:1151/2110 train_time:50660ms step_avg:44.01ms
step:1152/2110 train_time:50719ms step_avg:44.03ms
step:1153/2110 train_time:50781ms step_avg:44.04ms
step:1154/2110 train_time:50839ms step_avg:44.05ms
step:1155/2110 train_time:50899ms step_avg:44.07ms
step:1156/2110 train_time:50957ms step_avg:44.08ms
step:1157/2110 train_time:51018ms step_avg:44.10ms
step:1158/2110 train_time:51077ms step_avg:44.11ms
step:1159/2110 train_time:51138ms step_avg:44.12ms
step:1160/2110 train_time:51198ms step_avg:44.14ms
step:1161/2110 train_time:51259ms step_avg:44.15ms
step:1162/2110 train_time:51317ms step_avg:44.16ms
step:1163/2110 train_time:51378ms step_avg:44.18ms
step:1164/2110 train_time:51437ms step_avg:44.19ms
step:1165/2110 train_time:51497ms step_avg:44.20ms
step:1166/2110 train_time:51556ms step_avg:44.22ms
step:1167/2110 train_time:51617ms step_avg:44.23ms
step:1168/2110 train_time:51675ms step_avg:44.24ms
step:1169/2110 train_time:51737ms step_avg:44.26ms
step:1170/2110 train_time:51795ms step_avg:44.27ms
step:1171/2110 train_time:51856ms step_avg:44.28ms
step:1172/2110 train_time:51915ms step_avg:44.30ms
step:1173/2110 train_time:51975ms step_avg:44.31ms
step:1174/2110 train_time:52034ms step_avg:44.32ms
step:1175/2110 train_time:52095ms step_avg:44.34ms
step:1176/2110 train_time:52154ms step_avg:44.35ms
step:1177/2110 train_time:52216ms step_avg:44.36ms
step:1178/2110 train_time:52274ms step_avg:44.38ms
step:1179/2110 train_time:52335ms step_avg:44.39ms
step:1180/2110 train_time:52394ms step_avg:44.40ms
step:1181/2110 train_time:52455ms step_avg:44.42ms
step:1182/2110 train_time:52514ms step_avg:44.43ms
step:1183/2110 train_time:52575ms step_avg:44.44ms
step:1184/2110 train_time:52633ms step_avg:44.45ms
step:1185/2110 train_time:52694ms step_avg:44.47ms
step:1186/2110 train_time:52754ms step_avg:44.48ms
step:1187/2110 train_time:52814ms step_avg:44.49ms
step:1188/2110 train_time:52873ms step_avg:44.51ms
step:1189/2110 train_time:52933ms step_avg:44.52ms
step:1190/2110 train_time:52993ms step_avg:44.53ms
step:1191/2110 train_time:53053ms step_avg:44.55ms
step:1192/2110 train_time:53112ms step_avg:44.56ms
step:1193/2110 train_time:53173ms step_avg:44.57ms
step:1194/2110 train_time:53232ms step_avg:44.58ms
step:1195/2110 train_time:53293ms step_avg:44.60ms
step:1196/2110 train_time:53352ms step_avg:44.61ms
step:1197/2110 train_time:53413ms step_avg:44.62ms
step:1198/2110 train_time:53472ms step_avg:44.63ms
step:1199/2110 train_time:53532ms step_avg:44.65ms
step:1200/2110 train_time:53592ms step_avg:44.66ms
step:1201/2110 train_time:53652ms step_avg:44.67ms
step:1202/2110 train_time:53711ms step_avg:44.68ms
step:1203/2110 train_time:53772ms step_avg:44.70ms
step:1204/2110 train_time:53832ms step_avg:44.71ms
step:1205/2110 train_time:53893ms step_avg:44.72ms
step:1206/2110 train_time:53952ms step_avg:44.74ms
step:1207/2110 train_time:54012ms step_avg:44.75ms
step:1208/2110 train_time:54071ms step_avg:44.76ms
step:1209/2110 train_time:54132ms step_avg:44.77ms
step:1210/2110 train_time:54191ms step_avg:44.79ms
step:1211/2110 train_time:54251ms step_avg:44.80ms
step:1212/2110 train_time:54311ms step_avg:44.81ms
step:1213/2110 train_time:54371ms step_avg:44.82ms
step:1214/2110 train_time:54430ms step_avg:44.84ms
step:1215/2110 train_time:54491ms step_avg:44.85ms
step:1216/2110 train_time:54550ms step_avg:44.86ms
step:1217/2110 train_time:54610ms step_avg:44.87ms
step:1218/2110 train_time:54670ms step_avg:44.88ms
step:1219/2110 train_time:54730ms step_avg:44.90ms
step:1220/2110 train_time:54790ms step_avg:44.91ms
step:1221/2110 train_time:54850ms step_avg:44.92ms
step:1222/2110 train_time:54909ms step_avg:44.93ms
step:1223/2110 train_time:54969ms step_avg:44.95ms
step:1224/2110 train_time:55028ms step_avg:44.96ms
step:1225/2110 train_time:55088ms step_avg:44.97ms
step:1226/2110 train_time:55147ms step_avg:44.98ms
step:1227/2110 train_time:55207ms step_avg:44.99ms
step:1228/2110 train_time:55267ms step_avg:45.01ms
step:1229/2110 train_time:55328ms step_avg:45.02ms
step:1230/2110 train_time:55388ms step_avg:45.03ms
step:1231/2110 train_time:55449ms step_avg:45.04ms
step:1232/2110 train_time:55508ms step_avg:45.06ms
step:1233/2110 train_time:55568ms step_avg:45.07ms
step:1234/2110 train_time:55627ms step_avg:45.08ms
step:1235/2110 train_time:55688ms step_avg:45.09ms
step:1236/2110 train_time:55747ms step_avg:45.10ms
step:1237/2110 train_time:55808ms step_avg:45.12ms
step:1238/2110 train_time:55868ms step_avg:45.13ms
step:1239/2110 train_time:55928ms step_avg:45.14ms
step:1240/2110 train_time:55988ms step_avg:45.15ms
step:1241/2110 train_time:56048ms step_avg:45.16ms
step:1242/2110 train_time:56107ms step_avg:45.17ms
step:1243/2110 train_time:56168ms step_avg:45.19ms
step:1244/2110 train_time:56226ms step_avg:45.20ms
step:1245/2110 train_time:56287ms step_avg:45.21ms
step:1246/2110 train_time:56347ms step_avg:45.22ms
step:1247/2110 train_time:56407ms step_avg:45.23ms
step:1248/2110 train_time:56467ms step_avg:45.25ms
step:1249/2110 train_time:56527ms step_avg:45.26ms
step:1250/2110 train_time:56587ms step_avg:45.27ms
step:1250/2110 val_loss:3.5924 train_time:56649ms step_avg:45.32ms
step:1251/2110 train_time:56671ms step_avg:45.30ms
step:1252/2110 train_time:56707ms step_avg:45.29ms
step:1253/2110 train_time:56771ms step_avg:45.31ms
step:1254/2110 train_time:56834ms step_avg:45.32ms
step:1255/2110 train_time:56894ms step_avg:45.33ms
step:1256/2110 train_time:56953ms step_avg:45.35ms
step:1257/2110 train_time:57013ms step_avg:45.36ms
step:1258/2110 train_time:57072ms step_avg:45.37ms
step:1259/2110 train_time:57131ms step_avg:45.38ms
step:1260/2110 train_time:57190ms step_avg:45.39ms
step:1261/2110 train_time:57250ms step_avg:45.40ms
step:1262/2110 train_time:57308ms step_avg:45.41ms
step:1263/2110 train_time:57368ms step_avg:45.42ms
step:1264/2110 train_time:57426ms step_avg:45.43ms
step:1265/2110 train_time:57486ms step_avg:45.44ms
step:1266/2110 train_time:57545ms step_avg:45.45ms
step:1267/2110 train_time:57606ms step_avg:45.47ms
step:1268/2110 train_time:57666ms step_avg:45.48ms
step:1269/2110 train_time:57730ms step_avg:45.49ms
step:1270/2110 train_time:57790ms step_avg:45.50ms
step:1271/2110 train_time:57853ms step_avg:45.52ms
step:1272/2110 train_time:57912ms step_avg:45.53ms
step:1273/2110 train_time:57973ms step_avg:45.54ms
step:1274/2110 train_time:58031ms step_avg:45.55ms
step:1275/2110 train_time:58092ms step_avg:45.56ms
step:1276/2110 train_time:58150ms step_avg:45.57ms
step:1277/2110 train_time:58210ms step_avg:45.58ms
step:1278/2110 train_time:58268ms step_avg:45.59ms
step:1279/2110 train_time:58328ms step_avg:45.60ms
step:1280/2110 train_time:58387ms step_avg:45.61ms
step:1281/2110 train_time:58446ms step_avg:45.63ms
step:1282/2110 train_time:58505ms step_avg:45.64ms
step:1283/2110 train_time:58565ms step_avg:45.65ms
step:1284/2110 train_time:58624ms step_avg:45.66ms
step:1285/2110 train_time:58685ms step_avg:45.67ms
step:1286/2110 train_time:58745ms step_avg:45.68ms
step:1287/2110 train_time:58808ms step_avg:45.69ms
step:1288/2110 train_time:58867ms step_avg:45.70ms
step:1289/2110 train_time:58929ms step_avg:45.72ms
step:1290/2110 train_time:58988ms step_avg:45.73ms
step:1291/2110 train_time:59048ms step_avg:45.74ms
step:1292/2110 train_time:59106ms step_avg:45.75ms
step:1293/2110 train_time:59166ms step_avg:45.76ms
step:1294/2110 train_time:59224ms step_avg:45.77ms
step:1295/2110 train_time:59285ms step_avg:45.78ms
step:1296/2110 train_time:59343ms step_avg:45.79ms
step:1297/2110 train_time:59403ms step_avg:45.80ms
step:1298/2110 train_time:59461ms step_avg:45.81ms
step:1299/2110 train_time:59521ms step_avg:45.82ms
step:1300/2110 train_time:59580ms step_avg:45.83ms
step:1301/2110 train_time:59640ms step_avg:45.84ms
step:1302/2110 train_time:59699ms step_avg:45.85ms
step:1303/2110 train_time:59761ms step_avg:45.86ms
step:1304/2110 train_time:59821ms step_avg:45.87ms
step:1305/2110 train_time:59886ms step_avg:45.89ms
step:1306/2110 train_time:59946ms step_avg:45.90ms
step:1307/2110 train_time:60006ms step_avg:45.91ms
step:1308/2110 train_time:60065ms step_avg:45.92ms
step:1309/2110 train_time:60125ms step_avg:45.93ms
step:1310/2110 train_time:60185ms step_avg:45.94ms
step:1311/2110 train_time:60245ms step_avg:45.95ms
step:1312/2110 train_time:60304ms step_avg:45.96ms
step:1313/2110 train_time:60363ms step_avg:45.97ms
step:1314/2110 train_time:60423ms step_avg:45.98ms
step:1315/2110 train_time:60481ms step_avg:45.99ms
step:1316/2110 train_time:60542ms step_avg:46.00ms
step:1317/2110 train_time:60600ms step_avg:46.01ms
step:1318/2110 train_time:60661ms step_avg:46.03ms
step:1319/2110 train_time:60719ms step_avg:46.03ms
step:1320/2110 train_time:60780ms step_avg:46.05ms
step:1321/2110 train_time:60841ms step_avg:46.06ms
step:1322/2110 train_time:60902ms step_avg:46.07ms
step:1323/2110 train_time:60961ms step_avg:46.08ms
step:1324/2110 train_time:61022ms step_avg:46.09ms
step:1325/2110 train_time:61082ms step_avg:46.10ms
step:1326/2110 train_time:61143ms step_avg:46.11ms
step:1327/2110 train_time:61201ms step_avg:46.12ms
step:1328/2110 train_time:61262ms step_avg:46.13ms
step:1329/2110 train_time:61319ms step_avg:46.14ms
step:1330/2110 train_time:61380ms step_avg:46.15ms
step:1331/2110 train_time:61438ms step_avg:46.16ms
step:1332/2110 train_time:61497ms step_avg:46.17ms
step:1333/2110 train_time:61555ms step_avg:46.18ms
step:1334/2110 train_time:61616ms step_avg:46.19ms
step:1335/2110 train_time:61675ms step_avg:46.20ms
step:1336/2110 train_time:61735ms step_avg:46.21ms
step:1337/2110 train_time:61796ms step_avg:46.22ms
step:1338/2110 train_time:61858ms step_avg:46.23ms
step:1339/2110 train_time:61917ms step_avg:46.24ms
step:1340/2110 train_time:61978ms step_avg:46.25ms
step:1341/2110 train_time:62039ms step_avg:46.26ms
step:1342/2110 train_time:62099ms step_avg:46.27ms
step:1343/2110 train_time:62159ms step_avg:46.28ms
step:1344/2110 train_time:62218ms step_avg:46.29ms
step:1345/2110 train_time:62277ms step_avg:46.30ms
step:1346/2110 train_time:62337ms step_avg:46.31ms
step:1347/2110 train_time:62396ms step_avg:46.32ms
step:1348/2110 train_time:62455ms step_avg:46.33ms
step:1349/2110 train_time:62515ms step_avg:46.34ms
step:1350/2110 train_time:62575ms step_avg:46.35ms
step:1351/2110 train_time:62635ms step_avg:46.36ms
step:1352/2110 train_time:62695ms step_avg:46.37ms
step:1353/2110 train_time:62754ms step_avg:46.38ms
step:1354/2110 train_time:62814ms step_avg:46.39ms
step:1355/2110 train_time:62874ms step_avg:46.40ms
step:1356/2110 train_time:62935ms step_avg:46.41ms
step:1357/2110 train_time:62995ms step_avg:46.42ms
step:1358/2110 train_time:63056ms step_avg:46.43ms
step:1359/2110 train_time:63116ms step_avg:46.44ms
step:1360/2110 train_time:63177ms step_avg:46.45ms
step:1361/2110 train_time:63237ms step_avg:46.46ms
step:1362/2110 train_time:63296ms step_avg:46.47ms
step:1363/2110 train_time:63356ms step_avg:46.48ms
step:1364/2110 train_time:63415ms step_avg:46.49ms
step:1365/2110 train_time:63474ms step_avg:46.50ms
step:1366/2110 train_time:63533ms step_avg:46.51ms
step:1367/2110 train_time:63593ms step_avg:46.52ms
step:1368/2110 train_time:63653ms step_avg:46.53ms
step:1369/2110 train_time:63713ms step_avg:46.54ms
step:1370/2110 train_time:63773ms step_avg:46.55ms
step:1371/2110 train_time:63833ms step_avg:46.56ms
step:1372/2110 train_time:63894ms step_avg:46.57ms
step:1373/2110 train_time:63953ms step_avg:46.58ms
step:1374/2110 train_time:64013ms step_avg:46.59ms
step:1375/2110 train_time:64074ms step_avg:46.60ms
step:1376/2110 train_time:64135ms step_avg:46.61ms
step:1377/2110 train_time:64195ms step_avg:46.62ms
step:1378/2110 train_time:64254ms step_avg:46.63ms
step:1379/2110 train_time:64313ms step_avg:46.64ms
step:1380/2110 train_time:64373ms step_avg:46.65ms
step:1381/2110 train_time:64432ms step_avg:46.66ms
step:1382/2110 train_time:64519ms step_avg:46.69ms
step:1383/2110 train_time:64607ms step_avg:46.71ms
step:1384/2110 train_time:64692ms step_avg:46.74ms
step:1385/2110 train_time:64779ms step_avg:46.77ms
step:1386/2110 train_time:64866ms step_avg:46.80ms
step:1387/2110 train_time:64953ms step_avg:46.83ms
step:1388/2110 train_time:65040ms step_avg:46.86ms
step:1389/2110 train_time:65127ms step_avg:46.89ms
step:1390/2110 train_time:65213ms step_avg:46.92ms
step:1391/2110 train_time:65299ms step_avg:46.94ms
step:1392/2110 train_time:65384ms step_avg:46.97ms
step:1393/2110 train_time:65471ms step_avg:47.00ms
step:1394/2110 train_time:65557ms step_avg:47.03ms
step:1395/2110 train_time:65644ms step_avg:47.06ms
step:1396/2110 train_time:65730ms step_avg:47.08ms
step:1397/2110 train_time:65818ms step_avg:47.11ms
step:1398/2110 train_time:65904ms step_avg:47.14ms
step:1399/2110 train_time:65993ms step_avg:47.17ms
step:1400/2110 train_time:66078ms step_avg:47.20ms
step:1401/2110 train_time:66166ms step_avg:47.23ms
step:1402/2110 train_time:66252ms step_avg:47.26ms
step:1403/2110 train_time:66339ms step_avg:47.28ms
step:1404/2110 train_time:66425ms step_avg:47.31ms
step:1405/2110 train_time:66512ms step_avg:47.34ms
step:1406/2110 train_time:66598ms step_avg:47.37ms
step:1407/2110 train_time:66685ms step_avg:47.39ms
step:1408/2110 train_time:66770ms step_avg:47.42ms
step:1409/2110 train_time:66858ms step_avg:47.45ms
step:1410/2110 train_time:66945ms step_avg:47.48ms
step:1411/2110 train_time:67032ms step_avg:47.51ms
step:1412/2110 train_time:67119ms step_avg:47.53ms
step:1413/2110 train_time:67206ms step_avg:47.56ms
step:1414/2110 train_time:67292ms step_avg:47.59ms
step:1415/2110 train_time:67380ms step_avg:47.62ms
step:1416/2110 train_time:67466ms step_avg:47.65ms
step:1417/2110 train_time:67553ms step_avg:47.67ms
step:1418/2110 train_time:67639ms step_avg:47.70ms
step:1419/2110 train_time:67726ms step_avg:47.73ms
step:1420/2110 train_time:67811ms step_avg:47.75ms
step:1421/2110 train_time:67898ms step_avg:47.78ms
step:1422/2110 train_time:67985ms step_avg:47.81ms
step:1423/2110 train_time:68072ms step_avg:47.84ms
step:1424/2110 train_time:68160ms step_avg:47.86ms
step:1425/2110 train_time:68246ms step_avg:47.89ms
step:1426/2110 train_time:68332ms step_avg:47.92ms
step:1427/2110 train_time:68419ms step_avg:47.95ms
step:1428/2110 train_time:68505ms step_avg:47.97ms
step:1429/2110 train_time:68592ms step_avg:48.00ms
step:1430/2110 train_time:68679ms step_avg:48.03ms
step:1431/2110 train_time:68765ms step_avg:48.05ms
step:1432/2110 train_time:68852ms step_avg:48.08ms
step:1433/2110 train_time:68939ms step_avg:48.11ms
step:1434/2110 train_time:69025ms step_avg:48.13ms
step:1435/2110 train_time:69113ms step_avg:48.16ms
step:1436/2110 train_time:69200ms step_avg:48.19ms
step:1437/2110 train_time:69287ms step_avg:48.22ms
step:1438/2110 train_time:69373ms step_avg:48.24ms
step:1439/2110 train_time:69460ms step_avg:48.27ms
step:1440/2110 train_time:69547ms step_avg:48.30ms
step:1441/2110 train_time:69633ms step_avg:48.32ms
step:1442/2110 train_time:69719ms step_avg:48.35ms
step:1443/2110 train_time:69807ms step_avg:48.38ms
step:1444/2110 train_time:69893ms step_avg:48.40ms
step:1445/2110 train_time:69981ms step_avg:48.43ms
step:1446/2110 train_time:70068ms step_avg:48.46ms
step:1447/2110 train_time:70155ms step_avg:48.48ms
step:1448/2110 train_time:70241ms step_avg:48.51ms
step:1449/2110 train_time:70328ms step_avg:48.54ms
step:1450/2110 train_time:70415ms step_avg:48.56ms
step:1451/2110 train_time:70503ms step_avg:48.59ms
step:1452/2110 train_time:70589ms step_avg:48.62ms
step:1453/2110 train_time:70676ms step_avg:48.64ms
step:1454/2110 train_time:70762ms step_avg:48.67ms
step:1455/2110 train_time:70850ms step_avg:48.69ms
step:1456/2110 train_time:70935ms step_avg:48.72ms
step:1457/2110 train_time:71024ms step_avg:48.75ms
step:1458/2110 train_time:71111ms step_avg:48.77ms
step:1459/2110 train_time:71197ms step_avg:48.80ms
step:1460/2110 train_time:71285ms step_avg:48.83ms
step:1461/2110 train_time:71372ms step_avg:48.85ms
step:1462/2110 train_time:71459ms step_avg:48.88ms
step:1463/2110 train_time:71545ms step_avg:48.90ms
step:1464/2110 train_time:71631ms step_avg:48.93ms
step:1465/2110 train_time:71718ms step_avg:48.95ms
step:1466/2110 train_time:71804ms step_avg:48.98ms
step:1467/2110 train_time:71892ms step_avg:49.01ms
step:1468/2110 train_time:71977ms step_avg:49.03ms
step:1469/2110 train_time:72065ms step_avg:49.06ms
step:1470/2110 train_time:72151ms step_avg:49.08ms
step:1471/2110 train_time:72239ms step_avg:49.11ms
step:1472/2110 train_time:72325ms step_avg:49.13ms
step:1473/2110 train_time:72413ms step_avg:49.16ms
step:1474/2110 train_time:72499ms step_avg:49.19ms
step:1475/2110 train_time:72586ms step_avg:49.21ms
step:1476/2110 train_time:72671ms step_avg:49.24ms
step:1477/2110 train_time:72759ms step_avg:49.26ms
step:1478/2110 train_time:72845ms step_avg:49.29ms
step:1479/2110 train_time:72931ms step_avg:49.31ms
step:1480/2110 train_time:73017ms step_avg:49.34ms
step:1481/2110 train_time:73105ms step_avg:49.36ms
step:1482/2110 train_time:73192ms step_avg:49.39ms
step:1483/2110 train_time:73279ms step_avg:49.41ms
step:1484/2110 train_time:73366ms step_avg:49.44ms
step:1485/2110 train_time:73453ms step_avg:49.46ms
step:1486/2110 train_time:73539ms step_avg:49.49ms
step:1487/2110 train_time:73626ms step_avg:49.51ms
step:1488/2110 train_time:73712ms step_avg:49.54ms
step:1489/2110 train_time:73799ms step_avg:49.56ms
step:1490/2110 train_time:73886ms step_avg:49.59ms
step:1491/2110 train_time:73973ms step_avg:49.61ms
step:1492/2110 train_time:74059ms step_avg:49.64ms
step:1493/2110 train_time:74146ms step_avg:49.66ms
step:1494/2110 train_time:74233ms step_avg:49.69ms
step:1495/2110 train_time:74320ms step_avg:49.71ms
step:1496/2110 train_time:74406ms step_avg:49.74ms
step:1497/2110 train_time:74493ms step_avg:49.76ms
step:1498/2110 train_time:74579ms step_avg:49.79ms
step:1499/2110 train_time:74666ms step_avg:49.81ms
step:1500/2110 train_time:74752ms step_avg:49.83ms
step:1500/2110 val_loss:3.4921 train_time:74841ms step_avg:49.89ms
step:1501/2110 train_time:74863ms step_avg:49.88ms
step:1502/2110 train_time:74931ms step_avg:49.89ms
step:1503/2110 train_time:75023ms step_avg:49.92ms
step:1504/2110 train_time:75110ms step_avg:49.94ms
step:1505/2110 train_time:75198ms step_avg:49.97ms
step:1506/2110 train_time:75283ms step_avg:49.99ms
step:1507/2110 train_time:75369ms step_avg:50.01ms
step:1508/2110 train_time:75453ms step_avg:50.04ms
step:1509/2110 train_time:75540ms step_avg:50.06ms
step:1510/2110 train_time:75626ms step_avg:50.08ms
step:1511/2110 train_time:75712ms step_avg:50.11ms
step:1512/2110 train_time:75798ms step_avg:50.13ms
step:1513/2110 train_time:75888ms step_avg:50.16ms
step:1514/2110 train_time:75977ms step_avg:50.18ms
step:1515/2110 train_time:76066ms step_avg:50.21ms
step:1516/2110 train_time:76152ms step_avg:50.23ms
step:1517/2110 train_time:76240ms step_avg:50.26ms
step:1518/2110 train_time:76326ms step_avg:50.28ms
step:1519/2110 train_time:76412ms step_avg:50.30ms
step:1520/2110 train_time:76497ms step_avg:50.33ms
step:1521/2110 train_time:76584ms step_avg:50.35ms
step:1522/2110 train_time:76670ms step_avg:50.37ms
step:1523/2110 train_time:76756ms step_avg:50.40ms
step:1524/2110 train_time:76843ms step_avg:50.42ms
step:1525/2110 train_time:76933ms step_avg:50.45ms
step:1526/2110 train_time:77021ms step_avg:50.47ms
step:1527/2110 train_time:77109ms step_avg:50.50ms
step:1528/2110 train_time:77196ms step_avg:50.52ms
step:1529/2110 train_time:77283ms step_avg:50.54ms
step:1530/2110 train_time:77368ms step_avg:50.57ms
step:1531/2110 train_time:77455ms step_avg:50.59ms
step:1532/2110 train_time:77541ms step_avg:50.61ms
step:1533/2110 train_time:77627ms step_avg:50.64ms
step:1534/2110 train_time:77713ms step_avg:50.66ms
step:1535/2110 train_time:77800ms step_avg:50.68ms
step:1536/2110 train_time:77888ms step_avg:50.71ms
step:1537/2110 train_time:77977ms step_avg:50.73ms
step:1538/2110 train_time:78065ms step_avg:50.76ms
step:1539/2110 train_time:78152ms step_avg:50.78ms
step:1540/2110 train_time:78239ms step_avg:50.80ms
step:1541/2110 train_time:78325ms step_avg:50.83ms
step:1542/2110 train_time:78412ms step_avg:50.85ms
step:1543/2110 train_time:78498ms step_avg:50.87ms
step:1544/2110 train_time:78584ms step_avg:50.90ms
step:1545/2110 train_time:78671ms step_avg:50.92ms
step:1546/2110 train_time:78758ms step_avg:50.94ms
step:1547/2110 train_time:78844ms step_avg:50.97ms
step:1548/2110 train_time:78931ms step_avg:50.99ms
step:1549/2110 train_time:79020ms step_avg:51.01ms
step:1550/2110 train_time:79108ms step_avg:51.04ms
step:1551/2110 train_time:79194ms step_avg:51.06ms
step:1552/2110 train_time:79282ms step_avg:51.08ms
step:1553/2110 train_time:79368ms step_avg:51.11ms
step:1554/2110 train_time:79454ms step_avg:51.13ms
step:1555/2110 train_time:79540ms step_avg:51.15ms
step:1556/2110 train_time:79626ms step_avg:51.17ms
step:1557/2110 train_time:79713ms step_avg:51.20ms
step:1558/2110 train_time:79800ms step_avg:51.22ms
step:1559/2110 train_time:79887ms step_avg:51.24ms
step:1560/2110 train_time:79972ms step_avg:51.26ms
step:1561/2110 train_time:80061ms step_avg:51.29ms
step:1562/2110 train_time:80148ms step_avg:51.31ms
step:1563/2110 train_time:80235ms step_avg:51.33ms
step:1564/2110 train_time:80323ms step_avg:51.36ms
step:1565/2110 train_time:80410ms step_avg:51.38ms
step:1566/2110 train_time:80497ms step_avg:51.40ms
step:1567/2110 train_time:80582ms step_avg:51.42ms
step:1568/2110 train_time:80669ms step_avg:51.45ms
step:1569/2110 train_time:80756ms step_avg:51.47ms
step:1570/2110 train_time:80844ms step_avg:51.49ms
step:1571/2110 train_time:80931ms step_avg:51.52ms
step:1572/2110 train_time:81018ms step_avg:51.54ms
step:1573/2110 train_time:81106ms step_avg:51.56ms
step:1574/2110 train_time:81192ms step_avg:51.58ms
step:1575/2110 train_time:81280ms step_avg:51.61ms
step:1576/2110 train_time:81367ms step_avg:51.63ms
step:1577/2110 train_time:81453ms step_avg:51.65ms
step:1578/2110 train_time:81542ms step_avg:51.67ms
step:1579/2110 train_time:81628ms step_avg:51.70ms
step:1580/2110 train_time:81715ms step_avg:51.72ms
step:1581/2110 train_time:81801ms step_avg:51.74ms
step:1582/2110 train_time:81889ms step_avg:51.76ms
step:1583/2110 train_time:81976ms step_avg:51.79ms
step:1584/2110 train_time:82063ms step_avg:51.81ms
step:1585/2110 train_time:82150ms step_avg:51.83ms
step:1586/2110 train_time:82238ms step_avg:51.85ms
step:1587/2110 train_time:82324ms step_avg:51.87ms
step:1588/2110 train_time:82411ms step_avg:51.90ms
step:1589/2110 train_time:82499ms step_avg:51.92ms
step:1590/2110 train_time:82585ms step_avg:51.94ms
step:1591/2110 train_time:82672ms step_avg:51.96ms
step:1592/2110 train_time:82758ms step_avg:51.98ms
step:1593/2110 train_time:82846ms step_avg:52.01ms
step:1594/2110 train_time:82933ms step_avg:52.03ms
step:1595/2110 train_time:83019ms step_avg:52.05ms
step:1596/2110 train_time:83108ms step_avg:52.07ms
step:1597/2110 train_time:83193ms step_avg:52.09ms
step:1598/2110 train_time:83280ms step_avg:52.12ms
step:1599/2110 train_time:83367ms step_avg:52.14ms
step:1600/2110 train_time:83453ms step_avg:52.16ms
step:1601/2110 train_time:83541ms step_avg:52.18ms
step:1602/2110 train_time:83628ms step_avg:52.20ms
step:1603/2110 train_time:83714ms step_avg:52.22ms
step:1604/2110 train_time:83801ms step_avg:52.25ms
step:1605/2110 train_time:83887ms step_avg:52.27ms
step:1606/2110 train_time:83974ms step_avg:52.29ms
step:1607/2110 train_time:84061ms step_avg:52.31ms
step:1608/2110 train_time:84149ms step_avg:52.33ms
step:1609/2110 train_time:84235ms step_avg:52.35ms
step:1610/2110 train_time:84323ms step_avg:52.37ms
step:1611/2110 train_time:84410ms step_avg:52.40ms
step:1612/2110 train_time:84497ms step_avg:52.42ms
step:1613/2110 train_time:84584ms step_avg:52.44ms
step:1614/2110 train_time:84670ms step_avg:52.46ms
step:1615/2110 train_time:84757ms step_avg:52.48ms
step:1616/2110 train_time:84843ms step_avg:52.50ms
step:1617/2110 train_time:84930ms step_avg:52.52ms
step:1618/2110 train_time:85017ms step_avg:52.54ms
step:1619/2110 train_time:85105ms step_avg:52.57ms
step:1620/2110 train_time:85192ms step_avg:52.59ms
step:1621/2110 train_time:85278ms step_avg:52.61ms
step:1622/2110 train_time:85366ms step_avg:52.63ms
step:1623/2110 train_time:85452ms step_avg:52.65ms
step:1624/2110 train_time:85540ms step_avg:52.67ms
step:1625/2110 train_time:85627ms step_avg:52.69ms
step:1626/2110 train_time:85713ms step_avg:52.71ms
step:1627/2110 train_time:85801ms step_avg:52.74ms
step:1628/2110 train_time:85887ms step_avg:52.76ms
step:1629/2110 train_time:85974ms step_avg:52.78ms
step:1630/2110 train_time:86060ms step_avg:52.80ms
step:1631/2110 train_time:86148ms step_avg:52.82ms
step:1632/2110 train_time:86234ms step_avg:52.84ms
step:1633/2110 train_time:86322ms step_avg:52.86ms
step:1634/2110 train_time:86408ms step_avg:52.88ms
step:1635/2110 train_time:86496ms step_avg:52.90ms
step:1636/2110 train_time:86583ms step_avg:52.92ms
step:1637/2110 train_time:86670ms step_avg:52.94ms
step:1638/2110 train_time:86757ms step_avg:52.97ms
step:1639/2110 train_time:86844ms step_avg:52.99ms
step:1640/2110 train_time:86930ms step_avg:53.01ms
step:1641/2110 train_time:87017ms step_avg:53.03ms
step:1642/2110 train_time:87105ms step_avg:53.05ms
step:1643/2110 train_time:87191ms step_avg:53.07ms
step:1644/2110 train_time:87278ms step_avg:53.09ms
step:1645/2110 train_time:87364ms step_avg:53.11ms
step:1646/2110 train_time:87450ms step_avg:53.13ms
step:1647/2110 train_time:87539ms step_avg:53.15ms
step:1648/2110 train_time:87625ms step_avg:53.17ms
step:1649/2110 train_time:87712ms step_avg:53.19ms
step:1650/2110 train_time:87800ms step_avg:53.21ms
step:1651/2110 train_time:87887ms step_avg:53.23ms
step:1652/2110 train_time:87973ms step_avg:53.25ms
step:1653/2110 train_time:88060ms step_avg:53.27ms
step:1654/2110 train_time:88147ms step_avg:53.29ms
step:1655/2110 train_time:88234ms step_avg:53.31ms
step:1656/2110 train_time:88321ms step_avg:53.33ms
step:1657/2110 train_time:88408ms step_avg:53.35ms
step:1658/2110 train_time:88495ms step_avg:53.37ms
step:1659/2110 train_time:88585ms step_avg:53.40ms
step:1660/2110 train_time:88672ms step_avg:53.42ms
step:1661/2110 train_time:88760ms step_avg:53.44ms
step:1662/2110 train_time:88848ms step_avg:53.46ms
step:1663/2110 train_time:88937ms step_avg:53.48ms
step:1664/2110 train_time:89025ms step_avg:53.50ms
step:1665/2110 train_time:89114ms step_avg:53.52ms
step:1666/2110 train_time:89203ms step_avg:53.54ms
step:1667/2110 train_time:89291ms step_avg:53.56ms
step:1668/2110 train_time:89379ms step_avg:53.58ms
step:1669/2110 train_time:89468ms step_avg:53.61ms
step:1670/2110 train_time:89555ms step_avg:53.63ms
step:1671/2110 train_time:89644ms step_avg:53.65ms
step:1672/2110 train_time:89731ms step_avg:53.67ms
step:1673/2110 train_time:89819ms step_avg:53.69ms
step:1674/2110 train_time:89907ms step_avg:53.71ms
step:1675/2110 train_time:89996ms step_avg:53.73ms
step:1676/2110 train_time:90084ms step_avg:53.75ms
step:1677/2110 train_time:90173ms step_avg:53.77ms
step:1678/2110 train_time:90261ms step_avg:53.79ms
step:1679/2110 train_time:90351ms step_avg:53.81ms
step:1680/2110 train_time:90439ms step_avg:53.83ms
step:1681/2110 train_time:90527ms step_avg:53.85ms
step:1682/2110 train_time:90615ms step_avg:53.87ms
step:1683/2110 train_time:90703ms step_avg:53.89ms
step:1684/2110 train_time:90790ms step_avg:53.91ms
step:1685/2110 train_time:90880ms step_avg:53.93ms
step:1686/2110 train_time:90967ms step_avg:53.95ms
step:1687/2110 train_time:91055ms step_avg:53.97ms
step:1688/2110 train_time:91143ms step_avg:53.99ms
step:1689/2110 train_time:91232ms step_avg:54.02ms
step:1690/2110 train_time:91320ms step_avg:54.04ms
step:1691/2110 train_time:91407ms step_avg:54.06ms
step:1692/2110 train_time:91495ms step_avg:54.07ms
step:1693/2110 train_time:91583ms step_avg:54.10ms
step:1694/2110 train_time:91670ms step_avg:54.11ms
step:1695/2110 train_time:91759ms step_avg:54.13ms
step:1696/2110 train_time:91848ms step_avg:54.16ms
step:1697/2110 train_time:91936ms step_avg:54.18ms
step:1698/2110 train_time:92025ms step_avg:54.20ms
step:1699/2110 train_time:92112ms step_avg:54.22ms
step:1700/2110 train_time:92200ms step_avg:54.24ms
step:1701/2110 train_time:92289ms step_avg:54.26ms
step:1702/2110 train_time:92377ms step_avg:54.28ms
step:1703/2110 train_time:92465ms step_avg:54.30ms
step:1704/2110 train_time:92552ms step_avg:54.31ms
step:1705/2110 train_time:92642ms step_avg:54.34ms
step:1706/2110 train_time:92730ms step_avg:54.36ms
step:1707/2110 train_time:92819ms step_avg:54.38ms
step:1708/2110 train_time:92908ms step_avg:54.40ms
step:1709/2110 train_time:92996ms step_avg:54.42ms
step:1710/2110 train_time:93085ms step_avg:54.44ms
step:1711/2110 train_time:93172ms step_avg:54.45ms
step:1712/2110 train_time:93261ms step_avg:54.47ms
step:1713/2110 train_time:93349ms step_avg:54.49ms
step:1714/2110 train_time:93438ms step_avg:54.51ms
step:1715/2110 train_time:93527ms step_avg:54.53ms
step:1716/2110 train_time:93614ms step_avg:54.55ms
step:1717/2110 train_time:93703ms step_avg:54.57ms
step:1718/2110 train_time:93790ms step_avg:54.59ms
step:1719/2110 train_time:93879ms step_avg:54.61ms
step:1720/2110 train_time:93966ms step_avg:54.63ms
step:1721/2110 train_time:94055ms step_avg:54.65ms
step:1722/2110 train_time:94143ms step_avg:54.67ms
step:1723/2110 train_time:94231ms step_avg:54.69ms
step:1724/2110 train_time:94319ms step_avg:54.71ms
step:1725/2110 train_time:94409ms step_avg:54.73ms
step:1726/2110 train_time:94497ms step_avg:54.75ms
step:1727/2110 train_time:94585ms step_avg:54.77ms
step:1728/2110 train_time:94672ms step_avg:54.79ms
step:1729/2110 train_time:94762ms step_avg:54.81ms
step:1730/2110 train_time:94849ms step_avg:54.83ms
step:1731/2110 train_time:94938ms step_avg:54.85ms
step:1732/2110 train_time:95026ms step_avg:54.86ms
step:1733/2110 train_time:95114ms step_avg:54.88ms
step:1734/2110 train_time:95202ms step_avg:54.90ms
step:1735/2110 train_time:95290ms step_avg:54.92ms
step:1736/2110 train_time:95377ms step_avg:54.94ms
step:1737/2110 train_time:95466ms step_avg:54.96ms
step:1738/2110 train_time:95554ms step_avg:54.98ms
step:1739/2110 train_time:95643ms step_avg:55.00ms
step:1740/2110 train_time:95731ms step_avg:55.02ms
step:1741/2110 train_time:95820ms step_avg:55.04ms
step:1742/2110 train_time:95908ms step_avg:55.06ms
step:1743/2110 train_time:95996ms step_avg:55.08ms
step:1744/2110 train_time:96083ms step_avg:55.09ms
step:1745/2110 train_time:96173ms step_avg:55.11ms
step:1746/2110 train_time:96260ms step_avg:55.13ms
step:1747/2110 train_time:96350ms step_avg:55.15ms
step:1748/2110 train_time:96438ms step_avg:55.17ms
step:1749/2110 train_time:96526ms step_avg:55.19ms
step:1750/2110 train_time:96613ms step_avg:55.21ms
step:1750/2110 val_loss:3.3776 train_time:96704ms step_avg:55.26ms
step:1751/2110 train_time:96725ms step_avg:55.24ms
step:1752/2110 train_time:96796ms step_avg:55.25ms
step:1753/2110 train_time:96891ms step_avg:55.27ms
step:1754/2110 train_time:96980ms step_avg:55.29ms
step:1755/2110 train_time:97068ms step_avg:55.31ms
step:1756/2110 train_time:97155ms step_avg:55.33ms
step:1757/2110 train_time:97242ms step_avg:55.35ms
step:1758/2110 train_time:97329ms step_avg:55.36ms
step:1759/2110 train_time:97416ms step_avg:55.38ms
step:1760/2110 train_time:97502ms step_avg:55.40ms
step:1761/2110 train_time:97589ms step_avg:55.42ms
step:1762/2110 train_time:97677ms step_avg:55.44ms
step:1763/2110 train_time:97768ms step_avg:55.46ms
step:1764/2110 train_time:97859ms step_avg:55.48ms
step:1765/2110 train_time:97948ms step_avg:55.49ms
step:1766/2110 train_time:98036ms step_avg:55.51ms
step:1767/2110 train_time:98124ms step_avg:55.53ms
step:1768/2110 train_time:98211ms step_avg:55.55ms
step:1769/2110 train_time:98298ms step_avg:55.57ms
step:1770/2110 train_time:98384ms step_avg:55.58ms
step:1771/2110 train_time:98472ms step_avg:55.60ms
step:1772/2110 train_time:98560ms step_avg:55.62ms
step:1773/2110 train_time:98648ms step_avg:55.64ms
step:1774/2110 train_time:98739ms step_avg:55.66ms
step:1775/2110 train_time:98828ms step_avg:55.68ms
step:1776/2110 train_time:98917ms step_avg:55.70ms
step:1777/2110 train_time:99007ms step_avg:55.72ms
step:1778/2110 train_time:99095ms step_avg:55.73ms
step:1779/2110 train_time:99182ms step_avg:55.75ms
step:1780/2110 train_time:99270ms step_avg:55.77ms
step:1781/2110 train_time:99357ms step_avg:55.79ms
step:1782/2110 train_time:99443ms step_avg:55.80ms
step:1783/2110 train_time:99531ms step_avg:55.82ms
step:1784/2110 train_time:99619ms step_avg:55.84ms
step:1785/2110 train_time:99708ms step_avg:55.86ms
step:1786/2110 train_time:99798ms step_avg:55.88ms
step:1787/2110 train_time:99887ms step_avg:55.90ms
step:1788/2110 train_time:99975ms step_avg:55.91ms
step:1789/2110 train_time:100065ms step_avg:55.93ms
step:1790/2110 train_time:100153ms step_avg:55.95ms
step:1791/2110 train_time:100241ms step_avg:55.97ms
step:1792/2110 train_time:100328ms step_avg:55.99ms
step:1793/2110 train_time:100416ms step_avg:56.00ms
step:1794/2110 train_time:100503ms step_avg:56.02ms
step:1795/2110 train_time:100590ms step_avg:56.04ms
step:1796/2110 train_time:100678ms step_avg:56.06ms
step:1797/2110 train_time:100767ms step_avg:56.08ms
step:1798/2110 train_time:100855ms step_avg:56.09ms
step:1799/2110 train_time:100944ms step_avg:56.11ms
step:1800/2110 train_time:101034ms step_avg:56.13ms
step:1801/2110 train_time:101122ms step_avg:56.15ms
step:1802/2110 train_time:101210ms step_avg:56.17ms
step:1803/2110 train_time:101298ms step_avg:56.18ms
step:1804/2110 train_time:101385ms step_avg:56.20ms
step:1805/2110 train_time:101472ms step_avg:56.22ms
step:1806/2110 train_time:101561ms step_avg:56.24ms
step:1807/2110 train_time:101648ms step_avg:56.25ms
step:1808/2110 train_time:101737ms step_avg:56.27ms
step:1809/2110 train_time:101826ms step_avg:56.29ms
step:1810/2110 train_time:101914ms step_avg:56.31ms
step:1811/2110 train_time:102004ms step_avg:56.32ms
step:1812/2110 train_time:102092ms step_avg:56.34ms
step:1813/2110 train_time:102180ms step_avg:56.36ms
step:1814/2110 train_time:102268ms step_avg:56.38ms
step:1815/2110 train_time:102357ms step_avg:56.39ms
step:1816/2110 train_time:102444ms step_avg:56.41ms
step:1817/2110 train_time:102533ms step_avg:56.43ms
step:1818/2110 train_time:102621ms step_avg:56.45ms
step:1819/2110 train_time:102709ms step_avg:56.46ms
step:1820/2110 train_time:102797ms step_avg:56.48ms
step:1821/2110 train_time:102885ms step_avg:56.50ms
step:1822/2110 train_time:102973ms step_avg:56.52ms
step:1823/2110 train_time:103062ms step_avg:56.53ms
step:1824/2110 train_time:103151ms step_avg:56.55ms
step:1825/2110 train_time:103240ms step_avg:56.57ms
step:1826/2110 train_time:103327ms step_avg:56.59ms
step:1827/2110 train_time:103416ms step_avg:56.60ms
step:1828/2110 train_time:103503ms step_avg:56.62ms
step:1829/2110 train_time:103590ms step_avg:56.64ms
step:1830/2110 train_time:103678ms step_avg:56.65ms
step:1831/2110 train_time:103766ms step_avg:56.67ms
step:1832/2110 train_time:103854ms step_avg:56.69ms
step:1833/2110 train_time:103944ms step_avg:56.71ms
step:1834/2110 train_time:104031ms step_avg:56.72ms
step:1835/2110 train_time:104121ms step_avg:56.74ms
step:1836/2110 train_time:104208ms step_avg:56.76ms
step:1837/2110 train_time:104297ms step_avg:56.78ms
step:1838/2110 train_time:104384ms step_avg:56.79ms
step:1839/2110 train_time:104472ms step_avg:56.81ms
step:1840/2110 train_time:104560ms step_avg:56.83ms
step:1841/2110 train_time:104648ms step_avg:56.84ms
step:1842/2110 train_time:104736ms step_avg:56.86ms
step:1843/2110 train_time:104825ms step_avg:56.88ms
step:1844/2110 train_time:104914ms step_avg:56.89ms
step:1845/2110 train_time:105003ms step_avg:56.91ms
step:1846/2110 train_time:105090ms step_avg:56.93ms
step:1847/2110 train_time:105179ms step_avg:56.95ms
step:1848/2110 train_time:105267ms step_avg:56.96ms
step:1849/2110 train_time:105356ms step_avg:56.98ms
step:1850/2110 train_time:105443ms step_avg:57.00ms
step:1851/2110 train_time:105531ms step_avg:57.01ms
step:1852/2110 train_time:105619ms step_avg:57.03ms
step:1853/2110 train_time:105707ms step_avg:57.05ms
step:1854/2110 train_time:105795ms step_avg:57.06ms
step:1855/2110 train_time:105884ms step_avg:57.08ms
step:1856/2110 train_time:105973ms step_avg:57.10ms
step:1857/2110 train_time:106061ms step_avg:57.11ms
step:1858/2110 train_time:106149ms step_avg:57.13ms
step:1859/2110 train_time:106238ms step_avg:57.15ms
step:1860/2110 train_time:106324ms step_avg:57.16ms
step:1861/2110 train_time:106413ms step_avg:57.18ms
step:1862/2110 train_time:106500ms step_avg:57.20ms
step:1863/2110 train_time:106588ms step_avg:57.21ms
step:1864/2110 train_time:106677ms step_avg:57.23ms
step:1865/2110 train_time:106765ms step_avg:57.25ms
step:1866/2110 train_time:106853ms step_avg:57.26ms
step:1867/2110 train_time:106942ms step_avg:57.28ms
step:1868/2110 train_time:107029ms step_avg:57.30ms
step:1869/2110 train_time:107118ms step_avg:57.31ms
step:1870/2110 train_time:107205ms step_avg:57.33ms
step:1871/2110 train_time:107294ms step_avg:57.35ms
step:1872/2110 train_time:107382ms step_avg:57.36ms
step:1873/2110 train_time:107471ms step_avg:57.38ms
step:1874/2110 train_time:107558ms step_avg:57.39ms
step:1875/2110 train_time:107647ms step_avg:57.41ms
step:1876/2110 train_time:107735ms step_avg:57.43ms
step:1877/2110 train_time:107823ms step_avg:57.44ms
step:1878/2110 train_time:107911ms step_avg:57.46ms
step:1879/2110 train_time:108000ms step_avg:57.48ms
step:1880/2110 train_time:108086ms step_avg:57.49ms
step:1881/2110 train_time:108175ms step_avg:57.51ms
step:1882/2110 train_time:108262ms step_avg:57.53ms
step:1883/2110 train_time:108352ms step_avg:57.54ms
step:1884/2110 train_time:108441ms step_avg:57.56ms
step:1885/2110 train_time:108529ms step_avg:57.58ms
step:1886/2110 train_time:108617ms step_avg:57.59ms
step:1887/2110 train_time:108705ms step_avg:57.61ms
step:1888/2110 train_time:108793ms step_avg:57.62ms
step:1889/2110 train_time:108881ms step_avg:57.64ms
step:1890/2110 train_time:108970ms step_avg:57.66ms
step:1891/2110 train_time:109059ms step_avg:57.67ms
step:1892/2110 train_time:109147ms step_avg:57.69ms
step:1893/2110 train_time:109235ms step_avg:57.70ms
step:1894/2110 train_time:109323ms step_avg:57.72ms
step:1895/2110 train_time:109412ms step_avg:57.74ms
step:1896/2110 train_time:109500ms step_avg:57.75ms
step:1897/2110 train_time:109587ms step_avg:57.77ms
step:1898/2110 train_time:109675ms step_avg:57.78ms
step:1899/2110 train_time:109762ms step_avg:57.80ms
step:1900/2110 train_time:109851ms step_avg:57.82ms
step:1901/2110 train_time:109939ms step_avg:57.83ms
step:1902/2110 train_time:110027ms step_avg:57.85ms
step:1903/2110 train_time:110115ms step_avg:57.86ms
step:1904/2110 train_time:110203ms step_avg:57.88ms
step:1905/2110 train_time:110292ms step_avg:57.90ms
step:1906/2110 train_time:110380ms step_avg:57.91ms
step:1907/2110 train_time:110468ms step_avg:57.93ms
step:1908/2110 train_time:110555ms step_avg:57.94ms
step:1909/2110 train_time:110643ms step_avg:57.96ms
step:1910/2110 train_time:110730ms step_avg:57.97ms
step:1911/2110 train_time:110819ms step_avg:57.99ms
step:1912/2110 train_time:110906ms step_avg:58.01ms
step:1913/2110 train_time:110995ms step_avg:58.02ms
step:1914/2110 train_time:111082ms step_avg:58.04ms
step:1915/2110 train_time:111171ms step_avg:58.05ms
step:1916/2110 train_time:111260ms step_avg:58.07ms
step:1917/2110 train_time:111348ms step_avg:58.08ms
step:1918/2110 train_time:111437ms step_avg:58.10ms
step:1919/2110 train_time:111525ms step_avg:58.12ms
step:1920/2110 train_time:111612ms step_avg:58.13ms
step:1921/2110 train_time:111701ms step_avg:58.15ms
step:1922/2110 train_time:111789ms step_avg:58.16ms
step:1923/2110 train_time:111877ms step_avg:58.18ms
step:1924/2110 train_time:111965ms step_avg:58.19ms
step:1925/2110 train_time:112054ms step_avg:58.21ms
step:1926/2110 train_time:112142ms step_avg:58.23ms
step:1927/2110 train_time:112230ms step_avg:58.24ms
step:1928/2110 train_time:112318ms step_avg:58.26ms
step:1929/2110 train_time:112406ms step_avg:58.27ms
step:1930/2110 train_time:112494ms step_avg:58.29ms
step:1931/2110 train_time:112583ms step_avg:58.30ms
step:1932/2110 train_time:112671ms step_avg:58.32ms
step:1933/2110 train_time:112761ms step_avg:58.33ms
step:1934/2110 train_time:112849ms step_avg:58.35ms
step:1935/2110 train_time:112937ms step_avg:58.37ms
step:1936/2110 train_time:113025ms step_avg:58.38ms
step:1937/2110 train_time:113113ms step_avg:58.40ms
step:1938/2110 train_time:113201ms step_avg:58.41ms
step:1939/2110 train_time:113289ms step_avg:58.43ms
step:1940/2110 train_time:113377ms step_avg:58.44ms
step:1941/2110 train_time:113465ms step_avg:58.46ms
step:1942/2110 train_time:113553ms step_avg:58.47ms
step:1943/2110 train_time:113641ms step_avg:58.49ms
step:1944/2110 train_time:113730ms step_avg:58.50ms
step:1945/2110 train_time:113818ms step_avg:58.52ms
step:1946/2110 train_time:113905ms step_avg:58.53ms
step:1947/2110 train_time:113994ms step_avg:58.55ms
step:1948/2110 train_time:114082ms step_avg:58.56ms
step:1949/2110 train_time:114170ms step_avg:58.58ms
step:1950/2110 train_time:114258ms step_avg:58.59ms
step:1951/2110 train_time:114346ms step_avg:58.61ms
step:1952/2110 train_time:114433ms step_avg:58.62ms
step:1953/2110 train_time:114522ms step_avg:58.64ms
step:1954/2110 train_time:114610ms step_avg:58.65ms
step:1955/2110 train_time:114699ms step_avg:58.67ms
step:1956/2110 train_time:114786ms step_avg:58.68ms
step:1957/2110 train_time:114875ms step_avg:58.70ms
step:1958/2110 train_time:114962ms step_avg:58.71ms
step:1959/2110 train_time:115051ms step_avg:58.73ms
step:1960/2110 train_time:115139ms step_avg:58.74ms
step:1961/2110 train_time:115226ms step_avg:58.76ms
step:1962/2110 train_time:115315ms step_avg:58.77ms
step:1963/2110 train_time:115403ms step_avg:58.79ms
step:1964/2110 train_time:115491ms step_avg:58.80ms
step:1965/2110 train_time:115580ms step_avg:58.82ms
step:1966/2110 train_time:115668ms step_avg:58.83ms
step:1967/2110 train_time:115757ms step_avg:58.85ms
step:1968/2110 train_time:115845ms step_avg:58.86ms
step:1969/2110 train_time:115935ms step_avg:58.88ms
step:1970/2110 train_time:116022ms step_avg:58.89ms
step:1971/2110 train_time:116110ms step_avg:58.91ms
step:1972/2110 train_time:116199ms step_avg:58.92ms
step:1973/2110 train_time:116287ms step_avg:58.94ms
step:1974/2110 train_time:116375ms step_avg:58.95ms
step:1975/2110 train_time:116463ms step_avg:58.97ms
step:1976/2110 train_time:116552ms step_avg:58.98ms
step:1977/2110 train_time:116641ms step_avg:59.00ms
step:1978/2110 train_time:116730ms step_avg:59.01ms
step:1979/2110 train_time:116818ms step_avg:59.03ms
step:1980/2110 train_time:116906ms step_avg:59.04ms
step:1981/2110 train_time:116994ms step_avg:59.06ms
step:1982/2110 train_time:117082ms step_avg:59.07ms
step:1983/2110 train_time:117170ms step_avg:59.09ms
step:1984/2110 train_time:117257ms step_avg:59.10ms
step:1985/2110 train_time:117345ms step_avg:59.12ms
step:1986/2110 train_time:117433ms step_avg:59.13ms
step:1987/2110 train_time:117521ms step_avg:59.15ms
step:1988/2110 train_time:117610ms step_avg:59.16ms
step:1989/2110 train_time:117698ms step_avg:59.17ms
step:1990/2110 train_time:117785ms step_avg:59.19ms
step:1991/2110 train_time:117875ms step_avg:59.20ms
step:1992/2110 train_time:117963ms step_avg:59.22ms
step:1993/2110 train_time:118051ms step_avg:59.23ms
step:1994/2110 train_time:118140ms step_avg:59.25ms
step:1995/2110 train_time:118228ms step_avg:59.26ms
step:1996/2110 train_time:118315ms step_avg:59.28ms
step:1997/2110 train_time:118404ms step_avg:59.29ms
step:1998/2110 train_time:118492ms step_avg:59.31ms
step:1999/2110 train_time:118582ms step_avg:59.32ms
step:2000/2110 train_time:118669ms step_avg:59.33ms
step:2000/2110 val_loss:3.3033 train_time:118760ms step_avg:59.38ms
step:2001/2110 train_time:118782ms step_avg:59.36ms
step:2002/2110 train_time:118853ms step_avg:59.37ms
step:2003/2110 train_time:118945ms step_avg:59.38ms
step:2004/2110 train_time:119034ms step_avg:59.40ms
step:2005/2110 train_time:119122ms step_avg:59.41ms
step:2006/2110 train_time:119209ms step_avg:59.43ms
step:2007/2110 train_time:119296ms step_avg:59.44ms
step:2008/2110 train_time:119383ms step_avg:59.45ms
step:2009/2110 train_time:119470ms step_avg:59.47ms
step:2010/2110 train_time:119557ms step_avg:59.48ms
step:2011/2110 train_time:119645ms step_avg:59.50ms
step:2012/2110 train_time:119735ms step_avg:59.51ms
step:2013/2110 train_time:119827ms step_avg:59.53ms
step:2014/2110 train_time:119916ms step_avg:59.54ms
step:2015/2110 train_time:120006ms step_avg:59.56ms
step:2016/2110 train_time:120094ms step_avg:59.57ms
step:2017/2110 train_time:120182ms step_avg:59.58ms
step:2018/2110 train_time:120270ms step_avg:59.60ms
step:2019/2110 train_time:120357ms step_avg:59.61ms
step:2020/2110 train_time:120444ms step_avg:59.63ms
step:2021/2110 train_time:120532ms step_avg:59.64ms
step:2022/2110 train_time:120618ms step_avg:59.65ms
step:2023/2110 train_time:120708ms step_avg:59.67ms
step:2024/2110 train_time:120797ms step_avg:59.68ms
step:2025/2110 train_time:120888ms step_avg:59.70ms
step:2026/2110 train_time:120977ms step_avg:59.71ms
step:2027/2110 train_time:121066ms step_avg:59.73ms
step:2028/2110 train_time:121153ms step_avg:59.74ms
step:2029/2110 train_time:121241ms step_avg:59.75ms
step:2030/2110 train_time:121328ms step_avg:59.77ms
step:2031/2110 train_time:121416ms step_avg:59.78ms
step:2032/2110 train_time:121503ms step_avg:59.79ms
step:2033/2110 train_time:121591ms step_avg:59.81ms
step:2034/2110 train_time:121679ms step_avg:59.82ms
step:2035/2110 train_time:121769ms step_avg:59.84ms
step:2036/2110 train_time:121857ms step_avg:59.85ms
step:2037/2110 train_time:121948ms step_avg:59.87ms
step:2038/2110 train_time:122036ms step_avg:59.88ms
step:2039/2110 train_time:122124ms step_avg:59.89ms
step:2040/2110 train_time:122211ms step_avg:59.91ms
step:2041/2110 train_time:122299ms step_avg:59.92ms
step:2042/2110 train_time:122386ms step_avg:59.93ms
step:2043/2110 train_time:122474ms step_avg:59.95ms
step:2044/2110 train_time:122561ms step_avg:59.96ms
step:2045/2110 train_time:122649ms step_avg:59.97ms
step:2046/2110 train_time:122736ms step_avg:59.99ms
step:2047/2110 train_time:122825ms step_avg:60.00ms
step:2048/2110 train_time:122914ms step_avg:60.02ms
step:2049/2110 train_time:123002ms step_avg:60.03ms
step:2050/2110 train_time:123090ms step_avg:60.04ms
step:2051/2110 train_time:123178ms step_avg:60.06ms
step:2052/2110 train_time:123266ms step_avg:60.07ms
step:2053/2110 train_time:123355ms step_avg:60.09ms
step:2054/2110 train_time:123443ms step_avg:60.10ms
step:2055/2110 train_time:123531ms step_avg:60.11ms
step:2056/2110 train_time:123619ms step_avg:60.13ms
step:2057/2110 train_time:123708ms step_avg:60.14ms
step:2058/2110 train_time:123796ms step_avg:60.15ms
step:2059/2110 train_time:123887ms step_avg:60.17ms
step:2060/2110 train_time:123976ms step_avg:60.18ms
step:2061/2110 train_time:124064ms step_avg:60.20ms
step:2062/2110 train_time:124151ms step_avg:60.21ms
step:2063/2110 train_time:124240ms step_avg:60.22ms
step:2064/2110 train_time:124327ms step_avg:60.24ms
step:2065/2110 train_time:124416ms step_avg:60.25ms
step:2066/2110 train_time:124503ms step_avg:60.26ms
step:2067/2110 train_time:124591ms step_avg:60.28ms
step:2068/2110 train_time:124679ms step_avg:60.29ms
step:2069/2110 train_time:124768ms step_avg:60.30ms
step:2070/2110 train_time:124856ms step_avg:60.32ms
step:2071/2110 train_time:124945ms step_avg:60.33ms
step:2072/2110 train_time:125033ms step_avg:60.34ms
step:2073/2110 train_time:125123ms step_avg:60.36ms
step:2074/2110 train_time:125211ms step_avg:60.37ms
step:2075/2110 train_time:125300ms step_avg:60.39ms
step:2076/2110 train_time:125387ms step_avg:60.40ms
step:2077/2110 train_time:125476ms step_avg:60.41ms
step:2078/2110 train_time:125564ms step_avg:60.43ms
step:2079/2110 train_time:125652ms step_avg:60.44ms
step:2080/2110 train_time:125740ms step_avg:60.45ms
step:2081/2110 train_time:125829ms step_avg:60.47ms
step:2082/2110 train_time:125916ms step_avg:60.48ms
step:2083/2110 train_time:126007ms step_avg:60.49ms
step:2084/2110 train_time:126095ms step_avg:60.51ms
step:2085/2110 train_time:126184ms step_avg:60.52ms
step:2086/2110 train_time:126273ms step_avg:60.53ms
step:2087/2110 train_time:126360ms step_avg:60.55ms
step:2088/2110 train_time:126448ms step_avg:60.56ms
step:2089/2110 train_time:126536ms step_avg:60.57ms
step:2090/2110 train_time:126625ms step_avg:60.59ms
step:2091/2110 train_time:126713ms step_avg:60.60ms
step:2092/2110 train_time:126801ms step_avg:60.61ms
step:2093/2110 train_time:126891ms step_avg:60.63ms
step:2094/2110 train_time:126980ms step_avg:60.64ms
step:2095/2110 train_time:127070ms step_avg:60.65ms
step:2096/2110 train_time:127157ms step_avg:60.67ms
step:2097/2110 train_time:127247ms step_avg:60.68ms
step:2098/2110 train_time:127334ms step_avg:60.69ms
step:2099/2110 train_time:127423ms step_avg:60.71ms
step:2100/2110 train_time:127510ms step_avg:60.72ms
step:2101/2110 train_time:127599ms step_avg:60.73ms
step:2102/2110 train_time:127687ms step_avg:60.75ms
step:2103/2110 train_time:127775ms step_avg:60.76ms
step:2104/2110 train_time:127864ms step_avg:60.77ms
step:2105/2110 train_time:127954ms step_avg:60.79ms
step:2106/2110 train_time:128041ms step_avg:60.80ms
step:2107/2110 train_time:128130ms step_avg:60.81ms
step:2108/2110 train_time:128217ms step_avg:60.82ms
step:2109/2110 train_time:128307ms step_avg:60.84ms
step:2110/2110 train_time:128395ms step_avg:60.85ms
step:2110/2110 val_loss:3.2789 train_time:128485ms step_avg:60.89ms
peak memory allocated: 29892 MiB reserved: 44836 MiB

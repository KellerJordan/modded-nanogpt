import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 15 20:55:06 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   39C    P0            126W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   43C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   43C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   37C    P0            126W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   43C    P0            127W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              88      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              88      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2110 train_time:96ms step_avg:95.98ms
step:2/2110 train_time:130ms step_avg:64.93ms
step:3/2110 train_time:160ms step_avg:53.24ms
step:4/2110 train_time:186ms step_avg:46.53ms
step:5/2110 train_time:211ms step_avg:42.15ms
step:6/2110 train_time:396ms step_avg:65.96ms
step:7/2110 train_time:667ms step_avg:95.29ms
step:8/2110 train_time:700ms step_avg:87.53ms
step:9/2110 train_time:732ms step_avg:81.36ms
step:10/2110 train_time:765ms step_avg:76.55ms
step:11/2110 train_time:798ms step_avg:72.54ms
step:12/2110 train_time:831ms step_avg:69.24ms
step:13/2110 train_time:864ms step_avg:66.46ms
step:14/2110 train_time:897ms step_avg:64.06ms
step:15/2110 train_time:930ms step_avg:62.01ms
step:16/2110 train_time:964ms step_avg:60.24ms
step:17/2110 train_time:996ms step_avg:58.59ms
step:18/2110 train_time:1029ms step_avg:57.19ms
step:19/2110 train_time:1062ms step_avg:55.91ms
step:20/2110 train_time:1097ms step_avg:54.84ms
step:21/2110 train_time:1128ms step_avg:53.73ms
step:22/2110 train_time:1162ms step_avg:52.81ms
step:23/2110 train_time:1195ms step_avg:51.95ms
step:24/2110 train_time:1228ms step_avg:51.16ms
step:25/2110 train_time:1262ms step_avg:50.48ms
step:26/2110 train_time:1294ms step_avg:49.78ms
step:27/2110 train_time:1327ms step_avg:49.15ms
step:28/2110 train_time:1361ms step_avg:48.59ms
step:29/2110 train_time:1393ms step_avg:48.05ms
step:30/2110 train_time:1426ms step_avg:47.54ms
step:31/2110 train_time:1460ms step_avg:47.10ms
step:32/2110 train_time:1493ms step_avg:46.64ms
step:33/2110 train_time:1525ms step_avg:46.22ms
step:34/2110 train_time:1561ms step_avg:45.90ms
step:35/2110 train_time:1592ms step_avg:45.50ms
step:36/2110 train_time:1628ms step_avg:45.21ms
step:37/2110 train_time:1661ms step_avg:44.90ms
step:38/2110 train_time:1695ms step_avg:44.62ms
step:39/2110 train_time:1729ms step_avg:44.32ms
step:40/2110 train_time:1763ms step_avg:44.07ms
step:41/2110 train_time:1796ms step_avg:43.80ms
step:42/2110 train_time:1829ms step_avg:43.54ms
step:43/2110 train_time:1862ms step_avg:43.31ms
step:44/2110 train_time:1896ms step_avg:43.08ms
step:45/2110 train_time:1929ms step_avg:42.86ms
step:46/2110 train_time:1962ms step_avg:42.65ms
step:47/2110 train_time:1995ms step_avg:42.45ms
step:48/2110 train_time:2028ms step_avg:42.26ms
step:49/2110 train_time:2061ms step_avg:42.07ms
step:50/2110 train_time:2094ms step_avg:41.88ms
step:51/2110 train_time:2127ms step_avg:41.71ms
step:52/2110 train_time:2161ms step_avg:41.56ms
step:53/2110 train_time:2193ms step_avg:41.38ms
step:54/2110 train_time:2226ms step_avg:41.22ms
step:55/2110 train_time:2259ms step_avg:41.08ms
step:56/2110 train_time:2292ms step_avg:40.94ms
step:57/2110 train_time:2325ms step_avg:40.79ms
step:58/2110 train_time:2358ms step_avg:40.66ms
step:59/2110 train_time:2391ms step_avg:40.53ms
step:60/2110 train_time:2425ms step_avg:40.42ms
step:61/2110 train_time:2457ms step_avg:40.28ms
step:62/2110 train_time:2490ms step_avg:40.17ms
step:63/2110 train_time:2523ms step_avg:40.05ms
step:64/2110 train_time:2557ms step_avg:39.95ms
step:65/2110 train_time:2590ms step_avg:39.85ms
step:66/2110 train_time:2623ms step_avg:39.75ms
step:67/2110 train_time:2657ms step_avg:39.65ms
step:68/2110 train_time:2690ms step_avg:39.56ms
step:69/2110 train_time:2723ms step_avg:39.47ms
step:70/2110 train_time:2758ms step_avg:39.39ms
step:71/2110 train_time:2790ms step_avg:39.29ms
step:72/2110 train_time:2824ms step_avg:39.22ms
step:73/2110 train_time:2857ms step_avg:39.13ms
step:74/2110 train_time:2890ms step_avg:39.05ms
step:75/2110 train_time:2923ms step_avg:38.97ms
step:76/2110 train_time:2956ms step_avg:38.90ms
step:77/2110 train_time:2989ms step_avg:38.82ms
step:78/2110 train_time:3022ms step_avg:38.75ms
step:79/2110 train_time:3056ms step_avg:38.68ms
step:80/2110 train_time:3089ms step_avg:38.61ms
step:81/2110 train_time:3122ms step_avg:38.54ms
step:82/2110 train_time:3156ms step_avg:38.48ms
step:83/2110 train_time:3188ms step_avg:38.41ms
step:84/2110 train_time:3222ms step_avg:38.35ms
step:85/2110 train_time:3254ms step_avg:38.28ms
step:86/2110 train_time:3288ms step_avg:38.23ms
step:87/2110 train_time:3320ms step_avg:38.16ms
step:88/2110 train_time:3353ms step_avg:38.10ms
step:89/2110 train_time:3386ms step_avg:38.05ms
step:90/2110 train_time:3419ms step_avg:37.99ms
step:91/2110 train_time:3452ms step_avg:37.93ms
step:92/2110 train_time:3486ms step_avg:37.89ms
step:93/2110 train_time:3518ms step_avg:37.83ms
step:94/2110 train_time:3552ms step_avg:37.79ms
step:95/2110 train_time:3584ms step_avg:37.73ms
step:96/2110 train_time:3619ms step_avg:37.69ms
step:97/2110 train_time:3651ms step_avg:37.64ms
step:98/2110 train_time:3685ms step_avg:37.60ms
step:99/2110 train_time:3718ms step_avg:37.55ms
step:100/2110 train_time:3751ms step_avg:37.51ms
step:101/2110 train_time:3784ms step_avg:37.46ms
step:102/2110 train_time:3818ms step_avg:37.43ms
step:103/2110 train_time:3850ms step_avg:37.38ms
step:104/2110 train_time:3884ms step_avg:37.35ms
step:105/2110 train_time:3917ms step_avg:37.30ms
step:106/2110 train_time:3951ms step_avg:37.27ms
step:107/2110 train_time:3983ms step_avg:37.22ms
step:108/2110 train_time:4017ms step_avg:37.19ms
step:109/2110 train_time:4050ms step_avg:37.16ms
step:110/2110 train_time:4083ms step_avg:37.11ms
step:111/2110 train_time:4115ms step_avg:37.07ms
step:112/2110 train_time:4148ms step_avg:37.04ms
step:113/2110 train_time:4182ms step_avg:37.01ms
step:114/2110 train_time:4216ms step_avg:36.98ms
step:115/2110 train_time:4248ms step_avg:36.94ms
step:116/2110 train_time:4280ms step_avg:36.90ms
step:117/2110 train_time:4313ms step_avg:36.87ms
step:118/2110 train_time:4346ms step_avg:36.83ms
step:119/2110 train_time:4380ms step_avg:36.80ms
step:120/2110 train_time:4412ms step_avg:36.77ms
step:121/2110 train_time:4445ms step_avg:36.74ms
step:122/2110 train_time:4478ms step_avg:36.71ms
step:123/2110 train_time:4511ms step_avg:36.68ms
step:124/2110 train_time:4544ms step_avg:36.65ms
step:125/2110 train_time:4578ms step_avg:36.62ms
step:126/2110 train_time:4610ms step_avg:36.59ms
step:127/2110 train_time:4643ms step_avg:36.56ms
step:128/2110 train_time:4676ms step_avg:36.53ms
step:129/2110 train_time:4709ms step_avg:36.51ms
step:130/2110 train_time:4742ms step_avg:36.48ms
step:131/2110 train_time:4776ms step_avg:36.46ms
step:132/2110 train_time:4809ms step_avg:36.43ms
step:133/2110 train_time:4842ms step_avg:36.41ms
step:134/2110 train_time:4875ms step_avg:36.38ms
step:135/2110 train_time:4908ms step_avg:36.36ms
step:136/2110 train_time:4942ms step_avg:36.34ms
step:137/2110 train_time:4974ms step_avg:36.31ms
step:138/2110 train_time:5007ms step_avg:36.28ms
step:139/2110 train_time:5041ms step_avg:36.27ms
step:140/2110 train_time:5075ms step_avg:36.25ms
step:141/2110 train_time:5107ms step_avg:36.22ms
step:142/2110 train_time:5140ms step_avg:36.20ms
step:143/2110 train_time:5173ms step_avg:36.17ms
step:144/2110 train_time:5206ms step_avg:36.15ms
step:145/2110 train_time:5239ms step_avg:36.13ms
step:146/2110 train_time:5272ms step_avg:36.11ms
step:147/2110 train_time:5305ms step_avg:36.09ms
step:148/2110 train_time:5340ms step_avg:36.08ms
step:149/2110 train_time:5372ms step_avg:36.05ms
step:150/2110 train_time:5404ms step_avg:36.03ms
step:151/2110 train_time:5438ms step_avg:36.01ms
step:152/2110 train_time:5472ms step_avg:36.00ms
step:153/2110 train_time:5503ms step_avg:35.97ms
step:154/2110 train_time:5538ms step_avg:35.96ms
step:155/2110 train_time:5570ms step_avg:35.93ms
step:156/2110 train_time:5603ms step_avg:35.92ms
step:157/2110 train_time:5636ms step_avg:35.90ms
step:158/2110 train_time:5669ms step_avg:35.88ms
step:159/2110 train_time:5702ms step_avg:35.86ms
step:160/2110 train_time:5736ms step_avg:35.85ms
step:161/2110 train_time:5768ms step_avg:35.83ms
step:162/2110 train_time:5802ms step_avg:35.81ms
step:163/2110 train_time:5834ms step_avg:35.79ms
step:164/2110 train_time:5867ms step_avg:35.78ms
step:165/2110 train_time:5901ms step_avg:35.76ms
step:166/2110 train_time:5933ms step_avg:35.74ms
step:167/2110 train_time:5967ms step_avg:35.73ms
step:168/2110 train_time:6000ms step_avg:35.71ms
step:169/2110 train_time:6032ms step_avg:35.70ms
step:170/2110 train_time:6065ms step_avg:35.68ms
step:171/2110 train_time:6098ms step_avg:35.66ms
step:172/2110 train_time:6131ms step_avg:35.65ms
step:173/2110 train_time:6164ms step_avg:35.63ms
step:174/2110 train_time:6197ms step_avg:35.61ms
step:175/2110 train_time:6230ms step_avg:35.60ms
step:176/2110 train_time:6262ms step_avg:35.58ms
step:177/2110 train_time:6296ms step_avg:35.57ms
step:178/2110 train_time:6330ms step_avg:35.56ms
step:179/2110 train_time:6362ms step_avg:35.54ms
step:180/2110 train_time:6395ms step_avg:35.53ms
step:181/2110 train_time:6428ms step_avg:35.51ms
step:182/2110 train_time:6461ms step_avg:35.50ms
step:183/2110 train_time:6494ms step_avg:35.49ms
step:184/2110 train_time:6527ms step_avg:35.47ms
step:185/2110 train_time:6560ms step_avg:35.46ms
step:186/2110 train_time:6593ms step_avg:35.45ms
step:187/2110 train_time:6626ms step_avg:35.43ms
step:188/2110 train_time:6658ms step_avg:35.42ms
step:189/2110 train_time:6692ms step_avg:35.41ms
step:190/2110 train_time:6725ms step_avg:35.39ms
step:191/2110 train_time:6758ms step_avg:35.38ms
step:192/2110 train_time:6791ms step_avg:35.37ms
step:193/2110 train_time:6824ms step_avg:35.36ms
step:194/2110 train_time:6858ms step_avg:35.35ms
step:195/2110 train_time:6890ms step_avg:35.33ms
step:196/2110 train_time:6922ms step_avg:35.32ms
step:197/2110 train_time:6956ms step_avg:35.31ms
step:198/2110 train_time:6989ms step_avg:35.30ms
step:199/2110 train_time:7022ms step_avg:35.29ms
step:200/2110 train_time:7055ms step_avg:35.27ms
step:201/2110 train_time:7088ms step_avg:35.26ms
step:202/2110 train_time:7121ms step_avg:35.25ms
step:203/2110 train_time:7155ms step_avg:35.24ms
step:204/2110 train_time:7187ms step_avg:35.23ms
step:205/2110 train_time:7220ms step_avg:35.22ms
step:206/2110 train_time:7253ms step_avg:35.21ms
step:207/2110 train_time:7286ms step_avg:35.20ms
step:208/2110 train_time:7319ms step_avg:35.19ms
step:209/2110 train_time:7352ms step_avg:35.17ms
step:210/2110 train_time:7385ms step_avg:35.16ms
step:211/2110 train_time:7418ms step_avg:35.16ms
step:212/2110 train_time:7451ms step_avg:35.14ms
step:213/2110 train_time:7484ms step_avg:35.13ms
step:214/2110 train_time:7518ms step_avg:35.13ms
step:215/2110 train_time:7549ms step_avg:35.11ms
step:216/2110 train_time:7583ms step_avg:35.11ms
step:217/2110 train_time:7616ms step_avg:35.10ms
step:218/2110 train_time:7649ms step_avg:35.09ms
step:219/2110 train_time:7682ms step_avg:35.08ms
step:220/2110 train_time:7715ms step_avg:35.07ms
step:221/2110 train_time:7748ms step_avg:35.06ms
step:222/2110 train_time:7782ms step_avg:35.05ms
step:223/2110 train_time:7815ms step_avg:35.04ms
step:224/2110 train_time:7848ms step_avg:35.04ms
step:225/2110 train_time:7881ms step_avg:35.03ms
step:226/2110 train_time:7914ms step_avg:35.02ms
step:227/2110 train_time:7946ms step_avg:35.01ms
step:228/2110 train_time:7980ms step_avg:35.00ms
step:229/2110 train_time:8012ms step_avg:34.99ms
step:230/2110 train_time:8047ms step_avg:34.99ms
step:231/2110 train_time:8079ms step_avg:34.98ms
step:232/2110 train_time:8111ms step_avg:34.96ms
step:233/2110 train_time:8145ms step_avg:34.96ms
step:234/2110 train_time:8177ms step_avg:34.95ms
step:235/2110 train_time:8211ms step_avg:34.94ms
step:236/2110 train_time:8244ms step_avg:34.93ms
step:237/2110 train_time:8277ms step_avg:34.92ms
step:238/2110 train_time:8310ms step_avg:34.92ms
step:239/2110 train_time:8342ms step_avg:34.91ms
step:240/2110 train_time:8376ms step_avg:34.90ms
step:241/2110 train_time:8408ms step_avg:34.89ms
step:242/2110 train_time:8442ms step_avg:34.89ms
step:243/2110 train_time:8475ms step_avg:34.88ms
step:244/2110 train_time:8509ms step_avg:34.87ms
step:245/2110 train_time:8542ms step_avg:34.86ms
step:246/2110 train_time:8575ms step_avg:34.86ms
step:247/2110 train_time:8607ms step_avg:34.85ms
step:248/2110 train_time:8642ms step_avg:34.85ms
step:249/2110 train_time:8673ms step_avg:34.83ms
step:250/2110 train_time:8706ms step_avg:34.82ms
step:250/2110 val_loss:4.3022 train_time:8741ms step_avg:34.97ms
step:251/2110 train_time:8767ms step_avg:34.93ms
step:252/2110 train_time:8793ms step_avg:34.89ms
step:253/2110 train_time:8819ms step_avg:34.86ms
step:254/2110 train_time:8848ms step_avg:34.84ms
step:255/2110 train_time:8880ms step_avg:34.82ms
step:256/2110 train_time:8915ms step_avg:34.82ms
step:257/2110 train_time:8949ms step_avg:34.82ms
step:258/2110 train_time:8983ms step_avg:34.82ms
step:259/2110 train_time:9015ms step_avg:34.81ms
step:260/2110 train_time:9048ms step_avg:34.80ms
step:261/2110 train_time:9081ms step_avg:34.79ms
step:262/2110 train_time:9115ms step_avg:34.79ms
step:263/2110 train_time:9147ms step_avg:34.78ms
step:264/2110 train_time:9181ms step_avg:34.78ms
step:265/2110 train_time:9213ms step_avg:34.77ms
step:266/2110 train_time:9246ms step_avg:34.76ms
step:267/2110 train_time:9279ms step_avg:34.75ms
step:268/2110 train_time:9311ms step_avg:34.74ms
step:269/2110 train_time:9344ms step_avg:34.74ms
step:270/2110 train_time:9377ms step_avg:34.73ms
step:271/2110 train_time:9410ms step_avg:34.72ms
step:272/2110 train_time:9443ms step_avg:34.72ms
step:273/2110 train_time:9475ms step_avg:34.71ms
step:274/2110 train_time:9510ms step_avg:34.71ms
step:275/2110 train_time:9541ms step_avg:34.69ms
step:276/2110 train_time:9574ms step_avg:34.69ms
step:277/2110 train_time:9607ms step_avg:34.68ms
step:278/2110 train_time:9640ms step_avg:34.68ms
step:279/2110 train_time:9672ms step_avg:34.67ms
step:280/2110 train_time:9706ms step_avg:34.67ms
step:281/2110 train_time:9738ms step_avg:34.65ms
step:282/2110 train_time:9772ms step_avg:34.65ms
step:283/2110 train_time:9804ms step_avg:34.64ms
step:284/2110 train_time:9839ms step_avg:34.64ms
step:285/2110 train_time:9872ms step_avg:34.64ms
step:286/2110 train_time:9906ms step_avg:34.64ms
step:287/2110 train_time:9939ms step_avg:34.63ms
step:288/2110 train_time:9973ms step_avg:34.63ms
step:289/2110 train_time:10005ms step_avg:34.62ms
step:290/2110 train_time:10039ms step_avg:34.62ms
step:291/2110 train_time:10072ms step_avg:34.61ms
step:292/2110 train_time:10105ms step_avg:34.61ms
step:293/2110 train_time:10138ms step_avg:34.60ms
step:294/2110 train_time:10172ms step_avg:34.60ms
step:295/2110 train_time:10205ms step_avg:34.59ms
step:296/2110 train_time:10238ms step_avg:34.59ms
step:297/2110 train_time:10270ms step_avg:34.58ms
step:298/2110 train_time:10303ms step_avg:34.57ms
step:299/2110 train_time:10335ms step_avg:34.56ms
step:300/2110 train_time:10368ms step_avg:34.56ms
step:301/2110 train_time:10401ms step_avg:34.56ms
step:302/2110 train_time:10434ms step_avg:34.55ms
step:303/2110 train_time:10466ms step_avg:34.54ms
step:304/2110 train_time:10500ms step_avg:34.54ms
step:305/2110 train_time:10533ms step_avg:34.53ms
step:306/2110 train_time:10567ms step_avg:34.53ms
step:307/2110 train_time:10598ms step_avg:34.52ms
step:308/2110 train_time:10632ms step_avg:34.52ms
step:309/2110 train_time:10664ms step_avg:34.51ms
step:310/2110 train_time:10698ms step_avg:34.51ms
step:311/2110 train_time:10730ms step_avg:34.50ms
step:312/2110 train_time:10764ms step_avg:34.50ms
step:313/2110 train_time:10797ms step_avg:34.49ms
step:314/2110 train_time:10829ms step_avg:34.49ms
step:315/2110 train_time:10862ms step_avg:34.48ms
step:316/2110 train_time:10895ms step_avg:34.48ms
step:317/2110 train_time:10929ms step_avg:34.48ms
step:318/2110 train_time:10963ms step_avg:34.47ms
step:319/2110 train_time:10995ms step_avg:34.47ms
step:320/2110 train_time:11030ms step_avg:34.47ms
step:321/2110 train_time:11061ms step_avg:34.46ms
step:322/2110 train_time:11096ms step_avg:34.46ms
step:323/2110 train_time:11127ms step_avg:34.45ms
step:324/2110 train_time:11161ms step_avg:34.45ms
step:325/2110 train_time:11193ms step_avg:34.44ms
step:326/2110 train_time:11226ms step_avg:34.44ms
step:327/2110 train_time:11259ms step_avg:34.43ms
step:328/2110 train_time:11293ms step_avg:34.43ms
step:329/2110 train_time:11325ms step_avg:34.42ms
step:330/2110 train_time:11358ms step_avg:34.42ms
step:331/2110 train_time:11391ms step_avg:34.41ms
step:332/2110 train_time:11425ms step_avg:34.41ms
step:333/2110 train_time:11457ms step_avg:34.41ms
step:334/2110 train_time:11491ms step_avg:34.41ms
step:335/2110 train_time:11523ms step_avg:34.40ms
step:336/2110 train_time:11557ms step_avg:34.39ms
step:337/2110 train_time:11589ms step_avg:34.39ms
step:338/2110 train_time:11622ms step_avg:34.39ms
step:339/2110 train_time:11655ms step_avg:34.38ms
step:340/2110 train_time:11688ms step_avg:34.38ms
step:341/2110 train_time:11721ms step_avg:34.37ms
step:342/2110 train_time:11754ms step_avg:34.37ms
step:343/2110 train_time:11787ms step_avg:34.36ms
step:344/2110 train_time:11821ms step_avg:34.36ms
step:345/2110 train_time:11853ms step_avg:34.36ms
step:346/2110 train_time:11886ms step_avg:34.35ms
step:347/2110 train_time:11918ms step_avg:34.35ms
step:348/2110 train_time:11952ms step_avg:34.34ms
step:349/2110 train_time:11985ms step_avg:34.34ms
step:350/2110 train_time:12019ms step_avg:34.34ms
step:351/2110 train_time:12051ms step_avg:34.33ms
step:352/2110 train_time:12086ms step_avg:34.34ms
step:353/2110 train_time:12119ms step_avg:34.33ms
step:354/2110 train_time:12154ms step_avg:34.33ms
step:355/2110 train_time:12184ms step_avg:34.32ms
step:356/2110 train_time:12218ms step_avg:34.32ms
step:357/2110 train_time:12250ms step_avg:34.31ms
step:358/2110 train_time:12284ms step_avg:34.31ms
step:359/2110 train_time:12316ms step_avg:34.31ms
step:360/2110 train_time:12349ms step_avg:34.30ms
step:361/2110 train_time:12382ms step_avg:34.30ms
step:362/2110 train_time:12416ms step_avg:34.30ms
step:363/2110 train_time:12449ms step_avg:34.29ms
step:364/2110 train_time:12482ms step_avg:34.29ms
step:365/2110 train_time:12514ms step_avg:34.29ms
step:366/2110 train_time:12547ms step_avg:34.28ms
step:367/2110 train_time:12580ms step_avg:34.28ms
step:368/2110 train_time:12614ms step_avg:34.28ms
step:369/2110 train_time:12646ms step_avg:34.27ms
step:370/2110 train_time:12679ms step_avg:34.27ms
step:371/2110 train_time:12712ms step_avg:34.26ms
step:372/2110 train_time:12745ms step_avg:34.26ms
step:373/2110 train_time:12778ms step_avg:34.26ms
step:374/2110 train_time:12812ms step_avg:34.26ms
step:375/2110 train_time:12845ms step_avg:34.25ms
step:376/2110 train_time:12878ms step_avg:34.25ms
step:377/2110 train_time:12911ms step_avg:34.25ms
step:378/2110 train_time:12945ms step_avg:34.25ms
step:379/2110 train_time:12976ms step_avg:34.24ms
step:380/2110 train_time:13011ms step_avg:34.24ms
step:381/2110 train_time:13043ms step_avg:34.23ms
step:382/2110 train_time:13077ms step_avg:34.23ms
step:383/2110 train_time:13110ms step_avg:34.23ms
step:384/2110 train_time:13143ms step_avg:34.23ms
step:385/2110 train_time:13174ms step_avg:34.22ms
step:386/2110 train_time:13209ms step_avg:34.22ms
step:387/2110 train_time:13241ms step_avg:34.21ms
step:388/2110 train_time:13274ms step_avg:34.21ms
step:389/2110 train_time:13307ms step_avg:34.21ms
step:390/2110 train_time:13340ms step_avg:34.21ms
step:391/2110 train_time:13372ms step_avg:34.20ms
step:392/2110 train_time:13406ms step_avg:34.20ms
step:393/2110 train_time:13439ms step_avg:34.20ms
step:394/2110 train_time:13472ms step_avg:34.19ms
step:395/2110 train_time:13505ms step_avg:34.19ms
step:396/2110 train_time:13539ms step_avg:34.19ms
step:397/2110 train_time:13570ms step_avg:34.18ms
step:398/2110 train_time:13604ms step_avg:34.18ms
step:399/2110 train_time:13637ms step_avg:34.18ms
step:400/2110 train_time:13670ms step_avg:34.17ms
step:401/2110 train_time:13702ms step_avg:34.17ms
step:402/2110 train_time:13735ms step_avg:34.17ms
step:403/2110 train_time:13768ms step_avg:34.16ms
step:404/2110 train_time:13801ms step_avg:34.16ms
step:405/2110 train_time:13834ms step_avg:34.16ms
step:406/2110 train_time:13867ms step_avg:34.16ms
step:407/2110 train_time:13899ms step_avg:34.15ms
step:408/2110 train_time:13934ms step_avg:34.15ms
step:409/2110 train_time:13966ms step_avg:34.15ms
step:410/2110 train_time:14000ms step_avg:34.15ms
step:411/2110 train_time:14032ms step_avg:34.14ms
step:412/2110 train_time:14065ms step_avg:34.14ms
step:413/2110 train_time:14097ms step_avg:34.13ms
step:414/2110 train_time:14131ms step_avg:34.13ms
step:415/2110 train_time:14164ms step_avg:34.13ms
step:416/2110 train_time:14198ms step_avg:34.13ms
step:417/2110 train_time:14231ms step_avg:34.13ms
step:418/2110 train_time:14265ms step_avg:34.13ms
step:419/2110 train_time:14297ms step_avg:34.12ms
step:420/2110 train_time:14331ms step_avg:34.12ms
step:421/2110 train_time:14363ms step_avg:34.12ms
step:422/2110 train_time:14396ms step_avg:34.11ms
step:423/2110 train_time:14428ms step_avg:34.11ms
step:424/2110 train_time:14463ms step_avg:34.11ms
step:425/2110 train_time:14494ms step_avg:34.10ms
step:426/2110 train_time:14528ms step_avg:34.10ms
step:427/2110 train_time:14559ms step_avg:34.10ms
step:428/2110 train_time:14594ms step_avg:34.10ms
step:429/2110 train_time:14626ms step_avg:34.09ms
step:430/2110 train_time:14660ms step_avg:34.09ms
step:431/2110 train_time:14692ms step_avg:34.09ms
step:432/2110 train_time:14726ms step_avg:34.09ms
step:433/2110 train_time:14758ms step_avg:34.08ms
step:434/2110 train_time:14790ms step_avg:34.08ms
step:435/2110 train_time:14823ms step_avg:34.08ms
step:436/2110 train_time:14857ms step_avg:34.08ms
step:437/2110 train_time:14889ms step_avg:34.07ms
step:438/2110 train_time:14923ms step_avg:34.07ms
step:439/2110 train_time:14957ms step_avg:34.07ms
step:440/2110 train_time:14990ms step_avg:34.07ms
step:441/2110 train_time:15021ms step_avg:34.06ms
step:442/2110 train_time:15056ms step_avg:34.06ms
step:443/2110 train_time:15088ms step_avg:34.06ms
step:444/2110 train_time:15122ms step_avg:34.06ms
step:445/2110 train_time:15154ms step_avg:34.05ms
step:446/2110 train_time:15188ms step_avg:34.05ms
step:447/2110 train_time:15220ms step_avg:34.05ms
step:448/2110 train_time:15254ms step_avg:34.05ms
step:449/2110 train_time:15286ms step_avg:34.05ms
step:450/2110 train_time:15320ms step_avg:34.04ms
step:451/2110 train_time:15353ms step_avg:34.04ms
step:452/2110 train_time:15386ms step_avg:34.04ms
step:453/2110 train_time:15419ms step_avg:34.04ms
step:454/2110 train_time:15452ms step_avg:34.04ms
step:455/2110 train_time:15485ms step_avg:34.03ms
step:456/2110 train_time:15519ms step_avg:34.03ms
step:457/2110 train_time:15550ms step_avg:34.03ms
step:458/2110 train_time:15583ms step_avg:34.02ms
step:459/2110 train_time:15617ms step_avg:34.02ms
step:460/2110 train_time:15650ms step_avg:34.02ms
step:461/2110 train_time:15682ms step_avg:34.02ms
step:462/2110 train_time:15716ms step_avg:34.02ms
step:463/2110 train_time:15747ms step_avg:34.01ms
step:464/2110 train_time:15782ms step_avg:34.01ms
step:465/2110 train_time:15813ms step_avg:34.01ms
step:466/2110 train_time:15847ms step_avg:34.01ms
step:467/2110 train_time:15879ms step_avg:34.00ms
step:468/2110 train_time:15912ms step_avg:34.00ms
step:469/2110 train_time:15945ms step_avg:34.00ms
step:470/2110 train_time:15980ms step_avg:34.00ms
step:471/2110 train_time:16011ms step_avg:33.99ms
step:472/2110 train_time:16046ms step_avg:34.00ms
step:473/2110 train_time:16077ms step_avg:33.99ms
step:474/2110 train_time:16112ms step_avg:33.99ms
step:475/2110 train_time:16143ms step_avg:33.99ms
step:476/2110 train_time:16179ms step_avg:33.99ms
step:477/2110 train_time:16209ms step_avg:33.98ms
step:478/2110 train_time:16243ms step_avg:33.98ms
step:479/2110 train_time:16275ms step_avg:33.98ms
step:480/2110 train_time:16309ms step_avg:33.98ms
step:481/2110 train_time:16342ms step_avg:33.97ms
step:482/2110 train_time:16375ms step_avg:33.97ms
step:483/2110 train_time:16407ms step_avg:33.97ms
step:484/2110 train_time:16441ms step_avg:33.97ms
step:485/2110 train_time:16473ms step_avg:33.97ms
step:486/2110 train_time:16507ms step_avg:33.97ms
step:487/2110 train_time:16539ms step_avg:33.96ms
step:488/2110 train_time:16573ms step_avg:33.96ms
step:489/2110 train_time:16605ms step_avg:33.96ms
step:490/2110 train_time:16639ms step_avg:33.96ms
step:491/2110 train_time:16671ms step_avg:33.95ms
step:492/2110 train_time:16706ms step_avg:33.96ms
step:493/2110 train_time:16737ms step_avg:33.95ms
step:494/2110 train_time:16771ms step_avg:33.95ms
step:495/2110 train_time:16803ms step_avg:33.94ms
step:496/2110 train_time:16837ms step_avg:33.95ms
step:497/2110 train_time:16869ms step_avg:33.94ms
step:498/2110 train_time:16903ms step_avg:33.94ms
step:499/2110 train_time:16935ms step_avg:33.94ms
step:500/2110 train_time:16970ms step_avg:33.94ms
step:500/2110 val_loss:4.0323 train_time:17003ms step_avg:34.01ms
step:501/2110 train_time:17028ms step_avg:33.99ms
step:502/2110 train_time:17051ms step_avg:33.97ms
step:503/2110 train_time:17072ms step_avg:33.94ms
step:504/2110 train_time:17108ms step_avg:33.95ms
step:505/2110 train_time:17141ms step_avg:33.94ms
step:506/2110 train_time:17176ms step_avg:33.94ms
step:507/2110 train_time:17210ms step_avg:33.95ms
step:508/2110 train_time:17244ms step_avg:33.95ms
step:509/2110 train_time:17276ms step_avg:33.94ms
step:510/2110 train_time:17310ms step_avg:33.94ms
step:511/2110 train_time:17343ms step_avg:33.94ms
step:512/2110 train_time:17377ms step_avg:33.94ms
step:513/2110 train_time:17409ms step_avg:33.93ms
step:514/2110 train_time:17442ms step_avg:33.93ms
step:515/2110 train_time:17475ms step_avg:33.93ms
step:516/2110 train_time:17508ms step_avg:33.93ms
step:517/2110 train_time:17540ms step_avg:33.93ms
step:518/2110 train_time:17574ms step_avg:33.93ms
step:519/2110 train_time:17606ms step_avg:33.92ms
step:520/2110 train_time:17640ms step_avg:33.92ms
step:521/2110 train_time:17672ms step_avg:33.92ms
step:522/2110 train_time:17705ms step_avg:33.92ms
step:523/2110 train_time:17738ms step_avg:33.92ms
step:524/2110 train_time:17771ms step_avg:33.91ms
step:525/2110 train_time:17803ms step_avg:33.91ms
step:526/2110 train_time:17837ms step_avg:33.91ms
step:527/2110 train_time:17868ms step_avg:33.91ms
step:528/2110 train_time:17901ms step_avg:33.90ms
step:529/2110 train_time:17934ms step_avg:33.90ms
step:530/2110 train_time:17967ms step_avg:33.90ms
step:531/2110 train_time:18000ms step_avg:33.90ms
step:532/2110 train_time:18035ms step_avg:33.90ms
step:533/2110 train_time:18067ms step_avg:33.90ms
step:534/2110 train_time:18100ms step_avg:33.89ms
step:535/2110 train_time:18133ms step_avg:33.89ms
step:536/2110 train_time:18167ms step_avg:33.89ms
step:537/2110 train_time:18200ms step_avg:33.89ms
step:538/2110 train_time:18235ms step_avg:33.89ms
step:539/2110 train_time:18268ms step_avg:33.89ms
step:540/2110 train_time:18302ms step_avg:33.89ms
step:541/2110 train_time:18334ms step_avg:33.89ms
step:542/2110 train_time:18368ms step_avg:33.89ms
step:543/2110 train_time:18400ms step_avg:33.89ms
step:544/2110 train_time:18435ms step_avg:33.89ms
step:545/2110 train_time:18466ms step_avg:33.88ms
step:546/2110 train_time:18499ms step_avg:33.88ms
step:547/2110 train_time:18532ms step_avg:33.88ms
step:548/2110 train_time:18565ms step_avg:33.88ms
step:549/2110 train_time:18598ms step_avg:33.88ms
step:550/2110 train_time:18632ms step_avg:33.88ms
step:551/2110 train_time:18664ms step_avg:33.87ms
step:552/2110 train_time:18697ms step_avg:33.87ms
step:553/2110 train_time:18730ms step_avg:33.87ms
step:554/2110 train_time:18763ms step_avg:33.87ms
step:555/2110 train_time:18795ms step_avg:33.87ms
step:556/2110 train_time:18829ms step_avg:33.86ms
step:557/2110 train_time:18861ms step_avg:33.86ms
step:558/2110 train_time:18895ms step_avg:33.86ms
step:559/2110 train_time:18927ms step_avg:33.86ms
step:560/2110 train_time:18960ms step_avg:33.86ms
step:561/2110 train_time:18993ms step_avg:33.85ms
step:562/2110 train_time:19027ms step_avg:33.86ms
step:563/2110 train_time:19059ms step_avg:33.85ms
step:564/2110 train_time:19093ms step_avg:33.85ms
step:565/2110 train_time:19125ms step_avg:33.85ms
step:566/2110 train_time:19160ms step_avg:33.85ms
step:567/2110 train_time:19191ms step_avg:33.85ms
step:568/2110 train_time:19225ms step_avg:33.85ms
step:569/2110 train_time:19258ms step_avg:33.85ms
step:570/2110 train_time:19292ms step_avg:33.85ms
step:571/2110 train_time:19324ms step_avg:33.84ms
step:572/2110 train_time:19358ms step_avg:33.84ms
step:573/2110 train_time:19391ms step_avg:33.84ms
step:574/2110 train_time:19426ms step_avg:33.84ms
step:575/2110 train_time:19457ms step_avg:33.84ms
step:576/2110 train_time:19491ms step_avg:33.84ms
step:577/2110 train_time:19524ms step_avg:33.84ms
step:578/2110 train_time:19559ms step_avg:33.84ms
step:579/2110 train_time:19590ms step_avg:33.83ms
step:580/2110 train_time:19624ms step_avg:33.83ms
step:581/2110 train_time:19656ms step_avg:33.83ms
step:582/2110 train_time:19690ms step_avg:33.83ms
step:583/2110 train_time:19723ms step_avg:33.83ms
step:584/2110 train_time:19757ms step_avg:33.83ms
step:585/2110 train_time:19788ms step_avg:33.83ms
step:586/2110 train_time:19821ms step_avg:33.82ms
step:587/2110 train_time:19853ms step_avg:33.82ms
step:588/2110 train_time:19887ms step_avg:33.82ms
step:589/2110 train_time:19920ms step_avg:33.82ms
step:590/2110 train_time:19953ms step_avg:33.82ms
step:591/2110 train_time:19985ms step_avg:33.82ms
step:592/2110 train_time:20019ms step_avg:33.82ms
step:593/2110 train_time:20051ms step_avg:33.81ms
step:594/2110 train_time:20084ms step_avg:33.81ms
step:595/2110 train_time:20118ms step_avg:33.81ms
step:596/2110 train_time:20152ms step_avg:33.81ms
step:597/2110 train_time:20184ms step_avg:33.81ms
step:598/2110 train_time:20218ms step_avg:33.81ms
step:599/2110 train_time:20250ms step_avg:33.81ms
step:600/2110 train_time:20284ms step_avg:33.81ms
step:601/2110 train_time:20316ms step_avg:33.80ms
step:602/2110 train_time:20350ms step_avg:33.80ms
step:603/2110 train_time:20382ms step_avg:33.80ms
step:604/2110 train_time:20417ms step_avg:33.80ms
step:605/2110 train_time:20450ms step_avg:33.80ms
step:606/2110 train_time:20483ms step_avg:33.80ms
step:607/2110 train_time:20514ms step_avg:33.80ms
step:608/2110 train_time:20549ms step_avg:33.80ms
step:609/2110 train_time:20580ms step_avg:33.79ms
step:610/2110 train_time:20615ms step_avg:33.80ms
step:611/2110 train_time:20646ms step_avg:33.79ms
step:612/2110 train_time:20679ms step_avg:33.79ms
step:613/2110 train_time:20712ms step_avg:33.79ms
step:614/2110 train_time:20747ms step_avg:33.79ms
step:615/2110 train_time:20778ms step_avg:33.79ms
step:616/2110 train_time:20815ms step_avg:33.79ms
step:617/2110 train_time:20847ms step_avg:33.79ms
step:618/2110 train_time:20879ms step_avg:33.79ms
step:619/2110 train_time:20912ms step_avg:33.78ms
step:620/2110 train_time:20947ms step_avg:33.79ms
step:621/2110 train_time:20978ms step_avg:33.78ms
step:622/2110 train_time:21013ms step_avg:33.78ms
step:623/2110 train_time:21046ms step_avg:33.78ms
step:624/2110 train_time:21081ms step_avg:33.78ms
step:625/2110 train_time:21120ms step_avg:33.79ms
step:626/2110 train_time:21162ms step_avg:33.80ms
step:627/2110 train_time:21197ms step_avg:33.81ms
step:628/2110 train_time:21231ms step_avg:33.81ms
step:629/2110 train_time:21263ms step_avg:33.80ms
step:630/2110 train_time:21296ms step_avg:33.80ms
step:631/2110 train_time:21327ms step_avg:33.80ms
step:632/2110 train_time:21361ms step_avg:33.80ms
step:633/2110 train_time:21393ms step_avg:33.80ms
step:634/2110 train_time:21427ms step_avg:33.80ms
step:635/2110 train_time:21460ms step_avg:33.79ms
step:636/2110 train_time:21495ms step_avg:33.80ms
step:637/2110 train_time:21526ms step_avg:33.79ms
step:638/2110 train_time:21559ms step_avg:33.79ms
step:639/2110 train_time:21590ms step_avg:33.79ms
step:640/2110 train_time:21625ms step_avg:33.79ms
step:641/2110 train_time:21657ms step_avg:33.79ms
step:642/2110 train_time:21690ms step_avg:33.78ms
step:643/2110 train_time:21722ms step_avg:33.78ms
step:644/2110 train_time:21756ms step_avg:33.78ms
step:645/2110 train_time:21788ms step_avg:33.78ms
step:646/2110 train_time:21821ms step_avg:33.78ms
step:647/2110 train_time:21853ms step_avg:33.78ms
step:648/2110 train_time:21886ms step_avg:33.78ms
step:649/2110 train_time:21919ms step_avg:33.77ms
step:650/2110 train_time:21955ms step_avg:33.78ms
step:651/2110 train_time:21985ms step_avg:33.77ms
step:652/2110 train_time:22019ms step_avg:33.77ms
step:653/2110 train_time:22051ms step_avg:33.77ms
step:654/2110 train_time:22086ms step_avg:33.77ms
step:655/2110 train_time:22120ms step_avg:33.77ms
step:656/2110 train_time:22155ms step_avg:33.77ms
step:657/2110 train_time:22187ms step_avg:33.77ms
step:658/2110 train_time:22222ms step_avg:33.77ms
step:659/2110 train_time:22254ms step_avg:33.77ms
step:660/2110 train_time:22287ms step_avg:33.77ms
step:661/2110 train_time:22319ms step_avg:33.77ms
step:662/2110 train_time:22355ms step_avg:33.77ms
step:663/2110 train_time:22384ms step_avg:33.76ms
step:664/2110 train_time:22419ms step_avg:33.76ms
step:665/2110 train_time:22451ms step_avg:33.76ms
step:666/2110 train_time:22485ms step_avg:33.76ms
step:667/2110 train_time:22518ms step_avg:33.76ms
step:668/2110 train_time:22553ms step_avg:33.76ms
step:669/2110 train_time:22583ms step_avg:33.76ms
step:670/2110 train_time:22617ms step_avg:33.76ms
step:671/2110 train_time:22649ms step_avg:33.75ms
step:672/2110 train_time:22684ms step_avg:33.76ms
step:673/2110 train_time:22716ms step_avg:33.75ms
step:674/2110 train_time:22749ms step_avg:33.75ms
step:675/2110 train_time:22782ms step_avg:33.75ms
step:676/2110 train_time:22815ms step_avg:33.75ms
step:677/2110 train_time:22847ms step_avg:33.75ms
step:678/2110 train_time:22881ms step_avg:33.75ms
step:679/2110 train_time:22913ms step_avg:33.75ms
step:680/2110 train_time:22946ms step_avg:33.74ms
step:681/2110 train_time:22978ms step_avg:33.74ms
step:682/2110 train_time:23015ms step_avg:33.75ms
step:683/2110 train_time:23046ms step_avg:33.74ms
step:684/2110 train_time:23080ms step_avg:33.74ms
step:685/2110 train_time:23112ms step_avg:33.74ms
step:686/2110 train_time:23146ms step_avg:33.74ms
step:687/2110 train_time:23179ms step_avg:33.74ms
step:688/2110 train_time:23212ms step_avg:33.74ms
step:689/2110 train_time:23245ms step_avg:33.74ms
step:690/2110 train_time:23279ms step_avg:33.74ms
step:691/2110 train_time:23314ms step_avg:33.74ms
step:692/2110 train_time:23368ms step_avg:33.77ms
step:693/2110 train_time:23427ms step_avg:33.81ms
step:694/2110 train_time:23487ms step_avg:33.84ms
step:695/2110 train_time:23546ms step_avg:33.88ms
step:696/2110 train_time:23606ms step_avg:33.92ms
step:697/2110 train_time:23664ms step_avg:33.95ms
step:698/2110 train_time:23724ms step_avg:33.99ms
step:699/2110 train_time:23782ms step_avg:34.02ms
step:700/2110 train_time:23842ms step_avg:34.06ms
step:701/2110 train_time:23901ms step_avg:34.10ms
step:702/2110 train_time:23960ms step_avg:34.13ms
step:703/2110 train_time:24018ms step_avg:34.17ms
step:704/2110 train_time:24077ms step_avg:34.20ms
step:705/2110 train_time:24136ms step_avg:34.24ms
step:706/2110 train_time:24195ms step_avg:34.27ms
step:707/2110 train_time:24254ms step_avg:34.31ms
step:708/2110 train_time:24313ms step_avg:34.34ms
step:709/2110 train_time:24373ms step_avg:34.38ms
step:710/2110 train_time:24432ms step_avg:34.41ms
step:711/2110 train_time:24492ms step_avg:34.45ms
step:712/2110 train_time:24551ms step_avg:34.48ms
step:713/2110 train_time:24610ms step_avg:34.52ms
step:714/2110 train_time:24670ms step_avg:34.55ms
step:715/2110 train_time:24729ms step_avg:34.59ms
step:716/2110 train_time:24789ms step_avg:34.62ms
step:717/2110 train_time:24849ms step_avg:34.66ms
step:718/2110 train_time:24909ms step_avg:34.69ms
step:719/2110 train_time:24969ms step_avg:34.73ms
step:720/2110 train_time:25028ms step_avg:34.76ms
step:721/2110 train_time:25088ms step_avg:34.80ms
step:722/2110 train_time:25148ms step_avg:34.83ms
step:723/2110 train_time:25208ms step_avg:34.87ms
step:724/2110 train_time:25267ms step_avg:34.90ms
step:725/2110 train_time:25327ms step_avg:34.93ms
step:726/2110 train_time:25386ms step_avg:34.97ms
step:727/2110 train_time:25445ms step_avg:35.00ms
step:728/2110 train_time:25505ms step_avg:35.03ms
step:729/2110 train_time:25563ms step_avg:35.07ms
step:730/2110 train_time:25623ms step_avg:35.10ms
step:731/2110 train_time:25683ms step_avg:35.13ms
step:732/2110 train_time:25742ms step_avg:35.17ms
step:733/2110 train_time:25801ms step_avg:35.20ms
step:734/2110 train_time:25860ms step_avg:35.23ms
step:735/2110 train_time:25920ms step_avg:35.26ms
step:736/2110 train_time:25979ms step_avg:35.30ms
step:737/2110 train_time:26038ms step_avg:35.33ms
step:738/2110 train_time:26097ms step_avg:35.36ms
step:739/2110 train_time:26156ms step_avg:35.39ms
step:740/2110 train_time:26215ms step_avg:35.43ms
step:741/2110 train_time:26274ms step_avg:35.46ms
step:742/2110 train_time:26333ms step_avg:35.49ms
step:743/2110 train_time:26392ms step_avg:35.52ms
step:744/2110 train_time:26451ms step_avg:35.55ms
step:745/2110 train_time:26510ms step_avg:35.58ms
step:746/2110 train_time:26569ms step_avg:35.62ms
step:747/2110 train_time:26628ms step_avg:35.65ms
step:748/2110 train_time:26688ms step_avg:35.68ms
step:749/2110 train_time:26748ms step_avg:35.71ms
step:750/2110 train_time:26808ms step_avg:35.74ms
step:750/2110 val_loss:3.9066 train_time:26869ms step_avg:35.83ms
step:751/2110 train_time:26905ms step_avg:35.83ms
step:752/2110 train_time:26937ms step_avg:35.82ms
step:753/2110 train_time:26993ms step_avg:35.85ms
step:754/2110 train_time:27055ms step_avg:35.88ms
step:755/2110 train_time:27115ms step_avg:35.91ms
step:756/2110 train_time:27174ms step_avg:35.94ms
step:757/2110 train_time:27233ms step_avg:35.98ms
step:758/2110 train_time:27291ms step_avg:36.00ms
step:759/2110 train_time:27350ms step_avg:36.03ms
step:760/2110 train_time:27408ms step_avg:36.06ms
step:761/2110 train_time:27466ms step_avg:36.09ms
step:762/2110 train_time:27524ms step_avg:36.12ms
step:763/2110 train_time:27581ms step_avg:36.15ms
step:764/2110 train_time:27640ms step_avg:36.18ms
step:765/2110 train_time:27698ms step_avg:36.21ms
step:766/2110 train_time:27756ms step_avg:36.24ms
step:767/2110 train_time:27816ms step_avg:36.27ms
step:768/2110 train_time:27877ms step_avg:36.30ms
step:769/2110 train_time:27939ms step_avg:36.33ms
step:770/2110 train_time:27999ms step_avg:36.36ms
step:771/2110 train_time:28060ms step_avg:36.39ms
step:772/2110 train_time:28121ms step_avg:36.43ms
step:773/2110 train_time:28180ms step_avg:36.45ms
step:774/2110 train_time:28239ms step_avg:36.48ms
step:775/2110 train_time:28298ms step_avg:36.51ms
step:776/2110 train_time:28357ms step_avg:36.54ms
step:777/2110 train_time:28416ms step_avg:36.57ms
step:778/2110 train_time:28474ms step_avg:36.60ms
step:779/2110 train_time:28533ms step_avg:36.63ms
step:780/2110 train_time:28591ms step_avg:36.65ms
step:781/2110 train_time:28649ms step_avg:36.68ms
step:782/2110 train_time:28707ms step_avg:36.71ms
step:783/2110 train_time:28766ms step_avg:36.74ms
step:784/2110 train_time:28825ms step_avg:36.77ms
step:785/2110 train_time:28884ms step_avg:36.79ms
step:786/2110 train_time:28943ms step_avg:36.82ms
step:787/2110 train_time:29004ms step_avg:36.85ms
step:788/2110 train_time:29063ms step_avg:36.88ms
step:789/2110 train_time:29124ms step_avg:36.91ms
step:790/2110 train_time:29182ms step_avg:36.94ms
step:791/2110 train_time:29242ms step_avg:36.97ms
step:792/2110 train_time:29301ms step_avg:37.00ms
step:793/2110 train_time:29361ms step_avg:37.02ms
step:794/2110 train_time:29419ms step_avg:37.05ms
step:795/2110 train_time:29478ms step_avg:37.08ms
step:796/2110 train_time:29537ms step_avg:37.11ms
step:797/2110 train_time:29596ms step_avg:37.13ms
step:798/2110 train_time:29655ms step_avg:37.16ms
step:799/2110 train_time:29713ms step_avg:37.19ms
step:800/2110 train_time:29772ms step_avg:37.22ms
step:801/2110 train_time:29831ms step_avg:37.24ms
step:802/2110 train_time:29890ms step_avg:37.27ms
step:803/2110 train_time:29951ms step_avg:37.30ms
step:804/2110 train_time:30010ms step_avg:37.33ms
step:805/2110 train_time:30069ms step_avg:37.35ms
step:806/2110 train_time:30128ms step_avg:37.38ms
step:807/2110 train_time:30187ms step_avg:37.41ms
step:808/2110 train_time:30246ms step_avg:37.43ms
step:809/2110 train_time:30305ms step_avg:37.46ms
step:810/2110 train_time:30363ms step_avg:37.49ms
step:811/2110 train_time:30422ms step_avg:37.51ms
step:812/2110 train_time:30481ms step_avg:37.54ms
step:813/2110 train_time:30540ms step_avg:37.57ms
step:814/2110 train_time:30599ms step_avg:37.59ms
step:815/2110 train_time:30659ms step_avg:37.62ms
step:816/2110 train_time:30717ms step_avg:37.64ms
step:817/2110 train_time:30778ms step_avg:37.67ms
step:818/2110 train_time:30837ms step_avg:37.70ms
step:819/2110 train_time:30897ms step_avg:37.73ms
step:820/2110 train_time:30957ms step_avg:37.75ms
step:821/2110 train_time:31017ms step_avg:37.78ms
step:822/2110 train_time:31076ms step_avg:37.80ms
step:823/2110 train_time:31136ms step_avg:37.83ms
step:824/2110 train_time:31195ms step_avg:37.86ms
step:825/2110 train_time:31255ms step_avg:37.88ms
step:826/2110 train_time:31313ms step_avg:37.91ms
step:827/2110 train_time:31372ms step_avg:37.94ms
step:828/2110 train_time:31431ms step_avg:37.96ms
step:829/2110 train_time:31490ms step_avg:37.99ms
step:830/2110 train_time:31548ms step_avg:38.01ms
step:831/2110 train_time:31607ms step_avg:38.04ms
step:832/2110 train_time:31666ms step_avg:38.06ms
step:833/2110 train_time:31725ms step_avg:38.09ms
step:834/2110 train_time:31784ms step_avg:38.11ms
step:835/2110 train_time:31844ms step_avg:38.14ms
step:836/2110 train_time:31903ms step_avg:38.16ms
step:837/2110 train_time:31963ms step_avg:38.19ms
step:838/2110 train_time:32021ms step_avg:38.21ms
step:839/2110 train_time:32081ms step_avg:38.24ms
step:840/2110 train_time:32141ms step_avg:38.26ms
step:841/2110 train_time:32200ms step_avg:38.29ms
step:842/2110 train_time:32260ms step_avg:38.31ms
step:843/2110 train_time:32320ms step_avg:38.34ms
step:844/2110 train_time:32379ms step_avg:38.36ms
step:845/2110 train_time:32439ms step_avg:38.39ms
step:846/2110 train_time:32498ms step_avg:38.41ms
step:847/2110 train_time:32557ms step_avg:38.44ms
step:848/2110 train_time:32616ms step_avg:38.46ms
step:849/2110 train_time:32676ms step_avg:38.49ms
step:850/2110 train_time:32734ms step_avg:38.51ms
step:851/2110 train_time:32793ms step_avg:38.54ms
step:852/2110 train_time:32853ms step_avg:38.56ms
step:853/2110 train_time:32911ms step_avg:38.58ms
step:854/2110 train_time:32970ms step_avg:38.61ms
step:855/2110 train_time:33029ms step_avg:38.63ms
step:856/2110 train_time:33088ms step_avg:38.65ms
step:857/2110 train_time:33147ms step_avg:38.68ms
step:858/2110 train_time:33206ms step_avg:38.70ms
step:859/2110 train_time:33266ms step_avg:38.73ms
step:860/2110 train_time:33325ms step_avg:38.75ms
step:861/2110 train_time:33385ms step_avg:38.77ms
step:862/2110 train_time:33443ms step_avg:38.80ms
step:863/2110 train_time:33503ms step_avg:38.82ms
step:864/2110 train_time:33561ms step_avg:38.84ms
step:865/2110 train_time:33621ms step_avg:38.87ms
step:866/2110 train_time:33680ms step_avg:38.89ms
step:867/2110 train_time:33739ms step_avg:38.91ms
step:868/2110 train_time:33798ms step_avg:38.94ms
step:869/2110 train_time:33858ms step_avg:38.96ms
step:870/2110 train_time:33917ms step_avg:38.99ms
step:871/2110 train_time:33977ms step_avg:39.01ms
step:872/2110 train_time:34036ms step_avg:39.03ms
step:873/2110 train_time:34095ms step_avg:39.06ms
step:874/2110 train_time:34154ms step_avg:39.08ms
step:875/2110 train_time:34215ms step_avg:39.10ms
step:876/2110 train_time:34274ms step_avg:39.13ms
step:877/2110 train_time:34334ms step_avg:39.15ms
step:878/2110 train_time:34393ms step_avg:39.17ms
step:879/2110 train_time:34453ms step_avg:39.20ms
step:880/2110 train_time:34512ms step_avg:39.22ms
step:881/2110 train_time:34571ms step_avg:39.24ms
step:882/2110 train_time:34630ms step_avg:39.26ms
step:883/2110 train_time:34689ms step_avg:39.28ms
step:884/2110 train_time:34747ms step_avg:39.31ms
step:885/2110 train_time:34807ms step_avg:39.33ms
step:886/2110 train_time:34865ms step_avg:39.35ms
step:887/2110 train_time:34925ms step_avg:39.37ms
step:888/2110 train_time:34983ms step_avg:39.40ms
step:889/2110 train_time:35042ms step_avg:39.42ms
step:890/2110 train_time:35101ms step_avg:39.44ms
step:891/2110 train_time:35160ms step_avg:39.46ms
step:892/2110 train_time:35219ms step_avg:39.48ms
step:893/2110 train_time:35279ms step_avg:39.51ms
step:894/2110 train_time:35339ms step_avg:39.53ms
step:895/2110 train_time:35399ms step_avg:39.55ms
step:896/2110 train_time:35458ms step_avg:39.57ms
step:897/2110 train_time:35518ms step_avg:39.60ms
step:898/2110 train_time:35577ms step_avg:39.62ms
step:899/2110 train_time:35636ms step_avg:39.64ms
step:900/2110 train_time:35695ms step_avg:39.66ms
step:901/2110 train_time:35755ms step_avg:39.68ms
step:902/2110 train_time:35814ms step_avg:39.70ms
step:903/2110 train_time:35873ms step_avg:39.73ms
step:904/2110 train_time:35932ms step_avg:39.75ms
step:905/2110 train_time:35991ms step_avg:39.77ms
step:906/2110 train_time:36050ms step_avg:39.79ms
step:907/2110 train_time:36109ms step_avg:39.81ms
step:908/2110 train_time:36168ms step_avg:39.83ms
step:909/2110 train_time:36227ms step_avg:39.85ms
step:910/2110 train_time:36286ms step_avg:39.87ms
step:911/2110 train_time:36346ms step_avg:39.90ms
step:912/2110 train_time:36405ms step_avg:39.92ms
step:913/2110 train_time:36464ms step_avg:39.94ms
step:914/2110 train_time:36523ms step_avg:39.96ms
step:915/2110 train_time:36583ms step_avg:39.98ms
step:916/2110 train_time:36641ms step_avg:40.00ms
step:917/2110 train_time:36701ms step_avg:40.02ms
step:918/2110 train_time:36760ms step_avg:40.04ms
step:919/2110 train_time:36819ms step_avg:40.06ms
step:920/2110 train_time:36878ms step_avg:40.08ms
step:921/2110 train_time:36938ms step_avg:40.11ms
step:922/2110 train_time:36997ms step_avg:40.13ms
step:923/2110 train_time:37056ms step_avg:40.15ms
step:924/2110 train_time:37115ms step_avg:40.17ms
step:925/2110 train_time:37175ms step_avg:40.19ms
step:926/2110 train_time:37234ms step_avg:40.21ms
step:927/2110 train_time:37294ms step_avg:40.23ms
step:928/2110 train_time:37353ms step_avg:40.25ms
step:929/2110 train_time:37413ms step_avg:40.27ms
step:930/2110 train_time:37472ms step_avg:40.29ms
step:931/2110 train_time:37531ms step_avg:40.31ms
step:932/2110 train_time:37590ms step_avg:40.33ms
step:933/2110 train_time:37649ms step_avg:40.35ms
step:934/2110 train_time:37708ms step_avg:40.37ms
step:935/2110 train_time:37768ms step_avg:40.39ms
step:936/2110 train_time:37826ms step_avg:40.41ms
step:937/2110 train_time:37885ms step_avg:40.43ms
step:938/2110 train_time:37944ms step_avg:40.45ms
step:939/2110 train_time:38005ms step_avg:40.47ms
step:940/2110 train_time:38063ms step_avg:40.49ms
step:941/2110 train_time:38122ms step_avg:40.51ms
step:942/2110 train_time:38181ms step_avg:40.53ms
step:943/2110 train_time:38241ms step_avg:40.55ms
step:944/2110 train_time:38300ms step_avg:40.57ms
step:945/2110 train_time:38360ms step_avg:40.59ms
step:946/2110 train_time:38419ms step_avg:40.61ms
step:947/2110 train_time:38478ms step_avg:40.63ms
step:948/2110 train_time:38537ms step_avg:40.65ms
step:949/2110 train_time:38597ms step_avg:40.67ms
step:950/2110 train_time:38657ms step_avg:40.69ms
step:951/2110 train_time:38716ms step_avg:40.71ms
step:952/2110 train_time:38775ms step_avg:40.73ms
step:953/2110 train_time:38836ms step_avg:40.75ms
step:954/2110 train_time:38895ms step_avg:40.77ms
step:955/2110 train_time:38954ms step_avg:40.79ms
step:956/2110 train_time:39013ms step_avg:40.81ms
step:957/2110 train_time:39072ms step_avg:40.83ms
step:958/2110 train_time:39131ms step_avg:40.85ms
step:959/2110 train_time:39190ms step_avg:40.87ms
step:960/2110 train_time:39248ms step_avg:40.88ms
step:961/2110 train_time:39308ms step_avg:40.90ms
step:962/2110 train_time:39366ms step_avg:40.92ms
step:963/2110 train_time:39426ms step_avg:40.94ms
step:964/2110 train_time:39484ms step_avg:40.96ms
step:965/2110 train_time:39544ms step_avg:40.98ms
step:966/2110 train_time:39602ms step_avg:41.00ms
step:967/2110 train_time:39662ms step_avg:41.02ms
step:968/2110 train_time:39721ms step_avg:41.03ms
step:969/2110 train_time:39781ms step_avg:41.05ms
step:970/2110 train_time:39840ms step_avg:41.07ms
step:971/2110 train_time:39900ms step_avg:41.09ms
step:972/2110 train_time:39959ms step_avg:41.11ms
step:973/2110 train_time:40018ms step_avg:41.13ms
step:974/2110 train_time:40077ms step_avg:41.15ms
step:975/2110 train_time:40137ms step_avg:41.17ms
step:976/2110 train_time:40196ms step_avg:41.18ms
step:977/2110 train_time:40256ms step_avg:41.20ms
step:978/2110 train_time:40314ms step_avg:41.22ms
step:979/2110 train_time:40374ms step_avg:41.24ms
step:980/2110 train_time:40432ms step_avg:41.26ms
step:981/2110 train_time:40492ms step_avg:41.28ms
step:982/2110 train_time:40550ms step_avg:41.29ms
step:983/2110 train_time:40610ms step_avg:41.31ms
step:984/2110 train_time:40667ms step_avg:41.33ms
step:985/2110 train_time:40726ms step_avg:41.35ms
step:986/2110 train_time:40784ms step_avg:41.36ms
step:987/2110 train_time:40844ms step_avg:41.38ms
step:988/2110 train_time:40902ms step_avg:41.40ms
step:989/2110 train_time:40962ms step_avg:41.42ms
step:990/2110 train_time:41021ms step_avg:41.43ms
step:991/2110 train_time:41081ms step_avg:41.45ms
step:992/2110 train_time:41139ms step_avg:41.47ms
step:993/2110 train_time:41199ms step_avg:41.49ms
step:994/2110 train_time:41258ms step_avg:41.51ms
step:995/2110 train_time:41318ms step_avg:41.53ms
step:996/2110 train_time:41377ms step_avg:41.54ms
step:997/2110 train_time:41437ms step_avg:41.56ms
step:998/2110 train_time:41496ms step_avg:41.58ms
step:999/2110 train_time:41556ms step_avg:41.60ms
step:1000/2110 train_time:41615ms step_avg:41.61ms
step:1000/2110 val_loss:3.7626 train_time:41677ms step_avg:41.68ms
step:1001/2110 train_time:41706ms step_avg:41.66ms
step:1002/2110 train_time:41735ms step_avg:41.65ms
step:1003/2110 train_time:41799ms step_avg:41.67ms
step:1004/2110 train_time:41861ms step_avg:41.69ms
step:1005/2110 train_time:41921ms step_avg:41.71ms
step:1006/2110 train_time:41980ms step_avg:41.73ms
step:1007/2110 train_time:42039ms step_avg:41.75ms
step:1008/2110 train_time:42097ms step_avg:41.76ms
step:1009/2110 train_time:42156ms step_avg:41.78ms
step:1010/2110 train_time:42213ms step_avg:41.80ms
step:1011/2110 train_time:42272ms step_avg:41.81ms
step:1012/2110 train_time:42330ms step_avg:41.83ms
step:1013/2110 train_time:42389ms step_avg:41.85ms
step:1014/2110 train_time:42447ms step_avg:41.86ms
step:1015/2110 train_time:42507ms step_avg:41.88ms
step:1016/2110 train_time:42564ms step_avg:41.89ms
step:1017/2110 train_time:42624ms step_avg:41.91ms
step:1018/2110 train_time:42682ms step_avg:41.93ms
step:1019/2110 train_time:42743ms step_avg:41.95ms
step:1020/2110 train_time:42804ms step_avg:41.96ms
step:1021/2110 train_time:42865ms step_avg:41.98ms
step:1022/2110 train_time:42924ms step_avg:42.00ms
step:1023/2110 train_time:42986ms step_avg:42.02ms
step:1024/2110 train_time:43044ms step_avg:42.03ms
step:1025/2110 train_time:43103ms step_avg:42.05ms
step:1026/2110 train_time:43160ms step_avg:42.07ms
step:1027/2110 train_time:43220ms step_avg:42.08ms
step:1028/2110 train_time:43277ms step_avg:42.10ms
step:1029/2110 train_time:43336ms step_avg:42.11ms
step:1030/2110 train_time:43393ms step_avg:42.13ms
step:1031/2110 train_time:43453ms step_avg:42.15ms
step:1032/2110 train_time:43511ms step_avg:42.16ms
step:1033/2110 train_time:43570ms step_avg:42.18ms
step:1034/2110 train_time:43629ms step_avg:42.19ms
step:1035/2110 train_time:43689ms step_avg:42.21ms
step:1036/2110 train_time:43749ms step_avg:42.23ms
step:1037/2110 train_time:43810ms step_avg:42.25ms
step:1038/2110 train_time:43870ms step_avg:42.26ms
step:1039/2110 train_time:43931ms step_avg:42.28ms
step:1040/2110 train_time:43990ms step_avg:42.30ms
step:1041/2110 train_time:44051ms step_avg:42.32ms
step:1042/2110 train_time:44109ms step_avg:42.33ms
step:1043/2110 train_time:44169ms step_avg:42.35ms
step:1044/2110 train_time:44228ms step_avg:42.36ms
step:1045/2110 train_time:44287ms step_avg:42.38ms
step:1046/2110 train_time:44345ms step_avg:42.39ms
step:1047/2110 train_time:44405ms step_avg:42.41ms
step:1048/2110 train_time:44462ms step_avg:42.43ms
step:1049/2110 train_time:44521ms step_avg:42.44ms
step:1050/2110 train_time:44579ms step_avg:42.46ms
step:1051/2110 train_time:44638ms step_avg:42.47ms
step:1052/2110 train_time:44697ms step_avg:42.49ms
step:1053/2110 train_time:44757ms step_avg:42.50ms
step:1054/2110 train_time:44816ms step_avg:42.52ms
step:1055/2110 train_time:44877ms step_avg:42.54ms
step:1056/2110 train_time:44936ms step_avg:42.55ms
step:1057/2110 train_time:44996ms step_avg:42.57ms
step:1058/2110 train_time:45055ms step_avg:42.59ms
step:1059/2110 train_time:45115ms step_avg:42.60ms
step:1060/2110 train_time:45173ms step_avg:42.62ms
step:1061/2110 train_time:45233ms step_avg:42.63ms
step:1062/2110 train_time:45292ms step_avg:42.65ms
step:1063/2110 train_time:45351ms step_avg:42.66ms
step:1064/2110 train_time:45410ms step_avg:42.68ms
step:1065/2110 train_time:45469ms step_avg:42.69ms
step:1066/2110 train_time:45528ms step_avg:42.71ms
step:1067/2110 train_time:45587ms step_avg:42.72ms
step:1068/2110 train_time:45646ms step_avg:42.74ms
step:1069/2110 train_time:45705ms step_avg:42.76ms
step:1070/2110 train_time:45764ms step_avg:42.77ms
step:1071/2110 train_time:45823ms step_avg:42.79ms
step:1072/2110 train_time:45882ms step_avg:42.80ms
step:1073/2110 train_time:45941ms step_avg:42.82ms
step:1074/2110 train_time:46000ms step_avg:42.83ms
step:1075/2110 train_time:46060ms step_avg:42.85ms
step:1076/2110 train_time:46118ms step_avg:42.86ms
step:1077/2110 train_time:46178ms step_avg:42.88ms
step:1078/2110 train_time:46237ms step_avg:42.89ms
step:1079/2110 train_time:46295ms step_avg:42.91ms
step:1080/2110 train_time:46354ms step_avg:42.92ms
step:1081/2110 train_time:46413ms step_avg:42.94ms
step:1082/2110 train_time:46472ms step_avg:42.95ms
step:1083/2110 train_time:46531ms step_avg:42.97ms
step:1084/2110 train_time:46591ms step_avg:42.98ms
step:1085/2110 train_time:46651ms step_avg:43.00ms
step:1086/2110 train_time:46710ms step_avg:43.01ms
step:1087/2110 train_time:46770ms step_avg:43.03ms
step:1088/2110 train_time:46830ms step_avg:43.04ms
step:1089/2110 train_time:46890ms step_avg:43.06ms
step:1090/2110 train_time:46949ms step_avg:43.07ms
step:1091/2110 train_time:47008ms step_avg:43.09ms
step:1092/2110 train_time:47067ms step_avg:43.10ms
step:1093/2110 train_time:47127ms step_avg:43.12ms
step:1094/2110 train_time:47186ms step_avg:43.13ms
step:1095/2110 train_time:47245ms step_avg:43.15ms
step:1096/2110 train_time:47303ms step_avg:43.16ms
step:1097/2110 train_time:47362ms step_avg:43.17ms
step:1098/2110 train_time:47420ms step_avg:43.19ms
step:1099/2110 train_time:47479ms step_avg:43.20ms
step:1100/2110 train_time:47537ms step_avg:43.22ms
step:1101/2110 train_time:47597ms step_avg:43.23ms
step:1102/2110 train_time:47656ms step_avg:43.25ms
step:1103/2110 train_time:47716ms step_avg:43.26ms
step:1104/2110 train_time:47774ms step_avg:43.27ms
step:1105/2110 train_time:47834ms step_avg:43.29ms
step:1106/2110 train_time:47895ms step_avg:43.30ms
step:1107/2110 train_time:47954ms step_avg:43.32ms
step:1108/2110 train_time:48013ms step_avg:43.33ms
step:1109/2110 train_time:48072ms step_avg:43.35ms
step:1110/2110 train_time:48131ms step_avg:43.36ms
step:1111/2110 train_time:48191ms step_avg:43.38ms
step:1112/2110 train_time:48250ms step_avg:43.39ms
step:1113/2110 train_time:48310ms step_avg:43.40ms
step:1114/2110 train_time:48368ms step_avg:43.42ms
step:1115/2110 train_time:48428ms step_avg:43.43ms
step:1116/2110 train_time:48487ms step_avg:43.45ms
step:1117/2110 train_time:48546ms step_avg:43.46ms
step:1118/2110 train_time:48605ms step_avg:43.47ms
step:1119/2110 train_time:48664ms step_avg:43.49ms
step:1120/2110 train_time:48723ms step_avg:43.50ms
step:1121/2110 train_time:48782ms step_avg:43.52ms
step:1122/2110 train_time:48841ms step_avg:43.53ms
step:1123/2110 train_time:48900ms step_avg:43.54ms
step:1124/2110 train_time:48958ms step_avg:43.56ms
step:1125/2110 train_time:49018ms step_avg:43.57ms
step:1126/2110 train_time:49077ms step_avg:43.59ms
step:1127/2110 train_time:49136ms step_avg:43.60ms
step:1128/2110 train_time:49195ms step_avg:43.61ms
step:1129/2110 train_time:49255ms step_avg:43.63ms
step:1130/2110 train_time:49314ms step_avg:43.64ms
step:1131/2110 train_time:49373ms step_avg:43.65ms
step:1132/2110 train_time:49432ms step_avg:43.67ms
step:1133/2110 train_time:49491ms step_avg:43.68ms
step:1134/2110 train_time:49551ms step_avg:43.70ms
step:1135/2110 train_time:49610ms step_avg:43.71ms
step:1136/2110 train_time:49669ms step_avg:43.72ms
step:1137/2110 train_time:49729ms step_avg:43.74ms
step:1138/2110 train_time:49788ms step_avg:43.75ms
step:1139/2110 train_time:49848ms step_avg:43.76ms
step:1140/2110 train_time:49909ms step_avg:43.78ms
step:1141/2110 train_time:49968ms step_avg:43.79ms
step:1142/2110 train_time:50027ms step_avg:43.81ms
step:1143/2110 train_time:50087ms step_avg:43.82ms
step:1144/2110 train_time:50147ms step_avg:43.83ms
step:1145/2110 train_time:50207ms step_avg:43.85ms
step:1146/2110 train_time:50265ms step_avg:43.86ms
step:1147/2110 train_time:50325ms step_avg:43.88ms
step:1148/2110 train_time:50384ms step_avg:43.89ms
step:1149/2110 train_time:50443ms step_avg:43.90ms
step:1150/2110 train_time:50502ms step_avg:43.91ms
step:1151/2110 train_time:50562ms step_avg:43.93ms
step:1152/2110 train_time:50621ms step_avg:43.94ms
step:1153/2110 train_time:50681ms step_avg:43.96ms
step:1154/2110 train_time:50740ms step_avg:43.97ms
step:1155/2110 train_time:50799ms step_avg:43.98ms
step:1156/2110 train_time:50858ms step_avg:44.00ms
step:1157/2110 train_time:50919ms step_avg:44.01ms
step:1158/2110 train_time:50978ms step_avg:44.02ms
step:1159/2110 train_time:51038ms step_avg:44.04ms
step:1160/2110 train_time:51097ms step_avg:44.05ms
step:1161/2110 train_time:51158ms step_avg:44.06ms
step:1162/2110 train_time:51217ms step_avg:44.08ms
step:1163/2110 train_time:51277ms step_avg:44.09ms
step:1164/2110 train_time:51336ms step_avg:44.10ms
step:1165/2110 train_time:51396ms step_avg:44.12ms
step:1166/2110 train_time:51456ms step_avg:44.13ms
step:1167/2110 train_time:51516ms step_avg:44.14ms
step:1168/2110 train_time:51575ms step_avg:44.16ms
step:1169/2110 train_time:51635ms step_avg:44.17ms
step:1170/2110 train_time:51696ms step_avg:44.18ms
step:1171/2110 train_time:51755ms step_avg:44.20ms
step:1172/2110 train_time:51815ms step_avg:44.21ms
step:1173/2110 train_time:51876ms step_avg:44.22ms
step:1174/2110 train_time:51935ms step_avg:44.24ms
step:1175/2110 train_time:51995ms step_avg:44.25ms
step:1176/2110 train_time:52055ms step_avg:44.26ms
step:1177/2110 train_time:52116ms step_avg:44.28ms
step:1178/2110 train_time:52175ms step_avg:44.29ms
step:1179/2110 train_time:52236ms step_avg:44.31ms
step:1180/2110 train_time:52295ms step_avg:44.32ms
step:1181/2110 train_time:52355ms step_avg:44.33ms
step:1182/2110 train_time:52415ms step_avg:44.34ms
step:1183/2110 train_time:52475ms step_avg:44.36ms
step:1184/2110 train_time:52535ms step_avg:44.37ms
step:1185/2110 train_time:52595ms step_avg:44.38ms
step:1186/2110 train_time:52656ms step_avg:44.40ms
step:1187/2110 train_time:52715ms step_avg:44.41ms
step:1188/2110 train_time:52774ms step_avg:44.42ms
step:1189/2110 train_time:52835ms step_avg:44.44ms
step:1190/2110 train_time:52894ms step_avg:44.45ms
step:1191/2110 train_time:52955ms step_avg:44.46ms
step:1192/2110 train_time:53015ms step_avg:44.48ms
step:1193/2110 train_time:53075ms step_avg:44.49ms
step:1194/2110 train_time:53134ms step_avg:44.50ms
step:1195/2110 train_time:53195ms step_avg:44.51ms
step:1196/2110 train_time:53254ms step_avg:44.53ms
step:1197/2110 train_time:53314ms step_avg:44.54ms
step:1198/2110 train_time:53374ms step_avg:44.55ms
step:1199/2110 train_time:53434ms step_avg:44.57ms
step:1200/2110 train_time:53493ms step_avg:44.58ms
step:1201/2110 train_time:53553ms step_avg:44.59ms
step:1202/2110 train_time:53613ms step_avg:44.60ms
step:1203/2110 train_time:53673ms step_avg:44.62ms
step:1204/2110 train_time:53732ms step_avg:44.63ms
step:1205/2110 train_time:53793ms step_avg:44.64ms
step:1206/2110 train_time:53853ms step_avg:44.65ms
step:1207/2110 train_time:53914ms step_avg:44.67ms
step:1208/2110 train_time:53973ms step_avg:44.68ms
step:1209/2110 train_time:54033ms step_avg:44.69ms
step:1210/2110 train_time:54093ms step_avg:44.71ms
step:1211/2110 train_time:54153ms step_avg:44.72ms
step:1212/2110 train_time:54213ms step_avg:44.73ms
step:1213/2110 train_time:54273ms step_avg:44.74ms
step:1214/2110 train_time:54333ms step_avg:44.76ms
step:1215/2110 train_time:54392ms step_avg:44.77ms
step:1216/2110 train_time:54452ms step_avg:44.78ms
step:1217/2110 train_time:54512ms step_avg:44.79ms
step:1218/2110 train_time:54571ms step_avg:44.80ms
step:1219/2110 train_time:54631ms step_avg:44.82ms
step:1220/2110 train_time:54692ms step_avg:44.83ms
step:1221/2110 train_time:54752ms step_avg:44.84ms
step:1222/2110 train_time:54811ms step_avg:44.85ms
step:1223/2110 train_time:54872ms step_avg:44.87ms
step:1224/2110 train_time:54933ms step_avg:44.88ms
step:1225/2110 train_time:54994ms step_avg:44.89ms
step:1226/2110 train_time:55054ms step_avg:44.91ms
step:1227/2110 train_time:55114ms step_avg:44.92ms
step:1228/2110 train_time:55174ms step_avg:44.93ms
step:1229/2110 train_time:55235ms step_avg:44.94ms
step:1230/2110 train_time:55295ms step_avg:44.96ms
step:1231/2110 train_time:55355ms step_avg:44.97ms
step:1232/2110 train_time:55415ms step_avg:44.98ms
step:1233/2110 train_time:55474ms step_avg:44.99ms
step:1234/2110 train_time:55534ms step_avg:45.00ms
step:1235/2110 train_time:55594ms step_avg:45.02ms
step:1236/2110 train_time:55654ms step_avg:45.03ms
step:1237/2110 train_time:55715ms step_avg:45.04ms
step:1238/2110 train_time:55775ms step_avg:45.05ms
step:1239/2110 train_time:55834ms step_avg:45.06ms
step:1240/2110 train_time:55894ms step_avg:45.08ms
step:1241/2110 train_time:55953ms step_avg:45.09ms
step:1242/2110 train_time:56012ms step_avg:45.10ms
step:1243/2110 train_time:56073ms step_avg:45.11ms
step:1244/2110 train_time:56133ms step_avg:45.12ms
step:1245/2110 train_time:56194ms step_avg:45.14ms
step:1246/2110 train_time:56254ms step_avg:45.15ms
step:1247/2110 train_time:56314ms step_avg:45.16ms
step:1248/2110 train_time:56374ms step_avg:45.17ms
step:1249/2110 train_time:56433ms step_avg:45.18ms
step:1250/2110 train_time:56494ms step_avg:45.20ms
step:1250/2110 val_loss:3.5902 train_time:56555ms step_avg:45.24ms
step:1251/2110 train_time:56593ms step_avg:45.24ms
step:1252/2110 train_time:56630ms step_avg:45.23ms
step:1253/2110 train_time:56681ms step_avg:45.24ms
step:1254/2110 train_time:56743ms step_avg:45.25ms
step:1255/2110 train_time:56804ms step_avg:45.26ms
step:1256/2110 train_time:56864ms step_avg:45.27ms
step:1257/2110 train_time:56923ms step_avg:45.28ms
step:1258/2110 train_time:56982ms step_avg:45.30ms
step:1259/2110 train_time:57040ms step_avg:45.31ms
step:1260/2110 train_time:57099ms step_avg:45.32ms
step:1261/2110 train_time:57158ms step_avg:45.33ms
step:1262/2110 train_time:57216ms step_avg:45.34ms
step:1263/2110 train_time:57275ms step_avg:45.35ms
step:1264/2110 train_time:57335ms step_avg:45.36ms
step:1265/2110 train_time:57394ms step_avg:45.37ms
step:1266/2110 train_time:57454ms step_avg:45.38ms
step:1267/2110 train_time:57514ms step_avg:45.39ms
step:1268/2110 train_time:57574ms step_avg:45.41ms
step:1269/2110 train_time:57638ms step_avg:45.42ms
step:1270/2110 train_time:57698ms step_avg:45.43ms
step:1271/2110 train_time:57760ms step_avg:45.44ms
step:1272/2110 train_time:57820ms step_avg:45.46ms
step:1273/2110 train_time:57879ms step_avg:45.47ms
step:1274/2110 train_time:57939ms step_avg:45.48ms
step:1275/2110 train_time:57998ms step_avg:45.49ms
step:1276/2110 train_time:58057ms step_avg:45.50ms
step:1277/2110 train_time:58116ms step_avg:45.51ms
step:1278/2110 train_time:58175ms step_avg:45.52ms
step:1279/2110 train_time:58234ms step_avg:45.53ms
step:1280/2110 train_time:58294ms step_avg:45.54ms
step:1281/2110 train_time:58353ms step_avg:45.55ms
step:1282/2110 train_time:58413ms step_avg:45.56ms
step:1283/2110 train_time:58473ms step_avg:45.57ms
step:1284/2110 train_time:58534ms step_avg:45.59ms
step:1285/2110 train_time:58595ms step_avg:45.60ms
step:1286/2110 train_time:58656ms step_avg:45.61ms
step:1287/2110 train_time:58717ms step_avg:45.62ms
step:1288/2110 train_time:58778ms step_avg:45.63ms
step:1289/2110 train_time:58839ms step_avg:45.65ms
step:1290/2110 train_time:58898ms step_avg:45.66ms
step:1291/2110 train_time:58958ms step_avg:45.67ms
step:1292/2110 train_time:59018ms step_avg:45.68ms
step:1293/2110 train_time:59077ms step_avg:45.69ms
step:1294/2110 train_time:59136ms step_avg:45.70ms
step:1295/2110 train_time:59195ms step_avg:45.71ms
step:1296/2110 train_time:59254ms step_avg:45.72ms
step:1297/2110 train_time:59313ms step_avg:45.73ms
step:1298/2110 train_time:59373ms step_avg:45.74ms
step:1299/2110 train_time:59432ms step_avg:45.75ms
step:1300/2110 train_time:59493ms step_avg:45.76ms
step:1301/2110 train_time:59552ms step_avg:45.77ms
step:1302/2110 train_time:59613ms step_avg:45.79ms
step:1303/2110 train_time:59675ms step_avg:45.80ms
step:1304/2110 train_time:59737ms step_avg:45.81ms
step:1305/2110 train_time:59798ms step_avg:45.82ms
step:1306/2110 train_time:59857ms step_avg:45.83ms
step:1307/2110 train_time:59918ms step_avg:45.84ms
step:1308/2110 train_time:59977ms step_avg:45.85ms
step:1309/2110 train_time:60038ms step_avg:45.87ms
step:1310/2110 train_time:60097ms step_avg:45.88ms
step:1311/2110 train_time:60157ms step_avg:45.89ms
step:1312/2110 train_time:60216ms step_avg:45.90ms
step:1313/2110 train_time:60275ms step_avg:45.91ms
step:1314/2110 train_time:60336ms step_avg:45.92ms
step:1315/2110 train_time:60395ms step_avg:45.93ms
step:1316/2110 train_time:60455ms step_avg:45.94ms
step:1317/2110 train_time:60515ms step_avg:45.95ms
step:1318/2110 train_time:60574ms step_avg:45.96ms
step:1319/2110 train_time:60636ms step_avg:45.97ms
step:1320/2110 train_time:60696ms step_avg:45.98ms
step:1321/2110 train_time:60757ms step_avg:45.99ms
step:1322/2110 train_time:60817ms step_avg:46.00ms
step:1323/2110 train_time:60877ms step_avg:46.01ms
step:1324/2110 train_time:60937ms step_avg:46.03ms
step:1325/2110 train_time:60997ms step_avg:46.04ms
step:1326/2110 train_time:61056ms step_avg:46.05ms
step:1327/2110 train_time:61116ms step_avg:46.06ms
step:1328/2110 train_time:61175ms step_avg:46.07ms
step:1329/2110 train_time:61235ms step_avg:46.08ms
step:1330/2110 train_time:61294ms step_avg:46.09ms
step:1331/2110 train_time:61354ms step_avg:46.10ms
step:1332/2110 train_time:61414ms step_avg:46.11ms
step:1333/2110 train_time:61473ms step_avg:46.12ms
step:1334/2110 train_time:61534ms step_avg:46.13ms
step:1335/2110 train_time:61594ms step_avg:46.14ms
step:1336/2110 train_time:61656ms step_avg:46.15ms
step:1337/2110 train_time:61716ms step_avg:46.16ms
step:1338/2110 train_time:61776ms step_avg:46.17ms
step:1339/2110 train_time:61837ms step_avg:46.18ms
step:1340/2110 train_time:61897ms step_avg:46.19ms
step:1341/2110 train_time:61957ms step_avg:46.20ms
step:1342/2110 train_time:62017ms step_avg:46.21ms
step:1343/2110 train_time:62076ms step_avg:46.22ms
step:1344/2110 train_time:62136ms step_avg:46.23ms
step:1345/2110 train_time:62195ms step_avg:46.24ms
step:1346/2110 train_time:62255ms step_avg:46.25ms
step:1347/2110 train_time:62314ms step_avg:46.26ms
step:1348/2110 train_time:62374ms step_avg:46.27ms
step:1349/2110 train_time:62434ms step_avg:46.28ms
step:1350/2110 train_time:62494ms step_avg:46.29ms
step:1351/2110 train_time:62554ms step_avg:46.30ms
step:1352/2110 train_time:62613ms step_avg:46.31ms
step:1353/2110 train_time:62674ms step_avg:46.32ms
step:1354/2110 train_time:62735ms step_avg:46.33ms
step:1355/2110 train_time:62796ms step_avg:46.34ms
step:1356/2110 train_time:62856ms step_avg:46.35ms
step:1357/2110 train_time:62916ms step_avg:46.36ms
step:1358/2110 train_time:62976ms step_avg:46.37ms
step:1359/2110 train_time:63036ms step_avg:46.38ms
step:1360/2110 train_time:63096ms step_avg:46.39ms
step:1361/2110 train_time:63156ms step_avg:46.40ms
step:1362/2110 train_time:63216ms step_avg:46.41ms
step:1363/2110 train_time:63275ms step_avg:46.42ms
step:1364/2110 train_time:63335ms step_avg:46.43ms
step:1365/2110 train_time:63395ms step_avg:46.44ms
step:1366/2110 train_time:63455ms step_avg:46.45ms
step:1367/2110 train_time:63514ms step_avg:46.46ms
step:1368/2110 train_time:63574ms step_avg:46.47ms
step:1369/2110 train_time:63635ms step_avg:46.48ms
step:1370/2110 train_time:63696ms step_avg:46.49ms
step:1371/2110 train_time:63757ms step_avg:46.50ms
step:1372/2110 train_time:63817ms step_avg:46.51ms
step:1373/2110 train_time:63877ms step_avg:46.52ms
step:1374/2110 train_time:63939ms step_avg:46.53ms
step:1375/2110 train_time:63997ms step_avg:46.54ms
step:1376/2110 train_time:64056ms step_avg:46.55ms
step:1377/2110 train_time:64116ms step_avg:46.56ms
step:1378/2110 train_time:64176ms step_avg:46.57ms
step:1379/2110 train_time:64235ms step_avg:46.58ms
step:1380/2110 train_time:64296ms step_avg:46.59ms
step:1381/2110 train_time:64356ms step_avg:46.60ms
step:1382/2110 train_time:64443ms step_avg:46.63ms
step:1383/2110 train_time:64530ms step_avg:46.66ms
step:1384/2110 train_time:64617ms step_avg:46.69ms
step:1385/2110 train_time:64704ms step_avg:46.72ms
step:1386/2110 train_time:64792ms step_avg:46.75ms
step:1387/2110 train_time:64878ms step_avg:46.78ms
step:1388/2110 train_time:64966ms step_avg:46.81ms
step:1389/2110 train_time:65052ms step_avg:46.83ms
step:1390/2110 train_time:65139ms step_avg:46.86ms
step:1391/2110 train_time:65225ms step_avg:46.89ms
step:1392/2110 train_time:65312ms step_avg:46.92ms
step:1393/2110 train_time:65398ms step_avg:46.95ms
step:1394/2110 train_time:65485ms step_avg:46.98ms
step:1395/2110 train_time:65571ms step_avg:47.00ms
step:1396/2110 train_time:65659ms step_avg:47.03ms
step:1397/2110 train_time:65746ms step_avg:47.06ms
step:1398/2110 train_time:65833ms step_avg:47.09ms
step:1399/2110 train_time:65920ms step_avg:47.12ms
step:1400/2110 train_time:66008ms step_avg:47.15ms
step:1401/2110 train_time:66094ms step_avg:47.18ms
step:1402/2110 train_time:66181ms step_avg:47.20ms
step:1403/2110 train_time:66268ms step_avg:47.23ms
step:1404/2110 train_time:66353ms step_avg:47.26ms
step:1405/2110 train_time:66440ms step_avg:47.29ms
step:1406/2110 train_time:66528ms step_avg:47.32ms
step:1407/2110 train_time:66613ms step_avg:47.34ms
step:1408/2110 train_time:66701ms step_avg:47.37ms
step:1409/2110 train_time:66789ms step_avg:47.40ms
step:1410/2110 train_time:66876ms step_avg:47.43ms
step:1411/2110 train_time:66962ms step_avg:47.46ms
step:1412/2110 train_time:67050ms step_avg:47.49ms
step:1413/2110 train_time:67135ms step_avg:47.51ms
step:1414/2110 train_time:67223ms step_avg:47.54ms
step:1415/2110 train_time:67310ms step_avg:47.57ms
step:1416/2110 train_time:67397ms step_avg:47.60ms
step:1417/2110 train_time:67484ms step_avg:47.62ms
step:1418/2110 train_time:67571ms step_avg:47.65ms
step:1419/2110 train_time:67657ms step_avg:47.68ms
step:1420/2110 train_time:67744ms step_avg:47.71ms
step:1421/2110 train_time:67830ms step_avg:47.73ms
step:1422/2110 train_time:67917ms step_avg:47.76ms
step:1423/2110 train_time:68004ms step_avg:47.79ms
step:1424/2110 train_time:68091ms step_avg:47.82ms
step:1425/2110 train_time:68176ms step_avg:47.84ms
step:1426/2110 train_time:68264ms step_avg:47.87ms
step:1427/2110 train_time:68350ms step_avg:47.90ms
step:1428/2110 train_time:68438ms step_avg:47.93ms
step:1429/2110 train_time:68524ms step_avg:47.95ms
step:1430/2110 train_time:68610ms step_avg:47.98ms
step:1431/2110 train_time:68696ms step_avg:48.01ms
step:1432/2110 train_time:68783ms step_avg:48.03ms
step:1433/2110 train_time:68870ms step_avg:48.06ms
step:1434/2110 train_time:68957ms step_avg:48.09ms
step:1435/2110 train_time:69043ms step_avg:48.11ms
step:1436/2110 train_time:69130ms step_avg:48.14ms
step:1437/2110 train_time:69216ms step_avg:48.17ms
step:1438/2110 train_time:69303ms step_avg:48.19ms
step:1439/2110 train_time:69390ms step_avg:48.22ms
step:1440/2110 train_time:69477ms step_avg:48.25ms
step:1441/2110 train_time:69564ms step_avg:48.28ms
step:1442/2110 train_time:69651ms step_avg:48.30ms
step:1443/2110 train_time:69737ms step_avg:48.33ms
step:1444/2110 train_time:69824ms step_avg:48.35ms
step:1445/2110 train_time:69911ms step_avg:48.38ms
step:1446/2110 train_time:69998ms step_avg:48.41ms
step:1447/2110 train_time:70085ms step_avg:48.43ms
step:1448/2110 train_time:70172ms step_avg:48.46ms
step:1449/2110 train_time:70258ms step_avg:48.49ms
step:1450/2110 train_time:70345ms step_avg:48.51ms
step:1451/2110 train_time:70431ms step_avg:48.54ms
step:1452/2110 train_time:70518ms step_avg:48.57ms
step:1453/2110 train_time:70605ms step_avg:48.59ms
step:1454/2110 train_time:70691ms step_avg:48.62ms
step:1455/2110 train_time:70778ms step_avg:48.64ms
step:1456/2110 train_time:70867ms step_avg:48.67ms
step:1457/2110 train_time:70952ms step_avg:48.70ms
step:1458/2110 train_time:71039ms step_avg:48.72ms
step:1459/2110 train_time:71125ms step_avg:48.75ms
step:1460/2110 train_time:71212ms step_avg:48.78ms
step:1461/2110 train_time:71298ms step_avg:48.80ms
step:1462/2110 train_time:71386ms step_avg:48.83ms
step:1463/2110 train_time:71472ms step_avg:48.85ms
step:1464/2110 train_time:71559ms step_avg:48.88ms
step:1465/2110 train_time:71646ms step_avg:48.91ms
step:1466/2110 train_time:71732ms step_avg:48.93ms
step:1467/2110 train_time:71819ms step_avg:48.96ms
step:1468/2110 train_time:71907ms step_avg:48.98ms
step:1469/2110 train_time:71993ms step_avg:49.01ms
step:1470/2110 train_time:72080ms step_avg:49.03ms
step:1471/2110 train_time:72166ms step_avg:49.06ms
step:1472/2110 train_time:72253ms step_avg:49.08ms
step:1473/2110 train_time:72339ms step_avg:49.11ms
step:1474/2110 train_time:72427ms step_avg:49.14ms
step:1475/2110 train_time:72512ms step_avg:49.16ms
step:1476/2110 train_time:72600ms step_avg:49.19ms
step:1477/2110 train_time:72687ms step_avg:49.21ms
step:1478/2110 train_time:72773ms step_avg:49.24ms
step:1479/2110 train_time:72861ms step_avg:49.26ms
step:1480/2110 train_time:72949ms step_avg:49.29ms
step:1481/2110 train_time:73034ms step_avg:49.31ms
step:1482/2110 train_time:73121ms step_avg:49.34ms
step:1483/2110 train_time:73209ms step_avg:49.37ms
step:1484/2110 train_time:73296ms step_avg:49.39ms
step:1485/2110 train_time:73382ms step_avg:49.42ms
step:1486/2110 train_time:73469ms step_avg:49.44ms
step:1487/2110 train_time:73556ms step_avg:49.47ms
step:1488/2110 train_time:73643ms step_avg:49.49ms
step:1489/2110 train_time:73730ms step_avg:49.52ms
step:1490/2110 train_time:73818ms step_avg:49.54ms
step:1491/2110 train_time:73904ms step_avg:49.57ms
step:1492/2110 train_time:73991ms step_avg:49.59ms
step:1493/2110 train_time:74077ms step_avg:49.62ms
step:1494/2110 train_time:74165ms step_avg:49.64ms
step:1495/2110 train_time:74251ms step_avg:49.67ms
step:1496/2110 train_time:74338ms step_avg:49.69ms
step:1497/2110 train_time:74425ms step_avg:49.72ms
step:1498/2110 train_time:74512ms step_avg:49.74ms
step:1499/2110 train_time:74598ms step_avg:49.77ms
step:1500/2110 train_time:74686ms step_avg:49.79ms
step:1500/2110 val_loss:3.4922 train_time:74773ms step_avg:49.85ms
step:1501/2110 train_time:74809ms step_avg:49.84ms
step:1502/2110 train_time:74865ms step_avg:49.84ms
step:1503/2110 train_time:74957ms step_avg:49.87ms
step:1504/2110 train_time:75047ms step_avg:49.90ms
step:1505/2110 train_time:75133ms step_avg:49.92ms
step:1506/2110 train_time:75221ms step_avg:49.95ms
step:1507/2110 train_time:75308ms step_avg:49.97ms
step:1508/2110 train_time:75393ms step_avg:50.00ms
step:1509/2110 train_time:75478ms step_avg:50.02ms
step:1510/2110 train_time:75564ms step_avg:50.04ms
step:1511/2110 train_time:75649ms step_avg:50.07ms
step:1512/2110 train_time:75735ms step_avg:50.09ms
step:1513/2110 train_time:75823ms step_avg:50.11ms
step:1514/2110 train_time:75913ms step_avg:50.14ms
step:1515/2110 train_time:76001ms step_avg:50.17ms
step:1516/2110 train_time:76089ms step_avg:50.19ms
step:1517/2110 train_time:76176ms step_avg:50.21ms
step:1518/2110 train_time:76263ms step_avg:50.24ms
step:1519/2110 train_time:76349ms step_avg:50.26ms
step:1520/2110 train_time:76435ms step_avg:50.29ms
step:1521/2110 train_time:76521ms step_avg:50.31ms
step:1522/2110 train_time:76607ms step_avg:50.33ms
step:1523/2110 train_time:76692ms step_avg:50.36ms
step:1524/2110 train_time:76780ms step_avg:50.38ms
step:1525/2110 train_time:76868ms step_avg:50.41ms
step:1526/2110 train_time:76956ms step_avg:50.43ms
step:1527/2110 train_time:77044ms step_avg:50.45ms
step:1528/2110 train_time:77131ms step_avg:50.48ms
step:1529/2110 train_time:77217ms step_avg:50.50ms
step:1530/2110 train_time:77305ms step_avg:50.53ms
step:1531/2110 train_time:77390ms step_avg:50.55ms
step:1532/2110 train_time:77480ms step_avg:50.57ms
step:1533/2110 train_time:77566ms step_avg:50.60ms
step:1534/2110 train_time:77650ms step_avg:50.62ms
step:1535/2110 train_time:77735ms step_avg:50.64ms
step:1536/2110 train_time:77823ms step_avg:50.67ms
step:1537/2110 train_time:77910ms step_avg:50.69ms
step:1538/2110 train_time:77998ms step_avg:50.71ms
step:1539/2110 train_time:78085ms step_avg:50.74ms
step:1540/2110 train_time:78171ms step_avg:50.76ms
step:1541/2110 train_time:78259ms step_avg:50.78ms
step:1542/2110 train_time:78346ms step_avg:50.81ms
step:1543/2110 train_time:78432ms step_avg:50.83ms
step:1544/2110 train_time:78518ms step_avg:50.85ms
step:1545/2110 train_time:78605ms step_avg:50.88ms
step:1546/2110 train_time:78691ms step_avg:50.90ms
step:1547/2110 train_time:78779ms step_avg:50.92ms
step:1548/2110 train_time:78866ms step_avg:50.95ms
step:1549/2110 train_time:78954ms step_avg:50.97ms
step:1550/2110 train_time:79040ms step_avg:50.99ms
step:1551/2110 train_time:79127ms step_avg:51.02ms
step:1552/2110 train_time:79214ms step_avg:51.04ms
step:1553/2110 train_time:79302ms step_avg:51.06ms
step:1554/2110 train_time:79388ms step_avg:51.09ms
step:1555/2110 train_time:79474ms step_avg:51.11ms
step:1556/2110 train_time:79560ms step_avg:51.13ms
step:1557/2110 train_time:79647ms step_avg:51.15ms
step:1558/2110 train_time:79733ms step_avg:51.18ms
step:1559/2110 train_time:79821ms step_avg:51.20ms
step:1560/2110 train_time:79910ms step_avg:51.22ms
step:1561/2110 train_time:79996ms step_avg:51.25ms
step:1562/2110 train_time:80082ms step_avg:51.27ms
step:1563/2110 train_time:80169ms step_avg:51.29ms
step:1564/2110 train_time:80255ms step_avg:51.31ms
step:1565/2110 train_time:80343ms step_avg:51.34ms
step:1566/2110 train_time:80429ms step_avg:51.36ms
step:1567/2110 train_time:80515ms step_avg:51.38ms
step:1568/2110 train_time:80601ms step_avg:51.40ms
step:1569/2110 train_time:80689ms step_avg:51.43ms
step:1570/2110 train_time:80775ms step_avg:51.45ms
step:1571/2110 train_time:80862ms step_avg:51.47ms
step:1572/2110 train_time:80949ms step_avg:51.49ms
step:1573/2110 train_time:81035ms step_avg:51.52ms
step:1574/2110 train_time:81121ms step_avg:51.54ms
step:1575/2110 train_time:81208ms step_avg:51.56ms
step:1576/2110 train_time:81294ms step_avg:51.58ms
step:1577/2110 train_time:81380ms step_avg:51.60ms
step:1578/2110 train_time:81467ms step_avg:51.63ms
step:1579/2110 train_time:81553ms step_avg:51.65ms
step:1580/2110 train_time:81640ms step_avg:51.67ms
step:1581/2110 train_time:81728ms step_avg:51.69ms
step:1582/2110 train_time:81814ms step_avg:51.72ms
step:1583/2110 train_time:81901ms step_avg:51.74ms
step:1584/2110 train_time:81987ms step_avg:51.76ms
step:1585/2110 train_time:82074ms step_avg:51.78ms
step:1586/2110 train_time:82160ms step_avg:51.80ms
step:1587/2110 train_time:82249ms step_avg:51.83ms
step:1588/2110 train_time:82335ms step_avg:51.85ms
step:1589/2110 train_time:82423ms step_avg:51.87ms
step:1590/2110 train_time:82508ms step_avg:51.89ms
step:1591/2110 train_time:82595ms step_avg:51.91ms
step:1592/2110 train_time:82681ms step_avg:51.94ms
step:1593/2110 train_time:82770ms step_avg:51.96ms
step:1594/2110 train_time:82856ms step_avg:51.98ms
step:1595/2110 train_time:82944ms step_avg:52.00ms
step:1596/2110 train_time:83030ms step_avg:52.02ms
step:1597/2110 train_time:83117ms step_avg:52.05ms
step:1598/2110 train_time:83203ms step_avg:52.07ms
step:1599/2110 train_time:83290ms step_avg:52.09ms
step:1600/2110 train_time:83378ms step_avg:52.11ms
step:1601/2110 train_time:83464ms step_avg:52.13ms
step:1602/2110 train_time:83550ms step_avg:52.15ms
step:1603/2110 train_time:83637ms step_avg:52.18ms
step:1604/2110 train_time:83724ms step_avg:52.20ms
step:1605/2110 train_time:83810ms step_avg:52.22ms
step:1606/2110 train_time:83897ms step_avg:52.24ms
step:1607/2110 train_time:83984ms step_avg:52.26ms
step:1608/2110 train_time:84070ms step_avg:52.28ms
step:1609/2110 train_time:84156ms step_avg:52.30ms
step:1610/2110 train_time:84242ms step_avg:52.32ms
step:1611/2110 train_time:84330ms step_avg:52.35ms
step:1612/2110 train_time:84417ms step_avg:52.37ms
step:1613/2110 train_time:84505ms step_avg:52.39ms
step:1614/2110 train_time:84590ms step_avg:52.41ms
step:1615/2110 train_time:84677ms step_avg:52.43ms
step:1616/2110 train_time:84763ms step_avg:52.45ms
step:1617/2110 train_time:84850ms step_avg:52.47ms
step:1618/2110 train_time:84937ms step_avg:52.50ms
step:1619/2110 train_time:85024ms step_avg:52.52ms
step:1620/2110 train_time:85111ms step_avg:52.54ms
step:1621/2110 train_time:85199ms step_avg:52.56ms
step:1622/2110 train_time:85285ms step_avg:52.58ms
step:1623/2110 train_time:85373ms step_avg:52.60ms
step:1624/2110 train_time:85460ms step_avg:52.62ms
step:1625/2110 train_time:85548ms step_avg:52.64ms
step:1626/2110 train_time:85633ms step_avg:52.66ms
step:1627/2110 train_time:85721ms step_avg:52.69ms
step:1628/2110 train_time:85808ms step_avg:52.71ms
step:1629/2110 train_time:85894ms step_avg:52.73ms
step:1630/2110 train_time:85980ms step_avg:52.75ms
step:1631/2110 train_time:86068ms step_avg:52.77ms
step:1632/2110 train_time:86154ms step_avg:52.79ms
step:1633/2110 train_time:86242ms step_avg:52.81ms
step:1634/2110 train_time:86328ms step_avg:52.83ms
step:1635/2110 train_time:86415ms step_avg:52.85ms
step:1636/2110 train_time:86503ms step_avg:52.87ms
step:1637/2110 train_time:86590ms step_avg:52.90ms
step:1638/2110 train_time:86677ms step_avg:52.92ms
step:1639/2110 train_time:86765ms step_avg:52.94ms
step:1640/2110 train_time:86850ms step_avg:52.96ms
step:1641/2110 train_time:86937ms step_avg:52.98ms
step:1642/2110 train_time:87024ms step_avg:53.00ms
step:1643/2110 train_time:87111ms step_avg:53.02ms
step:1644/2110 train_time:87197ms step_avg:53.04ms
step:1645/2110 train_time:87285ms step_avg:53.06ms
step:1646/2110 train_time:87371ms step_avg:53.08ms
step:1647/2110 train_time:87459ms step_avg:53.10ms
step:1648/2110 train_time:87546ms step_avg:53.12ms
step:1649/2110 train_time:87633ms step_avg:53.14ms
step:1650/2110 train_time:87720ms step_avg:53.16ms
step:1651/2110 train_time:87807ms step_avg:53.18ms
step:1652/2110 train_time:87892ms step_avg:53.20ms
step:1653/2110 train_time:87980ms step_avg:53.22ms
step:1654/2110 train_time:88067ms step_avg:53.25ms
step:1655/2110 train_time:88154ms step_avg:53.27ms
step:1656/2110 train_time:88242ms step_avg:53.29ms
step:1657/2110 train_time:88329ms step_avg:53.31ms
step:1658/2110 train_time:88416ms step_avg:53.33ms
step:1659/2110 train_time:88504ms step_avg:53.35ms
step:1660/2110 train_time:88591ms step_avg:53.37ms
step:1661/2110 train_time:88681ms step_avg:53.39ms
step:1662/2110 train_time:88768ms step_avg:53.41ms
step:1663/2110 train_time:88857ms step_avg:53.43ms
step:1664/2110 train_time:88944ms step_avg:53.45ms
step:1665/2110 train_time:89034ms step_avg:53.47ms
step:1666/2110 train_time:89122ms step_avg:53.49ms
step:1667/2110 train_time:89210ms step_avg:53.52ms
step:1668/2110 train_time:89297ms step_avg:53.54ms
step:1669/2110 train_time:89387ms step_avg:53.56ms
step:1670/2110 train_time:89473ms step_avg:53.58ms
step:1671/2110 train_time:89561ms step_avg:53.60ms
step:1672/2110 train_time:89649ms step_avg:53.62ms
step:1673/2110 train_time:89738ms step_avg:53.64ms
step:1674/2110 train_time:89826ms step_avg:53.66ms
step:1675/2110 train_time:89915ms step_avg:53.68ms
step:1676/2110 train_time:90002ms step_avg:53.70ms
step:1677/2110 train_time:90091ms step_avg:53.72ms
step:1678/2110 train_time:90178ms step_avg:53.74ms
step:1679/2110 train_time:90268ms step_avg:53.76ms
step:1680/2110 train_time:90355ms step_avg:53.78ms
step:1681/2110 train_time:90443ms step_avg:53.80ms
step:1682/2110 train_time:90530ms step_avg:53.82ms
step:1683/2110 train_time:90619ms step_avg:53.84ms
step:1684/2110 train_time:90706ms step_avg:53.86ms
step:1685/2110 train_time:90794ms step_avg:53.88ms
step:1686/2110 train_time:90882ms step_avg:53.90ms
step:1687/2110 train_time:90970ms step_avg:53.92ms
step:1688/2110 train_time:91057ms step_avg:53.94ms
step:1689/2110 train_time:91147ms step_avg:53.96ms
step:1690/2110 train_time:91234ms step_avg:53.98ms
step:1691/2110 train_time:91322ms step_avg:54.00ms
step:1692/2110 train_time:91410ms step_avg:54.02ms
step:1693/2110 train_time:91498ms step_avg:54.05ms
step:1694/2110 train_time:91585ms step_avg:54.06ms
step:1695/2110 train_time:91674ms step_avg:54.08ms
step:1696/2110 train_time:91761ms step_avg:54.10ms
step:1697/2110 train_time:91850ms step_avg:54.12ms
step:1698/2110 train_time:91937ms step_avg:54.14ms
step:1699/2110 train_time:92026ms step_avg:54.16ms
step:1700/2110 train_time:92113ms step_avg:54.18ms
step:1701/2110 train_time:92202ms step_avg:54.20ms
step:1702/2110 train_time:92289ms step_avg:54.22ms
step:1703/2110 train_time:92377ms step_avg:54.24ms
step:1704/2110 train_time:92465ms step_avg:54.26ms
step:1705/2110 train_time:92553ms step_avg:54.28ms
step:1706/2110 train_time:92642ms step_avg:54.30ms
step:1707/2110 train_time:92730ms step_avg:54.32ms
step:1708/2110 train_time:92818ms step_avg:54.34ms
step:1709/2110 train_time:92907ms step_avg:54.36ms
step:1710/2110 train_time:92995ms step_avg:54.38ms
step:1711/2110 train_time:93083ms step_avg:54.40ms
step:1712/2110 train_time:93170ms step_avg:54.42ms
step:1713/2110 train_time:93258ms step_avg:54.44ms
step:1714/2110 train_time:93347ms step_avg:54.46ms
step:1715/2110 train_time:93435ms step_avg:54.48ms
step:1716/2110 train_time:93524ms step_avg:54.50ms
step:1717/2110 train_time:93612ms step_avg:54.52ms
step:1718/2110 train_time:93701ms step_avg:54.54ms
step:1719/2110 train_time:93788ms step_avg:54.56ms
step:1720/2110 train_time:93876ms step_avg:54.58ms
step:1721/2110 train_time:93964ms step_avg:54.60ms
step:1722/2110 train_time:94052ms step_avg:54.62ms
step:1723/2110 train_time:94140ms step_avg:54.64ms
step:1724/2110 train_time:94229ms step_avg:54.66ms
step:1725/2110 train_time:94317ms step_avg:54.68ms
step:1726/2110 train_time:94405ms step_avg:54.70ms
step:1727/2110 train_time:94493ms step_avg:54.72ms
step:1728/2110 train_time:94581ms step_avg:54.73ms
step:1729/2110 train_time:94669ms step_avg:54.75ms
step:1730/2110 train_time:94757ms step_avg:54.77ms
step:1731/2110 train_time:94845ms step_avg:54.79ms
step:1732/2110 train_time:94932ms step_avg:54.81ms
step:1733/2110 train_time:95020ms step_avg:54.83ms
step:1734/2110 train_time:95109ms step_avg:54.85ms
step:1735/2110 train_time:95196ms step_avg:54.87ms
step:1736/2110 train_time:95284ms step_avg:54.89ms
step:1737/2110 train_time:95372ms step_avg:54.91ms
step:1738/2110 train_time:95460ms step_avg:54.93ms
step:1739/2110 train_time:95548ms step_avg:54.94ms
step:1740/2110 train_time:95636ms step_avg:54.96ms
step:1741/2110 train_time:95724ms step_avg:54.98ms
step:1742/2110 train_time:95813ms step_avg:55.00ms
step:1743/2110 train_time:95901ms step_avg:55.02ms
step:1744/2110 train_time:95989ms step_avg:55.04ms
step:1745/2110 train_time:96078ms step_avg:55.06ms
step:1746/2110 train_time:96167ms step_avg:55.08ms
step:1747/2110 train_time:96254ms step_avg:55.10ms
step:1748/2110 train_time:96343ms step_avg:55.12ms
step:1749/2110 train_time:96431ms step_avg:55.14ms
step:1750/2110 train_time:96520ms step_avg:55.15ms
step:1750/2110 val_loss:3.3782 train_time:96609ms step_avg:55.21ms
step:1751/2110 train_time:96642ms step_avg:55.19ms
step:1752/2110 train_time:96700ms step_avg:55.19ms
step:1753/2110 train_time:96791ms step_avg:55.21ms
step:1754/2110 train_time:96879ms step_avg:55.23ms
step:1755/2110 train_time:96968ms step_avg:55.25ms
step:1756/2110 train_time:97055ms step_avg:55.27ms
step:1757/2110 train_time:97141ms step_avg:55.29ms
step:1758/2110 train_time:97229ms step_avg:55.31ms
step:1759/2110 train_time:97315ms step_avg:55.32ms
step:1760/2110 train_time:97403ms step_avg:55.34ms
step:1761/2110 train_time:97490ms step_avg:55.36ms
step:1762/2110 train_time:97578ms step_avg:55.38ms
step:1763/2110 train_time:97670ms step_avg:55.40ms
step:1764/2110 train_time:97760ms step_avg:55.42ms
step:1765/2110 train_time:97849ms step_avg:55.44ms
step:1766/2110 train_time:97937ms step_avg:55.46ms
step:1767/2110 train_time:98024ms step_avg:55.48ms
step:1768/2110 train_time:98112ms step_avg:55.49ms
step:1769/2110 train_time:98198ms step_avg:55.51ms
step:1770/2110 train_time:98286ms step_avg:55.53ms
step:1771/2110 train_time:98373ms step_avg:55.55ms
step:1772/2110 train_time:98461ms step_avg:55.56ms
step:1773/2110 train_time:98549ms step_avg:55.58ms
step:1774/2110 train_time:98638ms step_avg:55.60ms
step:1775/2110 train_time:98728ms step_avg:55.62ms
step:1776/2110 train_time:98817ms step_avg:55.64ms
step:1777/2110 train_time:98905ms step_avg:55.66ms
step:1778/2110 train_time:98993ms step_avg:55.68ms
step:1779/2110 train_time:99081ms step_avg:55.69ms
step:1780/2110 train_time:99169ms step_avg:55.71ms
step:1781/2110 train_time:99256ms step_avg:55.73ms
step:1782/2110 train_time:99344ms step_avg:55.75ms
step:1783/2110 train_time:99432ms step_avg:55.77ms
step:1784/2110 train_time:99521ms step_avg:55.79ms
step:1785/2110 train_time:99608ms step_avg:55.80ms
step:1786/2110 train_time:99696ms step_avg:55.82ms
step:1787/2110 train_time:99785ms step_avg:55.84ms
step:1788/2110 train_time:99874ms step_avg:55.86ms
step:1789/2110 train_time:99962ms step_avg:55.88ms
step:1790/2110 train_time:100050ms step_avg:55.89ms
step:1791/2110 train_time:100138ms step_avg:55.91ms
step:1792/2110 train_time:100226ms step_avg:55.93ms
step:1793/2110 train_time:100313ms step_avg:55.95ms
step:1794/2110 train_time:100401ms step_avg:55.96ms
step:1795/2110 train_time:100488ms step_avg:55.98ms
step:1796/2110 train_time:100576ms step_avg:56.00ms
step:1797/2110 train_time:100665ms step_avg:56.02ms
step:1798/2110 train_time:100755ms step_avg:56.04ms
step:1799/2110 train_time:100843ms step_avg:56.06ms
step:1800/2110 train_time:100933ms step_avg:56.07ms
step:1801/2110 train_time:101019ms step_avg:56.09ms
step:1802/2110 train_time:101108ms step_avg:56.11ms
step:1803/2110 train_time:101195ms step_avg:56.13ms
step:1804/2110 train_time:101282ms step_avg:56.14ms
step:1805/2110 train_time:101370ms step_avg:56.16ms
step:1806/2110 train_time:101458ms step_avg:56.18ms
step:1807/2110 train_time:101545ms step_avg:56.20ms
step:1808/2110 train_time:101634ms step_avg:56.21ms
step:1809/2110 train_time:101722ms step_avg:56.23ms
step:1810/2110 train_time:101812ms step_avg:56.25ms
step:1811/2110 train_time:101900ms step_avg:56.27ms
step:1812/2110 train_time:101988ms step_avg:56.28ms
step:1813/2110 train_time:102075ms step_avg:56.30ms
step:1814/2110 train_time:102164ms step_avg:56.32ms
step:1815/2110 train_time:102252ms step_avg:56.34ms
step:1816/2110 train_time:102340ms step_avg:56.35ms
step:1817/2110 train_time:102430ms step_avg:56.37ms
step:1818/2110 train_time:102517ms step_avg:56.39ms
step:1819/2110 train_time:102606ms step_avg:56.41ms
step:1820/2110 train_time:102695ms step_avg:56.43ms
step:1821/2110 train_time:102782ms step_avg:56.44ms
step:1822/2110 train_time:102872ms step_avg:56.46ms
step:1823/2110 train_time:102959ms step_avg:56.48ms
step:1824/2110 train_time:103047ms step_avg:56.50ms
step:1825/2110 train_time:103135ms step_avg:56.51ms
step:1826/2110 train_time:103223ms step_avg:56.53ms
step:1827/2110 train_time:103311ms step_avg:56.55ms
step:1828/2110 train_time:103398ms step_avg:56.56ms
step:1829/2110 train_time:103488ms step_avg:56.58ms
step:1830/2110 train_time:103575ms step_avg:56.60ms
step:1831/2110 train_time:103664ms step_avg:56.62ms
step:1832/2110 train_time:103753ms step_avg:56.63ms
step:1833/2110 train_time:103841ms step_avg:56.65ms
step:1834/2110 train_time:103930ms step_avg:56.67ms
step:1835/2110 train_time:104016ms step_avg:56.68ms
step:1836/2110 train_time:104104ms step_avg:56.70ms
step:1837/2110 train_time:104193ms step_avg:56.72ms
step:1838/2110 train_time:104280ms step_avg:56.74ms
step:1839/2110 train_time:104367ms step_avg:56.75ms
step:1840/2110 train_time:104456ms step_avg:56.77ms
step:1841/2110 train_time:104543ms step_avg:56.79ms
step:1842/2110 train_time:104633ms step_avg:56.80ms
step:1843/2110 train_time:104720ms step_avg:56.82ms
step:1844/2110 train_time:104809ms step_avg:56.84ms
step:1845/2110 train_time:104897ms step_avg:56.85ms
step:1846/2110 train_time:104985ms step_avg:56.87ms
step:1847/2110 train_time:105073ms step_avg:56.89ms
step:1848/2110 train_time:105161ms step_avg:56.91ms
step:1849/2110 train_time:105248ms step_avg:56.92ms
step:1850/2110 train_time:105337ms step_avg:56.94ms
step:1851/2110 train_time:105424ms step_avg:56.96ms
step:1852/2110 train_time:105512ms step_avg:56.97ms
step:1853/2110 train_time:105601ms step_avg:56.99ms
step:1854/2110 train_time:105689ms step_avg:57.01ms
step:1855/2110 train_time:105777ms step_avg:57.02ms
step:1856/2110 train_time:105866ms step_avg:57.04ms
step:1857/2110 train_time:105954ms step_avg:57.06ms
step:1858/2110 train_time:106044ms step_avg:57.07ms
step:1859/2110 train_time:106132ms step_avg:57.09ms
step:1860/2110 train_time:106221ms step_avg:57.11ms
step:1861/2110 train_time:106308ms step_avg:57.12ms
step:1862/2110 train_time:106395ms step_avg:57.14ms
step:1863/2110 train_time:106483ms step_avg:57.16ms
step:1864/2110 train_time:106572ms step_avg:57.17ms
step:1865/2110 train_time:106659ms step_avg:57.19ms
step:1866/2110 train_time:106748ms step_avg:57.21ms
step:1867/2110 train_time:106835ms step_avg:57.22ms
step:1868/2110 train_time:106924ms step_avg:57.24ms
step:1869/2110 train_time:107013ms step_avg:57.26ms
step:1870/2110 train_time:107102ms step_avg:57.27ms
step:1871/2110 train_time:107190ms step_avg:57.29ms
step:1872/2110 train_time:107277ms step_avg:57.31ms
step:1873/2110 train_time:107365ms step_avg:57.32ms
step:1874/2110 train_time:107454ms step_avg:57.34ms
step:1875/2110 train_time:107541ms step_avg:57.36ms
step:1876/2110 train_time:107630ms step_avg:57.37ms
step:1877/2110 train_time:107717ms step_avg:57.39ms
step:1878/2110 train_time:107805ms step_avg:57.40ms
step:1879/2110 train_time:107893ms step_avg:57.42ms
step:1880/2110 train_time:107981ms step_avg:57.44ms
step:1881/2110 train_time:108069ms step_avg:57.45ms
step:1882/2110 train_time:108157ms step_avg:57.47ms
step:1883/2110 train_time:108244ms step_avg:57.48ms
step:1884/2110 train_time:108332ms step_avg:57.50ms
step:1885/2110 train_time:108420ms step_avg:57.52ms
step:1886/2110 train_time:108508ms step_avg:57.53ms
step:1887/2110 train_time:108595ms step_avg:57.55ms
step:1888/2110 train_time:108684ms step_avg:57.57ms
step:1889/2110 train_time:108772ms step_avg:57.58ms
step:1890/2110 train_time:108861ms step_avg:57.60ms
step:1891/2110 train_time:108948ms step_avg:57.61ms
step:1892/2110 train_time:109036ms step_avg:57.63ms
step:1893/2110 train_time:109123ms step_avg:57.65ms
step:1894/2110 train_time:109212ms step_avg:57.66ms
step:1895/2110 train_time:109300ms step_avg:57.68ms
step:1896/2110 train_time:109389ms step_avg:57.69ms
step:1897/2110 train_time:109476ms step_avg:57.71ms
step:1898/2110 train_time:109565ms step_avg:57.73ms
step:1899/2110 train_time:109653ms step_avg:57.74ms
step:1900/2110 train_time:109742ms step_avg:57.76ms
step:1901/2110 train_time:109830ms step_avg:57.77ms
step:1902/2110 train_time:109917ms step_avg:57.79ms
step:1903/2110 train_time:110005ms step_avg:57.81ms
step:1904/2110 train_time:110092ms step_avg:57.82ms
step:1905/2110 train_time:110180ms step_avg:57.84ms
step:1906/2110 train_time:110269ms step_avg:57.85ms
step:1907/2110 train_time:110356ms step_avg:57.87ms
step:1908/2110 train_time:110445ms step_avg:57.89ms
step:1909/2110 train_time:110532ms step_avg:57.90ms
step:1910/2110 train_time:110621ms step_avg:57.92ms
step:1911/2110 train_time:110709ms step_avg:57.93ms
step:1912/2110 train_time:110798ms step_avg:57.95ms
step:1913/2110 train_time:110885ms step_avg:57.96ms
step:1914/2110 train_time:110974ms step_avg:57.98ms
step:1915/2110 train_time:111061ms step_avg:58.00ms
step:1916/2110 train_time:111150ms step_avg:58.01ms
step:1917/2110 train_time:111237ms step_avg:58.03ms
step:1918/2110 train_time:111326ms step_avg:58.04ms
step:1919/2110 train_time:111413ms step_avg:58.06ms
step:1920/2110 train_time:111502ms step_avg:58.07ms
step:1921/2110 train_time:111591ms step_avg:58.09ms
step:1922/2110 train_time:111678ms step_avg:58.11ms
step:1923/2110 train_time:111767ms step_avg:58.12ms
step:1924/2110 train_time:111856ms step_avg:58.14ms
step:1925/2110 train_time:111943ms step_avg:58.15ms
step:1926/2110 train_time:112032ms step_avg:58.17ms
step:1927/2110 train_time:112120ms step_avg:58.18ms
step:1928/2110 train_time:112209ms step_avg:58.20ms
step:1929/2110 train_time:112296ms step_avg:58.21ms
step:1930/2110 train_time:112384ms step_avg:58.23ms
step:1931/2110 train_time:112472ms step_avg:58.25ms
step:1932/2110 train_time:112561ms step_avg:58.26ms
step:1933/2110 train_time:112649ms step_avg:58.28ms
step:1934/2110 train_time:112737ms step_avg:58.29ms
step:1935/2110 train_time:112825ms step_avg:58.31ms
step:1936/2110 train_time:112914ms step_avg:58.32ms
step:1937/2110 train_time:113002ms step_avg:58.34ms
step:1938/2110 train_time:113090ms step_avg:58.35ms
step:1939/2110 train_time:113177ms step_avg:58.37ms
step:1940/2110 train_time:113266ms step_avg:58.38ms
step:1941/2110 train_time:113354ms step_avg:58.40ms
step:1942/2110 train_time:113442ms step_avg:58.42ms
step:1943/2110 train_time:113532ms step_avg:58.43ms
step:1944/2110 train_time:113620ms step_avg:58.45ms
step:1945/2110 train_time:113709ms step_avg:58.46ms
step:1946/2110 train_time:113796ms step_avg:58.48ms
step:1947/2110 train_time:113884ms step_avg:58.49ms
step:1948/2110 train_time:113973ms step_avg:58.51ms
step:1949/2110 train_time:114060ms step_avg:58.52ms
step:1950/2110 train_time:114149ms step_avg:58.54ms
step:1951/2110 train_time:114236ms step_avg:58.55ms
step:1952/2110 train_time:114325ms step_avg:58.57ms
step:1953/2110 train_time:114412ms step_avg:58.58ms
step:1954/2110 train_time:114501ms step_avg:58.60ms
step:1955/2110 train_time:114590ms step_avg:58.61ms
step:1956/2110 train_time:114680ms step_avg:58.63ms
step:1957/2110 train_time:114766ms step_avg:58.64ms
step:1958/2110 train_time:114854ms step_avg:58.66ms
step:1959/2110 train_time:114942ms step_avg:58.67ms
step:1960/2110 train_time:115030ms step_avg:58.69ms
step:1961/2110 train_time:115117ms step_avg:58.70ms
step:1962/2110 train_time:115205ms step_avg:58.72ms
step:1963/2110 train_time:115293ms step_avg:58.73ms
step:1964/2110 train_time:115381ms step_avg:58.75ms
step:1965/2110 train_time:115469ms step_avg:58.76ms
step:1966/2110 train_time:115556ms step_avg:58.78ms
step:1967/2110 train_time:115645ms step_avg:58.79ms
step:1968/2110 train_time:115733ms step_avg:58.81ms
step:1969/2110 train_time:115821ms step_avg:58.82ms
step:1970/2110 train_time:115910ms step_avg:58.84ms
step:1971/2110 train_time:115998ms step_avg:58.85ms
step:1972/2110 train_time:116086ms step_avg:58.87ms
step:1973/2110 train_time:116173ms step_avg:58.88ms
step:1974/2110 train_time:116261ms step_avg:58.90ms
step:1975/2110 train_time:116349ms step_avg:58.91ms
step:1976/2110 train_time:116436ms step_avg:58.93ms
step:1977/2110 train_time:116525ms step_avg:58.94ms
step:1978/2110 train_time:116613ms step_avg:58.95ms
step:1979/2110 train_time:116702ms step_avg:58.97ms
step:1980/2110 train_time:116790ms step_avg:58.98ms
step:1981/2110 train_time:116877ms step_avg:59.00ms
step:1982/2110 train_time:116966ms step_avg:59.01ms
step:1983/2110 train_time:117055ms step_avg:59.03ms
step:1984/2110 train_time:117142ms step_avg:59.04ms
step:1985/2110 train_time:117230ms step_avg:59.06ms
step:1986/2110 train_time:117317ms step_avg:59.07ms
step:1987/2110 train_time:117405ms step_avg:59.09ms
step:1988/2110 train_time:117493ms step_avg:59.10ms
step:1989/2110 train_time:117580ms step_avg:59.12ms
step:1990/2110 train_time:117669ms step_avg:59.13ms
step:1991/2110 train_time:117757ms step_avg:59.14ms
step:1992/2110 train_time:117845ms step_avg:59.16ms
step:1993/2110 train_time:117933ms step_avg:59.17ms
step:1994/2110 train_time:118021ms step_avg:59.19ms
step:1995/2110 train_time:118109ms step_avg:59.20ms
step:1996/2110 train_time:118196ms step_avg:59.22ms
step:1997/2110 train_time:118285ms step_avg:59.23ms
step:1998/2110 train_time:118373ms step_avg:59.25ms
step:1999/2110 train_time:118462ms step_avg:59.26ms
step:2000/2110 train_time:118550ms step_avg:59.28ms
step:2000/2110 val_loss:3.3022 train_time:118640ms step_avg:59.32ms
step:2001/2110 train_time:118674ms step_avg:59.31ms
step:2002/2110 train_time:118734ms step_avg:59.31ms
step:2003/2110 train_time:118828ms step_avg:59.32ms
step:2004/2110 train_time:118917ms step_avg:59.34ms
step:2005/2110 train_time:119005ms step_avg:59.35ms
step:2006/2110 train_time:119092ms step_avg:59.37ms
step:2007/2110 train_time:119179ms step_avg:59.38ms
step:2008/2110 train_time:119266ms step_avg:59.40ms
step:2009/2110 train_time:119352ms step_avg:59.41ms
step:2010/2110 train_time:119440ms step_avg:59.42ms
step:2011/2110 train_time:119527ms step_avg:59.44ms
step:2012/2110 train_time:119615ms step_avg:59.45ms
step:2013/2110 train_time:119707ms step_avg:59.47ms
step:2014/2110 train_time:119798ms step_avg:59.48ms
step:2015/2110 train_time:119886ms step_avg:59.50ms
step:2016/2110 train_time:119975ms step_avg:59.51ms
step:2017/2110 train_time:120063ms step_avg:59.53ms
step:2018/2110 train_time:120151ms step_avg:59.54ms
step:2019/2110 train_time:120238ms step_avg:59.55ms
step:2020/2110 train_time:120325ms step_avg:59.57ms
step:2021/2110 train_time:120412ms step_avg:59.58ms
step:2022/2110 train_time:120500ms step_avg:59.59ms
step:2023/2110 train_time:120587ms step_avg:59.61ms
step:2024/2110 train_time:120678ms step_avg:59.62ms
step:2025/2110 train_time:120766ms step_avg:59.64ms
step:2026/2110 train_time:120857ms step_avg:59.65ms
step:2027/2110 train_time:120945ms step_avg:59.67ms
step:2028/2110 train_time:121034ms step_avg:59.68ms
step:2029/2110 train_time:121123ms step_avg:59.70ms
step:2030/2110 train_time:121211ms step_avg:59.71ms
step:2031/2110 train_time:121299ms step_avg:59.72ms
step:2032/2110 train_time:121386ms step_avg:59.74ms
step:2033/2110 train_time:121473ms step_avg:59.75ms
step:2034/2110 train_time:121561ms step_avg:59.76ms
step:2035/2110 train_time:121650ms step_avg:59.78ms
step:2036/2110 train_time:121739ms step_avg:59.79ms
step:2037/2110 train_time:121827ms step_avg:59.81ms
step:2038/2110 train_time:121916ms step_avg:59.82ms
step:2039/2110 train_time:122004ms step_avg:59.84ms
step:2040/2110 train_time:122093ms step_avg:59.85ms
step:2041/2110 train_time:122181ms step_avg:59.86ms
step:2042/2110 train_time:122268ms step_avg:59.88ms
step:2043/2110 train_time:122355ms step_avg:59.89ms
step:2044/2110 train_time:122443ms step_avg:59.90ms
step:2045/2110 train_time:122529ms step_avg:59.92ms
step:2046/2110 train_time:122618ms step_avg:59.93ms
step:2047/2110 train_time:122706ms step_avg:59.94ms
step:2048/2110 train_time:122795ms step_avg:59.96ms
step:2049/2110 train_time:122884ms step_avg:59.97ms
step:2050/2110 train_time:122973ms step_avg:59.99ms
step:2051/2110 train_time:123062ms step_avg:60.00ms
step:2052/2110 train_time:123151ms step_avg:60.02ms
step:2053/2110 train_time:123238ms step_avg:60.03ms
step:2054/2110 train_time:123326ms step_avg:60.04ms
step:2055/2110 train_time:123414ms step_avg:60.06ms
step:2056/2110 train_time:123501ms step_avg:60.07ms
step:2057/2110 train_time:123588ms step_avg:60.08ms
step:2058/2110 train_time:123677ms step_avg:60.10ms
step:2059/2110 train_time:123766ms step_avg:60.11ms
step:2060/2110 train_time:123854ms step_avg:60.12ms
step:2061/2110 train_time:123942ms step_avg:60.14ms
step:2062/2110 train_time:124032ms step_avg:60.15ms
step:2063/2110 train_time:124120ms step_avg:60.16ms
step:2064/2110 train_time:124208ms step_avg:60.18ms
step:2065/2110 train_time:124295ms step_avg:60.19ms
step:2066/2110 train_time:124383ms step_avg:60.20ms
step:2067/2110 train_time:124471ms step_avg:60.22ms
step:2068/2110 train_time:124560ms step_avg:60.23ms
step:2069/2110 train_time:124648ms step_avg:60.25ms
step:2070/2110 train_time:124738ms step_avg:60.26ms
step:2071/2110 train_time:124826ms step_avg:60.27ms
step:2072/2110 train_time:124915ms step_avg:60.29ms
step:2073/2110 train_time:125004ms step_avg:60.30ms
step:2074/2110 train_time:125093ms step_avg:60.31ms
step:2075/2110 train_time:125181ms step_avg:60.33ms
step:2076/2110 train_time:125270ms step_avg:60.34ms
step:2077/2110 train_time:125358ms step_avg:60.36ms
step:2078/2110 train_time:125446ms step_avg:60.37ms
step:2079/2110 train_time:125534ms step_avg:60.38ms
step:2080/2110 train_time:125622ms step_avg:60.40ms
step:2081/2110 train_time:125711ms step_avg:60.41ms
step:2082/2110 train_time:125829ms step_avg:60.44ms
step:2083/2110 train_time:125903ms step_avg:60.44ms
step:2084/2110 train_time:125977ms step_avg:60.45ms
step:2085/2110 train_time:126075ms step_avg:60.47ms
step:2086/2110 train_time:126155ms step_avg:60.48ms
step:2087/2110 train_time:126243ms step_avg:60.49ms
step:2088/2110 train_time:126332ms step_avg:60.50ms
step:2089/2110 train_time:126420ms step_avg:60.52ms
step:2090/2110 train_time:126508ms step_avg:60.53ms
step:2091/2110 train_time:126596ms step_avg:60.54ms
step:2092/2110 train_time:126685ms step_avg:60.56ms
step:2093/2110 train_time:126774ms step_avg:60.57ms
step:2094/2110 train_time:126863ms step_avg:60.58ms
step:2095/2110 train_time:126952ms step_avg:60.60ms
step:2096/2110 train_time:127040ms step_avg:60.61ms
step:2097/2110 train_time:127128ms step_avg:60.62ms
step:2098/2110 train_time:127216ms step_avg:60.64ms
step:2099/2110 train_time:127305ms step_avg:60.65ms
step:2100/2110 train_time:127394ms step_avg:60.66ms
step:2101/2110 train_time:127483ms step_avg:60.68ms
step:2102/2110 train_time:127571ms step_avg:60.69ms
step:2103/2110 train_time:127659ms step_avg:60.70ms
step:2104/2110 train_time:127747ms step_avg:60.72ms
step:2105/2110 train_time:127836ms step_avg:60.73ms
step:2106/2110 train_time:127925ms step_avg:60.74ms
step:2107/2110 train_time:128012ms step_avg:60.76ms
step:2108/2110 train_time:128100ms step_avg:60.77ms
step:2109/2110 train_time:128188ms step_avg:60.78ms
step:2110/2110 train_time:128276ms step_avg:60.79ms
step:2110/2110 val_loss:3.2779 train_time:128366ms step_avg:60.84ms
peak memory allocated: 29892 MiB reserved: 44636 MiB

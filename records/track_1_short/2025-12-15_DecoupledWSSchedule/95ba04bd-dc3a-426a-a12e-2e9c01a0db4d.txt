import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Tue Dec 16 00:41:04 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   38C    P0            126W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   42C    P0            123W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            120W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   42C    P0            126W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   37C    P0            120W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              88      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              88      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:106ms step_avg:105.52ms
step:2/2110 train_time:146ms step_avg:73.09ms
step:3/2110 train_time:181ms step_avg:60.31ms
step:4/2110 train_time:219ms step_avg:54.68ms
step:5/2110 train_time:256ms step_avg:51.27ms
step:6/2110 train_time:438ms step_avg:73.00ms
step:7/2110 train_time:724ms step_avg:103.42ms
step:8/2110 train_time:760ms step_avg:95.00ms
step:9/2110 train_time:789ms step_avg:87.70ms
step:10/2110 train_time:824ms step_avg:82.39ms
step:11/2110 train_time:856ms step_avg:77.82ms
step:12/2110 train_time:890ms step_avg:74.14ms
step:13/2110 train_time:922ms step_avg:70.91ms
step:14/2110 train_time:957ms step_avg:68.32ms
step:15/2110 train_time:987ms step_avg:65.83ms
step:16/2110 train_time:1023ms step_avg:63.92ms
step:17/2110 train_time:1055ms step_avg:62.04ms
step:18/2110 train_time:1088ms step_avg:60.47ms
step:19/2110 train_time:1119ms step_avg:58.88ms
step:20/2110 train_time:1154ms step_avg:57.72ms
step:21/2110 train_time:1185ms step_avg:56.44ms
step:22/2110 train_time:1220ms step_avg:55.43ms
step:23/2110 train_time:1251ms step_avg:54.38ms
step:24/2110 train_time:1284ms step_avg:53.51ms
step:25/2110 train_time:1318ms step_avg:52.71ms
step:26/2110 train_time:1354ms step_avg:52.07ms
step:27/2110 train_time:1397ms step_avg:51.73ms
step:28/2110 train_time:1439ms step_avg:51.41ms
step:29/2110 train_time:1478ms step_avg:50.95ms
step:30/2110 train_time:1519ms step_avg:50.62ms
step:31/2110 train_time:1553ms step_avg:50.10ms
step:32/2110 train_time:1592ms step_avg:49.76ms
step:33/2110 train_time:1623ms step_avg:49.17ms
step:34/2110 train_time:1656ms step_avg:48.70ms
step:35/2110 train_time:1686ms step_avg:48.18ms
step:36/2110 train_time:1720ms step_avg:47.77ms
step:37/2110 train_time:1752ms step_avg:47.34ms
step:38/2110 train_time:1784ms step_avg:46.96ms
step:39/2110 train_time:1816ms step_avg:46.57ms
step:40/2110 train_time:1851ms step_avg:46.29ms
step:41/2110 train_time:1884ms step_avg:45.96ms
step:42/2110 train_time:1918ms step_avg:45.66ms
step:43/2110 train_time:1948ms step_avg:45.30ms
step:44/2110 train_time:1984ms step_avg:45.09ms
step:45/2110 train_time:2014ms step_avg:44.76ms
step:46/2110 train_time:2050ms step_avg:44.56ms
step:47/2110 train_time:2082ms step_avg:44.30ms
step:48/2110 train_time:2116ms step_avg:44.09ms
step:49/2110 train_time:2149ms step_avg:43.85ms
step:50/2110 train_time:2185ms step_avg:43.70ms
step:51/2110 train_time:2218ms step_avg:43.49ms
step:52/2110 train_time:2251ms step_avg:43.28ms
step:53/2110 train_time:2282ms step_avg:43.07ms
step:54/2110 train_time:2320ms step_avg:42.97ms
step:55/2110 train_time:2353ms step_avg:42.78ms
step:56/2110 train_time:2387ms step_avg:42.62ms
step:57/2110 train_time:2417ms step_avg:42.40ms
step:58/2110 train_time:2451ms step_avg:42.26ms
step:59/2110 train_time:2482ms step_avg:42.06ms
step:60/2110 train_time:2514ms step_avg:41.90ms
step:61/2110 train_time:2545ms step_avg:41.72ms
step:62/2110 train_time:2580ms step_avg:41.61ms
step:63/2110 train_time:2612ms step_avg:41.45ms
step:64/2110 train_time:2646ms step_avg:41.35ms
step:65/2110 train_time:2678ms step_avg:41.20ms
step:66/2110 train_time:2713ms step_avg:41.11ms
step:67/2110 train_time:2746ms step_avg:40.99ms
step:68/2110 train_time:2783ms step_avg:40.93ms
step:69/2110 train_time:2819ms step_avg:40.86ms
step:70/2110 train_time:2862ms step_avg:40.88ms
step:71/2110 train_time:2900ms step_avg:40.85ms
step:72/2110 train_time:2943ms step_avg:40.87ms
step:73/2110 train_time:2986ms step_avg:40.90ms
step:74/2110 train_time:3027ms step_avg:40.90ms
step:75/2110 train_time:3061ms step_avg:40.82ms
step:76/2110 train_time:3100ms step_avg:40.80ms
step:77/2110 train_time:3142ms step_avg:40.81ms
step:78/2110 train_time:3190ms step_avg:40.90ms
step:79/2110 train_time:3230ms step_avg:40.88ms
step:80/2110 train_time:3268ms step_avg:40.84ms
step:81/2110 train_time:3301ms step_avg:40.76ms
step:82/2110 train_time:3338ms step_avg:40.71ms
step:83/2110 train_time:3376ms step_avg:40.67ms
step:84/2110 train_time:3418ms step_avg:40.69ms
step:85/2110 train_time:3455ms step_avg:40.65ms
step:86/2110 train_time:3491ms step_avg:40.59ms
step:87/2110 train_time:3525ms step_avg:40.52ms
step:88/2110 train_time:3560ms step_avg:40.45ms
step:89/2110 train_time:3592ms step_avg:40.36ms
step:90/2110 train_time:3627ms step_avg:40.30ms
step:91/2110 train_time:3664ms step_avg:40.26ms
step:92/2110 train_time:3707ms step_avg:40.29ms
step:93/2110 train_time:3746ms step_avg:40.28ms
step:94/2110 train_time:3783ms step_avg:40.25ms
step:95/2110 train_time:3816ms step_avg:40.17ms
step:96/2110 train_time:3850ms step_avg:40.11ms
step:97/2110 train_time:3883ms step_avg:40.04ms
step:98/2110 train_time:3924ms step_avg:40.04ms
step:99/2110 train_time:3965ms step_avg:40.05ms
step:100/2110 train_time:4005ms step_avg:40.05ms
step:101/2110 train_time:4039ms step_avg:39.99ms
step:102/2110 train_time:4067ms step_avg:39.87ms
step:103/2110 train_time:4094ms step_avg:39.75ms
step:104/2110 train_time:4121ms step_avg:39.62ms
step:105/2110 train_time:4147ms step_avg:39.50ms
step:106/2110 train_time:4178ms step_avg:39.42ms
step:107/2110 train_time:4209ms step_avg:39.34ms
step:108/2110 train_time:4238ms step_avg:39.24ms
step:109/2110 train_time:4269ms step_avg:39.16ms
step:110/2110 train_time:4303ms step_avg:39.11ms
step:111/2110 train_time:4335ms step_avg:39.05ms
step:112/2110 train_time:4367ms step_avg:38.99ms
step:113/2110 train_time:4400ms step_avg:38.94ms
step:114/2110 train_time:4433ms step_avg:38.88ms
step:115/2110 train_time:4466ms step_avg:38.84ms
step:116/2110 train_time:4498ms step_avg:38.78ms
step:117/2110 train_time:4534ms step_avg:38.75ms
step:118/2110 train_time:4567ms step_avg:38.71ms
step:119/2110 train_time:4605ms step_avg:38.70ms
step:120/2110 train_time:4639ms step_avg:38.66ms
step:121/2110 train_time:4677ms step_avg:38.65ms
step:122/2110 train_time:4710ms step_avg:38.61ms
step:123/2110 train_time:4747ms step_avg:38.59ms
step:124/2110 train_time:4779ms step_avg:38.54ms
step:125/2110 train_time:4813ms step_avg:38.50ms
step:126/2110 train_time:4845ms step_avg:38.45ms
step:127/2110 train_time:4878ms step_avg:38.41ms
step:128/2110 train_time:4911ms step_avg:38.37ms
step:129/2110 train_time:4944ms step_avg:38.33ms
step:130/2110 train_time:4977ms step_avg:38.28ms
step:131/2110 train_time:5013ms step_avg:38.27ms
step:132/2110 train_time:5046ms step_avg:38.23ms
step:133/2110 train_time:5080ms step_avg:38.20ms
step:134/2110 train_time:5113ms step_avg:38.16ms
step:135/2110 train_time:5146ms step_avg:38.12ms
step:136/2110 train_time:5178ms step_avg:38.08ms
step:137/2110 train_time:5212ms step_avg:38.04ms
step:138/2110 train_time:5244ms step_avg:38.00ms
step:139/2110 train_time:5278ms step_avg:37.97ms
step:140/2110 train_time:5312ms step_avg:37.94ms
step:141/2110 train_time:5344ms step_avg:37.90ms
step:142/2110 train_time:5377ms step_avg:37.86ms
step:143/2110 train_time:5410ms step_avg:37.83ms
step:144/2110 train_time:5442ms step_avg:37.79ms
step:145/2110 train_time:5475ms step_avg:37.76ms
step:146/2110 train_time:5508ms step_avg:37.72ms
step:147/2110 train_time:5541ms step_avg:37.70ms
step:148/2110 train_time:5574ms step_avg:37.67ms
step:149/2110 train_time:5608ms step_avg:37.64ms
step:150/2110 train_time:5641ms step_avg:37.61ms
step:151/2110 train_time:5674ms step_avg:37.58ms
step:152/2110 train_time:5708ms step_avg:37.55ms
step:153/2110 train_time:5740ms step_avg:37.52ms
step:154/2110 train_time:5773ms step_avg:37.49ms
step:155/2110 train_time:5806ms step_avg:37.46ms
step:156/2110 train_time:5839ms step_avg:37.43ms
step:157/2110 train_time:5873ms step_avg:37.41ms
step:158/2110 train_time:5905ms step_avg:37.38ms
step:159/2110 train_time:5939ms step_avg:37.35ms
step:160/2110 train_time:5972ms step_avg:37.33ms
step:161/2110 train_time:6005ms step_avg:37.30ms
step:162/2110 train_time:6038ms step_avg:37.27ms
step:163/2110 train_time:6071ms step_avg:37.25ms
step:164/2110 train_time:6104ms step_avg:37.22ms
step:165/2110 train_time:6137ms step_avg:37.20ms
step:166/2110 train_time:6171ms step_avg:37.17ms
step:167/2110 train_time:6203ms step_avg:37.14ms
step:168/2110 train_time:6239ms step_avg:37.14ms
step:169/2110 train_time:6276ms step_avg:37.14ms
step:170/2110 train_time:6311ms step_avg:37.12ms
step:171/2110 train_time:6347ms step_avg:37.12ms
step:172/2110 train_time:6382ms step_avg:37.10ms
step:173/2110 train_time:6420ms step_avg:37.11ms
step:174/2110 train_time:6454ms step_avg:37.09ms
step:175/2110 train_time:6488ms step_avg:37.08ms
step:176/2110 train_time:6521ms step_avg:37.05ms
step:177/2110 train_time:6558ms step_avg:37.05ms
step:178/2110 train_time:6592ms step_avg:37.04ms
step:179/2110 train_time:6629ms step_avg:37.04ms
step:180/2110 train_time:6664ms step_avg:37.02ms
step:181/2110 train_time:6704ms step_avg:37.04ms
step:182/2110 train_time:6740ms step_avg:37.03ms
step:183/2110 train_time:6774ms step_avg:37.01ms
step:184/2110 train_time:6806ms step_avg:36.99ms
step:185/2110 train_time:6840ms step_avg:36.97ms
step:186/2110 train_time:6875ms step_avg:36.96ms
step:187/2110 train_time:6913ms step_avg:36.97ms
step:188/2110 train_time:6949ms step_avg:36.96ms
step:189/2110 train_time:6986ms step_avg:36.96ms
step:190/2110 train_time:7021ms step_avg:36.95ms
step:191/2110 train_time:7060ms step_avg:36.96ms
step:192/2110 train_time:7094ms step_avg:36.95ms
step:193/2110 train_time:7129ms step_avg:36.94ms
step:194/2110 train_time:7166ms step_avg:36.94ms
step:195/2110 train_time:7202ms step_avg:36.93ms
step:196/2110 train_time:7238ms step_avg:36.93ms
step:197/2110 train_time:7275ms step_avg:36.93ms
step:198/2110 train_time:7311ms step_avg:36.93ms
step:199/2110 train_time:7348ms step_avg:36.92ms
step:200/2110 train_time:7383ms step_avg:36.91ms
step:201/2110 train_time:7419ms step_avg:36.91ms
step:202/2110 train_time:7453ms step_avg:36.90ms
step:203/2110 train_time:7490ms step_avg:36.90ms
step:204/2110 train_time:7524ms step_avg:36.88ms
step:205/2110 train_time:7561ms step_avg:36.88ms
step:206/2110 train_time:7596ms step_avg:36.88ms
step:207/2110 train_time:7635ms step_avg:36.88ms
step:208/2110 train_time:7668ms step_avg:36.86ms
step:209/2110 train_time:7706ms step_avg:36.87ms
step:210/2110 train_time:7740ms step_avg:36.86ms
step:211/2110 train_time:7778ms step_avg:36.86ms
step:212/2110 train_time:7812ms step_avg:36.85ms
step:213/2110 train_time:7848ms step_avg:36.84ms
step:214/2110 train_time:7881ms step_avg:36.83ms
step:215/2110 train_time:7916ms step_avg:36.82ms
step:216/2110 train_time:7950ms step_avg:36.81ms
step:217/2110 train_time:7987ms step_avg:36.80ms
step:218/2110 train_time:8032ms step_avg:36.84ms
step:219/2110 train_time:8073ms step_avg:36.86ms
step:220/2110 train_time:8115ms step_avg:36.89ms
step:221/2110 train_time:8149ms step_avg:36.88ms
step:222/2110 train_time:8183ms step_avg:36.86ms
step:223/2110 train_time:8213ms step_avg:36.83ms
step:224/2110 train_time:8247ms step_avg:36.82ms
step:225/2110 train_time:8279ms step_avg:36.79ms
step:226/2110 train_time:8306ms step_avg:36.75ms
step:227/2110 train_time:8329ms step_avg:36.69ms
step:228/2110 train_time:8355ms step_avg:36.65ms
step:229/2110 train_time:8390ms step_avg:36.64ms
step:230/2110 train_time:8424ms step_avg:36.63ms
step:231/2110 train_time:8461ms step_avg:36.63ms
step:232/2110 train_time:8496ms step_avg:36.62ms
step:233/2110 train_time:8533ms step_avg:36.62ms
step:234/2110 train_time:8567ms step_avg:36.61ms
step:235/2110 train_time:8606ms step_avg:36.62ms
step:236/2110 train_time:8641ms step_avg:36.62ms
step:237/2110 train_time:8677ms step_avg:36.61ms
step:238/2110 train_time:8711ms step_avg:36.60ms
step:239/2110 train_time:8748ms step_avg:36.60ms
step:240/2110 train_time:8783ms step_avg:36.59ms
step:241/2110 train_time:8822ms step_avg:36.61ms
step:242/2110 train_time:8857ms step_avg:36.60ms
step:243/2110 train_time:8895ms step_avg:36.61ms
step:244/2110 train_time:8929ms step_avg:36.60ms
step:245/2110 train_time:8968ms step_avg:36.60ms
step:246/2110 train_time:9003ms step_avg:36.60ms
step:247/2110 train_time:9039ms step_avg:36.60ms
step:248/2110 train_time:9074ms step_avg:36.59ms
step:249/2110 train_time:9110ms step_avg:36.58ms
step:250/2110 train_time:9145ms step_avg:36.58ms
step:250/2110 val_loss:4.3049 train_time:9185ms step_avg:36.74ms
step:251/2110 train_time:9211ms step_avg:36.70ms
step:252/2110 train_time:9236ms step_avg:36.65ms
step:253/2110 train_time:9259ms step_avg:36.60ms
step:254/2110 train_time:9291ms step_avg:36.58ms
step:255/2110 train_time:9329ms step_avg:36.58ms
step:256/2110 train_time:9363ms step_avg:36.57ms
step:257/2110 train_time:9401ms step_avg:36.58ms
step:258/2110 train_time:9437ms step_avg:36.58ms
step:259/2110 train_time:9475ms step_avg:36.58ms
step:260/2110 train_time:9510ms step_avg:36.58ms
step:261/2110 train_time:9547ms step_avg:36.58ms
step:262/2110 train_time:9580ms step_avg:36.57ms
step:263/2110 train_time:9617ms step_avg:36.57ms
step:264/2110 train_time:9650ms step_avg:36.55ms
step:265/2110 train_time:9688ms step_avg:36.56ms
step:266/2110 train_time:9722ms step_avg:36.55ms
step:267/2110 train_time:9759ms step_avg:36.55ms
step:268/2110 train_time:9794ms step_avg:36.55ms
step:269/2110 train_time:9832ms step_avg:36.55ms
step:270/2110 train_time:9867ms step_avg:36.54ms
step:271/2110 train_time:9903ms step_avg:36.54ms
step:272/2110 train_time:9936ms step_avg:36.53ms
step:273/2110 train_time:9976ms step_avg:36.54ms
step:274/2110 train_time:10008ms step_avg:36.53ms
step:275/2110 train_time:10042ms step_avg:36.52ms
step:276/2110 train_time:10076ms step_avg:36.51ms
step:277/2110 train_time:10109ms step_avg:36.49ms
step:278/2110 train_time:10142ms step_avg:36.48ms
step:279/2110 train_time:10179ms step_avg:36.48ms
step:280/2110 train_time:10214ms step_avg:36.48ms
step:281/2110 train_time:10251ms step_avg:36.48ms
step:282/2110 train_time:10285ms step_avg:36.47ms
step:283/2110 train_time:10321ms step_avg:36.47ms
step:284/2110 train_time:10355ms step_avg:36.46ms
step:285/2110 train_time:10392ms step_avg:36.46ms
step:286/2110 train_time:10427ms step_avg:36.46ms
step:287/2110 train_time:10465ms step_avg:36.46ms
step:288/2110 train_time:10499ms step_avg:36.46ms
step:289/2110 train_time:10537ms step_avg:36.46ms
step:290/2110 train_time:10572ms step_avg:36.46ms
step:291/2110 train_time:10610ms step_avg:36.46ms
step:292/2110 train_time:10645ms step_avg:36.45ms
step:293/2110 train_time:10683ms step_avg:36.46ms
step:294/2110 train_time:10717ms step_avg:36.45ms
step:295/2110 train_time:10754ms step_avg:36.45ms
step:296/2110 train_time:10789ms step_avg:36.45ms
step:297/2110 train_time:10827ms step_avg:36.45ms
step:298/2110 train_time:10860ms step_avg:36.44ms
step:299/2110 train_time:10898ms step_avg:36.45ms
step:300/2110 train_time:10933ms step_avg:36.44ms
step:301/2110 train_time:10970ms step_avg:36.45ms
step:302/2110 train_time:11006ms step_avg:36.44ms
step:303/2110 train_time:11044ms step_avg:36.45ms
step:304/2110 train_time:11076ms step_avg:36.44ms
step:305/2110 train_time:11112ms step_avg:36.43ms
step:306/2110 train_time:11145ms step_avg:36.42ms
step:307/2110 train_time:11183ms step_avg:36.43ms
step:308/2110 train_time:11221ms step_avg:36.43ms
step:309/2110 train_time:11254ms step_avg:36.42ms
step:310/2110 train_time:11285ms step_avg:36.40ms
step:311/2110 train_time:11319ms step_avg:36.40ms
step:312/2110 train_time:11353ms step_avg:36.39ms
step:313/2110 train_time:11390ms step_avg:36.39ms
step:314/2110 train_time:11424ms step_avg:36.38ms
step:315/2110 train_time:11463ms step_avg:36.39ms
step:316/2110 train_time:11497ms step_avg:36.38ms
step:317/2110 train_time:11533ms step_avg:36.38ms
step:318/2110 train_time:11566ms step_avg:36.37ms
step:319/2110 train_time:11603ms step_avg:36.37ms
step:320/2110 train_time:11637ms step_avg:36.37ms
step:321/2110 train_time:11670ms step_avg:36.35ms
step:322/2110 train_time:11703ms step_avg:36.34ms
step:323/2110 train_time:11740ms step_avg:36.35ms
step:324/2110 train_time:11774ms step_avg:36.34ms
step:325/2110 train_time:11811ms step_avg:36.34ms
step:326/2110 train_time:11845ms step_avg:36.34ms
step:327/2110 train_time:11882ms step_avg:36.34ms
step:328/2110 train_time:11917ms step_avg:36.33ms
step:329/2110 train_time:11952ms step_avg:36.33ms
step:330/2110 train_time:11985ms step_avg:36.32ms
step:331/2110 train_time:12024ms step_avg:36.33ms
step:332/2110 train_time:12059ms step_avg:36.32ms
step:333/2110 train_time:12095ms step_avg:36.32ms
step:334/2110 train_time:12128ms step_avg:36.31ms
step:335/2110 train_time:12166ms step_avg:36.32ms
step:336/2110 train_time:12204ms step_avg:36.32ms
step:337/2110 train_time:12244ms step_avg:36.33ms
step:338/2110 train_time:12277ms step_avg:36.32ms
step:339/2110 train_time:12309ms step_avg:36.31ms
step:340/2110 train_time:12343ms step_avg:36.30ms
step:341/2110 train_time:12377ms step_avg:36.30ms
step:342/2110 train_time:12411ms step_avg:36.29ms
step:343/2110 train_time:12449ms step_avg:36.30ms
step:344/2110 train_time:12483ms step_avg:36.29ms
step:345/2110 train_time:12518ms step_avg:36.28ms
step:346/2110 train_time:12550ms step_avg:36.27ms
step:347/2110 train_time:12583ms step_avg:36.26ms
step:348/2110 train_time:12615ms step_avg:36.25ms
step:349/2110 train_time:12650ms step_avg:36.25ms
step:350/2110 train_time:12683ms step_avg:36.24ms
step:351/2110 train_time:12716ms step_avg:36.23ms
step:352/2110 train_time:12749ms step_avg:36.22ms
step:353/2110 train_time:12784ms step_avg:36.22ms
step:354/2110 train_time:12816ms step_avg:36.20ms
step:355/2110 train_time:12850ms step_avg:36.20ms
step:356/2110 train_time:12882ms step_avg:36.19ms
step:357/2110 train_time:12917ms step_avg:36.18ms
step:358/2110 train_time:12950ms step_avg:36.17ms
step:359/2110 train_time:12986ms step_avg:36.17ms
step:360/2110 train_time:13020ms step_avg:36.17ms
step:361/2110 train_time:13056ms step_avg:36.17ms
step:362/2110 train_time:13091ms step_avg:36.16ms
step:363/2110 train_time:13129ms step_avg:36.17ms
step:364/2110 train_time:13163ms step_avg:36.16ms
step:365/2110 train_time:13200ms step_avg:36.16ms
step:366/2110 train_time:13234ms step_avg:36.16ms
step:367/2110 train_time:13271ms step_avg:36.16ms
step:368/2110 train_time:13305ms step_avg:36.15ms
step:369/2110 train_time:13342ms step_avg:36.16ms
step:370/2110 train_time:13377ms step_avg:36.16ms
step:371/2110 train_time:13413ms step_avg:36.15ms
step:372/2110 train_time:13448ms step_avg:36.15ms
step:373/2110 train_time:13484ms step_avg:36.15ms
step:374/2110 train_time:13518ms step_avg:36.14ms
step:375/2110 train_time:13554ms step_avg:36.14ms
step:376/2110 train_time:13586ms step_avg:36.13ms
step:377/2110 train_time:13622ms step_avg:36.13ms
step:378/2110 train_time:13655ms step_avg:36.13ms
step:379/2110 train_time:13692ms step_avg:36.13ms
step:380/2110 train_time:13725ms step_avg:36.12ms
step:381/2110 train_time:13761ms step_avg:36.12ms
step:382/2110 train_time:13794ms step_avg:36.11ms
step:383/2110 train_time:13829ms step_avg:36.11ms
step:384/2110 train_time:13862ms step_avg:36.10ms
step:385/2110 train_time:13899ms step_avg:36.10ms
step:386/2110 train_time:13934ms step_avg:36.10ms
step:387/2110 train_time:13972ms step_avg:36.10ms
step:388/2110 train_time:14005ms step_avg:36.10ms
step:389/2110 train_time:14042ms step_avg:36.10ms
step:390/2110 train_time:14074ms step_avg:36.09ms
step:391/2110 train_time:14107ms step_avg:36.08ms
step:392/2110 train_time:14139ms step_avg:36.07ms
step:393/2110 train_time:14172ms step_avg:36.06ms
step:394/2110 train_time:14204ms step_avg:36.05ms
step:395/2110 train_time:14238ms step_avg:36.05ms
step:396/2110 train_time:14271ms step_avg:36.04ms
step:397/2110 train_time:14305ms step_avg:36.03ms
step:398/2110 train_time:14344ms step_avg:36.04ms
step:399/2110 train_time:14384ms step_avg:36.05ms
step:400/2110 train_time:14418ms step_avg:36.04ms
step:401/2110 train_time:14455ms step_avg:36.05ms
step:402/2110 train_time:14487ms step_avg:36.04ms
step:403/2110 train_time:14521ms step_avg:36.03ms
step:404/2110 train_time:14557ms step_avg:36.03ms
step:405/2110 train_time:14591ms step_avg:36.03ms
step:406/2110 train_time:14623ms step_avg:36.02ms
step:407/2110 train_time:14656ms step_avg:36.01ms
step:408/2110 train_time:14688ms step_avg:36.00ms
step:409/2110 train_time:14722ms step_avg:35.99ms
step:410/2110 train_time:14755ms step_avg:35.99ms
step:411/2110 train_time:14787ms step_avg:35.98ms
step:412/2110 train_time:14820ms step_avg:35.97ms
step:413/2110 train_time:14853ms step_avg:35.96ms
step:414/2110 train_time:14885ms step_avg:35.96ms
step:415/2110 train_time:14919ms step_avg:35.95ms
step:416/2110 train_time:14953ms step_avg:35.94ms
step:417/2110 train_time:14986ms step_avg:35.94ms
step:418/2110 train_time:15019ms step_avg:35.93ms
step:419/2110 train_time:15051ms step_avg:35.92ms
step:420/2110 train_time:15084ms step_avg:35.92ms
step:421/2110 train_time:15117ms step_avg:35.91ms
step:422/2110 train_time:15149ms step_avg:35.90ms
step:423/2110 train_time:15182ms step_avg:35.89ms
step:424/2110 train_time:15215ms step_avg:35.88ms
step:425/2110 train_time:15248ms step_avg:35.88ms
step:426/2110 train_time:15281ms step_avg:35.87ms
step:427/2110 train_time:15314ms step_avg:35.86ms
step:428/2110 train_time:15347ms step_avg:35.86ms
step:429/2110 train_time:15380ms step_avg:35.85ms
step:430/2110 train_time:15414ms step_avg:35.85ms
step:431/2110 train_time:15447ms step_avg:35.84ms
step:432/2110 train_time:15480ms step_avg:35.83ms
step:433/2110 train_time:15513ms step_avg:35.83ms
step:434/2110 train_time:15546ms step_avg:35.82ms
step:435/2110 train_time:15579ms step_avg:35.81ms
step:436/2110 train_time:15612ms step_avg:35.81ms
step:437/2110 train_time:15645ms step_avg:35.80ms
step:438/2110 train_time:15678ms step_avg:35.79ms
step:439/2110 train_time:15711ms step_avg:35.79ms
step:440/2110 train_time:15744ms step_avg:35.78ms
step:441/2110 train_time:15777ms step_avg:35.78ms
step:442/2110 train_time:15809ms step_avg:35.77ms
step:443/2110 train_time:15842ms step_avg:35.76ms
step:444/2110 train_time:15876ms step_avg:35.76ms
step:445/2110 train_time:15908ms step_avg:35.75ms
step:446/2110 train_time:15941ms step_avg:35.74ms
step:447/2110 train_time:15974ms step_avg:35.74ms
step:448/2110 train_time:16007ms step_avg:35.73ms
step:449/2110 train_time:16040ms step_avg:35.72ms
step:450/2110 train_time:16073ms step_avg:35.72ms
step:451/2110 train_time:16106ms step_avg:35.71ms
step:452/2110 train_time:16139ms step_avg:35.70ms
step:453/2110 train_time:16172ms step_avg:35.70ms
step:454/2110 train_time:16205ms step_avg:35.69ms
step:455/2110 train_time:16238ms step_avg:35.69ms
step:456/2110 train_time:16272ms step_avg:35.68ms
step:457/2110 train_time:16304ms step_avg:35.68ms
step:458/2110 train_time:16337ms step_avg:35.67ms
step:459/2110 train_time:16370ms step_avg:35.66ms
step:460/2110 train_time:16403ms step_avg:35.66ms
step:461/2110 train_time:16436ms step_avg:35.65ms
step:462/2110 train_time:16469ms step_avg:35.65ms
step:463/2110 train_time:16502ms step_avg:35.64ms
step:464/2110 train_time:16534ms step_avg:35.63ms
step:465/2110 train_time:16568ms step_avg:35.63ms
step:466/2110 train_time:16601ms step_avg:35.62ms
step:467/2110 train_time:16634ms step_avg:35.62ms
step:468/2110 train_time:16666ms step_avg:35.61ms
step:469/2110 train_time:16699ms step_avg:35.61ms
step:470/2110 train_time:16732ms step_avg:35.60ms
step:471/2110 train_time:16766ms step_avg:35.60ms
step:472/2110 train_time:16799ms step_avg:35.59ms
step:473/2110 train_time:16832ms step_avg:35.58ms
step:474/2110 train_time:16865ms step_avg:35.58ms
step:475/2110 train_time:16898ms step_avg:35.57ms
step:476/2110 train_time:16930ms step_avg:35.57ms
step:477/2110 train_time:16963ms step_avg:35.56ms
step:478/2110 train_time:16996ms step_avg:35.56ms
step:479/2110 train_time:17030ms step_avg:35.55ms
step:480/2110 train_time:17063ms step_avg:35.55ms
step:481/2110 train_time:17096ms step_avg:35.54ms
step:482/2110 train_time:17128ms step_avg:35.54ms
step:483/2110 train_time:17161ms step_avg:35.53ms
step:484/2110 train_time:17194ms step_avg:35.52ms
step:485/2110 train_time:17227ms step_avg:35.52ms
step:486/2110 train_time:17259ms step_avg:35.51ms
step:487/2110 train_time:17293ms step_avg:35.51ms
step:488/2110 train_time:17325ms step_avg:35.50ms
step:489/2110 train_time:17358ms step_avg:35.50ms
step:490/2110 train_time:17392ms step_avg:35.49ms
step:491/2110 train_time:17424ms step_avg:35.49ms
step:492/2110 train_time:17457ms step_avg:35.48ms
step:493/2110 train_time:17490ms step_avg:35.48ms
step:494/2110 train_time:17523ms step_avg:35.47ms
step:495/2110 train_time:17557ms step_avg:35.47ms
step:496/2110 train_time:17591ms step_avg:35.47ms
step:497/2110 train_time:17622ms step_avg:35.46ms
step:498/2110 train_time:17655ms step_avg:35.45ms
step:499/2110 train_time:17688ms step_avg:35.45ms
step:500/2110 train_time:17720ms step_avg:35.44ms
step:500/2110 val_loss:4.0321 train_time:17756ms step_avg:35.51ms
step:501/2110 train_time:17792ms step_avg:35.51ms
step:502/2110 train_time:17821ms step_avg:35.50ms
step:503/2110 train_time:17851ms step_avg:35.49ms
step:504/2110 train_time:17882ms step_avg:35.48ms
step:505/2110 train_time:17910ms step_avg:35.47ms
step:506/2110 train_time:17938ms step_avg:35.45ms
step:507/2110 train_time:17963ms step_avg:35.43ms
step:508/2110 train_time:17995ms step_avg:35.42ms
step:509/2110 train_time:18028ms step_avg:35.42ms
step:510/2110 train_time:18062ms step_avg:35.42ms
step:511/2110 train_time:18094ms step_avg:35.41ms
step:512/2110 train_time:18128ms step_avg:35.41ms
step:513/2110 train_time:18159ms step_avg:35.40ms
step:514/2110 train_time:18193ms step_avg:35.39ms
step:515/2110 train_time:18225ms step_avg:35.39ms
step:516/2110 train_time:18258ms step_avg:35.38ms
step:517/2110 train_time:18290ms step_avg:35.38ms
step:518/2110 train_time:18323ms step_avg:35.37ms
step:519/2110 train_time:18355ms step_avg:35.37ms
step:520/2110 train_time:18388ms step_avg:35.36ms
step:521/2110 train_time:18420ms step_avg:35.36ms
step:522/2110 train_time:18454ms step_avg:35.35ms
step:523/2110 train_time:18485ms step_avg:35.34ms
step:524/2110 train_time:18519ms step_avg:35.34ms
step:525/2110 train_time:18551ms step_avg:35.34ms
step:526/2110 train_time:18584ms step_avg:35.33ms
step:527/2110 train_time:18616ms step_avg:35.33ms
step:528/2110 train_time:18651ms step_avg:35.32ms
step:529/2110 train_time:18687ms step_avg:35.33ms
step:530/2110 train_time:18719ms step_avg:35.32ms
step:531/2110 train_time:18749ms step_avg:35.31ms
step:532/2110 train_time:18783ms step_avg:35.31ms
step:533/2110 train_time:18817ms step_avg:35.30ms
step:534/2110 train_time:18852ms step_avg:35.30ms
step:535/2110 train_time:18883ms step_avg:35.29ms
step:536/2110 train_time:18916ms step_avg:35.29ms
step:537/2110 train_time:18948ms step_avg:35.29ms
step:538/2110 train_time:18982ms step_avg:35.28ms
step:539/2110 train_time:19015ms step_avg:35.28ms
step:540/2110 train_time:19049ms step_avg:35.28ms
step:541/2110 train_time:19081ms step_avg:35.27ms
step:542/2110 train_time:19116ms step_avg:35.27ms
step:543/2110 train_time:19149ms step_avg:35.27ms
step:544/2110 train_time:19181ms step_avg:35.26ms
step:545/2110 train_time:19218ms step_avg:35.26ms
step:546/2110 train_time:19257ms step_avg:35.27ms
step:547/2110 train_time:19294ms step_avg:35.27ms
step:548/2110 train_time:19342ms step_avg:35.30ms
step:549/2110 train_time:19381ms step_avg:35.30ms
step:550/2110 train_time:19424ms step_avg:35.32ms
step:551/2110 train_time:19464ms step_avg:35.33ms
step:552/2110 train_time:19508ms step_avg:35.34ms
step:553/2110 train_time:19549ms step_avg:35.35ms
step:554/2110 train_time:19592ms step_avg:35.36ms
step:555/2110 train_time:19629ms step_avg:35.37ms
step:556/2110 train_time:19674ms step_avg:35.38ms
step:557/2110 train_time:19712ms step_avg:35.39ms
step:558/2110 train_time:19758ms step_avg:35.41ms
step:559/2110 train_time:19793ms step_avg:35.41ms
step:560/2110 train_time:19836ms step_avg:35.42ms
step:561/2110 train_time:19874ms step_avg:35.43ms
step:562/2110 train_time:19913ms step_avg:35.43ms
step:563/2110 train_time:19953ms step_avg:35.44ms
step:564/2110 train_time:19998ms step_avg:35.46ms
step:565/2110 train_time:20039ms step_avg:35.47ms
step:566/2110 train_time:20081ms step_avg:35.48ms
step:567/2110 train_time:20117ms step_avg:35.48ms
step:568/2110 train_time:20160ms step_avg:35.49ms
step:569/2110 train_time:20196ms step_avg:35.49ms
step:570/2110 train_time:20239ms step_avg:35.51ms
step:571/2110 train_time:20280ms step_avg:35.52ms
step:572/2110 train_time:20325ms step_avg:35.53ms
step:573/2110 train_time:20366ms step_avg:35.54ms
step:574/2110 train_time:20409ms step_avg:35.56ms
step:575/2110 train_time:20447ms step_avg:35.56ms
step:576/2110 train_time:20488ms step_avg:35.57ms
step:577/2110 train_time:20530ms step_avg:35.58ms
step:578/2110 train_time:20575ms step_avg:35.60ms
step:579/2110 train_time:20616ms step_avg:35.61ms
step:580/2110 train_time:20659ms step_avg:35.62ms
step:581/2110 train_time:20694ms step_avg:35.62ms
step:582/2110 train_time:20736ms step_avg:35.63ms
step:583/2110 train_time:20778ms step_avg:35.64ms
step:584/2110 train_time:20823ms step_avg:35.66ms
step:585/2110 train_time:20863ms step_avg:35.66ms
step:586/2110 train_time:20903ms step_avg:35.67ms
step:587/2110 train_time:20943ms step_avg:35.68ms
step:588/2110 train_time:20983ms step_avg:35.69ms
step:589/2110 train_time:21021ms step_avg:35.69ms
step:590/2110 train_time:21061ms step_avg:35.70ms
step:591/2110 train_time:21098ms step_avg:35.70ms
step:592/2110 train_time:21142ms step_avg:35.71ms
step:593/2110 train_time:21181ms step_avg:35.72ms
step:594/2110 train_time:21216ms step_avg:35.72ms
step:595/2110 train_time:21248ms step_avg:35.71ms
step:596/2110 train_time:21284ms step_avg:35.71ms
step:597/2110 train_time:21319ms step_avg:35.71ms
step:598/2110 train_time:21357ms step_avg:35.71ms
step:599/2110 train_time:21389ms step_avg:35.71ms
step:600/2110 train_time:21422ms step_avg:35.70ms
step:601/2110 train_time:21453ms step_avg:35.70ms
step:602/2110 train_time:21486ms step_avg:35.69ms
step:603/2110 train_time:21518ms step_avg:35.69ms
step:604/2110 train_time:21552ms step_avg:35.68ms
step:605/2110 train_time:21585ms step_avg:35.68ms
step:606/2110 train_time:21619ms step_avg:35.68ms
step:607/2110 train_time:21653ms step_avg:35.67ms
step:608/2110 train_time:21688ms step_avg:35.67ms
step:609/2110 train_time:21722ms step_avg:35.67ms
step:610/2110 train_time:21757ms step_avg:35.67ms
step:611/2110 train_time:21790ms step_avg:35.66ms
step:612/2110 train_time:21821ms step_avg:35.66ms
step:613/2110 train_time:21846ms step_avg:35.64ms
step:614/2110 train_time:21878ms step_avg:35.63ms
step:615/2110 train_time:21912ms step_avg:35.63ms
step:616/2110 train_time:21949ms step_avg:35.63ms
step:617/2110 train_time:21985ms step_avg:35.63ms
step:618/2110 train_time:22021ms step_avg:35.63ms
step:619/2110 train_time:22060ms step_avg:35.64ms
step:620/2110 train_time:22096ms step_avg:35.64ms
step:621/2110 train_time:22137ms step_avg:35.65ms
step:622/2110 train_time:22173ms step_avg:35.65ms
step:623/2110 train_time:22224ms step_avg:35.67ms
step:624/2110 train_time:22261ms step_avg:35.67ms
step:625/2110 train_time:22297ms step_avg:35.67ms
step:626/2110 train_time:22335ms step_avg:35.68ms
step:627/2110 train_time:22368ms step_avg:35.67ms
step:628/2110 train_time:22408ms step_avg:35.68ms
step:629/2110 train_time:22450ms step_avg:35.69ms
step:630/2110 train_time:22493ms step_avg:35.70ms
step:631/2110 train_time:22528ms step_avg:35.70ms
step:632/2110 train_time:22567ms step_avg:35.71ms
step:633/2110 train_time:22602ms step_avg:35.71ms
step:634/2110 train_time:22640ms step_avg:35.71ms
step:635/2110 train_time:22676ms step_avg:35.71ms
step:636/2110 train_time:22715ms step_avg:35.72ms
step:637/2110 train_time:22748ms step_avg:35.71ms
step:638/2110 train_time:22785ms step_avg:35.71ms
step:639/2110 train_time:22816ms step_avg:35.71ms
step:640/2110 train_time:22850ms step_avg:35.70ms
step:641/2110 train_time:22880ms step_avg:35.69ms
step:642/2110 train_time:22915ms step_avg:35.69ms
step:643/2110 train_time:22945ms step_avg:35.68ms
step:644/2110 train_time:22983ms step_avg:35.69ms
step:645/2110 train_time:23014ms step_avg:35.68ms
step:646/2110 train_time:23047ms step_avg:35.68ms
step:647/2110 train_time:23079ms step_avg:35.67ms
step:648/2110 train_time:23112ms step_avg:35.67ms
step:649/2110 train_time:23143ms step_avg:35.66ms
step:650/2110 train_time:23178ms step_avg:35.66ms
step:651/2110 train_time:23209ms step_avg:35.65ms
step:652/2110 train_time:23242ms step_avg:35.65ms
step:653/2110 train_time:23272ms step_avg:35.64ms
step:654/2110 train_time:23309ms step_avg:35.64ms
step:655/2110 train_time:23343ms step_avg:35.64ms
step:656/2110 train_time:23376ms step_avg:35.63ms
step:657/2110 train_time:23408ms step_avg:35.63ms
step:658/2110 train_time:23443ms step_avg:35.63ms
step:659/2110 train_time:23473ms step_avg:35.62ms
step:660/2110 train_time:23508ms step_avg:35.62ms
step:661/2110 train_time:23540ms step_avg:35.61ms
step:662/2110 train_time:23572ms step_avg:35.61ms
step:663/2110 train_time:23600ms step_avg:35.60ms
step:664/2110 train_time:23631ms step_avg:35.59ms
step:665/2110 train_time:23660ms step_avg:35.58ms
step:666/2110 train_time:23692ms step_avg:35.57ms
step:667/2110 train_time:23720ms step_avg:35.56ms
step:668/2110 train_time:23751ms step_avg:35.56ms
step:669/2110 train_time:23779ms step_avg:35.54ms
step:670/2110 train_time:23810ms step_avg:35.54ms
step:671/2110 train_time:23842ms step_avg:35.53ms
step:672/2110 train_time:23876ms step_avg:35.53ms
step:673/2110 train_time:23908ms step_avg:35.52ms
step:674/2110 train_time:23942ms step_avg:35.52ms
step:675/2110 train_time:23975ms step_avg:35.52ms
step:676/2110 train_time:24007ms step_avg:35.51ms
step:677/2110 train_time:24039ms step_avg:35.51ms
step:678/2110 train_time:24073ms step_avg:35.51ms
step:679/2110 train_time:24105ms step_avg:35.50ms
step:680/2110 train_time:24138ms step_avg:35.50ms
step:681/2110 train_time:24170ms step_avg:35.49ms
step:682/2110 train_time:24204ms step_avg:35.49ms
step:683/2110 train_time:24236ms step_avg:35.48ms
step:684/2110 train_time:24270ms step_avg:35.48ms
step:685/2110 train_time:24303ms step_avg:35.48ms
step:686/2110 train_time:24335ms step_avg:35.47ms
step:687/2110 train_time:24367ms step_avg:35.47ms
step:688/2110 train_time:24402ms step_avg:35.47ms
step:689/2110 train_time:24433ms step_avg:35.46ms
step:690/2110 train_time:24467ms step_avg:35.46ms
step:691/2110 train_time:24500ms step_avg:35.46ms
step:692/2110 train_time:24558ms step_avg:35.49ms
step:693/2110 train_time:24619ms step_avg:35.52ms
step:694/2110 train_time:24679ms step_avg:35.56ms
step:695/2110 train_time:24739ms step_avg:35.60ms
step:696/2110 train_time:24798ms step_avg:35.63ms
step:697/2110 train_time:24857ms step_avg:35.66ms
step:698/2110 train_time:24917ms step_avg:35.70ms
step:699/2110 train_time:24975ms step_avg:35.73ms
step:700/2110 train_time:25034ms step_avg:35.76ms
step:701/2110 train_time:25095ms step_avg:35.80ms
step:702/2110 train_time:25152ms step_avg:35.83ms
step:703/2110 train_time:25212ms step_avg:35.86ms
step:704/2110 train_time:25270ms step_avg:35.90ms
step:705/2110 train_time:25330ms step_avg:35.93ms
step:706/2110 train_time:25389ms step_avg:35.96ms
step:707/2110 train_time:25448ms step_avg:35.99ms
step:708/2110 train_time:25506ms step_avg:36.03ms
step:709/2110 train_time:25567ms step_avg:36.06ms
step:710/2110 train_time:25626ms step_avg:36.09ms
step:711/2110 train_time:25686ms step_avg:36.13ms
step:712/2110 train_time:25746ms step_avg:36.16ms
step:713/2110 train_time:25805ms step_avg:36.19ms
step:714/2110 train_time:25865ms step_avg:36.23ms
step:715/2110 train_time:25925ms step_avg:36.26ms
step:716/2110 train_time:25985ms step_avg:36.29ms
step:717/2110 train_time:26044ms step_avg:36.32ms
step:718/2110 train_time:26105ms step_avg:36.36ms
step:719/2110 train_time:26163ms step_avg:36.39ms
step:720/2110 train_time:26222ms step_avg:36.42ms
step:721/2110 train_time:26281ms step_avg:36.45ms
step:722/2110 train_time:26342ms step_avg:36.48ms
step:723/2110 train_time:26400ms step_avg:36.51ms
step:724/2110 train_time:26459ms step_avg:36.55ms
step:725/2110 train_time:26517ms step_avg:36.58ms
step:726/2110 train_time:26576ms step_avg:36.61ms
step:727/2110 train_time:26635ms step_avg:36.64ms
step:728/2110 train_time:26694ms step_avg:36.67ms
step:729/2110 train_time:26753ms step_avg:36.70ms
step:730/2110 train_time:26813ms step_avg:36.73ms
step:731/2110 train_time:26873ms step_avg:36.76ms
step:732/2110 train_time:26933ms step_avg:36.79ms
step:733/2110 train_time:26992ms step_avg:36.82ms
step:734/2110 train_time:27050ms step_avg:36.85ms
step:735/2110 train_time:27110ms step_avg:36.88ms
step:736/2110 train_time:27169ms step_avg:36.91ms
step:737/2110 train_time:27229ms step_avg:36.95ms
step:738/2110 train_time:27288ms step_avg:36.98ms
step:739/2110 train_time:27348ms step_avg:37.01ms
step:740/2110 train_time:27407ms step_avg:37.04ms
step:741/2110 train_time:27466ms step_avg:37.07ms
step:742/2110 train_time:27526ms step_avg:37.10ms
step:743/2110 train_time:27585ms step_avg:37.13ms
step:744/2110 train_time:27646ms step_avg:37.16ms
step:745/2110 train_time:27705ms step_avg:37.19ms
step:746/2110 train_time:27764ms step_avg:37.22ms
step:747/2110 train_time:27824ms step_avg:37.25ms
step:748/2110 train_time:27884ms step_avg:37.28ms
step:749/2110 train_time:27943ms step_avg:37.31ms
step:750/2110 train_time:28003ms step_avg:37.34ms
step:750/2110 val_loss:3.9057 train_time:28063ms step_avg:37.42ms
step:751/2110 train_time:28106ms step_avg:37.43ms
step:752/2110 train_time:28144ms step_avg:37.43ms
step:753/2110 train_time:28185ms step_avg:37.43ms
step:754/2110 train_time:28249ms step_avg:37.47ms
step:755/2110 train_time:28310ms step_avg:37.50ms
step:756/2110 train_time:28368ms step_avg:37.52ms
step:757/2110 train_time:28427ms step_avg:37.55ms
step:758/2110 train_time:28484ms step_avg:37.58ms
step:759/2110 train_time:28544ms step_avg:37.61ms
step:760/2110 train_time:28602ms step_avg:37.63ms
step:761/2110 train_time:28660ms step_avg:37.66ms
step:762/2110 train_time:28718ms step_avg:37.69ms
step:763/2110 train_time:28777ms step_avg:37.72ms
step:764/2110 train_time:28836ms step_avg:37.74ms
step:765/2110 train_time:28894ms step_avg:37.77ms
step:766/2110 train_time:28952ms step_avg:37.80ms
step:767/2110 train_time:29012ms step_avg:37.82ms
step:768/2110 train_time:29072ms step_avg:37.85ms
step:769/2110 train_time:29132ms step_avg:37.88ms
step:770/2110 train_time:29193ms step_avg:37.91ms
step:771/2110 train_time:29254ms step_avg:37.94ms
step:772/2110 train_time:29314ms step_avg:37.97ms
step:773/2110 train_time:29374ms step_avg:38.00ms
step:774/2110 train_time:29433ms step_avg:38.03ms
step:775/2110 train_time:29492ms step_avg:38.05ms
step:776/2110 train_time:29551ms step_avg:38.08ms
step:777/2110 train_time:29610ms step_avg:38.11ms
step:778/2110 train_time:29668ms step_avg:38.13ms
step:779/2110 train_time:29726ms step_avg:38.16ms
step:780/2110 train_time:29785ms step_avg:38.19ms
step:781/2110 train_time:29843ms step_avg:38.21ms
step:782/2110 train_time:29901ms step_avg:38.24ms
step:783/2110 train_time:29961ms step_avg:38.26ms
step:784/2110 train_time:30019ms step_avg:38.29ms
step:785/2110 train_time:30080ms step_avg:38.32ms
step:786/2110 train_time:30139ms step_avg:38.34ms
step:787/2110 train_time:30199ms step_avg:38.37ms
step:788/2110 train_time:30259ms step_avg:38.40ms
step:789/2110 train_time:30319ms step_avg:38.43ms
step:790/2110 train_time:30378ms step_avg:38.45ms
step:791/2110 train_time:30437ms step_avg:38.48ms
step:792/2110 train_time:30496ms step_avg:38.50ms
step:793/2110 train_time:30556ms step_avg:38.53ms
step:794/2110 train_time:30615ms step_avg:38.56ms
step:795/2110 train_time:30674ms step_avg:38.58ms
step:796/2110 train_time:30733ms step_avg:38.61ms
step:797/2110 train_time:30791ms step_avg:38.63ms
step:798/2110 train_time:30850ms step_avg:38.66ms
step:799/2110 train_time:30910ms step_avg:38.69ms
step:800/2110 train_time:30968ms step_avg:38.71ms
step:801/2110 train_time:31027ms step_avg:38.73ms
step:802/2110 train_time:31085ms step_avg:38.76ms
step:803/2110 train_time:31144ms step_avg:38.78ms
step:804/2110 train_time:31203ms step_avg:38.81ms
step:805/2110 train_time:31263ms step_avg:38.84ms
step:806/2110 train_time:31322ms step_avg:38.86ms
step:807/2110 train_time:31382ms step_avg:38.89ms
step:808/2110 train_time:31441ms step_avg:38.91ms
step:809/2110 train_time:31502ms step_avg:38.94ms
step:810/2110 train_time:31560ms step_avg:38.96ms
step:811/2110 train_time:31621ms step_avg:38.99ms
step:812/2110 train_time:31679ms step_avg:39.01ms
step:813/2110 train_time:31740ms step_avg:39.04ms
step:814/2110 train_time:31798ms step_avg:39.06ms
step:815/2110 train_time:31858ms step_avg:39.09ms
step:816/2110 train_time:31917ms step_avg:39.11ms
step:817/2110 train_time:31975ms step_avg:39.14ms
step:818/2110 train_time:32035ms step_avg:39.16ms
step:819/2110 train_time:32095ms step_avg:39.19ms
step:820/2110 train_time:32154ms step_avg:39.21ms
step:821/2110 train_time:32213ms step_avg:39.24ms
step:822/2110 train_time:32273ms step_avg:39.26ms
step:823/2110 train_time:32332ms step_avg:39.29ms
step:824/2110 train_time:32392ms step_avg:39.31ms
step:825/2110 train_time:32451ms step_avg:39.33ms
step:826/2110 train_time:32509ms step_avg:39.36ms
step:827/2110 train_time:32568ms step_avg:39.38ms
step:828/2110 train_time:32626ms step_avg:39.40ms
step:829/2110 train_time:32686ms step_avg:39.43ms
step:830/2110 train_time:32744ms step_avg:39.45ms
step:831/2110 train_time:32804ms step_avg:39.48ms
step:832/2110 train_time:32862ms step_avg:39.50ms
step:833/2110 train_time:32922ms step_avg:39.52ms
step:834/2110 train_time:32980ms step_avg:39.54ms
step:835/2110 train_time:33040ms step_avg:39.57ms
step:836/2110 train_time:33098ms step_avg:39.59ms
step:837/2110 train_time:33159ms step_avg:39.62ms
step:838/2110 train_time:33217ms step_avg:39.64ms
step:839/2110 train_time:33277ms step_avg:39.66ms
step:840/2110 train_time:33337ms step_avg:39.69ms
step:841/2110 train_time:33396ms step_avg:39.71ms
step:842/2110 train_time:33455ms step_avg:39.73ms
step:843/2110 train_time:33515ms step_avg:39.76ms
step:844/2110 train_time:33574ms step_avg:39.78ms
step:845/2110 train_time:33634ms step_avg:39.80ms
step:846/2110 train_time:33692ms step_avg:39.83ms
step:847/2110 train_time:33753ms step_avg:39.85ms
step:848/2110 train_time:33811ms step_avg:39.87ms
step:849/2110 train_time:33871ms step_avg:39.90ms
step:850/2110 train_time:33930ms step_avg:39.92ms
step:851/2110 train_time:33990ms step_avg:39.94ms
step:852/2110 train_time:34048ms step_avg:39.96ms
step:853/2110 train_time:34108ms step_avg:39.99ms
step:854/2110 train_time:34166ms step_avg:40.01ms
step:855/2110 train_time:34226ms step_avg:40.03ms
step:856/2110 train_time:34284ms step_avg:40.05ms
step:857/2110 train_time:34344ms step_avg:40.07ms
step:858/2110 train_time:34402ms step_avg:40.10ms
step:859/2110 train_time:34462ms step_avg:40.12ms
step:860/2110 train_time:34520ms step_avg:40.14ms
step:861/2110 train_time:34581ms step_avg:40.16ms
step:862/2110 train_time:34639ms step_avg:40.18ms
step:863/2110 train_time:34700ms step_avg:40.21ms
step:864/2110 train_time:34758ms step_avg:40.23ms
step:865/2110 train_time:34818ms step_avg:40.25ms
step:866/2110 train_time:34876ms step_avg:40.27ms
step:867/2110 train_time:34937ms step_avg:40.30ms
step:868/2110 train_time:34994ms step_avg:40.32ms
step:869/2110 train_time:35055ms step_avg:40.34ms
step:870/2110 train_time:35113ms step_avg:40.36ms
step:871/2110 train_time:35174ms step_avg:40.38ms
step:872/2110 train_time:35233ms step_avg:40.40ms
step:873/2110 train_time:35292ms step_avg:40.43ms
step:874/2110 train_time:35352ms step_avg:40.45ms
step:875/2110 train_time:35413ms step_avg:40.47ms
step:876/2110 train_time:35471ms step_avg:40.49ms
step:877/2110 train_time:35530ms step_avg:40.51ms
step:878/2110 train_time:35588ms step_avg:40.53ms
step:879/2110 train_time:35648ms step_avg:40.55ms
step:880/2110 train_time:35706ms step_avg:40.58ms
step:881/2110 train_time:35766ms step_avg:40.60ms
step:882/2110 train_time:35824ms step_avg:40.62ms
step:883/2110 train_time:35884ms step_avg:40.64ms
step:884/2110 train_time:35942ms step_avg:40.66ms
step:885/2110 train_time:36003ms step_avg:40.68ms
step:886/2110 train_time:36061ms step_avg:40.70ms
step:887/2110 train_time:36121ms step_avg:40.72ms
step:888/2110 train_time:36180ms step_avg:40.74ms
step:889/2110 train_time:36241ms step_avg:40.77ms
step:890/2110 train_time:36299ms step_avg:40.79ms
step:891/2110 train_time:36359ms step_avg:40.81ms
step:892/2110 train_time:36417ms step_avg:40.83ms
step:893/2110 train_time:36477ms step_avg:40.85ms
step:894/2110 train_time:36536ms step_avg:40.87ms
step:895/2110 train_time:36594ms step_avg:40.89ms
step:896/2110 train_time:36652ms step_avg:40.91ms
step:897/2110 train_time:36713ms step_avg:40.93ms
step:898/2110 train_time:36772ms step_avg:40.95ms
step:899/2110 train_time:36832ms step_avg:40.97ms
step:900/2110 train_time:36891ms step_avg:40.99ms
step:901/2110 train_time:36951ms step_avg:41.01ms
step:902/2110 train_time:37012ms step_avg:41.03ms
step:903/2110 train_time:37069ms step_avg:41.05ms
step:904/2110 train_time:37127ms step_avg:41.07ms
step:905/2110 train_time:37186ms step_avg:41.09ms
step:906/2110 train_time:37245ms step_avg:41.11ms
step:907/2110 train_time:37304ms step_avg:41.13ms
step:908/2110 train_time:37363ms step_avg:41.15ms
step:909/2110 train_time:37423ms step_avg:41.17ms
step:910/2110 train_time:37481ms step_avg:41.19ms
step:911/2110 train_time:37541ms step_avg:41.21ms
step:912/2110 train_time:37599ms step_avg:41.23ms
step:913/2110 train_time:37660ms step_avg:41.25ms
step:914/2110 train_time:37718ms step_avg:41.27ms
step:915/2110 train_time:37779ms step_avg:41.29ms
step:916/2110 train_time:37837ms step_avg:41.31ms
step:917/2110 train_time:37897ms step_avg:41.33ms
step:918/2110 train_time:37957ms step_avg:41.35ms
step:919/2110 train_time:38015ms step_avg:41.37ms
step:920/2110 train_time:38075ms step_avg:41.39ms
step:921/2110 train_time:38134ms step_avg:41.40ms
step:922/2110 train_time:38193ms step_avg:41.42ms
step:923/2110 train_time:38252ms step_avg:41.44ms
step:924/2110 train_time:38311ms step_avg:41.46ms
step:925/2110 train_time:38370ms step_avg:41.48ms
step:926/2110 train_time:38429ms step_avg:41.50ms
step:927/2110 train_time:38488ms step_avg:41.52ms
step:928/2110 train_time:38547ms step_avg:41.54ms
step:929/2110 train_time:38606ms step_avg:41.56ms
step:930/2110 train_time:38665ms step_avg:41.58ms
step:931/2110 train_time:38725ms step_avg:41.60ms
step:932/2110 train_time:38785ms step_avg:41.61ms
step:933/2110 train_time:38844ms step_avg:41.63ms
step:934/2110 train_time:38902ms step_avg:41.65ms
step:935/2110 train_time:38962ms step_avg:41.67ms
step:936/2110 train_time:39021ms step_avg:41.69ms
step:937/2110 train_time:39081ms step_avg:41.71ms
step:938/2110 train_time:39140ms step_avg:41.73ms
step:939/2110 train_time:39200ms step_avg:41.75ms
step:940/2110 train_time:39258ms step_avg:41.76ms
step:941/2110 train_time:39319ms step_avg:41.78ms
step:942/2110 train_time:39378ms step_avg:41.80ms
step:943/2110 train_time:39439ms step_avg:41.82ms
step:944/2110 train_time:39498ms step_avg:41.84ms
step:945/2110 train_time:39557ms step_avg:41.86ms
step:946/2110 train_time:39616ms step_avg:41.88ms
step:947/2110 train_time:39675ms step_avg:41.90ms
step:948/2110 train_time:39736ms step_avg:41.92ms
step:949/2110 train_time:39795ms step_avg:41.93ms
step:950/2110 train_time:39855ms step_avg:41.95ms
step:951/2110 train_time:39914ms step_avg:41.97ms
step:952/2110 train_time:39972ms step_avg:41.99ms
step:953/2110 train_time:40032ms step_avg:42.01ms
step:954/2110 train_time:40091ms step_avg:42.02ms
step:955/2110 train_time:40150ms step_avg:42.04ms
step:956/2110 train_time:40209ms step_avg:42.06ms
step:957/2110 train_time:40267ms step_avg:42.08ms
step:958/2110 train_time:40326ms step_avg:42.09ms
step:959/2110 train_time:40385ms step_avg:42.11ms
step:960/2110 train_time:40443ms step_avg:42.13ms
step:961/2110 train_time:40502ms step_avg:42.15ms
step:962/2110 train_time:40560ms step_avg:42.16ms
step:963/2110 train_time:40621ms step_avg:42.18ms
step:964/2110 train_time:40679ms step_avg:42.20ms
step:965/2110 train_time:40740ms step_avg:42.22ms
step:966/2110 train_time:40798ms step_avg:42.23ms
step:967/2110 train_time:40858ms step_avg:42.25ms
step:968/2110 train_time:40916ms step_avg:42.27ms
step:969/2110 train_time:40976ms step_avg:42.29ms
step:970/2110 train_time:41035ms step_avg:42.30ms
step:971/2110 train_time:41095ms step_avg:42.32ms
step:972/2110 train_time:41154ms step_avg:42.34ms
step:973/2110 train_time:41213ms step_avg:42.36ms
step:974/2110 train_time:41271ms step_avg:42.37ms
step:975/2110 train_time:41331ms step_avg:42.39ms
step:976/2110 train_time:41391ms step_avg:42.41ms
step:977/2110 train_time:41450ms step_avg:42.43ms
step:978/2110 train_time:41509ms step_avg:42.44ms
step:979/2110 train_time:41568ms step_avg:42.46ms
step:980/2110 train_time:41627ms step_avg:42.48ms
step:981/2110 train_time:41686ms step_avg:42.49ms
step:982/2110 train_time:41744ms step_avg:42.51ms
step:983/2110 train_time:41804ms step_avg:42.53ms
step:984/2110 train_time:41862ms step_avg:42.54ms
step:985/2110 train_time:41923ms step_avg:42.56ms
step:986/2110 train_time:41981ms step_avg:42.58ms
step:987/2110 train_time:42042ms step_avg:42.60ms
step:988/2110 train_time:42100ms step_avg:42.61ms
step:989/2110 train_time:42161ms step_avg:42.63ms
step:990/2110 train_time:42219ms step_avg:42.65ms
step:991/2110 train_time:42280ms step_avg:42.66ms
step:992/2110 train_time:42339ms step_avg:42.68ms
step:993/2110 train_time:42399ms step_avg:42.70ms
step:994/2110 train_time:42458ms step_avg:42.71ms
step:995/2110 train_time:42519ms step_avg:42.73ms
step:996/2110 train_time:42578ms step_avg:42.75ms
step:997/2110 train_time:42639ms step_avg:42.77ms
step:998/2110 train_time:42697ms step_avg:42.78ms
step:999/2110 train_time:42757ms step_avg:42.80ms
step:1000/2110 train_time:42815ms step_avg:42.82ms
step:1000/2110 val_loss:3.7586 train_time:42876ms step_avg:42.88ms
step:1001/2110 train_time:42906ms step_avg:42.86ms
step:1002/2110 train_time:42935ms step_avg:42.85ms
step:1003/2110 train_time:43000ms step_avg:42.87ms
step:1004/2110 train_time:43063ms step_avg:42.89ms
step:1005/2110 train_time:43125ms step_avg:42.91ms
step:1006/2110 train_time:43184ms step_avg:42.93ms
step:1007/2110 train_time:43242ms step_avg:42.94ms
step:1008/2110 train_time:43301ms step_avg:42.96ms
step:1009/2110 train_time:43361ms step_avg:42.97ms
step:1010/2110 train_time:43421ms step_avg:42.99ms
step:1011/2110 train_time:43478ms step_avg:43.00ms
step:1012/2110 train_time:43538ms step_avg:43.02ms
step:1013/2110 train_time:43594ms step_avg:43.03ms
step:1014/2110 train_time:43652ms step_avg:43.05ms
step:1015/2110 train_time:43709ms step_avg:43.06ms
step:1016/2110 train_time:43770ms step_avg:43.08ms
step:1017/2110 train_time:43828ms step_avg:43.10ms
step:1018/2110 train_time:43890ms step_avg:43.11ms
step:1019/2110 train_time:43949ms step_avg:43.13ms
step:1020/2110 train_time:44010ms step_avg:43.15ms
step:1021/2110 train_time:44071ms step_avg:43.16ms
step:1022/2110 train_time:44131ms step_avg:43.18ms
step:1023/2110 train_time:44190ms step_avg:43.20ms
step:1024/2110 train_time:44251ms step_avg:43.21ms
step:1025/2110 train_time:44310ms step_avg:43.23ms
step:1026/2110 train_time:44370ms step_avg:43.25ms
step:1027/2110 train_time:44428ms step_avg:43.26ms
step:1028/2110 train_time:44490ms step_avg:43.28ms
step:1029/2110 train_time:44546ms step_avg:43.29ms
step:1030/2110 train_time:44607ms step_avg:43.31ms
step:1031/2110 train_time:44663ms step_avg:43.32ms
step:1032/2110 train_time:44724ms step_avg:43.34ms
step:1033/2110 train_time:44780ms step_avg:43.35ms
step:1034/2110 train_time:44841ms step_avg:43.37ms
step:1035/2110 train_time:44899ms step_avg:43.38ms
step:1036/2110 train_time:44959ms step_avg:43.40ms
step:1037/2110 train_time:45018ms step_avg:43.41ms
step:1038/2110 train_time:45078ms step_avg:43.43ms
step:1039/2110 train_time:45136ms step_avg:43.44ms
step:1040/2110 train_time:45198ms step_avg:43.46ms
step:1041/2110 train_time:45255ms step_avg:43.47ms
step:1042/2110 train_time:45315ms step_avg:43.49ms
step:1043/2110 train_time:45373ms step_avg:43.50ms
step:1044/2110 train_time:45432ms step_avg:43.52ms
step:1045/2110 train_time:45492ms step_avg:43.53ms
step:1046/2110 train_time:45549ms step_avg:43.55ms
step:1047/2110 train_time:45609ms step_avg:43.56ms
step:1048/2110 train_time:45667ms step_avg:43.58ms
step:1049/2110 train_time:45728ms step_avg:43.59ms
step:1050/2110 train_time:45786ms step_avg:43.61ms
step:1051/2110 train_time:45845ms step_avg:43.62ms
step:1052/2110 train_time:45904ms step_avg:43.64ms
step:1053/2110 train_time:45964ms step_avg:43.65ms
step:1054/2110 train_time:46023ms step_avg:43.67ms
step:1055/2110 train_time:46083ms step_avg:43.68ms
step:1056/2110 train_time:46142ms step_avg:43.69ms
step:1057/2110 train_time:46201ms step_avg:43.71ms
step:1058/2110 train_time:46261ms step_avg:43.72ms
step:1059/2110 train_time:46319ms step_avg:43.74ms
step:1060/2110 train_time:46378ms step_avg:43.75ms
step:1061/2110 train_time:46437ms step_avg:43.77ms
step:1062/2110 train_time:46496ms step_avg:43.78ms
step:1063/2110 train_time:46555ms step_avg:43.80ms
step:1064/2110 train_time:46613ms step_avg:43.81ms
step:1065/2110 train_time:46672ms step_avg:43.82ms
step:1066/2110 train_time:46730ms step_avg:43.84ms
step:1067/2110 train_time:46790ms step_avg:43.85ms
step:1068/2110 train_time:46849ms step_avg:43.87ms
step:1069/2110 train_time:46909ms step_avg:43.88ms
step:1070/2110 train_time:46968ms step_avg:43.90ms
step:1071/2110 train_time:47028ms step_avg:43.91ms
step:1072/2110 train_time:47089ms step_avg:43.93ms
step:1073/2110 train_time:47147ms step_avg:43.94ms
step:1074/2110 train_time:47206ms step_avg:43.95ms
step:1075/2110 train_time:47267ms step_avg:43.97ms
step:1076/2110 train_time:47325ms step_avg:43.98ms
step:1077/2110 train_time:47386ms step_avg:44.00ms
step:1078/2110 train_time:47445ms step_avg:44.01ms
step:1079/2110 train_time:47504ms step_avg:44.03ms
step:1080/2110 train_time:47563ms step_avg:44.04ms
step:1081/2110 train_time:47622ms step_avg:44.05ms
step:1082/2110 train_time:47680ms step_avg:44.07ms
step:1083/2110 train_time:47739ms step_avg:44.08ms
step:1084/2110 train_time:47798ms step_avg:44.09ms
step:1085/2110 train_time:47857ms step_avg:44.11ms
step:1086/2110 train_time:47918ms step_avg:44.12ms
step:1087/2110 train_time:47976ms step_avg:44.14ms
step:1088/2110 train_time:48034ms step_avg:44.15ms
step:1089/2110 train_time:48094ms step_avg:44.16ms
step:1090/2110 train_time:48153ms step_avg:44.18ms
step:1091/2110 train_time:48215ms step_avg:44.19ms
step:1092/2110 train_time:48274ms step_avg:44.21ms
step:1093/2110 train_time:48333ms step_avg:44.22ms
step:1094/2110 train_time:48393ms step_avg:44.23ms
step:1095/2110 train_time:48451ms step_avg:44.25ms
step:1096/2110 train_time:48511ms step_avg:44.26ms
step:1097/2110 train_time:48570ms step_avg:44.28ms
step:1098/2110 train_time:48628ms step_avg:44.29ms
step:1099/2110 train_time:48688ms step_avg:44.30ms
step:1100/2110 train_time:48747ms step_avg:44.32ms
step:1101/2110 train_time:48807ms step_avg:44.33ms
step:1102/2110 train_time:48865ms step_avg:44.34ms
step:1103/2110 train_time:48926ms step_avg:44.36ms
step:1104/2110 train_time:48986ms step_avg:44.37ms
step:1105/2110 train_time:49044ms step_avg:44.38ms
step:1106/2110 train_time:49105ms step_avg:44.40ms
step:1107/2110 train_time:49163ms step_avg:44.41ms
step:1108/2110 train_time:49223ms step_avg:44.42ms
step:1109/2110 train_time:49282ms step_avg:44.44ms
step:1110/2110 train_time:49342ms step_avg:44.45ms
step:1111/2110 train_time:49402ms step_avg:44.47ms
step:1112/2110 train_time:49459ms step_avg:44.48ms
step:1113/2110 train_time:49518ms step_avg:44.49ms
step:1114/2110 train_time:49577ms step_avg:44.50ms
step:1115/2110 train_time:49637ms step_avg:44.52ms
step:1116/2110 train_time:49697ms step_avg:44.53ms
step:1117/2110 train_time:49754ms step_avg:44.54ms
step:1118/2110 train_time:49813ms step_avg:44.56ms
step:1119/2110 train_time:49873ms step_avg:44.57ms
step:1120/2110 train_time:49932ms step_avg:44.58ms
step:1121/2110 train_time:49992ms step_avg:44.60ms
step:1122/2110 train_time:50050ms step_avg:44.61ms
step:1123/2110 train_time:50109ms step_avg:44.62ms
step:1124/2110 train_time:50168ms step_avg:44.63ms
step:1125/2110 train_time:50229ms step_avg:44.65ms
step:1126/2110 train_time:50290ms step_avg:44.66ms
step:1127/2110 train_time:50348ms step_avg:44.67ms
step:1128/2110 train_time:50407ms step_avg:44.69ms
step:1129/2110 train_time:50467ms step_avg:44.70ms
step:1130/2110 train_time:50526ms step_avg:44.71ms
step:1131/2110 train_time:50587ms step_avg:44.73ms
step:1132/2110 train_time:50647ms step_avg:44.74ms
step:1133/2110 train_time:50705ms step_avg:44.75ms
step:1134/2110 train_time:50764ms step_avg:44.77ms
step:1135/2110 train_time:50824ms step_avg:44.78ms
step:1136/2110 train_time:50883ms step_avg:44.79ms
step:1137/2110 train_time:50942ms step_avg:44.80ms
step:1138/2110 train_time:51001ms step_avg:44.82ms
step:1139/2110 train_time:51060ms step_avg:44.83ms
step:1140/2110 train_time:51120ms step_avg:44.84ms
step:1141/2110 train_time:51178ms step_avg:44.85ms
step:1142/2110 train_time:51237ms step_avg:44.87ms
step:1143/2110 train_time:51297ms step_avg:44.88ms
step:1144/2110 train_time:51355ms step_avg:44.89ms
step:1145/2110 train_time:51418ms step_avg:44.91ms
step:1146/2110 train_time:51477ms step_avg:44.92ms
step:1147/2110 train_time:51537ms step_avg:44.93ms
step:1148/2110 train_time:51596ms step_avg:44.94ms
step:1149/2110 train_time:51656ms step_avg:44.96ms
step:1150/2110 train_time:51716ms step_avg:44.97ms
step:1151/2110 train_time:51775ms step_avg:44.98ms
step:1152/2110 train_time:51835ms step_avg:45.00ms
step:1153/2110 train_time:51896ms step_avg:45.01ms
step:1154/2110 train_time:51954ms step_avg:45.02ms
step:1155/2110 train_time:52014ms step_avg:45.03ms
step:1156/2110 train_time:52073ms step_avg:45.05ms
step:1157/2110 train_time:52134ms step_avg:45.06ms
step:1158/2110 train_time:52193ms step_avg:45.07ms
step:1159/2110 train_time:52253ms step_avg:45.08ms
step:1160/2110 train_time:52313ms step_avg:45.10ms
step:1161/2110 train_time:52373ms step_avg:45.11ms
step:1162/2110 train_time:52433ms step_avg:45.12ms
step:1163/2110 train_time:52494ms step_avg:45.14ms
step:1164/2110 train_time:52552ms step_avg:45.15ms
step:1165/2110 train_time:52613ms step_avg:45.16ms
step:1166/2110 train_time:52672ms step_avg:45.17ms
step:1167/2110 train_time:52733ms step_avg:45.19ms
step:1168/2110 train_time:52792ms step_avg:45.20ms
step:1169/2110 train_time:52852ms step_avg:45.21ms
step:1170/2110 train_time:52911ms step_avg:45.22ms
step:1171/2110 train_time:52971ms step_avg:45.24ms
step:1172/2110 train_time:53031ms step_avg:45.25ms
step:1173/2110 train_time:53091ms step_avg:45.26ms
step:1174/2110 train_time:53151ms step_avg:45.27ms
step:1175/2110 train_time:53211ms step_avg:45.29ms
step:1176/2110 train_time:53270ms step_avg:45.30ms
step:1177/2110 train_time:53331ms step_avg:45.31ms
step:1178/2110 train_time:53391ms step_avg:45.32ms
step:1179/2110 train_time:53451ms step_avg:45.34ms
step:1180/2110 train_time:53510ms step_avg:45.35ms
step:1181/2110 train_time:53571ms step_avg:45.36ms
step:1182/2110 train_time:53630ms step_avg:45.37ms
step:1183/2110 train_time:53690ms step_avg:45.38ms
step:1184/2110 train_time:53749ms step_avg:45.40ms
step:1185/2110 train_time:53810ms step_avg:45.41ms
step:1186/2110 train_time:53870ms step_avg:45.42ms
step:1187/2110 train_time:53931ms step_avg:45.43ms
step:1188/2110 train_time:53990ms step_avg:45.45ms
step:1189/2110 train_time:54050ms step_avg:45.46ms
step:1190/2110 train_time:54110ms step_avg:45.47ms
step:1191/2110 train_time:54170ms step_avg:45.48ms
step:1192/2110 train_time:54230ms step_avg:45.49ms
step:1193/2110 train_time:54290ms step_avg:45.51ms
step:1194/2110 train_time:54349ms step_avg:45.52ms
step:1195/2110 train_time:54410ms step_avg:45.53ms
step:1196/2110 train_time:54470ms step_avg:45.54ms
step:1197/2110 train_time:54531ms step_avg:45.56ms
step:1198/2110 train_time:54590ms step_avg:45.57ms
step:1199/2110 train_time:54650ms step_avg:45.58ms
step:1200/2110 train_time:54709ms step_avg:45.59ms
step:1201/2110 train_time:54770ms step_avg:45.60ms
step:1202/2110 train_time:54829ms step_avg:45.62ms
step:1203/2110 train_time:54891ms step_avg:45.63ms
step:1204/2110 train_time:54950ms step_avg:45.64ms
step:1205/2110 train_time:55010ms step_avg:45.65ms
step:1206/2110 train_time:55069ms step_avg:45.66ms
step:1207/2110 train_time:55129ms step_avg:45.67ms
step:1208/2110 train_time:55190ms step_avg:45.69ms
step:1209/2110 train_time:55249ms step_avg:45.70ms
step:1210/2110 train_time:55308ms step_avg:45.71ms
step:1211/2110 train_time:55369ms step_avg:45.72ms
step:1212/2110 train_time:55429ms step_avg:45.73ms
step:1213/2110 train_time:55489ms step_avg:45.75ms
step:1214/2110 train_time:55549ms step_avg:45.76ms
step:1215/2110 train_time:55609ms step_avg:45.77ms
step:1216/2110 train_time:55668ms step_avg:45.78ms
step:1217/2110 train_time:55730ms step_avg:45.79ms
step:1218/2110 train_time:55789ms step_avg:45.80ms
step:1219/2110 train_time:55849ms step_avg:45.82ms
step:1220/2110 train_time:55909ms step_avg:45.83ms
step:1221/2110 train_time:55969ms step_avg:45.84ms
step:1222/2110 train_time:56028ms step_avg:45.85ms
step:1223/2110 train_time:56089ms step_avg:45.86ms
step:1224/2110 train_time:56149ms step_avg:45.87ms
step:1225/2110 train_time:56208ms step_avg:45.88ms
step:1226/2110 train_time:56267ms step_avg:45.89ms
step:1227/2110 train_time:56329ms step_avg:45.91ms
step:1228/2110 train_time:56388ms step_avg:45.92ms
step:1229/2110 train_time:56448ms step_avg:45.93ms
step:1230/2110 train_time:56507ms step_avg:45.94ms
step:1231/2110 train_time:56567ms step_avg:45.95ms
step:1232/2110 train_time:56627ms step_avg:45.96ms
step:1233/2110 train_time:56687ms step_avg:45.98ms
step:1234/2110 train_time:56747ms step_avg:45.99ms
step:1235/2110 train_time:56807ms step_avg:46.00ms
step:1236/2110 train_time:56866ms step_avg:46.01ms
step:1237/2110 train_time:56926ms step_avg:46.02ms
step:1238/2110 train_time:56986ms step_avg:46.03ms
step:1239/2110 train_time:57046ms step_avg:46.04ms
step:1240/2110 train_time:57106ms step_avg:46.05ms
step:1241/2110 train_time:57165ms step_avg:46.06ms
step:1242/2110 train_time:57226ms step_avg:46.08ms
step:1243/2110 train_time:57286ms step_avg:46.09ms
step:1244/2110 train_time:57347ms step_avg:46.10ms
step:1245/2110 train_time:57406ms step_avg:46.11ms
step:1246/2110 train_time:57466ms step_avg:46.12ms
step:1247/2110 train_time:57526ms step_avg:46.13ms
step:1248/2110 train_time:57587ms step_avg:46.14ms
step:1249/2110 train_time:57647ms step_avg:46.15ms
step:1250/2110 train_time:57706ms step_avg:46.17ms
step:1250/2110 val_loss:3.5943 train_time:57767ms step_avg:46.21ms
step:1251/2110 train_time:57795ms step_avg:46.20ms
step:1252/2110 train_time:57828ms step_avg:46.19ms
step:1253/2110 train_time:57892ms step_avg:46.20ms
step:1254/2110 train_time:57955ms step_avg:46.22ms
step:1255/2110 train_time:58017ms step_avg:46.23ms
step:1256/2110 train_time:58076ms step_avg:46.24ms
step:1257/2110 train_time:58136ms step_avg:46.25ms
step:1258/2110 train_time:58194ms step_avg:46.26ms
step:1259/2110 train_time:58255ms step_avg:46.27ms
step:1260/2110 train_time:58313ms step_avg:46.28ms
step:1261/2110 train_time:58374ms step_avg:46.29ms
step:1262/2110 train_time:58432ms step_avg:46.30ms
step:1263/2110 train_time:58491ms step_avg:46.31ms
step:1264/2110 train_time:58549ms step_avg:46.32ms
step:1265/2110 train_time:58609ms step_avg:46.33ms
step:1266/2110 train_time:58668ms step_avg:46.34ms
step:1267/2110 train_time:58729ms step_avg:46.35ms
step:1268/2110 train_time:58789ms step_avg:46.36ms
step:1269/2110 train_time:58852ms step_avg:46.38ms
step:1270/2110 train_time:58913ms step_avg:46.39ms
step:1271/2110 train_time:58974ms step_avg:46.40ms
step:1272/2110 train_time:59034ms step_avg:46.41ms
step:1273/2110 train_time:59093ms step_avg:46.42ms
step:1274/2110 train_time:59152ms step_avg:46.43ms
step:1275/2110 train_time:59213ms step_avg:46.44ms
step:1276/2110 train_time:59271ms step_avg:46.45ms
step:1277/2110 train_time:59331ms step_avg:46.46ms
step:1278/2110 train_time:59390ms step_avg:46.47ms
step:1279/2110 train_time:59450ms step_avg:46.48ms
step:1280/2110 train_time:59507ms step_avg:46.49ms
step:1281/2110 train_time:59567ms step_avg:46.50ms
step:1282/2110 train_time:59626ms step_avg:46.51ms
step:1283/2110 train_time:59686ms step_avg:46.52ms
step:1284/2110 train_time:59746ms step_avg:46.53ms
step:1285/2110 train_time:59808ms step_avg:46.54ms
step:1286/2110 train_time:59869ms step_avg:46.55ms
step:1287/2110 train_time:59931ms step_avg:46.57ms
step:1288/2110 train_time:59990ms step_avg:46.58ms
step:1289/2110 train_time:60051ms step_avg:46.59ms
step:1290/2110 train_time:60110ms step_avg:46.60ms
step:1291/2110 train_time:60171ms step_avg:46.61ms
step:1292/2110 train_time:60231ms step_avg:46.62ms
step:1293/2110 train_time:60289ms step_avg:46.63ms
step:1294/2110 train_time:60348ms step_avg:46.64ms
step:1295/2110 train_time:60408ms step_avg:46.65ms
step:1296/2110 train_time:60467ms step_avg:46.66ms
step:1297/2110 train_time:60526ms step_avg:46.67ms
step:1298/2110 train_time:60585ms step_avg:46.68ms
step:1299/2110 train_time:60644ms step_avg:46.69ms
step:1300/2110 train_time:60703ms step_avg:46.69ms
step:1301/2110 train_time:60765ms step_avg:46.71ms
step:1302/2110 train_time:60825ms step_avg:46.72ms
step:1303/2110 train_time:60887ms step_avg:46.73ms
step:1304/2110 train_time:60947ms step_avg:46.74ms
step:1305/2110 train_time:61008ms step_avg:46.75ms
step:1306/2110 train_time:61067ms step_avg:46.76ms
step:1307/2110 train_time:61128ms step_avg:46.77ms
step:1308/2110 train_time:61188ms step_avg:46.78ms
step:1309/2110 train_time:61248ms step_avg:46.79ms
step:1310/2110 train_time:61308ms step_avg:46.80ms
step:1311/2110 train_time:61367ms step_avg:46.81ms
step:1312/2110 train_time:61426ms step_avg:46.82ms
step:1313/2110 train_time:61486ms step_avg:46.83ms
step:1314/2110 train_time:61544ms step_avg:46.84ms
step:1315/2110 train_time:61604ms step_avg:46.85ms
step:1316/2110 train_time:61663ms step_avg:46.86ms
step:1317/2110 train_time:61723ms step_avg:46.87ms
step:1318/2110 train_time:61783ms step_avg:46.88ms
step:1319/2110 train_time:61844ms step_avg:46.89ms
step:1320/2110 train_time:61903ms step_avg:46.90ms
step:1321/2110 train_time:61965ms step_avg:46.91ms
step:1322/2110 train_time:62024ms step_avg:46.92ms
step:1323/2110 train_time:62085ms step_avg:46.93ms
step:1324/2110 train_time:62145ms step_avg:46.94ms
step:1325/2110 train_time:62206ms step_avg:46.95ms
step:1326/2110 train_time:62266ms step_avg:46.96ms
step:1327/2110 train_time:62325ms step_avg:46.97ms
step:1328/2110 train_time:62383ms step_avg:46.98ms
step:1329/2110 train_time:62443ms step_avg:46.99ms
step:1330/2110 train_time:62502ms step_avg:46.99ms
step:1331/2110 train_time:62561ms step_avg:47.00ms
step:1332/2110 train_time:62619ms step_avg:47.01ms
step:1333/2110 train_time:62680ms step_avg:47.02ms
step:1334/2110 train_time:62739ms step_avg:47.03ms
step:1335/2110 train_time:62800ms step_avg:47.04ms
step:1336/2110 train_time:62859ms step_avg:47.05ms
step:1337/2110 train_time:62919ms step_avg:47.06ms
step:1338/2110 train_time:62979ms step_avg:47.07ms
step:1339/2110 train_time:63039ms step_avg:47.08ms
step:1340/2110 train_time:63098ms step_avg:47.09ms
step:1341/2110 train_time:63158ms step_avg:47.10ms
step:1342/2110 train_time:63217ms step_avg:47.11ms
step:1343/2110 train_time:63278ms step_avg:47.12ms
step:1344/2110 train_time:63337ms step_avg:47.13ms
step:1345/2110 train_time:63397ms step_avg:47.14ms
step:1346/2110 train_time:63456ms step_avg:47.14ms
step:1347/2110 train_time:63515ms step_avg:47.15ms
step:1348/2110 train_time:63575ms step_avg:47.16ms
step:1349/2110 train_time:63635ms step_avg:47.17ms
step:1350/2110 train_time:63694ms step_avg:47.18ms
step:1351/2110 train_time:63754ms step_avg:47.19ms
step:1352/2110 train_time:63813ms step_avg:47.20ms
step:1353/2110 train_time:63874ms step_avg:47.21ms
step:1354/2110 train_time:63934ms step_avg:47.22ms
step:1355/2110 train_time:63994ms step_avg:47.23ms
step:1356/2110 train_time:64053ms step_avg:47.24ms
step:1357/2110 train_time:64114ms step_avg:47.25ms
step:1358/2110 train_time:64174ms step_avg:47.26ms
step:1359/2110 train_time:64234ms step_avg:47.27ms
step:1360/2110 train_time:64292ms step_avg:47.27ms
step:1361/2110 train_time:64352ms step_avg:47.28ms
step:1362/2110 train_time:64411ms step_avg:47.29ms
step:1363/2110 train_time:64472ms step_avg:47.30ms
step:1364/2110 train_time:64531ms step_avg:47.31ms
step:1365/2110 train_time:64591ms step_avg:47.32ms
step:1366/2110 train_time:64650ms step_avg:47.33ms
step:1367/2110 train_time:64711ms step_avg:47.34ms
step:1368/2110 train_time:64770ms step_avg:47.35ms
step:1369/2110 train_time:64831ms step_avg:47.36ms
step:1370/2110 train_time:64890ms step_avg:47.36ms
step:1371/2110 train_time:64950ms step_avg:47.37ms
step:1372/2110 train_time:65010ms step_avg:47.38ms
step:1373/2110 train_time:65070ms step_avg:47.39ms
step:1374/2110 train_time:65129ms step_avg:47.40ms
step:1375/2110 train_time:65191ms step_avg:47.41ms
step:1376/2110 train_time:65250ms step_avg:47.42ms
step:1377/2110 train_time:65309ms step_avg:47.43ms
step:1378/2110 train_time:65369ms step_avg:47.44ms
step:1379/2110 train_time:65429ms step_avg:47.45ms
step:1380/2110 train_time:65490ms step_avg:47.46ms
step:1381/2110 train_time:65549ms step_avg:47.46ms
step:1382/2110 train_time:65635ms step_avg:47.49ms
step:1383/2110 train_time:65722ms step_avg:47.52ms
step:1384/2110 train_time:65808ms step_avg:47.55ms
step:1385/2110 train_time:65895ms step_avg:47.58ms
step:1386/2110 train_time:65983ms step_avg:47.61ms
step:1387/2110 train_time:66069ms step_avg:47.63ms
step:1388/2110 train_time:66157ms step_avg:47.66ms
step:1389/2110 train_time:66243ms step_avg:47.69ms
step:1390/2110 train_time:66329ms step_avg:47.72ms
step:1391/2110 train_time:66416ms step_avg:47.75ms
step:1392/2110 train_time:66502ms step_avg:47.77ms
step:1393/2110 train_time:66588ms step_avg:47.80ms
step:1394/2110 train_time:66674ms step_avg:47.83ms
step:1395/2110 train_time:66760ms step_avg:47.86ms
step:1396/2110 train_time:66846ms step_avg:47.88ms
step:1397/2110 train_time:66933ms step_avg:47.91ms
step:1398/2110 train_time:67020ms step_avg:47.94ms
step:1399/2110 train_time:67108ms step_avg:47.97ms
step:1400/2110 train_time:67195ms step_avg:48.00ms
step:1401/2110 train_time:67281ms step_avg:48.02ms
step:1402/2110 train_time:67367ms step_avg:48.05ms
step:1403/2110 train_time:67453ms step_avg:48.08ms
step:1404/2110 train_time:67540ms step_avg:48.11ms
step:1405/2110 train_time:67626ms step_avg:48.13ms
step:1406/2110 train_time:67713ms step_avg:48.16ms
step:1407/2110 train_time:67799ms step_avg:48.19ms
step:1408/2110 train_time:67886ms step_avg:48.21ms
step:1409/2110 train_time:67972ms step_avg:48.24ms
step:1410/2110 train_time:68059ms step_avg:48.27ms
step:1411/2110 train_time:68146ms step_avg:48.30ms
step:1412/2110 train_time:68233ms step_avg:48.32ms
step:1413/2110 train_time:68320ms step_avg:48.35ms
step:1414/2110 train_time:68406ms step_avg:48.38ms
step:1415/2110 train_time:68492ms step_avg:48.40ms
step:1416/2110 train_time:68579ms step_avg:48.43ms
step:1417/2110 train_time:68665ms step_avg:48.46ms
step:1418/2110 train_time:68751ms step_avg:48.48ms
step:1419/2110 train_time:68838ms step_avg:48.51ms
step:1420/2110 train_time:68924ms step_avg:48.54ms
step:1421/2110 train_time:69011ms step_avg:48.56ms
step:1422/2110 train_time:69097ms step_avg:48.59ms
step:1423/2110 train_time:69185ms step_avg:48.62ms
step:1424/2110 train_time:69272ms step_avg:48.65ms
step:1425/2110 train_time:69358ms step_avg:48.67ms
step:1426/2110 train_time:69444ms step_avg:48.70ms
step:1427/2110 train_time:69531ms step_avg:48.73ms
step:1428/2110 train_time:69618ms step_avg:48.75ms
step:1429/2110 train_time:69705ms step_avg:48.78ms
step:1430/2110 train_time:69791ms step_avg:48.81ms
step:1431/2110 train_time:69878ms step_avg:48.83ms
step:1432/2110 train_time:69965ms step_avg:48.86ms
step:1433/2110 train_time:70053ms step_avg:48.89ms
step:1434/2110 train_time:70138ms step_avg:48.91ms
step:1435/2110 train_time:70224ms step_avg:48.94ms
step:1436/2110 train_time:70310ms step_avg:48.96ms
step:1437/2110 train_time:70396ms step_avg:48.99ms
step:1438/2110 train_time:70483ms step_avg:49.01ms
step:1439/2110 train_time:70569ms step_avg:49.04ms
step:1440/2110 train_time:70656ms step_avg:49.07ms
step:1441/2110 train_time:70743ms step_avg:49.09ms
step:1442/2110 train_time:70829ms step_avg:49.12ms
step:1443/2110 train_time:70915ms step_avg:49.14ms
step:1444/2110 train_time:71003ms step_avg:49.17ms
step:1445/2110 train_time:71088ms step_avg:49.20ms
step:1446/2110 train_time:71176ms step_avg:49.22ms
step:1447/2110 train_time:71262ms step_avg:49.25ms
step:1448/2110 train_time:71348ms step_avg:49.27ms
step:1449/2110 train_time:71435ms step_avg:49.30ms
step:1450/2110 train_time:71522ms step_avg:49.33ms
step:1451/2110 train_time:71608ms step_avg:49.35ms
step:1452/2110 train_time:71695ms step_avg:49.38ms
step:1453/2110 train_time:71783ms step_avg:49.40ms
step:1454/2110 train_time:71870ms step_avg:49.43ms
step:1455/2110 train_time:71956ms step_avg:49.45ms
step:1456/2110 train_time:72042ms step_avg:49.48ms
step:1457/2110 train_time:72130ms step_avg:49.51ms
step:1458/2110 train_time:72217ms step_avg:49.53ms
step:1459/2110 train_time:72304ms step_avg:49.56ms
step:1460/2110 train_time:72391ms step_avg:49.58ms
step:1461/2110 train_time:72477ms step_avg:49.61ms
step:1462/2110 train_time:72563ms step_avg:49.63ms
step:1463/2110 train_time:72651ms step_avg:49.66ms
step:1464/2110 train_time:72738ms step_avg:49.68ms
step:1465/2110 train_time:72824ms step_avg:49.71ms
step:1466/2110 train_time:72910ms step_avg:49.73ms
step:1467/2110 train_time:72996ms step_avg:49.76ms
step:1468/2110 train_time:73083ms step_avg:49.78ms
step:1469/2110 train_time:73169ms step_avg:49.81ms
step:1470/2110 train_time:73256ms step_avg:49.83ms
step:1471/2110 train_time:73342ms step_avg:49.86ms
step:1472/2110 train_time:73430ms step_avg:49.88ms
step:1473/2110 train_time:73516ms step_avg:49.91ms
step:1474/2110 train_time:73602ms step_avg:49.93ms
step:1475/2110 train_time:73689ms step_avg:49.96ms
step:1476/2110 train_time:73776ms step_avg:49.98ms
step:1477/2110 train_time:73863ms step_avg:50.01ms
step:1478/2110 train_time:73949ms step_avg:50.03ms
step:1479/2110 train_time:74036ms step_avg:50.06ms
step:1480/2110 train_time:74122ms step_avg:50.08ms
step:1481/2110 train_time:74209ms step_avg:50.11ms
step:1482/2110 train_time:74296ms step_avg:50.13ms
step:1483/2110 train_time:74383ms step_avg:50.16ms
step:1484/2110 train_time:74470ms step_avg:50.18ms
step:1485/2110 train_time:74556ms step_avg:50.21ms
step:1486/2110 train_time:74642ms step_avg:50.23ms
step:1487/2110 train_time:74728ms step_avg:50.25ms
step:1488/2110 train_time:74816ms step_avg:50.28ms
step:1489/2110 train_time:74902ms step_avg:50.30ms
step:1490/2110 train_time:74988ms step_avg:50.33ms
step:1491/2110 train_time:75075ms step_avg:50.35ms
step:1492/2110 train_time:75162ms step_avg:50.38ms
step:1493/2110 train_time:75248ms step_avg:50.40ms
step:1494/2110 train_time:75334ms step_avg:50.42ms
step:1495/2110 train_time:75421ms step_avg:50.45ms
step:1496/2110 train_time:75506ms step_avg:50.47ms
step:1497/2110 train_time:75593ms step_avg:50.50ms
step:1498/2110 train_time:75681ms step_avg:50.52ms
step:1499/2110 train_time:75768ms step_avg:50.55ms
step:1500/2110 train_time:75854ms step_avg:50.57ms
step:1500/2110 val_loss:3.4956 train_time:75941ms step_avg:50.63ms
step:1501/2110 train_time:75976ms step_avg:50.62ms
step:1502/2110 train_time:76033ms step_avg:50.62ms
step:1503/2110 train_time:76126ms step_avg:50.65ms
step:1504/2110 train_time:76213ms step_avg:50.67ms
step:1505/2110 train_time:76300ms step_avg:50.70ms
step:1506/2110 train_time:76386ms step_avg:50.72ms
step:1507/2110 train_time:76471ms step_avg:50.74ms
step:1508/2110 train_time:76556ms step_avg:50.77ms
step:1509/2110 train_time:76642ms step_avg:50.79ms
step:1510/2110 train_time:76727ms step_avg:50.81ms
step:1511/2110 train_time:76814ms step_avg:50.84ms
step:1512/2110 train_time:76901ms step_avg:50.86ms
step:1513/2110 train_time:76992ms step_avg:50.89ms
step:1514/2110 train_time:77081ms step_avg:50.91ms
step:1515/2110 train_time:77170ms step_avg:50.94ms
step:1516/2110 train_time:77257ms step_avg:50.96ms
step:1517/2110 train_time:77344ms step_avg:50.98ms
step:1518/2110 train_time:77429ms step_avg:51.01ms
step:1519/2110 train_time:77515ms step_avg:51.03ms
step:1520/2110 train_time:77601ms step_avg:51.05ms
step:1521/2110 train_time:77686ms step_avg:51.08ms
step:1522/2110 train_time:77771ms step_avg:51.10ms
step:1523/2110 train_time:77859ms step_avg:51.12ms
step:1524/2110 train_time:77946ms step_avg:51.15ms
step:1525/2110 train_time:78035ms step_avg:51.17ms
step:1526/2110 train_time:78122ms step_avg:51.19ms
step:1527/2110 train_time:78211ms step_avg:51.22ms
step:1528/2110 train_time:78299ms step_avg:51.24ms
step:1529/2110 train_time:78384ms step_avg:51.26ms
step:1530/2110 train_time:78469ms step_avg:51.29ms
step:1531/2110 train_time:78555ms step_avg:51.31ms
step:1532/2110 train_time:78640ms step_avg:51.33ms
step:1533/2110 train_time:78729ms step_avg:51.36ms
step:1534/2110 train_time:78815ms step_avg:51.38ms
step:1535/2110 train_time:78901ms step_avg:51.40ms
step:1536/2110 train_time:78986ms step_avg:51.42ms
step:1537/2110 train_time:79074ms step_avg:51.45ms
step:1538/2110 train_time:79161ms step_avg:51.47ms
step:1539/2110 train_time:79249ms step_avg:51.49ms
step:1540/2110 train_time:79335ms step_avg:51.52ms
step:1541/2110 train_time:79422ms step_avg:51.54ms
step:1542/2110 train_time:79507ms step_avg:51.56ms
step:1543/2110 train_time:79593ms step_avg:51.58ms
step:1544/2110 train_time:79679ms step_avg:51.61ms
step:1545/2110 train_time:79765ms step_avg:51.63ms
step:1546/2110 train_time:79850ms step_avg:51.65ms
step:1547/2110 train_time:79938ms step_avg:51.67ms
step:1548/2110 train_time:80024ms step_avg:51.70ms
step:1549/2110 train_time:80114ms step_avg:51.72ms
step:1550/2110 train_time:80201ms step_avg:51.74ms
step:1551/2110 train_time:80287ms step_avg:51.76ms
step:1552/2110 train_time:80374ms step_avg:51.79ms
step:1553/2110 train_time:80460ms step_avg:51.81ms
step:1554/2110 train_time:80547ms step_avg:51.83ms
step:1555/2110 train_time:80632ms step_avg:51.85ms
step:1556/2110 train_time:80718ms step_avg:51.88ms
step:1557/2110 train_time:80807ms step_avg:51.90ms
step:1558/2110 train_time:80891ms step_avg:51.92ms
step:1559/2110 train_time:80979ms step_avg:51.94ms
step:1560/2110 train_time:81065ms step_avg:51.96ms
step:1561/2110 train_time:81153ms step_avg:51.99ms
step:1562/2110 train_time:81240ms step_avg:52.01ms
step:1563/2110 train_time:81328ms step_avg:52.03ms
step:1564/2110 train_time:81414ms step_avg:52.05ms
step:1565/2110 train_time:81500ms step_avg:52.08ms
step:1566/2110 train_time:81585ms step_avg:52.10ms
step:1567/2110 train_time:81672ms step_avg:52.12ms
step:1568/2110 train_time:81758ms step_avg:52.14ms
step:1569/2110 train_time:81845ms step_avg:52.16ms
step:1570/2110 train_time:81931ms step_avg:52.19ms
step:1571/2110 train_time:82018ms step_avg:52.21ms
step:1572/2110 train_time:82105ms step_avg:52.23ms
step:1573/2110 train_time:82193ms step_avg:52.25ms
step:1574/2110 train_time:82279ms step_avg:52.27ms
step:1575/2110 train_time:82366ms step_avg:52.30ms
step:1576/2110 train_time:82458ms step_avg:52.32ms
step:1577/2110 train_time:82539ms step_avg:52.34ms
step:1578/2110 train_time:82625ms step_avg:52.36ms
step:1579/2110 train_time:82711ms step_avg:52.38ms
step:1580/2110 train_time:82798ms step_avg:52.40ms
step:1581/2110 train_time:82885ms step_avg:52.43ms
step:1582/2110 train_time:82971ms step_avg:52.45ms
step:1583/2110 train_time:83059ms step_avg:52.47ms
step:1584/2110 train_time:83146ms step_avg:52.49ms
step:1585/2110 train_time:83234ms step_avg:52.51ms
step:1586/2110 train_time:83320ms step_avg:52.53ms
step:1587/2110 train_time:83408ms step_avg:52.56ms
step:1588/2110 train_time:83495ms step_avg:52.58ms
step:1589/2110 train_time:83580ms step_avg:52.60ms
step:1590/2110 train_time:83666ms step_avg:52.62ms
step:1591/2110 train_time:83752ms step_avg:52.64ms
step:1592/2110 train_time:83839ms step_avg:52.66ms
step:1593/2110 train_time:83926ms step_avg:52.68ms
step:1594/2110 train_time:84012ms step_avg:52.71ms
step:1595/2110 train_time:84099ms step_avg:52.73ms
step:1596/2110 train_time:84185ms step_avg:52.75ms
step:1597/2110 train_time:84273ms step_avg:52.77ms
step:1598/2110 train_time:84360ms step_avg:52.79ms
step:1599/2110 train_time:84447ms step_avg:52.81ms
step:1600/2110 train_time:84533ms step_avg:52.83ms
step:1601/2110 train_time:84620ms step_avg:52.85ms
step:1602/2110 train_time:84706ms step_avg:52.88ms
step:1603/2110 train_time:84793ms step_avg:52.90ms
step:1604/2110 train_time:84879ms step_avg:52.92ms
step:1605/2110 train_time:84965ms step_avg:52.94ms
step:1606/2110 train_time:85053ms step_avg:52.96ms
step:1607/2110 train_time:85139ms step_avg:52.98ms
step:1608/2110 train_time:85226ms step_avg:53.00ms
step:1609/2110 train_time:85313ms step_avg:53.02ms
step:1610/2110 train_time:85400ms step_avg:53.04ms
step:1611/2110 train_time:85487ms step_avg:53.06ms
step:1612/2110 train_time:85574ms step_avg:53.09ms
step:1613/2110 train_time:85659ms step_avg:53.11ms
step:1614/2110 train_time:85745ms step_avg:53.13ms
step:1615/2110 train_time:85832ms step_avg:53.15ms
step:1616/2110 train_time:85919ms step_avg:53.17ms
step:1617/2110 train_time:86005ms step_avg:53.19ms
step:1618/2110 train_time:86092ms step_avg:53.21ms
step:1619/2110 train_time:86178ms step_avg:53.23ms
step:1620/2110 train_time:86264ms step_avg:53.25ms
step:1621/2110 train_time:86352ms step_avg:53.27ms
step:1622/2110 train_time:86439ms step_avg:53.29ms
step:1623/2110 train_time:86526ms step_avg:53.31ms
step:1624/2110 train_time:86612ms step_avg:53.33ms
step:1625/2110 train_time:86698ms step_avg:53.35ms
step:1626/2110 train_time:86785ms step_avg:53.37ms
step:1627/2110 train_time:86871ms step_avg:53.39ms
step:1628/2110 train_time:86956ms step_avg:53.41ms
step:1629/2110 train_time:87045ms step_avg:53.43ms
step:1630/2110 train_time:87131ms step_avg:53.45ms
step:1631/2110 train_time:87219ms step_avg:53.48ms
step:1632/2110 train_time:87305ms step_avg:53.50ms
step:1633/2110 train_time:87393ms step_avg:53.52ms
step:1634/2110 train_time:87479ms step_avg:53.54ms
step:1635/2110 train_time:87566ms step_avg:53.56ms
step:1636/2110 train_time:87652ms step_avg:53.58ms
step:1637/2110 train_time:87739ms step_avg:53.60ms
step:1638/2110 train_time:87825ms step_avg:53.62ms
step:1639/2110 train_time:87911ms step_avg:53.64ms
step:1640/2110 train_time:87998ms step_avg:53.66ms
step:1641/2110 train_time:88086ms step_avg:53.68ms
step:1642/2110 train_time:88171ms step_avg:53.70ms
step:1643/2110 train_time:88258ms step_avg:53.72ms
step:1644/2110 train_time:88344ms step_avg:53.74ms
step:1645/2110 train_time:88431ms step_avg:53.76ms
step:1646/2110 train_time:88518ms step_avg:53.78ms
step:1647/2110 train_time:88604ms step_avg:53.80ms
step:1648/2110 train_time:88690ms step_avg:53.82ms
step:1649/2110 train_time:88778ms step_avg:53.84ms
step:1650/2110 train_time:88864ms step_avg:53.86ms
step:1651/2110 train_time:88951ms step_avg:53.88ms
step:1652/2110 train_time:89038ms step_avg:53.90ms
step:1653/2110 train_time:89125ms step_avg:53.92ms
step:1654/2110 train_time:89212ms step_avg:53.94ms
step:1655/2110 train_time:89299ms step_avg:53.96ms
step:1656/2110 train_time:89385ms step_avg:53.98ms
step:1657/2110 train_time:89472ms step_avg:54.00ms
step:1658/2110 train_time:89559ms step_avg:54.02ms
step:1659/2110 train_time:89648ms step_avg:54.04ms
step:1660/2110 train_time:89735ms step_avg:54.06ms
step:1661/2110 train_time:89823ms step_avg:54.08ms
step:1662/2110 train_time:89911ms step_avg:54.10ms
step:1663/2110 train_time:90000ms step_avg:54.12ms
step:1664/2110 train_time:90087ms step_avg:54.14ms
step:1665/2110 train_time:90176ms step_avg:54.16ms
step:1666/2110 train_time:90266ms step_avg:54.18ms
step:1667/2110 train_time:90354ms step_avg:54.20ms
step:1668/2110 train_time:90441ms step_avg:54.22ms
step:1669/2110 train_time:90529ms step_avg:54.24ms
step:1670/2110 train_time:90615ms step_avg:54.26ms
step:1671/2110 train_time:90704ms step_avg:54.28ms
step:1672/2110 train_time:90792ms step_avg:54.30ms
step:1673/2110 train_time:90880ms step_avg:54.32ms
step:1674/2110 train_time:90966ms step_avg:54.34ms
step:1675/2110 train_time:91056ms step_avg:54.36ms
step:1676/2110 train_time:91142ms step_avg:54.38ms
step:1677/2110 train_time:91231ms step_avg:54.40ms
step:1678/2110 train_time:91318ms step_avg:54.42ms
step:1679/2110 train_time:91408ms step_avg:54.44ms
step:1680/2110 train_time:91496ms step_avg:54.46ms
step:1681/2110 train_time:91584ms step_avg:54.48ms
step:1682/2110 train_time:91671ms step_avg:54.50ms
step:1683/2110 train_time:91759ms step_avg:54.52ms
step:1684/2110 train_time:91847ms step_avg:54.54ms
step:1685/2110 train_time:91937ms step_avg:54.56ms
step:1686/2110 train_time:92025ms step_avg:54.58ms
step:1687/2110 train_time:92114ms step_avg:54.60ms
step:1688/2110 train_time:92201ms step_avg:54.62ms
step:1689/2110 train_time:92289ms step_avg:54.64ms
step:1690/2110 train_time:92376ms step_avg:54.66ms
step:1691/2110 train_time:92465ms step_avg:54.68ms
step:1692/2110 train_time:92552ms step_avg:54.70ms
step:1693/2110 train_time:92640ms step_avg:54.72ms
step:1694/2110 train_time:92726ms step_avg:54.74ms
step:1695/2110 train_time:92815ms step_avg:54.76ms
step:1696/2110 train_time:92905ms step_avg:54.78ms
step:1697/2110 train_time:92993ms step_avg:54.80ms
step:1698/2110 train_time:93080ms step_avg:54.82ms
step:1699/2110 train_time:93168ms step_avg:54.84ms
step:1700/2110 train_time:93255ms step_avg:54.86ms
step:1701/2110 train_time:93343ms step_avg:54.88ms
step:1702/2110 train_time:93431ms step_avg:54.89ms
step:1703/2110 train_time:93519ms step_avg:54.91ms
step:1704/2110 train_time:93606ms step_avg:54.93ms
step:1705/2110 train_time:93695ms step_avg:54.95ms
step:1706/2110 train_time:93783ms step_avg:54.97ms
step:1707/2110 train_time:93872ms step_avg:54.99ms
step:1708/2110 train_time:93959ms step_avg:55.01ms
step:1709/2110 train_time:94048ms step_avg:55.03ms
step:1710/2110 train_time:94135ms step_avg:55.05ms
step:1711/2110 train_time:94223ms step_avg:55.07ms
step:1712/2110 train_time:94310ms step_avg:55.09ms
step:1713/2110 train_time:94398ms step_avg:55.11ms
step:1714/2110 train_time:94485ms step_avg:55.13ms
step:1715/2110 train_time:94574ms step_avg:55.15ms
step:1716/2110 train_time:94661ms step_avg:55.16ms
step:1717/2110 train_time:94750ms step_avg:55.18ms
step:1718/2110 train_time:94837ms step_avg:55.20ms
step:1719/2110 train_time:94927ms step_avg:55.22ms
step:1720/2110 train_time:95014ms step_avg:55.24ms
step:1721/2110 train_time:95101ms step_avg:55.26ms
step:1722/2110 train_time:95189ms step_avg:55.28ms
step:1723/2110 train_time:95277ms step_avg:55.30ms
step:1724/2110 train_time:95365ms step_avg:55.32ms
step:1725/2110 train_time:95454ms step_avg:55.34ms
step:1726/2110 train_time:95541ms step_avg:55.35ms
step:1727/2110 train_time:95630ms step_avg:55.37ms
step:1728/2110 train_time:95717ms step_avg:55.39ms
step:1729/2110 train_time:95805ms step_avg:55.41ms
step:1730/2110 train_time:95893ms step_avg:55.43ms
step:1731/2110 train_time:95982ms step_avg:55.45ms
step:1732/2110 train_time:96068ms step_avg:55.47ms
step:1733/2110 train_time:96157ms step_avg:55.49ms
step:1734/2110 train_time:96245ms step_avg:55.50ms
step:1735/2110 train_time:96333ms step_avg:55.52ms
step:1736/2110 train_time:96421ms step_avg:55.54ms
step:1737/2110 train_time:96510ms step_avg:55.56ms
step:1738/2110 train_time:96597ms step_avg:55.58ms
step:1739/2110 train_time:96686ms step_avg:55.60ms
step:1740/2110 train_time:96774ms step_avg:55.62ms
step:1741/2110 train_time:96862ms step_avg:55.64ms
step:1742/2110 train_time:96950ms step_avg:55.65ms
step:1743/2110 train_time:97038ms step_avg:55.67ms
step:1744/2110 train_time:97126ms step_avg:55.69ms
step:1745/2110 train_time:97215ms step_avg:55.71ms
step:1746/2110 train_time:97303ms step_avg:55.73ms
step:1747/2110 train_time:97391ms step_avg:55.75ms
step:1748/2110 train_time:97478ms step_avg:55.77ms
step:1749/2110 train_time:97567ms step_avg:55.78ms
step:1750/2110 train_time:97654ms step_avg:55.80ms
step:1750/2110 val_loss:3.3802 train_time:97745ms step_avg:55.85ms
step:1751/2110 train_time:97771ms step_avg:55.84ms
step:1752/2110 train_time:97835ms step_avg:55.84ms
step:1753/2110 train_time:97928ms step_avg:55.86ms
step:1754/2110 train_time:98015ms step_avg:55.88ms
step:1755/2110 train_time:98104ms step_avg:55.90ms
step:1756/2110 train_time:98190ms step_avg:55.92ms
step:1757/2110 train_time:98277ms step_avg:55.93ms
step:1758/2110 train_time:98364ms step_avg:55.95ms
step:1759/2110 train_time:98451ms step_avg:55.97ms
step:1760/2110 train_time:98537ms step_avg:55.99ms
step:1761/2110 train_time:98624ms step_avg:56.00ms
step:1762/2110 train_time:98713ms step_avg:56.02ms
step:1763/2110 train_time:98804ms step_avg:56.04ms
step:1764/2110 train_time:98894ms step_avg:56.06ms
step:1765/2110 train_time:98984ms step_avg:56.08ms
step:1766/2110 train_time:99071ms step_avg:56.10ms
step:1767/2110 train_time:99159ms step_avg:56.12ms
step:1768/2110 train_time:99245ms step_avg:56.13ms
step:1769/2110 train_time:99332ms step_avg:56.15ms
step:1770/2110 train_time:99418ms step_avg:56.17ms
step:1771/2110 train_time:99505ms step_avg:56.19ms
step:1772/2110 train_time:99592ms step_avg:56.20ms
step:1773/2110 train_time:99681ms step_avg:56.22ms
step:1774/2110 train_time:99771ms step_avg:56.24ms
step:1775/2110 train_time:99860ms step_avg:56.26ms
step:1776/2110 train_time:99949ms step_avg:56.28ms
step:1777/2110 train_time:100038ms step_avg:56.30ms
step:1778/2110 train_time:100125ms step_avg:56.31ms
step:1779/2110 train_time:100213ms step_avg:56.33ms
step:1780/2110 train_time:100300ms step_avg:56.35ms
step:1781/2110 train_time:100388ms step_avg:56.37ms
step:1782/2110 train_time:100473ms step_avg:56.38ms
step:1783/2110 train_time:100561ms step_avg:56.40ms
step:1784/2110 train_time:100649ms step_avg:56.42ms
step:1785/2110 train_time:100739ms step_avg:56.44ms
step:1786/2110 train_time:100827ms step_avg:56.45ms
step:1787/2110 train_time:100916ms step_avg:56.47ms
step:1788/2110 train_time:101005ms step_avg:56.49ms
step:1789/2110 train_time:101093ms step_avg:56.51ms
step:1790/2110 train_time:101179ms step_avg:56.52ms
step:1791/2110 train_time:101268ms step_avg:56.54ms
step:1792/2110 train_time:101354ms step_avg:56.56ms
step:1793/2110 train_time:101442ms step_avg:56.58ms
step:1794/2110 train_time:101529ms step_avg:56.59ms
step:1795/2110 train_time:101617ms step_avg:56.61ms
step:1796/2110 train_time:101704ms step_avg:56.63ms
step:1797/2110 train_time:101794ms step_avg:56.65ms
step:1798/2110 train_time:101882ms step_avg:56.66ms
step:1799/2110 train_time:101971ms step_avg:56.68ms
step:1800/2110 train_time:102059ms step_avg:56.70ms
step:1801/2110 train_time:102148ms step_avg:56.72ms
step:1802/2110 train_time:102235ms step_avg:56.73ms
step:1803/2110 train_time:102322ms step_avg:56.75ms
step:1804/2110 train_time:102409ms step_avg:56.77ms
step:1805/2110 train_time:102497ms step_avg:56.79ms
step:1806/2110 train_time:102584ms step_avg:56.80ms
step:1807/2110 train_time:102672ms step_avg:56.82ms
step:1808/2110 train_time:102760ms step_avg:56.84ms
step:1809/2110 train_time:102849ms step_avg:56.85ms
step:1810/2110 train_time:102936ms step_avg:56.87ms
step:1811/2110 train_time:103025ms step_avg:56.89ms
step:1812/2110 train_time:103112ms step_avg:56.90ms
step:1813/2110 train_time:103201ms step_avg:56.92ms
step:1814/2110 train_time:103288ms step_avg:56.94ms
step:1815/2110 train_time:103376ms step_avg:56.96ms
step:1816/2110 train_time:103464ms step_avg:56.97ms
step:1817/2110 train_time:103553ms step_avg:56.99ms
step:1818/2110 train_time:103640ms step_avg:57.01ms
step:1819/2110 train_time:103729ms step_avg:57.03ms
step:1820/2110 train_time:103816ms step_avg:57.04ms
step:1821/2110 train_time:103905ms step_avg:57.06ms
step:1822/2110 train_time:103992ms step_avg:57.08ms
step:1823/2110 train_time:104081ms step_avg:57.09ms
step:1824/2110 train_time:104168ms step_avg:57.11ms
step:1825/2110 train_time:104257ms step_avg:57.13ms
step:1826/2110 train_time:104344ms step_avg:57.14ms
step:1827/2110 train_time:104433ms step_avg:57.16ms
step:1828/2110 train_time:104520ms step_avg:57.18ms
step:1829/2110 train_time:104609ms step_avg:57.19ms
step:1830/2110 train_time:104696ms step_avg:57.21ms
step:1831/2110 train_time:104784ms step_avg:57.23ms
step:1832/2110 train_time:104871ms step_avg:57.24ms
step:1833/2110 train_time:104961ms step_avg:57.26ms
step:1834/2110 train_time:105048ms step_avg:57.28ms
step:1835/2110 train_time:105137ms step_avg:57.30ms
step:1836/2110 train_time:105224ms step_avg:57.31ms
step:1837/2110 train_time:105314ms step_avg:57.33ms
step:1838/2110 train_time:105401ms step_avg:57.35ms
step:1839/2110 train_time:105489ms step_avg:57.36ms
step:1840/2110 train_time:105576ms step_avg:57.38ms
step:1841/2110 train_time:105664ms step_avg:57.39ms
step:1842/2110 train_time:105751ms step_avg:57.41ms
step:1843/2110 train_time:105839ms step_avg:57.43ms
step:1844/2110 train_time:105926ms step_avg:57.44ms
step:1845/2110 train_time:106016ms step_avg:57.46ms
step:1846/2110 train_time:106103ms step_avg:57.48ms
step:1847/2110 train_time:106193ms step_avg:57.49ms
step:1848/2110 train_time:106281ms step_avg:57.51ms
step:1849/2110 train_time:106369ms step_avg:57.53ms
step:1850/2110 train_time:106456ms step_avg:57.54ms
step:1851/2110 train_time:106544ms step_avg:57.56ms
step:1852/2110 train_time:106631ms step_avg:57.58ms
step:1853/2110 train_time:106720ms step_avg:57.59ms
step:1854/2110 train_time:106807ms step_avg:57.61ms
step:1855/2110 train_time:106896ms step_avg:57.63ms
step:1856/2110 train_time:106983ms step_avg:57.64ms
step:1857/2110 train_time:107072ms step_avg:57.66ms
step:1858/2110 train_time:107159ms step_avg:57.67ms
step:1859/2110 train_time:107247ms step_avg:57.69ms
step:1860/2110 train_time:107334ms step_avg:57.71ms
step:1861/2110 train_time:107423ms step_avg:57.72ms
step:1862/2110 train_time:107509ms step_avg:57.74ms
step:1863/2110 train_time:107598ms step_avg:57.76ms
step:1864/2110 train_time:107686ms step_avg:57.77ms
step:1865/2110 train_time:107775ms step_avg:57.79ms
step:1866/2110 train_time:107863ms step_avg:57.80ms
step:1867/2110 train_time:107951ms step_avg:57.82ms
step:1868/2110 train_time:108038ms step_avg:57.84ms
step:1869/2110 train_time:108127ms step_avg:57.85ms
step:1870/2110 train_time:108215ms step_avg:57.87ms
step:1871/2110 train_time:108303ms step_avg:57.89ms
step:1872/2110 train_time:108390ms step_avg:57.90ms
step:1873/2110 train_time:108479ms step_avg:57.92ms
step:1874/2110 train_time:108566ms step_avg:57.93ms
step:1875/2110 train_time:108655ms step_avg:57.95ms
step:1876/2110 train_time:108741ms step_avg:57.96ms
step:1877/2110 train_time:108830ms step_avg:57.98ms
step:1878/2110 train_time:108917ms step_avg:58.00ms
step:1879/2110 train_time:109005ms step_avg:58.01ms
step:1880/2110 train_time:109092ms step_avg:58.03ms
step:1881/2110 train_time:109181ms step_avg:58.04ms
step:1882/2110 train_time:109268ms step_avg:58.06ms
step:1883/2110 train_time:109357ms step_avg:58.08ms
step:1884/2110 train_time:109444ms step_avg:58.09ms
step:1885/2110 train_time:109533ms step_avg:58.11ms
step:1886/2110 train_time:109619ms step_avg:58.12ms
step:1887/2110 train_time:109708ms step_avg:58.14ms
step:1888/2110 train_time:109794ms step_avg:58.15ms
step:1889/2110 train_time:109883ms step_avg:58.17ms
step:1890/2110 train_time:109970ms step_avg:58.19ms
step:1891/2110 train_time:110059ms step_avg:58.20ms
step:1892/2110 train_time:110146ms step_avg:58.22ms
step:1893/2110 train_time:110234ms step_avg:58.23ms
step:1894/2110 train_time:110322ms step_avg:58.25ms
step:1895/2110 train_time:110411ms step_avg:58.26ms
step:1896/2110 train_time:110498ms step_avg:58.28ms
step:1897/2110 train_time:110586ms step_avg:58.30ms
step:1898/2110 train_time:110673ms step_avg:58.31ms
step:1899/2110 train_time:110761ms step_avg:58.33ms
step:1900/2110 train_time:110849ms step_avg:58.34ms
step:1901/2110 train_time:110939ms step_avg:58.36ms
step:1902/2110 train_time:111027ms step_avg:58.37ms
step:1903/2110 train_time:111115ms step_avg:58.39ms
step:1904/2110 train_time:111202ms step_avg:58.40ms
step:1905/2110 train_time:111291ms step_avg:58.42ms
step:1906/2110 train_time:111377ms step_avg:58.44ms
step:1907/2110 train_time:111466ms step_avg:58.45ms
step:1908/2110 train_time:111553ms step_avg:58.47ms
step:1909/2110 train_time:111641ms step_avg:58.48ms
step:1910/2110 train_time:111729ms step_avg:58.50ms
step:1911/2110 train_time:111817ms step_avg:58.51ms
step:1912/2110 train_time:111904ms step_avg:58.53ms
step:1913/2110 train_time:111993ms step_avg:58.54ms
step:1914/2110 train_time:112080ms step_avg:58.56ms
step:1915/2110 train_time:112169ms step_avg:58.57ms
step:1916/2110 train_time:112257ms step_avg:58.59ms
step:1917/2110 train_time:112345ms step_avg:58.60ms
step:1918/2110 train_time:112432ms step_avg:58.62ms
step:1919/2110 train_time:112521ms step_avg:58.64ms
step:1920/2110 train_time:112608ms step_avg:58.65ms
step:1921/2110 train_time:112697ms step_avg:58.67ms
step:1922/2110 train_time:112785ms step_avg:58.68ms
step:1923/2110 train_time:112873ms step_avg:58.70ms
step:1924/2110 train_time:112961ms step_avg:58.71ms
step:1925/2110 train_time:113049ms step_avg:58.73ms
step:1926/2110 train_time:113136ms step_avg:58.74ms
step:1927/2110 train_time:113225ms step_avg:58.76ms
step:1928/2110 train_time:113312ms step_avg:58.77ms
step:1929/2110 train_time:113401ms step_avg:58.79ms
step:1930/2110 train_time:113488ms step_avg:58.80ms
step:1931/2110 train_time:113577ms step_avg:58.82ms
step:1932/2110 train_time:113664ms step_avg:58.83ms
step:1933/2110 train_time:113753ms step_avg:58.85ms
step:1934/2110 train_time:113841ms step_avg:58.86ms
step:1935/2110 train_time:113929ms step_avg:58.88ms
step:1936/2110 train_time:114017ms step_avg:58.89ms
step:1937/2110 train_time:114104ms step_avg:58.91ms
step:1938/2110 train_time:114191ms step_avg:58.92ms
step:1939/2110 train_time:114280ms step_avg:58.94ms
step:1940/2110 train_time:114367ms step_avg:58.95ms
step:1941/2110 train_time:114456ms step_avg:58.97ms
step:1942/2110 train_time:114542ms step_avg:58.98ms
step:1943/2110 train_time:114633ms step_avg:59.00ms
step:1944/2110 train_time:114720ms step_avg:59.01ms
step:1945/2110 train_time:114809ms step_avg:59.03ms
step:1946/2110 train_time:114897ms step_avg:59.04ms
step:1947/2110 train_time:114985ms step_avg:59.06ms
step:1948/2110 train_time:115072ms step_avg:59.07ms
step:1949/2110 train_time:115161ms step_avg:59.09ms
step:1950/2110 train_time:115248ms step_avg:59.10ms
step:1951/2110 train_time:115337ms step_avg:59.12ms
step:1952/2110 train_time:115424ms step_avg:59.13ms
step:1953/2110 train_time:115513ms step_avg:59.15ms
step:1954/2110 train_time:115599ms step_avg:59.16ms
step:1955/2110 train_time:115688ms step_avg:59.18ms
step:1956/2110 train_time:115775ms step_avg:59.19ms
step:1957/2110 train_time:115864ms step_avg:59.20ms
step:1958/2110 train_time:115951ms step_avg:59.22ms
step:1959/2110 train_time:116040ms step_avg:59.23ms
step:1960/2110 train_time:116129ms step_avg:59.25ms
step:1961/2110 train_time:116216ms step_avg:59.26ms
step:1962/2110 train_time:116303ms step_avg:59.28ms
step:1963/2110 train_time:116392ms step_avg:59.29ms
step:1964/2110 train_time:116479ms step_avg:59.31ms
step:1965/2110 train_time:116568ms step_avg:59.32ms
step:1966/2110 train_time:116655ms step_avg:59.34ms
step:1967/2110 train_time:116744ms step_avg:59.35ms
step:1968/2110 train_time:116832ms step_avg:59.37ms
step:1969/2110 train_time:116920ms step_avg:59.38ms
step:1970/2110 train_time:117007ms step_avg:59.39ms
step:1971/2110 train_time:117096ms step_avg:59.41ms
step:1972/2110 train_time:117183ms step_avg:59.42ms
step:1973/2110 train_time:117273ms step_avg:59.44ms
step:1974/2110 train_time:117360ms step_avg:59.45ms
step:1975/2110 train_time:117449ms step_avg:59.47ms
step:1976/2110 train_time:117536ms step_avg:59.48ms
step:1977/2110 train_time:117625ms step_avg:59.50ms
step:1978/2110 train_time:117712ms step_avg:59.51ms
step:1979/2110 train_time:117801ms step_avg:59.53ms
step:1980/2110 train_time:117889ms step_avg:59.54ms
step:1981/2110 train_time:117978ms step_avg:59.55ms
step:1982/2110 train_time:118065ms step_avg:59.57ms
step:1983/2110 train_time:118153ms step_avg:59.58ms
step:1984/2110 train_time:118241ms step_avg:59.60ms
step:1985/2110 train_time:118329ms step_avg:59.61ms
step:1986/2110 train_time:118417ms step_avg:59.63ms
step:1987/2110 train_time:118506ms step_avg:59.64ms
step:1988/2110 train_time:118593ms step_avg:59.65ms
step:1989/2110 train_time:118682ms step_avg:59.67ms
step:1990/2110 train_time:118768ms step_avg:59.68ms
step:1991/2110 train_time:118857ms step_avg:59.70ms
step:1992/2110 train_time:118945ms step_avg:59.71ms
step:1993/2110 train_time:119034ms step_avg:59.73ms
step:1994/2110 train_time:119121ms step_avg:59.74ms
step:1995/2110 train_time:119210ms step_avg:59.75ms
step:1996/2110 train_time:119297ms step_avg:59.77ms
step:1997/2110 train_time:119386ms step_avg:59.78ms
step:1998/2110 train_time:119473ms step_avg:59.80ms
step:1999/2110 train_time:119562ms step_avg:59.81ms
step:2000/2110 train_time:119650ms step_avg:59.82ms
step:2000/2110 val_loss:3.3048 train_time:119739ms step_avg:59.87ms
step:2001/2110 train_time:119774ms step_avg:59.86ms
step:2002/2110 train_time:119831ms step_avg:59.86ms
step:2003/2110 train_time:119922ms step_avg:59.87ms
step:2004/2110 train_time:120010ms step_avg:59.88ms
step:2005/2110 train_time:120098ms step_avg:59.90ms
step:2006/2110 train_time:120184ms step_avg:59.91ms
step:2007/2110 train_time:120271ms step_avg:59.93ms
step:2008/2110 train_time:120357ms step_avg:59.94ms
step:2009/2110 train_time:120445ms step_avg:59.95ms
step:2010/2110 train_time:120532ms step_avg:59.97ms
step:2011/2110 train_time:120620ms step_avg:59.98ms
step:2012/2110 train_time:120708ms step_avg:59.99ms
step:2013/2110 train_time:120801ms step_avg:60.01ms
step:2014/2110 train_time:120891ms step_avg:60.03ms
step:2015/2110 train_time:120980ms step_avg:60.04ms
step:2016/2110 train_time:121067ms step_avg:60.05ms
step:2017/2110 train_time:121155ms step_avg:60.07ms
step:2018/2110 train_time:121241ms step_avg:60.08ms
step:2019/2110 train_time:121328ms step_avg:60.09ms
step:2020/2110 train_time:121415ms step_avg:60.11ms
step:2021/2110 train_time:121502ms step_avg:60.12ms
step:2022/2110 train_time:121589ms step_avg:60.13ms
step:2023/2110 train_time:121678ms step_avg:60.15ms
step:2024/2110 train_time:121766ms step_avg:60.16ms
step:2025/2110 train_time:121857ms step_avg:60.18ms
step:2026/2110 train_time:121945ms step_avg:60.19ms
step:2027/2110 train_time:122034ms step_avg:60.20ms
step:2028/2110 train_time:122121ms step_avg:60.22ms
step:2029/2110 train_time:122209ms step_avg:60.23ms
step:2030/2110 train_time:122295ms step_avg:60.24ms
step:2031/2110 train_time:122384ms step_avg:60.26ms
step:2032/2110 train_time:122470ms step_avg:60.27ms
step:2033/2110 train_time:122558ms step_avg:60.28ms
step:2034/2110 train_time:122645ms step_avg:60.30ms
step:2035/2110 train_time:122736ms step_avg:60.31ms
step:2036/2110 train_time:122823ms step_avg:60.33ms
step:2037/2110 train_time:122913ms step_avg:60.34ms
step:2038/2110 train_time:123000ms step_avg:60.35ms
step:2039/2110 train_time:123089ms step_avg:60.37ms
step:2040/2110 train_time:123176ms step_avg:60.38ms
step:2041/2110 train_time:123264ms step_avg:60.39ms
step:2042/2110 train_time:123351ms step_avg:60.41ms
step:2043/2110 train_time:123438ms step_avg:60.42ms
step:2044/2110 train_time:123525ms step_avg:60.43ms
step:2045/2110 train_time:123612ms step_avg:60.45ms
step:2046/2110 train_time:123700ms step_avg:60.46ms
step:2047/2110 train_time:123789ms step_avg:60.47ms
step:2048/2110 train_time:123877ms step_avg:60.49ms
step:2049/2110 train_time:123966ms step_avg:60.50ms
step:2050/2110 train_time:124055ms step_avg:60.51ms
step:2051/2110 train_time:124143ms step_avg:60.53ms
step:2052/2110 train_time:124230ms step_avg:60.54ms
step:2053/2110 train_time:124318ms step_avg:60.55ms
step:2054/2110 train_time:124405ms step_avg:60.57ms
step:2055/2110 train_time:124493ms step_avg:60.58ms
step:2056/2110 train_time:124580ms step_avg:60.59ms
step:2057/2110 train_time:124668ms step_avg:60.61ms
step:2058/2110 train_time:124756ms step_avg:60.62ms
step:2059/2110 train_time:124846ms step_avg:60.63ms
step:2060/2110 train_time:124934ms step_avg:60.65ms
step:2061/2110 train_time:125023ms step_avg:60.66ms
step:2062/2110 train_time:125111ms step_avg:60.67ms
step:2063/2110 train_time:125199ms step_avg:60.69ms
step:2064/2110 train_time:125286ms step_avg:60.70ms
step:2065/2110 train_time:125374ms step_avg:60.71ms
step:2066/2110 train_time:125461ms step_avg:60.73ms
step:2067/2110 train_time:125550ms step_avg:60.74ms
step:2068/2110 train_time:125637ms step_avg:60.75ms
step:2069/2110 train_time:125727ms step_avg:60.77ms
step:2070/2110 train_time:125814ms step_avg:60.78ms
step:2071/2110 train_time:125904ms step_avg:60.79ms
step:2072/2110 train_time:125992ms step_avg:60.81ms
step:2073/2110 train_time:126080ms step_avg:60.82ms
step:2074/2110 train_time:126167ms step_avg:60.83ms
step:2075/2110 train_time:126256ms step_avg:60.85ms
step:2076/2110 train_time:126343ms step_avg:60.86ms
step:2077/2110 train_time:126432ms step_avg:60.87ms
step:2078/2110 train_time:126520ms step_avg:60.89ms
step:2079/2110 train_time:126608ms step_avg:60.90ms
step:2080/2110 train_time:126695ms step_avg:60.91ms
step:2081/2110 train_time:126785ms step_avg:60.92ms
step:2082/2110 train_time:126872ms step_avg:60.94ms
step:2083/2110 train_time:126962ms step_avg:60.95ms
step:2084/2110 train_time:127049ms step_avg:60.96ms
step:2085/2110 train_time:127139ms step_avg:60.98ms
step:2086/2110 train_time:127228ms step_avg:60.99ms
step:2087/2110 train_time:127316ms step_avg:61.00ms
step:2088/2110 train_time:127402ms step_avg:61.02ms
step:2089/2110 train_time:127491ms step_avg:61.03ms
step:2090/2110 train_time:127579ms step_avg:61.04ms
step:2091/2110 train_time:127669ms step_avg:61.06ms
step:2092/2110 train_time:127757ms step_avg:61.07ms
step:2093/2110 train_time:127845ms step_avg:61.08ms
step:2094/2110 train_time:127934ms step_avg:61.10ms
step:2095/2110 train_time:128023ms step_avg:61.11ms
step:2096/2110 train_time:128110ms step_avg:61.12ms
step:2097/2110 train_time:128200ms step_avg:61.13ms
step:2098/2110 train_time:128287ms step_avg:61.15ms
step:2099/2110 train_time:128376ms step_avg:61.16ms
step:2100/2110 train_time:128463ms step_avg:61.17ms
step:2101/2110 train_time:128552ms step_avg:61.19ms
step:2102/2110 train_time:128639ms step_avg:61.20ms
step:2103/2110 train_time:128728ms step_avg:61.21ms
step:2104/2110 train_time:128815ms step_avg:61.22ms
step:2105/2110 train_time:128904ms step_avg:61.24ms
step:2106/2110 train_time:128992ms step_avg:61.25ms
step:2107/2110 train_time:129082ms step_avg:61.26ms
step:2108/2110 train_time:129171ms step_avg:61.28ms
step:2109/2110 train_time:129260ms step_avg:61.29ms
step:2110/2110 train_time:129348ms step_avg:61.30ms
step:2110/2110 val_loss:3.2802 train_time:129438ms step_avg:61.35ms
peak memory allocated: 29892 MiB reserved: 38856 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (2, 5, 11)
    ws_transitions: tuple = (0.55, 0.80, 1.0)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = 0
    for i, threshold in enumerate(args.ws_transitions):
        if x < threshold:
            ws_idx = i
            break
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 3
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)

training_configs = []
for step in range(args.num_iterations + 1):
    _, ws_long = get_ws(step)
    bs = get_bs(step)
    if not training_configs or (bs, ws_long) != training_configs[-1]:
        training_configs.append((bs, ws_long))

seen = set()
unique_configs = []
for config in training_configs:
    if config not in seen:
        seen.add(config)
        unique_configs.append(config)

if master_process:
    print0(f"Warmup: {len(unique_configs)} unique (batch_size, window_size) configurations", console=True)

train_loader = distributed_data_generator(
    args.train_files, unique_configs[0][0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps
)

ws_long = unique_configs[0][1]
model.train()
model.yarn.reset()

for idx, (bs, new_ws_long) in enumerate(unique_configs):
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    send_args = (bs, args.train_max_seq_len, grad_accum_steps) if idx > 0 else None
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        send_args = None
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
ws_schedule = list(args.ws_schedule) + [args.ws_final]
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 01:58:27) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 15 23:34:01 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:7F:00.0 Off |                    0 |
| N/A   39C    P0            126W /  700W |    5874MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:8F:00.0 Off |                    0 |
| N/A   43C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:9F:00.0 Off |                    0 |
| N/A   43C    P0            127W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0            122W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:CF:00.0 Off |                    0 |
| N/A   43C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:DF:00.0 Off |                    0 |
| N/A   43C    P0            125W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:EF:00.0 Off |                    0 |
| N/A   38C    P0            121W /  700W |    1521MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              81      C   /usr/local/bin/python                  1512MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              83      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              84      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              85      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              86      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              87      C   /usr/local/bin/python                   616MiB |
|    0   N/A  N/A              88      C   /usr/local/bin/python                   616MiB |
|    1   N/A  N/A              82      C   /usr/local/bin/python                  1512MiB |
|    2   N/A  N/A              83      C   /usr/local/bin/python                  1512MiB |
|    3   N/A  N/A              84      C   /usr/local/bin/python                  1512MiB |
|    4   N/A  N/A              85      C   /usr/local/bin/python                  1512MiB |
|    5   N/A  N/A              86      C   /usr/local/bin/python                  1512MiB |
|    6   N/A  N/A              87      C   /usr/local/bin/python                  1512MiB |
|    7   N/A  N/A              88      C   /usr/local/bin/python                  1512MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Warmup: 6 unique (batch_size, window_size) configurations
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2110 train_time:104ms step_avg:104.29ms
step:2/2110 train_time:139ms step_avg:69.63ms
step:3/2110 train_time:170ms step_avg:56.68ms
step:4/2110 train_time:202ms step_avg:50.51ms
step:5/2110 train_time:233ms step_avg:46.60ms
step:6/2110 train_time:604ms step_avg:100.72ms
step:7/2110 train_time:634ms step_avg:90.56ms
step:8/2110 train_time:663ms step_avg:82.83ms
step:9/2110 train_time:689ms step_avg:76.59ms
step:10/2110 train_time:719ms step_avg:71.87ms
step:11/2110 train_time:745ms step_avg:67.76ms
step:12/2110 train_time:777ms step_avg:64.73ms
step:13/2110 train_time:809ms step_avg:62.26ms
step:14/2110 train_time:842ms step_avg:60.17ms
step:15/2110 train_time:875ms step_avg:58.33ms
step:16/2110 train_time:908ms step_avg:56.77ms
step:17/2110 train_time:941ms step_avg:55.35ms
step:18/2110 train_time:974ms step_avg:54.09ms
step:19/2110 train_time:1008ms step_avg:53.03ms
step:20/2110 train_time:1040ms step_avg:51.98ms
step:21/2110 train_time:1072ms step_avg:51.06ms
step:22/2110 train_time:1106ms step_avg:50.26ms
step:23/2110 train_time:1138ms step_avg:49.48ms
step:24/2110 train_time:1172ms step_avg:48.82ms
step:25/2110 train_time:1205ms step_avg:48.19ms
step:26/2110 train_time:1237ms step_avg:47.57ms
step:27/2110 train_time:1270ms step_avg:47.03ms
step:28/2110 train_time:1303ms step_avg:46.53ms
step:29/2110 train_time:1336ms step_avg:46.06ms
step:30/2110 train_time:1369ms step_avg:45.62ms
step:31/2110 train_time:1402ms step_avg:45.23ms
step:32/2110 train_time:1435ms step_avg:44.83ms
step:33/2110 train_time:1468ms step_avg:44.49ms
step:34/2110 train_time:1503ms step_avg:44.21ms
step:35/2110 train_time:1537ms step_avg:43.93ms
step:36/2110 train_time:1572ms step_avg:43.68ms
step:37/2110 train_time:1605ms step_avg:43.38ms
step:38/2110 train_time:1639ms step_avg:43.14ms
step:39/2110 train_time:1672ms step_avg:42.87ms
step:40/2110 train_time:1706ms step_avg:42.65ms
step:41/2110 train_time:1738ms step_avg:42.40ms
step:42/2110 train_time:1772ms step_avg:42.19ms
step:43/2110 train_time:1805ms step_avg:41.97ms
step:44/2110 train_time:1837ms step_avg:41.75ms
step:45/2110 train_time:1871ms step_avg:41.58ms
step:46/2110 train_time:1904ms step_avg:41.39ms
step:47/2110 train_time:1937ms step_avg:41.22ms
step:48/2110 train_time:1970ms step_avg:41.04ms
step:49/2110 train_time:2003ms step_avg:40.88ms
step:50/2110 train_time:2037ms step_avg:40.73ms
step:51/2110 train_time:2070ms step_avg:40.58ms
step:52/2110 train_time:2103ms step_avg:40.45ms
step:53/2110 train_time:2136ms step_avg:40.30ms
step:54/2110 train_time:2169ms step_avg:40.17ms
step:55/2110 train_time:2203ms step_avg:40.05ms
step:56/2110 train_time:2235ms step_avg:39.91ms
step:57/2110 train_time:2268ms step_avg:39.79ms
step:58/2110 train_time:2300ms step_avg:39.66ms
step:59/2110 train_time:2334ms step_avg:39.56ms
step:60/2110 train_time:2367ms step_avg:39.45ms
step:61/2110 train_time:2400ms step_avg:39.34ms
step:62/2110 train_time:2432ms step_avg:39.23ms
step:63/2110 train_time:2466ms step_avg:39.15ms
step:64/2110 train_time:2499ms step_avg:39.05ms
step:65/2110 train_time:2533ms step_avg:38.97ms
step:66/2110 train_time:2566ms step_avg:38.87ms
step:67/2110 train_time:2600ms step_avg:38.80ms
step:68/2110 train_time:2632ms step_avg:38.71ms
step:69/2110 train_time:2666ms step_avg:38.64ms
step:70/2110 train_time:2699ms step_avg:38.55ms
step:71/2110 train_time:2732ms step_avg:38.48ms
step:72/2110 train_time:2765ms step_avg:38.41ms
step:73/2110 train_time:2800ms step_avg:38.35ms
step:74/2110 train_time:2831ms step_avg:38.26ms
step:75/2110 train_time:2865ms step_avg:38.20ms
step:76/2110 train_time:2897ms step_avg:38.12ms
step:77/2110 train_time:2931ms step_avg:38.07ms
step:78/2110 train_time:2964ms step_avg:37.99ms
step:79/2110 train_time:2997ms step_avg:37.94ms
step:80/2110 train_time:3029ms step_avg:37.87ms
step:81/2110 train_time:3063ms step_avg:37.81ms
step:82/2110 train_time:3097ms step_avg:37.77ms
step:83/2110 train_time:3129ms step_avg:37.70ms
step:84/2110 train_time:3162ms step_avg:37.64ms
step:85/2110 train_time:3195ms step_avg:37.59ms
step:86/2110 train_time:3228ms step_avg:37.53ms
step:87/2110 train_time:3261ms step_avg:37.48ms
step:88/2110 train_time:3294ms step_avg:37.43ms
step:89/2110 train_time:3327ms step_avg:37.38ms
step:90/2110 train_time:3360ms step_avg:37.34ms
step:91/2110 train_time:3393ms step_avg:37.28ms
step:92/2110 train_time:3426ms step_avg:37.23ms
step:93/2110 train_time:3459ms step_avg:37.19ms
step:94/2110 train_time:3492ms step_avg:37.15ms
step:95/2110 train_time:3525ms step_avg:37.11ms
step:96/2110 train_time:3558ms step_avg:37.06ms
step:97/2110 train_time:3591ms step_avg:37.03ms
step:98/2110 train_time:3624ms step_avg:36.98ms
step:99/2110 train_time:3658ms step_avg:36.95ms
step:100/2110 train_time:3690ms step_avg:36.90ms
step:101/2110 train_time:3724ms step_avg:36.87ms
step:102/2110 train_time:3756ms step_avg:36.83ms
step:103/2110 train_time:3790ms step_avg:36.79ms
step:104/2110 train_time:3823ms step_avg:36.76ms
step:105/2110 train_time:3856ms step_avg:36.73ms
step:106/2110 train_time:3889ms step_avg:36.69ms
step:107/2110 train_time:3923ms step_avg:36.66ms
step:108/2110 train_time:3955ms step_avg:36.62ms
step:109/2110 train_time:3989ms step_avg:36.59ms
step:110/2110 train_time:4021ms step_avg:36.56ms
step:111/2110 train_time:4055ms step_avg:36.53ms
step:112/2110 train_time:4087ms step_avg:36.49ms
step:113/2110 train_time:4120ms step_avg:36.46ms
step:114/2110 train_time:4153ms step_avg:36.43ms
step:115/2110 train_time:4186ms step_avg:36.40ms
step:116/2110 train_time:4218ms step_avg:36.37ms
step:117/2110 train_time:4253ms step_avg:36.35ms
step:118/2110 train_time:4285ms step_avg:36.31ms
step:119/2110 train_time:4318ms step_avg:36.29ms
step:120/2110 train_time:4351ms step_avg:36.26ms
step:121/2110 train_time:4384ms step_avg:36.23ms
step:122/2110 train_time:4417ms step_avg:36.20ms
step:123/2110 train_time:4450ms step_avg:36.18ms
step:124/2110 train_time:4483ms step_avg:36.15ms
step:125/2110 train_time:4516ms step_avg:36.13ms
step:126/2110 train_time:4549ms step_avg:36.10ms
step:127/2110 train_time:4583ms step_avg:36.08ms
step:128/2110 train_time:4615ms step_avg:36.06ms
step:129/2110 train_time:4649ms step_avg:36.04ms
step:130/2110 train_time:4682ms step_avg:36.01ms
step:131/2110 train_time:4715ms step_avg:36.00ms
step:132/2110 train_time:4748ms step_avg:35.97ms
step:133/2110 train_time:4781ms step_avg:35.95ms
step:134/2110 train_time:4814ms step_avg:35.92ms
step:135/2110 train_time:4848ms step_avg:35.91ms
step:136/2110 train_time:4880ms step_avg:35.88ms
step:137/2110 train_time:4914ms step_avg:35.87ms
step:138/2110 train_time:4947ms step_avg:35.85ms
step:139/2110 train_time:4980ms step_avg:35.83ms
step:140/2110 train_time:5013ms step_avg:35.81ms
step:141/2110 train_time:5045ms step_avg:35.78ms
step:142/2110 train_time:5078ms step_avg:35.76ms
step:143/2110 train_time:5111ms step_avg:35.74ms
step:144/2110 train_time:5144ms step_avg:35.72ms
step:145/2110 train_time:5177ms step_avg:35.71ms
step:146/2110 train_time:5210ms step_avg:35.68ms
step:147/2110 train_time:5243ms step_avg:35.67ms
step:148/2110 train_time:5276ms step_avg:35.65ms
step:149/2110 train_time:5309ms step_avg:35.63ms
step:150/2110 train_time:5341ms step_avg:35.61ms
step:151/2110 train_time:5375ms step_avg:35.59ms
step:152/2110 train_time:5407ms step_avg:35.57ms
step:153/2110 train_time:5440ms step_avg:35.56ms
step:154/2110 train_time:5473ms step_avg:35.54ms
step:155/2110 train_time:5507ms step_avg:35.53ms
step:156/2110 train_time:5539ms step_avg:35.51ms
step:157/2110 train_time:5572ms step_avg:35.49ms
step:158/2110 train_time:5605ms step_avg:35.47ms
step:159/2110 train_time:5638ms step_avg:35.46ms
step:160/2110 train_time:5671ms step_avg:35.44ms
step:161/2110 train_time:5705ms step_avg:35.43ms
step:162/2110 train_time:5737ms step_avg:35.42ms
step:163/2110 train_time:5771ms step_avg:35.40ms
step:164/2110 train_time:5803ms step_avg:35.39ms
step:165/2110 train_time:5836ms step_avg:35.37ms
step:166/2110 train_time:5869ms step_avg:35.36ms
step:167/2110 train_time:5903ms step_avg:35.35ms
step:168/2110 train_time:5936ms step_avg:35.33ms
step:169/2110 train_time:5969ms step_avg:35.32ms
step:170/2110 train_time:6001ms step_avg:35.30ms
step:171/2110 train_time:6035ms step_avg:35.29ms
step:172/2110 train_time:6071ms step_avg:35.29ms
step:173/2110 train_time:6102ms step_avg:35.27ms
step:174/2110 train_time:6134ms step_avg:35.25ms
step:175/2110 train_time:6167ms step_avg:35.24ms
step:176/2110 train_time:6200ms step_avg:35.23ms
step:177/2110 train_time:6233ms step_avg:35.21ms
step:178/2110 train_time:6266ms step_avg:35.20ms
step:179/2110 train_time:6299ms step_avg:35.19ms
step:180/2110 train_time:6331ms step_avg:35.17ms
step:181/2110 train_time:6365ms step_avg:35.16ms
step:182/2110 train_time:6397ms step_avg:35.15ms
step:183/2110 train_time:6430ms step_avg:35.14ms
step:184/2110 train_time:6463ms step_avg:35.13ms
step:185/2110 train_time:6496ms step_avg:35.11ms
step:186/2110 train_time:6528ms step_avg:35.10ms
step:187/2110 train_time:6562ms step_avg:35.09ms
step:188/2110 train_time:6594ms step_avg:35.08ms
step:189/2110 train_time:6628ms step_avg:35.07ms
step:190/2110 train_time:6660ms step_avg:35.05ms
step:191/2110 train_time:6693ms step_avg:35.04ms
step:192/2110 train_time:6726ms step_avg:35.03ms
step:193/2110 train_time:6760ms step_avg:35.02ms
step:194/2110 train_time:6793ms step_avg:35.01ms
step:195/2110 train_time:6826ms step_avg:35.00ms
step:196/2110 train_time:6858ms step_avg:34.99ms
step:197/2110 train_time:6891ms step_avg:34.98ms
step:198/2110 train_time:6924ms step_avg:34.97ms
step:199/2110 train_time:6957ms step_avg:34.96ms
step:200/2110 train_time:6990ms step_avg:34.95ms
step:201/2110 train_time:7024ms step_avg:34.94ms
step:202/2110 train_time:7056ms step_avg:34.93ms
step:203/2110 train_time:7090ms step_avg:34.92ms
step:204/2110 train_time:7122ms step_avg:34.91ms
step:205/2110 train_time:7155ms step_avg:34.90ms
step:206/2110 train_time:7188ms step_avg:34.89ms
step:207/2110 train_time:7221ms step_avg:34.88ms
step:208/2110 train_time:7254ms step_avg:34.87ms
step:209/2110 train_time:7287ms step_avg:34.87ms
step:210/2110 train_time:7319ms step_avg:34.85ms
step:211/2110 train_time:7353ms step_avg:34.85ms
step:212/2110 train_time:7385ms step_avg:34.84ms
step:213/2110 train_time:7418ms step_avg:34.83ms
step:214/2110 train_time:7451ms step_avg:34.82ms
step:215/2110 train_time:7484ms step_avg:34.81ms
step:216/2110 train_time:7517ms step_avg:34.80ms
step:217/2110 train_time:7550ms step_avg:34.79ms
step:218/2110 train_time:7582ms step_avg:34.78ms
step:219/2110 train_time:7616ms step_avg:34.78ms
step:220/2110 train_time:7649ms step_avg:34.77ms
step:221/2110 train_time:7682ms step_avg:34.76ms
step:222/2110 train_time:7715ms step_avg:34.75ms
step:223/2110 train_time:7748ms step_avg:34.74ms
step:224/2110 train_time:7780ms step_avg:34.73ms
step:225/2110 train_time:7814ms step_avg:34.73ms
step:226/2110 train_time:7847ms step_avg:34.72ms
step:227/2110 train_time:7884ms step_avg:34.73ms
step:228/2110 train_time:7914ms step_avg:34.71ms
step:229/2110 train_time:7947ms step_avg:34.70ms
step:230/2110 train_time:7980ms step_avg:34.70ms
step:231/2110 train_time:8013ms step_avg:34.69ms
step:232/2110 train_time:8047ms step_avg:34.68ms
step:233/2110 train_time:8079ms step_avg:34.67ms
step:234/2110 train_time:8112ms step_avg:34.66ms
step:235/2110 train_time:8145ms step_avg:34.66ms
step:236/2110 train_time:8178ms step_avg:34.65ms
step:237/2110 train_time:8210ms step_avg:34.64ms
step:238/2110 train_time:8244ms step_avg:34.64ms
step:239/2110 train_time:8276ms step_avg:34.63ms
step:240/2110 train_time:8310ms step_avg:34.63ms
step:241/2110 train_time:8342ms step_avg:34.62ms
step:242/2110 train_time:8375ms step_avg:34.61ms
step:243/2110 train_time:8408ms step_avg:34.60ms
step:244/2110 train_time:8441ms step_avg:34.59ms
step:245/2110 train_time:8474ms step_avg:34.59ms
step:246/2110 train_time:8507ms step_avg:34.58ms
step:247/2110 train_time:8540ms step_avg:34.58ms
step:248/2110 train_time:8573ms step_avg:34.57ms
step:249/2110 train_time:8606ms step_avg:34.56ms
step:250/2110 train_time:8639ms step_avg:34.56ms
step:250/2110 val_loss:4.2990 train_time:8674ms step_avg:34.70ms
step:251/2110 train_time:8702ms step_avg:34.67ms
step:252/2110 train_time:8729ms step_avg:34.64ms
step:253/2110 train_time:8754ms step_avg:34.60ms
step:254/2110 train_time:8781ms step_avg:34.57ms
step:255/2110 train_time:8815ms step_avg:34.57ms
step:256/2110 train_time:8849ms step_avg:34.57ms
step:257/2110 train_time:8884ms step_avg:34.57ms
step:258/2110 train_time:8917ms step_avg:34.56ms
step:259/2110 train_time:8949ms step_avg:34.55ms
step:260/2110 train_time:8982ms step_avg:34.55ms
step:261/2110 train_time:9015ms step_avg:34.54ms
step:262/2110 train_time:9049ms step_avg:34.54ms
step:263/2110 train_time:9081ms step_avg:34.53ms
step:264/2110 train_time:9114ms step_avg:34.52ms
step:265/2110 train_time:9147ms step_avg:34.52ms
step:266/2110 train_time:9179ms step_avg:34.51ms
step:267/2110 train_time:9212ms step_avg:34.50ms
step:268/2110 train_time:9244ms step_avg:34.49ms
step:269/2110 train_time:9277ms step_avg:34.49ms
step:270/2110 train_time:9310ms step_avg:34.48ms
step:271/2110 train_time:9342ms step_avg:34.47ms
step:272/2110 train_time:9374ms step_avg:34.46ms
step:273/2110 train_time:9407ms step_avg:34.46ms
step:274/2110 train_time:9440ms step_avg:34.45ms
step:275/2110 train_time:9473ms step_avg:34.45ms
step:276/2110 train_time:9505ms step_avg:34.44ms
step:277/2110 train_time:9538ms step_avg:34.43ms
step:278/2110 train_time:9575ms step_avg:34.44ms
step:279/2110 train_time:9609ms step_avg:34.44ms
step:280/2110 train_time:9642ms step_avg:34.43ms
step:281/2110 train_time:9674ms step_avg:34.43ms
step:282/2110 train_time:9709ms step_avg:34.43ms
step:283/2110 train_time:9740ms step_avg:34.42ms
step:284/2110 train_time:9773ms step_avg:34.41ms
step:285/2110 train_time:9806ms step_avg:34.41ms
step:286/2110 train_time:9839ms step_avg:34.40ms
step:287/2110 train_time:9872ms step_avg:34.40ms
step:288/2110 train_time:9906ms step_avg:34.39ms
step:289/2110 train_time:9941ms step_avg:34.40ms
step:290/2110 train_time:9975ms step_avg:34.40ms
step:291/2110 train_time:10007ms step_avg:34.39ms
step:292/2110 train_time:10041ms step_avg:34.39ms
step:293/2110 train_time:10071ms step_avg:34.37ms
step:294/2110 train_time:10101ms step_avg:34.36ms
step:295/2110 train_time:10133ms step_avg:34.35ms
step:296/2110 train_time:10166ms step_avg:34.35ms
step:297/2110 train_time:10199ms step_avg:34.34ms
step:298/2110 train_time:10232ms step_avg:34.34ms
step:299/2110 train_time:10265ms step_avg:34.33ms
step:300/2110 train_time:10298ms step_avg:34.33ms
step:301/2110 train_time:10331ms step_avg:34.32ms
step:302/2110 train_time:10364ms step_avg:34.32ms
step:303/2110 train_time:10396ms step_avg:34.31ms
step:304/2110 train_time:10429ms step_avg:34.31ms
step:305/2110 train_time:10462ms step_avg:34.30ms
step:306/2110 train_time:10495ms step_avg:34.30ms
step:307/2110 train_time:10527ms step_avg:34.29ms
step:308/2110 train_time:10561ms step_avg:34.29ms
step:309/2110 train_time:10593ms step_avg:34.28ms
step:310/2110 train_time:10627ms step_avg:34.28ms
step:311/2110 train_time:10659ms step_avg:34.27ms
step:312/2110 train_time:10692ms step_avg:34.27ms
step:313/2110 train_time:10725ms step_avg:34.27ms
step:314/2110 train_time:10759ms step_avg:34.26ms
step:315/2110 train_time:10792ms step_avg:34.26ms
step:316/2110 train_time:10825ms step_avg:34.26ms
step:317/2110 train_time:10858ms step_avg:34.25ms
step:318/2110 train_time:10891ms step_avg:34.25ms
step:319/2110 train_time:10925ms step_avg:34.25ms
step:320/2110 train_time:10958ms step_avg:34.24ms
step:321/2110 train_time:10990ms step_avg:34.24ms
step:322/2110 train_time:11024ms step_avg:34.24ms
step:323/2110 train_time:11056ms step_avg:34.23ms
step:324/2110 train_time:11090ms step_avg:34.23ms
step:325/2110 train_time:11122ms step_avg:34.22ms
step:326/2110 train_time:11155ms step_avg:34.22ms
step:327/2110 train_time:11188ms step_avg:34.21ms
step:328/2110 train_time:11221ms step_avg:34.21ms
step:329/2110 train_time:11254ms step_avg:34.21ms
step:330/2110 train_time:11288ms step_avg:34.20ms
step:331/2110 train_time:11320ms step_avg:34.20ms
step:332/2110 train_time:11353ms step_avg:34.20ms
step:333/2110 train_time:11386ms step_avg:34.19ms
step:334/2110 train_time:11419ms step_avg:34.19ms
step:335/2110 train_time:11451ms step_avg:34.18ms
step:336/2110 train_time:11484ms step_avg:34.18ms
step:337/2110 train_time:11517ms step_avg:34.17ms
step:338/2110 train_time:11550ms step_avg:34.17ms
step:339/2110 train_time:11582ms step_avg:34.17ms
step:340/2110 train_time:11615ms step_avg:34.16ms
step:341/2110 train_time:11649ms step_avg:34.16ms
step:342/2110 train_time:11681ms step_avg:34.16ms
step:343/2110 train_time:11714ms step_avg:34.15ms
step:344/2110 train_time:11748ms step_avg:34.15ms
step:345/2110 train_time:11781ms step_avg:34.15ms
step:346/2110 train_time:11814ms step_avg:34.14ms
step:347/2110 train_time:11847ms step_avg:34.14ms
step:348/2110 train_time:11880ms step_avg:34.14ms
step:349/2110 train_time:11913ms step_avg:34.13ms
step:350/2110 train_time:11946ms step_avg:34.13ms
step:351/2110 train_time:11979ms step_avg:34.13ms
step:352/2110 train_time:12012ms step_avg:34.13ms
step:353/2110 train_time:12045ms step_avg:34.12ms
step:354/2110 train_time:12079ms step_avg:34.12ms
step:355/2110 train_time:12112ms step_avg:34.12ms
step:356/2110 train_time:12145ms step_avg:34.11ms
step:357/2110 train_time:12177ms step_avg:34.11ms
step:358/2110 train_time:12210ms step_avg:34.11ms
step:359/2110 train_time:12243ms step_avg:34.10ms
step:360/2110 train_time:12277ms step_avg:34.10ms
step:361/2110 train_time:12308ms step_avg:34.09ms
step:362/2110 train_time:12341ms step_avg:34.09ms
step:363/2110 train_time:12374ms step_avg:34.09ms
step:364/2110 train_time:12406ms step_avg:34.08ms
step:365/2110 train_time:12440ms step_avg:34.08ms
step:366/2110 train_time:12472ms step_avg:34.08ms
step:367/2110 train_time:12506ms step_avg:34.08ms
step:368/2110 train_time:12538ms step_avg:34.07ms
step:369/2110 train_time:12572ms step_avg:34.07ms
step:370/2110 train_time:12604ms step_avg:34.06ms
step:371/2110 train_time:12637ms step_avg:34.06ms
step:372/2110 train_time:12669ms step_avg:34.06ms
step:373/2110 train_time:12703ms step_avg:34.06ms
step:374/2110 train_time:12736ms step_avg:34.05ms
step:375/2110 train_time:12769ms step_avg:34.05ms
step:376/2110 train_time:12802ms step_avg:34.05ms
step:377/2110 train_time:12835ms step_avg:34.05ms
step:378/2110 train_time:12868ms step_avg:34.04ms
step:379/2110 train_time:12901ms step_avg:34.04ms
step:380/2110 train_time:12934ms step_avg:34.04ms
step:381/2110 train_time:12967ms step_avg:34.03ms
step:382/2110 train_time:13000ms step_avg:34.03ms
step:383/2110 train_time:13034ms step_avg:34.03ms
step:384/2110 train_time:13066ms step_avg:34.03ms
step:385/2110 train_time:13099ms step_avg:34.02ms
step:386/2110 train_time:13132ms step_avg:34.02ms
step:387/2110 train_time:13165ms step_avg:34.02ms
step:388/2110 train_time:13197ms step_avg:34.01ms
step:389/2110 train_time:13231ms step_avg:34.01ms
step:390/2110 train_time:13264ms step_avg:34.01ms
step:391/2110 train_time:13297ms step_avg:34.01ms
step:392/2110 train_time:13330ms step_avg:34.00ms
step:393/2110 train_time:13363ms step_avg:34.00ms
step:394/2110 train_time:13395ms step_avg:34.00ms
step:395/2110 train_time:13428ms step_avg:34.00ms
step:396/2110 train_time:13461ms step_avg:33.99ms
step:397/2110 train_time:13494ms step_avg:33.99ms
step:398/2110 train_time:13527ms step_avg:33.99ms
step:399/2110 train_time:13560ms step_avg:33.98ms
step:400/2110 train_time:13592ms step_avg:33.98ms
step:401/2110 train_time:13626ms step_avg:33.98ms
step:402/2110 train_time:13658ms step_avg:33.98ms
step:403/2110 train_time:13691ms step_avg:33.97ms
step:404/2110 train_time:13724ms step_avg:33.97ms
step:405/2110 train_time:13758ms step_avg:33.97ms
step:406/2110 train_time:13790ms step_avg:33.97ms
step:407/2110 train_time:13824ms step_avg:33.97ms
step:408/2110 train_time:13856ms step_avg:33.96ms
step:409/2110 train_time:13889ms step_avg:33.96ms
step:410/2110 train_time:13922ms step_avg:33.96ms
step:411/2110 train_time:13955ms step_avg:33.95ms
step:412/2110 train_time:13988ms step_avg:33.95ms
step:413/2110 train_time:14021ms step_avg:33.95ms
step:414/2110 train_time:14054ms step_avg:33.95ms
step:415/2110 train_time:14088ms step_avg:33.95ms
step:416/2110 train_time:14121ms step_avg:33.94ms
step:417/2110 train_time:14154ms step_avg:33.94ms
step:418/2110 train_time:14186ms step_avg:33.94ms
step:419/2110 train_time:14220ms step_avg:33.94ms
step:420/2110 train_time:14252ms step_avg:33.93ms
step:421/2110 train_time:14286ms step_avg:33.93ms
step:422/2110 train_time:14318ms step_avg:33.93ms
step:423/2110 train_time:14351ms step_avg:33.93ms
step:424/2110 train_time:14384ms step_avg:33.93ms
step:425/2110 train_time:14418ms step_avg:33.92ms
step:426/2110 train_time:14450ms step_avg:33.92ms
step:427/2110 train_time:14483ms step_avg:33.92ms
step:428/2110 train_time:14516ms step_avg:33.92ms
step:429/2110 train_time:14549ms step_avg:33.91ms
step:430/2110 train_time:14582ms step_avg:33.91ms
step:431/2110 train_time:14615ms step_avg:33.91ms
step:432/2110 train_time:14647ms step_avg:33.91ms
step:433/2110 train_time:14681ms step_avg:33.90ms
step:434/2110 train_time:14713ms step_avg:33.90ms
step:435/2110 train_time:14746ms step_avg:33.90ms
step:436/2110 train_time:14779ms step_avg:33.90ms
step:437/2110 train_time:14812ms step_avg:33.90ms
step:438/2110 train_time:14844ms step_avg:33.89ms
step:439/2110 train_time:14878ms step_avg:33.89ms
step:440/2110 train_time:14910ms step_avg:33.89ms
step:441/2110 train_time:14944ms step_avg:33.89ms
step:442/2110 train_time:14976ms step_avg:33.88ms
step:443/2110 train_time:15010ms step_avg:33.88ms
step:444/2110 train_time:15043ms step_avg:33.88ms
step:445/2110 train_time:15076ms step_avg:33.88ms
step:446/2110 train_time:15108ms step_avg:33.88ms
step:447/2110 train_time:15142ms step_avg:33.87ms
step:448/2110 train_time:15174ms step_avg:33.87ms
step:449/2110 train_time:15208ms step_avg:33.87ms
step:450/2110 train_time:15240ms step_avg:33.87ms
step:451/2110 train_time:15274ms step_avg:33.87ms
step:452/2110 train_time:15310ms step_avg:33.87ms
step:453/2110 train_time:15343ms step_avg:33.87ms
step:454/2110 train_time:15375ms step_avg:33.87ms
step:455/2110 train_time:15407ms step_avg:33.86ms
step:456/2110 train_time:15445ms step_avg:33.87ms
step:457/2110 train_time:15472ms step_avg:33.85ms
step:458/2110 train_time:15506ms step_avg:33.86ms
step:459/2110 train_time:15538ms step_avg:33.85ms
step:460/2110 train_time:15570ms step_avg:33.85ms
step:461/2110 train_time:15603ms step_avg:33.85ms
step:462/2110 train_time:15637ms step_avg:33.85ms
step:463/2110 train_time:15670ms step_avg:33.85ms
step:464/2110 train_time:15702ms step_avg:33.84ms
step:465/2110 train_time:15735ms step_avg:33.84ms
step:466/2110 train_time:15770ms step_avg:33.84ms
step:467/2110 train_time:15801ms step_avg:33.83ms
step:468/2110 train_time:15836ms step_avg:33.84ms
step:469/2110 train_time:15867ms step_avg:33.83ms
step:470/2110 train_time:15899ms step_avg:33.83ms
step:471/2110 train_time:15933ms step_avg:33.83ms
step:472/2110 train_time:15966ms step_avg:33.83ms
step:473/2110 train_time:15999ms step_avg:33.82ms
step:474/2110 train_time:16032ms step_avg:33.82ms
step:475/2110 train_time:16065ms step_avg:33.82ms
step:476/2110 train_time:16097ms step_avg:33.82ms
step:477/2110 train_time:16131ms step_avg:33.82ms
step:478/2110 train_time:16164ms step_avg:33.82ms
step:479/2110 train_time:16197ms step_avg:33.81ms
step:480/2110 train_time:16230ms step_avg:33.81ms
step:481/2110 train_time:16263ms step_avg:33.81ms
step:482/2110 train_time:16295ms step_avg:33.81ms
step:483/2110 train_time:16329ms step_avg:33.81ms
step:484/2110 train_time:16361ms step_avg:33.80ms
step:485/2110 train_time:16394ms step_avg:33.80ms
step:486/2110 train_time:16429ms step_avg:33.80ms
step:487/2110 train_time:16460ms step_avg:33.80ms
step:488/2110 train_time:16493ms step_avg:33.80ms
step:489/2110 train_time:16526ms step_avg:33.79ms
step:490/2110 train_time:16558ms step_avg:33.79ms
step:491/2110 train_time:16592ms step_avg:33.79ms
step:492/2110 train_time:16626ms step_avg:33.79ms
step:493/2110 train_time:16658ms step_avg:33.79ms
step:494/2110 train_time:16692ms step_avg:33.79ms
step:495/2110 train_time:16725ms step_avg:33.79ms
step:496/2110 train_time:16758ms step_avg:33.79ms
step:497/2110 train_time:16790ms step_avg:33.78ms
step:498/2110 train_time:16823ms step_avg:33.78ms
step:499/2110 train_time:16856ms step_avg:33.78ms
step:500/2110 train_time:16888ms step_avg:33.78ms
step:500/2110 val_loss:4.0301 train_time:16924ms step_avg:33.85ms
step:501/2110 train_time:16950ms step_avg:33.83ms
step:502/2110 train_time:16974ms step_avg:33.81ms
step:503/2110 train_time:16996ms step_avg:33.79ms
step:504/2110 train_time:17028ms step_avg:33.79ms
step:505/2110 train_time:17064ms step_avg:33.79ms
step:506/2110 train_time:17096ms step_avg:33.79ms
step:507/2110 train_time:17130ms step_avg:33.79ms
step:508/2110 train_time:17162ms step_avg:33.78ms
step:509/2110 train_time:17196ms step_avg:33.78ms
step:510/2110 train_time:17228ms step_avg:33.78ms
step:511/2110 train_time:17261ms step_avg:33.78ms
step:512/2110 train_time:17294ms step_avg:33.78ms
step:513/2110 train_time:17327ms step_avg:33.78ms
step:514/2110 train_time:17360ms step_avg:33.78ms
step:515/2110 train_time:17392ms step_avg:33.77ms
step:516/2110 train_time:17425ms step_avg:33.77ms
step:517/2110 train_time:17458ms step_avg:33.77ms
step:518/2110 train_time:17490ms step_avg:33.77ms
step:519/2110 train_time:17523ms step_avg:33.76ms
step:520/2110 train_time:17555ms step_avg:33.76ms
step:521/2110 train_time:17588ms step_avg:33.76ms
step:522/2110 train_time:17620ms step_avg:33.76ms
step:523/2110 train_time:17653ms step_avg:33.75ms
step:524/2110 train_time:17685ms step_avg:33.75ms
step:525/2110 train_time:17718ms step_avg:33.75ms
step:526/2110 train_time:17751ms step_avg:33.75ms
step:527/2110 train_time:17784ms step_avg:33.75ms
step:528/2110 train_time:17816ms step_avg:33.74ms
step:529/2110 train_time:17849ms step_avg:33.74ms
step:530/2110 train_time:17882ms step_avg:33.74ms
step:531/2110 train_time:17915ms step_avg:33.74ms
step:532/2110 train_time:17948ms step_avg:33.74ms
step:533/2110 train_time:17982ms step_avg:33.74ms
step:534/2110 train_time:18015ms step_avg:33.74ms
step:535/2110 train_time:18049ms step_avg:33.74ms
step:536/2110 train_time:18082ms step_avg:33.73ms
step:537/2110 train_time:18115ms step_avg:33.73ms
step:538/2110 train_time:18147ms step_avg:33.73ms
step:539/2110 train_time:18181ms step_avg:33.73ms
step:540/2110 train_time:18214ms step_avg:33.73ms
step:541/2110 train_time:18247ms step_avg:33.73ms
step:542/2110 train_time:18280ms step_avg:33.73ms
step:543/2110 train_time:18314ms step_avg:33.73ms
step:544/2110 train_time:18346ms step_avg:33.73ms
step:545/2110 train_time:18380ms step_avg:33.72ms
step:546/2110 train_time:18412ms step_avg:33.72ms
step:547/2110 train_time:18445ms step_avg:33.72ms
step:548/2110 train_time:18478ms step_avg:33.72ms
step:549/2110 train_time:18511ms step_avg:33.72ms
step:550/2110 train_time:18544ms step_avg:33.72ms
step:551/2110 train_time:18577ms step_avg:33.71ms
step:552/2110 train_time:18609ms step_avg:33.71ms
step:553/2110 train_time:18642ms step_avg:33.71ms
step:554/2110 train_time:18675ms step_avg:33.71ms
step:555/2110 train_time:18708ms step_avg:33.71ms
step:556/2110 train_time:18740ms step_avg:33.70ms
step:557/2110 train_time:18773ms step_avg:33.70ms
step:558/2110 train_time:18805ms step_avg:33.70ms
step:559/2110 train_time:18839ms step_avg:33.70ms
step:560/2110 train_time:18871ms step_avg:33.70ms
step:561/2110 train_time:18905ms step_avg:33.70ms
step:562/2110 train_time:18938ms step_avg:33.70ms
step:563/2110 train_time:18971ms step_avg:33.70ms
step:564/2110 train_time:19004ms step_avg:33.69ms
step:565/2110 train_time:19038ms step_avg:33.69ms
step:566/2110 train_time:19070ms step_avg:33.69ms
step:567/2110 train_time:19104ms step_avg:33.69ms
step:568/2110 train_time:19136ms step_avg:33.69ms
step:569/2110 train_time:19170ms step_avg:33.69ms
step:570/2110 train_time:19203ms step_avg:33.69ms
step:571/2110 train_time:19236ms step_avg:33.69ms
step:572/2110 train_time:19269ms step_avg:33.69ms
step:573/2110 train_time:19303ms step_avg:33.69ms
step:574/2110 train_time:19335ms step_avg:33.68ms
step:575/2110 train_time:19369ms step_avg:33.68ms
step:576/2110 train_time:19401ms step_avg:33.68ms
step:577/2110 train_time:19435ms step_avg:33.68ms
step:578/2110 train_time:19467ms step_avg:33.68ms
step:579/2110 train_time:19501ms step_avg:33.68ms
step:580/2110 train_time:19534ms step_avg:33.68ms
step:581/2110 train_time:19567ms step_avg:33.68ms
step:582/2110 train_time:19600ms step_avg:33.68ms
step:583/2110 train_time:19633ms step_avg:33.68ms
step:584/2110 train_time:19665ms step_avg:33.67ms
step:585/2110 train_time:19698ms step_avg:33.67ms
step:586/2110 train_time:19731ms step_avg:33.67ms
step:587/2110 train_time:19764ms step_avg:33.67ms
step:588/2110 train_time:19796ms step_avg:33.67ms
step:589/2110 train_time:19830ms step_avg:33.67ms
step:590/2110 train_time:19862ms step_avg:33.66ms
step:591/2110 train_time:19895ms step_avg:33.66ms
step:592/2110 train_time:19927ms step_avg:33.66ms
step:593/2110 train_time:19961ms step_avg:33.66ms
step:594/2110 train_time:19994ms step_avg:33.66ms
step:595/2110 train_time:20027ms step_avg:33.66ms
step:596/2110 train_time:20059ms step_avg:33.66ms
step:597/2110 train_time:20093ms step_avg:33.66ms
step:598/2110 train_time:20125ms step_avg:33.65ms
step:599/2110 train_time:20159ms step_avg:33.65ms
step:600/2110 train_time:20192ms step_avg:33.65ms
step:601/2110 train_time:20225ms step_avg:33.65ms
step:602/2110 train_time:20258ms step_avg:33.65ms
step:603/2110 train_time:20291ms step_avg:33.65ms
step:604/2110 train_time:20324ms step_avg:33.65ms
step:605/2110 train_time:20357ms step_avg:33.65ms
step:606/2110 train_time:20390ms step_avg:33.65ms
step:607/2110 train_time:20423ms step_avg:33.65ms
step:608/2110 train_time:20456ms step_avg:33.64ms
step:609/2110 train_time:20489ms step_avg:33.64ms
step:610/2110 train_time:20522ms step_avg:33.64ms
step:611/2110 train_time:20555ms step_avg:33.64ms
step:612/2110 train_time:20587ms step_avg:33.64ms
step:613/2110 train_time:20621ms step_avg:33.64ms
step:614/2110 train_time:20653ms step_avg:33.64ms
step:615/2110 train_time:20686ms step_avg:33.64ms
step:616/2110 train_time:20719ms step_avg:33.64ms
step:617/2110 train_time:20753ms step_avg:33.63ms
step:618/2110 train_time:20785ms step_avg:33.63ms
step:619/2110 train_time:20819ms step_avg:33.63ms
step:620/2110 train_time:20851ms step_avg:33.63ms
step:621/2110 train_time:20885ms step_avg:33.63ms
step:622/2110 train_time:20917ms step_avg:33.63ms
step:623/2110 train_time:20951ms step_avg:33.63ms
step:624/2110 train_time:20983ms step_avg:33.63ms
step:625/2110 train_time:21016ms step_avg:33.63ms
step:626/2110 train_time:21054ms step_avg:33.63ms
step:627/2110 train_time:21092ms step_avg:33.64ms
step:628/2110 train_time:21131ms step_avg:33.65ms
step:629/2110 train_time:21169ms step_avg:33.66ms
step:630/2110 train_time:21206ms step_avg:33.66ms
step:631/2110 train_time:21243ms step_avg:33.67ms
step:632/2110 train_time:21276ms step_avg:33.67ms
step:633/2110 train_time:21313ms step_avg:33.67ms
step:634/2110 train_time:21347ms step_avg:33.67ms
step:635/2110 train_time:21384ms step_avg:33.68ms
step:636/2110 train_time:21417ms step_avg:33.67ms
step:637/2110 train_time:21454ms step_avg:33.68ms
step:638/2110 train_time:21486ms step_avg:33.68ms
step:639/2110 train_time:21523ms step_avg:33.68ms
step:640/2110 train_time:21556ms step_avg:33.68ms
step:641/2110 train_time:21589ms step_avg:33.68ms
step:642/2110 train_time:21621ms step_avg:33.68ms
step:643/2110 train_time:21656ms step_avg:33.68ms
step:644/2110 train_time:21689ms step_avg:33.68ms
step:645/2110 train_time:21725ms step_avg:33.68ms
step:646/2110 train_time:21759ms step_avg:33.68ms
step:647/2110 train_time:21793ms step_avg:33.68ms
step:648/2110 train_time:21825ms step_avg:33.68ms
step:649/2110 train_time:21858ms step_avg:33.68ms
step:650/2110 train_time:21890ms step_avg:33.68ms
step:651/2110 train_time:21924ms step_avg:33.68ms
step:652/2110 train_time:21956ms step_avg:33.67ms
step:653/2110 train_time:21989ms step_avg:33.67ms
step:654/2110 train_time:22021ms step_avg:33.67ms
step:655/2110 train_time:22055ms step_avg:33.67ms
step:656/2110 train_time:22088ms step_avg:33.67ms
step:657/2110 train_time:22121ms step_avg:33.67ms
step:658/2110 train_time:22154ms step_avg:33.67ms
step:659/2110 train_time:22187ms step_avg:33.67ms
step:660/2110 train_time:22220ms step_avg:33.67ms
step:661/2110 train_time:22253ms step_avg:33.67ms
step:662/2110 train_time:22286ms step_avg:33.66ms
step:663/2110 train_time:22319ms step_avg:33.66ms
step:664/2110 train_time:22352ms step_avg:33.66ms
step:665/2110 train_time:22385ms step_avg:33.66ms
step:666/2110 train_time:22419ms step_avg:33.66ms
step:667/2110 train_time:22451ms step_avg:33.66ms
step:668/2110 train_time:22484ms step_avg:33.66ms
step:669/2110 train_time:22518ms step_avg:33.66ms
step:670/2110 train_time:22550ms step_avg:33.66ms
step:671/2110 train_time:22584ms step_avg:33.66ms
step:672/2110 train_time:22617ms step_avg:33.66ms
step:673/2110 train_time:22650ms step_avg:33.66ms
step:674/2110 train_time:22683ms step_avg:33.65ms
step:675/2110 train_time:22716ms step_avg:33.65ms
step:676/2110 train_time:22748ms step_avg:33.65ms
step:677/2110 train_time:22782ms step_avg:33.65ms
step:678/2110 train_time:22815ms step_avg:33.65ms
step:679/2110 train_time:22848ms step_avg:33.65ms
step:680/2110 train_time:22880ms step_avg:33.65ms
step:681/2110 train_time:22913ms step_avg:33.65ms
step:682/2110 train_time:22946ms step_avg:33.64ms
step:683/2110 train_time:22979ms step_avg:33.64ms
step:684/2110 train_time:23012ms step_avg:33.64ms
step:685/2110 train_time:23045ms step_avg:33.64ms
step:686/2110 train_time:23078ms step_avg:33.64ms
step:687/2110 train_time:23111ms step_avg:33.64ms
step:688/2110 train_time:23144ms step_avg:33.64ms
step:689/2110 train_time:23178ms step_avg:33.64ms
step:690/2110 train_time:23210ms step_avg:33.64ms
step:691/2110 train_time:23244ms step_avg:33.64ms
step:692/2110 train_time:23303ms step_avg:33.67ms
step:693/2110 train_time:23363ms step_avg:33.71ms
step:694/2110 train_time:23421ms step_avg:33.75ms
step:695/2110 train_time:23481ms step_avg:33.79ms
step:696/2110 train_time:23539ms step_avg:33.82ms
step:697/2110 train_time:23599ms step_avg:33.86ms
step:698/2110 train_time:23658ms step_avg:33.89ms
step:699/2110 train_time:23718ms step_avg:33.93ms
step:700/2110 train_time:23776ms step_avg:33.97ms
step:701/2110 train_time:23836ms step_avg:34.00ms
step:702/2110 train_time:23894ms step_avg:34.04ms
step:703/2110 train_time:23954ms step_avg:34.07ms
step:704/2110 train_time:24012ms step_avg:34.11ms
step:705/2110 train_time:24073ms step_avg:34.15ms
step:706/2110 train_time:24131ms step_avg:34.18ms
step:707/2110 train_time:24191ms step_avg:34.22ms
step:708/2110 train_time:24250ms step_avg:34.25ms
step:709/2110 train_time:24310ms step_avg:34.29ms
step:710/2110 train_time:24369ms step_avg:34.32ms
step:711/2110 train_time:24429ms step_avg:34.36ms
step:712/2110 train_time:24488ms step_avg:34.39ms
step:713/2110 train_time:24548ms step_avg:34.43ms
step:714/2110 train_time:24607ms step_avg:34.46ms
step:715/2110 train_time:24667ms step_avg:34.50ms
step:716/2110 train_time:24726ms step_avg:34.53ms
step:717/2110 train_time:24787ms step_avg:34.57ms
step:718/2110 train_time:24845ms step_avg:34.60ms
step:719/2110 train_time:24906ms step_avg:34.64ms
step:720/2110 train_time:24964ms step_avg:34.67ms
step:721/2110 train_time:25025ms step_avg:34.71ms
step:722/2110 train_time:25083ms step_avg:34.74ms
step:723/2110 train_time:25143ms step_avg:34.78ms
step:724/2110 train_time:25202ms step_avg:34.81ms
step:725/2110 train_time:25262ms step_avg:34.84ms
step:726/2110 train_time:25321ms step_avg:34.88ms
step:727/2110 train_time:25381ms step_avg:34.91ms
step:728/2110 train_time:25440ms step_avg:34.95ms
step:729/2110 train_time:25500ms step_avg:34.98ms
step:730/2110 train_time:25559ms step_avg:35.01ms
step:731/2110 train_time:25619ms step_avg:35.05ms
step:732/2110 train_time:25677ms step_avg:35.08ms
step:733/2110 train_time:25737ms step_avg:35.11ms
step:734/2110 train_time:25795ms step_avg:35.14ms
step:735/2110 train_time:25857ms step_avg:35.18ms
step:736/2110 train_time:25915ms step_avg:35.21ms
step:737/2110 train_time:25976ms step_avg:35.25ms
step:738/2110 train_time:26034ms step_avg:35.28ms
step:739/2110 train_time:26094ms step_avg:35.31ms
step:740/2110 train_time:26152ms step_avg:35.34ms
step:741/2110 train_time:26212ms step_avg:35.37ms
step:742/2110 train_time:26270ms step_avg:35.40ms
step:743/2110 train_time:26330ms step_avg:35.44ms
step:744/2110 train_time:26389ms step_avg:35.47ms
step:745/2110 train_time:26449ms step_avg:35.50ms
step:746/2110 train_time:26508ms step_avg:35.53ms
step:747/2110 train_time:26568ms step_avg:35.57ms
step:748/2110 train_time:26626ms step_avg:35.60ms
step:749/2110 train_time:26686ms step_avg:35.63ms
step:750/2110 train_time:26745ms step_avg:35.66ms
step:750/2110 val_loss:3.9125 train_time:26807ms step_avg:35.74ms
step:751/2110 train_time:26832ms step_avg:35.73ms
step:752/2110 train_time:26866ms step_avg:35.73ms
step:753/2110 train_time:26930ms step_avg:35.76ms
step:754/2110 train_time:26991ms step_avg:35.80ms
step:755/2110 train_time:27052ms step_avg:35.83ms
step:756/2110 train_time:27110ms step_avg:35.86ms
step:757/2110 train_time:27170ms step_avg:35.89ms
step:758/2110 train_time:27228ms step_avg:35.92ms
step:759/2110 train_time:27287ms step_avg:35.95ms
step:760/2110 train_time:27345ms step_avg:35.98ms
step:761/2110 train_time:27404ms step_avg:36.01ms
step:762/2110 train_time:27462ms step_avg:36.04ms
step:763/2110 train_time:27521ms step_avg:36.07ms
step:764/2110 train_time:27578ms step_avg:36.10ms
step:765/2110 train_time:27637ms step_avg:36.13ms
step:766/2110 train_time:27695ms step_avg:36.16ms
step:767/2110 train_time:27755ms step_avg:36.19ms
step:768/2110 train_time:27815ms step_avg:36.22ms
step:769/2110 train_time:27877ms step_avg:36.25ms
step:770/2110 train_time:27937ms step_avg:36.28ms
step:771/2110 train_time:27998ms step_avg:36.31ms
step:772/2110 train_time:28057ms step_avg:36.34ms
step:773/2110 train_time:28118ms step_avg:36.37ms
step:774/2110 train_time:28176ms step_avg:36.40ms
step:775/2110 train_time:28236ms step_avg:36.43ms
step:776/2110 train_time:28295ms step_avg:36.46ms
step:777/2110 train_time:28355ms step_avg:36.49ms
step:778/2110 train_time:28413ms step_avg:36.52ms
step:779/2110 train_time:28473ms step_avg:36.55ms
step:780/2110 train_time:28531ms step_avg:36.58ms
step:781/2110 train_time:28589ms step_avg:36.61ms
step:782/2110 train_time:28647ms step_avg:36.63ms
step:783/2110 train_time:28706ms step_avg:36.66ms
step:784/2110 train_time:28764ms step_avg:36.69ms
step:785/2110 train_time:28824ms step_avg:36.72ms
step:786/2110 train_time:28883ms step_avg:36.75ms
step:787/2110 train_time:28945ms step_avg:36.78ms
step:788/2110 train_time:29004ms step_avg:36.81ms
step:789/2110 train_time:29065ms step_avg:36.84ms
step:790/2110 train_time:29124ms step_avg:36.87ms
step:791/2110 train_time:29185ms step_avg:36.90ms
step:792/2110 train_time:29243ms step_avg:36.92ms
step:793/2110 train_time:29303ms step_avg:36.95ms
step:794/2110 train_time:29361ms step_avg:36.98ms
step:795/2110 train_time:29421ms step_avg:37.01ms
step:796/2110 train_time:29479ms step_avg:37.03ms
step:797/2110 train_time:29539ms step_avg:37.06ms
step:798/2110 train_time:29596ms step_avg:37.09ms
step:799/2110 train_time:29656ms step_avg:37.12ms
step:800/2110 train_time:29714ms step_avg:37.14ms
step:801/2110 train_time:29774ms step_avg:37.17ms
step:802/2110 train_time:29833ms step_avg:37.20ms
step:803/2110 train_time:29893ms step_avg:37.23ms
step:804/2110 train_time:29954ms step_avg:37.26ms
step:805/2110 train_time:30014ms step_avg:37.28ms
step:806/2110 train_time:30073ms step_avg:37.31ms
step:807/2110 train_time:30133ms step_avg:37.34ms
step:808/2110 train_time:30192ms step_avg:37.37ms
step:809/2110 train_time:30252ms step_avg:37.39ms
step:810/2110 train_time:30311ms step_avg:37.42ms
step:811/2110 train_time:30370ms step_avg:37.45ms
step:812/2110 train_time:30428ms step_avg:37.47ms
step:813/2110 train_time:30487ms step_avg:37.50ms
step:814/2110 train_time:30545ms step_avg:37.53ms
step:815/2110 train_time:30606ms step_avg:37.55ms
step:816/2110 train_time:30664ms step_avg:37.58ms
step:817/2110 train_time:30724ms step_avg:37.61ms
step:818/2110 train_time:30783ms step_avg:37.63ms
step:819/2110 train_time:30843ms step_avg:37.66ms
step:820/2110 train_time:30902ms step_avg:37.68ms
step:821/2110 train_time:30962ms step_avg:37.71ms
step:822/2110 train_time:31021ms step_avg:37.74ms
step:823/2110 train_time:31083ms step_avg:37.77ms
step:824/2110 train_time:31142ms step_avg:37.79ms
step:825/2110 train_time:31202ms step_avg:37.82ms
step:826/2110 train_time:31260ms step_avg:37.85ms
step:827/2110 train_time:31321ms step_avg:37.87ms
step:828/2110 train_time:31379ms step_avg:37.90ms
step:829/2110 train_time:31438ms step_avg:37.92ms
step:830/2110 train_time:31496ms step_avg:37.95ms
step:831/2110 train_time:31556ms step_avg:37.97ms
step:832/2110 train_time:31615ms step_avg:38.00ms
step:833/2110 train_time:31674ms step_avg:38.02ms
step:834/2110 train_time:31733ms step_avg:38.05ms
step:835/2110 train_time:31792ms step_avg:38.07ms
step:836/2110 train_time:31851ms step_avg:38.10ms
step:837/2110 train_time:31912ms step_avg:38.13ms
step:838/2110 train_time:31971ms step_avg:38.15ms
step:839/2110 train_time:32032ms step_avg:38.18ms
step:840/2110 train_time:32090ms step_avg:38.20ms
step:841/2110 train_time:32151ms step_avg:38.23ms
step:842/2110 train_time:32210ms step_avg:38.25ms
step:843/2110 train_time:32270ms step_avg:38.28ms
step:844/2110 train_time:32328ms step_avg:38.30ms
step:845/2110 train_time:32388ms step_avg:38.33ms
step:846/2110 train_time:32446ms step_avg:38.35ms
step:847/2110 train_time:32505ms step_avg:38.38ms
step:848/2110 train_time:32564ms step_avg:38.40ms
step:849/2110 train_time:32624ms step_avg:38.43ms
step:850/2110 train_time:32682ms step_avg:38.45ms
step:851/2110 train_time:32742ms step_avg:38.48ms
step:852/2110 train_time:32801ms step_avg:38.50ms
step:853/2110 train_time:32861ms step_avg:38.52ms
step:854/2110 train_time:32919ms step_avg:38.55ms
step:855/2110 train_time:32980ms step_avg:38.57ms
step:856/2110 train_time:33038ms step_avg:38.60ms
step:857/2110 train_time:33099ms step_avg:38.62ms
step:858/2110 train_time:33158ms step_avg:38.65ms
step:859/2110 train_time:33218ms step_avg:38.67ms
step:860/2110 train_time:33277ms step_avg:38.69ms
step:861/2110 train_time:33337ms step_avg:38.72ms
step:862/2110 train_time:33395ms step_avg:38.74ms
step:863/2110 train_time:33455ms step_avg:38.77ms
step:864/2110 train_time:33513ms step_avg:38.79ms
step:865/2110 train_time:33573ms step_avg:38.81ms
step:866/2110 train_time:33632ms step_avg:38.84ms
step:867/2110 train_time:33692ms step_avg:38.86ms
step:868/2110 train_time:33752ms step_avg:38.89ms
step:869/2110 train_time:33812ms step_avg:38.91ms
step:870/2110 train_time:33871ms step_avg:38.93ms
step:871/2110 train_time:33931ms step_avg:38.96ms
step:872/2110 train_time:33990ms step_avg:38.98ms
step:873/2110 train_time:34050ms step_avg:39.00ms
step:874/2110 train_time:34109ms step_avg:39.03ms
step:875/2110 train_time:34170ms step_avg:39.05ms
step:876/2110 train_time:34228ms step_avg:39.07ms
step:877/2110 train_time:34288ms step_avg:39.10ms
step:878/2110 train_time:34346ms step_avg:39.12ms
step:879/2110 train_time:34407ms step_avg:39.14ms
step:880/2110 train_time:34465ms step_avg:39.16ms
step:881/2110 train_time:34524ms step_avg:39.19ms
step:882/2110 train_time:34583ms step_avg:39.21ms
step:883/2110 train_time:34644ms step_avg:39.23ms
step:884/2110 train_time:34702ms step_avg:39.26ms
step:885/2110 train_time:34762ms step_avg:39.28ms
step:886/2110 train_time:34821ms step_avg:39.30ms
step:887/2110 train_time:34881ms step_avg:39.32ms
step:888/2110 train_time:34940ms step_avg:39.35ms
step:889/2110 train_time:35000ms step_avg:39.37ms
step:890/2110 train_time:35059ms step_avg:39.39ms
step:891/2110 train_time:35118ms step_avg:39.41ms
step:892/2110 train_time:35177ms step_avg:39.44ms
step:893/2110 train_time:35238ms step_avg:39.46ms
step:894/2110 train_time:35296ms step_avg:39.48ms
step:895/2110 train_time:35356ms step_avg:39.50ms
step:896/2110 train_time:35415ms step_avg:39.53ms
step:897/2110 train_time:35475ms step_avg:39.55ms
step:898/2110 train_time:35533ms step_avg:39.57ms
step:899/2110 train_time:35594ms step_avg:39.59ms
step:900/2110 train_time:35653ms step_avg:39.61ms
step:901/2110 train_time:35713ms step_avg:39.64ms
step:902/2110 train_time:35772ms step_avg:39.66ms
step:903/2110 train_time:35832ms step_avg:39.68ms
step:904/2110 train_time:35891ms step_avg:39.70ms
step:905/2110 train_time:35951ms step_avg:39.72ms
step:906/2110 train_time:36009ms step_avg:39.75ms
step:907/2110 train_time:36069ms step_avg:39.77ms
step:908/2110 train_time:36128ms step_avg:39.79ms
step:909/2110 train_time:36188ms step_avg:39.81ms
step:910/2110 train_time:36246ms step_avg:39.83ms
step:911/2110 train_time:36306ms step_avg:39.85ms
step:912/2110 train_time:36365ms step_avg:39.87ms
step:913/2110 train_time:36426ms step_avg:39.90ms
step:914/2110 train_time:36484ms step_avg:39.92ms
step:915/2110 train_time:36544ms step_avg:39.94ms
step:916/2110 train_time:36603ms step_avg:39.96ms
step:917/2110 train_time:36663ms step_avg:39.98ms
step:918/2110 train_time:36721ms step_avg:40.00ms
step:919/2110 train_time:36782ms step_avg:40.02ms
step:920/2110 train_time:36841ms step_avg:40.04ms
step:921/2110 train_time:36901ms step_avg:40.07ms
step:922/2110 train_time:36959ms step_avg:40.09ms
step:923/2110 train_time:37019ms step_avg:40.11ms
step:924/2110 train_time:37077ms step_avg:40.13ms
step:925/2110 train_time:37137ms step_avg:40.15ms
step:926/2110 train_time:37195ms step_avg:40.17ms
step:927/2110 train_time:37255ms step_avg:40.19ms
step:928/2110 train_time:37314ms step_avg:40.21ms
step:929/2110 train_time:37375ms step_avg:40.23ms
step:930/2110 train_time:37433ms step_avg:40.25ms
step:931/2110 train_time:37493ms step_avg:40.27ms
step:932/2110 train_time:37552ms step_avg:40.29ms
step:933/2110 train_time:37612ms step_avg:40.31ms
step:934/2110 train_time:37671ms step_avg:40.33ms
step:935/2110 train_time:37731ms step_avg:40.35ms
step:936/2110 train_time:37790ms step_avg:40.37ms
step:937/2110 train_time:37850ms step_avg:40.39ms
step:938/2110 train_time:37909ms step_avg:40.41ms
step:939/2110 train_time:37969ms step_avg:40.44ms
step:940/2110 train_time:38027ms step_avg:40.45ms
step:941/2110 train_time:38087ms step_avg:40.48ms
step:942/2110 train_time:38145ms step_avg:40.49ms
step:943/2110 train_time:38205ms step_avg:40.51ms
step:944/2110 train_time:38263ms step_avg:40.53ms
step:945/2110 train_time:38324ms step_avg:40.55ms
step:946/2110 train_time:38382ms step_avg:40.57ms
step:947/2110 train_time:38443ms step_avg:40.59ms
step:948/2110 train_time:38501ms step_avg:40.61ms
step:949/2110 train_time:38561ms step_avg:40.63ms
step:950/2110 train_time:38620ms step_avg:40.65ms
step:951/2110 train_time:38681ms step_avg:40.67ms
step:952/2110 train_time:38740ms step_avg:40.69ms
step:953/2110 train_time:38800ms step_avg:40.71ms
step:954/2110 train_time:38858ms step_avg:40.73ms
step:955/2110 train_time:38919ms step_avg:40.75ms
step:956/2110 train_time:38977ms step_avg:40.77ms
step:957/2110 train_time:39038ms step_avg:40.79ms
step:958/2110 train_time:39096ms step_avg:40.81ms
step:959/2110 train_time:39156ms step_avg:40.83ms
step:960/2110 train_time:39215ms step_avg:40.85ms
step:961/2110 train_time:39275ms step_avg:40.87ms
step:962/2110 train_time:39333ms step_avg:40.89ms
step:963/2110 train_time:39393ms step_avg:40.91ms
step:964/2110 train_time:39453ms step_avg:40.93ms
step:965/2110 train_time:39512ms step_avg:40.95ms
step:966/2110 train_time:39571ms step_avg:40.96ms
step:967/2110 train_time:39631ms step_avg:40.98ms
step:968/2110 train_time:39691ms step_avg:41.00ms
step:969/2110 train_time:39750ms step_avg:41.02ms
step:970/2110 train_time:39808ms step_avg:41.04ms
step:971/2110 train_time:39868ms step_avg:41.06ms
step:972/2110 train_time:39927ms step_avg:41.08ms
step:973/2110 train_time:39987ms step_avg:41.10ms
step:974/2110 train_time:40045ms step_avg:41.11ms
step:975/2110 train_time:40106ms step_avg:41.13ms
step:976/2110 train_time:40164ms step_avg:41.15ms
step:977/2110 train_time:40223ms step_avg:41.17ms
step:978/2110 train_time:40282ms step_avg:41.19ms
step:979/2110 train_time:40342ms step_avg:41.21ms
step:980/2110 train_time:40401ms step_avg:41.23ms
step:981/2110 train_time:40462ms step_avg:41.25ms
step:982/2110 train_time:40520ms step_avg:41.26ms
step:983/2110 train_time:40581ms step_avg:41.28ms
step:984/2110 train_time:40639ms step_avg:41.30ms
step:985/2110 train_time:40700ms step_avg:41.32ms
step:986/2110 train_time:40758ms step_avg:41.34ms
step:987/2110 train_time:40818ms step_avg:41.36ms
step:988/2110 train_time:40876ms step_avg:41.37ms
step:989/2110 train_time:40936ms step_avg:41.39ms
step:990/2110 train_time:40995ms step_avg:41.41ms
step:991/2110 train_time:41055ms step_avg:41.43ms
step:992/2110 train_time:41114ms step_avg:41.45ms
step:993/2110 train_time:41174ms step_avg:41.46ms
step:994/2110 train_time:41233ms step_avg:41.48ms
step:995/2110 train_time:41293ms step_avg:41.50ms
step:996/2110 train_time:41353ms step_avg:41.52ms
step:997/2110 train_time:41413ms step_avg:41.54ms
step:998/2110 train_time:41472ms step_avg:41.55ms
step:999/2110 train_time:41532ms step_avg:41.57ms
step:1000/2110 train_time:41590ms step_avg:41.59ms
step:1000/2110 val_loss:3.7571 train_time:41652ms step_avg:41.65ms
step:1001/2110 train_time:41681ms step_avg:41.64ms
step:1002/2110 train_time:41712ms step_avg:41.63ms
step:1003/2110 train_time:41776ms step_avg:41.65ms
step:1004/2110 train_time:41839ms step_avg:41.67ms
step:1005/2110 train_time:41899ms step_avg:41.69ms
step:1006/2110 train_time:41958ms step_avg:41.71ms
step:1007/2110 train_time:42017ms step_avg:41.73ms
step:1008/2110 train_time:42074ms step_avg:41.74ms
step:1009/2110 train_time:42134ms step_avg:41.76ms
step:1010/2110 train_time:42191ms step_avg:41.77ms
step:1011/2110 train_time:42250ms step_avg:41.79ms
step:1012/2110 train_time:42308ms step_avg:41.81ms
step:1013/2110 train_time:42367ms step_avg:41.82ms
step:1014/2110 train_time:42424ms step_avg:41.84ms
step:1015/2110 train_time:42483ms step_avg:41.86ms
step:1016/2110 train_time:42541ms step_avg:41.87ms
step:1017/2110 train_time:42602ms step_avg:41.89ms
step:1018/2110 train_time:42661ms step_avg:41.91ms
step:1019/2110 train_time:42722ms step_avg:41.93ms
step:1020/2110 train_time:42782ms step_avg:41.94ms
step:1021/2110 train_time:42842ms step_avg:41.96ms
step:1022/2110 train_time:42902ms step_avg:41.98ms
step:1023/2110 train_time:42962ms step_avg:42.00ms
step:1024/2110 train_time:43020ms step_avg:42.01ms
step:1025/2110 train_time:43080ms step_avg:42.03ms
step:1026/2110 train_time:43138ms step_avg:42.04ms
step:1027/2110 train_time:43197ms step_avg:42.06ms
step:1028/2110 train_time:43256ms step_avg:42.08ms
step:1029/2110 train_time:43315ms step_avg:42.09ms
step:1030/2110 train_time:43373ms step_avg:42.11ms
step:1031/2110 train_time:43432ms step_avg:42.13ms
step:1032/2110 train_time:43490ms step_avg:42.14ms
step:1033/2110 train_time:43550ms step_avg:42.16ms
step:1034/2110 train_time:43609ms step_avg:42.17ms
step:1035/2110 train_time:43669ms step_avg:42.19ms
step:1036/2110 train_time:43728ms step_avg:42.21ms
step:1037/2110 train_time:43790ms step_avg:42.23ms
step:1038/2110 train_time:43850ms step_avg:42.24ms
step:1039/2110 train_time:43911ms step_avg:42.26ms
step:1040/2110 train_time:43970ms step_avg:42.28ms
step:1041/2110 train_time:44032ms step_avg:42.30ms
step:1042/2110 train_time:44090ms step_avg:42.31ms
step:1043/2110 train_time:44150ms step_avg:42.33ms
step:1044/2110 train_time:44208ms step_avg:42.34ms
step:1045/2110 train_time:44267ms step_avg:42.36ms
step:1046/2110 train_time:44325ms step_avg:42.38ms
step:1047/2110 train_time:44385ms step_avg:42.39ms
step:1048/2110 train_time:44443ms step_avg:42.41ms
step:1049/2110 train_time:44502ms step_avg:42.42ms
step:1050/2110 train_time:44561ms step_avg:42.44ms
step:1051/2110 train_time:44621ms step_avg:42.46ms
step:1052/2110 train_time:44679ms step_avg:42.47ms
step:1053/2110 train_time:44739ms step_avg:42.49ms
step:1054/2110 train_time:44798ms step_avg:42.50ms
step:1055/2110 train_time:44859ms step_avg:42.52ms
step:1056/2110 train_time:44918ms step_avg:42.54ms
step:1057/2110 train_time:44979ms step_avg:42.55ms
step:1058/2110 train_time:45037ms step_avg:42.57ms
step:1059/2110 train_time:45097ms step_avg:42.58ms
step:1060/2110 train_time:45155ms step_avg:42.60ms
step:1061/2110 train_time:45215ms step_avg:42.62ms
step:1062/2110 train_time:45273ms step_avg:42.63ms
step:1063/2110 train_time:45333ms step_avg:42.65ms
step:1064/2110 train_time:45391ms step_avg:42.66ms
step:1065/2110 train_time:45451ms step_avg:42.68ms
step:1066/2110 train_time:45509ms step_avg:42.69ms
step:1067/2110 train_time:45570ms step_avg:42.71ms
step:1068/2110 train_time:45628ms step_avg:42.72ms
step:1069/2110 train_time:45689ms step_avg:42.74ms
step:1070/2110 train_time:45748ms step_avg:42.75ms
step:1071/2110 train_time:45808ms step_avg:42.77ms
step:1072/2110 train_time:45868ms step_avg:42.79ms
step:1073/2110 train_time:45929ms step_avg:42.80ms
step:1074/2110 train_time:45987ms step_avg:42.82ms
step:1075/2110 train_time:46048ms step_avg:42.84ms
step:1076/2110 train_time:46106ms step_avg:42.85ms
step:1077/2110 train_time:46166ms step_avg:42.87ms
step:1078/2110 train_time:46225ms step_avg:42.88ms
step:1079/2110 train_time:46284ms step_avg:42.89ms
step:1080/2110 train_time:46342ms step_avg:42.91ms
step:1081/2110 train_time:46402ms step_avg:42.92ms
step:1082/2110 train_time:46460ms step_avg:42.94ms
step:1083/2110 train_time:46520ms step_avg:42.95ms
step:1084/2110 train_time:46579ms step_avg:42.97ms
step:1085/2110 train_time:46638ms step_avg:42.98ms
step:1086/2110 train_time:46696ms step_avg:43.00ms
step:1087/2110 train_time:46756ms step_avg:43.01ms
step:1088/2110 train_time:46815ms step_avg:43.03ms
step:1089/2110 train_time:46875ms step_avg:43.04ms
step:1090/2110 train_time:46934ms step_avg:43.06ms
step:1091/2110 train_time:46994ms step_avg:43.07ms
step:1092/2110 train_time:47053ms step_avg:43.09ms
step:1093/2110 train_time:47113ms step_avg:43.10ms
step:1094/2110 train_time:47172ms step_avg:43.12ms
step:1095/2110 train_time:47232ms step_avg:43.13ms
step:1096/2110 train_time:47290ms step_avg:43.15ms
step:1097/2110 train_time:47350ms step_avg:43.16ms
step:1098/2110 train_time:47408ms step_avg:43.18ms
step:1099/2110 train_time:47469ms step_avg:43.19ms
step:1100/2110 train_time:47527ms step_avg:43.21ms
step:1101/2110 train_time:47587ms step_avg:43.22ms
step:1102/2110 train_time:47645ms step_avg:43.24ms
step:1103/2110 train_time:47705ms step_avg:43.25ms
step:1104/2110 train_time:47764ms step_avg:43.26ms
step:1105/2110 train_time:47823ms step_avg:43.28ms
step:1106/2110 train_time:47881ms step_avg:43.29ms
step:1107/2110 train_time:47943ms step_avg:43.31ms
step:1108/2110 train_time:48002ms step_avg:43.32ms
step:1109/2110 train_time:48061ms step_avg:43.34ms
step:1110/2110 train_time:48121ms step_avg:43.35ms
step:1111/2110 train_time:48181ms step_avg:43.37ms
step:1112/2110 train_time:48240ms step_avg:43.38ms
step:1113/2110 train_time:48300ms step_avg:43.40ms
step:1114/2110 train_time:48358ms step_avg:43.41ms
step:1115/2110 train_time:48418ms step_avg:43.42ms
step:1116/2110 train_time:48476ms step_avg:43.44ms
step:1117/2110 train_time:48536ms step_avg:43.45ms
step:1118/2110 train_time:48594ms step_avg:43.47ms
step:1119/2110 train_time:48655ms step_avg:43.48ms
step:1120/2110 train_time:48713ms step_avg:43.49ms
step:1121/2110 train_time:48774ms step_avg:43.51ms
step:1122/2110 train_time:48832ms step_avg:43.52ms
step:1123/2110 train_time:48894ms step_avg:43.54ms
step:1124/2110 train_time:48952ms step_avg:43.55ms
step:1125/2110 train_time:49013ms step_avg:43.57ms
step:1126/2110 train_time:49072ms step_avg:43.58ms
step:1127/2110 train_time:49132ms step_avg:43.60ms
step:1128/2110 train_time:49192ms step_avg:43.61ms
step:1129/2110 train_time:49252ms step_avg:43.62ms
step:1130/2110 train_time:49310ms step_avg:43.64ms
step:1131/2110 train_time:49370ms step_avg:43.65ms
step:1132/2110 train_time:49429ms step_avg:43.66ms
step:1133/2110 train_time:49489ms step_avg:43.68ms
step:1134/2110 train_time:49547ms step_avg:43.69ms
step:1135/2110 train_time:49607ms step_avg:43.71ms
step:1136/2110 train_time:49665ms step_avg:43.72ms
step:1137/2110 train_time:49725ms step_avg:43.73ms
step:1138/2110 train_time:49784ms step_avg:43.75ms
step:1139/2110 train_time:49843ms step_avg:43.76ms
step:1140/2110 train_time:49903ms step_avg:43.77ms
step:1141/2110 train_time:49962ms step_avg:43.79ms
step:1142/2110 train_time:50022ms step_avg:43.80ms
step:1143/2110 train_time:50081ms step_avg:43.82ms
step:1144/2110 train_time:50142ms step_avg:43.83ms
step:1145/2110 train_time:50202ms step_avg:43.84ms
step:1146/2110 train_time:50261ms step_avg:43.86ms
step:1147/2110 train_time:50321ms step_avg:43.87ms
step:1148/2110 train_time:50380ms step_avg:43.88ms
step:1149/2110 train_time:50440ms step_avg:43.90ms
step:1150/2110 train_time:50499ms step_avg:43.91ms
step:1151/2110 train_time:50559ms step_avg:43.93ms
step:1152/2110 train_time:50618ms step_avg:43.94ms
step:1153/2110 train_time:50679ms step_avg:43.95ms
step:1154/2110 train_time:50742ms step_avg:43.97ms
step:1155/2110 train_time:50799ms step_avg:43.98ms
step:1156/2110 train_time:50864ms step_avg:44.00ms
step:1157/2110 train_time:50918ms step_avg:44.01ms
step:1158/2110 train_time:50977ms step_avg:44.02ms
step:1159/2110 train_time:51038ms step_avg:44.04ms
step:1160/2110 train_time:51096ms step_avg:44.05ms
step:1161/2110 train_time:51157ms step_avg:44.06ms
step:1162/2110 train_time:51216ms step_avg:44.08ms
step:1163/2110 train_time:51277ms step_avg:44.09ms
step:1164/2110 train_time:51336ms step_avg:44.10ms
step:1165/2110 train_time:51396ms step_avg:44.12ms
step:1166/2110 train_time:51455ms step_avg:44.13ms
step:1167/2110 train_time:51515ms step_avg:44.14ms
step:1168/2110 train_time:51574ms step_avg:44.16ms
step:1169/2110 train_time:51635ms step_avg:44.17ms
step:1170/2110 train_time:51695ms step_avg:44.18ms
step:1171/2110 train_time:51755ms step_avg:44.20ms
step:1172/2110 train_time:51814ms step_avg:44.21ms
step:1173/2110 train_time:51874ms step_avg:44.22ms
step:1174/2110 train_time:51933ms step_avg:44.24ms
step:1175/2110 train_time:51995ms step_avg:44.25ms
step:1176/2110 train_time:52054ms step_avg:44.26ms
step:1177/2110 train_time:52115ms step_avg:44.28ms
step:1178/2110 train_time:52174ms step_avg:44.29ms
step:1179/2110 train_time:52235ms step_avg:44.30ms
step:1180/2110 train_time:52294ms step_avg:44.32ms
step:1181/2110 train_time:52355ms step_avg:44.33ms
step:1182/2110 train_time:52415ms step_avg:44.34ms
step:1183/2110 train_time:52475ms step_avg:44.36ms
step:1184/2110 train_time:52533ms step_avg:44.37ms
step:1185/2110 train_time:52594ms step_avg:44.38ms
step:1186/2110 train_time:52653ms step_avg:44.40ms
step:1187/2110 train_time:52714ms step_avg:44.41ms
step:1188/2110 train_time:52773ms step_avg:44.42ms
step:1189/2110 train_time:52834ms step_avg:44.44ms
step:1190/2110 train_time:52893ms step_avg:44.45ms
step:1191/2110 train_time:52955ms step_avg:44.46ms
step:1192/2110 train_time:53014ms step_avg:44.47ms
step:1193/2110 train_time:53075ms step_avg:44.49ms
step:1194/2110 train_time:53133ms step_avg:44.50ms
step:1195/2110 train_time:53195ms step_avg:44.51ms
step:1196/2110 train_time:53254ms step_avg:44.53ms
step:1197/2110 train_time:53314ms step_avg:44.54ms
step:1198/2110 train_time:53373ms step_avg:44.55ms
step:1199/2110 train_time:53434ms step_avg:44.57ms
step:1200/2110 train_time:53493ms step_avg:44.58ms
step:1201/2110 train_time:53554ms step_avg:44.59ms
step:1202/2110 train_time:53613ms step_avg:44.60ms
step:1203/2110 train_time:53674ms step_avg:44.62ms
step:1204/2110 train_time:53732ms step_avg:44.63ms
step:1205/2110 train_time:53794ms step_avg:44.64ms
step:1206/2110 train_time:53853ms step_avg:44.65ms
step:1207/2110 train_time:53914ms step_avg:44.67ms
step:1208/2110 train_time:53972ms step_avg:44.68ms
step:1209/2110 train_time:54033ms step_avg:44.69ms
step:1210/2110 train_time:54093ms step_avg:44.70ms
step:1211/2110 train_time:54154ms step_avg:44.72ms
step:1212/2110 train_time:54213ms step_avg:44.73ms
step:1213/2110 train_time:54274ms step_avg:44.74ms
step:1214/2110 train_time:54333ms step_avg:44.76ms
step:1215/2110 train_time:54394ms step_avg:44.77ms
step:1216/2110 train_time:54453ms step_avg:44.78ms
step:1217/2110 train_time:54515ms step_avg:44.79ms
step:1218/2110 train_time:54574ms step_avg:44.81ms
step:1219/2110 train_time:54634ms step_avg:44.82ms
step:1220/2110 train_time:54693ms step_avg:44.83ms
step:1221/2110 train_time:54754ms step_avg:44.84ms
step:1222/2110 train_time:54813ms step_avg:44.85ms
step:1223/2110 train_time:54874ms step_avg:44.87ms
step:1224/2110 train_time:54933ms step_avg:44.88ms
step:1225/2110 train_time:54993ms step_avg:44.89ms
step:1226/2110 train_time:55054ms step_avg:44.91ms
step:1227/2110 train_time:55113ms step_avg:44.92ms
step:1228/2110 train_time:55171ms step_avg:44.93ms
step:1229/2110 train_time:55233ms step_avg:44.94ms
step:1230/2110 train_time:55292ms step_avg:44.95ms
step:1231/2110 train_time:55354ms step_avg:44.97ms
step:1232/2110 train_time:55411ms step_avg:44.98ms
step:1233/2110 train_time:55472ms step_avg:44.99ms
step:1234/2110 train_time:55531ms step_avg:45.00ms
step:1235/2110 train_time:55592ms step_avg:45.01ms
step:1236/2110 train_time:55651ms step_avg:45.02ms
step:1237/2110 train_time:55712ms step_avg:45.04ms
step:1238/2110 train_time:55771ms step_avg:45.05ms
step:1239/2110 train_time:55832ms step_avg:45.06ms
step:1240/2110 train_time:55892ms step_avg:45.07ms
step:1241/2110 train_time:55953ms step_avg:45.09ms
step:1242/2110 train_time:56012ms step_avg:45.10ms
step:1243/2110 train_time:56072ms step_avg:45.11ms
step:1244/2110 train_time:56131ms step_avg:45.12ms
step:1245/2110 train_time:56192ms step_avg:45.13ms
step:1246/2110 train_time:56251ms step_avg:45.15ms
step:1247/2110 train_time:56312ms step_avg:45.16ms
step:1248/2110 train_time:56370ms step_avg:45.17ms
step:1249/2110 train_time:56431ms step_avg:45.18ms
step:1250/2110 train_time:56490ms step_avg:45.19ms
step:1250/2110 val_loss:3.5935 train_time:56553ms step_avg:45.24ms
step:1251/2110 train_time:56580ms step_avg:45.23ms
step:1252/2110 train_time:56614ms step_avg:45.22ms
step:1253/2110 train_time:56678ms step_avg:45.23ms
step:1254/2110 train_time:56741ms step_avg:45.25ms
step:1255/2110 train_time:56802ms step_avg:45.26ms
step:1256/2110 train_time:56861ms step_avg:45.27ms
step:1257/2110 train_time:56922ms step_avg:45.28ms
step:1258/2110 train_time:56981ms step_avg:45.29ms
step:1259/2110 train_time:57040ms step_avg:45.31ms
step:1260/2110 train_time:57099ms step_avg:45.32ms
step:1261/2110 train_time:57159ms step_avg:45.33ms
step:1262/2110 train_time:57216ms step_avg:45.34ms
step:1263/2110 train_time:57277ms step_avg:45.35ms
step:1264/2110 train_time:57335ms step_avg:45.36ms
step:1265/2110 train_time:57396ms step_avg:45.37ms
step:1266/2110 train_time:57455ms step_avg:45.38ms
step:1267/2110 train_time:57516ms step_avg:45.40ms
step:1268/2110 train_time:57577ms step_avg:45.41ms
step:1269/2110 train_time:57641ms step_avg:45.42ms
step:1270/2110 train_time:57701ms step_avg:45.43ms
step:1271/2110 train_time:57762ms step_avg:45.45ms
step:1272/2110 train_time:57821ms step_avg:45.46ms
step:1273/2110 train_time:57882ms step_avg:45.47ms
step:1274/2110 train_time:57942ms step_avg:45.48ms
step:1275/2110 train_time:58001ms step_avg:45.49ms
step:1276/2110 train_time:58061ms step_avg:45.50ms
step:1277/2110 train_time:58120ms step_avg:45.51ms
step:1278/2110 train_time:58180ms step_avg:45.52ms
step:1279/2110 train_time:58240ms step_avg:45.54ms
step:1280/2110 train_time:58299ms step_avg:45.55ms
step:1281/2110 train_time:58358ms step_avg:45.56ms
step:1282/2110 train_time:58418ms step_avg:45.57ms
step:1283/2110 train_time:58478ms step_avg:45.58ms
step:1284/2110 train_time:58538ms step_avg:45.59ms
step:1285/2110 train_time:58600ms step_avg:45.60ms
step:1286/2110 train_time:58660ms step_avg:45.61ms
step:1287/2110 train_time:58720ms step_avg:45.63ms
step:1288/2110 train_time:58780ms step_avg:45.64ms
step:1289/2110 train_time:58841ms step_avg:45.65ms
step:1290/2110 train_time:58900ms step_avg:45.66ms
step:1291/2110 train_time:58960ms step_avg:45.67ms
step:1292/2110 train_time:59019ms step_avg:45.68ms
step:1293/2110 train_time:59079ms step_avg:45.69ms
step:1294/2110 train_time:59138ms step_avg:45.70ms
step:1295/2110 train_time:59198ms step_avg:45.71ms
step:1296/2110 train_time:59257ms step_avg:45.72ms
step:1297/2110 train_time:59316ms step_avg:45.73ms
step:1298/2110 train_time:59376ms step_avg:45.74ms
step:1299/2110 train_time:59436ms step_avg:45.76ms
step:1300/2110 train_time:59496ms step_avg:45.77ms
step:1301/2110 train_time:59556ms step_avg:45.78ms
step:1302/2110 train_time:59616ms step_avg:45.79ms
step:1303/2110 train_time:59677ms step_avg:45.80ms
step:1304/2110 train_time:59737ms step_avg:45.81ms
step:1305/2110 train_time:59798ms step_avg:45.82ms
step:1306/2110 train_time:59858ms step_avg:45.83ms
step:1307/2110 train_time:59919ms step_avg:45.84ms
step:1308/2110 train_time:59978ms step_avg:45.85ms
step:1309/2110 train_time:60038ms step_avg:45.87ms
step:1310/2110 train_time:60098ms step_avg:45.88ms
step:1311/2110 train_time:60158ms step_avg:45.89ms
step:1312/2110 train_time:60217ms step_avg:45.90ms
step:1313/2110 train_time:60276ms step_avg:45.91ms
step:1314/2110 train_time:60335ms step_avg:45.92ms
step:1315/2110 train_time:60395ms step_avg:45.93ms
step:1316/2110 train_time:60456ms step_avg:45.94ms
step:1317/2110 train_time:60514ms step_avg:45.95ms
step:1318/2110 train_time:60573ms step_avg:45.96ms
step:1319/2110 train_time:60635ms step_avg:45.97ms
step:1320/2110 train_time:60695ms step_avg:45.98ms
step:1321/2110 train_time:60756ms step_avg:45.99ms
step:1322/2110 train_time:60817ms step_avg:46.00ms
step:1323/2110 train_time:60878ms step_avg:46.02ms
step:1324/2110 train_time:60938ms step_avg:46.03ms
step:1325/2110 train_time:60998ms step_avg:46.04ms
step:1326/2110 train_time:61057ms step_avg:46.05ms
step:1327/2110 train_time:61117ms step_avg:46.06ms
step:1328/2110 train_time:61177ms step_avg:46.07ms
step:1329/2110 train_time:61236ms step_avg:46.08ms
step:1330/2110 train_time:61295ms step_avg:46.09ms
step:1331/2110 train_time:61356ms step_avg:46.10ms
step:1332/2110 train_time:61415ms step_avg:46.11ms
step:1333/2110 train_time:61476ms step_avg:46.12ms
step:1334/2110 train_time:61534ms step_avg:46.13ms
step:1335/2110 train_time:61595ms step_avg:46.14ms
step:1336/2110 train_time:61654ms step_avg:46.15ms
step:1337/2110 train_time:61714ms step_avg:46.16ms
step:1338/2110 train_time:61773ms step_avg:46.17ms
step:1339/2110 train_time:61835ms step_avg:46.18ms
step:1340/2110 train_time:61894ms step_avg:46.19ms
step:1341/2110 train_time:61956ms step_avg:46.20ms
step:1342/2110 train_time:62017ms step_avg:46.21ms
step:1343/2110 train_time:62075ms step_avg:46.22ms
step:1344/2110 train_time:62134ms step_avg:46.23ms
step:1345/2110 train_time:62195ms step_avg:46.24ms
step:1346/2110 train_time:62254ms step_avg:46.25ms
step:1347/2110 train_time:62313ms step_avg:46.26ms
step:1348/2110 train_time:62373ms step_avg:46.27ms
step:1349/2110 train_time:62433ms step_avg:46.28ms
step:1350/2110 train_time:62492ms step_avg:46.29ms
step:1351/2110 train_time:62552ms step_avg:46.30ms
step:1352/2110 train_time:62612ms step_avg:46.31ms
step:1353/2110 train_time:62673ms step_avg:46.32ms
step:1354/2110 train_time:62733ms step_avg:46.33ms
step:1355/2110 train_time:62795ms step_avg:46.34ms
step:1356/2110 train_time:62854ms step_avg:46.35ms
step:1357/2110 train_time:62915ms step_avg:46.36ms
step:1358/2110 train_time:62975ms step_avg:46.37ms
step:1359/2110 train_time:63035ms step_avg:46.38ms
step:1360/2110 train_time:63094ms step_avg:46.39ms
step:1361/2110 train_time:63155ms step_avg:46.40ms
step:1362/2110 train_time:63214ms step_avg:46.41ms
step:1363/2110 train_time:63275ms step_avg:46.42ms
step:1364/2110 train_time:63334ms step_avg:46.43ms
step:1365/2110 train_time:63394ms step_avg:46.44ms
step:1366/2110 train_time:63453ms step_avg:46.45ms
step:1367/2110 train_time:63513ms step_avg:46.46ms
step:1368/2110 train_time:63572ms step_avg:46.47ms
step:1369/2110 train_time:63633ms step_avg:46.48ms
step:1370/2110 train_time:63692ms step_avg:46.49ms
step:1371/2110 train_time:63753ms step_avg:46.50ms
step:1372/2110 train_time:63814ms step_avg:46.51ms
step:1373/2110 train_time:63875ms step_avg:46.52ms
step:1374/2110 train_time:63934ms step_avg:46.53ms
step:1375/2110 train_time:63996ms step_avg:46.54ms
step:1376/2110 train_time:64055ms step_avg:46.55ms
step:1377/2110 train_time:64115ms step_avg:46.56ms
step:1378/2110 train_time:64174ms step_avg:46.57ms
step:1379/2110 train_time:64234ms step_avg:46.58ms
step:1380/2110 train_time:64293ms step_avg:46.59ms
step:1381/2110 train_time:64354ms step_avg:46.60ms
step:1382/2110 train_time:64440ms step_avg:46.63ms
step:1383/2110 train_time:64526ms step_avg:46.66ms
step:1384/2110 train_time:64614ms step_avg:46.69ms
step:1385/2110 train_time:64700ms step_avg:46.71ms
step:1386/2110 train_time:64788ms step_avg:46.74ms
step:1387/2110 train_time:64874ms step_avg:46.77ms
step:1388/2110 train_time:64962ms step_avg:46.80ms
step:1389/2110 train_time:65048ms step_avg:46.83ms
step:1390/2110 train_time:65135ms step_avg:46.86ms
step:1391/2110 train_time:65222ms step_avg:46.89ms
step:1392/2110 train_time:65309ms step_avg:46.92ms
step:1393/2110 train_time:65396ms step_avg:46.95ms
step:1394/2110 train_time:65483ms step_avg:46.97ms
step:1395/2110 train_time:65569ms step_avg:47.00ms
step:1396/2110 train_time:65656ms step_avg:47.03ms
step:1397/2110 train_time:65744ms step_avg:47.06ms
step:1398/2110 train_time:65830ms step_avg:47.09ms
step:1399/2110 train_time:65917ms step_avg:47.12ms
step:1400/2110 train_time:66004ms step_avg:47.15ms
step:1401/2110 train_time:66092ms step_avg:47.17ms
step:1402/2110 train_time:66179ms step_avg:47.20ms
step:1403/2110 train_time:66266ms step_avg:47.23ms
step:1404/2110 train_time:66353ms step_avg:47.26ms
step:1405/2110 train_time:66438ms step_avg:47.29ms
step:1406/2110 train_time:66525ms step_avg:47.32ms
step:1407/2110 train_time:66611ms step_avg:47.34ms
step:1408/2110 train_time:66699ms step_avg:47.37ms
step:1409/2110 train_time:66785ms step_avg:47.40ms
step:1410/2110 train_time:66873ms step_avg:47.43ms
step:1411/2110 train_time:66959ms step_avg:47.46ms
step:1412/2110 train_time:67046ms step_avg:47.48ms
step:1413/2110 train_time:67134ms step_avg:47.51ms
step:1414/2110 train_time:67221ms step_avg:47.54ms
step:1415/2110 train_time:67307ms step_avg:47.57ms
step:1416/2110 train_time:67394ms step_avg:47.59ms
step:1417/2110 train_time:67480ms step_avg:47.62ms
step:1418/2110 train_time:67567ms step_avg:47.65ms
step:1419/2110 train_time:67654ms step_avg:47.68ms
step:1420/2110 train_time:67741ms step_avg:47.71ms
step:1421/2110 train_time:67828ms step_avg:47.73ms
step:1422/2110 train_time:67915ms step_avg:47.76ms
step:1423/2110 train_time:68001ms step_avg:47.79ms
step:1424/2110 train_time:68087ms step_avg:47.81ms
step:1425/2110 train_time:68175ms step_avg:47.84ms
step:1426/2110 train_time:68262ms step_avg:47.87ms
step:1427/2110 train_time:68348ms step_avg:47.90ms
step:1428/2110 train_time:68435ms step_avg:47.92ms
step:1429/2110 train_time:68522ms step_avg:47.95ms
step:1430/2110 train_time:68609ms step_avg:47.98ms
step:1431/2110 train_time:68695ms step_avg:48.00ms
step:1432/2110 train_time:68783ms step_avg:48.03ms
step:1433/2110 train_time:68869ms step_avg:48.06ms
step:1434/2110 train_time:68962ms step_avg:48.09ms
step:1435/2110 train_time:69044ms step_avg:48.11ms
step:1436/2110 train_time:69131ms step_avg:48.14ms
step:1437/2110 train_time:69218ms step_avg:48.17ms
step:1438/2110 train_time:69305ms step_avg:48.20ms
step:1439/2110 train_time:69391ms step_avg:48.22ms
step:1440/2110 train_time:69479ms step_avg:48.25ms
step:1441/2110 train_time:69565ms step_avg:48.28ms
step:1442/2110 train_time:69653ms step_avg:48.30ms
step:1443/2110 train_time:69739ms step_avg:48.33ms
step:1444/2110 train_time:69826ms step_avg:48.36ms
step:1445/2110 train_time:69914ms step_avg:48.38ms
step:1446/2110 train_time:70001ms step_avg:48.41ms
step:1447/2110 train_time:70087ms step_avg:48.44ms
step:1448/2110 train_time:70174ms step_avg:48.46ms
step:1449/2110 train_time:70261ms step_avg:48.49ms
step:1450/2110 train_time:70348ms step_avg:48.52ms
step:1451/2110 train_time:70436ms step_avg:48.54ms
step:1452/2110 train_time:70523ms step_avg:48.57ms
step:1453/2110 train_time:70609ms step_avg:48.60ms
step:1454/2110 train_time:70696ms step_avg:48.62ms
step:1455/2110 train_time:70784ms step_avg:48.65ms
step:1456/2110 train_time:70871ms step_avg:48.67ms
step:1457/2110 train_time:70958ms step_avg:48.70ms
step:1458/2110 train_time:71045ms step_avg:48.73ms
step:1459/2110 train_time:71131ms step_avg:48.75ms
step:1460/2110 train_time:71219ms step_avg:48.78ms
step:1461/2110 train_time:71305ms step_avg:48.81ms
step:1462/2110 train_time:71392ms step_avg:48.83ms
step:1463/2110 train_time:71479ms step_avg:48.86ms
step:1464/2110 train_time:71566ms step_avg:48.88ms
step:1465/2110 train_time:71653ms step_avg:48.91ms
step:1466/2110 train_time:71741ms step_avg:48.94ms
step:1467/2110 train_time:71829ms step_avg:48.96ms
step:1468/2110 train_time:71916ms step_avg:48.99ms
step:1469/2110 train_time:72002ms step_avg:49.01ms
step:1470/2110 train_time:72090ms step_avg:49.04ms
step:1471/2110 train_time:72176ms step_avg:49.07ms
step:1472/2110 train_time:72263ms step_avg:49.09ms
step:1473/2110 train_time:72349ms step_avg:49.12ms
step:1474/2110 train_time:72436ms step_avg:49.14ms
step:1475/2110 train_time:72523ms step_avg:49.17ms
step:1476/2110 train_time:72609ms step_avg:49.19ms
step:1477/2110 train_time:72697ms step_avg:49.22ms
step:1478/2110 train_time:72784ms step_avg:49.25ms
step:1479/2110 train_time:72871ms step_avg:49.27ms
step:1480/2110 train_time:72959ms step_avg:49.30ms
step:1481/2110 train_time:73045ms step_avg:49.32ms
step:1482/2110 train_time:73132ms step_avg:49.35ms
step:1483/2110 train_time:73219ms step_avg:49.37ms
step:1484/2110 train_time:73306ms step_avg:49.40ms
step:1485/2110 train_time:73392ms step_avg:49.42ms
step:1486/2110 train_time:73480ms step_avg:49.45ms
step:1487/2110 train_time:73567ms step_avg:49.47ms
step:1488/2110 train_time:73654ms step_avg:49.50ms
step:1489/2110 train_time:73740ms step_avg:49.52ms
step:1490/2110 train_time:73826ms step_avg:49.55ms
step:1491/2110 train_time:73914ms step_avg:49.57ms
step:1492/2110 train_time:74002ms step_avg:49.60ms
step:1493/2110 train_time:74087ms step_avg:49.62ms
step:1494/2110 train_time:74175ms step_avg:49.65ms
step:1495/2110 train_time:74261ms step_avg:49.67ms
step:1496/2110 train_time:74348ms step_avg:49.70ms
step:1497/2110 train_time:74435ms step_avg:49.72ms
step:1498/2110 train_time:74522ms step_avg:49.75ms
step:1499/2110 train_time:74609ms step_avg:49.77ms
step:1500/2110 train_time:74697ms step_avg:49.80ms
step:1500/2110 val_loss:3.4922 train_time:74785ms step_avg:49.86ms
step:1501/2110 train_time:74823ms step_avg:49.85ms
step:1502/2110 train_time:74878ms step_avg:49.85ms
step:1503/2110 train_time:74971ms step_avg:49.88ms
step:1504/2110 train_time:75060ms step_avg:49.91ms
step:1505/2110 train_time:75147ms step_avg:49.93ms
step:1506/2110 train_time:75233ms step_avg:49.96ms
step:1507/2110 train_time:75319ms step_avg:49.98ms
step:1508/2110 train_time:75405ms step_avg:50.00ms
step:1509/2110 train_time:75489ms step_avg:50.03ms
step:1510/2110 train_time:75576ms step_avg:50.05ms
step:1511/2110 train_time:75662ms step_avg:50.07ms
step:1512/2110 train_time:75749ms step_avg:50.10ms
step:1513/2110 train_time:75837ms step_avg:50.12ms
step:1514/2110 train_time:75926ms step_avg:50.15ms
step:1515/2110 train_time:76015ms step_avg:50.17ms
step:1516/2110 train_time:76103ms step_avg:50.20ms
step:1517/2110 train_time:76189ms step_avg:50.22ms
step:1518/2110 train_time:76276ms step_avg:50.25ms
step:1519/2110 train_time:76361ms step_avg:50.27ms
step:1520/2110 train_time:76447ms step_avg:50.29ms
step:1521/2110 train_time:76533ms step_avg:50.32ms
step:1522/2110 train_time:76619ms step_avg:50.34ms
step:1523/2110 train_time:76706ms step_avg:50.36ms
step:1524/2110 train_time:76793ms step_avg:50.39ms
step:1525/2110 train_time:76881ms step_avg:50.41ms
step:1526/2110 train_time:76968ms step_avg:50.44ms
step:1527/2110 train_time:77057ms step_avg:50.46ms
step:1528/2110 train_time:77145ms step_avg:50.49ms
step:1529/2110 train_time:77230ms step_avg:50.51ms
step:1530/2110 train_time:77318ms step_avg:50.53ms
step:1531/2110 train_time:77405ms step_avg:50.56ms
step:1532/2110 train_time:77488ms step_avg:50.58ms
step:1533/2110 train_time:77575ms step_avg:50.60ms
step:1534/2110 train_time:77662ms step_avg:50.63ms
step:1535/2110 train_time:77749ms step_avg:50.65ms
step:1536/2110 train_time:77837ms step_avg:50.67ms
step:1537/2110 train_time:77925ms step_avg:50.70ms
step:1538/2110 train_time:78012ms step_avg:50.72ms
step:1539/2110 train_time:78100ms step_avg:50.75ms
step:1540/2110 train_time:78186ms step_avg:50.77ms
step:1541/2110 train_time:78274ms step_avg:50.79ms
step:1542/2110 train_time:78362ms step_avg:50.82ms
step:1543/2110 train_time:78449ms step_avg:50.84ms
step:1544/2110 train_time:78534ms step_avg:50.86ms
step:1545/2110 train_time:78621ms step_avg:50.89ms
step:1546/2110 train_time:78706ms step_avg:50.91ms
step:1547/2110 train_time:78794ms step_avg:50.93ms
step:1548/2110 train_time:78882ms step_avg:50.96ms
step:1549/2110 train_time:78970ms step_avg:50.98ms
step:1550/2110 train_time:79056ms step_avg:51.00ms
step:1551/2110 train_time:79144ms step_avg:51.03ms
step:1552/2110 train_time:79230ms step_avg:51.05ms
step:1553/2110 train_time:79317ms step_avg:51.07ms
step:1554/2110 train_time:79403ms step_avg:51.10ms
step:1555/2110 train_time:79490ms step_avg:51.12ms
step:1556/2110 train_time:79575ms step_avg:51.14ms
step:1557/2110 train_time:79663ms step_avg:51.16ms
step:1558/2110 train_time:79749ms step_avg:51.19ms
step:1559/2110 train_time:79836ms step_avg:51.21ms
step:1560/2110 train_time:79924ms step_avg:51.23ms
step:1561/2110 train_time:80012ms step_avg:51.26ms
step:1562/2110 train_time:80098ms step_avg:51.28ms
step:1563/2110 train_time:80185ms step_avg:51.30ms
step:1564/2110 train_time:80271ms step_avg:51.32ms
step:1565/2110 train_time:80359ms step_avg:51.35ms
step:1566/2110 train_time:80446ms step_avg:51.37ms
step:1567/2110 train_time:80532ms step_avg:51.39ms
step:1568/2110 train_time:80618ms step_avg:51.41ms
step:1569/2110 train_time:80705ms step_avg:51.44ms
step:1570/2110 train_time:80790ms step_avg:51.46ms
step:1571/2110 train_time:80879ms step_avg:51.48ms
step:1572/2110 train_time:80966ms step_avg:51.51ms
step:1573/2110 train_time:81054ms step_avg:51.53ms
step:1574/2110 train_time:81141ms step_avg:51.55ms
step:1575/2110 train_time:81227ms step_avg:51.57ms
step:1576/2110 train_time:81313ms step_avg:51.59ms
step:1577/2110 train_time:81401ms step_avg:51.62ms
step:1578/2110 train_time:81487ms step_avg:51.64ms
step:1579/2110 train_time:81574ms step_avg:51.66ms
step:1580/2110 train_time:81661ms step_avg:51.68ms
step:1581/2110 train_time:81748ms step_avg:51.71ms
step:1582/2110 train_time:81835ms step_avg:51.73ms
step:1583/2110 train_time:81922ms step_avg:51.75ms
step:1584/2110 train_time:82009ms step_avg:51.77ms
step:1585/2110 train_time:82097ms step_avg:51.80ms
step:1586/2110 train_time:82184ms step_avg:51.82ms
step:1587/2110 train_time:82272ms step_avg:51.84ms
step:1588/2110 train_time:82358ms step_avg:51.86ms
step:1589/2110 train_time:82444ms step_avg:51.88ms
step:1590/2110 train_time:82530ms step_avg:51.91ms
step:1591/2110 train_time:82617ms step_avg:51.93ms
step:1592/2110 train_time:82704ms step_avg:51.95ms
step:1593/2110 train_time:82792ms step_avg:51.97ms
step:1594/2110 train_time:82878ms step_avg:51.99ms
step:1595/2110 train_time:82967ms step_avg:52.02ms
step:1596/2110 train_time:83053ms step_avg:52.04ms
step:1597/2110 train_time:83140ms step_avg:52.06ms
step:1598/2110 train_time:83228ms step_avg:52.08ms
step:1599/2110 train_time:83315ms step_avg:52.10ms
step:1600/2110 train_time:83401ms step_avg:52.13ms
step:1601/2110 train_time:83488ms step_avg:52.15ms
step:1602/2110 train_time:83574ms step_avg:52.17ms
step:1603/2110 train_time:83662ms step_avg:52.19ms
step:1604/2110 train_time:83749ms step_avg:52.21ms
step:1605/2110 train_time:83835ms step_avg:52.23ms
step:1606/2110 train_time:83923ms step_avg:52.26ms
step:1607/2110 train_time:84009ms step_avg:52.28ms
step:1608/2110 train_time:84096ms step_avg:52.30ms
step:1609/2110 train_time:84183ms step_avg:52.32ms
step:1610/2110 train_time:84269ms step_avg:52.34ms
step:1611/2110 train_time:84358ms step_avg:52.36ms
step:1612/2110 train_time:84443ms step_avg:52.38ms
step:1613/2110 train_time:84530ms step_avg:52.41ms
step:1614/2110 train_time:84617ms step_avg:52.43ms
step:1615/2110 train_time:84704ms step_avg:52.45ms
step:1616/2110 train_time:84790ms step_avg:52.47ms
step:1617/2110 train_time:84877ms step_avg:52.49ms
step:1618/2110 train_time:84964ms step_avg:52.51ms
step:1619/2110 train_time:85051ms step_avg:52.53ms
step:1620/2110 train_time:85139ms step_avg:52.55ms
step:1621/2110 train_time:85227ms step_avg:52.58ms
step:1622/2110 train_time:85315ms step_avg:52.60ms
step:1623/2110 train_time:85402ms step_avg:52.62ms
step:1624/2110 train_time:85487ms step_avg:52.64ms
step:1625/2110 train_time:85575ms step_avg:52.66ms
step:1626/2110 train_time:85661ms step_avg:52.68ms
step:1627/2110 train_time:85748ms step_avg:52.70ms
step:1628/2110 train_time:85834ms step_avg:52.72ms
step:1629/2110 train_time:85921ms step_avg:52.74ms
step:1630/2110 train_time:86008ms step_avg:52.77ms
step:1631/2110 train_time:86096ms step_avg:52.79ms
step:1632/2110 train_time:86183ms step_avg:52.81ms
step:1633/2110 train_time:86270ms step_avg:52.83ms
step:1634/2110 train_time:86356ms step_avg:52.85ms
step:1635/2110 train_time:86443ms step_avg:52.87ms
step:1636/2110 train_time:86529ms step_avg:52.89ms
step:1637/2110 train_time:86617ms step_avg:52.91ms
step:1638/2110 train_time:86704ms step_avg:52.93ms
step:1639/2110 train_time:86790ms step_avg:52.95ms
step:1640/2110 train_time:86877ms step_avg:52.97ms
step:1641/2110 train_time:86964ms step_avg:52.99ms
step:1642/2110 train_time:87050ms step_avg:53.01ms
step:1643/2110 train_time:87138ms step_avg:53.04ms
step:1644/2110 train_time:87225ms step_avg:53.06ms
step:1645/2110 train_time:87313ms step_avg:53.08ms
step:1646/2110 train_time:87399ms step_avg:53.10ms
step:1647/2110 train_time:87486ms step_avg:53.12ms
step:1648/2110 train_time:87572ms step_avg:53.14ms
step:1649/2110 train_time:87660ms step_avg:53.16ms
step:1650/2110 train_time:87746ms step_avg:53.18ms
step:1651/2110 train_time:87833ms step_avg:53.20ms
step:1652/2110 train_time:87920ms step_avg:53.22ms
step:1653/2110 train_time:88007ms step_avg:53.24ms
step:1654/2110 train_time:88094ms step_avg:53.26ms
step:1655/2110 train_time:88181ms step_avg:53.28ms
step:1656/2110 train_time:88267ms step_avg:53.30ms
step:1657/2110 train_time:88355ms step_avg:53.32ms
step:1658/2110 train_time:88443ms step_avg:53.34ms
step:1659/2110 train_time:88530ms step_avg:53.36ms
step:1660/2110 train_time:88618ms step_avg:53.38ms
step:1661/2110 train_time:88707ms step_avg:53.41ms
step:1662/2110 train_time:88794ms step_avg:53.43ms
step:1663/2110 train_time:88883ms step_avg:53.45ms
step:1664/2110 train_time:88971ms step_avg:53.47ms
step:1665/2110 train_time:89060ms step_avg:53.49ms
step:1666/2110 train_time:89148ms step_avg:53.51ms
step:1667/2110 train_time:89237ms step_avg:53.53ms
step:1668/2110 train_time:89326ms step_avg:53.55ms
step:1669/2110 train_time:89415ms step_avg:53.57ms
step:1670/2110 train_time:89502ms step_avg:53.59ms
step:1671/2110 train_time:89590ms step_avg:53.61ms
step:1672/2110 train_time:89677ms step_avg:53.63ms
step:1673/2110 train_time:89766ms step_avg:53.66ms
step:1674/2110 train_time:89854ms step_avg:53.68ms
step:1675/2110 train_time:89943ms step_avg:53.70ms
step:1676/2110 train_time:90031ms step_avg:53.72ms
step:1677/2110 train_time:90121ms step_avg:53.74ms
step:1678/2110 train_time:90207ms step_avg:53.76ms
step:1679/2110 train_time:90296ms step_avg:53.78ms
step:1680/2110 train_time:90384ms step_avg:53.80ms
step:1681/2110 train_time:90472ms step_avg:53.82ms
step:1682/2110 train_time:90559ms step_avg:53.84ms
step:1683/2110 train_time:90648ms step_avg:53.86ms
step:1684/2110 train_time:90736ms step_avg:53.88ms
step:1685/2110 train_time:90824ms step_avg:53.90ms
step:1686/2110 train_time:90911ms step_avg:53.92ms
step:1687/2110 train_time:91002ms step_avg:53.94ms
step:1688/2110 train_time:91089ms step_avg:53.96ms
step:1689/2110 train_time:91177ms step_avg:53.98ms
step:1690/2110 train_time:91266ms step_avg:54.00ms
step:1691/2110 train_time:91355ms step_avg:54.02ms
step:1692/2110 train_time:91443ms step_avg:54.04ms
step:1693/2110 train_time:91531ms step_avg:54.06ms
step:1694/2110 train_time:91619ms step_avg:54.08ms
step:1695/2110 train_time:91707ms step_avg:54.10ms
step:1696/2110 train_time:91794ms step_avg:54.12ms
step:1697/2110 train_time:91883ms step_avg:54.14ms
step:1698/2110 train_time:91971ms step_avg:54.16ms
step:1699/2110 train_time:92060ms step_avg:54.18ms
step:1700/2110 train_time:92148ms step_avg:54.20ms
step:1701/2110 train_time:92237ms step_avg:54.22ms
step:1702/2110 train_time:92325ms step_avg:54.25ms
step:1703/2110 train_time:92413ms step_avg:54.27ms
step:1704/2110 train_time:92501ms step_avg:54.28ms
step:1705/2110 train_time:92589ms step_avg:54.30ms
step:1706/2110 train_time:92677ms step_avg:54.32ms
step:1707/2110 train_time:92765ms step_avg:54.34ms
step:1708/2110 train_time:92852ms step_avg:54.36ms
step:1709/2110 train_time:92941ms step_avg:54.38ms
step:1710/2110 train_time:93030ms step_avg:54.40ms
step:1711/2110 train_time:93119ms step_avg:54.42ms
step:1712/2110 train_time:93206ms step_avg:54.44ms
step:1713/2110 train_time:93295ms step_avg:54.46ms
step:1714/2110 train_time:93383ms step_avg:54.48ms
step:1715/2110 train_time:93470ms step_avg:54.50ms
step:1716/2110 train_time:93558ms step_avg:54.52ms
step:1717/2110 train_time:93646ms step_avg:54.54ms
step:1718/2110 train_time:93733ms step_avg:54.56ms
step:1719/2110 train_time:93822ms step_avg:54.58ms
step:1720/2110 train_time:93909ms step_avg:54.60ms
step:1721/2110 train_time:93998ms step_avg:54.62ms
step:1722/2110 train_time:94086ms step_avg:54.64ms
step:1723/2110 train_time:94175ms step_avg:54.66ms
step:1724/2110 train_time:94265ms step_avg:54.68ms
step:1725/2110 train_time:94352ms step_avg:54.70ms
step:1726/2110 train_time:94440ms step_avg:54.72ms
step:1727/2110 train_time:94529ms step_avg:54.74ms
step:1728/2110 train_time:94616ms step_avg:54.75ms
step:1729/2110 train_time:94705ms step_avg:54.77ms
step:1730/2110 train_time:94792ms step_avg:54.79ms
step:1731/2110 train_time:94881ms step_avg:54.81ms
step:1732/2110 train_time:94969ms step_avg:54.83ms
step:1733/2110 train_time:95058ms step_avg:54.85ms
step:1734/2110 train_time:95145ms step_avg:54.87ms
step:1735/2110 train_time:95233ms step_avg:54.89ms
step:1736/2110 train_time:95321ms step_avg:54.91ms
step:1737/2110 train_time:95409ms step_avg:54.93ms
step:1738/2110 train_time:95498ms step_avg:54.95ms
step:1739/2110 train_time:95586ms step_avg:54.97ms
step:1740/2110 train_time:95673ms step_avg:54.98ms
step:1741/2110 train_time:95763ms step_avg:55.00ms
step:1742/2110 train_time:95850ms step_avg:55.02ms
step:1743/2110 train_time:95940ms step_avg:55.04ms
step:1744/2110 train_time:96029ms step_avg:55.06ms
step:1745/2110 train_time:96118ms step_avg:55.08ms
step:1746/2110 train_time:96205ms step_avg:55.10ms
step:1747/2110 train_time:96295ms step_avg:55.12ms
step:1748/2110 train_time:96382ms step_avg:55.14ms
step:1749/2110 train_time:96472ms step_avg:55.16ms
step:1750/2110 train_time:96559ms step_avg:55.18ms
step:1750/2110 val_loss:3.3785 train_time:96649ms step_avg:55.23ms
step:1751/2110 train_time:96682ms step_avg:55.22ms
step:1752/2110 train_time:96744ms step_avg:55.22ms
step:1753/2110 train_time:96837ms step_avg:55.24ms
step:1754/2110 train_time:96927ms step_avg:55.26ms
step:1755/2110 train_time:97014ms step_avg:55.28ms
step:1756/2110 train_time:97100ms step_avg:55.30ms
step:1757/2110 train_time:97188ms step_avg:55.31ms
step:1758/2110 train_time:97274ms step_avg:55.33ms
step:1759/2110 train_time:97361ms step_avg:55.35ms
step:1760/2110 train_time:97449ms step_avg:55.37ms
step:1761/2110 train_time:97535ms step_avg:55.39ms
step:1762/2110 train_time:97623ms step_avg:55.40ms
step:1763/2110 train_time:97714ms step_avg:55.42ms
step:1764/2110 train_time:97805ms step_avg:55.44ms
step:1765/2110 train_time:97897ms step_avg:55.47ms
step:1766/2110 train_time:97985ms step_avg:55.48ms
step:1767/2110 train_time:98074ms step_avg:55.50ms
step:1768/2110 train_time:98160ms step_avg:55.52ms
step:1769/2110 train_time:98247ms step_avg:55.54ms
step:1770/2110 train_time:98334ms step_avg:55.56ms
step:1771/2110 train_time:98421ms step_avg:55.57ms
step:1772/2110 train_time:98508ms step_avg:55.59ms
step:1773/2110 train_time:98597ms step_avg:55.61ms
step:1774/2110 train_time:98686ms step_avg:55.63ms
step:1775/2110 train_time:98776ms step_avg:55.65ms
step:1776/2110 train_time:98865ms step_avg:55.67ms
step:1777/2110 train_time:98955ms step_avg:55.69ms
step:1778/2110 train_time:99042ms step_avg:55.70ms
step:1779/2110 train_time:99130ms step_avg:55.72ms
step:1780/2110 train_time:99217ms step_avg:55.74ms
step:1781/2110 train_time:99304ms step_avg:55.76ms
step:1782/2110 train_time:99392ms step_avg:55.78ms
step:1783/2110 train_time:99480ms step_avg:55.79ms
step:1784/2110 train_time:99568ms step_avg:55.81ms
step:1785/2110 train_time:99657ms step_avg:55.83ms
step:1786/2110 train_time:99745ms step_avg:55.85ms
step:1787/2110 train_time:99835ms step_avg:55.87ms
step:1788/2110 train_time:99923ms step_avg:55.89ms
step:1789/2110 train_time:100013ms step_avg:55.90ms
step:1790/2110 train_time:100100ms step_avg:55.92ms
step:1791/2110 train_time:100188ms step_avg:55.94ms
step:1792/2110 train_time:100275ms step_avg:55.96ms
step:1793/2110 train_time:100362ms step_avg:55.97ms
step:1794/2110 train_time:100450ms step_avg:55.99ms
step:1795/2110 train_time:100537ms step_avg:56.01ms
step:1796/2110 train_time:100625ms step_avg:56.03ms
step:1797/2110 train_time:100714ms step_avg:56.05ms
step:1798/2110 train_time:100802ms step_avg:56.06ms
step:1799/2110 train_time:100891ms step_avg:56.08ms
step:1800/2110 train_time:100979ms step_avg:56.10ms
step:1801/2110 train_time:101070ms step_avg:56.12ms
step:1802/2110 train_time:101157ms step_avg:56.14ms
step:1803/2110 train_time:101245ms step_avg:56.15ms
step:1804/2110 train_time:101332ms step_avg:56.17ms
step:1805/2110 train_time:101419ms step_avg:56.19ms
step:1806/2110 train_time:101505ms step_avg:56.20ms
step:1807/2110 train_time:101594ms step_avg:56.22ms
step:1808/2110 train_time:101681ms step_avg:56.24ms
step:1809/2110 train_time:101771ms step_avg:56.26ms
step:1810/2110 train_time:101859ms step_avg:56.28ms
step:1811/2110 train_time:101948ms step_avg:56.29ms
step:1812/2110 train_time:102036ms step_avg:56.31ms
step:1813/2110 train_time:102125ms step_avg:56.33ms
step:1814/2110 train_time:102213ms step_avg:56.35ms
step:1815/2110 train_time:102301ms step_avg:56.36ms
step:1816/2110 train_time:102390ms step_avg:56.38ms
step:1817/2110 train_time:102476ms step_avg:56.40ms
step:1818/2110 train_time:102563ms step_avg:56.42ms
step:1819/2110 train_time:102652ms step_avg:56.43ms
step:1820/2110 train_time:102739ms step_avg:56.45ms
step:1821/2110 train_time:102829ms step_avg:56.47ms
step:1822/2110 train_time:102918ms step_avg:56.49ms
step:1823/2110 train_time:103005ms step_avg:56.50ms
step:1824/2110 train_time:103094ms step_avg:56.52ms
step:1825/2110 train_time:103182ms step_avg:56.54ms
step:1826/2110 train_time:103269ms step_avg:56.55ms
step:1827/2110 train_time:103358ms step_avg:56.57ms
step:1828/2110 train_time:103445ms step_avg:56.59ms
step:1829/2110 train_time:103534ms step_avg:56.61ms
step:1830/2110 train_time:103621ms step_avg:56.62ms
step:1831/2110 train_time:103709ms step_avg:56.64ms
step:1832/2110 train_time:103797ms step_avg:56.66ms
step:1833/2110 train_time:103885ms step_avg:56.68ms
step:1834/2110 train_time:103975ms step_avg:56.69ms
step:1835/2110 train_time:104064ms step_avg:56.71ms
step:1836/2110 train_time:104151ms step_avg:56.73ms
step:1837/2110 train_time:104239ms step_avg:56.74ms
step:1838/2110 train_time:104326ms step_avg:56.76ms
step:1839/2110 train_time:104414ms step_avg:56.78ms
step:1840/2110 train_time:104501ms step_avg:56.79ms
step:1841/2110 train_time:104589ms step_avg:56.81ms
step:1842/2110 train_time:104678ms step_avg:56.83ms
step:1843/2110 train_time:104766ms step_avg:56.85ms
step:1844/2110 train_time:104855ms step_avg:56.86ms
step:1845/2110 train_time:104943ms step_avg:56.88ms
step:1846/2110 train_time:105031ms step_avg:56.90ms
step:1847/2110 train_time:105119ms step_avg:56.91ms
step:1848/2110 train_time:105207ms step_avg:56.93ms
step:1849/2110 train_time:105295ms step_avg:56.95ms
step:1850/2110 train_time:105383ms step_avg:56.96ms
step:1851/2110 train_time:105471ms step_avg:56.98ms
step:1852/2110 train_time:105558ms step_avg:57.00ms
step:1853/2110 train_time:105647ms step_avg:57.01ms
step:1854/2110 train_time:105736ms step_avg:57.03ms
step:1855/2110 train_time:105824ms step_avg:57.05ms
step:1856/2110 train_time:105912ms step_avg:57.06ms
step:1857/2110 train_time:106000ms step_avg:57.08ms
step:1858/2110 train_time:106088ms step_avg:57.10ms
step:1859/2110 train_time:106176ms step_avg:57.11ms
step:1860/2110 train_time:106263ms step_avg:57.13ms
step:1861/2110 train_time:106352ms step_avg:57.15ms
step:1862/2110 train_time:106439ms step_avg:57.16ms
step:1863/2110 train_time:106527ms step_avg:57.18ms
step:1864/2110 train_time:106616ms step_avg:57.20ms
step:1865/2110 train_time:106704ms step_avg:57.21ms
step:1866/2110 train_time:106792ms step_avg:57.23ms
step:1867/2110 train_time:106881ms step_avg:57.25ms
step:1868/2110 train_time:106969ms step_avg:57.26ms
step:1869/2110 train_time:107058ms step_avg:57.28ms
step:1870/2110 train_time:107145ms step_avg:57.30ms
step:1871/2110 train_time:107233ms step_avg:57.31ms
step:1872/2110 train_time:107321ms step_avg:57.33ms
step:1873/2110 train_time:107409ms step_avg:57.35ms
step:1874/2110 train_time:107497ms step_avg:57.36ms
step:1875/2110 train_time:107587ms step_avg:57.38ms
step:1876/2110 train_time:107675ms step_avg:57.40ms
step:1877/2110 train_time:107763ms step_avg:57.41ms
step:1878/2110 train_time:107851ms step_avg:57.43ms
step:1879/2110 train_time:107939ms step_avg:57.44ms
step:1880/2110 train_time:108026ms step_avg:57.46ms
step:1881/2110 train_time:108115ms step_avg:57.48ms
step:1882/2110 train_time:108202ms step_avg:57.49ms
step:1883/2110 train_time:108291ms step_avg:57.51ms
step:1884/2110 train_time:108378ms step_avg:57.53ms
step:1885/2110 train_time:108466ms step_avg:57.54ms
step:1886/2110 train_time:108555ms step_avg:57.56ms
step:1887/2110 train_time:108643ms step_avg:57.57ms
step:1888/2110 train_time:108730ms step_avg:57.59ms
step:1889/2110 train_time:108818ms step_avg:57.61ms
step:1890/2110 train_time:108906ms step_avg:57.62ms
step:1891/2110 train_time:108995ms step_avg:57.64ms
step:1892/2110 train_time:109082ms step_avg:57.65ms
step:1893/2110 train_time:109171ms step_avg:57.67ms
step:1894/2110 train_time:109259ms step_avg:57.69ms
step:1895/2110 train_time:109347ms step_avg:57.70ms
step:1896/2110 train_time:109436ms step_avg:57.72ms
step:1897/2110 train_time:109524ms step_avg:57.74ms
step:1898/2110 train_time:109612ms step_avg:57.75ms
step:1899/2110 train_time:109700ms step_avg:57.77ms
step:1900/2110 train_time:109788ms step_avg:57.78ms
step:1901/2110 train_time:109877ms step_avg:57.80ms
step:1902/2110 train_time:109965ms step_avg:57.82ms
step:1903/2110 train_time:110054ms step_avg:57.83ms
step:1904/2110 train_time:110142ms step_avg:57.85ms
step:1905/2110 train_time:110230ms step_avg:57.86ms
step:1906/2110 train_time:110317ms step_avg:57.88ms
step:1907/2110 train_time:110406ms step_avg:57.90ms
step:1908/2110 train_time:110496ms step_avg:57.91ms
step:1909/2110 train_time:110583ms step_avg:57.93ms
step:1910/2110 train_time:110671ms step_avg:57.94ms
step:1911/2110 train_time:110760ms step_avg:57.96ms
step:1912/2110 train_time:110848ms step_avg:57.97ms
step:1913/2110 train_time:110936ms step_avg:57.99ms
step:1914/2110 train_time:111023ms step_avg:58.01ms
step:1915/2110 train_time:111112ms step_avg:58.02ms
step:1916/2110 train_time:111199ms step_avg:58.04ms
step:1917/2110 train_time:111287ms step_avg:58.05ms
step:1918/2110 train_time:111375ms step_avg:58.07ms
step:1919/2110 train_time:111465ms step_avg:58.08ms
step:1920/2110 train_time:111551ms step_avg:58.10ms
step:1921/2110 train_time:111640ms step_avg:58.12ms
step:1922/2110 train_time:111728ms step_avg:58.13ms
step:1923/2110 train_time:111816ms step_avg:58.15ms
step:1924/2110 train_time:111903ms step_avg:58.16ms
step:1925/2110 train_time:111991ms step_avg:58.18ms
step:1926/2110 train_time:112079ms step_avg:58.19ms
step:1927/2110 train_time:112168ms step_avg:58.21ms
step:1928/2110 train_time:112256ms step_avg:58.22ms
step:1929/2110 train_time:112344ms step_avg:58.24ms
step:1930/2110 train_time:112431ms step_avg:58.25ms
step:1931/2110 train_time:112519ms step_avg:58.27ms
step:1932/2110 train_time:112608ms step_avg:58.29ms
step:1933/2110 train_time:112698ms step_avg:58.30ms
step:1934/2110 train_time:112786ms step_avg:58.32ms
step:1935/2110 train_time:112874ms step_avg:58.33ms
step:1936/2110 train_time:112962ms step_avg:58.35ms
step:1937/2110 train_time:113050ms step_avg:58.36ms
step:1938/2110 train_time:113138ms step_avg:58.38ms
step:1939/2110 train_time:113226ms step_avg:58.39ms
step:1940/2110 train_time:113314ms step_avg:58.41ms
step:1941/2110 train_time:113403ms step_avg:58.43ms
step:1942/2110 train_time:113490ms step_avg:58.44ms
step:1943/2110 train_time:113578ms step_avg:58.46ms
step:1944/2110 train_time:113667ms step_avg:58.47ms
step:1945/2110 train_time:113755ms step_avg:58.49ms
step:1946/2110 train_time:113842ms step_avg:58.50ms
step:1947/2110 train_time:113931ms step_avg:58.52ms
step:1948/2110 train_time:114020ms step_avg:58.53ms
step:1949/2110 train_time:114107ms step_avg:58.55ms
step:1950/2110 train_time:114195ms step_avg:58.56ms
step:1951/2110 train_time:114284ms step_avg:58.58ms
step:1952/2110 train_time:114372ms step_avg:58.59ms
step:1953/2110 train_time:114460ms step_avg:58.61ms
step:1954/2110 train_time:114548ms step_avg:58.62ms
step:1955/2110 train_time:114636ms step_avg:58.64ms
step:1956/2110 train_time:114724ms step_avg:58.65ms
step:1957/2110 train_time:114812ms step_avg:58.67ms
step:1958/2110 train_time:114899ms step_avg:58.68ms
step:1959/2110 train_time:114987ms step_avg:58.70ms
step:1960/2110 train_time:115076ms step_avg:58.71ms
step:1961/2110 train_time:115164ms step_avg:58.73ms
step:1962/2110 train_time:115251ms step_avg:58.74ms
step:1963/2110 train_time:115340ms step_avg:58.76ms
step:1964/2110 train_time:115429ms step_avg:58.77ms
step:1965/2110 train_time:115518ms step_avg:58.79ms
step:1966/2110 train_time:115605ms step_avg:58.80ms
step:1967/2110 train_time:115694ms step_avg:58.82ms
step:1968/2110 train_time:115781ms step_avg:58.83ms
step:1969/2110 train_time:115870ms step_avg:58.85ms
step:1970/2110 train_time:115958ms step_avg:58.86ms
step:1971/2110 train_time:116047ms step_avg:58.88ms
step:1972/2110 train_time:116136ms step_avg:58.89ms
step:1973/2110 train_time:116224ms step_avg:58.91ms
step:1974/2110 train_time:116311ms step_avg:58.92ms
step:1975/2110 train_time:116400ms step_avg:58.94ms
step:1976/2110 train_time:116487ms step_avg:58.95ms
step:1977/2110 train_time:116576ms step_avg:58.97ms
step:1978/2110 train_time:116664ms step_avg:58.98ms
step:1979/2110 train_time:116753ms step_avg:59.00ms
step:1980/2110 train_time:116840ms step_avg:59.01ms
step:1981/2110 train_time:116929ms step_avg:59.03ms
step:1982/2110 train_time:117017ms step_avg:59.04ms
step:1983/2110 train_time:117105ms step_avg:59.05ms
step:1984/2110 train_time:117193ms step_avg:59.07ms
step:1985/2110 train_time:117282ms step_avg:59.08ms
step:1986/2110 train_time:117369ms step_avg:59.10ms
step:1987/2110 train_time:117458ms step_avg:59.11ms
step:1988/2110 train_time:117545ms step_avg:59.13ms
step:1989/2110 train_time:117634ms step_avg:59.14ms
step:1990/2110 train_time:117721ms step_avg:59.16ms
step:1991/2110 train_time:117809ms step_avg:59.17ms
step:1992/2110 train_time:117900ms step_avg:59.19ms
step:1993/2110 train_time:117987ms step_avg:59.20ms
step:1994/2110 train_time:118075ms step_avg:59.21ms
step:1995/2110 train_time:118164ms step_avg:59.23ms
step:1996/2110 train_time:118252ms step_avg:59.24ms
step:1997/2110 train_time:118339ms step_avg:59.26ms
step:1998/2110 train_time:118427ms step_avg:59.27ms
step:1999/2110 train_time:118515ms step_avg:59.29ms
step:2000/2110 train_time:118601ms step_avg:59.30ms
step:2000/2110 val_loss:3.3032 train_time:118692ms step_avg:59.35ms
step:2001/2110 train_time:118722ms step_avg:59.33ms
step:2002/2110 train_time:118784ms step_avg:59.33ms
step:2003/2110 train_time:118879ms step_avg:59.35ms
step:2004/2110 train_time:118968ms step_avg:59.37ms
step:2005/2110 train_time:119056ms step_avg:59.38ms
step:2006/2110 train_time:119143ms step_avg:59.39ms
step:2007/2110 train_time:119230ms step_avg:59.41ms
step:2008/2110 train_time:119317ms step_avg:59.42ms
step:2009/2110 train_time:119403ms step_avg:59.43ms
step:2010/2110 train_time:119490ms step_avg:59.45ms
step:2011/2110 train_time:119578ms step_avg:59.46ms
step:2012/2110 train_time:119666ms step_avg:59.48ms
step:2013/2110 train_time:119758ms step_avg:59.49ms
step:2014/2110 train_time:119848ms step_avg:59.51ms
step:2015/2110 train_time:119938ms step_avg:59.52ms
step:2016/2110 train_time:120026ms step_avg:59.54ms
step:2017/2110 train_time:120114ms step_avg:59.55ms
step:2018/2110 train_time:120201ms step_avg:59.56ms
step:2019/2110 train_time:120288ms step_avg:59.58ms
step:2020/2110 train_time:120374ms step_avg:59.59ms
step:2021/2110 train_time:120462ms step_avg:59.61ms
step:2022/2110 train_time:120548ms step_avg:59.62ms
step:2023/2110 train_time:120637ms step_avg:59.63ms
step:2024/2110 train_time:120726ms step_avg:59.65ms
step:2025/2110 train_time:120816ms step_avg:59.66ms
step:2026/2110 train_time:120905ms step_avg:59.68ms
step:2027/2110 train_time:120993ms step_avg:59.69ms
step:2028/2110 train_time:121081ms step_avg:59.70ms
step:2029/2110 train_time:121169ms step_avg:59.72ms
step:2030/2110 train_time:121257ms step_avg:59.73ms
step:2031/2110 train_time:121345ms step_avg:59.75ms
step:2032/2110 train_time:121431ms step_avg:59.76ms
step:2033/2110 train_time:121519ms step_avg:59.77ms
step:2034/2110 train_time:121607ms step_avg:59.79ms
step:2035/2110 train_time:121697ms step_avg:59.80ms
step:2036/2110 train_time:121786ms step_avg:59.82ms
step:2037/2110 train_time:121876ms step_avg:59.83ms
step:2038/2110 train_time:121963ms step_avg:59.84ms
step:2039/2110 train_time:122053ms step_avg:59.86ms
step:2040/2110 train_time:122141ms step_avg:59.87ms
step:2041/2110 train_time:122229ms step_avg:59.89ms
step:2042/2110 train_time:122315ms step_avg:59.90ms
step:2043/2110 train_time:122403ms step_avg:59.91ms
step:2044/2110 train_time:122490ms step_avg:59.93ms
step:2045/2110 train_time:122578ms step_avg:59.94ms
step:2046/2110 train_time:122667ms step_avg:59.95ms
step:2047/2110 train_time:122755ms step_avg:59.97ms
step:2048/2110 train_time:122844ms step_avg:59.98ms
step:2049/2110 train_time:122931ms step_avg:60.00ms
step:2050/2110 train_time:123019ms step_avg:60.01ms
step:2051/2110 train_time:123108ms step_avg:60.02ms
step:2052/2110 train_time:123197ms step_avg:60.04ms
step:2053/2110 train_time:123285ms step_avg:60.05ms
step:2054/2110 train_time:123371ms step_avg:60.06ms
step:2055/2110 train_time:123459ms step_avg:60.08ms
step:2056/2110 train_time:123546ms step_avg:60.09ms
step:2057/2110 train_time:123634ms step_avg:60.10ms
step:2058/2110 train_time:123722ms step_avg:60.12ms
step:2059/2110 train_time:123811ms step_avg:60.13ms
step:2060/2110 train_time:123900ms step_avg:60.15ms
step:2061/2110 train_time:123989ms step_avg:60.16ms
step:2062/2110 train_time:124076ms step_avg:60.17ms
step:2063/2110 train_time:124165ms step_avg:60.19ms
step:2064/2110 train_time:124252ms step_avg:60.20ms
step:2065/2110 train_time:124340ms step_avg:60.21ms
step:2066/2110 train_time:124428ms step_avg:60.23ms
step:2067/2110 train_time:124516ms step_avg:60.24ms
step:2068/2110 train_time:124605ms step_avg:60.25ms
step:2069/2110 train_time:124693ms step_avg:60.27ms
step:2070/2110 train_time:124781ms step_avg:60.28ms
step:2071/2110 train_time:124870ms step_avg:60.29ms
step:2072/2110 train_time:124959ms step_avg:60.31ms
step:2073/2110 train_time:125047ms step_avg:60.32ms
step:2074/2110 train_time:125134ms step_avg:60.33ms
step:2075/2110 train_time:125224ms step_avg:60.35ms
step:2076/2110 train_time:125311ms step_avg:60.36ms
step:2077/2110 train_time:125401ms step_avg:60.38ms
step:2078/2110 train_time:125490ms step_avg:60.39ms
step:2079/2110 train_time:125579ms step_avg:60.40ms
step:2080/2110 train_time:125666ms step_avg:60.42ms
step:2081/2110 train_time:125756ms step_avg:60.43ms
step:2082/2110 train_time:125845ms step_avg:60.44ms
step:2083/2110 train_time:125933ms step_avg:60.46ms
step:2084/2110 train_time:126020ms step_avg:60.47ms
step:2085/2110 train_time:126110ms step_avg:60.48ms
step:2086/2110 train_time:126198ms step_avg:60.50ms
step:2087/2110 train_time:126286ms step_avg:60.51ms
step:2088/2110 train_time:126373ms step_avg:60.52ms
step:2089/2110 train_time:126462ms step_avg:60.54ms
step:2090/2110 train_time:126549ms step_avg:60.55ms
step:2091/2110 train_time:126638ms step_avg:60.56ms
step:2092/2110 train_time:126726ms step_avg:60.58ms
step:2093/2110 train_time:126816ms step_avg:60.59ms
step:2094/2110 train_time:126905ms step_avg:60.60ms
step:2095/2110 train_time:126993ms step_avg:60.62ms
step:2096/2110 train_time:127081ms step_avg:60.63ms
step:2097/2110 train_time:127170ms step_avg:60.64ms
step:2098/2110 train_time:127258ms step_avg:60.66ms
step:2099/2110 train_time:127346ms step_avg:60.67ms
step:2100/2110 train_time:127434ms step_avg:60.68ms
step:2101/2110 train_time:127523ms step_avg:60.70ms
step:2102/2110 train_time:127611ms step_avg:60.71ms
step:2103/2110 train_time:127700ms step_avg:60.72ms
step:2104/2110 train_time:127788ms step_avg:60.74ms
step:2105/2110 train_time:127878ms step_avg:60.75ms
step:2106/2110 train_time:127967ms step_avg:60.76ms
step:2107/2110 train_time:128055ms step_avg:60.78ms
step:2108/2110 train_time:128142ms step_avg:60.79ms
step:2109/2110 train_time:128231ms step_avg:60.80ms
step:2110/2110 train_time:128319ms step_avg:60.81ms
step:2110/2110 val_loss:3.2792 train_time:128409ms step_avg:60.86ms
peak memory allocated: 29892 MiB reserved: 39656 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int, mtp_weights: Tensor = None):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1880  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

# Precompute MTP weights for all steps to avoid tensor allocation during training
# Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
mtp_weights_schedule = []
for s in range(args.num_iterations + 1):
    x = s / args.num_scheduled_iterations
    if x < 1/3:
        w = [1.0, 0.5, 0.25 * (1 - 3*x)]
    elif x < 2/3:
        w = [1.0, 0.5 * (1 - (3*x - 1))]
    else:
        w = [1.0]
    mtp_weights_schedule.append(torch.tensor(w, device=device))

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps

    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0

    if in_transition:
        adam_optimizers[1].transition_steps -= 1

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

base_weights = torch.tensor([1.0, 0.5, 0.25], device=device)
for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    mtp_weights = base_weights[:max(1, 3 - idx)].clone()
    model.split_embed = (idx >= args.split_embed_frac * len(args.ws_schedule))
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        (model(inputs, targets, cum_seqlens, ws_long//2, ws_long, mtp_weights) / grad_accum_steps).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        model.split_embed = (ws_idx >= 1)
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state
model.split_embed = False


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations)
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True
    if step == split_step:
        model.split_embed = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps)
        is_transition = True

    step_batch_size = new_step_batch_size
    mtp_weights = mtp_weights_schedule[step]
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long, mtp_weights) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    # copy lm_head weights/state to embed before split
    if step == (split_step - 2) | 1:
        adam_optimizers[0].copy_lm_to_embed()

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 (main, Dec  9 2025, 19:02:36) [Clang 21.1.4 ]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 22 11:08:58 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            131W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   34C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   35C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   44C    P0            134W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   43C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   42C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   35C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    230144      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    230145      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    230146      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    230147      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    230148      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    230149      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    230150      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    230151      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    230145      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    230146      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    230147      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    230148      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    230149      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    230150      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    230151      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1920 val_loss:10.8406 train_time:0ms step_avg:0.04ms
step:1/1920 train_time:75ms step_avg:75.00ms
step:2/1920 train_time:96ms step_avg:48.11ms
step:3/1920 train_time:120ms step_avg:40.00ms
step:4/1920 train_time:154ms step_avg:38.52ms
step:5/1920 train_time:188ms step_avg:37.61ms
step:6/1920 train_time:268ms step_avg:44.67ms
step:7/1920 train_time:299ms step_avg:42.66ms
step:8/1920 train_time:333ms step_avg:41.59ms
step:9/1920 train_time:367ms step_avg:40.76ms
step:10/1920 train_time:401ms step_avg:40.08ms
step:11/1920 train_time:435ms step_avg:39.57ms
step:12/1920 train_time:469ms step_avg:39.12ms
step:13/1920 train_time:504ms step_avg:38.79ms
step:14/1920 train_time:538ms step_avg:38.46ms
step:15/1920 train_time:573ms step_avg:38.18ms
step:16/1920 train_time:607ms step_avg:37.92ms
step:17/1920 train_time:641ms step_avg:37.73ms
step:18/1920 train_time:676ms step_avg:37.54ms
step:19/1920 train_time:710ms step_avg:37.37ms
step:20/1920 train_time:744ms step_avg:37.20ms
step:21/1920 train_time:779ms step_avg:37.08ms
step:22/1920 train_time:813ms step_avg:36.95ms
step:23/1920 train_time:847ms step_avg:36.84ms
step:24/1920 train_time:882ms step_avg:36.73ms
step:25/1920 train_time:916ms step_avg:36.63ms
step:26/1920 train_time:950ms step_avg:36.53ms
step:27/1920 train_time:984ms step_avg:36.45ms
step:28/1920 train_time:1018ms step_avg:36.37ms
step:29/1920 train_time:1053ms step_avg:36.30ms
step:30/1920 train_time:1087ms step_avg:36.22ms
step:31/1920 train_time:1121ms step_avg:36.17ms
step:32/1920 train_time:1155ms step_avg:36.11ms
step:33/1920 train_time:1190ms step_avg:36.07ms
step:34/1920 train_time:1225ms step_avg:36.03ms
step:35/1920 train_time:1260ms step_avg:36.00ms
step:36/1920 train_time:1295ms step_avg:35.96ms
step:37/1920 train_time:1329ms step_avg:35.93ms
step:38/1920 train_time:1364ms step_avg:35.89ms
step:39/1920 train_time:1399ms step_avg:35.87ms
step:40/1920 train_time:1433ms step_avg:35.83ms
step:41/1920 train_time:1468ms step_avg:35.80ms
step:42/1920 train_time:1502ms step_avg:35.76ms
step:43/1920 train_time:1537ms step_avg:35.74ms
step:44/1920 train_time:1571ms step_avg:35.70ms
step:45/1920 train_time:1606ms step_avg:35.69ms
step:46/1920 train_time:1640ms step_avg:35.66ms
step:47/1920 train_time:1675ms step_avg:35.63ms
step:48/1920 train_time:1709ms step_avg:35.60ms
step:49/1920 train_time:1743ms step_avg:35.58ms
step:50/1920 train_time:1777ms step_avg:35.55ms
step:51/1920 train_time:1812ms step_avg:35.52ms
step:52/1920 train_time:1846ms step_avg:35.50ms
step:53/1920 train_time:1880ms step_avg:35.48ms
step:54/1920 train_time:1915ms step_avg:35.46ms
step:55/1920 train_time:1949ms step_avg:35.44ms
step:56/1920 train_time:1983ms step_avg:35.42ms
step:57/1920 train_time:2018ms step_avg:35.40ms
step:58/1920 train_time:2052ms step_avg:35.38ms
step:59/1920 train_time:2087ms step_avg:35.36ms
step:60/1920 train_time:2121ms step_avg:35.35ms
step:61/1920 train_time:2155ms step_avg:35.34ms
step:62/1920 train_time:2190ms step_avg:35.32ms
step:63/1920 train_time:2224ms step_avg:35.31ms
step:64/1920 train_time:2259ms step_avg:35.29ms
step:65/1920 train_time:2293ms step_avg:35.28ms
step:66/1920 train_time:2328ms step_avg:35.27ms
step:67/1920 train_time:2363ms step_avg:35.27ms
step:68/1920 train_time:2397ms step_avg:35.26ms
step:69/1920 train_time:2432ms step_avg:35.24ms
step:70/1920 train_time:2466ms step_avg:35.23ms
step:71/1920 train_time:2501ms step_avg:35.22ms
step:72/1920 train_time:2535ms step_avg:35.21ms
step:73/1920 train_time:2570ms step_avg:35.20ms
step:74/1920 train_time:2604ms step_avg:35.19ms
step:75/1920 train_time:2638ms step_avg:35.18ms
step:76/1920 train_time:2673ms step_avg:35.17ms
step:77/1920 train_time:2707ms step_avg:35.16ms
step:78/1920 train_time:2741ms step_avg:35.15ms
step:79/1920 train_time:2776ms step_avg:35.14ms
step:80/1920 train_time:2810ms step_avg:35.12ms
step:81/1920 train_time:2844ms step_avg:35.11ms
step:82/1920 train_time:2879ms step_avg:35.10ms
step:83/1920 train_time:2913ms step_avg:35.09ms
step:84/1920 train_time:2947ms step_avg:35.08ms
step:85/1920 train_time:2982ms step_avg:35.08ms
step:86/1920 train_time:3016ms step_avg:35.07ms
step:87/1920 train_time:3050ms step_avg:35.06ms
step:88/1920 train_time:3084ms step_avg:35.05ms
step:89/1920 train_time:3118ms step_avg:35.04ms
step:90/1920 train_time:3153ms step_avg:35.03ms
step:91/1920 train_time:3187ms step_avg:35.02ms
step:92/1920 train_time:3221ms step_avg:35.01ms
step:93/1920 train_time:3256ms step_avg:35.01ms
step:94/1920 train_time:3290ms step_avg:35.00ms
step:95/1920 train_time:3325ms step_avg:35.00ms
step:96/1920 train_time:3359ms step_avg:34.99ms
step:97/1920 train_time:3394ms step_avg:34.99ms
step:98/1920 train_time:3428ms step_avg:34.98ms
step:99/1920 train_time:3463ms step_avg:34.98ms
step:100/1920 train_time:3497ms step_avg:34.97ms
step:101/1920 train_time:3531ms step_avg:34.97ms
step:102/1920 train_time:3566ms step_avg:34.96ms
step:103/1920 train_time:3601ms step_avg:34.96ms
step:104/1920 train_time:3636ms step_avg:34.96ms
step:105/1920 train_time:3670ms step_avg:34.96ms
step:106/1920 train_time:3704ms step_avg:34.95ms
step:107/1920 train_time:3739ms step_avg:34.94ms
step:108/1920 train_time:3773ms step_avg:34.94ms
step:109/1920 train_time:3808ms step_avg:34.94ms
step:110/1920 train_time:3842ms step_avg:34.93ms
step:111/1920 train_time:3877ms step_avg:34.93ms
step:112/1920 train_time:3911ms step_avg:34.92ms
step:113/1920 train_time:3946ms step_avg:34.92ms
step:114/1920 train_time:3980ms step_avg:34.91ms
step:115/1920 train_time:4014ms step_avg:34.90ms
step:116/1920 train_time:4048ms step_avg:34.90ms
step:117/1920 train_time:4083ms step_avg:34.89ms
step:118/1920 train_time:4117ms step_avg:34.89ms
step:119/1920 train_time:4151ms step_avg:34.88ms
step:120/1920 train_time:4185ms step_avg:34.88ms
step:121/1920 train_time:4220ms step_avg:34.88ms
step:122/1920 train_time:4254ms step_avg:34.87ms
step:123/1920 train_time:4289ms step_avg:34.87ms
step:124/1920 train_time:4323ms step_avg:34.86ms
step:125/1920 train_time:4358ms step_avg:34.86ms
step:126/1920 train_time:4392ms step_avg:34.85ms
step:127/1920 train_time:4426ms step_avg:34.85ms
step:128/1920 train_time:4460ms step_avg:34.85ms
step:129/1920 train_time:4495ms step_avg:34.85ms
step:130/1920 train_time:4529ms step_avg:34.84ms
step:131/1920 train_time:4564ms step_avg:34.84ms
step:132/1920 train_time:4599ms step_avg:34.84ms
step:133/1920 train_time:4633ms step_avg:34.83ms
step:134/1920 train_time:4667ms step_avg:34.83ms
step:135/1920 train_time:4702ms step_avg:34.83ms
step:136/1920 train_time:4736ms step_avg:34.83ms
step:137/1920 train_time:4771ms step_avg:34.82ms
step:138/1920 train_time:4805ms step_avg:34.82ms
step:139/1920 train_time:4840ms step_avg:34.82ms
step:140/1920 train_time:4874ms step_avg:34.82ms
step:141/1920 train_time:4908ms step_avg:34.81ms
step:142/1920 train_time:4943ms step_avg:34.81ms
step:143/1920 train_time:4977ms step_avg:34.80ms
step:144/1920 train_time:5011ms step_avg:34.80ms
step:145/1920 train_time:5045ms step_avg:34.80ms
step:146/1920 train_time:5080ms step_avg:34.79ms
step:147/1920 train_time:5114ms step_avg:34.79ms
step:148/1920 train_time:5148ms step_avg:34.78ms
step:149/1920 train_time:5183ms step_avg:34.78ms
step:150/1920 train_time:5217ms step_avg:34.78ms
step:151/1920 train_time:5252ms step_avg:34.78ms
step:152/1920 train_time:5286ms step_avg:34.77ms
step:153/1920 train_time:5320ms step_avg:34.77ms
step:154/1920 train_time:5354ms step_avg:34.77ms
step:155/1920 train_time:5389ms step_avg:34.77ms
step:156/1920 train_time:5423ms step_avg:34.76ms
step:157/1920 train_time:5458ms step_avg:34.76ms
step:158/1920 train_time:5492ms step_avg:34.76ms
step:159/1920 train_time:5527ms step_avg:34.76ms
step:160/1920 train_time:5561ms step_avg:34.76ms
step:161/1920 train_time:5596ms step_avg:34.76ms
step:162/1920 train_time:5630ms step_avg:34.75ms
step:163/1920 train_time:5664ms step_avg:34.75ms
step:164/1920 train_time:5699ms step_avg:34.75ms
step:165/1920 train_time:5733ms step_avg:34.75ms
step:166/1920 train_time:5767ms step_avg:34.74ms
step:167/1920 train_time:5802ms step_avg:34.74ms
step:168/1920 train_time:5836ms step_avg:34.74ms
step:169/1920 train_time:5871ms step_avg:34.74ms
step:170/1920 train_time:5905ms step_avg:34.73ms
step:171/1920 train_time:5939ms step_avg:34.73ms
step:172/1920 train_time:5974ms step_avg:34.73ms
step:173/1920 train_time:6008ms step_avg:34.73ms
step:174/1920 train_time:6043ms step_avg:34.73ms
step:175/1920 train_time:6077ms step_avg:34.73ms
step:176/1920 train_time:6112ms step_avg:34.72ms
step:177/1920 train_time:6146ms step_avg:34.72ms
step:178/1920 train_time:6180ms step_avg:34.72ms
step:179/1920 train_time:6215ms step_avg:34.72ms
step:180/1920 train_time:6249ms step_avg:34.72ms
step:181/1920 train_time:6284ms step_avg:34.72ms
step:182/1920 train_time:6318ms step_avg:34.71ms
step:183/1920 train_time:6352ms step_avg:34.71ms
step:184/1920 train_time:6386ms step_avg:34.71ms
step:185/1920 train_time:6421ms step_avg:34.71ms
step:186/1920 train_time:6455ms step_avg:34.70ms
step:187/1920 train_time:6490ms step_avg:34.71ms
step:188/1920 train_time:6524ms step_avg:34.70ms
step:189/1920 train_time:6558ms step_avg:34.70ms
step:190/1920 train_time:6593ms step_avg:34.70ms
step:191/1920 train_time:6627ms step_avg:34.70ms
step:192/1920 train_time:6662ms step_avg:34.70ms
step:193/1920 train_time:6696ms step_avg:34.70ms
step:194/1920 train_time:6731ms step_avg:34.69ms
step:195/1920 train_time:6766ms step_avg:34.69ms
step:196/1920 train_time:6800ms step_avg:34.69ms
step:197/1920 train_time:6834ms step_avg:34.69ms
step:198/1920 train_time:6868ms step_avg:34.69ms
step:199/1920 train_time:6903ms step_avg:34.69ms
step:200/1920 train_time:6938ms step_avg:34.69ms
step:201/1920 train_time:6972ms step_avg:34.69ms
step:202/1920 train_time:7006ms step_avg:34.68ms
step:203/1920 train_time:7041ms step_avg:34.68ms
step:204/1920 train_time:7075ms step_avg:34.68ms
step:205/1920 train_time:7110ms step_avg:34.68ms
step:206/1920 train_time:7144ms step_avg:34.68ms
step:207/1920 train_time:7178ms step_avg:34.68ms
step:208/1920 train_time:7212ms step_avg:34.68ms
step:209/1920 train_time:7247ms step_avg:34.67ms
step:210/1920 train_time:7281ms step_avg:34.67ms
step:211/1920 train_time:7315ms step_avg:34.67ms
step:212/1920 train_time:7349ms step_avg:34.67ms
step:213/1920 train_time:7384ms step_avg:34.67ms
step:214/1920 train_time:7418ms step_avg:34.66ms
step:215/1920 train_time:7452ms step_avg:34.66ms
step:216/1920 train_time:7486ms step_avg:34.66ms
step:217/1920 train_time:7521ms step_avg:34.66ms
step:218/1920 train_time:7555ms step_avg:34.66ms
step:219/1920 train_time:7590ms step_avg:34.66ms
step:220/1920 train_time:7624ms step_avg:34.65ms
step:221/1920 train_time:7658ms step_avg:34.65ms
step:222/1920 train_time:7692ms step_avg:34.65ms
step:223/1920 train_time:7727ms step_avg:34.65ms
step:224/1920 train_time:7762ms step_avg:34.65ms
step:225/1920 train_time:7796ms step_avg:34.65ms
step:226/1920 train_time:7830ms step_avg:34.65ms
step:227/1920 train_time:7865ms step_avg:34.65ms
step:228/1920 train_time:7899ms step_avg:34.65ms
step:229/1920 train_time:7934ms step_avg:34.64ms
step:230/1920 train_time:7968ms step_avg:34.64ms
step:231/1920 train_time:8002ms step_avg:34.64ms
step:232/1920 train_time:8037ms step_avg:34.64ms
step:233/1920 train_time:8071ms step_avg:34.64ms
step:234/1920 train_time:8105ms step_avg:34.64ms
step:235/1920 train_time:8140ms step_avg:34.64ms
step:236/1920 train_time:8174ms step_avg:34.63ms
step:237/1920 train_time:8208ms step_avg:34.63ms
step:238/1920 train_time:8242ms step_avg:34.63ms
step:239/1920 train_time:8277ms step_avg:34.63ms
step:240/1920 train_time:8311ms step_avg:34.63ms
step:241/1920 train_time:8345ms step_avg:34.63ms
step:242/1920 train_time:8380ms step_avg:34.63ms
step:243/1920 train_time:8414ms step_avg:34.62ms
step:244/1920 train_time:8448ms step_avg:34.62ms
step:245/1920 train_time:8483ms step_avg:34.62ms
step:246/1920 train_time:8517ms step_avg:34.62ms
step:247/1920 train_time:8551ms step_avg:34.62ms
step:248/1920 train_time:8586ms step_avg:34.62ms
step:249/1920 train_time:8621ms step_avg:34.62ms
step:250/1920 train_time:8655ms step_avg:34.62ms
step:250/1920 val_loss:4.6107 train_time:8692ms step_avg:34.77ms
step:251/1920 train_time:8711ms step_avg:34.70ms
step:252/1920 train_time:8728ms step_avg:34.63ms
step:253/1920 train_time:8760ms step_avg:34.62ms
step:254/1920 train_time:8795ms step_avg:34.63ms
step:255/1920 train_time:8830ms step_avg:34.63ms
step:256/1920 train_time:8864ms step_avg:34.63ms
step:257/1920 train_time:8899ms step_avg:34.63ms
step:258/1920 train_time:8933ms step_avg:34.62ms
step:259/1920 train_time:8967ms step_avg:34.62ms
step:260/1920 train_time:9002ms step_avg:34.62ms
step:261/1920 train_time:9036ms step_avg:34.62ms
step:262/1920 train_time:9070ms step_avg:34.62ms
step:263/1920 train_time:9104ms step_avg:34.62ms
step:264/1920 train_time:9138ms step_avg:34.61ms
step:265/1920 train_time:9172ms step_avg:34.61ms
step:266/1920 train_time:9206ms step_avg:34.61ms
step:267/1920 train_time:9240ms step_avg:34.61ms
step:268/1920 train_time:9274ms step_avg:34.61ms
step:269/1920 train_time:9309ms step_avg:34.61ms
step:270/1920 train_time:9343ms step_avg:34.60ms
step:271/1920 train_time:9377ms step_avg:34.60ms
step:272/1920 train_time:9411ms step_avg:34.60ms
step:273/1920 train_time:9445ms step_avg:34.60ms
step:274/1920 train_time:9479ms step_avg:34.60ms
step:275/1920 train_time:9513ms step_avg:34.59ms
step:276/1920 train_time:9547ms step_avg:34.59ms
step:277/1920 train_time:9582ms step_avg:34.59ms
step:278/1920 train_time:9616ms step_avg:34.59ms
step:279/1920 train_time:9650ms step_avg:34.59ms
step:280/1920 train_time:9685ms step_avg:34.59ms
step:281/1920 train_time:9719ms step_avg:34.59ms
step:282/1920 train_time:9754ms step_avg:34.59ms
step:283/1920 train_time:9788ms step_avg:34.59ms
step:284/1920 train_time:9823ms step_avg:34.59ms
step:285/1920 train_time:9858ms step_avg:34.59ms
step:286/1920 train_time:9892ms step_avg:34.59ms
step:287/1920 train_time:9926ms step_avg:34.59ms
step:288/1920 train_time:9960ms step_avg:34.58ms
step:289/1920 train_time:9995ms step_avg:34.58ms
step:290/1920 train_time:10029ms step_avg:34.58ms
step:291/1920 train_time:10064ms step_avg:34.58ms
step:292/1920 train_time:10098ms step_avg:34.58ms
step:293/1920 train_time:10132ms step_avg:34.58ms
step:294/1920 train_time:10166ms step_avg:34.58ms
step:295/1920 train_time:10201ms step_avg:34.58ms
step:296/1920 train_time:10235ms step_avg:34.58ms
step:297/1920 train_time:10269ms step_avg:34.58ms
step:298/1920 train_time:10303ms step_avg:34.57ms
step:299/1920 train_time:10337ms step_avg:34.57ms
step:300/1920 train_time:10371ms step_avg:34.57ms
step:301/1920 train_time:10406ms step_avg:34.57ms
step:302/1920 train_time:10440ms step_avg:34.57ms
step:303/1920 train_time:10474ms step_avg:34.57ms
step:304/1920 train_time:10508ms step_avg:34.57ms
step:305/1920 train_time:10542ms step_avg:34.57ms
step:306/1920 train_time:10577ms step_avg:34.56ms
step:307/1920 train_time:10611ms step_avg:34.56ms
step:308/1920 train_time:10645ms step_avg:34.56ms
step:309/1920 train_time:10679ms step_avg:34.56ms
step:310/1920 train_time:10713ms step_avg:34.56ms
step:311/1920 train_time:10748ms step_avg:34.56ms
step:312/1920 train_time:10782ms step_avg:34.56ms
step:313/1920 train_time:10817ms step_avg:34.56ms
step:314/1920 train_time:10851ms step_avg:34.56ms
step:315/1920 train_time:10885ms step_avg:34.56ms
step:316/1920 train_time:10920ms step_avg:34.56ms
step:317/1920 train_time:10954ms step_avg:34.55ms
step:318/1920 train_time:10988ms step_avg:34.55ms
step:319/1920 train_time:11023ms step_avg:34.55ms
step:320/1920 train_time:11057ms step_avg:34.55ms
step:321/1920 train_time:11091ms step_avg:34.55ms
step:322/1920 train_time:11125ms step_avg:34.55ms
step:323/1920 train_time:11160ms step_avg:34.55ms
step:324/1920 train_time:11194ms step_avg:34.55ms
step:325/1920 train_time:11228ms step_avg:34.55ms
step:326/1920 train_time:11262ms step_avg:34.55ms
step:327/1920 train_time:11296ms step_avg:34.55ms
step:328/1920 train_time:11331ms step_avg:34.54ms
step:329/1920 train_time:11365ms step_avg:34.54ms
step:330/1920 train_time:11399ms step_avg:34.54ms
step:331/1920 train_time:11433ms step_avg:34.54ms
step:332/1920 train_time:11467ms step_avg:34.54ms
step:333/1920 train_time:11502ms step_avg:34.54ms
step:334/1920 train_time:11536ms step_avg:34.54ms
step:335/1920 train_time:11570ms step_avg:34.54ms
step:336/1920 train_time:11604ms step_avg:34.54ms
step:337/1920 train_time:11639ms step_avg:34.54ms
step:338/1920 train_time:11673ms step_avg:34.54ms
step:339/1920 train_time:11708ms step_avg:34.54ms
step:340/1920 train_time:11742ms step_avg:34.53ms
step:341/1920 train_time:11776ms step_avg:34.53ms
step:342/1920 train_time:11810ms step_avg:34.53ms
step:343/1920 train_time:11845ms step_avg:34.53ms
step:344/1920 train_time:11879ms step_avg:34.53ms
step:345/1920 train_time:11914ms step_avg:34.53ms
step:346/1920 train_time:11948ms step_avg:34.53ms
step:347/1920 train_time:11983ms step_avg:34.53ms
step:348/1920 train_time:12017ms step_avg:34.53ms
step:349/1920 train_time:12052ms step_avg:34.53ms
step:350/1920 train_time:12086ms step_avg:34.53ms
step:351/1920 train_time:12121ms step_avg:34.53ms
step:352/1920 train_time:12155ms step_avg:34.53ms
step:353/1920 train_time:12190ms step_avg:34.53ms
step:354/1920 train_time:12224ms step_avg:34.53ms
step:355/1920 train_time:12258ms step_avg:34.53ms
step:356/1920 train_time:12292ms step_avg:34.53ms
step:357/1920 train_time:12327ms step_avg:34.53ms
step:358/1920 train_time:12361ms step_avg:34.53ms
step:359/1920 train_time:12395ms step_avg:34.53ms
step:360/1920 train_time:12429ms step_avg:34.53ms
step:361/1920 train_time:12464ms step_avg:34.53ms
step:362/1920 train_time:12498ms step_avg:34.53ms
step:363/1920 train_time:12532ms step_avg:34.52ms
step:364/1920 train_time:12566ms step_avg:34.52ms
step:365/1920 train_time:12600ms step_avg:34.52ms
step:366/1920 train_time:12635ms step_avg:34.52ms
step:367/1920 train_time:12669ms step_avg:34.52ms
step:368/1920 train_time:12703ms step_avg:34.52ms
step:369/1920 train_time:12737ms step_avg:34.52ms
step:370/1920 train_time:12771ms step_avg:34.52ms
step:371/1920 train_time:12806ms step_avg:34.52ms
step:372/1920 train_time:12840ms step_avg:34.52ms
step:373/1920 train_time:12874ms step_avg:34.52ms
step:374/1920 train_time:12908ms step_avg:34.51ms
step:375/1920 train_time:12943ms step_avg:34.51ms
step:376/1920 train_time:12977ms step_avg:34.51ms
step:377/1920 train_time:13011ms step_avg:34.51ms
step:378/1920 train_time:13046ms step_avg:34.51ms
step:379/1920 train_time:13080ms step_avg:34.51ms
step:380/1920 train_time:13114ms step_avg:34.51ms
step:381/1920 train_time:13149ms step_avg:34.51ms
step:382/1920 train_time:13183ms step_avg:34.51ms
step:383/1920 train_time:13217ms step_avg:34.51ms
step:384/1920 train_time:13251ms step_avg:34.51ms
step:385/1920 train_time:13285ms step_avg:34.51ms
step:386/1920 train_time:13319ms step_avg:34.51ms
step:387/1920 train_time:13353ms step_avg:34.50ms
step:388/1920 train_time:13387ms step_avg:34.50ms
step:389/1920 train_time:13422ms step_avg:34.50ms
step:390/1920 train_time:13456ms step_avg:34.50ms
step:391/1920 train_time:13490ms step_avg:34.50ms
step:392/1920 train_time:13525ms step_avg:34.50ms
step:393/1920 train_time:13559ms step_avg:34.50ms
step:394/1920 train_time:13593ms step_avg:34.50ms
step:395/1920 train_time:13628ms step_avg:34.50ms
step:396/1920 train_time:13662ms step_avg:34.50ms
step:397/1920 train_time:13696ms step_avg:34.50ms
step:398/1920 train_time:13730ms step_avg:34.50ms
step:399/1920 train_time:13765ms step_avg:34.50ms
step:400/1920 train_time:13799ms step_avg:34.50ms
step:401/1920 train_time:13833ms step_avg:34.50ms
step:402/1920 train_time:13867ms step_avg:34.50ms
step:403/1920 train_time:13902ms step_avg:34.50ms
step:404/1920 train_time:13936ms step_avg:34.49ms
step:405/1920 train_time:13970ms step_avg:34.50ms
step:406/1920 train_time:14005ms step_avg:34.49ms
step:407/1920 train_time:14039ms step_avg:34.49ms
step:408/1920 train_time:14073ms step_avg:34.49ms
step:409/1920 train_time:14108ms step_avg:34.49ms
step:410/1920 train_time:14142ms step_avg:34.49ms
step:411/1920 train_time:14176ms step_avg:34.49ms
step:412/1920 train_time:14210ms step_avg:34.49ms
step:413/1920 train_time:14245ms step_avg:34.49ms
step:414/1920 train_time:14279ms step_avg:34.49ms
step:415/1920 train_time:14313ms step_avg:34.49ms
step:416/1920 train_time:14348ms step_avg:34.49ms
step:417/1920 train_time:14382ms step_avg:34.49ms
step:418/1920 train_time:14417ms step_avg:34.49ms
step:419/1920 train_time:14451ms step_avg:34.49ms
step:420/1920 train_time:14485ms step_avg:34.49ms
step:421/1920 train_time:14520ms step_avg:34.49ms
step:422/1920 train_time:14554ms step_avg:34.49ms
step:423/1920 train_time:14589ms step_avg:34.49ms
step:424/1920 train_time:14623ms step_avg:34.49ms
step:425/1920 train_time:14658ms step_avg:34.49ms
step:426/1920 train_time:14692ms step_avg:34.49ms
step:427/1920 train_time:14726ms step_avg:34.49ms
step:428/1920 train_time:14760ms step_avg:34.49ms
step:429/1920 train_time:14794ms step_avg:34.49ms
step:430/1920 train_time:14828ms step_avg:34.48ms
step:431/1920 train_time:14863ms step_avg:34.48ms
step:432/1920 train_time:14897ms step_avg:34.48ms
step:433/1920 train_time:14931ms step_avg:34.48ms
step:434/1920 train_time:14965ms step_avg:34.48ms
step:435/1920 train_time:15000ms step_avg:34.48ms
step:436/1920 train_time:15034ms step_avg:34.48ms
step:437/1920 train_time:15069ms step_avg:34.48ms
step:438/1920 train_time:15103ms step_avg:34.48ms
step:439/1920 train_time:15137ms step_avg:34.48ms
step:440/1920 train_time:15171ms step_avg:34.48ms
step:441/1920 train_time:15206ms step_avg:34.48ms
step:442/1920 train_time:15240ms step_avg:34.48ms
step:443/1920 train_time:15274ms step_avg:34.48ms
step:444/1920 train_time:15308ms step_avg:34.48ms
step:445/1920 train_time:15342ms step_avg:34.48ms
step:446/1920 train_time:15376ms step_avg:34.48ms
step:447/1920 train_time:15411ms step_avg:34.48ms
step:448/1920 train_time:15445ms step_avg:34.48ms
step:449/1920 train_time:15479ms step_avg:34.48ms
step:450/1920 train_time:15513ms step_avg:34.47ms
step:451/1920 train_time:15548ms step_avg:34.47ms
step:452/1920 train_time:15582ms step_avg:34.47ms
step:453/1920 train_time:15617ms step_avg:34.47ms
step:454/1920 train_time:15650ms step_avg:34.47ms
step:455/1920 train_time:15685ms step_avg:34.47ms
step:456/1920 train_time:15719ms step_avg:34.47ms
step:457/1920 train_time:15754ms step_avg:34.47ms
step:458/1920 train_time:15788ms step_avg:34.47ms
step:459/1920 train_time:15822ms step_avg:34.47ms
step:460/1920 train_time:15856ms step_avg:34.47ms
step:461/1920 train_time:15891ms step_avg:34.47ms
step:462/1920 train_time:15926ms step_avg:34.47ms
step:463/1920 train_time:15961ms step_avg:34.47ms
step:464/1920 train_time:15995ms step_avg:34.47ms
step:465/1920 train_time:16029ms step_avg:34.47ms
step:466/1920 train_time:16063ms step_avg:34.47ms
step:467/1920 train_time:16097ms step_avg:34.47ms
step:468/1920 train_time:16132ms step_avg:34.47ms
step:469/1920 train_time:16166ms step_avg:34.47ms
step:470/1920 train_time:16200ms step_avg:34.47ms
step:471/1920 train_time:16234ms step_avg:34.47ms
step:472/1920 train_time:16268ms step_avg:34.47ms
step:473/1920 train_time:16303ms step_avg:34.47ms
step:474/1920 train_time:16337ms step_avg:34.47ms
step:475/1920 train_time:16371ms step_avg:34.47ms
step:476/1920 train_time:16406ms step_avg:34.47ms
step:477/1920 train_time:16440ms step_avg:34.47ms
step:478/1920 train_time:16474ms step_avg:34.47ms
step:479/1920 train_time:16509ms step_avg:34.47ms
step:480/1920 train_time:16543ms step_avg:34.46ms
step:481/1920 train_time:16577ms step_avg:34.46ms
step:482/1920 train_time:16611ms step_avg:34.46ms
step:483/1920 train_time:16646ms step_avg:34.46ms
step:484/1920 train_time:16680ms step_avg:34.46ms
step:485/1920 train_time:16714ms step_avg:34.46ms
step:486/1920 train_time:16748ms step_avg:34.46ms
step:487/1920 train_time:16783ms step_avg:34.46ms
step:488/1920 train_time:16817ms step_avg:34.46ms
step:489/1920 train_time:16852ms step_avg:34.46ms
step:490/1920 train_time:16886ms step_avg:34.46ms
step:491/1920 train_time:16920ms step_avg:34.46ms
step:492/1920 train_time:16955ms step_avg:34.46ms
step:493/1920 train_time:16989ms step_avg:34.46ms
step:494/1920 train_time:17023ms step_avg:34.46ms
step:495/1920 train_time:17058ms step_avg:34.46ms
step:496/1920 train_time:17092ms step_avg:34.46ms
step:497/1920 train_time:17126ms step_avg:34.46ms
step:498/1920 train_time:17160ms step_avg:34.46ms
step:499/1920 train_time:17195ms step_avg:34.46ms
step:500/1920 train_time:17229ms step_avg:34.46ms
step:500/1920 val_loss:4.2925 train_time:17266ms step_avg:34.53ms
step:501/1920 train_time:17283ms step_avg:34.50ms
step:502/1920 train_time:17301ms step_avg:34.46ms
step:503/1920 train_time:17336ms step_avg:34.47ms
step:504/1920 train_time:17371ms step_avg:34.47ms
step:505/1920 train_time:17407ms step_avg:34.47ms
step:506/1920 train_time:17442ms step_avg:34.47ms
step:507/1920 train_time:17477ms step_avg:34.47ms
step:508/1920 train_time:17511ms step_avg:34.47ms
step:509/1920 train_time:17546ms step_avg:34.47ms
step:510/1920 train_time:17580ms step_avg:34.47ms
step:511/1920 train_time:17615ms step_avg:34.47ms
step:512/1920 train_time:17649ms step_avg:34.47ms
step:513/1920 train_time:17684ms step_avg:34.47ms
step:514/1920 train_time:17718ms step_avg:34.47ms
step:515/1920 train_time:17752ms step_avg:34.47ms
step:516/1920 train_time:17786ms step_avg:34.47ms
step:517/1920 train_time:17821ms step_avg:34.47ms
step:518/1920 train_time:17855ms step_avg:34.47ms
step:519/1920 train_time:17889ms step_avg:34.47ms
step:520/1920 train_time:17923ms step_avg:34.47ms
step:521/1920 train_time:17958ms step_avg:34.47ms
step:522/1920 train_time:17992ms step_avg:34.47ms
step:523/1920 train_time:18026ms step_avg:34.47ms
step:524/1920 train_time:18060ms step_avg:34.47ms
step:525/1920 train_time:18094ms step_avg:34.46ms
step:526/1920 train_time:18128ms step_avg:34.46ms
step:527/1920 train_time:18162ms step_avg:34.46ms
step:528/1920 train_time:18196ms step_avg:34.46ms
step:529/1920 train_time:18232ms step_avg:34.46ms
step:530/1920 train_time:18266ms step_avg:34.46ms
step:531/1920 train_time:18300ms step_avg:34.46ms
step:532/1920 train_time:18334ms step_avg:34.46ms
step:533/1920 train_time:18369ms step_avg:34.46ms
step:534/1920 train_time:18404ms step_avg:34.46ms
step:535/1920 train_time:18438ms step_avg:34.46ms
step:536/1920 train_time:18473ms step_avg:34.46ms
step:537/1920 train_time:18508ms step_avg:34.46ms
step:538/1920 train_time:18542ms step_avg:34.46ms
step:539/1920 train_time:18576ms step_avg:34.46ms
step:540/1920 train_time:18611ms step_avg:34.46ms
step:541/1920 train_time:18646ms step_avg:34.47ms
step:542/1920 train_time:18680ms step_avg:34.46ms
step:543/1920 train_time:18714ms step_avg:34.46ms
step:544/1920 train_time:18749ms step_avg:34.46ms
step:545/1920 train_time:18783ms step_avg:34.46ms
step:546/1920 train_time:18817ms step_avg:34.46ms
step:547/1920 train_time:18852ms step_avg:34.46ms
step:548/1920 train_time:18886ms step_avg:34.46ms
step:549/1920 train_time:18921ms step_avg:34.46ms
step:550/1920 train_time:18955ms step_avg:34.46ms
step:551/1920 train_time:18990ms step_avg:34.46ms
step:552/1920 train_time:19024ms step_avg:34.46ms
step:553/1920 train_time:19058ms step_avg:34.46ms
step:554/1920 train_time:19092ms step_avg:34.46ms
step:555/1920 train_time:19126ms step_avg:34.46ms
step:556/1920 train_time:19161ms step_avg:34.46ms
step:557/1920 train_time:19195ms step_avg:34.46ms
step:558/1920 train_time:19229ms step_avg:34.46ms
step:559/1920 train_time:19264ms step_avg:34.46ms
step:560/1920 train_time:19298ms step_avg:34.46ms
step:561/1920 train_time:19333ms step_avg:34.46ms
step:562/1920 train_time:19367ms step_avg:34.46ms
step:563/1920 train_time:19402ms step_avg:34.46ms
step:564/1920 train_time:19436ms step_avg:34.46ms
step:565/1920 train_time:19471ms step_avg:34.46ms
step:566/1920 train_time:19505ms step_avg:34.46ms
step:567/1920 train_time:19539ms step_avg:34.46ms
step:568/1920 train_time:19574ms step_avg:34.46ms
step:569/1920 train_time:19608ms step_avg:34.46ms
step:570/1920 train_time:19642ms step_avg:34.46ms
step:571/1920 train_time:19677ms step_avg:34.46ms
step:572/1920 train_time:19711ms step_avg:34.46ms
step:573/1920 train_time:19746ms step_avg:34.46ms
step:574/1920 train_time:19780ms step_avg:34.46ms
step:575/1920 train_time:19814ms step_avg:34.46ms
step:576/1920 train_time:19848ms step_avg:34.46ms
step:577/1920 train_time:19883ms step_avg:34.46ms
step:578/1920 train_time:19917ms step_avg:34.46ms
step:579/1920 train_time:19951ms step_avg:34.46ms
step:580/1920 train_time:19985ms step_avg:34.46ms
step:581/1920 train_time:20020ms step_avg:34.46ms
step:582/1920 train_time:20054ms step_avg:34.46ms
step:583/1920 train_time:20088ms step_avg:34.46ms
step:584/1920 train_time:20123ms step_avg:34.46ms
step:585/1920 train_time:20157ms step_avg:34.46ms
step:586/1920 train_time:20191ms step_avg:34.46ms
step:587/1920 train_time:20225ms step_avg:34.46ms
step:588/1920 train_time:20259ms step_avg:34.45ms
step:589/1920 train_time:20294ms step_avg:34.46ms
step:590/1920 train_time:20329ms step_avg:34.46ms
step:591/1920 train_time:20363ms step_avg:34.46ms
step:592/1920 train_time:20397ms step_avg:34.45ms
step:593/1920 train_time:20432ms step_avg:34.46ms
step:594/1920 train_time:20467ms step_avg:34.46ms
step:595/1920 train_time:20501ms step_avg:34.46ms
step:596/1920 train_time:20535ms step_avg:34.46ms
step:597/1920 train_time:20570ms step_avg:34.45ms
step:598/1920 train_time:20604ms step_avg:34.45ms
step:599/1920 train_time:20638ms step_avg:34.45ms
step:600/1920 train_time:20672ms step_avg:34.45ms
step:601/1920 train_time:20706ms step_avg:34.45ms
step:602/1920 train_time:20741ms step_avg:34.45ms
step:603/1920 train_time:20775ms step_avg:34.45ms
step:604/1920 train_time:20809ms step_avg:34.45ms
step:605/1920 train_time:20844ms step_avg:34.45ms
step:606/1920 train_time:20878ms step_avg:34.45ms
step:607/1920 train_time:20912ms step_avg:34.45ms
step:608/1920 train_time:20947ms step_avg:34.45ms
step:609/1920 train_time:20981ms step_avg:34.45ms
step:610/1920 train_time:21015ms step_avg:34.45ms
step:611/1920 train_time:21050ms step_avg:34.45ms
step:612/1920 train_time:21084ms step_avg:34.45ms
step:613/1920 train_time:21118ms step_avg:34.45ms
step:614/1920 train_time:21152ms step_avg:34.45ms
step:615/1920 train_time:21187ms step_avg:34.45ms
step:616/1920 train_time:21221ms step_avg:34.45ms
step:617/1920 train_time:21256ms step_avg:34.45ms
step:618/1920 train_time:21290ms step_avg:34.45ms
step:619/1920 train_time:21324ms step_avg:34.45ms
step:620/1920 train_time:21359ms step_avg:34.45ms
step:621/1920 train_time:21393ms step_avg:34.45ms
step:622/1920 train_time:21428ms step_avg:34.45ms
step:623/1920 train_time:21462ms step_avg:34.45ms
step:624/1920 train_time:21496ms step_avg:34.45ms
step:625/1920 train_time:21531ms step_avg:34.45ms
step:626/1920 train_time:21565ms step_avg:34.45ms
step:627/1920 train_time:21600ms step_avg:34.45ms
step:628/1920 train_time:21634ms step_avg:34.45ms
step:629/1920 train_time:21696ms step_avg:34.49ms
step:630/1920 train_time:21758ms step_avg:34.54ms
step:631/1920 train_time:21820ms step_avg:34.58ms
step:632/1920 train_time:21882ms step_avg:34.62ms
step:633/1920 train_time:21944ms step_avg:34.67ms
step:634/1920 train_time:22006ms step_avg:34.71ms
step:635/1920 train_time:22068ms step_avg:34.75ms
step:636/1920 train_time:22130ms step_avg:34.79ms
step:637/1920 train_time:22192ms step_avg:34.84ms
step:638/1920 train_time:22254ms step_avg:34.88ms
step:639/1920 train_time:22318ms step_avg:34.93ms
step:640/1920 train_time:22380ms step_avg:34.97ms
step:641/1920 train_time:22442ms step_avg:35.01ms
step:642/1920 train_time:22504ms step_avg:35.05ms
step:643/1920 train_time:22566ms step_avg:35.10ms
step:644/1920 train_time:22628ms step_avg:35.14ms
step:645/1920 train_time:22690ms step_avg:35.18ms
step:646/1920 train_time:22752ms step_avg:35.22ms
step:647/1920 train_time:22815ms step_avg:35.26ms
step:648/1920 train_time:22877ms step_avg:35.30ms
step:649/1920 train_time:22940ms step_avg:35.35ms
step:650/1920 train_time:23001ms step_avg:35.39ms
step:651/1920 train_time:23064ms step_avg:35.43ms
step:652/1920 train_time:23125ms step_avg:35.47ms
step:653/1920 train_time:23187ms step_avg:35.51ms
step:654/1920 train_time:23249ms step_avg:35.55ms
step:655/1920 train_time:23312ms step_avg:35.59ms
step:656/1920 train_time:23373ms step_avg:35.63ms
step:657/1920 train_time:23437ms step_avg:35.67ms
step:658/1920 train_time:23499ms step_avg:35.71ms
step:659/1920 train_time:23563ms step_avg:35.76ms
step:660/1920 train_time:23624ms step_avg:35.79ms
step:661/1920 train_time:23687ms step_avg:35.83ms
step:662/1920 train_time:23749ms step_avg:35.87ms
step:663/1920 train_time:23811ms step_avg:35.91ms
step:664/1920 train_time:23873ms step_avg:35.95ms
step:665/1920 train_time:23936ms step_avg:35.99ms
step:666/1920 train_time:23998ms step_avg:36.03ms
step:667/1920 train_time:24062ms step_avg:36.07ms
step:668/1920 train_time:24124ms step_avg:36.11ms
step:669/1920 train_time:24187ms step_avg:36.15ms
step:670/1920 train_time:24248ms step_avg:36.19ms
step:671/1920 train_time:24310ms step_avg:36.23ms
step:672/1920 train_time:24372ms step_avg:36.27ms
step:673/1920 train_time:24435ms step_avg:36.31ms
step:674/1920 train_time:24497ms step_avg:36.35ms
step:675/1920 train_time:24559ms step_avg:36.38ms
step:676/1920 train_time:24622ms step_avg:36.42ms
step:677/1920 train_time:24684ms step_avg:36.46ms
step:678/1920 train_time:24746ms step_avg:36.50ms
step:679/1920 train_time:24808ms step_avg:36.54ms
step:680/1920 train_time:24870ms step_avg:36.57ms
step:681/1920 train_time:24932ms step_avg:36.61ms
step:682/1920 train_time:24994ms step_avg:36.65ms
step:683/1920 train_time:25057ms step_avg:36.69ms
step:684/1920 train_time:25119ms step_avg:36.72ms
step:685/1920 train_time:25182ms step_avg:36.76ms
step:686/1920 train_time:25244ms step_avg:36.80ms
step:687/1920 train_time:25306ms step_avg:36.84ms
step:688/1920 train_time:25368ms step_avg:36.87ms
step:689/1920 train_time:25430ms step_avg:36.91ms
step:690/1920 train_time:25492ms step_avg:36.94ms
step:691/1920 train_time:25556ms step_avg:36.98ms
step:692/1920 train_time:25619ms step_avg:37.02ms
step:693/1920 train_time:25681ms step_avg:37.06ms
step:694/1920 train_time:25743ms step_avg:37.09ms
step:695/1920 train_time:25806ms step_avg:37.13ms
step:696/1920 train_time:25868ms step_avg:37.17ms
step:697/1920 train_time:25930ms step_avg:37.20ms
step:698/1920 train_time:25992ms step_avg:37.24ms
step:699/1920 train_time:26055ms step_avg:37.27ms
step:700/1920 train_time:26117ms step_avg:37.31ms
step:701/1920 train_time:26180ms step_avg:37.35ms
step:702/1920 train_time:26242ms step_avg:37.38ms
step:703/1920 train_time:26304ms step_avg:37.42ms
step:704/1920 train_time:26366ms step_avg:37.45ms
step:705/1920 train_time:26428ms step_avg:37.49ms
step:706/1920 train_time:26490ms step_avg:37.52ms
step:707/1920 train_time:26552ms step_avg:37.56ms
step:708/1920 train_time:26613ms step_avg:37.59ms
step:709/1920 train_time:26677ms step_avg:37.63ms
step:710/1920 train_time:26739ms step_avg:37.66ms
step:711/1920 train_time:26802ms step_avg:37.70ms
step:712/1920 train_time:26864ms step_avg:37.73ms
step:713/1920 train_time:26927ms step_avg:37.77ms
step:714/1920 train_time:26989ms step_avg:37.80ms
step:715/1920 train_time:27051ms step_avg:37.83ms
step:716/1920 train_time:27112ms step_avg:37.87ms
step:717/1920 train_time:27176ms step_avg:37.90ms
step:718/1920 train_time:27238ms step_avg:37.94ms
step:719/1920 train_time:27301ms step_avg:37.97ms
step:720/1920 train_time:27363ms step_avg:38.00ms
step:721/1920 train_time:27426ms step_avg:38.04ms
step:722/1920 train_time:27487ms step_avg:38.07ms
step:723/1920 train_time:27550ms step_avg:38.10ms
step:724/1920 train_time:27612ms step_avg:38.14ms
step:725/1920 train_time:27675ms step_avg:38.17ms
step:726/1920 train_time:27737ms step_avg:38.21ms
step:727/1920 train_time:27800ms step_avg:38.24ms
step:728/1920 train_time:27862ms step_avg:38.27ms
step:729/1920 train_time:27925ms step_avg:38.31ms
step:730/1920 train_time:27987ms step_avg:38.34ms
step:731/1920 train_time:28049ms step_avg:38.37ms
step:732/1920 train_time:28111ms step_avg:38.40ms
step:733/1920 train_time:28173ms step_avg:38.44ms
step:734/1920 train_time:28235ms step_avg:38.47ms
step:735/1920 train_time:28299ms step_avg:38.50ms
step:736/1920 train_time:28361ms step_avg:38.53ms
step:737/1920 train_time:28423ms step_avg:38.57ms
step:738/1920 train_time:28485ms step_avg:38.60ms
step:739/1920 train_time:28547ms step_avg:38.63ms
step:740/1920 train_time:28609ms step_avg:38.66ms
step:741/1920 train_time:28672ms step_avg:38.69ms
step:742/1920 train_time:28734ms step_avg:38.72ms
step:743/1920 train_time:28797ms step_avg:38.76ms
step:744/1920 train_time:28860ms step_avg:38.79ms
step:745/1920 train_time:28922ms step_avg:38.82ms
step:746/1920 train_time:28984ms step_avg:38.85ms
step:747/1920 train_time:29047ms step_avg:38.88ms
step:748/1920 train_time:29109ms step_avg:38.92ms
step:749/1920 train_time:29171ms step_avg:38.95ms
step:750/1920 train_time:29233ms step_avg:38.98ms
step:750/1920 val_loss:4.0318 train_time:29298ms step_avg:39.06ms
step:751/1920 train_time:29317ms step_avg:39.04ms
step:752/1920 train_time:29359ms step_avg:39.04ms
step:753/1920 train_time:29421ms step_avg:39.07ms
step:754/1920 train_time:29488ms step_avg:39.11ms
step:755/1920 train_time:29553ms step_avg:39.14ms
step:756/1920 train_time:29614ms step_avg:39.17ms
step:757/1920 train_time:29676ms step_avg:39.20ms
step:758/1920 train_time:29737ms step_avg:39.23ms
step:759/1920 train_time:29799ms step_avg:39.26ms
step:760/1920 train_time:29860ms step_avg:39.29ms
step:761/1920 train_time:29922ms step_avg:39.32ms
step:762/1920 train_time:29982ms step_avg:39.35ms
step:763/1920 train_time:30044ms step_avg:39.38ms
step:764/1920 train_time:30105ms step_avg:39.40ms
step:765/1920 train_time:30167ms step_avg:39.43ms
step:766/1920 train_time:30233ms step_avg:39.47ms
step:767/1920 train_time:30297ms step_avg:39.50ms
step:768/1920 train_time:30360ms step_avg:39.53ms
step:769/1920 train_time:30423ms step_avg:39.56ms
step:770/1920 train_time:30485ms step_avg:39.59ms
step:771/1920 train_time:30547ms step_avg:39.62ms
step:772/1920 train_time:30609ms step_avg:39.65ms
step:773/1920 train_time:30672ms step_avg:39.68ms
step:774/1920 train_time:30733ms step_avg:39.71ms
step:775/1920 train_time:30796ms step_avg:39.74ms
step:776/1920 train_time:30857ms step_avg:39.76ms
step:777/1920 train_time:30919ms step_avg:39.79ms
step:778/1920 train_time:30981ms step_avg:39.82ms
step:779/1920 train_time:31043ms step_avg:39.85ms
step:780/1920 train_time:31104ms step_avg:39.88ms
step:781/1920 train_time:31167ms step_avg:39.91ms
step:782/1920 train_time:31230ms step_avg:39.94ms
step:783/1920 train_time:31293ms step_avg:39.97ms
step:784/1920 train_time:31355ms step_avg:39.99ms
step:785/1920 train_time:31419ms step_avg:40.02ms
step:786/1920 train_time:31481ms step_avg:40.05ms
step:787/1920 train_time:31544ms step_avg:40.08ms
step:788/1920 train_time:31605ms step_avg:40.11ms
step:789/1920 train_time:31667ms step_avg:40.14ms
step:790/1920 train_time:31729ms step_avg:40.16ms
step:791/1920 train_time:31792ms step_avg:40.19ms
step:792/1920 train_time:31854ms step_avg:40.22ms
step:793/1920 train_time:31916ms step_avg:40.25ms
step:794/1920 train_time:31977ms step_avg:40.27ms
step:795/1920 train_time:32040ms step_avg:40.30ms
step:796/1920 train_time:32101ms step_avg:40.33ms
step:797/1920 train_time:32164ms step_avg:40.36ms
step:798/1920 train_time:32225ms step_avg:40.38ms
step:799/1920 train_time:32288ms step_avg:40.41ms
step:800/1920 train_time:32350ms step_avg:40.44ms
step:801/1920 train_time:32414ms step_avg:40.47ms
step:802/1920 train_time:32476ms step_avg:40.49ms
step:803/1920 train_time:32539ms step_avg:40.52ms
step:804/1920 train_time:32601ms step_avg:40.55ms
step:805/1920 train_time:32664ms step_avg:40.58ms
step:806/1920 train_time:32726ms step_avg:40.60ms
step:807/1920 train_time:32788ms step_avg:40.63ms
step:808/1920 train_time:32850ms step_avg:40.66ms
step:809/1920 train_time:32913ms step_avg:40.68ms
step:810/1920 train_time:32975ms step_avg:40.71ms
step:811/1920 train_time:33038ms step_avg:40.74ms
step:812/1920 train_time:33100ms step_avg:40.76ms
step:813/1920 train_time:33163ms step_avg:40.79ms
step:814/1920 train_time:33224ms step_avg:40.82ms
step:815/1920 train_time:33286ms step_avg:40.84ms
step:816/1920 train_time:33348ms step_avg:40.87ms
step:817/1920 train_time:33411ms step_avg:40.89ms
step:818/1920 train_time:33473ms step_avg:40.92ms
step:819/1920 train_time:33536ms step_avg:40.95ms
step:820/1920 train_time:33598ms step_avg:40.97ms
step:821/1920 train_time:33660ms step_avg:41.00ms
step:822/1920 train_time:33722ms step_avg:41.02ms
step:823/1920 train_time:33785ms step_avg:41.05ms
step:824/1920 train_time:33847ms step_avg:41.08ms
step:825/1920 train_time:33910ms step_avg:41.10ms
step:826/1920 train_time:33972ms step_avg:41.13ms
step:827/1920 train_time:34035ms step_avg:41.15ms
step:828/1920 train_time:34097ms step_avg:41.18ms
step:829/1920 train_time:34160ms step_avg:41.21ms
step:830/1920 train_time:34221ms step_avg:41.23ms
step:831/1920 train_time:34283ms step_avg:41.26ms
step:832/1920 train_time:34345ms step_avg:41.28ms
step:833/1920 train_time:34407ms step_avg:41.30ms
step:834/1920 train_time:34469ms step_avg:41.33ms
step:835/1920 train_time:34532ms step_avg:41.36ms
step:836/1920 train_time:34595ms step_avg:41.38ms
step:837/1920 train_time:34657ms step_avg:41.41ms
step:838/1920 train_time:34719ms step_avg:41.43ms
step:839/1920 train_time:34782ms step_avg:41.46ms
step:840/1920 train_time:34843ms step_avg:41.48ms
step:841/1920 train_time:34906ms step_avg:41.51ms
step:842/1920 train_time:34968ms step_avg:41.53ms
step:843/1920 train_time:35030ms step_avg:41.55ms
step:844/1920 train_time:35092ms step_avg:41.58ms
step:845/1920 train_time:35156ms step_avg:41.60ms
step:846/1920 train_time:35217ms step_avg:41.63ms
step:847/1920 train_time:35279ms step_avg:41.65ms
step:848/1920 train_time:35341ms step_avg:41.68ms
step:849/1920 train_time:35404ms step_avg:41.70ms
step:850/1920 train_time:35466ms step_avg:41.73ms
step:851/1920 train_time:35529ms step_avg:41.75ms
step:852/1920 train_time:35591ms step_avg:41.77ms
step:853/1920 train_time:35655ms step_avg:41.80ms
step:854/1920 train_time:35716ms step_avg:41.82ms
step:855/1920 train_time:35779ms step_avg:41.85ms
step:856/1920 train_time:35841ms step_avg:41.87ms
step:857/1920 train_time:35903ms step_avg:41.89ms
step:858/1920 train_time:35965ms step_avg:41.92ms
step:859/1920 train_time:36028ms step_avg:41.94ms
step:860/1920 train_time:36090ms step_avg:41.96ms
step:861/1920 train_time:36153ms step_avg:41.99ms
step:862/1920 train_time:36215ms step_avg:42.01ms
step:863/1920 train_time:36278ms step_avg:42.04ms
step:864/1920 train_time:36340ms step_avg:42.06ms
step:865/1920 train_time:36401ms step_avg:42.08ms
step:866/1920 train_time:36463ms step_avg:42.11ms
step:867/1920 train_time:36526ms step_avg:42.13ms
step:868/1920 train_time:36588ms step_avg:42.15ms
step:869/1920 train_time:36651ms step_avg:42.18ms
step:870/1920 train_time:36714ms step_avg:42.20ms
step:871/1920 train_time:36777ms step_avg:42.22ms
step:872/1920 train_time:36839ms step_avg:42.25ms
step:873/1920 train_time:36901ms step_avg:42.27ms
step:874/1920 train_time:36963ms step_avg:42.29ms
step:875/1920 train_time:37025ms step_avg:42.31ms
step:876/1920 train_time:37087ms step_avg:42.34ms
step:877/1920 train_time:37150ms step_avg:42.36ms
step:878/1920 train_time:37212ms step_avg:42.38ms
step:879/1920 train_time:37275ms step_avg:42.41ms
step:880/1920 train_time:37336ms step_avg:42.43ms
step:881/1920 train_time:37399ms step_avg:42.45ms
step:882/1920 train_time:37461ms step_avg:42.47ms
step:883/1920 train_time:37523ms step_avg:42.50ms
step:884/1920 train_time:37585ms step_avg:42.52ms
step:885/1920 train_time:37648ms step_avg:42.54ms
step:886/1920 train_time:37710ms step_avg:42.56ms
step:887/1920 train_time:37774ms step_avg:42.59ms
step:888/1920 train_time:37836ms step_avg:42.61ms
step:889/1920 train_time:37898ms step_avg:42.63ms
step:890/1920 train_time:37960ms step_avg:42.65ms
step:891/1920 train_time:38023ms step_avg:42.67ms
step:892/1920 train_time:38085ms step_avg:42.70ms
step:893/1920 train_time:38148ms step_avg:42.72ms
step:894/1920 train_time:38209ms step_avg:42.74ms
step:895/1920 train_time:38273ms step_avg:42.76ms
step:896/1920 train_time:38335ms step_avg:42.78ms
step:897/1920 train_time:38398ms step_avg:42.81ms
step:898/1920 train_time:38460ms step_avg:42.83ms
step:899/1920 train_time:38522ms step_avg:42.85ms
step:900/1920 train_time:38584ms step_avg:42.87ms
step:901/1920 train_time:38646ms step_avg:42.89ms
step:902/1920 train_time:38708ms step_avg:42.91ms
step:903/1920 train_time:38771ms step_avg:42.94ms
step:904/1920 train_time:38833ms step_avg:42.96ms
step:905/1920 train_time:38896ms step_avg:42.98ms
step:906/1920 train_time:38958ms step_avg:43.00ms
step:907/1920 train_time:39021ms step_avg:43.02ms
step:908/1920 train_time:39082ms step_avg:43.04ms
step:909/1920 train_time:39145ms step_avg:43.06ms
step:910/1920 train_time:39207ms step_avg:43.09ms
step:911/1920 train_time:39270ms step_avg:43.11ms
step:912/1920 train_time:39332ms step_avg:43.13ms
step:913/1920 train_time:39395ms step_avg:43.15ms
step:914/1920 train_time:39457ms step_avg:43.17ms
step:915/1920 train_time:39520ms step_avg:43.19ms
step:916/1920 train_time:39582ms step_avg:43.21ms
step:917/1920 train_time:39644ms step_avg:43.23ms
step:918/1920 train_time:39707ms step_avg:43.25ms
step:919/1920 train_time:39769ms step_avg:43.27ms
step:920/1920 train_time:39831ms step_avg:43.29ms
step:921/1920 train_time:39894ms step_avg:43.32ms
step:922/1920 train_time:39956ms step_avg:43.34ms
step:923/1920 train_time:40019ms step_avg:43.36ms
step:924/1920 train_time:40082ms step_avg:43.38ms
step:925/1920 train_time:40144ms step_avg:43.40ms
step:926/1920 train_time:40205ms step_avg:43.42ms
step:927/1920 train_time:40268ms step_avg:43.44ms
step:928/1920 train_time:40330ms step_avg:43.46ms
step:929/1920 train_time:40393ms step_avg:43.48ms
step:930/1920 train_time:40455ms step_avg:43.50ms
step:931/1920 train_time:40518ms step_avg:43.52ms
step:932/1920 train_time:40580ms step_avg:43.54ms
step:933/1920 train_time:40642ms step_avg:43.56ms
step:934/1920 train_time:40704ms step_avg:43.58ms
step:935/1920 train_time:40766ms step_avg:43.60ms
step:936/1920 train_time:40828ms step_avg:43.62ms
step:937/1920 train_time:40891ms step_avg:43.64ms
step:938/1920 train_time:40954ms step_avg:43.66ms
step:939/1920 train_time:41018ms step_avg:43.68ms
step:940/1920 train_time:41080ms step_avg:43.70ms
step:941/1920 train_time:41142ms step_avg:43.72ms
step:942/1920 train_time:41204ms step_avg:43.74ms
step:943/1920 train_time:41266ms step_avg:43.76ms
step:944/1920 train_time:41329ms step_avg:43.78ms
step:945/1920 train_time:41391ms step_avg:43.80ms
step:946/1920 train_time:41454ms step_avg:43.82ms
step:947/1920 train_time:41517ms step_avg:43.84ms
step:948/1920 train_time:41579ms step_avg:43.86ms
step:949/1920 train_time:41642ms step_avg:43.88ms
step:950/1920 train_time:41703ms step_avg:43.90ms
step:951/1920 train_time:41767ms step_avg:43.92ms
step:952/1920 train_time:41829ms step_avg:43.94ms
step:953/1920 train_time:41892ms step_avg:43.96ms
step:954/1920 train_time:41954ms step_avg:43.98ms
step:955/1920 train_time:42017ms step_avg:44.00ms
step:956/1920 train_time:42079ms step_avg:44.02ms
step:957/1920 train_time:42142ms step_avg:44.04ms
step:958/1920 train_time:42204ms step_avg:44.05ms
step:959/1920 train_time:42266ms step_avg:44.07ms
step:960/1920 train_time:42328ms step_avg:44.09ms
step:961/1920 train_time:42390ms step_avg:44.11ms
step:962/1920 train_time:42453ms step_avg:44.13ms
step:963/1920 train_time:42516ms step_avg:44.15ms
step:964/1920 train_time:42578ms step_avg:44.17ms
step:965/1920 train_time:42641ms step_avg:44.19ms
step:966/1920 train_time:42703ms step_avg:44.21ms
step:967/1920 train_time:42765ms step_avg:44.22ms
step:968/1920 train_time:42827ms step_avg:44.24ms
step:969/1920 train_time:42890ms step_avg:44.26ms
step:970/1920 train_time:42952ms step_avg:44.28ms
step:971/1920 train_time:43016ms step_avg:44.30ms
step:972/1920 train_time:43077ms step_avg:44.32ms
step:973/1920 train_time:43140ms step_avg:44.34ms
step:974/1920 train_time:43201ms step_avg:44.35ms
step:975/1920 train_time:43264ms step_avg:44.37ms
step:976/1920 train_time:43326ms step_avg:44.39ms
step:977/1920 train_time:43388ms step_avg:44.41ms
step:978/1920 train_time:43450ms step_avg:44.43ms
step:979/1920 train_time:43513ms step_avg:44.45ms
step:980/1920 train_time:43575ms step_avg:44.46ms
step:981/1920 train_time:43638ms step_avg:44.48ms
step:982/1920 train_time:43700ms step_avg:44.50ms
step:983/1920 train_time:43763ms step_avg:44.52ms
step:984/1920 train_time:43825ms step_avg:44.54ms
step:985/1920 train_time:43887ms step_avg:44.56ms
step:986/1920 train_time:43949ms step_avg:44.57ms
step:987/1920 train_time:44013ms step_avg:44.59ms
step:988/1920 train_time:44075ms step_avg:44.61ms
step:989/1920 train_time:44138ms step_avg:44.63ms
step:990/1920 train_time:44199ms step_avg:44.65ms
step:991/1920 train_time:44262ms step_avg:44.66ms
step:992/1920 train_time:44323ms step_avg:44.68ms
step:993/1920 train_time:44385ms step_avg:44.70ms
step:994/1920 train_time:44447ms step_avg:44.72ms
step:995/1920 train_time:44510ms step_avg:44.73ms
step:996/1920 train_time:44572ms step_avg:44.75ms
step:997/1920 train_time:44636ms step_avg:44.77ms
step:998/1920 train_time:44698ms step_avg:44.79ms
step:999/1920 train_time:44761ms step_avg:44.81ms
step:1000/1920 train_time:44823ms step_avg:44.82ms
step:1000/1920 val_loss:3.7832 train_time:44888ms step_avg:44.89ms
step:1001/1920 train_time:44905ms step_avg:44.86ms
step:1002/1920 train_time:44950ms step_avg:44.86ms
step:1003/1920 train_time:45015ms step_avg:44.88ms
step:1004/1920 train_time:45080ms step_avg:44.90ms
step:1005/1920 train_time:45143ms step_avg:44.92ms
step:1006/1920 train_time:45205ms step_avg:44.93ms
step:1007/1920 train_time:45266ms step_avg:44.95ms
step:1008/1920 train_time:45327ms step_avg:44.97ms
step:1009/1920 train_time:45389ms step_avg:44.98ms
step:1010/1920 train_time:45450ms step_avg:45.00ms
step:1011/1920 train_time:45513ms step_avg:45.02ms
step:1012/1920 train_time:45574ms step_avg:45.03ms
step:1013/1920 train_time:45637ms step_avg:45.05ms
step:1014/1920 train_time:45699ms step_avg:45.07ms
step:1015/1920 train_time:45761ms step_avg:45.08ms
step:1016/1920 train_time:45823ms step_avg:45.10ms
step:1017/1920 train_time:45887ms step_avg:45.12ms
step:1018/1920 train_time:45949ms step_avg:45.14ms
step:1019/1920 train_time:46013ms step_avg:45.16ms
step:1020/1920 train_time:46075ms step_avg:45.17ms
step:1021/1920 train_time:46140ms step_avg:45.19ms
step:1022/1920 train_time:46201ms step_avg:45.21ms
step:1023/1920 train_time:46264ms step_avg:45.22ms
step:1024/1920 train_time:46326ms step_avg:45.24ms
step:1025/1920 train_time:46388ms step_avg:45.26ms
step:1026/1920 train_time:46449ms step_avg:45.27ms
step:1027/1920 train_time:46511ms step_avg:45.29ms
step:1028/1920 train_time:46572ms step_avg:45.30ms
step:1029/1920 train_time:46634ms step_avg:45.32ms
step:1030/1920 train_time:46696ms step_avg:45.34ms
step:1031/1920 train_time:46759ms step_avg:45.35ms
step:1032/1920 train_time:46821ms step_avg:45.37ms
step:1033/1920 train_time:46885ms step_avg:45.39ms
step:1034/1920 train_time:46948ms step_avg:45.40ms
step:1035/1920 train_time:47011ms step_avg:45.42ms
step:1036/1920 train_time:47074ms step_avg:45.44ms
step:1037/1920 train_time:47137ms step_avg:45.46ms
step:1038/1920 train_time:47200ms step_avg:45.47ms
step:1039/1920 train_time:47262ms step_avg:45.49ms
step:1040/1920 train_time:47324ms step_avg:45.50ms
step:1041/1920 train_time:47387ms step_avg:45.52ms
step:1042/1920 train_time:47449ms step_avg:45.54ms
step:1043/1920 train_time:47511ms step_avg:45.55ms
step:1044/1920 train_time:47572ms step_avg:45.57ms
step:1045/1920 train_time:47634ms step_avg:45.58ms
step:1046/1920 train_time:47696ms step_avg:45.60ms
step:1047/1920 train_time:47759ms step_avg:45.61ms
step:1048/1920 train_time:47820ms step_avg:45.63ms
step:1049/1920 train_time:47884ms step_avg:45.65ms
step:1050/1920 train_time:47945ms step_avg:45.66ms
step:1051/1920 train_time:48009ms step_avg:45.68ms
step:1052/1920 train_time:48071ms step_avg:45.69ms
step:1053/1920 train_time:48134ms step_avg:45.71ms
step:1054/1920 train_time:48197ms step_avg:45.73ms
step:1055/1920 train_time:48260ms step_avg:45.74ms
step:1056/1920 train_time:48322ms step_avg:45.76ms
step:1057/1920 train_time:48385ms step_avg:45.78ms
step:1058/1920 train_time:48447ms step_avg:45.79ms
step:1059/1920 train_time:48510ms step_avg:45.81ms
step:1060/1920 train_time:48571ms step_avg:45.82ms
step:1061/1920 train_time:48632ms step_avg:45.84ms
step:1062/1920 train_time:48694ms step_avg:45.85ms
step:1063/1920 train_time:48757ms step_avg:45.87ms
step:1064/1920 train_time:48819ms step_avg:45.88ms
step:1065/1920 train_time:48882ms step_avg:45.90ms
step:1066/1920 train_time:48944ms step_avg:45.91ms
step:1067/1920 train_time:49008ms step_avg:45.93ms
step:1068/1920 train_time:49070ms step_avg:45.95ms
step:1069/1920 train_time:49132ms step_avg:45.96ms
step:1070/1920 train_time:49194ms step_avg:45.98ms
step:1071/1920 train_time:49257ms step_avg:45.99ms
step:1072/1920 train_time:49319ms step_avg:46.01ms
step:1073/1920 train_time:49382ms step_avg:46.02ms
step:1074/1920 train_time:49444ms step_avg:46.04ms
step:1075/1920 train_time:49507ms step_avg:46.05ms
step:1076/1920 train_time:49569ms step_avg:46.07ms
step:1077/1920 train_time:49631ms step_avg:46.08ms
step:1078/1920 train_time:49693ms step_avg:46.10ms
step:1079/1920 train_time:49755ms step_avg:46.11ms
step:1080/1920 train_time:49817ms step_avg:46.13ms
step:1081/1920 train_time:49880ms step_avg:46.14ms
step:1082/1920 train_time:49942ms step_avg:46.16ms
step:1083/1920 train_time:50005ms step_avg:46.17ms
step:1084/1920 train_time:50067ms step_avg:46.19ms
step:1085/1920 train_time:50130ms step_avg:46.20ms
step:1086/1920 train_time:50192ms step_avg:46.22ms
step:1087/1920 train_time:50255ms step_avg:46.23ms
step:1088/1920 train_time:50318ms step_avg:46.25ms
step:1089/1920 train_time:50381ms step_avg:46.26ms
step:1090/1920 train_time:50443ms step_avg:46.28ms
step:1091/1920 train_time:50506ms step_avg:46.29ms
step:1092/1920 train_time:50567ms step_avg:46.31ms
step:1093/1920 train_time:50629ms step_avg:46.32ms
step:1094/1920 train_time:50691ms step_avg:46.34ms
step:1095/1920 train_time:50753ms step_avg:46.35ms
step:1096/1920 train_time:50814ms step_avg:46.36ms
step:1097/1920 train_time:50877ms step_avg:46.38ms
step:1098/1920 train_time:50940ms step_avg:46.39ms
step:1099/1920 train_time:51003ms step_avg:46.41ms
step:1100/1920 train_time:51065ms step_avg:46.42ms
step:1101/1920 train_time:51127ms step_avg:46.44ms
step:1102/1920 train_time:51190ms step_avg:46.45ms
step:1103/1920 train_time:51252ms step_avg:46.47ms
step:1104/1920 train_time:51314ms step_avg:46.48ms
step:1105/1920 train_time:51378ms step_avg:46.50ms
step:1106/1920 train_time:51440ms step_avg:46.51ms
step:1107/1920 train_time:51504ms step_avg:46.53ms
step:1108/1920 train_time:51566ms step_avg:46.54ms
step:1109/1920 train_time:51628ms step_avg:46.55ms
step:1110/1920 train_time:51690ms step_avg:46.57ms
step:1111/1920 train_time:51752ms step_avg:46.58ms
step:1112/1920 train_time:51813ms step_avg:46.59ms
step:1113/1920 train_time:51876ms step_avg:46.61ms
step:1114/1920 train_time:51939ms step_avg:46.62ms
step:1115/1920 train_time:52002ms step_avg:46.64ms
step:1116/1920 train_time:52064ms step_avg:46.65ms
step:1117/1920 train_time:52127ms step_avg:46.67ms
step:1118/1920 train_time:52190ms step_avg:46.68ms
step:1119/1920 train_time:52252ms step_avg:46.70ms
step:1120/1920 train_time:52315ms step_avg:46.71ms
step:1121/1920 train_time:52378ms step_avg:46.72ms
step:1122/1920 train_time:52440ms step_avg:46.74ms
step:1123/1920 train_time:52504ms step_avg:46.75ms
step:1124/1920 train_time:52566ms step_avg:46.77ms
step:1125/1920 train_time:52628ms step_avg:46.78ms
step:1126/1920 train_time:52690ms step_avg:46.79ms
step:1127/1920 train_time:52752ms step_avg:46.81ms
step:1128/1920 train_time:52814ms step_avg:46.82ms
step:1129/1920 train_time:52877ms step_avg:46.83ms
step:1130/1920 train_time:52939ms step_avg:46.85ms
step:1131/1920 train_time:53002ms step_avg:46.86ms
step:1132/1920 train_time:53064ms step_avg:46.88ms
step:1133/1920 train_time:53127ms step_avg:46.89ms
step:1134/1920 train_time:53189ms step_avg:46.90ms
step:1135/1920 train_time:53251ms step_avg:46.92ms
step:1136/1920 train_time:53314ms step_avg:46.93ms
step:1137/1920 train_time:53377ms step_avg:46.95ms
step:1138/1920 train_time:53439ms step_avg:46.96ms
step:1139/1920 train_time:53502ms step_avg:46.97ms
step:1140/1920 train_time:53564ms step_avg:46.99ms
step:1141/1920 train_time:53627ms step_avg:47.00ms
step:1142/1920 train_time:53688ms step_avg:47.01ms
step:1143/1920 train_time:53750ms step_avg:47.03ms
step:1144/1920 train_time:53813ms step_avg:47.04ms
step:1145/1920 train_time:53875ms step_avg:47.05ms
step:1146/1920 train_time:53936ms step_avg:47.06ms
step:1147/1920 train_time:53999ms step_avg:47.08ms
step:1148/1920 train_time:54061ms step_avg:47.09ms
step:1149/1920 train_time:54124ms step_avg:47.11ms
step:1150/1920 train_time:54186ms step_avg:47.12ms
step:1151/1920 train_time:54249ms step_avg:47.13ms
step:1152/1920 train_time:54311ms step_avg:47.15ms
step:1153/1920 train_time:54374ms step_avg:47.16ms
step:1154/1920 train_time:54436ms step_avg:47.17ms
step:1155/1920 train_time:54499ms step_avg:47.19ms
step:1156/1920 train_time:54561ms step_avg:47.20ms
step:1157/1920 train_time:54624ms step_avg:47.21ms
step:1158/1920 train_time:54686ms step_avg:47.22ms
step:1159/1920 train_time:54749ms step_avg:47.24ms
step:1160/1920 train_time:54810ms step_avg:47.25ms
step:1161/1920 train_time:54873ms step_avg:47.26ms
step:1162/1920 train_time:54934ms step_avg:47.28ms
step:1163/1920 train_time:54997ms step_avg:47.29ms
step:1164/1920 train_time:55059ms step_avg:47.30ms
step:1165/1920 train_time:55123ms step_avg:47.32ms
step:1166/1920 train_time:55185ms step_avg:47.33ms
step:1167/1920 train_time:55248ms step_avg:47.34ms
step:1168/1920 train_time:55309ms step_avg:47.35ms
step:1169/1920 train_time:55372ms step_avg:47.37ms
step:1170/1920 train_time:55434ms step_avg:47.38ms
step:1171/1920 train_time:55497ms step_avg:47.39ms
step:1172/1920 train_time:55559ms step_avg:47.41ms
step:1173/1920 train_time:55623ms step_avg:47.42ms
step:1174/1920 train_time:55685ms step_avg:47.43ms
step:1175/1920 train_time:55748ms step_avg:47.45ms
step:1176/1920 train_time:55810ms step_avg:47.46ms
step:1177/1920 train_time:55873ms step_avg:47.47ms
step:1178/1920 train_time:55935ms step_avg:47.48ms
step:1179/1920 train_time:55998ms step_avg:47.50ms
step:1180/1920 train_time:56059ms step_avg:47.51ms
step:1181/1920 train_time:56122ms step_avg:47.52ms
step:1182/1920 train_time:56184ms step_avg:47.53ms
step:1183/1920 train_time:56247ms step_avg:47.55ms
step:1184/1920 train_time:56309ms step_avg:47.56ms
step:1185/1920 train_time:56372ms step_avg:47.57ms
step:1186/1920 train_time:56433ms step_avg:47.58ms
step:1187/1920 train_time:56497ms step_avg:47.60ms
step:1188/1920 train_time:56559ms step_avg:47.61ms
step:1189/1920 train_time:56622ms step_avg:47.62ms
step:1190/1920 train_time:56684ms step_avg:47.63ms
step:1191/1920 train_time:56747ms step_avg:47.65ms
step:1192/1920 train_time:56810ms step_avg:47.66ms
step:1193/1920 train_time:56873ms step_avg:47.67ms
step:1194/1920 train_time:56934ms step_avg:47.68ms
step:1195/1920 train_time:56997ms step_avg:47.70ms
step:1196/1920 train_time:57059ms step_avg:47.71ms
step:1197/1920 train_time:57122ms step_avg:47.72ms
step:1198/1920 train_time:57184ms step_avg:47.73ms
step:1199/1920 train_time:57247ms step_avg:47.75ms
step:1200/1920 train_time:57308ms step_avg:47.76ms
step:1201/1920 train_time:57371ms step_avg:47.77ms
step:1202/1920 train_time:57433ms step_avg:47.78ms
step:1203/1920 train_time:57496ms step_avg:47.79ms
step:1204/1920 train_time:57558ms step_avg:47.81ms
step:1205/1920 train_time:57621ms step_avg:47.82ms
step:1206/1920 train_time:57683ms step_avg:47.83ms
step:1207/1920 train_time:57746ms step_avg:47.84ms
step:1208/1920 train_time:57807ms step_avg:47.85ms
step:1209/1920 train_time:57869ms step_avg:47.87ms
step:1210/1920 train_time:57931ms step_avg:47.88ms
step:1211/1920 train_time:57993ms step_avg:47.89ms
step:1212/1920 train_time:58056ms step_avg:47.90ms
step:1213/1920 train_time:58119ms step_avg:47.91ms
step:1214/1920 train_time:58181ms step_avg:47.93ms
step:1215/1920 train_time:58244ms step_avg:47.94ms
step:1216/1920 train_time:58306ms step_avg:47.95ms
step:1217/1920 train_time:58369ms step_avg:47.96ms
step:1218/1920 train_time:58431ms step_avg:47.97ms
step:1219/1920 train_time:58493ms step_avg:47.98ms
step:1220/1920 train_time:58555ms step_avg:48.00ms
step:1221/1920 train_time:58619ms step_avg:48.01ms
step:1222/1920 train_time:58681ms step_avg:48.02ms
step:1223/1920 train_time:58745ms step_avg:48.03ms
step:1224/1920 train_time:58807ms step_avg:48.05ms
step:1225/1920 train_time:58870ms step_avg:48.06ms
step:1226/1920 train_time:58931ms step_avg:48.07ms
step:1227/1920 train_time:58993ms step_avg:48.08ms
step:1228/1920 train_time:59055ms step_avg:48.09ms
step:1229/1920 train_time:59118ms step_avg:48.10ms
step:1230/1920 train_time:59180ms step_avg:48.11ms
step:1231/1920 train_time:59243ms step_avg:48.13ms
step:1232/1920 train_time:59305ms step_avg:48.14ms
step:1233/1920 train_time:59368ms step_avg:48.15ms
step:1234/1920 train_time:59430ms step_avg:48.16ms
step:1235/1920 train_time:59492ms step_avg:48.17ms
step:1236/1920 train_time:59554ms step_avg:48.18ms
step:1237/1920 train_time:59616ms step_avg:48.19ms
step:1238/1920 train_time:59679ms step_avg:48.21ms
step:1239/1920 train_time:59742ms step_avg:48.22ms
step:1240/1920 train_time:59804ms step_avg:48.23ms
step:1241/1920 train_time:59867ms step_avg:48.24ms
step:1242/1920 train_time:59928ms step_avg:48.25ms
step:1243/1920 train_time:59991ms step_avg:48.26ms
step:1244/1920 train_time:60052ms step_avg:48.27ms
step:1245/1920 train_time:60115ms step_avg:48.28ms
step:1246/1920 train_time:60177ms step_avg:48.30ms
step:1247/1920 train_time:60240ms step_avg:48.31ms
step:1248/1920 train_time:60302ms step_avg:48.32ms
step:1249/1920 train_time:60365ms step_avg:48.33ms
step:1250/1920 train_time:60427ms step_avg:48.34ms
step:1250/1920 val_loss:3.5548 train_time:60492ms step_avg:48.39ms
step:1251/1920 train_time:60511ms step_avg:48.37ms
step:1252/1920 train_time:60554ms step_avg:48.37ms
step:1253/1920 train_time:60618ms step_avg:48.38ms
step:1254/1920 train_time:60681ms step_avg:48.39ms
step:1255/1920 train_time:60745ms step_avg:48.40ms
step:1256/1920 train_time:60833ms step_avg:48.43ms
step:1257/1920 train_time:60921ms step_avg:48.47ms
step:1258/1920 train_time:61007ms step_avg:48.50ms
step:1259/1920 train_time:61096ms step_avg:48.53ms
step:1260/1920 train_time:61183ms step_avg:48.56ms
step:1261/1920 train_time:61271ms step_avg:48.59ms
step:1262/1920 train_time:61357ms step_avg:48.62ms
step:1263/1920 train_time:61448ms step_avg:48.65ms
step:1264/1920 train_time:61538ms step_avg:48.69ms
step:1265/1920 train_time:61628ms step_avg:48.72ms
step:1266/1920 train_time:61717ms step_avg:48.75ms
step:1267/1920 train_time:61806ms step_avg:48.78ms
step:1268/1920 train_time:61894ms step_avg:48.81ms
step:1269/1920 train_time:61982ms step_avg:48.84ms
step:1270/1920 train_time:62069ms step_avg:48.87ms
step:1271/1920 train_time:62158ms step_avg:48.90ms
step:1272/1920 train_time:62245ms step_avg:48.93ms
step:1273/1920 train_time:62333ms step_avg:48.97ms
step:1274/1920 train_time:62422ms step_avg:49.00ms
step:1275/1920 train_time:62511ms step_avg:49.03ms
step:1276/1920 train_time:62601ms step_avg:49.06ms
step:1277/1920 train_time:62691ms step_avg:49.09ms
step:1278/1920 train_time:62779ms step_avg:49.12ms
step:1279/1920 train_time:62867ms step_avg:49.15ms
step:1280/1920 train_time:62956ms step_avg:49.18ms
step:1281/1920 train_time:63045ms step_avg:49.22ms
step:1282/1920 train_time:63132ms step_avg:49.24ms
step:1283/1920 train_time:63220ms step_avg:49.28ms
step:1284/1920 train_time:63307ms step_avg:49.30ms
step:1285/1920 train_time:63396ms step_avg:49.34ms
step:1286/1920 train_time:63485ms step_avg:49.37ms
step:1287/1920 train_time:63575ms step_avg:49.40ms
step:1288/1920 train_time:63664ms step_avg:49.43ms
step:1289/1920 train_time:63754ms step_avg:49.46ms
step:1290/1920 train_time:63843ms step_avg:49.49ms
step:1291/1920 train_time:63932ms step_avg:49.52ms
step:1292/1920 train_time:64020ms step_avg:49.55ms
step:1293/1920 train_time:64108ms step_avg:49.58ms
step:1294/1920 train_time:64196ms step_avg:49.61ms
step:1295/1920 train_time:64285ms step_avg:49.64ms
step:1296/1920 train_time:64373ms step_avg:49.67ms
step:1297/1920 train_time:64463ms step_avg:49.70ms
step:1298/1920 train_time:64551ms step_avg:49.73ms
step:1299/1920 train_time:64640ms step_avg:49.76ms
step:1300/1920 train_time:64728ms step_avg:49.79ms
step:1301/1920 train_time:64817ms step_avg:49.82ms
step:1302/1920 train_time:64905ms step_avg:49.85ms
step:1303/1920 train_time:64993ms step_avg:49.88ms
step:1304/1920 train_time:65082ms step_avg:49.91ms
step:1305/1920 train_time:65170ms step_avg:49.94ms
step:1306/1920 train_time:65258ms step_avg:49.97ms
step:1307/1920 train_time:65346ms step_avg:50.00ms
step:1308/1920 train_time:65435ms step_avg:50.03ms
step:1309/1920 train_time:65525ms step_avg:50.06ms
step:1310/1920 train_time:65614ms step_avg:50.09ms
step:1311/1920 train_time:65703ms step_avg:50.12ms
step:1312/1920 train_time:65791ms step_avg:50.15ms
step:1313/1920 train_time:65880ms step_avg:50.18ms
step:1314/1920 train_time:65968ms step_avg:50.20ms
step:1315/1920 train_time:66056ms step_avg:50.23ms
step:1316/1920 train_time:66144ms step_avg:50.26ms
step:1317/1920 train_time:66232ms step_avg:50.29ms
step:1318/1920 train_time:66320ms step_avg:50.32ms
step:1319/1920 train_time:66409ms step_avg:50.35ms
step:1320/1920 train_time:66498ms step_avg:50.38ms
step:1321/1920 train_time:66587ms step_avg:50.41ms
step:1322/1920 train_time:66676ms step_avg:50.44ms
step:1323/1920 train_time:66764ms step_avg:50.46ms
step:1324/1920 train_time:66852ms step_avg:50.49ms
step:1325/1920 train_time:66941ms step_avg:50.52ms
step:1326/1920 train_time:67028ms step_avg:50.55ms
step:1327/1920 train_time:67116ms step_avg:50.58ms
step:1328/1920 train_time:67204ms step_avg:50.61ms
step:1329/1920 train_time:67292ms step_avg:50.63ms
step:1330/1920 train_time:67381ms step_avg:50.66ms
step:1331/1920 train_time:67470ms step_avg:50.69ms
step:1332/1920 train_time:67561ms step_avg:50.72ms
step:1333/1920 train_time:67649ms step_avg:50.75ms
step:1334/1920 train_time:67737ms step_avg:50.78ms
step:1335/1920 train_time:67826ms step_avg:50.81ms
step:1336/1920 train_time:67914ms step_avg:50.83ms
step:1337/1920 train_time:68003ms step_avg:50.86ms
step:1338/1920 train_time:68091ms step_avg:50.89ms
step:1339/1920 train_time:68179ms step_avg:50.92ms
step:1340/1920 train_time:68267ms step_avg:50.95ms
step:1341/1920 train_time:68356ms step_avg:50.97ms
step:1342/1920 train_time:68444ms step_avg:51.00ms
step:1343/1920 train_time:68533ms step_avg:51.03ms
step:1344/1920 train_time:68621ms step_avg:51.06ms
step:1345/1920 train_time:68710ms step_avg:51.09ms
step:1346/1920 train_time:68798ms step_avg:51.11ms
step:1347/1920 train_time:68888ms step_avg:51.14ms
step:1348/1920 train_time:68977ms step_avg:51.17ms
step:1349/1920 train_time:69066ms step_avg:51.20ms
step:1350/1920 train_time:69154ms step_avg:51.23ms
step:1351/1920 train_time:69243ms step_avg:51.25ms
step:1352/1920 train_time:69331ms step_avg:51.28ms
step:1353/1920 train_time:69420ms step_avg:51.31ms
step:1354/1920 train_time:69507ms step_avg:51.33ms
step:1355/1920 train_time:69597ms step_avg:51.36ms
step:1356/1920 train_time:69684ms step_avg:51.39ms
step:1357/1920 train_time:69773ms step_avg:51.42ms
step:1358/1920 train_time:69861ms step_avg:51.44ms
step:1359/1920 train_time:69949ms step_avg:51.47ms
step:1360/1920 train_time:70037ms step_avg:51.50ms
step:1361/1920 train_time:70126ms step_avg:51.53ms
step:1362/1920 train_time:70215ms step_avg:51.55ms
step:1363/1920 train_time:70304ms step_avg:51.58ms
step:1364/1920 train_time:70392ms step_avg:51.61ms
step:1365/1920 train_time:70481ms step_avg:51.63ms
step:1366/1920 train_time:70569ms step_avg:51.66ms
step:1367/1920 train_time:70658ms step_avg:51.69ms
step:1368/1920 train_time:70746ms step_avg:51.71ms
step:1369/1920 train_time:70834ms step_avg:51.74ms
step:1370/1920 train_time:70923ms step_avg:51.77ms
step:1371/1920 train_time:71012ms step_avg:51.80ms
step:1372/1920 train_time:71101ms step_avg:51.82ms
step:1373/1920 train_time:71189ms step_avg:51.85ms
step:1374/1920 train_time:71278ms step_avg:51.88ms
step:1375/1920 train_time:71366ms step_avg:51.90ms
step:1376/1920 train_time:71454ms step_avg:51.93ms
step:1377/1920 train_time:71543ms step_avg:51.96ms
step:1378/1920 train_time:71631ms step_avg:51.98ms
step:1379/1920 train_time:71720ms step_avg:52.01ms
step:1380/1920 train_time:71808ms step_avg:52.03ms
step:1381/1920 train_time:71898ms step_avg:52.06ms
step:1382/1920 train_time:71986ms step_avg:52.09ms
step:1383/1920 train_time:72074ms step_avg:52.11ms
step:1384/1920 train_time:72162ms step_avg:52.14ms
step:1385/1920 train_time:72251ms step_avg:52.17ms
step:1386/1920 train_time:72339ms step_avg:52.19ms
step:1387/1920 train_time:72428ms step_avg:52.22ms
step:1388/1920 train_time:72516ms step_avg:52.25ms
step:1389/1920 train_time:72605ms step_avg:52.27ms
step:1390/1920 train_time:72693ms step_avg:52.30ms
step:1391/1920 train_time:72782ms step_avg:52.32ms
step:1392/1920 train_time:72870ms step_avg:52.35ms
step:1393/1920 train_time:72959ms step_avg:52.38ms
step:1394/1920 train_time:73046ms step_avg:52.40ms
step:1395/1920 train_time:73135ms step_avg:52.43ms
step:1396/1920 train_time:73224ms step_avg:52.45ms
step:1397/1920 train_time:73312ms step_avg:52.48ms
step:1398/1920 train_time:73400ms step_avg:52.50ms
step:1399/1920 train_time:73489ms step_avg:52.53ms
step:1400/1920 train_time:73577ms step_avg:52.55ms
step:1401/1920 train_time:73665ms step_avg:52.58ms
step:1402/1920 train_time:73753ms step_avg:52.61ms
step:1403/1920 train_time:73843ms step_avg:52.63ms
step:1404/1920 train_time:73932ms step_avg:52.66ms
step:1405/1920 train_time:74020ms step_avg:52.68ms
step:1406/1920 train_time:74108ms step_avg:52.71ms
step:1407/1920 train_time:74198ms step_avg:52.74ms
step:1408/1920 train_time:74287ms step_avg:52.76ms
step:1409/1920 train_time:74375ms step_avg:52.79ms
step:1410/1920 train_time:74463ms step_avg:52.81ms
step:1411/1920 train_time:74552ms step_avg:52.84ms
step:1412/1920 train_time:74640ms step_avg:52.86ms
step:1413/1920 train_time:74728ms step_avg:52.89ms
step:1414/1920 train_time:74816ms step_avg:52.91ms
step:1415/1920 train_time:74906ms step_avg:52.94ms
step:1416/1920 train_time:74995ms step_avg:52.96ms
step:1417/1920 train_time:75086ms step_avg:52.99ms
step:1418/1920 train_time:75175ms step_avg:53.02ms
step:1419/1920 train_time:75265ms step_avg:53.04ms
step:1420/1920 train_time:75353ms step_avg:53.07ms
step:1421/1920 train_time:75441ms step_avg:53.09ms
step:1422/1920 train_time:75529ms step_avg:53.11ms
step:1423/1920 train_time:75619ms step_avg:53.14ms
step:1424/1920 train_time:75705ms step_avg:53.16ms
step:1425/1920 train_time:75794ms step_avg:53.19ms
step:1426/1920 train_time:75883ms step_avg:53.21ms
step:1427/1920 train_time:75971ms step_avg:53.24ms
step:1428/1920 train_time:76059ms step_avg:53.26ms
step:1429/1920 train_time:76147ms step_avg:53.29ms
step:1430/1920 train_time:76237ms step_avg:53.31ms
step:1431/1920 train_time:76326ms step_avg:53.34ms
step:1432/1920 train_time:76414ms step_avg:53.36ms
step:1433/1920 train_time:76503ms step_avg:53.39ms
step:1434/1920 train_time:76592ms step_avg:53.41ms
step:1435/1920 train_time:76681ms step_avg:53.44ms
step:1436/1920 train_time:76769ms step_avg:53.46ms
step:1437/1920 train_time:76858ms step_avg:53.48ms
step:1438/1920 train_time:76945ms step_avg:53.51ms
step:1439/1920 train_time:77034ms step_avg:53.53ms
step:1440/1920 train_time:77123ms step_avg:53.56ms
step:1441/1920 train_time:77211ms step_avg:53.58ms
step:1442/1920 train_time:77300ms step_avg:53.61ms
step:1443/1920 train_time:77388ms step_avg:53.63ms
step:1444/1920 train_time:77476ms step_avg:53.65ms
step:1445/1920 train_time:77565ms step_avg:53.68ms
step:1446/1920 train_time:77653ms step_avg:53.70ms
step:1447/1920 train_time:77742ms step_avg:53.73ms
step:1448/1920 train_time:77830ms step_avg:53.75ms
step:1449/1920 train_time:77919ms step_avg:53.77ms
step:1450/1920 train_time:78007ms step_avg:53.80ms
step:1451/1920 train_time:78096ms step_avg:53.82ms
step:1452/1920 train_time:78184ms step_avg:53.85ms
step:1453/1920 train_time:78272ms step_avg:53.87ms
step:1454/1920 train_time:78360ms step_avg:53.89ms
step:1455/1920 train_time:78449ms step_avg:53.92ms
step:1456/1920 train_time:78538ms step_avg:53.94ms
step:1457/1920 train_time:78627ms step_avg:53.96ms
step:1458/1920 train_time:78714ms step_avg:53.99ms
step:1459/1920 train_time:78803ms step_avg:54.01ms
step:1460/1920 train_time:78891ms step_avg:54.03ms
step:1461/1920 train_time:78979ms step_avg:54.06ms
step:1462/1920 train_time:79067ms step_avg:54.08ms
step:1463/1920 train_time:79155ms step_avg:54.10ms
step:1464/1920 train_time:79244ms step_avg:54.13ms
step:1465/1920 train_time:79332ms step_avg:54.15ms
step:1466/1920 train_time:79421ms step_avg:54.17ms
step:1467/1920 train_time:79509ms step_avg:54.20ms
step:1468/1920 train_time:79598ms step_avg:54.22ms
step:1469/1920 train_time:79686ms step_avg:54.25ms
step:1470/1920 train_time:79775ms step_avg:54.27ms
step:1471/1920 train_time:79866ms step_avg:54.29ms
step:1472/1920 train_time:79954ms step_avg:54.32ms
step:1473/1920 train_time:80044ms step_avg:54.34ms
step:1474/1920 train_time:80132ms step_avg:54.36ms
step:1475/1920 train_time:80222ms step_avg:54.39ms
step:1476/1920 train_time:80309ms step_avg:54.41ms
step:1477/1920 train_time:80398ms step_avg:54.43ms
step:1478/1920 train_time:80485ms step_avg:54.46ms
step:1479/1920 train_time:80575ms step_avg:54.48ms
step:1480/1920 train_time:80663ms step_avg:54.50ms
step:1481/1920 train_time:80751ms step_avg:54.52ms
step:1482/1920 train_time:80839ms step_avg:54.55ms
step:1483/1920 train_time:80928ms step_avg:54.57ms
step:1484/1920 train_time:81016ms step_avg:54.59ms
step:1485/1920 train_time:81105ms step_avg:54.62ms
step:1486/1920 train_time:81193ms step_avg:54.64ms
step:1487/1920 train_time:81282ms step_avg:54.66ms
step:1488/1920 train_time:81370ms step_avg:54.68ms
step:1489/1920 train_time:81459ms step_avg:54.71ms
step:1490/1920 train_time:81547ms step_avg:54.73ms
step:1491/1920 train_time:81637ms step_avg:54.75ms
step:1492/1920 train_time:81726ms step_avg:54.78ms
step:1493/1920 train_time:81814ms step_avg:54.80ms
step:1494/1920 train_time:81902ms step_avg:54.82ms
step:1495/1920 train_time:81991ms step_avg:54.84ms
step:1496/1920 train_time:82080ms step_avg:54.87ms
step:1497/1920 train_time:82169ms step_avg:54.89ms
step:1498/1920 train_time:82257ms step_avg:54.91ms
step:1499/1920 train_time:82346ms step_avg:54.93ms
step:1500/1920 train_time:82435ms step_avg:54.96ms
step:1500/1920 val_loss:3.4160 train_time:82526ms step_avg:55.02ms
step:1501/1920 train_time:82544ms step_avg:54.99ms
step:1502/1920 train_time:82618ms step_avg:55.01ms
step:1503/1920 train_time:82709ms step_avg:55.03ms
step:1504/1920 train_time:82797ms step_avg:55.05ms
step:1505/1920 train_time:82884ms step_avg:55.07ms
step:1506/1920 train_time:82971ms step_avg:55.09ms
step:1507/1920 train_time:83059ms step_avg:55.12ms
step:1508/1920 train_time:83147ms step_avg:55.14ms
step:1509/1920 train_time:83235ms step_avg:55.16ms
step:1510/1920 train_time:83323ms step_avg:55.18ms
step:1511/1920 train_time:83412ms step_avg:55.20ms
step:1512/1920 train_time:83502ms step_avg:55.23ms
step:1513/1920 train_time:83593ms step_avg:55.25ms
step:1514/1920 train_time:83683ms step_avg:55.27ms
step:1515/1920 train_time:83772ms step_avg:55.30ms
step:1516/1920 train_time:83860ms step_avg:55.32ms
step:1517/1920 train_time:83949ms step_avg:55.34ms
step:1518/1920 train_time:84036ms step_avg:55.36ms
step:1519/1920 train_time:84123ms step_avg:55.38ms
step:1520/1920 train_time:84211ms step_avg:55.40ms
step:1521/1920 train_time:84300ms step_avg:55.42ms
step:1522/1920 train_time:84388ms step_avg:55.45ms
step:1523/1920 train_time:84477ms step_avg:55.47ms
step:1524/1920 train_time:84566ms step_avg:55.49ms
step:1525/1920 train_time:84656ms step_avg:55.51ms
step:1526/1920 train_time:84746ms step_avg:55.53ms
step:1527/1920 train_time:84835ms step_avg:55.56ms
step:1528/1920 train_time:84922ms step_avg:55.58ms
step:1529/1920 train_time:85010ms step_avg:55.60ms
step:1530/1920 train_time:85098ms step_avg:55.62ms
step:1531/1920 train_time:85186ms step_avg:55.64ms
step:1532/1920 train_time:85273ms step_avg:55.66ms
step:1533/1920 train_time:85362ms step_avg:55.68ms
step:1534/1920 train_time:85451ms step_avg:55.70ms
step:1535/1920 train_time:85542ms step_avg:55.73ms
step:1536/1920 train_time:85630ms step_avg:55.75ms
step:1537/1920 train_time:85720ms step_avg:55.77ms
step:1538/1920 train_time:85808ms step_avg:55.79ms
step:1539/1920 train_time:85897ms step_avg:55.81ms
step:1540/1920 train_time:85985ms step_avg:55.83ms
step:1541/1920 train_time:86073ms step_avg:55.86ms
step:1542/1920 train_time:86161ms step_avg:55.88ms
step:1543/1920 train_time:86249ms step_avg:55.90ms
step:1544/1920 train_time:86337ms step_avg:55.92ms
step:1545/1920 train_time:86425ms step_avg:55.94ms
step:1546/1920 train_time:86514ms step_avg:55.96ms
step:1547/1920 train_time:86603ms step_avg:55.98ms
step:1548/1920 train_time:86692ms step_avg:56.00ms
step:1549/1920 train_time:86782ms step_avg:56.02ms
step:1550/1920 train_time:86871ms step_avg:56.05ms
step:1551/1920 train_time:86961ms step_avg:56.07ms
step:1552/1920 train_time:87049ms step_avg:56.09ms
step:1553/1920 train_time:87138ms step_avg:56.11ms
step:1554/1920 train_time:87225ms step_avg:56.13ms
step:1555/1920 train_time:87314ms step_avg:56.15ms
step:1556/1920 train_time:87402ms step_avg:56.17ms
step:1557/1920 train_time:87491ms step_avg:56.19ms
step:1558/1920 train_time:87579ms step_avg:56.21ms
step:1559/1920 train_time:87668ms step_avg:56.23ms
step:1560/1920 train_time:87757ms step_avg:56.25ms
step:1561/1920 train_time:87846ms step_avg:56.28ms
step:1562/1920 train_time:87934ms step_avg:56.30ms
step:1563/1920 train_time:88023ms step_avg:56.32ms
step:1564/1920 train_time:88111ms step_avg:56.34ms
step:1565/1920 train_time:88200ms step_avg:56.36ms
step:1566/1920 train_time:88288ms step_avg:56.38ms
step:1567/1920 train_time:88377ms step_avg:56.40ms
step:1568/1920 train_time:88464ms step_avg:56.42ms
step:1569/1920 train_time:88553ms step_avg:56.44ms
step:1570/1920 train_time:88641ms step_avg:56.46ms
step:1571/1920 train_time:88730ms step_avg:56.48ms
step:1572/1920 train_time:88819ms step_avg:56.50ms
step:1573/1920 train_time:88908ms step_avg:56.52ms
step:1574/1920 train_time:88996ms step_avg:56.54ms
step:1575/1920 train_time:89083ms step_avg:56.56ms
step:1576/1920 train_time:89171ms step_avg:56.58ms
step:1577/1920 train_time:89260ms step_avg:56.60ms
step:1578/1920 train_time:89348ms step_avg:56.62ms
step:1579/1920 train_time:89437ms step_avg:56.64ms
step:1580/1920 train_time:89525ms step_avg:56.66ms
step:1581/1920 train_time:89614ms step_avg:56.68ms
step:1582/1920 train_time:89702ms step_avg:56.70ms
step:1583/1920 train_time:89790ms step_avg:56.72ms
step:1584/1920 train_time:89879ms step_avg:56.74ms
step:1585/1920 train_time:89968ms step_avg:56.76ms
step:1586/1920 train_time:90056ms step_avg:56.78ms
step:1587/1920 train_time:90145ms step_avg:56.80ms
step:1588/1920 train_time:90233ms step_avg:56.82ms
step:1589/1920 train_time:90322ms step_avg:56.84ms
step:1590/1920 train_time:90410ms step_avg:56.86ms
step:1591/1920 train_time:90500ms step_avg:56.88ms
step:1592/1920 train_time:90589ms step_avg:56.90ms
step:1593/1920 train_time:90679ms step_avg:56.92ms
step:1594/1920 train_time:90766ms step_avg:56.94ms
step:1595/1920 train_time:90855ms step_avg:56.96ms
step:1596/1920 train_time:90942ms step_avg:56.98ms
step:1597/1920 train_time:91030ms step_avg:57.00ms
step:1598/1920 train_time:91119ms step_avg:57.02ms
step:1599/1920 train_time:91208ms step_avg:57.04ms
step:1600/1920 train_time:91297ms step_avg:57.06ms
step:1601/1920 train_time:91385ms step_avg:57.08ms
step:1602/1920 train_time:91473ms step_avg:57.10ms
step:1603/1920 train_time:91563ms step_avg:57.12ms
step:1604/1920 train_time:91651ms step_avg:57.14ms
step:1605/1920 train_time:91741ms step_avg:57.16ms
step:1606/1920 train_time:91829ms step_avg:57.18ms
step:1607/1920 train_time:91919ms step_avg:57.20ms
step:1608/1920 train_time:92007ms step_avg:57.22ms
step:1609/1920 train_time:92096ms step_avg:57.24ms
step:1610/1920 train_time:92183ms step_avg:57.26ms
step:1611/1920 train_time:92272ms step_avg:57.28ms
step:1612/1920 train_time:92360ms step_avg:57.29ms
step:1613/1920 train_time:92448ms step_avg:57.31ms
step:1614/1920 train_time:92537ms step_avg:57.33ms
step:1615/1920 train_time:92626ms step_avg:57.35ms
step:1616/1920 train_time:92714ms step_avg:57.37ms
step:1617/1920 train_time:92803ms step_avg:57.39ms
step:1618/1920 train_time:92892ms step_avg:57.41ms
step:1619/1920 train_time:92982ms step_avg:57.43ms
step:1620/1920 train_time:93069ms step_avg:57.45ms
step:1621/1920 train_time:93158ms step_avg:57.47ms
step:1622/1920 train_time:93245ms step_avg:57.49ms
step:1623/1920 train_time:93335ms step_avg:57.51ms
step:1624/1920 train_time:93423ms step_avg:57.53ms
step:1625/1920 train_time:93512ms step_avg:57.55ms
step:1626/1920 train_time:93600ms step_avg:57.56ms
step:1627/1920 train_time:93688ms step_avg:57.58ms
step:1628/1920 train_time:93777ms step_avg:57.60ms
step:1629/1920 train_time:93865ms step_avg:57.62ms
step:1630/1920 train_time:93955ms step_avg:57.64ms
step:1631/1920 train_time:94044ms step_avg:57.66ms
step:1632/1920 train_time:94132ms step_avg:57.68ms
step:1633/1920 train_time:94221ms step_avg:57.70ms
step:1634/1920 train_time:94309ms step_avg:57.72ms
step:1635/1920 train_time:94398ms step_avg:57.74ms
step:1636/1920 train_time:94485ms step_avg:57.75ms
step:1637/1920 train_time:94574ms step_avg:57.77ms
step:1638/1920 train_time:94661ms step_avg:57.79ms
step:1639/1920 train_time:94751ms step_avg:57.81ms
step:1640/1920 train_time:94840ms step_avg:57.83ms
step:1641/1920 train_time:94928ms step_avg:57.85ms
step:1642/1920 train_time:95017ms step_avg:57.87ms
step:1643/1920 train_time:95106ms step_avg:57.89ms
step:1644/1920 train_time:95196ms step_avg:57.90ms
step:1645/1920 train_time:95284ms step_avg:57.92ms
step:1646/1920 train_time:95373ms step_avg:57.94ms
step:1647/1920 train_time:95462ms step_avg:57.96ms
step:1648/1920 train_time:95550ms step_avg:57.98ms
step:1649/1920 train_time:95640ms step_avg:58.00ms
step:1650/1920 train_time:95727ms step_avg:58.02ms
step:1651/1920 train_time:95816ms step_avg:58.04ms
step:1652/1920 train_time:95903ms step_avg:58.05ms
step:1653/1920 train_time:95993ms step_avg:58.07ms
step:1654/1920 train_time:96081ms step_avg:58.09ms
step:1655/1920 train_time:96168ms step_avg:58.11ms
step:1656/1920 train_time:96257ms step_avg:58.13ms
step:1657/1920 train_time:96345ms step_avg:58.14ms
step:1658/1920 train_time:96434ms step_avg:58.16ms
step:1659/1920 train_time:96523ms step_avg:58.18ms
step:1660/1920 train_time:96611ms step_avg:58.20ms
step:1661/1920 train_time:96700ms step_avg:58.22ms
step:1662/1920 train_time:96788ms step_avg:58.24ms
step:1663/1920 train_time:96876ms step_avg:58.25ms
step:1664/1920 train_time:96964ms step_avg:58.27ms
step:1665/1920 train_time:97052ms step_avg:58.29ms
step:1666/1920 train_time:97141ms step_avg:58.31ms
step:1667/1920 train_time:97229ms step_avg:58.33ms
step:1668/1920 train_time:97318ms step_avg:58.34ms
step:1669/1920 train_time:97408ms step_avg:58.36ms
step:1670/1920 train_time:97495ms step_avg:58.38ms
step:1671/1920 train_time:97584ms step_avg:58.40ms
step:1672/1920 train_time:97673ms step_avg:58.42ms
step:1673/1920 train_time:97764ms step_avg:58.44ms
step:1674/1920 train_time:97852ms step_avg:58.45ms
step:1675/1920 train_time:97942ms step_avg:58.47ms
step:1676/1920 train_time:98030ms step_avg:58.49ms
step:1677/1920 train_time:98119ms step_avg:58.51ms
step:1678/1920 train_time:98207ms step_avg:58.53ms
step:1679/1920 train_time:98296ms step_avg:58.54ms
step:1680/1920 train_time:98383ms step_avg:58.56ms
step:1681/1920 train_time:98472ms step_avg:58.58ms
step:1682/1920 train_time:98559ms step_avg:58.60ms
step:1683/1920 train_time:98648ms step_avg:58.61ms
step:1684/1920 train_time:98737ms step_avg:58.63ms
step:1685/1920 train_time:98826ms step_avg:58.65ms
step:1686/1920 train_time:98914ms step_avg:58.67ms
step:1687/1920 train_time:99003ms step_avg:58.69ms
step:1688/1920 train_time:99090ms step_avg:58.70ms
step:1689/1920 train_time:99179ms step_avg:58.72ms
step:1690/1920 train_time:99267ms step_avg:58.74ms
step:1691/1920 train_time:99356ms step_avg:58.76ms
step:1692/1920 train_time:99444ms step_avg:58.77ms
step:1693/1920 train_time:99533ms step_avg:58.79ms
step:1694/1920 train_time:99620ms step_avg:58.81ms
step:1695/1920 train_time:99709ms step_avg:58.83ms
step:1696/1920 train_time:99797ms step_avg:58.84ms
step:1697/1920 train_time:99886ms step_avg:58.86ms
step:1698/1920 train_time:99974ms step_avg:58.88ms
step:1699/1920 train_time:100063ms step_avg:58.90ms
step:1700/1920 train_time:100151ms step_avg:58.91ms
step:1701/1920 train_time:100240ms step_avg:58.93ms
step:1702/1920 train_time:100329ms step_avg:58.95ms
step:1703/1920 train_time:100419ms step_avg:58.97ms
step:1704/1920 train_time:100506ms step_avg:58.98ms
step:1705/1920 train_time:100595ms step_avg:59.00ms
step:1706/1920 train_time:100683ms step_avg:59.02ms
step:1707/1920 train_time:100771ms step_avg:59.03ms
step:1708/1920 train_time:100859ms step_avg:59.05ms
step:1709/1920 train_time:100948ms step_avg:59.07ms
step:1710/1920 train_time:101036ms step_avg:59.09ms
step:1711/1920 train_time:101126ms step_avg:59.10ms
step:1712/1920 train_time:101214ms step_avg:59.12ms
step:1713/1920 train_time:101303ms step_avg:59.14ms
step:1714/1920 train_time:101391ms step_avg:59.15ms
step:1715/1920 train_time:101480ms step_avg:59.17ms
step:1716/1920 train_time:101568ms step_avg:59.19ms
step:1717/1920 train_time:101657ms step_avg:59.21ms
step:1718/1920 train_time:101745ms step_avg:59.22ms
step:1719/1920 train_time:101833ms step_avg:59.24ms
step:1720/1920 train_time:101921ms step_avg:59.26ms
step:1721/1920 train_time:102010ms step_avg:59.27ms
step:1722/1920 train_time:102098ms step_avg:59.29ms
step:1723/1920 train_time:102187ms step_avg:59.31ms
step:1724/1920 train_time:102275ms step_avg:59.32ms
step:1725/1920 train_time:102364ms step_avg:59.34ms
step:1726/1920 train_time:102452ms step_avg:59.36ms
step:1727/1920 train_time:102541ms step_avg:59.38ms
step:1728/1920 train_time:102629ms step_avg:59.39ms
step:1729/1920 train_time:102719ms step_avg:59.41ms
step:1730/1920 train_time:102806ms step_avg:59.43ms
step:1731/1920 train_time:102896ms step_avg:59.44ms
step:1732/1920 train_time:102984ms step_avg:59.46ms
step:1733/1920 train_time:103072ms step_avg:59.48ms
step:1734/1920 train_time:103160ms step_avg:59.49ms
step:1735/1920 train_time:103248ms step_avg:59.51ms
step:1736/1920 train_time:103336ms step_avg:59.53ms
step:1737/1920 train_time:103425ms step_avg:59.54ms
step:1738/1920 train_time:103513ms step_avg:59.56ms
step:1739/1920 train_time:103603ms step_avg:59.58ms
step:1740/1920 train_time:103691ms step_avg:59.59ms
step:1741/1920 train_time:103781ms step_avg:59.61ms
step:1742/1920 train_time:103869ms step_avg:59.63ms
step:1743/1920 train_time:103958ms step_avg:59.64ms
step:1744/1920 train_time:104045ms step_avg:59.66ms
step:1745/1920 train_time:104134ms step_avg:59.68ms
step:1746/1920 train_time:104222ms step_avg:59.69ms
step:1747/1920 train_time:104310ms step_avg:59.71ms
step:1748/1920 train_time:104399ms step_avg:59.73ms
step:1749/1920 train_time:104488ms step_avg:59.74ms
step:1750/1920 train_time:104576ms step_avg:59.76ms
step:1750/1920 val_loss:3.3245 train_time:104667ms step_avg:59.81ms
step:1751/1920 train_time:104685ms step_avg:59.79ms
step:1752/1920 train_time:104759ms step_avg:59.79ms
step:1753/1920 train_time:104851ms step_avg:59.81ms
step:1754/1920 train_time:104940ms step_avg:59.83ms
step:1755/1920 train_time:105028ms step_avg:59.84ms
step:1756/1920 train_time:105115ms step_avg:59.86ms
step:1757/1920 train_time:105201ms step_avg:59.88ms
step:1758/1920 train_time:105288ms step_avg:59.89ms
step:1759/1920 train_time:105376ms step_avg:59.91ms
step:1760/1920 train_time:105464ms step_avg:59.92ms
step:1761/1920 train_time:105553ms step_avg:59.94ms
step:1762/1920 train_time:105643ms step_avg:59.96ms
step:1763/1920 train_time:105734ms step_avg:59.97ms
step:1764/1920 train_time:105825ms step_avg:59.99ms
step:1765/1920 train_time:105916ms step_avg:60.01ms
step:1766/1920 train_time:106003ms step_avg:60.02ms
step:1767/1920 train_time:106093ms step_avg:60.04ms
step:1768/1920 train_time:106179ms step_avg:60.06ms
step:1769/1920 train_time:106266ms step_avg:60.07ms
step:1770/1920 train_time:106354ms step_avg:60.09ms
step:1771/1920 train_time:106442ms step_avg:60.10ms
step:1772/1920 train_time:106529ms step_avg:60.12ms
step:1773/1920 train_time:106620ms step_avg:60.14ms
step:1774/1920 train_time:106709ms step_avg:60.15ms
step:1775/1920 train_time:106801ms step_avg:60.17ms
step:1776/1920 train_time:106891ms step_avg:60.19ms
step:1777/1920 train_time:106980ms step_avg:60.20ms
step:1778/1920 train_time:107068ms step_avg:60.22ms
step:1779/1920 train_time:107157ms step_avg:60.23ms
step:1780/1920 train_time:107243ms step_avg:60.25ms
step:1781/1920 train_time:107331ms step_avg:60.26ms
step:1782/1920 train_time:107418ms step_avg:60.28ms
step:1783/1920 train_time:107506ms step_avg:60.30ms
step:1784/1920 train_time:107595ms step_avg:60.31ms
step:1785/1920 train_time:107685ms step_avg:60.33ms
step:1786/1920 train_time:107776ms step_avg:60.34ms
step:1787/1920 train_time:107866ms step_avg:60.36ms
step:1788/1920 train_time:107955ms step_avg:60.38ms
step:1789/1920 train_time:108043ms step_avg:60.39ms
step:1790/1920 train_time:108131ms step_avg:60.41ms
step:1791/1920 train_time:108219ms step_avg:60.42ms
step:1792/1920 train_time:108306ms step_avg:60.44ms
step:1793/1920 train_time:108394ms step_avg:60.45ms
step:1794/1920 train_time:108481ms step_avg:60.47ms
step:1795/1920 train_time:108570ms step_avg:60.48ms
step:1796/1920 train_time:108658ms step_avg:60.50ms
step:1797/1920 train_time:108748ms step_avg:60.52ms
step:1798/1920 train_time:108837ms step_avg:60.53ms
step:1799/1920 train_time:108927ms step_avg:60.55ms
step:1800/1920 train_time:109015ms step_avg:60.56ms
step:1801/1920 train_time:109103ms step_avg:60.58ms
step:1802/1920 train_time:109191ms step_avg:60.59ms
step:1803/1920 train_time:109280ms step_avg:60.61ms
step:1804/1920 train_time:109367ms step_avg:60.62ms
step:1805/1920 train_time:109456ms step_avg:60.64ms
step:1806/1920 train_time:109545ms step_avg:60.66ms
step:1807/1920 train_time:109634ms step_avg:60.67ms
step:1808/1920 train_time:109723ms step_avg:60.69ms
step:1809/1920 train_time:109812ms step_avg:60.70ms
step:1810/1920 train_time:109900ms step_avg:60.72ms
step:1811/1920 train_time:109990ms step_avg:60.73ms
step:1812/1920 train_time:110078ms step_avg:60.75ms
step:1813/1920 train_time:110166ms step_avg:60.76ms
step:1814/1920 train_time:110254ms step_avg:60.78ms
step:1815/1920 train_time:110342ms step_avg:60.79ms
step:1816/1920 train_time:110430ms step_avg:60.81ms
step:1817/1920 train_time:110519ms step_avg:60.82ms
step:1818/1920 train_time:110608ms step_avg:60.84ms
step:1819/1920 train_time:110698ms step_avg:60.86ms
step:1820/1920 train_time:110786ms step_avg:60.87ms
step:1821/1920 train_time:110876ms step_avg:60.89ms
step:1822/1920 train_time:110965ms step_avg:60.90ms
step:1823/1920 train_time:111055ms step_avg:60.92ms
step:1824/1920 train_time:111142ms step_avg:60.93ms
step:1825/1920 train_time:111230ms step_avg:60.95ms
step:1826/1920 train_time:111318ms step_avg:60.96ms
step:1827/1920 train_time:111406ms step_avg:60.98ms
step:1828/1920 train_time:111495ms step_avg:60.99ms
step:1829/1920 train_time:111584ms step_avg:61.01ms
step:1830/1920 train_time:111672ms step_avg:61.02ms
step:1831/1920 train_time:111761ms step_avg:61.04ms
step:1832/1920 train_time:111850ms step_avg:61.05ms
step:1833/1920 train_time:111939ms step_avg:61.07ms
step:1834/1920 train_time:112028ms step_avg:61.08ms
step:1835/1920 train_time:112118ms step_avg:61.10ms
step:1836/1920 train_time:112206ms step_avg:61.11ms
step:1837/1920 train_time:112295ms step_avg:61.13ms
step:1838/1920 train_time:112383ms step_avg:61.14ms
step:1839/1920 train_time:112471ms step_avg:61.16ms
step:1840/1920 train_time:112559ms step_avg:61.17ms
step:1841/1920 train_time:112648ms step_avg:61.19ms
step:1842/1920 train_time:112736ms step_avg:61.20ms
step:1843/1920 train_time:112825ms step_avg:61.22ms
step:1844/1920 train_time:112913ms step_avg:61.23ms
step:1845/1920 train_time:113001ms step_avg:61.25ms
step:1846/1920 train_time:113090ms step_avg:61.26ms
step:1847/1920 train_time:113179ms step_avg:61.28ms
step:1848/1920 train_time:113268ms step_avg:61.29ms
step:1849/1920 train_time:113358ms step_avg:61.31ms
step:1850/1920 train_time:113446ms step_avg:61.32ms
step:1851/1920 train_time:113535ms step_avg:61.34ms
step:1852/1920 train_time:113622ms step_avg:61.35ms
step:1853/1920 train_time:113711ms step_avg:61.37ms
step:1854/1920 train_time:113799ms step_avg:61.38ms
step:1855/1920 train_time:113888ms step_avg:61.40ms
step:1856/1920 train_time:113976ms step_avg:61.41ms
step:1857/1920 train_time:114065ms step_avg:61.42ms
step:1858/1920 train_time:114154ms step_avg:61.44ms
step:1859/1920 train_time:114242ms step_avg:61.45ms
step:1860/1920 train_time:114331ms step_avg:61.47ms
step:1861/1920 train_time:114421ms step_avg:61.48ms
step:1862/1920 train_time:114509ms step_avg:61.50ms
step:1863/1920 train_time:114599ms step_avg:61.51ms
step:1864/1920 train_time:114687ms step_avg:61.53ms
step:1865/1920 train_time:114777ms step_avg:61.54ms
step:1866/1920 train_time:114866ms step_avg:61.56ms
step:1867/1920 train_time:114955ms step_avg:61.57ms
step:1868/1920 train_time:115043ms step_avg:61.59ms
step:1869/1920 train_time:115131ms step_avg:61.60ms
step:1870/1920 train_time:115219ms step_avg:61.61ms
step:1871/1920 train_time:115307ms step_avg:61.63ms
step:1872/1920 train_time:115395ms step_avg:61.64ms
step:1873/1920 train_time:115484ms step_avg:61.66ms
step:1874/1920 train_time:115574ms step_avg:61.67ms
step:1875/1920 train_time:115662ms step_avg:61.69ms
step:1876/1920 train_time:115750ms step_avg:61.70ms
step:1877/1920 train_time:115839ms step_avg:61.71ms
step:1878/1920 train_time:115927ms step_avg:61.73ms
step:1879/1920 train_time:116017ms step_avg:61.74ms
step:1880/1920 train_time:116105ms step_avg:61.76ms
step:1881/1920 train_time:116195ms step_avg:61.77ms
step:1882/1920 train_time:116283ms step_avg:61.79ms
step:1883/1920 train_time:116372ms step_avg:61.80ms
step:1884/1920 train_time:116460ms step_avg:61.82ms
step:1885/1920 train_time:116548ms step_avg:61.83ms
step:1886/1920 train_time:116636ms step_avg:61.84ms
step:1887/1920 train_time:116725ms step_avg:61.86ms
step:1888/1920 train_time:116815ms step_avg:61.87ms
step:1889/1920 train_time:116904ms step_avg:61.89ms
step:1890/1920 train_time:116993ms step_avg:61.90ms
step:1891/1920 train_time:117081ms step_avg:61.92ms
step:1892/1920 train_time:117170ms step_avg:61.93ms
step:1893/1920 train_time:117260ms step_avg:61.94ms
step:1894/1920 train_time:117348ms step_avg:61.96ms
step:1895/1920 train_time:117438ms step_avg:61.97ms
step:1896/1920 train_time:117526ms step_avg:61.99ms
step:1897/1920 train_time:117616ms step_avg:62.00ms
step:1898/1920 train_time:117703ms step_avg:62.01ms
step:1899/1920 train_time:117793ms step_avg:62.03ms
step:1900/1920 train_time:117881ms step_avg:62.04ms
step:1901/1920 train_time:117970ms step_avg:62.06ms
step:1902/1920 train_time:118059ms step_avg:62.07ms
step:1903/1920 train_time:118148ms step_avg:62.09ms
step:1904/1920 train_time:118236ms step_avg:62.10ms
step:1905/1920 train_time:118326ms step_avg:62.11ms
step:1906/1920 train_time:118415ms step_avg:62.13ms
step:1907/1920 train_time:118504ms step_avg:62.14ms
step:1908/1920 train_time:118592ms step_avg:62.16ms
step:1909/1920 train_time:118681ms step_avg:62.17ms
step:1910/1920 train_time:118769ms step_avg:62.18ms
step:1911/1920 train_time:118858ms step_avg:62.20ms
step:1912/1920 train_time:118947ms step_avg:62.21ms
step:1913/1920 train_time:119037ms step_avg:62.23ms
step:1914/1920 train_time:119127ms step_avg:62.24ms
step:1915/1920 train_time:119217ms step_avg:62.25ms
step:1916/1920 train_time:119306ms step_avg:62.27ms
step:1917/1920 train_time:119397ms step_avg:62.28ms
step:1918/1920 train_time:119486ms step_avg:62.30ms
step:1919/1920 train_time:119575ms step_avg:62.31ms
step:1920/1920 train_time:119663ms step_avg:62.32ms
step:1920/1920 val_loss:3.2798 train_time:119754ms step_avg:62.37ms
peak memory allocated: 29976 MiB reserved: 44758 MiB

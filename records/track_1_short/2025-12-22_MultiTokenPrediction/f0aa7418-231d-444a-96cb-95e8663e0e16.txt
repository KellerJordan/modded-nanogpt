import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int, mtp_weights: Tensor = None):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1880  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

# Precompute MTP weights for all steps to avoid tensor allocation during training
# Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
mtp_weights_schedule = []
for s in range(args.num_iterations + 1):
    x = s / args.num_scheduled_iterations
    if x < 1/3:
        w = [1.0, 0.5, 0.25 * (1 - 3*x)]
    elif x < 2/3:
        w = [1.0, 0.5 * (1 - (3*x - 1))]
    else:
        w = [1.0]
    mtp_weights_schedule.append(torch.tensor(w, device=device))

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps

    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0

    if in_transition:
        adam_optimizers[1].transition_steps -= 1

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

base_weights = torch.tensor([1.0, 0.5, 0.25], device=device)
for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    mtp_weights = base_weights[:max(1, 3 - idx)].clone()
    model.split_embed = (idx >= args.split_embed_frac * len(args.ws_schedule))
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        (model(inputs, targets, cum_seqlens, ws_long//2, ws_long, mtp_weights) / grad_accum_steps).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        model.split_embed = (ws_idx >= 1)
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state
model.split_embed = False


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations)
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True
    if step == split_step:
        model.split_embed = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps)
        is_transition = True

    step_batch_size = new_step_batch_size
    mtp_weights = mtp_weights_schedule[step]
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long, mtp_weights) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    # copy lm_head weights/state to embed before split
    if step == (split_step - 2) | 1:
        adam_optimizers[0].copy_lm_to_embed()

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 (main, Dec  9 2025, 19:02:36) [Clang 21.1.4 ]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 22 11:23:31 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            131W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   35C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   44C    P0            133W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   43C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   42C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   35C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    240421      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    240422      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    240423      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    240424      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    240425      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    240426      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    240427      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    240428      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    240422      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    240423      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    240424      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    240425      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    240426      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    240427      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    240428      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1920 val_loss:10.8365 train_time:0ms step_avg:0.04ms
step:1/1920 train_time:68ms step_avg:67.56ms
step:2/1920 train_time:97ms step_avg:48.51ms
step:3/1920 train_time:131ms step_avg:43.76ms
step:4/1920 train_time:165ms step_avg:41.26ms
step:5/1920 train_time:199ms step_avg:39.84ms
step:6/1920 train_time:256ms step_avg:42.71ms
step:7/1920 train_time:303ms step_avg:43.32ms
step:8/1920 train_time:337ms step_avg:42.16ms
step:9/1920 train_time:372ms step_avg:41.29ms
step:10/1920 train_time:406ms step_avg:40.56ms
step:11/1920 train_time:440ms step_avg:39.99ms
step:12/1920 train_time:474ms step_avg:39.50ms
step:13/1920 train_time:508ms step_avg:39.10ms
step:14/1920 train_time:542ms step_avg:38.75ms
step:15/1920 train_time:577ms step_avg:38.47ms
step:16/1920 train_time:611ms step_avg:38.20ms
step:17/1920 train_time:646ms step_avg:37.99ms
step:18/1920 train_time:680ms step_avg:37.79ms
step:19/1920 train_time:714ms step_avg:37.60ms
step:20/1920 train_time:749ms step_avg:37.43ms
step:21/1920 train_time:783ms step_avg:37.29ms
step:22/1920 train_time:817ms step_avg:37.16ms
step:23/1920 train_time:852ms step_avg:37.03ms
step:24/1920 train_time:886ms step_avg:36.91ms
step:25/1920 train_time:920ms step_avg:36.80ms
step:26/1920 train_time:954ms step_avg:36.70ms
step:27/1920 train_time:988ms step_avg:36.61ms
step:28/1920 train_time:1023ms step_avg:36.52ms
step:29/1920 train_time:1057ms step_avg:36.45ms
step:30/1920 train_time:1091ms step_avg:36.37ms
step:31/1920 train_time:1125ms step_avg:36.31ms
step:32/1920 train_time:1160ms step_avg:36.24ms
step:33/1920 train_time:1195ms step_avg:36.21ms
step:34/1920 train_time:1230ms step_avg:36.18ms
step:35/1920 train_time:1266ms step_avg:36.16ms
step:36/1920 train_time:1300ms step_avg:36.11ms
step:37/1920 train_time:1335ms step_avg:36.08ms
step:38/1920 train_time:1369ms step_avg:36.03ms
step:39/1920 train_time:1404ms step_avg:36.00ms
step:40/1920 train_time:1438ms step_avg:35.96ms
step:41/1920 train_time:1472ms step_avg:35.91ms
step:42/1920 train_time:1507ms step_avg:35.88ms
step:43/1920 train_time:1541ms step_avg:35.85ms
step:44/1920 train_time:1576ms step_avg:35.81ms
step:45/1920 train_time:1610ms step_avg:35.78ms
step:46/1920 train_time:1645ms step_avg:35.75ms
step:47/1920 train_time:1679ms step_avg:35.72ms
step:48/1920 train_time:1713ms step_avg:35.69ms
step:49/1920 train_time:1748ms step_avg:35.67ms
step:50/1920 train_time:1782ms step_avg:35.65ms
step:51/1920 train_time:1817ms step_avg:35.62ms
step:52/1920 train_time:1851ms step_avg:35.59ms
step:53/1920 train_time:1885ms step_avg:35.57ms
step:54/1920 train_time:1919ms step_avg:35.54ms
step:55/1920 train_time:1953ms step_avg:35.52ms
step:56/1920 train_time:1987ms step_avg:35.49ms
step:57/1920 train_time:2022ms step_avg:35.47ms
step:58/1920 train_time:2056ms step_avg:35.45ms
step:59/1920 train_time:2091ms step_avg:35.43ms
step:60/1920 train_time:2125ms step_avg:35.41ms
step:61/1920 train_time:2159ms step_avg:35.40ms
step:62/1920 train_time:2193ms step_avg:35.38ms
step:63/1920 train_time:2228ms step_avg:35.36ms
step:64/1920 train_time:2262ms step_avg:35.35ms
step:65/1920 train_time:2297ms step_avg:35.34ms
step:66/1920 train_time:2331ms step_avg:35.32ms
step:67/1920 train_time:2366ms step_avg:35.31ms
step:68/1920 train_time:2400ms step_avg:35.29ms
step:69/1920 train_time:2435ms step_avg:35.28ms
step:70/1920 train_time:2469ms step_avg:35.27ms
step:71/1920 train_time:2504ms step_avg:35.26ms
step:72/1920 train_time:2538ms step_avg:35.25ms
step:73/1920 train_time:2572ms step_avg:35.24ms
step:74/1920 train_time:2607ms step_avg:35.23ms
step:75/1920 train_time:2641ms step_avg:35.22ms
step:76/1920 train_time:2676ms step_avg:35.21ms
step:77/1920 train_time:2710ms step_avg:35.19ms
step:78/1920 train_time:2744ms step_avg:35.18ms
step:79/1920 train_time:2778ms step_avg:35.17ms
step:80/1920 train_time:2813ms step_avg:35.16ms
step:81/1920 train_time:2847ms step_avg:35.15ms
step:82/1920 train_time:2882ms step_avg:35.14ms
step:83/1920 train_time:2916ms step_avg:35.13ms
step:84/1920 train_time:2950ms step_avg:35.12ms
step:85/1920 train_time:2984ms step_avg:35.11ms
step:86/1920 train_time:3019ms step_avg:35.10ms
step:87/1920 train_time:3053ms step_avg:35.09ms
step:88/1920 train_time:3087ms step_avg:35.08ms
step:89/1920 train_time:3122ms step_avg:35.07ms
step:90/1920 train_time:3156ms step_avg:35.06ms
step:91/1920 train_time:3191ms step_avg:35.06ms
step:92/1920 train_time:3225ms step_avg:35.05ms
step:93/1920 train_time:3259ms step_avg:35.05ms
step:94/1920 train_time:3294ms step_avg:35.04ms
step:95/1920 train_time:3328ms step_avg:35.04ms
step:96/1920 train_time:3363ms step_avg:35.03ms
step:97/1920 train_time:3397ms step_avg:35.02ms
step:98/1920 train_time:3432ms step_avg:35.02ms
step:99/1920 train_time:3467ms step_avg:35.02ms
step:100/1920 train_time:3501ms step_avg:35.01ms
step:101/1920 train_time:3536ms step_avg:35.01ms
step:102/1920 train_time:3570ms step_avg:35.00ms
step:103/1920 train_time:3604ms step_avg:34.99ms
step:104/1920 train_time:3639ms step_avg:34.99ms
step:105/1920 train_time:3673ms step_avg:34.98ms
step:106/1920 train_time:3708ms step_avg:34.98ms
step:107/1920 train_time:3742ms step_avg:34.98ms
step:108/1920 train_time:3777ms step_avg:34.97ms
step:109/1920 train_time:3811ms step_avg:34.97ms
step:110/1920 train_time:3846ms step_avg:34.96ms
step:111/1920 train_time:3880ms step_avg:34.96ms
step:112/1920 train_time:3914ms step_avg:34.95ms
step:113/1920 train_time:3949ms step_avg:34.95ms
step:114/1920 train_time:3984ms step_avg:34.95ms
step:115/1920 train_time:4018ms step_avg:34.94ms
step:116/1920 train_time:4052ms step_avg:34.93ms
step:117/1920 train_time:4087ms step_avg:34.93ms
step:118/1920 train_time:4121ms step_avg:34.93ms
step:119/1920 train_time:4156ms step_avg:34.92ms
step:120/1920 train_time:4190ms step_avg:34.92ms
step:121/1920 train_time:4224ms step_avg:34.91ms
step:122/1920 train_time:4259ms step_avg:34.91ms
step:123/1920 train_time:4293ms step_avg:34.90ms
step:124/1920 train_time:4327ms step_avg:34.89ms
step:125/1920 train_time:4362ms step_avg:34.89ms
step:126/1920 train_time:4396ms step_avg:34.89ms
step:127/1920 train_time:4431ms step_avg:34.89ms
step:128/1920 train_time:4465ms step_avg:34.88ms
step:129/1920 train_time:4500ms step_avg:34.88ms
step:130/1920 train_time:4534ms step_avg:34.88ms
step:131/1920 train_time:4569ms step_avg:34.88ms
step:132/1920 train_time:4603ms step_avg:34.87ms
step:133/1920 train_time:4638ms step_avg:34.87ms
step:134/1920 train_time:4672ms step_avg:34.86ms
step:135/1920 train_time:4706ms step_avg:34.86ms
step:136/1920 train_time:4741ms step_avg:34.86ms
step:137/1920 train_time:4775ms step_avg:34.85ms
step:138/1920 train_time:4809ms step_avg:34.85ms
step:139/1920 train_time:4844ms step_avg:34.85ms
step:140/1920 train_time:4878ms step_avg:34.84ms
step:141/1920 train_time:4913ms step_avg:34.84ms
step:142/1920 train_time:4947ms step_avg:34.84ms
step:143/1920 train_time:4982ms step_avg:34.84ms
step:144/1920 train_time:5016ms step_avg:34.83ms
step:145/1920 train_time:5050ms step_avg:34.83ms
step:146/1920 train_time:5085ms step_avg:34.83ms
step:147/1920 train_time:5119ms step_avg:34.83ms
step:148/1920 train_time:5154ms step_avg:34.82ms
step:149/1920 train_time:5188ms step_avg:34.82ms
step:150/1920 train_time:5223ms step_avg:34.82ms
step:151/1920 train_time:5257ms step_avg:34.81ms
step:152/1920 train_time:5291ms step_avg:34.81ms
step:153/1920 train_time:5325ms step_avg:34.80ms
step:154/1920 train_time:5359ms step_avg:34.80ms
step:155/1920 train_time:5394ms step_avg:34.80ms
step:156/1920 train_time:5428ms step_avg:34.80ms
step:157/1920 train_time:5462ms step_avg:34.79ms
step:158/1920 train_time:5497ms step_avg:34.79ms
step:159/1920 train_time:5531ms step_avg:34.79ms
step:160/1920 train_time:5565ms step_avg:34.78ms
step:161/1920 train_time:5600ms step_avg:34.78ms
step:162/1920 train_time:5634ms step_avg:34.78ms
step:163/1920 train_time:5669ms step_avg:34.78ms
step:164/1920 train_time:5703ms step_avg:34.78ms
step:165/1920 train_time:5738ms step_avg:34.77ms
step:166/1920 train_time:5772ms step_avg:34.77ms
step:167/1920 train_time:5807ms step_avg:34.77ms
step:168/1920 train_time:5841ms step_avg:34.77ms
step:169/1920 train_time:5875ms step_avg:34.76ms
step:170/1920 train_time:5910ms step_avg:34.76ms
step:171/1920 train_time:5944ms step_avg:34.76ms
step:172/1920 train_time:5978ms step_avg:34.76ms
step:173/1920 train_time:6013ms step_avg:34.75ms
step:174/1920 train_time:6047ms step_avg:34.75ms
step:175/1920 train_time:6081ms step_avg:34.75ms
step:176/1920 train_time:6115ms step_avg:34.74ms
step:177/1920 train_time:6149ms step_avg:34.74ms
step:178/1920 train_time:6184ms step_avg:34.74ms
step:179/1920 train_time:6218ms step_avg:34.74ms
step:180/1920 train_time:6252ms step_avg:34.73ms
step:181/1920 train_time:6286ms step_avg:34.73ms
step:182/1920 train_time:6321ms step_avg:34.73ms
step:183/1920 train_time:6355ms step_avg:34.73ms
step:184/1920 train_time:6390ms step_avg:34.73ms
step:185/1920 train_time:6424ms step_avg:34.72ms
step:186/1920 train_time:6458ms step_avg:34.72ms
step:187/1920 train_time:6493ms step_avg:34.72ms
step:188/1920 train_time:6527ms step_avg:34.72ms
step:189/1920 train_time:6561ms step_avg:34.72ms
step:190/1920 train_time:6595ms step_avg:34.71ms
step:191/1920 train_time:6630ms step_avg:34.71ms
step:192/1920 train_time:6664ms step_avg:34.71ms
step:193/1920 train_time:6699ms step_avg:34.71ms
step:194/1920 train_time:6733ms step_avg:34.71ms
step:195/1920 train_time:6768ms step_avg:34.71ms
step:196/1920 train_time:6802ms step_avg:34.70ms
step:197/1920 train_time:6837ms step_avg:34.71ms
step:198/1920 train_time:6871ms step_avg:34.70ms
step:199/1920 train_time:6906ms step_avg:34.71ms
step:200/1920 train_time:6941ms step_avg:34.70ms
step:201/1920 train_time:6976ms step_avg:34.70ms
step:202/1920 train_time:7010ms step_avg:34.70ms
step:203/1920 train_time:7044ms step_avg:34.70ms
step:204/1920 train_time:7078ms step_avg:34.70ms
step:205/1920 train_time:7113ms step_avg:34.70ms
step:206/1920 train_time:7148ms step_avg:34.70ms
step:207/1920 train_time:7182ms step_avg:34.70ms
step:208/1920 train_time:7216ms step_avg:34.69ms
step:209/1920 train_time:7251ms step_avg:34.69ms
step:210/1920 train_time:7285ms step_avg:34.69ms
step:211/1920 train_time:7320ms step_avg:34.69ms
step:212/1920 train_time:7353ms step_avg:34.69ms
step:213/1920 train_time:7388ms step_avg:34.69ms
step:214/1920 train_time:7422ms step_avg:34.68ms
step:215/1920 train_time:7457ms step_avg:34.68ms
step:216/1920 train_time:7491ms step_avg:34.68ms
step:217/1920 train_time:7525ms step_avg:34.68ms
step:218/1920 train_time:7559ms step_avg:34.68ms
step:219/1920 train_time:7594ms step_avg:34.67ms
step:220/1920 train_time:7628ms step_avg:34.67ms
step:221/1920 train_time:7662ms step_avg:34.67ms
step:222/1920 train_time:7697ms step_avg:34.67ms
step:223/1920 train_time:7731ms step_avg:34.67ms
step:224/1920 train_time:7765ms step_avg:34.66ms
step:225/1920 train_time:7799ms step_avg:34.66ms
step:226/1920 train_time:7833ms step_avg:34.66ms
step:227/1920 train_time:7868ms step_avg:34.66ms
step:228/1920 train_time:7902ms step_avg:34.66ms
step:229/1920 train_time:7937ms step_avg:34.66ms
step:230/1920 train_time:7971ms step_avg:34.66ms
step:231/1920 train_time:8006ms step_avg:34.66ms
step:232/1920 train_time:8040ms step_avg:34.66ms
step:233/1920 train_time:8075ms step_avg:34.65ms
step:234/1920 train_time:8109ms step_avg:34.65ms
step:235/1920 train_time:8143ms step_avg:34.65ms
step:236/1920 train_time:8177ms step_avg:34.65ms
step:237/1920 train_time:8212ms step_avg:34.65ms
step:238/1920 train_time:8246ms step_avg:34.65ms
step:239/1920 train_time:8280ms step_avg:34.64ms
step:240/1920 train_time:8314ms step_avg:34.64ms
step:241/1920 train_time:8349ms step_avg:34.64ms
step:242/1920 train_time:8384ms step_avg:34.64ms
step:243/1920 train_time:8418ms step_avg:34.64ms
step:244/1920 train_time:8452ms step_avg:34.64ms
step:245/1920 train_time:8487ms step_avg:34.64ms
step:246/1920 train_time:8521ms step_avg:34.64ms
step:247/1920 train_time:8555ms step_avg:34.64ms
step:248/1920 train_time:8590ms step_avg:34.64ms
step:249/1920 train_time:8624ms step_avg:34.63ms
step:250/1920 train_time:8658ms step_avg:34.63ms
step:250/1920 val_loss:4.6003 train_time:8695ms step_avg:34.78ms
step:251/1920 train_time:8712ms step_avg:34.71ms
step:252/1920 train_time:8730ms step_avg:34.64ms
step:253/1920 train_time:8764ms step_avg:34.64ms
step:254/1920 train_time:8799ms step_avg:34.64ms
step:255/1920 train_time:8835ms step_avg:34.65ms
step:256/1920 train_time:8870ms step_avg:34.65ms
step:257/1920 train_time:8904ms step_avg:34.65ms
step:258/1920 train_time:8939ms step_avg:34.65ms
step:259/1920 train_time:8973ms step_avg:34.64ms
step:260/1920 train_time:9007ms step_avg:34.64ms
step:261/1920 train_time:9042ms step_avg:34.64ms
step:262/1920 train_time:9076ms step_avg:34.64ms
step:263/1920 train_time:9110ms step_avg:34.64ms
step:264/1920 train_time:9144ms step_avg:34.64ms
step:265/1920 train_time:9178ms step_avg:34.63ms
step:266/1920 train_time:9212ms step_avg:34.63ms
step:267/1920 train_time:9246ms step_avg:34.63ms
step:268/1920 train_time:9280ms step_avg:34.63ms
step:269/1920 train_time:9314ms step_avg:34.63ms
step:270/1920 train_time:9348ms step_avg:34.62ms
step:271/1920 train_time:9382ms step_avg:34.62ms
step:272/1920 train_time:9416ms step_avg:34.62ms
step:273/1920 train_time:9450ms step_avg:34.62ms
step:274/1920 train_time:9484ms step_avg:34.61ms
step:275/1920 train_time:9518ms step_avg:34.61ms
step:276/1920 train_time:9552ms step_avg:34.61ms
step:277/1920 train_time:9587ms step_avg:34.61ms
step:278/1920 train_time:9621ms step_avg:34.61ms
step:279/1920 train_time:9656ms step_avg:34.61ms
step:280/1920 train_time:9690ms step_avg:34.61ms
step:281/1920 train_time:9725ms step_avg:34.61ms
step:282/1920 train_time:9760ms step_avg:34.61ms
step:283/1920 train_time:9794ms step_avg:34.61ms
step:284/1920 train_time:9828ms step_avg:34.61ms
step:285/1920 train_time:9863ms step_avg:34.61ms
step:286/1920 train_time:9897ms step_avg:34.61ms
step:287/1920 train_time:9932ms step_avg:34.61ms
step:288/1920 train_time:9966ms step_avg:34.60ms
step:289/1920 train_time:10000ms step_avg:34.60ms
step:290/1920 train_time:10034ms step_avg:34.60ms
step:291/1920 train_time:10069ms step_avg:34.60ms
step:292/1920 train_time:10103ms step_avg:34.60ms
step:293/1920 train_time:10138ms step_avg:34.60ms
step:294/1920 train_time:10172ms step_avg:34.60ms
step:295/1920 train_time:10206ms step_avg:34.60ms
step:296/1920 train_time:10240ms step_avg:34.60ms
step:297/1920 train_time:10275ms step_avg:34.59ms
step:298/1920 train_time:10309ms step_avg:34.59ms
step:299/1920 train_time:10343ms step_avg:34.59ms
step:300/1920 train_time:10377ms step_avg:34.59ms
step:301/1920 train_time:10412ms step_avg:34.59ms
step:302/1920 train_time:10446ms step_avg:34.59ms
step:303/1920 train_time:10480ms step_avg:34.59ms
step:304/1920 train_time:10514ms step_avg:34.59ms
step:305/1920 train_time:10548ms step_avg:34.58ms
step:306/1920 train_time:10582ms step_avg:34.58ms
step:307/1920 train_time:10617ms step_avg:34.58ms
step:308/1920 train_time:10651ms step_avg:34.58ms
step:309/1920 train_time:10685ms step_avg:34.58ms
step:310/1920 train_time:10720ms step_avg:34.58ms
step:311/1920 train_time:10754ms step_avg:34.58ms
step:312/1920 train_time:10788ms step_avg:34.58ms
step:313/1920 train_time:10822ms step_avg:34.58ms
step:314/1920 train_time:10857ms step_avg:34.58ms
step:315/1920 train_time:10891ms step_avg:34.57ms
step:316/1920 train_time:10925ms step_avg:34.57ms
step:317/1920 train_time:10960ms step_avg:34.57ms
step:318/1920 train_time:10994ms step_avg:34.57ms
step:319/1920 train_time:11029ms step_avg:34.57ms
step:320/1920 train_time:11063ms step_avg:34.57ms
step:321/1920 train_time:11097ms step_avg:34.57ms
step:322/1920 train_time:11131ms step_avg:34.57ms
step:323/1920 train_time:11166ms step_avg:34.57ms
step:324/1920 train_time:11201ms step_avg:34.57ms
step:325/1920 train_time:11235ms step_avg:34.57ms
step:326/1920 train_time:11269ms step_avg:34.57ms
step:327/1920 train_time:11304ms step_avg:34.57ms
step:328/1920 train_time:11338ms step_avg:34.57ms
step:329/1920 train_time:11372ms step_avg:34.57ms
step:330/1920 train_time:11406ms step_avg:34.56ms
step:331/1920 train_time:11440ms step_avg:34.56ms
step:332/1920 train_time:11475ms step_avg:34.56ms
step:333/1920 train_time:11509ms step_avg:34.56ms
step:334/1920 train_time:11543ms step_avg:34.56ms
step:335/1920 train_time:11577ms step_avg:34.56ms
step:336/1920 train_time:11612ms step_avg:34.56ms
step:337/1920 train_time:11646ms step_avg:34.56ms
step:338/1920 train_time:11680ms step_avg:34.56ms
step:339/1920 train_time:11714ms step_avg:34.55ms
step:340/1920 train_time:11748ms step_avg:34.55ms
step:341/1920 train_time:11782ms step_avg:34.55ms
step:342/1920 train_time:11816ms step_avg:34.55ms
step:343/1920 train_time:11851ms step_avg:34.55ms
step:344/1920 train_time:11885ms step_avg:34.55ms
step:345/1920 train_time:11919ms step_avg:34.55ms
step:346/1920 train_time:11954ms step_avg:34.55ms
step:347/1920 train_time:11988ms step_avg:34.55ms
step:348/1920 train_time:12022ms step_avg:34.55ms
step:349/1920 train_time:12057ms step_avg:34.55ms
step:350/1920 train_time:12091ms step_avg:34.55ms
step:351/1920 train_time:12126ms step_avg:34.55ms
step:352/1920 train_time:12160ms step_avg:34.55ms
step:353/1920 train_time:12194ms step_avg:34.54ms
step:354/1920 train_time:12228ms step_avg:34.54ms
step:355/1920 train_time:12263ms step_avg:34.54ms
step:356/1920 train_time:12296ms step_avg:34.54ms
step:357/1920 train_time:12331ms step_avg:34.54ms
step:358/1920 train_time:12365ms step_avg:34.54ms
step:359/1920 train_time:12399ms step_avg:34.54ms
step:360/1920 train_time:12433ms step_avg:34.54ms
step:361/1920 train_time:12468ms step_avg:34.54ms
step:362/1920 train_time:12502ms step_avg:34.54ms
step:363/1920 train_time:12536ms step_avg:34.54ms
step:364/1920 train_time:12571ms step_avg:34.53ms
step:365/1920 train_time:12605ms step_avg:34.53ms
step:366/1920 train_time:12639ms step_avg:34.53ms
step:367/1920 train_time:12674ms step_avg:34.53ms
step:368/1920 train_time:12708ms step_avg:34.53ms
step:369/1920 train_time:12742ms step_avg:34.53ms
step:370/1920 train_time:12776ms step_avg:34.53ms
step:371/1920 train_time:12811ms step_avg:34.53ms
step:372/1920 train_time:12845ms step_avg:34.53ms
step:373/1920 train_time:12879ms step_avg:34.53ms
step:374/1920 train_time:12913ms step_avg:34.53ms
step:375/1920 train_time:12947ms step_avg:34.53ms
step:376/1920 train_time:12981ms step_avg:34.52ms
step:377/1920 train_time:13016ms step_avg:34.52ms
step:378/1920 train_time:13050ms step_avg:34.52ms
step:379/1920 train_time:13085ms step_avg:34.52ms
step:380/1920 train_time:13119ms step_avg:34.52ms
step:381/1920 train_time:13153ms step_avg:34.52ms
step:382/1920 train_time:13187ms step_avg:34.52ms
step:383/1920 train_time:13222ms step_avg:34.52ms
step:384/1920 train_time:13256ms step_avg:34.52ms
step:385/1920 train_time:13290ms step_avg:34.52ms
step:386/1920 train_time:13325ms step_avg:34.52ms
step:387/1920 train_time:13359ms step_avg:34.52ms
step:388/1920 train_time:13394ms step_avg:34.52ms
step:389/1920 train_time:13428ms step_avg:34.52ms
step:390/1920 train_time:13462ms step_avg:34.52ms
step:391/1920 train_time:13496ms step_avg:34.52ms
step:392/1920 train_time:13530ms step_avg:34.52ms
step:393/1920 train_time:13564ms step_avg:34.51ms
step:394/1920 train_time:13599ms step_avg:34.51ms
step:395/1920 train_time:13633ms step_avg:34.51ms
step:396/1920 train_time:13667ms step_avg:34.51ms
step:397/1920 train_time:13701ms step_avg:34.51ms
step:398/1920 train_time:13735ms step_avg:34.51ms
step:399/1920 train_time:13769ms step_avg:34.51ms
step:400/1920 train_time:13804ms step_avg:34.51ms
step:401/1920 train_time:13838ms step_avg:34.51ms
step:402/1920 train_time:13872ms step_avg:34.51ms
step:403/1920 train_time:13906ms step_avg:34.51ms
step:404/1920 train_time:13941ms step_avg:34.51ms
step:405/1920 train_time:13974ms step_avg:34.50ms
step:406/1920 train_time:14009ms step_avg:34.50ms
step:407/1920 train_time:14043ms step_avg:34.50ms
step:408/1920 train_time:14077ms step_avg:34.50ms
step:409/1920 train_time:14111ms step_avg:34.50ms
step:410/1920 train_time:14146ms step_avg:34.50ms
step:411/1920 train_time:14180ms step_avg:34.50ms
step:412/1920 train_time:14214ms step_avg:34.50ms
step:413/1920 train_time:14248ms step_avg:34.50ms
step:414/1920 train_time:14283ms step_avg:34.50ms
step:415/1920 train_time:14317ms step_avg:34.50ms
step:416/1920 train_time:14351ms step_avg:34.50ms
step:417/1920 train_time:14385ms step_avg:34.50ms
step:418/1920 train_time:14420ms step_avg:34.50ms
step:419/1920 train_time:14454ms step_avg:34.50ms
step:420/1920 train_time:14488ms step_avg:34.50ms
step:421/1920 train_time:14522ms step_avg:34.49ms
step:422/1920 train_time:14557ms step_avg:34.49ms
step:423/1920 train_time:14590ms step_avg:34.49ms
step:424/1920 train_time:14625ms step_avg:34.49ms
step:425/1920 train_time:14659ms step_avg:34.49ms
step:426/1920 train_time:14693ms step_avg:34.49ms
step:427/1920 train_time:14727ms step_avg:34.49ms
step:428/1920 train_time:14762ms step_avg:34.49ms
step:429/1920 train_time:14796ms step_avg:34.49ms
step:430/1920 train_time:14830ms step_avg:34.49ms
step:431/1920 train_time:14864ms step_avg:34.49ms
step:432/1920 train_time:14898ms step_avg:34.49ms
step:433/1920 train_time:14933ms step_avg:34.49ms
step:434/1920 train_time:14967ms step_avg:34.49ms
step:435/1920 train_time:15001ms step_avg:34.49ms
step:436/1920 train_time:15036ms step_avg:34.49ms
step:437/1920 train_time:15071ms step_avg:34.49ms
step:438/1920 train_time:15105ms step_avg:34.49ms
step:439/1920 train_time:15139ms step_avg:34.49ms
step:440/1920 train_time:15173ms step_avg:34.48ms
step:441/1920 train_time:15208ms step_avg:34.48ms
step:442/1920 train_time:15242ms step_avg:34.48ms
step:443/1920 train_time:15276ms step_avg:34.48ms
step:444/1920 train_time:15310ms step_avg:34.48ms
step:445/1920 train_time:15345ms step_avg:34.48ms
step:446/1920 train_time:15379ms step_avg:34.48ms
step:447/1920 train_time:15413ms step_avg:34.48ms
step:448/1920 train_time:15447ms step_avg:34.48ms
step:449/1920 train_time:15481ms step_avg:34.48ms
step:450/1920 train_time:15515ms step_avg:34.48ms
step:451/1920 train_time:15550ms step_avg:34.48ms
step:452/1920 train_time:15585ms step_avg:34.48ms
step:453/1920 train_time:15619ms step_avg:34.48ms
step:454/1920 train_time:15653ms step_avg:34.48ms
step:455/1920 train_time:15687ms step_avg:34.48ms
step:456/1920 train_time:15721ms step_avg:34.48ms
step:457/1920 train_time:15755ms step_avg:34.48ms
step:458/1920 train_time:15790ms step_avg:34.48ms
step:459/1920 train_time:15824ms step_avg:34.48ms
step:460/1920 train_time:15858ms step_avg:34.47ms
step:461/1920 train_time:15892ms step_avg:34.47ms
step:462/1920 train_time:15927ms step_avg:34.47ms
step:463/1920 train_time:15961ms step_avg:34.47ms
step:464/1920 train_time:15995ms step_avg:34.47ms
step:465/1920 train_time:16029ms step_avg:34.47ms
step:466/1920 train_time:16063ms step_avg:34.47ms
step:467/1920 train_time:16098ms step_avg:34.47ms
step:468/1920 train_time:16133ms step_avg:34.47ms
step:469/1920 train_time:16167ms step_avg:34.47ms
step:470/1920 train_time:16202ms step_avg:34.47ms
step:471/1920 train_time:16236ms step_avg:34.47ms
step:472/1920 train_time:16270ms step_avg:34.47ms
step:473/1920 train_time:16305ms step_avg:34.47ms
step:474/1920 train_time:16339ms step_avg:34.47ms
step:475/1920 train_time:16373ms step_avg:34.47ms
step:476/1920 train_time:16407ms step_avg:34.47ms
step:477/1920 train_time:16442ms step_avg:34.47ms
step:478/1920 train_time:16476ms step_avg:34.47ms
step:479/1920 train_time:16511ms step_avg:34.47ms
step:480/1920 train_time:16545ms step_avg:34.47ms
step:481/1920 train_time:16579ms step_avg:34.47ms
step:482/1920 train_time:16613ms step_avg:34.47ms
step:483/1920 train_time:16647ms step_avg:34.47ms
step:484/1920 train_time:16682ms step_avg:34.47ms
step:485/1920 train_time:16716ms step_avg:34.47ms
step:486/1920 train_time:16751ms step_avg:34.47ms
step:487/1920 train_time:16784ms step_avg:34.47ms
step:488/1920 train_time:16819ms step_avg:34.46ms
step:489/1920 train_time:16853ms step_avg:34.46ms
step:490/1920 train_time:16887ms step_avg:34.46ms
step:491/1920 train_time:16921ms step_avg:34.46ms
step:492/1920 train_time:16955ms step_avg:34.46ms
step:493/1920 train_time:16989ms step_avg:34.46ms
step:494/1920 train_time:17024ms step_avg:34.46ms
step:495/1920 train_time:17058ms step_avg:34.46ms
step:496/1920 train_time:17092ms step_avg:34.46ms
step:497/1920 train_time:17126ms step_avg:34.46ms
step:498/1920 train_time:17161ms step_avg:34.46ms
step:499/1920 train_time:17195ms step_avg:34.46ms
step:500/1920 train_time:17229ms step_avg:34.46ms
step:500/1920 val_loss:4.2991 train_time:17266ms step_avg:34.53ms
step:501/1920 train_time:17284ms step_avg:34.50ms
step:502/1920 train_time:17302ms step_avg:34.47ms
step:503/1920 train_time:17335ms step_avg:34.46ms
step:504/1920 train_time:17370ms step_avg:34.46ms
step:505/1920 train_time:17406ms step_avg:34.47ms
step:506/1920 train_time:17440ms step_avg:34.47ms
step:507/1920 train_time:17475ms step_avg:34.47ms
step:508/1920 train_time:17509ms step_avg:34.47ms
step:509/1920 train_time:17543ms step_avg:34.47ms
step:510/1920 train_time:17577ms step_avg:34.46ms
step:511/1920 train_time:17612ms step_avg:34.46ms
step:512/1920 train_time:17646ms step_avg:34.46ms
step:513/1920 train_time:17680ms step_avg:34.46ms
step:514/1920 train_time:17714ms step_avg:34.46ms
step:515/1920 train_time:17748ms step_avg:34.46ms
step:516/1920 train_time:17782ms step_avg:34.46ms
step:517/1920 train_time:17816ms step_avg:34.46ms
step:518/1920 train_time:17850ms step_avg:34.46ms
step:519/1920 train_time:17884ms step_avg:34.46ms
step:520/1920 train_time:17918ms step_avg:34.46ms
step:521/1920 train_time:17953ms step_avg:34.46ms
step:522/1920 train_time:17987ms step_avg:34.46ms
step:523/1920 train_time:18021ms step_avg:34.46ms
step:524/1920 train_time:18055ms step_avg:34.46ms
step:525/1920 train_time:18089ms step_avg:34.46ms
step:526/1920 train_time:18123ms step_avg:34.45ms
step:527/1920 train_time:18157ms step_avg:34.45ms
step:528/1920 train_time:18191ms step_avg:34.45ms
step:529/1920 train_time:18225ms step_avg:34.45ms
step:530/1920 train_time:18260ms step_avg:34.45ms
step:531/1920 train_time:18294ms step_avg:34.45ms
step:532/1920 train_time:18329ms step_avg:34.45ms
step:533/1920 train_time:18363ms step_avg:34.45ms
step:534/1920 train_time:18397ms step_avg:34.45ms
step:535/1920 train_time:18432ms step_avg:34.45ms
step:536/1920 train_time:18467ms step_avg:34.45ms
step:537/1920 train_time:18501ms step_avg:34.45ms
step:538/1920 train_time:18535ms step_avg:34.45ms
step:539/1920 train_time:18570ms step_avg:34.45ms
step:540/1920 train_time:18604ms step_avg:34.45ms
step:541/1920 train_time:18639ms step_avg:34.45ms
step:542/1920 train_time:18673ms step_avg:34.45ms
step:543/1920 train_time:18707ms step_avg:34.45ms
step:544/1920 train_time:18741ms step_avg:34.45ms
step:545/1920 train_time:18775ms step_avg:34.45ms
step:546/1920 train_time:18810ms step_avg:34.45ms
step:547/1920 train_time:18844ms step_avg:34.45ms
step:548/1920 train_time:18878ms step_avg:34.45ms
step:549/1920 train_time:18912ms step_avg:34.45ms
step:550/1920 train_time:18946ms step_avg:34.45ms
step:551/1920 train_time:18980ms step_avg:34.45ms
step:552/1920 train_time:19014ms step_avg:34.45ms
step:553/1920 train_time:19048ms step_avg:34.44ms
step:554/1920 train_time:19082ms step_avg:34.44ms
step:555/1920 train_time:19116ms step_avg:34.44ms
step:556/1920 train_time:19150ms step_avg:34.44ms
step:557/1920 train_time:19184ms step_avg:34.44ms
step:558/1920 train_time:19218ms step_avg:34.44ms
step:559/1920 train_time:19253ms step_avg:34.44ms
step:560/1920 train_time:19287ms step_avg:34.44ms
step:561/1920 train_time:19321ms step_avg:34.44ms
step:562/1920 train_time:19355ms step_avg:34.44ms
step:563/1920 train_time:19390ms step_avg:34.44ms
step:564/1920 train_time:19425ms step_avg:34.44ms
step:565/1920 train_time:19459ms step_avg:34.44ms
step:566/1920 train_time:19493ms step_avg:34.44ms
step:567/1920 train_time:19527ms step_avg:34.44ms
step:568/1920 train_time:19562ms step_avg:34.44ms
step:569/1920 train_time:19596ms step_avg:34.44ms
step:570/1920 train_time:19630ms step_avg:34.44ms
step:571/1920 train_time:19665ms step_avg:34.44ms
step:572/1920 train_time:19699ms step_avg:34.44ms
step:573/1920 train_time:19733ms step_avg:34.44ms
step:574/1920 train_time:19768ms step_avg:34.44ms
step:575/1920 train_time:19802ms step_avg:34.44ms
step:576/1920 train_time:19836ms step_avg:34.44ms
step:577/1920 train_time:19871ms step_avg:34.44ms
step:578/1920 train_time:19905ms step_avg:34.44ms
step:579/1920 train_time:19939ms step_avg:34.44ms
step:580/1920 train_time:19973ms step_avg:34.44ms
step:581/1920 train_time:20007ms step_avg:34.44ms
step:582/1920 train_time:20042ms step_avg:34.44ms
step:583/1920 train_time:20076ms step_avg:34.44ms
step:584/1920 train_time:20110ms step_avg:34.43ms
step:585/1920 train_time:20144ms step_avg:34.43ms
step:586/1920 train_time:20178ms step_avg:34.43ms
step:587/1920 train_time:20213ms step_avg:34.43ms
step:588/1920 train_time:20247ms step_avg:34.43ms
step:589/1920 train_time:20281ms step_avg:34.43ms
step:590/1920 train_time:20315ms step_avg:34.43ms
step:591/1920 train_time:20350ms step_avg:34.43ms
step:592/1920 train_time:20384ms step_avg:34.43ms
step:593/1920 train_time:20418ms step_avg:34.43ms
step:594/1920 train_time:20453ms step_avg:34.43ms
step:595/1920 train_time:20487ms step_avg:34.43ms
step:596/1920 train_time:20521ms step_avg:34.43ms
step:597/1920 train_time:20555ms step_avg:34.43ms
step:598/1920 train_time:20590ms step_avg:34.43ms
step:599/1920 train_time:20624ms step_avg:34.43ms
step:600/1920 train_time:20658ms step_avg:34.43ms
step:601/1920 train_time:20692ms step_avg:34.43ms
step:602/1920 train_time:20727ms step_avg:34.43ms
step:603/1920 train_time:20761ms step_avg:34.43ms
step:604/1920 train_time:20795ms step_avg:34.43ms
step:605/1920 train_time:20829ms step_avg:34.43ms
step:606/1920 train_time:20864ms step_avg:34.43ms
step:607/1920 train_time:20898ms step_avg:34.43ms
step:608/1920 train_time:20932ms step_avg:34.43ms
step:609/1920 train_time:20966ms step_avg:34.43ms
step:610/1920 train_time:21000ms step_avg:34.43ms
step:611/1920 train_time:21034ms step_avg:34.43ms
step:612/1920 train_time:21069ms step_avg:34.43ms
step:613/1920 train_time:21103ms step_avg:34.43ms
step:614/1920 train_time:21137ms step_avg:34.42ms
step:615/1920 train_time:21171ms step_avg:34.42ms
step:616/1920 train_time:21205ms step_avg:34.42ms
step:617/1920 train_time:21240ms step_avg:34.42ms
step:618/1920 train_time:21274ms step_avg:34.42ms
step:619/1920 train_time:21308ms step_avg:34.42ms
step:620/1920 train_time:21342ms step_avg:34.42ms
step:621/1920 train_time:21377ms step_avg:34.42ms
step:622/1920 train_time:21411ms step_avg:34.42ms
step:623/1920 train_time:21445ms step_avg:34.42ms
step:624/1920 train_time:21480ms step_avg:34.42ms
step:625/1920 train_time:21514ms step_avg:34.42ms
step:626/1920 train_time:21548ms step_avg:34.42ms
step:627/1920 train_time:21582ms step_avg:34.42ms
step:628/1920 train_time:21617ms step_avg:34.42ms
step:629/1920 train_time:21680ms step_avg:34.47ms
step:630/1920 train_time:21741ms step_avg:34.51ms
step:631/1920 train_time:21804ms step_avg:34.55ms
step:632/1920 train_time:21865ms step_avg:34.60ms
step:633/1920 train_time:21928ms step_avg:34.64ms
step:634/1920 train_time:21990ms step_avg:34.68ms
step:635/1920 train_time:22052ms step_avg:34.73ms
step:636/1920 train_time:22114ms step_avg:34.77ms
step:637/1920 train_time:22176ms step_avg:34.81ms
step:638/1920 train_time:22238ms step_avg:34.86ms
step:639/1920 train_time:22301ms step_avg:34.90ms
step:640/1920 train_time:22363ms step_avg:34.94ms
step:641/1920 train_time:22425ms step_avg:34.98ms
step:642/1920 train_time:22487ms step_avg:35.03ms
step:643/1920 train_time:22550ms step_avg:35.07ms
step:644/1920 train_time:22611ms step_avg:35.11ms
step:645/1920 train_time:22674ms step_avg:35.15ms
step:646/1920 train_time:22736ms step_avg:35.20ms
step:647/1920 train_time:22799ms step_avg:35.24ms
step:648/1920 train_time:22861ms step_avg:35.28ms
step:649/1920 train_time:22923ms step_avg:35.32ms
step:650/1920 train_time:22984ms step_avg:35.36ms
step:651/1920 train_time:23047ms step_avg:35.40ms
step:652/1920 train_time:23109ms step_avg:35.44ms
step:653/1920 train_time:23171ms step_avg:35.48ms
step:654/1920 train_time:23233ms step_avg:35.52ms
step:655/1920 train_time:23296ms step_avg:35.57ms
step:656/1920 train_time:23358ms step_avg:35.61ms
step:657/1920 train_time:23421ms step_avg:35.65ms
step:658/1920 train_time:23483ms step_avg:35.69ms
step:659/1920 train_time:23545ms step_avg:35.73ms
step:660/1920 train_time:23607ms step_avg:35.77ms
step:661/1920 train_time:23669ms step_avg:35.81ms
step:662/1920 train_time:23731ms step_avg:35.85ms
step:663/1920 train_time:23793ms step_avg:35.89ms
step:664/1920 train_time:23854ms step_avg:35.93ms
step:665/1920 train_time:23917ms step_avg:35.97ms
step:666/1920 train_time:23979ms step_avg:36.00ms
step:667/1920 train_time:24043ms step_avg:36.05ms
step:668/1920 train_time:24105ms step_avg:36.08ms
step:669/1920 train_time:24167ms step_avg:36.12ms
step:670/1920 train_time:24229ms step_avg:36.16ms
step:671/1920 train_time:24292ms step_avg:36.20ms
step:672/1920 train_time:24354ms step_avg:36.24ms
step:673/1920 train_time:24416ms step_avg:36.28ms
step:674/1920 train_time:24477ms step_avg:36.32ms
step:675/1920 train_time:24540ms step_avg:36.36ms
step:676/1920 train_time:24602ms step_avg:36.39ms
step:677/1920 train_time:24665ms step_avg:36.43ms
step:678/1920 train_time:24726ms step_avg:36.47ms
step:679/1920 train_time:24789ms step_avg:36.51ms
step:680/1920 train_time:24851ms step_avg:36.55ms
step:681/1920 train_time:24913ms step_avg:36.58ms
step:682/1920 train_time:24975ms step_avg:36.62ms
step:683/1920 train_time:25037ms step_avg:36.66ms
step:684/1920 train_time:25099ms step_avg:36.69ms
step:685/1920 train_time:25162ms step_avg:36.73ms
step:686/1920 train_time:25225ms step_avg:36.77ms
step:687/1920 train_time:25287ms step_avg:36.81ms
step:688/1920 train_time:25349ms step_avg:36.84ms
step:689/1920 train_time:25411ms step_avg:36.88ms
step:690/1920 train_time:25473ms step_avg:36.92ms
step:691/1920 train_time:25535ms step_avg:36.95ms
step:692/1920 train_time:25598ms step_avg:36.99ms
step:693/1920 train_time:25661ms step_avg:37.03ms
step:694/1920 train_time:25723ms step_avg:37.06ms
step:695/1920 train_time:25785ms step_avg:37.10ms
step:696/1920 train_time:25847ms step_avg:37.14ms
step:697/1920 train_time:25910ms step_avg:37.17ms
step:698/1920 train_time:25972ms step_avg:37.21ms
step:699/1920 train_time:26035ms step_avg:37.25ms
step:700/1920 train_time:26097ms step_avg:37.28ms
step:701/1920 train_time:26159ms step_avg:37.32ms
step:702/1920 train_time:26221ms step_avg:37.35ms
step:703/1920 train_time:26284ms step_avg:37.39ms
step:704/1920 train_time:26346ms step_avg:37.42ms
step:705/1920 train_time:26408ms step_avg:37.46ms
step:706/1920 train_time:26470ms step_avg:37.49ms
step:707/1920 train_time:26533ms step_avg:37.53ms
step:708/1920 train_time:26594ms step_avg:37.56ms
step:709/1920 train_time:26656ms step_avg:37.60ms
step:710/1920 train_time:26718ms step_avg:37.63ms
step:711/1920 train_time:26780ms step_avg:37.67ms
step:712/1920 train_time:26842ms step_avg:37.70ms
step:713/1920 train_time:26905ms step_avg:37.74ms
step:714/1920 train_time:26967ms step_avg:37.77ms
step:715/1920 train_time:27030ms step_avg:37.80ms
step:716/1920 train_time:27092ms step_avg:37.84ms
step:717/1920 train_time:27156ms step_avg:37.87ms
step:718/1920 train_time:27217ms step_avg:37.91ms
step:719/1920 train_time:27280ms step_avg:37.94ms
step:720/1920 train_time:27342ms step_avg:37.97ms
step:721/1920 train_time:27404ms step_avg:38.01ms
step:722/1920 train_time:27466ms step_avg:38.04ms
step:723/1920 train_time:27529ms step_avg:38.08ms
step:724/1920 train_time:27590ms step_avg:38.11ms
step:725/1920 train_time:27653ms step_avg:38.14ms
step:726/1920 train_time:27714ms step_avg:38.17ms
step:727/1920 train_time:27776ms step_avg:38.21ms
step:728/1920 train_time:27838ms step_avg:38.24ms
step:729/1920 train_time:27901ms step_avg:38.27ms
step:730/1920 train_time:27963ms step_avg:38.30ms
step:731/1920 train_time:28025ms step_avg:38.34ms
step:732/1920 train_time:28087ms step_avg:38.37ms
step:733/1920 train_time:28150ms step_avg:38.40ms
step:734/1920 train_time:28212ms step_avg:38.44ms
step:735/1920 train_time:28275ms step_avg:38.47ms
step:736/1920 train_time:28338ms step_avg:38.50ms
step:737/1920 train_time:28400ms step_avg:38.53ms
step:738/1920 train_time:28462ms step_avg:38.57ms
step:739/1920 train_time:28525ms step_avg:38.60ms
step:740/1920 train_time:28586ms step_avg:38.63ms
step:741/1920 train_time:28649ms step_avg:38.66ms
step:742/1920 train_time:28710ms step_avg:38.69ms
step:743/1920 train_time:28772ms step_avg:38.72ms
step:744/1920 train_time:28834ms step_avg:38.76ms
step:745/1920 train_time:28896ms step_avg:38.79ms
step:746/1920 train_time:28958ms step_avg:38.82ms
step:747/1920 train_time:29021ms step_avg:38.85ms
step:748/1920 train_time:29083ms step_avg:38.88ms
step:749/1920 train_time:29145ms step_avg:38.91ms
step:750/1920 train_time:29207ms step_avg:38.94ms
step:750/1920 val_loss:4.0395 train_time:29273ms step_avg:39.03ms
step:751/1920 train_time:29291ms step_avg:39.00ms
step:752/1920 train_time:29333ms step_avg:39.01ms
step:753/1920 train_time:29399ms step_avg:39.04ms
step:754/1920 train_time:29465ms step_avg:39.08ms
step:755/1920 train_time:29527ms step_avg:39.11ms
step:756/1920 train_time:29589ms step_avg:39.14ms
step:757/1920 train_time:29650ms step_avg:39.17ms
step:758/1920 train_time:29712ms step_avg:39.20ms
step:759/1920 train_time:29774ms step_avg:39.23ms
step:760/1920 train_time:29835ms step_avg:39.26ms
step:761/1920 train_time:29897ms step_avg:39.29ms
step:762/1920 train_time:29958ms step_avg:39.31ms
step:763/1920 train_time:30019ms step_avg:39.34ms
step:764/1920 train_time:30080ms step_avg:39.37ms
step:765/1920 train_time:30143ms step_avg:39.40ms
step:766/1920 train_time:30210ms step_avg:39.44ms
step:767/1920 train_time:30275ms step_avg:39.47ms
step:768/1920 train_time:30338ms step_avg:39.50ms
step:769/1920 train_time:30402ms step_avg:39.53ms
step:770/1920 train_time:30464ms step_avg:39.56ms
step:771/1920 train_time:30526ms step_avg:39.59ms
step:772/1920 train_time:30588ms step_avg:39.62ms
step:773/1920 train_time:30650ms step_avg:39.65ms
step:774/1920 train_time:30711ms step_avg:39.68ms
step:775/1920 train_time:30774ms step_avg:39.71ms
step:776/1920 train_time:30835ms step_avg:39.74ms
step:777/1920 train_time:30898ms step_avg:39.77ms
step:778/1920 train_time:30959ms step_avg:39.79ms
step:779/1920 train_time:31021ms step_avg:39.82ms
step:780/1920 train_time:31082ms step_avg:39.85ms
step:781/1920 train_time:31145ms step_avg:39.88ms
step:782/1920 train_time:31207ms step_avg:39.91ms
step:783/1920 train_time:31271ms step_avg:39.94ms
step:784/1920 train_time:31334ms step_avg:39.97ms
step:785/1920 train_time:31398ms step_avg:40.00ms
step:786/1920 train_time:31460ms step_avg:40.03ms
step:787/1920 train_time:31523ms step_avg:40.05ms
step:788/1920 train_time:31585ms step_avg:40.08ms
step:789/1920 train_time:31647ms step_avg:40.11ms
step:790/1920 train_time:31708ms step_avg:40.14ms
step:791/1920 train_time:31771ms step_avg:40.17ms
step:792/1920 train_time:31832ms step_avg:40.19ms
step:793/1920 train_time:31895ms step_avg:40.22ms
step:794/1920 train_time:31957ms step_avg:40.25ms
step:795/1920 train_time:32019ms step_avg:40.28ms
step:796/1920 train_time:32081ms step_avg:40.30ms
step:797/1920 train_time:32144ms step_avg:40.33ms
step:798/1920 train_time:32205ms step_avg:40.36ms
step:799/1920 train_time:32268ms step_avg:40.39ms
step:800/1920 train_time:32331ms step_avg:40.41ms
step:801/1920 train_time:32394ms step_avg:40.44ms
step:802/1920 train_time:32456ms step_avg:40.47ms
step:803/1920 train_time:32519ms step_avg:40.50ms
step:804/1920 train_time:32581ms step_avg:40.52ms
step:805/1920 train_time:32643ms step_avg:40.55ms
step:806/1920 train_time:32705ms step_avg:40.58ms
step:807/1920 train_time:32768ms step_avg:40.60ms
step:808/1920 train_time:32829ms step_avg:40.63ms
step:809/1920 train_time:32891ms step_avg:40.66ms
step:810/1920 train_time:32953ms step_avg:40.68ms
step:811/1920 train_time:33016ms step_avg:40.71ms
step:812/1920 train_time:33078ms step_avg:40.74ms
step:813/1920 train_time:33140ms step_avg:40.76ms
step:814/1920 train_time:33202ms step_avg:40.79ms
step:815/1920 train_time:33264ms step_avg:40.82ms
step:816/1920 train_time:33326ms step_avg:40.84ms
step:817/1920 train_time:33390ms step_avg:40.87ms
step:818/1920 train_time:33453ms step_avg:40.90ms
step:819/1920 train_time:33516ms step_avg:40.92ms
step:820/1920 train_time:33578ms step_avg:40.95ms
step:821/1920 train_time:33640ms step_avg:40.97ms
step:822/1920 train_time:33702ms step_avg:41.00ms
step:823/1920 train_time:33765ms step_avg:41.03ms
step:824/1920 train_time:33826ms step_avg:41.05ms
step:825/1920 train_time:33889ms step_avg:41.08ms
step:826/1920 train_time:33951ms step_avg:41.10ms
step:827/1920 train_time:34014ms step_avg:41.13ms
step:828/1920 train_time:34076ms step_avg:41.15ms
step:829/1920 train_time:34138ms step_avg:41.18ms
step:830/1920 train_time:34200ms step_avg:41.20ms
step:831/1920 train_time:34262ms step_avg:41.23ms
step:832/1920 train_time:34324ms step_avg:41.25ms
step:833/1920 train_time:34387ms step_avg:41.28ms
step:834/1920 train_time:34449ms step_avg:41.31ms
step:835/1920 train_time:34512ms step_avg:41.33ms
step:836/1920 train_time:34574ms step_avg:41.36ms
step:837/1920 train_time:34637ms step_avg:41.38ms
step:838/1920 train_time:34699ms step_avg:41.41ms
step:839/1920 train_time:34762ms step_avg:41.43ms
step:840/1920 train_time:34824ms step_avg:41.46ms
step:841/1920 train_time:34886ms step_avg:41.48ms
step:842/1920 train_time:34948ms step_avg:41.51ms
step:843/1920 train_time:35010ms step_avg:41.53ms
step:844/1920 train_time:35072ms step_avg:41.56ms
step:845/1920 train_time:35136ms step_avg:41.58ms
step:846/1920 train_time:35198ms step_avg:41.61ms
step:847/1920 train_time:35261ms step_avg:41.63ms
step:848/1920 train_time:35323ms step_avg:41.65ms
step:849/1920 train_time:35385ms step_avg:41.68ms
step:850/1920 train_time:35447ms step_avg:41.70ms
step:851/1920 train_time:35510ms step_avg:41.73ms
step:852/1920 train_time:35571ms step_avg:41.75ms
step:853/1920 train_time:35635ms step_avg:41.78ms
step:854/1920 train_time:35697ms step_avg:41.80ms
step:855/1920 train_time:35759ms step_avg:41.82ms
step:856/1920 train_time:35821ms step_avg:41.85ms
step:857/1920 train_time:35884ms step_avg:41.87ms
step:858/1920 train_time:35945ms step_avg:41.89ms
step:859/1920 train_time:36008ms step_avg:41.92ms
step:860/1920 train_time:36070ms step_avg:41.94ms
step:861/1920 train_time:36133ms step_avg:41.97ms
step:862/1920 train_time:36195ms step_avg:41.99ms
step:863/1920 train_time:36258ms step_avg:42.01ms
step:864/1920 train_time:36319ms step_avg:42.04ms
step:865/1920 train_time:36382ms step_avg:42.06ms
step:866/1920 train_time:36444ms step_avg:42.08ms
step:867/1920 train_time:36506ms step_avg:42.11ms
step:868/1920 train_time:36568ms step_avg:42.13ms
step:869/1920 train_time:36631ms step_avg:42.15ms
step:870/1920 train_time:36693ms step_avg:42.18ms
step:871/1920 train_time:36756ms step_avg:42.20ms
step:872/1920 train_time:36818ms step_avg:42.22ms
step:873/1920 train_time:36881ms step_avg:42.25ms
step:874/1920 train_time:36943ms step_avg:42.27ms
step:875/1920 train_time:37006ms step_avg:42.29ms
step:876/1920 train_time:37067ms step_avg:42.31ms
step:877/1920 train_time:37130ms step_avg:42.34ms
step:878/1920 train_time:37193ms step_avg:42.36ms
step:879/1920 train_time:37256ms step_avg:42.38ms
step:880/1920 train_time:37318ms step_avg:42.41ms
step:881/1920 train_time:37380ms step_avg:42.43ms
step:882/1920 train_time:37442ms step_avg:42.45ms
step:883/1920 train_time:37505ms step_avg:42.47ms
step:884/1920 train_time:37566ms step_avg:42.50ms
step:885/1920 train_time:37629ms step_avg:42.52ms
step:886/1920 train_time:37691ms step_avg:42.54ms
step:887/1920 train_time:37754ms step_avg:42.56ms
step:888/1920 train_time:37815ms step_avg:42.59ms
step:889/1920 train_time:37878ms step_avg:42.61ms
step:890/1920 train_time:37940ms step_avg:42.63ms
step:891/1920 train_time:38003ms step_avg:42.65ms
step:892/1920 train_time:38064ms step_avg:42.67ms
step:893/1920 train_time:38126ms step_avg:42.69ms
step:894/1920 train_time:38188ms step_avg:42.72ms
step:895/1920 train_time:38251ms step_avg:42.74ms
step:896/1920 train_time:38313ms step_avg:42.76ms
step:897/1920 train_time:38376ms step_avg:42.78ms
step:898/1920 train_time:38438ms step_avg:42.80ms
step:899/1920 train_time:38501ms step_avg:42.83ms
step:900/1920 train_time:38563ms step_avg:42.85ms
step:901/1920 train_time:38625ms step_avg:42.87ms
step:902/1920 train_time:38687ms step_avg:42.89ms
step:903/1920 train_time:38750ms step_avg:42.91ms
step:904/1920 train_time:38812ms step_avg:42.93ms
step:905/1920 train_time:38875ms step_avg:42.96ms
step:906/1920 train_time:38937ms step_avg:42.98ms
step:907/1920 train_time:38999ms step_avg:43.00ms
step:908/1920 train_time:39061ms step_avg:43.02ms
step:909/1920 train_time:39124ms step_avg:43.04ms
step:910/1920 train_time:39186ms step_avg:43.06ms
step:911/1920 train_time:39249ms step_avg:43.08ms
step:912/1920 train_time:39311ms step_avg:43.10ms
step:913/1920 train_time:39375ms step_avg:43.13ms
step:914/1920 train_time:39437ms step_avg:43.15ms
step:915/1920 train_time:39499ms step_avg:43.17ms
step:916/1920 train_time:39561ms step_avg:43.19ms
step:917/1920 train_time:39623ms step_avg:43.21ms
step:918/1920 train_time:39685ms step_avg:43.23ms
step:919/1920 train_time:39748ms step_avg:43.25ms
step:920/1920 train_time:39809ms step_avg:43.27ms
step:921/1920 train_time:39872ms step_avg:43.29ms
step:922/1920 train_time:39934ms step_avg:43.31ms
step:923/1920 train_time:39997ms step_avg:43.33ms
step:924/1920 train_time:40059ms step_avg:43.35ms
step:925/1920 train_time:40122ms step_avg:43.37ms
step:926/1920 train_time:40184ms step_avg:43.39ms
step:927/1920 train_time:40246ms step_avg:43.42ms
step:928/1920 train_time:40308ms step_avg:43.44ms
step:929/1920 train_time:40372ms step_avg:43.46ms
step:930/1920 train_time:40434ms step_avg:43.48ms
step:931/1920 train_time:40497ms step_avg:43.50ms
step:932/1920 train_time:40559ms step_avg:43.52ms
step:933/1920 train_time:40621ms step_avg:43.54ms
step:934/1920 train_time:40684ms step_avg:43.56ms
step:935/1920 train_time:40746ms step_avg:43.58ms
step:936/1920 train_time:40807ms step_avg:43.60ms
step:937/1920 train_time:40870ms step_avg:43.62ms
step:938/1920 train_time:40932ms step_avg:43.64ms
step:939/1920 train_time:40995ms step_avg:43.66ms
step:940/1920 train_time:41057ms step_avg:43.68ms
step:941/1920 train_time:41119ms step_avg:43.70ms
step:942/1920 train_time:41181ms step_avg:43.72ms
step:943/1920 train_time:41244ms step_avg:43.74ms
step:944/1920 train_time:41306ms step_avg:43.76ms
step:945/1920 train_time:41369ms step_avg:43.78ms
step:946/1920 train_time:41431ms step_avg:43.80ms
step:947/1920 train_time:41494ms step_avg:43.82ms
step:948/1920 train_time:41556ms step_avg:43.84ms
step:949/1920 train_time:41619ms step_avg:43.86ms
step:950/1920 train_time:41681ms step_avg:43.87ms
step:951/1920 train_time:41744ms step_avg:43.89ms
step:952/1920 train_time:41805ms step_avg:43.91ms
step:953/1920 train_time:41868ms step_avg:43.93ms
step:954/1920 train_time:41930ms step_avg:43.95ms
step:955/1920 train_time:41994ms step_avg:43.97ms
step:956/1920 train_time:42055ms step_avg:43.99ms
step:957/1920 train_time:42118ms step_avg:44.01ms
step:958/1920 train_time:42180ms step_avg:44.03ms
step:959/1920 train_time:42243ms step_avg:44.05ms
step:960/1920 train_time:42305ms step_avg:44.07ms
step:961/1920 train_time:42368ms step_avg:44.09ms
step:962/1920 train_time:42429ms step_avg:44.11ms
step:963/1920 train_time:42492ms step_avg:44.12ms
step:964/1920 train_time:42554ms step_avg:44.14ms
step:965/1920 train_time:42617ms step_avg:44.16ms
step:966/1920 train_time:42679ms step_avg:44.18ms
step:967/1920 train_time:42741ms step_avg:44.20ms
step:968/1920 train_time:42803ms step_avg:44.22ms
step:969/1920 train_time:42866ms step_avg:44.24ms
step:970/1920 train_time:42928ms step_avg:44.26ms
step:971/1920 train_time:42991ms step_avg:44.27ms
step:972/1920 train_time:43053ms step_avg:44.29ms
step:973/1920 train_time:43117ms step_avg:44.31ms
step:974/1920 train_time:43178ms step_avg:44.33ms
step:975/1920 train_time:43241ms step_avg:44.35ms
step:976/1920 train_time:43303ms step_avg:44.37ms
step:977/1920 train_time:43365ms step_avg:44.39ms
step:978/1920 train_time:43427ms step_avg:44.40ms
step:979/1920 train_time:43490ms step_avg:44.42ms
step:980/1920 train_time:43552ms step_avg:44.44ms
step:981/1920 train_time:43616ms step_avg:44.46ms
step:982/1920 train_time:43678ms step_avg:44.48ms
step:983/1920 train_time:43740ms step_avg:44.50ms
step:984/1920 train_time:43803ms step_avg:44.51ms
step:985/1920 train_time:43865ms step_avg:44.53ms
step:986/1920 train_time:43927ms step_avg:44.55ms
step:987/1920 train_time:43989ms step_avg:44.57ms
step:988/1920 train_time:44051ms step_avg:44.59ms
step:989/1920 train_time:44115ms step_avg:44.61ms
step:990/1920 train_time:44178ms step_avg:44.62ms
step:991/1920 train_time:44240ms step_avg:44.64ms
step:992/1920 train_time:44302ms step_avg:44.66ms
step:993/1920 train_time:44365ms step_avg:44.68ms
step:994/1920 train_time:44427ms step_avg:44.69ms
step:995/1920 train_time:44489ms step_avg:44.71ms
step:996/1920 train_time:44552ms step_avg:44.73ms
step:997/1920 train_time:44614ms step_avg:44.75ms
step:998/1920 train_time:44676ms step_avg:44.77ms
step:999/1920 train_time:44739ms step_avg:44.78ms
step:1000/1920 train_time:44801ms step_avg:44.80ms
step:1000/1920 val_loss:3.7787 train_time:44866ms step_avg:44.87ms
step:1001/1920 train_time:44884ms step_avg:44.84ms
step:1002/1920 train_time:44928ms step_avg:44.84ms
step:1003/1920 train_time:44991ms step_avg:44.86ms
step:1004/1920 train_time:45054ms step_avg:44.87ms
step:1005/1920 train_time:45117ms step_avg:44.89ms
step:1006/1920 train_time:45179ms step_avg:44.91ms
step:1007/1920 train_time:45241ms step_avg:44.93ms
step:1008/1920 train_time:45302ms step_avg:44.94ms
step:1009/1920 train_time:45364ms step_avg:44.96ms
step:1010/1920 train_time:45425ms step_avg:44.98ms
step:1011/1920 train_time:45487ms step_avg:44.99ms
step:1012/1920 train_time:45548ms step_avg:45.01ms
step:1013/1920 train_time:45611ms step_avg:45.03ms
step:1014/1920 train_time:45672ms step_avg:45.04ms
step:1015/1920 train_time:45735ms step_avg:45.06ms
step:1016/1920 train_time:45797ms step_avg:45.08ms
step:1017/1920 train_time:45861ms step_avg:45.09ms
step:1018/1920 train_time:45925ms step_avg:45.11ms
step:1019/1920 train_time:45988ms step_avg:45.13ms
step:1020/1920 train_time:46051ms step_avg:45.15ms
step:1021/1920 train_time:46113ms step_avg:45.16ms
step:1022/1920 train_time:46175ms step_avg:45.18ms
step:1023/1920 train_time:46239ms step_avg:45.20ms
step:1024/1920 train_time:46300ms step_avg:45.21ms
step:1025/1920 train_time:46363ms step_avg:45.23ms
step:1026/1920 train_time:46424ms step_avg:45.25ms
step:1027/1920 train_time:46486ms step_avg:45.26ms
step:1028/1920 train_time:46548ms step_avg:45.28ms
step:1029/1920 train_time:46609ms step_avg:45.30ms
step:1030/1920 train_time:46671ms step_avg:45.31ms
step:1031/1920 train_time:46734ms step_avg:45.33ms
step:1032/1920 train_time:46797ms step_avg:45.35ms
step:1033/1920 train_time:46861ms step_avg:45.36ms
step:1034/1920 train_time:46923ms step_avg:45.38ms
step:1035/1920 train_time:46986ms step_avg:45.40ms
step:1036/1920 train_time:47048ms step_avg:45.41ms
step:1037/1920 train_time:47111ms step_avg:45.43ms
step:1038/1920 train_time:47174ms step_avg:45.45ms
step:1039/1920 train_time:47237ms step_avg:45.46ms
step:1040/1920 train_time:47298ms step_avg:45.48ms
step:1041/1920 train_time:47361ms step_avg:45.50ms
step:1042/1920 train_time:47423ms step_avg:45.51ms
step:1043/1920 train_time:47485ms step_avg:45.53ms
step:1044/1920 train_time:47546ms step_avg:45.54ms
step:1045/1920 train_time:47608ms step_avg:45.56ms
step:1046/1920 train_time:47669ms step_avg:45.57ms
step:1047/1920 train_time:47732ms step_avg:45.59ms
step:1048/1920 train_time:47795ms step_avg:45.61ms
step:1049/1920 train_time:47858ms step_avg:45.62ms
step:1050/1920 train_time:47920ms step_avg:45.64ms
step:1051/1920 train_time:47982ms step_avg:45.65ms
step:1052/1920 train_time:48045ms step_avg:45.67ms
step:1053/1920 train_time:48108ms step_avg:45.69ms
step:1054/1920 train_time:48170ms step_avg:45.70ms
step:1055/1920 train_time:48234ms step_avg:45.72ms
step:1056/1920 train_time:48296ms step_avg:45.73ms
step:1057/1920 train_time:48359ms step_avg:45.75ms
step:1058/1920 train_time:48421ms step_avg:45.77ms
step:1059/1920 train_time:48483ms step_avg:45.78ms
step:1060/1920 train_time:48545ms step_avg:45.80ms
step:1061/1920 train_time:48607ms step_avg:45.81ms
step:1062/1920 train_time:48668ms step_avg:45.83ms
step:1063/1920 train_time:48731ms step_avg:45.84ms
step:1064/1920 train_time:48793ms step_avg:45.86ms
step:1065/1920 train_time:48855ms step_avg:45.87ms
step:1066/1920 train_time:48917ms step_avg:45.89ms
step:1067/1920 train_time:48980ms step_avg:45.90ms
step:1068/1920 train_time:49042ms step_avg:45.92ms
step:1069/1920 train_time:49105ms step_avg:45.94ms
step:1070/1920 train_time:49168ms step_avg:45.95ms
step:1071/1920 train_time:49231ms step_avg:45.97ms
step:1072/1920 train_time:49293ms step_avg:45.98ms
step:1073/1920 train_time:49356ms step_avg:46.00ms
step:1074/1920 train_time:49418ms step_avg:46.01ms
step:1075/1920 train_time:49481ms step_avg:46.03ms
step:1076/1920 train_time:49542ms step_avg:46.04ms
step:1077/1920 train_time:49604ms step_avg:46.06ms
step:1078/1920 train_time:49667ms step_avg:46.07ms
step:1079/1920 train_time:49729ms step_avg:46.09ms
step:1080/1920 train_time:49791ms step_avg:46.10ms
step:1081/1920 train_time:49853ms step_avg:46.12ms
step:1082/1920 train_time:49915ms step_avg:46.13ms
step:1083/1920 train_time:49979ms step_avg:46.15ms
step:1084/1920 train_time:50041ms step_avg:46.16ms
step:1085/1920 train_time:50104ms step_avg:46.18ms
step:1086/1920 train_time:50166ms step_avg:46.19ms
step:1087/1920 train_time:50229ms step_avg:46.21ms
step:1088/1920 train_time:50291ms step_avg:46.22ms
step:1089/1920 train_time:50354ms step_avg:46.24ms
step:1090/1920 train_time:50416ms step_avg:46.25ms
step:1091/1920 train_time:50479ms step_avg:46.27ms
step:1092/1920 train_time:50540ms step_avg:46.28ms
step:1093/1920 train_time:50603ms step_avg:46.30ms
step:1094/1920 train_time:50665ms step_avg:46.31ms
step:1095/1920 train_time:50727ms step_avg:46.33ms
step:1096/1920 train_time:50788ms step_avg:46.34ms
step:1097/1920 train_time:50850ms step_avg:46.35ms
step:1098/1920 train_time:50912ms step_avg:46.37ms
step:1099/1920 train_time:50975ms step_avg:46.38ms
step:1100/1920 train_time:51037ms step_avg:46.40ms
step:1101/1920 train_time:51100ms step_avg:46.41ms
step:1102/1920 train_time:51162ms step_avg:46.43ms
step:1103/1920 train_time:51225ms step_avg:46.44ms
step:1104/1920 train_time:51287ms step_avg:46.46ms
step:1105/1920 train_time:51349ms step_avg:46.47ms
step:1106/1920 train_time:51411ms step_avg:46.48ms
step:1107/1920 train_time:51474ms step_avg:46.50ms
step:1108/1920 train_time:51536ms step_avg:46.51ms
step:1109/1920 train_time:51600ms step_avg:46.53ms
step:1110/1920 train_time:51662ms step_avg:46.54ms
step:1111/1920 train_time:51724ms step_avg:46.56ms
step:1112/1920 train_time:51786ms step_avg:46.57ms
step:1113/1920 train_time:51848ms step_avg:46.58ms
step:1114/1920 train_time:51910ms step_avg:46.60ms
step:1115/1920 train_time:51973ms step_avg:46.61ms
step:1116/1920 train_time:52035ms step_avg:46.63ms
step:1117/1920 train_time:52098ms step_avg:46.64ms
step:1118/1920 train_time:52161ms step_avg:46.66ms
step:1119/1920 train_time:52223ms step_avg:46.67ms
step:1120/1920 train_time:52285ms step_avg:46.68ms
step:1121/1920 train_time:52347ms step_avg:46.70ms
step:1122/1920 train_time:52409ms step_avg:46.71ms
step:1123/1920 train_time:52472ms step_avg:46.72ms
step:1124/1920 train_time:52534ms step_avg:46.74ms
step:1125/1920 train_time:52598ms step_avg:46.75ms
step:1126/1920 train_time:52660ms step_avg:46.77ms
step:1127/1920 train_time:52723ms step_avg:46.78ms
step:1128/1920 train_time:52785ms step_avg:46.80ms
step:1129/1920 train_time:52847ms step_avg:46.81ms
step:1130/1920 train_time:52908ms step_avg:46.82ms
step:1131/1920 train_time:52971ms step_avg:46.84ms
step:1132/1920 train_time:53033ms step_avg:46.85ms
step:1133/1920 train_time:53096ms step_avg:46.86ms
step:1134/1920 train_time:53158ms step_avg:46.88ms
step:1135/1920 train_time:53221ms step_avg:46.89ms
step:1136/1920 train_time:53283ms step_avg:46.90ms
step:1137/1920 train_time:53345ms step_avg:46.92ms
step:1138/1920 train_time:53407ms step_avg:46.93ms
step:1139/1920 train_time:53469ms step_avg:46.94ms
step:1140/1920 train_time:53531ms step_avg:46.96ms
step:1141/1920 train_time:53594ms step_avg:46.97ms
step:1142/1920 train_time:53657ms step_avg:46.98ms
step:1143/1920 train_time:53720ms step_avg:47.00ms
step:1144/1920 train_time:53782ms step_avg:47.01ms
step:1145/1920 train_time:53844ms step_avg:47.03ms
step:1146/1920 train_time:53906ms step_avg:47.04ms
step:1147/1920 train_time:53968ms step_avg:47.05ms
step:1148/1920 train_time:54030ms step_avg:47.06ms
step:1149/1920 train_time:54093ms step_avg:47.08ms
step:1150/1920 train_time:54155ms step_avg:47.09ms
step:1151/1920 train_time:54219ms step_avg:47.11ms
step:1152/1920 train_time:54281ms step_avg:47.12ms
step:1153/1920 train_time:54343ms step_avg:47.13ms
step:1154/1920 train_time:54405ms step_avg:47.14ms
step:1155/1920 train_time:54468ms step_avg:47.16ms
step:1156/1920 train_time:54530ms step_avg:47.17ms
step:1157/1920 train_time:54593ms step_avg:47.19ms
step:1158/1920 train_time:54656ms step_avg:47.20ms
step:1159/1920 train_time:54719ms step_avg:47.21ms
step:1160/1920 train_time:54781ms step_avg:47.22ms
step:1161/1920 train_time:54843ms step_avg:47.24ms
step:1162/1920 train_time:54906ms step_avg:47.25ms
step:1163/1920 train_time:54968ms step_avg:47.26ms
step:1164/1920 train_time:55030ms step_avg:47.28ms
step:1165/1920 train_time:55093ms step_avg:47.29ms
step:1166/1920 train_time:55155ms step_avg:47.30ms
step:1167/1920 train_time:55218ms step_avg:47.32ms
step:1168/1920 train_time:55279ms step_avg:47.33ms
step:1169/1920 train_time:55342ms step_avg:47.34ms
step:1170/1920 train_time:55404ms step_avg:47.35ms
step:1171/1920 train_time:55466ms step_avg:47.37ms
step:1172/1920 train_time:55529ms step_avg:47.38ms
step:1173/1920 train_time:55591ms step_avg:47.39ms
step:1174/1920 train_time:55654ms step_avg:47.41ms
step:1175/1920 train_time:55718ms step_avg:47.42ms
step:1176/1920 train_time:55779ms step_avg:47.43ms
step:1177/1920 train_time:55842ms step_avg:47.44ms
step:1178/1920 train_time:55904ms step_avg:47.46ms
step:1179/1920 train_time:55966ms step_avg:47.47ms
step:1180/1920 train_time:56028ms step_avg:47.48ms
step:1181/1920 train_time:56091ms step_avg:47.49ms
step:1182/1920 train_time:56153ms step_avg:47.51ms
step:1183/1920 train_time:56216ms step_avg:47.52ms
step:1184/1920 train_time:56279ms step_avg:47.53ms
step:1185/1920 train_time:56341ms step_avg:47.55ms
step:1186/1920 train_time:56403ms step_avg:47.56ms
step:1187/1920 train_time:56466ms step_avg:47.57ms
step:1188/1920 train_time:56528ms step_avg:47.58ms
step:1189/1920 train_time:56591ms step_avg:47.60ms
step:1190/1920 train_time:56653ms step_avg:47.61ms
step:1191/1920 train_time:56716ms step_avg:47.62ms
step:1192/1920 train_time:56778ms step_avg:47.63ms
step:1193/1920 train_time:56842ms step_avg:47.65ms
step:1194/1920 train_time:56903ms step_avg:47.66ms
step:1195/1920 train_time:56965ms step_avg:47.67ms
step:1196/1920 train_time:57027ms step_avg:47.68ms
step:1197/1920 train_time:57089ms step_avg:47.69ms
step:1198/1920 train_time:57151ms step_avg:47.71ms
step:1199/1920 train_time:57215ms step_avg:47.72ms
step:1200/1920 train_time:57277ms step_avg:47.73ms
step:1201/1920 train_time:57340ms step_avg:47.74ms
step:1202/1920 train_time:57402ms step_avg:47.76ms
step:1203/1920 train_time:57465ms step_avg:47.77ms
step:1204/1920 train_time:57527ms step_avg:47.78ms
step:1205/1920 train_time:57590ms step_avg:47.79ms
step:1206/1920 train_time:57651ms step_avg:47.80ms
step:1207/1920 train_time:57715ms step_avg:47.82ms
step:1208/1920 train_time:57777ms step_avg:47.83ms
step:1209/1920 train_time:57840ms step_avg:47.84ms
step:1210/1920 train_time:57902ms step_avg:47.85ms
step:1211/1920 train_time:57965ms step_avg:47.87ms
step:1212/1920 train_time:58026ms step_avg:47.88ms
step:1213/1920 train_time:58088ms step_avg:47.89ms
step:1214/1920 train_time:58151ms step_avg:47.90ms
step:1215/1920 train_time:58214ms step_avg:47.91ms
step:1216/1920 train_time:58276ms step_avg:47.92ms
step:1217/1920 train_time:58340ms step_avg:47.94ms
step:1218/1920 train_time:58402ms step_avg:47.95ms
step:1219/1920 train_time:58464ms step_avg:47.96ms
step:1220/1920 train_time:58526ms step_avg:47.97ms
step:1221/1920 train_time:58589ms step_avg:47.98ms
step:1222/1920 train_time:58651ms step_avg:48.00ms
step:1223/1920 train_time:58715ms step_avg:48.01ms
step:1224/1920 train_time:58777ms step_avg:48.02ms
step:1225/1920 train_time:58840ms step_avg:48.03ms
step:1226/1920 train_time:58902ms step_avg:48.04ms
step:1227/1920 train_time:58965ms step_avg:48.06ms
step:1228/1920 train_time:59026ms step_avg:48.07ms
step:1229/1920 train_time:59089ms step_avg:48.08ms
step:1230/1920 train_time:59151ms step_avg:48.09ms
step:1231/1920 train_time:59214ms step_avg:48.10ms
step:1232/1920 train_time:59276ms step_avg:48.11ms
step:1233/1920 train_time:59340ms step_avg:48.13ms
step:1234/1920 train_time:59401ms step_avg:48.14ms
step:1235/1920 train_time:59464ms step_avg:48.15ms
step:1236/1920 train_time:59526ms step_avg:48.16ms
step:1237/1920 train_time:59588ms step_avg:48.17ms
step:1238/1920 train_time:59650ms step_avg:48.18ms
step:1239/1920 train_time:59713ms step_avg:48.19ms
step:1240/1920 train_time:59775ms step_avg:48.21ms
step:1241/1920 train_time:59838ms step_avg:48.22ms
step:1242/1920 train_time:59900ms step_avg:48.23ms
step:1243/1920 train_time:59962ms step_avg:48.24ms
step:1244/1920 train_time:60025ms step_avg:48.25ms
step:1245/1920 train_time:60087ms step_avg:48.26ms
step:1246/1920 train_time:60149ms step_avg:48.27ms
step:1247/1920 train_time:60212ms step_avg:48.29ms
step:1248/1920 train_time:60274ms step_avg:48.30ms
step:1249/1920 train_time:60338ms step_avg:48.31ms
step:1250/1920 train_time:60400ms step_avg:48.32ms
step:1250/1920 val_loss:3.5525 train_time:60466ms step_avg:48.37ms
step:1251/1920 train_time:60484ms step_avg:48.35ms
step:1252/1920 train_time:60526ms step_avg:48.34ms
step:1253/1920 train_time:60591ms step_avg:48.36ms
step:1254/1920 train_time:60654ms step_avg:48.37ms
step:1255/1920 train_time:60716ms step_avg:48.38ms
step:1256/1920 train_time:60804ms step_avg:48.41ms
step:1257/1920 train_time:60891ms step_avg:48.44ms
step:1258/1920 train_time:60978ms step_avg:48.47ms
step:1259/1920 train_time:61066ms step_avg:48.50ms
step:1260/1920 train_time:61153ms step_avg:48.53ms
step:1261/1920 train_time:61241ms step_avg:48.57ms
step:1262/1920 train_time:61328ms step_avg:48.60ms
step:1263/1920 train_time:61418ms step_avg:48.63ms
step:1264/1920 train_time:61509ms step_avg:48.66ms
step:1265/1920 train_time:61600ms step_avg:48.70ms
step:1266/1920 train_time:61689ms step_avg:48.73ms
step:1267/1920 train_time:61777ms step_avg:48.76ms
step:1268/1920 train_time:61864ms step_avg:48.79ms
step:1269/1920 train_time:61952ms step_avg:48.82ms
step:1270/1920 train_time:62039ms step_avg:48.85ms
step:1271/1920 train_time:62127ms step_avg:48.88ms
step:1272/1920 train_time:62214ms step_avg:48.91ms
step:1273/1920 train_time:62302ms step_avg:48.94ms
step:1274/1920 train_time:62391ms step_avg:48.97ms
step:1275/1920 train_time:62481ms step_avg:49.00ms
step:1276/1920 train_time:62571ms step_avg:49.04ms
step:1277/1920 train_time:62661ms step_avg:49.07ms
step:1278/1920 train_time:62749ms step_avg:49.10ms
step:1279/1920 train_time:62838ms step_avg:49.13ms
step:1280/1920 train_time:62926ms step_avg:49.16ms
step:1281/1920 train_time:63015ms step_avg:49.19ms
step:1282/1920 train_time:63101ms step_avg:49.22ms
step:1283/1920 train_time:63190ms step_avg:49.25ms
step:1284/1920 train_time:63277ms step_avg:49.28ms
step:1285/1920 train_time:63366ms step_avg:49.31ms
step:1286/1920 train_time:63456ms step_avg:49.34ms
step:1287/1920 train_time:63545ms step_avg:49.37ms
step:1288/1920 train_time:63634ms step_avg:49.41ms
step:1289/1920 train_time:63722ms step_avg:49.44ms
step:1290/1920 train_time:63810ms step_avg:49.47ms
step:1291/1920 train_time:63899ms step_avg:49.50ms
step:1292/1920 train_time:63987ms step_avg:49.53ms
step:1293/1920 train_time:64075ms step_avg:49.56ms
step:1294/1920 train_time:64162ms step_avg:49.58ms
step:1295/1920 train_time:64250ms step_avg:49.61ms
step:1296/1920 train_time:64338ms step_avg:49.64ms
step:1297/1920 train_time:64427ms step_avg:49.67ms
step:1298/1920 train_time:64516ms step_avg:49.70ms
step:1299/1920 train_time:64605ms step_avg:49.73ms
step:1300/1920 train_time:64693ms step_avg:49.76ms
step:1301/1920 train_time:64781ms step_avg:49.79ms
step:1302/1920 train_time:64869ms step_avg:49.82ms
step:1303/1920 train_time:64958ms step_avg:49.85ms
step:1304/1920 train_time:65046ms step_avg:49.88ms
step:1305/1920 train_time:65136ms step_avg:49.91ms
step:1306/1920 train_time:65222ms step_avg:49.94ms
step:1307/1920 train_time:65310ms step_avg:49.97ms
step:1308/1920 train_time:65397ms step_avg:50.00ms
step:1309/1920 train_time:65487ms step_avg:50.03ms
step:1310/1920 train_time:65575ms step_avg:50.06ms
step:1311/1920 train_time:65664ms step_avg:50.09ms
step:1312/1920 train_time:65752ms step_avg:50.12ms
step:1313/1920 train_time:65841ms step_avg:50.15ms
step:1314/1920 train_time:65929ms step_avg:50.17ms
step:1315/1920 train_time:66018ms step_avg:50.20ms
step:1316/1920 train_time:66106ms step_avg:50.23ms
step:1317/1920 train_time:66195ms step_avg:50.26ms
step:1318/1920 train_time:66282ms step_avg:50.29ms
step:1319/1920 train_time:66371ms step_avg:50.32ms
step:1320/1920 train_time:66459ms step_avg:50.35ms
step:1321/1920 train_time:66548ms step_avg:50.38ms
step:1322/1920 train_time:66636ms step_avg:50.41ms
step:1323/1920 train_time:66725ms step_avg:50.43ms
step:1324/1920 train_time:66813ms step_avg:50.46ms
step:1325/1920 train_time:66901ms step_avg:50.49ms
step:1326/1920 train_time:66988ms step_avg:50.52ms
step:1327/1920 train_time:67078ms step_avg:50.55ms
step:1328/1920 train_time:67167ms step_avg:50.58ms
step:1329/1920 train_time:67256ms step_avg:50.61ms
step:1330/1920 train_time:67344ms step_avg:50.63ms
step:1331/1920 train_time:67435ms step_avg:50.66ms
step:1332/1920 train_time:67522ms step_avg:50.69ms
step:1333/1920 train_time:67612ms step_avg:50.72ms
step:1334/1920 train_time:67700ms step_avg:50.75ms
step:1335/1920 train_time:67789ms step_avg:50.78ms
step:1336/1920 train_time:67877ms step_avg:50.81ms
step:1337/1920 train_time:67965ms step_avg:50.83ms
step:1338/1920 train_time:68053ms step_avg:50.86ms
step:1339/1920 train_time:68141ms step_avg:50.89ms
step:1340/1920 train_time:68230ms step_avg:50.92ms
step:1341/1920 train_time:68319ms step_avg:50.95ms
step:1342/1920 train_time:68409ms step_avg:50.98ms
step:1343/1920 train_time:68498ms step_avg:51.00ms
step:1344/1920 train_time:68586ms step_avg:51.03ms
step:1345/1920 train_time:68675ms step_avg:51.06ms
step:1346/1920 train_time:68762ms step_avg:51.09ms
step:1347/1920 train_time:68851ms step_avg:51.11ms
step:1348/1920 train_time:68939ms step_avg:51.14ms
step:1349/1920 train_time:69028ms step_avg:51.17ms
step:1350/1920 train_time:69115ms step_avg:51.20ms
step:1351/1920 train_time:69204ms step_avg:51.22ms
step:1352/1920 train_time:69293ms step_avg:51.25ms
step:1353/1920 train_time:69381ms step_avg:51.28ms
step:1354/1920 train_time:69469ms step_avg:51.31ms
step:1355/1920 train_time:69558ms step_avg:51.33ms
step:1356/1920 train_time:69646ms step_avg:51.36ms
step:1357/1920 train_time:69735ms step_avg:51.39ms
step:1358/1920 train_time:69822ms step_avg:51.42ms
step:1359/1920 train_time:69911ms step_avg:51.44ms
step:1360/1920 train_time:69998ms step_avg:51.47ms
step:1361/1920 train_time:70087ms step_avg:51.50ms
step:1362/1920 train_time:70175ms step_avg:51.52ms
step:1363/1920 train_time:70263ms step_avg:51.55ms
step:1364/1920 train_time:70352ms step_avg:51.58ms
step:1365/1920 train_time:70440ms step_avg:51.60ms
step:1366/1920 train_time:70528ms step_avg:51.63ms
step:1367/1920 train_time:70618ms step_avg:51.66ms
step:1368/1920 train_time:70707ms step_avg:51.69ms
step:1369/1920 train_time:70797ms step_avg:51.71ms
step:1370/1920 train_time:70887ms step_avg:51.74ms
step:1371/1920 train_time:70976ms step_avg:51.77ms
step:1372/1920 train_time:71063ms step_avg:51.80ms
step:1373/1920 train_time:71152ms step_avg:51.82ms
step:1374/1920 train_time:71240ms step_avg:51.85ms
step:1375/1920 train_time:71329ms step_avg:51.88ms
step:1376/1920 train_time:71417ms step_avg:51.90ms
step:1377/1920 train_time:71506ms step_avg:51.93ms
step:1378/1920 train_time:71595ms step_avg:51.96ms
step:1379/1920 train_time:71684ms step_avg:51.98ms
step:1380/1920 train_time:71773ms step_avg:52.01ms
step:1381/1920 train_time:71861ms step_avg:52.04ms
step:1382/1920 train_time:71950ms step_avg:52.06ms
step:1383/1920 train_time:72039ms step_avg:52.09ms
step:1384/1920 train_time:72128ms step_avg:52.12ms
step:1385/1920 train_time:72216ms step_avg:52.14ms
step:1386/1920 train_time:72304ms step_avg:52.17ms
step:1387/1920 train_time:72393ms step_avg:52.19ms
step:1388/1920 train_time:72481ms step_avg:52.22ms
step:1389/1920 train_time:72569ms step_avg:52.25ms
step:1390/1920 train_time:72657ms step_avg:52.27ms
step:1391/1920 train_time:72746ms step_avg:52.30ms
step:1392/1920 train_time:72834ms step_avg:52.32ms
step:1393/1920 train_time:72922ms step_avg:52.35ms
step:1394/1920 train_time:73010ms step_avg:52.37ms
step:1395/1920 train_time:73100ms step_avg:52.40ms
step:1396/1920 train_time:73189ms step_avg:52.43ms
step:1397/1920 train_time:73278ms step_avg:52.45ms
step:1398/1920 train_time:73366ms step_avg:52.48ms
step:1399/1920 train_time:73454ms step_avg:52.50ms
step:1400/1920 train_time:73542ms step_avg:52.53ms
step:1401/1920 train_time:73630ms step_avg:52.56ms
step:1402/1920 train_time:73718ms step_avg:52.58ms
step:1403/1920 train_time:73808ms step_avg:52.61ms
step:1404/1920 train_time:73896ms step_avg:52.63ms
step:1405/1920 train_time:73985ms step_avg:52.66ms
step:1406/1920 train_time:74073ms step_avg:52.68ms
step:1407/1920 train_time:74161ms step_avg:52.71ms
step:1408/1920 train_time:74250ms step_avg:52.73ms
step:1409/1920 train_time:74339ms step_avg:52.76ms
step:1410/1920 train_time:74427ms step_avg:52.79ms
step:1411/1920 train_time:74516ms step_avg:52.81ms
step:1412/1920 train_time:74603ms step_avg:52.84ms
step:1413/1920 train_time:74692ms step_avg:52.86ms
step:1414/1920 train_time:74780ms step_avg:52.89ms
step:1415/1920 train_time:74869ms step_avg:52.91ms
step:1416/1920 train_time:74957ms step_avg:52.94ms
step:1417/1920 train_time:75046ms step_avg:52.96ms
step:1418/1920 train_time:75135ms step_avg:52.99ms
step:1419/1920 train_time:75223ms step_avg:53.01ms
step:1420/1920 train_time:75310ms step_avg:53.04ms
step:1421/1920 train_time:75399ms step_avg:53.06ms
step:1422/1920 train_time:75487ms step_avg:53.09ms
step:1423/1920 train_time:75577ms step_avg:53.11ms
step:1424/1920 train_time:75665ms step_avg:53.14ms
step:1425/1920 train_time:75754ms step_avg:53.16ms
step:1426/1920 train_time:75842ms step_avg:53.18ms
step:1427/1920 train_time:75931ms step_avg:53.21ms
step:1428/1920 train_time:76019ms step_avg:53.23ms
step:1429/1920 train_time:76107ms step_avg:53.26ms
step:1430/1920 train_time:76195ms step_avg:53.28ms
step:1431/1920 train_time:76284ms step_avg:53.31ms
step:1432/1920 train_time:76373ms step_avg:53.33ms
step:1433/1920 train_time:76461ms step_avg:53.36ms
step:1434/1920 train_time:76550ms step_avg:53.38ms
step:1435/1920 train_time:76639ms step_avg:53.41ms
step:1436/1920 train_time:76728ms step_avg:53.43ms
step:1437/1920 train_time:76818ms step_avg:53.46ms
step:1438/1920 train_time:76906ms step_avg:53.48ms
step:1439/1920 train_time:76996ms step_avg:53.51ms
step:1440/1920 train_time:77084ms step_avg:53.53ms
step:1441/1920 train_time:77172ms step_avg:53.55ms
step:1442/1920 train_time:77259ms step_avg:53.58ms
step:1443/1920 train_time:77347ms step_avg:53.60ms
step:1444/1920 train_time:77436ms step_avg:53.63ms
step:1445/1920 train_time:77524ms step_avg:53.65ms
step:1446/1920 train_time:77612ms step_avg:53.67ms
step:1447/1920 train_time:77700ms step_avg:53.70ms
step:1448/1920 train_time:77788ms step_avg:53.72ms
step:1449/1920 train_time:77878ms step_avg:53.75ms
step:1450/1920 train_time:77967ms step_avg:53.77ms
step:1451/1920 train_time:78056ms step_avg:53.79ms
step:1452/1920 train_time:78144ms step_avg:53.82ms
step:1453/1920 train_time:78232ms step_avg:53.84ms
step:1454/1920 train_time:78320ms step_avg:53.87ms
step:1455/1920 train_time:78408ms step_avg:53.89ms
step:1456/1920 train_time:78497ms step_avg:53.91ms
step:1457/1920 train_time:78586ms step_avg:53.94ms
step:1458/1920 train_time:78673ms step_avg:53.96ms
step:1459/1920 train_time:78761ms step_avg:53.98ms
step:1460/1920 train_time:78850ms step_avg:54.01ms
step:1461/1920 train_time:78940ms step_avg:54.03ms
step:1462/1920 train_time:79028ms step_avg:54.05ms
step:1463/1920 train_time:79118ms step_avg:54.08ms
step:1464/1920 train_time:79206ms step_avg:54.10ms
step:1465/1920 train_time:79295ms step_avg:54.13ms
step:1466/1920 train_time:79383ms step_avg:54.15ms
step:1467/1920 train_time:79471ms step_avg:54.17ms
step:1468/1920 train_time:79559ms step_avg:54.20ms
step:1469/1920 train_time:79647ms step_avg:54.22ms
step:1470/1920 train_time:79735ms step_avg:54.24ms
step:1471/1920 train_time:79824ms step_avg:54.26ms
step:1472/1920 train_time:79911ms step_avg:54.29ms
step:1473/1920 train_time:79999ms step_avg:54.31ms
step:1474/1920 train_time:80088ms step_avg:54.33ms
step:1475/1920 train_time:80179ms step_avg:54.36ms
step:1476/1920 train_time:80267ms step_avg:54.38ms
step:1477/1920 train_time:80357ms step_avg:54.41ms
step:1478/1920 train_time:80445ms step_avg:54.43ms
step:1479/1920 train_time:80535ms step_avg:54.45ms
step:1480/1920 train_time:80622ms step_avg:54.47ms
step:1481/1920 train_time:80710ms step_avg:54.50ms
step:1482/1920 train_time:80798ms step_avg:54.52ms
step:1483/1920 train_time:80887ms step_avg:54.54ms
step:1484/1920 train_time:80975ms step_avg:54.57ms
step:1485/1920 train_time:81064ms step_avg:54.59ms
step:1486/1920 train_time:81152ms step_avg:54.61ms
step:1487/1920 train_time:81240ms step_avg:54.63ms
step:1488/1920 train_time:81330ms step_avg:54.66ms
step:1489/1920 train_time:81418ms step_avg:54.68ms
step:1490/1920 train_time:81507ms step_avg:54.70ms
step:1491/1920 train_time:81596ms step_avg:54.73ms
step:1492/1920 train_time:81684ms step_avg:54.75ms
step:1493/1920 train_time:81772ms step_avg:54.77ms
step:1494/1920 train_time:81860ms step_avg:54.79ms
step:1495/1920 train_time:81949ms step_avg:54.82ms
step:1496/1920 train_time:82038ms step_avg:54.84ms
step:1497/1920 train_time:82127ms step_avg:54.86ms
step:1498/1920 train_time:82215ms step_avg:54.88ms
step:1499/1920 train_time:82303ms step_avg:54.91ms
step:1500/1920 train_time:82392ms step_avg:54.93ms
step:1500/1920 val_loss:3.4152 train_time:82482ms step_avg:54.99ms
step:1501/1920 train_time:82501ms step_avg:54.96ms
step:1502/1920 train_time:82571ms step_avg:54.97ms
step:1503/1920 train_time:82664ms step_avg:55.00ms
step:1504/1920 train_time:82752ms step_avg:55.02ms
step:1505/1920 train_time:82839ms step_avg:55.04ms
step:1506/1920 train_time:82926ms step_avg:55.06ms
step:1507/1920 train_time:83014ms step_avg:55.09ms
step:1508/1920 train_time:83101ms step_avg:55.11ms
step:1509/1920 train_time:83189ms step_avg:55.13ms
step:1510/1920 train_time:83277ms step_avg:55.15ms
step:1511/1920 train_time:83365ms step_avg:55.17ms
step:1512/1920 train_time:83456ms step_avg:55.20ms
step:1513/1920 train_time:83547ms step_avg:55.22ms
step:1514/1920 train_time:83635ms step_avg:55.24ms
step:1515/1920 train_time:83725ms step_avg:55.26ms
step:1516/1920 train_time:83813ms step_avg:55.29ms
step:1517/1920 train_time:83902ms step_avg:55.31ms
step:1518/1920 train_time:83990ms step_avg:55.33ms
step:1519/1920 train_time:84078ms step_avg:55.35ms
step:1520/1920 train_time:84166ms step_avg:55.37ms
step:1521/1920 train_time:84254ms step_avg:55.39ms
step:1522/1920 train_time:84342ms step_avg:55.41ms
step:1523/1920 train_time:84432ms step_avg:55.44ms
step:1524/1920 train_time:84520ms step_avg:55.46ms
step:1525/1920 train_time:84610ms step_avg:55.48ms
step:1526/1920 train_time:84697ms step_avg:55.50ms
step:1527/1920 train_time:84786ms step_avg:55.52ms
step:1528/1920 train_time:84874ms step_avg:55.55ms
step:1529/1920 train_time:84963ms step_avg:55.57ms
step:1530/1920 train_time:85051ms step_avg:55.59ms
step:1531/1920 train_time:85139ms step_avg:55.61ms
step:1532/1920 train_time:85226ms step_avg:55.63ms
step:1533/1920 train_time:85316ms step_avg:55.65ms
step:1534/1920 train_time:85405ms step_avg:55.67ms
step:1535/1920 train_time:85495ms step_avg:55.70ms
step:1536/1920 train_time:85584ms step_avg:55.72ms
step:1537/1920 train_time:85673ms step_avg:55.74ms
step:1538/1920 train_time:85762ms step_avg:55.76ms
step:1539/1920 train_time:85851ms step_avg:55.78ms
step:1540/1920 train_time:85938ms step_avg:55.80ms
step:1541/1920 train_time:86026ms step_avg:55.82ms
step:1542/1920 train_time:86113ms step_avg:55.85ms
step:1543/1920 train_time:86203ms step_avg:55.87ms
step:1544/1920 train_time:86290ms step_avg:55.89ms
step:1545/1920 train_time:86380ms step_avg:55.91ms
step:1546/1920 train_time:86468ms step_avg:55.93ms
step:1547/1920 train_time:86556ms step_avg:55.95ms
step:1548/1920 train_time:86645ms step_avg:55.97ms
step:1549/1920 train_time:86734ms step_avg:55.99ms
step:1550/1920 train_time:86822ms step_avg:56.01ms
step:1551/1920 train_time:86911ms step_avg:56.04ms
step:1552/1920 train_time:86999ms step_avg:56.06ms
step:1553/1920 train_time:87088ms step_avg:56.08ms
step:1554/1920 train_time:87175ms step_avg:56.10ms
step:1555/1920 train_time:87263ms step_avg:56.12ms
step:1556/1920 train_time:87351ms step_avg:56.14ms
step:1557/1920 train_time:87440ms step_avg:56.16ms
step:1558/1920 train_time:87529ms step_avg:56.18ms
step:1559/1920 train_time:87617ms step_avg:56.20ms
step:1560/1920 train_time:87706ms step_avg:56.22ms
step:1561/1920 train_time:87795ms step_avg:56.24ms
step:1562/1920 train_time:87883ms step_avg:56.26ms
step:1563/1920 train_time:87972ms step_avg:56.28ms
step:1564/1920 train_time:88060ms step_avg:56.30ms
step:1565/1920 train_time:88148ms step_avg:56.32ms
step:1566/1920 train_time:88235ms step_avg:56.34ms
step:1567/1920 train_time:88324ms step_avg:56.36ms
step:1568/1920 train_time:88411ms step_avg:56.38ms
step:1569/1920 train_time:88500ms step_avg:56.41ms
step:1570/1920 train_time:88589ms step_avg:56.43ms
step:1571/1920 train_time:88677ms step_avg:56.45ms
step:1572/1920 train_time:88766ms step_avg:56.47ms
step:1573/1920 train_time:88854ms step_avg:56.49ms
step:1574/1920 train_time:88942ms step_avg:56.51ms
step:1575/1920 train_time:89031ms step_avg:56.53ms
step:1576/1920 train_time:89119ms step_avg:56.55ms
step:1577/1920 train_time:89207ms step_avg:56.57ms
step:1578/1920 train_time:89294ms step_avg:56.59ms
step:1579/1920 train_time:89384ms step_avg:56.61ms
step:1580/1920 train_time:89471ms step_avg:56.63ms
step:1581/1920 train_time:89561ms step_avg:56.65ms
step:1582/1920 train_time:89649ms step_avg:56.67ms
step:1583/1920 train_time:89737ms step_avg:56.69ms
step:1584/1920 train_time:89825ms step_avg:56.71ms
step:1585/1920 train_time:89915ms step_avg:56.73ms
step:1586/1920 train_time:90004ms step_avg:56.75ms
step:1587/1920 train_time:90093ms step_avg:56.77ms
step:1588/1920 train_time:90182ms step_avg:56.79ms
step:1589/1920 train_time:90272ms step_avg:56.81ms
step:1590/1920 train_time:90360ms step_avg:56.83ms
step:1591/1920 train_time:90450ms step_avg:56.85ms
step:1592/1920 train_time:90537ms step_avg:56.87ms
step:1593/1920 train_time:90626ms step_avg:56.89ms
step:1594/1920 train_time:90714ms step_avg:56.91ms
step:1595/1920 train_time:90802ms step_avg:56.93ms
step:1596/1920 train_time:90890ms step_avg:56.95ms
step:1597/1920 train_time:90979ms step_avg:56.97ms
step:1598/1920 train_time:91067ms step_avg:56.99ms
step:1599/1920 train_time:91156ms step_avg:57.01ms
step:1600/1920 train_time:91245ms step_avg:57.03ms
step:1601/1920 train_time:91334ms step_avg:57.05ms
step:1602/1920 train_time:91422ms step_avg:57.07ms
step:1603/1920 train_time:91511ms step_avg:57.09ms
step:1604/1920 train_time:91598ms step_avg:57.11ms
step:1605/1920 train_time:91687ms step_avg:57.13ms
step:1606/1920 train_time:91775ms step_avg:57.15ms
step:1607/1920 train_time:91865ms step_avg:57.17ms
step:1608/1920 train_time:91953ms step_avg:57.18ms
step:1609/1920 train_time:92042ms step_avg:57.20ms
step:1610/1920 train_time:92131ms step_avg:57.22ms
step:1611/1920 train_time:92219ms step_avg:57.24ms
step:1612/1920 train_time:92307ms step_avg:57.26ms
step:1613/1920 train_time:92396ms step_avg:57.28ms
step:1614/1920 train_time:92486ms step_avg:57.30ms
step:1615/1920 train_time:92576ms step_avg:57.32ms
step:1616/1920 train_time:92664ms step_avg:57.34ms
step:1617/1920 train_time:92753ms step_avg:57.36ms
step:1618/1920 train_time:92842ms step_avg:57.38ms
step:1619/1920 train_time:92930ms step_avg:57.40ms
step:1620/1920 train_time:93017ms step_avg:57.42ms
step:1621/1920 train_time:93106ms step_avg:57.44ms
step:1622/1920 train_time:93194ms step_avg:57.46ms
step:1623/1920 train_time:93283ms step_avg:57.48ms
step:1624/1920 train_time:93372ms step_avg:57.50ms
step:1625/1920 train_time:93462ms step_avg:57.51ms
step:1626/1920 train_time:93550ms step_avg:57.53ms
step:1627/1920 train_time:93638ms step_avg:57.55ms
step:1628/1920 train_time:93726ms step_avg:57.57ms
step:1629/1920 train_time:93814ms step_avg:57.59ms
step:1630/1920 train_time:93903ms step_avg:57.61ms
step:1631/1920 train_time:93991ms step_avg:57.63ms
step:1632/1920 train_time:94079ms step_avg:57.65ms
step:1633/1920 train_time:94167ms step_avg:57.66ms
step:1634/1920 train_time:94255ms step_avg:57.68ms
step:1635/1920 train_time:94343ms step_avg:57.70ms
step:1636/1920 train_time:94432ms step_avg:57.72ms
step:1637/1920 train_time:94520ms step_avg:57.74ms
step:1638/1920 train_time:94609ms step_avg:57.76ms
step:1639/1920 train_time:94697ms step_avg:57.78ms
step:1640/1920 train_time:94785ms step_avg:57.80ms
step:1641/1920 train_time:94874ms step_avg:57.81ms
step:1642/1920 train_time:94963ms step_avg:57.83ms
step:1643/1920 train_time:95052ms step_avg:57.85ms
step:1644/1920 train_time:95140ms step_avg:57.87ms
step:1645/1920 train_time:95227ms step_avg:57.89ms
step:1646/1920 train_time:95315ms step_avg:57.91ms
step:1647/1920 train_time:95403ms step_avg:57.93ms
step:1648/1920 train_time:95491ms step_avg:57.94ms
step:1649/1920 train_time:95580ms step_avg:57.96ms
step:1650/1920 train_time:95668ms step_avg:57.98ms
step:1651/1920 train_time:95757ms step_avg:58.00ms
step:1652/1920 train_time:95845ms step_avg:58.02ms
step:1653/1920 train_time:95934ms step_avg:58.04ms
step:1654/1920 train_time:96023ms step_avg:58.05ms
step:1655/1920 train_time:96111ms step_avg:58.07ms
step:1656/1920 train_time:96199ms step_avg:58.09ms
step:1657/1920 train_time:96288ms step_avg:58.11ms
step:1658/1920 train_time:96375ms step_avg:58.13ms
step:1659/1920 train_time:96464ms step_avg:58.15ms
step:1660/1920 train_time:96552ms step_avg:58.16ms
step:1661/1920 train_time:96640ms step_avg:58.18ms
step:1662/1920 train_time:96728ms step_avg:58.20ms
step:1663/1920 train_time:96817ms step_avg:58.22ms
step:1664/1920 train_time:96905ms step_avg:58.24ms
step:1665/1920 train_time:96996ms step_avg:58.26ms
step:1666/1920 train_time:97084ms step_avg:58.27ms
step:1667/1920 train_time:97173ms step_avg:58.29ms
step:1668/1920 train_time:97261ms step_avg:58.31ms
step:1669/1920 train_time:97350ms step_avg:58.33ms
step:1670/1920 train_time:97437ms step_avg:58.35ms
step:1671/1920 train_time:97525ms step_avg:58.36ms
step:1672/1920 train_time:97613ms step_avg:58.38ms
step:1673/1920 train_time:97702ms step_avg:58.40ms
step:1674/1920 train_time:97791ms step_avg:58.42ms
step:1675/1920 train_time:97880ms step_avg:58.44ms
step:1676/1920 train_time:97969ms step_avg:58.45ms
step:1677/1920 train_time:98058ms step_avg:58.47ms
step:1678/1920 train_time:98146ms step_avg:58.49ms
step:1679/1920 train_time:98235ms step_avg:58.51ms
step:1680/1920 train_time:98323ms step_avg:58.53ms
step:1681/1920 train_time:98413ms step_avg:58.54ms
step:1682/1920 train_time:98501ms step_avg:58.56ms
step:1683/1920 train_time:98590ms step_avg:58.58ms
step:1684/1920 train_time:98678ms step_avg:58.60ms
step:1685/1920 train_time:98767ms step_avg:58.62ms
step:1686/1920 train_time:98855ms step_avg:58.63ms
step:1687/1920 train_time:98944ms step_avg:58.65ms
step:1688/1920 train_time:99032ms step_avg:58.67ms
step:1689/1920 train_time:99121ms step_avg:58.69ms
step:1690/1920 train_time:99210ms step_avg:58.70ms
step:1691/1920 train_time:99298ms step_avg:58.72ms
step:1692/1920 train_time:99386ms step_avg:58.74ms
step:1693/1920 train_time:99475ms step_avg:58.76ms
step:1694/1920 train_time:99562ms step_avg:58.77ms
step:1695/1920 train_time:99651ms step_avg:58.79ms
step:1696/1920 train_time:99739ms step_avg:58.81ms
step:1697/1920 train_time:99828ms step_avg:58.83ms
step:1698/1920 train_time:99915ms step_avg:58.84ms
step:1699/1920 train_time:100004ms step_avg:58.86ms
step:1700/1920 train_time:100093ms step_avg:58.88ms
step:1701/1920 train_time:100182ms step_avg:58.90ms
step:1702/1920 train_time:100270ms step_avg:58.91ms
step:1703/1920 train_time:100359ms step_avg:58.93ms
step:1704/1920 train_time:100447ms step_avg:58.95ms
step:1705/1920 train_time:100536ms step_avg:58.97ms
step:1706/1920 train_time:100624ms step_avg:58.98ms
step:1707/1920 train_time:100714ms step_avg:59.00ms
step:1708/1920 train_time:100802ms step_avg:59.02ms
step:1709/1920 train_time:100890ms step_avg:59.03ms
step:1710/1920 train_time:100978ms step_avg:59.05ms
step:1711/1920 train_time:101068ms step_avg:59.07ms
step:1712/1920 train_time:101156ms step_avg:59.09ms
step:1713/1920 train_time:101246ms step_avg:59.10ms
step:1714/1920 train_time:101334ms step_avg:59.12ms
step:1715/1920 train_time:101423ms step_avg:59.14ms
step:1716/1920 train_time:101511ms step_avg:59.16ms
step:1717/1920 train_time:101599ms step_avg:59.17ms
step:1718/1920 train_time:101687ms step_avg:59.19ms
step:1719/1920 train_time:101776ms step_avg:59.21ms
step:1720/1920 train_time:101865ms step_avg:59.22ms
step:1721/1920 train_time:101954ms step_avg:59.24ms
step:1722/1920 train_time:102042ms step_avg:59.26ms
step:1723/1920 train_time:102131ms step_avg:59.28ms
step:1724/1920 train_time:102219ms step_avg:59.29ms
step:1725/1920 train_time:102308ms step_avg:59.31ms
step:1726/1920 train_time:102395ms step_avg:59.33ms
step:1727/1920 train_time:102484ms step_avg:59.34ms
step:1728/1920 train_time:102573ms step_avg:59.36ms
step:1729/1920 train_time:102662ms step_avg:59.38ms
step:1730/1920 train_time:102750ms step_avg:59.39ms
step:1731/1920 train_time:102838ms step_avg:59.41ms
step:1732/1920 train_time:102926ms step_avg:59.43ms
step:1733/1920 train_time:103015ms step_avg:59.44ms
step:1734/1920 train_time:103104ms step_avg:59.46ms
step:1735/1920 train_time:103194ms step_avg:59.48ms
step:1736/1920 train_time:103282ms step_avg:59.49ms
step:1737/1920 train_time:103371ms step_avg:59.51ms
step:1738/1920 train_time:103458ms step_avg:59.53ms
step:1739/1920 train_time:103547ms step_avg:59.54ms
step:1740/1920 train_time:103635ms step_avg:59.56ms
step:1741/1920 train_time:103723ms step_avg:59.58ms
step:1742/1920 train_time:103812ms step_avg:59.59ms
step:1743/1920 train_time:103900ms step_avg:59.61ms
step:1744/1920 train_time:103988ms step_avg:59.63ms
step:1745/1920 train_time:104076ms step_avg:59.64ms
step:1746/1920 train_time:104165ms step_avg:59.66ms
step:1747/1920 train_time:104254ms step_avg:59.68ms
step:1748/1920 train_time:104343ms step_avg:59.69ms
step:1749/1920 train_time:104432ms step_avg:59.71ms
step:1750/1920 train_time:104520ms step_avg:59.73ms
step:1750/1920 val_loss:3.3236 train_time:104611ms step_avg:59.78ms
step:1751/1920 train_time:104628ms step_avg:59.75ms
step:1752/1920 train_time:104700ms step_avg:59.76ms
step:1753/1920 train_time:104793ms step_avg:59.78ms
step:1754/1920 train_time:104881ms step_avg:59.80ms
step:1755/1920 train_time:104969ms step_avg:59.81ms
step:1756/1920 train_time:105058ms step_avg:59.83ms
step:1757/1920 train_time:105145ms step_avg:59.84ms
step:1758/1920 train_time:105232ms step_avg:59.86ms
step:1759/1920 train_time:105319ms step_avg:59.87ms
step:1760/1920 train_time:105409ms step_avg:59.89ms
step:1761/1920 train_time:105496ms step_avg:59.91ms
step:1762/1920 train_time:105586ms step_avg:59.92ms
step:1763/1920 train_time:105676ms step_avg:59.94ms
step:1764/1920 train_time:105767ms step_avg:59.96ms
step:1765/1920 train_time:105856ms step_avg:59.97ms
step:1766/1920 train_time:105944ms step_avg:59.99ms
step:1767/1920 train_time:106032ms step_avg:60.01ms
step:1768/1920 train_time:106120ms step_avg:60.02ms
step:1769/1920 train_time:106209ms step_avg:60.04ms
step:1770/1920 train_time:106296ms step_avg:60.05ms
step:1771/1920 train_time:106385ms step_avg:60.07ms
step:1772/1920 train_time:106472ms step_avg:60.09ms
step:1773/1920 train_time:106562ms step_avg:60.10ms
step:1774/1920 train_time:106651ms step_avg:60.12ms
step:1775/1920 train_time:106741ms step_avg:60.14ms
step:1776/1920 train_time:106830ms step_avg:60.15ms
step:1777/1920 train_time:106920ms step_avg:60.17ms
step:1778/1920 train_time:107008ms step_avg:60.18ms
step:1779/1920 train_time:107095ms step_avg:60.20ms
step:1780/1920 train_time:107182ms step_avg:60.21ms
step:1781/1920 train_time:107272ms step_avg:60.23ms
step:1782/1920 train_time:107359ms step_avg:60.25ms
step:1783/1920 train_time:107449ms step_avg:60.26ms
step:1784/1920 train_time:107537ms step_avg:60.28ms
step:1785/1920 train_time:107626ms step_avg:60.29ms
step:1786/1920 train_time:107715ms step_avg:60.31ms
step:1787/1920 train_time:107803ms step_avg:60.33ms
step:1788/1920 train_time:107891ms step_avg:60.34ms
step:1789/1920 train_time:107980ms step_avg:60.36ms
step:1790/1920 train_time:108069ms step_avg:60.37ms
step:1791/1920 train_time:108156ms step_avg:60.39ms
step:1792/1920 train_time:108245ms step_avg:60.40ms
step:1793/1920 train_time:108332ms step_avg:60.42ms
step:1794/1920 train_time:108421ms step_avg:60.44ms
step:1795/1920 train_time:108510ms step_avg:60.45ms
step:1796/1920 train_time:108599ms step_avg:60.47ms
step:1797/1920 train_time:108688ms step_avg:60.48ms
step:1798/1920 train_time:108776ms step_avg:60.50ms
step:1799/1920 train_time:108865ms step_avg:60.51ms
step:1800/1920 train_time:108954ms step_avg:60.53ms
step:1801/1920 train_time:109042ms step_avg:60.55ms
step:1802/1920 train_time:109130ms step_avg:60.56ms
step:1803/1920 train_time:109219ms step_avg:60.58ms
step:1804/1920 train_time:109307ms step_avg:60.59ms
step:1805/1920 train_time:109395ms step_avg:60.61ms
step:1806/1920 train_time:109485ms step_avg:60.62ms
step:1807/1920 train_time:109573ms step_avg:60.64ms
step:1808/1920 train_time:109661ms step_avg:60.65ms
step:1809/1920 train_time:109752ms step_avg:60.67ms
step:1810/1920 train_time:109841ms step_avg:60.69ms
step:1811/1920 train_time:109930ms step_avg:60.70ms
step:1812/1920 train_time:110018ms step_avg:60.72ms
step:1813/1920 train_time:110106ms step_avg:60.73ms
step:1814/1920 train_time:110194ms step_avg:60.75ms
step:1815/1920 train_time:110283ms step_avg:60.76ms
step:1816/1920 train_time:110370ms step_avg:60.78ms
step:1817/1920 train_time:110460ms step_avg:60.79ms
step:1818/1920 train_time:110549ms step_avg:60.81ms
step:1819/1920 train_time:110638ms step_avg:60.82ms
step:1820/1920 train_time:110727ms step_avg:60.84ms
step:1821/1920 train_time:110816ms step_avg:60.85ms
step:1822/1920 train_time:110905ms step_avg:60.87ms
step:1823/1920 train_time:110993ms step_avg:60.89ms
step:1824/1920 train_time:111081ms step_avg:60.90ms
step:1825/1920 train_time:111170ms step_avg:60.92ms
step:1826/1920 train_time:111258ms step_avg:60.93ms
step:1827/1920 train_time:111348ms step_avg:60.95ms
step:1828/1920 train_time:111435ms step_avg:60.96ms
step:1829/1920 train_time:111523ms step_avg:60.98ms
step:1830/1920 train_time:111612ms step_avg:60.99ms
step:1831/1920 train_time:111701ms step_avg:61.01ms
step:1832/1920 train_time:111789ms step_avg:61.02ms
step:1833/1920 train_time:111878ms step_avg:61.04ms
step:1834/1920 train_time:111967ms step_avg:61.05ms
step:1835/1920 train_time:112055ms step_avg:61.07ms
step:1836/1920 train_time:112143ms step_avg:61.08ms
step:1837/1920 train_time:112232ms step_avg:61.10ms
step:1838/1920 train_time:112321ms step_avg:61.11ms
step:1839/1920 train_time:112410ms step_avg:61.13ms
step:1840/1920 train_time:112498ms step_avg:61.14ms
step:1841/1920 train_time:112587ms step_avg:61.16ms
step:1842/1920 train_time:112676ms step_avg:61.17ms
step:1843/1920 train_time:112764ms step_avg:61.19ms
step:1844/1920 train_time:112852ms step_avg:61.20ms
step:1845/1920 train_time:112941ms step_avg:61.21ms
step:1846/1920 train_time:113030ms step_avg:61.23ms
step:1847/1920 train_time:113118ms step_avg:61.24ms
step:1848/1920 train_time:113207ms step_avg:61.26ms
step:1849/1920 train_time:113296ms step_avg:61.27ms
step:1850/1920 train_time:113384ms step_avg:61.29ms
step:1851/1920 train_time:113472ms step_avg:61.30ms
step:1852/1920 train_time:113560ms step_avg:61.32ms
step:1853/1920 train_time:113652ms step_avg:61.33ms
step:1854/1920 train_time:113740ms step_avg:61.35ms
step:1855/1920 train_time:113830ms step_avg:61.36ms
step:1856/1920 train_time:113918ms step_avg:61.38ms
step:1857/1920 train_time:114007ms step_avg:61.39ms
step:1858/1920 train_time:114094ms step_avg:61.41ms
step:1859/1920 train_time:114182ms step_avg:61.42ms
step:1860/1920 train_time:114271ms step_avg:61.44ms
step:1861/1920 train_time:114359ms step_avg:61.45ms
step:1862/1920 train_time:114448ms step_avg:61.47ms
step:1863/1920 train_time:114537ms step_avg:61.48ms
step:1864/1920 train_time:114625ms step_avg:61.49ms
step:1865/1920 train_time:114714ms step_avg:61.51ms
step:1866/1920 train_time:114802ms step_avg:61.52ms
step:1867/1920 train_time:114892ms step_avg:61.54ms
step:1868/1920 train_time:114981ms step_avg:61.55ms
step:1869/1920 train_time:115071ms step_avg:61.57ms
step:1870/1920 train_time:115159ms step_avg:61.58ms
step:1871/1920 train_time:115248ms step_avg:61.60ms
step:1872/1920 train_time:115335ms step_avg:61.61ms
step:1873/1920 train_time:115424ms step_avg:61.62ms
step:1874/1920 train_time:115512ms step_avg:61.64ms
step:1875/1920 train_time:115601ms step_avg:61.65ms
step:1876/1920 train_time:115690ms step_avg:61.67ms
step:1877/1920 train_time:115779ms step_avg:61.68ms
step:1878/1920 train_time:115867ms step_avg:61.70ms
step:1879/1920 train_time:115955ms step_avg:61.71ms
step:1880/1920 train_time:116043ms step_avg:61.73ms
step:1881/1920 train_time:116132ms step_avg:61.74ms
step:1882/1920 train_time:116220ms step_avg:61.75ms
step:1883/1920 train_time:116310ms step_avg:61.77ms
step:1884/1920 train_time:116398ms step_avg:61.78ms
step:1885/1920 train_time:116487ms step_avg:61.80ms
step:1886/1920 train_time:116575ms step_avg:61.81ms
step:1887/1920 train_time:116664ms step_avg:61.83ms
step:1888/1920 train_time:116752ms step_avg:61.84ms
step:1889/1920 train_time:116842ms step_avg:61.85ms
step:1890/1920 train_time:116931ms step_avg:61.87ms
step:1891/1920 train_time:117020ms step_avg:61.88ms
step:1892/1920 train_time:117109ms step_avg:61.90ms
step:1893/1920 train_time:117197ms step_avg:61.91ms
step:1894/1920 train_time:117285ms step_avg:61.92ms
step:1895/1920 train_time:117373ms step_avg:61.94ms
step:1896/1920 train_time:117461ms step_avg:61.95ms
step:1897/1920 train_time:117550ms step_avg:61.97ms
step:1898/1920 train_time:117638ms step_avg:61.98ms
step:1899/1920 train_time:117729ms step_avg:62.00ms
step:1900/1920 train_time:117818ms step_avg:62.01ms
step:1901/1920 train_time:117907ms step_avg:62.02ms
step:1902/1920 train_time:117995ms step_avg:62.04ms
step:1903/1920 train_time:118085ms step_avg:62.05ms
step:1904/1920 train_time:118172ms step_avg:62.07ms
step:1905/1920 train_time:118262ms step_avg:62.08ms
step:1906/1920 train_time:118350ms step_avg:62.09ms
step:1907/1920 train_time:118438ms step_avg:62.11ms
step:1908/1920 train_time:118527ms step_avg:62.12ms
step:1909/1920 train_time:118617ms step_avg:62.14ms
step:1910/1920 train_time:118705ms step_avg:62.15ms
step:1911/1920 train_time:118794ms step_avg:62.16ms
step:1912/1920 train_time:118882ms step_avg:62.18ms
step:1913/1920 train_time:118971ms step_avg:62.19ms
step:1914/1920 train_time:119060ms step_avg:62.20ms
step:1915/1920 train_time:119149ms step_avg:62.22ms
step:1916/1920 train_time:119237ms step_avg:62.23ms
step:1917/1920 train_time:119327ms step_avg:62.25ms
step:1918/1920 train_time:119416ms step_avg:62.26ms
step:1919/1920 train_time:119505ms step_avg:62.27ms
step:1920/1920 train_time:119594ms step_avg:62.29ms
step:1920/1920 val_loss:3.2795 train_time:119685ms step_avg:62.34ms
peak memory allocated: 29863 MiB reserved: 44898 MiB

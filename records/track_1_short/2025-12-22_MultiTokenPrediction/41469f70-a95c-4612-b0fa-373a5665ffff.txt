import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int, mtp_weights: Tensor = None):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1880  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

# Precompute MTP weights for all steps to avoid tensor allocation during training
# Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
mtp_weights_schedule = []
for s in range(args.num_iterations + 1):
    x = s / args.num_scheduled_iterations
    if x < 1/3:
        w = [1.0, 0.5, 0.25 * (1 - 3*x)]
    elif x < 2/3:
        w = [1.0, 0.5 * (1 - (3*x - 1))]
    else:
        w = [1.0]
    mtp_weights_schedule.append(torch.tensor(w, device=device))

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps

    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0

    if in_transition:
        adam_optimizers[1].transition_steps -= 1

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

base_weights = torch.tensor([1.0, 0.5, 0.25], device=device)
for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    mtp_weights = base_weights[:max(1, 3 - idx)].clone()
    model.split_embed = (idx >= args.split_embed_frac * len(args.ws_schedule))
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        (model(inputs, targets, cum_seqlens, ws_long//2, ws_long, mtp_weights) / grad_accum_steps).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        model.split_embed = (ws_idx >= 1)
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state
model.split_embed = False


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations)
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True
    if step == split_step:
        model.split_embed = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps)
        is_transition = True

    step_batch_size = new_step_batch_size
    mtp_weights = mtp_weights_schedule[step]
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long, mtp_weights) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    # copy lm_head weights/state to embed before split
    if step == (split_step - 2) | 1:
        adam_optimizers[0].copy_lm_to_embed()

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 (main, Dec  9 2025, 19:02:36) [Clang 21.1.4 ]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 22 11:19:53 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            129W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   35C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   44C    P0            133W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   43C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   42C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   35C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    237849      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    237850      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    237851      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    237852      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    237853      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    237854      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    237855      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    237856      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    237850      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    237851      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    237852      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    237853      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    237854      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    237855      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    237856      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1920 val_loss:10.8368 train_time:0ms step_avg:0.04ms
step:1/1920 train_time:68ms step_avg:67.84ms
step:2/1920 train_time:89ms step_avg:44.74ms
step:3/1920 train_time:118ms step_avg:39.43ms
step:4/1920 train_time:152ms step_avg:38.04ms
step:5/1920 train_time:186ms step_avg:37.27ms
step:6/1920 train_time:258ms step_avg:43.03ms
step:7/1920 train_time:282ms step_avg:40.22ms
step:8/1920 train_time:316ms step_avg:39.46ms
step:9/1920 train_time:350ms step_avg:38.88ms
step:10/1920 train_time:384ms step_avg:38.40ms
step:11/1920 train_time:418ms step_avg:38.03ms
step:12/1920 train_time:453ms step_avg:37.71ms
step:13/1920 train_time:487ms step_avg:37.45ms
step:14/1920 train_time:521ms step_avg:37.21ms
step:15/1920 train_time:556ms step_avg:37.04ms
step:16/1920 train_time:590ms step_avg:36.87ms
step:17/1920 train_time:624ms step_avg:36.72ms
step:18/1920 train_time:658ms step_avg:36.58ms
step:19/1920 train_time:693ms step_avg:36.45ms
step:20/1920 train_time:727ms step_avg:36.34ms
step:21/1920 train_time:761ms step_avg:36.25ms
step:22/1920 train_time:795ms step_avg:36.15ms
step:23/1920 train_time:830ms step_avg:36.07ms
step:24/1920 train_time:864ms step_avg:36.00ms
step:25/1920 train_time:898ms step_avg:35.93ms
step:26/1920 train_time:932ms step_avg:35.86ms
step:27/1920 train_time:967ms step_avg:35.81ms
step:28/1920 train_time:1001ms step_avg:35.75ms
step:29/1920 train_time:1036ms step_avg:35.71ms
step:30/1920 train_time:1070ms step_avg:35.66ms
step:31/1920 train_time:1104ms step_avg:35.61ms
step:32/1920 train_time:1138ms step_avg:35.57ms
step:33/1920 train_time:1173ms step_avg:35.54ms
step:34/1920 train_time:1207ms step_avg:35.51ms
step:35/1920 train_time:1243ms step_avg:35.50ms
step:36/1920 train_time:1277ms step_avg:35.48ms
step:37/1920 train_time:1313ms step_avg:35.49ms
step:38/1920 train_time:1348ms step_avg:35.46ms
step:39/1920 train_time:1382ms step_avg:35.44ms
step:40/1920 train_time:1417ms step_avg:35.43ms
step:41/1920 train_time:1452ms step_avg:35.42ms
step:42/1920 train_time:1486ms step_avg:35.39ms
step:43/1920 train_time:1521ms step_avg:35.37ms
step:44/1920 train_time:1555ms step_avg:35.35ms
step:45/1920 train_time:1590ms step_avg:35.34ms
step:46/1920 train_time:1625ms step_avg:35.32ms
step:47/1920 train_time:1659ms step_avg:35.30ms
step:48/1920 train_time:1694ms step_avg:35.28ms
step:49/1920 train_time:1728ms step_avg:35.27ms
step:50/1920 train_time:1762ms step_avg:35.25ms
step:51/1920 train_time:1797ms step_avg:35.23ms
step:52/1920 train_time:1831ms step_avg:35.21ms
step:53/1920 train_time:1866ms step_avg:35.20ms
step:54/1920 train_time:1900ms step_avg:35.18ms
step:55/1920 train_time:1934ms step_avg:35.17ms
step:56/1920 train_time:1969ms step_avg:35.15ms
step:57/1920 train_time:2003ms step_avg:35.14ms
step:58/1920 train_time:2037ms step_avg:35.12ms
step:59/1920 train_time:2072ms step_avg:35.11ms
step:60/1920 train_time:2106ms step_avg:35.09ms
step:61/1920 train_time:2140ms step_avg:35.08ms
step:62/1920 train_time:2175ms step_avg:35.07ms
step:63/1920 train_time:2209ms step_avg:35.07ms
step:64/1920 train_time:2244ms step_avg:35.06ms
step:65/1920 train_time:2278ms step_avg:35.05ms
step:66/1920 train_time:2313ms step_avg:35.04ms
step:67/1920 train_time:2347ms step_avg:35.03ms
step:68/1920 train_time:2382ms step_avg:35.03ms
step:69/1920 train_time:2416ms step_avg:35.02ms
step:70/1920 train_time:2450ms step_avg:35.00ms
step:71/1920 train_time:2485ms step_avg:35.00ms
step:72/1920 train_time:2519ms step_avg:34.99ms
step:73/1920 train_time:2554ms step_avg:34.98ms
step:74/1920 train_time:2588ms step_avg:34.97ms
step:75/1920 train_time:2622ms step_avg:34.96ms
step:76/1920 train_time:2657ms step_avg:34.96ms
step:77/1920 train_time:2691ms step_avg:34.95ms
step:78/1920 train_time:2726ms step_avg:34.95ms
step:79/1920 train_time:2760ms step_avg:34.94ms
step:80/1920 train_time:2794ms step_avg:34.93ms
step:81/1920 train_time:2829ms step_avg:34.93ms
step:82/1920 train_time:2863ms step_avg:34.92ms
step:83/1920 train_time:2898ms step_avg:34.91ms
step:84/1920 train_time:2932ms step_avg:34.91ms
step:85/1920 train_time:2966ms step_avg:34.90ms
step:86/1920 train_time:3001ms step_avg:34.89ms
step:87/1920 train_time:3035ms step_avg:34.88ms
step:88/1920 train_time:3069ms step_avg:34.88ms
step:89/1920 train_time:3103ms step_avg:34.87ms
step:90/1920 train_time:3138ms step_avg:34.86ms
step:91/1920 train_time:3172ms step_avg:34.86ms
step:92/1920 train_time:3207ms step_avg:34.85ms
step:93/1920 train_time:3241ms step_avg:34.85ms
step:94/1920 train_time:3275ms step_avg:34.84ms
step:95/1920 train_time:3310ms step_avg:34.84ms
step:96/1920 train_time:3345ms step_avg:34.84ms
step:97/1920 train_time:3379ms step_avg:34.83ms
step:98/1920 train_time:3413ms step_avg:34.83ms
step:99/1920 train_time:3448ms step_avg:34.83ms
step:100/1920 train_time:3482ms step_avg:34.82ms
step:101/1920 train_time:3517ms step_avg:34.82ms
step:102/1920 train_time:3551ms step_avg:34.81ms
step:103/1920 train_time:3586ms step_avg:34.81ms
step:104/1920 train_time:3620ms step_avg:34.80ms
step:105/1920 train_time:3655ms step_avg:34.81ms
step:106/1920 train_time:3689ms step_avg:34.80ms
step:107/1920 train_time:3723ms step_avg:34.79ms
step:108/1920 train_time:3757ms step_avg:34.79ms
step:109/1920 train_time:3792ms step_avg:34.79ms
step:110/1920 train_time:3826ms step_avg:34.78ms
step:111/1920 train_time:3861ms step_avg:34.79ms
step:112/1920 train_time:3895ms step_avg:34.78ms
step:113/1920 train_time:3929ms step_avg:34.77ms
step:114/1920 train_time:3964ms step_avg:34.77ms
step:115/1920 train_time:3998ms step_avg:34.76ms
step:116/1920 train_time:4032ms step_avg:34.76ms
step:117/1920 train_time:4067ms step_avg:34.76ms
step:118/1920 train_time:4101ms step_avg:34.75ms
step:119/1920 train_time:4135ms step_avg:34.75ms
step:120/1920 train_time:4169ms step_avg:34.75ms
step:121/1920 train_time:4204ms step_avg:34.74ms
step:122/1920 train_time:4238ms step_avg:34.74ms
step:123/1920 train_time:4273ms step_avg:34.74ms
step:124/1920 train_time:4306ms step_avg:34.73ms
step:125/1920 train_time:4341ms step_avg:34.73ms
step:126/1920 train_time:4376ms step_avg:34.73ms
step:127/1920 train_time:4411ms step_avg:34.73ms
step:128/1920 train_time:4446ms step_avg:34.73ms
step:129/1920 train_time:4480ms step_avg:34.73ms
step:130/1920 train_time:4515ms step_avg:34.73ms
step:131/1920 train_time:4549ms step_avg:34.73ms
step:132/1920 train_time:4584ms step_avg:34.72ms
step:133/1920 train_time:4618ms step_avg:34.72ms
step:134/1920 train_time:4652ms step_avg:34.72ms
step:135/1920 train_time:4686ms step_avg:34.71ms
step:136/1920 train_time:4721ms step_avg:34.71ms
step:137/1920 train_time:4755ms step_avg:34.71ms
step:138/1920 train_time:4789ms step_avg:34.70ms
step:139/1920 train_time:4823ms step_avg:34.70ms
step:140/1920 train_time:4858ms step_avg:34.70ms
step:141/1920 train_time:4892ms step_avg:34.70ms
step:142/1920 train_time:4927ms step_avg:34.70ms
step:143/1920 train_time:4961ms step_avg:34.69ms
step:144/1920 train_time:4995ms step_avg:34.69ms
step:145/1920 train_time:5029ms step_avg:34.69ms
step:146/1920 train_time:5063ms step_avg:34.68ms
step:147/1920 train_time:5098ms step_avg:34.68ms
step:148/1920 train_time:5132ms step_avg:34.68ms
step:149/1920 train_time:5166ms step_avg:34.67ms
step:150/1920 train_time:5201ms step_avg:34.67ms
step:151/1920 train_time:5235ms step_avg:34.67ms
step:152/1920 train_time:5269ms step_avg:34.66ms
step:153/1920 train_time:5303ms step_avg:34.66ms
step:154/1920 train_time:5337ms step_avg:34.66ms
step:155/1920 train_time:5373ms step_avg:34.66ms
step:156/1920 train_time:5407ms step_avg:34.66ms
step:157/1920 train_time:5441ms step_avg:34.66ms
step:158/1920 train_time:5476ms step_avg:34.66ms
step:159/1920 train_time:5511ms step_avg:34.66ms
step:160/1920 train_time:5545ms step_avg:34.66ms
step:161/1920 train_time:5580ms step_avg:34.66ms
step:162/1920 train_time:5614ms step_avg:34.66ms
step:163/1920 train_time:5649ms step_avg:34.66ms
step:164/1920 train_time:5683ms step_avg:34.65ms
step:165/1920 train_time:5718ms step_avg:34.65ms
step:166/1920 train_time:5752ms step_avg:34.65ms
step:167/1920 train_time:5786ms step_avg:34.65ms
step:168/1920 train_time:5821ms step_avg:34.65ms
step:169/1920 train_time:5855ms step_avg:34.64ms
step:170/1920 train_time:5889ms step_avg:34.64ms
step:171/1920 train_time:5923ms step_avg:34.64ms
step:172/1920 train_time:5958ms step_avg:34.64ms
step:173/1920 train_time:5992ms step_avg:34.64ms
step:174/1920 train_time:6027ms step_avg:34.64ms
step:175/1920 train_time:6061ms step_avg:34.63ms
step:176/1920 train_time:6095ms step_avg:34.63ms
step:177/1920 train_time:6130ms step_avg:34.63ms
step:178/1920 train_time:6164ms step_avg:34.63ms
step:179/1920 train_time:6199ms step_avg:34.63ms
step:180/1920 train_time:6233ms step_avg:34.63ms
step:181/1920 train_time:6268ms step_avg:34.63ms
step:182/1920 train_time:6302ms step_avg:34.63ms
step:183/1920 train_time:6336ms step_avg:34.62ms
step:184/1920 train_time:6371ms step_avg:34.62ms
step:185/1920 train_time:6405ms step_avg:34.62ms
step:186/1920 train_time:6439ms step_avg:34.62ms
step:187/1920 train_time:6474ms step_avg:34.62ms
step:188/1920 train_time:6508ms step_avg:34.62ms
step:189/1920 train_time:6543ms step_avg:34.62ms
step:190/1920 train_time:6577ms step_avg:34.62ms
step:191/1920 train_time:6613ms step_avg:34.62ms
step:192/1920 train_time:6647ms step_avg:34.62ms
step:193/1920 train_time:6681ms step_avg:34.62ms
step:194/1920 train_time:6715ms step_avg:34.62ms
step:195/1920 train_time:6750ms step_avg:34.62ms
step:196/1920 train_time:6785ms step_avg:34.62ms
step:197/1920 train_time:6819ms step_avg:34.62ms
step:198/1920 train_time:6854ms step_avg:34.61ms
step:199/1920 train_time:6888ms step_avg:34.61ms
step:200/1920 train_time:6922ms step_avg:34.61ms
step:201/1920 train_time:6956ms step_avg:34.61ms
step:202/1920 train_time:6991ms step_avg:34.61ms
step:203/1920 train_time:7025ms step_avg:34.61ms
step:204/1920 train_time:7059ms step_avg:34.60ms
step:205/1920 train_time:7094ms step_avg:34.60ms
step:206/1920 train_time:7128ms step_avg:34.60ms
step:207/1920 train_time:7162ms step_avg:34.60ms
step:208/1920 train_time:7196ms step_avg:34.60ms
step:209/1920 train_time:7231ms step_avg:34.60ms
step:210/1920 train_time:7265ms step_avg:34.59ms
step:211/1920 train_time:7300ms step_avg:34.60ms
step:212/1920 train_time:7334ms step_avg:34.59ms
step:213/1920 train_time:7369ms step_avg:34.59ms
step:214/1920 train_time:7403ms step_avg:34.59ms
step:215/1920 train_time:7438ms step_avg:34.59ms
step:216/1920 train_time:7472ms step_avg:34.59ms
step:217/1920 train_time:7506ms step_avg:34.59ms
step:218/1920 train_time:7541ms step_avg:34.59ms
step:219/1920 train_time:7575ms step_avg:34.59ms
step:220/1920 train_time:7609ms step_avg:34.59ms
step:221/1920 train_time:7644ms step_avg:34.59ms
step:222/1920 train_time:7678ms step_avg:34.59ms
step:223/1920 train_time:7713ms step_avg:34.59ms
step:224/1920 train_time:7748ms step_avg:34.59ms
step:225/1920 train_time:7782ms step_avg:34.59ms
step:226/1920 train_time:7817ms step_avg:34.59ms
step:227/1920 train_time:7852ms step_avg:34.59ms
step:228/1920 train_time:7886ms step_avg:34.59ms
step:229/1920 train_time:7921ms step_avg:34.59ms
step:230/1920 train_time:7955ms step_avg:34.59ms
step:231/1920 train_time:7990ms step_avg:34.59ms
step:232/1920 train_time:8024ms step_avg:34.59ms
step:233/1920 train_time:8058ms step_avg:34.59ms
step:234/1920 train_time:8093ms step_avg:34.58ms
step:235/1920 train_time:8127ms step_avg:34.58ms
step:236/1920 train_time:8161ms step_avg:34.58ms
step:237/1920 train_time:8195ms step_avg:34.58ms
step:238/1920 train_time:8229ms step_avg:34.58ms
step:239/1920 train_time:8264ms step_avg:34.58ms
step:240/1920 train_time:8298ms step_avg:34.57ms
step:241/1920 train_time:8332ms step_avg:34.57ms
step:242/1920 train_time:8366ms step_avg:34.57ms
step:243/1920 train_time:8401ms step_avg:34.57ms
step:244/1920 train_time:8435ms step_avg:34.57ms
step:245/1920 train_time:8470ms step_avg:34.57ms
step:246/1920 train_time:8504ms step_avg:34.57ms
step:247/1920 train_time:8539ms step_avg:34.57ms
step:248/1920 train_time:8573ms step_avg:34.57ms
step:249/1920 train_time:8608ms step_avg:34.57ms
step:250/1920 train_time:8642ms step_avg:34.57ms
step:250/1920 val_loss:4.6157 train_time:8680ms step_avg:34.72ms
step:251/1920 train_time:8701ms step_avg:34.67ms
step:252/1920 train_time:8719ms step_avg:34.60ms
step:253/1920 train_time:8749ms step_avg:34.58ms
step:254/1920 train_time:8784ms step_avg:34.58ms
step:255/1920 train_time:8819ms step_avg:34.59ms
step:256/1920 train_time:8854ms step_avg:34.59ms
step:257/1920 train_time:8889ms step_avg:34.59ms
step:258/1920 train_time:8923ms step_avg:34.59ms
step:259/1920 train_time:8958ms step_avg:34.59ms
step:260/1920 train_time:8992ms step_avg:34.58ms
step:261/1920 train_time:9026ms step_avg:34.58ms
step:262/1920 train_time:9060ms step_avg:34.58ms
step:263/1920 train_time:9094ms step_avg:34.58ms
step:264/1920 train_time:9128ms step_avg:34.58ms
step:265/1920 train_time:9162ms step_avg:34.57ms
step:266/1920 train_time:9196ms step_avg:34.57ms
step:267/1920 train_time:9230ms step_avg:34.57ms
step:268/1920 train_time:9265ms step_avg:34.57ms
step:269/1920 train_time:9299ms step_avg:34.57ms
step:270/1920 train_time:9333ms step_avg:34.57ms
step:271/1920 train_time:9367ms step_avg:34.56ms
step:272/1920 train_time:9401ms step_avg:34.56ms
step:273/1920 train_time:9435ms step_avg:34.56ms
step:274/1920 train_time:9469ms step_avg:34.56ms
step:275/1920 train_time:9503ms step_avg:34.56ms
step:276/1920 train_time:9537ms step_avg:34.55ms
step:277/1920 train_time:9571ms step_avg:34.55ms
step:278/1920 train_time:9606ms step_avg:34.55ms
step:279/1920 train_time:9640ms step_avg:34.55ms
step:280/1920 train_time:9674ms step_avg:34.55ms
step:281/1920 train_time:9709ms step_avg:34.55ms
step:282/1920 train_time:9743ms step_avg:34.55ms
step:283/1920 train_time:9778ms step_avg:34.55ms
step:284/1920 train_time:9813ms step_avg:34.55ms
step:285/1920 train_time:9848ms step_avg:34.55ms
step:286/1920 train_time:9882ms step_avg:34.55ms
step:287/1920 train_time:9917ms step_avg:34.55ms
step:288/1920 train_time:9951ms step_avg:34.55ms
step:289/1920 train_time:9985ms step_avg:34.55ms
step:290/1920 train_time:10020ms step_avg:34.55ms
step:291/1920 train_time:10054ms step_avg:34.55ms
step:292/1920 train_time:10088ms step_avg:34.55ms
step:293/1920 train_time:10123ms step_avg:34.55ms
step:294/1920 train_time:10157ms step_avg:34.55ms
step:295/1920 train_time:10191ms step_avg:34.55ms
step:296/1920 train_time:10225ms step_avg:34.54ms
step:297/1920 train_time:10259ms step_avg:34.54ms
step:298/1920 train_time:10293ms step_avg:34.54ms
step:299/1920 train_time:10328ms step_avg:34.54ms
step:300/1920 train_time:10362ms step_avg:34.54ms
step:301/1920 train_time:10396ms step_avg:34.54ms
step:302/1920 train_time:10430ms step_avg:34.54ms
step:303/1920 train_time:10465ms step_avg:34.54ms
step:304/1920 train_time:10499ms step_avg:34.54ms
step:305/1920 train_time:10533ms step_avg:34.53ms
step:306/1920 train_time:10567ms step_avg:34.53ms
step:307/1920 train_time:10601ms step_avg:34.53ms
step:308/1920 train_time:10636ms step_avg:34.53ms
step:309/1920 train_time:10670ms step_avg:34.53ms
step:310/1920 train_time:10704ms step_avg:34.53ms
step:311/1920 train_time:10739ms step_avg:34.53ms
step:312/1920 train_time:10774ms step_avg:34.53ms
step:313/1920 train_time:10808ms step_avg:34.53ms
step:314/1920 train_time:10842ms step_avg:34.53ms
step:315/1920 train_time:10877ms step_avg:34.53ms
step:316/1920 train_time:10912ms step_avg:34.53ms
step:317/1920 train_time:10946ms step_avg:34.53ms
step:318/1920 train_time:10980ms step_avg:34.53ms
step:319/1920 train_time:11015ms step_avg:34.53ms
step:320/1920 train_time:11049ms step_avg:34.53ms
step:321/1920 train_time:11083ms step_avg:34.53ms
step:322/1920 train_time:11118ms step_avg:34.53ms
step:323/1920 train_time:11152ms step_avg:34.53ms
step:324/1920 train_time:11186ms step_avg:34.53ms
step:325/1920 train_time:11220ms step_avg:34.52ms
step:326/1920 train_time:11254ms step_avg:34.52ms
step:327/1920 train_time:11288ms step_avg:34.52ms
step:328/1920 train_time:11323ms step_avg:34.52ms
step:329/1920 train_time:11357ms step_avg:34.52ms
step:330/1920 train_time:11391ms step_avg:34.52ms
step:331/1920 train_time:11426ms step_avg:34.52ms
step:332/1920 train_time:11460ms step_avg:34.52ms
step:333/1920 train_time:11494ms step_avg:34.52ms
step:334/1920 train_time:11528ms step_avg:34.52ms
step:335/1920 train_time:11563ms step_avg:34.52ms
step:336/1920 train_time:11597ms step_avg:34.51ms
step:337/1920 train_time:11631ms step_avg:34.51ms
step:338/1920 train_time:11665ms step_avg:34.51ms
step:339/1920 train_time:11700ms step_avg:34.51ms
step:340/1920 train_time:11734ms step_avg:34.51ms
step:341/1920 train_time:11769ms step_avg:34.51ms
step:342/1920 train_time:11803ms step_avg:34.51ms
step:343/1920 train_time:11837ms step_avg:34.51ms
step:344/1920 train_time:11872ms step_avg:34.51ms
step:345/1920 train_time:11906ms step_avg:34.51ms
step:346/1920 train_time:11940ms step_avg:34.51ms
step:347/1920 train_time:11974ms step_avg:34.51ms
step:348/1920 train_time:12009ms step_avg:34.51ms
step:349/1920 train_time:12043ms step_avg:34.51ms
step:350/1920 train_time:12077ms step_avg:34.51ms
step:351/1920 train_time:12112ms step_avg:34.51ms
step:352/1920 train_time:12146ms step_avg:34.51ms
step:353/1920 train_time:12180ms step_avg:34.50ms
step:354/1920 train_time:12214ms step_avg:34.50ms
step:355/1920 train_time:12249ms step_avg:34.50ms
step:356/1920 train_time:12282ms step_avg:34.50ms
step:357/1920 train_time:12317ms step_avg:34.50ms
step:358/1920 train_time:12351ms step_avg:34.50ms
step:359/1920 train_time:12385ms step_avg:34.50ms
step:360/1920 train_time:12419ms step_avg:34.50ms
step:361/1920 train_time:12454ms step_avg:34.50ms
step:362/1920 train_time:12488ms step_avg:34.50ms
step:363/1920 train_time:12523ms step_avg:34.50ms
step:364/1920 train_time:12557ms step_avg:34.50ms
step:365/1920 train_time:12591ms step_avg:34.50ms
step:366/1920 train_time:12626ms step_avg:34.50ms
step:367/1920 train_time:12660ms step_avg:34.49ms
step:368/1920 train_time:12694ms step_avg:34.49ms
step:369/1920 train_time:12728ms step_avg:34.49ms
step:370/1920 train_time:12762ms step_avg:34.49ms
step:371/1920 train_time:12797ms step_avg:34.49ms
step:372/1920 train_time:12831ms step_avg:34.49ms
step:373/1920 train_time:12865ms step_avg:34.49ms
step:374/1920 train_time:12900ms step_avg:34.49ms
step:375/1920 train_time:12934ms step_avg:34.49ms
step:376/1920 train_time:12968ms step_avg:34.49ms
step:377/1920 train_time:13002ms step_avg:34.49ms
step:378/1920 train_time:13037ms step_avg:34.49ms
step:379/1920 train_time:13072ms step_avg:34.49ms
step:380/1920 train_time:13106ms step_avg:34.49ms
step:381/1920 train_time:13140ms step_avg:34.49ms
step:382/1920 train_time:13174ms step_avg:34.49ms
step:383/1920 train_time:13209ms step_avg:34.49ms
step:384/1920 train_time:13243ms step_avg:34.49ms
step:385/1920 train_time:13278ms step_avg:34.49ms
step:386/1920 train_time:13312ms step_avg:34.49ms
step:387/1920 train_time:13347ms step_avg:34.49ms
step:388/1920 train_time:13381ms step_avg:34.49ms
step:389/1920 train_time:13415ms step_avg:34.49ms
step:390/1920 train_time:13449ms step_avg:34.48ms
step:391/1920 train_time:13483ms step_avg:34.48ms
step:392/1920 train_time:13517ms step_avg:34.48ms
step:393/1920 train_time:13552ms step_avg:34.48ms
step:394/1920 train_time:13586ms step_avg:34.48ms
step:395/1920 train_time:13620ms step_avg:34.48ms
step:396/1920 train_time:13654ms step_avg:34.48ms
step:397/1920 train_time:13689ms step_avg:34.48ms
step:398/1920 train_time:13723ms step_avg:34.48ms
step:399/1920 train_time:13757ms step_avg:34.48ms
step:400/1920 train_time:13791ms step_avg:34.48ms
step:401/1920 train_time:13825ms step_avg:34.48ms
step:402/1920 train_time:13860ms step_avg:34.48ms
step:403/1920 train_time:13894ms step_avg:34.48ms
step:404/1920 train_time:13928ms step_avg:34.48ms
step:405/1920 train_time:13963ms step_avg:34.48ms
step:406/1920 train_time:13997ms step_avg:34.48ms
step:407/1920 train_time:14031ms step_avg:34.48ms
step:408/1920 train_time:14065ms step_avg:34.47ms
step:409/1920 train_time:14100ms step_avg:34.47ms
step:410/1920 train_time:14134ms step_avg:34.47ms
step:411/1920 train_time:14169ms step_avg:34.47ms
step:412/1920 train_time:14203ms step_avg:34.47ms
step:413/1920 train_time:14237ms step_avg:34.47ms
step:414/1920 train_time:14271ms step_avg:34.47ms
step:415/1920 train_time:14306ms step_avg:34.47ms
step:416/1920 train_time:14340ms step_avg:34.47ms
step:417/1920 train_time:14375ms step_avg:34.47ms
step:418/1920 train_time:14409ms step_avg:34.47ms
step:419/1920 train_time:14443ms step_avg:34.47ms
step:420/1920 train_time:14478ms step_avg:34.47ms
step:421/1920 train_time:14512ms step_avg:34.47ms
step:422/1920 train_time:14546ms step_avg:34.47ms
step:423/1920 train_time:14580ms step_avg:34.47ms
step:424/1920 train_time:14615ms step_avg:34.47ms
step:425/1920 train_time:14649ms step_avg:34.47ms
step:426/1920 train_time:14683ms step_avg:34.47ms
step:427/1920 train_time:14718ms step_avg:34.47ms
step:428/1920 train_time:14752ms step_avg:34.47ms
step:429/1920 train_time:14786ms step_avg:34.47ms
step:430/1920 train_time:14820ms step_avg:34.47ms
step:431/1920 train_time:14855ms step_avg:34.47ms
step:432/1920 train_time:14889ms step_avg:34.46ms
step:433/1920 train_time:14923ms step_avg:34.46ms
step:434/1920 train_time:14957ms step_avg:34.46ms
step:435/1920 train_time:14992ms step_avg:34.46ms
step:436/1920 train_time:15026ms step_avg:34.46ms
step:437/1920 train_time:15061ms step_avg:34.46ms
step:438/1920 train_time:15095ms step_avg:34.46ms
step:439/1920 train_time:15129ms step_avg:34.46ms
step:440/1920 train_time:15163ms step_avg:34.46ms
step:441/1920 train_time:15198ms step_avg:34.46ms
step:442/1920 train_time:15232ms step_avg:34.46ms
step:443/1920 train_time:15267ms step_avg:34.46ms
step:444/1920 train_time:15301ms step_avg:34.46ms
step:445/1920 train_time:15336ms step_avg:34.46ms
step:446/1920 train_time:15370ms step_avg:34.46ms
step:447/1920 train_time:15404ms step_avg:34.46ms
step:448/1920 train_time:15438ms step_avg:34.46ms
step:449/1920 train_time:15473ms step_avg:34.46ms
step:450/1920 train_time:15507ms step_avg:34.46ms
step:451/1920 train_time:15542ms step_avg:34.46ms
step:452/1920 train_time:15576ms step_avg:34.46ms
step:453/1920 train_time:15611ms step_avg:34.46ms
step:454/1920 train_time:15645ms step_avg:34.46ms
step:455/1920 train_time:15679ms step_avg:34.46ms
step:456/1920 train_time:15713ms step_avg:34.46ms
step:457/1920 train_time:15748ms step_avg:34.46ms
step:458/1920 train_time:15782ms step_avg:34.46ms
step:459/1920 train_time:15816ms step_avg:34.46ms
step:460/1920 train_time:15851ms step_avg:34.46ms
step:461/1920 train_time:15885ms step_avg:34.46ms
step:462/1920 train_time:15919ms step_avg:34.46ms
step:463/1920 train_time:15954ms step_avg:34.46ms
step:464/1920 train_time:15988ms step_avg:34.46ms
step:465/1920 train_time:16023ms step_avg:34.46ms
step:466/1920 train_time:16057ms step_avg:34.46ms
step:467/1920 train_time:16091ms step_avg:34.46ms
step:468/1920 train_time:16125ms step_avg:34.46ms
step:469/1920 train_time:16159ms step_avg:34.46ms
step:470/1920 train_time:16194ms step_avg:34.45ms
step:471/1920 train_time:16228ms step_avg:34.45ms
step:472/1920 train_time:16262ms step_avg:34.45ms
step:473/1920 train_time:16296ms step_avg:34.45ms
step:474/1920 train_time:16330ms step_avg:34.45ms
step:475/1920 train_time:16365ms step_avg:34.45ms
step:476/1920 train_time:16399ms step_avg:34.45ms
step:477/1920 train_time:16433ms step_avg:34.45ms
step:478/1920 train_time:16468ms step_avg:34.45ms
step:479/1920 train_time:16502ms step_avg:34.45ms
step:480/1920 train_time:16536ms step_avg:34.45ms
step:481/1920 train_time:16570ms step_avg:34.45ms
step:482/1920 train_time:16604ms step_avg:34.45ms
step:483/1920 train_time:16639ms step_avg:34.45ms
step:484/1920 train_time:16673ms step_avg:34.45ms
step:485/1920 train_time:16707ms step_avg:34.45ms
step:486/1920 train_time:16741ms step_avg:34.45ms
step:487/1920 train_time:16776ms step_avg:34.45ms
step:488/1920 train_time:16810ms step_avg:34.45ms
step:489/1920 train_time:16844ms step_avg:34.45ms
step:490/1920 train_time:16879ms step_avg:34.45ms
step:491/1920 train_time:16913ms step_avg:34.45ms
step:492/1920 train_time:16948ms step_avg:34.45ms
step:493/1920 train_time:16982ms step_avg:34.45ms
step:494/1920 train_time:17016ms step_avg:34.45ms
step:495/1920 train_time:17051ms step_avg:34.45ms
step:496/1920 train_time:17085ms step_avg:34.45ms
step:497/1920 train_time:17120ms step_avg:34.45ms
step:498/1920 train_time:17154ms step_avg:34.45ms
step:499/1920 train_time:17189ms step_avg:34.45ms
step:500/1920 train_time:17223ms step_avg:34.45ms
step:500/1920 val_loss:4.2973 train_time:17261ms step_avg:34.52ms
step:501/1920 train_time:17280ms step_avg:34.49ms
step:502/1920 train_time:17299ms step_avg:34.46ms
step:503/1920 train_time:17329ms step_avg:34.45ms
step:504/1920 train_time:17364ms step_avg:34.45ms
step:505/1920 train_time:17401ms step_avg:34.46ms
step:506/1920 train_time:17435ms step_avg:34.46ms
step:507/1920 train_time:17470ms step_avg:34.46ms
step:508/1920 train_time:17505ms step_avg:34.46ms
step:509/1920 train_time:17540ms step_avg:34.46ms
step:510/1920 train_time:17574ms step_avg:34.46ms
step:511/1920 train_time:17608ms step_avg:34.46ms
step:512/1920 train_time:17642ms step_avg:34.46ms
step:513/1920 train_time:17677ms step_avg:34.46ms
step:514/1920 train_time:17711ms step_avg:34.46ms
step:515/1920 train_time:17745ms step_avg:34.46ms
step:516/1920 train_time:17779ms step_avg:34.46ms
step:517/1920 train_time:17813ms step_avg:34.45ms
step:518/1920 train_time:17847ms step_avg:34.45ms
step:519/1920 train_time:17881ms step_avg:34.45ms
step:520/1920 train_time:17915ms step_avg:34.45ms
step:521/1920 train_time:17950ms step_avg:34.45ms
step:522/1920 train_time:17984ms step_avg:34.45ms
step:523/1920 train_time:18018ms step_avg:34.45ms
step:524/1920 train_time:18051ms step_avg:34.45ms
step:525/1920 train_time:18086ms step_avg:34.45ms
step:526/1920 train_time:18120ms step_avg:34.45ms
step:527/1920 train_time:18154ms step_avg:34.45ms
step:528/1920 train_time:18188ms step_avg:34.45ms
step:529/1920 train_time:18223ms step_avg:34.45ms
step:530/1920 train_time:18257ms step_avg:34.45ms
step:531/1920 train_time:18291ms step_avg:34.45ms
step:532/1920 train_time:18326ms step_avg:34.45ms
step:533/1920 train_time:18360ms step_avg:34.45ms
step:534/1920 train_time:18395ms step_avg:34.45ms
step:535/1920 train_time:18430ms step_avg:34.45ms
step:536/1920 train_time:18464ms step_avg:34.45ms
step:537/1920 train_time:18499ms step_avg:34.45ms
step:538/1920 train_time:18534ms step_avg:34.45ms
step:539/1920 train_time:18568ms step_avg:34.45ms
step:540/1920 train_time:18602ms step_avg:34.45ms
step:541/1920 train_time:18637ms step_avg:34.45ms
step:542/1920 train_time:18671ms step_avg:34.45ms
step:543/1920 train_time:18705ms step_avg:34.45ms
step:544/1920 train_time:18739ms step_avg:34.45ms
step:545/1920 train_time:18774ms step_avg:34.45ms
step:546/1920 train_time:18808ms step_avg:34.45ms
step:547/1920 train_time:18842ms step_avg:34.45ms
step:548/1920 train_time:18876ms step_avg:34.45ms
step:549/1920 train_time:18910ms step_avg:34.45ms
step:550/1920 train_time:18945ms step_avg:34.44ms
step:551/1920 train_time:18979ms step_avg:34.44ms
step:552/1920 train_time:19013ms step_avg:34.44ms
step:553/1920 train_time:19047ms step_avg:34.44ms
step:554/1920 train_time:19081ms step_avg:34.44ms
step:555/1920 train_time:19116ms step_avg:34.44ms
step:556/1920 train_time:19150ms step_avg:34.44ms
step:557/1920 train_time:19184ms step_avg:34.44ms
step:558/1920 train_time:19218ms step_avg:34.44ms
step:559/1920 train_time:19253ms step_avg:34.44ms
step:560/1920 train_time:19287ms step_avg:34.44ms
step:561/1920 train_time:19322ms step_avg:34.44ms
step:562/1920 train_time:19356ms step_avg:34.44ms
step:563/1920 train_time:19391ms step_avg:34.44ms
step:564/1920 train_time:19426ms step_avg:34.44ms
step:565/1920 train_time:19460ms step_avg:34.44ms
step:566/1920 train_time:19495ms step_avg:34.44ms
step:567/1920 train_time:19529ms step_avg:34.44ms
step:568/1920 train_time:19563ms step_avg:34.44ms
step:569/1920 train_time:19597ms step_avg:34.44ms
step:570/1920 train_time:19632ms step_avg:34.44ms
step:571/1920 train_time:19666ms step_avg:34.44ms
step:572/1920 train_time:19700ms step_avg:34.44ms
step:573/1920 train_time:19735ms step_avg:34.44ms
step:574/1920 train_time:19769ms step_avg:34.44ms
step:575/1920 train_time:19803ms step_avg:34.44ms
step:576/1920 train_time:19838ms step_avg:34.44ms
step:577/1920 train_time:19872ms step_avg:34.44ms
step:578/1920 train_time:19906ms step_avg:34.44ms
step:579/1920 train_time:19940ms step_avg:34.44ms
step:580/1920 train_time:19975ms step_avg:34.44ms
step:581/1920 train_time:20009ms step_avg:34.44ms
step:582/1920 train_time:20043ms step_avg:34.44ms
step:583/1920 train_time:20077ms step_avg:34.44ms
step:584/1920 train_time:20111ms step_avg:34.44ms
step:585/1920 train_time:20145ms step_avg:34.44ms
step:586/1920 train_time:20180ms step_avg:34.44ms
step:587/1920 train_time:20214ms step_avg:34.44ms
step:588/1920 train_time:20249ms step_avg:34.44ms
step:589/1920 train_time:20283ms step_avg:34.44ms
step:590/1920 train_time:20317ms step_avg:34.44ms
step:591/1920 train_time:20352ms step_avg:34.44ms
step:592/1920 train_time:20386ms step_avg:34.44ms
step:593/1920 train_time:20420ms step_avg:34.44ms
step:594/1920 train_time:20455ms step_avg:34.44ms
step:595/1920 train_time:20489ms step_avg:34.44ms
step:596/1920 train_time:20523ms step_avg:34.44ms
step:597/1920 train_time:20558ms step_avg:34.44ms
step:598/1920 train_time:20592ms step_avg:34.44ms
step:599/1920 train_time:20626ms step_avg:34.43ms
step:600/1920 train_time:20661ms step_avg:34.43ms
step:601/1920 train_time:20695ms step_avg:34.43ms
step:602/1920 train_time:20729ms step_avg:34.43ms
step:603/1920 train_time:20763ms step_avg:34.43ms
step:604/1920 train_time:20797ms step_avg:34.43ms
step:605/1920 train_time:20832ms step_avg:34.43ms
step:606/1920 train_time:20866ms step_avg:34.43ms
step:607/1920 train_time:20901ms step_avg:34.43ms
step:608/1920 train_time:20935ms step_avg:34.43ms
step:609/1920 train_time:20970ms step_avg:34.43ms
step:610/1920 train_time:21004ms step_avg:34.43ms
step:611/1920 train_time:21038ms step_avg:34.43ms
step:612/1920 train_time:21073ms step_avg:34.43ms
step:613/1920 train_time:21107ms step_avg:34.43ms
step:614/1920 train_time:21141ms step_avg:34.43ms
step:615/1920 train_time:21175ms step_avg:34.43ms
step:616/1920 train_time:21209ms step_avg:34.43ms
step:617/1920 train_time:21244ms step_avg:34.43ms
step:618/1920 train_time:21278ms step_avg:34.43ms
step:619/1920 train_time:21312ms step_avg:34.43ms
step:620/1920 train_time:21346ms step_avg:34.43ms
step:621/1920 train_time:21381ms step_avg:34.43ms
step:622/1920 train_time:21415ms step_avg:34.43ms
step:623/1920 train_time:21449ms step_avg:34.43ms
step:624/1920 train_time:21484ms step_avg:34.43ms
step:625/1920 train_time:21518ms step_avg:34.43ms
step:626/1920 train_time:21553ms step_avg:34.43ms
step:627/1920 train_time:21588ms step_avg:34.43ms
step:628/1920 train_time:21623ms step_avg:34.43ms
step:629/1920 train_time:21684ms step_avg:34.47ms
step:630/1920 train_time:21745ms step_avg:34.52ms
step:631/1920 train_time:21809ms step_avg:34.56ms
step:632/1920 train_time:21870ms step_avg:34.60ms
step:633/1920 train_time:21934ms step_avg:34.65ms
step:634/1920 train_time:21995ms step_avg:34.69ms
step:635/1920 train_time:22058ms step_avg:34.74ms
step:636/1920 train_time:22119ms step_avg:34.78ms
step:637/1920 train_time:22182ms step_avg:34.82ms
step:638/1920 train_time:22243ms step_avg:34.86ms
step:639/1920 train_time:22306ms step_avg:34.91ms
step:640/1920 train_time:22367ms step_avg:34.95ms
step:641/1920 train_time:22430ms step_avg:34.99ms
step:642/1920 train_time:22492ms step_avg:35.03ms
step:643/1920 train_time:22555ms step_avg:35.08ms
step:644/1920 train_time:22617ms step_avg:35.12ms
step:645/1920 train_time:22679ms step_avg:35.16ms
step:646/1920 train_time:22741ms step_avg:35.20ms
step:647/1920 train_time:22804ms step_avg:35.25ms
step:648/1920 train_time:22866ms step_avg:35.29ms
step:649/1920 train_time:22929ms step_avg:35.33ms
step:650/1920 train_time:22991ms step_avg:35.37ms
step:651/1920 train_time:23054ms step_avg:35.41ms
step:652/1920 train_time:23116ms step_avg:35.45ms
step:653/1920 train_time:23178ms step_avg:35.50ms
step:654/1920 train_time:23240ms step_avg:35.53ms
step:655/1920 train_time:23303ms step_avg:35.58ms
step:656/1920 train_time:23364ms step_avg:35.62ms
step:657/1920 train_time:23427ms step_avg:35.66ms
step:658/1920 train_time:23489ms step_avg:35.70ms
step:659/1920 train_time:23553ms step_avg:35.74ms
step:660/1920 train_time:23614ms step_avg:35.78ms
step:661/1920 train_time:23676ms step_avg:35.82ms
step:662/1920 train_time:23739ms step_avg:35.86ms
step:663/1920 train_time:23802ms step_avg:35.90ms
step:664/1920 train_time:23863ms step_avg:35.94ms
step:665/1920 train_time:23926ms step_avg:35.98ms
step:666/1920 train_time:23988ms step_avg:36.02ms
step:667/1920 train_time:24052ms step_avg:36.06ms
step:668/1920 train_time:24114ms step_avg:36.10ms
step:669/1920 train_time:24176ms step_avg:36.14ms
step:670/1920 train_time:24238ms step_avg:36.18ms
step:671/1920 train_time:24301ms step_avg:36.22ms
step:672/1920 train_time:24363ms step_avg:36.25ms
step:673/1920 train_time:24425ms step_avg:36.29ms
step:674/1920 train_time:24486ms step_avg:36.33ms
step:675/1920 train_time:24548ms step_avg:36.37ms
step:676/1920 train_time:24610ms step_avg:36.41ms
step:677/1920 train_time:24673ms step_avg:36.45ms
step:678/1920 train_time:24735ms step_avg:36.48ms
step:679/1920 train_time:24798ms step_avg:36.52ms
step:680/1920 train_time:24860ms step_avg:36.56ms
step:681/1920 train_time:24924ms step_avg:36.60ms
step:682/1920 train_time:24985ms step_avg:36.63ms
step:683/1920 train_time:25048ms step_avg:36.67ms
step:684/1920 train_time:25110ms step_avg:36.71ms
step:685/1920 train_time:25173ms step_avg:36.75ms
step:686/1920 train_time:25234ms step_avg:36.78ms
step:687/1920 train_time:25297ms step_avg:36.82ms
step:688/1920 train_time:25359ms step_avg:36.86ms
step:689/1920 train_time:25421ms step_avg:36.90ms
step:690/1920 train_time:25483ms step_avg:36.93ms
step:691/1920 train_time:25545ms step_avg:36.97ms
step:692/1920 train_time:25607ms step_avg:37.00ms
step:693/1920 train_time:25670ms step_avg:37.04ms
step:694/1920 train_time:25732ms step_avg:37.08ms
step:695/1920 train_time:25795ms step_avg:37.12ms
step:696/1920 train_time:25857ms step_avg:37.15ms
step:697/1920 train_time:25920ms step_avg:37.19ms
step:698/1920 train_time:25982ms step_avg:37.22ms
step:699/1920 train_time:26044ms step_avg:37.26ms
step:700/1920 train_time:26106ms step_avg:37.29ms
step:701/1920 train_time:26169ms step_avg:37.33ms
step:702/1920 train_time:26232ms step_avg:37.37ms
step:703/1920 train_time:26295ms step_avg:37.40ms
step:704/1920 train_time:26356ms step_avg:37.44ms
step:705/1920 train_time:26419ms step_avg:37.47ms
step:706/1920 train_time:26481ms step_avg:37.51ms
step:707/1920 train_time:26543ms step_avg:37.54ms
step:708/1920 train_time:26604ms step_avg:37.58ms
step:709/1920 train_time:26667ms step_avg:37.61ms
step:710/1920 train_time:26729ms step_avg:37.65ms
step:711/1920 train_time:26793ms step_avg:37.68ms
step:712/1920 train_time:26855ms step_avg:37.72ms
step:713/1920 train_time:26918ms step_avg:37.75ms
step:714/1920 train_time:26980ms step_avg:37.79ms
step:715/1920 train_time:27043ms step_avg:37.82ms
step:716/1920 train_time:27104ms step_avg:37.86ms
step:717/1920 train_time:27168ms step_avg:37.89ms
step:718/1920 train_time:27230ms step_avg:37.92ms
step:719/1920 train_time:27293ms step_avg:37.96ms
step:720/1920 train_time:27355ms step_avg:37.99ms
step:721/1920 train_time:27418ms step_avg:38.03ms
step:722/1920 train_time:27480ms step_avg:38.06ms
step:723/1920 train_time:27542ms step_avg:38.09ms
step:724/1920 train_time:27604ms step_avg:38.13ms
step:725/1920 train_time:27667ms step_avg:38.16ms
step:726/1920 train_time:27728ms step_avg:38.19ms
step:727/1920 train_time:27791ms step_avg:38.23ms
step:728/1920 train_time:27853ms step_avg:38.26ms
step:729/1920 train_time:27916ms step_avg:38.29ms
step:730/1920 train_time:27978ms step_avg:38.33ms
step:731/1920 train_time:28040ms step_avg:38.36ms
step:732/1920 train_time:28103ms step_avg:38.39ms
step:733/1920 train_time:28165ms step_avg:38.42ms
step:734/1920 train_time:28227ms step_avg:38.46ms
step:735/1920 train_time:28290ms step_avg:38.49ms
step:736/1920 train_time:28353ms step_avg:38.52ms
step:737/1920 train_time:28416ms step_avg:38.56ms
step:738/1920 train_time:28478ms step_avg:38.59ms
step:739/1920 train_time:28540ms step_avg:38.62ms
step:740/1920 train_time:28601ms step_avg:38.65ms
step:741/1920 train_time:28664ms step_avg:38.68ms
step:742/1920 train_time:28725ms step_avg:38.71ms
step:743/1920 train_time:28788ms step_avg:38.75ms
step:744/1920 train_time:28850ms step_avg:38.78ms
step:745/1920 train_time:28913ms step_avg:38.81ms
step:746/1920 train_time:28975ms step_avg:38.84ms
step:747/1920 train_time:29038ms step_avg:38.87ms
step:748/1920 train_time:29100ms step_avg:38.90ms
step:749/1920 train_time:29162ms step_avg:38.93ms
step:750/1920 train_time:29224ms step_avg:38.97ms
step:750/1920 val_loss:4.0349 train_time:29290ms step_avg:39.05ms
step:751/1920 train_time:29308ms step_avg:39.03ms
step:752/1920 train_time:29350ms step_avg:39.03ms
step:753/1920 train_time:29416ms step_avg:39.06ms
step:754/1920 train_time:29483ms step_avg:39.10ms
step:755/1920 train_time:29548ms step_avg:39.14ms
step:756/1920 train_time:29610ms step_avg:39.17ms
step:757/1920 train_time:29671ms step_avg:39.20ms
step:758/1920 train_time:29732ms step_avg:39.22ms
step:759/1920 train_time:29795ms step_avg:39.26ms
step:760/1920 train_time:29856ms step_avg:39.28ms
step:761/1920 train_time:29917ms step_avg:39.31ms
step:762/1920 train_time:29978ms step_avg:39.34ms
step:763/1920 train_time:30040ms step_avg:39.37ms
step:764/1920 train_time:30101ms step_avg:39.40ms
step:765/1920 train_time:30163ms step_avg:39.43ms
step:766/1920 train_time:30226ms step_avg:39.46ms
step:767/1920 train_time:30289ms step_avg:39.49ms
step:768/1920 train_time:30351ms step_avg:39.52ms
step:769/1920 train_time:30415ms step_avg:39.55ms
step:770/1920 train_time:30478ms step_avg:39.58ms
step:771/1920 train_time:30542ms step_avg:39.61ms
step:772/1920 train_time:30605ms step_avg:39.64ms
step:773/1920 train_time:30668ms step_avg:39.67ms
step:774/1920 train_time:30729ms step_avg:39.70ms
step:775/1920 train_time:30792ms step_avg:39.73ms
step:776/1920 train_time:30853ms step_avg:39.76ms
step:777/1920 train_time:30915ms step_avg:39.79ms
step:778/1920 train_time:30976ms step_avg:39.81ms
step:779/1920 train_time:31037ms step_avg:39.84ms
step:780/1920 train_time:31098ms step_avg:39.87ms
step:781/1920 train_time:31161ms step_avg:39.90ms
step:782/1920 train_time:31223ms step_avg:39.93ms
step:783/1920 train_time:31285ms step_avg:39.96ms
step:784/1920 train_time:31348ms step_avg:39.98ms
step:785/1920 train_time:31411ms step_avg:40.01ms
step:786/1920 train_time:31474ms step_avg:40.04ms
step:787/1920 train_time:31536ms step_avg:40.07ms
step:788/1920 train_time:31598ms step_avg:40.10ms
step:789/1920 train_time:31662ms step_avg:40.13ms
step:790/1920 train_time:31725ms step_avg:40.16ms
step:791/1920 train_time:31788ms step_avg:40.19ms
step:792/1920 train_time:31850ms step_avg:40.21ms
step:793/1920 train_time:31912ms step_avg:40.24ms
step:794/1920 train_time:31974ms step_avg:40.27ms
step:795/1920 train_time:32035ms step_avg:40.30ms
step:796/1920 train_time:32097ms step_avg:40.32ms
step:797/1920 train_time:32160ms step_avg:40.35ms
step:798/1920 train_time:32221ms step_avg:40.38ms
step:799/1920 train_time:32283ms step_avg:40.40ms
step:800/1920 train_time:32345ms step_avg:40.43ms
step:801/1920 train_time:32409ms step_avg:40.46ms
step:802/1920 train_time:32471ms step_avg:40.49ms
step:803/1920 train_time:32534ms step_avg:40.52ms
step:804/1920 train_time:32596ms step_avg:40.54ms
step:805/1920 train_time:32659ms step_avg:40.57ms
step:806/1920 train_time:32721ms step_avg:40.60ms
step:807/1920 train_time:32784ms step_avg:40.62ms
step:808/1920 train_time:32847ms step_avg:40.65ms
step:809/1920 train_time:32910ms step_avg:40.68ms
step:810/1920 train_time:32971ms step_avg:40.70ms
step:811/1920 train_time:33033ms step_avg:40.73ms
step:812/1920 train_time:33094ms step_avg:40.76ms
step:813/1920 train_time:33157ms step_avg:40.78ms
step:814/1920 train_time:33218ms step_avg:40.81ms
step:815/1920 train_time:33280ms step_avg:40.83ms
step:816/1920 train_time:33343ms step_avg:40.86ms
step:817/1920 train_time:33406ms step_avg:40.89ms
step:818/1920 train_time:33468ms step_avg:40.91ms
step:819/1920 train_time:33531ms step_avg:40.94ms
step:820/1920 train_time:33593ms step_avg:40.97ms
step:821/1920 train_time:33656ms step_avg:40.99ms
step:822/1920 train_time:33717ms step_avg:41.02ms
step:823/1920 train_time:33780ms step_avg:41.04ms
step:824/1920 train_time:33842ms step_avg:41.07ms
step:825/1920 train_time:33906ms step_avg:41.10ms
step:826/1920 train_time:33968ms step_avg:41.12ms
step:827/1920 train_time:34030ms step_avg:41.15ms
step:828/1920 train_time:34091ms step_avg:41.17ms
step:829/1920 train_time:34154ms step_avg:41.20ms
step:830/1920 train_time:34215ms step_avg:41.22ms
step:831/1920 train_time:34278ms step_avg:41.25ms
step:832/1920 train_time:34340ms step_avg:41.27ms
step:833/1920 train_time:34402ms step_avg:41.30ms
step:834/1920 train_time:34464ms step_avg:41.32ms
step:835/1920 train_time:34527ms step_avg:41.35ms
step:836/1920 train_time:34589ms step_avg:41.37ms
step:837/1920 train_time:34653ms step_avg:41.40ms
step:838/1920 train_time:34715ms step_avg:41.43ms
step:839/1920 train_time:34778ms step_avg:41.45ms
step:840/1920 train_time:34839ms step_avg:41.48ms
step:841/1920 train_time:34902ms step_avg:41.50ms
step:842/1920 train_time:34965ms step_avg:41.53ms
step:843/1920 train_time:35027ms step_avg:41.55ms
step:844/1920 train_time:35089ms step_avg:41.57ms
step:845/1920 train_time:35152ms step_avg:41.60ms
step:846/1920 train_time:35214ms step_avg:41.62ms
step:847/1920 train_time:35276ms step_avg:41.65ms
step:848/1920 train_time:35337ms step_avg:41.67ms
step:849/1920 train_time:35400ms step_avg:41.70ms
step:850/1920 train_time:35462ms step_avg:41.72ms
step:851/1920 train_time:35525ms step_avg:41.74ms
step:852/1920 train_time:35587ms step_avg:41.77ms
step:853/1920 train_time:35650ms step_avg:41.79ms
step:854/1920 train_time:35712ms step_avg:41.82ms
step:855/1920 train_time:35775ms step_avg:41.84ms
step:856/1920 train_time:35836ms step_avg:41.86ms
step:857/1920 train_time:35899ms step_avg:41.89ms
step:858/1920 train_time:35961ms step_avg:41.91ms
step:859/1920 train_time:36024ms step_avg:41.94ms
step:860/1920 train_time:36086ms step_avg:41.96ms
step:861/1920 train_time:36149ms step_avg:41.98ms
step:862/1920 train_time:36210ms step_avg:42.01ms
step:863/1920 train_time:36272ms step_avg:42.03ms
step:864/1920 train_time:36334ms step_avg:42.05ms
step:865/1920 train_time:36397ms step_avg:42.08ms
step:866/1920 train_time:36458ms step_avg:42.10ms
step:867/1920 train_time:36521ms step_avg:42.12ms
step:868/1920 train_time:36583ms step_avg:42.15ms
step:869/1920 train_time:36646ms step_avg:42.17ms
step:870/1920 train_time:36708ms step_avg:42.19ms
step:871/1920 train_time:36771ms step_avg:42.22ms
step:872/1920 train_time:36833ms step_avg:42.24ms
step:873/1920 train_time:36895ms step_avg:42.26ms
step:874/1920 train_time:36957ms step_avg:42.29ms
step:875/1920 train_time:37020ms step_avg:42.31ms
step:876/1920 train_time:37082ms step_avg:42.33ms
step:877/1920 train_time:37146ms step_avg:42.36ms
step:878/1920 train_time:37208ms step_avg:42.38ms
step:879/1920 train_time:37270ms step_avg:42.40ms
step:880/1920 train_time:37331ms step_avg:42.42ms
step:881/1920 train_time:37394ms step_avg:42.44ms
step:882/1920 train_time:37456ms step_avg:42.47ms
step:883/1920 train_time:37518ms step_avg:42.49ms
step:884/1920 train_time:37580ms step_avg:42.51ms
step:885/1920 train_time:37643ms step_avg:42.53ms
step:886/1920 train_time:37706ms step_avg:42.56ms
step:887/1920 train_time:37768ms step_avg:42.58ms
step:888/1920 train_time:37830ms step_avg:42.60ms
step:889/1920 train_time:37893ms step_avg:42.62ms
step:890/1920 train_time:37955ms step_avg:42.65ms
step:891/1920 train_time:38017ms step_avg:42.67ms
step:892/1920 train_time:38079ms step_avg:42.69ms
step:893/1920 train_time:38142ms step_avg:42.71ms
step:894/1920 train_time:38204ms step_avg:42.73ms
step:895/1920 train_time:38267ms step_avg:42.76ms
step:896/1920 train_time:38329ms step_avg:42.78ms
step:897/1920 train_time:38391ms step_avg:42.80ms
step:898/1920 train_time:38453ms step_avg:42.82ms
step:899/1920 train_time:38515ms step_avg:42.84ms
step:900/1920 train_time:38578ms step_avg:42.86ms
step:901/1920 train_time:38640ms step_avg:42.89ms
step:902/1920 train_time:38702ms step_avg:42.91ms
step:903/1920 train_time:38765ms step_avg:42.93ms
step:904/1920 train_time:38827ms step_avg:42.95ms
step:905/1920 train_time:38890ms step_avg:42.97ms
step:906/1920 train_time:38953ms step_avg:42.99ms
step:907/1920 train_time:39016ms step_avg:43.02ms
step:908/1920 train_time:39078ms step_avg:43.04ms
step:909/1920 train_time:39140ms step_avg:43.06ms
step:910/1920 train_time:39203ms step_avg:43.08ms
step:911/1920 train_time:39267ms step_avg:43.10ms
step:912/1920 train_time:39328ms step_avg:43.12ms
step:913/1920 train_time:39391ms step_avg:43.14ms
step:914/1920 train_time:39452ms step_avg:43.16ms
step:915/1920 train_time:39515ms step_avg:43.19ms
step:916/1920 train_time:39577ms step_avg:43.21ms
step:917/1920 train_time:39639ms step_avg:43.23ms
step:918/1920 train_time:39701ms step_avg:43.25ms
step:919/1920 train_time:39764ms step_avg:43.27ms
step:920/1920 train_time:39826ms step_avg:43.29ms
step:921/1920 train_time:39889ms step_avg:43.31ms
step:922/1920 train_time:39951ms step_avg:43.33ms
step:923/1920 train_time:40013ms step_avg:43.35ms
step:924/1920 train_time:40076ms step_avg:43.37ms
step:925/1920 train_time:40139ms step_avg:43.39ms
step:926/1920 train_time:40200ms step_avg:43.41ms
step:927/1920 train_time:40264ms step_avg:43.43ms
step:928/1920 train_time:40326ms step_avg:43.45ms
step:929/1920 train_time:40390ms step_avg:43.48ms
step:930/1920 train_time:40452ms step_avg:43.50ms
step:931/1920 train_time:40514ms step_avg:43.52ms
step:932/1920 train_time:40575ms step_avg:43.54ms
step:933/1920 train_time:40638ms step_avg:43.56ms
step:934/1920 train_time:40699ms step_avg:43.58ms
step:935/1920 train_time:40762ms step_avg:43.60ms
step:936/1920 train_time:40825ms step_avg:43.62ms
step:937/1920 train_time:40888ms step_avg:43.64ms
step:938/1920 train_time:40950ms step_avg:43.66ms
step:939/1920 train_time:41012ms step_avg:43.68ms
step:940/1920 train_time:41074ms step_avg:43.70ms
step:941/1920 train_time:41137ms step_avg:43.72ms
step:942/1920 train_time:41200ms step_avg:43.74ms
step:943/1920 train_time:41263ms step_avg:43.76ms
step:944/1920 train_time:41326ms step_avg:43.78ms
step:945/1920 train_time:41389ms step_avg:43.80ms
step:946/1920 train_time:41451ms step_avg:43.82ms
step:947/1920 train_time:41514ms step_avg:43.84ms
step:948/1920 train_time:41576ms step_avg:43.86ms
step:949/1920 train_time:41638ms step_avg:43.88ms
step:950/1920 train_time:41699ms step_avg:43.89ms
step:951/1920 train_time:41762ms step_avg:43.91ms
step:952/1920 train_time:41825ms step_avg:43.93ms
step:953/1920 train_time:41888ms step_avg:43.95ms
step:954/1920 train_time:41950ms step_avg:43.97ms
step:955/1920 train_time:42014ms step_avg:43.99ms
step:956/1920 train_time:42076ms step_avg:44.01ms
step:957/1920 train_time:42138ms step_avg:44.03ms
step:958/1920 train_time:42200ms step_avg:44.05ms
step:959/1920 train_time:42262ms step_avg:44.07ms
step:960/1920 train_time:42325ms step_avg:44.09ms
step:961/1920 train_time:42389ms step_avg:44.11ms
step:962/1920 train_time:42451ms step_avg:44.13ms
step:963/1920 train_time:42513ms step_avg:44.15ms
step:964/1920 train_time:42574ms step_avg:44.16ms
step:965/1920 train_time:42637ms step_avg:44.18ms
step:966/1920 train_time:42699ms step_avg:44.20ms
step:967/1920 train_time:42762ms step_avg:44.22ms
step:968/1920 train_time:42823ms step_avg:44.24ms
step:969/1920 train_time:42886ms step_avg:44.26ms
step:970/1920 train_time:42949ms step_avg:44.28ms
step:971/1920 train_time:43012ms step_avg:44.30ms
step:972/1920 train_time:43074ms step_avg:44.31ms
step:973/1920 train_time:43136ms step_avg:44.33ms
step:974/1920 train_time:43198ms step_avg:44.35ms
step:975/1920 train_time:43260ms step_avg:44.37ms
step:976/1920 train_time:43322ms step_avg:44.39ms
step:977/1920 train_time:43386ms step_avg:44.41ms
step:978/1920 train_time:43448ms step_avg:44.43ms
step:979/1920 train_time:43511ms step_avg:44.44ms
step:980/1920 train_time:43573ms step_avg:44.46ms
step:981/1920 train_time:43635ms step_avg:44.48ms
step:982/1920 train_time:43697ms step_avg:44.50ms
step:983/1920 train_time:43759ms step_avg:44.52ms
step:984/1920 train_time:43821ms step_avg:44.53ms
step:985/1920 train_time:43884ms step_avg:44.55ms
step:986/1920 train_time:43946ms step_avg:44.57ms
step:987/1920 train_time:44010ms step_avg:44.59ms
step:988/1920 train_time:44072ms step_avg:44.61ms
step:989/1920 train_time:44135ms step_avg:44.63ms
step:990/1920 train_time:44196ms step_avg:44.64ms
step:991/1920 train_time:44259ms step_avg:44.66ms
step:992/1920 train_time:44320ms step_avg:44.68ms
step:993/1920 train_time:44384ms step_avg:44.70ms
step:994/1920 train_time:44446ms step_avg:44.71ms
step:995/1920 train_time:44509ms step_avg:44.73ms
step:996/1920 train_time:44571ms step_avg:44.75ms
step:997/1920 train_time:44633ms step_avg:44.77ms
step:998/1920 train_time:44695ms step_avg:44.78ms
step:999/1920 train_time:44758ms step_avg:44.80ms
step:1000/1920 train_time:44820ms step_avg:44.82ms
step:1000/1920 val_loss:3.7758 train_time:44885ms step_avg:44.89ms
step:1001/1920 train_time:44903ms step_avg:44.86ms
step:1002/1920 train_time:44947ms step_avg:44.86ms
step:1003/1920 train_time:45013ms step_avg:44.88ms
step:1004/1920 train_time:45075ms step_avg:44.90ms
step:1005/1920 train_time:45138ms step_avg:44.91ms
step:1006/1920 train_time:45200ms step_avg:44.93ms
step:1007/1920 train_time:45262ms step_avg:44.95ms
step:1008/1920 train_time:45323ms step_avg:44.96ms
step:1009/1920 train_time:45385ms step_avg:44.98ms
step:1010/1920 train_time:45446ms step_avg:45.00ms
step:1011/1920 train_time:45509ms step_avg:45.01ms
step:1012/1920 train_time:45570ms step_avg:45.03ms
step:1013/1920 train_time:45634ms step_avg:45.05ms
step:1014/1920 train_time:45696ms step_avg:45.06ms
step:1015/1920 train_time:45757ms step_avg:45.08ms
step:1016/1920 train_time:45820ms step_avg:45.10ms
step:1017/1920 train_time:45883ms step_avg:45.12ms
step:1018/1920 train_time:45946ms step_avg:45.13ms
step:1019/1920 train_time:46010ms step_avg:45.15ms
step:1020/1920 train_time:46072ms step_avg:45.17ms
step:1021/1920 train_time:46136ms step_avg:45.19ms
step:1022/1920 train_time:46198ms step_avg:45.20ms
step:1023/1920 train_time:46261ms step_avg:45.22ms
step:1024/1920 train_time:46322ms step_avg:45.24ms
step:1025/1920 train_time:46384ms step_avg:45.25ms
step:1026/1920 train_time:46446ms step_avg:45.27ms
step:1027/1920 train_time:46508ms step_avg:45.29ms
step:1028/1920 train_time:46570ms step_avg:45.30ms
step:1029/1920 train_time:46633ms step_avg:45.32ms
step:1030/1920 train_time:46695ms step_avg:45.34ms
step:1031/1920 train_time:46758ms step_avg:45.35ms
step:1032/1920 train_time:46820ms step_avg:45.37ms
step:1033/1920 train_time:46883ms step_avg:45.39ms
step:1034/1920 train_time:46946ms step_avg:45.40ms
step:1035/1920 train_time:47009ms step_avg:45.42ms
step:1036/1920 train_time:47072ms step_avg:45.44ms
step:1037/1920 train_time:47136ms step_avg:45.45ms
step:1038/1920 train_time:47198ms step_avg:45.47ms
step:1039/1920 train_time:47261ms step_avg:45.49ms
step:1040/1920 train_time:47322ms step_avg:45.50ms
step:1041/1920 train_time:47384ms step_avg:45.52ms
step:1042/1920 train_time:47446ms step_avg:45.53ms
step:1043/1920 train_time:47508ms step_avg:45.55ms
step:1044/1920 train_time:47570ms step_avg:45.57ms
step:1045/1920 train_time:47633ms step_avg:45.58ms
step:1046/1920 train_time:47696ms step_avg:45.60ms
step:1047/1920 train_time:47759ms step_avg:45.61ms
step:1048/1920 train_time:47820ms step_avg:45.63ms
step:1049/1920 train_time:47883ms step_avg:45.65ms
step:1050/1920 train_time:47945ms step_avg:45.66ms
step:1051/1920 train_time:48008ms step_avg:45.68ms
step:1052/1920 train_time:48070ms step_avg:45.69ms
step:1053/1920 train_time:48134ms step_avg:45.71ms
step:1054/1920 train_time:48196ms step_avg:45.73ms
step:1055/1920 train_time:48259ms step_avg:45.74ms
step:1056/1920 train_time:48321ms step_avg:45.76ms
step:1057/1920 train_time:48383ms step_avg:45.77ms
step:1058/1920 train_time:48445ms step_avg:45.79ms
step:1059/1920 train_time:48508ms step_avg:45.81ms
step:1060/1920 train_time:48569ms step_avg:45.82ms
step:1061/1920 train_time:48632ms step_avg:45.84ms
step:1062/1920 train_time:48694ms step_avg:45.85ms
step:1063/1920 train_time:48757ms step_avg:45.87ms
step:1064/1920 train_time:48819ms step_avg:45.88ms
step:1065/1920 train_time:48881ms step_avg:45.90ms
step:1066/1920 train_time:48944ms step_avg:45.91ms
step:1067/1920 train_time:49006ms step_avg:45.93ms
step:1068/1920 train_time:49068ms step_avg:45.94ms
step:1069/1920 train_time:49132ms step_avg:45.96ms
step:1070/1920 train_time:49194ms step_avg:45.98ms
step:1071/1920 train_time:49257ms step_avg:45.99ms
step:1072/1920 train_time:49319ms step_avg:46.01ms
step:1073/1920 train_time:49382ms step_avg:46.02ms
step:1074/1920 train_time:49444ms step_avg:46.04ms
step:1075/1920 train_time:49507ms step_avg:46.05ms
step:1076/1920 train_time:49569ms step_avg:46.07ms
step:1077/1920 train_time:49632ms step_avg:46.08ms
step:1078/1920 train_time:49694ms step_avg:46.10ms
step:1079/1920 train_time:49757ms step_avg:46.11ms
step:1080/1920 train_time:49819ms step_avg:46.13ms
step:1081/1920 train_time:49882ms step_avg:46.14ms
step:1082/1920 train_time:49944ms step_avg:46.16ms
step:1083/1920 train_time:50006ms step_avg:46.17ms
step:1084/1920 train_time:50068ms step_avg:46.19ms
step:1085/1920 train_time:50131ms step_avg:46.20ms
step:1086/1920 train_time:50193ms step_avg:46.22ms
step:1087/1920 train_time:50256ms step_avg:46.23ms
step:1088/1920 train_time:50318ms step_avg:46.25ms
step:1089/1920 train_time:50380ms step_avg:46.26ms
step:1090/1920 train_time:50443ms step_avg:46.28ms
step:1091/1920 train_time:50505ms step_avg:46.29ms
step:1092/1920 train_time:50567ms step_avg:46.31ms
step:1093/1920 train_time:50630ms step_avg:46.32ms
step:1094/1920 train_time:50691ms step_avg:46.34ms
step:1095/1920 train_time:50754ms step_avg:46.35ms
step:1096/1920 train_time:50816ms step_avg:46.37ms
step:1097/1920 train_time:50878ms step_avg:46.38ms
step:1098/1920 train_time:50940ms step_avg:46.39ms
step:1099/1920 train_time:51003ms step_avg:46.41ms
step:1100/1920 train_time:51066ms step_avg:46.42ms
step:1101/1920 train_time:51128ms step_avg:46.44ms
step:1102/1920 train_time:51190ms step_avg:46.45ms
step:1103/1920 train_time:51253ms step_avg:46.47ms
step:1104/1920 train_time:51315ms step_avg:46.48ms
step:1105/1920 train_time:51378ms step_avg:46.50ms
step:1106/1920 train_time:51441ms step_avg:46.51ms
step:1107/1920 train_time:51503ms step_avg:46.53ms
step:1108/1920 train_time:51565ms step_avg:46.54ms
step:1109/1920 train_time:51628ms step_avg:46.55ms
step:1110/1920 train_time:51691ms step_avg:46.57ms
step:1111/1920 train_time:51754ms step_avg:46.58ms
step:1112/1920 train_time:51816ms step_avg:46.60ms
step:1113/1920 train_time:51879ms step_avg:46.61ms
step:1114/1920 train_time:51941ms step_avg:46.63ms
step:1115/1920 train_time:52003ms step_avg:46.64ms
step:1116/1920 train_time:52065ms step_avg:46.65ms
step:1117/1920 train_time:52128ms step_avg:46.67ms
step:1118/1920 train_time:52190ms step_avg:46.68ms
step:1119/1920 train_time:52253ms step_avg:46.70ms
step:1120/1920 train_time:52315ms step_avg:46.71ms
step:1121/1920 train_time:52378ms step_avg:46.72ms
step:1122/1920 train_time:52440ms step_avg:46.74ms
step:1123/1920 train_time:52502ms step_avg:46.75ms
step:1124/1920 train_time:52564ms step_avg:46.77ms
step:1125/1920 train_time:52627ms step_avg:46.78ms
step:1126/1920 train_time:52688ms step_avg:46.79ms
step:1127/1920 train_time:52751ms step_avg:46.81ms
step:1128/1920 train_time:52814ms step_avg:46.82ms
step:1129/1920 train_time:52876ms step_avg:46.83ms
step:1130/1920 train_time:52938ms step_avg:46.85ms
step:1131/1920 train_time:53001ms step_avg:46.86ms
step:1132/1920 train_time:53063ms step_avg:46.88ms
step:1133/1920 train_time:53125ms step_avg:46.89ms
step:1134/1920 train_time:53187ms step_avg:46.90ms
step:1135/1920 train_time:53251ms step_avg:46.92ms
step:1136/1920 train_time:53313ms step_avg:46.93ms
step:1137/1920 train_time:53377ms step_avg:46.95ms
step:1138/1920 train_time:53439ms step_avg:46.96ms
step:1139/1920 train_time:53502ms step_avg:46.97ms
step:1140/1920 train_time:53564ms step_avg:46.99ms
step:1141/1920 train_time:53626ms step_avg:47.00ms
step:1142/1920 train_time:53688ms step_avg:47.01ms
step:1143/1920 train_time:53752ms step_avg:47.03ms
step:1144/1920 train_time:53814ms step_avg:47.04ms
step:1145/1920 train_time:53877ms step_avg:47.05ms
step:1146/1920 train_time:53939ms step_avg:47.07ms
step:1147/1920 train_time:54002ms step_avg:47.08ms
step:1148/1920 train_time:54064ms step_avg:47.09ms
step:1149/1920 train_time:54127ms step_avg:47.11ms
step:1150/1920 train_time:54189ms step_avg:47.12ms
step:1151/1920 train_time:54253ms step_avg:47.14ms
step:1152/1920 train_time:54315ms step_avg:47.15ms
step:1153/1920 train_time:54378ms step_avg:47.16ms
step:1154/1920 train_time:54440ms step_avg:47.17ms
step:1155/1920 train_time:54502ms step_avg:47.19ms
step:1156/1920 train_time:54565ms step_avg:47.20ms
step:1157/1920 train_time:54627ms step_avg:47.21ms
step:1158/1920 train_time:54689ms step_avg:47.23ms
step:1159/1920 train_time:54753ms step_avg:47.24ms
step:1160/1920 train_time:54815ms step_avg:47.25ms
step:1161/1920 train_time:54879ms step_avg:47.27ms
step:1162/1920 train_time:54940ms step_avg:47.28ms
step:1163/1920 train_time:55003ms step_avg:47.29ms
step:1164/1920 train_time:55064ms step_avg:47.31ms
step:1165/1920 train_time:55127ms step_avg:47.32ms
step:1166/1920 train_time:55190ms step_avg:47.33ms
step:1167/1920 train_time:55253ms step_avg:47.35ms
step:1168/1920 train_time:55315ms step_avg:47.36ms
step:1169/1920 train_time:55378ms step_avg:47.37ms
step:1170/1920 train_time:55439ms step_avg:47.38ms
step:1171/1920 train_time:55503ms step_avg:47.40ms
step:1172/1920 train_time:55565ms step_avg:47.41ms
step:1173/1920 train_time:55627ms step_avg:47.42ms
step:1174/1920 train_time:55689ms step_avg:47.44ms
step:1175/1920 train_time:55752ms step_avg:47.45ms
step:1176/1920 train_time:55815ms step_avg:47.46ms
step:1177/1920 train_time:55878ms step_avg:47.48ms
step:1178/1920 train_time:55940ms step_avg:47.49ms
step:1179/1920 train_time:56003ms step_avg:47.50ms
step:1180/1920 train_time:56064ms step_avg:47.51ms
step:1181/1920 train_time:56128ms step_avg:47.53ms
step:1182/1920 train_time:56190ms step_avg:47.54ms
step:1183/1920 train_time:56253ms step_avg:47.55ms
step:1184/1920 train_time:56316ms step_avg:47.56ms
step:1185/1920 train_time:56380ms step_avg:47.58ms
step:1186/1920 train_time:56442ms step_avg:47.59ms
step:1187/1920 train_time:56505ms step_avg:47.60ms
step:1188/1920 train_time:56566ms step_avg:47.61ms
step:1189/1920 train_time:56629ms step_avg:47.63ms
step:1190/1920 train_time:56691ms step_avg:47.64ms
step:1191/1920 train_time:56754ms step_avg:47.65ms
step:1192/1920 train_time:56816ms step_avg:47.66ms
step:1193/1920 train_time:56879ms step_avg:47.68ms
step:1194/1920 train_time:56940ms step_avg:47.69ms
step:1195/1920 train_time:57003ms step_avg:47.70ms
step:1196/1920 train_time:57064ms step_avg:47.71ms
step:1197/1920 train_time:57127ms step_avg:47.73ms
step:1198/1920 train_time:57189ms step_avg:47.74ms
step:1199/1920 train_time:57253ms step_avg:47.75ms
step:1200/1920 train_time:57315ms step_avg:47.76ms
step:1201/1920 train_time:57378ms step_avg:47.78ms
step:1202/1920 train_time:57440ms step_avg:47.79ms
step:1203/1920 train_time:57502ms step_avg:47.80ms
step:1204/1920 train_time:57564ms step_avg:47.81ms
step:1205/1920 train_time:57627ms step_avg:47.82ms
step:1206/1920 train_time:57689ms step_avg:47.83ms
step:1207/1920 train_time:57752ms step_avg:47.85ms
step:1208/1920 train_time:57814ms step_avg:47.86ms
step:1209/1920 train_time:57878ms step_avg:47.87ms
step:1210/1920 train_time:57939ms step_avg:47.88ms
step:1211/1920 train_time:58002ms step_avg:47.90ms
step:1212/1920 train_time:58064ms step_avg:47.91ms
step:1213/1920 train_time:58126ms step_avg:47.92ms
step:1214/1920 train_time:58188ms step_avg:47.93ms
step:1215/1920 train_time:58251ms step_avg:47.94ms
step:1216/1920 train_time:58313ms step_avg:47.95ms
step:1217/1920 train_time:58377ms step_avg:47.97ms
step:1218/1920 train_time:58438ms step_avg:47.98ms
step:1219/1920 train_time:58501ms step_avg:47.99ms
step:1220/1920 train_time:58563ms step_avg:48.00ms
step:1221/1920 train_time:58626ms step_avg:48.01ms
step:1222/1920 train_time:58688ms step_avg:48.03ms
step:1223/1920 train_time:58751ms step_avg:48.04ms
step:1224/1920 train_time:58814ms step_avg:48.05ms
step:1225/1920 train_time:58877ms step_avg:48.06ms
step:1226/1920 train_time:58939ms step_avg:48.07ms
step:1227/1920 train_time:59001ms step_avg:48.09ms
step:1228/1920 train_time:59063ms step_avg:48.10ms
step:1229/1920 train_time:59126ms step_avg:48.11ms
step:1230/1920 train_time:59187ms step_avg:48.12ms
step:1231/1920 train_time:59251ms step_avg:48.13ms
step:1232/1920 train_time:59313ms step_avg:48.14ms
step:1233/1920 train_time:59376ms step_avg:48.16ms
step:1234/1920 train_time:59438ms step_avg:48.17ms
step:1235/1920 train_time:59502ms step_avg:48.18ms
step:1236/1920 train_time:59564ms step_avg:48.19ms
step:1237/1920 train_time:59626ms step_avg:48.20ms
step:1238/1920 train_time:59688ms step_avg:48.21ms
step:1239/1920 train_time:59751ms step_avg:48.23ms
step:1240/1920 train_time:59813ms step_avg:48.24ms
step:1241/1920 train_time:59876ms step_avg:48.25ms
step:1242/1920 train_time:59938ms step_avg:48.26ms
step:1243/1920 train_time:60001ms step_avg:48.27ms
step:1244/1920 train_time:60062ms step_avg:48.28ms
step:1245/1920 train_time:60125ms step_avg:48.29ms
step:1246/1920 train_time:60187ms step_avg:48.30ms
step:1247/1920 train_time:60250ms step_avg:48.32ms
step:1248/1920 train_time:60312ms step_avg:48.33ms
step:1249/1920 train_time:60376ms step_avg:48.34ms
step:1250/1920 train_time:60438ms step_avg:48.35ms
step:1250/1920 val_loss:3.5530 train_time:60503ms step_avg:48.40ms
step:1251/1920 train_time:60523ms step_avg:48.38ms
step:1252/1920 train_time:60565ms step_avg:48.37ms
step:1253/1920 train_time:60631ms step_avg:48.39ms
step:1254/1920 train_time:60694ms step_avg:48.40ms
step:1255/1920 train_time:60757ms step_avg:48.41ms
step:1256/1920 train_time:60844ms step_avg:48.44ms
step:1257/1920 train_time:60932ms step_avg:48.47ms
step:1258/1920 train_time:61019ms step_avg:48.50ms
step:1259/1920 train_time:61108ms step_avg:48.54ms
step:1260/1920 train_time:61194ms step_avg:48.57ms
step:1261/1920 train_time:61282ms step_avg:48.60ms
step:1262/1920 train_time:61369ms step_avg:48.63ms
step:1263/1920 train_time:61460ms step_avg:48.66ms
step:1264/1920 train_time:61550ms step_avg:48.69ms
step:1265/1920 train_time:61641ms step_avg:48.73ms
step:1266/1920 train_time:61730ms step_avg:48.76ms
step:1267/1920 train_time:61819ms step_avg:48.79ms
step:1268/1920 train_time:61906ms step_avg:48.82ms
step:1269/1920 train_time:61993ms step_avg:48.85ms
step:1270/1920 train_time:62081ms step_avg:48.88ms
step:1271/1920 train_time:62170ms step_avg:48.91ms
step:1272/1920 train_time:62258ms step_avg:48.94ms
step:1273/1920 train_time:62346ms step_avg:48.98ms
step:1274/1920 train_time:62434ms step_avg:49.01ms
step:1275/1920 train_time:62523ms step_avg:49.04ms
step:1276/1920 train_time:62611ms step_avg:49.07ms
step:1277/1920 train_time:62700ms step_avg:49.10ms
step:1278/1920 train_time:62789ms step_avg:49.13ms
step:1279/1920 train_time:62878ms step_avg:49.16ms
step:1280/1920 train_time:62966ms step_avg:49.19ms
step:1281/1920 train_time:63054ms step_avg:49.22ms
step:1282/1920 train_time:63141ms step_avg:49.25ms
step:1283/1920 train_time:63229ms step_avg:49.28ms
step:1284/1920 train_time:63317ms step_avg:49.31ms
step:1285/1920 train_time:63406ms step_avg:49.34ms
step:1286/1920 train_time:63494ms step_avg:49.37ms
step:1287/1920 train_time:63584ms step_avg:49.40ms
step:1288/1920 train_time:63672ms step_avg:49.43ms
step:1289/1920 train_time:63761ms step_avg:49.47ms
step:1290/1920 train_time:63851ms step_avg:49.50ms
step:1291/1920 train_time:63939ms step_avg:49.53ms
step:1292/1920 train_time:64026ms step_avg:49.56ms
step:1293/1920 train_time:64114ms step_avg:49.59ms
step:1294/1920 train_time:64201ms step_avg:49.61ms
step:1295/1920 train_time:64289ms step_avg:49.64ms
step:1296/1920 train_time:64377ms step_avg:49.67ms
step:1297/1920 train_time:64466ms step_avg:49.70ms
step:1298/1920 train_time:64555ms step_avg:49.73ms
step:1299/1920 train_time:64645ms step_avg:49.77ms
step:1300/1920 train_time:64732ms step_avg:49.79ms
step:1301/1920 train_time:64821ms step_avg:49.82ms
step:1302/1920 train_time:64910ms step_avg:49.85ms
step:1303/1920 train_time:64999ms step_avg:49.88ms
step:1304/1920 train_time:65087ms step_avg:49.91ms
step:1305/1920 train_time:65174ms step_avg:49.94ms
step:1306/1920 train_time:65262ms step_avg:49.97ms
step:1307/1920 train_time:65350ms step_avg:50.00ms
step:1308/1920 train_time:65438ms step_avg:50.03ms
step:1309/1920 train_time:65527ms step_avg:50.06ms
step:1310/1920 train_time:65614ms step_avg:50.09ms
step:1311/1920 train_time:65703ms step_avg:50.12ms
step:1312/1920 train_time:65791ms step_avg:50.15ms
step:1313/1920 train_time:65880ms step_avg:50.18ms
step:1314/1920 train_time:65969ms step_avg:50.20ms
step:1315/1920 train_time:66057ms step_avg:50.23ms
step:1316/1920 train_time:66145ms step_avg:50.26ms
step:1317/1920 train_time:66233ms step_avg:50.29ms
step:1318/1920 train_time:66321ms step_avg:50.32ms
step:1319/1920 train_time:66410ms step_avg:50.35ms
step:1320/1920 train_time:66499ms step_avg:50.38ms
step:1321/1920 train_time:66587ms step_avg:50.41ms
step:1322/1920 train_time:66675ms step_avg:50.44ms
step:1323/1920 train_time:66764ms step_avg:50.46ms
step:1324/1920 train_time:66851ms step_avg:50.49ms
step:1325/1920 train_time:66939ms step_avg:50.52ms
step:1326/1920 train_time:67028ms step_avg:50.55ms
step:1327/1920 train_time:67117ms step_avg:50.58ms
step:1328/1920 train_time:67205ms step_avg:50.61ms
step:1329/1920 train_time:67293ms step_avg:50.63ms
step:1330/1920 train_time:67381ms step_avg:50.66ms
step:1331/1920 train_time:67471ms step_avg:50.69ms
step:1332/1920 train_time:67561ms step_avg:50.72ms
step:1333/1920 train_time:67652ms step_avg:50.75ms
step:1334/1920 train_time:67740ms step_avg:50.78ms
step:1335/1920 train_time:67831ms step_avg:50.81ms
step:1336/1920 train_time:67919ms step_avg:50.84ms
step:1337/1920 train_time:68008ms step_avg:50.87ms
step:1338/1920 train_time:68096ms step_avg:50.89ms
step:1339/1920 train_time:68184ms step_avg:50.92ms
step:1340/1920 train_time:68273ms step_avg:50.95ms
step:1341/1920 train_time:68362ms step_avg:50.98ms
step:1342/1920 train_time:68450ms step_avg:51.01ms
step:1343/1920 train_time:68538ms step_avg:51.03ms
step:1344/1920 train_time:68626ms step_avg:51.06ms
step:1345/1920 train_time:68715ms step_avg:51.09ms
step:1346/1920 train_time:68803ms step_avg:51.12ms
step:1347/1920 train_time:68893ms step_avg:51.15ms
step:1348/1920 train_time:68981ms step_avg:51.17ms
step:1349/1920 train_time:69070ms step_avg:51.20ms
step:1350/1920 train_time:69159ms step_avg:51.23ms
step:1351/1920 train_time:69247ms step_avg:51.26ms
step:1352/1920 train_time:69335ms step_avg:51.28ms
step:1353/1920 train_time:69424ms step_avg:51.31ms
step:1354/1920 train_time:69512ms step_avg:51.34ms
step:1355/1920 train_time:69601ms step_avg:51.37ms
step:1356/1920 train_time:69689ms step_avg:51.39ms
step:1357/1920 train_time:69778ms step_avg:51.42ms
step:1358/1920 train_time:69867ms step_avg:51.45ms
step:1359/1920 train_time:69955ms step_avg:51.48ms
step:1360/1920 train_time:70043ms step_avg:51.50ms
step:1361/1920 train_time:70131ms step_avg:51.53ms
step:1362/1920 train_time:70219ms step_avg:51.56ms
step:1363/1920 train_time:70308ms step_avg:51.58ms
step:1364/1920 train_time:70396ms step_avg:51.61ms
step:1365/1920 train_time:70486ms step_avg:51.64ms
step:1366/1920 train_time:70574ms step_avg:51.66ms
step:1367/1920 train_time:70663ms step_avg:51.69ms
step:1368/1920 train_time:70751ms step_avg:51.72ms
step:1369/1920 train_time:70841ms step_avg:51.75ms
step:1370/1920 train_time:70928ms step_avg:51.77ms
step:1371/1920 train_time:71017ms step_avg:51.80ms
step:1372/1920 train_time:71105ms step_avg:51.83ms
step:1373/1920 train_time:71194ms step_avg:51.85ms
step:1374/1920 train_time:71281ms step_avg:51.88ms
step:1375/1920 train_time:71370ms step_avg:51.91ms
step:1376/1920 train_time:71458ms step_avg:51.93ms
step:1377/1920 train_time:71548ms step_avg:51.96ms
step:1378/1920 train_time:71635ms step_avg:51.99ms
step:1379/1920 train_time:71725ms step_avg:52.01ms
step:1380/1920 train_time:71813ms step_avg:52.04ms
step:1381/1920 train_time:71902ms step_avg:52.07ms
step:1382/1920 train_time:71990ms step_avg:52.09ms
step:1383/1920 train_time:72078ms step_avg:52.12ms
step:1384/1920 train_time:72166ms step_avg:52.14ms
step:1385/1920 train_time:72254ms step_avg:52.17ms
step:1386/1920 train_time:72342ms step_avg:52.19ms
step:1387/1920 train_time:72430ms step_avg:52.22ms
step:1388/1920 train_time:72519ms step_avg:52.25ms
step:1389/1920 train_time:72609ms step_avg:52.27ms
step:1390/1920 train_time:72697ms step_avg:52.30ms
step:1391/1920 train_time:72786ms step_avg:52.33ms
step:1392/1920 train_time:72873ms step_avg:52.35ms
step:1393/1920 train_time:72963ms step_avg:52.38ms
step:1394/1920 train_time:73051ms step_avg:52.40ms
step:1395/1920 train_time:73140ms step_avg:52.43ms
step:1396/1920 train_time:73228ms step_avg:52.46ms
step:1397/1920 train_time:73316ms step_avg:52.48ms
step:1398/1920 train_time:73404ms step_avg:52.51ms
step:1399/1920 train_time:73493ms step_avg:52.53ms
step:1400/1920 train_time:73581ms step_avg:52.56ms
step:1401/1920 train_time:73671ms step_avg:52.58ms
step:1402/1920 train_time:73759ms step_avg:52.61ms
step:1403/1920 train_time:73848ms step_avg:52.64ms
step:1404/1920 train_time:73935ms step_avg:52.66ms
step:1405/1920 train_time:74024ms step_avg:52.69ms
step:1406/1920 train_time:74111ms step_avg:52.71ms
step:1407/1920 train_time:74201ms step_avg:52.74ms
step:1408/1920 train_time:74288ms step_avg:52.76ms
step:1409/1920 train_time:74377ms step_avg:52.79ms
step:1410/1920 train_time:74465ms step_avg:52.81ms
step:1411/1920 train_time:74553ms step_avg:52.84ms
step:1412/1920 train_time:74642ms step_avg:52.86ms
step:1413/1920 train_time:74730ms step_avg:52.89ms
step:1414/1920 train_time:74818ms step_avg:52.91ms
step:1415/1920 train_time:74907ms step_avg:52.94ms
step:1416/1920 train_time:74995ms step_avg:52.96ms
step:1417/1920 train_time:75084ms step_avg:52.99ms
step:1418/1920 train_time:75172ms step_avg:53.01ms
step:1419/1920 train_time:75261ms step_avg:53.04ms
step:1420/1920 train_time:75349ms step_avg:53.06ms
step:1421/1920 train_time:75438ms step_avg:53.09ms
step:1422/1920 train_time:75526ms step_avg:53.11ms
step:1423/1920 train_time:75615ms step_avg:53.14ms
step:1424/1920 train_time:75704ms step_avg:53.16ms
step:1425/1920 train_time:75792ms step_avg:53.19ms
step:1426/1920 train_time:75880ms step_avg:53.21ms
step:1427/1920 train_time:75969ms step_avg:53.24ms
step:1428/1920 train_time:76058ms step_avg:53.26ms
step:1429/1920 train_time:76146ms step_avg:53.29ms
step:1430/1920 train_time:76234ms step_avg:53.31ms
step:1431/1920 train_time:76322ms step_avg:53.33ms
step:1432/1920 train_time:76410ms step_avg:53.36ms
step:1433/1920 train_time:76498ms step_avg:53.38ms
step:1434/1920 train_time:76587ms step_avg:53.41ms
step:1435/1920 train_time:76676ms step_avg:53.43ms
step:1436/1920 train_time:76764ms step_avg:53.46ms
step:1437/1920 train_time:76853ms step_avg:53.48ms
step:1438/1920 train_time:76941ms step_avg:53.51ms
step:1439/1920 train_time:77030ms step_avg:53.53ms
step:1440/1920 train_time:77119ms step_avg:53.55ms
step:1441/1920 train_time:77207ms step_avg:53.58ms
step:1442/1920 train_time:77295ms step_avg:53.60ms
step:1443/1920 train_time:77383ms step_avg:53.63ms
step:1444/1920 train_time:77470ms step_avg:53.65ms
step:1445/1920 train_time:77559ms step_avg:53.67ms
step:1446/1920 train_time:77648ms step_avg:53.70ms
step:1447/1920 train_time:77736ms step_avg:53.72ms
step:1448/1920 train_time:77824ms step_avg:53.75ms
step:1449/1920 train_time:77913ms step_avg:53.77ms
step:1450/1920 train_time:78001ms step_avg:53.79ms
step:1451/1920 train_time:78091ms step_avg:53.82ms
step:1452/1920 train_time:78179ms step_avg:53.84ms
step:1453/1920 train_time:78269ms step_avg:53.87ms
step:1454/1920 train_time:78358ms step_avg:53.89ms
step:1455/1920 train_time:78447ms step_avg:53.92ms
step:1456/1920 train_time:78535ms step_avg:53.94ms
step:1457/1920 train_time:78624ms step_avg:53.96ms
step:1458/1920 train_time:78711ms step_avg:53.99ms
step:1459/1920 train_time:78800ms step_avg:54.01ms
step:1460/1920 train_time:78889ms step_avg:54.03ms
step:1461/1920 train_time:78978ms step_avg:54.06ms
step:1462/1920 train_time:79067ms step_avg:54.08ms
step:1463/1920 train_time:79156ms step_avg:54.11ms
step:1464/1920 train_time:79245ms step_avg:54.13ms
step:1465/1920 train_time:79333ms step_avg:54.15ms
step:1466/1920 train_time:79422ms step_avg:54.18ms
step:1467/1920 train_time:79511ms step_avg:54.20ms
step:1468/1920 train_time:79599ms step_avg:54.22ms
step:1469/1920 train_time:79689ms step_avg:54.25ms
step:1470/1920 train_time:79777ms step_avg:54.27ms
step:1471/1920 train_time:79867ms step_avg:54.29ms
step:1472/1920 train_time:79955ms step_avg:54.32ms
step:1473/1920 train_time:80044ms step_avg:54.34ms
step:1474/1920 train_time:80132ms step_avg:54.36ms
step:1475/1920 train_time:80221ms step_avg:54.39ms
step:1476/1920 train_time:80309ms step_avg:54.41ms
step:1477/1920 train_time:80397ms step_avg:54.43ms
step:1478/1920 train_time:80486ms step_avg:54.46ms
step:1479/1920 train_time:80574ms step_avg:54.48ms
step:1480/1920 train_time:80662ms step_avg:54.50ms
step:1481/1920 train_time:80750ms step_avg:54.52ms
step:1482/1920 train_time:80839ms step_avg:54.55ms
step:1483/1920 train_time:80927ms step_avg:54.57ms
step:1484/1920 train_time:81015ms step_avg:54.59ms
step:1485/1920 train_time:81103ms step_avg:54.61ms
step:1486/1920 train_time:81191ms step_avg:54.64ms
step:1487/1920 train_time:81280ms step_avg:54.66ms
step:1488/1920 train_time:81367ms step_avg:54.68ms
step:1489/1920 train_time:81456ms step_avg:54.71ms
step:1490/1920 train_time:81545ms step_avg:54.73ms
step:1491/1920 train_time:81633ms step_avg:54.75ms
step:1492/1920 train_time:81721ms step_avg:54.77ms
step:1493/1920 train_time:81810ms step_avg:54.80ms
step:1494/1920 train_time:81898ms step_avg:54.82ms
step:1495/1920 train_time:81989ms step_avg:54.84ms
step:1496/1920 train_time:82078ms step_avg:54.86ms
step:1497/1920 train_time:82168ms step_avg:54.89ms
step:1498/1920 train_time:82256ms step_avg:54.91ms
step:1499/1920 train_time:82345ms step_avg:54.93ms
step:1500/1920 train_time:82432ms step_avg:54.95ms
step:1500/1920 val_loss:3.4151 train_time:82523ms step_avg:55.02ms
step:1501/1920 train_time:82541ms step_avg:54.99ms
step:1502/1920 train_time:82611ms step_avg:55.00ms
step:1503/1920 train_time:82705ms step_avg:55.03ms
step:1504/1920 train_time:82793ms step_avg:55.05ms
step:1505/1920 train_time:82882ms step_avg:55.07ms
step:1506/1920 train_time:82969ms step_avg:55.09ms
step:1507/1920 train_time:83057ms step_avg:55.11ms
step:1508/1920 train_time:83144ms step_avg:55.14ms
step:1509/1920 train_time:83232ms step_avg:55.16ms
step:1510/1920 train_time:83319ms step_avg:55.18ms
step:1511/1920 train_time:83408ms step_avg:55.20ms
step:1512/1920 train_time:83498ms step_avg:55.22ms
step:1513/1920 train_time:83590ms step_avg:55.25ms
step:1514/1920 train_time:83681ms step_avg:55.27ms
step:1515/1920 train_time:83770ms step_avg:55.29ms
step:1516/1920 train_time:83858ms step_avg:55.32ms
step:1517/1920 train_time:83948ms step_avg:55.34ms
step:1518/1920 train_time:84036ms step_avg:55.36ms
step:1519/1920 train_time:84123ms step_avg:55.38ms
step:1520/1920 train_time:84210ms step_avg:55.40ms
step:1521/1920 train_time:84297ms step_avg:55.42ms
step:1522/1920 train_time:84385ms step_avg:55.44ms
step:1523/1920 train_time:84476ms step_avg:55.47ms
step:1524/1920 train_time:84565ms step_avg:55.49ms
step:1525/1920 train_time:84655ms step_avg:55.51ms
step:1526/1920 train_time:84744ms step_avg:55.53ms
step:1527/1920 train_time:84833ms step_avg:55.56ms
step:1528/1920 train_time:84922ms step_avg:55.58ms
step:1529/1920 train_time:85010ms step_avg:55.60ms
step:1530/1920 train_time:85098ms step_avg:55.62ms
step:1531/1920 train_time:85187ms step_avg:55.64ms
step:1532/1920 train_time:85273ms step_avg:55.66ms
step:1533/1920 train_time:85363ms step_avg:55.68ms
step:1534/1920 train_time:85450ms step_avg:55.70ms
step:1535/1920 train_time:85540ms step_avg:55.73ms
step:1536/1920 train_time:85628ms step_avg:55.75ms
step:1537/1920 train_time:85718ms step_avg:55.77ms
step:1538/1920 train_time:85807ms step_avg:55.79ms
step:1539/1920 train_time:85895ms step_avg:55.81ms
step:1540/1920 train_time:85983ms step_avg:55.83ms
step:1541/1920 train_time:86072ms step_avg:55.85ms
step:1542/1920 train_time:86159ms step_avg:55.87ms
step:1543/1920 train_time:86248ms step_avg:55.90ms
step:1544/1920 train_time:86336ms step_avg:55.92ms
step:1545/1920 train_time:86426ms step_avg:55.94ms
step:1546/1920 train_time:86514ms step_avg:55.96ms
step:1547/1920 train_time:86604ms step_avg:55.98ms
step:1548/1920 train_time:86692ms step_avg:56.00ms
step:1549/1920 train_time:86781ms step_avg:56.02ms
step:1550/1920 train_time:86869ms step_avg:56.04ms
step:1551/1920 train_time:86958ms step_avg:56.07ms
step:1552/1920 train_time:87046ms step_avg:56.09ms
step:1553/1920 train_time:87134ms step_avg:56.11ms
step:1554/1920 train_time:87222ms step_avg:56.13ms
step:1555/1920 train_time:87310ms step_avg:56.15ms
step:1556/1920 train_time:87398ms step_avg:56.17ms
step:1557/1920 train_time:87488ms step_avg:56.19ms
step:1558/1920 train_time:87578ms step_avg:56.21ms
step:1559/1920 train_time:87669ms step_avg:56.23ms
step:1560/1920 train_time:87758ms step_avg:56.26ms
step:1561/1920 train_time:87848ms step_avg:56.28ms
step:1562/1920 train_time:87936ms step_avg:56.30ms
step:1563/1920 train_time:88025ms step_avg:56.32ms
step:1564/1920 train_time:88113ms step_avg:56.34ms
step:1565/1920 train_time:88201ms step_avg:56.36ms
step:1566/1920 train_time:88289ms step_avg:56.38ms
step:1567/1920 train_time:88378ms step_avg:56.40ms
step:1568/1920 train_time:88465ms step_avg:56.42ms
step:1569/1920 train_time:88554ms step_avg:56.44ms
step:1570/1920 train_time:88645ms step_avg:56.46ms
step:1571/1920 train_time:88734ms step_avg:56.48ms
step:1572/1920 train_time:88822ms step_avg:56.50ms
step:1573/1920 train_time:88911ms step_avg:56.52ms
step:1574/1920 train_time:89000ms step_avg:56.54ms
step:1575/1920 train_time:89089ms step_avg:56.56ms
step:1576/1920 train_time:89177ms step_avg:56.58ms
step:1577/1920 train_time:89266ms step_avg:56.60ms
step:1578/1920 train_time:89354ms step_avg:56.62ms
step:1579/1920 train_time:89442ms step_avg:56.64ms
step:1580/1920 train_time:89530ms step_avg:56.66ms
step:1581/1920 train_time:89619ms step_avg:56.68ms
step:1582/1920 train_time:89708ms step_avg:56.71ms
step:1583/1920 train_time:89798ms step_avg:56.73ms
step:1584/1920 train_time:89887ms step_avg:56.75ms
step:1585/1920 train_time:89975ms step_avg:56.77ms
step:1586/1920 train_time:90064ms step_avg:56.79ms
step:1587/1920 train_time:90152ms step_avg:56.81ms
step:1588/1920 train_time:90241ms step_avg:56.83ms
step:1589/1920 train_time:90330ms step_avg:56.85ms
step:1590/1920 train_time:90418ms step_avg:56.87ms
step:1591/1920 train_time:90508ms step_avg:56.89ms
step:1592/1920 train_time:90598ms step_avg:56.91ms
step:1593/1920 train_time:90687ms step_avg:56.93ms
step:1594/1920 train_time:90775ms step_avg:56.95ms
step:1595/1920 train_time:90864ms step_avg:56.97ms
step:1596/1920 train_time:90951ms step_avg:56.99ms
step:1597/1920 train_time:91040ms step_avg:57.01ms
step:1598/1920 train_time:91128ms step_avg:57.03ms
step:1599/1920 train_time:91217ms step_avg:57.05ms
step:1600/1920 train_time:91306ms step_avg:57.07ms
step:1601/1920 train_time:91395ms step_avg:57.09ms
step:1602/1920 train_time:91483ms step_avg:57.11ms
step:1603/1920 train_time:91572ms step_avg:57.13ms
step:1604/1920 train_time:91659ms step_avg:57.14ms
step:1605/1920 train_time:91749ms step_avg:57.16ms
step:1606/1920 train_time:91837ms step_avg:57.18ms
step:1607/1920 train_time:91927ms step_avg:57.20ms
step:1608/1920 train_time:92014ms step_avg:57.22ms
step:1609/1920 train_time:92103ms step_avg:57.24ms
step:1610/1920 train_time:92191ms step_avg:57.26ms
step:1611/1920 train_time:92279ms step_avg:57.28ms
step:1612/1920 train_time:92367ms step_avg:57.30ms
step:1613/1920 train_time:92456ms step_avg:57.32ms
step:1614/1920 train_time:92544ms step_avg:57.34ms
step:1615/1920 train_time:92633ms step_avg:57.36ms
step:1616/1920 train_time:92722ms step_avg:57.38ms
step:1617/1920 train_time:92810ms step_avg:57.40ms
step:1618/1920 train_time:92899ms step_avg:57.42ms
step:1619/1920 train_time:92988ms step_avg:57.44ms
step:1620/1920 train_time:93076ms step_avg:57.45ms
step:1621/1920 train_time:93164ms step_avg:57.47ms
step:1622/1920 train_time:93252ms step_avg:57.49ms
step:1623/1920 train_time:93341ms step_avg:57.51ms
step:1624/1920 train_time:93428ms step_avg:57.53ms
step:1625/1920 train_time:93518ms step_avg:57.55ms
step:1626/1920 train_time:93607ms step_avg:57.57ms
step:1627/1920 train_time:93695ms step_avg:57.59ms
step:1628/1920 train_time:93783ms step_avg:57.61ms
step:1629/1920 train_time:93872ms step_avg:57.63ms
step:1630/1920 train_time:93961ms step_avg:57.64ms
step:1631/1920 train_time:94050ms step_avg:57.66ms
step:1632/1920 train_time:94138ms step_avg:57.68ms
step:1633/1920 train_time:94227ms step_avg:57.70ms
step:1634/1920 train_time:94315ms step_avg:57.72ms
step:1635/1920 train_time:94404ms step_avg:57.74ms
step:1636/1920 train_time:94493ms step_avg:57.76ms
step:1637/1920 train_time:94582ms step_avg:57.78ms
step:1638/1920 train_time:94669ms step_avg:57.80ms
step:1639/1920 train_time:94759ms step_avg:57.81ms
step:1640/1920 train_time:94846ms step_avg:57.83ms
step:1641/1920 train_time:94934ms step_avg:57.85ms
step:1642/1920 train_time:95023ms step_avg:57.87ms
step:1643/1920 train_time:95111ms step_avg:57.89ms
step:1644/1920 train_time:95199ms step_avg:57.91ms
step:1645/1920 train_time:95288ms step_avg:57.93ms
step:1646/1920 train_time:95376ms step_avg:57.94ms
step:1647/1920 train_time:95465ms step_avg:57.96ms
step:1648/1920 train_time:95553ms step_avg:57.98ms
step:1649/1920 train_time:95641ms step_avg:58.00ms
step:1650/1920 train_time:95729ms step_avg:58.02ms
step:1651/1920 train_time:95819ms step_avg:58.04ms
step:1652/1920 train_time:95907ms step_avg:58.05ms
step:1653/1920 train_time:95995ms step_avg:58.07ms
step:1654/1920 train_time:96084ms step_avg:58.09ms
step:1655/1920 train_time:96172ms step_avg:58.11ms
step:1656/1920 train_time:96260ms step_avg:58.13ms
step:1657/1920 train_time:96349ms step_avg:58.15ms
step:1658/1920 train_time:96438ms step_avg:58.17ms
step:1659/1920 train_time:96527ms step_avg:58.18ms
step:1660/1920 train_time:96615ms step_avg:58.20ms
step:1661/1920 train_time:96705ms step_avg:58.22ms
step:1662/1920 train_time:96793ms step_avg:58.24ms
step:1663/1920 train_time:96882ms step_avg:58.26ms
step:1664/1920 train_time:96970ms step_avg:58.28ms
step:1665/1920 train_time:97059ms step_avg:58.29ms
step:1666/1920 train_time:97148ms step_avg:58.31ms
step:1667/1920 train_time:97236ms step_avg:58.33ms
step:1668/1920 train_time:97325ms step_avg:58.35ms
step:1669/1920 train_time:97414ms step_avg:58.37ms
step:1670/1920 train_time:97503ms step_avg:58.38ms
step:1671/1920 train_time:97591ms step_avg:58.40ms
step:1672/1920 train_time:97679ms step_avg:58.42ms
step:1673/1920 train_time:97769ms step_avg:58.44ms
step:1674/1920 train_time:97858ms step_avg:58.46ms
step:1675/1920 train_time:97948ms step_avg:58.48ms
step:1676/1920 train_time:98036ms step_avg:58.49ms
step:1677/1920 train_time:98125ms step_avg:58.51ms
step:1678/1920 train_time:98214ms step_avg:58.53ms
step:1679/1920 train_time:98304ms step_avg:58.55ms
step:1680/1920 train_time:98392ms step_avg:58.57ms
step:1681/1920 train_time:98480ms step_avg:58.58ms
step:1682/1920 train_time:98568ms step_avg:58.60ms
step:1683/1920 train_time:98656ms step_avg:58.62ms
step:1684/1920 train_time:98745ms step_avg:58.64ms
step:1685/1920 train_time:98833ms step_avg:58.65ms
step:1686/1920 train_time:98923ms step_avg:58.67ms
step:1687/1920 train_time:99011ms step_avg:58.69ms
step:1688/1920 train_time:99100ms step_avg:58.71ms
step:1689/1920 train_time:99189ms step_avg:58.73ms
step:1690/1920 train_time:99277ms step_avg:58.74ms
step:1691/1920 train_time:99367ms step_avg:58.76ms
step:1692/1920 train_time:99455ms step_avg:58.78ms
step:1693/1920 train_time:99545ms step_avg:58.80ms
step:1694/1920 train_time:99632ms step_avg:58.81ms
step:1695/1920 train_time:99721ms step_avg:58.83ms
step:1696/1920 train_time:99808ms step_avg:58.85ms
step:1697/1920 train_time:99897ms step_avg:58.87ms
step:1698/1920 train_time:99985ms step_avg:58.88ms
step:1699/1920 train_time:100074ms step_avg:58.90ms
step:1700/1920 train_time:100162ms step_avg:58.92ms
step:1701/1920 train_time:100250ms step_avg:58.94ms
step:1702/1920 train_time:100338ms step_avg:58.95ms
step:1703/1920 train_time:100428ms step_avg:58.97ms
step:1704/1920 train_time:100517ms step_avg:58.99ms
step:1705/1920 train_time:100607ms step_avg:59.01ms
step:1706/1920 train_time:100695ms step_avg:59.02ms
step:1707/1920 train_time:100784ms step_avg:59.04ms
step:1708/1920 train_time:100872ms step_avg:59.06ms
step:1709/1920 train_time:100959ms step_avg:59.08ms
step:1710/1920 train_time:101048ms step_avg:59.09ms
step:1711/1920 train_time:101136ms step_avg:59.11ms
step:1712/1920 train_time:101225ms step_avg:59.13ms
step:1713/1920 train_time:101314ms step_avg:59.14ms
step:1714/1920 train_time:101402ms step_avg:59.16ms
step:1715/1920 train_time:101491ms step_avg:59.18ms
step:1716/1920 train_time:101580ms step_avg:59.20ms
step:1717/1920 train_time:101669ms step_avg:59.21ms
step:1718/1920 train_time:101757ms step_avg:59.23ms
step:1719/1920 train_time:101848ms step_avg:59.25ms
step:1720/1920 train_time:101937ms step_avg:59.27ms
step:1721/1920 train_time:102027ms step_avg:59.28ms
step:1722/1920 train_time:102116ms step_avg:59.30ms
step:1723/1920 train_time:102205ms step_avg:59.32ms
step:1724/1920 train_time:102293ms step_avg:59.33ms
step:1725/1920 train_time:102382ms step_avg:59.35ms
step:1726/1920 train_time:102469ms step_avg:59.37ms
step:1727/1920 train_time:102558ms step_avg:59.39ms
step:1728/1920 train_time:102646ms step_avg:59.40ms
step:1729/1920 train_time:102735ms step_avg:59.42ms
step:1730/1920 train_time:102822ms step_avg:59.43ms
step:1731/1920 train_time:102911ms step_avg:59.45ms
step:1732/1920 train_time:102999ms step_avg:59.47ms
step:1733/1920 train_time:103088ms step_avg:59.49ms
step:1734/1920 train_time:103177ms step_avg:59.50ms
step:1735/1920 train_time:103267ms step_avg:59.52ms
step:1736/1920 train_time:103354ms step_avg:59.54ms
step:1737/1920 train_time:103443ms step_avg:59.55ms
step:1738/1920 train_time:103531ms step_avg:59.57ms
step:1739/1920 train_time:103621ms step_avg:59.59ms
step:1740/1920 train_time:103709ms step_avg:59.60ms
step:1741/1920 train_time:103800ms step_avg:59.62ms
step:1742/1920 train_time:103888ms step_avg:59.64ms
step:1743/1920 train_time:103977ms step_avg:59.65ms
step:1744/1920 train_time:104065ms step_avg:59.67ms
step:1745/1920 train_time:104153ms step_avg:59.69ms
step:1746/1920 train_time:104241ms step_avg:59.70ms
step:1747/1920 train_time:104330ms step_avg:59.72ms
step:1748/1920 train_time:104418ms step_avg:59.74ms
step:1749/1920 train_time:104509ms step_avg:59.75ms
step:1750/1920 train_time:104599ms step_avg:59.77ms
step:1750/1920 val_loss:3.3233 train_time:104691ms step_avg:59.82ms
step:1751/1920 train_time:104709ms step_avg:59.80ms
step:1752/1920 train_time:104779ms step_avg:59.81ms
step:1753/1920 train_time:104870ms step_avg:59.82ms
step:1754/1920 train_time:104958ms step_avg:59.84ms
step:1755/1920 train_time:105046ms step_avg:59.86ms
step:1756/1920 train_time:105133ms step_avg:59.87ms
step:1757/1920 train_time:105220ms step_avg:59.89ms
step:1758/1920 train_time:105308ms step_avg:59.90ms
step:1759/1920 train_time:105398ms step_avg:59.92ms
step:1760/1920 train_time:105486ms step_avg:59.94ms
step:1761/1920 train_time:105575ms step_avg:59.95ms
step:1762/1920 train_time:105665ms step_avg:59.97ms
step:1763/1920 train_time:105754ms step_avg:59.99ms
step:1764/1920 train_time:105843ms step_avg:60.00ms
step:1765/1920 train_time:105932ms step_avg:60.02ms
step:1766/1920 train_time:106020ms step_avg:60.03ms
step:1767/1920 train_time:106108ms step_avg:60.05ms
step:1768/1920 train_time:106196ms step_avg:60.07ms
step:1769/1920 train_time:106284ms step_avg:60.08ms
step:1770/1920 train_time:106372ms step_avg:60.10ms
step:1771/1920 train_time:106460ms step_avg:60.11ms
step:1772/1920 train_time:106548ms step_avg:60.13ms
step:1773/1920 train_time:106638ms step_avg:60.15ms
step:1774/1920 train_time:106727ms step_avg:60.16ms
step:1775/1920 train_time:106816ms step_avg:60.18ms
step:1776/1920 train_time:106904ms step_avg:60.19ms
step:1777/1920 train_time:106993ms step_avg:60.21ms
step:1778/1920 train_time:107081ms step_avg:60.23ms
step:1779/1920 train_time:107169ms step_avg:60.24ms
step:1780/1920 train_time:107257ms step_avg:60.26ms
step:1781/1920 train_time:107345ms step_avg:60.27ms
step:1782/1920 train_time:107432ms step_avg:60.29ms
step:1783/1920 train_time:107521ms step_avg:60.30ms
step:1784/1920 train_time:107609ms step_avg:60.32ms
step:1785/1920 train_time:107699ms step_avg:60.34ms
step:1786/1920 train_time:107788ms step_avg:60.35ms
step:1787/1920 train_time:107879ms step_avg:60.37ms
step:1788/1920 train_time:107966ms step_avg:60.38ms
step:1789/1920 train_time:108055ms step_avg:60.40ms
step:1790/1920 train_time:108142ms step_avg:60.41ms
step:1791/1920 train_time:108231ms step_avg:60.43ms
step:1792/1920 train_time:108319ms step_avg:60.45ms
step:1793/1920 train_time:108407ms step_avg:60.46ms
step:1794/1920 train_time:108496ms step_avg:60.48ms
step:1795/1920 train_time:108585ms step_avg:60.49ms
step:1796/1920 train_time:108673ms step_avg:60.51ms
step:1797/1920 train_time:108763ms step_avg:60.52ms
step:1798/1920 train_time:108851ms step_avg:60.54ms
step:1799/1920 train_time:108940ms step_avg:60.56ms
step:1800/1920 train_time:109028ms step_avg:60.57ms
step:1801/1920 train_time:109117ms step_avg:60.59ms
step:1802/1920 train_time:109205ms step_avg:60.60ms
step:1803/1920 train_time:109293ms step_avg:60.62ms
step:1804/1920 train_time:109381ms step_avg:60.63ms
step:1805/1920 train_time:109470ms step_avg:60.65ms
step:1806/1920 train_time:109558ms step_avg:60.66ms
step:1807/1920 train_time:109648ms step_avg:60.68ms
step:1808/1920 train_time:109736ms step_avg:60.69ms
step:1809/1920 train_time:109825ms step_avg:60.71ms
step:1810/1920 train_time:109915ms step_avg:60.73ms
step:1811/1920 train_time:110004ms step_avg:60.74ms
step:1812/1920 train_time:110094ms step_avg:60.76ms
step:1813/1920 train_time:110182ms step_avg:60.77ms
step:1814/1920 train_time:110270ms step_avg:60.79ms
step:1815/1920 train_time:110359ms step_avg:60.80ms
step:1816/1920 train_time:110447ms step_avg:60.82ms
step:1817/1920 train_time:110536ms step_avg:60.83ms
step:1818/1920 train_time:110624ms step_avg:60.85ms
step:1819/1920 train_time:110713ms step_avg:60.86ms
step:1820/1920 train_time:110801ms step_avg:60.88ms
step:1821/1920 train_time:110889ms step_avg:60.89ms
step:1822/1920 train_time:110978ms step_avg:60.91ms
step:1823/1920 train_time:111066ms step_avg:60.92ms
step:1824/1920 train_time:111155ms step_avg:60.94ms
step:1825/1920 train_time:111243ms step_avg:60.96ms
step:1826/1920 train_time:111332ms step_avg:60.97ms
step:1827/1920 train_time:111420ms step_avg:60.99ms
step:1828/1920 train_time:111508ms step_avg:61.00ms
step:1829/1920 train_time:111597ms step_avg:61.02ms
step:1830/1920 train_time:111685ms step_avg:61.03ms
step:1831/1920 train_time:111773ms step_avg:61.04ms
step:1832/1920 train_time:111862ms step_avg:61.06ms
step:1833/1920 train_time:111951ms step_avg:61.08ms
step:1834/1920 train_time:112040ms step_avg:61.09ms
step:1835/1920 train_time:112129ms step_avg:61.11ms
step:1836/1920 train_time:112217ms step_avg:61.12ms
step:1837/1920 train_time:112306ms step_avg:61.14ms
step:1838/1920 train_time:112395ms step_avg:61.15ms
step:1839/1920 train_time:112484ms step_avg:61.17ms
step:1840/1920 train_time:112571ms step_avg:61.18ms
step:1841/1920 train_time:112661ms step_avg:61.20ms
step:1842/1920 train_time:112750ms step_avg:61.21ms
step:1843/1920 train_time:112838ms step_avg:61.23ms
step:1844/1920 train_time:112926ms step_avg:61.24ms
step:1845/1920 train_time:113015ms step_avg:61.25ms
step:1846/1920 train_time:113103ms step_avg:61.27ms
step:1847/1920 train_time:113192ms step_avg:61.28ms
step:1848/1920 train_time:113279ms step_avg:61.30ms
step:1849/1920 train_time:113368ms step_avg:61.31ms
step:1850/1920 train_time:113457ms step_avg:61.33ms
step:1851/1920 train_time:113546ms step_avg:61.34ms
step:1852/1920 train_time:113634ms step_avg:61.36ms
step:1853/1920 train_time:113723ms step_avg:61.37ms
step:1854/1920 train_time:113811ms step_avg:61.39ms
step:1855/1920 train_time:113901ms step_avg:61.40ms
step:1856/1920 train_time:113988ms step_avg:61.42ms
step:1857/1920 train_time:114079ms step_avg:61.43ms
step:1858/1920 train_time:114169ms step_avg:61.45ms
step:1859/1920 train_time:114258ms step_avg:61.46ms
step:1860/1920 train_time:114346ms step_avg:61.48ms
step:1861/1920 train_time:114435ms step_avg:61.49ms
step:1862/1920 train_time:114523ms step_avg:61.51ms
step:1863/1920 train_time:114611ms step_avg:61.52ms
step:1864/1920 train_time:114700ms step_avg:61.53ms
step:1865/1920 train_time:114789ms step_avg:61.55ms
step:1866/1920 train_time:114877ms step_avg:61.56ms
step:1867/1920 train_time:114966ms step_avg:61.58ms
step:1868/1920 train_time:115055ms step_avg:61.59ms
step:1869/1920 train_time:115144ms step_avg:61.61ms
step:1870/1920 train_time:115232ms step_avg:61.62ms
step:1871/1920 train_time:115321ms step_avg:61.64ms
step:1872/1920 train_time:115409ms step_avg:61.65ms
step:1873/1920 train_time:115499ms step_avg:61.67ms
step:1874/1920 train_time:115587ms step_avg:61.68ms
step:1875/1920 train_time:115676ms step_avg:61.69ms
step:1876/1920 train_time:115763ms step_avg:61.71ms
step:1877/1920 train_time:115852ms step_avg:61.72ms
step:1878/1920 train_time:115939ms step_avg:61.74ms
step:1879/1920 train_time:116029ms step_avg:61.75ms
step:1880/1920 train_time:116116ms step_avg:61.76ms
step:1881/1920 train_time:116205ms step_avg:61.78ms
step:1882/1920 train_time:116294ms step_avg:61.79ms
step:1883/1920 train_time:116382ms step_avg:61.81ms
step:1884/1920 train_time:116471ms step_avg:61.82ms
step:1885/1920 train_time:116561ms step_avg:61.84ms
step:1886/1920 train_time:116649ms step_avg:61.85ms
step:1887/1920 train_time:116739ms step_avg:61.86ms
step:1888/1920 train_time:116827ms step_avg:61.88ms
step:1889/1920 train_time:116916ms step_avg:61.89ms
step:1890/1920 train_time:117004ms step_avg:61.91ms
step:1891/1920 train_time:117093ms step_avg:61.92ms
step:1892/1920 train_time:117181ms step_avg:61.93ms
step:1893/1920 train_time:117271ms step_avg:61.95ms
step:1894/1920 train_time:117359ms step_avg:61.96ms
step:1895/1920 train_time:117449ms step_avg:61.98ms
step:1896/1920 train_time:117538ms step_avg:61.99ms
step:1897/1920 train_time:117627ms step_avg:62.01ms
step:1898/1920 train_time:117715ms step_avg:62.02ms
step:1899/1920 train_time:117803ms step_avg:62.03ms
step:1900/1920 train_time:117892ms step_avg:62.05ms
step:1901/1920 train_time:117982ms step_avg:62.06ms
step:1902/1920 train_time:118071ms step_avg:62.08ms
step:1903/1920 train_time:118160ms step_avg:62.09ms
step:1904/1920 train_time:118248ms step_avg:62.11ms
step:1905/1920 train_time:118338ms step_avg:62.12ms
step:1906/1920 train_time:118427ms step_avg:62.13ms
step:1907/1920 train_time:118515ms step_avg:62.15ms
step:1908/1920 train_time:118603ms step_avg:62.16ms
step:1909/1920 train_time:118693ms step_avg:62.18ms
step:1910/1920 train_time:118781ms step_avg:62.19ms
step:1911/1920 train_time:118871ms step_avg:62.20ms
step:1912/1920 train_time:118959ms step_avg:62.22ms
step:1913/1920 train_time:119048ms step_avg:62.23ms
step:1914/1920 train_time:119137ms step_avg:62.25ms
step:1915/1920 train_time:119227ms step_avg:62.26ms
step:1916/1920 train_time:119316ms step_avg:62.27ms
step:1917/1920 train_time:119405ms step_avg:62.29ms
step:1918/1920 train_time:119493ms step_avg:62.30ms
step:1919/1920 train_time:119582ms step_avg:62.31ms
step:1920/1920 train_time:119670ms step_avg:62.33ms
step:1920/1920 val_loss:3.2784 train_time:119762ms step_avg:62.38ms
peak memory allocated: 29817 MiB reserved: 44678 MiB

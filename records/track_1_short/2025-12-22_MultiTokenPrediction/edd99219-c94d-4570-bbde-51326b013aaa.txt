import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, module_group_order: list[str], group_sizes: list[int], lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if group_sizes is not None and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params, module_group_order, group_sizes)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params, module_group_order: list[str], group_sizes: list[int]):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int, mtp_weights: Tensor = None):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1880  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    transition_steps: int = 40  # steps to pause scalar optimizer during batch size/ws transitions
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
scalar_labels = ['scalars']
muon_labels = ['attn_gate', 'attn', 'mlp']
muon_group_sizes = [10, 16, 16]
all_labels = set(getattr(p, 'label', None) for p in model.parameters())
assert all(getattr(p, 'label', None) is not None for p in model.parameters()), "All params must have a label"
assert set(adam_labels + scalar_labels + muon_labels) == all_labels, f"Label mismatch: {set(adam_labels + muon_labels)} != {all_labels}"
adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]


# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
adam_optimizers = [
    DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005),
    DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005),  # higher betas for smoothing
]
muon_optimizers = [NorMuon(muon_params, muon_labels, muon_group_sizes, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)]
optimizers = adam_optimizers + muon_optimizers
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

# Precompute MTP weights for all steps to avoid tensor allocation during training
# Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
mtp_weights_schedule = []
for s in range(args.num_iterations + 1):
    x = s / args.num_scheduled_iterations
    if x < 1/3:
        w = [1.0, 0.5, 0.25 * (1 - 3*x)]
    elif x < 2/3:
        w = [1.0, 0.5 * (1 - (3*x - 1))]
    else:
        w = [1.0]
    mtp_weights_schedule.append(torch.tensor(w, device=device))

def step_optimizers(step: int, optimizers, model, start_transition: bool):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # Initialize transition counter if needed
    if start_transition:
        adam_optimizers[1].transition_steps = args.transition_steps

    # Check if we are currently in a transition
    steps_remaining = getattr(adam_optimizers[1], 'transition_steps', 0)
    in_transition = steps_remaining > 0

    if in_transition:
        adam_optimizers[1].transition_steps -= 1

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for opt in muon_optimizers:
        for group in opt.param_groups:
            group["momentum"] = momentum

    # on even steps, only step Muon params
    if step%2==0:
        for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
    # on odd steps, step all params except scalar optimizer during transition
    else:
        for opt in optimizers:
            if in_transition and opt is adam_optimizers[1]:
                continue # skip scalar optimizer during transition
            else:
                opt.step()
        model.zero_grad(set_to_none=True)
        for opt in adam_optimizers: opt.should_sync = False

    # During transition, zero scalar grads to prevent accumulation on any step.
    if in_transition:
        for p in scalar_params:
            p.grad = None


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

base_weights = torch.tensor([1.0, 0.5, 0.25], device=device)
for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    mtp_weights = base_weights[:max(1, 3 - idx)].clone()
    model.split_embed = (idx >= args.split_embed_frac * len(args.ws_schedule))
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        (model(inputs, targets, cum_seqlens, ws_long//2, ws_long, mtp_weights) / grad_accum_steps).backward()
        if step % 2 == 0:
            for opt in muon_optimizers: opt.step(), opt.zero_grad(set_to_none=True)
        else:
            for opt in optimizers: opt.step()
            model.zero_grad(set_to_none=True)
            for opt in adam_optimizers: opt.should_sync = False

model.zero_grad(set_to_none=True)
for opt in adam_optimizers: opt.should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        model.split_embed = (ws_idx >= 1)
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
for opt in muon_optimizers: opt.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state
model.split_embed = False


########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations)
ws_short, ws_long = get_ws(0)

for step in range(train_steps + 1):
    last_step = (step == train_steps)
    is_transition = False # track sudden shifts
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long
        is_transition = True
    if step == split_step:
        model.split_embed = True

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
            is_transition = True
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = None
    if new_step_batch_size != step_batch_size:
        send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps)
        is_transition = True

    step_batch_size = new_step_batch_size
    mtp_weights = mtp_weights_schedule[step]
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            for opt in adam_optimizers: opt.should_sync = True
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long, mtp_weights) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model, is_transition)
    # copy lm_head weights/state to embed before split
    if step == (split_step - 2) | 1:
        adam_optimizers[0].copy_lm_to_embed()

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 (main, Dec  9 2025, 19:02:36) [Clang 21.1.4 ]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 22 10:58:04 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            131W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   34C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   36C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   44C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   44C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   36C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   42C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   35C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    222291      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    222292      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    222293      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    222294      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    222295      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    222296      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    222297      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    222298      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    222292      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    222293      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    222294      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    222295      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    222296      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    222297      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    222298      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1920 val_loss:10.8374 train_time:0ms step_avg:0.04ms
step:1/1920 train_time:85ms step_avg:84.90ms
step:2/1920 train_time:106ms step_avg:53.13ms
step:3/1920 train_time:130ms step_avg:43.35ms
step:4/1920 train_time:164ms step_avg:40.98ms
step:5/1920 train_time:198ms step_avg:39.61ms
step:6/1920 train_time:282ms step_avg:47.04ms
step:7/1920 train_time:300ms step_avg:42.79ms
step:8/1920 train_time:334ms step_avg:41.71ms
step:9/1920 train_time:368ms step_avg:40.86ms
step:10/1920 train_time:402ms step_avg:40.17ms
step:11/1920 train_time:436ms step_avg:39.67ms
step:12/1920 train_time:470ms step_avg:39.21ms
step:13/1920 train_time:505ms step_avg:38.84ms
step:14/1920 train_time:539ms step_avg:38.50ms
step:15/1920 train_time:574ms step_avg:38.25ms
step:16/1920 train_time:608ms step_avg:38.00ms
step:17/1920 train_time:642ms step_avg:37.79ms
step:18/1920 train_time:677ms step_avg:37.59ms
step:19/1920 train_time:711ms step_avg:37.43ms
step:20/1920 train_time:745ms step_avg:37.26ms
step:21/1920 train_time:780ms step_avg:37.14ms
step:22/1920 train_time:814ms step_avg:37.01ms
step:23/1920 train_time:849ms step_avg:36.90ms
step:24/1920 train_time:883ms step_avg:36.79ms
step:25/1920 train_time:917ms step_avg:36.70ms
step:26/1920 train_time:952ms step_avg:36.61ms
step:27/1920 train_time:986ms step_avg:36.52ms
step:28/1920 train_time:1020ms step_avg:36.44ms
step:29/1920 train_time:1055ms step_avg:36.37ms
step:30/1920 train_time:1089ms step_avg:36.29ms
step:31/1920 train_time:1123ms step_avg:36.24ms
step:32/1920 train_time:1158ms step_avg:36.19ms
step:33/1920 train_time:1194ms step_avg:36.18ms
step:34/1920 train_time:1228ms step_avg:36.13ms
step:35/1920 train_time:1264ms step_avg:36.10ms
step:36/1920 train_time:1298ms step_avg:36.06ms
step:37/1920 train_time:1333ms step_avg:36.04ms
step:38/1920 train_time:1368ms step_avg:35.99ms
step:39/1920 train_time:1402ms step_avg:35.96ms
step:40/1920 train_time:1437ms step_avg:35.91ms
step:41/1920 train_time:1471ms step_avg:35.88ms
step:42/1920 train_time:1505ms step_avg:35.84ms
step:43/1920 train_time:1540ms step_avg:35.82ms
step:44/1920 train_time:1575ms step_avg:35.78ms
step:45/1920 train_time:1609ms step_avg:35.76ms
step:46/1920 train_time:1643ms step_avg:35.72ms
step:47/1920 train_time:1678ms step_avg:35.70ms
step:48/1920 train_time:1712ms step_avg:35.67ms
step:49/1920 train_time:1746ms step_avg:35.64ms
step:50/1920 train_time:1781ms step_avg:35.61ms
step:51/1920 train_time:1815ms step_avg:35.59ms
step:52/1920 train_time:1849ms step_avg:35.55ms
step:53/1920 train_time:1883ms step_avg:35.53ms
step:54/1920 train_time:1917ms step_avg:35.51ms
step:55/1920 train_time:1952ms step_avg:35.49ms
step:56/1920 train_time:1986ms step_avg:35.47ms
step:57/1920 train_time:2021ms step_avg:35.46ms
step:58/1920 train_time:2055ms step_avg:35.44ms
step:59/1920 train_time:2089ms step_avg:35.42ms
step:60/1920 train_time:2124ms step_avg:35.39ms
step:61/1920 train_time:2159ms step_avg:35.39ms
step:62/1920 train_time:2193ms step_avg:35.37ms
step:63/1920 train_time:2228ms step_avg:35.36ms
step:64/1920 train_time:2262ms step_avg:35.34ms
step:65/1920 train_time:2297ms step_avg:35.33ms
step:66/1920 train_time:2331ms step_avg:35.32ms
step:67/1920 train_time:2366ms step_avg:35.31ms
step:68/1920 train_time:2400ms step_avg:35.29ms
step:69/1920 train_time:2435ms step_avg:35.29ms
step:70/1920 train_time:2469ms step_avg:35.27ms
step:71/1920 train_time:2504ms step_avg:35.26ms
step:72/1920 train_time:2538ms step_avg:35.25ms
step:73/1920 train_time:2573ms step_avg:35.25ms
step:74/1920 train_time:2607ms step_avg:35.24ms
step:75/1920 train_time:2642ms step_avg:35.23ms
step:76/1920 train_time:2676ms step_avg:35.21ms
step:77/1920 train_time:2711ms step_avg:35.21ms
step:78/1920 train_time:2745ms step_avg:35.19ms
step:79/1920 train_time:2780ms step_avg:35.19ms
step:80/1920 train_time:2814ms step_avg:35.17ms
step:81/1920 train_time:2848ms step_avg:35.17ms
step:82/1920 train_time:2882ms step_avg:35.15ms
step:83/1920 train_time:2917ms step_avg:35.14ms
step:84/1920 train_time:2952ms step_avg:35.14ms
step:85/1920 train_time:2986ms step_avg:35.13ms
step:86/1920 train_time:3020ms step_avg:35.11ms
step:87/1920 train_time:3055ms step_avg:35.11ms
step:88/1920 train_time:3089ms step_avg:35.11ms
step:89/1920 train_time:3124ms step_avg:35.10ms
step:90/1920 train_time:3158ms step_avg:35.09ms
step:91/1920 train_time:3193ms step_avg:35.09ms
step:92/1920 train_time:3228ms step_avg:35.08ms
step:93/1920 train_time:3262ms step_avg:35.08ms
step:94/1920 train_time:3297ms step_avg:35.07ms
step:95/1920 train_time:3331ms step_avg:35.07ms
step:96/1920 train_time:3366ms step_avg:35.06ms
step:97/1920 train_time:3400ms step_avg:35.05ms
step:98/1920 train_time:3435ms step_avg:35.05ms
step:99/1920 train_time:3469ms step_avg:35.04ms
step:100/1920 train_time:3503ms step_avg:35.03ms
step:101/1920 train_time:3538ms step_avg:35.03ms
step:102/1920 train_time:3572ms step_avg:35.02ms
step:103/1920 train_time:3607ms step_avg:35.02ms
step:104/1920 train_time:3641ms step_avg:35.01ms
step:105/1920 train_time:3676ms step_avg:35.01ms
step:106/1920 train_time:3710ms step_avg:35.00ms
step:107/1920 train_time:3745ms step_avg:35.00ms
step:108/1920 train_time:3779ms step_avg:34.99ms
step:109/1920 train_time:3814ms step_avg:34.99ms
step:110/1920 train_time:3848ms step_avg:34.98ms
step:111/1920 train_time:3882ms step_avg:34.98ms
step:112/1920 train_time:3917ms step_avg:34.97ms
step:113/1920 train_time:3951ms step_avg:34.96ms
step:114/1920 train_time:3985ms step_avg:34.96ms
step:115/1920 train_time:4020ms step_avg:34.96ms
step:116/1920 train_time:4054ms step_avg:34.95ms
step:117/1920 train_time:4089ms step_avg:34.95ms
step:118/1920 train_time:4123ms step_avg:34.94ms
step:119/1920 train_time:4158ms step_avg:34.94ms
step:120/1920 train_time:4192ms step_avg:34.94ms
step:121/1920 train_time:4227ms step_avg:34.93ms
step:122/1920 train_time:4261ms step_avg:34.93ms
step:123/1920 train_time:4296ms step_avg:34.93ms
step:124/1920 train_time:4330ms step_avg:34.92ms
step:125/1920 train_time:4365ms step_avg:34.92ms
step:126/1920 train_time:4399ms step_avg:34.91ms
step:127/1920 train_time:4434ms step_avg:34.91ms
step:128/1920 train_time:4468ms step_avg:34.91ms
step:129/1920 train_time:4503ms step_avg:34.91ms
step:130/1920 train_time:4537ms step_avg:34.90ms
step:131/1920 train_time:4572ms step_avg:34.90ms
step:132/1920 train_time:4606ms step_avg:34.89ms
step:133/1920 train_time:4640ms step_avg:34.89ms
step:134/1920 train_time:4674ms step_avg:34.88ms
step:135/1920 train_time:4709ms step_avg:34.88ms
step:136/1920 train_time:4743ms step_avg:34.88ms
step:137/1920 train_time:4778ms step_avg:34.88ms
step:138/1920 train_time:4812ms step_avg:34.87ms
step:139/1920 train_time:4847ms step_avg:34.87ms
step:140/1920 train_time:4881ms step_avg:34.86ms
step:141/1920 train_time:4916ms step_avg:34.86ms
step:142/1920 train_time:4950ms step_avg:34.86ms
step:143/1920 train_time:4984ms step_avg:34.85ms
step:144/1920 train_time:5018ms step_avg:34.85ms
step:145/1920 train_time:5052ms step_avg:34.84ms
step:146/1920 train_time:5087ms step_avg:34.84ms
step:147/1920 train_time:5121ms step_avg:34.84ms
step:148/1920 train_time:5156ms step_avg:34.84ms
step:149/1920 train_time:5190ms step_avg:34.83ms
step:150/1920 train_time:5224ms step_avg:34.83ms
step:151/1920 train_time:5259ms step_avg:34.83ms
step:152/1920 train_time:5293ms step_avg:34.83ms
step:153/1920 train_time:5328ms step_avg:34.83ms
step:154/1920 train_time:5363ms step_avg:34.82ms
step:155/1920 train_time:5398ms step_avg:34.82ms
step:156/1920 train_time:5432ms step_avg:34.82ms
step:157/1920 train_time:5466ms step_avg:34.82ms
step:158/1920 train_time:5500ms step_avg:34.81ms
step:159/1920 train_time:5535ms step_avg:34.81ms
step:160/1920 train_time:5569ms step_avg:34.81ms
step:161/1920 train_time:5604ms step_avg:34.81ms
step:162/1920 train_time:5638ms step_avg:34.80ms
step:163/1920 train_time:5673ms step_avg:34.80ms
step:164/1920 train_time:5707ms step_avg:34.80ms
step:165/1920 train_time:5742ms step_avg:34.80ms
step:166/1920 train_time:5776ms step_avg:34.79ms
step:167/1920 train_time:5810ms step_avg:34.79ms
step:168/1920 train_time:5845ms step_avg:34.79ms
step:169/1920 train_time:5879ms step_avg:34.79ms
step:170/1920 train_time:5914ms step_avg:34.79ms
step:171/1920 train_time:5948ms step_avg:34.78ms
step:172/1920 train_time:5982ms step_avg:34.78ms
step:173/1920 train_time:6017ms step_avg:34.78ms
step:174/1920 train_time:6051ms step_avg:34.78ms
step:175/1920 train_time:6085ms step_avg:34.77ms
step:176/1920 train_time:6119ms step_avg:34.77ms
step:177/1920 train_time:6154ms step_avg:34.77ms
step:178/1920 train_time:6188ms step_avg:34.77ms
step:179/1920 train_time:6223ms step_avg:34.77ms
step:180/1920 train_time:6257ms step_avg:34.76ms
step:181/1920 train_time:6292ms step_avg:34.76ms
step:182/1920 train_time:6326ms step_avg:34.76ms
step:183/1920 train_time:6361ms step_avg:34.76ms
step:184/1920 train_time:6395ms step_avg:34.76ms
step:185/1920 train_time:6430ms step_avg:34.76ms
step:186/1920 train_time:6464ms step_avg:34.75ms
step:187/1920 train_time:6499ms step_avg:34.76ms
step:188/1920 train_time:6534ms step_avg:34.75ms
step:189/1920 train_time:6568ms step_avg:34.75ms
step:190/1920 train_time:6602ms step_avg:34.75ms
step:191/1920 train_time:6636ms step_avg:34.75ms
step:192/1920 train_time:6671ms step_avg:34.74ms
step:193/1920 train_time:6705ms step_avg:34.74ms
step:194/1920 train_time:6739ms step_avg:34.74ms
step:195/1920 train_time:6774ms step_avg:34.74ms
step:196/1920 train_time:6808ms step_avg:34.73ms
step:197/1920 train_time:6842ms step_avg:34.73ms
step:198/1920 train_time:6876ms step_avg:34.73ms
step:199/1920 train_time:6911ms step_avg:34.73ms
step:200/1920 train_time:6945ms step_avg:34.73ms
step:201/1920 train_time:6980ms step_avg:34.73ms
step:202/1920 train_time:7014ms step_avg:34.73ms
step:203/1920 train_time:7049ms step_avg:34.72ms
step:204/1920 train_time:7083ms step_avg:34.72ms
step:205/1920 train_time:7118ms step_avg:34.72ms
step:206/1920 train_time:7152ms step_avg:34.72ms
step:207/1920 train_time:7186ms step_avg:34.71ms
step:208/1920 train_time:7220ms step_avg:34.71ms
step:209/1920 train_time:7255ms step_avg:34.71ms
step:210/1920 train_time:7289ms step_avg:34.71ms
step:211/1920 train_time:7324ms step_avg:34.71ms
step:212/1920 train_time:7358ms step_avg:34.71ms
step:213/1920 train_time:7392ms step_avg:34.71ms
step:214/1920 train_time:7427ms step_avg:34.70ms
step:215/1920 train_time:7461ms step_avg:34.70ms
step:216/1920 train_time:7495ms step_avg:34.70ms
step:217/1920 train_time:7530ms step_avg:34.70ms
step:218/1920 train_time:7564ms step_avg:34.70ms
step:219/1920 train_time:7599ms step_avg:34.70ms
step:220/1920 train_time:7633ms step_avg:34.70ms
step:221/1920 train_time:7667ms step_avg:34.69ms
step:222/1920 train_time:7702ms step_avg:34.69ms
step:223/1920 train_time:7736ms step_avg:34.69ms
step:224/1920 train_time:7770ms step_avg:34.69ms
step:225/1920 train_time:7805ms step_avg:34.69ms
step:226/1920 train_time:7839ms step_avg:34.68ms
step:227/1920 train_time:7873ms step_avg:34.68ms
step:228/1920 train_time:7907ms step_avg:34.68ms
step:229/1920 train_time:7941ms step_avg:34.68ms
step:230/1920 train_time:7976ms step_avg:34.68ms
step:231/1920 train_time:8010ms step_avg:34.68ms
step:232/1920 train_time:8044ms step_avg:34.67ms
step:233/1920 train_time:8079ms step_avg:34.67ms
step:234/1920 train_time:8113ms step_avg:34.67ms
step:235/1920 train_time:8147ms step_avg:34.67ms
step:236/1920 train_time:8181ms step_avg:34.67ms
step:237/1920 train_time:8216ms step_avg:34.67ms
step:238/1920 train_time:8250ms step_avg:34.66ms
step:239/1920 train_time:8285ms step_avg:34.66ms
step:240/1920 train_time:8319ms step_avg:34.66ms
step:241/1920 train_time:8354ms step_avg:34.66ms
step:242/1920 train_time:8388ms step_avg:34.66ms
step:243/1920 train_time:8423ms step_avg:34.66ms
step:244/1920 train_time:8457ms step_avg:34.66ms
step:245/1920 train_time:8491ms step_avg:34.66ms
step:246/1920 train_time:8525ms step_avg:34.66ms
step:247/1920 train_time:8560ms step_avg:34.66ms
step:248/1920 train_time:8594ms step_avg:34.65ms
step:249/1920 train_time:8629ms step_avg:34.65ms
step:250/1920 train_time:8663ms step_avg:34.65ms
step:250/1920 val_loss:4.5985 train_time:8700ms step_avg:34.80ms
step:251/1920 train_time:8718ms step_avg:34.73ms
step:252/1920 train_time:8735ms step_avg:34.66ms
step:253/1920 train_time:8771ms step_avg:34.67ms
step:254/1920 train_time:8805ms step_avg:34.67ms
step:255/1920 train_time:8840ms step_avg:34.67ms
step:256/1920 train_time:8876ms step_avg:34.67ms
step:257/1920 train_time:8910ms step_avg:34.67ms
step:258/1920 train_time:8944ms step_avg:34.67ms
step:259/1920 train_time:8979ms step_avg:34.67ms
step:260/1920 train_time:9013ms step_avg:34.66ms
step:261/1920 train_time:9047ms step_avg:34.66ms
step:262/1920 train_time:9081ms step_avg:34.66ms
step:263/1920 train_time:9115ms step_avg:34.66ms
step:264/1920 train_time:9149ms step_avg:34.66ms
step:265/1920 train_time:9183ms step_avg:34.65ms
step:266/1920 train_time:9217ms step_avg:34.65ms
step:267/1920 train_time:9252ms step_avg:34.65ms
step:268/1920 train_time:9286ms step_avg:34.65ms
step:269/1920 train_time:9320ms step_avg:34.65ms
step:270/1920 train_time:9354ms step_avg:34.64ms
step:271/1920 train_time:9388ms step_avg:34.64ms
step:272/1920 train_time:9422ms step_avg:34.64ms
step:273/1920 train_time:9457ms step_avg:34.64ms
step:274/1920 train_time:9490ms step_avg:34.64ms
step:275/1920 train_time:9525ms step_avg:34.64ms
step:276/1920 train_time:9559ms step_avg:34.63ms
step:277/1920 train_time:9593ms step_avg:34.63ms
step:278/1920 train_time:9628ms step_avg:34.63ms
step:279/1920 train_time:9662ms step_avg:34.63ms
step:280/1920 train_time:9696ms step_avg:34.63ms
step:281/1920 train_time:9731ms step_avg:34.63ms
step:282/1920 train_time:9765ms step_avg:34.63ms
step:283/1920 train_time:9800ms step_avg:34.63ms
step:284/1920 train_time:9834ms step_avg:34.63ms
step:285/1920 train_time:9869ms step_avg:34.63ms
step:286/1920 train_time:9904ms step_avg:34.63ms
step:287/1920 train_time:9938ms step_avg:34.63ms
step:288/1920 train_time:9972ms step_avg:34.63ms
step:289/1920 train_time:10007ms step_avg:34.63ms
step:290/1920 train_time:10041ms step_avg:34.62ms
step:291/1920 train_time:10075ms step_avg:34.62ms
step:292/1920 train_time:10110ms step_avg:34.62ms
step:293/1920 train_time:10144ms step_avg:34.62ms
step:294/1920 train_time:10178ms step_avg:34.62ms
step:295/1920 train_time:10212ms step_avg:34.62ms
step:296/1920 train_time:10246ms step_avg:34.61ms
step:297/1920 train_time:10280ms step_avg:34.61ms
step:298/1920 train_time:10314ms step_avg:34.61ms
step:299/1920 train_time:10349ms step_avg:34.61ms
step:300/1920 train_time:10382ms step_avg:34.61ms
step:301/1920 train_time:10417ms step_avg:34.61ms
step:302/1920 train_time:10451ms step_avg:34.61ms
step:303/1920 train_time:10486ms step_avg:34.61ms
step:304/1920 train_time:10520ms step_avg:34.60ms
step:305/1920 train_time:10554ms step_avg:34.60ms
step:306/1920 train_time:10588ms step_avg:34.60ms
step:307/1920 train_time:10623ms step_avg:34.60ms
step:308/1920 train_time:10657ms step_avg:34.60ms
step:309/1920 train_time:10691ms step_avg:34.60ms
step:310/1920 train_time:10726ms step_avg:34.60ms
step:311/1920 train_time:10760ms step_avg:34.60ms
step:312/1920 train_time:10794ms step_avg:34.60ms
step:313/1920 train_time:10829ms step_avg:34.60ms
step:314/1920 train_time:10863ms step_avg:34.60ms
step:315/1920 train_time:10898ms step_avg:34.60ms
step:316/1920 train_time:10932ms step_avg:34.59ms
step:317/1920 train_time:10966ms step_avg:34.59ms
step:318/1920 train_time:11001ms step_avg:34.59ms
step:319/1920 train_time:11035ms step_avg:34.59ms
step:320/1920 train_time:11070ms step_avg:34.59ms
step:321/1920 train_time:11104ms step_avg:34.59ms
step:322/1920 train_time:11138ms step_avg:34.59ms
step:323/1920 train_time:11173ms step_avg:34.59ms
step:324/1920 train_time:11207ms step_avg:34.59ms
step:325/1920 train_time:11241ms step_avg:34.59ms
step:326/1920 train_time:11275ms step_avg:34.59ms
step:327/1920 train_time:11309ms step_avg:34.58ms
step:328/1920 train_time:11343ms step_avg:34.58ms
step:329/1920 train_time:11378ms step_avg:34.58ms
step:330/1920 train_time:11412ms step_avg:34.58ms
step:331/1920 train_time:11446ms step_avg:34.58ms
step:332/1920 train_time:11480ms step_avg:34.58ms
step:333/1920 train_time:11515ms step_avg:34.58ms
step:334/1920 train_time:11549ms step_avg:34.58ms
step:335/1920 train_time:11584ms step_avg:34.58ms
step:336/1920 train_time:11618ms step_avg:34.58ms
step:337/1920 train_time:11652ms step_avg:34.58ms
step:338/1920 train_time:11686ms step_avg:34.58ms
step:339/1920 train_time:11721ms step_avg:34.58ms
step:340/1920 train_time:11755ms step_avg:34.57ms
step:341/1920 train_time:11790ms step_avg:34.57ms
step:342/1920 train_time:11824ms step_avg:34.57ms
step:343/1920 train_time:11858ms step_avg:34.57ms
step:344/1920 train_time:11892ms step_avg:34.57ms
step:345/1920 train_time:11927ms step_avg:34.57ms
step:346/1920 train_time:11961ms step_avg:34.57ms
step:347/1920 train_time:11996ms step_avg:34.57ms
step:348/1920 train_time:12030ms step_avg:34.57ms
step:349/1920 train_time:12064ms step_avg:34.57ms
step:350/1920 train_time:12099ms step_avg:34.57ms
step:351/1920 train_time:12133ms step_avg:34.57ms
step:352/1920 train_time:12167ms step_avg:34.56ms
step:353/1920 train_time:12201ms step_avg:34.57ms
step:354/1920 train_time:12236ms step_avg:34.56ms
step:355/1920 train_time:12270ms step_avg:34.56ms
step:356/1920 train_time:12304ms step_avg:34.56ms
step:357/1920 train_time:12339ms step_avg:34.56ms
step:358/1920 train_time:12373ms step_avg:34.56ms
step:359/1920 train_time:12408ms step_avg:34.56ms
step:360/1920 train_time:12442ms step_avg:34.56ms
step:361/1920 train_time:12477ms step_avg:34.56ms
step:362/1920 train_time:12511ms step_avg:34.56ms
step:363/1920 train_time:12546ms step_avg:34.56ms
step:364/1920 train_time:12580ms step_avg:34.56ms
step:365/1920 train_time:12615ms step_avg:34.56ms
step:366/1920 train_time:12649ms step_avg:34.56ms
step:367/1920 train_time:12684ms step_avg:34.56ms
step:368/1920 train_time:12718ms step_avg:34.56ms
step:369/1920 train_time:12752ms step_avg:34.56ms
step:370/1920 train_time:12786ms step_avg:34.56ms
step:371/1920 train_time:12821ms step_avg:34.56ms
step:372/1920 train_time:12855ms step_avg:34.56ms
step:373/1920 train_time:12889ms step_avg:34.56ms
step:374/1920 train_time:12923ms step_avg:34.55ms
step:375/1920 train_time:12958ms step_avg:34.55ms
step:376/1920 train_time:12992ms step_avg:34.55ms
step:377/1920 train_time:13027ms step_avg:34.55ms
step:378/1920 train_time:13061ms step_avg:34.55ms
step:379/1920 train_time:13095ms step_avg:34.55ms
step:380/1920 train_time:13129ms step_avg:34.55ms
step:381/1920 train_time:13164ms step_avg:34.55ms
step:382/1920 train_time:13198ms step_avg:34.55ms
step:383/1920 train_time:13233ms step_avg:34.55ms
step:384/1920 train_time:13267ms step_avg:34.55ms
step:385/1920 train_time:13301ms step_avg:34.55ms
step:386/1920 train_time:13336ms step_avg:34.55ms
step:387/1920 train_time:13370ms step_avg:34.55ms
step:388/1920 train_time:13404ms step_avg:34.55ms
step:389/1920 train_time:13439ms step_avg:34.55ms
step:390/1920 train_time:13473ms step_avg:34.55ms
step:391/1920 train_time:13507ms step_avg:34.55ms
step:392/1920 train_time:13541ms step_avg:34.54ms
step:393/1920 train_time:13576ms step_avg:34.54ms
step:394/1920 train_time:13610ms step_avg:34.54ms
step:395/1920 train_time:13645ms step_avg:34.54ms
step:396/1920 train_time:13679ms step_avg:34.54ms
step:397/1920 train_time:13714ms step_avg:34.54ms
step:398/1920 train_time:13748ms step_avg:34.54ms
step:399/1920 train_time:13783ms step_avg:34.54ms
step:400/1920 train_time:13817ms step_avg:34.54ms
step:401/1920 train_time:13851ms step_avg:34.54ms
step:402/1920 train_time:13885ms step_avg:34.54ms
step:403/1920 train_time:13920ms step_avg:34.54ms
step:404/1920 train_time:13954ms step_avg:34.54ms
step:405/1920 train_time:13988ms step_avg:34.54ms
step:406/1920 train_time:14023ms step_avg:34.54ms
step:407/1920 train_time:14057ms step_avg:34.54ms
step:408/1920 train_time:14091ms step_avg:34.54ms
step:409/1920 train_time:14126ms step_avg:34.54ms
step:410/1920 train_time:14160ms step_avg:34.54ms
step:411/1920 train_time:14195ms step_avg:34.54ms
step:412/1920 train_time:14229ms step_avg:34.54ms
step:413/1920 train_time:14263ms step_avg:34.54ms
step:414/1920 train_time:14297ms step_avg:34.53ms
step:415/1920 train_time:14332ms step_avg:34.53ms
step:416/1920 train_time:14366ms step_avg:34.53ms
step:417/1920 train_time:14400ms step_avg:34.53ms
step:418/1920 train_time:14434ms step_avg:34.53ms
step:419/1920 train_time:14469ms step_avg:34.53ms
step:420/1920 train_time:14503ms step_avg:34.53ms
step:421/1920 train_time:14537ms step_avg:34.53ms
step:422/1920 train_time:14572ms step_avg:34.53ms
step:423/1920 train_time:14605ms step_avg:34.53ms
step:424/1920 train_time:14640ms step_avg:34.53ms
step:425/1920 train_time:14674ms step_avg:34.53ms
step:426/1920 train_time:14709ms step_avg:34.53ms
step:427/1920 train_time:14743ms step_avg:34.53ms
step:428/1920 train_time:14778ms step_avg:34.53ms
step:429/1920 train_time:14812ms step_avg:34.53ms
step:430/1920 train_time:14846ms step_avg:34.53ms
step:431/1920 train_time:14881ms step_avg:34.53ms
step:432/1920 train_time:14915ms step_avg:34.52ms
step:433/1920 train_time:14949ms step_avg:34.52ms
step:434/1920 train_time:14983ms step_avg:34.52ms
step:435/1920 train_time:15018ms step_avg:34.52ms
step:436/1920 train_time:15052ms step_avg:34.52ms
step:437/1920 train_time:15086ms step_avg:34.52ms
step:438/1920 train_time:15120ms step_avg:34.52ms
step:439/1920 train_time:15155ms step_avg:34.52ms
step:440/1920 train_time:15189ms step_avg:34.52ms
step:441/1920 train_time:15224ms step_avg:34.52ms
step:442/1920 train_time:15258ms step_avg:34.52ms
step:443/1920 train_time:15292ms step_avg:34.52ms
step:444/1920 train_time:15326ms step_avg:34.52ms
step:445/1920 train_time:15361ms step_avg:34.52ms
step:446/1920 train_time:15395ms step_avg:34.52ms
step:447/1920 train_time:15429ms step_avg:34.52ms
step:448/1920 train_time:15463ms step_avg:34.52ms
step:449/1920 train_time:15498ms step_avg:34.52ms
step:450/1920 train_time:15532ms step_avg:34.52ms
step:451/1920 train_time:15566ms step_avg:34.52ms
step:452/1920 train_time:15601ms step_avg:34.51ms
step:453/1920 train_time:15636ms step_avg:34.52ms
step:454/1920 train_time:15670ms step_avg:34.51ms
step:455/1920 train_time:15704ms step_avg:34.51ms
step:456/1920 train_time:15739ms step_avg:34.51ms
step:457/1920 train_time:15773ms step_avg:34.52ms
step:458/1920 train_time:15808ms step_avg:34.51ms
step:459/1920 train_time:15842ms step_avg:34.51ms
step:460/1920 train_time:15876ms step_avg:34.51ms
step:461/1920 train_time:15911ms step_avg:34.51ms
step:462/1920 train_time:15945ms step_avg:34.51ms
step:463/1920 train_time:15980ms step_avg:34.51ms
step:464/1920 train_time:16014ms step_avg:34.51ms
step:465/1920 train_time:16048ms step_avg:34.51ms
step:466/1920 train_time:16083ms step_avg:34.51ms
step:467/1920 train_time:16117ms step_avg:34.51ms
step:468/1920 train_time:16151ms step_avg:34.51ms
step:469/1920 train_time:16185ms step_avg:34.51ms
step:470/1920 train_time:16219ms step_avg:34.51ms
step:471/1920 train_time:16253ms step_avg:34.51ms
step:472/1920 train_time:16288ms step_avg:34.51ms
step:473/1920 train_time:16322ms step_avg:34.51ms
step:474/1920 train_time:16357ms step_avg:34.51ms
step:475/1920 train_time:16391ms step_avg:34.51ms
step:476/1920 train_time:16425ms step_avg:34.51ms
step:477/1920 train_time:16459ms step_avg:34.51ms
step:478/1920 train_time:16493ms step_avg:34.51ms
step:479/1920 train_time:16528ms step_avg:34.50ms
step:480/1920 train_time:16562ms step_avg:34.50ms
step:481/1920 train_time:16597ms step_avg:34.50ms
step:482/1920 train_time:16631ms step_avg:34.50ms
step:483/1920 train_time:16665ms step_avg:34.50ms
step:484/1920 train_time:16700ms step_avg:34.50ms
step:485/1920 train_time:16735ms step_avg:34.50ms
step:486/1920 train_time:16769ms step_avg:34.50ms
step:487/1920 train_time:16803ms step_avg:34.50ms
step:488/1920 train_time:16838ms step_avg:34.50ms
step:489/1920 train_time:16872ms step_avg:34.50ms
step:490/1920 train_time:16906ms step_avg:34.50ms
step:491/1920 train_time:16940ms step_avg:34.50ms
step:492/1920 train_time:16974ms step_avg:34.50ms
step:493/1920 train_time:17009ms step_avg:34.50ms
step:494/1920 train_time:17043ms step_avg:34.50ms
step:495/1920 train_time:17078ms step_avg:34.50ms
step:496/1920 train_time:17112ms step_avg:34.50ms
step:497/1920 train_time:17146ms step_avg:34.50ms
step:498/1920 train_time:17180ms step_avg:34.50ms
step:499/1920 train_time:17215ms step_avg:34.50ms
step:500/1920 train_time:17249ms step_avg:34.50ms
step:500/1920 val_loss:4.2881 train_time:17286ms step_avg:34.57ms
step:501/1920 train_time:17304ms step_avg:34.54ms
step:502/1920 train_time:17322ms step_avg:34.51ms
step:503/1920 train_time:17355ms step_avg:34.50ms
step:504/1920 train_time:17389ms step_avg:34.50ms
step:505/1920 train_time:17426ms step_avg:34.51ms
step:506/1920 train_time:17460ms step_avg:34.51ms
step:507/1920 train_time:17495ms step_avg:34.51ms
step:508/1920 train_time:17529ms step_avg:34.51ms
step:509/1920 train_time:17564ms step_avg:34.51ms
step:510/1920 train_time:17598ms step_avg:34.51ms
step:511/1920 train_time:17633ms step_avg:34.51ms
step:512/1920 train_time:17667ms step_avg:34.51ms
step:513/1920 train_time:17701ms step_avg:34.50ms
step:514/1920 train_time:17735ms step_avg:34.50ms
step:515/1920 train_time:17769ms step_avg:34.50ms
step:516/1920 train_time:17803ms step_avg:34.50ms
step:517/1920 train_time:17838ms step_avg:34.50ms
step:518/1920 train_time:17872ms step_avg:34.50ms
step:519/1920 train_time:17906ms step_avg:34.50ms
step:520/1920 train_time:17940ms step_avg:34.50ms
step:521/1920 train_time:17974ms step_avg:34.50ms
step:522/1920 train_time:18008ms step_avg:34.50ms
step:523/1920 train_time:18042ms step_avg:34.50ms
step:524/1920 train_time:18076ms step_avg:34.50ms
step:525/1920 train_time:18111ms step_avg:34.50ms
step:526/1920 train_time:18145ms step_avg:34.50ms
step:527/1920 train_time:18179ms step_avg:34.50ms
step:528/1920 train_time:18213ms step_avg:34.49ms
step:529/1920 train_time:18247ms step_avg:34.49ms
step:530/1920 train_time:18282ms step_avg:34.49ms
step:531/1920 train_time:18316ms step_avg:34.49ms
step:532/1920 train_time:18351ms step_avg:34.49ms
step:533/1920 train_time:18386ms step_avg:34.49ms
step:534/1920 train_time:18420ms step_avg:34.49ms
step:535/1920 train_time:18454ms step_avg:34.49ms
step:536/1920 train_time:18489ms step_avg:34.49ms
step:537/1920 train_time:18523ms step_avg:34.49ms
step:538/1920 train_time:18557ms step_avg:34.49ms
step:539/1920 train_time:18592ms step_avg:34.49ms
step:540/1920 train_time:18626ms step_avg:34.49ms
step:541/1920 train_time:18660ms step_avg:34.49ms
step:542/1920 train_time:18694ms step_avg:34.49ms
step:543/1920 train_time:18729ms step_avg:34.49ms
step:544/1920 train_time:18764ms step_avg:34.49ms
step:545/1920 train_time:18798ms step_avg:34.49ms
step:546/1920 train_time:18832ms step_avg:34.49ms
step:547/1920 train_time:18866ms step_avg:34.49ms
step:548/1920 train_time:18900ms step_avg:34.49ms
step:549/1920 train_time:18935ms step_avg:34.49ms
step:550/1920 train_time:18969ms step_avg:34.49ms
step:551/1920 train_time:19003ms step_avg:34.49ms
step:552/1920 train_time:19037ms step_avg:34.49ms
step:553/1920 train_time:19071ms step_avg:34.49ms
step:554/1920 train_time:19105ms step_avg:34.49ms
step:555/1920 train_time:19139ms step_avg:34.49ms
step:556/1920 train_time:19173ms step_avg:34.48ms
step:557/1920 train_time:19208ms step_avg:34.48ms
step:558/1920 train_time:19242ms step_avg:34.48ms
step:559/1920 train_time:19277ms step_avg:34.48ms
step:560/1920 train_time:19311ms step_avg:34.48ms
step:561/1920 train_time:19346ms step_avg:34.48ms
step:562/1920 train_time:19380ms step_avg:34.48ms
step:563/1920 train_time:19414ms step_avg:34.48ms
step:564/1920 train_time:19449ms step_avg:34.48ms
step:565/1920 train_time:19483ms step_avg:34.48ms
step:566/1920 train_time:19518ms step_avg:34.48ms
step:567/1920 train_time:19552ms step_avg:34.48ms
step:568/1920 train_time:19586ms step_avg:34.48ms
step:569/1920 train_time:19621ms step_avg:34.48ms
step:570/1920 train_time:19655ms step_avg:34.48ms
step:571/1920 train_time:19690ms step_avg:34.48ms
step:572/1920 train_time:19724ms step_avg:34.48ms
step:573/1920 train_time:19759ms step_avg:34.48ms
step:574/1920 train_time:19793ms step_avg:34.48ms
step:575/1920 train_time:19828ms step_avg:34.48ms
step:576/1920 train_time:19862ms step_avg:34.48ms
step:577/1920 train_time:19896ms step_avg:34.48ms
step:578/1920 train_time:19930ms step_avg:34.48ms
step:579/1920 train_time:19965ms step_avg:34.48ms
step:580/1920 train_time:19999ms step_avg:34.48ms
step:581/1920 train_time:20034ms step_avg:34.48ms
step:582/1920 train_time:20068ms step_avg:34.48ms
step:583/1920 train_time:20102ms step_avg:34.48ms
step:584/1920 train_time:20136ms step_avg:34.48ms
step:585/1920 train_time:20171ms step_avg:34.48ms
step:586/1920 train_time:20205ms step_avg:34.48ms
step:587/1920 train_time:20239ms step_avg:34.48ms
step:588/1920 train_time:20274ms step_avg:34.48ms
step:589/1920 train_time:20308ms step_avg:34.48ms
step:590/1920 train_time:20342ms step_avg:34.48ms
step:591/1920 train_time:20377ms step_avg:34.48ms
step:592/1920 train_time:20412ms step_avg:34.48ms
step:593/1920 train_time:20446ms step_avg:34.48ms
step:594/1920 train_time:20480ms step_avg:34.48ms
step:595/1920 train_time:20515ms step_avg:34.48ms
step:596/1920 train_time:20549ms step_avg:34.48ms
step:597/1920 train_time:20584ms step_avg:34.48ms
step:598/1920 train_time:20618ms step_avg:34.48ms
step:599/1920 train_time:20653ms step_avg:34.48ms
step:600/1920 train_time:20687ms step_avg:34.48ms
step:601/1920 train_time:20721ms step_avg:34.48ms
step:602/1920 train_time:20756ms step_avg:34.48ms
step:603/1920 train_time:20790ms step_avg:34.48ms
step:604/1920 train_time:20825ms step_avg:34.48ms
step:605/1920 train_time:20859ms step_avg:34.48ms
step:606/1920 train_time:20893ms step_avg:34.48ms
step:607/1920 train_time:20927ms step_avg:34.48ms
step:608/1920 train_time:20961ms step_avg:34.48ms
step:609/1920 train_time:20996ms step_avg:34.48ms
step:610/1920 train_time:21030ms step_avg:34.48ms
step:611/1920 train_time:21065ms step_avg:34.48ms
step:612/1920 train_time:21099ms step_avg:34.47ms
step:613/1920 train_time:21133ms step_avg:34.48ms
step:614/1920 train_time:21167ms step_avg:34.47ms
step:615/1920 train_time:21202ms step_avg:34.47ms
step:616/1920 train_time:21236ms step_avg:34.47ms
step:617/1920 train_time:21270ms step_avg:34.47ms
step:618/1920 train_time:21304ms step_avg:34.47ms
step:619/1920 train_time:21339ms step_avg:34.47ms
step:620/1920 train_time:21373ms step_avg:34.47ms
step:621/1920 train_time:21407ms step_avg:34.47ms
step:622/1920 train_time:21441ms step_avg:34.47ms
step:623/1920 train_time:21476ms step_avg:34.47ms
step:624/1920 train_time:21510ms step_avg:34.47ms
step:625/1920 train_time:21544ms step_avg:34.47ms
step:626/1920 train_time:21579ms step_avg:34.47ms
step:627/1920 train_time:21613ms step_avg:34.47ms
step:628/1920 train_time:21648ms step_avg:34.47ms
step:629/1920 train_time:21709ms step_avg:34.51ms
step:630/1920 train_time:21771ms step_avg:34.56ms
step:631/1920 train_time:21833ms step_avg:34.60ms
step:632/1920 train_time:21895ms step_avg:34.64ms
step:633/1920 train_time:21958ms step_avg:34.69ms
step:634/1920 train_time:22020ms step_avg:34.73ms
step:635/1920 train_time:22084ms step_avg:34.78ms
step:636/1920 train_time:22145ms step_avg:34.82ms
step:637/1920 train_time:22208ms step_avg:34.86ms
step:638/1920 train_time:22269ms step_avg:34.90ms
step:639/1920 train_time:22332ms step_avg:34.95ms
step:640/1920 train_time:22393ms step_avg:34.99ms
step:641/1920 train_time:22456ms step_avg:35.03ms
step:642/1920 train_time:22518ms step_avg:35.07ms
step:643/1920 train_time:22580ms step_avg:35.12ms
step:644/1920 train_time:22642ms step_avg:35.16ms
step:645/1920 train_time:22705ms step_avg:35.20ms
step:646/1920 train_time:22766ms step_avg:35.24ms
step:647/1920 train_time:22829ms step_avg:35.28ms
step:648/1920 train_time:22890ms step_avg:35.32ms
step:649/1920 train_time:22952ms step_avg:35.37ms
step:650/1920 train_time:23014ms step_avg:35.41ms
step:651/1920 train_time:23076ms step_avg:35.45ms
step:652/1920 train_time:23139ms step_avg:35.49ms
step:653/1920 train_time:23202ms step_avg:35.53ms
step:654/1920 train_time:23264ms step_avg:35.57ms
step:655/1920 train_time:23326ms step_avg:35.61ms
step:656/1920 train_time:23387ms step_avg:35.65ms
step:657/1920 train_time:23451ms step_avg:35.69ms
step:658/1920 train_time:23512ms step_avg:35.73ms
step:659/1920 train_time:23575ms step_avg:35.77ms
step:660/1920 train_time:23636ms step_avg:35.81ms
step:661/1920 train_time:23699ms step_avg:35.85ms
step:662/1920 train_time:23761ms step_avg:35.89ms
step:663/1920 train_time:23825ms step_avg:35.93ms
step:664/1920 train_time:23886ms step_avg:35.97ms
step:665/1920 train_time:23949ms step_avg:36.01ms
step:666/1920 train_time:24011ms step_avg:36.05ms
step:667/1920 train_time:24073ms step_avg:36.09ms
step:668/1920 train_time:24135ms step_avg:36.13ms
step:669/1920 train_time:24198ms step_avg:36.17ms
step:670/1920 train_time:24261ms step_avg:36.21ms
step:671/1920 train_time:24324ms step_avg:36.25ms
step:672/1920 train_time:24385ms step_avg:36.29ms
step:673/1920 train_time:24448ms step_avg:36.33ms
step:674/1920 train_time:24510ms step_avg:36.36ms
step:675/1920 train_time:24572ms step_avg:36.40ms
step:676/1920 train_time:24634ms step_avg:36.44ms
step:677/1920 train_time:24697ms step_avg:36.48ms
step:678/1920 train_time:24759ms step_avg:36.52ms
step:679/1920 train_time:24822ms step_avg:36.56ms
step:680/1920 train_time:24883ms step_avg:36.59ms
step:681/1920 train_time:24946ms step_avg:36.63ms
step:682/1920 train_time:25008ms step_avg:36.67ms
step:683/1920 train_time:25071ms step_avg:36.71ms
step:684/1920 train_time:25132ms step_avg:36.74ms
step:685/1920 train_time:25194ms step_avg:36.78ms
step:686/1920 train_time:25256ms step_avg:36.82ms
step:687/1920 train_time:25319ms step_avg:36.85ms
step:688/1920 train_time:25381ms step_avg:36.89ms
step:689/1920 train_time:25444ms step_avg:36.93ms
step:690/1920 train_time:25506ms step_avg:36.97ms
step:691/1920 train_time:25569ms step_avg:37.00ms
step:692/1920 train_time:25630ms step_avg:37.04ms
step:693/1920 train_time:25692ms step_avg:37.07ms
step:694/1920 train_time:25754ms step_avg:37.11ms
step:695/1920 train_time:25817ms step_avg:37.15ms
step:696/1920 train_time:25879ms step_avg:37.18ms
step:697/1920 train_time:25942ms step_avg:37.22ms
step:698/1920 train_time:26004ms step_avg:37.26ms
step:699/1920 train_time:26067ms step_avg:37.29ms
step:700/1920 train_time:26129ms step_avg:37.33ms
step:701/1920 train_time:26191ms step_avg:37.36ms
step:702/1920 train_time:26253ms step_avg:37.40ms
step:703/1920 train_time:26315ms step_avg:37.43ms
step:704/1920 train_time:26377ms step_avg:37.47ms
step:705/1920 train_time:26440ms step_avg:37.50ms
step:706/1920 train_time:26502ms step_avg:37.54ms
step:707/1920 train_time:26565ms step_avg:37.57ms
step:708/1920 train_time:26628ms step_avg:37.61ms
step:709/1920 train_time:26691ms step_avg:37.65ms
step:710/1920 train_time:26752ms step_avg:37.68ms
step:711/1920 train_time:26814ms step_avg:37.71ms
step:712/1920 train_time:26876ms step_avg:37.75ms
step:713/1920 train_time:26939ms step_avg:37.78ms
step:714/1920 train_time:27001ms step_avg:37.82ms
step:715/1920 train_time:27065ms step_avg:37.85ms
step:716/1920 train_time:27126ms step_avg:37.89ms
step:717/1920 train_time:27189ms step_avg:37.92ms
step:718/1920 train_time:27251ms step_avg:37.95ms
step:719/1920 train_time:27313ms step_avg:37.99ms
step:720/1920 train_time:27375ms step_avg:38.02ms
step:721/1920 train_time:27438ms step_avg:38.06ms
step:722/1920 train_time:27500ms step_avg:38.09ms
step:723/1920 train_time:27564ms step_avg:38.12ms
step:724/1920 train_time:27626ms step_avg:38.16ms
step:725/1920 train_time:27689ms step_avg:38.19ms
step:726/1920 train_time:27751ms step_avg:38.22ms
step:727/1920 train_time:27813ms step_avg:38.26ms
step:728/1920 train_time:27875ms step_avg:38.29ms
step:729/1920 train_time:27937ms step_avg:38.32ms
step:730/1920 train_time:27999ms step_avg:38.36ms
step:731/1920 train_time:28062ms step_avg:38.39ms
step:732/1920 train_time:28124ms step_avg:38.42ms
step:733/1920 train_time:28188ms step_avg:38.46ms
step:734/1920 train_time:28249ms step_avg:38.49ms
step:735/1920 train_time:28312ms step_avg:38.52ms
step:736/1920 train_time:28373ms step_avg:38.55ms
step:737/1920 train_time:28436ms step_avg:38.58ms
step:738/1920 train_time:28498ms step_avg:38.62ms
step:739/1920 train_time:28561ms step_avg:38.65ms
step:740/1920 train_time:28624ms step_avg:38.68ms
step:741/1920 train_time:28686ms step_avg:38.71ms
step:742/1920 train_time:28748ms step_avg:38.74ms
step:743/1920 train_time:28811ms step_avg:38.78ms
step:744/1920 train_time:28872ms step_avg:38.81ms
step:745/1920 train_time:28935ms step_avg:38.84ms
step:746/1920 train_time:28997ms step_avg:38.87ms
step:747/1920 train_time:29060ms step_avg:38.90ms
step:748/1920 train_time:29122ms step_avg:38.93ms
step:749/1920 train_time:29184ms step_avg:38.96ms
step:750/1920 train_time:29246ms step_avg:39.00ms
step:750/1920 val_loss:4.0319 train_time:29311ms step_avg:39.08ms
step:751/1920 train_time:29330ms step_avg:39.05ms
step:752/1920 train_time:29372ms step_avg:39.06ms
step:753/1920 train_time:29441ms step_avg:39.10ms
step:754/1920 train_time:29508ms step_avg:39.13ms
step:755/1920 train_time:29572ms step_avg:39.17ms
step:756/1920 train_time:29633ms step_avg:39.20ms
step:757/1920 train_time:29695ms step_avg:39.23ms
step:758/1920 train_time:29756ms step_avg:39.26ms
step:759/1920 train_time:29818ms step_avg:39.29ms
step:760/1920 train_time:29879ms step_avg:39.31ms
step:761/1920 train_time:29941ms step_avg:39.34ms
step:762/1920 train_time:30002ms step_avg:39.37ms
step:763/1920 train_time:30064ms step_avg:39.40ms
step:764/1920 train_time:30125ms step_avg:39.43ms
step:765/1920 train_time:30187ms step_avg:39.46ms
step:766/1920 train_time:30249ms step_avg:39.49ms
step:767/1920 train_time:30312ms step_avg:39.52ms
step:768/1920 train_time:30375ms step_avg:39.55ms
step:769/1920 train_time:30440ms step_avg:39.58ms
step:770/1920 train_time:30504ms step_avg:39.62ms
step:771/1920 train_time:30568ms step_avg:39.65ms
step:772/1920 train_time:30631ms step_avg:39.68ms
step:773/1920 train_time:30695ms step_avg:39.71ms
step:774/1920 train_time:30756ms step_avg:39.74ms
step:775/1920 train_time:30818ms step_avg:39.77ms
step:776/1920 train_time:30879ms step_avg:39.79ms
step:777/1920 train_time:30940ms step_avg:39.82ms
step:778/1920 train_time:31001ms step_avg:39.85ms
step:779/1920 train_time:31064ms step_avg:39.88ms
step:780/1920 train_time:31125ms step_avg:39.90ms
step:781/1920 train_time:31188ms step_avg:39.93ms
step:782/1920 train_time:31250ms step_avg:39.96ms
step:783/1920 train_time:31313ms step_avg:39.99ms
step:784/1920 train_time:31375ms step_avg:40.02ms
step:785/1920 train_time:31439ms step_avg:40.05ms
step:786/1920 train_time:31502ms step_avg:40.08ms
step:787/1920 train_time:31566ms step_avg:40.11ms
step:788/1920 train_time:31629ms step_avg:40.14ms
step:789/1920 train_time:31693ms step_avg:40.17ms
step:790/1920 train_time:31755ms step_avg:40.20ms
step:791/1920 train_time:31817ms step_avg:40.22ms
step:792/1920 train_time:31878ms step_avg:40.25ms
step:793/1920 train_time:31940ms step_avg:40.28ms
step:794/1920 train_time:32001ms step_avg:40.30ms
step:795/1920 train_time:32063ms step_avg:40.33ms
step:796/1920 train_time:32124ms step_avg:40.36ms
step:797/1920 train_time:32187ms step_avg:40.39ms
step:798/1920 train_time:32249ms step_avg:40.41ms
step:799/1920 train_time:32311ms step_avg:40.44ms
step:800/1920 train_time:32373ms step_avg:40.47ms
step:801/1920 train_time:32436ms step_avg:40.49ms
step:802/1920 train_time:32499ms step_avg:40.52ms
step:803/1920 train_time:32562ms step_avg:40.55ms
step:804/1920 train_time:32625ms step_avg:40.58ms
step:805/1920 train_time:32688ms step_avg:40.61ms
step:806/1920 train_time:32750ms step_avg:40.63ms
step:807/1920 train_time:32813ms step_avg:40.66ms
step:808/1920 train_time:32875ms step_avg:40.69ms
step:809/1920 train_time:32937ms step_avg:40.71ms
step:810/1920 train_time:32998ms step_avg:40.74ms
step:811/1920 train_time:33060ms step_avg:40.76ms
step:812/1920 train_time:33121ms step_avg:40.79ms
step:813/1920 train_time:33183ms step_avg:40.82ms
step:814/1920 train_time:33245ms step_avg:40.84ms
step:815/1920 train_time:33308ms step_avg:40.87ms
step:816/1920 train_time:33370ms step_avg:40.89ms
step:817/1920 train_time:33433ms step_avg:40.92ms
step:818/1920 train_time:33495ms step_avg:40.95ms
step:819/1920 train_time:33558ms step_avg:40.97ms
step:820/1920 train_time:33620ms step_avg:41.00ms
step:821/1920 train_time:33683ms step_avg:41.03ms
step:822/1920 train_time:33746ms step_avg:41.05ms
step:823/1920 train_time:33809ms step_avg:41.08ms
step:824/1920 train_time:33871ms step_avg:41.11ms
step:825/1920 train_time:33934ms step_avg:41.13ms
step:826/1920 train_time:33995ms step_avg:41.16ms
step:827/1920 train_time:34057ms step_avg:41.18ms
step:828/1920 train_time:34118ms step_avg:41.21ms
step:829/1920 train_time:34180ms step_avg:41.23ms
step:830/1920 train_time:34242ms step_avg:41.26ms
step:831/1920 train_time:34304ms step_avg:41.28ms
step:832/1920 train_time:34366ms step_avg:41.31ms
step:833/1920 train_time:34429ms step_avg:41.33ms
step:834/1920 train_time:34492ms step_avg:41.36ms
step:835/1920 train_time:34555ms step_avg:41.38ms
step:836/1920 train_time:34616ms step_avg:41.41ms
step:837/1920 train_time:34679ms step_avg:41.43ms
step:838/1920 train_time:34741ms step_avg:41.46ms
step:839/1920 train_time:34804ms step_avg:41.48ms
step:840/1920 train_time:34867ms step_avg:41.51ms
step:841/1920 train_time:34929ms step_avg:41.53ms
step:842/1920 train_time:34991ms step_avg:41.56ms
step:843/1920 train_time:35054ms step_avg:41.58ms
step:844/1920 train_time:35116ms step_avg:41.61ms
step:845/1920 train_time:35178ms step_avg:41.63ms
step:846/1920 train_time:35240ms step_avg:41.65ms
step:847/1920 train_time:35302ms step_avg:41.68ms
step:848/1920 train_time:35363ms step_avg:41.70ms
step:849/1920 train_time:35426ms step_avg:41.73ms
step:850/1920 train_time:35488ms step_avg:41.75ms
step:851/1920 train_time:35552ms step_avg:41.78ms
step:852/1920 train_time:35614ms step_avg:41.80ms
step:853/1920 train_time:35677ms step_avg:41.82ms
step:854/1920 train_time:35738ms step_avg:41.85ms
step:855/1920 train_time:35801ms step_avg:41.87ms
step:856/1920 train_time:35863ms step_avg:41.90ms
step:857/1920 train_time:35926ms step_avg:41.92ms
step:858/1920 train_time:35988ms step_avg:41.94ms
step:859/1920 train_time:36051ms step_avg:41.97ms
step:860/1920 train_time:36113ms step_avg:41.99ms
step:861/1920 train_time:36176ms step_avg:42.02ms
step:862/1920 train_time:36237ms step_avg:42.04ms
step:863/1920 train_time:36299ms step_avg:42.06ms
step:864/1920 train_time:36361ms step_avg:42.08ms
step:865/1920 train_time:36423ms step_avg:42.11ms
step:866/1920 train_time:36485ms step_avg:42.13ms
step:867/1920 train_time:36548ms step_avg:42.15ms
step:868/1920 train_time:36610ms step_avg:42.18ms
step:869/1920 train_time:36673ms step_avg:42.20ms
step:870/1920 train_time:36735ms step_avg:42.22ms
step:871/1920 train_time:36797ms step_avg:42.25ms
step:872/1920 train_time:36860ms step_avg:42.27ms
step:873/1920 train_time:36922ms step_avg:42.29ms
step:874/1920 train_time:36984ms step_avg:42.32ms
step:875/1920 train_time:37047ms step_avg:42.34ms
step:876/1920 train_time:37110ms step_avg:42.36ms
step:877/1920 train_time:37173ms step_avg:42.39ms
step:878/1920 train_time:37235ms step_avg:42.41ms
step:879/1920 train_time:37297ms step_avg:42.43ms
step:880/1920 train_time:37358ms step_avg:42.45ms
step:881/1920 train_time:37421ms step_avg:42.48ms
step:882/1920 train_time:37483ms step_avg:42.50ms
step:883/1920 train_time:37546ms step_avg:42.52ms
step:884/1920 train_time:37608ms step_avg:42.54ms
step:885/1920 train_time:37671ms step_avg:42.57ms
step:886/1920 train_time:37733ms step_avg:42.59ms
step:887/1920 train_time:37797ms step_avg:42.61ms
step:888/1920 train_time:37858ms step_avg:42.63ms
step:889/1920 train_time:37921ms step_avg:42.66ms
step:890/1920 train_time:37983ms step_avg:42.68ms
step:891/1920 train_time:38046ms step_avg:42.70ms
step:892/1920 train_time:38108ms step_avg:42.72ms
step:893/1920 train_time:38172ms step_avg:42.75ms
step:894/1920 train_time:38234ms step_avg:42.77ms
step:895/1920 train_time:38296ms step_avg:42.79ms
step:896/1920 train_time:38358ms step_avg:42.81ms
step:897/1920 train_time:38420ms step_avg:42.83ms
step:898/1920 train_time:38482ms step_avg:42.85ms
step:899/1920 train_time:38544ms step_avg:42.87ms
step:900/1920 train_time:38606ms step_avg:42.90ms
step:901/1920 train_time:38670ms step_avg:42.92ms
step:902/1920 train_time:38732ms step_avg:42.94ms
step:903/1920 train_time:38795ms step_avg:42.96ms
step:904/1920 train_time:38856ms step_avg:42.98ms
step:905/1920 train_time:38919ms step_avg:43.00ms
step:906/1920 train_time:38981ms step_avg:43.03ms
step:907/1920 train_time:39044ms step_avg:43.05ms
step:908/1920 train_time:39106ms step_avg:43.07ms
step:909/1920 train_time:39170ms step_avg:43.09ms
step:910/1920 train_time:39232ms step_avg:43.11ms
step:911/1920 train_time:39295ms step_avg:43.13ms
step:912/1920 train_time:39356ms step_avg:43.15ms
step:913/1920 train_time:39419ms step_avg:43.17ms
step:914/1920 train_time:39480ms step_avg:43.20ms
step:915/1920 train_time:39543ms step_avg:43.22ms
step:916/1920 train_time:39605ms step_avg:43.24ms
step:917/1920 train_time:39668ms step_avg:43.26ms
step:918/1920 train_time:39730ms step_avg:43.28ms
step:919/1920 train_time:39793ms step_avg:43.30ms
step:920/1920 train_time:39854ms step_avg:43.32ms
step:921/1920 train_time:39917ms step_avg:43.34ms
step:922/1920 train_time:39979ms step_avg:43.36ms
step:923/1920 train_time:40041ms step_avg:43.38ms
step:924/1920 train_time:40103ms step_avg:43.40ms
step:925/1920 train_time:40166ms step_avg:43.42ms
step:926/1920 train_time:40230ms step_avg:43.44ms
step:927/1920 train_time:40293ms step_avg:43.47ms
step:928/1920 train_time:40354ms step_avg:43.49ms
step:929/1920 train_time:40417ms step_avg:43.51ms
step:930/1920 train_time:40479ms step_avg:43.53ms
step:931/1920 train_time:40541ms step_avg:43.55ms
step:932/1920 train_time:40603ms step_avg:43.57ms
step:933/1920 train_time:40666ms step_avg:43.59ms
step:934/1920 train_time:40729ms step_avg:43.61ms
step:935/1920 train_time:40793ms step_avg:43.63ms
step:936/1920 train_time:40854ms step_avg:43.65ms
step:937/1920 train_time:40917ms step_avg:43.67ms
step:938/1920 train_time:40979ms step_avg:43.69ms
step:939/1920 train_time:41041ms step_avg:43.71ms
step:940/1920 train_time:41103ms step_avg:43.73ms
step:941/1920 train_time:41166ms step_avg:43.75ms
step:942/1920 train_time:41228ms step_avg:43.77ms
step:943/1920 train_time:41292ms step_avg:43.79ms
step:944/1920 train_time:41354ms step_avg:43.81ms
step:945/1920 train_time:41416ms step_avg:43.83ms
step:946/1920 train_time:41477ms step_avg:43.84ms
step:947/1920 train_time:41539ms step_avg:43.86ms
step:948/1920 train_time:41601ms step_avg:43.88ms
step:949/1920 train_time:41664ms step_avg:43.90ms
step:950/1920 train_time:41726ms step_avg:43.92ms
step:951/1920 train_time:41790ms step_avg:43.94ms
step:952/1920 train_time:41852ms step_avg:43.96ms
step:953/1920 train_time:41916ms step_avg:43.98ms
step:954/1920 train_time:41977ms step_avg:44.00ms
step:955/1920 train_time:42040ms step_avg:44.02ms
step:956/1920 train_time:42102ms step_avg:44.04ms
step:957/1920 train_time:42165ms step_avg:44.06ms
step:958/1920 train_time:42227ms step_avg:44.08ms
step:959/1920 train_time:42291ms step_avg:44.10ms
step:960/1920 train_time:42353ms step_avg:44.12ms
step:961/1920 train_time:42415ms step_avg:44.14ms
step:962/1920 train_time:42477ms step_avg:44.16ms
step:963/1920 train_time:42540ms step_avg:44.17ms
step:964/1920 train_time:42601ms step_avg:44.19ms
step:965/1920 train_time:42664ms step_avg:44.21ms
step:966/1920 train_time:42726ms step_avg:44.23ms
step:967/1920 train_time:42790ms step_avg:44.25ms
step:968/1920 train_time:42852ms step_avg:44.27ms
step:969/1920 train_time:42915ms step_avg:44.29ms
step:970/1920 train_time:42976ms step_avg:44.31ms
step:971/1920 train_time:43039ms step_avg:44.32ms
step:972/1920 train_time:43101ms step_avg:44.34ms
step:973/1920 train_time:43164ms step_avg:44.36ms
step:974/1920 train_time:43226ms step_avg:44.38ms
step:975/1920 train_time:43289ms step_avg:44.40ms
step:976/1920 train_time:43351ms step_avg:44.42ms
step:977/1920 train_time:43414ms step_avg:44.44ms
step:978/1920 train_time:43476ms step_avg:44.45ms
step:979/1920 train_time:43539ms step_avg:44.47ms
step:980/1920 train_time:43600ms step_avg:44.49ms
step:981/1920 train_time:43663ms step_avg:44.51ms
step:982/1920 train_time:43725ms step_avg:44.53ms
step:983/1920 train_time:43788ms step_avg:44.55ms
step:984/1920 train_time:43850ms step_avg:44.56ms
step:985/1920 train_time:43914ms step_avg:44.58ms
step:986/1920 train_time:43975ms step_avg:44.60ms
step:987/1920 train_time:44038ms step_avg:44.62ms
step:988/1920 train_time:44099ms step_avg:44.63ms
step:989/1920 train_time:44162ms step_avg:44.65ms
step:990/1920 train_time:44224ms step_avg:44.67ms
step:991/1920 train_time:44287ms step_avg:44.69ms
step:992/1920 train_time:44349ms step_avg:44.71ms
step:993/1920 train_time:44412ms step_avg:44.73ms
step:994/1920 train_time:44474ms step_avg:44.74ms
step:995/1920 train_time:44536ms step_avg:44.76ms
step:996/1920 train_time:44598ms step_avg:44.78ms
step:997/1920 train_time:44660ms step_avg:44.79ms
step:998/1920 train_time:44722ms step_avg:44.81ms
step:999/1920 train_time:44785ms step_avg:44.83ms
step:1000/1920 train_time:44847ms step_avg:44.85ms
step:1000/1920 val_loss:3.7810 train_time:44912ms step_avg:44.91ms
step:1001/1920 train_time:44930ms step_avg:44.89ms
step:1002/1920 train_time:44973ms step_avg:44.88ms
step:1003/1920 train_time:45038ms step_avg:44.90ms
step:1004/1920 train_time:45100ms step_avg:44.92ms
step:1005/1920 train_time:45162ms step_avg:44.94ms
step:1006/1920 train_time:45223ms step_avg:44.95ms
step:1007/1920 train_time:45286ms step_avg:44.97ms
step:1008/1920 train_time:45347ms step_avg:44.99ms
step:1009/1920 train_time:45411ms step_avg:45.01ms
step:1010/1920 train_time:45472ms step_avg:45.02ms
step:1011/1920 train_time:45535ms step_avg:45.04ms
step:1012/1920 train_time:45596ms step_avg:45.06ms
step:1013/1920 train_time:45658ms step_avg:45.07ms
step:1014/1920 train_time:45720ms step_avg:45.09ms
step:1015/1920 train_time:45781ms step_avg:45.10ms
step:1016/1920 train_time:45845ms step_avg:45.12ms
step:1017/1920 train_time:45910ms step_avg:45.14ms
step:1018/1920 train_time:45974ms step_avg:45.16ms
step:1019/1920 train_time:46037ms step_avg:45.18ms
step:1020/1920 train_time:46099ms step_avg:45.19ms
step:1021/1920 train_time:46161ms step_avg:45.21ms
step:1022/1920 train_time:46223ms step_avg:45.23ms
step:1023/1920 train_time:46285ms step_avg:45.24ms
step:1024/1920 train_time:46348ms step_avg:45.26ms
step:1025/1920 train_time:46411ms step_avg:45.28ms
step:1026/1920 train_time:46473ms step_avg:45.30ms
step:1027/1920 train_time:46536ms step_avg:45.31ms
step:1028/1920 train_time:46598ms step_avg:45.33ms
step:1029/1920 train_time:46660ms step_avg:45.35ms
step:1030/1920 train_time:46721ms step_avg:45.36ms
step:1031/1920 train_time:46784ms step_avg:45.38ms
step:1032/1920 train_time:46847ms step_avg:45.39ms
step:1033/1920 train_time:46911ms step_avg:45.41ms
step:1034/1920 train_time:46973ms step_avg:45.43ms
step:1035/1920 train_time:47037ms step_avg:45.45ms
step:1036/1920 train_time:47098ms step_avg:45.46ms
step:1037/1920 train_time:47161ms step_avg:45.48ms
step:1038/1920 train_time:47223ms step_avg:45.49ms
step:1039/1920 train_time:47285ms step_avg:45.51ms
step:1040/1920 train_time:47347ms step_avg:45.53ms
step:1041/1920 train_time:47410ms step_avg:45.54ms
step:1042/1920 train_time:47471ms step_avg:45.56ms
step:1043/1920 train_time:47534ms step_avg:45.57ms
step:1044/1920 train_time:47595ms step_avg:45.59ms
step:1045/1920 train_time:47657ms step_avg:45.61ms
step:1046/1920 train_time:47719ms step_avg:45.62ms
step:1047/1920 train_time:47781ms step_avg:45.64ms
step:1048/1920 train_time:47843ms step_avg:45.65ms
step:1049/1920 train_time:47906ms step_avg:45.67ms
step:1050/1920 train_time:47969ms step_avg:45.69ms
step:1051/1920 train_time:48032ms step_avg:45.70ms
step:1052/1920 train_time:48094ms step_avg:45.72ms
step:1053/1920 train_time:48157ms step_avg:45.73ms
step:1054/1920 train_time:48219ms step_avg:45.75ms
step:1055/1920 train_time:48281ms step_avg:45.76ms
step:1056/1920 train_time:48343ms step_avg:45.78ms
step:1057/1920 train_time:48406ms step_avg:45.80ms
step:1058/1920 train_time:48468ms step_avg:45.81ms
step:1059/1920 train_time:48532ms step_avg:45.83ms
step:1060/1920 train_time:48593ms step_avg:45.84ms
step:1061/1920 train_time:48656ms step_avg:45.86ms
step:1062/1920 train_time:48718ms step_avg:45.87ms
step:1063/1920 train_time:48780ms step_avg:45.89ms
step:1064/1920 train_time:48842ms step_avg:45.90ms
step:1065/1920 train_time:48905ms step_avg:45.92ms
step:1066/1920 train_time:48967ms step_avg:45.94ms
step:1067/1920 train_time:49031ms step_avg:45.95ms
step:1068/1920 train_time:49093ms step_avg:45.97ms
step:1069/1920 train_time:49155ms step_avg:45.98ms
step:1070/1920 train_time:49217ms step_avg:46.00ms
step:1071/1920 train_time:49280ms step_avg:46.01ms
step:1072/1920 train_time:49342ms step_avg:46.03ms
step:1073/1920 train_time:49404ms step_avg:46.04ms
step:1074/1920 train_time:49466ms step_avg:46.06ms
step:1075/1920 train_time:49530ms step_avg:46.07ms
step:1076/1920 train_time:49592ms step_avg:46.09ms
step:1077/1920 train_time:49655ms step_avg:46.11ms
step:1078/1920 train_time:49717ms step_avg:46.12ms
step:1079/1920 train_time:49779ms step_avg:46.13ms
step:1080/1920 train_time:49840ms step_avg:46.15ms
step:1081/1920 train_time:49903ms step_avg:46.16ms
step:1082/1920 train_time:49965ms step_avg:46.18ms
step:1083/1920 train_time:50028ms step_avg:46.19ms
step:1084/1920 train_time:50090ms step_avg:46.21ms
step:1085/1920 train_time:50154ms step_avg:46.22ms
step:1086/1920 train_time:50215ms step_avg:46.24ms
step:1087/1920 train_time:50278ms step_avg:46.25ms
step:1088/1920 train_time:50340ms step_avg:46.27ms
step:1089/1920 train_time:50404ms step_avg:46.28ms
step:1090/1920 train_time:50466ms step_avg:46.30ms
step:1091/1920 train_time:50528ms step_avg:46.31ms
step:1092/1920 train_time:50590ms step_avg:46.33ms
step:1093/1920 train_time:50654ms step_avg:46.34ms
step:1094/1920 train_time:50715ms step_avg:46.36ms
step:1095/1920 train_time:50777ms step_avg:46.37ms
step:1096/1920 train_time:50839ms step_avg:46.39ms
step:1097/1920 train_time:50901ms step_avg:46.40ms
step:1098/1920 train_time:50963ms step_avg:46.41ms
step:1099/1920 train_time:51026ms step_avg:46.43ms
step:1100/1920 train_time:51088ms step_avg:46.44ms
step:1101/1920 train_time:51153ms step_avg:46.46ms
step:1102/1920 train_time:51214ms step_avg:46.47ms
step:1103/1920 train_time:51278ms step_avg:46.49ms
step:1104/1920 train_time:51339ms step_avg:46.50ms
step:1105/1920 train_time:51402ms step_avg:46.52ms
step:1106/1920 train_time:51464ms step_avg:46.53ms
step:1107/1920 train_time:51526ms step_avg:46.55ms
step:1108/1920 train_time:51589ms step_avg:46.56ms
step:1109/1920 train_time:51652ms step_avg:46.58ms
step:1110/1920 train_time:51714ms step_avg:46.59ms
step:1111/1920 train_time:51777ms step_avg:46.60ms
step:1112/1920 train_time:51839ms step_avg:46.62ms
step:1113/1920 train_time:51901ms step_avg:46.63ms
step:1114/1920 train_time:51964ms step_avg:46.65ms
step:1115/1920 train_time:52026ms step_avg:46.66ms
step:1116/1920 train_time:52089ms step_avg:46.67ms
step:1117/1920 train_time:52153ms step_avg:46.69ms
step:1118/1920 train_time:52215ms step_avg:46.70ms
step:1119/1920 train_time:52278ms step_avg:46.72ms
step:1120/1920 train_time:52339ms step_avg:46.73ms
step:1121/1920 train_time:52401ms step_avg:46.74ms
step:1122/1920 train_time:52463ms step_avg:46.76ms
step:1123/1920 train_time:52526ms step_avg:46.77ms
step:1124/1920 train_time:52588ms step_avg:46.79ms
step:1125/1920 train_time:52652ms step_avg:46.80ms
step:1126/1920 train_time:52714ms step_avg:46.82ms
step:1127/1920 train_time:52777ms step_avg:46.83ms
step:1128/1920 train_time:52838ms step_avg:46.84ms
step:1129/1920 train_time:52900ms step_avg:46.86ms
step:1130/1920 train_time:52962ms step_avg:46.87ms
step:1131/1920 train_time:53025ms step_avg:46.88ms
step:1132/1920 train_time:53087ms step_avg:46.90ms
step:1133/1920 train_time:53151ms step_avg:46.91ms
step:1134/1920 train_time:53213ms step_avg:46.93ms
step:1135/1920 train_time:53276ms step_avg:46.94ms
step:1136/1920 train_time:53338ms step_avg:46.95ms
step:1137/1920 train_time:53401ms step_avg:46.97ms
step:1138/1920 train_time:53463ms step_avg:46.98ms
step:1139/1920 train_time:53525ms step_avg:46.99ms
step:1140/1920 train_time:53587ms step_avg:47.01ms
step:1141/1920 train_time:53651ms step_avg:47.02ms
step:1142/1920 train_time:53713ms step_avg:47.03ms
step:1143/1920 train_time:53776ms step_avg:47.05ms
step:1144/1920 train_time:53837ms step_avg:47.06ms
step:1145/1920 train_time:53899ms step_avg:47.07ms
step:1146/1920 train_time:53961ms step_avg:47.09ms
step:1147/1920 train_time:54024ms step_avg:47.10ms
step:1148/1920 train_time:54086ms step_avg:47.11ms
step:1149/1920 train_time:54150ms step_avg:47.13ms
step:1150/1920 train_time:54212ms step_avg:47.14ms
step:1151/1920 train_time:54275ms step_avg:47.15ms
step:1152/1920 train_time:54337ms step_avg:47.17ms
step:1153/1920 train_time:54399ms step_avg:47.18ms
step:1154/1920 train_time:54461ms step_avg:47.19ms
step:1155/1920 train_time:54523ms step_avg:47.21ms
step:1156/1920 train_time:54585ms step_avg:47.22ms
step:1157/1920 train_time:54649ms step_avg:47.23ms
step:1158/1920 train_time:54711ms step_avg:47.25ms
step:1159/1920 train_time:54774ms step_avg:47.26ms
step:1160/1920 train_time:54836ms step_avg:47.27ms
step:1161/1920 train_time:54898ms step_avg:47.29ms
step:1162/1920 train_time:54960ms step_avg:47.30ms
step:1163/1920 train_time:55023ms step_avg:47.31ms
step:1164/1920 train_time:55085ms step_avg:47.32ms
step:1165/1920 train_time:55148ms step_avg:47.34ms
step:1166/1920 train_time:55211ms step_avg:47.35ms
step:1167/1920 train_time:55274ms step_avg:47.36ms
step:1168/1920 train_time:55336ms step_avg:47.38ms
step:1169/1920 train_time:55399ms step_avg:47.39ms
step:1170/1920 train_time:55461ms step_avg:47.40ms
step:1171/1920 train_time:55524ms step_avg:47.42ms
step:1172/1920 train_time:55585ms step_avg:47.43ms
step:1173/1920 train_time:55648ms step_avg:47.44ms
step:1174/1920 train_time:55711ms step_avg:47.45ms
step:1175/1920 train_time:55774ms step_avg:47.47ms
step:1176/1920 train_time:55836ms step_avg:47.48ms
step:1177/1920 train_time:55899ms step_avg:47.49ms
step:1178/1920 train_time:55960ms step_avg:47.50ms
step:1179/1920 train_time:56023ms step_avg:47.52ms
step:1180/1920 train_time:56085ms step_avg:47.53ms
step:1181/1920 train_time:56148ms step_avg:47.54ms
step:1182/1920 train_time:56210ms step_avg:47.56ms
step:1183/1920 train_time:56273ms step_avg:47.57ms
step:1184/1920 train_time:56336ms step_avg:47.58ms
step:1185/1920 train_time:56399ms step_avg:47.59ms
step:1186/1920 train_time:56460ms step_avg:47.61ms
step:1187/1920 train_time:56523ms step_avg:47.62ms
step:1188/1920 train_time:56585ms step_avg:47.63ms
step:1189/1920 train_time:56648ms step_avg:47.64ms
step:1190/1920 train_time:56710ms step_avg:47.66ms
step:1191/1920 train_time:56774ms step_avg:47.67ms
step:1192/1920 train_time:56836ms step_avg:47.68ms
step:1193/1920 train_time:56899ms step_avg:47.69ms
step:1194/1920 train_time:56961ms step_avg:47.71ms
step:1195/1920 train_time:57023ms step_avg:47.72ms
step:1196/1920 train_time:57085ms step_avg:47.73ms
step:1197/1920 train_time:57148ms step_avg:47.74ms
step:1198/1920 train_time:57210ms step_avg:47.75ms
step:1199/1920 train_time:57274ms step_avg:47.77ms
step:1200/1920 train_time:57336ms step_avg:47.78ms
step:1201/1920 train_time:57399ms step_avg:47.79ms
step:1202/1920 train_time:57461ms step_avg:47.80ms
step:1203/1920 train_time:57524ms step_avg:47.82ms
step:1204/1920 train_time:57585ms step_avg:47.83ms
step:1205/1920 train_time:57648ms step_avg:47.84ms
step:1206/1920 train_time:57710ms step_avg:47.85ms
step:1207/1920 train_time:57774ms step_avg:47.87ms
step:1208/1920 train_time:57836ms step_avg:47.88ms
step:1209/1920 train_time:57899ms step_avg:47.89ms
step:1210/1920 train_time:57961ms step_avg:47.90ms
step:1211/1920 train_time:58023ms step_avg:47.91ms
step:1212/1920 train_time:58085ms step_avg:47.93ms
step:1213/1920 train_time:58148ms step_avg:47.94ms
step:1214/1920 train_time:58210ms step_avg:47.95ms
step:1215/1920 train_time:58273ms step_avg:47.96ms
step:1216/1920 train_time:58335ms step_avg:47.97ms
step:1217/1920 train_time:58398ms step_avg:47.99ms
step:1218/1920 train_time:58460ms step_avg:48.00ms
step:1219/1920 train_time:58522ms step_avg:48.01ms
step:1220/1920 train_time:58584ms step_avg:48.02ms
step:1221/1920 train_time:58646ms step_avg:48.03ms
step:1222/1920 train_time:58709ms step_avg:48.04ms
step:1223/1920 train_time:58773ms step_avg:48.06ms
step:1224/1920 train_time:58835ms step_avg:48.07ms
step:1225/1920 train_time:58898ms step_avg:48.08ms
step:1226/1920 train_time:58960ms step_avg:48.09ms
step:1227/1920 train_time:59022ms step_avg:48.10ms
step:1228/1920 train_time:59084ms step_avg:48.11ms
step:1229/1920 train_time:59147ms step_avg:48.13ms
step:1230/1920 train_time:59209ms step_avg:48.14ms
step:1231/1920 train_time:59272ms step_avg:48.15ms
step:1232/1920 train_time:59335ms step_avg:48.16ms
step:1233/1920 train_time:59397ms step_avg:48.17ms
step:1234/1920 train_time:59458ms step_avg:48.18ms
step:1235/1920 train_time:59520ms step_avg:48.19ms
step:1236/1920 train_time:59582ms step_avg:48.21ms
step:1237/1920 train_time:59645ms step_avg:48.22ms
step:1238/1920 train_time:59707ms step_avg:48.23ms
step:1239/1920 train_time:59770ms step_avg:48.24ms
step:1240/1920 train_time:59832ms step_avg:48.25ms
step:1241/1920 train_time:59895ms step_avg:48.26ms
step:1242/1920 train_time:59956ms step_avg:48.27ms
step:1243/1920 train_time:60019ms step_avg:48.29ms
step:1244/1920 train_time:60081ms step_avg:48.30ms
step:1245/1920 train_time:60144ms step_avg:48.31ms
step:1246/1920 train_time:60205ms step_avg:48.32ms
step:1247/1920 train_time:60268ms step_avg:48.33ms
step:1248/1920 train_time:60331ms step_avg:48.34ms
step:1249/1920 train_time:60395ms step_avg:48.35ms
step:1250/1920 train_time:60456ms step_avg:48.36ms
step:1250/1920 val_loss:3.5535 train_time:60521ms step_avg:48.42ms
step:1251/1920 train_time:60538ms step_avg:48.39ms
step:1252/1920 train_time:60583ms step_avg:48.39ms
step:1253/1920 train_time:60647ms step_avg:48.40ms
step:1254/1920 train_time:60709ms step_avg:48.41ms
step:1255/1920 train_time:60772ms step_avg:48.42ms
step:1256/1920 train_time:60860ms step_avg:48.46ms
step:1257/1920 train_time:60949ms step_avg:48.49ms
step:1258/1920 train_time:61036ms step_avg:48.52ms
step:1259/1920 train_time:61125ms step_avg:48.55ms
step:1260/1920 train_time:61212ms step_avg:48.58ms
step:1261/1920 train_time:61301ms step_avg:48.61ms
step:1262/1920 train_time:61388ms step_avg:48.64ms
step:1263/1920 train_time:61480ms step_avg:48.68ms
step:1264/1920 train_time:61572ms step_avg:48.71ms
step:1265/1920 train_time:61661ms step_avg:48.74ms
step:1266/1920 train_time:61749ms step_avg:48.77ms
step:1267/1920 train_time:61838ms step_avg:48.81ms
step:1268/1920 train_time:61926ms step_avg:48.84ms
step:1269/1920 train_time:62014ms step_avg:48.87ms
step:1270/1920 train_time:62101ms step_avg:48.90ms
step:1271/1920 train_time:62189ms step_avg:48.93ms
step:1272/1920 train_time:62276ms step_avg:48.96ms
step:1273/1920 train_time:62366ms step_avg:48.99ms
step:1274/1920 train_time:62454ms step_avg:49.02ms
step:1275/1920 train_time:62545ms step_avg:49.06ms
step:1276/1920 train_time:62633ms step_avg:49.09ms
step:1277/1920 train_time:62722ms step_avg:49.12ms
step:1278/1920 train_time:62810ms step_avg:49.15ms
step:1279/1920 train_time:62898ms step_avg:49.18ms
step:1280/1920 train_time:62986ms step_avg:49.21ms
step:1281/1920 train_time:63074ms step_avg:49.24ms
step:1282/1920 train_time:63161ms step_avg:49.27ms
step:1283/1920 train_time:63250ms step_avg:49.30ms
step:1284/1920 train_time:63338ms step_avg:49.33ms
step:1285/1920 train_time:63429ms step_avg:49.36ms
step:1286/1920 train_time:63520ms step_avg:49.39ms
step:1287/1920 train_time:63610ms step_avg:49.43ms
step:1288/1920 train_time:63700ms step_avg:49.46ms
step:1289/1920 train_time:63789ms step_avg:49.49ms
step:1290/1920 train_time:63876ms step_avg:49.52ms
step:1291/1920 train_time:63965ms step_avg:49.55ms
step:1292/1920 train_time:64052ms step_avg:49.58ms
step:1293/1920 train_time:64141ms step_avg:49.61ms
step:1294/1920 train_time:64229ms step_avg:49.64ms
step:1295/1920 train_time:64317ms step_avg:49.67ms
step:1296/1920 train_time:64405ms step_avg:49.70ms
step:1297/1920 train_time:64495ms step_avg:49.73ms
step:1298/1920 train_time:64585ms step_avg:49.76ms
step:1299/1920 train_time:64673ms step_avg:49.79ms
step:1300/1920 train_time:64761ms step_avg:49.82ms
step:1301/1920 train_time:64850ms step_avg:49.85ms
step:1302/1920 train_time:64938ms step_avg:49.88ms
step:1303/1920 train_time:65027ms step_avg:49.91ms
step:1304/1920 train_time:65114ms step_avg:49.93ms
step:1305/1920 train_time:65203ms step_avg:49.96ms
step:1306/1920 train_time:65290ms step_avg:49.99ms
step:1307/1920 train_time:65379ms step_avg:50.02ms
step:1308/1920 train_time:65467ms step_avg:50.05ms
step:1309/1920 train_time:65556ms step_avg:50.08ms
step:1310/1920 train_time:65646ms step_avg:50.11ms
step:1311/1920 train_time:65734ms step_avg:50.14ms
step:1312/1920 train_time:65823ms step_avg:50.17ms
step:1313/1920 train_time:65912ms step_avg:50.20ms
step:1314/1920 train_time:66001ms step_avg:50.23ms
step:1315/1920 train_time:66089ms step_avg:50.26ms
step:1316/1920 train_time:66177ms step_avg:50.29ms
step:1317/1920 train_time:66266ms step_avg:50.32ms
step:1318/1920 train_time:66353ms step_avg:50.34ms
step:1319/1920 train_time:66442ms step_avg:50.37ms
step:1320/1920 train_time:66530ms step_avg:50.40ms
step:1321/1920 train_time:66619ms step_avg:50.43ms
step:1322/1920 train_time:66708ms step_avg:50.46ms
step:1323/1920 train_time:66796ms step_avg:50.49ms
step:1324/1920 train_time:66886ms step_avg:50.52ms
step:1325/1920 train_time:66973ms step_avg:50.55ms
step:1326/1920 train_time:67061ms step_avg:50.57ms
step:1327/1920 train_time:67150ms step_avg:50.60ms
step:1328/1920 train_time:67238ms step_avg:50.63ms
step:1329/1920 train_time:67329ms step_avg:50.66ms
step:1330/1920 train_time:67418ms step_avg:50.69ms
step:1331/1920 train_time:67506ms step_avg:50.72ms
step:1332/1920 train_time:67594ms step_avg:50.75ms
step:1333/1920 train_time:67683ms step_avg:50.77ms
step:1334/1920 train_time:67770ms step_avg:50.80ms
step:1335/1920 train_time:67859ms step_avg:50.83ms
step:1336/1920 train_time:67948ms step_avg:50.86ms
step:1337/1920 train_time:68036ms step_avg:50.89ms
step:1338/1920 train_time:68125ms step_avg:50.92ms
step:1339/1920 train_time:68213ms step_avg:50.94ms
step:1340/1920 train_time:68302ms step_avg:50.97ms
step:1341/1920 train_time:68393ms step_avg:51.00ms
step:1342/1920 train_time:68482ms step_avg:51.03ms
step:1343/1920 train_time:68571ms step_avg:51.06ms
step:1344/1920 train_time:68660ms step_avg:51.09ms
step:1345/1920 train_time:68749ms step_avg:51.11ms
step:1346/1920 train_time:68837ms step_avg:51.14ms
step:1347/1920 train_time:68926ms step_avg:51.17ms
step:1348/1920 train_time:69014ms step_avg:51.20ms
step:1349/1920 train_time:69103ms step_avg:51.23ms
step:1350/1920 train_time:69191ms step_avg:51.25ms
step:1351/1920 train_time:69279ms step_avg:51.28ms
step:1352/1920 train_time:69368ms step_avg:51.31ms
step:1353/1920 train_time:69457ms step_avg:51.34ms
step:1354/1920 train_time:69546ms step_avg:51.36ms
step:1355/1920 train_time:69635ms step_avg:51.39ms
step:1356/1920 train_time:69724ms step_avg:51.42ms
step:1357/1920 train_time:69812ms step_avg:51.45ms
step:1358/1920 train_time:69900ms step_avg:51.47ms
step:1359/1920 train_time:69991ms step_avg:51.50ms
step:1360/1920 train_time:70080ms step_avg:51.53ms
step:1361/1920 train_time:70169ms step_avg:51.56ms
step:1362/1920 train_time:70256ms step_avg:51.58ms
step:1363/1920 train_time:70345ms step_avg:51.61ms
step:1364/1920 train_time:70433ms step_avg:51.64ms
step:1365/1920 train_time:70522ms step_avg:51.66ms
step:1366/1920 train_time:70609ms step_avg:51.69ms
step:1367/1920 train_time:70698ms step_avg:51.72ms
step:1368/1920 train_time:70787ms step_avg:51.74ms
step:1369/1920 train_time:70875ms step_avg:51.77ms
step:1370/1920 train_time:70963ms step_avg:51.80ms
step:1371/1920 train_time:71052ms step_avg:51.82ms
step:1372/1920 train_time:71140ms step_avg:51.85ms
step:1373/1920 train_time:71230ms step_avg:51.88ms
step:1374/1920 train_time:71319ms step_avg:51.91ms
step:1375/1920 train_time:71408ms step_avg:51.93ms
step:1376/1920 train_time:71496ms step_avg:51.96ms
step:1377/1920 train_time:71585ms step_avg:51.99ms
step:1378/1920 train_time:71673ms step_avg:52.01ms
step:1379/1920 train_time:71761ms step_avg:52.04ms
step:1380/1920 train_time:71850ms step_avg:52.07ms
step:1381/1920 train_time:71940ms step_avg:52.09ms
step:1382/1920 train_time:72028ms step_avg:52.12ms
step:1383/1920 train_time:72116ms step_avg:52.14ms
step:1384/1920 train_time:72205ms step_avg:52.17ms
step:1385/1920 train_time:72293ms step_avg:52.20ms
step:1386/1920 train_time:72381ms step_avg:52.22ms
step:1387/1920 train_time:72471ms step_avg:52.25ms
step:1388/1920 train_time:72560ms step_avg:52.28ms
step:1389/1920 train_time:72649ms step_avg:52.30ms
step:1390/1920 train_time:72737ms step_avg:52.33ms
step:1391/1920 train_time:72828ms step_avg:52.36ms
step:1392/1920 train_time:72915ms step_avg:52.38ms
step:1393/1920 train_time:73004ms step_avg:52.41ms
step:1394/1920 train_time:73091ms step_avg:52.43ms
step:1395/1920 train_time:73180ms step_avg:52.46ms
step:1396/1920 train_time:73267ms step_avg:52.48ms
step:1397/1920 train_time:73355ms step_avg:52.51ms
step:1398/1920 train_time:73444ms step_avg:52.53ms
step:1399/1920 train_time:73533ms step_avg:52.56ms
step:1400/1920 train_time:73622ms step_avg:52.59ms
step:1401/1920 train_time:73710ms step_avg:52.61ms
step:1402/1920 train_time:73799ms step_avg:52.64ms
step:1403/1920 train_time:73889ms step_avg:52.67ms
step:1404/1920 train_time:73977ms step_avg:52.69ms
step:1405/1920 train_time:74065ms step_avg:52.72ms
step:1406/1920 train_time:74152ms step_avg:52.74ms
step:1407/1920 train_time:74241ms step_avg:52.77ms
step:1408/1920 train_time:74329ms step_avg:52.79ms
step:1409/1920 train_time:74417ms step_avg:52.82ms
step:1410/1920 train_time:74505ms step_avg:52.84ms
step:1411/1920 train_time:74593ms step_avg:52.87ms
step:1412/1920 train_time:74681ms step_avg:52.89ms
step:1413/1920 train_time:74771ms step_avg:52.92ms
step:1414/1920 train_time:74859ms step_avg:52.94ms
step:1415/1920 train_time:74950ms step_avg:52.97ms
step:1416/1920 train_time:75038ms step_avg:52.99ms
step:1417/1920 train_time:75127ms step_avg:53.02ms
step:1418/1920 train_time:75215ms step_avg:53.04ms
step:1419/1920 train_time:75304ms step_avg:53.07ms
step:1420/1920 train_time:75391ms step_avg:53.09ms
step:1421/1920 train_time:75480ms step_avg:53.12ms
step:1422/1920 train_time:75569ms step_avg:53.14ms
step:1423/1920 train_time:75659ms step_avg:53.17ms
step:1424/1920 train_time:75747ms step_avg:53.19ms
step:1425/1920 train_time:75837ms step_avg:53.22ms
step:1426/1920 train_time:75925ms step_avg:53.24ms
step:1427/1920 train_time:76014ms step_avg:53.27ms
step:1428/1920 train_time:76102ms step_avg:53.29ms
step:1429/1920 train_time:76191ms step_avg:53.32ms
step:1430/1920 train_time:76279ms step_avg:53.34ms
step:1431/1920 train_time:76368ms step_avg:53.37ms
step:1432/1920 train_time:76455ms step_avg:53.39ms
step:1433/1920 train_time:76543ms step_avg:53.41ms
step:1434/1920 train_time:76631ms step_avg:53.44ms
step:1435/1920 train_time:76721ms step_avg:53.46ms
step:1436/1920 train_time:76809ms step_avg:53.49ms
step:1437/1920 train_time:76898ms step_avg:53.51ms
step:1438/1920 train_time:76986ms step_avg:53.54ms
step:1439/1920 train_time:77076ms step_avg:53.56ms
step:1440/1920 train_time:77164ms step_avg:53.59ms
step:1441/1920 train_time:77254ms step_avg:53.61ms
step:1442/1920 train_time:77342ms step_avg:53.64ms
step:1443/1920 train_time:77432ms step_avg:53.66ms
step:1444/1920 train_time:77519ms step_avg:53.68ms
step:1445/1920 train_time:77608ms step_avg:53.71ms
step:1446/1920 train_time:77697ms step_avg:53.73ms
step:1447/1920 train_time:77786ms step_avg:53.76ms
step:1448/1920 train_time:77874ms step_avg:53.78ms
step:1449/1920 train_time:77963ms step_avg:53.80ms
step:1450/1920 train_time:78051ms step_avg:53.83ms
step:1451/1920 train_time:78139ms step_avg:53.85ms
step:1452/1920 train_time:78228ms step_avg:53.88ms
step:1453/1920 train_time:78317ms step_avg:53.90ms
step:1454/1920 train_time:78406ms step_avg:53.92ms
step:1455/1920 train_time:78495ms step_avg:53.95ms
step:1456/1920 train_time:78583ms step_avg:53.97ms
step:1457/1920 train_time:78671ms step_avg:54.00ms
step:1458/1920 train_time:78759ms step_avg:54.02ms
step:1459/1920 train_time:78849ms step_avg:54.04ms
step:1460/1920 train_time:78937ms step_avg:54.07ms
step:1461/1920 train_time:79027ms step_avg:54.09ms
step:1462/1920 train_time:79115ms step_avg:54.11ms
step:1463/1920 train_time:79204ms step_avg:54.14ms
step:1464/1920 train_time:79291ms step_avg:54.16ms
step:1465/1920 train_time:79380ms step_avg:54.18ms
step:1466/1920 train_time:79469ms step_avg:54.21ms
step:1467/1920 train_time:79558ms step_avg:54.23ms
step:1468/1920 train_time:79646ms step_avg:54.25ms
step:1469/1920 train_time:79734ms step_avg:54.28ms
step:1470/1920 train_time:79823ms step_avg:54.30ms
step:1471/1920 train_time:79913ms step_avg:54.33ms
step:1472/1920 train_time:80001ms step_avg:54.35ms
step:1473/1920 train_time:80090ms step_avg:54.37ms
step:1474/1920 train_time:80177ms step_avg:54.39ms
step:1475/1920 train_time:80267ms step_avg:54.42ms
step:1476/1920 train_time:80355ms step_avg:54.44ms
step:1477/1920 train_time:80444ms step_avg:54.46ms
step:1478/1920 train_time:80532ms step_avg:54.49ms
step:1479/1920 train_time:80621ms step_avg:54.51ms
step:1480/1920 train_time:80709ms step_avg:54.53ms
step:1481/1920 train_time:80797ms step_avg:54.56ms
step:1482/1920 train_time:80885ms step_avg:54.58ms
step:1483/1920 train_time:80973ms step_avg:54.60ms
step:1484/1920 train_time:81062ms step_avg:54.62ms
step:1485/1920 train_time:81151ms step_avg:54.65ms
step:1486/1920 train_time:81239ms step_avg:54.67ms
step:1487/1920 train_time:81329ms step_avg:54.69ms
step:1488/1920 train_time:81417ms step_avg:54.72ms
step:1489/1920 train_time:81507ms step_avg:54.74ms
step:1490/1920 train_time:81594ms step_avg:54.76ms
step:1491/1920 train_time:81682ms step_avg:54.78ms
step:1492/1920 train_time:81770ms step_avg:54.81ms
step:1493/1920 train_time:81858ms step_avg:54.83ms
step:1494/1920 train_time:81948ms step_avg:54.85ms
step:1495/1920 train_time:82037ms step_avg:54.87ms
step:1496/1920 train_time:82125ms step_avg:54.90ms
step:1497/1920 train_time:82213ms step_avg:54.92ms
step:1498/1920 train_time:82302ms step_avg:54.94ms
step:1499/1920 train_time:82391ms step_avg:54.96ms
step:1500/1920 train_time:82480ms step_avg:54.99ms
step:1500/1920 val_loss:3.4157 train_time:82573ms step_avg:55.05ms
step:1501/1920 train_time:82591ms step_avg:55.02ms
step:1502/1920 train_time:82663ms step_avg:55.04ms
step:1503/1920 train_time:82754ms step_avg:55.06ms
step:1504/1920 train_time:82841ms step_avg:55.08ms
step:1505/1920 train_time:82929ms step_avg:55.10ms
step:1506/1920 train_time:83017ms step_avg:55.12ms
step:1507/1920 train_time:83104ms step_avg:55.15ms
step:1508/1920 train_time:83192ms step_avg:55.17ms
step:1509/1920 train_time:83281ms step_avg:55.19ms
step:1510/1920 train_time:83368ms step_avg:55.21ms
step:1511/1920 train_time:83457ms step_avg:55.23ms
step:1512/1920 train_time:83547ms step_avg:55.26ms
step:1513/1920 train_time:83637ms step_avg:55.28ms
step:1514/1920 train_time:83727ms step_avg:55.30ms
step:1515/1920 train_time:83818ms step_avg:55.33ms
step:1516/1920 train_time:83905ms step_avg:55.35ms
step:1517/1920 train_time:83993ms step_avg:55.37ms
step:1518/1920 train_time:84081ms step_avg:55.39ms
step:1519/1920 train_time:84169ms step_avg:55.41ms
step:1520/1920 train_time:84258ms step_avg:55.43ms
step:1521/1920 train_time:84346ms step_avg:55.45ms
step:1522/1920 train_time:84433ms step_avg:55.48ms
step:1523/1920 train_time:84523ms step_avg:55.50ms
step:1524/1920 train_time:84611ms step_avg:55.52ms
step:1525/1920 train_time:84702ms step_avg:55.54ms
step:1526/1920 train_time:84790ms step_avg:55.56ms
step:1527/1920 train_time:84879ms step_avg:55.59ms
step:1528/1920 train_time:84966ms step_avg:55.61ms
step:1529/1920 train_time:85054ms step_avg:55.63ms
step:1530/1920 train_time:85143ms step_avg:55.65ms
step:1531/1920 train_time:85231ms step_avg:55.67ms
step:1532/1920 train_time:85319ms step_avg:55.69ms
step:1533/1920 train_time:85408ms step_avg:55.71ms
step:1534/1920 train_time:85496ms step_avg:55.73ms
step:1535/1920 train_time:85587ms step_avg:55.76ms
step:1536/1920 train_time:85676ms step_avg:55.78ms
step:1537/1920 train_time:85765ms step_avg:55.80ms
step:1538/1920 train_time:85854ms step_avg:55.82ms
step:1539/1920 train_time:85943ms step_avg:55.84ms
step:1540/1920 train_time:86031ms step_avg:55.86ms
step:1541/1920 train_time:86121ms step_avg:55.89ms
step:1542/1920 train_time:86208ms step_avg:55.91ms
step:1543/1920 train_time:86297ms step_avg:55.93ms
step:1544/1920 train_time:86384ms step_avg:55.95ms
step:1545/1920 train_time:86472ms step_avg:55.97ms
step:1546/1920 train_time:86562ms step_avg:55.99ms
step:1547/1920 train_time:86651ms step_avg:56.01ms
step:1548/1920 train_time:86740ms step_avg:56.03ms
step:1549/1920 train_time:86829ms step_avg:56.05ms
step:1550/1920 train_time:86917ms step_avg:56.08ms
step:1551/1920 train_time:87007ms step_avg:56.10ms
step:1552/1920 train_time:87095ms step_avg:56.12ms
step:1553/1920 train_time:87184ms step_avg:56.14ms
step:1554/1920 train_time:87272ms step_avg:56.16ms
step:1555/1920 train_time:87361ms step_avg:56.18ms
step:1556/1920 train_time:87449ms step_avg:56.20ms
step:1557/1920 train_time:87538ms step_avg:56.22ms
step:1558/1920 train_time:87625ms step_avg:56.24ms
step:1559/1920 train_time:87715ms step_avg:56.26ms
step:1560/1920 train_time:87803ms step_avg:56.28ms
step:1561/1920 train_time:87892ms step_avg:56.30ms
step:1562/1920 train_time:87981ms step_avg:56.33ms
step:1563/1920 train_time:88070ms step_avg:56.35ms
step:1564/1920 train_time:88158ms step_avg:56.37ms
step:1565/1920 train_time:88247ms step_avg:56.39ms
step:1566/1920 train_time:88335ms step_avg:56.41ms
step:1567/1920 train_time:88425ms step_avg:56.43ms
step:1568/1920 train_time:88513ms step_avg:56.45ms
step:1569/1920 train_time:88603ms step_avg:56.47ms
step:1570/1920 train_time:88692ms step_avg:56.49ms
step:1571/1920 train_time:88783ms step_avg:56.51ms
step:1572/1920 train_time:88871ms step_avg:56.53ms
step:1573/1920 train_time:88961ms step_avg:56.56ms
step:1574/1920 train_time:89049ms step_avg:56.57ms
step:1575/1920 train_time:89139ms step_avg:56.60ms
step:1576/1920 train_time:89225ms step_avg:56.61ms
step:1577/1920 train_time:89313ms step_avg:56.63ms
step:1578/1920 train_time:89401ms step_avg:56.65ms
step:1579/1920 train_time:89490ms step_avg:56.67ms
step:1580/1920 train_time:89577ms step_avg:56.69ms
step:1581/1920 train_time:89667ms step_avg:56.72ms
step:1582/1920 train_time:89756ms step_avg:56.74ms
step:1583/1920 train_time:89846ms step_avg:56.76ms
step:1584/1920 train_time:89934ms step_avg:56.78ms
step:1585/1920 train_time:90024ms step_avg:56.80ms
step:1586/1920 train_time:90112ms step_avg:56.82ms
step:1587/1920 train_time:90202ms step_avg:56.84ms
step:1588/1920 train_time:90290ms step_avg:56.86ms
step:1589/1920 train_time:90379ms step_avg:56.88ms
step:1590/1920 train_time:90467ms step_avg:56.90ms
step:1591/1920 train_time:90556ms step_avg:56.92ms
step:1592/1920 train_time:90644ms step_avg:56.94ms
step:1593/1920 train_time:90733ms step_avg:56.96ms
step:1594/1920 train_time:90821ms step_avg:56.98ms
step:1595/1920 train_time:90910ms step_avg:57.00ms
step:1596/1920 train_time:90999ms step_avg:57.02ms
step:1597/1920 train_time:91087ms step_avg:57.04ms
step:1598/1920 train_time:91177ms step_avg:57.06ms
step:1599/1920 train_time:91267ms step_avg:57.08ms
step:1600/1920 train_time:91355ms step_avg:57.10ms
step:1601/1920 train_time:91444ms step_avg:57.12ms
step:1602/1920 train_time:91532ms step_avg:57.14ms
step:1603/1920 train_time:91622ms step_avg:57.16ms
step:1604/1920 train_time:91710ms step_avg:57.18ms
step:1605/1920 train_time:91798ms step_avg:57.20ms
step:1606/1920 train_time:91886ms step_avg:57.21ms
step:1607/1920 train_time:91975ms step_avg:57.23ms
step:1608/1920 train_time:92064ms step_avg:57.25ms
step:1609/1920 train_time:92153ms step_avg:57.27ms
step:1610/1920 train_time:92242ms step_avg:57.29ms
step:1611/1920 train_time:92330ms step_avg:57.31ms
step:1612/1920 train_time:92418ms step_avg:57.33ms
step:1613/1920 train_time:92508ms step_avg:57.35ms
step:1614/1920 train_time:92598ms step_avg:57.37ms
step:1615/1920 train_time:92687ms step_avg:57.39ms
step:1616/1920 train_time:92775ms step_avg:57.41ms
step:1617/1920 train_time:92865ms step_avg:57.43ms
step:1618/1920 train_time:92955ms step_avg:57.45ms
step:1619/1920 train_time:93043ms step_avg:57.47ms
step:1620/1920 train_time:93131ms step_avg:57.49ms
step:1621/1920 train_time:93220ms step_avg:57.51ms
step:1622/1920 train_time:93308ms step_avg:57.53ms
step:1623/1920 train_time:93398ms step_avg:57.55ms
step:1624/1920 train_time:93485ms step_avg:57.56ms
step:1625/1920 train_time:93574ms step_avg:57.58ms
step:1626/1920 train_time:93663ms step_avg:57.60ms
step:1627/1920 train_time:93751ms step_avg:57.62ms
step:1628/1920 train_time:93840ms step_avg:57.64ms
step:1629/1920 train_time:93929ms step_avg:57.66ms
step:1630/1920 train_time:94017ms step_avg:57.68ms
step:1631/1920 train_time:94107ms step_avg:57.70ms
step:1632/1920 train_time:94195ms step_avg:57.72ms
step:1633/1920 train_time:94284ms step_avg:57.74ms
step:1634/1920 train_time:94372ms step_avg:57.76ms
step:1635/1920 train_time:94462ms step_avg:57.78ms
step:1636/1920 train_time:94550ms step_avg:57.79ms
step:1637/1920 train_time:94639ms step_avg:57.81ms
step:1638/1920 train_time:94727ms step_avg:57.83ms
step:1639/1920 train_time:94817ms step_avg:57.85ms
step:1640/1920 train_time:94905ms step_avg:57.87ms
step:1641/1920 train_time:94993ms step_avg:57.89ms
step:1642/1920 train_time:95081ms step_avg:57.91ms
step:1643/1920 train_time:95170ms step_avg:57.92ms
step:1644/1920 train_time:95259ms step_avg:57.94ms
step:1645/1920 train_time:95348ms step_avg:57.96ms
step:1646/1920 train_time:95436ms step_avg:57.98ms
step:1647/1920 train_time:95526ms step_avg:58.00ms
step:1648/1920 train_time:95615ms step_avg:58.02ms
step:1649/1920 train_time:95705ms step_avg:58.04ms
step:1650/1920 train_time:95794ms step_avg:58.06ms
step:1651/1920 train_time:95883ms step_avg:58.08ms
step:1652/1920 train_time:95971ms step_avg:58.09ms
step:1653/1920 train_time:96060ms step_avg:58.11ms
step:1654/1920 train_time:96148ms step_avg:58.13ms
step:1655/1920 train_time:96238ms step_avg:58.15ms
step:1656/1920 train_time:96325ms step_avg:58.17ms
step:1657/1920 train_time:96413ms step_avg:58.19ms
step:1658/1920 train_time:96502ms step_avg:58.20ms
step:1659/1920 train_time:96591ms step_avg:58.22ms
step:1660/1920 train_time:96680ms step_avg:58.24ms
step:1661/1920 train_time:96769ms step_avg:58.26ms
step:1662/1920 train_time:96859ms step_avg:58.28ms
step:1663/1920 train_time:96948ms step_avg:58.30ms
step:1664/1920 train_time:97037ms step_avg:58.32ms
step:1665/1920 train_time:97126ms step_avg:58.33ms
step:1666/1920 train_time:97214ms step_avg:58.35ms
step:1667/1920 train_time:97304ms step_avg:58.37ms
step:1668/1920 train_time:97392ms step_avg:58.39ms
step:1669/1920 train_time:97481ms step_avg:58.41ms
step:1670/1920 train_time:97569ms step_avg:58.42ms
step:1671/1920 train_time:97658ms step_avg:58.44ms
step:1672/1920 train_time:97746ms step_avg:58.46ms
step:1673/1920 train_time:97834ms step_avg:58.48ms
step:1674/1920 train_time:97922ms step_avg:58.50ms
step:1675/1920 train_time:98011ms step_avg:58.51ms
step:1676/1920 train_time:98100ms step_avg:58.53ms
step:1677/1920 train_time:98188ms step_avg:58.55ms
step:1678/1920 train_time:98278ms step_avg:58.57ms
step:1679/1920 train_time:98368ms step_avg:58.59ms
step:1680/1920 train_time:98456ms step_avg:58.60ms
step:1681/1920 train_time:98545ms step_avg:58.62ms
step:1682/1920 train_time:98635ms step_avg:58.64ms
step:1683/1920 train_time:98726ms step_avg:58.66ms
step:1684/1920 train_time:98814ms step_avg:58.68ms
step:1685/1920 train_time:98902ms step_avg:58.70ms
step:1686/1920 train_time:98991ms step_avg:58.71ms
step:1687/1920 train_time:99079ms step_avg:58.73ms
step:1688/1920 train_time:99167ms step_avg:58.75ms
step:1689/1920 train_time:99256ms step_avg:58.77ms
step:1690/1920 train_time:99344ms step_avg:58.78ms
step:1691/1920 train_time:99432ms step_avg:58.80ms
step:1692/1920 train_time:99520ms step_avg:58.82ms
step:1693/1920 train_time:99609ms step_avg:58.84ms
step:1694/1920 train_time:99698ms step_avg:58.85ms
step:1695/1920 train_time:99787ms step_avg:58.87ms
step:1696/1920 train_time:99875ms step_avg:58.89ms
step:1697/1920 train_time:99965ms step_avg:58.91ms
step:1698/1920 train_time:100054ms step_avg:58.92ms
step:1699/1920 train_time:100143ms step_avg:58.94ms
step:1700/1920 train_time:100230ms step_avg:58.96ms
step:1701/1920 train_time:100319ms step_avg:58.98ms
step:1702/1920 train_time:100407ms step_avg:58.99ms
step:1703/1920 train_time:100496ms step_avg:59.01ms
step:1704/1920 train_time:100584ms step_avg:59.03ms
step:1705/1920 train_time:100674ms step_avg:59.05ms
step:1706/1920 train_time:100763ms step_avg:59.06ms
step:1707/1920 train_time:100853ms step_avg:59.08ms
step:1708/1920 train_time:100941ms step_avg:59.10ms
step:1709/1920 train_time:101029ms step_avg:59.12ms
step:1710/1920 train_time:101117ms step_avg:59.13ms
step:1711/1920 train_time:101206ms step_avg:59.15ms
step:1712/1920 train_time:101294ms step_avg:59.17ms
step:1713/1920 train_time:101384ms step_avg:59.19ms
step:1714/1920 train_time:101473ms step_avg:59.20ms
step:1715/1920 train_time:101564ms step_avg:59.22ms
step:1716/1920 train_time:101652ms step_avg:59.24ms
step:1717/1920 train_time:101741ms step_avg:59.26ms
step:1718/1920 train_time:101829ms step_avg:59.27ms
step:1719/1920 train_time:101918ms step_avg:59.29ms
step:1720/1920 train_time:102005ms step_avg:59.31ms
step:1721/1920 train_time:102094ms step_avg:59.32ms
step:1722/1920 train_time:102182ms step_avg:59.34ms
step:1723/1920 train_time:102271ms step_avg:59.36ms
step:1724/1920 train_time:102359ms step_avg:59.37ms
step:1725/1920 train_time:102448ms step_avg:59.39ms
step:1726/1920 train_time:102536ms step_avg:59.41ms
step:1727/1920 train_time:102625ms step_avg:59.42ms
step:1728/1920 train_time:102713ms step_avg:59.44ms
step:1729/1920 train_time:102802ms step_avg:59.46ms
step:1730/1920 train_time:102889ms step_avg:59.47ms
step:1731/1920 train_time:102978ms step_avg:59.49ms
step:1732/1920 train_time:103067ms step_avg:59.51ms
step:1733/1920 train_time:103156ms step_avg:59.52ms
step:1734/1920 train_time:103244ms step_avg:59.54ms
step:1735/1920 train_time:103333ms step_avg:59.56ms
step:1736/1920 train_time:103421ms step_avg:59.57ms
step:1737/1920 train_time:103511ms step_avg:59.59ms
step:1738/1920 train_time:103600ms step_avg:59.61ms
step:1739/1920 train_time:103690ms step_avg:59.63ms
step:1740/1920 train_time:103779ms step_avg:59.64ms
step:1741/1920 train_time:103868ms step_avg:59.66ms
step:1742/1920 train_time:103957ms step_avg:59.68ms
step:1743/1920 train_time:104047ms step_avg:59.69ms
step:1744/1920 train_time:104136ms step_avg:59.71ms
step:1745/1920 train_time:104225ms step_avg:59.73ms
step:1746/1920 train_time:104312ms step_avg:59.74ms
step:1747/1920 train_time:104401ms step_avg:59.76ms
step:1748/1920 train_time:104489ms step_avg:59.78ms
step:1749/1920 train_time:104578ms step_avg:59.79ms
step:1750/1920 train_time:104667ms step_avg:59.81ms
step:1750/1920 val_loss:3.3243 train_time:104759ms step_avg:59.86ms
step:1751/1920 train_time:104778ms step_avg:59.84ms
step:1752/1920 train_time:104848ms step_avg:59.84ms
step:1753/1920 train_time:104939ms step_avg:59.86ms
step:1754/1920 train_time:105027ms step_avg:59.88ms
step:1755/1920 train_time:105116ms step_avg:59.89ms
step:1756/1920 train_time:105204ms step_avg:59.91ms
step:1757/1920 train_time:105292ms step_avg:59.93ms
step:1758/1920 train_time:105379ms step_avg:59.94ms
step:1759/1920 train_time:105467ms step_avg:59.96ms
step:1760/1920 train_time:105555ms step_avg:59.97ms
step:1761/1920 train_time:105644ms step_avg:59.99ms
step:1762/1920 train_time:105733ms step_avg:60.01ms
step:1763/1920 train_time:105826ms step_avg:60.03ms
step:1764/1920 train_time:105917ms step_avg:60.04ms
step:1765/1920 train_time:106008ms step_avg:60.06ms
step:1766/1920 train_time:106096ms step_avg:60.08ms
step:1767/1920 train_time:106185ms step_avg:60.09ms
step:1768/1920 train_time:106273ms step_avg:60.11ms
step:1769/1920 train_time:106361ms step_avg:60.12ms
step:1770/1920 train_time:106447ms step_avg:60.14ms
step:1771/1920 train_time:106534ms step_avg:60.15ms
step:1772/1920 train_time:106623ms step_avg:60.17ms
step:1773/1920 train_time:106713ms step_avg:60.19ms
step:1774/1920 train_time:106804ms step_avg:60.21ms
step:1775/1920 train_time:106895ms step_avg:60.22ms
step:1776/1920 train_time:106984ms step_avg:60.24ms
step:1777/1920 train_time:107072ms step_avg:60.25ms
step:1778/1920 train_time:107161ms step_avg:60.27ms
step:1779/1920 train_time:107249ms step_avg:60.29ms
step:1780/1920 train_time:107336ms step_avg:60.30ms
step:1781/1920 train_time:107425ms step_avg:60.32ms
step:1782/1920 train_time:107513ms step_avg:60.33ms
step:1783/1920 train_time:107603ms step_avg:60.35ms
step:1784/1920 train_time:107692ms step_avg:60.37ms
step:1785/1920 train_time:107783ms step_avg:60.38ms
step:1786/1920 train_time:107872ms step_avg:60.40ms
step:1787/1920 train_time:107961ms step_avg:60.41ms
step:1788/1920 train_time:108050ms step_avg:60.43ms
step:1789/1920 train_time:108139ms step_avg:60.45ms
step:1790/1920 train_time:108226ms step_avg:60.46ms
step:1791/1920 train_time:108314ms step_avg:60.48ms
step:1792/1920 train_time:108401ms step_avg:60.49ms
step:1793/1920 train_time:108489ms step_avg:60.51ms
step:1794/1920 train_time:108577ms step_avg:60.52ms
step:1795/1920 train_time:108667ms step_avg:60.54ms
step:1796/1920 train_time:108756ms step_avg:60.55ms
step:1797/1920 train_time:108847ms step_avg:60.57ms
step:1798/1920 train_time:108936ms step_avg:60.59ms
step:1799/1920 train_time:109026ms step_avg:60.60ms
step:1800/1920 train_time:109114ms step_avg:60.62ms
step:1801/1920 train_time:109203ms step_avg:60.63ms
step:1802/1920 train_time:109291ms step_avg:60.65ms
step:1803/1920 train_time:109379ms step_avg:60.67ms
step:1804/1920 train_time:109467ms step_avg:60.68ms
step:1805/1920 train_time:109555ms step_avg:60.70ms
step:1806/1920 train_time:109644ms step_avg:60.71ms
step:1807/1920 train_time:109734ms step_avg:60.73ms
step:1808/1920 train_time:109823ms step_avg:60.74ms
step:1809/1920 train_time:109912ms step_avg:60.76ms
step:1810/1920 train_time:110001ms step_avg:60.77ms
step:1811/1920 train_time:110091ms step_avg:60.79ms
step:1812/1920 train_time:110179ms step_avg:60.81ms
step:1813/1920 train_time:110268ms step_avg:60.82ms
step:1814/1920 train_time:110356ms step_avg:60.84ms
step:1815/1920 train_time:110446ms step_avg:60.85ms
step:1816/1920 train_time:110534ms step_avg:60.87ms
step:1817/1920 train_time:110624ms step_avg:60.88ms
step:1818/1920 train_time:110712ms step_avg:60.90ms
step:1819/1920 train_time:110801ms step_avg:60.91ms
step:1820/1920 train_time:110890ms step_avg:60.93ms
step:1821/1920 train_time:110979ms step_avg:60.94ms
step:1822/1920 train_time:111068ms step_avg:60.96ms
step:1823/1920 train_time:111157ms step_avg:60.97ms
step:1824/1920 train_time:111245ms step_avg:60.99ms
step:1825/1920 train_time:111333ms step_avg:61.00ms
step:1826/1920 train_time:111421ms step_avg:61.02ms
step:1827/1920 train_time:111510ms step_avg:61.03ms
step:1828/1920 train_time:111598ms step_avg:61.05ms
step:1829/1920 train_time:111686ms step_avg:61.06ms
step:1830/1920 train_time:111774ms step_avg:61.08ms
step:1831/1920 train_time:111864ms step_avg:61.09ms
step:1832/1920 train_time:111952ms step_avg:61.11ms
step:1833/1920 train_time:112041ms step_avg:61.12ms
step:1834/1920 train_time:112128ms step_avg:61.14ms
step:1835/1920 train_time:112217ms step_avg:61.15ms
step:1836/1920 train_time:112305ms step_avg:61.17ms
step:1837/1920 train_time:112393ms step_avg:61.18ms
step:1838/1920 train_time:112481ms step_avg:61.20ms
step:1839/1920 train_time:112570ms step_avg:61.21ms
step:1840/1920 train_time:112658ms step_avg:61.23ms
step:1841/1920 train_time:112748ms step_avg:61.24ms
step:1842/1920 train_time:112838ms step_avg:61.26ms
step:1843/1920 train_time:112927ms step_avg:61.27ms
step:1844/1920 train_time:113015ms step_avg:61.29ms
step:1845/1920 train_time:113104ms step_avg:61.30ms
step:1846/1920 train_time:113192ms step_avg:61.32ms
step:1847/1920 train_time:113282ms step_avg:61.33ms
step:1848/1920 train_time:113369ms step_avg:61.35ms
step:1849/1920 train_time:113458ms step_avg:61.36ms
step:1850/1920 train_time:113546ms step_avg:61.38ms
step:1851/1920 train_time:113635ms step_avg:61.39ms
step:1852/1920 train_time:113724ms step_avg:61.41ms
step:1853/1920 train_time:113813ms step_avg:61.42ms
step:1854/1920 train_time:113902ms step_avg:61.44ms
step:1855/1920 train_time:113991ms step_avg:61.45ms
step:1856/1920 train_time:114079ms step_avg:61.47ms
step:1857/1920 train_time:114169ms step_avg:61.48ms
step:1858/1920 train_time:114257ms step_avg:61.49ms
step:1859/1920 train_time:114346ms step_avg:61.51ms
step:1860/1920 train_time:114435ms step_avg:61.52ms
step:1861/1920 train_time:114525ms step_avg:61.54ms
step:1862/1920 train_time:114613ms step_avg:61.55ms
step:1863/1920 train_time:114702ms step_avg:61.57ms
step:1864/1920 train_time:114790ms step_avg:61.58ms
step:1865/1920 train_time:114881ms step_avg:61.60ms
step:1866/1920 train_time:114968ms step_avg:61.61ms
step:1867/1920 train_time:115057ms step_avg:61.63ms
step:1868/1920 train_time:115145ms step_avg:61.64ms
step:1869/1920 train_time:115233ms step_avg:61.66ms
step:1870/1920 train_time:115322ms step_avg:61.67ms
step:1871/1920 train_time:115411ms step_avg:61.68ms
step:1872/1920 train_time:115499ms step_avg:61.70ms
step:1873/1920 train_time:115587ms step_avg:61.71ms
step:1874/1920 train_time:115676ms step_avg:61.73ms
step:1875/1920 train_time:115767ms step_avg:61.74ms
step:1876/1920 train_time:115855ms step_avg:61.76ms
step:1877/1920 train_time:115945ms step_avg:61.77ms
step:1878/1920 train_time:116033ms step_avg:61.79ms
step:1879/1920 train_time:116122ms step_avg:61.80ms
step:1880/1920 train_time:116209ms step_avg:61.81ms
step:1881/1920 train_time:116298ms step_avg:61.83ms
step:1882/1920 train_time:116386ms step_avg:61.84ms
step:1883/1920 train_time:116475ms step_avg:61.86ms
step:1884/1920 train_time:116563ms step_avg:61.87ms
step:1885/1920 train_time:116652ms step_avg:61.88ms
step:1886/1920 train_time:116740ms step_avg:61.90ms
step:1887/1920 train_time:116830ms step_avg:61.91ms
step:1888/1920 train_time:116920ms step_avg:61.93ms
step:1889/1920 train_time:117009ms step_avg:61.94ms
step:1890/1920 train_time:117097ms step_avg:61.96ms
step:1891/1920 train_time:117186ms step_avg:61.97ms
step:1892/1920 train_time:117274ms step_avg:61.98ms
step:1893/1920 train_time:117365ms step_avg:62.00ms
step:1894/1920 train_time:117452ms step_avg:62.01ms
step:1895/1920 train_time:117542ms step_avg:62.03ms
step:1896/1920 train_time:117630ms step_avg:62.04ms
step:1897/1920 train_time:117719ms step_avg:62.06ms
step:1898/1920 train_time:117808ms step_avg:62.07ms
step:1899/1920 train_time:117897ms step_avg:62.08ms
step:1900/1920 train_time:117985ms step_avg:62.10ms
step:1901/1920 train_time:118074ms step_avg:62.11ms
step:1902/1920 train_time:118163ms step_avg:62.13ms
step:1903/1920 train_time:118252ms step_avg:62.14ms
step:1904/1920 train_time:118342ms step_avg:62.15ms
step:1905/1920 train_time:118431ms step_avg:62.17ms
step:1906/1920 train_time:118519ms step_avg:62.18ms
step:1907/1920 train_time:118609ms step_avg:62.20ms
step:1908/1920 train_time:118697ms step_avg:62.21ms
step:1909/1920 train_time:118788ms step_avg:62.23ms
step:1910/1920 train_time:118878ms step_avg:62.24ms
step:1911/1920 train_time:118967ms step_avg:62.25ms
step:1912/1920 train_time:119055ms step_avg:62.27ms
step:1913/1920 train_time:119146ms step_avg:62.28ms
step:1914/1920 train_time:119236ms step_avg:62.30ms
step:1915/1920 train_time:119327ms step_avg:62.31ms
step:1916/1920 train_time:119416ms step_avg:62.33ms
step:1917/1920 train_time:119506ms step_avg:62.34ms
step:1918/1920 train_time:119595ms step_avg:62.35ms
step:1919/1920 train_time:119684ms step_avg:62.37ms
step:1920/1920 train_time:119772ms step_avg:62.38ms
step:1920/1920 val_loss:3.2797 train_time:119865ms step_avg:62.43ms
peak memory allocated: 29817 MiB reserved: 44658 MiB
